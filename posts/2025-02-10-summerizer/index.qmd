---
date: 2025-02-10
title: "Summarization Task"
description: "Concepts, slide commentaries and Lecture notes on Automatic text Summarization by Masa Nekic"
categories: 
    - NLP     
    - Notes
    - Literature review
    - Summarization task
    - Relevance
    - Conference talk
keywords: 
    - Automatic extracting
    - Automatic abstracting
    - Cue words
    - Comparative evaluation 
    - Content words
    - Document screening
    - Key words
    - Pragmatic words
    - Parameterization
    - Research methodology
    - Sentence location
    - Sentence selection
    - Sentence significance
    - Title words
---

## Motivation

In the Deeplearning.ai NLP specialization, we implemented both **Q&A** and **Summarization tasks**. However, we only looked at these tasks in terms of the attention models. It is now time to dive deeper into the summarization task. In this article  I will review a talk covering a body of research on summarization, which has many ideas about features. Consider some of these and see what aspects are relevant to a modern task implementation.


## Ideas

Several ideas surfaced while working on the assignment for building a hybrid generative/abstractive summarizer based on GPT2^[ a Generative Pre-training Transformer or a decoder transformer]. Many ideas came from prior work developing search engines. I had noticed that some issues were anathema to summarization from its inception.  

### Coverage ranking content by importance

The first issue is **coverage**, which is how much of the original document to include. Coverage is a much more difficult problem than it appears. For example, technical documents, like patents, headings, academic writing cues, and even IR statistics, may serve as features that we may feed into a regression that can guide us in discarding the chuff from the grain. On the other hand, for movie scripts or novels, we can't use all that, and we need to decide what is essential based on the narrative structure, which requires sophisticated processing and extensive domain knowledge to extract. So, When we want to create a synopsis of a book like War and Peace, specific details are part of the narrative structure that stands out as essential to us, and I can say that even in 2025, LLM is not good at picking these out of a large document. For attentive readers with a suitable background, if we double the length of the text, they might pick additional plot points and then less significant subplots. If again we double we would, they would include more details concerning characters, their motivations, etc.

To conclude, different documents can have radically different structures and cues. These suggest different strategies for selecting and ranking the material to be included in a summary of a given length. More so, when we consider summaries of different lengths, one can see that we are talking about a hierarchal structure.

## Avoiding repetition

A second challenge that is pervasive is avoiding **repetition**. In the generative settings we have a tougher challenge since sentences that are generated may well be equivalent to sentences we have already generated, and we want the model to redirect it attention from material already covered. Luckily we have learned to detect similar sentences in the Q&A task^[using Siamese networks for oneshot similarity detection.] 


check if it is similar to any sentence in the summary

Alg1: similarity detection

```
summary = []
tokenized_doc = tokenize(doc)
embeddings_doc= embed(tokenized_doc)
while len(summary) < doc_tokens * summary_ratio: 
    a = gen_a_sentence(embeddings_doc,summary)
    for s in summary:
        if sim(a,s) > threshold:
            continue
    else:
        summary.append(a)
s -> gen a sentence,
```

alg2



## Context window constraints

The third challenge is technical and is related to the **length of the context window**. as transformer models have a limit on the number of tokens that they can process in a context window. A second issue here is that the summary itself needs to also fit in the context window. If the structure is linear and incremental we can chunk it into many smaller pieced say using sections. However in reality we are often dealing with trees of graphs structures and chunking can erode the model's ability to inspect said hierarchy of the structure it is tasked with summarizing. In Q&A or IR tasks since we can process each chunk separately and then combine the results. 

If we are not using a generative model one imagines a tree data structure that can efficiently track what what part of the document has been summarized. If we can use that to work with the attention mechanism we may be able to reduce the effective window size as we progress with the summary. A more nuanced idea would come from **bayesian search** where we may want to keep moving the probability mass for the attention mechanism to regions that are significant but have not been covered.

Another challenge is that we may want to grow our summary in a non-linear fashion. We may consider the main narrative structure then add in the subplots and then the character motivations. This suggests two approaches - a top down **planning** based approach^[using rl or dynamic programming] and a bottom up approach where we start with the most important details, then add in the less important ones. In the second case we may want to revise the initial sentences to efficiently incorporate more details as we progress.

I also interviewed with a company that told me they often worked with patents and that these documents were frequently in excess of a hundred pages and they were having lots of problems with the summarization task. This suggest that an efficient transformer^[reformer layers] would be needed if we are to support context windows that can span hundreds of pages. But that besides the context window we need some good way to ensure that the summary is balanced and that it is not too repetitive. 

Another idea I thought about which should be obvious is that the generated summary should be **compressive** i.e. we should be shorter than an alternative summary. This is another area where we diverge from Q&A where we do not have a length constraint and may often want to include all relevant information.

When  I looked for more information I found the following video which together with a review paper can provide a good intro to this subject.  I also found links to the papers mentioned and extracted some of their abstracts.

I have to admit that looking at all the algorithms critically I found some new ideas for tackling problems, beyond what I had come up with on my own.

