---
date: 2025-02-05
Title: 'Practical and Optimal LSH for Angular Distance'
Subtitle: Review
Categories: [Paper, Review]
keywords: [Locality-Sensitive Hashing, LSH, Cosine similarity, NLP, Near Neighbor Search]
# draft: True
---

![Litrature review](./cover.jpg){.column-margin}

In [@andoni2015practical] the authors delve into Locality-Sensitive Hashing (LSH) for the angular distance. They show the existence of an LSH family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal running time exponent. Unlike earlier algorithms with this property (e.g., Spherical LSH), their algorithm is also practical, improving upon the well-studied hyperplane LSH in practice.

In the NLP specialization we have covered and used LSH a number of times. This paper is a good introduction to LSH for the angular distance.

# Abstract 

> We show the existence of a Locality-Sensitive Hashing (LSH) family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal running time exponent. Unlike earlier algorithms with this property (e.g., Spherical LSH [1 , 2]), our algorithm is also practical, improving upon the well-studied hyperplane LSH [3] in practice.
>
> We also introduce a **multiprobe** version of this algorithm, and conduct experimental evaluation on real and synthetic data sets. 
>
> We complement the above positive results with a fine-grained lower bound for the quality of any LSH family for angular distance. Our lower bound implies that the above LSH family exhibits a trade-off between evaluation time and quality that is close to optimal for a natural class of LSH functions. -- [@andoni2015practical]

<!-- TODO: add outline, review, terminology and podcast for this paper-->
<!-- TODO: link here from LSH lessons-->

## The Paper

![paper](./paper.pdf){.col-page width=800px height=1000px group="slides"}
