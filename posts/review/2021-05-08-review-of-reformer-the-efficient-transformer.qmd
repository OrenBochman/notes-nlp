---
date: 2021-05-08
title: "review of Reformer: The Efficient Transformer"
description: 'review of the 2020 paper "Reformer The Efficient Transformer" on improving the transformer architecture for the deeplearning.ai NLP specialization.'
categories: [nlp, notes, paper, attention, deep learning, literature review]
img: img/literature-review-open-book.jpg
draft: true
---

![deeplearning.ai](img/logo_deeplearning_ai.png)

# Reformer: The Efficient Transformer

Reformer presents some innovations which allow a more more efficient transformer.
Location sensitive hashing allows attending back to distances of 1,000,000 positions back in the sequence.

# Introduction

![page 1](img/rtet-page0.png)
<hr>
Hashing attention
Locality sensitive hashing
LSH attention
![page 2](img/rtet-page2.png)
<hr>
![page 3](img/rtet-page3.png)
<hr>
![page 4](img/rtet-page4.png)
<hr>
![page 5](img/rtet-page5.png)
<hr>
![page 6](img/rtet-page6.png)
<hr>
![page 7](img/rtet-page7.png)
<hr>
![page 8](img/rtet-page8.png)
<hr>
![page 9](img/rtet-page9.png)
<hr>
![page 10](img/rtet-page12.png)
<hr>
![page 11](img/rtet-page11.png)
