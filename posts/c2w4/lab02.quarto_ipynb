{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "date: 2020-10-25\n",
        "title: 'Word Embeddings: Intro to CBOW model, activation functions and working with Numpy'\n",
        "categories: \n",
        "  - NLP \n",
        "  - Coursera \n",
        "  - Lab\n",
        "  - Probabilistic Models\n",
        "jupyter: python3\n",
        "# execute: \n",
        "#     error: true\n",
        "---\n",
        "\n",
        "::: {.column-margin .nolightbox}\n",
        "![course banner](/images/Course-Logo-2-3.webp)\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "In this lecture notebook we will be given an introduction to the continuous bag-of-words model, its activation functions and some considerations when working with Numpy. \n",
        "\n",
        "Let's dive into it!\n"
      ],
      "id": "799c69f3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np"
      ],
      "id": "1630c071",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The continuous bag-of-words model\n",
        "\n",
        "The CBOW model is based on a neural network, the architecture of which looks like the figure below, as you'll recall from the lecture.\n",
        "\n",
        "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='cbow_model_architecture.png' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:917;height:337;\" /> Figure 1 </div>\n",
        "\n",
        "## Activation functions\n",
        "\n",
        "Let's start by implementing the activation functions, ReLU and softmax.\n",
        "\n",
        "### ReLU\n",
        "\n",
        "ReLU is used to calculate the values of the hidden layer, in the following formulas:\n",
        "\n",
        "\\begin{align}\n",
        " \\mathbf{z_1} &= \\mathbf{W_1}\\mathbf{x} + \\mathbf{b_1}  \\tag{1} \\\\\n",
        " \\mathbf{h} &= \\mathrm{ReLU}(\\mathbf{z_1})  \\tag{2} \\\\\n",
        "\\end{align}\n",
        "\n",
        "Let's fix a value for $\\mathbf{z_1}$ as a working example.\n"
      ],
      "id": "e65f12ca"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define a random seed so all random outcomes can be reproduced\n",
        "np.random.seed(10)\n",
        "\n",
        "# Define a 5X1 column vector using numpy\n",
        "z_1 = 10*np.random.rand(5, 1)-5\n",
        "\n",
        "# Print the vector\n",
        "z_1"
      ],
      "id": "8eb8c30a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that using numpy's `random.rand` function returns a numpy array filled with values taken from a uniform distribution over [0, 1). Numpy allows vectorization so each value is multiplied by 10 and then substracted 5.\n",
        "\n",
        "To get the ReLU of this vector, we want all the negative values to become zeros.\n",
        "\n",
        "First create a copy of this vector.\n"
      ],
      "id": "b6c4a461"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create copy of vector and save it in the 'h' variable\n",
        "h = z_1.copy()"
      ],
      "id": "750564c9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now determine which of its values are negative.\n"
      ],
      "id": "c279a7f9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Determine which values met the criteria (this is possible because of vectorization)\n",
        "h < 0"
      ],
      "id": "444e9ab1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now simply set all of the values which are negative to 0.\n"
      ],
      "id": "fe03e236"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Slice the array or vector. This is the same as applying ReLU to it\n",
        "h[h < 0] = 0"
      ],
      "id": "a10e3440",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And that's it: we have the ReLU of $\\mathbf{z_1}$!\n"
      ],
      "id": "a3217eb3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Print the vector after ReLU\n",
        "h"
      ],
      "id": "7cca8214",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Now implement ReLU as a function.**\n"
      ],
      "id": "3fe32b85"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define the 'relu' function that will include the steps previously seen\n",
        "def relu(z):\n",
        "    result = z.copy()\n",
        "    result[result < 0] = 0\n",
        "    return result"
      ],
      "id": "0b6fb7e6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**And check that it's working.**\n"
      ],
      "id": "fb90f6d0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define a new vector and save it in the 'z' variable\n",
        "z = np.array([[-1.25459881], [ 4.50714306], [ 2.31993942], [ 0.98658484], [-3.4398136 ]])\n",
        "\n",
        "# Apply ReLU to it\n",
        "relu(z)"
      ],
      "id": "8a8765e0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Expected output:\n",
        "\n",
        "    array([[0.        ],\n",
        "           [4.50714306],\n",
        "           [2.31993942],\n",
        "           [0.98658484],\n",
        "           [0.        ]])\n",
        "\n",
        "### Softmax\n",
        "\n",
        "The second activation function that we need is softmax. This function is used to calculate the values of the output layer of the neural network, using the following formulas:\n",
        "\n",
        "\\begin{align}\n",
        " \\mathbf{z_2} &= \\mathbf{W_2}\\mathbf{h} + \\mathbf{b_2}   \\tag{3} \\\\\n",
        " \\mathbf{\\hat y} &= \\mathrm{softmax}(\\mathbf{z_2})   \\tag{4} \\\\\n",
        "\\end{align}\n",
        "\n",
        "To calculate softmax of a vector $\\mathbf{z}$, the $i$-th component of the resulting vector is given by:\n",
        "\n",
        "$$ \\textrm{softmax}(\\textbf{z})_i = \\frac{e^{z_i} }{\\sum\\limits_{j=1}^{V} e^{z_j} }  \\tag{5} $$\n",
        "\n",
        "Let's work through an example.\n"
      ],
      "id": "e8002ff7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define a new vector and save it in the 'z' variable\n",
        "z = np.array([9, 8, 11, 10, 8.5])\n",
        "\n",
        "# Print the vector\n",
        "z"
      ],
      "id": "09414f87",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You'll need to calculate the exponentials of each element, both for the numerator and for the denominator.\n"
      ],
      "id": "a1fae2a8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Save exponentials of the values in a new vector\n",
        "e_z = np.exp(z)\n",
        "\n",
        "# Print the vector with the exponential values\n",
        "e_z"
      ],
      "id": "97b112c5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The denominator is equal to the sum of these exponentials.\n"
      ],
      "id": "38786fe0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Save the sum of the exponentials\n",
        "sum_e_z = np.sum(e_z)\n",
        "\n",
        "# Print sum of exponentials\n",
        "sum_e_z"
      ],
      "id": "e632662d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And the value of the first element of $\\textrm{softmax}(\\textbf{z})$ is given by:\n"
      ],
      "id": "e5083bd9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Print softmax value of the first element in the original vector\n",
        "e_z[0]/sum_e_z"
      ],
      "id": "1f5c3f70",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is for one element. We can use numpy's vectorized operations to calculate the values of all the elements of the $\\textrm{softmax}(\\textbf{z})$ vector in one go.\n",
        "\n",
        "**Implement the softmax function.**\n"
      ],
      "id": "3c431246"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define the 'softmax' function that will include the steps previously seen\n",
        "def softmax(z):\n",
        "    e_z = np.exp(z)\n",
        "    sum_e_z = np.sum(e_z)\n",
        "    return e_z / sum_e_z"
      ],
      "id": "b9c882d8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Now check that it works.**\n"
      ],
      "id": "a32a6242"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Print softmax values for original vector\n",
        "softmax([9, 8, 11, 10, 8.5])"
      ],
      "id": "68fbbbe7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Expected output:\n",
        "\n",
        "    array([0.08276948, 0.03044919, 0.61158833, 0.22499077, 0.05020223])\n",
        "\n",
        "Notice that the sum of all these values is equal to 1.\n"
      ],
      "id": "cbfa636d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Assert that the sum of the softmax values is equal to 1\n",
        "np.sum(softmax([9, 8, 11, 10, 8.5])) == 1"
      ],
      "id": "73e67d99",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dimensions: 1-D arrays vs 2-D column vectors\n",
        "\n",
        "Before moving on to implement forward propagation, backpropagation, and gradient descent in the next lecture notebook, let's have a look at the dimensions of the vectors you've been handling until now.\n",
        "\n",
        "Create a vector of length $V$ filled with zeros.\n"
      ],
      "id": "35fea42c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true
      },
      "source": [
        "# Define V. Remember this was the size of the vocabulary in the previous lecture notebook\n",
        "V = 5\n",
        "\n",
        "# Define vector of length V filled with zeros\n",
        "x_array = np.zeros(V)\n",
        "\n",
        "# Print vector\n",
        "x_array"
      ],
      "id": "b5332cd1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is a 1-dimensional array, as revealed by the `.shape` property of the array.\n"
      ],
      "id": "ee178b4a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Print vector's shape\n",
        "x_array.shape"
      ],
      "id": "a6153cb4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To perform matrix multiplication in the next steps, we actually need your column vectors to be represented as a matrix with one column. In numpy, this matrix is represented as a 2-dimensional array.\n",
        "\n",
        "The easiest way to convert a 1D vector to a 2D column matrix is to set its `.shape` property to the number of rows and one column, as shown in the next cell.\n"
      ],
      "id": "c806d37a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Copy vector\n",
        "x_column_vector = x_array.copy()\n",
        "\n",
        "# Reshape copy of vector\n",
        "x_column_vector.shape = (V, 1)  # alternatively ... = (x_array.shape[0], 1)\n",
        "\n",
        "# Print vector\n",
        "x_column_vector"
      ],
      "id": "2fc30ec2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The shape of the resulting \"vector\" is:\n"
      ],
      "id": "bb85b8d3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Print vector's shape\n",
        "x_column_vector.shape"
      ],
      "id": "2b63c81f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So we now have a 5x1 matrix that we can use to perform standard matrix multiplication.\n",
        "\n",
        "**Congratulations on finishing this lecture notebook!** Hopefully we now have a better understanding of the activation functions used in the continuous bag-of-words model, as well as a clearer idea of how to leverage Numpy's power for these types of mathematical computations.\n",
        "\n",
        "In the next lecture notebook we will get a comprehensive dive into:\n",
        "\n",
        "- Forward propagation.\n",
        "- Cross-entropy loss.\n",
        "- Backpropagation.\n",
        "- Gradient descent.\n",
        "\n",
        "**See we next time!**\n"
      ],
      "id": "f9b951bd"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/home/oren/.local/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}