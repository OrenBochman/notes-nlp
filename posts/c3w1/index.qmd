---
title: "Neural Networks for Sentiment Analysis"
subtitle: "Sequence Models"
description: "we cover Neural networks for deep learning, then build a tweet classifier that places tweets into positive or negative sentiment categories, using a DNN."
date: 2020-10-23
categories: 
  - NLP 
  - Coursera 
  - Notes
  - Sequence Models
  - Sentiment Analysis
---

::: {#fig-00 .column-margin}
![course banner](/images/Course-Logo-3-3.webp){.column-margin group="slides"}
:::

::: {#fig-slide-deck .column-margin}
![This week's slides](slides.pdf){width="420px" height="340px" style="@page {size: 16in 9in;  margin: 0;  group="slides"}"}
:::

My irreverent notes for Week 1 of the [Natural Language Processing with Sequence Models](https://www.coursera.org/learn/sequence-models-in-nlp/home/welcome) Course in the Natural Language Processing Specialization Offered by [DeepLearning.AI](DeepLearning.AI) on [Coursera](https://www.coursera.org/)


::: {.callout-important}
## Learning Objectives

- Feature extraction
- Supervised machine learning
- Binary classification
- Text preprocessing
- ReLU
- Python classes
- Trax
- Neural networks

:::

## Neural Networks for Sentiment Analysis

Previously in the course you did sentiment analysis with logistic regression and naive Bayes. Those models were in a sense more naive, and are not able to catch the sentiment off a tweet like: "I am not happy " or "If only it was a good day". When using a neural network to predict the sentiment of a sentence, you can use the following. Note that the image below has three outputs, in this case you might want to predict, "positive", "neutral ", or "negative". 

![](img/slide01.png){.column-margin group="slides"}

Note that the network above has three layers. To go from one layer to another you can use a 
$W$ matrix to propagate to the next layer. Hence, we call this concept of going from the input until the final layer, forward propagation. To represent a tweet, you can use the following: 


Note, that we add zeros for padding to match the size of the longest tweet. 

![](img/slide02.png){.column-margin group="slides"}


## Trax: Neural Networks

Trax has several advantages: 

- Runs fast on CPUs, GPUs and TPUs
- Parallel computing
- Record algebraic computations for gradient evaluation

Here is an example of how you can code a neural network in Trax: 

![](img/slide03.png){.column-margin group="slides"}

## Reading: (Optional) Trax and JAX, docs and code

Official Trax documentation maintained by the Google Brain team:

- https://trax-ml.readthedocs.io/en/latest/

Trax source code on GitHub:

- https://github.com/google/trax

JAX library:

- https://jax.readthedocs.io/en/latest/index.html

## Lab: Introduction to Trax

[Introduction to Trax](lab01.qmd)

## Trax: Layers

Trax makes use of classes. If you are not familiar with classes in python, don't worry about it, here is an example. 

![](img/slide04.png){.column-margin group="slides"}


In the example above, you can see that a class takes in an `__init__` and a `__call__` method. 

These methods allow you to initialize your internal variables and allow you to execute your function when called. 

To the right you can see how you can initialize your class. When you call `MyClass(7)` , you are setting the $y$ variable to 7. Now when you call `f(3)` you are adding 7 + 3. 

You can change the `my_method` function to do whatever you want, and you can have as many methods as you want in a class.  

## Lab: Classes and Subclasses

[Classes and Subclasses](lab02.qmd)

## Dense and ReLU layer

The Dense layer is the computation of the inner product between a set of trainable weights (weight matrix) and an input vector.  The visualization of the dense layer could be seen in the image below. 

![](img/slide05.png){.column-margin group="slides"}

The orange box shows the dense layer. An activation layer is the set of blue nodes. Concretely one of the most commonly used activation layers is the rectified linear unit (ReLU).

![](img/slide06.png){.column-margin group="slides"}

ReLU(x) is defined as max(0,x) for any input x. 


## Serial Layer

A serial layer allows you to compose layers in a serial arrangement:

![](img/slide07.png){.column-margin group="slides"}

It is a composition of sublayers. These layers are usually dense layers followed by activation layers. 

## Other Layers

Other layers could include embedding layers and mean layers. For example, you can learn word embeddings for each word in your vocabulary as follows: 

![](img/slide08.png){.column-margin group="slides"}

The mean layer allows you to take the average of the embeddings. You can visualize it as follows: 

![](img/slide09.png){.column-margin group="slides"}

This layer does not have any trainable parameters. 

## Training

In Trax, the function grad allows you to compute the gradient. You can use it as follows: 

![](img/slide10.png){.column-margin group="slides"}

Now if you were to evaluate grad_f at a certain value, namely z, it would be the same as computing 6z+1.  Now to do the training, it becomes very simple: 


![](img/slide11.png){.column-margin group="slides"}


You simply compute the gradients by feeding in y.forward (the latest value of y), the weights, and the input x, and then it does the back-propagation for you in a single line. You can then have the loop that allows you to update the weights (i.e. gradient descent!).

## Lab: Data Generators

[Data Generators](lab03.qmd)

# Resources:


- [@Chadha2020DistilledNotesCourseraDLSpec] [Aman Chadha's Notes](https://aman.ai/coursera-nlp/logistic-regression/)
- [Ibrahim Jelliti's Notes](https://github.com/ibrahimjelliti/Deeplearning.ai-Natural-Language-Processing-Specialization/tree/master/3%20-%20Natural%20Language%20Processing%20with%20Sequence%20Models)
