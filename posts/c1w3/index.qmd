---
date: 2020-10-07
title: "Vector Space Models"
subtitle: "Classification & Vector Spaces"
description: "Vector space models capture semantic meaning and relationships between words. You'll learn how to create word vectors that capture dependencies between words, then visualize their relationships in two dimensions using PCA."
categories: 
  - NLP 
  - Coursera 
  - Notes
  - Classification & Vector Spaces
---


![course banner](/images/Course-Logo-1-3.webp){#fig-00 .column-margin .nolightbox}

::: {#fig-slide-deck .column-margin}
![This week's slides](slides.pdf){width="420px" height="340px" style="@page {size: 16in 9in;  margin: 0;  group="slides"}"}
:::

### Vector Space Models

::: {#fig-slide-01 .column-margin}
![linear representation](img/slide01.png)

Understanding Vector Space
:::

Vector spaces are fundamental in many applications in NLP.
If we were to represent a word, document, tweet, or any form of text, we will probably be encoding it as a vector.
These vectors are important in tasks like information extraction, machine translation, and chatbots.
Vector spaces could also be used to help we identify relationships between words as follows:

-   We eat **cereal** from a **bowl**
-   We **buy** something and someone else sells **it**

{{< pagebreak >}}

::: {#fig-slide-firth .column-margin}
![Distributed Representation](img/slide00.png)

Distributed Representation is A fundamental concept in Linguistics
:::

> "We shall know a word by the company it keeps".
> -- J.R.
> Firth

When we are learning these vectors, we usually make use of the neighboring words to extract meaning and information about the center word.
If we were to cluster these vectors together, we will see that adjectives, nouns, verbs, etc. tend to be near one another.
Another aspect of the vector space representation, is that synonyms and antonyms are also very close to one another.
This is because we can easily interchange them in a sentence and they tend to have similar neighboring words!

::: {#fig-slide-02 .column-margin}
![linear representation](img/slide02.png)

Some applications of the vector space model in NLP
:::

{{< pagebreak >}}

### Word-by-Word and Word By Document

We now delve into constructing vector word representations, based off a co-occurrence matrix.
Specifically, depending on the task we are trying to solve, we can choose from several possible designs.

#### Word by word Design

We will start by exploring the **word by word** design.
Assume that we are trying to come up with a vector that will represent a certain word.
One possible design would be to [create a matrix where each row and column corresponds to a word in your vocabulary. Then we can iterate over a document and see the number of times each word shows up next each other word]{.mark}.
We can keep track of the number in the matrix.
In the video I spoke about a parameter $K$.

We can think of $K$ as the window size for the context which determines if two words are considered adjacent according to Firth's law.

::: {#fig-slide-03 .column-margin}
![Word by word design](img/slide03.png)

Word Level aggregation: Creating a word by word vector space using a word by word design
:::

In the example above, we can see how we are keeping track of the number of times words occur together within a certain distance k.
At the end, we can represent the word data, as a vector $v=[2,1,1,0].$

{{< pagebreak >}}

#### Word by Document Design

We can now apply the same concept and map words to documents.
[The rows could correspond to words and the columns to documents. The numbers in the matrix correspond to the number of times each word showed up in the document]{.mark}.

::: {#fig-slide-04 .column-margin}
![Word by document](img/slide04.png){.column-margin group="slides"}

Document Level aggregation: Creating a document by word vector space using a word by word design
:::

In the example above, we can see how we are keeping track of the number of times words occur together within a certain distance k.

At the end, we can represent the word data, as a vector $v = [2, 1, 1, 0]$.

::: {#fig-slide-05 .column-margin}
![linear representation](img/slide05.png)

In this slide we see the vector space matrix at the top left, and on the right a plot of the it geometry of two words in three document categories.
We can use the angle and the distance between these vectors to understand the similarity between the vectors.
:::

### Euclidean Distance

Let us assume that we want to compute the distance between two points: A,B.
To do so, we can use the euclidean distance defined as

$$
d(B,A) = \sqrt{\sum_{i=1}^{n} (B_i - A_i)^2}
$$ {#eq-euclidean-distance}

::: {#fig-slide-06 .column-margin}
![Euclidean distance](img/slide06.png)

In this slide we see the vector space matrix at the top left, and on the right a plot of the it geometry of two words in three document categories.
We can use the angle and the distance between these vectors to understand the similarity between the vectors.
:::

::: {#fig-slide-07 .column-margin}
![Distance calculation](img/slide07.png)

Calculate the distance between two vectors of dimensional size three.
:::

## LAB: Linear algebra in Python with `numpy`

[The Numpy lab](lab01.qmd)

### Cosine Similarity Intuition

One of the issues with euclidean distance is that it is not always accurate and sometimes we are not looking for that type of similarity metric.
For example, when comparing large documents to smaller ones with euclidean distance one could get an inaccurate result.

Look at the [diagram](@fig-slide-08):

::: {#fig-slide-08 .column-margin}
![Cosine Similarity: Intuition](img/slide08.png)

The cosine similarity is the cosine of the angle between two vectors.
:::

Normally the **food** corpus and the **agriculture** corpus are more similar because they have the same proportion of words.
However the food corpus is much smaller than the agriculture corpus.
To further clarify, although the history corpus and the agriculture corpus are different, they have a smaller euclidean distance.
Hence $d_2<d_1$.

To solve this problem, we look at the cosine between the vectors.
This allows us to compare $B$ and $α$.

### Background

Before getting into the cosine similarity function remember that the norm of a vector is defined as:

### Norm of a Vector

$$
||\vec{A}|| = \sqrt{\sum_{i=1}^{n} a_i^2}
$$ {#eq-norm-vector}

### Dot-product of Two Vectors

The dot product is then defined as:

$$
\vec{A} \cdot \vec{B} = \sum_{i=1}^{n} a_i \cdot b_i
$$ {#eq-dot-product}

::: {#fig-slide-09 .column-margin}
![Cosine Similarity](img/slide09.png)

The cosine similarity is the cosine of the angle between two vectors.
:::

The following cosine similarity equation makes sense:

$$
\cos(\theta) = \frac{\vec{v} \cdot \vec{w}}{||\vec{v}|| \cdot ||\vec{w}||}
$$ {#eq-cosine-similarity}

## Implementation

When $\vec{v}$ and $\vec{u}$ are parallel the numerator is equal to the denominator so $cos(\beta)=1$ thus $\angle \beta=0$.

On the other hand, the dot product of two orthogonal (perpendicular) vectors is $0$.
That takes place when $\angle \beta=90$.

::: {#fig-slide-10 .column-margin}
![Cosine Similarity Examples](img/slide10.png)

Examples of cosine similarity between similar and dissimilar vectors.
:::

# Word Manipulation in Vector Spaces

We can use word vectors to actually extract patterns and identify certain structures in your text.
For example:

::: {#fig-slide-11 .column-margin}
![Word analogy conceptual](img/slide11.png)

Word analogy have a geometric interpretation in vector spaces.
:::

We can use the word vector for Russia, USA, and DC to actually compute a vector that would be very similar to that of Moscow.
We can then use cosine similarity of the vector with all the other word vectors we have and we can see that the vector of Moscow is the closest.

Isn't that goofy?

::: {#fig-slide-12 .column-margin}
![Word analogy](img/slide12.png)

Some word analogies
:::

Note that the distance (and direction) between a country and its capital is relatively the same.
Hence manipulating word vectors allows we identify patterns in the text.

## Lab on Manipulating Word Vectors

[The lab](lab02.qmd)

# PCA

## Visualization of Word Vectors

Principal component analysis is an unsupervised learning algorithm which can be used to reduce the dimension of your data.
As a result, it allows we to visualize your data.
It tries to combine variances across features.
Here is a concrete example of PCA:

::: {#fig-slide-13 .column-margin}
![Word analogy](img/slide13.png)

Some word analogies
:::

::: {#fig-slide-14 .column-margin}
![Word analogy](img/slide14.png)

Some word analogies
:::

Those are the results of plotting a couple of vectors in two dimensions.
Note that words with similar part of speech (POS) tags are next to one another.
This is because many of the training algorithms learn words by identifying the neighboring words.
Thus, words with similar POS tags tend to be found in similar locations.
An interesting insight is that synonyms and antonyms tend to be found next to each other in the plot.
Why is that the case?

## Implementation

## PCA algorithm

PCA is commonly used to reduce the dimension of your data.
Intuitively the model collapses the data across principal components.
We can think of the first principal component (in a 2D dataset) as the line where there is the most amount of variance.
We can then collapse the data points on that line.
Hence we went from 2D to 1D.
We can generalize this intuition to several dimensions.

::: {#fig-slide-15 .column-margin}
![PCA](img/slide15.png)

Some word analogies
:::

Eigenvector

:   the resulting vectors, also known as the uncorrelated features of your data

Eigenvalue

:   the amount of information retained by each new feature.
    We can think of it as the variance in the eigenvector.

Also each **eigenvalue** has a corresponding eigenvector.
The eigenvalue tells we how much variance there is in the eigenvector.
Here are the steps required to compute PCA:

::: {#fig-slide-16 .column-margin}
![PCA](img/slide16.png)

Some word analogies
:::

### Steps to Compute PCA:

-   Mean normalize your data
-   Compute the covariance matrix
-   Compute SVD on your covariance matrix. This returns $[USV]=svd(Σ)$ . The three matrices $U$, $S$, $V$ are drawn above. $U$ is labelled with eigenvectors, and $S$ is labelled with eigenvalues.
-   We can then use the first n columns of vector $U$, to get your new data by multiplying $XU[:,0:n]$.

## Putting It Together with Code

```{python}
import numpy as np 

def PCA(X , num_components):
  # center data around the mean
  X_meaned = X - np.mean(X , axis = 0) 
  # calculate the covariance matrix   
  cov_mat = np.cov(X_meaned , rowvar = False) 
  # compute an uncorrelated feature basis (eigen vectors) 
  eigen_values , eigen_vectors = np.linalg.eigh(cov_mat)
  # sort the new basis by decreasing eigen values (variance) 
  sorted_index = np.argsort(eigen_values)[::-1] 
  sorted_eigenvalue = eigen_values[sorted_index] 
  sorted_eigenvectors = eigen_vectors[:,sorted_index] 
  # by subseting the most leading features  
  eigenvector_subset = sorted_eigenvectors[:,0:num_components] 
  #Step-6 
  X_reduced = np.dot(eigenvector_subset.transpose() ,     
                   X_meaned.transpose() ).transpose() 
  return X_reduced 
```

## Lab on PCA

[PCA lab](assignment.qmd)

# Document As a Vector

## Resources

-   Alex Williams - [Everything we did and didn't know about PCA](https://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/)
-   Udell et al. (2015). [Generalized Low-Rank Models](http://arxiv.org/abs/1410.0342)  arxiv preprint
-   Tipping & Bishop (1999). [Probabilistic principal component analysis](http://dx.doi.org/10.1111/1467-9868.00196) Journal of the Royal Statistical Society: Series B
-   Ilin & Raiko (2010) [Practical Approaches to Principal Component Analysis in the Presence of Missing Values](http://www.jmlr.org/papers/volume11/ilin10a/ilin10a.pdf) Journal of Machine Learning Research
-   Gordon (2002). [Generalized^2^ Linear^2^ Models](http://www.cs.cmu.edu/~ggordon/ggllm.pdf) NIPS
-   Cunningham & Ghahramani (2015)  [Linear dimensionality reduction: survey, insights, and generalizations](http://jmlr.org/papers/volume16/cunningham15a/cunningham15a.pdf) Journal of Machine Learning Research
-   Burges (2009). [Dimension Reduction: A Guided Tour](http://dx.doi.org/10.1561/2200000002) Foundations varia Trends in Machine Learning
-   M. Gavish and D. L. Donoho, [The Optimal Hard Threshold for Singular Values is $\frac{4}{\sqrt{3}}$](https://ieeexplore.ieee.org/document/6846297) in IEEE Transactions on Information Theory, vol.
    60, no. 8, pp. 5040-5053, Aug. 2014, doi: 10.1109/TIT.2014.2323359.
-   Thomas P. Minka [Automatic choice of dimensionality for PCA](http://hd.media.mit.edu/tech-reports/TR-514.pdf) Dec. 2000

