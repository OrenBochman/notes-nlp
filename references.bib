@article{10.1093/ooec/odad002,
    author = {Basu, Kaushik},
    title = "{The morphing of dictators: why dictators get worse over time}",
    journal = {Oxford Open Economics},
    volume = {2},
    pages = {odad002},
    year = {2023},
    month = {02},
    abstract = "{Dictators, even those who seize power with the intention of helping the nation, frequently morph over time into tyrants. There may be many reasons for this. This paper focuses on one interesting and arguably pervasive driver behind this process. A model is developed which shows that the series of decisions taken over time by an authoritarian leader concerning how much political intrigue and evil to indulge in in order to stay in power leads to a dynamic inconsistency converting the leader into a tyrant. It is possible that the dictator will, eventually, come to regret this, but by then they have no exit options. The analysis prompts us to think about ex ante rules and term-limit provisions to prevent this from happening.}",
    issn = {2752-5074},
    doi = {10.1093/ooec/odad002},
    url = {https://doi.org/10.1093/ooec/odad002},
    eprint = {https://academic.oup.com/ooec/article-pdf/doi/10.1093/ooec/odad002/51233936/odad002.pdf},
    keywords = {{dynamic inconsistency}, procrastination, {term limit}, tyrant, dictator},
}

@article{10.1214/ss/1177012488,
author = {Sandy Zabell},
title = {{R. A. Fisher on the History of Inverse Probability}},
volume = {4},
journal = {Statistical Science},
number = {3},
publisher = {Institute of Mathematical Statistics},
pages = {247 -- 256},
keywords = {History of statistics, inverse probability, R. A. Fisher},
year = {1989},
doi = {10.1214/ss/1177012488},
URL = {https://doi.org/10.1214/ss/1177012488}
}

@AUDIO{10.13:96,
  ENTRYSUBTYPE   = {speech},
  AUTHOR         = {King, Jr., M. L.},
  TITLE          = {I Have a Dream},
  PUBLISHER      = {American Rhetoric},
  DATE           = {1963-08-28},
  URL            = {https://www.americanrhetoric.com/speeches/mlkihaveadream.htm}
}

@inproceedings{Ahn2012Bayesian,
author = {Ahn, Sungjin and Korattikara, Anoop and Welling, Max},
title = {Bayesian posterior sampling via stochastic gradient fisher scoring},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In this paper we address the following question: "Can we approximately sample from a Bayesian posterior distribution if we are only allowed to touch a small mini-batch of data-items for every sample we generate?". An algorithm based on the Langevin equation with stochastic gradients (SGLD) was previously proposed to solve this, but its mixing rate was slow. By leveraging the Bayesian Central Limit Theorem, we extend the SGLD algorithm so that at high mixing rates it will sample from a normal approximation of the posterior, while for slow mixing rates it will mimic the behavior of SGLD with a pre-conditioner matrix. As a bonus, the proposed algorithm is reminiscent of Fisher scoring (with stochastic gradients) and as such an efficient optimizer during burn-in.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1771–1778},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12},
doi={10.5555/3042573.3042799}
}

@Article{Akerlof1970Lemons,
  author={Akerlof, George A.},
  title={The Market for "Lemons": Quality Uncertainty and the Market Mechanism},
  journal={The Quarterly Journal of Economics},
  year=1970,
  volume={84},
  number={3},
  pages={488-500},
  keywords={"Adverse Selection"},
  abstract={I. Introduction, 488. — II. The model with automobiles as an example, 489. — III. Examples and applications, 492. — IV. Counteracting institutions, 499. — V. Conclusion, 500.},
}

@misc{Alammar2024Jan,
	author = {Alammar, Jay},
	title = {{The Illustrated Transformer}},
	year = {2024},
	month = jan,
	note = {[Online; accessed 7. Apr. 2024]},
	url = {https://jalammar.github.io/illustrated-transformer}
}

@article{Aldrich2008Fisher,
author = {Aldrich, John},
year = {2008},
month = {03},
pages = {},
title = {R. A. Fisher on Bayes and Bayes' theorem},
volume = {3},
journal = {Bayesian Analysis},
doi = {10.1214/08-BA306}
}
@article{Anderson72MoreIsDifferent,
author = {P. W. Anderson },
title = {More Is Different},
journal = {Science},
volume = {177},
number = {4047},
pages = {393-396},
year = {1972},
doi = {10.1126/science.177.4047.393},
URL = {https://www.science.org/doi/abs/10.1126/science.177.4047.393},
eprint = {https://www.science.org/doi/pdf/10.1126/science.177.4047.393}}

@article{Atanasov2021Human,
author = {Atanasov, Pavel and Joseph, Regina and Feijoo, Felipe and Marshall, Max and Siddiqui, Sauleh},
year = {2021},
month = {01},
pages = {},
url ={https://www.youtube.com/playlist?list=PLKVCRT3MRed6y8M64INTtlZ3fc8O9ah1L},
title = {Human Forest vs. Random Forest in Time-Sensitive COVID-19 Clinical Trial Prediction},
journal = {SSRN Electronic Journal},
doi = {10.2139/ssrn.3981732}
}

@article{Axelrod1997DisseminationofCulture,
author = {Robert Axelrod},
title ={The Dissemination of Culture: A Model with Local Convergence and Global Polarization},
journal = {Journal of Conflict Resolution},
volume = {41},
number = {2},
pages = {203-226},
year = {1997},
doi = {10.1177/0022002797041002001},
URL = {https://doi.org/10.1177/0022002797041002001},
eprint = {https://doi.org/10.1177/0022002797041002001},
abstract = { Despite tendencies toward convergence, differences between individuals and groups continue to exist in beliefs, attitudes, and behavior. An agent-based adaptive model reveals the effects of a mechanism of convergent social influence. The actors are placed at fixed sites. The basic premise is that the more similar an actor is to a neighbor, the more likely that that actor will adopt one of the neighbor's traits. Unlike previous models of social influence or cultural change that treat features one at a time, the proposed model takes into account the interaction between different features. The model illustrates how local convergence can generate global polarization. Simulations show that the number of stable homogeneous regions decreases with the number of features, increases with the number of alternative traits per feature, decreases with the range of interaction, and (most surprisingly) decreases when the geographic territory grows beyond a certain size. }
}

@book{BBdM1983war,
title={The war trap},
author={De Mesquita, Bruce Bueno},
year={1983},
publisher={Yale University Press}
}

@book{BBdM1983war,
title={The war trap},
author={De Mesquita, Bruce Bueno},
year={1983},
publisher={Yale University Press}
}

@article{BBdM1984Forecasting,
title={Forecasting policy decisions: an expected utility approach to post-Khomeini Iran},
author={De Mesquita, Bruce Bueno},
journal={PS: Political Science \& Politics},
volume={17},
number={2},
pages={226--236},
year={1984},
publisher={Cambridge University Press}
}

@article{BBdM1984Forecasting,
title={Forecasting policy decisions: an expected utility approach to post-Khomeini Iran},
author={De Mesquita, Bruce Bueno},
journal={PS: Political Science \& Politics},
volume={17},
number={2},
pages={226--236},
year={1984},
publisher={Cambridge University Press}
}

@article{BBdM1986reason,
title={Reason and war},
author={De Mesquita, Bruce Bueno and Lalman, David},
journal={American Political Science Review},
volume={80},
number={4},
pages={1113--1129},
year={1986},
publisher={Cambridge University Press}
}

@article{BBdM1986reason,
title={Reason and war},
author={De Mesquita, Bruce Bueno and Lalman, David},
journal={American Political Science Review},
volume={80},
number={4},
pages={1113--1129},
year={1986},
publisher={Cambridge University Press}
}

@book{BBdM1994european,
title={European Community decision making: Models, applications, and comparisons},
author={De Mesquita, Bruce Bueno and Stokman, Frans N},
year={1994},
publisher={Yale University Press}
}

@article{BBdM1997decision,
title={A decision making model: Its structure and form},
author={De Mesquita, Bruce Bueno},
journal={International Interactions},
volume={23},
number={3-4},
pages={235--266},
year={1997},
publisher={Taylor \& Francis}
}


@article{BBdM1998end,
title={The end of the Cold War: Predicting an emergent property},
author={De Mesquita, Bruce Bueno},
journal={Journal of Conflict Resolution},
volume={42},
number={2},
pages={131--155},
year={1998},
publisher={Sage Periodicals Press 2455 Teller Road, Thousand Oaks, CA 91320}
}

@article{BBdM1998end,
title={The end of the Cold War: Predicting an emergent property},
author={De Mesquita, Bruce Bueno},
journal={Journal of Conflict Resolution},
volume={42},
number={2},
pages={131--155},
year={1998},
publisher={Sage Periodicals Press 2455 Teller Road, Thousand Oaks, CA 91320}
}

@book{BBdM2005logic,
title={The logic of political survival},
author={De Mesquita, Bruce Bueno and Smith, Alastair and Siverson, Randolph M and Morrow, James D},
year={2005},
publisher={MIT press}
}

@book{BBdM2010predictioneer,
title={The predictioneer's game: Using the logic of brazen self-interest to see and shape the future},
author={De Mesquita, Bruce Bueno},
year={2010},
publisher={Random House Trade Paperbacks},
url={https://www.google.co.il/books/edition/The_Predictioneer_s_Game/EMMMMUqGIboC?hl=en&gbpv=0},
}

@book{BBdM2011dictator,
title={The dictator's handbook: why bad behavior is almost always good politics},
author={De Mesquita, Bruce Bueno and Smith, Alastair},
year={2011},
publisher={Hachette UK}
}

@article{BBdM2011new,
 ISSN = {07388942, 15499219},
 URL = {http://www.jstor.org/stable/26275398},
 abstract = {A new forecasting model, solved for Bayesian Perfect Equilibria, is introduced. It, along with several alternative models, is tested on data from the European Union. The new model, which allows for contingent forecasts and for generating confidence intervals around predictions, outperforms competing models in most tests despite the absence of variance on a critical variable in all but nine cases. The more proximate the political setting of the issues is to the new model's underlying theory of competitive and potentially coercive politics, the better the new model does relative to other models tested in the European Union context.},
 author = {De Mesquita, Bruce Bueno},
 journal = {Conflict Management and Peace Science},
 number = {1},
 pages = {65--85},
 publisher = {Sage Publications, Ltd.},
 title = {A New Model for Predicting Policy Choices: Preliminary Tests},
 urldate = {2024-01-31},
 volume = {28},
 year = {2011}
}

@article{BBdM2013principles,
title={Principles of international politics},
author={De Mesquita, Bruce Bueno},
year={2013},
publisher={Sage}
}

@article{BBdM2013principles,
title={Principles of international politics},
author={De Mesquita, Bruce Bueno},
year={2013},
publisher={Sage}
}

@Manual{Barret2022GGally,
  title = {GGally: Extension to 'ggplot2'},
  author = {Barret Schloerke and Di Cook and Joseph Larmarange and Francois Briatte and Moritz Marbach and Edwin Thoen and Amos Elberg and Jason Crowley},
  year = {2022},
  note = {https://ggobi.github.io/ggally/, https://github.com/ggobi/ggally},
}

@article{Barrett2006Numerical,
	author = {Jeffrey A. Barrett},
	journal = {Working Paper MBS06–09},
	title = {Numerical Simulations of the Lewis Signaling Game: Learning Strategies, Pooling Equilibria, and the Evolution of Grammar},
	year = {2006},
	abstract = {David Lewis (1969) introduced sender-receiver games as a way of investigating how meaningful language might evolve from initially random signals. In this report I investigate the conditions under which Lewis signaling games evolve to perfect signaling systems under various learning dynamics. While the 2-state/2- term Lewis signaling game with basic urn learning always approaches a signaling system, I will show that with more than two states suboptimal pooling equilibria can evolve. Inhomogeneous state distributions increase the likelihood of pooling equilibria, but learning strategies with negative reinforcement or certain sorts of mutation can decrease the likelihood of, and even eliminate, pooling equilibria. Both Moran and APR learning strategies (Bereby-Meyer and Erev 1998) are shown to promote successful convergence to signaling systems. A model is presented that illustrates how a language that codes state-act pairs in an order-based grammar might evolve in the context of a Lewis signaling game. The terms, grammar, and the corresponding partitions of the state space co-evolve to generate a language whose structure appears to reflect canonical natural kinds. The evolution of these apparent natural kinds, however, is entirely in service of the rewards that accompany successful distinctions between the sender and receiver. Any metaphysics grounded on the structure of a natural language that evolved in this way would track arbitrary, but pragmatically useful distinctions.}

}

@article{Barrett2007Dynamic,
	author = {Jeffrey A. Barrett},
	doi = {10.1086/524714},
	journal = {Philosophy of Science},
	number = {4},
	pages = {527--546},
	publisher = {University of Chicago Press},
	title = {Dynamic Partitioning and the Conventionality of Kinds},
	volume = {74},
	year = {2007}
}

@article{Barrett2009Evolution,
	author = {Jeffrey A. Barrett},
	doi = {10.1007/s11238-007-9064-0},
	journal = {Theory and Decision},
	number = {2},
	pages = {223--237},
	publisher = {Springer},
	title = {The Evolution of Coding in Signaling Games},
	volume = {67},
	year = {2009}
}

@article{Barrett2017-BARSG-6,
	author = {Jeffrey A. Barrett and Brian Skyrms},
	doi = {10.1093/bjps/axv043},
	journal = {British Journal for the Philosophy of Science},
	number = {2},
	pages = {329--353},
	publisher = {University of Chicago Press},
	title = {Self-Assembling Games},
	volume = {68},
	year = {2017}
}

@article{Barrett2019-BARSN-5,
	author = {Jeffrey A. Barrett and Brian Skyrms and Aydin Mohseni},
	doi = {10.1093/bjps/axx039},
	journal = {British Journal for the Philosophy of Science},
	number = {1},
	pages = {1--25},
	publisher = {University of Chicago Press},
	title = {Self-Assembling Networks},
	volume = {70},
	year = {2019}
}


@article{Barrett2023SelfAssembling,
	author = {Jeffrey A. Barrett},
	doi = {10.1086/714789},
	journal = {British Journal for the Philosophy of Science},
	number = {1},
	pages = {75--89},
	publisher = {University of Chicago Press},
	title = {Self-Assembling Games and the Evolution of Salience},
	volume = {74},
	year = {2023}
}

@article{Barrett_Cochran_Skyrms_2020, 
  title={On the Evolution of Compositional Language}, 
  volume={87}, 
  DOI={10.1086/710367}, 
  number={5}, 
  journal={Philosophy of Science}, 
  author={Barrett, Jeffrey A. and Cochran, Calvin and Skyrms, Brian}, 
  year={2020}, 
  pages={910–920}
}

@ARTICLE{Baxendale1958Machine,
  author={Baxendale, P. B.},
  journal={IBM Journal of Research and Development}, 
  title={Machine-Made Index for Technical Literature—An Experiment}, 
  year={1958},
  volume={2},
  number={4},
  pages={354-361},
  keywords={},
  doi={10.1147/rd.24.0354}}

% delphi method
@article{Blume2002,
 ISSN = {08837252, 10991255},
 URL = {http://www.jstor.org/stable/4129228},
 abstract = {This paper compares stimulus response (SR) and belief-based learning (BBL) using data from experiments with sender-receiver games. The environment, extensive form games played in a population setting, is novel in the empirical literature on learning in games. Both the SR and BBL models fit the data reasonably well in games where the preferences of senders and receivers are perfectly aligned and where the population history of the senders is known. The test results accept SR and reject BBL in games without population history and in all but one of the games where senders and receivers have different preferences over equilibria. Estimation is challenging since the likelihood function is not globally concave and the data become uninformative about learning once equilibrium is achieved.},
 author = {Andreas Blume and Douglas V. Dejong and George R. Neumann and N. E. Savin},
 journal = {Journal of Applied Econometrics},
 number = {3},
 pages = {225--247},
 publisher = {Wiley},
 title = {Learning and Communication in Sender-Receiver Games: An Econometric Investigation},
 urldate = {2024-05-05},
 volume = {17},
 year = {2002}
}

@inproceedings{Carbonell1998TheUO,
  title={The use of MMR, diversity-based reranking for reordering documents and producing summaries},
  author={Jaime G. Carbonell and Jade Goldstein-Stewart},
  booktitle={Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
  year={1998},
  url={https://www.cs.cmu.edu/~jgc/publication/The_Use_MMR_Diversity_Based_LTMIR_1998.pdf},
}

@inproceedings{Catteeuw2011RotherevLI,
  title={Roth-erev learning in signaling and language games},
  author={David Catteeuw and Joachim De Beule and Bernard Manderick},
  booktitle={Belgium-Netherlands Conference on Artificial Intelligence},
  year={2011},
  url={https://api.semanticscholar.org/CorpusID:59547999}
}

@misc{Chadha2020DistilledNotesCourseraDLSpec,
  author        = {Chadha, Aman},
  title         = {Distilled Notes for the Natural Language Processing Specialization on Coursera (offered by deeplearning.ai)},
  howpublished  = {\url{https://www.aman.ai}},
  year          = {2020},
  note          = {Accessed: 2020-07-01},
  url           = {www.aman.ai}
}

@inbook {Chapter2AssessingtheSystemicImplicationsofFinancialLinkages,
      author = " International Monetary Fund. Monetary and Capital Markets Department",
      title = "Chapter 2: Assessing the Systemic Implications of Financial Linkages",
      booktitle = "Global Financial Stability Report, April 2009",
      publisher = "International Monetary Fund",
      address = "USA",
      isbn = "9781616352080",
      doi = "10.5089/9781616352080.082.ch002",
      pages=      "ch02",
      url = "https://www.elibrary.imf.org/view/book/9781616352080/ch02.xml"
}
@online{Cole2020SOM,
author={Lewis Cole},
title =  {Standing Ovation Model},
url = {https://lewiscoleblog.com/standing-ovation},
year={2020},
urldate = {2024-03-13},
}

@inproceedings{Collobert2008Unified,
author = {Collobert, Ronan and Weston, Jason},
title = {A unified architecture for natural language processing: deep neural networks with multitask learning},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390177},
doi = {10.1145/1390156.1390177},
abstract = {We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {160–167},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}


@article{sarzynska2021detecting,
  title={Detecting formal thought disorder by deep contextualized word representations},
  author={Sarzynska-Wawer, Justyna and Wawer, Aleksander and Pawlak, Aleksandra and Szymanowska, Julia and Stefaniak, Izabela and Jarkiewicz, Michal and Okruszek, Lukasz},
  journal={Psychiatry Research},
  volume={304},
  pages={114135},
  year={2021},
  publisher={Elsevier}
}
@article{DBLP:journals/corr/abs-1802-05365,
  author       = {Matthew E. Peters and
                  Mark Neumann and
                  Mohit Iyyer and
                  Matt Gardner and
                  Christopher Clark and
                  Kenton Lee and
                  Luke Zettlemoyer},
  title        = {Deep contextualized word representations},
  journal      = {CoRR},
  volume       = {abs/1802.05365},
  year         = {2018},
  url          = {http://arxiv.org/abs/1802.05365},
  eprinttype    = {arXiv},
  eprint       = {1802.05365},
  timestamp    = {Mon, 13 Aug 2018 16:48:54 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1802-05365.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1910-10683,
  author       = {Colin Raffel and
                  Noam Shazeer and
                  Adam Roberts and
                  Katherine Lee and
                  Sharan Narang and
                  Michael Matena and
                  Yanqi Zhou and
                  Wei Li and
                  Peter J. Liu},
  title        = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text
                  Transformer},
  journal      = {CoRR},
  volume       = {abs/1910.10683},
  year         = {2019},
  url          = {http://arxiv.org/abs/1910.10683},
  eprinttype    = {arXiv},
  eprint       = {1910.10683},
  timestamp    = {Fri, 05 Feb 2021 15:43:41 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1910-10683.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2103-13076,
  author       = {Jungo Kasai and
                  Hao Peng and
                  Yizhe Zhang and
                  Dani Yogatama and
                  Gabriel Ilharco and
                  Nikolaos Pappas and
                  Yi Mao and
                  Weizhu Chen and
                  Noah A. Smith},
  title        = {Finetuning Pretrained Transformers into RNNs},
  journal      = {CoRR},
  volume       = {abs/2103.13076},
  year         = {2021},
  url          = {https://arxiv.org/abs/2103.13076},
  eprinttype    = {arXiv},
  eprint       = {2103.13076},
  timestamp    = {Wed, 21 Feb 2024 11:48:06 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2103-13076.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inbook{Dandl_2020,
   title={Multi-Objective Counterfactual Explanations},
   ISBN={9783030581121},
   ISSN={1611-3349},
   url={http://dx.doi.org/10.1007/978-3-030-58112-1_31},
   DOI={10.1007/978-3-030-58112-1_31},
   booktitle={Lecture Notes in Computer Science},
   publisher={Springer International Publishing},
   author={Dandl, Susanne and Molnar, Christoph and Binder, Martin and Bischl, Bernd},
   year={2020},
   pages={448–469} 
}


@misc{DarpaXAI,
  author = {DARPA},
  title = {XAI Concept},
  howpublished = {\url{https://www.darpa.mil/program/explainable-artificial-intelligence}},
  note = {Accessed: 2024-02-19}
}


@misc{Dastin2018Amazon,
  author = {Jeffrey Dastin },
  title = {Amazon Scraps Secret AI Recruiting Tool That Showed Bias against Women},
  howpublished = {\url{https://www.reuters.com/article/amazoncom-jobs-automation/insight-amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSL2N1VB1FQ/?feedType=RSS%26feedName=companyNews}},
  note = {Accessed: 2022-04-19},
  	date = {2018-10-10},
}

% XAI - Variable Importance Permutation issues
@article{Dawid1973MarginalizationParadoxes ,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2984907},
 abstract = {We describe a range of routine statistical problems in which marginal posterior distributions derived from improper prior measures are found to have an unBayesian property--one that could not occur if proper prior measures were employed. This paradoxical possibility is shown to have several facets that can be successfully analysed in the framework of a general group structure. The results cast a shadow on the uncritical use of improper prior measures. A separate examination of a particular application of Fraser's structural theory shows that it is intrinsically paradoxical under marginalization.},
 author = {A. P. Dawid and M. Stone and J. V. Zidek},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {2},
 pages = {189--233},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Marginalization Paradoxes in Bayesian and Structural Inference},
 urldate = {2023-04-25},
 volume = {35},
 year = {1973}
}



@article{DeJong1979PredictionAS,
  title={Prediction and Substantiation: A New Approach to Natural Language Processing},
  author={Gerald DeJong},
  journal={Cogn. Sci.},
  year={1979},
  volume={3},
  pages={251-273},
  url={https://api.semanticscholar.org/CorpusID:28841837}
}


@article{Duchi2011Adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={7},
  year={2011}
}

@article{Erkan2004LexRankGL,
  title={LexRank: Graph-based Lexical Centrality as Salience in Text Summarization},
  author={G{\"u}nes Erkan and Dragomir R. Radev},
  journal={ArXiv},
  year={2004},
  volume={abs/1109.2128},
  url={https://api.semanticscholar.org/CorpusID:506350}
}

@article{Eugene2019Information,
  author       = {Eugene Kharitonov and
                  Rahma Chaabouni and
                  Diane Bouchacourt and
                  Marco Baroni},
  title        = {Information Minimization In Emergent Languages},
  journal      = {CoRR},
  volume       = {abs/1905.13687},
  year         = {2019},
  url          = {http://arxiv.org/abs/1905.13687},
  eprinttype    = {arXiv},
  eprint       = {1905.13687},
  timestamp    = {Wed, 12 Jul 2023 08:36:32 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1905-13687.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract     = {There is growing interest in studying the languages that emerge when neural agents are jointly trained to solve tasks requiring communication through a discrete channel. We investigate here the information-theoretic complexity of such languages, focusing on the basic two-agent, one-exchange setup. We find that, under common training procedures, the emergent languages are subject to an entropy minimization pressure that has also been detected in human language, whereby the mutual information between the communicating agent's inputs and the messages is minimized, within the range afforded by the need for successful communication. That is, emergent languages are (nearly) as simple as the task they are developed for allow them to be. This pressure is amplified as we increase communication channel discreteness. Further, we observe that stronger discrete-channel-driven entropy minimization leads to representations with increased robustness to overfitting and adversarial attacks. We conclude by discussing the implications of our findings for the study of natural and artificial communication systems},
}

# cite talk at deepmind https://www.youtube.com/watch?v=FgN01DCHfjU titled Towards Multi-agent Emergent Communication by Angeliki Lazaridou from  ICARL in 2023

@phdthesis{Finn2018Learning,
    Author = {Finn, Chelsea},
    Title = {Learning to Learn with Gradients},
    School = {EECS Department, University of California, Berkeley},
    Year = {2018},
    Month = {Aug},
    URL = {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2018/EECS-2018-105.html},
    Number = {UCB/EECS-2018-105},
    Abstract = {Humans have a remarkable ability to learn new concepts from only a few examples and quickly adapt to unforeseen circumstances. To do so, they build upon their prior experience and prepare for the ability to adapt, allowing the combination of previous observations with small amounts of new evidence for fast learning. In most machine learning systems, however, there are distinct train and test phases: training consists of updating the model using data, and at test time, the model is deployed as a rigid decision-making engine. In this thesis, we discuss gradient-based algorithms for learning to learn, or meta-learning, which aim to endow machines with flexibility akin to that of humans. Instead of deploying a fixed, non-adaptable system, these meta-learning techniques explicitly train for the ability to quickly adapt so that, at test time, they can learn quickly when faced with new scenarios.

To study the problem of learning to learn, we first develop a clear and formal definition of the meta-learning problem, its terminology, and desirable properties of meta-learning algorithms. Building upon these foundations, we present a class of model-agnostic meta-learning methods that embed gradient-based optimization into the learner. Unlike prior approaches to learning to learn, this class of methods focus on acquiring a transferable representation rather than a good learning rule. As a result, these methods inherit a number of desirable properties from using a fixed optimization as the learning rule, while still maintaining full expressivity, since the learned representations can control the update rule.

We show how these methods can be extended for applications in motor control by combining elements of meta-learning with techniques for deep model-based reinforcement learning, imitation learning, and inverse reinforcement learning. By doing so, we build simulated agents that can adapt in dynamic environments, enable real robots to learn to manipulate new objects by watching a video of a human, and allow humans to convey goals to robots with only a few images. Finally, we conclude by discussing open questions and future directions in meta-learning, aiming to identify the key shortcomings and limiting assumptions of our existing approaches.}
}


@book{Fisher1925Statistical,
  author = {Fisher, R.A.},
  keywords = {imported},
  publisher = {Edinburgh Oliver \& Boyd},
  title = {Statistical methods for research workers},
  edition = 1,
  year = 1925
}
@book{Fisher1932Statistical,
  author = {Fisher, R.A.},
  keywords = {imported},
  publisher = {Edinburgh Oliver \& Boyd},
  title = {Statistical methods for research workers},
  edition = 4,
  year = 1932
}
@book{Fisher1935Design,
  author = {Fisher, R.A.},
  keywords = {imported},
  publisher = {Edinburgh Oliver \& Boyd},
  title = {The Design of Experiments.},
  edition = 1,
  year = 1935
}
@book{Flanagan2008Ruby,
  title     = {The Ruby Programming Language},
  author    = {Flanagan, David and Matsumoto, Yukihiro},
  year      = {2008},
  publisher = {O'Reilly Media}
}

@online{GefforyHinton2013MOOC,
  author = {Geffory Hinton},
  title = {Neural Networks for Machine Learning [MOOC]},
  publisher= {Coursera},
  howpublished = {\url{https://www.coursera.org/learn/neural-networks}},
  year = {2012},
  month={11},
  note = {Accessed: 2019-06-17},
}

@book{GelmanHill2007Regression,
  abstract = {\emph{Data Analysis Using Regression and Multilevel/Hierarchical Models} is a comprehensive manual for the applied researcher who wants to perform data analysis using linear and nonlinear regression and multilevel models. The book introduces a wide variety of models, whilst at the same time instructing the reader in how to fit these models using available software packages. The book illustrates the concepts by working through scores of real data examples that have arisen from the authors' own applied research, with programming codes provided for each one. Topics covered include causal inference, including regression, poststratification, matching, regression discontinuity, and instrumental variables, as well as multilevel logistic regression and missing-data imputation. Practical tips regarding building, fitting, and understanding are provided throughout.},
  added-at = {2010-03-02T17:25:53.000+0100},
  address = {New York},
  author = {Gelman, Andrew and Hill, Jennifer},
  biburl = {https://www.bibsonomy.org/bibtex/2977dbf8708e1f5ad2a321eb00ec08724/jrennstich},
  booktitle = {Data Analysis Using Regression and Multilevel/Hierarchical Models},
  date-modified = {2010-02-28 21:03:32 -0500},
  interhash = {51719b25389e0e96757c89f059207b1b},
  intrahash = {977dbf8708e1f5ad2a321eb00ec08724},
  keywords = {data methodology},
  pages = {xxii, 625 p},
  publisher = {Cambridge University Press},
  timestamp = {2010-03-07T08:28:08.000+0100},
  title = {Data analysis using regression and multilevel/hierarchical models},
  volume = {Analytical methods for social research},
  year = 2007
}


@book {GlobalFinancialStabilityReportApril2009,
      author = " International Monetary Fund. Monetary and Capital Markets Department",
      title = "Global Financial Stability Report, April 2009: Responding to the Financial Crisis and Measuring Systemic Risks",
      year = "2009",
      publisher = "International Monetary Fund",
      address = "USA",
      isbn = "9781616352080",
      doi = "10.5089/9781616352080.082",
      url = "https://www.elibrary.imf.org/view/book/9781616352080/9781616352080.xml"
}

@article{Goldstein2013PeekingIT,
  title={Peeking Inside the Black Box: Visualizing Statistical Learning With Plots of Individual Conditional Expectation},
  author={Alex Goldstein and Adam Kapelner and Justin Bleich and Emily Pitkin},
  journal={Journal of Computational and Graphical Statistics},
  year={2013},
  volume={24},
  pages={44 - 65},
  url={https://api.semanticscholar.org/CorpusID:88519447}
}


% - FACE alg for XAI
@article{Granovetter1973WeakTies,
 ISSN = {00029602, 15375390},
 URL = {http://www.jstor.org/stable/2776392},
 abstract = {Analysis of social networks is suggested as a tool for linking micro and macro levels of sociological theory. The procedure is illustrated by elaboration of the macro implications of one aspect of small-scale interaction: the strength of dyadic ties. It is argued that the degree of overlap of two individuals' friendship networks varies directly with the strength of their tie to one another. The impact of this principle on diffusion of influence and information, mobility opportunity, and community organization is explored. Stress is laid on the cohesive power of weak ties. Most network models deal, implicitly, with strong ties, thus confining their applicability to small, well-defined groups. Emphasis on weak ties lends itself to discussion of relations between groups and to analysis of segments of social structure not easily defined in terms of primary groups.},
 author = {Mark S. Granovetter},
 journal = {American Journal of Sociology},
 number = {6},
 pages = {1360--1380},
 publisher = {University of Chicago Press},
 title = {The Strength of Weak Ties},
 urldate = {2023-12-31},
 volume = {78},
 year = {1973}
}

@book{GurievTreisman+2022,
  url = {https://doi.org/10.1515/9780691224466},
  title = {Spin Dictators},
  title = {The Changing Face of Tyranny in the 21st Century},
  author = {Sergei Guriev and Daniel Treisman},
  publisher = {Princeton University Press},
  address = {Princeton},
  doi = {doi:10.1515/9780691224466},
  isbn = {9780691224466},
  year = {2022},
  lastchecked = {2024-01-21},
  keywords = {dictatorship, censorship, term limit, tyrant, dictator, authoritarianism, autocracy}
}

@article{Hatna2012Schelling,
  added-at = {2019-05-23T00:00:00.000+0200},
  author = {Hatna, Erez and Benenson, Itzhak},
  biburl = {https://www.bibsonomy.org/bibtex/2b7ba45c03feba93b834966c9aa0e2c20/dblp},
  ee = {https://doi.org/10.18564/jasss.1873},
  interhash = {6278f0d45f078a214721fb053f8a7b8a},
  intrahash = {b7ba45c03feba93b834966c9aa0e2c20},
  journal = {J. Artificial Societies and Social Simulation},
  keywords = {dblp},
  number = 1,
  timestamp = {2019-05-24T11:37:35.000+0200},
  title = {The Schelling Model of Ethnic Residential Dynamics: Beyond the Integrated - Segregated Dichotomy of Patterns.},
  url = {http://dblp.uni-trier.de/db/journals/jasss/jasss15.html#HatnaB12},
  volume = 15,
  year = 2012
}


@inbook{John2003raven,
  title = "Raven Progressive Matrices",
  author = "John and Raven, Jean",
  year = 2003,
  booktitle = "Handbook of Nonverbal Assessment",
  publisher = "Springer US",
  address = "Boston, MA",
  pages = "223--237",
  doi = "10.1007/978-1-4615-0153-4_11",
  isbn = "978-1-4615-0153-4",
  url = "https://doi.org/10.1007/978-1-4615-0153-4_11",
  editor = "McCallum, R. Steve",
  abstract = "The Raven Progressive Matrices (RPM) tests measure ``general cognitive ability'' or, better, eductive, or ``meaning making,'' ability (Raven, Raven, {\&} Court, 1998a,2000). The term ``eductive'' comes from the Latin root educere, which means, ``to draw out.'' The basic version of the test, known as the Standard Progressive Matrices (or SPM), consists of five sets of items of the kind shown in Figures 11.1 and 11.2. Within each set, the items become progressively more difficult. At the beginning of each set, the items, although easy again, follow a different logic. The sets in turn become progressively more difficult. The five sets offer those taking the test five opportunities to become familiar with the method of thought required to solve the problems. In addition to the Standard series, there is the Coloured Progressive Matrices (CPM), which is designed to spread the scores of children and less able adults and the Advanced Progressive Matrices (APM), developed to spread the scores of the top 20{\%} of the population.",
}
@article{Kirby2000,
    author = {Kirby, Simon},
    title = "{Natural Language From Artificial Life}",
    journal = {Artificial Life},
    volume = {8},
    number = {2},
    pages = {185-215},
    year = {2002},
    month = {04},
    abstract = "{This article aims to show that linguistics, in particular the study of the lexico-syntactic aspects of language, provides fertile ground for artificial life modeling. A survey of the models that have been developed over the last decade and a half is presented to demonstrate that ALife techniques have a lot to offer an explanatory theory of language. It is argued that this is because much of the structure of language is determined by the interaction of three complex adaptive systems: learning, culture, and biological evolution. Computational simulation, informed by theoretical linguistics, is an appropriate response to the challenge of explaining real linguistic data in terms of the processes that underpin human language.}",
    issn = {1064-5462},
    doi = {10.1162/106454602320184248},
    url = {https://doi.org/10.1162/106454602320184248},
    eprint = {https://direct.mit.edu/artl/article-pdf/8/2/185/1661892/106454602320184248.pdf},
}


@inproceedings{Kupiec1995ATD,
  title={A trainable document summarizer},
  author={Julian Kupiec and Jan O. Pedersen and Francine R. Chen},
  booktitle={Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
  year={1995},
  url={https://courses.ischool.berkeley.edu/i256/f06/papers/kupiec95.pdf}
}

@article{Lazaridou2023Towards,
  author       = {Angeliki Lazaridou},
  title        = {Towards Multi-agent Emergent Communication},
  year         = {2023},
  url          = {https://www.youtube.com/watch?v=FgN01DCHfjU},
  timestamp    = {Wed, 12 Jul 2023 08:36:32 +0200},
  abstract     = {In this talk, Angeliki Lazaridou discusses the latest research on multi-agent emergent communication.},
  
}

@article{Massey1988Segregation,
 ISSN = {00377732, 15347605},
 URL = {http://www.jstor.org/stable/2579183},
 abstract = {This paper conceives of residential segregation as a multidimensional phenomenon varying along five distinct axes of measurement: evenness, exposure, concentration, centralization, and clustering. Twenty indices of segregation are surveyed and related conceptually to one of the five dimensions. Using data from a large set of U.S. metropolitan areas, the indices are intercorrelated and factor analyzed. Orthogonal and oblique rotations produce pattern matrices consistent with the postulated dimensional structure. Based on the factor analyses and other information, one index was chosen to represent each of the five dimensions, and these selections were confirmed with a principal components analysis. The paper recommends adopting these indices as standard indicators in future studies of segregation.},
 author = {Douglas S. Massey and Nancy A. Denton},
 journal = {Social Forces},
 number = {2},
 pages = {281--315},
 publisher = {Oxford University Press},
 title = {The Dimensions of Residential Segregation},
 urldate = {2024-03-13},
 volume = {67},
 year = {1988}
}


@online{Mauro2022segregation,
author={Mirco Nanni},
title =  {Segregation models},
url = {http://didawiki.cli.di.unipi.it/lib/exe/fetch.php/geospatialanalytics/gsa/lesson_09_-_segregation.pdf},
year={2023},
urldate = {2024-03-13},
}

@book{McElreath2020Rethinking,
  author = {McElreath, Richard},
  keywords = {book stats},
  title = {Statistical Rethinking, A Course in R and Stan},
  year = 2015
}
@article{Miller17Explanation,
  author       = {Tim Miller},
  title        = {Explanation in Artificial Intelligence: Insights from the Social Sciences},
  journal      = {CoRR},
  volume       = {abs/1706.07269},
  year         = {2017},
  url          = {http://arxiv.org/abs/1706.07269},
  eprinttype    = {arXiv},
  eprint       = {1706.07269},
  timestamp    = {Mon, 13 Aug 2018 16:47:22 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/Miller17a.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


% - LIME algorithm for XAI
@inproceedings{Mnih2012Learning,
author = {Mnih, Volodymyr and Hinton, Geoffrey},
title = {Learning to label aerial images from noisy data},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {When training a system to label images, the amount of labeled training data tends to be a limiting factor. We consider the task of learning to label aerial images from existing maps. These provide abundant labels, but the labels are often incomplete and sometimes poorly registered. We propose two robust loss functions for dealing with these kinds of label noise and use the loss functions to train a deep neural network on two challenging aerial image datasets. The robust loss functions lead to big improvements in performance and our best system substantially outperforms the best published results on the task we consider.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {203–210},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{NIPS2006_87f4d79e,
 author = {Ranzato, Marc\textquotesingle aurelio and Poultney, Christopher and Chopra, Sumit and Cun, Yann},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {B. Sch\"{o}lkopf and J. Platt and T. Hoffman},
 pages = {},
 publisher = {MIT Press},
 title = {Efficient Learning of Sparse Representations with an Energy-Based Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/2006/file/87f4d79e36d68c3031ccf6c55e9bbd39-Paper.pdf},
 volume = {19},
 year = {2006}
}

@inproceedings{NIPS2012_c399862d,
 author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
 volume = {25},
 year = {2012}
}


@inproceedings{NIPS2012_c399862d,
 author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
 volume = {25},
 year = {2012}
}


@article{Nowak1999,
author = {Martin A. Nowak  and David C. Krakauer },
title = {The evolution of language},
journal = {Proceedings of the National Academy of Sciences},
volume = {96},
number = {14},
pages = {8028-8033},
year = {1999},
doi = {10.1073/pnas.96.14.8028},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.96.14.8028},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.96.14.8028},
abstract = {The emergence of language was a defining moment in the evolution of
 modern humans. It was an innovation that changed radically the
 character of human society. Here, we provide an approach to language
 evolution based on evolutionary game theory. We explore the ways in
 which protolanguages can evolve in a nonlinguistic society and how
 specific signals can become associated with specific objects. We assume
 that early in the evolution of language, errors in signaling and
 perception would be common. We model the probability of
 misunderstanding a signal and show that this limits the number of
 objects that can be described by a protolanguage. This “error
 limit” is not overcome by employing more sounds but by combining a
 small set of more easily distinguishable sounds into words. The process
 of “word formation” enables a language to encode an essentially
 unlimited number of objects. Next, we analyze how words can be combined
 into sentences and specify the conditions for the evolution of very
 simple grammatical rules. We argue that grammar originated as a
 simplified rule system that evolved by natural selection to reduce
 mistakes in communication. Our theory provides a systematic approach
 for thinking about the origin and evolution of human language.}}


@book{Nowak2006Evolutionary,
   title =     {Evolutionary Dynamics: Exploring the Equations of Life},
   author =    {Martin A. Nowak},
   publisher = {Belknap Press of Harvard University Press},
   isbn =      {0674023382,9780674023383},
   year =      {2006},
}

@inproceedings{Osborne2002UsingME,
  title={Using maximum entropy for sentence extraction},
  author={Miles Osborne},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2002},
  url={https://aclanthology.org/W02-0401.pdf}
}

@misc{PR2022AI,
  author = {Precedence Research},
  title = {Artificial Intelligence Market Size to Surpass Around US$ 1,597.1 Bn By 2030},
  howpublished = {\url{https://www.globenewswire.com/news-release/2022/04/19/2424179/0/en/Artificial-Intelligence-Market-Size-to-Surpass-Around-US-1-597-1-Bn-By-2030.html}},
  note = {Accessed: 2022-04-19}
}

@article{Poyiadzi2019Feasible,
  author       = {Rafael Poyiadzi and
                  Kacper Sokol and
                  Ra{\'{u}}l Santos{-}Rodriguez and
                  Tijl De Bie and
                  Peter A. Flach},
  title        = {{FACE:} Feasible and Actionable Counterfactual Explanations},
  journal      = {CoRR},
  volume       = {abs/1909.09369},
  year         = {2019},
  url          = {http://arxiv.org/abs/1909.09369},
  eprinttype    = {arXiv},
  eprint       = {1909.09369},
  timestamp    = {Thu, 14 Oct 2021 09:17:24 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1909-09369.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

% - DeepRED alg for XAI
@article{Radev2000CentroidbasedSO,
  title={Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies},
  author={Dragomir R. Radev and Hongyan Jing and Malgorzata Budzikowska},
  journal={ArXiv},
  year={2000},
  volume={cs.CL/0005020},
  url={https://arxiv.org/pdf/cs/0005020.pdf},
}

@ARTICLE{RePEc:spr:joevec:v:29:y:2019:i:1:d:10.1007_s00191-019-00609-y,
title = {More is different... and complex! the case for agent-based macroeconomics},
author = {Dosi, Giovanni and Roventini, Andrea},
year = {2019},
journal = {Journal of Evolutionary Economics},
volume = {29},
number = {1},
pages = {1-37},
abstract = {Abstract This work nests the Agent-Based macroeconomic perspective into the earlier history of macroeconomics. We discuss how the discipline in the 70’s took a perverse path relying on models grounded on fictitious rational representative agent in order to try to pathetically circumvent aggregation and coordination problems. The Great Recession was a natural experiment for macroeconomics, showing the inadequacy of the predominant theoretical framework grounded on DSGE models. After discussing the pathological fallacies of the DSGE-based approach, we claim that macroeconomics should consider the economy as a complex evolving system, i.e. as an ecology populated by heterogenous agents, whose far-from-equilibrium interactions continuously change the structure of the system. This in turn implies that more is different: macroeconomics cannot be shrink to representative-agent micro, but agents’ complex interactions lead to emergence of new phenomena and hierarchical structure at the macro level. This is what is taken into account by agent-based models, which provide a novel way to model complex economies from the bottom-up, with sound empirically-based microfoundations. We present the foundations of Agent-Based macroeconomics and we discuss how the contributions of this special issue push its frontier forward. Finally, we conclude by discussing the ways ahead for the fully acknowledgement of agent-based models as the standard way of theorizing in macroeconomics.},
keywords = {Macroeconomics; Economic policy; Keynesian theory; New neoclassical synthesis; New Keynesian models; DSGE models; Agent-based evolutionary models; Complexity theory; Great recession; Crisis},
url = {https://EconPapers.repec.org/RePEc:spr:joevec:v:29:y:2019:i:1:d:10.1007_s00191-019-00609-y}
}

@inproceedings{Ribeiro2016Why,
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
title = {{Why Should I Trust You?}: Explaining the Predictions of Any Classifier},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939778},
doi = {10.1145/2939672.2939778},
abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1135–1144},
numpages = {10},
keywords = {interpretable machine learning, interpretability, explaining machine learning, black box classifier},
location = {San Francisco, California, USA},
series = {KDD '16}
}

% - SHAP algorithm for XAI
@online{SEuser2017Deep,
  author = {user8272359},
  title = {Deep Neural Network using Keras/Tensorflow solves Spiral Dataset Classification. But Accuracy is stuck around 50%},
  date = {2017-08-05},
  url = {https://datascience.stackexchange.com/questions/22830/deep-neural-network-using-keras-tensorflow-solves-spiral-dataset-classification},
  langid = {en}
}

@book{Samuelson1998Evolutionary,
   title =     {Evolutionary Games and Equilibrium Selection  },
   author =    {Larry Samuelson},
   publisher = {MIT Press},
   isbn =      {0262692198,9780262692199},
   year =      {1998},
}

@book{Schank1977Scripts,
  added-at = {2010-01-18T12:17:53.000+0100},
  author = {Schank, R.C. and Abelson, R.},
  keywords = {imported},
  owner = {blev},
  publisher = {Hillsdale, NJ: Earlbaum Assoc},
  timestamp = {2010-01-18T12:17:57.000+0100},
  title = {Scripts, Plans, Goals, and Understanding},
  year = 1977
}

@article{Selvaraju2016GradCam,
  author       = {Ramprasaath R. Selvaraju and
                  Abhishek Das and
                  Ramakrishna Vedantam and
                  Michael Cogswell and
                  Devi Parikh and
                  Dhruv Batra},
  title        = {Grad-CAM: Why did you say that? Visual Explanations from Deep Networks
                  via Gradient-based Localization},
  journal      = {CoRR},
  volume       = {abs/1610.02391},
  year         = {2016},
  url          = {http://arxiv.org/abs/1610.02391},
  eprinttype    = {arXiv},
  eprint       = {1610.02391},
  timestamp    = {Mon, 13 Aug 2018 16:46:58 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/SelvarajuDVCPB16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

% - MIE alg for XAI
@incollection{Skyrms2010signals,
    author = {Skyrms, Brian},
    isbn = {9780199580828},
    title = "{14512 Complex Signals and Compositionality}",
    booktitle = "{Signals: Evolution, Learning, and Information}",
    publisher = {Oxford University Press},
    year = {2010},
    month = {04},
    abstract = "{This chapter focuses on an earlier point in the evolution of signaling. It considers how one might come to have — in the most primitive way — a complex signal composed of simple signals. This is done with the smallest departure possible from signaling models that have been previously examined in this book.}",
    doi = {10.1093/acprof:oso/9780199580828.003.0013},
    url = {https://doi.org/10.1093/acprof:oso/9780199580828.003.0013},
}

@incollection{Skyrms2010signalsCh12,
    author = {Skyrms, Brian},
    isbn = {9780199580828},
    title = "{14512 Complex Signals and Compositionality}",
    booktitle = "{Signals: Evolution, Learning, and Information}",
    publisher = {Oxford University Press},
    year = {2010},
    month = {04},
    abstract = "{This chapter focuses on an earlier point in the evolution of signaling. It considers how one might come to have — in the most primitive way — a complex signal composed of simple signals. This is done with the smallest departure possible from signaling models that have been previously examined in this book.}",
    doi = {10.1093/acprof:oso/9780199580828.003.0013},
    url = {https://doi.org/10.1093/acprof:oso/9780199580828.003.0013},
    eprint = {https://academic.oup.com/book/0/chapter/143895157/chapter-ag-pdf/45018345/book\_3092\_section\_143895157.ag.pdf},
}

@article{Smith1973LogicAnimalConflict,
  title={The Logic of Animal Conflict},
  author={J. Maynard Smith and G. Randall Price},
  journal={Nature},
  year={1973},
  volume={246},
  pages={15-18},
  url={https://api.semanticscholar.org/CorpusID:4224989}
}


@book{sutton1998reinforcement,
  added-at = {2019-07-13T10:11:53.000+0200},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  biburl = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
  edition = {Second},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  keywords = {},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  title = {Reinforcement Learning: An Introduction},
  url = {http://incompleteideas.net/book/the-book-2nd.html},
  year = {2018 }
}


@article{Taylor1978ESS,
  title={Evolutionarily Stable Strategies and Game Dynamics},
  author={Peter D. Taylor and Leo B. Jonker},
  journal={Bellman Prize in Mathematical Biosciences},
  year={1978},
  volume={40},
  pages={145-156},
  url={https://api.semanticscholar.org/CorpusID:15554796}
}

@article{Textor2017Dagity,
    author = {Textor, Johannes and van der Zander, Benito and Gilthorpe, Mark S and Liśkiewicz, Maciej and Ellison, George TH},
    title = "{Robust causal inference using directed acyclic graphs: the R package ‘dagitty’}",
    journal = {International Journal of Epidemiology},
    volume = {45},
    number = {6},
    pages = {1887-1894},
    year = {2017},
    month = {01},
    abstract = "{Directed acyclic graphs (DAGs), which offer systematic representations of causal relationships, have become an established framework for the analysis of causal inference in epidemiology, often being used to determine covariate adjustment sets for minimizing confounding bias. DAGitty is a popular web application for drawing and analysing DAGs. Here we introduce the R package ‘dagitty’, which provides access to all of the capabilities of the DAGitty web application within the R platform for statistical computing, and also offers several new functions. We describe how the R package ‘dagitty’ can be used to: evaluate whether a DAG is consistent with the dataset it is intended to represent; enumerate ‘statistically equivalent’ but causally different DAGs; and identify exposure-outcome adjustment sets that are valid for causally different but statistically equivalent DAGs. This functionality enables epidemiologists to detect causal misspecifications in DAGs and make robust inferences that remain valid for a range of different DAGs. The R package ‘dagitty’ is available through the comprehensive R archive network (CRAN) at [https://cran.r-project.org/web/packages/dagitty/]. The source code is available on github at [https://github.com/jtextor/dagitty]. The web application ‘DAGitty’ is free software, licensed under the GNU general public licence (GPL) version 2 and is available at [http://dagitty.net/].}",
    issn = {0300-5771},
    doi = {10.1093/ije/dyw341},
    url = {https://doi.org/10.1093/ije/dyw341},
    eprint = {https://academic.oup.com/ije/article-pdf/45/6/1887/11120744/dyw341.pdf},
}
@inproceedings{Tolomei2017,
author = {Tolomei, Gabriele and Silvestri, Fabrizio and Haines, Andrew and Lalmas, Mounia},
title = {Interpretable Predictions of Tree-based Ensembles via Actionable Feature Tweaking},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098039},
doi = {10.1145/3097983.3098039},
abstract = {Machine-learned models are often described as "black boxes". In many real-world applications however, models may have to sacrifice predictive power in favour of human-interpretability. When this is the case, feature engineering becomes a crucial task, which requires significant and time-consuming human effort. Whilst some features are inherently static, representing properties that cannot be influenced (e.g., the age of an individual), others capture characteristics that could be adjusted (e.g., the daily amount of carbohydrates taken). Nonetheless, once a model is learned from the data, each prediction it makes on new instances is irreversible - assuming every instance to be a static point located in the chosen feature space. There are many circumstances however where it is important to understand (i) why a model outputs a certain prediction on a given instance, (ii) which adjustable features of that instance should be modified, and finally (iii) how to alter such a prediction when the mutated instance is input back to the model.In this paper, we present a technique that exploits the internals of a tree-based ensemble classifier to offer recommendations for transforming true negative instances into positively predicted ones. We demonstrate the validity of our approach using an online advertising application. First, we design a Random Forest classifier that effectively separates between two types of ads: low (negative) and high (positive) quality ads (instances). Then, we introduce an algorithm that provides recommendations that aim to transform a low quality ad (negative instance) into a high quality one (positive instance). Finally, we evaluate our approach on a subset of the active inventory of a large ad network, Yahoo Gemini.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {465–474},
numpages = {10},
keywords = {actionable feature tweaking, altering model predictions, model interpretability, random forest, recommending feature changes},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@Article{Wagner2013Costly,
AUTHOR = {Wagner, Elliott O.},
TITLE = {The Dynamics of Costly Signaling},
JOURNAL = {Games},
VOLUME = {4},
YEAR = {2013},
NUMBER = {2},
PAGES = {163--181},
URL = {https://www.mdpi.com/2073-4336/4/2/163},
ISSN = {2073-4336},
ABSTRACT = {Costly signaling is a mechanism through which the honesty of signals can be secured in equilibrium, even in interactions where communicators have conflicting interests. This paper explores the dynamics of one such signaling game: Spence’s model of education. It is found that separating equilibria are unlikely to emerge under either the replicator or best response dynamics, but that partially communicative mixed equilibria are quite important dynamically. These mixtures are Lyapunov stable in the replicator dynamic and asymptotically stable in the best response dynamic. Moreover, they have large basins of attraction, in fact larger than those of either pooling or separating equilibria. This suggests that these mixtures may play significant, and underappreciated, roles in the explanation of the emergence and stability of information transfer.},
DOI = {10.3390/g4020163}
}

@misc{Webster, 
author = {Dr Kevin Webster},
title = {Emmy {Amalie} {Noether}},
howpublished = "\url{http://www-groups.dcs.st-and.ac.uk/~history/Biographies/Noether_Emmy.html}",
year = {2014}, 
note = "Accessed 11/02/16",
}

@book{Weibull1997,
  added-at = {2011-01-13T13:26:37.000+0100},
  author = {Weibull, Jörgen W.},
  biburl = {https://www.bibsonomy.org/bibtex/2193693bcaf4950c4054a64f359294b17/rincedd},
  interhash = {79846b2680f56bad863ff85f206e863e},
  intrahash = {193693bcaf4950c4054a64f359294b17},
  keywords = {game-theory evolution},
  publisher = {The MIT Press},
  timestamp = {2011-01-13T13:26:37.000+0100},
  title = {Evolutionary Game Theory},
  year = 1997
}

@inproceedings{Wheeler1999InformationPQ,
  title={Information, physics, quantum: the search for links},
  author={John Archibald Wheeler},
  year={1999},
  url={https://api.semanticscholar.org/CorpusID:118325979}
}

  @article{Wiesenfarth2020Quantification,
author = {Wiesenfarth, Manuel and Calderazzo, Silvia},
title = {Quantification of prior impact in terms of effective current sample size},
journal = {Biometrics},
volume = {76},
number = {1},
pages = {326-336},
keywords = {Bayesian adaptive clinical trial design, prior-data conflict, prior effective sample size, prior elicitation, prior information, robust priors},
doi = {https://doi.org/10.1111/biom.13124},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/biom.13124},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/biom.13124},
abstract = {Abstract Bayesian methods allow borrowing of historical information through prior distributions. The concept of prior effective sample size (prior ESS) facilitates quantification and communication of such prior information by equating it to a sample size. Prior information can arise from historical observations; thus, the traditional approach identifies the ESS with such a historical sample size. However, this measure is independent of newly observed data, and thus would not capture an actual “loss of information” induced by the prior in case of prior-data conflict. We build on a recent work to relate prior impact to the number of (virtual) samples from the current data model and introduce the effective current sample size (ECSS) of a prior, tailored to the application in Bayesian clinical trial designs. Special emphasis is put on robust mixture, power, and commensurate priors. We apply the approach to an adaptive design in which the number of recruited patients is adjusted depending on the effective sample size at an interim analysis. We argue that the ECSS is the appropriate measure in this case, as the aim is to save current (as opposed to historical) patients from recruitment. Furthermore, the ECSS can help overcome lack of consensus in the ESS assessment of mixture priors and can, more broadly, provide further insights into the impact of priors. An R package accompanies the paper.},
year = {2020}
}

@article{Wiesenfarth2020Quantification,
author = {Wiesenfarth, Manuel and Calderazzo, Silvia},
title = {Quantification of prior impact in terms of effective current sample size},
journal = {Biometrics},
volume = {76},
number = {1},
pages = {326-336},
keywords = {Bayesian adaptive clinical trial design, prior-data conflict, prior effective sample size, prior elicitation, prior information, robust priors},
doi = {https://doi.org/10.1111/biom.13124},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/biom.13124},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/biom.13124},
abstract = {Abstract Bayesian methods allow borrowing of historical information through prior distributions. The concept of prior effective sample size (prior ESS) facilitates quantification and communication of such prior information by equating it to a sample size. Prior information can arise from historical observations; thus, the traditional approach identifies the ESS with such a historical sample size. However, this measure is independent of newly observed data, and thus would not capture an actual “loss of information” induced by the prior in case of prior-data conflict. We build on a recent work to relate prior impact to the number of (virtual) samples from the current data model and introduce the effective current sample size (ECSS) of a prior, tailored to the application in Bayesian clinical trial designs. Special emphasis is put on robust mixture, power, and commensurate priors. We apply the approach to an adaptive design in which the number of recruited patients is adjusted depending on the effective sample size at an interim analysis. We argue that the ECSS is the appropriate measure in this case, as the aim is to save current (as opposed to historical) patients from recruitment. Furthermore, the ECSS can help overcome lack of consensus in the ESS assessment of mixture priors and can, more broadly, provide further insights into the impact of priors. An R package accompanies the paper.},
year = {2020}
}

@book{Winston2020Clear,
    author = {Winston, Patrick Henry},
    title = "{Make It Clear: Speak and Write to Persuade and Inform}",
    publisher = {The MIT Press},
    year = {2020},
    month = {08},
    abstract = "{The essentials of communication for professionals, educators, students, and entrepreneurs, from organizing your thoughts to inspiring your audience.Do you give presentations at meetings? Do you ever have to explain a complicated subject to audiences unfamiliar with your field? Do you make pitches for ideas or products? Do you want to interest a lecture hall of restless students in subjects that you find fascinating? Then you need this book. Make It Clear explains how to communicate—how to speak and write to get your ideas across. Written by an MIT professor who taught his students these techniques for more than forty years, the book starts with the basics—finding your voice, organizing your ideas, making sure what you say is remembered, and receiving critiques (“do not ask for brutal honesty”)—and goes on to cover such specifics as preparing slides, writing and rewriting, and even choosing a type family. The book explains why you should start with an empowerment promise and conclude by noting you delivered on that promise. It describes how a well-crafted, explicitly identified slogan, symbol, salient idea, surprise, and story combine to make you and your work memorable. The book lays out the VSN-C (Vision, Steps, News–Contributions) framework as an organizing structure and then describes how to create and organize your ideas with a “broken–glass” outline, how to write to be understood, how to inspire, how to defeat writer's block—and much more. Learning how to speak and write well will empower you and make you smarter. Effective communication can be life-changing—making use of just one principle in this book can get you the job, make the sale, convince your boss, inspire a student, or even start a revolution.}",
    isbn = {9780262360395},
    doi = {10.7551/mitpress/12406.001.0001},
    url = {https://doi.org/10.7551/mitpress/12406.001.0001},
}

@misc{XaitkSaliency,
  title = {XAItk Saliency},
  howpublished = {\url{https://xaitk-saliency.readthedocs.io/en/latest/installation.html}},
  note = {Accessed: 2024-02-19}
}

% - scikit-learn library
@inproceedings{Zilke2016DeepREDR,
  title={DeepRED - Rule Extraction from Deep Neural Networks},
  author={Jan Ruben Zilke and Eneldo Loza Menc{\'i}a and Frederik Janssen},
  booktitle={IFIP Working Conference on Database Semantics},
  year={2016},
  url={https://api.semanticscholar.org/CorpusID:10289003}
}

% - Grad-CAM alg for XAI
@book{acemoglu2012nations,
  title={Why Nations Fail: The Origins of Power, Prosperity and Poverty},
  author={Acemoglu, D. and Robinson, J.A.},
  isbn={9781847654618},
  year={2012},
  publisher={Profile}
}


@article{aengenheyster2017realtime,
	title = {Real-{Time} {Delphi} in practice — {A} comparative analysis of existing software-based tools},
	volume = {118},
	issn = {00401625},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0040162517301117},
	doi = {10.1016/j.techfore.2017.01.023},
	language = {en},
	urldate = {2021-09-19},
	journal = {Technological Forecasting and Social Change},
	author = {Aengenheyster, Stefan and Cuhls, Kerstin and Gerhold, Lars and Heiskanen-Schüttler, Maria and Huck, Jana and Muszynska, Monika},
	month = may,
	year = {2017},
	pages = {15--27},
}

@book{alexander2022telling,
  title = {Telling Stories with Data},
  author = {Rohan Alexander},
  year = 2022,
  publisher = {CRC Press},
  url = {https://tellingstorieswithdata.com},
}
@book{alexander2022telling,
  title = {Telling Stories with Data},
  author = {Rohan Alexander},
  year = 2022,
  publisher = {CRC Press},
  url = {https://tellingstorieswithdata.com},
}
@book{alvin1995war,
  title={War and Anti-war},
  author={Alvin Toffler and Toffler, H.},
  isbn={9780446602594},
  lccn={93020789},
  series={Warner Books},
  url={https://www.google.com/books?id=lFXSHAAACAAJ},
  year={1995},
  publisher={Warner Books},
  keywords = {war, {international relations}, {political science},}
}

@incollection{amarel1981representations,
  title = {On Representations of Problems of Reasoning about Actions},
  author = {Saul Amarel},
  year = 1981,
  booktitle = {Readings in Artificial Intelligence},
  publisher = {Morgan Kaufmann},
  pages = {2--22},
  doi = {https://doi.org/10.1016/B978-0-934613-03-3.50006-4},
  isbn = {978-0-934613-03-3},
  url = {https://www.sciencedirect.com/science/article/pii/B9780934613033500064},
  editor = {Bonnie Lynn Webber and Nils J. Nilsson},
  abstract = {Publisher Summary This chapter discusses the basic issues of choice of representation for problems of reasoning about actions. The general problem of representation is concerned with the relationship between different ways of formulating a problem to a problem solving system and the efficiency with which the system can be expected to find a solution to the problem. An understanding of the relationship between problem formulation and problem solving efficiency is a prerequisite for the design of procedures that can automatically choose the most appropriate representation of a problem—they can find a point of view of the problem that maximally simplifies the process of finding a solution. The chapter discusses a specific problem of transportation scheduling—the missionaries and cannibals problem—to evaluate the effects of alternative formulations of this problem on the expected efficiency of mechanical procedures for solving it and also to examine the processes that come into play when a transition takes place from a given problem formulation into a better one.},
}
@incollection{amarel1981representations,
  title = {On Representations of Problems of Reasoning about Actions},
  author = {Saul Amarel},
  year = 1981,
  booktitle = {Readings in Artificial Intelligence},
  publisher = {Morgan Kaufmann},
  pages = {2--22},
  doi = {https://doi.org/10.1016/B978-0-934613-03-3.50006-4},
  isbn = {978-0-934613-03-3},
  url = {https://www.sciencedirect.com/science/article/pii/B9780934613033500064},
  editor = {Bonnie Lynn Webber and Nils J. Nilsson},
  abstract = {Publisher Summary This chapter discusses the basic issues of choice of representation for problems of reasoning about actions. The general problem of representation is concerned with the relationship between different ways of formulating a problem to a problem solving system and the efficiency with which the system can be expected to find a solution to the problem. An understanding of the relationship between problem formulation and problem solving efficiency is a prerequisite for the design of procedures that can automatically choose the most appropriate representation of a problem—they can find a point of view of the problem that maximally simplifies the process of finding a solution. The chapter discusses a specific problem of transportation scheduling—the missionaries and cannibals problem—to evaluate the effects of alternative formulations of this problem on the expected efficiency of mechanical procedures for solving it and also to examine the processes that come into play when a transition takes place from a given problem formulation into a better one.},
}
@article{arkes1981impediments,
  title={Impediments to accurate clinical judgment and possible ways to minimize their impact.},
  author={Arkes, Hal R},
  journal={Journal of consulting and clinical psychology},
  volume={49},
  number={3},
  pages={323},
  year={1981},
  publisher={American Psychological Association}
}

@article{arkes1988eliminating,
  title={Eliminating the hindsight bias.},
  author={Arkes, Hal R and Faust, David and Guilmette, Thomas J and Hart, Kathleen},
  journal={Journal of applied psychology},
  volume={73},
  number={2},
  pages={305},
  year={1988},
  publisher={American Psychological Association},
  abstract={Those who consider the likelihood of an event after it has occurred exaggerate their likelihood of having been able to predict that event in advance. We attempted to eliminate this hindsight bias among 194 neuropsychologists. Foresight subjects read a case history and were asked to estimate the probability of three different diagnoses. Subjects in each of the three hindsight groups were told that one of the three diagnoses was correct and were asked to state what probability they would have assigned to each diagnosis if they were making the original diagnosis. Foresight-reasons and hindsight-reasons subjects performed the same task as their foresight and hindsight counterparts, except they had to list one reason why each of the possible diagnoses might be correct. The frequency of subjects succumbing to the hindsight bias was lower in the hindsight-reasons groups than in the hindsight groups not asked to list reasons χ–2 (1, N= 140)= 4.12, p<. 05.}
}

@article{arkes1991costs,
  title={Costs and benefits of judgment errors: Implications for debiasing.},
  author={Arkes, Hal R},
  journal={Psychological bulletin},
  volume={110},
  number={3},
  pages={486},
  year={1991},
  publisher={American Psychological Association},
  abstract={Questioned the ecological validity of judgmental biases demonstrated in the laboratory. One objection to these demonstrations is that evolutionary pressures would have rendered such maladaptive behaviors extinct if they had any impact in the" real world." The author attempts to show that even beneficial adaptations may have costs. This argument is extended to propose 3 types of judgment errors (strategy-based errors, association-based errors, and psychophysical based errors), each of which is a cost of a highly adaptive system. This taxonomy of judgment behaviors is used to advance hypotheses as to which debiasing techniques are likely to succeed in each category.}
}

@ARTICLE{arrow1950figgiculty,
title = {A Difficulty in the Concept of Social Welfare},
author = {Arrow, Kenneth},
year = {1950},
journal = {Journal of Political Economy},
volume = {58},
doi = {doi:10.1086/256963},
pages = {328-346},
url = {https://EconPapers.repec.org/RePEc:ucp:jpolec:v:58:y:1950:p:328}
}

@misc{balestriero2023cookbook,
      title={A Cookbook of Self-Supervised Learning}, 
      author={Randall Balestriero and Mark Ibrahim and Vlad Sobal and Ari Morcos and Shashank Shekhar and Tom Goldstein and Florian Bordes and Adrien Bardes and Gregoire Mialon and Yuandong Tian and Avi Schwarzschild and Andrew Gordon Wilson and Jonas Geiping and Quentin Garrido and Pierre Fernandez and Amir Bar and Hamed Pirsiavash and Yann LeCun and Micah Goldblum},
      year={2023},
      eprint={2304.12210},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inbook{barlow1992,
	title = {Introduction to de Finetti (1937) Foresight: Its Logical Laws, Its Subjective Sources},
	author = {Barlow, R. E.},
	year = {1992},
	date = {1992},
	publisher = {Springer New York},
	pages = {127--133},
	doi = {10.1007/978-1-4612-0919-5_9},
	url = {http://dx.doi.org/10.1007/978-1-4612-0919-5_9}
}

@incollection{batali1998,
  added-at = {2006-07-13T17:11:36.000+0200},
  address = {Cambridge},
  author = {Batali, J.},
  booktitle = {Approaches to the Evolution of Language: Social and Cognitive bases},
  editor = {Hurford, J. and Knight, C. and Studdert-Kennedy, M.},
  interhash = {5224ca9320442b90a840f3288272d01b},
  intrahash = {9c612c8c6d95d3b4b9d92f2b3a1c925b},
  keywords = {imported},
  pages = {405--426},
  publisher = {Cambridge University Press},
  timestamp = {2006-07-13T17:11:36.000+0200},
  title = {Computational Simulations of the Emergence of Grammar},
  year = 1998
}

@article{bayes1763InverseProbability,
author = {Bayes, Thomas  and Price, null },
title = {LII. An essay towards solving a problem in the doctrine of chances. By the late Rev. Mr. Bayes, F. R. S. communicated by Mr. Price, in a letter to John Canton, A. M. F. R. S},
journal = {Philosophical Transactions of the Royal Society of London},
volume = {53},
number = {},
pages = {370-418},
year = {1763},
doi = {10.1098/rstl.1763.0053},
URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rstl.1763.0053},
eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rstl.1763.0053},
abstract = { Dear Sir, I Now send you an essay which I have found among the papers of our deceased friend Mr. Bayes, and which, in my opinion, has great merit, and well deserves to be preserved. }
}
@article{belle2021,
	title = {Principles and Practice of Explainable Machine Learning},
	author = {Belle, Vaishak and Papantonis, Ioannis},
	year = {2021},
	month = {07},
	date = {2021-07-01},
	journal = {Frontiers in Big Data},
	volume = {4},
	doi = {10.3389/fdata.2021.688969},
	url = {http://dx.doi.org/10.3389/fdata.2021.688969}
}

@book{belsley1980,
	title = {Regression Diagnostics},
	author = {Belsley, David A. and Kuh, Edwin and Welsch, Roy E.},
	year = {1980},
	month = {06},
	date = {1980-06-24},
	publisher = {John Wiley & Sons, Inc.},
	doi = {10.1002/0471725153},
	url = {http://dx.doi.org/10.1002/0471725153}
}

@book{bernoulli1713ars,
  title={Ars conjectandi [The Art of Conjecturing]},
  author={Bernoulli, J.},
  url={https://books.google.co.il/books?id=Ba5DAAAAcAAJ},
  year={1713},
  publisher={Impensis Thurnisiorum}
}
@article{bertrand2003enjoying,
  title={Enjoying the quiet life? Corporate governance and managerial preferences},
  author={Bertrand, Marianne and Mullainathan, Sendhil},
  journal={Journal of political Economy},
  volume={111},
  number={5},
  pages={1043--1075},
  year={2003},
  publisher={The University of Chicago Press}
}

@article{bianchi2010,
	title = {Tempered infinitely divisible distributions and processes},
	author = {Bianchi, M and Bianchi, M and {Рачев}, {Светлозар Тодоров} and Rachev, Svetlozar Todorov and Kim, Y S and Kim, Y S and Fabozzi, F J and Fabozzi, F J},
	year = {2010},
	date = {2010},
	journal = {Teoriya Veroyatnostei i ee Primeneniya},
	pages = {59--86},
	volume = {55},
	number = {1},
	doi = {10.4213/tvp4176},
	url = {http://dx.doi.org/10.4213/tvp4176},
	langid = {ru}
}

@book{bishop2009big,
  title={The Big Sort: Why the Clustering of Like-Minded America Is Tearing Us Apart},
  author={Bishop, B.},
  isbn={9780547525198},
  year={2009},
  publisher={Houghton Mifflin Harcourt}
}

@book{bishop2013pattern,
  title={Pattern Recognition and Machine Learning},
  author={Bishop, C.M.},
  isbn={9780387310732},
  series={Information science and statistics},
  url={https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf},
  year={2006},
  publisher={Springer (India) Private Limited}
}

@article{blundell2015,
	title = {Weight Uncertainty in Neural Networks},
	author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
	year = {2015},
	date = {2015},
	doi = {10.48550/ARXIV.1505.05424},
	url = {https://arxiv.org/abs/1505.05424}
}


@misc{blöbaum2022dowhy,
  title = {DoWhy-GCM: An extension of DoWhy for causal inference in graphical causal models},
  author = {Patrick Blöbaum and Peter Götz and Kailash Budhathoki and Atalanti A. Mastakouri and Dominik Janzing},
  year = 2022,
  eprint = {2206.06821},
  archiveprefix = {arXiv},
  primaryclass = {stat.ME},
}
@misc{blöbaum2022dowhy,
  title = {DoWhy-GCM: An extension of DoWhy for causal inference in graphical causal models},
  author = {Patrick Blöbaum and Peter Götz and Kailash Budhathoki and Atalanti A. Mastakouri and Dominik Janzing},
  year = 2022,
  eprint = {2206.06821},
  archiveprefix = {arXiv},
  primaryclass = {stat.ME},
}
@article{bose2002contemporary,
  title={A contemporary review and bibliography of infinitely divisible distributions and processes},
  author={Bose, Arup and Dasgupta, Anirban and Rubin, Herman},
  journal={Sankhy{\=a}: The Indian Journal of Statistics, Series A},
  pages={763--819},
  year={2002},
  publisher={JSTOR}
}

@article{box1976,
	title = {Science and Statistics},
	author = {Box, George E. P.},
	year = {1976},
	month = {12},
	date = {1976-12},
	journal = {Journal of the American Statistical Association},
	pages = {791--799},
	volume = {71},
	number = {356},
	doi = {10.1080/01621459.1976.10480949},
	url = {http://dx.doi.org/10.1080/01621459.1976.10480949},
	langid = {en}
}


@book{bradsher2004high,
  title={High and Mighty: The Dangerous Rise of the SUV},
  author={Bradsher, K.},
  isbn={9781586482039},
  lccn={2002028722},
  year={2004},
  publisher={PublicAffairs}
}


@book{breiman1984classification,
  title={Classification and Regression Trees},
  author={Breiman, L. and Friedman, J. and Stone, C.J. and Olshen, R.A.},
  isbn={9780412048418},
  lccn={83019708},
  url={https://www.google.com/books?id=JwQx-WOmSyQC},
  year={1984},
  publisher={Taylor \& Francis}
}

% - Rhetoric Course
% ENTRYSUBTYPE is a localisation string
@article{breiman2001,
	author = {Breiman, Leo},
	year = {2001},
	date = {2001},
	journal = {Machine Learning},
	pages = {5--32},
	volume = {45},
	number = {1},
	doi = {10.1023/a:1010933404324},
	url = {http://dx.doi.org/10.1023/A:1010933404324}
}


% - feature selection etc
@article{breiman2001a,
  title = {Random Forests},
	author = {Breiman, Leo},
	year = {2001},
	date = {2001},
	journal = {Machine Learning},
	pages = {5--32},
	volume = {45},
	number = {1},
	doi = {10.1023/a:1010933404324},
	url = {http://dx.doi.org/10.1023/A:1010933404324}
}

@article{bush1953stochastic,
  author = {Robert R. Bush and Frederick Mosteller},
  title = {{A Stochastic Model with Applications to Learning}},
  volume = {24},
  journal = {The Annals of Mathematical Statistics},
  number = {4},
  publisher = {Institute of Mathematical Statistics},
  pages = {559 -- 585},
  year = {1953},
  doi = {10.1214/aoms/1177728914},
  URL = {https://doi.org/10.1214/aoms/1177728914}
}

## References

@Book{cano2012sixsigma,
    author = {Emilio L. Cano and Javier M. Moguerza and Andrés
      Redchuk},
    title = {Six Sigma with R. Statistical Engineering for Process
      Improvement},
    publisher = {Springer},
    year = {2012},
    volume = {36},
    series = {Use R!},
    address = {New York},
    doi = {10.1007/978-1-4614-3652-2},
    isbn = {978-1-4614-3651-5},
    pages = {323},
    date = {2012-07-01},
  }

  @Book{cano2015qcr,
    author = {Emilio L. Cano and Javier M. Moguerza and Mariano {Prieto
      Corcoba}},
    title = {Quality Control with R. An ISO Standards Approach},
    publisher = {Springer},
    address = {Switzerland},
    year = {2015},
    series = {Use R!},
    doi = {10.1007/978-3-319-24046-6},
    isbn = {978-3-319-24044-2},
    pages = {349},
    date = {2015-12-01},
  }

  @book{carlin2008bayesian,
  title = {Bayesian Methods for Data Analysis},
  author = {Carlin, B.P. and Louis, T.A.},
  year = 2008,
  publisher = {CRC Press},
  series = {Chapman \& Hall/CRC Texts in Statistical Science},
  isbn = 9781584886983,
  url = {https://books.google.co.il/books?id=GTJUt8fcFx8C},
}
@book{carlin2008bayesian,
  title = {Bayesian Methods for Data Analysis},
  author = {Carlin, B.P. and Louis, T.A.},
  year = 2008,
  publisher = {CRC Press},
  series = {Chapman \& Hall/CRC Texts in Statistical Science},
  isbn = 9781584886983,
  url = {https://books.google.co.il/books?id=GTJUt8fcFx8C},
}
@book{casella2002statistical,
  title={Statistical Inference},
  author={Casella, G. and Berger, R.L.},
  isbn={9780534243128},
  lccn={20010257},
  series={Duxbury advanced series in statistics and decision sciences},
  url={http://home.ustc.edu.cn/~zt001062/MaStmaterials/George%20Casella&Roger%20L.Berger--Statistical%20Inference.pdf},
  year={2002},
  publisher={Thomson Learning},
}
@misc{chaabouni2019antiefficient,
      title={Anti-efficient encoding in emergent communication}, 
      author={Rahma Chaabouni and Eugene Kharitonov and Emmanuel Dupoux and Marco Baroni},
      keywords={Zipf's Law of Abbreviation, emergent communication, signaling games, least-effort},
      year={2019},
      eprint={1905.12561},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      abstract={Despite renewed interest in emergent language simulations with neural networks, little is known about the basic properties of the induced code, and how they compare to human language. One fundamental characteristic of the latter, known as Zipf's Law of Abbreviation (ZLA), is that more frequent words are efficiently associated to shorter strings. We study whether the same pattern emerges when two neural networks, a "speaker" and a "listener", are trained to play a signaling game. Surprisingly, we find that networks develop an \emph{anti-efficient} encoding scheme, in which the most frequent inputs are associated to the longest messages, and messages in general are skewed towards the maximum length threshold. This anti-efficient code appears easier to discriminate for the listener, and, unlike in human communication, the speaker does not impose a contrasting least-effort pressure towards brevity. Indeed, when the cost function includes a penalty for longer messages, the resulting message distribution starts respecting ZLA. Our analysis stresses the importance of studying the basic features of emergent communication in a highly controlled setup, to ensure the latter will not strand too far from human language. Moreover, we present a concrete illustration of how different functional pressures can lead to successful communication codes that lack basic properties of human language, thus highlighting the role such pressures play in the latter.}
}

@misc{chadha2020distilled,
  title = {Distilled Notes for the Natural Language Processing Specialization on Coursera (offered by deeplearning.ai)},
  author = {Chadha, Aman},
  year = 2020,
  url = {www.aman.ai},
  note = {Accessed: 2023-03-21},
  howpublished = {\url{https://www.aman.ai}},
}
@misc{chadha2020distilled,
  title = {Distilled Notes for the Natural Language Processing Specialization on Coursera (offered by deeplearning.ai)},
  author = {Chadha, Aman},
  year = 2020,
  url = {www.aman.ai},
  note = {Accessed: 2023-03-21},
  howpublished = {\url{https://www.aman.ai}},
}
@book{chambers2008software,
  title = {Software for data analysis programming with R},
  author = {Chambers, John M.},
  year = 2008,
  publisher = {Springer},
  address = {New York; London},
  isbn = {0387759360 9780387759364},
  url = {http://www.amazon.de/Software-Data-Analysis-Programming-Statistics/dp/0387759352},
  added-at = {2013-09-01T13:14:37.000+0200},
  biburl = {https://www.bibsonomy.org/bibtex/281e36af33c66c45a7e1db48cc910f0ad/vivion},
  description = {Software for Data Analysis: Programming with R (Statistics and Computing): Amazon.de: John Chambers: Englische Bücher},
  interhash = {e1dde4b9a5a216be4f75516a7519ba1b},
  intrahash = {81e36af33c66c45a7e1db48cc910f0ad},
  keywords = {data programming r statistics},
  refid = 436978847,
  timestamp = {2013-09-01T13:14:37.000+0200},
}
@book{chambers2008software,
  title = {Software for data analysis programming with R},
  author = {Chambers, John M.},
  year = 2008,
  publisher = {Springer},
  address = {New York; London},
  isbn = {0387759360 9780387759364},
  url = {http://www.amazon.de/Software-Data-Analysis-Programming-Statistics/dp/0387759352},
  added-at = {2013-09-01T13:14:37.000+0200},
  biburl = {https://www.bibsonomy.org/bibtex/281e36af33c66c45a7e1db48cc910f0ad/vivion},
  description = {Software for Data Analysis: Programming with R (Statistics and Computing): Amazon.de: John Chambers: Englische Bücher},
  interhash = {e1dde4b9a5a216be4f75516a7519ba1b},
  intrahash = {81e36af33c66c45a7e1db48cc910f0ad},
  keywords = {data programming r statistics},
  refid = 436978847,
  timestamp = {2013-09-01T13:14:37.000+0200},
}
@misc{chen1996empirical,
  title = {An Empirical Study of Smoothing Techniques for Language Modeling},
  author = {Stanley F. Chen and Joshua T. Goodman},
  year = 1996,
  url = {https://arxiv.org/abs/cmp-lg/9606011},
  eprint = {cmp-lg/9606011},
  archiveprefix = {arXiv},
  primaryclass = {cmp-lg},
  abstract = {We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling, including those described by Jelinek and Mercer (1980), Katz (1987), and Church and Gale (1991). We investigate for the first time how factors such as training data size, corpus (e.g., Brown versus Wall Street Journal), and n-gram order (bigram versus trigram) affect the relative performance of these methods, which we measure through the cross-entropy of test data. In addition, we introduce two novel smoothing techniques, one a variation of Jelinek-Mercer smoothing and one a very simple linear interpolation technique, both of which outperform existing methods.},
}
@misc{chen1996empirical,
  title = {An Empirical Study of Smoothing Techniques for Language Modeling},
  author = {Stanley F. Chen and Joshua T. Goodman},
  year = 1996,
  url = {https://arxiv.org/abs/cmp-lg/9606011},
  eprint = {cmp-lg/9606011},
  archiveprefix = {arXiv},
  primaryclass = {cmp-lg},
  abstract = {We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling, including those described by Jelinek and Mercer (1980), Katz (1987), and Church and Gale (1991). We investigate for the first time how factors such as training data size, corpus (e.g., Brown versus Wall Street Journal), and n-gram order (bigram versus trigram) affect the relative performance of these methods, which we measure through the cross-entropy of test data. In addition, we introduce two novel smoothing techniques, one a variation of Jelinek-Mercer smoothing and one a very simple linear interpolation technique, both of which outperform existing methods.},
}
@book{chomsky1969aspects,
  title={Aspects of the Theory of Syntax},
  author={Chomsky, N.},
  isbn={9780262260503},
  series={The MIT Press},
  url={https://www.google.com/books?id=u0ksbFqagU8C},
  year={1969},
  publisher={MIT Press}
}

@book{christakis2009connected,
  title={Connected: The Surprising Power of Our Social Networks and How They Shape Our Lives},
  author={Christakis, N.A. and Fowler, J.H.},
  isbn={9780316071345},
  year={2009},
  publisher={Little, Brown}
}

@book{collins2001good,
  title={Good to Great: Why Some Companies Make the Leap-- and Others Don't},
  author={Collins, J.C.},
  isbn={9780712676090},
  lccn={2001024818},
  series={Random House Business books},
  year={2001},
  publisher={Random House Business}
}

@article{cook1977,
	title = {Detection of Influential Observation in Linear Regression},
	author = {Cook, R. Dennis},
	year = {1977},
	month = {02},
	date = {1977-02},
	journal = {Technometrics},
	pages = {15},
	volume = {19},
	number = {1},
	doi = {10.2307/1268249},
	url = {http://dx.doi.org/10.2307/1268249}
}

@book{dale1999history,
  title={A History of Inverse Probability: From Thomas Bayes to Karl Pearson, 2nd edition},
  author={Dale, A.I. and Buchwald, J.Z. and Toomer, G.J.},
  isbn={9780387988078},
  lccn={99018596},
  series={Sources and Studies in the History of Mathematics and Physical Sciences},
  url={https://books.google.co.il/books?id=dGweIIXbAgMC},
  year={1999},
  publisher={Springer New York}
}
@article{dalkey_experimental_1963,
	title = {An {Experimental} {Application} of the {DELPHI} {Method} to the {Use} of {Experts}},
	volume = {9},
	issn = {0025-1909, 1526-5501},
	url = {http://pubsonline.informs.org/doi/abs/10.1287/mnsc.9.3.458},
	doi = {10.1287/mnsc.9.3.458},
	language = {en},
	number = {3},
	urldate = {2021-09-19},
	journal = {Management Science},
	author = {Dalkey, Norman and Helmer, Olaf},
	month = apr,
	year = {1963},
	pages = {458--467},
}

% delphi method
@book{davidson1996principles,
  title = {Principles of Statistical Data Handling},
  author = {Davidson, Fred},
  year = 1996,
  doi = {10.4135/9781483348902},
  url = {https://methods.sagepub.com/book/principles-of-statistical-data-handling},
  city = {Thousand Oaks, California},
}
@book{davidson1996principles,
  title = {Principles of Statistical Data Handling},
  author = {Davidson, Fred},
  year = 1996,
  doi = {10.4135/9781483348902},
  url = {https://methods.sagepub.com/book/principles-of-statistical-data-handling},
  city = {Thousand Oaks, California},
}
@book{dawkins1976selfish,
  added-at = {2009-04-07T11:01:54.000+0200},
  author = {Dawkins, R},
  biburl = {https://www.bibsonomy.org/bibtex/2d1fac0b1967909865eaf7d12113a954d/selmarsmit},
  booktitle = {The Selfish Gene},
  date-modified = {2008-09-19 14:47:06 +0200},
  description = {Books},
  interhash = {0e376c87057d36d820970aa0965591c2},
  intrahash = {d1fac0b1967909865eaf7d12113a954d},
  keywords = {imported},
  publisher = {Oxford University Press, Oxford, UK},
  timestamp = {2009-04-07T11:01:55.000+0200},
  title = {The Selfish Gene},
  year = 1976
}

@book{de2002predicting,
title={Predicting politics},
author={De Mesquita, Bruce Bueno},
year={2002},
publisher={Ohio State University Press}
}


@book{de2005logic,
  title={The Logic of Political Survival},
  author={De Mesquita, Bruce Bueno. and Smith, A. and Siverson, R.M. and Morrow, J.D.},
  isbn={9780262261777},
  series={The MIT Press},
  year={2005},
  publisher={MIT Press}
}

@book{de2009predictioneer,
  title={The Predictioneer's Game: Using the Logic of Brazen Self-Interest to See and Shape the Future},
  author={De Mesquita, Bruce Bueno},
  isbn={9781588369086},
  lccn={2009005686},
  year={2009},
  publisher={Random House Publishing Group},
  url={https://www.google.com/books?id=EMMMMUqGIboC&gbpv=0}
}

@book{de2011dictator,
  title={The Dictator's Handbook: Why Bad Behavior is Almost Always Good Politics},
  author={De Mesquita, Bruce Bueno and Smith, A.},
  isbn={9781610390453},
  lccn={2011024164},
  year={2011},
  publisher={PublicAffairs}
}

@book{de2011prediction,
  title={Prediction: How to See and Shape the Future with Game Theory},
  author={De Mesquita, Bruce Bueno},
  isbn={9781446444368},
  year={2011},
  publisher={Random House}
}

@book{deMoivre1718doctrine,
  title = {The Doctrine of Chances},
  author = {Abraham De Moivre},
  year = 1718,
  publisher = {H. Woodfall.},
  url = {https://tellingstorieswithdata.com},
}

@book{de_bono2002lateral,
  title = {Lateral thinking : a textbook of creativity},
  author = {De Bono, Edward},
  year = 2002,
  publisher = {Penguin Books},
  keywords = {algorithms, logic},
  language = English,
}
@book{de_bono2002lateral,
  title = {Lateral thinking : a textbook of creativity},
  author = {De Bono, Edward},
  year = 2002,
  publisher = {Penguin Books},
  keywords = {algorithms, logic},
  language = English,
}
@inbook{definetti1992foresight,
	title = {Foresight: Its Logical Laws, Its Subjective Sources},
	author = {de Finetti, Bruno},
	year = {1992},
	date = {1992},
	publisher = {Springer New York},
	pages = {134--174},
	doi = {10.1007/978-1-4612-0919-5_10},
	url = {http://dx.doi.org/10.1007/978-1-4612-0919-5_10}
}

@article{definetti2017theory,
	title = {Theory of Probability},
	author = {de Finetti, Bruno},
	editor = {{Machí}, Antonio and Smith, Adrian},
	year = {2017},
	month = {01},
	date = {2017-01-20},
	journal = {Wiley Series in Probability and Statistics},
	doi = {10.1002/9781119286387},
	url = {http://dx.doi.org/10.1002/9781119286387}
}

@article{diaconis1980,
	title = {Finite Exchangeable Sequences},
	author = {Diaconis, P. and Freedman, D.},
	year = {1980},
	month = {08},
	date = {1980-08-01},
	journal = {The Annals of Probability},
	volume = {8},
	number = {4},
	doi = {10.1214/aop/1176994663},
	url = {http://dx.doi.org/10.1214/aop/1176994663}
}

@book{diamond2011collapse,
  title={Collapse: How Societies Choose to Fail or Succeed: Revised Edition},
  author={Diamond, J.},
  isbn={9781101502006},
  year={2011},
  publisher={Penguin Publishing Group}
}

@book{dirac,
  title     = {The Principles of Quantum Mechanics},
  author    = {Paul Adrien Maurice Dirac},
  isbn      = {9780198520115},
  series    = {International series of monographs on physics},
  year      = {1981},
  publisher = {Clarendon Press},
  keywords  = {physics}
}

@book{dirac,
  title     = {The Principles of Quantum Mechanics},
  author    = {Paul Adrien Maurice Dirac},
  isbn      = {9780198520115},
  series    = {International series of monographs on physics},
  year      = {1981},
  publisher = {Clarendon Press},
  keywords  = {physics}
}

@article{doi:10.1098/rsta.1922.0009,
author = {Fisher, R. A.  and Russell, Edward John },
title = {On the mathematical foundations of theoretical statistics},
journal = {Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character},
volume = {222},
number = {594-604},
pages = {309-368},
year = {1922},
doi = {10.1098/rsta.1922.0009},

URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rsta.1922.0009},
eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rsta.1922.0009}
,
    abstract = { Several reasons have contributed to the prolonged neglect into which the study of statistics, in its theoretical aspects, has fallen. In spite of the immense amount of fruitful labour which has been expended in its practical applications, the basic principles of this organ of science are still in a state of obscurity, and it cannot be denied that, during the recent rapid development of practical methods, fundamental problems have been ignored and fundamental paradoxes left unresolved. This anomalous state of statistical science is strikingly exemplified by a recent paper entitled "The Fundamental Problem of Practical Statistics," in which one of the most eminent of modern statisticians presents what purports to be a general proof of BAYES' postulate, a proof which, in the opinion of a second statistician of equal eminence, "seems to rest upon a very peculiar -- not to say hardly supposable -- relation." }
}


@article{donohue2001impact,
  title={The impact of legalized abortion on crime},
  author={Donohue III, John J and Levitt, Steven D},
  journal={The Quarterly Journal of Economics},
  volume={116},
  number={2},
  pages={379--420},
  year={2001},
  publisher={MIT Press},
  abstract={We offer evidence that legalized abortion has contributed significantly to recent crime reductions. Crime began to fall roughly eighteen years after abortion legalization. The five states that allowed abortion in 1970 experienced declines earlier than the rest of the nation, which legalized in 1973 with Roe v. Wade. States with high abortion rates in the 1970s and 1980s experienced greater crime reductions in the 1990s. In high abortion states, only arrests of those born after abortion legalization fall relative to low abortion states. Legalized abortion appears to account for as much as 50 percent of the recent drop in crime.},
  url={https://pricetheory.uchicago.edu/levitt/Papers/DonohueLevittTheImpactOfLegalized2001.pdf}
}

@article{dowhy,
  title={DoWhy: An End-to-End Library for Causal Inference},
  author={Sharma, Amit and Kiciman, Emre},
  journal={arXiv preprint arXiv:2011.04216},
  year={2020}
}

@article{dowhy_gcm,
    author = {Bl{\"o}baum, Patrick and G{\"o}tz, Peter and Budhathoki, Kailash and Mastakouri, Atalanti A. and Janzing, Dominik},
    title = {DoWhy-GCM: An extension of DoWhy for causal inference in graphical causal models},
    journal={arXiv preprint arXiv:2206.06821},
    year={2022}
}

@book{downey2011think,
  title={Think Stats},
  author={Downey, A.},
  isbn={9781449307110},
  lccn={2011276382},
  series={O'Reilly Series},
  url={https://books.google.co.il/books?id=TCfZ7d6skT4C},
  year={2011},
  publisher={O'Reilly Media}
}

@book{downey2021think,
  title={Think Bayes},
  author={Downey, A.B.},
  isbn={9781492089438},
  url={https://books.google.co.il/books?id=Vh4vEAAAQBAJ},
  year={2021},
  publisher={O'Reilly Media}
}

@book{dromey1982how,
  title = {How to Solve It by Computer},
  author = {Dromey, R. G.},
  year = 1982,
  publisher = {Prentice-Hall, Inc.},
  address = {USA},
  isbn = {0134340019},
  keywords = {algorithms, math, logic},
}
@book{dromey1982how,
  title = {How to Solve It by Computer},
  author = {Dromey, R. G.},
  year = 1982,
  publisher = {Prentice-Hall, Inc.},
  address = {USA},
  isbn = {0134340019},
  keywords = {algorithms, math, logic},
}
@article{duchi2011adaptive,
  title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  author = {Duchi, John and Hazan, Elad},
  year = 2011,
  month = {07},
  journal = {Journal of Machine Learning Research},
  volume = 12,
  pages = {2121--2159},
  url = {http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf},
}
@article{duchi2011adaptive,
  title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  author = {Duchi, John and Hazan, Elad},
  year = 2011,
  month = {07},
  journal = {Journal of Machine Learning Research},
  volume = 12,
  pages = {2121--2159},
  url = {http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf},
}
@book{duke2018thinking,
  title={Thinking in Bets: Making Smarter Decisions When You Don't Have All the Facts},
  author={Duke, A.},
  isbn={9780735216365},
  lccn={2017042666},
  url={https://www.google.com/books?id=VcouDwAAQBAJ},
  year={2018},
  publisher={Penguin Publishing Group}
}

@book{duke2020decide,
  title={How to Decide: Simple Tools for Making Better Choices},
  author={Duke, A.},
  isbn={9780593084618},
  lccn={2020015472},
  url={https://www.google.com/books?id=scifDwAAQBAJ},
  year={2020},
  publisher={Penguin Publishing Group}
}

@article{duncan1955methodological,
  title={A methodological analysis of segregation indexes},
  author={Duncan, Otis Dudley and Duncan, Beverly},
  journal={American sociological review},
  volume={20},
  number={2},
  pages={210--217},
  year={1955},
  publisher={JSTOR}
}

@article{edmundson-1969,
  added-at  = {2008-10-29T01:27:19.000+0100},
  author    = {Edmundson, H.P.},
  url       = {https://courses.ischool.berkeley.edu/i256/f06/papers/edmonson69.pdf},
  journal   = {Journal of theACM},
  keywords  = {automatic_extracting},
  number    = {2},
  pages     = {264-285},
  timestamp = {2008-11-05T16:42:20.000+0100},
  title     = {New methods in automatic extracting},
  volume    = {16},
  year      = {1969}
}

@article{edmundson-1969,
  added-at  = {2008-10-29T01:27:19.000+0100},
  author    = {Edmundson, H.P.},
  url       = {https://courses.ischool.berkeley.edu/i256/f06/papers/edmonson69.pdf},
  journal   = {Journal of theACM},
  keywords  = {automatic_extracting},
  number    = {2},
  pages     = {264-285},
  timestamp = {2008-11-05T16:42:20.000+0100},
  title     = {New methods in automatic extracting},
  volume    = {16},
  year      = {1969}
}

@book{efron2016computer,
  title={Computer Age Statistical Inference},
  author={Efron, B. and Hastie, T.},
  isbn={9781107149892},
  lccn={2016028353},
  series={Institute of Mathematical Statistics Monographs},
  url={https://books.google.co.il/books?id=Sj1yDAAAQBAJ},
  year={2016},
  publisher={Cambridge University Press}
}


@article{einstein,
  author   = {Albert Einstein},
  title    = {{Zur Elektrodynamik bewegter K{\"o}rper}. ({German})
    [{On} the electrodynamics of moving bodies]},
  journal  = {Annalen der Physik},
  volume   = {322},
  number   = {10},
  pages    = {891--921},
  year     = {1905},
  doi      = {http://dx.doi.org/10.1002/andp.19053221004},
  keywords = {physics}
}

@article{einstein,
  author   = {Albert Einstein},
  title    = {{Zur Elektrodynamik bewegter K{\"o}rper}. ({German})
    [{On} the electrodynamics of moving bodies]},
  journal  = {Annalen der Physik},
  volume   = {322},
  number   = {10},
  pages    = {891--921},
  year     = {1905},
  doi      = {http://dx.doi.org/10.1002/andp.19053221004},
  keywords = {physics}
}

@article{elshamy2023improving,
  title={Improving the efficiency of RMSProp optimizer by utilizing Nestrove in deep learning},
  author={Elshamy, Reham and Abu-Elnasr, Osama and Elhoseny, Mohamed and Elmougy, Samir},
  journal={Scientific Reports},
  volume={13},
  number={1},
  pages={8814},
  year={2023},
  doi={10.1038/s41598-023-35663-x},
  publisher={Nature Publishing Group UK London}
}


@misc{enwiki-empirical,
    author = "{Wikipedia contributors}",
    title = "68–95–99.7 rule --- {Wikipedia}",
    year = "2023",
    howpublished = "\url{https://en.wikipedia.org/w/index.php?title=68%E2%80%9395%E2%80%9399.7_rule}",
    note = "[Online; accessed 18-May-2023]"
  }

@misc{enwiki-functional,
    author = "{Wikipedia contributors}",
    title = "Functional (mathematics) --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2023",
    url = "https://en.wikipedia.org/w/index.php?title=Functional_(mathematics)&oldid=1148699341",
    note = "[Online; accessed 18-April-2023]"
}

@misc{enwiki:1195848318,
    author = "{Wikipedia contributors}",
    title = "Perceptron --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2024",
    url = "https://en.wikipedia.org/w/index.php?title=Perceptron&oldid=1195848318",
    note = "[Online; accessed 4-February-2024]"
  }

@misc{enwiki:poly_urn,
    author = "{Wikipedia contributors}",
    title = "Pólya urn model --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2023",
    howpublished = "\url{https://en.wikipedia.org/w/index.php?title=P%C3%B3lya_urn_model&oldid=1152130337}",
    note = "[Online; accessed 16-May-2023]"
  }

@book{epstein1996growing,
  title={Growing Artificial Societies: Social Science from the Bottom Up},
  author={Epstein, J.M. and Axtell, R. and Project, 2050},
  isbn={9780262050531},
  lccn={lc96025332},
  series={A Bradford book},
  url={https://books.google.co.il/books?id=xXvelSs2caQC},
  year={1996},
  publisher={MIT Press}
  
}@book{epstein2019range,
  title={Range: How Generalists Triumph in a Specialized World},
  author={Epstein, D.},
  isbn={9781509843510},
  url={https://www.google.com/books?id=oEGCDwAAQBAJ},
  year={2019},
  publisher={Pan Macmillan}
}

@online{fisher2017modelthinking,
  author = {Steve Fisher},
  title = {Model Thinking -- TA Notes},
  publisher= {Coursera},
  howpublished = {\url{https://www.coursera.org/learn/model-thinking}},
  year = {2014},
  month={11},
  note = {Accessed: 204-02-24},
}

@article{forster2014delphi,
	title = {Delphi-based strategic issue management: crafting consumer goods supply chain strategy},
	volume = {44},
	issn = {0960-0035},
	shorttitle = {Delphi-based strategic issue management},
	url = {https://www.emerald.com/insight/content/doi/10.1108/IJPDLM-09-2012-0289/full/html},
	doi = {10.1108/IJPDLM-09-2012-0289},
	language = {en},
	number = {5},
	urldate = {2021-09-19},
	journal = {International Journal of Physical Distribution \& Logistics Management},
	author = {Förster, Bernadette and Keller, Jonas and A. von der Gracht, Heiko and Darkow, Inga-Lena},
	month = may,
	year = {2014},
	pages = {373--391},
}

@inproceedings{gal2016dropout,
  title={Dropout as a bayesian approximation: Representing model uncertainty in deep learning},
  author={Gal, Yarin and Ghahramani, Zoubin},
  booktitle={international conference on machine learning},
  pages={1050--1059},
  year={2016},
  organization={PMLR}
}

@book{galef2021scout,
  title = {The Scout Mindset: Why Some People See Things Clearly and Others Don't},
  author = {Galef, J.},
  year = 2021,
  publisher = {Penguin Publishing Group},
  isbn = 9780735217553,
  url = {https://books.google.co.il/books?id=wJ0jEAAAQBAJ},
  lccn = 2020024770,
}
@book{galef2021scout,
  title = {The Scout Mindset: Why Some People See Things Clearly and Others Don't},
  author = {Galef, J.},
  year = 2021,
  publisher = {Penguin Publishing Group},
  isbn = 9780735217553,
  url = {https://books.google.co.il/books?id=wJ0jEAAAQBAJ},
  lccn = 2020024770,
}
@book{gawande2010checklist,
  title={The Checklist Manifesto: How to Get Things Right},
  author={Gawande, A.},
  isbn={9781429953382},
  year={2010},
  publisher={Henry Holt and Company}
}

@article{gelman2008,
	title = {A weakly informative default prior distribution for logistic and other regression models},
	author = {Gelman, Andrew and Jakulin, Aleks and Pittau, Maria Grazia and Su, Yu-Sung},
	year = {2008},
	month = {12},
	date = {2008-12-01},
	journal = {The Annals of Applied Statistics},
	volume = {2},
	number = {4},
	doi = {10.1214/08-aoas191},
	url = {http://dx.doi.org/10.1214/08-AOAS191}
}

@book{gelman2013bayesian,
  title = {Bayesian Data Analysis},
  author = {Gelman, Andrew and Carlin, John and Stern, Hal and Dunson, David and Vehtari, Aki and Rubin, Donald},
  year = 2013,
  month = 11,
  pages = {},
  doi = {10.1201/b16018},
  isbn = 9780429113079,
}
@book{gelman2013bayesian,
  title = {Bayesian Data Analysis},
  author = {Gelman, Andrew and Carlin, John and Stern, Hal and Dunson, David and Vehtari, Aki and Rubin, Donald},
  year = 2013,
  month = 11,
  pages = {},
  doi = {10.1201/b16018},
  isbn = 9780429113079,
}
@article{geman1984,
	title = {Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images},
	author = {Geman, Stuart and Geman, Donald},
	year = {1984},
	month = {11},
	date = {1984-11},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	pages = {721--741},
	volume = {PAMI-6},
	number = {6},
	doi = {10.1109/tpami.1984.4767596},
	url = {http://dx.doi.org/10.1109/TPAMI.1984.4767596}
}

@book{geron2019hands,
  title={Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems},
  author={G{\'e}ron, A.},
  isbn={9781492032595},
  url={https://books.google.co.il/books?id=HnetDwAAQBAJ},
  year={2019},
  publisher={O'Reilly Media}
}

@article{ghosh2018,
	title = {On the Use of Cauchy Prior Distributions for Bayesian Logistic Regression},
	author = {Ghosh, Joyee and Li, Yingbo and Mitra, Robin},
	year = {2018},
	month = {06},
	date = {2018-06-01},
	journal = {Bayesian Analysis},
	volume = {13},
	number = {2},
	doi = {10.1214/17-ba1051},
	url = {http://dx.doi.org/10.1214/17-BA1051}
}

@Misc{gimond2021tukeyedar,
  title = {tukeyedar: A package of Tukey inspired EDA functions},
  author = {Manuel Gimond},
  url = {https://mgimond.github.io/tukeyedar/},
  year = {2021},
}

@book{gladwell2006tipping,
  title={The Tipping Point: How Little Things Can Make a Big Difference},
  author={Gladwell, M.},
  isbn={9780759574731},
  lccn={99047576},
  year={2006},
  publisher={Little, Brown}
}


@article{gnatzy2011validating,
	title = {Validating an innovative real-time {Delphi} approach - {A} methodological comparison between real-time and conventional {Delphi} studies},
	volume = {78},
	issn = {00401625},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0040162511000813},
	doi = {10.1016/j.techfore.2011.04.006},
	language = {en},
	number = {9},
	urldate = {2021-09-19},
	journal = {Technological Forecasting and Social Change},
	author = {Gnatzy, Tobias and Warth, Johannes and von der Gracht, Heiko and Darkow, Inga-Lena},
	month = nov,
	year = {2011},
	pages = {1681--1694},
}

@article{good1953population,
  title = "The population frequencies of species and the estimation of population parameters",
  author = {Good, I. J.},
  year = 1953,
  month = 12,
  journal = {Biometrika},
  volume = 40,
  number = {3-4},
  pages = {237--264},
  doi = {10.1093/biomet/40.3-4.237},
  issn = {0006-3444},
  url = {https://doi.org/10.1093/biomet/40.3-4.237},
  abstract = "A random sample is drawn from a population of animals of various species. (The theory may also be applied to studies of literary vocabulary, for example.) If a particular species is represented r times in the sample of size N, then r/N is not a good estimate of the population frequency, p, when r is small. Methods are given for estimating p, assuming virtually nothing about the underlying population. The estimates are expressed in terms of smoothed values of the numbers nr (r= 1, 2, 3, ...), where nr is the number of distinct species that are each represented r times in the sample. (nr may be described as ‘the frequency of the frequency r’.) Turing is acknowledged for the most interesting formula in this part of the work. An estimate of the proportion of the population represented by the species occurring in the sample is an immediate corollary. Estimates are made of measures of heterogeneity of the population, including Yule's ‘characteristic’ and Shannon's ‘entropy’. Methods are then discussed that do depend on assumptions about the underlying population. It is here that most work has been done by other writers. It is pointed out that a hypothesis can give a good fit to the numbers nr but can give quite the wrong value for Yule's characteristic. An example of this is Fisher's fit to some data of Williams's on Macrolepidoptera.",
  eprint = {https://academic.oup.com/biomet/article-pdf/40/3-4/237/492571/40-3-4-237.pdf},
}
@article{good1953population,
  title = "The population frequencies of species and the estimation of population parameters",
  author = {Good, I. J.},
  year = 1953,
  month = 12,
  journal = {Biometrika},
  volume = 40,
  number = {3-4},
  pages = {237--264},
  doi = {10.1093/biomet/40.3-4.237},
  issn = {0006-3444},
  url = {https://doi.org/10.1093/biomet/40.3-4.237},
  abstract = "A random sample is drawn from a population of animals of various species. (The theory may also be applied to studies of literary vocabulary, for example.) If a particular species is represented r times in the sample of size N, then r/N is not a good estimate of the population frequency, p, when r is small. Methods are given for estimating p, assuming virtually nothing about the underlying population. The estimates are expressed in terms of smoothed values of the numbers nr (r= 1, 2, 3, ...), where nr is the number of distinct species that are each represented r times in the sample. (nr may be described as ‘the frequency of the frequency r’.) Turing is acknowledged for the most interesting formula in this part of the work. An estimate of the proportion of the population represented by the species occurring in the sample is an immediate corollary. Estimates are made of measures of heterogeneity of the population, including Yule's ‘characteristic’ and Shannon's ‘entropy’. Methods are then discussed that do depend on assumptions about the underlying population. It is here that most work has been done by other writers. It is pointed out that a hypothesis can give a good fit to the numbers nr but can give quite the wrong value for Yule's characteristic. An example of this is Fisher's fit to some data of Williams's on Macrolepidoptera.",
  eprint = {https://academic.oup.com/biomet/article-pdf/40/3-4/237/492571/40-3-4-237.pdf},
}
@article{granovetter1983threshold,
  title={Threshold models of diffusion and collective behavior.},
  author={Granovetter, Mark and Soong, Roland},
  journal={Journal of Mathematical sociology},
  year={1983},
  publisher={Gordon \& Breach Science Publishers, Inc.}
}


@article{granovetter1986threshold,
  title={Threshold models of interpersonal effects in consumer demand},
  author={Granovetter, Mark and Soong, Roland},
  journal={Journal of Economic Behavior \& Organization},
  volume={7},
  number={1},
  pages={83--99},
  year={1986},
  publisher={Elsevier}
}

@article{granovetter1988threshold,
  title={Threshold models of diversity: Chinese restaurants, residential segregation, and the spiral of silence},
  author={Granovetter, Mark and Soong, Roland},
  journal={Sociological methodology},
  pages={69--104},
  year={1988},
  publisher={JSTOR}
}

@book{grant2021think,
  title={Think Again: The Power of Knowing What You Don't Know},
  author={Grant, A.},
  isbn={9780753553909},
  url={https://www.google.com/books?id=FdjgDwAAQBAJ},
  year={2021},
  publisher={Ebury Publishing}
}
@online{groh2017model,
  author = {Rainer Groh},
  title = {Model Thinking - Course Notes by Rainer Groh},
  howpublished = {\url{https://aerospaceengineeringblog.com/wp-content/uploads/2017/11/ModelThinking.pdf}},
  year = {2017},
  month={11},
  note = {Accessed: 204-02-24},
}

@book{harford2021data,
  title={The Data Detective: Ten Easy Rules to Make Sense of Statistics},
  author={Harford, T.},
  isbn={9780593084670},
  url={https://www.google.com/books?id=_JvmDwAAQBAJ},
  year={2021},
  publisher={Penguin Publishing Group}
}

@book{herrick2015history,
  title={The History and Theory of Rhetoric: An Introduction (Subscription)},
  author={Herrick, J.A.},
  isbn={9781317347842},
  url={https://www.google.com/books?id=29VRCgAAQBAJ},
  year={2015},
  keywords = {rhetoric, public speaking, history},
  publisher={Taylor \& Francis}
}

@book{herrick2015history,
  title={The History and Theory of Rhetoric: An Introduction (Subscription)},
  author={Herrick, J.A.},
  isbn={9781317347842},
  url={https://www.google.com/books?id=29VRCgAAQBAJ},
  year={2015},
  keywords = {rhetoric, public speaking, history},
  publisher={Taylor \& Francis}
}

@inproceedings{hinton1991adaptive,
 author = {Nowlan, Steven and Hinton, Geoffrey E},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {R.P. Lippmann and J. Moody and D. Touretzky},
 pages = {},
 publisher = {Morgan-Kaufmann},
 title = {Evaluation of Adaptive Mixtures of Competing Experts},
 url = {https://proceedings.neurips.cc/paper_files/paper/1990/file/432aca3a1e345e339f35a30c8f65edce-Paper.pdf},
 volume = {3},
 year = {1990}
}


@inproceedings{hinton2001new,
  title={A new view of ICA},
  author={Hinton, Geoffrey E and Welling, Max and Teh, Yee Whye and Osindero, Simon},
  booktitle={Proceedings of 3rd International Conference on Independent Component Analysis and Blind Signal Separation (ICA’01)},
  pages={746--751},
  year={2001}
}

@article{hinton2006fast,
  title={A fast learning algorithm for deep belief nets},
  author={Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye},
  journal={Neural computation},
  volume={18},
  number={7},
  pages={1527--1554},
  year={2006},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{hinton2006unsupervised,
  title={Unsupervised discovery of nonlinear structure using contrastive backpropagation},
  author={Hinton, Geoffrey and Osindero, Simon and Welling, Max and Teh, Yee-Whye},
  journal={Cognitive science},
  volume={30},
  number={4},
  pages={725--731},
  year={2006},
  publisher={Wiley Online Library}
}


@article{hinton2012,
	title = {Improving neural networks by preventing co-adaptation of feature detectors},
	author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
	year = {2012},
	date = {2012},
	doi = {10.48550/ARXIV.1207.0580},
	url = {https://arxiv.org/abs/1207.0580}
}

@article{hinton2012deep,
  title={Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups},
  author={Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George E and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara N and others},
  journal={IEEE Signal processing magazine},
  volume={29},
  number={6},
  pages={82--97},
  year={2012},
  publisher={IEEE}
}


@misc{hinton2012improving,
  title = {Improving neural networks by preventing co-adaptation of feature detectors},
  author = {Geoffrey E. Hinton and Nitish Srivastava and Alex Krizhevsky and Ilya Sutskever and Ruslan R. Salakhutdinov},
  year = 2012,
  doi = {https://doi.org/10.48550/arXiv.1207.0580},
  url = {https://arxiv.org/pdf/1207.0580},
  eprint = {1207.0580},
  archiveprefix = {arXiv},
  primaryclass = {cs.NE},
}
@misc{hinton2012improving,
  title = {Improving neural networks by preventing co-adaptation of feature detectors},
  author = {Geoffrey E. Hinton and Nitish Srivastava and Alex Krizhevsky and Ilya Sutskever and Ruslan R. Salakhutdinov},
  year = 2012,
  doi = {https://doi.org/10.48550/arXiv.1207.0580},
  url = {https://arxiv.org/pdf/1207.0580},
  eprint = {1207.0580},
  archiveprefix = {arXiv},
  primaryclass = {cs.NE},
}
@inproceedings{ho1995random,
  author={Tin Kam Ho},
  booktitle={Proceedings of 3rd International Conference on Document Analysis and Recognition}, 
  title={Random decision forests}, 
  year={1995},
  volume={1},
  number={},
  pages={278-282 vol.1},
  keywords={Classification tree analysis;Decision trees;Training data;Optimization methods;Testing;Tin;Stochastic processes;Handwriting recognition;Hidden Markov models;Multilayer perceptrons},
  doi={10.1109/ICDAR.1995.598994}
}

@book{hobbs2015bayesian,
 URL = {http://www.jstor.org/stable/j.ctt1dr36kz},
 abstract = {Bayesian modeling has become an indispensable tool for ecological research because it is uniquely suited to deal with complexity in a statistically coherent way. This textbook provides a comprehensive and accessible introduction to the latest Bayesian methods-in language ecologists can understand. Unlike other books on the subject, this one emphasizes the principles behind the computations, giving ecologists a big-picture understanding of how to implement this powerful statistical approach.Bayesian Modelsis an essential primer for non-statisticians. It begins with a definition of probability and develops a step-by-step sequence of connected ideas, including basic distribution theory, network diagrams, hierarchical models, Markov chain Monte Carlo, and inference from single and multiple models. This unique book places less emphasis on computer coding, favoring instead a concise presentation of the mathematical statistics needed to understand how and why Bayesian analysis works. It also explains how to write out properly formulated hierarchical Bayesian models and use them in computing, research papers, and proposals.This primer enables ecologists to understand the statistical principles behind Bayesian modeling and apply them to research, teaching, policy, and management.Presents the mathematical and statistical foundations of Bayesian modeling in language accessible to non-statisticiansCovers basic distribution theory, network diagrams, hierarchical models, Markov chain Monte Carlo, and moreDeemphasizes computer coding in favor of basic principlesExplains how to write out properly factored statistical expressions representing Bayesian models},
 author = {N. Thompson Hobbs and Mevin B. Hooten},
 edition = {STU - Student edition},
 publisher = {Princeton University Press},
 title = {Bayesian Models: A Statistical Primer for Ecologists},
 year = {2015}
}

@book{hofbauer1998evolutionary,
  title={Evolutionary Games and Population Dynamics},
  author={Hofbauer, J. and Sigmund, K.},
  isbn={9780521625708},
  lccn={97029058},
  url={https://books.google.co.il/books?id=Xu-H0ClCHN8C},
  year={1998},
  publisher={Cambridge University Press}
}

@book{hoff2009,
	title = {A First Course in Bayesian Statistical Methods},
	author = {Hoff, Peter D.},
	year = {2009},
	date = {2009},
	publisher = {Springer New York},
	doi = {10.1007/978-0-387-92407-6},
	url = {http://dx.doi.org/10.1007/978-0-387-92407-6}
}

@misc{hooker2021unrestricted,
      title={Unrestricted Permutation forces Extrapolation: Variable Importance Requires at least One More Model, or There Is No Free Variable Importance}, 
      author={Giles Hooker and Lucas Mentch and Siyu Zhou},
      year={2021},
      eprint={1905.03151},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}


@article{hopfield-neural-networks-and-1982,
  title={Neural networks and physical systems with emergent collective computational abilities.},
  author={Hopfield, John J},
  journal={Proceedings of the national academy of sciences},
  volume={79},
  number={8},
  pages={2554--2558},
  year={1982},
  publisher={National Acad Sciences}
}

@article{hopfield1982,
	title = {Neural networks and physical systems with emergent collective computational abilities.},
	author = {Hopfield, J J},
	year = {1982},
	month = {04},
	date = {1982-04},
	journal = {Proceedings of the National Academy of Sciences},
	pages = {2554--2558},
	volume = {79},
	number = {8},
	doi = {10.1073/pnas.79.8.2554},
	url = {http://dx.doi.org/10.1073/pnas.79.8.2554},
	langid = {en}
}

@article{hopfield1982neural,
  title = {Neural networks and physical systems with emergent collective computational abilities},
  author = {Hopfield, J. J.},
  year = 1982,
  month = apr,
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = 79,
  number = 8,
  pages = {2554--2558},
  issn = {0027-8424},
  url = {http://view.ncbi.nlm.nih.gov/pubmed/6953413]},
  abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
  added-at = {2011-06-02T00:22:00.000+0200},
  biburl = {https://www.bibsonomy.org/bibtex/2a3074d3b833b6b02b6fc991407e86804/mhwombat},
  citeulike-article-id = 8137707,
  citeulike-linkout-0 = {http://view.ncbi.nlm.nih.gov/pubmed/6953413]},
  citeulike-linkout-1 = {http://www.hubmed.org/display.cgi?uids=6953413]},
  file = {:neural_nets/Hopfield_1982__pnas00447-0135.pdf:PDF},
  groups = {public},
  interhash = {cf60d9bad127184617e0ad10ae86b78b},
  intrahash = {a3074d3b833b6b02b6fc991407e86804},
  keywords = {network neural, seminal},
  pmid = {6953413]},
  posted-at = {2010-10-28 14:55:59},
  priority = 2,
  timestamp = {2016-07-12T19:25:30.000+0200},
  username = {mhwombat},
}
@article{hopfield1982neural,
  title = {Neural networks and physical systems with emergent collective computational abilities},
  author = {Hopfield, J. J.},
  year = 1982,
  month = apr,
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = 79,
  number = 8,
  pages = {2554--2558},
  issn = {0027-8424},
  url = {http://view.ncbi.nlm.nih.gov/pubmed/6953413]},
  abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
  added-at = {2011-06-02T00:22:00.000+0200},
  biburl = {https://www.bibsonomy.org/bibtex/2a3074d3b833b6b02b6fc991407e86804/mhwombat},
  citeulike-article-id = 8137707,
  citeulike-linkout-0 = {http://view.ncbi.nlm.nih.gov/pubmed/6953413]},
  citeulike-linkout-1 = {http://www.hubmed.org/display.cgi?uids=6953413]},
  file = {:neural_nets/Hopfield_1982__pnas00447-0135.pdf:PDF},
  groups = {public},
  interhash = {cf60d9bad127184617e0ad10ae86b78b},
  intrahash = {a3074d3b833b6b02b6fc991407e86804},
  keywords = {network neural, seminal},
  pmid = {6953413]},
  posted-at = {2010-10-28 14:55:59},
  priority = 2,
  timestamp = {2016-07-12T19:25:30.000+0200},
  username = {mhwombat},
}
@book{huyen2022designing,
  title={Designing Machine Learning Systems},
  author={Chip Huyen},
  isbn={9781098107932},
  url={https://books.google.co.il/books?id=EzhwEAAAQBAJ},
  year={2022},
  publisher={O'Reilly Media}
}

@book{härdle2019,
	title = {Applied Multivariate Statistical Analysis},
	author = {{Härdle}, Wolfgang Karl and Simar, {Léopold}},
	year = {2019},
	date = {2019},
	publisher = {Springer International Publishing},
	doi = {10.1007/978-3-030-26006-4},
	url = {http://dx.doi.org/10.1007/978-3-030-26006-4}
}

@inbook{inbook,
author = {Brearcliffe, Dale and Crooks, Andrew},
year = {2021},
month = {01},
pages = {31-58},
title = {Creating Intelligent Agents: Combining Agent-Based Modeling with Machine Learning},
isbn = {978-3-030-83417-3},
doi = {10.1007/978-3-030-83418-0_3}
}

@article{jackman2009,
	title = {Bayesian Analysis for the Social Sciences},
	author = {Jackman, Simon},
	year = {2009},
	month = {10},
	date = {2009-10-23},
	journal = {Wiley Series in Probability and Statistics},
	doi = {10.1002/9780470686621},
	url = {http://dx.doi.org/10.1002/9780470686621}
}

@article{jacobs1991adaptive,
  title = {Adaptive Mixtures of Local Experts},
  author = {Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J. and Hinton, Geoffrey E.},
  year = 1991,
  journal = {Neural Computation},
  volume = 3,
  number = 1,
  pages = {79--87},
  doi = {10.1162/neco.1991.3.1.79},
  url = {http://www.cs.toronto.edu/~fritz/absps/jjnh91.pdf},
}
@article{jacobs1991adaptive,
  title = {Adaptive Mixtures of Local Experts},
  author = {Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J. and Hinton, Geoffrey E.},
  year = 1991,
  journal = {Neural Computation},
  volume = 3,
  number = 1,
  pages = {79--87},
  doi = {10.1162/neco.1991.3.1.79},
  url = {http://www.cs.toronto.edu/~fritz/absps/jjnh91.pdf},
}
@article{jacobs1991adaptive,
  title = {Adaptive Mixtures of Local Experts},
  author = {Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J. and Hinton, Geoffrey E.},
  year = 1991,
  journal = {Neural Computation},
  volume = 3,
  number = 1,
  pages = {79--87},
  doi = {10.1162/neco.1991.3.1.79},
  url = {http://www.cs.toronto.edu/~fritz/absps/jjnh91.pdf},
}
@article{jacobs1991task,
  title={Task decomposition through competition in a modular connectionist architecture: The what and where vision tasks},
  author={Jacobs, Robert A and Jordan, Michael I and Barto, Andrew G},
  journal={Cognitive science},
  volume={15},
  number={2},
  pages={219--250},
  year={1991},
  publisher={Elsevier},
  url={https://www.sciencedirect.com/science/article/abs/pii/036402139180006Q}
}


@book{jank2011business,
  title={Business Analytics for Managers},
  author={Jank, W.},
  isbn={9781461404064},
  series={Use R!},
  url={https://books.google.co.il/books?id=4-uU74-Jss4C},
  year={2011},
  publisher={Springer New York}
}

@book{jeffreys1983theory,
  title={Theory of Probability},
  author={Jeffreys, H.},
  isbn={9780198531937},
  lccn={99175168},
  series={International series of monographs on physics},
  year={1983},
  publisher={Clarendon Press}
}

@misc{jelliti2020nlp,
  title = {NLP Specialization Courses Notes (offered by deeplearning.ai)},
  author = {Jelliti, Ibrahim},
  year = 2020,
  journal = {GitHub repository},
  publisher = {GitHub},
  url = {https://github.com/ibrahimjelliti/Deeplearning.ai-Natural-Language-Processing-Specialization},
  note = {Accessed: 2023-03-21},
  howpublished = {\url{https://github.com/ibrahimjelliti/Deeplearning.ai-Natural-Language-Processing-Specialization}},
  commit = {403196f6dc8808020b8d826890fb0ee554e34a4f},
}
@misc{jelliti2020nlp,
  title = {NLP Specialization Courses Notes (offered by deeplearning.ai)},
  author = {Jelliti, Ibrahim},
  year = 2020,
  journal = {GitHub repository},
  publisher = {GitHub},
  url = {https://github.com/ibrahimjelliti/Deeplearning.ai-Natural-Language-Processing-Specialization},
  note = {Accessed: 2023-03-21},
  howpublished = {\url{https://github.com/ibrahimjelliti/Deeplearning.ai-Natural-Language-Processing-Specialization}},
  commit = {403196f6dc8808020b8d826890fb0ee554e34a4f},
}
@book{johnson1977urn,
  title={Urn Models and Their Application: An Approach to Modern Discrete Probability Theory},
  author={Johnson, N.L. and Kotz, S.},
  isbn={9780471446309},
  lccn={76058846},
  series={Approach to Modern Discrete Probability Theory},
  url={https://books.google.co.il/books?id=ZBfvAAAAMAAJ},
  year={1977},
  publisher={Wiley}
}

@book{johnson2019applied,
  title={Applied Multivariate Statistical Analysis},
  author={Johnson, R.A. and Wichern, D.W.},
  isbn={978-0-13-187715-3},
  series={Pearson Modern Classics for Advanced Statistics Series},
  url={https://books.google.co.il/books?id=QBqlswEACAAJ},
  year={2001},
  publisher={Prentice Hall}
}
@book{jurafsky2000speech,
  title = {Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
  author = {Jurafsky, D. and Martin, J.H.},
  year = 2000,
  publisher = {Prentice Hall},
  series = {Prentice Hall series in artificial intelligence},
  isbn = 9780131227989,
  url = {https://books.google.co.il/books?id=85BvQgAACAAJ},
  lccn = 99087845,
}

@book{jurafsky2000speech,
  title = {Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
  author = {Jurafsky, D. and Martin, J.H.},
  year = 2000,
  publisher = {Prentice Hall},
  series = {Prentice Hall series in artificial intelligence},
  isbn = 9780131227989,
  url = {https://books.google.co.il/books?id=85BvQgAACAAJ},
  lccn = 99087845,
}

@book{kahneman1982judgment,
  title={Judgment under uncertainty: Heuristics and biases},
  author={Daniel Kahneman and Paul Slovic and Amos Tversky },
  year={1982},
  publisher={Cambridge university press}
}

@book{kahneman2011thinking,
  title={Thinking, Fast and Slow},
  author={Daniel Kahneman},
  isbn={9781429969352},
  lccn={2011027143},
  year={2011},
  publisher={Farrar, Straus and Giroux}
}

@book{kahneman2021noise,
  title={Noise: A Flaw in Human Judgment},
  author={Daniel Kahneman and Sibony, O. and Sunstein, C.R.},
  isbn={9780008308995},
  url={https://www.google.com/books?id=vCZMzQEACAAJ},
  year={2021},
  publisher={William Collins}
}
@book{keresztes1995practical,
  title={A practical Hungarian grammar},
  author={Keresztes, L.},
  url={https://books.google.co.il/books?id=O3SInQAACAAJ},
  year={1995},
  publisher={Debreceni nyari egyetem},
  keywords={Hungarian language--Grammar}
}

@article{kerns2006,
	title = {Definetti{\textquoteright}s Theorem for Abstract Finite Exchangeable Sequences},
	author = {Kerns, G. Jay. and {Székely}, {Gábor J.}},
	year = {2006},
	month = {08},
	date = {2006-08-18},
	journal = {Journal of Theoretical Probability},
	pages = {589--608},
	volume = {19},
	number = {3},
	doi = {10.1007/s10959-006-0028-z},
	url = {http://dx.doi.org/10.1007/s10959-006-0028-z},
	langid = {en}
}

@book{kerns2018introduction,
  title = {Introduction to Probability and Statistics Using R},
  author = {G. Jay Kerns},
  year = 2018,
  file = {:IPSUR.pdf:PDF},
  groups = {Ecologia Numérica, Modelação Ecológica},
}

@book{kerns2018introduction,
  title = {Introduction to Probability and Statistics Using R},
  author = {G. Jay Kerns},
  year = 2018,
  file = {:IPSUR.pdf:PDF},
  groups = {Ecologia Numérica, Modelação Ecológica},
}

@online{kleinikink2016naturalmoney,
  author = {Klein Ikink, Bart},
  title = {Model Thinking},
  howpublished = {\url{https://naturalmoney.org/modelthinking-01.html}},
  year = {2016},
  month={03},
  note = {Accessed: 204-02-24},
}

@inbook{knuth-fa,
  author    = {Donald E. Knuth},
  title     = {Fundamental Algorithms},
  publisher = {Addison-Wesley},
  year      = {1973},
  chapter   = {1.2},
  keywords  = {knuth,programming}
}

@inbook{knuth-fa,
  author    = {Donald E. Knuth},
  title     = {Fundamental Algorithms},
  publisher = {Addison-Wesley},
  year      = {1973},
  chapter   = {1.2},
  keywords  = {knuth,programming}
}

@online{knuthwebsite,
  author   = {Donald Knuth},
  title    = {Knuth: Computers and Typesetting},
  url      = {http://www-cs-faculty.stanford.edu/~uno/abcde.html},
  addendum = {(accessed: 01.09.2016)},
  keywords = {latex,knuth}
}

@online{knuthwebsite,
  author   = {Donald Knuth},
  title    = {Knuth: Computers and Typesetting},
  url      = {http://www-cs-faculty.stanford.edu/~uno/abcde.html},
  addendum = {(accessed: 01.09.2016)},
  keywords = {latex,knuth}
}

@article{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}


@book{kruschke2011doing,
  title = {Doing Bayesian data analysis: a tutorial with {R} and {BUGS}},
  author = {Kruschke, John K.},
  year = 2011,
  publisher = {Academic Press},
  address = {Burlington, MA},
  isbn = {9780123814852 0123814855},
  url = {http://www.amazon.com/Doing-Bayesian-Data-Analysis-Tutorial/dp/0123814855},
  abstract = {There is an explosion of interest in {Bayesian statistics}, primarily because recently created computational methods have finally made Bayesian analysis tractable and accessible to a wide audience. Doing Bayesian Data Analysis, A Tutorial Introduction with R and BUGS, is for first year graduate students or advanced undergraduates and provides an accessible approach, as all mathematics is explained intuitively and with concrete examples. It assumes only algebra and a rustya calculus. Unlike other textbooks, this book begins with the basics, including essential concepts of probability and random sampling. The book gradually climbs all the way to advanced hierarchical modeling methods for realistic data. The text provides complete examples with the R programming language and BUGS software (both freeware), and begins with basic programming examples, working up gradually to complete programs for complex analyses and presentation graphics. These templates can be easily adapted for a large variety of students and their own research needs. The textbook bridges the students from their undergraduate training into modern Bayesian methods.},
  added-at = {2016-05-11T11:06:36.000+0200},
  biburl = {https://www.bibsonomy.org/bibtex/2f7502614d5262ce0f764de992271e896/thoni},
  description = {Amazon.com: Doing Bayesian Data Analysis: A Tutorial with R and BUGS (8601300089751): John K. Kruschke: Books},
  interhash = {92085c933548da90543a89cac3ca2f76},
  intrahash = {f7502614d5262ce0f764de992271e896},
  keywords = {analysis bayesian data},
  refid = 653121532,
  timestamp = {2016-09-06T08:23:07.000+0200},
}

@book{kruschke2011doing,
  title = {Doing Bayesian data analysis: a tutorial with {R} and {BUGS}},
  author = {Kruschke, John K.},
  year = 2011,
  publisher = {Academic Press},
  address = {Burlington, MA},
  isbn = {9780123814852 0123814855},
  url = {http://www.amazon.com/Doing-Bayesian-Data-Analysis-Tutorial/dp/0123814855},
  abstract = {There is an explosion of interest in {Bayesian statistics}, primarily because recently created computational methods have finally made Bayesian analysis tractable and accessible to a wide audience. Doing Bayesian Data Analysis, A Tutorial Introduction with R and BUGS, is for first year graduate students or advanced undergraduates and provides an accessible approach, as all mathematics is explained intuitively and with concrete examples. It assumes only algebra and a rustya calculus. Unlike other textbooks, this book begins with the basics, including essential concepts of probability and random sampling. The book gradually climbs all the way to advanced hierarchical modeling methods for realistic data. The text provides complete examples with the R programming language and BUGS software (both freeware), and begins with basic programming examples, working up gradually to complete programs for complex analyses and presentation graphics. These templates can be easily adapted for a large variety of students and their own research needs. The textbook bridges the students from their undergraduate training into modern Bayesian methods.},
  added-at = {2016-05-11T11:06:36.000+0200},
  biburl = {https://www.bibsonomy.org/bibtex/2f7502614d5262ce0f764de992271e896/thoni},
  description = {Amazon.com: Doing Bayesian Data Analysis: A Tutorial with R and BUGS (8601300089751): John K. Kruschke: Books},
  interhash = {92085c933548da90543a89cac3ca2f76},
  intrahash = {f7502614d5262ce0f764de992271e896},
  keywords = {analysis bayesian data},
  refid = 653121532,
  timestamp = {2016-09-06T08:23:07.000+0200},
}

@inproceedings{kusner2017counterfactual,
  title = {Counterfactual Fairness},
  author = {Kusner, Matt J and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
  year = 2017,
  booktitle = {Advances in Neural Information Processing Systems},
  publisher = {Curran Associates, Inc.},
  volume = 30,
  url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf},
  editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  abstract = {Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.},
}

@inproceedings{kusner2017counterfactual,
  title = {Counterfactual Fairness},
  author = {Kusner, Matt J and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
  year = 2017,
  booktitle = {Advances in Neural Information Processing Systems},
  publisher = {Curran Associates, Inc.},
  volume = 30,
  url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf},
  editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  abstract = {Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.},
}

@book{lattimore2020bandit,
  title={Bandit Algorithms},
  author={Lattimore, T. and Szepesv{\'a}ri, C.},
  isbn={9781108486828},
  lccn={2019053276},
  url={https://tor-lattimore.com/downloads/book/book.pdf},
  year={2020},
  publisher={Cambridge University Press}
}

@Manual{le2022treeheatr,
  title = {treeheatr: Heatmap-Integrated Decision Tree Visualizations},
  author = {Trang Le and Jason Moore},
  year = {2022},
  note = {https://trangdata.github.io/treeheatr/index.html,
https://trangdata.github.io/treeheatr-manuscript/},
}

@book{levitt2004freakonomics,
  title={Freakonomics A Rogue Economist Explores the Hidden Side of Everything},
  author={Steven D. Levitt and Stephen J. Dubner},
  url={https://books.google.com/books?id=R8qDnQAACAAJ},
  year={2004}
}

@book{levitt2009super,
  title={Super Freakonomics},
  author={Steven D. Levitt and Stephen J. Dubner},
  isbn={9780060889579},
  lccn={2009035852},
  url={https://books.google.co.il/books?id=_-H_zwEACAAJ},
  year={2009},
  publisher={HarperCollins}
}

@inproceedings{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013",
    pages = "74--81",
}


@article{luhn-58,
  author  = {Luhn, H. P.},
  journal = {IBM Journal of Research and Development},
  title   = {The Automatic Creation of Literature Abstracts},
  url     = {https://courses.ischool.berkeley.edu/i256/f06/papers/luhn58.pdf},
  year    = {1958},
  volume  = {2},
  number  = {2},
  pages   = {159-165},
  doi     = {10.1147/rd.22.0159}
}

@article{luhn-58,
  author  = {Luhn, H. P.},
  journal = {IBM Journal of Research and Development},
  title   = {The Automatic Creation of Literature Abstracts},
  url     = {https://courses.ischool.berkeley.edu/i256/f06/papers/luhn58.pdf},
  year    = {1958},
  volume  = {2},
  number  = {2},
  pages   = {159-165},
  doi     = {10.1147/rd.22.0159}
}

@misc{lundberg2017unified,
      title={A Unified Approach to Interpreting Model Predictions}, 
      author={Scott Lundberg and Su-In Lee},
      year={2017},
      eprint={1705.07874},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

% - ICE alg for XAI
@book{mackay2003information,
  title = {Information Theory, Inference, and Learning Algorithms},
  author = {MacKay, David J. C.},
  year = 2003,
  publisher = {Copyright Cambridge University Press},
  added-at = {2007-05-24T14:43:04.000+0200},
  biburl = {https://www.bibsonomy.org/bibtex/24c23fea472f6e75c0964badd83883d77/tmalsburg},
  interhash = {86f621d9d6f9f159448f768d792d4511},
  intrahash = {4c23fea472f6e75c0964badd83883d77},
  keywords = {bayesianinference book informationtheory neuralnetworks patternrecognition probabilitytheory},
  timestamp = {2007-05-24T14:43:04.000+0200},
}

@book{mackay2003information,
  title = {Information Theory, Inference, and Learning Algorithms},
  author = {MacKay, David J. C.},
  year = 2003,
  publisher = {Copyright Cambridge University Press},
  added-at = {2007-05-24T14:43:04.000+0200},
  biburl = {https://www.bibsonomy.org/bibtex/24c23fea472f6e75c0964badd83883d77/tmalsburg},
  interhash = {86f621d9d6f9f159448f768d792d4511},
  intrahash = {4c23fea472f6e75c0964badd83883d77},
  keywords = {bayesianinference book informationtheory neuralnetworks patternrecognition probabilitytheory},
  timestamp = {2007-05-24T14:43:04.000+0200},
}

@book{mahmoud2008polya,
  title={Polya Urn Models},
  author={Mahmoud, H.},
  isbn={9781420059847},
  series={Chapman \& Hall/CRC Texts in Statistical Science},
  url={https://books.google.co.il/books?id=7Bizo28c2LQC},
  year={2008},
  publisher={CRC Press}
}

@book{malkiel2007random,
  title={A Random Walk Down Wall Street: The Time-Tested Strategy for Successful Investing (Ninth Edition)},
  author={Malkiel, B.G.},
  isbn={9780393330335},
  lccn={2010455330},
  series={Business book summary},
  year={2007},
  publisher={W. W. Norton}
}

@book{mauboussin2012success,
  title={The Success Equation: Untangling Skill and Luck in Business, Sports, and Investing},
  author={Mauboussin, M.J.},
  isbn={9781422184233},
  lccn={2012018975},
  series={G - Reference,Information and Interdisciplinary Subjects Series},
  year={2012},
  publisher={Harvard Business Review Press}
}


@article{mcculloch1943logical,
  title={A logical calculus of the ideas immanent in nervous activity},
  author={McCulloch, Warren S and Pitts, Walter},
  journal={The bulletin of mathematical biophysics},
  volume={5},
  pages={115--133},
  year={1943},
  publisher={Springer}
}

@article{mcculloch43a,
  added-at = {2008-02-26T11:58:58.000+0100},
  author = {Mcculloch, Warren and Pitts, Walter},
  biburl = {https://www.bibsonomy.org/bibtex/26fbacb0ae04bc17d296d9265dfc90dff/schaul},
  citeulike-article-id = {2380493},
  description = {idsia},
  interhash = {3e8e0d06f376f3eb95af89d5a2f15957},
  intrahash = {6fbacb0ae04bc17d296d9265dfc90dff},
  journal = {Bulletin of Mathematical Biophysics},
  keywords = {evolutionary},
  pages = {127--147},
  priority = {2},
  timestamp = {2008-02-26T12:00:58.000+0100},
  title = {A Logical Calculus of Ideas Immanent in Nervous Activity},
  volume = 5,
  year = 1943
}

@book{mckinney2022python,
  title={Python for Data Analysis},
  author={McKinney, W.},
  isbn={9781098104009},
  url={https://books.google.co.il/books?id=EgKBEAAAQBAJ},
  year={2022},
  publisher={O'Reilly Media}
}


@book{mcluhan1988understanding,
  added-at = {2015-12-09T19:54:14.000+0100},
  address = {New York},
  author = {McLuhan, Marshall},
  biburl = {https://www.bibsonomy.org/bibtex/287087f562ceb3e10bbc578e9d8ec6bdf/mikaelbook},
  interhash = {857d529ae5d94c1251a77564dfeddd12},
  intrahash = {87087f562ceb3e10bbc578e9d8ec6bdf},
  isbn = {0451624963 9780451624963},
  keywords = {library media,media,tetrad},
  publisher = {New American Library},
  refid = {18998166},
  timestamp = {2015-12-09T19:54:14.000+0100},
  title = {Understanding media : the extensions of man},
  url = {http://www.worldcat.org/search?qt=worldcat_org_all&q=0451624963},
  year = 1988
}


@article{mellers2015identifying,
  title={Identifying and cultivating superforecasters as a method of improving probabilistic predictions},
  author={Mellers, Barbara and Stone, Eric and Murray, Terry and Minster, Angela and Rohrbaugh, Nick and Bishop, Michael and Chen, Eva and Baker, Joshua and Hou, Yuan and Horowitz, Michael and others},
  journal={Perspectives on Psychological Science},
  volume={10},
  number={3},
  pages={267--281},
  year={2015},
  publisher={Sage Publications Sage CA: Los Angeles, CA},
  abstract={Across a wide range of tasks, research has shown that people make poor probabilistic predictions of future events. Recently, the U.S. Intelligence Community sponsored a series of forecasting tournaments designed to explore the best strategies for generating accurate subjective probability estimates of geopolitical events. In this article, we describe the winning strategy: culling off top performers each year and assigning them into elite teams of superforecasters. Defying expectations of regression toward the mean 2 years in a row, superforecasters maintained high accuracy across hundreds of questions and a wide array of topics. We find support for four mutually reinforcing explanations of superforecaster performance: (a) cognitive abilities and styles, (b) task-specific skills, (c) motivation and commitment, and (d) enriched environments. These findings suggest that superforecasters are partly discovered and partly created—and that the high-performance incentives of tournaments highlight aspects of human judgment that would not come to light in laboratory paradigms focused on typical performance}
}

@misc{miller2004standing,
  title={The standing ovation problem},
  author={Miller, John H and Page, Scott E},
  journal={Complexity},
  volume={9},
  number={5},
  pages={8--16},
  year={2004},
  publisher={Wiley Subscription Services, Inc., A Wiley Company Hoboken},
  langid = {en}

}

@book{minsky69perceptrons,
  added-at = {2008-05-16T13:57:01.000+0200},
  address = {Cambridge, MA, USA},
  author = {Minsky, Marvin and Papert, Seymour},
  biburl = {https://www.bibsonomy.org/bibtex/206a5a6751b3e61408455fca2ed8d87fc/sb3000},
  description = {: mf : blob : » bibtex},
  interhash = {d80d4948a422623047f1b800272c0389},
  intrahash = {06a5a6751b3e61408455fca2ed8d87fc},
  keywords = {linear-classification neural-networks seminal},
  publisher = {MIT Press},
  timestamp = {2008-05-16T13:57:02.000+0200},
  title = {Perceptrons: An Introduction to Computational Geometry},
  year = 1969
}

  @book{minto1996minto,
  title={The Minto Pyramid Principle: Logic in Writing, Thinking, and Problem Solving},
  author={Minto, B. and Deutsch, V.},
  isbn={9780960191048},
  lccn={95094799},
  url={https://www.google.com/books?id=rPJVAAAACAAJ},
  year={1996},
  publisher={Minto International, Incorporated}
}

@article{mnih2009improving,
  title={Improving a statistical language model through non-linear prediction},
  author={Mnih, Andriy and Yuecheng, Zhang and Hinton, Geoffrey},
  journal={Neurocomputing},
  volume={72},
  number={7-9},
  pages={1414--1418},
  year={2009},
  doi={https://doi.org/10.1016/j.neucom.2008.12.025},
  publisher={Elsevier Science Publishers BV Amsterdam, The Netherlands, The Netherlands}
}


@MISC {mo-1302543,
    TITLE = {Why Square Brackets for Expectation},
    AUTHOR = {Autolatry (https://math.stackexchange.com/users/25097/autolatry)},
    HOWPUBLISHED = {Mathematics Stack Exchange},
    NOTE = {URL:https://math.stackexchange.com/q/1302543 (version: 2015-05-28)},
    EPRINT = {https://math.stackexchange.com/q/1302543},
    URL = {https://math.stackexchange.com/q/1302543}
}

@ARTICLE{mohamed2012acoustic,
  author={Mohamed, Abdel-rahman and Dahl, George E. and Hinton, Geoffrey},
  journal={IEEE Transactions on Audio, Speech, and Language Processing}, 
  title={Acoustic Modeling Using Deep Belief Networks}, 
  year={2012},
  volume={20},
  number={1},
  pages={14-22},
  keywords={Hidden Markov models;Data models;Training;Artificial neural networks;Speech;Speech recognition;Computational modeling;Acoustic modeling;deep belief networks (DBNs);neural networks;phone recognition},
  doi={10.1109/TASL.2011.2109382}}

@book{mokyr2011gifts,
  title={The Gifts of Athena: Historical Origins of the Knowledge Economy},
  author={Mokyr, J.},
  isbn={9781400829439},
  lccn={2002025105},
  year={2011},
  publisher={Princeton University Press}
}

@book{molnar2022,
  title      = {Interpretable Machine Learning},
  author     = {Christoph Molnar},
  year       = {2022},
  subtitle   = {A Guide for Making Black Box Models Explainable},
  edition    = {2},
  url        = {https://christophm.github.io/interpretable-ml-book}
}


% - Delphi technique Decision Model
$misc{admin_delphi_2021,
	title = {Delphi {Technique} {Steps} - {Examples} {And} {Definition} ({Free} {Guide})},
	url = {https://www.zambianguardian.com/delphi-technique/},
	abstract = {Delphi technique steps created by the Rand corporation to forecast on the impact and provide future outcomes through expert opinions.},
	language = {en-US},
	urldate = {2021-09-19},
	journal = {Zambianguardian.com},
	author = {{Admin}},
	month = may,
	year = {2021},
}

% - Definition of XAI
@book{moore2020perfectly,
  title={Perfectly Confident: How to Calibrate Your Decisions Wisely},
  author={Moore, D.A.},
  isbn={9780062887771},
  lccn={2019052525},
  url={https://www.google.com/books?id=CminDwAAQBAJ},
  year={2020},
  publisher={HarperCollins}
}

@article{morita2008determining,
  title={Determining the effective sample size of a parametric prior},
  author={Morita, Satoshi and Thall, Peter F and M{\"u}ller, Peter},
  journal={Biometrics},
  volume={64},
  number={2},
  pages={595--602},
  year={2008},
  publisher={Wiley Online Library}
}


@inproceedings{mothilal2020dice,
  title = {Explaining machine learning classifiers through diverse counterfactual explanations},
  author = {Ramaravind K. Mothilal and Amit Sharma and Chenhao Tan},
  year = 2020,
  month = jan,
  booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
  publisher = {ACM},
  doi = {10.1145/3351095.3372850},
  url = {https://doi.org/10.1145%2F3351095.3372850},
}

@inproceedings{mothilal2020explaining,
  title = {Explaining machine learning classifiers through diverse counterfactual explanations},
  author = {Ramaravind K. Mothilal and Amit Sharma and Chenhao Tan},
  year = 2020,
  month = jan,
  booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
  publisher = {ACM},
  doi = {10.1145/3351095.3372850},
  url = {https://doi.org/10.1145%2F3351095.3372850},
}
@book{máté2001igéző,
  title={Ig{\'e}z{\H{o}}: igek{\"o}t{\H{o}}s ig{\'e}k gyakorl{\'o}k{\"o}nyve},
  author={M{\'a}t{\'e}, J. and Hlavacska, E. and S{\'a}ndor, M. and Cs{\'a}tth, A.},
  url={https://books.google.co.il/books?id=hWEBzgEACAAJ},
  year={2001},
  publisher={Debreceni Ny{\'a}ri Egyetem},
  keywords={Hungarian language--Grammar}
}

@inbook{n2003raven,
  title = "Raven Progressive Matrices",
  author = "John and Raven, Jean",
  year = 2003,
  booktitle = "Handbook of Nonverbal Assessment",
  publisher = "Springer US",
  address = "Boston, MA",
  pages = "223--237",
  doi = "10.1007/978-1-4615-0153-4_11",
  isbn = "978-1-4615-0153-4",
  url = "https://doi.org/10.1007/978-1-4615-0153-4_11",
  editor = "McCallum, R. Steve",
  abstract = "The Raven Progressive Matrices (RPM) tests measure ``general cognitive ability'' or, better, eductive, or ``meaning making,'' ability (Raven, Raven, {\&} Court, 1998a,2000). The term ``eductive'' comes from the Latin root educere, which means, ``to draw out.'' The basic version of the test, known as the Standard Progressive Matrices (or SPM), consists of five sets of items of the kind shown in Figures 11.1 and 11.2. Within each set, the items become progressively more difficult. At the beginning of each set, the items, although easy again, follow a different logic. The sets in turn become progressively more difficult. The five sets offer those taking the test five opportunities to become familiar with the method of thought required to solve the problems. In addition to the Standard series, there is the Coloured Progressive Matrices (CPM), which is designed to spread the scores of children and less able adults and the Advanced Progressive Matrices (APM), developed to spread the scores of the top 20{\%} of the population.",
}

@book{navas2013triz,
  title = {TRIZ: Design {Problem Solving} with Systematic Innovation},
  author = {Helena V. G. Navas},
  year = 2013,
  booktitle = {Advances in Industrial Design Engineering},
  publisher = {IntechOpen},
  address = {Rijeka},
  doi = {10.5772/55979},
  url = {https://doi.org/10.5772/55979},
  abstract = {The Scout Mindset challenges readers to move beyond gut reactions and preconceptions and rethink problems. The book offers instructions for overcoming bias and central beliefs to gather more objective data. Julia Galef encourages readers to act more like scouts than soldiers and gather information without judging to make more informed decisions. The text outlines the common reasons folks jump to conclusions and offers advice on how to avoid incorrect assumptions and conduct level-headed analyses. The Scout Mindset is a call to action for objectivity and an instruction manual for breaking away from unhelpful mental patterns that can lead to poor choices.},
  keywords = {problem solving},
  editor = {Denis A. Coelho},
  chapter = 4,
}

@article{neal1992connectionist,
  title={Connectionist learning of belief networks},
  author={Neal, Radford M},
  journal={Artificial intelligence},
  volume={56},
  number={1},
  pages={71--113},
  year={1992},
  publisher={Elsevier}
}

@book{newman2010networks,
  title={Networks: An Introduction},
  author={Newman, M.},
  isbn={9780191500701},
  year={2010},
  publisher={OUP Oxford}
}

@article{ney1994structuring,
  title = {On structuring probabilistic dependences in stochastic language modelling},
  author = {Hermann Ney and Ute Essen and Reinhard Kneser},
  year = 1994,
  journal = {Computer Speech & Language},
  volume = 8,
  number = 1,
  pages = {1--38},
  doi = {https://doi.org/10.1006/csla.1994.1001},
  issn = {0885-2308},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230884710011},
  abstract = {In this paper, we study the problem of stochastic language modelling from the viewpoint of introducing suitable structures into the conditional probability distributions. The task of these distributions is to predict the probability of a new word by looking at M or even all predecessor words. The conventional approach is to limit M to 1 or 2 and to interpolate the resulting bigram and trigram models with a unigram model in a linear fashion. However, there are many other structures that can be used to model the probabilistic dependences between the predecessor word and the word to be predicted. The structures considered in this paper are: nonlinear interpolation as an alternative to linear interpolation; equivalence classes for word histories and single words; cache memory and word associations. For the optimal estimation of nonlinear and linear interpolation parameters, the leaving-one-out method is systematically used. For the determination of word equivalence classes in a bigram model, an automatic clustering procedure has been adapted. To capture long-distance dependences, we consider various models for word-by-word dependences; the cache model may be viewed as a special type of self-association. Experimental results are presented for two text databases, a Germany database and an English database.},
}

@inproceedings{ng2001discriminative,
  title = {On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes},
  author = {Ng, Andrew and Jordan, Michael},
  year = 2001,
  booktitle = {Advances in Neural Information Processing Systems},
  publisher = {MIT Press},
  volume = 14,
  pages = {},
  url = {https://proceedings.neurips.cc/paper_files/paper/2001/file/7b7a53e239400a13bd6be6c91c4f6c4e-Paper.pdf},
  editor = {T. Dietterich and S. Becker and Z. Ghahramani},
}
@book{nowak2012supercooperators,
  title={SuperCooperators: Altruism, Evolution, and Why We Need Each Other to Succeed},
  author={Nowak, M. and Highfield, R.},
  isbn={9781451626636},
  lccn={2010035517},
  year={2012},
  publisher={Free Press}
}

@book{ouhalla1999introducing,
  title={Introducing Transformational Grammar: From Principles and Parameters to Minimalism},
  author={Ouhalla, J.},
  isbn={9780340740361},
  lccn={98040839},
  series={A Hodder Arnold Publication},
  url={https://www.google.com/books?id=ZP-ZQ0lKI9QC},
  year={1999},
  publisher={Arnold}
}

@book{page2007difference,
  title={The Difference: How the Power of Diversity Creates Better Groups, Firms, Schools, and Societies},
  author={Page, S.E.},
  isbn={9780691128382},
  lccn={2006044678},
  series={The William G. Bowen Memorial Series in Higher Education Series},
  year={2007},
  publisher={Princeton University Press}
}

@online{page2017modelthinking,
  author = {Page, Scott E},
  title = {Model Thinking [MOOC]},
  publisher= {Coursera},
  howpublished = {\url{https://www.coursera.org/learn/model-thinking}},
  year = {2014},
  month={11},
  note = {Accessed: 204-02-24},
}


@book{page2018model,
  title={The Model Thinker: What You Need to Know to Make Data Work for You},
  author={Page, Scott E},
  isbn={9780465094622},
  url={https://www.hachettebookgroup.com/titles/scott-e-page/the-model-thinker/9780465094622/?lens=basic-books},
  year={2018},
  publisher={Basic Books},
  pages={448},
  langid = {en}

}

@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002},
  url={https://www.aclweb.org/anthology/P02-1040.pdf}
}

@article{pearl2009,
	title = {Causality},
	author = {Pearl, Judea},
	year = {2009},
	month = {09},
	date = {2009-09-14},
	doi = {10.1017/cbo9780511803161},
	url = {http://dx.doi.org/10.1017/CBO9780511803161}
}


@book{pearl2018book,
  title={The Book of Why: The New Science of Cause and Effect},
  author={Pearl, Judea and Mackenzie, D.},
  isbn={9780241242643},
  url={https://books.google.co.il/books?id=EmY8DwAAQBAJ},
  year={2018},
  publisher={Penguin Books Limited}
}

@book{pearson1990student,
  title={Student: A Statistical Biography of William Sealy Gosset},
  author={Pearson, E.S. and Gosset, W.S. and Plackett, R.L. and Barnard, G.A.},
  isbn={9780198522270},
  lccn={89072137},
  url={https://books.google.co.il/books?id=LBDvAAAAMAAJ},
  year={1990},
  publisher={Clarendon Press}
}

@InProceedings{pmlr-v48-gal16,
  title = 	 {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},
  author = 	 {Gal, Yarin and Ghahramani, Zoubin},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {1050--1059},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/gal16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/gal16.html},
  abstract = 	 {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs – extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout’s uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout’s uncertainty in deep reinforcement learning.}
}

@misc{poisson2019english,
      title={English Translation of Poisson's "Recherches sur la probabilit\'e des jugements en mati\`ere criminelle et en mati\`ere civile" / "Researches into the Probabilities of Judgements in Criminal and Civil Cases"}, 
      author={S. -D. Poisson},
      year={2019},
      eprint={1902.02782},
      archivePrefix={arXiv},
      primaryClass={math.HO}
}

@book{polya1945,
	title = {How to Solve It},
	author = {Polya, G.},
	year = {1945},
	month = {12},
	date = {1945-12-31},
	publisher = {Princeton University Press},
	doi = {10.1515/9781400828678},
	url = {http://dx.doi.org/10.1515/9781400828678}
}

@book{polya1954patterns,
  title={Patterns of Plausible Inference},
  author={Polya, G.},
  isbn={9780691025103},
  lccn={54010695},
  series={Princeton Paperbacks},
  url={https://www.google.com/books?id=Zu2hEAAAQBAJ},
  year={1954},
  publisher={Princeton University Press}
}

@book{polya2014how,
  title = {How to Solve It: A New Aspect of Mathematical Method},
  author = {Polya, G. and Conway, J.H.},
  year = 2014,
  publisher = {Princeton University Press},
  series = {Princeton Science Library},
  pages = 288,
  isbn = 9780691164076,
  url = {https://books.google.co.il/books?id=Zu2hEAAAQBAJ},
  lccn = 2014941268,
  howpublished = {Paperback},
  keywords = {algorithms, math, logic, geometry},
  biburl = {https://www.bibsonomy.org/bibtex/2237322f30081bafcc21a4dfb67d46d94/chato},
  abstract = {A perennial bestseller by eminent mathematician G. Polya, <I>How to Solve It</I> will show anyone in any field how to think straight.<P>In lucid and appealing prose, Polya reveals how the mathematical method of demonstrating a proof or finding an unknown can be of help in attacking any problem that can be "reasoned" out--from building a bridge to winning a game of anagrams. Generations of readers have relished Polya's deft--indeed, brilliant--instructions on stripping away irrelevancies and going straight to the heart of the problem.<P>},
}
@book{polya2014how,
  title = {How to Solve It: A New Aspect of Mathematical Method},
  author = {Polya, G. and Conway, J.H.},
  year = 2014,
  publisher = {Princeton University Press},
  series = {Princeton Science Library},
  pages = 288,
  isbn = 9780691164076,
  url = {https://books.google.co.il/books?id=Zu2hEAAAQBAJ},
  lccn = 2014941268,
  howpublished = {Paperback},
  keywords = {algorithms, math, logic, geometry},
  biburl = {https://www.bibsonomy.org/bibtex/2237322f30081bafcc21a4dfb67d46d94/chato},
  abstract = {A perennial bestseller by eminent mathematician G. Polya, <I>How to Solve It</I> will show anyone in any field how to think straight.<P>In lucid and appealing prose, Polya reveals how the mathematical method of demonstrating a proof or finding an unknown can be of help in attacking any problem that can be "reasoned" out--from building a bridge to winning a game of anagrams. Generations of readers have relished Polya's deft--indeed, brilliant--instructions on stripping away irrelevancies and going straight to the heart of the problem.<P>},
}
@book{pontifex1993hungarian,
  title={Hungarian: A Complete Course for Beginners},
  author={Pontifex, Z.},
  isbn={9780340562864},
  lccn={gb94007165},
  series={Teach yourself books},
  url={https://books.google.co.il/books?id=dCrvAQAACAAJ},
  year={1993},
  publisher={Hodder \& Stoughton},
  keywords={Hungarian language--Grammar}
}

@book{pontifex2011complete,
  title={Complete Hungarian Beginner to Intermediate Book and Audio Course: Learn to read, write, speak and understand a new language with Teach Yourself},
  author={Pontifex, Z.},
  isbn={9781444134353},
  series={Complete Languages},
  url={https://books.google.co.il/books?id=SrYuFsszTKYC},
  year={2011},
  publisher={John Murray Press}
}

@INCOLLECTION{ramsey1926,
title = {Truth and Probability},
author = {Ramsey, Frank P.},
year = {1926},
chapter = {7},
pages = {156-198},
booktitle = {The Foundations of Mathematics and other Logical Essays},
editor = {Braithwaite, R. B.},
publisher = {McMaster University Archive for the History of Economic Thought},
abstract = {Contains two other essays as well: Further Considerations & Last Papers: Probability and Partial Belief.},
url = {https://EconPapers.repec.org/RePEc:hay:hetcha:ramsey1926}
}

@book{resnick1994turtles,
  title={Turtles, Termites, and Traffic Jams: Explorations in Massively Parallel Microworlds},
  author={Resnick, M.},
  isbn={9780262181624},
  lccn={94010956},
  series={A Bradford book},
  url={https://books.google.co.il/books?id=kl6zQgAACAAJ},
  year={1994},
  publisher={MIT Press}
}

@inproceedings{ribeiro-etal-2020-beyond,
    title = "Beyond Accuracy: Behavioral Testing of {NLP} Models with {C}heck{L}ist",
    author = "Ribeiro, Marco Tulio  and
      Wu, Tongshuang  and
      Guestrin, Carlos  and
      Singh, Sameer",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.442",
    doi = "10.18653/v1/2020.acl-main.442",
    pages = "4902--4912",
    abstract = "Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.",
}

@article{ribeiro2018,
	title = {Anchors: High-Precision Model-Agnostic Explanations},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	year = {2018},
	month = {04},
	date = {2018-04-25},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume = {32},
	number = {1},
	doi = {10.1609/aaai.v32i1.11491},
	url = {http://dx.doi.org/10.1609/aaai.v32i1.11491}
}

@book{rich1991artificial,
  title={Artificial Intelligence},
  author={Rich, E. and Knight, K.},
  isbn={9780071008945},
  lccn={lc90020608},
  series={Artificial Intelligence Series},
  url={https://www.google.com/books?id=6P6jPwAACAAJ},
  year={1991},
  publisher={McGraw-Hill}
}


@misc{rita2020lazimpa,
      title={"LazImpa": Lazy and Impatient neural agents learn to communicate efficiently}, 
      author={Mathieu Rita and Rahma Chaabouni and Emmanuel Dupoux},
      year={2020},
      eprint={2010.01878},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      keywords={Emergent communication, {Zipf's Law of Abbreviation}, laziness, impatience, signaling games},
      
}


@misc{rita2022emergent,
      title={Emergent Communication: Generalization and Overfitting in Lewis Games}, 
      author={Mathieu Rita and Corentin Tallec and Paul Michel and Jean-Bastien Grill and Olivier Pietquin and Emmanuel Dupoux and Florian Strub},
      year={2022},
      eprint={2209.15342},
      archivePrefix={arXiv},
      primaryClass={cs.MA}
}

@misc{rita2022role,
      title={On the role of population heterogeneity in emergent communication}, 
      author={Mathieu Rita and Florian Strub and Jean-Bastien Grill and Olivier Pietquin and Emmanuel Dupoux},
      year={2022},
      eprint={2204.12982},
      archivePrefix={arXiv},
      primaryClass={cs.MA},
      keywords={Emergent communication, population heterogeneity, generalization, overfitting, Lewis games},
}

@book{rizzo2019statistical,
  title = {Statistical computing with R Maria L. Rizzo.},
  author = {Rizzo, Maria L},
  year = 2019,
  booktitle = {Statistical computing with R},
  publisher = {CRC Press, Taylor & Francis Group},
  address = {Boca Raton},
  series = {Chapman & Hall/CRC The R Series},
  isbn = 9780429192760,
  abstract = {Computational statistics and statistical computing are two areas that employ computational, graphical, and numerical approaches to solve statistical problems, making the versatile R language an ideal computing environment for these fields. This second edition continues to encompass the traditional core material of computational statistics, with an},
  edition = {Second edition.},
  language = {eng},
  keywords = {Mathematical statistics -- Data processing; Statistics -- Data processing; R (Computer program language)},
}
@book{rizzo2019statistical,
  title = {Statistical computing with R Maria L. Rizzo.},
  author = {Rizzo, Maria L},
  year = 2019,
  booktitle = {Statistical computing with R},
  publisher = {CRC Press, Taylor & Francis Group},
  address = {Boca Raton},
  series = {Chapman & Hall/CRC The R Series},
  isbn = 9780429192760,
  abstract = {Computational statistics and statistical computing are two areas that employ computational, graphical, and numerical approaches to solve statistical problems, making the versatile R language an ideal computing environment for these fields. This second edition continues to encompass the traditional core material of computational statistics, with an},
  edition = {Second edition.},
  language = {eng},
  keywords = {Mathematical statistics -- Data processing; Statistics -- Data processing; R (Computer program language)},
}
@book{rosenblatt1962principles,
  title={Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms},
  author={Rosenblatt, F.},
  lccn={62012882},
  series={Cornell Aeronautical Laboratory. Report no. VG-1196-G-8},
  url={https://www.google.com/books?id=7FhRAAAAMAAJ},
  year={1962},
  publisher={Spartan Books}
}

@book{rounds2013hungarian,
  title={Hungarian: An Essential Grammar},
  author={Rounds, C.H.},
  isbn={9781134589364},
  series={Routledge Essential Grammars},
  url={https://books.google.co.il/books?id=cQ5EaoGJwhsC},
  year={2013},
  publisher={Taylor \& Francis},
  keywords={Hungarian language--Grammar}
}

@book{rounds2015colloquial,
  title={Colloquial Hungarian: The Complete Course for Beginners},
  author={Rounds, C. and Solyom, E.},
  isbn={9781317306252},
  series={Colloquial Series},
  url={https://books.google.co.il/books?id=dLtgCgAAQBAJ},
  year={2015},
  publisher={Taylor \& Francis}
}

@article{rowe1999delphi,
	title = {The {Delphi} technique as a forecasting tool: issues and analysis},
	volume = {15},
	issn = {01692070},
	shorttitle = {The {Delphi} technique as a forecasting tool},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169207099000187},
	doi = {10.1016/S0169-2070(99)00018-7},
	language = {en},
	number = {4},
	urldate = {2021-09-19},
	journal = {International Journal of Forecasting},
	author = {Rowe, Gene and Wright, George},
	month = oct,
	year = {1999},
	pages = {353--375},
}

% delphi method
@book{ruby,
  title     = {The Ruby Programming Language},
  author    = {Flanagan, David and Matsumoto, Yukihiro},
  year      = {2008},
  publisher = {O'Reilly Media}
}


@article{russell2019efficient,
  title = {Efficient Search for Diverse Coherent Explanations},
  author = {Chris Russell},
  year = 2019,
  journal = {CoRR},
  volume = {abs/1901.04909},
  url = {http://arxiv.org/abs/1901.04909},
  eprinttype = {arXiv},
  eprint = {1901.04909},
  timestamp = {Mon, 04 Feb 2019 09:14:09 +0100},
  biburl = {https://dblp.org/rec/journals/corr/abs-1901-04909.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
}
@article{russell2019efficient,
  title = {Efficient Search for Diverse Coherent Explanations},
  author = {Chris Russell},
  year = 2019,
  journal = {CoRR},
  volume = {abs/1901.04909},
  url = {http://arxiv.org/abs/1901.04909},
  eprinttype = {arXiv},
  eprint = {1901.04909},
  timestamp = {Mon, 04 Feb 2019 09:14:09 +0100},
  biburl = {https://dblp.org/rec/journals/corr/abs-1901-04909.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
}
@article{russell2019efficient,
  title = {Efficient Search for Diverse Coherent Explanations},
  author = {Chris Russell},
  year = 2019,
  journal = {CoRR},
  volume = {abs/1901.04909},
  url = {http://arxiv.org/abs/1901.04909},
  eprinttype = {arXiv},
  eprint = {1901.04909},
  timestamp = {Mon, 04 Feb 2019 09:14:09 +0100},
  biburl = {https://dblp.org/rec/journals/corr/abs-1901-04909.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
}
@book{russell2021artificial,
  title={Artificial Intelligence: A Modern Approach, Global Edition},
  author={Russell, S. and Norvig, P.},
  isbn={9781292401171},
  url={https://www.google.com/books?id=cb0qEAAAQBAJ},
  year={2021},
  publisher={Pearson Education}
}

@article{satheesh2003,
	title = {A Supplement To The Bose-Dasgupta-Rubin (2002) Review Of Infinitely Divisible Laws And Processes},
	author = {Satheesh, S.},
	year = {2003},
	date = {2003},
	doi = {10.48550/ARXIV.MATH/0305126},
	url = {https://arxiv.org/abs/math/0305126}
}

@misc{schaul2013pesky,
      title={No More Pesky Learning Rates}, 
      author={Tom Schaul and Sixin Zhang and Yann LeCun},
      year={2013},
      eprint={1206.1106},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{schelling1969models,
  title={Models of segregation},
  author={Schelling, Thomas C},
  journal={The American economic review},
  volume={59},
  number={2},
  pages={488--493},
  year={1969},
  publisher={JSTOR}
}

@article{schelling1971dynamic,
  title={Dynamic models of segregation},
  author={Schelling, Thomas C},
  journal={Journal of mathematical sociology},
  volume={1},
  number={2},
  pages={143--186},
  year={1971},
  publisher={Taylor \& Francis}
}

@book{schelling1978micromotives,
  title={Micromotives and Macrobehavior},
  author={Schelling, Thomas C},
  isbn={9780393090093},
  lccn={78017119},
  series={Fels lectures on public policy analysis},
  url={https://www.google.com/books?id=4C5mQgAACAAJ},
  year={1978},
  publisher={Norton}
}



@article{schick2023,
	title = {Toolformer: Language Models Can Teach Themselves to Use Tools},
	author = {Schick, Timo and Dwivedi-Yu, Jane and {Dessì}, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
	year = {2023},
	date = {2023},
	doi = {10.48550/ARXIV.2302.04761},
	url = {https://arxiv.org/abs/2302.04761}
}

@article{scikit-learn,
  title={Scikit-learn: Machine Learning in {P}ython},
  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
          and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
          and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
          Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal={Journal of Machine Learning Research},
  volume={12},
  pages={2825--2830},
  year={2011}
}

% - scikit-learn api
@article{sharma-2020-dowhy,
 author = {Sharma, Amit and Kiciman, Emre},
 journal = {arXiv preprint arXiv:2011.04216},
 title = {DoWhy: An End-to-End Library for Causal Inference},
 year = {2020}
}


@misc{sharma2020dowhy,
  title = {DoWhy: An End-to-End Library for Causal Inference},
  author = {Amit Sharma and Emre Kiciman},
  year = 2020,
  eprint = {2011.04216},
  archiveprefix = {arXiv},
  primaryclass = {stat.ME},
}
@misc{sharma2020dowhy,
  title = {DoWhy: An End-to-End Library for Causal Inference},
  author = {Amit Sharma and Emre Kiciman},
  year = 2020,
  eprint = {2011.04216},
  archiveprefix = {arXiv},
  primaryclass = {stat.ME},
}
@book{sheather2009,
	title = {A Modern Approach to Regression with R},
	author = {Sheather, Simon},
	year = {2009},
	date = {2009},
	publisher = {Springer New York},
	doi = {10.1007/978-0-387-09608-7},
	url = {http://dx.doi.org/10.1007/978-0-387-09608-7}
}

@book{silver2012signal,
  title={The Signal and the Noise: Why So Many Predictions Fail-but Some Don't},
  author={Silver, N.},
  isbn={9781101595954},
  url={https://www.google.com/books?id=SI-VqAT4_hYC},
  year={2012},
  publisher={Penguin Publishing Group}
}

@Misc{silver2015,
  author = {David Silver},
  title = {Lectures on Reinforcement Learning},
  howpublished = {\textsc{url:}~\url{https://www.davidsilver.uk/teaching/}},
  year = {2015}}
  
  @inproceedings{sklearn_api,
  author    = {Lars Buitinck and Gilles Louppe and Mathieu Blondel and
                Fabian Pedregosa and Andreas Mueller and Olivier Grisel and
                Vlad Niculae and Peter Prettenhofer and Alexandre Gramfort
                and Jaques Grobler and Robert Layton and Jake VanderPlas and
                Arnaud Joly and Brian Holt and Ga{\"{e}}l Varoquaux},
  title     = {{API} design for machine learning software: experiences from the scikit-learn
                project},
  booktitle = {ECML PKDD Workshop: Languages for Data Mining and Machine Learning},
  year      = {2013},
  pages = {108--122},
}


@book{smuts1926holism,
  title={Holism and evolution},
  author={Smuts, J.C.},
  isbn={9785871112274},
  lccn={26016900},
  series={Books for college libraries},
  year={1926},
  publisher={Macmillan}
}

@book{spanos2019probability,
  title={Probability Theory and Statistical Inference},
  author={Spanos, A.},
  isbn={9781107185142},
  lccn={2019016182},
  url={https://books.google.co.il/books?id=9nCiDwAAQBAJ},
  year={2019},
  publisher={Cambridge University Press}
}

@article{steels1997synthetic,
author = {Steels, Luc},
year = {1997},
month = {10},
pages = {},
title = {The Synthetic Modeling of Language Origins},
volume = {1},
journal = {Evolution of Communication Journal},
doi = {10.1075/eoc.1.1.02ste}
}


@article{steels1998origins,
author = {Steels, Luc},
title = {The origins of syntax in visually grounded robotic agents},
year = {1998},
issue_date = {Aug. 1998},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {103},
number = {1–2},
issn = {0004-3702},
url = {https://doi.org/10.1016/S0004-3702(98)00066-6},
doi = {10.1016/S0004-3702(98)00066-6},
journal = {Artif. Intell.},
month = {aug},
pages = {133–156},
numpages = {24},
keywords = {origins of language and meaning, robotic agents, symbolic models}
}

@article{steels2001LanguageGames,
  author = {Steels, Luc},
  journal={IEEE Intelligent Systems}, 
  title={Language games for autonomous robots}, 
  year={2001},
  volume={16},
  number={5},
  pages={16-22},
  keywords={Grounding;Artificial intelligence;Humanoid robots;Pattern recognition;Signal processing algorithms;Hardware;Concurrent computing;Distributed computing;Steel;Computer science},
  doi={10.1109/MIS.2001.956077}}
  
@book{stefik1995introduction,
  title={Introduction to Knowledge Systems},
  author={Stefik, M.},
  isbn={9781558601666},
  lccn={95016537},
  url={https://www.google.com/books?id=zbJQAAAAMAAJ},
  year={1995},
  publisher={Elsevier Science}
}

@book{surowiecki2005wisdom,
  title={The Wisdom of Crowds},
  author={Surowiecki, J.},
  isbn={9780307275059},
  year={2005},
  publisher={Knopf Doubleday Publishing Group}
}

@inproceedings{sutskever2011generating,
  title={Generating text with recurrent neural networks},
  author={Sutskever, Ilya and Martens, James and Hinton, Geoffrey E},
  booktitle={Proceedings of the 28th international conference on machine learning (ICML-11)},
  url= {https://www.cs.toronto.edu/~jmartens/docs/RNN_Language.pdf},
  pages={1017--1024},
  year={2011}
}

@book{sutton2018reinforcement,
  title={Reinforcement Learning, second edition: An Introduction},
  author={Sutton, R.S. and Barto, A.G.},
  isbn={9780262039246},
  lccn={2018023826},
  series={Adaptive Computation and Machine Learning series},
  url={http://incompleteideas.net/book/RLbook2020.pdf},
  year={2018},
  publisher={MIT Press}
}

@book{szepesvári2010algorithms,
  title={Algorithms for Reinforcement Learning},
  author={Szepesv{\'a}ri, C.},
  isbn={9781608454921},
  series={Synthesis lectures on artificial intelligence and machine learning},
  url={http://www.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf},
  year={2010},
  keywords={Reinforcement learning, Markov decision processes, Dynamic programming, Temporal difference learning, Monte Carlo methods, Bandit problems, Exploration-exploitation, Reinforcement learning theory, Reinforcement learning algorithms, Reinforcement learning applications},
  publisher={Morgan \& Claypool}
}


@book{taleb2009black,
  title={The Black Swan},
  author={Taleb, N.N.},
  isbn={9780812979183},
  series={A Random House international edition},
  url={https://www.google.com/books?id=YdOYmYA2TJYC},
  year={2009},
  publisher={Random House}
}



@manual{team2021r,
  title = {R: A Language and Environment for Statistical Computing},
  author = {R Core Team},
  year = 2021,
  address = {Vienna, Austria},
  url = {https://www.R-project.org/},
  organization = {R Foundation for Statistical Computing},
}
@manual{team2021r,
  title = {R: A Language and Environment for Statistical Computing},
  author = {R Core Team},
  year = 2021,
  address = {Vienna, Austria},
  url = {https://www.R-project.org/},
  organization = {R Foundation for Statistical Computing},
}
@book{tetlock2015superforecasting,
  title={Superforecasting: The Art and Science of Prediction},
  author={Philip E. Tetlock and Dan Gardner},
  isbn={9780804136716},
  year={2016},
  publisher={Random House},
  url={https://www.google.com/books?id=hC_qBQAAQBAJ}
  
}

@book{thaler2012nudge,
  title={Nudge: The Final Edition},
  author={Thaler, R.H. and Sunstein, C.R.},
  isbn={9780141976105},
  lccn={2009417516},
  year={2012},
  publisher={Penguin Books Limited}
}


@book{tukey1970exploratory,
  title={Exploratory Data Analysis},
  author={Tukey, J.W.},
  isbn={9780608082257},
  lccn={71083638},
  series={Exploratory Data Analysis},
  url={https://books.google.co.il/books?id=F6IIxgEACAAJ},
  year={1970},
  publisher={Addison Wesley Publishing Company}
}

@article{tversky1974judgment,
  title={Judgment under Uncertainty: Heuristics and Biases: Biases in judgments reveal some heuristics of thinking under uncertainty.},
  author={Tversky, Amos and Daniel Kahneman},
  journal={science},
  volume={185},
  number={4157},
  pages={1124--1131},
  year={1974},
  publisher={American association for the advancement of science}
}

@article{ustun2019,
	title = {Actionable Recourse in Linear Classification},
	author = {Ustun, Berk and Spangher, Alexander and Liu, Yang},
	year = {2019},
	month = {01},
	date = {2019-01-29},
	journal = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
	doi = {10.1145/3287560.3287566},
	url = {http://dx.doi.org/10.1145/3287560.3287566}
}

@book{vanderplas2016python,
  title={Python Data Science Handbook: Essential Tools for Working with Data},
  author={VanderPlas, J.},
  isbn={9781491912133},
  lccn={2017385426},
  url={https://jakevdp.github.io/PythonDataScienceHandbook/},
  year={2016},
  publisher={O'Reilly Media}
}

  
@book{vanderplas2022python,
  title={Python Data Science Handbook: Essential Tools for Working with Data},
  author={VanderPlas, J.},
  isbn={9781098121198},
  url={https://books.google.co.il/books?id=rimgEAAAQBAJ},
  year={2022},
  publisher={O'Reilly Media}
}

@book{vonneumann1947,
  added-at = {2007-02-09T09:58:08.000+0100},
  author = {von Neumann, J. and Morgenstern, O.},
  biburl = {https://www.bibsonomy.org/bibtex/27b0a8d891eca4103155bec877f572aa5/vittorio.loreto},
  citeulike-article-id = {988689},
  interhash = {43a00d641d5aa16c23ba45ea210b15fc},
  intrahash = {7b0a8d891eca4103155bec877f572aa5},
  keywords = {RMP_CFL game theory behaviour economic morgenstern vonneumann},
  priority = {2},
  publisher = {Princeton University Press},
  timestamp = {2007-02-09T09:58:08.000+0100},
  title = {Theory of games and economic behavior},
  year = 1947
}

@article{vos1990ask,
  title={Ask marilyn},
  author={Vos Savant, Marilyn},
  journal={Parade Magazine},
  volume={15},
  number={9},
  year={1990},
  publisher={Parade Publications Incorporated New York, New York}
}

@article{vosSavant1990,
  author = {Marilyn vos Savant},
  title = {Ask Marilyn Column},
  journal = {Parade Magazine},
  volume = {75},
  issue = {473},
  pages = {275--277},
  year = {1991},
  month = {October},
  doi = {10.2307/3619484},
  url = {https://doi.org/10.2307/3619484}
  
}

@misc{wachter2018counterfactual,
  title = {Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR},
  author = {Sandra Wachter and Brent Mittelstadt and Chris Russell},
  year = 2018,
  abstract = {There has been much discussion of the right to explanation in the EU General Data Protection Regulation, and its existence, merits, and disadvantages. Implementing a right to explanation that opens the black box of algorithmic decision-making faces major legal and technical barriers. Explaining the functionality of complex algorithmic decision-making systems and their rationale in specific cases is a technically challenging problem. Some explanations may offer little meaningful information to data subjects, raising questions around their value. Explanations of automated decisions need not hinge on the general public understanding how algorithmic systems function. Even though such interpretability is of great importance and should be pursued, explanations can, in principle, be offered without opening the black box. Looking at explanations as a means to help a data subject act rather than merely understand, one could gauge the scope and content of explanations according to the specific goal or action they are intended to support. From the perspective of individuals affected by automated decision-making, we propose three aims for explanations: (1) to inform and help the individual understand why a particular decision was reached, (2) to provide grounds to contest the decision if the outcome is undesired, and (3) to understand what would need to change in order to receive a desired result in the future, based on the current decision-making model. We assess how each of these goals finds support in the GDPR. We suggest data controllers should offer a particular type of explanation, unconditional counterfactual explanations, to support these three aims. These counterfactual explanations describe the smallest change to the world that can be made to obtain a desirable outcome, or to arrive at the closest possible world, without needing to explain the internal logic of the system.},
  eprint = {1711.00399},
  archiveprefix = {arXiv},
  primaryclass = {cs.AI},
}
@misc{wachter2018counterfactual,
  title = {Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR},
  author = {Sandra Wachter and Brent Mittelstadt and Chris Russell},
  year = 2018,
  abstract = {There has been much discussion of the right to explanation in the EU General Data Protection Regulation, and its existence, merits, and disadvantages. Implementing a right to explanation that opens the black box of algorithmic decision-making faces major legal and technical barriers. Explaining the functionality of complex algorithmic decision-making systems and their rationale in specific cases is a technically challenging problem. Some explanations may offer little meaningful information to data subjects, raising questions around their value. Explanations of automated decisions need not hinge on the general public understanding how algorithmic systems function. Even though such interpretability is of great importance and should be pursued, explanations can, in principle, be offered without opening the black box. Looking at explanations as a means to help a data subject act rather than merely understand, one could gauge the scope and content of explanations according to the specific goal or action they are intended to support. From the perspective of individuals affected by automated decision-making, we propose three aims for explanations: (1) to inform and help the individual understand why a particular decision was reached, (2) to provide grounds to contest the decision if the outcome is undesired, and (3) to understand what would need to change in order to receive a desired result in the future, based on the current decision-making model. We assess how each of these goals finds support in the GDPR. We suggest data controllers should offer a particular type of explanation, unconditional counterfactual explanations, to support these three aims. These counterfactual explanations describe the smallest change to the world that can be made to obtain a desirable outcome, or to arrive at the closest possible world, without needing to explain the internal logic of the system.},
  eprint = {1711.00399},
  archiveprefix = {arXiv},
  primaryclass = {cs.AI},
}
@book{walpole2007probability,
  title = {Probability \& statistics for engineers and scientists},
  author = {Walpole, Ronald E. and Myers, Raymond H. and Myers, Sharon L. and Ye, Keying},
  year = 2007,
  publisher = {Pearson Education},
  address = {Upper Saddle River},
  added-at = {2011-04-12T12:51:01.000+0200},
  biburl = {https://www.bibsonomy.org/bibtex/219a59df44d8aa5a63e6844e0c066cb21/arnsholt},
  edition = {8th},
  interhash = {e1efaca49b5c9c0d8e89a50bccb98901},
  intrahash = {19a59df44d8aa5a63e6844e0c066cb21},
  keywords = {statistics},
  timestamp = {2011-04-29T13:06:24.000+0200},
}
@book{walpole2007probability,
  title = {Probability \& statistics for engineers and scientists},
  author = {Walpole, Ronald E. and Myers, Raymond H. and Myers, Sharon L. and Ye, Keying},
  year = 2007,
  publisher = {Pearson Education},
  address = {Upper Saddle River},
  added-at = {2011-04-12T12:51:01.000+0200},
  biburl = {https://www.bibsonomy.org/bibtex/219a59df44d8aa5a63e6844e0c066cb21/arnsholt},
  edition = {8th},
  interhash = {e1efaca49b5c9c0d8e89a50bccb98901},
  intrahash = {19a59df44d8aa5a63e6844e0c066cb21},
  keywords = {statistics},
  timestamp = {2011-04-29T13:06:24.000+0200},
}
@article{weichselbaumer2019,
	title = {Multiple Discrimination against Female Immigrants Wearing Headscarves},
	author = {Weichselbaumer, Doris},
	year = {2019},
	month = {09},
	date = {2019-09-17},
	journal = {ILR Review},
	pages = {600--627},
	volume = {73},
	number = {3},
	doi = {10.1177/0019793919875707},
	url = {http://dx.doi.org/10.1177/0019793919875707},
	langid = {en}
}

@Misc{white2020fundamental,
author = {Matha White, Adam White},
organization = {University of Alberta},
title = {Fundmnentrals on Reinforcement Learning},
howpublished = {\textsc{url:}~\url{https://www.coursera.org/learn/fundamentals-of-reinforcement-learning/}},
keywords = {},
lastchecked = {2024-05-05},
year = {2020}
}


@book{wickham2009ggplot2,
  title = {ggplot2: Elegant Graphics for Data Analysis},
  author = {Wickham, Hadley},
  year = 2009,
  publisher = {Springer},
  series = {UseR!},
  doi = {10.1007/978-0-387-98141-3},
  isbn = {978-0-387-98140-6},
  url = {http://ggplot2.org/book/},
  abstract = {Teaches how to create graphics in R using ggplot Discusses the theoretical framework that underlies ggplot. This book describes ggplot2, a new data visualization package for R that uses the insights from Leland Wilkison's Grammar of Graphics to create a powerful and flexible system for creating data graphics. With ggplot2, it's easy to: produce handsome, publication-quality plots, with automatic legends created from the plot specification superpose multiple layers (points, lines, maps, tiles, box plots to name a few) from different data sources, with automatically adjusted common scales add customisable smoothers that use the powerful modelling capabilities of R, such as loess, linear models, generalised additive models and robust regression save any ggplot2 plot (or part thereof) for later modification or reuse create custom themes that capture in-house or journal style requirements, and that can easily be applied to multiple plots approach your graph from a visual perspective, thinking about how each component of the data is represented on the final plot This book will be useful to everyone who has struggled with displaying their data in an informative and attractive way. You will need some basic knowledge of R (i.e. you should be able to get your data into R), but ggplot2 is a mini-language specifically tailored for producing graphics, and you'll learn everything you need in the book. After reading this book you'll be able to produce graphics customized precisely for your problems, and you'll find it easy to get graphics out of your head and on to the screen or page. Hadley Wickham is an Assistant Professor of Statistics at Rice University, and is interested in developing computational and cognitive tools for making data preparation, visualization, and analysis easier. He has developed 15 R packages and in 2006 he won the John Chambers Award for Statistical Computing for his work on the ggplot and reshape R packages.},
  added-at = {2018-06-18T21:23:34.000+0200},
  biburl = {https://www.bibsonomy.org/bibtex/28e95c4e5f32a9fe74e4ce23ca7ca9fd6/pbett},
  citeulike-article-id = 5445806,
  citeulike-attachment-1 = {HadleyWickham2009_ggplot2_book.pdf; /pdf/user/pbett/article/5445806/958182/HadleyWickham2009_ggplot2_book.pdf; d5e08302e67648b3cdbaf619362aa5ff29eb5018},
  citeulike-linkout-0 = {http://ggplot2.org/book/},
  citeulike-linkout-1 = {http://dx.doi.org/10.1007/978-0-387-98141-3},
  citeulike-linkout-2 = {http://www.worldcat.org/isbn/9780387981406},
  citeulike-linkout-3 = {http://books.google.com/books?vid=ISBN9780387981406},
  citeulike-linkout-4 = {http://www.amazon.com/gp/search?keywords=9780387981406&index=books&linkCode=qs},
  citeulike-linkout-5 = {http://www.librarything.com/isbn/9780387981406},
  citeulike-linkout-6 = {http://www.worldcat.org/oclc/416289643},
  comment = {(private-note)Freely available from Springer web site.},
  file = {HadleyWickham2009_ggplot2_book.pdf},
  interhash = {207c42e56f05318e953a3fcaec50ea15},
  intrahash = {8e95c4e5f32a9fe74e4ce23ca7ca9fd6},
  keywords = {visualisation textbook rpackage},
  posted-at = {2014-04-02 19:42:54},
  priority = 2,
  timestamp = {2018-06-22T18:34:20.000+0200},
}
@book{wickham2009ggplot2,
  title = {ggplot2: Elegant Graphics for Data Analysis},
  author = {Wickham, Hadley},
  year = 2009,
  publisher = {Springer},
  series = {UseR!},
  doi = {10.1007/978-0-387-98141-3},
  isbn = {978-0-387-98140-6},
  url = {http://ggplot2.org/book/},
  abstract = {Teaches how to create graphics in R using ggplot Discusses the theoretical framework that underlies ggplot. This book describes ggplot2, a new data visualization package for R that uses the insights from Leland Wilkison's Grammar of Graphics to create a powerful and flexible system for creating data graphics. With ggplot2, it's easy to: produce handsome, publication-quality plots, with automatic legends created from the plot specification superpose multiple layers (points, lines, maps, tiles, box plots to name a few) from different data sources, with automatically adjusted common scales add customisable smoothers that use the powerful modelling capabilities of R, such as loess, linear models, generalised additive models and robust regression save any ggplot2 plot (or part thereof) for later modification or reuse create custom themes that capture in-house or journal style requirements, and that can easily be applied to multiple plots approach your graph from a visual perspective, thinking about how each component of the data is represented on the final plot This book will be useful to everyone who has struggled with displaying their data in an informative and attractive way. You will need some basic knowledge of R (i.e. you should be able to get your data into R), but ggplot2 is a mini-language specifically tailored for producing graphics, and you'll learn everything you need in the book. After reading this book you'll be able to produce graphics customized precisely for your problems, and you'll find it easy to get graphics out of your head and on to the screen or page. Hadley Wickham is an Assistant Professor of Statistics at Rice University, and is interested in developing computational and cognitive tools for making data preparation, visualization, and analysis easier. He has developed 15 R packages and in 2006 he won the John Chambers Award for Statistical Computing for his work on the ggplot and reshape R packages.},
  added-at = {2018-06-18T21:23:34.000+0200},
  biburl = {https://www.bibsonomy.org/bibtex/28e95c4e5f32a9fe74e4ce23ca7ca9fd6/pbett},
  citeulike-article-id = 5445806,
  citeulike-attachment-1 = {HadleyWickham2009_ggplot2_book.pdf; /pdf/user/pbett/article/5445806/958182/HadleyWickham2009_ggplot2_book.pdf; d5e08302e67648b3cdbaf619362aa5ff29eb5018},
  citeulike-linkout-0 = {http://ggplot2.org/book/},
  citeulike-linkout-1 = {http://dx.doi.org/10.1007/978-0-387-98141-3},
  citeulike-linkout-2 = {http://www.worldcat.org/isbn/9780387981406},
  citeulike-linkout-3 = {http://books.google.com/books?vid=ISBN9780387981406},
  citeulike-linkout-4 = {http://www.amazon.com/gp/search?keywords=9780387981406&index=books&linkCode=qs},
  citeulike-linkout-5 = {http://www.librarything.com/isbn/9780387981406},
  citeulike-linkout-6 = {http://www.worldcat.org/oclc/416289643},
  comment = {(private-note)Freely available from Springer web site.},
  file = {HadleyWickham2009_ggplot2_book.pdf},
  interhash = {207c42e56f05318e953a3fcaec50ea15},
  intrahash = {8e95c4e5f32a9fe74e4ce23ca7ca9fd6},
  keywords = {visualisation textbook rpackage},
  posted-at = {2014-04-02 19:42:54},
  priority = 2,
  timestamp = {2018-06-22T18:34:20.000+0200},
}
@misc{wikipedia2021delphi,
	title = {Delphi method},
	copyright = {CC-SA},
	url = {https://en.wikipedia.org/wiki/Delphi_method},
	abstract = {The Delphi method or Delphi technique ( DEL-fy; also known as Estimate-Talk-Estimate or ETE) is a structured communication technique or method, originally developed as a systematic, interactive forecasting method which relies on a panel of experts. The technique can also be adapted for use in face-to-face meetings, and is then called mini-Delphi or Estimate-Talk-Estimate (ETE). Delphi has been widely used for business forecasting and has certain advantages over another structured forecasting approach, prediction markets.Delphi is based on the principle that forecasts (or decisions) from a structured group of individuals are more accurate than those from unstructured groups. The experts answer questionnaires in two or more rounds. After each round, a facilitator or change agent provides an anonymised summary of the experts' forecasts from the previous round as well as the reasons they provided for their judgments. Thus, experts are encouraged to revise their earlier answers in light of the replies of other members of their panel. It is believed that during this process the range of the answers will decrease and the group will converge towards the "correct" answer. Finally, the process is stopped after a predefined stop criterion (e.g., number of rounds, achievement of consensus, stability of results), and the mean or median scores of the final rounds determine the results.Special attention has to be paid to the formulation of the Delphi theses and the definition and selection of the experts in order to avoid methodological weaknesses that severely threaten the validity and reliability of the results.},
	language = {en},
	urldate = {2021-09-19},
	journal = {Wikipedia},
	month = sep,
	year = {2021},
	note = {Page Version ID: 1045048927},
}

% delphi method
@article{williams1987,
	title = {Generalized Linear Model Diagnostics Using the Deviance and Single Case Deletions},
	author = {Williams, D. A.},
	year = {1987},
	date = {1987},
	journal = {Applied Statistics},
	pages = {181},
	volume = {36},
	number = {2},
	doi = {10.2307/2347550},
	url = {http://dx.doi.org/10.2307/2347550}
}

@book{winston1992artificial,
  title = {Artificial Intelligence},
  author = {Winston, Patrick Henry},
  year = 1992,
  publisher = {Addison-Wesley},
  address = {Reading, MA},
  isbn = {978-0-201-53377-4},
  abstract = {This book is one of the oldest and most popular introductions to artificial intelligence. An accomplished artificial intelligence (AI) scientist, Winston heads MIT's Artificial Intelligence Laboratory, and his hands-on AI research experience lends authority to what he writes. Winston provides detailed pseudo-code for most of the algorithms discussed, so you will be able to implement and test the algorithms immediately. The book contains exercises to test your knowledge of the subject and helpful introductions and summaries to guide you through the material.},
  added-at = {2016-09-23T14:14:26.000+0200},
  biburl = {https://www.bibsonomy.org/bibtex/270046e5ce145e71ce16c0f29810cc5d1/flint63},
  description = {Edition 2 1984 978-0-201-08259-3},
  edition = 3,
  file = {Amazon Search inside:http\://www.amazon.de/gp/reader/0201533774/:URL;Google Books:http\://books.google.de/books?isbn=9780201533774:URL},
  groups = {public},
  interhash = {b210c9ab9e96b0116c7e27ce4e7959f3},
  intrahash = {70046e5ce145e71ce16c0f29810cc5d1},
  keywords = {01801 101 book shelf ai general},
  timestamp = {2018-04-16T11:33:09.000+0200},
  username = {flint63},
}
@book{winston1992artificial,
  title = {Artificial Intelligence},
  author = {Winston, Patrick Henry},
  year = 1992,
  publisher = {Addison-Wesley},
  address = {Reading, MA},
  isbn = {978-0-201-53377-4},
  abstract = {This book is one of the oldest and most popular introductions to artificial intelligence. An accomplished artificial intelligence (AI) scientist, Winston heads MIT's Artificial Intelligence Laboratory, and his hands-on AI research experience lends authority to what he writes. Winston provides detailed pseudo-code for most of the algorithms discussed, so you will be able to implement and test the algorithms immediately. The book contains exercises to test your knowledge of the subject and helpful introductions and summaries to guide you through the material.},
  added-at = {2016-09-23T14:14:26.000+0200},
  biburl = {https://www.bibsonomy.org/bibtex/270046e5ce145e71ce16c0f29810cc5d1/flint63},
  description = {Edition 2 1984 978-0-201-08259-3},
  edition = 3,
  file = {Amazon Search inside:http\://www.amazon.de/gp/reader/0201533774/:URL;Google Books:http\://books.google.de/books?isbn=9780201533774:URL},
  groups = {public},
  interhash = {b210c9ab9e96b0116c7e27ce4e7959f3},
  intrahash = {70046e5ce145e71ce16c0f29810cc5d1},
  keywords = {01801 101 book shelf ai general},
  timestamp = {2018-04-16T11:33:09.000+0200},
  username = {flint63},
}
@book{wolfram2018new,
  title={A New Kind of Science},
  author={Wolfram, S.},
  isbn={9781579550257},
  year={2018},
  publisher={WOLFRAM MEDIA Incorporated}
}

@misc{yu2022cfx,
      title={Towards Counterfactual Image Manipulation via CLIP}, 
      author={Yingchen Yu and Fangneng Zhan and Rongliang Wu and Jiahui Zhang and Shijian Lu and Miaomiao Cui and Xuansong Xie and Xian-Sheng Hua and Chunyan Miao},
      year={2022},
      eprint={2207.02812},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{Pajzs2002ACB,
  title={A Corpus Based Investigation of Collocations in Hungarian},
  author={J{\'u}lia Pajzs},
  year={2002},
  url={https://api.semanticscholar.org/CorpusID:54042328}
}

@Article{Borkar2010RiskconstrainedMD,
 author = {V. Borkar and R. Jain},
 booktitle = {IEEE Conference on Decision and Control},
 journal = {49th IEEE Conference on Decision and Control (CDC)},
 pages = {2664-2669},
 title = {Risk-constrained Markov decision processes},
 year = {2010}
}

@Article{Hessel2018MultitaskDR,
 author = {Matteo Hessel and Hubert Soyer and L. Espeholt and Wojciech M. Czarnecki and Simon Schmitt and H. V. Hasselt},
 booktitle = {AAAI Conference on Artificial Intelligence},
 journal = {ArXiv},
 title = {Multi-task Deep Reinforcement Learning with PopArt},
 volume = {abs/1809.04474},
 year = {2018}
}



@Article{Team2021OpenEndedLL,
 author = {Open-Ended Learning Team and Adam Stooke and Anuj Mahajan and C. Barros and Charlie Deck and Jakob Bauer and Jakub Sygnowski and Maja Trebacz and Max Jaderberg and Michaël Mathieu and Nathan McAleese and N. Bradley-Schmieg and Nathaniel Wong and Nicolas Porcel and Roberta Raileanu and Steph Hughes-Fitt and Valentin Dalibard and Wojciech M. Czarnecki},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {Open-Ended Learning Leads to Generally Capable Agents},
 volume = {abs/2107.12808},
 year = {2021}
}


@inproceedings{nikulin2023xlandminigrid,
    title={{XL}and-MiniGrid: Scalable Meta-Reinforcement Learning Environments in {JAX}},
    author={Alexander Nikulin and Vladislav Kurenkov and Ilya Zisman and Viacheslav Sinii and Artem Agarkov and Sergey Kolesnikov},
    booktitle={Intrinsically-Motivated and Open-Ended Learning Workshop, NeurIPS2023},
    year={2023},
    url={https://openreview.net/forum?id=xALDC4aHGz}
}

@Article{Bak1988SelforganizedC,
 author = {P. Bak and Chao Tang and K. Wiesenfeld},
 booktitle = {Physical review. A, General physics},
 journal = {Physical review. A, General physics},
 pages = {
          364-374
        },
 title = {Self-organized criticality.},
 volume = {38 1},
 year = {1988}
}


@Article{Bak1997HowNW,
 author = {P. Bak},
 journal = {American Journal of Physics},
 pages = {579-580},
 title = {How Nature Works: The Science of Self-Organized Criticality},
 volume = {65},
 year = {1997}
}


@inproceedings{impala2018,
  title={IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures},
  author={Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Volodymir and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and others},
  booktitle={Proceedings of the International Conference on Machine Learning (ICML)},
  year={2018}
  
}@Article{Mnih2015HumanlevelCT,
 author = {Volodymyr Mnih and K. Kavukcuoglu and David Silver and Andrei A. Rusu and J. Veness and Marc G. Bellemare and Alex Graves and Martin A. Riedmiller and A. Fidjeland and Georg Ostrovski and Stig Petersen and Charlie Beattie and Amir Sadik and Ioannis Antonoglou and Helen King and D. Kumaran and Daan Wierstra and S. Legg and D. Hassabis},
 booktitle = {Nature},
 journal = {Nature},
 pages = {529-533},
 title = {Human-level control through deep reinforcement learning},
 volume = {518},
 year = {2015}
}


@Article{Hasselt2015DeepRL,
 author = {H. V. Hasselt and A. Guez and David Silver},
 booktitle = {AAAI Conference on Artificial Intelligence},
 pages = {2094-2100},
 title = {Deep Reinforcement Learning with Double Q-Learning},
 year = {2015}
}


@Article{Wang2015DuelingNA,
 author = {Ziyun Wang and T. Schaul and Matteo Hessel and H. V. Hasselt and Marc Lanctot and Nando de Freitas},
 booktitle = {International Conference on Machine Learning},
 pages = {1995-2003},
 title = {Dueling Network Architectures for Deep Reinforcement Learning},
 year = {2015}
}


@Article{Lillicrap2015ContinuousCW,
 author = {T. Lillicrap and Jonathan J. Hunt and A. Pritzel and N. Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
 booktitle = {International Conference on Learning Representations},
 journal = {CoRR},
 title = {Continuous control with deep reinforcement learning},
 volume = {abs/1509.02971},
 year = {2015}
}


@Article{Fujimoto2018AddressingFA,
 author = {Scott Fujimoto and H. V. Hoof and D. Meger},
 booktitle = {International Conference on Machine Learning},
 pages = {1582-1591},
 title = {Addressing Function Approximation Error in Actor-Critic Methods},
 year = {2018}
}


@Article{Haarnoja2018SoftAO,
 author = {Tuomas Haarnoja and Aurick Zhou and P. Abbeel and S. Levine},
 booktitle = {International Conference on Machine Learning},
 journal = {ArXiv},
 title = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
 volume = {abs/1801.01290},
 year = {2018}
}


@article{dennis2020emergent,
  title={Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design},
  author={Dennis, Michael and Jaques, Natasha and Vinitsky, Eugene and Bayen, Alexandre and Russell, Stuart and Critch, Andrew and Levine, Sergey},
  journal={Neural Information Processing Systems (NeurIPS)},
  year={2020}
}@Article{Schulman2017ProximalPO,
 author = {John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {Proximal Policy Optimization Algorithms},
 volume = {abs/1707.06347},
 year = {2017}
}


@Article{Schulman2015TrustRP,
 author = {John Schulman and S. Levine and P. Abbeel and Michael I. Jordan and Philipp Moritz},
 booktitle = {International Conference on Machine Learning},
 journal = {ArXiv},
 title = {Trust Region Policy Optimization},
 volume = {abs/1502.05477},
 year = {2015}
}


@Article{Mnih2016AsynchronousMF,
 author = {Volodymyr Mnih and Adrià Puigdomènech Badia and Mehdi Mirza and Alex Graves and T. Lillicrap and Tim Harley and David Silver and K. Kavukcuoglu},
 booktitle = {International Conference on Machine Learning},
 pages = {1928-1937},
 title = {Asynchronous Methods for Deep Reinforcement Learning},
 year = {2016}
}


@Article{Feinberg2018ModelBasedVE,
 author = {Vladimir Feinberg and Alvin Wan and I. Stoica and Michael I. Jordan and Joseph E. Gonzalez and S. Levine},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {Model-Based Value Estimation for Efficient Model-Free Reinforcement Learning},
 volume = {abs/1803.00101},
 year = {2018}
}


@Article{Clavera2018ModelBasedRL,
 author = {I. Clavera and Jonas Rothfuss and John Schulman and Yasuhiro Fujita and T. Asfour and P. Abbeel},
 booktitle = {Conference on Robot Learning},
 pages = {617-629},
 title = {Model-Based Reinforcement Learning via Meta-Policy Optimization},
 year = {2018}
}


@Article{Silver2016MasteringTG,
 author = {David Silver and Aja Huang and Chris J. Maddison and A. Guez and L. Sifre and George van den Driessche and Julian Schrittwieser and Ioannis Antonoglou and Vedavyas Panneershelvam and Marc Lanctot and S. Dieleman and Dominik Grewe and John Nham and Nal Kalchbrenner and I. Sutskever and T. Lillicrap and M. Leach and K. Kavukcuoglu and T. Graepel and D. Hassabis},
 booktitle = {Nature},
 journal = {Nature},
 pages = {484-489},
 title = {Mastering the game of Go with deep neural networks and tree search},
 volume = {529},
 year = {2016}
}


@Article{Silver2017MasteringTG,
 author = {David Silver and Julian Schrittwieser and K. Simonyan and Ioannis Antonoglou and Aja Huang and A. Guez and T. Hubert and Lucas baker and Matthew Lai and Adrian Bolton and Yutian Chen and T. Lillicrap and Fan Hui and L. Sifre and George van den Driessche and T. Graepel and D. Hassabis},
 booktitle = {Nature},
 journal = {Nature},
 pages = {354-359},
 title = {Mastering the game of Go without human knowledge},
 volume = {550},
 year = {2017}
}


@Article{Silver2018AGR,
 author = {David Silver and T. Hubert and Julian Schrittwieser and Ioannis Antonoglou and Matthew Lai and A. Guez and Marc Lanctot and L. Sifre and D. Kumaran and T. Graepel and T. Lillicrap and K. Simonyan and D. Hassabis},
 booktitle = {Science},
 journal = {Science},
 pages = {1140 - 1144},
 title = {A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
 volume = {362},
 year = {2018}
}

@ARTICLE{1055186,
  author={Bahl, L. and Cocke, J. and Jelinek, F. and Raviv, J.},
  journal={IEEE Transactions on Information Theory}, 
  title={Optimal decoding of linear codes for minimizing symbol error rate (Corresp.)}, 
  year={1974},
  volume={20},
  number={2},
  pages={284-287},
  keywords={},
  doi={10.1109/TIT.1974.1055186}}

@Article{Agostinelli2019SolvingTR,
 author = {Forest Agostinelli and S. McAleer and A. Shmakov and P. Baldi},
 booktitle = {Nature Machine Intelligence},
 journal = {Nature Machine Intelligence},
 pages = {356 - 363},
 title = {Solving the Rubik’s cube with deep reinforcement learning and search},
 volume = {1},
 year = {2019}
}


@Article{Bellemare2012TheAL,
 author = {Marc G. Bellemare and Yavar Naddaf and J. Veness and Michael Bowling},
 booktitle = {Journal of Artificial Intelligence Research},
 journal = {ArXiv},
 title = {The Arcade Learning Environment: An Evaluation Platform for General Agents},
 volume = {abs/1207.4708},
 year = {2012}
}


@Article{Todorov2012MuJoCoAP,
 author = {E. Todorov and Tom Erez and Yuval Tassa},
 booktitle = {2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
 journal = {2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
 pages = {5026-5033},
 title = {MuJoCo: A physics engine for model-based control},
 year = {2012}
}


@Inproceedings{Shen2018MWalkLT,
 author = {Yelong Shen and Jianshu Chen and Po-Sen Huang and Yuqing Guo and Jianfeng Gao},
 booktitle = {Neural Information Processing Systems},
 title = {M-Walk: Learning to Walk in Graph with Monte Carlo Tree Search},
 year = {2018}
}


@Article{Ellis2019WriteEA,
 author = {Kevin Ellis and Maxwell Nye and Yewen Pu and Felix Sosa and J. Tenenbaum and Armando Solar-Lezama},
 booktitle = {Neural Information Processing Systems},
 pages = {9165-9174},
 title = {Write, Execute, Assess: Program Synthesis with a REPL},
 year = {2019}
}


@Article{Russo2017ATO,
 author = {Daniel Russo and Benjamin Van Roy and Abbas Kazerouni and Ian Osband},
 booktitle = {Found. Trends Mach. Learn.},
 journal = {ArXiv},
 title = {A Tutorial on Thompson Sampling},
 volume = {abs/1707.02038},
 year = {2017}
}


@Article{Nowlan1992SimplifyingNN,
 author = {S. Nowlan and Geoffrey E. Hinton},
 booktitle = {Neural Computation},
 journal = {Neural Computation},
 pages = {473-493},
 title = {Simplifying Neural Networks by Soft Weight-Sharing},
 volume = {4},
 year = {1992},
 url = {https://www.cs.utoronto.ca/~hinton/absps/sunspots.pdf}
}


@Article{Smith2005TheDO,
 author = {Linda B. Smith and M. Gasser},
 booktitle = {Artificial Life},
 journal = {Artificial Life},
 pages = {13-29},
 title = {The Development of Embodied Cognition: Six Lessons from Babies},
 volume = {11},
 year = {2005}
}


@Article{Tassa2012SynthesisAS,
 author = {Yuval Tassa and Tom Erez and E. Todorov},
 booktitle = {2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
 journal = {2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
 pages = {4906-4913},
 title = {Synthesis and stabilization of complex behaviors through online trajectory optimization},
 year = {2012}
}

  @misc{enwiki:MPC,
    author = "{Wikipedia contributors}",
    title = "Model predictive control --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2024",
    howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Model_predictive_control&oldid=1223992680}",
    note = "[Online; accessed 25-June-2024]"
  }
@Article{Dosovitskiy2016LearningTA,
 author = {Alexey Dosovitskiy and V. Koltun},
 booktitle = {International Conference on Learning Representations},
 journal = {ArXiv},
 title = {Learning to Act by Predicting the Future},
 volume = {abs/1611.01779},
 year = {2016}
}

@book{precup2000temporal,
  title={Temporal abstraction in reinforcement learning},
  author={Precup, Doina},
  year={2000},
  publisher={University of Massachusetts Amherst}
}
@Article{Sutton1999BetweenMA,
 author = {R. Sutton and Doina Precup and Satinder Singh},
 booktitle = {Artificial Intelligence},
 journal = {Artif. Intell.},
 pages = {181-211},
 title = {Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning},
 volume = {112},
 year = {1999}
}


@Article{Bacon2016TheOA,
 author = {Pierre-Luc Bacon and J. Harb and Doina Precup},
 booktitle = {AAAI Conference on Artificial Intelligence},
 journal = {ArXiv},
 title = {The Option-Critic Architecture},
 volume = {abs/1609.05140},
 year = {2016}
}


@article{bradtke1994reinforcement,
  title={Reinforcement learning methods for continuous-time Markov decision problems},
  author={Bradtke, Steven and Duff, Michael},
  journal={Advances in neural information processing systems},
  volume={7},
  year={1994}
}


@misc{sean2021online,
author  = {Law, Sean M.},
title = {Modern Time Series Analysis with STUMPY},
howpublished = {\url{https://youtu.be/XKNdXN-Jfmo}},
month = {aug},
year = {2021},
note = {(Accessed on 08/08/2024)}
}

@INPROCEEDINGS{Yeh2016MatrixProfileI,
  author={Yeh, Chin-Chia Michael and Zhu, Yan and Ulanova, Liudmila and Begum, Nurjahan and Ding, Yifei and Dau, Hoang Anh and Silva, Diego Furtado and Mueen, Abdullah and Keogh, Eamonn},
  booktitle={2016 IEEE 16th International Conference on Data Mining (ICDM)}, 
  title={Matrix Profile I: All Pairs Similarity Joins for Time Series: A Unifying View That Includes Motifs, Discords and Shapelets}, 
  year={2016},
  volume={},
  number={},
  pages={1317-1322},
  keywords={Time series analysis;Approximation algorithms;Euclidean distance;Data mining;Indexes;Clustering algorithms;Text processing;Time Series;Similarity Joins;Motif Discovery},
  doi={10.1109/ICDM.2016.0179}}

@article{law2019stumpy,
  author  = {Law, Sean M.},
  title   = {{STUMPY: A Powerful and Scalable Python Library for Time Series Data Mining}},
  journal = {{The Journal of Open Source Software}},
  volume  = {4},
  number  = {39},
  pages   = {1504},
  year    = {2019}
}


# Reward Hypothesis
@article{SILVER2021103535,
title = {Reward is enough},
journal = {Artificial Intelligence},
volume = {299},
pages = {103535},
year = {2021},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2021.103535},
url = {https://www.sciencedirect.com/science/article/pii/S0004370221000862},
author = {David Silver and Satinder Singh and Doina Precup and Richard S. Sutton},
keywords = {Artificial intelligence, Artificial general intelligence, Reinforcement learning, Reward},
abstract = {In this article we hypothesise that intelligence, and its associated abilities, can be understood as subserving the maximisation of reward. Accordingly, reward is enough to drive behaviour that exhibits abilities studied in natural and artificial intelligence, including knowledge, learning, perception, social intelligence, language, generalisation and imitation. This is in contrast to the view that specialised problem formulations are needed for each ability, based on other signals or objectives. Furthermore, we suggest that agents that learn through trial and error experience to maximise reward could learn behaviour that exhibits most if not all of these abilities, and therefore that powerful reinforcement learning agents could constitute a solution to artificial general intelligence.}
}


# SARA algorithm
@inproceedings{Rummery1994OnlineQU,
  title={On-line Q-learning using connectionist systems},
  author={Gavin Adrian Rummery and Mahesan Niranjan},
  year={1994},
  url={https://api.semanticscholar.org/CorpusID:59872172}
}

# SARSA convergence
@article{singh2000convergence,
  title={Convergence results for single-step on-policy reinforcement-learning algorithms},
  author={Singh, Satinder and Jaakkola, Tommi and Littman, Michael L and Szepesv{\'a}ri, Csaba},
  journal={Machine learning},
  volume={38},
  pages={287--308},
  year={2000},
  publisher={Springer}
}

# SARSA convergence
@inproceedings{zhang2023convergence,
  title={On the convergence of SARSA with linear function approximation},
  author={Zhang, Shangtong and Des Combes, Remi Tachet and Laroche, Romain},
  booktitle={International Conference on Machine Learning},
  pages={41613--41646},
  year={2023},
  organization={PMLR}
}

@misc{patterson2024generalizedprojectedbellmanerror,
      title={A Generalized Projected Bellman Error for Off-policy Value Estimation in Reinforcement Learning}, 
      author={Andrew Patterson and Adam White and Martha White},
      year={2024},
      eprint={2104.13844},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2104.13844}, 
}

@book{pennebaker2013secret,
  title={The Secret Life of Pronouns: What Our Words Say About Us},
  author={Pennebaker, J.W.},
  isbn={9781608194964},
  lccn={2011001289},
  url={https://books.google.co.il/books?id=p9KmCAAAQBAJ},
  year={2013},
  publisher={Bloomsbury USA}
}

@Book{jm3,
  author =       "Daniel Jurafsky and James H. Martin",
  title =        "Speech and Language Processing: An Introduction to
                 Natural Language Processing, Computational Linguistics,
                 and Speech Recognition with Language Models",
  year =         "2025",
  url = {https://web.stanford.edu/~jurafsky/slp3/},
  note = "Online manuscript released January 12, 2025",
  edition =         "3rd",
  }

@ARTICLE{Hochreiter1997LSTM,
  author={Hochreiter, Sepp and Schmidhuber, Jürgen},
  journal={Neural Computation}, 
  title={Long Short-Term Memory}, 
  year={1997},
  volume={9},
  number={8},
  pages={1735-1780},
  keywords={},
  doi={10.1162/neco.1997.9.8.1735}}

  @article{Gers2000PeepHole,
  title={Recurrent nets that time and count},
  author={Felix Alexander Gers and J{\"u}rgen Schmidhuber},
  journal={Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium},
  year={2000},
  volume={3},
  pages={189-194 vol.3},
  url={https://api.semanticscholar.org/CorpusID:36867983}
}

@article{beck2024xlstm,
  title={xLSTM: Extended Long Short-Term Memory},
  author={Beck, Maximilian and P{\"o}ppel, Korbinian and Spanring, Markus and Auer, Andreas and Prudnikova, Oleksandra and Kopp, Michael and Klambauer, G{\"u}nter and Brandstetter, Johannes and Hochreiter, Sepp},
  journal={arXiv preprint arXiv:2405.04517},
  year={2024}
}


@book{mcluhan1988understanding,
  address = {New York},
  author = {McLuhan, Marshall},
  isbn = {0451624963 9780451624963},
  keywords = {library media},
  publisher = {New American Library},
  refid = {18998166},
  timestamp = {2015-12-09T19:54:14.000+0100},
  title = {Understanding media : the extensions of man},
  url = {http://www.worldcat.org/search?qt=worldcat_org_all&q=0451624963},
  year = 1988
}

@misc{chen2024computationallimitsstatespacemodels,
      title={The Computational Limits of State-Space Models and Mamba via the Lens of Circuit Complexity}, 
      author={Yifang Chen and Xiaoyu Li and Yingyu Liang and Zhenmei Shi and Zhao Song},
      year={2024},
      eprint={2412.06148},
      archivePrefix={arXiv},
      primaryClass={cs.CC},
      url={https://arxiv.org/abs/2412.06148}, 
}

# iductive biases
@article{battaglia2018relational,
  title={Relational inductive biases, deep learning, and graph networks},
  author={Battaglia, Peter W and Hamrick, Jessica B and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and others},
  journal={arXiv preprint arXiv:1806.01261},
  year={2018}
}

@article{NEY19941,
title = {On structuring probabilistic dependences in stochastic language modelling},
journal = {Computer Speech & Language},
volume = {8},
number = {1},
pages = {1-38},
year = {1994},
issn = {0885-2308},
doi = {https://doi.org/10.1006/csla.1994.1001},
url = {https://www.sciencedirect.com/science/article/pii/S0885230884710011},
author = {Hermann Ney and Ute Essen and Reinhard Kneser},
abstract = {In this paper, we study the problem of stochastic language modelling from the viewpoint of introducing suitable structures into the conditional probability distributions. The task of these distributions is to predict the probability of a new word by looking at M or even all predecessor words. The conventional approach is to limit M to 1 or 2 and to interpolate the resulting bigram and trigram models with a unigram model in a linear fashion. However, there are many other structures that can be used to model the probabilistic dependences between the predecessor word and the word to be predicted. The structures considered in this paper are: nonlinear interpolation as an alternative to linear interpolation; equivalence classes for word histories and single words; cache memory and word associations. For the optimal estimation of nonlinear and linear interpolation parameters, the leaving-one-out method is systematically used. For the determination of word equivalence classes in a bigram model, an automatic clustering procedure has been adapted. To capture long-distance dependences, we consider various models for word-by-word dependences; the cache model may be viewed as a special type of self-association. Experimental results are presented for two text databases, a Germany database and an English database.}
}

@article{andoni2015practical,
  title={Practical and optimal LSH for angular distance},
  author={Andoni, Alexandr and Indyk, Piotr and Laarhoven, Thijs and Razenshteyn, Ilya and Schmidt, Ludwig},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{rush2015neural,
  title={A neural attention model for abstractive sentence summarization},
  author={Rush, AM},
  journal={arXiv preprint arXiv:1509.00685},
  year={2015}
}

@article{weng2018attention,
  title   = "Attention? Attention!",
  author  = "Weng, Lilian",
  journal = "lilianweng.github.io",
  year    = "2018",
  url     = "https://lilianweng.github.io/posts/2018-06-24-attention/"
}

# Transformers
@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

# C4 model
@misc{raffel2023exploringlimitstransferlearning,
      title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}, 
      author={Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
      year={2023},
      eprint={1910.10683},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1910.10683}, 
}

@inproceedings{NIPS2001_7b7a53e2,
 author = {Ng, Andrew and Jordan, Michael},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Dietterich and S. Becker and Z. Ghahramani},
 pages = {},
 publisher = {MIT Press},
 title = {On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/file/7b7a53e239400a13bd6be6c91c4f6c4e-Paper.pdf},
 volume = {14},
 year = {2001}
}

@article{cotterell2019morphological,
  title={Morphological word embeddings},
  author={Cotterell, Ryan and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:1907.02423},
  year={2019}
}

@inproceedings{kann-etal-2016-neural,
    title = "Neural Morphological Analysis: Encoding-Decoding Canonical Segments",
    author = {Kann, Katharina  and
      Cotterell, Ryan  and
      Sch{\"u}tze, Hinrich},
    editor = "Su, Jian  and
      Duh, Kevin  and
      Carreras, Xavier",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1097/",
    doi = "10.18653/v1/D16-1097",
    pages = "961--967"
}

# PCA and Dimension Reduction
@article{MAL-002,
url = {http://dx.doi.org/10.1561/2200000002},
year = {2010},
volume = {2},
journal = {Foundations and Trends in Machine Learning},
title = {Dimension Reduction: A Guided Tour},
doi = {10.1561/2200000002},
issn = {1935-8237},
number = {4},
pages = {275-365},
author = {Christopher J. C. Burges}
}

@misc{luong2015effectiveapproachesattentionbasedneural,
      title={Effective Approaches to Attention-based Neural Machine Translation}, 
      author={Minh-Thang Luong and Hieu Pham and Christopher D. Manning},
      year={2015},
      eprint={1508.04025},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1508.04025}, 
}

@article{see2017get,
  title={Get to the point: Summarization with pointer-generator networks},
  author={See, Abigail and Liu, Peter J and Manning, Christopher D},
  journal={arXiv preprint arXiv:1704.04368},
  year={2017}
}

@misc{bertsch2023itsmbrwaydown,
      title={It's MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk}, 
      author={Amanda Bertsch and Alex Xie and Graham Neubig and Matthew R. Gormley},
      year={2023},
      eprint={2310.01387},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.01387}, 
}

@inproceedings{jinnai-etal-2024-generating,
    title = "Generating Diverse and High-Quality Texts by Minimum {B}ayes Risk Decoding",
    author = "Jinnai, Yuu  and
      Honda, Ukyo  and
      Morimura, Tetsuro  and
      Zhang, Peinan",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.503/",
    doi = "10.18653/v1/2024.findings-acl.503",
    pages = "8494--8525",
    abstract = "One of the most important challenges in text generation systems is to produce outputs that are not only correct but also diverse.Recently, Minimum Bayes-Risk (MBR) decoding has gained prominence for generating sentences of the highest quality among the decoding algorithms. However, existing algorithms proposed to generate diverse outputs are predominantly based on beam search or random sampling, thus their output quality is capped by these underlying decoding algorithms. In this paper, we investigate an alternative approach {--} we develop diversity-promoting decoding algorithms by enforcing diversity objectives to MBR decoding.We propose two variants of MBR; (i) Diverse MBR (DMBR) that adds a diversity penalty to the decoding objective and (ii) $k$-medoids MBR (KMBR) that reformulates the decoding task as a clustering problem.We evaluate DMBR and KMBR on a variety of directed text generation tasks using encoder-decoder models and a language model with prompting. The experimental results show that the proposed method achieves a better trade-off than the diverse beam search and sampling algorithms overall."
}

LSTMs beat Transformers
@misc{liu2023exposingattentionglitchesflipflop,
      title={Exposing Attention Glitches with Flip-Flop Language Modeling}, 
      author={Bingbin Liu and Jordan T. Ash and Surbhi Goel and Akshay Krishnamurthy and Cyril Zhang},
      year={2023},
      eprint={2306.00946},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2306.00946}, 
}

@misc{bahdanau2016neuralmachinetranslationjointly,
      title={Neural Machine Translation by Jointly Learning to Align and Translate}, 
      author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
      year={2016},
      eprint={1409.0473},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1409.0473}, 
}

## hellucination
@misc{nan2021entitylevelfactualconsistencyabstractive,
      title={Entity-level Factual Consistency of Abstractive Text Summarization}, 
      author={Feng Nan and Ramesh Nallapati and Zhiguo Wang and Cicero Nogueira dos Santos and Henghui Zhu and Dejiao Zhang and Kathleen McKeown and Bing Xiang},
      year={2021},
      eprint={2102.09130},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2102.09130}, 
}

@inproceedings{qi-etal-2018-pre,
    title = "When and Why Are Pre-Trained Word Embeddings Useful for Neural Machine Translation?",
    author = "Qi, Ye  and
      Sachan, Devendra  and
      Felix, Matthieu  and
      Padmanabhan, Sarguna  and
      Neubig, Graham",
    editor = "Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-2084/",
    doi = "10.18653/v1/N18-2084",
    pages = "529--535",
    abstract = "The performance of Neural Machine Translation (NMT) systems often suffers in low-resource scenarios where sufficiently large-scale parallel corpora cannot be obtained. Pre-trained word embeddings have proven to be invaluable for improving performance in natural language analysis tasks, which often suffer from paucity of data. However, their utility for NMT has not been extensively explored. In this work, we perform five sets of experiments that analyze when we can expect pre-trained word embeddings to help in NMT tasks. We show that such embeddings can be surprisingly effective in some cases {--} providing gains of up to 20 BLEU points in the most favorable setting."
}

@article{mckeown1997floating,
  title={Floating constraints in lexical choice},
  author={McKeown, Kathleen and Elhadad, Michael and Robin, Jacques},
  year={1997}
}

@misc{kitaev2020reformerefficienttransformer,
      title={Reformer: The Efficient Transformer}, 
      author={Nikita Kitaev and Łukasz Kaiser and Anselm Levskaya},
      year={2020},
      eprint={2001.04451},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2001.04451}, 
}

@misc{kasai2021finetuningpretrainedtransformersrnns,
      title={Finetuning Pretrained Transformers into RNNs}, 
      author={Jungo Kasai and Hao Peng and Yizhe Zhang and Dani Yogatama and Gabriel Ilharco and Nikolaos Pappas and Yi Mao and Weizhu Chen and Noah A. Smith},
      year={2021},
      eprint={2103.13076},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2103.13076}, 
}

@inproceedings{mi-etal-2016-coverage,
    title = "Coverage Embedding Models for Neural Machine Translation",
    author = "Mi, Haitao  and
      Sankaran, Baskaran  and
      Wang, Zhiguo  and
      Ittycheriah, Abe",
    editor = "Su, Jian  and
      Duh, Kevin  and
      Carreras, Xavier",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1096/",
    doi = "10.18653/v1/D16-1096",
    pages = "955--960"
}

# LLM Bert
@misc{devlin2019bertpretrainingdeepbidirectional,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1810.04805}, 
}

@misc{allamanis2016convolutionalattentionnetworkextreme,
      title={A Convolutional Attention Network for Extreme Summarization of Source Code}, 
      author={Miltiadis Allamanis and Hao Peng and Charles Sutton},
      year={2016},
      eprint={1602.03001},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1602.03001}, 
}

@inproceedings{xia-etal-2019-generalized,
    title = "Generalized Data Augmentation for Low-Resource Translation",
    author = "Xia, Mengzhou  and
      Kong, Xiang  and
      Anastasopoulos, Antonios  and
      Neubig, Graham",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1579/",
    doi = "10.18653/v1/P19-1579",
    pages = "5786--5796",
    abstract = "Low-resource language pairs with a paucity of parallel data pose challenges for machine translation in terms of both adequacy and fluency. Data augmentation utilizing a large amount of monolingual data is regarded as an effective way to alleviate the problem. In this paper, we propose a general framework of data augmentation for low-resource machine translation not only using target-side monolingual data, but also by pivoting through a related high-resource language. Specifically, we experiment with a two-step pivoting method to convert high-resource data to the low-resource language, making best use of available resources to better approximate the true distribution of the low-resource language. First, we inject low-resource words into high-resource sentences through an induced bilingual dictionary. Second, we further edit the high-resource data injected with low-resource words using a modified unsupervised machine translation framework. Extensive experiments on four low-resource datasets show that under extreme low-resource settings, our data augmentation techniques improve translation quality by up to 1.5 to 8 BLEU points compared to supervised back-translation baselines."
}

@inproceedings{zoph-etal-2016-transfer,
    title = "Transfer Learning for Low-Resource Neural Machine Translation",
    author = "Zoph, Barret  and
      Yuret, Deniz  and
      May, Jonathan  and
      Knight, Kevin",
    editor = "Su, Jian  and
      Duh, Kevin  and
      Carreras, Xavier",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1163/",
    doi = "10.18653/v1/D16-1163",
    pages = "1568--1575"
}


@inproceedings{nguyen-chiang-2017-transfer,
    title = "Transfer Learning across Low-Resource, Related Languages for Neural Machine Translation",
    author = "Nguyen, Toan Q.  and
      Chiang, David",
    editor = "Kondrak, Greg  and
      Watanabe, Taro",
    booktitle = "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = nov,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://aclanthology.org/I17-2050/",
    pages = "296--301",
    abstract = "We present a simple method to improve neural translation of a low-resource language pair using parallel data from a related, also low-resource, language pair. The method is based on the transfer method of Zoph et al., but whereas their method ignores any source vocabulary overlap, ours exploits it. First, we split words using Byte Pair Encoding (BPE) to increase vocabulary overlap. Then, we train a model on the first language pair and transfer its parameters, including its source word embeddings, to another model and continue training on the second language pair. Our experiments show that transfer learning helps word-based translation only slightly, but when used on top of a much stronger BPE baseline, it yields larger improvements of up to 4.3 BLEU."
}

@inproceedings{sennrich-etal-2016-improving,
    title = "Improving Neural Machine Translation Models with Monolingual Data",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    editor = "Erk, Katrin  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1009/",
    doi = "10.18653/v1/P16-1009",
    pages = "86--96"
}

@inproceedings{edunov-etal-2018-understanding,
    title = "Understanding Back-Translation at Scale",
    author = "Edunov, Sergey  and
      Ott, Myle  and
      Auli, Michael  and
      Grangier, David",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1045/",
    doi = "10.18653/v1/D18-1045",
    pages = "489--500",
    abstract = "An effective method to improve neural machine translation with monolingual data is to augment the parallel training corpus with back-translations of target language sentences. This work broadens the understanding of back-translation and investigates a number of methods to generate synthetic source sentences. We find that in all but resource poor settings back-translations obtained via sampling or noised beam outputs are most effective. Our analysis shows that sampling or noisy synthetic data gives a much stronger training signal than data generated by beam or greedy search. We also compare how synthetic data compares to genuine bitext and study various domain effects. Finally, we scale to hundreds of millions of monolingual sentences and achieve a new state of the art of 35 BLEU on the WMT`14 English-German test set."
}

@misc{pham2021metabacktranslation,
      title={Meta Back-translation}, 
      author={Hieu Pham and Xinyi Wang and Yiming Yang and Graham Neubig},
      year={2021},
      eprint={2102.07847},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2102.07847}, 
}

@article{johnson-etal-2017-googles,
    title = "{G}oogle`s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation",
    author = "Johnson, Melvin  and
      Schuster, Mike  and
      Le, Quoc V.  and
      Krikun, Maxim  and
      Wu, Yonghui  and
      Chen, Zhifeng  and
      Thorat, Nikhil  and
      Vi{\'e}gas, Fernanda  and
      Wattenberg, Martin  and
      Corrado, Greg  and
      Hughes, Macduff  and
      Dean, Jeffrey",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "5",
    year = "2017",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q17-1024/",
    doi = "10.1162/tacl_a_00065",
    pages = "339--351",
    abstract = "We propose a simple solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no changes to the model architecture from a standard NMT system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT systems using a single model. On the WMT`14 benchmarks, a single multilingual model achieves comparable performance for English{\textrightarrow}French and surpasses state-of-theart results for English{\textrightarrow}German. Similarly, a single multilingual model surpasses state-of-the-art results for French{\textrightarrow}English and German{\textrightarrow}English on WMT`14 and WMT`15 benchmarks, respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. Our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and also show some interesting examples when mixing languages."
}

@inproceedings{neubig-hu-2018-rapid,
    title = "Rapid Adaptation of Neural Machine Translation to New Languages",
    author = "Neubig, Graham  and
      Hu, Junjie",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1103/",
    doi = "10.18653/v1/D18-1103",
    pages = "875--880",
    abstract = "This paper examines the problem of adapting neural machine translation systems to new, low-resourced languages (LRLs) as effectively and rapidly as possible. We propose methods based on starting with massively multilingual {\textquotedblleft}seed models{\textquotedblright}, which can be trained ahead-of-time, and then continuing training on data related to the LRL. We contrast a number of strategies, leading to a novel, simple, yet effective method of {\textquotedblleft}similar-language regularization{\textquotedblright}, where we jointly train on both a LRL of interest and a similar high-resourced language to prevent over-fitting to small LRL data. Experiments demonstrate that massively multilingual models, even without any explicit adaptation, are surprisingly effective, achieving BLEU scores of up to 15.5 with no data from the LRL, and that the proposed similar-language regularization method improves over other adaptation methods by 1.7 BLEU points average over 4 LRL settings."
}

@inproceedings{watanabe18_interspeech,
  title     = {ESPnet: End-to-End Speech Processing Toolkit},
  author    = {Shinji Watanabe and Takaaki Hori and Shigeki Karita and Tomoki Hayashi and Jiro Nishitoba and Yuya Unno and Nelson {Enrique Yalta Soplin} and Jahn Heymann and Matthew Wiesner and Nanxin Chen and Adithya Renduchintala and Tsubasa Ochiai},
  year      = {2018},
  booktitle = {Proc. Interspeech},
  pages     = {2207--2211},
  doi       = {10.21437/Interspeech.2018-1456},
  issn      = {2958-1796},
}

@inproceedings{gershuni2022restoring,
  title={Restoring Hebrew Diacritics Without a Dictionary},
  author={Gershuni, Elazar and Pinter, Yuval},
  booktitle={Findings of the Association for Computational Linguistics: NAACL 2022},
  pages={1010--1018},
  year={2022}
}

@inproceedings{currey-etal-2017-copied,
    title = "Copied Monolingual Data Improves Low-Resource Neural Machine Translation",
    author = "Currey, Anna  and
      Miceli Barone, Antonio Valerio  and
      Heafield, Kenneth",
    editor = "Bojar, Ond{\v{r}}ej  and
      Buck, Christian  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Kreutzer, Julia",
    booktitle = "Proceedings of the Second Conference on Machine Translation",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-4715/",
    doi = "10.18653/v1/W17-4715",
    pages = "148--156"
}

@inproceedings{fadaee-etal-2017-data,
    title = "Data Augmentation for Low-Resource Neural Machine Translation",
    author = "Fadaee, Marzieh  and
      Bisazza, Arianna  and
      Monz, Christof",
    editor = "Barzilay, Regina  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-2090/",
    doi = "10.18653/v1/P17-2090",
    pages = "567--573",
    abstract = "The quality of a Neural Machine Translation system depends substantially on the availability of sizable parallel corpora. For low-resource language pairs this is not the case, resulting in poor translation quality. Inspired by work in computer vision, we propose a novel data augmentation approach that targets low-frequency words by generating new sentence pairs containing rare words in new, synthetically created contexts. Experimental results on simulated low-resource settings show that our method improves translation quality by up to 2.9 BLEU points over the baseline and up to 3.2 BLEU over back-translation."
}

@misc{dou2021wordalignmentfinetuningembeddings,
      title={Word Alignment by Fine-tuning Embeddings on Parallel Corpora}, 
      author={Zi-Yi Dou and Graham Neubig},
      year={2021},
      eprint={2101.08231},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2101.08231}, 
}


@misc{lample2018unsupervisedmachinetranslationusing,
      title={Unsupervised Machine Translation Using Monolingual Corpora Only}, 
      author={Guillaume Lample and Alexis Conneau and Ludovic Denoyer and Marc'Aurelio Ranzato},
      year={2018},
      eprint={1711.00043},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1711.00043}, 
}

@misc{zhou2019handlingsyntacticdivergencelowresource,
      title={Handling Syntactic Divergence in Low-resource Machine Translation}, 
      author={Chunting Zhou and Xuezhe Ma and Junjie Hu and Graham Neubig},
      year={2019},
      eprint={1909.00040},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1909.00040}, 
}


@misc{jauhiainen2018automaticlanguageidentificationtexts,
      title={Automatic Language Identification in Texts: A Survey}, 
      author={Tommi Jauhiainen and Marco Lui and Marcos Zampieri and Timothy Baldwin and Krister Lindén},
      year={2018},
      eprint={1804.08186},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1804.08186}, 
}

@inproceedings{caswell-etal-2020-language,
    title = "Language {ID} in the Wild: Unexpected Challenges on the Path to a Thousand-Language Web Text Corpus",
    author = "Caswell, Isaac  and
      Breiner, Theresa  and
      van Esch, Daan  and
      Bapna, Ankur",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.579/",
    doi = "10.18653/v1/2020.coling-main.579",
    pages = "6588--6608",
    abstract = "Large text corpora are increasingly important for a wide variety of Natural Language Processing (NLP) tasks, and automatic language identification (LangID) is a core technology needed to collect such datasets in a multilingual context. LangID is largely treated as solved in the literature, with models reported that achieve over 90{\%} average F1 on as many as 1,366 languages. We train LangID models on up to 1,629 languages with comparable quality on held-out test sets, but find that human-judged LangID accuracy for web-crawl text corpora created using these models is only around 5{\%} for many lower-resource languages, suggesting a need for more robust evaluation. Further analysis revealed a variety of error modes, arising from domain mismatch, class imbalance, language similarity, and insufficiently expressive models. We propose two classes of techniques to mitigate these errors: wordlist-based tunable-precision filters (for which we release curated lists in about 500 languages) and transformer-based semi-supervised LangID models, which increase median dataset precision from 5.5{\%} to 71.2{\%}. These techniques enable us to create an initial data set covering 100K or more relatively clean sentences in each of 500+ languages, paving the way towards a 1,000-language web text corpus."
}

@inproceedings{schwenk-li-2018-corpus,
    title = "A Corpus for Multilingual Document Classification in Eight Languages",
    author = "Schwenk, Holger  and
      Li, Xian",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Hasida, Koiti  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Tokunaga, Takenobu",
    booktitle = "Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)",
    month = may,
    year = "2018",
    address = "Miyazaki, Japan",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L18-1560/"
}

@inproceedings{qi-etal-2022-enhancing,
    title = "Enhancing Cross-lingual Natural Language Inference by Prompt-learning from Cross-lingual Templates",
    author = "Qi, Kunxun  and
      Wan, Hai  and
      Du, Jianfeng  and
      Chen, Haolan",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.134/",
    doi = "10.18653/v1/2022.acl-long.134",
    pages = "1910--1923",
    abstract = "Cross-lingual natural language inference (XNLI) is a fundamental task in cross-lingual natural language understanding. Recently this task is commonly addressed by pre-trained cross-lingual language models. Existing methods usually enhance pre-trained language models with additional data, such as annotated parallel corpora. These additional data, however, are rare in practice, especially for low-resource languages. Inspired by recent promising results achieved by prompt-learning, this paper proposes a novel prompt-learning based framework for enhancing XNLI. It reformulates the XNLI problem to a masked language modeling problem by constructing cloze-style questions through cross-lingual templates. To enforce correspondence between different languages, the framework augments a new question for every question using a sampled template in another language and then introduces a consistency loss to make the answer probability distribution obtained from the new question as similar as possible with the corresponding distribution obtained from the original question. Experimental results on two benchmark datasets demonstrate that XNLI models enhanced by our proposed framework significantly outperform original ones under both the full-shot and few-shot cross-lingual transfer settings."
}

@inproceedings{yang-etal-2019-paws,
    title = "{PAWS}-{X}: A Cross-lingual Adversarial Dataset for Paraphrase Identification",
    author = "Yang, Yinfei  and
      Zhang, Yuan  and
      Tar, Chris  and
      Baldridge, Jason",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1382/",
    doi = "10.18653/v1/D19-1382",
    pages = "3687--3692",
    abstract = "Most existing work on adversarial data generation focuses on English. For example, PAWS (Paraphrase Adversaries from Word Scrambling) consists of challenging English paraphrase identification pairs from Wikipedia and Quora. We remedy this gap with PAWS-X, a new dataset of 23,659 human translated PAWS evaluation pairs in six typologically distinct languages: French, Spanish, German, Chinese, Japanese, and Korean. We provide baseline numbers for three models with different capacity to capture non-local context and sentence structure, and using different multilingual training and evaluation regimes. Multilingual BERT fine-tuned on PAWS English plus machine-translated data performs the best, with a range of 83.1-90.8 accuracy across the non-English languages and an average accuracy gain of 23{\%} over the next best model. PAWS-X shows the effectiveness of deep, multilingual pre-training while also leaving considerable headroom as a new challenge to drive multilingual research that better captures structure and contextual information."
}

@article{ponti-etal-2019-modeling,
    title = "Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing",
    author = "Ponti, Edoardo Maria  and
      O{'}Horan, Helen  and
      Berzak, Yevgeni  and
      Vuli{\'c}, Ivan  and
      Reichart, Roi  and
      Poibeau, Thierry  and
      Shutova, Ekaterina  and
      Korhonen, Anna",
    journal = "Computational Linguistics",
    volume = "45",
    number = "3",
    month = sep,
    year = "2019",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/J19-3005/",
    doi = "10.1162/coli_a_00357",
    pages = "559--601",
    abstract = "Linguistic typology aims to capture structural and semantic variation across the world`s languages. A large-scale typology could provide excellent guidance for multilingual Natural Language Processing (NLP), particularly for languages that suffer from the lack of human labeled resources. We present an extensive literature survey on the use of typological information in the development of NLP techniques. Our survey demonstrates that to date, the use of information in existing typological databases has resulted in consistent but modest improvements in system performance. We show that this is due to both intrinsic limitations of databases (in terms of coverage and feature granularity) and under-utilization of the typological features included in them. We advocate for a new approach that adapts the broad and discrete nature of typological categories to the contextual and continuous nature of machine learning algorithms used in contemporary NLP. In particular, we suggest that such an approach could be facilitated by recent developments in data-driven induction of typological knowledge."
}

@article{sang2003introduction,
  title={Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition},
  author={Sang, Erik F and De Meulder, Fien},
  journal={arXiv preprint cs/0306050},
  year={2003}
}

@inproceedings{pan2017cross,
  title={Cross-lingual name tagging and linking for 282 languages},
  author={Pan, Xiaoman and Zhang, Boliang and May, Jonathan and Nothman, Joel and Knight, Kevin and Ji, Heng},
  booktitle={Proceedings of the 55th annual meeting of the association for computational linguistics (volume 1: long papers)},
  pages={1946--1958},
  year={2017}
}


@inproceedings{hu2020xtreme,
  title={Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation},
  author={Hu, Junjie and Ruder, Sebastian and Siddhant, Aditya and Neubig, Graham and Firat, Orhan and Johnson, Melvin},
  booktitle={International Conference on Machine Learning},
  pages={4411--4421},
  year={2020},
  organization={PMLR}
}

@inproceedings{liang-etal-2020-xglue,
    title = "{XGLUE}: A New Benchmark Dataset for Cross-lingual Pre-training, Understanding and Generation",
    author = "Liang, Yaobo  and
      Duan, Nan  and
      Gong, Yeyun  and
      Wu, Ning  and
      Guo, Fenfei  and
      Qi, Weizhen  and
      Gong, Ming  and
      Shou, Linjun  and
      Jiang, Daxin  and
      Cao, Guihong  and
      Fan, Xiaodong  and
      Zhang, Ruofei  and
      Agrawal, Rahul  and
      Cui, Edward  and
      Wei, Sining  and
      Bharti, Taroon  and
      Qiao, Ying  and
      Chen, Jiun-Hung  and
      Wu, Winnie  and
      Liu, Shuguang  and
      Yang, Fan  and
      Campos, Daniel  and
      Majumder, Rangan  and
      Zhou, Ming",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.484/",
    doi = "10.18653/v1/2020.emnlp-main.484",
    pages = "6008--6018",
    abstract = "In this paper, we introduce XGLUE, a new benchmark dataset to train large-scale cross-lingual pre-trained models using multilingual and bilingual corpora, and evaluate their performance across a diverse set of cross-lingual tasks. Comparing to GLUE (Wang et al.,2019), which is labeled in English and includes natural language understanding tasks only, XGLUE has three main advantages: (1) it provides two corpora with different sizes for cross-lingual pre-training; (2) it provides 11 diversified tasks that cover both natural language understanding and generation scenarios; (3) for each task, it provides labeled data in multiple languages. We extend a recent cross-lingual pre-trained model Unicoder (Huang et al., 2019) to cover both understanding and generation tasks, which is evaluated on XGLUE as a strong baseline. We also evaluate the base versions (12-layer) of Multilingual BERT, XLM and XLM-R for comparison."
}

@article{ponti-etal-2019-modeling,
    title = "Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing",
    author = "Ponti, Edoardo Maria  and
      O{'}Horan, Helen  and
      Berzak, Yevgeni  and
      Vuli{\'c}, Ivan  and
      Reichart, Roi  and
      Poibeau, Thierry  and
      Shutova, Ekaterina  and
      Korhonen, Anna",
    journal = "Computational Linguistics",
    volume = "45",
    number = "3",
    month = sep,
    year = "2019",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/J19-3005/",
    doi = "10.1162/coli_a_00357",
    pages = "559--601",
    abstract = "Linguistic typology aims to capture structural and semantic variation across the world`s languages. A large-scale typology could provide excellent guidance for multilingual Natural Language Processing (NLP), particularly for languages that suffer from the lack of human labeled resources. We present an extensive literature survey on the use of typological information in the development of NLP techniques. Our survey demonstrates that to date, the use of information in existing typological databases has resulted in consistent but modest improvements in system performance. We show that this is due to both intrinsic limitations of databases (in terms of coverage and feature granularity) and under-utilization of the typological features included in them. We advocate for a new approach that adapts the broad and discrete nature of typological categories to the contextual and continuous nature of machine learning algorithms used in contemporary NLP. In particular, we suggest that such an approach could be facilitated by recent developments in data-driven induction of typological knowledge."
}

@inproceedings{lin-etal-2019-choosing,
    title = "Choosing Transfer Languages for Cross-Lingual Learning",
    author = "Lin, Yu-Hsiang  and
      Chen, Chian-Yu  and
      Lee, Jean  and
      Li, Zirui  and
      Zhang, Yuyan  and
      Xia, Mengzhou  and
      Rijhwani, Shruti  and
      He, Junxian  and
      Zhang, Zhisong  and
      Ma, Xuezhe  and
      Anastasopoulos, Antonios  and
      Littell, Patrick  and
      Neubig, Graham",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1301/",
    doi = "10.18653/v1/P19-1301",
    pages = "3125--3135",
    abstract = "Cross-lingual transfer, where a high-resource transfer language is used to improve the accuracy of a low-resource task language, is now an invaluable tool for improving performance of natural language processing (NLP) on low-resource languages. However, given a particular task language, it is not clear which language to transfer from, and the standard strategy is to select languages based on ad hoc criteria, usually the intuition of the experimenter. Since a large number of features contribute to the success of cross-lingual transfer (including phylogenetic similarity, typological properties, lexical overlap, or size of available data), even the most enlightened experimenter rarely considers all these factors for the particular task at hand. In this paper, we consider this task of automatically selecting optimal transfer languages as a ranking problem, and build models that consider the aforementioned features to perform this prediction. In experiments on representative NLP tasks, we demonstrate that our model predicts good transfer languages much better than ad hoc baselines considering single features in isolation, and glean insights on what features are most informative for each different NLP tasks, which may inform future ad hoc selection even without use of our method."
}

@inproceedings{littell-etal-2017-uriel,
    title = "{URIEL} and lang2vec: Representing languages as typological, geographical, and phylogenetic vectors",
    author = "Littell, Patrick  and
      Mortensen, David R.  and
      Lin, Ke  and
      Kairis, Katherine  and
      Turner, Carlisle  and
      Levin, Lori",
    editor = "Lapata, Mirella  and
      Blunsom, Phil  and
      Koller, Alexander",
    booktitle = "Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/E17-2002/",
    pages = "8--14",
    abstract = "We introduce the URIEL knowledge base for massively multilingual NLP and the lang2vec utility, which provides information-rich vector identifications of languages drawn from typological, geographical, and phylogenetic databases and normalized to have straightforward and consistent formats, naming, and semantics. The goal of URIEL and lang2vec is to enable multilingual NLP, especially on less-resourced languages and make possible types of experiments (especially but not exclusively related to NLP tasks) that are otherwise difficult or impossible due to the sparsity and incommensurability of the data sources. lang2vec vectors have been shown to reduce perplexity in multilingual language modeling, when compared to one-hot language identification vectors."
}

@inproceedings{malaviya-etal-2017-learning,
    title = "Learning Language Representations for Typology Prediction",
    author = "Malaviya, Chaitanya  and
      Neubig, Graham  and
      Littell, Patrick",
    editor = "Palmer, Martha  and
      Hwa, Rebecca  and
      Riedel, Sebastian",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1268/",
    doi = "10.18653/v1/D17-1268",
    pages = "2529--2535",
    abstract = "One central mystery of neural NLP is what neural models {\textquotedblleft}know{\textquotedblright} about their subject matter. When a neural machine translation system learns to translate from one language to another, does it learn the syntax or semantics of the languages? Can this knowledge be extracted from the system to fill holes in human scientific knowledge? Existing typological databases contain relatively full feature specifications for only a few hundred languages. Exploiting the existence of parallel texts in more than a thousand languages, we build a massive many-to-one NMT system from 1017 languages into English, and use this to predict information missing from typological databases. Experiments show that the proposed method is able to infer not only syntactic, but also phonological and phonetic inventory features, and improves over a baseline that has access to information about the languages geographic and phylogenetic neighbors."
}

@inproceedings{georgi-etal-2010-comparing,
    title = "Comparing Language Similarity across Genetic and Typologically-Based Groupings",
    author = "Georgi, Ryan  and
      Xia, Fei  and
      Lewis, William",
    editor = "Huang, Chu-Ren  and
      Jurafsky, Dan",
    booktitle = "Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010)",
    month = aug,
    year = "2010",
    address = "Beijing, China",
    publisher = "Coling 2010 Organizing Committee",
    url = "https://aclanthology.org/C10-1044/",
    pages = "385--393"
}

@misc{xiong2017achievinghumanparityconversational,
      title={Achieving Human Parity in Conversational Speech Recognition}, 
      author={W. Xiong and J. Droppo and X. Huang and F. Seide and M. Seltzer and A. Stolcke and D. Yu and G. Zweig},
      year={2017},
      eprint={1610.05256},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1610.05256}, 
}

@article{saon2015ibm,
  title={The IBM 2015 English conversational telephone speech recognition system},
  author={Saon, George and Kuo, Hong-Kwang J and Rennie, Steven and Picheny, Michael},
  journal={arXiv preprint arXiv:1505.05899},
  year={2015}
}

@article{Pallett2003ALA,
  title={A look at NIST'S benchmark ASR tests: past, present, and future},
  author={David Pallett},
  journal={2003 IEEE Workshop on Automatic Speech Recognition and Understanding (IEEE Cat. No.03EX721)},
  year={2003},
  pages={483-488},
  url={https://api.semanticscholar.org/CorpusID:17986888}
}

@misc{kim2017jointctcattentionbasedendtoend,
      title={Joint CTC-Attention based End-to-End Speech Recognition using Multi-task Learning}, 
      author={Suyoun Kim and Takaaki Hori and Shinji Watanabe},
      year={2017},
      eprint={1609.06773},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1609.06773}, 
}

@misc{baevski2020wav2vec20frameworkselfsupervised,
      title={wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations}, 
      author={Alexei Baevski and Henry Zhou and Abdelrahman Mohamed and Michael Auli},
      year={2020},
      eprint={2006.11477},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2006.11477}, 
}

@misc{hsu2021hubertselfsupervisedspeechrepresentation,
      title={HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units}, 
      author={Wei-Ning Hsu and Benjamin Bolte and Yao-Hung Hubert Tsai and Kushal Lakhotia and Ruslan Salakhutdinov and Abdelrahman Mohamed},
      year={2021},
      eprint={2106.07447},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2106.07447}, 
}

@misc{chang2021explorationselfsupervisedpretrainedrepresentations,
      title={An Exploration of Self-Supervised Pretrained Representations for End-to-End Speech Recognition}, 
      author={Xuankai Chang and Takashi Maekaku and Pengcheng Guo and Jing Shi and Yen-Ju Lu and Aswin Shanmugam Subramanian and Tianzi Wang and Shu-wen Yang and Yu Tsao and Hung-yi Lee and Shinji Watanabe},
      year={2021},
      eprint={2110.04590},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2110.04590}, 
}

@INPROCEEDINGS{hershey2016deepclustering,
  author={Hershey, John R. and Chen, Zhuo and Le Roux, Jonathan and Watanabe, Shinji},
  booktitle={2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Deep clustering: Discriminative embeddings for segmentation and separation}, 
  year={2016},
  volume={},
  number={},
  pages={31-35},
  keywords={Speech;Training;Time-frequency analysis;Machine learning;Spectrogram;Indexes;Neural networks;speech separation;embedding;deep learning;clustering},
  doi={10.1109/ICASSP.2016.7471631}}


@misc{khanuja2020gluecosevaluationbenchmark,
      title={GLUECoS : An Evaluation Benchmark for Code-Switched NLP}, 
      author={Simran Khanuja and Sandipan Dandapat and Anirudh Srinivasan and Sunayana Sitaram and Monojit Choudhury},
      year={2020},
      eprint={2004.12376},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2004.12376}, 
}