project:
  type: website
  output-dir: docs
  preview:
    port: 7780
author: "Oren Bochman"
website:
  title: "NLP Specialization"
  navbar:
    left:
      - href: index.qmd
        text: Home
      - about.qmd
    right:
      - about.qmd
      - icon: github
        href: https://github.com/
      - icon: twitter
        href: https://twitter.com
      - icon: rss
        href: index.xml
  site-url: https://orenbochman.github.io/notes-nlp/
  description: "Course and Research notes"
  repo-url: https://github.com/OrenBochman/notes-nlp/
  repo-actions: [edit, issue]
  #favicon: favicon.ico
  sidebar:
    #background: "#f1de67" # Arylide Yellow
    contents:
      - section: "Classification & Vector Spaces"        
        contents:                
        - section: "Logistic Regression"
          contents:  
          - href: posts/c1w1/index.qmd
            text: Notes
          - href: posts/c1w1/lab01.qmd
            text: L1 - Preprocessing
          - href: posts/c1w1/lab02.qmd
            text: L2 - Frequencies
          - href: posts/c1w1/lab03.qmd
            text: L3 - Visualizing tweets
          - href: posts/c1w1/assignment.qmd
            text: A1 - Logistic Regression
        - section: "Probability & Bayes Rule"
          contents:  
          - href: posts/c1w2/index.qmd          
            text: Notes
          - href: posts/c1w2/lab01.qmd
            text: L1 - Visualizing Naive Bayes
          - href: posts/c1w2/assignment.qmd
            text: A2 - Logistic Regression
        - section: "Vector Space Models & PCA"
          contents:  
          - href: posts/c1w3/index.qmd
            text: Notes
          - href: posts/c1w3/lab01.qmd
            text: L1 - Linear algebra with NumPy
          - href: posts/c1w3/lab02.qmd
            text: L2 - Manipulating word embeddings
          - href: posts/c1w3/assignment.qmd
            text: A3 - Hello Vectors     
        - section: "MT & Document Search via KNN"
          contents:  
          - href: posts/c1w4/index.qmd
            text: Notes
          - href: posts/c1w3/lab01.qmd
            text: L1 - Vector manipulation
          - href: posts/c1w3/lab02.qmd
            text: L2 - Hash functions and multiplanes
          - href: posts/c1w3/assignment.qmd
            text: A4 - Naive Machine Translation and LSH

      - section: "Probabilistic Models"
        contents:
        - section: "Autocorrect & Dynamic Programming"
          contents:  
          - href: posts/c2w1/index.qmd
            text: Notes
          - href: posts/c2w1/lab01.qmd
            text: L1 - Building the vocabulary
          - href: posts/c2w1/lab02.qmd
            text: L2 - Candidates from String Edits
          - href: posts/c2w1/assignment.qmd
            text: A1 - Auto Correct
        - section: "POS tagging & HMMS"
          contents:  
          - href: posts/c2w2/index.qmd
            text: Notes
          - href: posts/c2w2/lab01.qmd
            text: L1 - Vocabulary with unknowns
          - href: posts/c2w2/lab02.qmd
            text: L2 - Working with tags and Numpy
          - href: posts/c2w2/assignment.qmd
            text: A2 - POS tagging
        - section: "Autocomplete & Language Models"
          contents:                
          - href: posts/c2w3/index.qmd
            text: Notes
          - href: posts/c2w3/lab01.qmd
            text: L1 - N-grams Corpus preprocessing
          - href: posts/c2w3/lab02.qmd
            text: L2 - Building the language model
          - href: posts/c2w3/lab03.qmd
            text: L3 - Out of vocabulary words
          - href: posts/c2w3/assignment.qmd
            text: A3 - Auto-Complete
        - section: "Word embeddings with neural networks"
          contents:              
          - href: posts/c2w4/index.qmd
            text: Notes
          - href: posts/c2w4/lab01.qmd
            text: L1 - Data preparation
          - href: posts/c2w4/lab02.qmd
            text: L2 - Intro to CBOW
          - href: posts/c2w4/lab03.qmd
            text: L3 - Training the CBOW
          - href: posts/c2w4/assignment.qmd
            text: A4 - Word embeddings
      - section: "Sequence Models"
        contents:
        - section: "Neural Networks for Sentiment Analysis"
          contents:  
          - href: posts/c3w1/index.qmd
            text: Notes
          - href: posts/c3w1/lab01.qmd
            text: L1 - Introduction to Trax
          - href: posts/c3w1/lab02.qmd
            text: L2 - Classes and Subclasses
          - href: posts/c3w1/lab03.qmd
            text: L3 - Data Generators
          - href: posts/c3w1/assignment.qmd
            text: A1 - Sentiment with Deep Neural Networks
        - section: "RNN for Language Modeling"
          contents:  
          - href: posts/c3w2/index.qmd
            text: Notes
          - href: posts/c3w2/lab01.qmd
            text: L1 - Hidden State Activation
          - href: posts/c3w2/lab02.qmd
            text: L2 - Calculating Perplexity
          - href: posts/c3w2/lab03.qmd
            text: L3 - Vanilla RNNs, GRUs and the scan function
          - href: posts/c3w2/lab04.qmd
            text: L4 - Creating a GRU model using Trax
          - href: posts/c3w2/assignment.qmd
            text: A2 - Deep N-grams

        - section: "LSTMs and Named Entity Recognition"
          contents:  
          - href: posts/c3w3/index.qmd
            text: Notes
          - href: posts/c3w3/lab01.qmd
            text: L1 - Vanishing Gradients          
          - href: posts/c3w3/assignment.qmd
            text: A3 - NER

        - section: "Siamese Networks"
          contents:  
          - href: posts/c3w4/index.qmd
            text: Notes
          - href: posts/c3w4/lab01.qmd
            text: L1 - Creating a Siamese Model using Trax
          - href: posts/c3w4/lab02.qmd
            text: L2 - Modified Triplet Loss
          - href: posts/c3w4/lab03.qmd
            text: L3 - Evaluate a Siamese Model
          - href: posts/c3w4/lab04.qmd
            text: L4 - Creating a GRU model using Trax
          - href: posts/c3w4/assignment.qmd
            text: A4 - Question duplicates


      - section: "NLP with Attention Models"
        contents:
        - section: "Neural Machine Translation"
          contents:  
          - href: posts/c4w1/index.qmd
            text: Notes
          - href: posts/c4w1/lab01.qmd
            text: L1 - Stack Semantics
          - href: posts/c4w1/lab02.qmd
            text: L2 - BLEU Score
          - href: posts/c4w1/assignment.qmd
            text: A4 - NMT with Attention          
        - section: "Text Summarization"
          contents:  
          - href: posts/c4w2/index.qmd
            text: Notes
          - href: posts/c4w2/lab01.qmd
            text: L1 - Attention
          - href: posts/c4w2/lab02.qmd
            text: L2 - The Transformer Decoder
          - href: posts/c4w2/assignment.qmd
            text: A4 - Transformer Summarizer            
        - section: "Question Answering"
          contents:  
          - href: posts/c4w3/index.qmd
            text: Notes
          - href: posts/c4w3/lab01.qmd
            text: L1 - SentencePiece and BPE
          - href: posts/c4w3/lab02.qmd
            text: L2 - BERT Loss
          - href: posts/c4w3/lab03.qmd
            text: L3 - T5
          - href: posts/c4w3/assignment.qmd
            text: A4 - Question Answering            
        - section: "Chat Bots"
          contents:  
          - href: posts/c4w4/index.qmd
            text: Notes
          - href: posts/c4w4/lab01.qmd
            text: L1 - Reformer LSH
          - href: posts/c4w4/lab02.qmd
            text: L2 - Revnet 
          - href: posts/c4w4/assignment.qmd
            text: A4 - Chatbot            
  page-footer:
    right: "This page is built with ðŸ’› and [Quarto](https://quarto.org/)."
    left: "&copy; Copyright 2023-2025, Oren Bochman"
    background: "#AB0520" # AZ Red

date-format: full
date: last-modified

format:
  html:
    theme:
      light: [flatly]
      dark: [darkly]
      #light: [flatly, style/isl.scss]
      #dark: [darkly,  style/isl.scss, style/dark.scss]
    css: styles.css
    grid:
      sidebar-width: 300px
      body-width: 1600px
      margin-width: 420px
      gutter-width: 1.5rem
    toc: true
    page-layout: full
    from: "markdown+emoji"
    reference-location: margin
    citation: true
    citation-location: document
    ipynb-shell-interactivity: all 
    anchor-sections: true
    code-fold: false
    html-math-method: katex
    link-external-icon: true
    link-external-newwindow: true
    link-external-filter: '^(?:http:|https:)\/\/www\.quarto\.org\/custom'
    smooth-scroll: true
    image: images/nlp-brain-wordcloud.jpg
    #title-block-banner: images/banner_black_3.jpg
    image-placeholder: images/dnn_cover.png
    title-block-banner: images/banner_deep.jpg


bibliography: references.bib

editor: 
  markdown: 
    wrap: sentence
    #wrap: 72    

execute:
  freeze: auto
  cache: true