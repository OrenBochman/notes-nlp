---
date: 2024-02-11
title: 'When and Why are Pre-trained Word Embeddings Useful  for Neural Machine Translation?'
subtitle: Review
description: ''
categories:
  - Attention
  - Bidirectional LSTM
  - Deep learning
  - Embeddings
  - LLM
  - NLP 
  - Paper
  - Review
  - Podcast
draft: false
---
::: {#vid-01}
{{< video https://youtu.be/o41avjKIg5o >}}

Talk covering this paper by Roee Aharoni 

:::

![Litrature review](/images/literature-review-open-book.jpg){.column-margin}


::: {#sup-01}
![Roee Aharoni's slides](https://roeeaharoni.com/NMT_slides_orginal.pdf)
:::

## Podcast

<audio controls="1"><source src="podcast.mp3" type="audio/mpeg"></audio>

## Abstract

> The performance of Neural Machine Translation (NMT) systems often suffers in lowresource scenarios where sufficiently largescale parallel corpora cannot be obtained. Pretrained word embeddings have proven to be invaluable for improving performance in natural language analysis tasks, which often suffer from paucity of data. However, their utility for NMT has not been extensively explored. In this work, we perform five sets of experiments that analyze when we can expect pre-trained word embeddings to help in NMT tasks. We show that such embeddings can be surprisingly effective in some cases – providing gains of up to 20 BLEU points in the most favorable setting.. --[@qi-etal-2018-pre]

## Outline

- **Introduction
    - Describes the problem of low-resource scenarios in Neural Machine Translation (NMT) and the potential utility of pre-trained word embeddings.
    - Highlights the success of pre-trained embeddings in natural language analysis tasks and the lack of extensive exploration in NMT.
    - Poses five researcb questions:
        - Q1 Is the behavior of pre-training affected by language families and other linguistic features of source and target languages? (§3)
        - Q2 Do pre-trained embeddings help more when the size of the training data is small? (§4)
        - Q3 How much does the similarity of the source and target languages affect the efficacy of using pre-trained embeddings? (§5)
        - Q4 Is it helpful to align the embedding spaces between the source and target languages? (§6)
        - Q5 Do pre-trained embeddings help more in multilingual systems as compared to bilingual systems? (§7)
- Experimental Setup
    - Details the five sets of experiments conducted to evaluate the effectiveness of pre-trained word embeddings in NMT.
    - Describes the datasets used, including the WMT14 English-German and English-French translation tasks.
    - Outlines the models and training procedures employed in the experiments.
- Results and Analysis
    - Presents the results of the experiments, showing the impact of pre-trained word embeddings on NMT performance.
    - Discusses the observed gains in BLEU scores and the factors influencing the effectiveness of pre-trained embeddings.
    - Analyzes the relationship between the quality of pre-trained embeddings and the performance of NMT systems.
- Analysis
    - Considers the implications of the findings for NMT research and practice.
    - Discusses the potential benefits and limitations of using pre-trained word embeddings in NMT tasks.
- Conclusion
    - The sweet-spot is wgere there is very little training data yet enough to train the system.
    - PTWE are more effective if there are more similar translation pairs.
    - A priori alignment of embeddings may not be necessary in bilingual scenarios, but is helpful in multi-lingual training scenarios.

## The Paper

![paper](./paper.pdf){.col-page width=800px height=1000px group="slides"}

