---
date: 2021-05-08
title: "Reformer: The Efficient Transformer"
subtitle: "Atte"
description: 'review of the 2020 paper "Reformer The Efficient Transformer" on improving the transformer architecture for the deeplearning.ai NLP specialization.'
categories: [NLP, Paper, Attention, Deep learning, Review, Stub]
draft: true
---

![Literature review](/images/literature-review-open-book.jpg){.column-margin .nolightbox width="200px"}

## Reformer: The Efficient Transformer

Reformer presents some innovations which allow a more more efficient transformer.
Location sensitive hashing allows attending back to distances of 1,000,000 positions back in the sequence.

## Abstract

> Large Transformer models routinely achieve state-of-the-art results on a number
of tasks but training these models can be prohibitively costly, especially on long
sequences. We introduce two techniques to improve the efficiency of Transform-
ers. For one, we replace dot-product attention by one that uses locality-sensitive
hashing, changing its complexity from O(L2) to O(L log L), where L is the length
of the sequence. Furthermore, we use reversible residual layers instead of the
standard residuals, which allows storing activations only once in the training pro-
cess instead of N times, where N is the number of layers. The resulting model,
the Reformer, performs on par with Transformer models while being much more
memory-efficient and much faster on long sequences. --@kitaev2020reformerefficienttransformer]

## The Paper

![paper](./paper.pdf){.col-page width=800px height=1000px group="slides"}



