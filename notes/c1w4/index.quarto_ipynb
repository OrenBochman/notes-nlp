{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "date: 2020-10-11\n",
        "title: \"Machine Translation and Document Search\"\n",
        "subtitle: \"Classification & Vector Spaces\"\n",
        "description: \"Concepts, code snippets, and slide commentaries for this week's lesson of the  Course notes from the deeplearning.ai natural language programming specialization.\"\n",
        "categories: \n",
        "  - NLP \n",
        "  - Coursera \n",
        "  - notes\n",
        "  - Word Embeddings\n",
        "  - Translation task\n",
        "  - Search Task\n",
        "---\n",
        "\n",
        "\n",
        "![course banner](/images/Course-Logo-1-3.webp){#fig-00 .column-margin .nolightbox}\n",
        "\n",
        "::: {#fig-slide-deck .column-margin}\n",
        "![This week's slides](slides.pdf){width=\"420px\" height=\"340px\" style=\"@page {size: 16in 9in;  margin: 0;  group=\"slides\"}\"}\n",
        ":::\n",
        "\n",
        "### Video: Overview\n",
        "\n",
        "This week we will be looking at machine translation and document search.\n",
        "We will start by looking at how we can represent words in a vector space.\n",
        "We will then look at how we can use these vectors to translate words from one language to another.\n",
        "We will also look at how we can use these vectors to search for documents that are similar to a given document.\n",
        "\n",
        "::: {#fig-slide-01 .column-margin}\n",
        "![translation and search](img/slide01.png)\n",
        "\n",
        "Using Vector Space Models for Translation and Search\n",
        ":::\n",
        "\n",
        "::: {#fig-slide-02 .column-margin}\n",
        "![Objectives](img/slide02.png)\n",
        "\n",
        "Learning Objectives\n",
        ":::\n",
        "\n",
        "::: callout-tip\n",
        "## Learning Objectives\n",
        "\n",
        "This week's learning objectives include:\n",
        "\n",
        "-   Gradient descent\n",
        "-   Approximate nearest neighbors\n",
        "-   Locality sensitive hashing\n",
        "-   Hash functions\n",
        "-   Hash tables\n",
        "-   K nearest neighbors\n",
        "-   Document search\n",
        "-   Machine translation\n",
        "-   Frobenius norm\n",
        ":::\n",
        "\n",
        "\n",
        "{{< pagebreak >}}\n",
        "\n",
        "\n",
        "### Transforming word vectors\n",
        "\n",
        "In the previous week, I showed we how we can plot word vectors.\n",
        "Now, we will see how we can take a word vector and learn a mapping that will allow we to translate words by learning a \"transformation matrix\".\n",
        "Here is a visualization:\n",
        "\n",
        "::: {#fig-slide-03 .column-margin}\n",
        "![Caption](img/slide03.png)\n",
        "\n",
        "Figure Text\n",
        ":::\n",
        "\n",
        "Note that the word \"chat\" in french means cat.\n",
        "We can learn that by taking the vector corresponding to \"cat\" in english, multiplying it by a matrix that we learn and then we can use cosine similarity between the output and all the french vectors.\n",
        "We should see that the closest result is the vector which corresponds to \"chat\".\n",
        "\n",
        "Here is a visualization of that showing we the aligned vectors:\n",
        "\n",
        "Note that:\n",
        "\n",
        "-   $X$ corresponds to the matrix of English word vectors and\n",
        "-   $Y$ corresponds to the matrix of French word vectors.\n",
        "-   $R$ is the mapping matrix.\n",
        "\n",
        "::: {#fig-slide-04 .column-margin}\n",
        "![Caption](img/slide04.png)\n",
        "\n",
        "Figure Text\n",
        ":::\n",
        "\n",
        "Steps required to learn $R$:\n",
        "\n",
        "-   Initialize R\n",
        "-   For loop​\n",
        "\n",
        "$$\n",
        "\\text{Loss } \\mathcal{L} = || XR-Y||_F\n",
        "$$\n",
        "\n",
        "$$\n",
        "g= \\frac{dLoss}{dR}\n",
        "$$\n",
        "\n",
        "$$\n",
        "R = R−\\alpha ∗ g\n",
        "$$\n",
        "\n",
        "::: {#fig-slide-05 .column-margin}\n",
        "![Caption](img/slide05.png)\n",
        "\n",
        "Figure Text\n",
        ":::\n",
        "\n",
        "Here is an worked out example to show we how the Frobenius norm works.\n",
        "\n",
        "::: {#fig-slide-06 .column-margin}\n",
        "![Caption](img/slide06.png)\n",
        "\n",
        "Figure Text\n",
        ":::\n",
        "\n",
        "$$\n",
        "∥XR−Y∥_F\n",
        "$$\n",
        "\n",
        "$$\n",
        "A = \\begin{bmatrix} 2 & 2 \\\\ 2 & 2 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "∥A_F∥= \\sqrt{2^2 + 2^2 + 2^2 + 2^2}  = 4\n",
        "$$\n",
        "\n",
        "$$\n",
        "∥A∥_F ≡ \\sum _{i=1}^m\\sum _{j=1}^n |a_{ij}|^2\n",
        "$$ \n",
        "\n",
        "In summary we are making use of the following:\n",
        "\n",
        "-   $XR≈Y$\n",
        "-   minimize $∥XR−Y∥_F^2$ ​\n",
        "\n",
        "::: {#fig-slide-07 .column-margin}\n",
        "\n",
        "![Caption](img/slide08.png)\n",
        "\n",
        "Figure Text\n",
        ":::\n",
        "\n",
        "::: {#fig-slide-09 .column-margin}\n",
        "![Caption](img/slide09.png)\n",
        "\n",
        "Figure Text\n",
        ":::\n",
        "\n",
        "::: {#fig-slide-10 .column-margin}\n",
        "![Caption](img/slide10.png)\n",
        "\n",
        "Figure Text\n",
        ":::\n",
        "\n",
        "\n",
        "{{< pagebreak >}}\n",
        "\n",
        "\n",
        "### Ungraded Lab: Rotation matrices in R2\n",
        "\n",
        "[Rotation matrices in R^2^](lab01.qmd)\n",
        "\n",
        "### Video: K-nearest neighbors\n",
        "\n",
        "After we have computed the output of $XR$ we get a vector. \n",
        "\n",
        "We then need to find the most similar vectors to your output. \n",
        "Here is a visual example: \n",
        "\n",
        "::: {#fig-slide-11 .column-margin}\n",
        "![Caption](img/slide11.png)\n",
        "\n",
        "Finding Translation\n",
        ":::\n",
        "\n",
        "In the video, we mentioned if we were in San Francisco, and we had friends all over the world, we would want to find the nearest neighbors. \n",
        "To do that it might be expensive to go over all the countries one at a time. \n",
        "So we will introduce hashing to show we how we can do a look up much faster. \n",
        "\n",
        "::: {#fig-slide-12 .column-margin}\n",
        "![Caption](img/slide12.png)\n",
        "\n",
        "KNN\n",
        ":::\n",
        "\n",
        "::: {#fig-slide-13 .column-margin}\n",
        "![Caption](img/slide13.png)\n",
        "\n",
        "KNN\n",
        ":::\n",
        "\n",
        "\n",
        "::: {#fig-slide-14 .column-margin}\n",
        "![Caption](img/slide14.png)\n",
        "\n",
        "KNN\n",
        ":::\n",
        "\n",
        "### Hash tables and hash functions\n",
        "\n",
        "Imagine we had to cluster the following figures into different buckets: \n",
        "\n",
        "::: {#fig-slide-15 .column-margin}\n",
        "![Hashing](img/slide15.png)\n",
        "\n",
        "Hashing\n",
        ":::\n",
        "\n",
        "Note that the figures blue, red, and gray ones would each be clustered with each other\n",
        "\n",
        "\n",
        "::: {#fig-slide-16 .column-margin}\n",
        "![Hash Tables](img/slide16.png)\n",
        "\n",
        "Hashing\n",
        ":::\n",
        "\n",
        "We can think of hash function as a function that takes data of arbitrary sizes and maps it to a fixed value. \n",
        "The values returned are known as hash values or even hashes. \n",
        "\n",
        "\n",
        "::: {#fig-slide-17 .column-margin}\n",
        "![Hash Tables](img/slide17.png)\n",
        "\n",
        "Hashing\n",
        ":::\n",
        "\n",
        "\n",
        "The diagram above shows a concrete example of a hash function which takes a vector and returns a value. \n",
        "Then we can mod that value by the number of buckets and put that number in its corresponding bucket. For example, 14 is in the 4th bucker, 17 & 97 are in the 7th bucket. \n",
        "Let's take a look at how we can do it using some code. \n",
        "\n",
        "\n",
        "::: {#fig-slide-18 .column-margin}\n",
        "![Hash Tables](img/slide18.png)\n",
        "\n",
        "Hashing\n",
        ":::\n",
        "\n",
        "The code snippet above creates a basic hash table which consists of hashed values inside their buckets. hash_function takes in value_l (a list of values to be hashed) and n_buckets and mods the value by the buckets. Now to create the hash_table, we first initialize a list to be of dimension n_buckets (each value will go to a bucket). For each value in your list of values, we will feed it into your hash_function, get the hash_value, and append it to the list of values in the corresponding bucket.\n",
        "\n",
        "Now given an input, we don't have to compare it to all the other examples, we can just compare it to all the values in the same hash_bucket that input has been hashed to. \n",
        "\n",
        "When hashing we sometimes want similar words or similar numbers to be hashed to the same bucket. To do this, we will use “locality sensitive hashing.”  Locality is another word for “location”.  So locality sensitive hashing is a hashing method that cares very deeply about assigning items based on where they’re located in vector space. \n",
        "\n",
        "::: {#fig-slide-19 .column-margin}\n",
        "![Hash Tables](img/slide19.png)\n",
        "\n",
        "Hashing\n",
        ":::\n",
        "\n",
        "::: {#fig-slide-20 .column-margin}\n",
        "![Hash Tables](img/slide20.png)\n",
        "\n",
        "Hashing\n",
        ":::\n",
        "\n",
        "### Locality sensitive hashing\n",
        "\n",
        "Locality sensitive hashing is a technique that allows we to hash similar inputs into the same buckets with high probability. \n",
        "\n",
        "::: {#fig-slide-21 .column-margin}\n",
        "![Hash Tables](img/slide21.png)\n",
        "\n",
        "Locality sensitive hashing\n",
        ":::\n",
        "\n",
        "Instead of the typical buckets we have been using, we can think of clustering the points by deciding whether they are above or below the line. Now as we go to higher dimensions (say n-dimensional vectors), we would be using planes instead of lines. Let's look at a concrete example: \n",
        "\n",
        "\n",
        "::: {#fig-slide-24 .column-margin}\n",
        "![Hash Tables](img/slide24.png)\n",
        "\n",
        "Locality sensitive hashing\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "Given some point located at (1,1) and three vectors $V_1=(1,2)$, $V_2=(-1,1)$, $V_3=(-2,-1)$ we will see what happens when we take the dot product. First note that the dashed line is our plane. The vector with point $P=(1,1)$ is perpendicular to that line (plane). Now any vector above the dashed line that is multiplied by $(1,1)$ would have a positive number. Any vector below the dashed line when dotted with $(1,1)$ will have a negative number. Any vector on the dashed line multiplied by $(1,1)$ will give we a dot product of 0.\n",
        "​\n",
        "Here is how to visualize a projection (i.e. a dot product between two vectors):\n",
        "\n",
        "\n",
        "::: {#fig-slide-25 .column-margin}\n",
        "![Hash Tables](img/slide25.png)\n",
        "\n",
        "Locality sensitive hashing\n",
        ":::\n",
        "\n",
        "\n",
        "When we take the dot product of a vector $V$ and a $P$, then we take the magnitude or length of that vector,  we get the black line (labelled as  Projection). The sign indicates on which side of the plane the projection vector lies.\n",
        "\n",
        "### Multiple Planes\n",
        "\n",
        "We can use multiple planes to get a single hash value. Let's take a look at the following example: \n",
        "\n",
        "::: {#fig-slide-35 .column-margin}\n",
        "![Hash Tables](img/slide35.png)\n",
        "\n",
        "Locality sensitive hashing\n",
        ":::\n",
        "\n",
        "\n",
        "Given some point denoted by v,  we can run it through several projections $P_1, P_2, P_3$ to get one hash value. If we compute $P_1v^T P_1v^T$ we get a positive number, so we set $h_1=1$. $P_2v^T P_2v^T$ gives we a positive number so we get $h_2=1$. $P_3v^T P_3v^T$ is a negative number so we set $h_3$ to be 0. We can then compute the hash value as follows.\n",
        "​\n",
        "$$\n",
        "hash=2^0×h_1+2^1×h_2+2^2×h_3 = 1×1+2×1+4×0=3\n",
        "$$\n",
        "\n",
        "Another way to think of it, is at each time we are asking the plane to which side will we find the point (i.e. 1 or 0) until we find your point bounded by the surrounding planes.The hash value is then defined as:\n",
        "\n",
        "$$\n",
        "hash_{value}=\\sum_i^H 2^i×h_i\n",
        "$$\n",
        "\n",
        "Here is how we can code it up: \n"
      ],
      "id": "ee973c31"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def hash_value_of_vector(P, v):\n",
        "    hash_value = 0\n",
        "    for i, plane in enumerate(P):\n",
        "        sign = side_of_plane(plane, v)\n",
        "        hash_i = 1 if sign else 0\n",
        "        hash_value += hash_i * 2**i\n",
        "    return hash_value"
      ],
      "id": "687fdb85",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$P_l$ is the list of planes. We initialize the value to 0, and then we iterate over all the planes (P), and we keep track of the index. We get the sign by finding the sign of the dot product between v and your plane P. If it is positive we set it equal to 1, otherwise we set it equal to 0. We then add the score for the ith plane to the hash value by computing $2^i×h_i$. \n",
        "\n",
        "::: {#fig-slide-37 .column-margin}\n",
        "![hash_multiple_plane()](img/slide37.png)\n",
        "\n",
        "Code for computing hash value for multiple planes\n",
        ":::\n",
        "\n",
        "<!-- TODO: convert slide to code -->\n",
        "\n",
        "### Ungraded Lab: Hash tables\n",
        "\n",
        "[Hash Tables Lab](lab02.qmd)\n",
        "\n",
        "### Reading: Approximate nearest neighbors\n",
        "\n",
        "Approximate nearest neighbors does not give we the full nearest neighbors but gives we an approximation of the nearest neighbors. \n",
        "It usually trades off accuracy for efficiency. \n",
        "\n",
        "Look at the following plot: \n",
        "\n",
        "::: {#fig-slide-39 .column-margin}\n",
        "![nearest neighnours](img/slide39.png)\n",
        "\n",
        "Approximate nearest Neighbors\n",
        ":::\n",
        "\n",
        "<!-- TODO: convert slide to code -->\n",
        "\n",
        "We are trying to find the nearest neighbor for the red vector (point). The first time, the plane gave we green points. We then ran it a second time, but this time we got the blue points. The third time we got the orange points to be the neighbors. So we can see as we do it more times, we are likely to get all the neighbors. Here is the code for one set of random planes. Make sure we understand what is going on. \n",
        "\n",
        "::: {#fig-slide-40 .column-margin}\n",
        "![hash_multiple_plane()](img/slide40.png)\n",
        "\n",
        "Code for computing hash value for multiple planes\n",
        ":::\n",
        "\n",
        "<!-- TODO: convert slide to code -->\n",
        "\n",
        "::: {#fig-slide-41 .column-margin}\n",
        "![Document representation](img/slide41.png)\n",
        "\n",
        "A combination of word vectors to get document vectors based on Document search with KNN\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "### Searching documents\n",
        "\n",
        "The previous video shows we a toy example of how we can actually represent a document as a vector.  \n",
        "\n",
        "::: {#fig-slide-39 .column-margin}\n",
        "![hash_multiple_plane()](img/slide42.png)\n",
        "\n",
        "Code for combining word vectors to get document vectors\n",
        ":::\n",
        "\n",
        "In this example, we just add the word vectors of a document to get the document vector. \n",
        "So in summary we should now be familiar with the following concepts:  \n",
        "\n",
        "\n",
        "::: {#fig-slide-39 .column-margin}\n",
        "![hash_multiple_plane()](img/slide02.png)\n",
        "\n",
        "Code for combining word vectors to get document vectors\n",
        ":::\n",
        "\n",
        "### Video: Andrew Ng with Kathleen McKeown\n",
        "\n",
        "> I am delighted to have with me here today, Kathy McKeown, who is the Henry and Gertrude Rothschild Professor of Computer Science at Columbia University. Where she is also the founding director of the Institute for Data Sciences and Engineering. She is also an Amazon Scholar and is well known for the work that she's done over many years on text summarization and many other topics in NLP. Welcome, Kathy, and thanks for joining me. \n",
        "\n",
        "> Thanks Andrew, and thanks for having me. \n",
        "\n",
        "> So today we lead a large group doing NLP research, but your journey to becoming an AI researcher and an NLP searcher has been an unusual one. If I remember correctly, we actually majored in comparative literature when we were in undergrad, even though you're also very mathematically oriented at the time. So tell us we story of how we became an NLP researcher. \n",
        "\n",
        "> Yeah, so when I started out at Brown I didn't know what I wanted to major in. So I took courses both in math and comparative literature. And as I went on, I became more interested in comparative literature. Probably in part because of the teachers who I had who really influenced me. It was only as I came near the end of my time at Brown that and when I graduated, I took a job as a programmer, which I found actually very boring. And I thought if I was going to have to be working 40 hours a week, I wanted to be doing something that I enjoyed. And it was then that a friend of mine who was a linguistics major at Brown told me about computational linguistics. And so I spent a lot of the year in the library reading about AI and natural language processing. And when I applied for Graduate School the following year I knew that was what I wanted to do. Because it gave me a way to bring together my interest in language and in math. \n",
        "\n",
        "> So that's fascinating, so as a comparative literature major, we spent a lot of time in the Brown University Library reading about computational linguistics and NLP. Today, with a lot of learners, maybe some watching this video that may not yet be a NLP researcher or AI engineer wanting to break into the field. So I'd love to hear more about what your experience was like reading so much. Were we doing it by yourself? Did we have a study group or what was that like? \n",
        "\n",
        "> I was doing it entirely by myself. I really had no guidance in terms of what to look at. I guess this friend that I had made a few suggestions. And then I traced references when I first began reading. I would follow up on references to go further. Yeah, and when I first entered Graduate School, and I had essentially switched fields, I found it very frightening. I was sure that I was and impostor, that I didn't know enough. And before long they would find out that I really shouldn't be there after all. Yeah, but that's something we overcome with time and we learn that it's not the case and people value your input. \n",
        "\n",
        "> That's really inspiring. Thank we for sharing that. Would we have any advice for someone maybe today that is trying to do this themselves and wondering if they know enough, or are good enough, or should be in the field? It sounds like we got through that and you've been incredibly successful. But what would we say to someone today maybe looking to follow in your footsteps and wondering if this is very for them or if they'll make it? \n",
        "\n",
        "> So I I guess I have a couple pieces of advice. I do think reaching out to people and talking to people is useful. Until I got to Graduate School I wasn't in an environment where I had people to talk to. So I do think it's really helpful, especially to talk to your peers about what they're doing and what they're interested in. When we pick problems to work on. And I guess, especially in today's world of deep learning and neural nets, I would advise choosing problems that are different from what everybody else works on. Sort of strike out in a different direction, choose something new, a new task, and take off from there. \n",
        "\n",
        "> And I think I would love to come back to the different problems stopping within. And when I speak with learners from around the world, I do hear from some that they feel lonely or isolated. They're kind of out somewhere or maybe not living in the major tech hub, and they sometimes feel like they're doing this by themselves. So I find it actually really inspiring that we were able to do that by yourself in a library in Brown University. I don't know if we have any other thoughts to offer learners that may feel like they're somewhere in a company or in a city just trying to do this by themselves. \n",
        "\n",
        "> I'm not sure I do have a lot more to say about it. I guess read what we enjoy about, if we can be part of a reading group, an online reading group, that would be helpful. There are a lot of reading groups now, and that's a good way to get sort of insights. There are online videos and course experiences, like yours. And I think that's a way to find out what's going on and get in touch with what people are doing. So I think today, the online environment can help people get connected and hear what's going on. I was lucky, I mean, I was really lucky, I applied to Penn. I didn't know that at the time it was the best place in natural language processing, and that was totally luck. So I don't know that I would recommend doing it blind again today, I think getting advice is great.  \n",
        "\n",
        "\n",
        "> Everyone that's been successful has had many elements of luck. But the preparation makes we ready to take advantage of when good luck falls into your lap. \n",
        "\n",
        "> Yeah.\n",
        "\n",
        "> Thank we for sharing that, that was really inspiring to hear about your early days as a developing researcher. And today we lead a large group at Columbia University doing very interdisciplinary work and doing a lot of work on summarization and other topics. So tell us a bit more about your current work and which excites,\n",
        "\n",
        "> So I mean, summarization has really been the bulk of my work over the most recent years. We've done work on summarization of all kinds of different genres, from personal novels to emails. One thread of research in summarization that I'm particularly excited about is work that I've done with researchers at Amazon and which was published at ACL. And this is work on summarization of novel chapters. And it's very new, no one has been working on this task, it's very challenging, very different from summarization. So the chapters are much longer than the news articles on which most current work in summarization today is done. And that is a challenge for current neural models. And a big problem is that there is an extreme amount of paraphrasing between the input, which is 19th century novels, and the output, which is a summary written in today's language. None of the current models can handle the kind of paraphrasing that we see there. And that, in general, is a topic that I'm really interested in is a sort of very abstractive summarization. Where the sentences use different words than the input document, where the syntactic structure is different. And that is very different from the vast Majority of work today which is done on summarization of news. And it's done on summarization of news because that's where the data is. So some of the other areas that I'm looking at are summarization of personal narratives that we find online where the personal narratives are very informal language and the summary is more formal. Summarization of debates. In past work, we've also done summarization of email. Which has some of those same characteristics. \n",
        "\n",
        "> Why did we choose to work on the novel summarization task? \n",
        "\n",
        "> Well, so we had done work on novels even earlier. I would say in 2010, when one of my students who was very interested in creative writing, and I really thought he should do a PhD. And so to convince him to stay, we came to a topic that he would be happy with, which was analysis and generation of creative language. And I felt then that my work came full circle, that we collaborated then with a professor in comparative literature. So I acme back to my roots in comparative literature and that was a lot of fun. So, when I first went to Amazon, I knew because of Kindle and online on Amazon, they have a lot of novels. And I thought, what would be more fun than being able to summarize novels? \n",
        "\n",
        "> Sounds like a fun project, I read a lot on my Kindle, so maybe your work will be a feature on Kindle some day. One aspect of your work that stood out as well is that you're known for doing highly interdisciplinary work. So rather than focusing  narrowly on NLP research, your work spans AI, and the media where I know the Columbia University is a great journalism school. So wonderful journalists we work with there or the application of NLP to social networks. I think we work with medical problems. So tell us a bit about how we think about interdisciplinary work because you've done more of it I think than most NLP researchers have. \n",
        "\n",
        "> Yeah, I really enjoy interdisciplinary work. I think it's my most favorite kind of research to do. And in part we get a really different perspective on research in the world when we talk to people in other fields. It takes us out of our sort of technical, narrow field. \n",
        "\n",
        "> And so earlier, we alluded to picking research topics that are novel and I think your research portfolio has certainly touched on a lot of problems that very few others are working on. So can we say more about that? So how do we pick research projects we work on and how do we advise others to pick topics to work on? \n",
        "\n",
        "> I think it's important to pick a task that matters. So that for me is one thing to look at. For example, most of the work in tech summarization today is done on summarization of what's called single document summarization of news. So take one news article in and generate a summary of that news article. And the reason for that is because that's where the data is. There's a huge amount of data that has been pulled together from first the CNN Daily Mail Corpus, and later New York Times, and there are a number of other corpora as well. The problem is that's not really a task that we need. We've known for a long time that the lead of the news article can help people pretty well in serving as a summary of the document. And in fact, for years it was hard to beat. The lead people just worked on, that wasn't a problem that people worked on in the early years of summarization. \n",
        "\n",
        "> The lead being the first sentence or the first- \n",
        "\n",
        "> The first couple of sentences in the news article. So, Yeah, I mean, people work on a problem like that because that's where the data is. We have leaderboards. People are competitive. They like to be The leaderboard, but I would question does that one or even half a point in Rouge, which the automated network used to score them, really make a difference? If we look at the output we can see that actually the summaries are quite similar and either one of them might be fine. So I prefer to go in directions that people haven't gone in before and to choose a task where if we solve it, it's going to help people. It's going to be a useful application that you've developed. So this is why I have done things like summarization of personal narrative, which we did in the course of disaster. So that we could summarize, think of having a browsing view of summaries of what people have experienced after they've lived through a disaster. Or the current work on summarization of novels where it'd be helpful to have a summary of an input chapter. I'd like to go in a different direction, in part because I want to solve the task that matters. But I also like to go in a different direction because in this day in age of deep learning where results come so fast, everybody works on the same problem trying to beat the previous state of the art. It can be hard to be the first one to get there, and if we go in a different direction, nobody else is working on, we are going to be the first one to come to solution. And that's what I like to do in my research. Overtime I like to be first on a problem. I see, cool yeah. And I feel like, for myself, I have a lot of respect for people that could push that extra half points of performance on the leaderboard because hopefully that advances the whole field. And this all shifts. I also have a lot of respect for people with your creativity in the inside, to charter the new problem that no one else has thought of, and advances the whole field in a different direction. I think the field of AI NLP is broad enough. I think it's actually not a bad thing if we have lots of people working on lots of different things, including standardized benchmarks. And a bunch of new things. \n",
        "\n",
        "> Yeah, sure, I feel that there are not as many people who want to go in that new direction and it does take some sort of guts to do it. Because the first thing that happens when we submit a paper is there is no benchmark. There is no baseline of prior work and reviewers have a very hard time dealing with that. How can they judge whether it's really a good step forward. Whereas if we can show on a leaderboard that you've improved by a certain amount and we stay within the traditional trajectory, it's easier to judge. \n",
        "\n",
        "> Yeah, I'm with we on that. Actually, I was recently chatting with one of my friends Sharon Joe who mentioned that sometimes the way benchmarks and metrics are established is that some researcher publishing a paper publishes something using some metric. Maybe a good one, maybe an okay one. But to make sure that subsequent papers can compare to earlier work, then everyone, and more and more people end up using the same metric. More for historical reasons and it makes things comparable rather than because it is actually the most useful metric. It's funny how metrics get established in academia.\n",
        "\n",
        "> Yeah, I mean that has happened in the summarization field. And I think also in machine translation where we want an automated metric because it's easier to develop a system to train over and over again. And yet everybody knows that the automated metrics that we currently have are really flawed. And but everyone keeps using them, because that's what we've always done. One of my most stark memories was I remember going to a the Information Retrieval Conference and attending a workshop in text summarization. And I remember being fascinated, but struck that about half of that workshop was on text summarization algorithms and the other half of that workshop was on how to develop metrics to evaluate. Text summarization especially, the development of automated metrics has been challenging. \n",
        "\n",
        "> Yeah. \n",
        "\n",
        "> Hey, so in terms of choosing. Lead topics to work on. One of the pieces of work that you've been doing that I thought was fascinating, was we were taking texts from the black community from Harlem, near I guess where we teach at Columbia University and analyzing that as well. Tell us about that. \n",
        "\n",
        "> This is where I'm moving with my work with a researcher from social work. And we're also beginning to involve a linguist who works on African American vernacular. And what we're doing is we're looking at what people say, what kind of emotions they express in reaction to major events that are going on today. So, for example, in reaction to Black Lives Matter and in reaction to COVID-19. So this is work that we're just beginning. We've begun with developing an interface where people can post about their experiences with these events and how they're feeling. And I guess what we're hoping to do with that in part, so we have two directions to go. One on the natural language side is to be able to understand how people express different kinds of language. Sorry, different kinds of emotion and African American vernacular. And how that difference from how people express it in standard American English. And look at the difference in language and probably even the difference in content in terms of what's expressed. And this can help us in developing algorithms that are not biased as we move forward. Most of the work in natural language, all of the systems have been trained on language that comes from news like the Wall Street Journal. \n",
        "\n",
        "> Yeah, that's great. If this type of work can help fight bias or build bridges between communities or just play some role in understanding and helping to advance the Black Lives Matter movement, that seems to be a wonderful thing to me. \n",
        "\n",
        "> Yeah, I mean, we also want in that work to look at the impact of trauma. So it's a different kind of trauma and sometimes it's not your personal trauma, but the trauma of seeing what has happened to other people who are like you. So yeah, we want to look at how that is expressed in the different kinds of emotions, the intensity of emotion, and so forth. \n",
        "\n",
        "> I find it really wonderful that NLP researchers, AI researchers can play an active role in some of these most important societal questions and issues of our time. It feels like the work we do as AI researchers, it could matter in these really important times. \n",
        "\n",
        "> Yeah, I mean I think so. And I've sort of been trying to do that for a while. I think it really attract students to work with you, and often different kinds of students into the field to work with you. On our first work on with analyzing social media posts of gang involved youth, we didn't have funding. We did that entirely with undergraduates who were just totally amazing. In earlier work we were looking at being able to automatically generate updates about disaster as it unfolded. We did that after Hurricane Sandy hit New York. And again it was something that students came to me and they had seen this happen and they have seen their neighborhoods hurt. Or they lived through the uncertainty of it and they wanted to help. They wanted to know what can we do and that was at that point in time, this whistle pre-neural net we began developing systems that could automatically generate updates as an event unfolded. \n",
        "\n",
        "> I think that we don't need a PhD, don't need a long publication record, but in undergrad spotting an opportunity with a desire to help. Can step in and start to work on systems that they can make a difference. \n",
        "\n",
        "> Yes, they're really passionate about it and There are really good, the work that came out of that was really excellent. \n",
        "\n",
        "> Yeah, thank you, so Kathy, this is great stuff. And switching tracks a bit, you've been working in NLP and associated areas for a long time. In fact, I saw that even way back in 1985 we written an early book on text generation before the modern neural text generation techniques were around. So you've been a leader in the field for a long time and seen a lot of things change. I'd love to hear your thoughts on how the field of NLP has evolved over these many years. \n",
        "\n",
        "> Sure, so when I started, which I got my PhD in 82. so I spent those earlier years at Penn. And there were some characteristics of the field that were salient. So one of them is that there was a lot of interdisciplinary work. There was in developing NLP systems, we drew a lot on work from linguistics, from philosophy, from psychology, from cognitive science. And so when I was at Penn I interacted a lot with faculty from linguistics. Ellen Prince was one of the people or faculty from philosophy. We spent time in these interdisciplinary meetings. And I can remember walking across campus with my advisor to go from the computer science department to the psychology department, for example. I was influenced a lot and I have to mention this, although it's not exactly what we asked, I was influenced a lot by senior women at the time. If I look back to who was most influential in how I progressed in my early research. My advisor, of course, who was a male Aravan Joshi. But then also Bonnie Weber, who was there in computer science, Eva Hychova from Charles University at Prague who was a linguist. Barbara Gross who lives at Sanford at that time in the CFLI Institute. And Karen Spark Jones was very influential to me. She was from the field of information retrieval. And she and I spent a lot of time talking about summarization. So interdisciplinary is one main feature of that time. A second was drawing on theories from these other areas, so we drew on theories from linguistics. One main kind of theory that we looked at was the focus of attention and how that changed over the course of a discourse. And how that influenced how we made choices and how we realized text in language. So for example, did we use a pronoun or did we use a full noun phrase? What kind of syntactic structure did we use? We might use different syntactic structures to make a concept more prominent in the discourse. We also drew on work from philosophy. So we drew on work from theories from Cyril about intention, and work from Grice about conversational implicature. And so we looked at these theories and we looked at how we could embody them in our natural language approaches. \n",
        "\n",
        "> It's great to hear about some of your early sources of inspiration. Much as I think today we will be a source of inspiration to many others. So you've seen a lot and see a lot in NLP, which continues to be a rapidly evolving field. So I'm actually curious, Kathy, what do we find most exciting in terms of emerging or exciting NLP technologies? \n",
        "\n",
        "> For me, personally, some of the work that I've already talked about today on truly abstracted summarization that uses extreme paraphrasing. Work on analyzing the language from diverse community. So we've been looking at the black community, but I think there are other communities we could look at as well. I'm interested in looking at how we deal with bias and data. And another very important topic is being able to arrive at what I would call para linguistic meaning. So this pragmatics information about emotion, about intention, would be another important direction to go. And I also think more work on events, being able to understand what events have happened, and to be able to follow them. I also think about often if I look back. Is it okay, if I talk about this now. I look back? \n",
        "\n",
        "> Okay. If I look back on my favorite technologies and papers, I can think of papers from thee points in time. The first would be older, and this was very early work in language generation, on how we pick the words in our sentence. And we thought then that it was a hard problem that constraints came from many different sources. And we wrote a paper called Floating Constraints on Lexical Choice. Where we looked at how information from different parts of language, from the discourse, from the lexicon, from syntax, from semantics, will influence what we chose. And we worked in two different domains. One was basketball and one with stock markets. And I give it the example of the floating constraint, where we want to express both the time at which something happened and the manner. In the first example we expressed time and the verb, and the manner and the adverbs. So Wall Street indexes open strongly, open is the time. And in the second weeks press manner in the verb and time and the propositional phrase. So stock index surged at the start of the trading day. And so we wanted to look at how we could control that choice. And I think control is something that's missing in language generation and summarization. Today using deep learning methods, how do we control what the output is and make sure it's true to what our intention is? In more recent work, my favorite is work on News Blaster. That's still about 15 years ago, but it feels recent to me. And that was where we took a real world problem. We did do some collaboration with journalists, and we've developed a testbed, where we could identify the events that happened during the day, and produce summary on each event. And then we also looked at how we could track that overtime. And this platform gave us a common sort of application, in which my students could address really hard research questions. And so that was where we looked at, did some of our first work on abstract and summarization. Looking at how we might compress sentences, how we could fuse phrases together, how we might reference, edit references, so that the summary was more coherent. And we also did work on multi lingual summarization. Yeah. \n",
        "\n",
        "> Cool, Thank you. Lots of exciting, very distinct projects over the years. Do we have any wrap up? Did we have any lost thoughts? We can just say, do we have any final thoughts? \n",
        "\n",
        "> Well, I guess I would just say that natural language is a really exciting field today. There's been a huge amount of progress with deep learning. We've seen dramatic increases in accuracy, but we still have a lot of directions to go. And I guess I would like to see more of the interdisciplinary work being brought back in. I'd like to see people looking at the data more and at their output more, rather than just numbers. But I think there are many exciting directions for people to work in, and I hope we'll see many people joining the field. \n",
        "\n",
        "> Thank you, that was great. Yeah, I saw the hope that will have a lot more people join NLP and contribute to all of this exciting work. So thanks Kathy, it was cool\n",
        "\n",
        "> Thanks we much for asking me, it was fun. \n",
        "\n",
        "> For more interviews with NLP thought leaders, check out the DeepLearning.AI YouTube channel, or enroll in the NLP specialization on Coursera.\n",
        "\n",
        "<!--\n",
        "## Assignment \n",
        "\n",
        "[Assignment](assignment.qmd)\n",
        "-->\n",
        "\n",
        "### Reading: Bibliography\n",
        "\n",
        "- [@jm3] - Speech and Language Processing"
      ],
      "id": "ea2dfed0"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/home/oren/.local/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}