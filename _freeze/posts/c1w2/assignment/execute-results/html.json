{
  "hash": "71316eac60f9b7f20f6e393903e95a89",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'Assignment 2: Naive Bayes'\njupyter: python3\ncategories: \n  - NLP \n  - Coursera \n  - Lab\n  - Logistic regression\n  - Sentiment analysis task\n  - Classification & Vector Spaces\nexecute: \n    error: true\n---\n\n\n![course banner](/images/banner_c1.jpg){.column-margin}\n\n::: {.callout-warning}\n## Honor code alert\n\nDue to the Coursera Honor Code, I cannot provide the solutions to the assignments. \n\n- This notebook is the original notebook provided by the course\n- It is setup to run without stopping for errors. \n- It is also likely to be out of date as the course has had some updates since I took it.\n:::\n\nWelcome to week two of this specialization. You will learn about Naive Bayes. Concretely, you will be using Naive Bayes for sentiment analysis on tweets. Given a tweet, you will decide if it has a positive sentiment or a negative one. Specifically you will: \n\n* Train a naive bayes model on a sentiment analysis task\n* Test using your model\n* Compute ratios of positive words to negative words\n* Do some error analysis\n* Predict on your own tweet\n\nYou may already be familiar with Naive Bayes and its justification in terms of conditional probabilities and independence.\n* In this week's lectures and assignments we used the ratio of probabilities between positive and negative sentiments.\n* This approach gives us simpler formulas for these 2-way classification tasks.\n\nLoad the cell below to import some packages.\nYou  may want to browse the documentation of unfamiliar libraries and functions.\n\n::: {#3e915daf .cell execution_count=2}\n``` {.python .cell-code}\nfrom utils import process_tweet, lookup\nimport pdb\nfrom nltk.corpus import stopwords, twitter_samples\nimport numpy as np\nimport pandas as pd\nimport nltk\nimport string\nfrom nltk.tokenize import TweetTokenizer\nfrom os import getcwd\n```\n:::\n\n\nIf you are running this notebook in your local computer,\ndon't forget to download the twitter samples and stopwords from nltk.\n\n```\nnltk.download('stopwords')\nnltk.download('twitter_samples')\n```\n\n::: {#40724915 .cell execution_count=3}\n``` {.python .cell-code}\n# add folder, tmp2, from our local workspace containing pre-downloaded corpora files to nltk's data path\nfilePath = f\"{getcwd()}/../tmp2/\"\nnltk.data.path.append(filePath)\n```\n:::\n\n\n::: {#fa5cb575 .cell execution_count=4}\n``` {.python .cell-code}\n# get the sets of positive and negative tweets\nall_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')\n\n# split the data into two pieces, one for training and one for testing (validation set)\ntest_pos = all_positive_tweets[4000:]\ntrain_pos = all_positive_tweets[:4000]\ntest_neg = all_negative_tweets[4000:]\ntrain_neg = all_negative_tweets[:4000]\n\ntrain_x = train_pos + train_neg\ntest_x = test_pos + test_neg\n\n# avoid assumptions about the length of all_positive_tweets\ntrain_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\ntest_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))\n```\n:::\n\n\n# Part 1: Process the Data\n\nFor any machine learning project, once you've gathered the data, the first step is to process it to make useful inputs to your model.\n- **Remove noise**: You will first want to remove noise from your data -- that is, remove words that don't tell you much about the content. These include all common words like 'I, you, are, is, etc...' that would not give us enough information on the sentiment.\n- We'll also remove stock market tickers, retweet symbols, hyperlinks, and hashtags because they can not tell you a lot of information on the sentiment.\n- You also want to remove all the punctuation from a tweet. The reason for doing this is because we want to treat words with or without the punctuation as the same word, instead of treating \"happy\", \"happy?\", \"happy!\", \"happy,\" and \"happy.\" as different words.\n- Finally you want to use stemming to only keep track of one variation of each word. In other words, we'll treat \"motivation\", \"motivated\", and \"motivate\" similarly by grouping them within the same stem of \"motiv-\".\n\nWe have given you the function `process_tweet()` that does this for you.\n\n::: {#b1764a9b .cell execution_count=5}\n``` {.python .cell-code}\ncustom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n\n# print cleaned tweet\nprint(process_tweet(custom_tweet))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['hello', 'great', 'day', ':)', 'good', 'morn']\n```\n:::\n:::\n\n\n## Part 1.1 Implementing your helper functions\n\nTo help train your naive bayes model, you will need to build a dictionary where the keys are a (word, label) tuple and the values are the corresponding frequency.  Note that the labels we'll use here are 1 for positive and 0 for negative.\n\nYou will also implement a `lookup()` helper function that takes in the `freqs` dictionary, a word, and a label (1 or 0) and returns the number of times that word and label tuple appears in the collection of tweets.\n\nFor example: given a list of tweets `[\"i am rather excited\", \"you are rather happy\"]` and the label 1, the function will return a dictionary that contains the following key-value pairs:\n\n{\n    (\"rather\", 1): 2\n    (\"happi\", 1) : 1\n    (\"excit\", 1) : 1\n}\n\n- Notice how for each word in the given string, the same label 1 is assigned to each word.\n- Notice how the words \"i\" and \"am\" are not saved, since it was removed by process_tweet because it is a stopword.\n- Notice how the word \"rather\" appears twice in the list of tweets, and so its count value is 2.\n\n#### Instructions\nCreate a function `count_tweets()` that takes a list of tweets as input, cleans all of them, and returns a dictionary.\n- The key in the dictionary is a tuple containing the stemmed word and its class label, e.g. (\"happi\",1).\n- The value the number of times this word appears in the given collection of tweets (an integer).\n\n<details>\n<summary>\n    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n</summary>\n<p>\n<ul>\n    <li>Please use the `process_tweet` function that was imported above, and then store the words in their respective dictionaries and sets.</li>\n    <li>You may find it useful to use the `zip` function to match each element in `tweets` with each element in `ys`.</li>\n    <li>Remember to check if the key in the dictionary exists before adding that key to the dictionary, or incrementing its value.</li>\n    <li>Assume that the `result` dictionary that is input will contain clean key-value pairs (you can assume that the values will be integers that can be incremented).  It is good practice to check the datatype before incrementing the value, but it's not required here.</li>\n</ul>\n</p>\n\n::: {#be1ee431 .cell execution_count=6}\n``` {.python .cell-code}\n# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef count_tweets(result, tweets, ys):\n    '''\n    Input:\n        result: a dictionary that will be used to map each pair to its frequency\n        tweets: a list of tweets\n        ys: a list corresponding to the sentiment of each tweet (either 0 or 1)\n    Output:\n        result: a dictionary mapping each pair to its frequency\n    '''\n\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    for y, tweet in zip(ys, tweets):\n        for word in process_tweet(tweet):\n            # define the key, which is the word and label tuple\n            pair = None\n\n            # if the key exists in the dictionary, increment the count\n            if pair in result:\n                result[pair] += None\n\n            # else, if the key is new, add it to the dictionary and set the count to 1\n            else:\n                result[pair] = None\n    ### END CODE HERE ###\n\n    return result\n```\n:::\n\n\n::: {#7de04c00 .cell execution_count=7}\n``` {.python .cell-code}\n# Testing your function\n\n\nresult = {}\ntweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\nys = [1, 0, 0, 0, 0]\ncount_tweets(result, tweets, ys)\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">TypeError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[6], line 7</span>\n<span class=\"ansi-green-fg ansi-bold\">      5</span> tweets <span style=\"color:rgb(98,98,98)\">=</span> [<span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">i am happy</span><span style=\"color:rgb(175,0,0)\">'</span>, <span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">i am tricked</span><span style=\"color:rgb(175,0,0)\">'</span>, <span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">i am sad</span><span style=\"color:rgb(175,0,0)\">'</span>, <span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">i am tired</span><span style=\"color:rgb(175,0,0)\">'</span>, <span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">i am tired</span><span style=\"color:rgb(175,0,0)\">'</span>]\n<span class=\"ansi-green-fg ansi-bold\">      6</span> ys <span style=\"color:rgb(98,98,98)\">=</span> [<span style=\"color:rgb(98,98,98)\">1</span>, <span style=\"color:rgb(98,98,98)\">0</span>, <span style=\"color:rgb(98,98,98)\">0</span>, <span style=\"color:rgb(98,98,98)\">0</span>, <span style=\"color:rgb(98,98,98)\">0</span>]\n<span class=\"ansi-green-fg\">----&gt; 7</span> <span class=\"ansi-yellow-bg\">count_tweets</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">result</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">tweets</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">ys</span><span class=\"ansi-yellow-bg\">)</span>\n\nCell <span class=\"ansi-green-fg\">In[5], line 20</span>, in <span class=\"ansi-cyan-fg\">count_tweets</span><span class=\"ansi-blue-fg\">(result, tweets, ys)</span>\n<span class=\"ansi-green-fg ansi-bold\">     18</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># if the key exists in the dictionary, increment the count</span>\n<span class=\"ansi-green-fg ansi-bold\">     19</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">if</span> pair <span style=\"font-weight:bold;color:rgb(175,0,255)\">in</span> result:\n<span class=\"ansi-green-fg\">---&gt; 20</span>     result[pair] <span style=\"color:rgb(98,98,98)\">+</span><span style=\"color:rgb(98,98,98)\">=</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>\n<span class=\"ansi-green-fg ansi-bold\">     22</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># else, if the key is new, add it to the dictionary and set the count to 1</span>\n<span class=\"ansi-green-fg ansi-bold\">     23</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">else</span>:\n<span class=\"ansi-green-fg ansi-bold\">     24</span>     result[pair] <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>\n\n<span class=\"ansi-red-fg\">TypeError</span>: unsupported operand type(s) for +=: 'NoneType' and 'NoneType'</pre>\n```\n:::\n\n:::\n:::\n\n\n**Expected Output**: {('happi', 1): 1, ('trick', 0): 1, ('sad', 0): 1, ('tire', 0): 2}\n\n# Part 2: Train your model using Naive Bayes\n\nNaive bayes is an algorithm that could be used for sentiment analysis. It takes a short time to train and also has a short prediction time.\n\n#### So how do you train a Naive Bayes classifier?\n- The first part of training a naive bayes classifier is to identify the number of classes that you have.\n- You will create a probability for each class.\n$P(D_{pos})$ is the probability that the document is positive.\n$P(D_{neg})$ is the probability that the document is negative.\nUse the formulas as follows and store the values in a dictionary:\n\n$$P(D_{pos}) = \\frac{D_{pos}}{D}\\tag{1}$$\n\n$$P(D_{neg}) = \\frac{D_{neg}}{D}\\tag{2}$$\n\nWhere $D$ is the total number of documents, or tweets in this case, $D_{pos}$ is the total number of positive tweets and $D_{neg}$ is the total number of negative tweets.\n\n#### Prior and Logprior\n\nThe prior probability represents the underlying probability in the target population that a tweet is positive versus negative.  In other words, if we had no specific information and blindly picked a tweet out of the population set, what is the probability that it will be positive versus that it will be negative? That is the \"prior\".\n\nThe prior is the ratio of the probabilities $\\frac{P(D_{pos})}{P(D_{neg})}$.\nWe can take the log of the prior to rescale it, and we'll call this the logprior\n\n$$\\text{logprior} = log \\left( \\frac{P(D_{pos})}{P(D_{neg})} \\right) = log \\left( \\frac{D_{pos}}{D_{neg}} \\right)$$.\n\nNote that $log(\\frac{A}{B})$ is the same as $log(A) - log(B)$.  So the logprior can also be calculated as the difference between two logs:\n\n$$\\text{logprior} = \\log (P(D_{pos})) - \\log (P(D_{neg})) = \\log (D_{pos}) - \\log (D_{neg})\\tag{3}$$\n\n#### Positive and Negative Probability of a Word\nTo compute the positive probability and the negative probability for a specific word in the vocabulary, we'll use the following inputs:\n\n- $freq_{pos}$ and $freq_{neg}$ are the frequencies of that specific word in the positive or negative class. In other words, the positive frequency of a word is the number of times the word is counted with the label of 1.\n- $N_{pos}$ and $N_{neg}$ are the total number of positive and negative words for all documents (for all tweets), respectively.\n- $V$ is the number of unique words in the entire set of documents, for all classes, whether positive or negative.\n\nWe'll use these to compute the positive and negative probability for a specific word using this formula:\n\n$$ P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V}\\tag{4} $$\n$$ P(W_{neg}) = \\frac{freq_{neg} + 1}{N_{neg} + V}\\tag{5} $$\n\nNotice that we add the \"+1\" in the numerator for additive smoothing.  This [wiki article](https://en.wikipedia.org/wiki/Additive_smoothing) explains more about additive smoothing.\n\n#### Log likelihood\nTo compute the loglikelihood of that very same word, we can implement the following equations:\n\n$$\\text{loglikelihood} = \\log \\left(\\frac{P(W_{pos})}{P(W_{neg})} \\right)\\tag{6}$$\n\n##### Create `freqs` dictionary\n- Given your `count_tweets()` function, you can compute a dictionary called `freqs` that contains all the frequencies.\n- In this `freqs` dictionary, the key is the tuple (word, label)\n- The value is the number of times it has appeared.\n\nWe will use this dictionary in several parts of this assignment.\n\n::: {#f3a67243 .cell execution_count=8}\n``` {.python .cell-code}\n# Build the freqs dictionary for later uses\n\nfreqs = count_tweets({}, train_x, train_y)\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">TypeError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[7], line 3</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Build the freqs dictionary for later uses</span>\n<span class=\"ansi-green-fg\">----&gt; 3</span> freqs <span style=\"color:rgb(98,98,98)\">=</span> <span class=\"ansi-yellow-bg\">count_tweets</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">{</span><span class=\"ansi-yellow-bg\">}</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">train_x</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">train_y</span><span class=\"ansi-yellow-bg\">)</span>\n\nCell <span class=\"ansi-green-fg\">In[5], line 20</span>, in <span class=\"ansi-cyan-fg\">count_tweets</span><span class=\"ansi-blue-fg\">(result, tweets, ys)</span>\n<span class=\"ansi-green-fg ansi-bold\">     18</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># if the key exists in the dictionary, increment the count</span>\n<span class=\"ansi-green-fg ansi-bold\">     19</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">if</span> pair <span style=\"font-weight:bold;color:rgb(175,0,255)\">in</span> result:\n<span class=\"ansi-green-fg\">---&gt; 20</span>     result[pair] <span style=\"color:rgb(98,98,98)\">+</span><span style=\"color:rgb(98,98,98)\">=</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>\n<span class=\"ansi-green-fg ansi-bold\">     22</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># else, if the key is new, add it to the dictionary and set the count to 1</span>\n<span class=\"ansi-green-fg ansi-bold\">     23</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">else</span>:\n<span class=\"ansi-green-fg ansi-bold\">     24</span>     result[pair] <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>\n\n<span class=\"ansi-red-fg\">TypeError</span>: unsupported operand type(s) for +=: 'NoneType' and 'NoneType'</pre>\n```\n:::\n\n:::\n:::\n\n\n#### Instructions\nGiven a freqs dictionary, `train_x` (a list of tweets) and a `train_y` (a list of labels for each tweet), implement a naive bayes classifier.\n\n##### Calculate $V$\n- You can then compute the number of unique words that appear in the `freqs` dictionary to get your $V$ (you can use the `set` function).\n\n##### Calculate $freq_{pos}$ and $freq_{neg}$\n- Using your `freqs` dictionary, you can compute the positive and negative frequency of each word $freq_{pos}$ and $freq_{neg}$.\n\n##### Calculate $N_{pos}$ and $N_{neg}$\n- Using `freqs` dictionary, you can also compute the total number of positive words and total number of negative words $N_{pos}$ and $N_{neg}$.\n\n##### Calculate $D$, $D_{pos}$, $D_{neg}$\n- Using the `train_y` input list of labels, calculate the number of documents (tweets) $D$, as well as the number of positive documents (tweets) $D_{pos}$ and number of negative documents (tweets) $D_{neg}$.\n- Calculate the probability that a document (tweet) is positive $P(D_{pos})$, and the probability that a document (tweet) is negative $P(D_{neg})$\n\n##### Calculate the logprior\n- the logprior is $log(D_{pos}) - log(D_{neg})$\n\n##### Calculate log likelihood\n- Finally, you can iterate over each word in the vocabulary, use your `lookup` function to get the positive frequencies, $freq_{pos}$, and the negative frequencies, $freq_{neg}$, for that specific word.\n- Compute the positive probability of each word $P(W_{pos})$, negative probability of each word $P(W_{neg})$ using equations 4 & 5.\n\n$$ P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V}\\tag{4} $$\n$$ P(W_{neg}) = \\frac{freq_{neg} + 1}{N_{neg} + V}\\tag{5} $$\n\n**Note:** We'll use a dictionary to store the log likelihoods for each word.  The key is the word, the value is the log likelihood of that word).\n\n- You can then compute the loglikelihood: $log \\left( \\frac{P(W_{pos})}{P(W_{neg})} \\right)\\tag{6}$.\n\n::: {#3e5395de .cell execution_count=9}\n``` {.python .cell-code}\n# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef train_naive_bayes(freqs, train_x, train_y):\n    '''\n    Input:\n        freqs: dictionary from (word, label) to how often the word appears\n        train_x: a list of tweets\n        train_y: a list of labels correponding to the tweets (0,1)\n    Output:\n        logprior: the log prior. (equation 3 above)\n        loglikelihood: the log likelihood of you Naive bayes equation. (equation 6 above)\n    '''\n    loglikelihood = {}\n    logprior = 0\n\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n\n    # calculate V, the number of unique words in the vocabulary\n    vocab = set([pair[0] for pair in freqs.keys()])\n    V = len(vocab)\n\n    # calculate N_pos and N_neg\n    N_pos = N_neg = 0\n    for pair in freqs.keys():\n        # if the label is positive (greater than zero)\n        if pair[1] > 0:\n\n            # Increment the number of positive words by the count for this (word, label) pair\n            N_pos += None\n\n        # else, the label is negative\n        else:\n\n            # increment the number of negative words by the count for this (word,label) pair\n            N_neg += None\n\n    # Calculate D, the number of documents\n    D = None\n\n    # Calculate D_pos, the number of positive documents (*hint: use sum(<np_array>))\n    D_pos = None\n\n    # Calculate D_neg, the number of negative documents (*hint: compute using D and D_pos)\n    D_neg = None\n\n    # Calculate logprior\n    logprior = None\n\n    # For each word in the vocabulary...\n    for word in vocab:\n        # get the positive and negative frequency of the word\n        freq_pos = None\n        freq_neg = None\n\n        # calculate the probability that each word is positive, and negative\n        p_w_pos = None\n        p_w_neg = None\n\n        # calculate the log likelihood of the word\n        loglikelihood[word] = None\n\n    ### END CODE HERE ###\n\n    return logprior, loglikelihood\n```\n:::\n\n\n::: {#fdec3bcc .cell execution_count=10}\n``` {.python .cell-code}\n# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\nlogprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\nprint(logprior)\nprint(len(loglikelihood))\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[9], line 3</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span>\n<span class=\"ansi-green-fg ansi-bold\">      2</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything</span>\n<span class=\"ansi-green-fg\">----&gt; 3</span> logprior, loglikelihood <span style=\"color:rgb(98,98,98)\">=</span> train_naive_bayes(<span class=\"ansi-yellow-bg\">freqs</span>, train_x, train_y)\n<span class=\"ansi-green-fg ansi-bold\">      4</span> <span style=\"color:rgb(0,135,0)\">print</span>(logprior)\n<span class=\"ansi-green-fg ansi-bold\">      5</span> <span style=\"color:rgb(0,135,0)\">print</span>(<span style=\"color:rgb(0,135,0)\">len</span>(loglikelihood))\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'freqs' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n**Expected Output**:\n\n0.0\n\n9089\n\n# Part 3: Test your naive bayes\n\nNow that we have the `logprior` and `loglikelihood`, we can test the naive bayes function by making predicting on some tweets!\n\n#### Implement `naive_bayes_predict`\n**Instructions**:\nImplement the `naive_bayes_predict` function to make predictions on tweets.\n* The function takes in the `tweet`, `logprior`, `loglikelihood`.\n* It returns the probability that the tweet belongs to the positive or negative class.\n* For each tweet, sum up loglikelihoods of each word in the tweet.\n* Also add the logprior to this sum to get the predicted sentiment of that tweet.\n\n$$ p = logprior + \\sum_i^N (loglikelihood_i)$$\n\n#### Note\nNote we calculate the prior from the training data, and that the training data is evenly split between positive and negative labels (4000 positive and 4000 negative tweets).  This means that the ratio of positive to negative 1, and the logprior is 0.\n\nThe value of 0.0 means that when we add the logprior to the log likelihood, we're just adding zero to the log likelihood.  However, please remember to include the logprior, because whenever the data is not perfectly balanced, the logprior will be a non-zero value.\n\n::: {#277e3f7b .cell execution_count=11}\n``` {.python .cell-code}\n# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef naive_bayes_predict(tweet, logprior, loglikelihood):\n    '''\n    Input:\n        tweet: a string\n        logprior: a number\n        loglikelihood: a dictionary of words mapping to numbers\n    Output:\n        p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n\n    '''\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    # process the tweet to get a list of words\n    word_l = None\n\n    # initialize probability to zero\n    p = 0\n\n    # add the logprior\n    p += None\n\n    for word in word_l:\n\n        # check if the word exists in the loglikelihood dictionary\n        if word in loglikelihood:\n            # add the log likelihood of that word to the probability\n            p += None\n\n    ### END CODE HERE ###\n\n    return p\n```\n:::\n\n\n::: {#a425f766 .cell execution_count=12}\n``` {.python .cell-code}\n# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n\n# Experiment with your own tweet.\nmy_tweet = 'She smiled.'\np = naive_bayes_predict(my_tweet, logprior, loglikelihood)\nprint('The expected output is', p)\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[11], line 6</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span>\n<span class=\"ansi-green-fg ansi-bold\">      2</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything</span>\n<span class=\"ansi-green-fg ansi-bold\">      3</span> \n<span class=\"ansi-green-fg ansi-bold\">      4</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Experiment with your own tweet.</span>\n<span class=\"ansi-green-fg ansi-bold\">      5</span> my_tweet <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">She smiled.</span><span style=\"color:rgb(175,0,0)\">'</span>\n<span class=\"ansi-green-fg\">----&gt; 6</span> p <span style=\"color:rgb(98,98,98)\">=</span> naive_bayes_predict(my_tweet, <span class=\"ansi-yellow-bg\">logprior</span>, loglikelihood)\n<span class=\"ansi-green-fg ansi-bold\">      7</span> <span style=\"color:rgb(0,135,0)\">print</span>(<span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">The expected output is</span><span style=\"color:rgb(175,0,0)\">'</span>, p)\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'logprior' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n**Expected Output**:\n- The expected output is around 1.57\n- The sentiment is positive.\n\n#### Implement test_naive_bayes\n**Instructions**:\n* Implement `test_naive_bayes` to check the accuracy of your predictions.\n* The function takes in your `test_x`, `test_y`, log_prior, and loglikelihood\n* It returns the accuracy of your model.\n* First, use `naive_bayes_predict` function to make predictions for each tweet in text_x.\n\n::: {#6fe4b301 .cell execution_count=13}\n``` {.python .cell-code}\n# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n    \"\"\"\n    Input:\n        test_x: A list of tweets\n        test_y: the corresponding labels for the list of tweets\n        logprior: the logprior\n        loglikelihood: a dictionary with the loglikelihoods for each word\n    Output:\n        accuracy: (# of tweets classified correctly)/(total # of tweets)\n    \"\"\"\n    accuracy = 0  # return this properly\n\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    y_hats = []\n    for tweet in test_x:\n        # if the prediction is > 0\n        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n            # the predicted class is 1\n            y_hat_i = None\n        else:\n            # otherwise the predicted class is 0\n            y_hat_i = None\n\n        # append the predicted class to the list y_hats\n        None\n\n    # error is the average of the absolute values of the differences between y_hats and test_y\n    error = None\n\n    # Accuracy is 1 minus the error\n    accuracy = None\n\n    ### END CODE HERE ###\n\n    return accuracy\n```\n:::\n\n\n::: {#e21f75a0 .cell execution_count=14}\n``` {.python .cell-code}\nprint(\"Naive Bayes accuracy = %0.4f\" %\n      (test_naive_bayes(test_x, test_y, logprior, loglikelihood)))\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[13], line 2</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> <span style=\"color:rgb(0,135,0)\">print</span>(<span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">Naive Bayes accuracy = </span><span style=\"font-weight:bold;color:rgb(175,95,135)\">%0.4f</span><span style=\"color:rgb(175,0,0)\">\"</span> <span style=\"color:rgb(98,98,98)\">%</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span>       (test_naive_bayes(test_x, test_y, <span class=\"ansi-yellow-bg\">logprior</span>, loglikelihood)))\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'logprior' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n**Expected Accuracy**:\n\n0.9940\n\n::: {#3d13b62c .cell execution_count=15}\n``` {.python .cell-code}\n# UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n\n# Run this cell to test your function\nfor tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n    # print( '%s -> %f' % (tweet, naive_bayes_predict(tweet, logprior, loglikelihood)))\n    p = naive_bayes_predict(tweet, logprior, loglikelihood)\n#     print(f'{tweet} -> {p:.2f} ({p_category})')\n    print(f'{tweet} -> {p:.2f}')\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[14], line 7</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span>\n<span class=\"ansi-green-fg ansi-bold\">      2</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything</span>\n<span class=\"ansi-green-fg ansi-bold\">      3</span> \n<span class=\"ansi-green-fg ansi-bold\">      4</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Run this cell to test your function</span>\n<span class=\"ansi-green-fg ansi-bold\">      5</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">for</span> tweet <span style=\"font-weight:bold;color:rgb(175,0,255)\">in</span> [<span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">I am happy</span><span style=\"color:rgb(175,0,0)\">'</span>, <span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">I am bad</span><span style=\"color:rgb(175,0,0)\">'</span>, <span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">this movie should have been great.</span><span style=\"color:rgb(175,0,0)\">'</span>, <span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">great</span><span style=\"color:rgb(175,0,0)\">'</span>, <span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">great great</span><span style=\"color:rgb(175,0,0)\">'</span>, <span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">great great great</span><span style=\"color:rgb(175,0,0)\">'</span>, <span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">great great great great</span><span style=\"color:rgb(175,0,0)\">'</span>]:\n<span class=\"ansi-green-fg ansi-bold\">      6</span>     <span style=\"font-style:italic;color:rgb(95,135,135)\"># print( '%s -&gt; %f' % (tweet, naive_bayes_predict(tweet, logprior, loglikelihood)))</span>\n<span class=\"ansi-green-fg\">----&gt; 7</span>     p <span style=\"color:rgb(98,98,98)\">=</span> naive_bayes_predict(tweet, <span class=\"ansi-yellow-bg\">logprior</span>, loglikelihood)\n<span class=\"ansi-green-fg ansi-bold\">      8</span> <span style=\"font-style:italic;color:rgb(95,135,135)\">#     print(f'{tweet} -&gt; {p:.2f} ({p_category})')</span>\n<span class=\"ansi-green-fg ansi-bold\">      9</span>     <span style=\"color:rgb(0,135,0)\">print</span>(<span style=\"color:rgb(175,0,0)\">f</span><span style=\"color:rgb(175,0,0)\">'</span><span style=\"font-weight:bold;color:rgb(175,95,135)\">{</span>tweet<span style=\"font-weight:bold;color:rgb(175,95,135)\">}</span><span style=\"color:rgb(175,0,0)\"> -&gt; </span><span style=\"font-weight:bold;color:rgb(175,95,135)\">{</span>p<span style=\"font-weight:bold;color:rgb(175,95,135)\">:</span><span style=\"color:rgb(175,0,0)\">.2f</span><span style=\"font-weight:bold;color:rgb(175,95,135)\">}</span><span style=\"color:rgb(175,0,0)\">'</span>)\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'logprior' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n**Expected Output**:\n- I am happy -> 2.15\n- I am bad -> -1.29\n- this movie should have been great. -> 2.14\n- great -> 2.14\n- great great -> 4.28\n- great great great -> 6.41\n- great great great great -> 8.55\n\n::: {#7b932a1a .cell execution_count=16}\n``` {.python .cell-code}\n# Feel free to check the sentiment of your own tweet below\nmy_tweet = 'you are bad :('\nnaive_bayes_predict(my_tweet, logprior, loglikelihood)\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[15], line 3</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Feel free to check the sentiment of your own tweet below</span>\n<span class=\"ansi-green-fg ansi-bold\">      2</span> my_tweet <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">you are bad :(</span><span style=\"color:rgb(175,0,0)\">'</span>\n<span class=\"ansi-green-fg\">----&gt; 3</span> naive_bayes_predict(my_tweet, <span class=\"ansi-yellow-bg\">logprior</span>, loglikelihood)\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'logprior' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n# Part 4: Filter words by Ratio of positive to negative counts\n\n- Some words have more positive counts than others, and can be considered \"more positive\".  Likewise, some words can be considered more negative than others.\n- One way for us to define the level of positiveness or negativeness, without calculating the log likelihood, is to compare the positive to negative frequency of the word.\n    - Note that we can also use the log likelihood calculations to compare relative positivity or negativity of words.\n- We can calculate the ratio of positive to negative frequencies of a word.\n- Once we're able to calculate these ratios, we can also filter a subset of words that have a minimum ratio of positivity / negativity or higher.\n- Similarly, we can also filter a subset of words that have a maximum ratio of positivity / negativity or lower (words that are at least as negative, or even more negative than a given threshold).\n\n#### Implement `get_ratio()`\n- Given the `freqs` dictionary of words and a particular word, use `lookup(freqs,word,1)` to get the positive count of the word.\n- Similarly, use the `lookup()` function to get the negative count of that word.\n- Calculate the ratio of positive divided by negative counts\n\n$$ ratio = \\frac{\\text{pos_words} + 1}{\\text{neg_words} + 1} $$\n\nWhere pos_words and neg_words correspond to the frequency of the words in their respective classes. \n<table>\n    <tr>\n        <td>\n            <b>Words</b>\n        </td>\n        <td>\n        Positive word count\n        </td>\n         <td>\n        Negative Word Count\n        </td>\n  </tr>\n    <tr>\n        <td>\n        glad\n        </td>\n         <td>\n        41\n        </td>\n    <td>\n        2\n        </td>\n  </tr>\n    <tr>\n        <td>\n        arriv\n        </td>\n         <td>\n        57\n        </td>\n    <td>\n        4\n        </td>\n  </tr>\n    <tr>\n        <td>\n        :(\n        </td>\n         <td>\n        1\n        </td>\n    <td>\n        3663\n        </td>\n  </tr>\n    <tr>\n        <td>\n        :-(\n        </td>\n         <td>\n        0\n        </td>\n    <td>\n        378\n        </td>\n  </tr>\n</table>\n\n::: {#ca17bff5 .cell execution_count=17}\n``` {.python .cell-code}\n# UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef get_ratio(freqs, word):\n    '''\n    Input:\n        freqs: dictionary containing the words\n        word: string to lookup\n\n    Output: a dictionary with keys 'positive', 'negative', and 'ratio'.\n        Example: {'positive': 10, 'negative': 20, 'ratio': 0.5}\n    '''\n    pos_neg_ratio = {'positive': 0, 'negative': 0, 'ratio': 0.0}\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    # use lookup() to find positive counts for the word (denoted by the integer 1)\n    pos_neg_ratio['positive'] = None\n\n    # use lookup() to find negative counts for the word (denoted by integer 0)\n    pos_neg_ratio['negative'] = None\n\n    # calculate the ratio of positive to negative counts for the word\n    pos_neg_ratio['ratio'] = None\n    ### END CODE HERE ###\n    return pos_neg_ratio\n```\n:::\n\n\n::: {#6d89d214 .cell execution_count=18}\n``` {.python .cell-code}\nget_ratio(freqs, 'happi')\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[17], line 1</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span> get_ratio(<span class=\"ansi-yellow-bg\">freqs</span>, <span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">happi</span><span style=\"color:rgb(175,0,0)\">'</span>)\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'freqs' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n#### Implement `get_words_by_threshold(freqs,label,threshold)`\n\n* If we set the label to 1, then we'll look for all words whose threshold of positive/negative is at least as high as that threshold, or higher.\n* If we set the label to 0, then we'll look for all words whose threshold of positive/negative is at most as low as the given threshold, or lower.\n* Use the `get_ratio()` function to get a dictionary containing the positive count, negative count, and the ratio of positive to negative counts.\n* Append a dictionary to a list, where the key is the word, and the dictionary is the dictionary `pos_neg_ratio` that is returned by the `get_ratio()` function.\nAn example key-value pair would have this structure:\n```\n{'happi':\n    {'positive': 10, 'negative': 20, 'ratio': 0.5}\n}\n```\n\n::: {#2a50531e .cell execution_count=19}\n``` {.python .cell-code}\n# UNQ_C9 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef get_words_by_threshold(freqs, label, threshold):\n    '''\n    Input:\n        freqs: dictionary of words\n        label: 1 for positive, 0 for negative\n        threshold: ratio that will be used as the cutoff for including a word in the returned dictionary\n    Output:\n        word_set: dictionary containing the word and information on its positive count, negative count, and ratio of positive to negative counts.\n        example of a key value pair:\n        {'happi':\n            {'positive': 10, 'negative': 20, 'ratio': 0.5}\n        }\n    '''\n    word_list = {}\n\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    for key in freqs.keys():\n        word, _ = key\n\n        # get the positive/negative ratio for a word\n        pos_neg_ratio = None\n\n        # if the label is 1 and the ratio is greater than or equal to the threshold...\n        if label == 1 and None:\n\n            # Add the pos_neg_ratio to the dictionary\n            None\n\n        # If the label is 0 and the pos_neg_ratio is less than or equal to the threshold...\n        elif label == 0 and None:\n\n            # Add the pos_neg_ratio to the dictionary\n            None\n\n        # otherwise, do not include this word in the list (do nothing)\n\n    ### END CODE HERE ###\n    return word_list\n```\n:::\n\n\n::: {#16583409 .cell execution_count=20}\n``` {.python .cell-code}\n# Test your function: find negative words at or below a threshold\nget_words_by_threshold(freqs, label=0, threshold=0.05)\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[19], line 2</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Test your function: find negative words at or below a threshold</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span> get_words_by_threshold(<span class=\"ansi-yellow-bg\">freqs</span>, label<span style=\"color:rgb(98,98,98)\">=</span><span style=\"color:rgb(98,98,98)\">0</span>, threshold<span style=\"color:rgb(98,98,98)\">=</span><span style=\"color:rgb(98,98,98)\">0.05</span>)\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'freqs' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n::: {#56c7b38c .cell execution_count=21}\n``` {.python .cell-code}\n# Test your function; find positive words at or above a threshold\nget_words_by_threshold(freqs, label=1, threshold=10)\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[20], line 2</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Test your function; find positive words at or above a threshold</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span> get_words_by_threshold(<span class=\"ansi-yellow-bg\">freqs</span>, label<span style=\"color:rgb(98,98,98)\">=</span><span style=\"color:rgb(98,98,98)\">1</span>, threshold<span style=\"color:rgb(98,98,98)\">=</span><span style=\"color:rgb(98,98,98)\">10</span>)\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'freqs' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\nNotice the difference between the positive and negative ratios. Emojis like :( and words like 'me' tend to have a negative connotation. Other words like 'glad', 'community', and 'arrives' tend to be found in the positive tweets.\n\n# Part 5: Error Analysis\n\nIn this part you will see some tweets that your model missclassified. Why do you think the misclassifications happened? Were there any assumptions made by the naive bayes model?\n\n::: {#36f9a3f8 .cell execution_count=22}\n``` {.python .cell-code}\n# Some error analysis done for you\nprint('Truth Predicted Tweet')\nfor x, y in zip(test_x, test_y):\n    y_hat = naive_bayes_predict(x, logprior, loglikelihood)\n    if y != (np.sign(y_hat) > 0):\n        print('%d\\t%0.2f\\t%s' % (y, np.sign(y_hat) > 0, ' '.join(\n            process_tweet(x)).encode('ascii', 'ignore')))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTruth Predicted Tweet\n```\n:::\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[21], line 4</span>\n<span class=\"ansi-green-fg ansi-bold\">      2</span> <span style=\"color:rgb(0,135,0)\">print</span>(<span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">Truth Predicted Tweet</span><span style=\"color:rgb(175,0,0)\">'</span>)\n<span class=\"ansi-green-fg ansi-bold\">      3</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">for</span> x, y <span style=\"font-weight:bold;color:rgb(175,0,255)\">in</span> <span style=\"color:rgb(0,135,0)\">zip</span>(test_x, test_y):\n<span class=\"ansi-green-fg\">----&gt; 4</span>     y_hat <span style=\"color:rgb(98,98,98)\">=</span> naive_bayes_predict(x, <span class=\"ansi-yellow-bg\">logprior</span>, loglikelihood)\n<span class=\"ansi-green-fg ansi-bold\">      5</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">if</span> y <span style=\"color:rgb(98,98,98)\">!=</span> (np<span style=\"color:rgb(98,98,98)\">.</span>sign(y_hat) <span style=\"color:rgb(98,98,98)\">&gt;</span> <span style=\"color:rgb(98,98,98)\">0</span>):\n<span class=\"ansi-green-fg ansi-bold\">      6</span>         <span style=\"color:rgb(0,135,0)\">print</span>(<span style=\"color:rgb(175,0,0)\">'</span><span style=\"font-weight:bold;color:rgb(175,95,135)\">%d</span><span style=\"font-weight:bold;color:rgb(175,95,0)\">\\t</span><span style=\"font-weight:bold;color:rgb(175,95,135)\">%0.2f</span><span style=\"font-weight:bold;color:rgb(175,95,0)\">\\t</span><span style=\"font-weight:bold;color:rgb(175,95,135)\">%s</span><span style=\"color:rgb(175,0,0)\">'</span> <span style=\"color:rgb(98,98,98)\">%</span> (y, np<span style=\"color:rgb(98,98,98)\">.</span>sign(y_hat) <span style=\"color:rgb(98,98,98)\">&gt;</span> <span style=\"color:rgb(98,98,98)\">0</span>, <span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\"> </span><span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(98,98,98)\">.</span>join(\n<span class=\"ansi-green-fg ansi-bold\">      7</span>             process_tweet(x))<span style=\"color:rgb(98,98,98)\">.</span>encode(<span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">ascii</span><span style=\"color:rgb(175,0,0)\">'</span>, <span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">ignore</span><span style=\"color:rgb(175,0,0)\">'</span>)))\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'logprior' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n# Part 6: Predict with your own tweet\n\nIn this part you can predict the sentiment of your own tweet.\n\n::: {#1ab6d995 .cell execution_count=23}\n``` {.python .cell-code}\n# Test with your own tweet - feel free to modify `my_tweet`\nmy_tweet = 'I am happy because I am learning :)'\n\np = naive_bayes_predict(my_tweet, logprior, loglikelihood)\nprint(p)\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[22], line 4</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Test with your own tweet - feel free to modify `my_tweet`</span>\n<span class=\"ansi-green-fg ansi-bold\">      2</span> my_tweet <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">I am happy because I am learning :)</span><span style=\"color:rgb(175,0,0)\">'</span>\n<span class=\"ansi-green-fg\">----&gt; 4</span> p <span style=\"color:rgb(98,98,98)\">=</span> naive_bayes_predict(my_tweet, <span class=\"ansi-yellow-bg\">logprior</span>, loglikelihood)\n<span class=\"ansi-green-fg ansi-bold\">      5</span> <span style=\"color:rgb(0,135,0)\">print</span>(p)\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'logprior' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\nCongratulations on completing this assignment. See you next week!\n\n",
    "supporting": [
      "assignment_files"
    ],
    "filters": [],
    "includes": {}
  }
}