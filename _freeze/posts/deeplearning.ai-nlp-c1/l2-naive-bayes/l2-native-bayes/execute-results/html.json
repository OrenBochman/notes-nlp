{
  "hash": "91cadf4a243a6b57d6e7e3e58d96cb85",
  "result": {
    "engine": "jupyter",
    "markdown": "---\nlayout: post\ntitle: Classification & Vector Spaces - Probability and Bayes’ Rule\nsubtitle: Course 1 of NLP Specialization\ndescription: \"Concepts, code snippets, and slide commentaries for this week's lesson of the  Course notes from the deeplearning.ai natural language programming specialization.\"\ndate: 2020-10-23\ncategories: \n  - NLP \n  - Coursera \n  - notes\n  - deeplearning.ai\n  - course notes\n  - Conditional Probability\n  - Bayes rule\n  - Naïve Bayes\n  - Laplace smoothing\n  - Log-likelihood\n  - classification \n  - sentiment analysis task\n  - bibliography\nlastmod: 2021-04-01T11:13:20.956Z\nauthor: Oren Bochman\nimage: course-banner.jpg\nfig-caption: Notes about ... Attention Models\n---\n\n\n\nSince I majored in Mathematics, I glossed over many details when I took the initial notes, since the course caters to all levels of students. When I migrated the notes from OneNotes to the web, I updated and reviewed the current course material and at times additional notes by [@Chadha2020NLP] and [@jelliti2020nlp]. I have tried to add my own insights from other sources, books I read or others courses I have taken. \n::: callout-note\n\n# Naïve Bayes\n**Naïve Bayes** is a probabilistic algorithm commonly used in *machine learning* for *classification* problems. It's based on *Bayes' theorem*, which is a fundamental concept in *probability theory*. Naïve Bayes assumes that all the features of the input data are independent of each other, which is why it's called \"naïve.\"\n:::\nThe following two results are due to [@NIPS2001_7b7a53e2] by way of [Naive_Bayes_classifier ](https://en.wikipedia.org/wiki/Naive_Bayes_classifier#cite_note-pair-17) wikipedia article.\nAnother detail that can help you make sense of this lesson is the following result relating Naïve Bayes to Logistic Regression which we covered last week.\nIn the case of discrete inputs like indicator or frequency features for discrete events, naive Bayes classifiers form a generative-discriminative pair with multi-nomial logistic regression classifiers: each naive Bayes classifier can be considered a way of fitting a probability model that optimizes the joint likelihood $p (C,x)$, while logistic regression fits the same probability model to optimize the conditional $p(C∣x)$.\n\n::: callout-note\n\n## Theorem---Naive Bayes classifiers on binary features are subsumed by logistic regression classifiers.\n\n### Proof\n\nConsider a generic multi-class classification problem, with possible classes ${\\displaystyle Y \\in \\{1,\\ldots, n\\}}$ , then the non-naive Bayes classifier gives, by Bayes theorem:\n\n$$\np(Y\\mid X=x) = {\\text{softmax}}(\\{\\ln p(Y=k)+\\ln p(X=x\\mid Y=k)\\}_{k})\n$$\n\nThe naive Bayes classifier gives\n\n$$\n{ {\\text{softmax}}\\left(\\left\\{\\ln p(Y=k)+{\\frac {1}{2}}\\sum _{i}(a_{i,k}^{+}-a_{i,k}^{-})x_{i}+(a_{i,k}^{+}+a_{i,k}^{-})\\right\\}_{k}\\right)}\n$$\n\nwhere\n\n$$\na_{i,s}^{+}=\\ln p(X_{i}=+1\\mid Y=s);\\quad a_{i,s}^{-}=\\ln p(X_{i}=-1\\mid Y=s)\n$$\nThis is exactly a logistic regression classifier. \n\n:::\n\n# Introduction\n\n![our corpus](slide_020.png){.column-margin}\n\nWe start with a corpus of 20 tweets that we want to categorize as having either **positive** `+` or **negative** `-` sentiment, but not both.\n\n::: callout-caution\n\n# Research questions\n-   How can we model our corpus using probability theory?\n-   How can we infer the sentiment of a tweet based on our corpus\n:::\nSince we can use the `sum rule`, `product rule` and `Bayes rule` which we shall cover shortly to manipulate probabilities we start by representing what we know about our corpus using probabilities.\n\n## Probability of a randomly selected tweet's sentiment\n\n-   To calculate a `probability` of a certain event happening, you take the count of that specific event and divide it by the sum of all events.\n-   Furthermore, the sum of all probabilities has to equal 1.\nIf we pick a tweet at random, what is the probability of it being `+`? We define an event `A:` \"A tweet is positive\" and calculate its probability\n\n$$\nP(A) = P(+) = \\frac{N_{+}}{N}=\\frac{13}{20}=0.65 \n$$\n\nAnd since probabilities add up to one:\n\n$$\nP(-) = 1- P(+)=0.35 \n$$\n\n## Probabiliy for a specific word's sentiment\n\nWithin that corpus, the word `happy` is sometimes labeled `+` and in other cases, `-`. This indicates that some negative tweets contain the word `happy`.\nShown below is a graphical representation of this \"overlap\". Let's explore how we may represent this graphically using a [venn diagram](https://en.wikipedia.org/wiki/Venn_diagram) and then derive a probability-based representation.\n\n![Tweets with \"Happy\"](slide_021.png){.column-margin}\n\nFirst, we need to estimate the probability of the event B: \"tweets containing the word happy\"\n\n$$\nP(B) = P(Happy)=\\frac{N_{happy}}{N}=\\frac{4}{20}=0.2\n$$\n\n![Venn diagram for defining probabilities from events](slide_026.png){.column-margin}\n\nTo compute the probability of 2 events happening like `happy` **and** `+` in the picture you would be looking at the intersection, or overlap of the two events, In this case, the red and the blue boxes overlap in three boxes, So the answer is:\n$$\nP(A \\cap B) = P(A,B) = \\frac{2}{20}\n$$\n\nThe Event \"A is labeled `+`\", - The probability of events A shown as P(A) is calculated as the ratio between the count of positive tweets and the corpus divided by the total number of tweets in the corpus.\n\n::: {.column-margin layout-ncol=\"1\"}\n![counting the intersection](slide_027.png)\nspecific tweets color coded per the Venn diagram\n:::\n\n::: callout-note\n# Definition of conditional probability\n\n**Conditional probability** is the *probability of an outcome `B` when we already know for certain that an event `A` has already happened*.\nNotation:\n$$\nP(B|A)\n$$\n:::\n\n- and there more + than - more specifically our prior knowledge is that :\n$$\n    \\frac{P(+)}{P(−)}=\\frac{13}{7}\n$$\n\n- the likelihood of a tweet with happy being + is\n- the challenge arises from some words being in both + and - tweets\nConditional probabilities help us reduce the sample search space by restricting it to a specific event which is a given. We should understand the difference between $P(A|B)$ and $P(B|A)$\n\n## what is $P(+|happy)$\n\n-   We start with the Venn diagram for the $P(A|B)$. ![Venn diagram for \\$P(A\\|B)\\$](slide_028.png){.column-margin}\n-   Where we restricted the diagram to just A the subset of happy tweets.\n-   And we just want those tweets that are also `+` i.e. (B).\n-   all we need is to plug in the counts from our count chart. ![Counts for \\$P(A\\|B)\\$](slide_029.png){.column-margin}\n-   which we now estimate\n$$\nP(A|B) = P(Positive|happy) = \\frac{3}{4} = 0.75\n$$\n\n## what is $P(happy|+)$\n-   We start with the Venn diagram for the $P(B|A)$\n-   where we have restricted the diagram to just B the subset of `+` tweets. ![Venn diagram for the \\$P(B\\|A)\\$](slide_030.png){.column-margin}\n-   and we just want from those the tweets that are also `happy` i.e. (A).\n-   and the counts for $P(B|A)$ ![Counts for \\$P(B\\|A)\\$](slide_031.png){.column-margin}\n-   which we now estimate\n$$\nP(B|A) = P(happy|Positive) = \\frac{3}{13} = 0.231\n$$\n\n# Bayes' rule\n\n![Venn diagram for defining probabilities from events](slide_026.png){.column-margin}\n\nFrom this, we can now write:\n\n$$\nP(+|happy) = \\frac{P(+ \\cap happy) }{P(happy)}\n$$\n\nand \n\n$$\nP(happy|+) = \\frac{P(happy \\cap +) }{P(+)}\n$$\n\nwe can combine these since the intersections are the same\nand we get\n\n$$\nP(+|happy) = \\frac{P(+ \\cap happy) }{P(happy)} =  \\frac{P(happy|+) \\times P(+) }{P(happy)}\n$$\n\nwhich generalizes to:\n\n$$\nP(X|Y) = \\frac{P(Y|X) \\times P(X) }{P(Y)}\n$$\n\nwhich we call Bayes rule\n\n::: callout-note\n## Bayes Rule\n\n**Bayes Rule** is the rule for inverting conditional probabilities.\n:::\n\nHowever, we gain a deeper insight by considering that Bayes's rule is more than just a tool for inverting conditional probabilities but the basis of a casual framework for updating our beliefs as we uncover new evidence.\n\n$$ \np(H|e)=\\frac{P(e|H) \\times P(H)}{P(e|H)+P(e|\\bar H)}  = \\frac{P(e|H) \\times P(H)}{P(e)} \n$$\n\nto reflect the notion of updating using new data.\n\nwhere we call:\n\n-   $p(H|e)$ the posterior\n-   $P(H)$ the prior\n-   $P(e|H)$ the likelihood (of evidence given the Hypothesis is true).\n-   $P(e)$ the marginal\n\n## Naïve Bayes Introduction\n\nHere is a sample corpus\n\n::: {#table-tweets }\n|[+]{ style=\"color:blue\"} tweets| [-]{ style=\"color:red\"} tweets |\n|---------------|-------------------|\n|[I am happy because I am learning NLP]{ style=\"color:blue\"} |[I am sad, I am not learning NLP]{ style=\"color:red\"} |\n|[I am happy]{ style=\"color:blue\"} |[I am sad]{ style=\"color:red\"} |\nAnd these are the class frequencies and probabilities\nTable of tweets\n:::\n\n::: {#9ece4717 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport string \nraw_tweets=[\n  \"I am happy because I am learning NLP\",\n  \"I am sad, I am not learning NLP\",\n  \"I am happy, not sad\",\n  \"I am sad, not happy\",\n]\ndef clean(tweet:str):\n  return  tweet.translate(str.maketrans('', '', string.punctuation)).lower()\ntweets = [clean(tweet) for tweet in raw_tweets]\nlabels=['+','-','+','-']\ndf = pd.DataFrame({'tweets': tweets, 'labels': labels})\ndf\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweets</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>i am happy because i am learning nlp</td>\n      <td>+</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>i am sad i am not learning nlp</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>i am happy not sad</td>\n      <td>+</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>i am sad not happy</td>\n      <td>-</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#95cb10d0 .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nfrom collections import Counter\np_freq,n_freq = Counter(), Counter()\n#print( df[df.labels == '+']['tweets'].to_list())\n[p_freq.update(tweet.split()) for tweet in df[df.labels == '+']['tweets'].to_list()]\n[n_freq.update(tweet.split()) for tweet in df[df.labels == '-']['tweets'].to_list()]\nprint(p_freq)\nprint(n_freq)\nvocab = list(set(p_freq.keys()).union(set(n_freq.keys())))\npos_freq = [p_freq[word] for word in vocab ]\nneg_freq = [n_freq[word] for word in vocab ]\nvocab_df=pd.DataFrame({'vocab':vocab,'pos_freq':pos_freq,'neg_freq':neg_freq})\nvocab_df['p_pos']=vocab_df.pos_freq/vocab_df.pos_freq.sum()\nvocab_df['p_neg']=vocab_df.neg_freq/vocab_df.neg_freq.sum()\nvocab_df['p_pos_sm']=(vocab_df.pos_freq+1)/(vocab_df.pos_freq.sum()+vocab_df.shape[1])\nvocab_df['p_neg_sm']=(vocab_df.neg_freq+1)/(vocab_df.neg_freq.sum()+vocab_df.shape[1])\nvocab_df['ratio']= vocab_df.p_pos_sm/vocab_df.p_neg_sm\nvocab_df['lambda']= np.log(vocab_df.p_pos_sm/vocab_df.p_neg_sm)\npd.set_option('display.float_format', '{:.2f}'.format)\nvocab_df\nprint(vocab_df.shape)\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\n[None, None]\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\n[None, None]\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nCounter({'i': 3, 'am': 3, 'happy': 2, 'because': 1, 'learning': 1, 'nlp': 1, 'not': 1, 'sad': 1})\nCounter({'i': 3, 'am': 3, 'sad': 2, 'not': 2, 'learning': 1, 'nlp': 1, 'happy': 1})\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>vocab</th>\n      <th>pos_freq</th>\n      <th>neg_freq</th>\n      <th>p_pos</th>\n      <th>p_neg</th>\n      <th>p_pos_sm</th>\n      <th>p_neg_sm</th>\n      <th>ratio</th>\n      <th>lambda</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>because</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.08</td>\n      <td>0.00</td>\n      <td>0.11</td>\n      <td>0.05</td>\n      <td>2.11</td>\n      <td>0.75</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>not</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0.08</td>\n      <td>0.15</td>\n      <td>0.11</td>\n      <td>0.16</td>\n      <td>0.70</td>\n      <td>-0.35</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>am</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0.23</td>\n      <td>0.23</td>\n      <td>0.22</td>\n      <td>0.21</td>\n      <td>1.06</td>\n      <td>0.05</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sad</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0.08</td>\n      <td>0.15</td>\n      <td>0.11</td>\n      <td>0.16</td>\n      <td>0.70</td>\n      <td>-0.35</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>happy</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0.15</td>\n      <td>0.08</td>\n      <td>0.17</td>\n      <td>0.11</td>\n      <td>1.58</td>\n      <td>0.46</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>i</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0.23</td>\n      <td>0.23</td>\n      <td>0.22</td>\n      <td>0.21</td>\n      <td>1.06</td>\n      <td>0.05</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>learning</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.08</td>\n      <td>0.08</td>\n      <td>0.11</td>\n      <td>0.11</td>\n      <td>1.06</td>\n      <td>0.05</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>nlp</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.08</td>\n      <td>0.08</td>\n      <td>0.11</td>\n      <td>0.11</td>\n      <td>1.06</td>\n      <td>0.05</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(8, 9)\n```\n:::\n:::\n\n\n::: {#tbl-planet-measures .cell tbl-cap='Planets' execution_count=3}\n``` {.python .cell-code}\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\ntable = [[\"Sun\",696000,1989100000],\n         [\"Earth\",6371,5973.6],\n         [\"Moon\",1737,73.5],\n         [\"Mars\",3390,641.85]]\nMarkdown(tabulate(\n  table, \n  headers=[\"Planet\",\"R (km)\", \"mass (x 10^29 kg)\"]\n))\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown execution_count=3}\nPlanet      R (km)    mass (x 10^29 kg)\n--------  --------  -------------------\nSun         696000           1.9891e+09\nEarth         6371        5973.6\nMoon          1737          73.5\nMars          3390         641.85\n:::\n:::\n\n\n::: {#tbl-vocab layout-ncol=2 }\n\n| word     |  +   |  -   |\n|----------|:----:|:----:|\n| I        |  3   |  3   |\n| am       |  3   |  3   |\n| happy    |  2   |  1   |\n| because  |  1   |  0   |\n| learning |  1   |  1   |\n| NLP      |  1   |  1   |\n| sad      |  1   |  2   |\n| not      |  1   |  2   |\n| Nclass   |  13  | 12   | \n: Frequency Table {#tbl-first}\n| word     |  +   |  -   |\n|----------|:----:|:----:|\n| I        | 0.24 | 0.25 |\n| am       | 0.24 | 0.25 |\n| happy    | 0.15 | 0.08 |\n| because  | 0.08 | 0.00 |\n| learning | 0.08 | 0.08 |\n| NLP      | 0.08 | 0.08 |\n| sad      | 0.08 | 0.17 |\n| not      | 0.08 | 0.17 |\n: $P(w_i|class)$ table {#tbl-second}\nProbabilities\n:::\n\nLet's motivate the Naïve Bayes inference condition rule for binary classification:\n\nTo build a classifier, we will first start by creating conditional probabilities given the table;\n\n![Naïve Bayes](slide_032.png){.column-margin}\n\n-   We want to find if given our prior knowledge of $P(+)$ and $P(-)$ if a new tweet has + or - sentiment.\n-   To do that we will estimate $p(+|T)$ and $p(-|T)$ and then decide based on which is greater than 0.5.\n\n![Table of probabilities](slide_033.png){.column-margin}\n\nWe can use the Bayes rule:\n\n$$\np(+|T) = \\frac{ p(T|+) \\times p(+) }{ p(T) } \n$$\n\nand\n\n$$\np(-|T) = \\frac{ p(T|-) \\times p(-) }{ p(T) } \n$$\n\nwhere:\n\n-   $p(+|T)$ is the **posterior probability** of a label `+` given tweet `T`\n-   $p(+)$ is our **prior** knowledge\n-   $p(T|+)$ is the **likelihood** of tweet T being `+`.\n-   ${p(T)}$\n\nThe term $p(T)$ is in both terms and can be eliminated. However, it will cancel out when we use the ratio for the inference.\nThis lets us compute the following table of probabilities; word am learning NLP Pos 0.24 0.08 0.08 Neg 0.25 0.08 0.08 .17\nNaïve Bayes is the simplest probabilistic graphical model which comes with an independence assumption for the features. \n\n$$\np(T|+) = \\prod^m_{i=1}P(w_i|+) \\implies p(+|T)=\\frac{P(+)}{P(T)} \\prod^m_{i=1}P(w_i|+)\n$$\n\nand\n\n$$\np(T|−) = \\prod^m_{i=1}P(w_i|−) \\implies p(−|T) =  \\frac{P(−)}{P(T)} \\prod^m_{i=1} P(w_i|−)\n$$\n\nOnce you have the probabilities, you can compute the likelihood score as follows:\n\n**Tweet**: [I am happy today: I am learning]{style=\"color: blue;\"}.\n - Since there is no entry for today in our conditional probabilities table, this implies that this word is not in your vocabulary. So we’ll ignore its contribution to the overall score.\n- All the neutral words in the tweet such as I and am cancel out in the expression, as shown in the figure below.\n\n$$\n   \\prod^m_{i=1} \\frac{P(w_i|+)}{P(w_i|-)}= \\frac {0.14}{0.10} =1.4 > 1\n$$\n\n-  A score greater than 1 indicates that the class is positive, otherwise, it is negative.\n\n$$\n P(+|T) > P(−|T)\n$$ \n\nthen we infer that the T has + sentiment.\ndividing by the right term we get the inference rule:\n\n$$\n\\frac{P(+|T)}{P(−|T)} > 1 \n$$\nwhich expands to :\n$$\n  \\frac {P(+|T)}{P(−|T)} = \\frac {P(+)}{P(-)}\\prod^m_{i=1} \\frac {P(w_i|+)}{P(w_i|−)} > 1\n$$\n\nThis is the inference rule for naïve Bayes.\n\nNote: Naïve Bayes is a model which assumes all features are independent, so the basic component here is:\n\n$$\n\\frac{P(w_i|+)}{P(w_i|-)} > 1\n$$\nthe ratio of the probability that a word appears in a positive tweet and that it appears in a negative tweet \n\n# Laplace smoothing\n\n::: callout-tip\n## Smoothing\nA probability distribution can take zero values at certain points i.e. for certain inputs. `Smoothing` is a family of techniques for improving our estimate of conditional class probabilities to estimate probabilities for missing words. The name smoothing comes from the fact that these techniques tend to make distributions more uniform, by adjusting low probabilities such as zero probabilities upward, and high probabilities downward. Not only do smoothing methods generally prevent zero probabilities, but they also attempt to improve the accuracy of the model as a whole. Whenever a probability is estimated from a few counts, smoothing has the potential to improve estimation.\n:::\n\n![Laplace smoothing](slide_076.png){.column-margin}\n\nThe course introduces smoothing here since sparsity breaks the naive Bayes model. But smoothing and filtering are big topics in NLP and Data Science. I have added some extra info drawn from [@jurafsky2000speech]\n`Laplace smoothing` also called `add one smoothing` replaces a distribution with zero probabilities (due to sparse data) with a distribution that steals some mass and spreads it over the points which were zero. It solves the data sparsity issue but `Laplace smoothing` will skew/bias the probabilities (it affects rare and common probabilities differently) giving you behavior that is hard to explain, as it assigns too much mass to unknown words.\nWhen we compute the conditional probability $P(w|class)$ using:\n\n$$\nP(w_i|class) = \\frac{freq(w_i,class)}{N_{class}} \\qquad class \\in \\{ +, -\\}\n$$\n\nIf a word does not appear in the training corpus or is missing from one of the classes then its frequency is 0 so it gets a probability of 0.\nSince we are taking products of probabilities, and soon we will take logs of probabilities and zeros present us with a numerical problem that we can address using `smoothing` as follows:\n\n$$\nP(w_i|class) = \\frac{freq(w_i,class) + 1}{N_{class} + |V|} \n$$\n\nwhere:\n\n- $N_{class}$ is the frequency of all words in a class.\n- $V$ is unique words in the vocabulary\n\nNote: that we added a 1 in the numerator and since there are $|V|$ words to normalize we add $|V|$ in the denominator so that all the probabilities sum up to 1.\n\n::: {#tbl-vocab layout-ncol=2 }\n\n| word     |  +   |  -   |\n|----------|:----:|:----:|\n| I        | 0.24 | 0.25 |\n| am       | 0.24 | 0.25 |\n| happy    | 0.15 | 0.08 |\n| because  | 0.08 | [0.00]{ style='color:red'} |\n| learning | 0.08 | 0.08 |\n| NLP      | 0.08 | 0.08 |\n| sad      | 0.08 | 0.17 |\n| not      | 0.08 | 0.17 |\n\n: $P(w_i|class)$ no smoothing {#tbl-unsmoothed}\n\n| word     |  +   |  -   |\n|----------|:----:|:----:|\n| I        | 0.20 | 0.20 |\n| am       | 0.20 | 0.20 |\n| happy    | 0.14 | 0.10 |\n| because  | 0.10 | [0.05]{ style='color:blue'} |\n| learning | 0.10 | 0.10 |\n| NLP      | 0.10 | 0.10 |\n| sad      | 0.10 | 0.15 |\n| not      | 0.10 | 0.15 |\n\n: $P(w_i|class)$ with smoothing {#tbl-smoothing}\nProbabilities\n:::\n \n## Additive smoothing:\n\n$$\np_{addative}(w_i|class)=\\frac{ freq(w,class)+\\delta}{ N_{class} + \\delta \\times V}\n$$\n\n## More alternatives to Laplacian smoothing\n\n![Good Turing smoothing](slide_035.png){.column-margin}\n\n- `Kneser-Ney smoothing` [@NEY19941] which corrects better for smaller data sets. ![Kneser-Ney smoothing](slide_034.png){.column-margin}\n- `Good-Turing smoothing` [@good1953population] which uses order statistics to give even better estimates. \n\nwith a survey of the subject here: [@chen1996empirical](https://dash.harvard.edu/bitstream/handle/1/25104739/tr-10-98.pdf;jsessionid=CC44A707FD117F63DB2870EA0CE00370?sequence=1)\n\n# Ratio of probabilities\n\n$$\nratio(w_i)=\\frac{P(w_i|+)}{P(w_i|-)} \\approx \\frac{freq(w_i|+)+1}{freq(w_i|-)+1}\n$$\n\n# Log Likelihood\n\nrecall that \n$$\n  \\frac {P(+|T)}{P(−|T)} = \\frac {P(+)}{P(-)}\\prod^m_{i=1} \\frac {P(w_i|+)}{P(w_i|−)} > 1\n$$\n\ntaking logs we get\n\n$$\n  \\log \\frac{P(+|T)}{P(−|T)} = \\log \\frac {P(+)}{P(-)} + \\sum^m_{i=1} \\lambda(w_i) > 0 \n$$\n\nwhere \n\n$$\n \\lambda(w_i) = log \\frac {P(w_i|+)}{P(w_i|−)} \n$$\n\nTo compute the likelihood, we need to get the ratios and use them to compute a score that Will allow us to decide if a tweet is positive or negative. \n\nThe higher the ratio, the more positive the word.\n\n- Long Products of small probabilities create a risk of numeric underflow.\n- logs let us mitigate this risk.\n\n$$\n\\lambda(w) = \\log \\frac{p(w|+)}{p(w|-)}\n$$ \n\n$$\n\\log prior = \\log \\frac{p(+)}{p(-)}\n$$ \n\nWhere $D_{+}$ and $D_{-}$ correspond to the number Of negative documents respectively.\n\n# Training Naïve Bayes\n\nTo train a naïve Bayes classifier, we should perform the following steps:\n\n1.  Get or annotate a dataset with positive and negative tweets\n2.  preprocess the tweets. ![Training](slide_045.jpg){.column-margin}\n    -   Tokenize sentences\n    -   Remove punctuation, URLs and names\n    -   Remove stop words\n    -   Stem words    \n3.  Compute the vocaulary: freq(word, class) ![Training](slide_046.jpg){.column-margin}\n4.  Use Laplacian smoothing to estimate word class probabilites $P(w|+)$ and $P(w|-)$. ![Training](slide_047.jpg){.column-margin}\n5.  Compute $\\lambda(w) = \\log \\frac{p(w|+)}{p(w|-)}$ ![Training](slide_048.jpg){.column-margin}\n6.  Compute $logprior = \\log \\frac{p(w|+)}{p(w|-)}$ Where $D_{+}$ and $D_{-}$ correspond to the number Of negative documents respectively.\n\n# Testing Naïve Bayes \n\n![Inference](slide_050.jpg){.column-margin}\n\nLet's work on applying the Naïve Bayes classifier on validation examples to compute the model's accuracy. The steps involved in testing a Naïve Bayes model for sentiment analysis are as follows:\n\n1. Use the validation set to test the model on tweets it has not seen. Which is comprised of a set of raw tweets $X_{val}$, and their corresponding sentiments, $Y_{val}$.\n1. We use the conditional $p(word|state)$ and use them to predict the sentiments of new unseen tweets, \n2. We apply **Pre-processing**: as before in training.\n3. Lookup the $\\lambda$ score for each unique word: Using the $\\lambda$ table (i.e., the log-likelihood table).\n  - Words that have entries in the table, are summed over all the corresponding $\\lambda$ terms.\n  - **Unknown words are skipped** as words that lack a log-likelihood in the table are considered neutral.\n4. **Obtain the overall score** by summing up the scores of all the individual words, along with with our estimation of the log prior (important for an unbalanced dataset), we get the overall sentiment score of the new tweet.\n5. **Check against the threshold** we check if the sentiment score \\>0 .\nLet's consider an example tweet, `\"I passed the NLP interview\"`, and use our trained model to predict if this is a positive or negative tweet:\n-   Look up each word from the vector in your log-likelihood table. Words such as \"I\", \"pass\", \"the\", and \"NLP\", have entries in the table, while the word interview does not (which implies that it needs to be ignored). Now, add the log before accounting for the imbalance of classes in the dataset. Thus, the overall score sums up to 0.48, as shown in the figure below.\n-   Recall that if the overall score of the tweet is larger than 0, then this tweet has a positive sentiment, so the overall prediction is that this tweet has a positive sentiment. Even in real life, passing the NLP interview is a very positive thing.\n\n# Naïve Bayes Applications\n\nThere are many applications of naïve Bayes including:\n\n- Author identification\n- Spam filtering\n- Information retrieval\n- Word disambiguation\n- This method is usually used as a simple baseline. \n- It is also really fast.\n\n# Naïve Bayes Assumptions\n\nNaïve Bayes makes the **independence** assumption and is affected by the word frequencies in the corpus. \nFor example, if you had the following `\"It is sunny and hot in the Sahara desert.\"` \\`\"It's always cold and snowy in ...\"`\nIn the first image, you can see the word sunny and hot tend to depend on each other and are correlated to a certain extent with the\"desert\", Naïve Independence throughout, \n\nFurthermore, if you were to fill in the sentence on the right. this naïve model will assign equal weight to the words :\n\n- spring. \n- summer, \n- fall,\n- winter,\n\nRelative frequencies in the corpus On Twitter, there are usually more positive tweets than negative ones However, some clean datasets are artificially balanced to have the same amount of positive and negative tweets. Just keep in mind, that in me real world. the data could be much noisier.\n\n\n## Sources of Errors in Naïve Bayes\n\n### Error Analysis\n\nBad sentiment classifications are due to:\n\n1. preprocessing dropping punctuation that encodes emotion like a `sad smiley`.\n1. Word order can contribute to meaning - breaking the independence assumption of our model\n1. Pronouns removed as stop words - may encode emotion\n1. Sarcasm can confound the model\n1. Euphemisms are also a challenge\n\n",
    "supporting": [
      "l2-native-bayes_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}