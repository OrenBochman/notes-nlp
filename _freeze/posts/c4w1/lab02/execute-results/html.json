{
  "hash": "956b1e35a6705eb79b65dfd240c121d4",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'Calculating the Bilingual Evaluation Understudy (BLEU) score: Ungraded Lab'\njupyter: python3\n---\n\n\n\n\n\n\n\nIn this ungraded lab, we will implement a popular metric for evaluating the quality of machine-translated text: the BLEU score proposed by Kishore Papineni, et al. In their 2002 paper [\"BLEU: a Method for Automatic Evaluation of Machine Translation\"](https://www.aclweb.org/anthology/P02-1040.pdf), the BLEU score works by comparing \"candidate\" text to one or more \"reference\" translations. The result is better the closer the score is to 1. Let's see how to get this value in the following sections.\n\n# Part 1:  BLEU Score\n\n## 1.1  Importing the Libraries\n\nWe will first start by importing the Python libraries we will use in the first part of this lab. For learning, we will implement our own version of the BLEU Score using Numpy. To verify that our implementation is correct, we will compare our results with those generated by the [SacreBLEU library](https://github.com/mjpost/sacrebleu). This package provides hassle-free computation of shareable, comparable, and reproducible BLEU scores. It also knows all the standard test sets and handles downloading, processing, and tokenization.\n\n::: {#75020328 .cell tags='[]' execution_count=1}\n``` {.python .cell-code}\nimport numpy as np                  # import numpy to make numerical computations.\nimport nltk                         # import NLTK to handle simple NL tasks like tokenization.\nnltk.download(\"punkt\")\nfrom nltk.util import ngrams\nfrom collections import Counter     # import the Counter module.\n!pip3 install 'sacrebleu'           # install the sacrebleu package.\nimport sacrebleu                    # import sacrebleu in order compute the BLEU score.\nimport matplotlib.pyplot as plt     # import pyplot in order to make some illustrations.\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n[nltk_data] Downloading package punkt to /home/oren/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\nTrue\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nRequirement already satisfied: sacrebleu in /home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages (2.5.1)\r\nRequirement already satisfied: colorama in /home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\r\nRequirement already satisfied: portalocker in /home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages (from sacrebleu) (3.1.1)\r\nRequirement already satisfied: regex in /home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages (from sacrebleu) (2024.11.6)\r\nRequirement already satisfied: lxml in /home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages (from sacrebleu) (5.3.0)\r\nRequirement already satisfied: tabulate>=0.8.9 in /home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\r\nRequirement already satisfied: numpy>=1.17 in /home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages (from sacrebleu) (2.0.2)\r\n```\n:::\n:::\n\n\n## 1.2  Defining the BLEU Score\n\nYou have seen the formula for calculating the BLEU score in this week's lectures. More formally, we can express the BLEU score as:\n\n$$BLEU = BP\\Bigl(\\prod_{i=1}^{4}precision_i\\Bigr)^{(1/4)}$$\n\nwith the Brevity Penalty and precision defined as:\n\n$$BP = min\\Bigl(1, e^{(1-({ref}/{cand}))}\\Bigr)$$\n\n$$precision_i = \\frac {\\sum_{snt \\in{cand}}\\sum_{i\\in{snt}}min\\Bigl(m^{i}_{cand}, m^{i}_{ref}\\Bigr)}{w^{i}_{t}}$$\n\nwhere:\n\n* $m^{i}_{cand}$, is the count of i-gram in candidate matching the reference translation.\n* $m^{i}_{ref}$, is the count of i-gram in the reference translation.\n* $w^{i}_{t}$, is the total number of i-grams in candidate translation.\n\n## 1.3 Explaining the BLEU score\n\n### Brevity Penalty (example):\n\n::: {#39745cfe .cell execution_count=2}\n``` {.python .cell-code}\nref_length = np.ones(100)\ncan_length = np.linspace(1.5, 0.5, 100)\nx = ref_length / can_length\ny = 1 - x\ny = np.exp(y)\ny = np.minimum(np.ones(y.shape), y)\n\n# Code for in order to make the plot\nfig, ax = plt.subplots(1)\nlines = ax.plot(x, y)\nax.set(\n    xlabel=\"Ratio of the length of the reference to the candidate text\",\n    ylabel=\"Brevity Penalty\",\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\n[Text(0.5, 0, 'Ratio of the length of the reference to the candidate text'),\n Text(0, 0.5, 'Brevity Penalty')]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](lab02_files/figure-html/cell-3-output-2.png){width=589 height=429}\n:::\n:::\n\n\nThe brevity penalty penalizes generated translations that are too short compared to the closest reference length with an exponential decay. The brevity penalty compensates for the fact that the BLEU score has no recall term.\n\n### N-Gram Precision (example):\n\n::: {#be505a71 .cell execution_count=3}\n``` {.python .cell-code}\ndata = {\"1-gram\": 0.8, \"2-gram\": 0.7, \"3-gram\": 0.6, \"4-gram\": 0.5}\nnames = list(data.keys())\nvalues = list(data.values())\n\nfig, ax = plt.subplots(1)\nbars = ax.bar(names, values)\nax.set(ylabel=\"N-gram precision\")\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](lab02_files/figure-html/cell-4-output-1.png){width=589 height=411}\n:::\n:::\n\n\nThe n-gram precision counts how many unigrams, bigrams, trigrams, and four-grams (i=1,...,4) match their n-gram counterpart in the reference translations. This term acts as a precision metric. Unigrams account for adequacy while longer n-grams account for fluency of the translation. To avoid overcounting, the n-gram counts are clipped to the maximal n-gram count occurring in the reference ($m_{n}^{ref}$). Typically precision shows exponential decay with the with the degree of the n-gram.\n\n### N-gram BLEU score (example):\n\n::: {#e7acb0b1 .cell execution_count=4}\n``` {.python .cell-code}\ndata = {\"1-gram\": 0.8, \"2-gram\": 0.77, \"3-gram\": 0.74, \"4-gram\": 0.71}\nnames = list(data.keys())\nvalues = list(data.values())\n\nfig, ax = plt.subplots(1)\nbars = ax.bar(names, values)\nax.set(ylabel=\"Modified N-gram precision\")\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](lab02_files/figure-html/cell-5-output-1.png){width=589 height=411}\n:::\n:::\n\n\nWhen the n-gram precision is multiplied by the BP, then the exponential decay of n-grams is almost fully compensated. The BLEU score corresponds to a geometric average of this modified n-gram precision.\n\n## 1.4 Example Calculations of the BLEU score\n\nIn this example we will have a reference translation and 2 candidates translations. We will tokenize all sentences using the NLTK package introduced in Course 2 of this NLP specialization.\n\n::: {#a3f19160 .cell tags='[]' execution_count=5}\n``` {.python .cell-code}\nreference = \"The NASA Opportunity rover is battling a massive dust storm on planet Mars.\"\ncandidate_1 = \"The Opportunity rover is combating a big sandstorm on planet Mars.\"\ncandidate_2 = \"A NASA rover is fighting a massive storm on planet Mars.\"\n\ntokenized_ref = nltk.word_tokenize(reference.lower())\ntokenized_cand_1 = nltk.word_tokenize(candidate_1.lower())\ntokenized_cand_2 = nltk.word_tokenize(candidate_2.lower())\n\nprint(f\"{reference} -> {tokenized_ref}\")\nprint(\"\\n\")\nprint(f\"{candidate_1} -> {tokenized_cand_1}\")\nprint(\"\\n\")\nprint(f\"{candidate_2} -> {tokenized_cand_2}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe NASA Opportunity rover is battling a massive dust storm on planet Mars. -> ['the', 'nasa', 'opportunity', 'rover', 'is', 'battling', 'a', 'massive', 'dust', 'storm', 'on', 'planet', 'mars', '.']\n\n\nThe Opportunity rover is combating a big sandstorm on planet Mars. -> ['the', 'opportunity', 'rover', 'is', 'combating', 'a', 'big', 'sandstorm', 'on', 'planet', 'mars', '.']\n\n\nA NASA rover is fighting a massive storm on planet Mars. -> ['a', 'nasa', 'rover', 'is', 'fighting', 'a', 'massive', 'storm', 'on', 'planet', 'mars', '.']\n```\n:::\n:::\n\n\n### STEP 1: Computing the Brevity Penalty\n\n::: {#9a93851e .cell execution_count=6}\n``` {.python .cell-code}\ndef brevity_penalty(reference, candidate):\n    ref_length = len(reference)\n    can_length = len(candidate)\n\n    # Brevity Penalty\n    if ref_length > can_length:\n        BP = 1\n    else:\n        penalty = 1 - (ref_length / can_length)\n        BP = np.exp(penalty)\n\n    return BP\n```\n:::\n\n\n### STEP 2: Computing the Precision\n\n::: {#509ec0f6 .cell execution_count=7}\n``` {.python .cell-code}\ndef clipped_precision(reference, candidate):\n    \"\"\"\n    Bleu score function given a original and a machine translated sentences\n    \"\"\"\n\n    clipped_precision_score = []\n\n    for i in range(1, 5):\n        candidate_n_gram = Counter(\n            ngrams(candidate, i)\n        )  # counts of n-gram n=1...4 tokens for the candidate\n        reference_n_gram = Counter(\n            ngrams(reference, i)\n        )  # counts of n-gram n=1...4 tokens for the reference\n\n        c = sum(\n            reference_n_gram.values()\n        )  # sum of the values of the reference the denominator in the precision formula\n\n        for j in reference_n_gram:  # for every n_gram token in the reference\n            if j in candidate_n_gram:  # check if it is in the candidate n-gram\n\n                if (\n                    reference_n_gram[j] > candidate_n_gram[j]\n                ):  # if the count of the reference n-gram is bigger\n                    # than the corresponding count in the candidate n-gram\n                    reference_n_gram[j] = candidate_n_gram[\n                        j\n                    ]  # then set the count of the reference n-gram to be equal\n                    # to the count of the candidate n-gram\n            else:\n\n                reference_n_gram[j] = 0  # else reference n-gram = 0\n\n        clipped_precision_score.append(sum(reference_n_gram.values()) / c)\n\n    weights = [0.25] * 4\n\n    s = (w_i * np.log(p_i) for w_i, p_i in zip(weights, clipped_precision_score))\n    s = np.exp(np.sum(s))\n    return s\n```\n:::\n\n\n### STEP 3: Computing the BLEU score\n\n::: {#c74e1455 .cell execution_count=8}\n``` {.python .cell-code}\ndef bleu_score(reference, candidate):\n    BP = brevity_penalty(reference, candidate)\n    precision = clipped_precision(reference, candidate)\n    return BP * precision\n```\n:::\n\n\n### STEP 4: Testing with our Example Reference and Candidates Sentences\n\n::: {#dc976070 .cell tags='[]' execution_count=9}\n``` {.python .cell-code}\nprint(\n    \"Results reference versus candidate 1 our own code BLEU: \",\n    round(bleu_score(tokenized_ref, tokenized_cand_1) * 100, 1),\n)\nprint(\n    \"Results reference versus candidate 2 our own code BLEU: \",\n    round(bleu_score(tokenized_ref, tokenized_cand_2) * 100, 1),\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nResults reference versus candidate 1 our own code BLEU:  27.4\nResults reference versus candidate 2 our own code BLEU:  35.0\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/tmp/ipykernel_1096692/273199063.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n  s = np.exp(np.sum(s))\n```\n:::\n:::\n\n\n### STEP 5: Comparing the Results from our Code with the SacreBLEU Library\n\n::: {#dd729a98 .cell execution_count=10}\n``` {.python .cell-code}\nprint(\n    \"Results reference versus candidate 1 sacrebleu library sentence BLEU: \",\n    round(sacrebleu.corpus_bleu(reference, candidate_1).score, 1),\n)\nprint(\n    \"Results reference versus candidate 2 sacrebleu library sentence BLEU: \",\n    round(sacrebleu.corpus_bleu(reference, candidate_2).score, 1),\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nResults reference versus candidate 1 sacrebleu library sentence BLEU:  0.0\nResults reference versus candidate 2 sacrebleu library sentence BLEU:  0.0\n```\n:::\n:::\n\n\n# Part 2:  BLEU computation on a corpus\n\n## Loading Data Sets for Evaluation Using the BLEU Score\n\nIn this section, we will show a simple pipeline for evaluating machine translated text. Due to storage and speed constraints, we will not be using our own model in this lab (you'll get to do that in the assignment!). Instead, we will be using [Google Translate](https://translate.google.com) to generate English to German translations and we will evaluate it against a known evaluation set. There are three files we will need:\n\n1. A source text in English. In this lab, we will use the first 1671 words of the [wmt19](http://statmt.org/wmt19/translation-task.html) evaluation dataset downloaded via SacreBLEU. We just grabbed a subset because of limitations in the number of words that can be translated using Google Translate. \n2. A reference translation to German of the corresponding first 1671 words from the original English text. This is also provided by SacreBLEU.\n3. A candidate machine translation to German from the same 1671 words. This is generated by feeding the source text to a machine translation model. As mentioned above, we will use Google Translate to generate the translations in this file.\n\nWith that, we can now compare the reference an candidate translation to get the BLEU Score.\n\n::: {#1ed521ff .cell tags='[]' execution_count=11}\n``` {.python .cell-code}\n# Loading the raw data\nwmt19news_src = open(\"wmt19_src.txt\", \"rU\")\nwmt19news_src_1 = wmt19news_src.read()\nwmt19news_src.close()\nwmt19news_ref = open(\"wmt19_ref.txt\", \"rU\")\nwmt19news_ref_1 = wmt19news_ref.read()\nwmt19news_ref.close()\nwmt19news_can = open(\"wmt19_can.txt\", \"rU\")\nwmt19news_can_1 = wmt19news_can.read()\nwmt19news_can.close()\n\n# Tokenizing the raw data\ntokenized_corpus_src = nltk.word_tokenize(wmt19news_src_1.lower())\ntokenized_corpus_ref = nltk.word_tokenize(wmt19news_ref_1.lower())\ntokenized_corpus_cand = nltk.word_tokenize(wmt19news_can_1.lower())\n```\n:::\n\n\nInspecting the first sentence of the data.\n\n::: {#d563c3f5 .cell tags='[]' execution_count=12}\n``` {.python .cell-code}\nprint(\"English source text:\")\nprint(\"\\n\")\nprint(f\"{wmt19news_src_1[0:170]} -> {tokenized_corpus_src[0:30]}\")\nprint(\"\\n\")\nprint(\"German reference translation:\")\nprint(\"\\n\")\nprint(f\"{wmt19news_ref_1[0:219]} -> {tokenized_corpus_ref[0:35]}\")\nprint(\"\\n\")\nprint(\"German machine translation:\")\nprint(\"\\n\")\nprint(f\"{wmt19news_can_1[0:199]} -> {tokenized_corpus_cand[0:29]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEnglish source text:\n\n\n﻿Welsh AMs worried about 'looking like muppets'\nThere is consternation among some AMs at a suggestion their title should change to MWPs (Member of the Welsh Parliament).\n -> ['\\ufeffwelsh', 'ams', 'worried', 'about', \"'looking\", 'like', 'muppets', \"'\", 'there', 'is', 'consternation', 'among', 'some', 'ams', 'at', 'a', 'suggestion', 'their', 'title', 'should', 'change', 'to', 'mwps', '(', 'member', 'of', 'the', 'welsh', 'parliament', ')']\n\n\nGerman reference translation:\n\n\n﻿Walisische Ageordnete sorgen sich \"wie Dödel auszusehen\"\nEs herrscht Bestürzung unter einigen Mitgliedern der Versammlung über einen Vorschlag, der ihren Titel zu MWPs (Mitglied der walisischen Parlament) ändern soll.\n -> ['\\ufeffwalisische', 'ageordnete', 'sorgen', 'sich', '``', 'wie', 'dödel', 'auszusehen', \"''\", 'es', 'herrscht', 'bestürzung', 'unter', 'einigen', 'mitgliedern', 'der', 'versammlung', 'über', 'einen', 'vorschlag', ',', 'der', 'ihren', 'titel', 'zu', 'mwps', '(', 'mitglied', 'der', 'walisischen', 'parlament', ')', 'ändern', 'soll', '.']\n\n\nGerman machine translation:\n\n\n﻿Walisische AMs machten sich Sorgen, dass sie wie Muppets aussehen könnten\nEinige AMs sind bestürzt über den Vorschlag, ihren Titel in MWPs (Mitglied des walisischen Parlaments) zu ändern.\nEs ist auf -> ['\\ufeffwalisische', 'ams', 'machten', 'sich', 'sorgen', ',', 'dass', 'sie', 'wie', 'muppets', 'aussehen', 'könnten', 'einige', 'ams', 'sind', 'bestürzt', 'über', 'den', 'vorschlag', ',', 'ihren', 'titel', 'in', 'mwps', '(', 'mitglied', 'des', 'walisischen', 'parlaments']\n```\n:::\n:::\n\n\n::: {#5c90c064 .cell tags='[]' execution_count=13}\n``` {.python .cell-code}\nprint(\n    \"Results reference versus candidate 1 our own BLEU implementation: \",\n    round(bleu_score(tokenized_corpus_ref, tokenized_corpus_cand) * 100, 1),\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nResults reference versus candidate 1 our own BLEU implementation:  23.6\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/tmp/ipykernel_1096692/273199063.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n  s = np.exp(np.sum(s))\n```\n:::\n:::\n\n\n::: {#54590c18 .cell tags='[]' execution_count=14}\n``` {.python .cell-code}\nprint(\n    \"Results reference versus candidate 1 sacrebleu library BLEU: \",\n    round(sacrebleu.corpus_bleu(wmt19news_ref_1, wmt19news_can_1).score, 1),\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nResults reference versus candidate 1 sacrebleu library BLEU:  0.0\n```\n:::\n:::\n\n\n**BLEU Score Interpretation on a Corpus**\n\n|Score      | Interpretation                                                |\n|:---------:|:-------------------------------------------------------------:|\n| < 10      | Almost useless                                                |\n| 10 - 19   | Hard to get the gist                                          |\n| 20 - 29   | The gist is clear, but has significant grammatical errors     |\n| 30 - 40   | Understandable to good translations                           |\n| 40 - 50   | High quality translations                                     |\n| 50 - 60   | Very high quality, adequate, and fluent translations          |\n| > 60      | Quality often better than human                               |\n\nFrom the table above (taken [here](https://cloud.google.com/translate/automl/docs/evaluate)), we can see the gist of the translation is clear but has significant grammatical errors. Nonetheless, the results of our coded BLEU score are almost identical to those of the SacreBLEU package.\n\n",
    "supporting": [
      "lab02_files"
    ],
    "filters": [],
    "includes": {}
  }
}