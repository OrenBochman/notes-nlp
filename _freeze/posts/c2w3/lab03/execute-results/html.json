{
  "hash": "5ba103281281ca8778adb3de0b6fe5e1",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ndate: 2020-10-25\ntitle: \"Out of vocabulary words (OOV)\"\nsubtitle: \"Probabilistic Models\"\ncategories: \n  - NLP \n  - Coursera \n  - Notes\n  - Probabilistic Models\njupyter: python3\n---\n\n\n\n\n::: {#fig-00 .column-margin .nolightbox}\n![course banner](/images/Course-Logo-2-3.webp)\n:::\n\n### Vocabulary {#vocabulary}\n\nIn the video about the out of vocabulary words, we saw that the first step in dealing with the unknown words is to decide which words belong to the vocabulary. \n\nIn the code assignment, we will try the method based on minimum frequency - all words appearing in the training set with frequency >= minimum frequency are added to the vocabulary.\n\nHere is a code for the other method, where the target size of the vocabulary is known in advance and the vocabulary is filled with words based on their frequency in the training set.\n\n::: {#80e23b02 .cell execution_count=1}\n``` {.python .cell-code}\n# build the vocabulary from M most frequent words\n# use Counter object from the collections library to find M most common words\nfrom collections import Counter\n\n# the target size of the vocabulary\nM = 3\n\n# pre-calculated word counts\n# Counter could be used to build this dictionary from the source corpus\nword_counts = {'happy': 5, 'because': 3, 'i': 2, 'am': 2, 'learning': 3, '.': 1}\n\nvocabulary = Counter(word_counts).most_common(M)\n\n# remove the frequencies and leave just the words\nvocabulary = [w[0] for w in vocabulary]\n\nprint(f\"the new vocabulary containing {M} most frequent words: {vocabulary}\\n\") \n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nthe new vocabulary containing 3 most frequent words: ['happy', 'because', 'learning']\n\n```\n:::\n:::\n\n\nNow that the vocabulary is ready, we can use it to replace the OOV words with $<UNK>$ as we saw in the lecture.\n\n::: {#ab23e318 .cell execution_count=2}\n``` {.python .cell-code}\n# test if words in the input sentences are in the vocabulary, if OOV, print <UNK>\nsentence = ['am', 'i', 'learning']\noutput_sentence = []\nprint(f\"input sentence: {sentence}\")\n\nfor w in sentence:\n    # test if word w is in vocabulary\n    if w in vocabulary:\n        output_sentence.append(w)\n    else:\n        output_sentence.append('<UNK>')\n        \nprint(f\"output sentence: {output_sentence}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ninput sentence: ['am', 'i', 'learning']\noutput sentence: ['<UNK>', '<UNK>', 'learning']\n```\n:::\n:::\n\n\nWhen building the vocabulary in the code assignment, we will need to know how to iterate through the word counts dictionary. \n\nHere is an example of a similar task showing how to go through all the word counts and print out only the words with the frequency equal to f. \n\n::: {#42182597 .cell execution_count=3}\n``` {.python .cell-code}\n# iterate through all word counts and print words with given frequency f\nf = 3\n\nword_counts = {'happy': 5, 'because': 3, 'i': 2, 'am': 2, 'learning':3, '.': 1}\n\nfor word, freq in word_counts.items():\n    if freq == f:\n        print(word)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nbecause\nlearning\n```\n:::\n:::\n\n\nAs mentioned in the videos, if there are many $<UNK>$ replacements in your train and test set, we may get a very low perplexity even though the model itself wouldn't be very helpful. \n    \nHere is a sample code showing this unwanted effect. \n\n::: {#9db0c6cb .cell execution_count=4}\n``` {.python .cell-code}\n# many <unk> low perplexity \ntraining_set = ['i', 'am', 'happy', 'because','i', 'am', 'learning', '.']\ntraining_set_unk = ['i', 'am', '<UNK>', '<UNK>','i', 'am', '<UNK>', '<UNK>']\n\ntest_set = ['i', 'am', 'learning']\ntest_set_unk = ['i', 'am', '<UNK>']\n\nM = len(test_set)\nprobability = 1\nprobability_unk = 1\n\n# pre-calculated probabilities\nbigram_probabilities = {('i', 'am'): 1.0, ('am', 'happy'): 0.5, ('happy', 'because'): 1.0, ('because', 'i'): 1.0, ('am', 'learning'): 0.5, ('learning', '.'): 1.0}\nbigram_probabilities_unk = {('i', 'am'): 1.0, ('am', '<UNK>'): 1.0, ('<UNK>', '<UNK>'): 0.5, ('<UNK>', 'i'): 0.25}\n\n# got through the test set and calculate its bigram probability\nfor i in range(len(test_set) - 2 + 1):\n    bigram = tuple(test_set[i: i + 2])\n    probability = probability * bigram_probabilities[bigram]\n        \n    bigram_unk = tuple(test_set_unk[i: i + 2])\n    probability_unk = probability_unk * bigram_probabilities_unk[bigram_unk]\n\n# calculate perplexity for both original test set and test set with <UNK>\nperplexity = probability ** (-1 / M)\nperplexity_unk = probability_unk ** (-1 / M)\n\nprint(f\"perplexity for the training set: {perplexity}\")\nprint(f\"perplexity for the training set with <UNK>: {perplexity_unk}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nperplexity for the training set: 1.2599210498948732\nperplexity for the training set with <UNK>: 1.0\n```\n:::\n:::\n\n\n### Smoothing {#smoothing}\n\nAdd-k smoothing was described as a method for smoothing of the probabilities for previously unseen n-grams. \n\nHere is an example code that shows how to implement add-k smoothing but also highlights a disadvantage of this method. The downside is that n-grams not previously seen in the training dataset get too high probability. \n\nIn the code output bellow you'll see that a phrase that is in the training set gets the same probability as an unknown phrase.\n\n::: {#dd9ca267 .cell execution_count=5}\n``` {.python .cell-code}\ndef add_k_smooting_probability(k, vocabulary_size, n_gram_count, n_gram_prefix_count):\n    numerator = n_gram_count + k\n    denominator = n_gram_prefix_count + k * vocabulary_size\n    return numerator / denominator\n\ntrigram_probabilities = {('i', 'am', 'happy') : 2}\nbigram_probabilities = {( 'am', 'happy') : 10}\nvocabulary_size = 5\nk = 1\n\nprobability_known_trigram = add_k_smooting_probability(k, vocabulary_size, trigram_probabilities[('i', 'am', 'happy')], \n                           bigram_probabilities[( 'am', 'happy')])\n\nprobability_unknown_trigram = add_k_smooting_probability(k, vocabulary_size, 0, 0)\n\nprint(f\"probability_known_trigram: {probability_known_trigram}\")\nprint(f\"probability_unknown_trigram: {probability_unknown_trigram}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nprobability_known_trigram: 0.2\nprobability_unknown_trigram: 0.2\n```\n:::\n:::\n\n\n### Back-off {#backoff}\n\nBack-off is a model generalization method that leverages information from lower order n-grams in case information about the high order n-grams is missing. For example, if the probability of an trigram is missing, use bigram information and so on.\n\nHere we can see an example of a simple back-off technique.\n\n::: {#4fba4b0f .cell execution_count=6}\n``` {.python .cell-code}\n# pre-calculated probabilities of all types of n-grams\ntrigram_probabilities = {('i', 'am', 'happy'): 0}\nbigram_probabilities = {( 'am', 'happy'): 0.3}\nunigram_probabilities = {'happy': 0.4}\n\n# this is the input trigram we need to estimate\ntrigram = ('are', 'you', 'happy')\n\n# find the last bigram and unigram of the input\nbigram = trigram[1: 3]\nunigram = trigram[2]\nprint(f\"besides the trigram {trigram} we also use bigram {bigram} and unigram ({unigram})\\n\")\n\n# 0.4 is used as an example, experimentally found for web-scale corpuses when using the \"stupid\" back-off\nlambda_factor = 0.4\nprobability_hat_trigram = 0\n\n# search for first non-zero probability starting with trigram\n# to generalize this for any order of n-gram hierarchy, \n# we could loop through the probability dictionaries instead of if/else cascade\nif trigram not in trigram_probabilities or trigram_probabilities[trigram] == 0:\n    print(f\"probability for trigram {trigram} not found\")\n    \n    if bigram not in bigram_probabilities or bigram_probabilities[bigram] == 0:\n        print(f\"probability for bigram {bigram} not found\")\n        \n        if unigram in unigram_probabilities:\n            print(f\"probability for unigram {unigram} found\\n\")\n            probability_hat_trigram = lambda_factor * lambda_factor * unigram_probabilities[unigram]\n        else:\n            probability_hat_trigram = 0\n    else:\n        probability_hat_trigram = lambda_factor * bigram_probabilities[bigram]\nelse:\n    probability_hat_trigram = trigram_probabilities[trigram]\n\nprint(f\"probability for trigram {trigram} estimated as {probability_hat_trigram}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nbesides the trigram ('are', 'you', 'happy') we also use bigram ('you', 'happy') and unigram (happy)\n\nprobability for trigram ('are', 'you', 'happy') not found\nprobability for bigram ('you', 'happy') not found\nprobability for unigram happy found\n\nprobability for trigram ('are', 'you', 'happy') estimated as 0.06400000000000002\n```\n:::\n:::\n\n\n### Interpolation {#interpolation}\n\nThe other method for using probabilities of lower order n-grams is the interpolation. In this case, we use weighted probabilities of n-grams of all orders every time, not just when high order information is missing. \n\nFor example, we always combine trigram, bigram and unigram probability. We can see how this in the following code snippet.\n\n::: {#d6394f85 .cell execution_count=7}\n``` {.python .cell-code}\n# pre-calculated probabilities of all types of n-grams\ntrigram_probabilities = {('i', 'am', 'happy'): 0.15}\nbigram_probabilities = {( 'am', 'happy'): 0.3}\nunigram_probabilities = {'happy': 0.4}\n\n# the weights come from optimization on a validation set\nlambda_1 = 0.8\nlambda_2 = 0.15\nlambda_3 = 0.05\n\n# this is the input trigram we need to estimate\ntrigram = ('i', 'am', 'happy')\n\n# find the last bigram and unigram of the input\nbigram = trigram[1: 3]\nunigram = trigram[2]\nprint(f\"besides the trigram {trigram} we also use bigram {bigram} and unigram ({unigram})\\n\")\n\n# in the production code, we would need to check if the probability n-gram dictionary contains the n-gram\nprobability_hat_trigram = lambda_1 * trigram_probabilities[trigram] \n+ lambda_2 * bigram_probabilities[bigram]\n+ lambda_3 * unigram_probabilities[unigram]\n\nprint(f\"estimated probability of the input trigram {trigram} is {probability_hat_trigram}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nbesides the trigram ('i', 'am', 'happy') we also use bigram ('am', 'happy') and unigram (happy)\n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n0.045\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n0.020000000000000004\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nestimated probability of the input trigram ('i', 'am', 'happy') is 0.12\n```\n:::\n:::\n\n\nThat's it for week 3, we should be ready now for the code assignment. \n\n",
    "supporting": [
      "lab03_files"
    ],
    "filters": [],
    "includes": {}
  }
}