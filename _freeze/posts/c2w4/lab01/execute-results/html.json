{
  "hash": "e0744f9517dc333ef11c4f896cddc581",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'Word Embeddings First Steps: Data Preparation'\ncategories: \n  - NLP \n  - Coursera \n  - Lab\n  - Probabilistic Models\njupyter: python3\n# execute: \n#     error: true\n---\n\n\n\n\n\n::: {#fig-00 .column-margin}\n![course banner](/images/banner_c2.jpg)\n:::\n\nIn this series of ungraded notebooks, you'll try out all the individual techniques that you learned about in the lectures. Practicing on small examples will prepare you for the graded assignment, where you will combine the techniques in more advanced ways to create word embeddings from a real-life corpus. \n\nThis notebook focuses on data preparation, which is the first step of any machine learning algorithm. It is a very important step because models are only as good as the data they are trained on and the models used require the data to have a particular structure to process it properly.\n\nTo get started, import and initialize all the libraries you will need.\n\n::: {#d6e1ccfc .cell execution_count=1}\n``` {.python .cell-code}\nimport re\nimport nltk\nimport emoji\nimport numpy as np\nfrom nltk.tokenize import word_tokenize\nfrom utils2 import get_dict\n```\n:::\n\n\n# Data preparation\n\nIn the data preparation phase, starting with a corpus of text, you will:\n\n- Clean and tokenize the corpus.\n\n- Extract the pairs of context words and center word that will make up the training data set for the CBOW model. The context words are the features that will be fed into the model, and the center words are the target values that the model will learn to predict.\n\n- Create simple vector representations of the context words (features) and center words (targets) that can be used by the neural network of the CBOW model.\n\n## Cleaning and tokenization\n\nTo demonstrate the cleaning and tokenization process, consider a corpus that contains emojis and various punctuation signs.\n\n::: {#17300f62 .cell execution_count=2}\n``` {.python .cell-code}\n# Define a corpus\ncorpus = 'Who ❤️ \"word embeddings\" in 2020? I do!!!'\n```\n:::\n\n\nFirst, replace all interrupting punctuation signs — such as commas and exclamation marks — with periods.\n\n::: {#c03cfa8d .cell execution_count=3}\n``` {.python .cell-code}\n# Print original corpus\nprint(f'Corpus:  {corpus}')\n\n# Do the substitution\ndata = re.sub(r'[,!?;-]+', '.', corpus)\n\n# Print cleaned corpus\nprint(f'After cleaning punctuation:  {data}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorpus:  Who ❤️ \"word embeddings\" in 2020? I do!!!\nAfter cleaning punctuation:  Who ❤️ \"word embeddings\" in 2020. I do.\n```\n:::\n:::\n\n\nNext, use NLTK's tokenization engine to split the corpus into individual tokens.\n\n::: {#c12fe30c .cell execution_count=4}\n``` {.python .cell-code}\n# Print cleaned corpus\nprint(f'Initial string:  {data}')\n\n# Tokenize the cleaned corpus\ndata = nltk.word_tokenize(data)\n\n# Print the tokenized version of the corpus\nprint(f'After tokenization:  {data}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nInitial string:  Who ❤️ \"word embeddings\" in 2020. I do.\nAfter tokenization:  ['Who', '❤️', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '.']\n```\n:::\n:::\n\n\nFinally, as you saw in the lecture, get rid of numbers and punctuation other than periods, and convert all the remaining tokens to lowercase.\n\n::: {#d7658c1e .cell execution_count=5}\n``` {.python .cell-code}\n# Print the tokenized version of the corpus\nprint(f'Initial list of tokens:  {data}')\n\n# Filter tokenized corpus using list comprehension\ndata = [ ch.lower() for ch in data\n         if ch.isalpha()\n         or ch == '.'\n         or emoji.get_emoji_regexp().search(ch)\n       ]\n\n# Print the tokenized and filtered version of the corpus\nprint(f'After cleaning:  {data}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nInitial list of tokens:  ['Who', '❤️', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '.']\nAfter cleaning:  ['who', '❤️', 'word', 'embeddings', 'in', '.', 'i', 'do', '.']\n```\n:::\n:::\n\n\nNote that the heart emoji is considered as a token just like any normal word.\n\nNow let's streamline the cleaning and tokenization process by wrapping the previous steps in a function.\n\n::: {#8e62e052 .cell execution_count=6}\n``` {.python .cell-code}\n# Define the 'tokenize' function that will include the steps previously seen\ndef tokenize(corpus):\n    data = re.sub(r'[,!?;-]+', '.', corpus)\n    data = nltk.word_tokenize(data)  # tokenize string to words\n    data = [ ch.lower() for ch in data\n             if ch.isalpha()\n             or ch == '.'\n             or emoji.get_emoji_regexp().search(ch)\n           ]\n    return data\n```\n:::\n\n\nApply this function to the corpus that you'll be working on in the rest of this notebook: \"I am happy because I am learning\"\n\n::: {#c9e2fc29 .cell execution_count=7}\n``` {.python .cell-code}\n# Define new corpus\ncorpus = 'I am happy because I am learning'\n\n# Print new corpus\nprint(f'Corpus:  {corpus}')\n\n# Save tokenized version of corpus into 'words' variable\nwords = tokenize(corpus)\n\n# Print the tokenized version of the corpus\nprint(f'Words (tokens):  {words}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorpus:  I am happy because I am learning\nWords (tokens):  ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n```\n:::\n:::\n\n\n**Now try it out yourself with your own sentence.**\n\n::: {#74099031 .cell execution_count=8}\n``` {.python .cell-code}\n# Run this with any sentence\ntokenize(\"Now it's your turn: try with your own sentence!\")\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n['now', 'it', 'your', 'turn', 'try', 'with', 'your', 'own', 'sentence', '.']\n```\n:::\n:::\n\n\n## Sliding window of words\n\nNow that you have transformed the corpus into a list of clean tokens, you can slide a window of words across this list. For each window you can extract a center word and the context words.\n\nThe `get_windows` function in the next cell was introduced in the lecture.\n\n::: {#3d2c8a3b .cell execution_count=9}\n``` {.python .cell-code}\n# Define the 'get_windows' function\ndef get_windows(words, C):\n    i = C\n    while i < len(words) - C:\n        center_word = words[i]\n        context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n        yield context_words, center_word\n        i += 1\n```\n:::\n\n\nThe first argument of this function is a list of words (or tokens). The second argument, `C`, is the context half-size. Recall that for a given center word, the context words are made of `C` words to the left and `C` words to the right of the center word.\n\nHere is how you can use this function to extract context words and center words from a list of tokens. These context and center words will make up the training set that you will use to train the CBOW model.\n\n::: {#118582f8 .cell execution_count=10}\n``` {.python .cell-code}\n# Print 'context_words' and 'center_word' for the new corpus with a 'context half-size' of 2\nfor x, y in get_windows(['i', 'am', 'happy', 'because', 'i', 'am', 'learning'], 2):\n    print(f'{x}\\t{y}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['i', 'am', 'because', 'i']\thappy\n['am', 'happy', 'i', 'am']\tbecause\n['happy', 'because', 'am', 'learning']\ti\n```\n:::\n:::\n\n\nThe first example of the training set is made of:\n\n- the context words \"i\", \"am\", \"because\", \"i\",\n\n- and the center word to be predicted: \"happy\".\n\n**Now try it out yourself. In the next cell, you can change both the sentence and the context half-size.**\n\n::: {#f583a526 .cell execution_count=11}\n``` {.python .cell-code}\n# Print 'context_words' and 'center_word' for any sentence with a 'context half-size' of 1\nfor x, y in get_windows(tokenize(\"Now it's your turn: try with your own sentence!\"), 1):\n    print(f'{x}\\t{y}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['now', 'your']\tit\n['it', 'turn']\tyour\n['your', 'try']\tturn\n['turn', 'with']\ttry\n['try', 'your']\twith\n['with', 'own']\tyour\n['your', 'sentence']\town\n['own', '.']\tsentence\n```\n:::\n:::\n\n\n## Transforming words into vectors for the training set\n\nTo finish preparing the training set, you need to transform the context words and center words into vectors.\n\n### Mapping words to indices and indices to words\n\nThe center words will be represented as one-hot vectors, and the vectors that represent context words are also based on one-hot vectors.\n\nTo create one-hot word vectors, you can start by mapping each unique word to a unique integer (or index). We have provided a helper function, `get_dict`, that creates a Python dictionary that maps words to integers and back.\n\n::: {#6c270e62 .cell execution_count=12}\n``` {.python .cell-code}\n# Get 'word2Ind' and 'Ind2word' dictionaries for the tokenized corpus\nword2Ind, Ind2word = get_dict(words)\n```\n:::\n\n\nHere's the dictionary that maps words to numeric indices.\n\n::: {#ce5fa2ba .cell execution_count=13}\n``` {.python .cell-code}\n# Print 'word2Ind' dictionary\nword2Ind\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\n{'am': 0, 'because': 1, 'happy': 2, 'i': 3, 'learning': 4}\n```\n:::\n:::\n\n\nYou can use this dictionary to get the index of a word.\n\n::: {#2191e588 .cell execution_count=14}\n``` {.python .cell-code}\n# Print value for the key 'i' within word2Ind dictionary\nprint(\"Index of the word 'i':  \",word2Ind['i'])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIndex of the word 'i':   3\n```\n:::\n:::\n\n\nAnd conversely, here's the dictionary that maps indices to words.\n\n::: {#cef1a7ff .cell execution_count=15}\n``` {.python .cell-code}\n# Print 'Ind2word' dictionary\nInd2word\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\n{0: 'am', 1: 'because', 2: 'happy', 3: 'i', 4: 'learning'}\n```\n:::\n:::\n\n\n::: {#e2c4a367 .cell execution_count=16}\n``` {.python .cell-code}\n# Print value for the key '2' within Ind2word dictionary\nprint(\"Word which has index 2:  \",Ind2word[2] )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWord which has index 2:   happy\n```\n:::\n:::\n\n\nFinally, get the length of either of these dictionaries to get the size of the vocabulary of your corpus, in other words the number of different words making up the corpus.\n\n::: {#defbccfe .cell execution_count=17}\n``` {.python .cell-code}\n# Save length of word2Ind dictionary into the 'V' variable\nV = len(word2Ind)\n\n# Print length of word2Ind dictionary\nprint(\"Size of vocabulary: \", V)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSize of vocabulary:  5\n```\n:::\n:::\n\n\n### Getting one-hot word vectors\n\nRecall from the lecture that you can easily convert an integer, $n$, into a one-hot vector.\n\nConsider the word \"happy\". First, retrieve its numeric index.\n\n::: {#c536335e .cell execution_count=18}\n``` {.python .cell-code}\n# Save index of word 'happy' into the 'n' variable\nn = word2Ind['happy']\n\n# Print index of word 'happy'\nn\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\n2\n```\n:::\n:::\n\n\nNow create a vector with the size of the vocabulary, and fill it with zeros.\n\n::: {#fcf2897f .cell execution_count=19}\n``` {.python .cell-code}\n# Create vector with the same length as the vocabulary, filled with zeros\ncenter_word_vector = np.zeros(V)\n\n# Print vector\ncenter_word_vector\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\narray([0., 0., 0., 0., 0.])\n```\n:::\n:::\n\n\nYou can confirm that the vector has the right size.\n\n::: {#be0ebdce .cell execution_count=20}\n``` {.python .cell-code}\n# Assert that the length of the vector is the same as the size of the vocabulary\nlen(center_word_vector) == V\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\nTrue\n```\n:::\n:::\n\n\nNext, replace the 0 of the $n$-th element with a 1.\n\n::: {#4ec5e132 .cell execution_count=21}\n``` {.python .cell-code}\n# Replace element number 'n' with a 1\ncenter_word_vector[n] = 1\n```\n:::\n\n\nAnd you have your one-hot word vector.\n\n::: {#b5d09cca .cell execution_count=22}\n``` {.python .cell-code}\n# Print vector\ncenter_word_vector\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\narray([0., 0., 1., 0., 0.])\n```\n:::\n:::\n\n\n**You can now group all of these steps in a convenient function, which takes as parameters: a word to be encoded, a dictionary that maps words to indices, and the size of the vocabulary.**\n\n::: {#40142dbb .cell execution_count=23}\n``` {.python .cell-code}\n# Define the 'word_to_one_hot_vector' function that will include the steps previously seen\ndef word_to_one_hot_vector(word, word2Ind, V):\n    one_hot_vector = np.zeros(V)\n    one_hot_vector[word2Ind[word]] = 1\n    return one_hot_vector\n```\n:::\n\n\nCheck that it works as intended.\n\n::: {#5159f87a .cell execution_count=24}\n``` {.python .cell-code}\n# Print output of 'word_to_one_hot_vector' function for word 'happy'\nword_to_one_hot_vector('happy', word2Ind, V)\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```\narray([0., 0., 1., 0., 0.])\n```\n:::\n:::\n\n\n**What is the word vector for \"learning\"?**\n\n::: {#a4ebedf2 .cell execution_count=25}\n``` {.python .cell-code}\n# Print output of 'word_to_one_hot_vector' function for word 'learning'\nword_to_one_hot_vector('learning', word2Ind, V)\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```\narray([0., 0., 0., 0., 1.])\n```\n:::\n:::\n\n\nExpected output:\n\n    array([0., 0., 0., 0., 1.])\n\n### Getting context word vectors\n\nTo create the vectors that represent context words, you will calculate the average of the one-hot vectors representing the individual words.\n\nLet's start with a list of context words.\n\n::: {#0d8d5ae3 .cell execution_count=26}\n``` {.python .cell-code}\n# Define list containing context words\ncontext_words = ['i', 'am', 'because', 'i']\n```\n:::\n\n\nUsing Python's list comprehension construct and the `word_to_one_hot_vector` function that you created in the previous section, you can create a list of one-hot vectors representing each of the context words.\n\n::: {#05325ebd .cell execution_count=27}\n``` {.python .cell-code}\n# Create one-hot vectors for each context word using list comprehension\ncontext_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n\n# Print one-hot vectors for each context word\ncontext_words_vectors\n```\n\n::: {.cell-output .cell-output-display execution_count=27}\n```\n[array([0., 0., 0., 1., 0.]),\n array([1., 0., 0., 0., 0.]),\n array([0., 1., 0., 0., 0.]),\n array([0., 0., 0., 1., 0.])]\n```\n:::\n:::\n\n\nAnd you can now simply get the average of these vectors using numpy's `mean` function, to get the vector representation of the context words.\n\n::: {#efd4e8c0 .cell execution_count=28}\n``` {.python .cell-code}\n# Compute mean of the vectors using numpy\nnp.mean(context_words_vectors, axis=0)\n```\n\n::: {.cell-output .cell-output-display execution_count=28}\n```\narray([0.25, 0.25, 0.  , 0.5 , 0.  ])\n```\n:::\n:::\n\n\nNote the `axis=0` parameter that tells `mean` to calculate the average of the rows (if you had wanted the average of the columns, you would have used `axis=1`).\n\n**Now create the `context_words_to_vector` function that takes in a list of context words, a word-to-index dictionary, and a vocabulary size, and outputs the vector representation of the context words.**\n\n::: {#d833c205 .cell execution_count=29}\n``` {.python .cell-code}\n# Define the 'context_words_to_vector' function that will include the steps previously seen\ndef context_words_to_vector(context_words, word2Ind, V):\n    context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n    context_words_vectors = np.mean(context_words_vectors, axis=0)\n    return context_words_vectors\n```\n:::\n\n\nAnd check that you obtain the same output as the manual approach above.\n\n::: {#386428dd .cell execution_count=30}\n``` {.python .cell-code}\n# Print output of 'context_words_to_vector' function for context words: 'i', 'am', 'because', 'i'\ncontext_words_to_vector(['i', 'am', 'because', 'i'], word2Ind, V)\n```\n\n::: {.cell-output .cell-output-display execution_count=30}\n```\narray([0.25, 0.25, 0.  , 0.5 , 0.  ])\n```\n:::\n:::\n\n\n**What is the vector representation of the context words \"am happy i am\"?**\n\n::: {#4ba4026b .cell execution_count=31}\n``` {.python .cell-code}\n# Print output of 'context_words_to_vector' function for context words: 'am', 'happy', 'i', 'am'\ncontext_words_to_vector(['am', 'happy', 'i', 'am'], word2Ind, V)\n```\n\n::: {.cell-output .cell-output-display execution_count=31}\n```\narray([0.5 , 0.  , 0.25, 0.25, 0.  ])\n```\n:::\n:::\n\n\nExpected output:\n\n    array([0.5 , 0.  , 0.25, 0.25, 0.  ])\n\n\n## Building the training set\n\nYou can now combine the functions that you created in the previous sections, to build a training set for the CBOW model, starting from the following tokenized corpus.\n\n::: {#22855760 .cell execution_count=32}\n``` {.python .cell-code}\n# Print corpus\nwords\n```\n\n::: {.cell-output .cell-output-display execution_count=32}\n```\n['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n```\n:::\n:::\n\n\nTo do this you need to use the sliding window function (`get_windows`) to extract the context words and center words, and you then convert these sets of words into a basic vector representation using `word_to_one_hot_vector` and `context_words_to_vector`.\n\n::: {#37bbc984 .cell execution_count=33}\n``` {.python .cell-code}\n# Print vectors associated to center and context words for corpus\nfor context_words, center_word in get_windows(words, 2):  # reminder: 2 is the context half-size\n    print(f'Context words:  {context_words} -> {context_words_to_vector(context_words, word2Ind, V)}')\n    print(f'Center word:  {center_word} -> {word_to_one_hot_vector(center_word, word2Ind, V)}')\n    print()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nContext words:  ['i', 'am', 'because', 'i'] -> [0.25 0.25 0.   0.5  0.  ]\nCenter word:  happy -> [0. 0. 1. 0. 0.]\n\nContext words:  ['am', 'happy', 'i', 'am'] -> [0.5  0.   0.25 0.25 0.  ]\nCenter word:  because -> [0. 1. 0. 0. 0.]\n\nContext words:  ['happy', 'because', 'am', 'learning'] -> [0.25 0.25 0.25 0.   0.25]\nCenter word:  i -> [0. 0. 0. 1. 0.]\n\n```\n:::\n:::\n\n\nIn this practice notebook you'll be performing a single iteration of training using a single example, but in this week's assignment you'll train the CBOW model using several iterations and batches of example.\nHere is how you would use a Python generator function (remember the `yield` keyword from the lecture?) to make it easier to iterate over a set of examples.\n\n::: {#a38216a9 .cell execution_count=34}\n``` {.python .cell-code}\n# Define the generator function 'get_training_example'\ndef get_training_example(words, C, word2Ind, V):\n    for context_words, center_word in get_windows(words, C):\n        yield context_words_to_vector(context_words, word2Ind, V), word_to_one_hot_vector(center_word, word2Ind, V)\n```\n:::\n\n\nThe output of this function can be iterated on to get successive context word vectors and center word vectors, as demonstrated in the next cell.\n\n::: {#73335a80 .cell execution_count=35}\n``` {.python .cell-code}\n# Print vectors associated to center and context words for corpus using the generator function\nfor context_words_vector, center_word_vector in get_training_example(words, 2, word2Ind, V):\n    print(f'Context words vector:  {context_words_vector}')\n    print(f'Center word vector:  {center_word_vector}')\n    print()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nContext words vector:  [0.25 0.25 0.   0.5  0.  ]\nCenter word vector:  [0. 0. 1. 0. 0.]\n\nContext words vector:  [0.5  0.   0.25 0.25 0.  ]\nCenter word vector:  [0. 1. 0. 0. 0.]\n\nContext words vector:  [0.25 0.25 0.25 0.   0.25]\nCenter word vector:  [0. 0. 0. 1. 0.]\n\n```\n:::\n:::\n\n\nYour training set is ready, you can now move on to the CBOW model itself which will be covered in the next lecture notebook.\n\n**Congratulations on finishing this lecture notebook!** Hopefully you now have a better understanding of how to prepare your data before feeding it to a continuous bag-of-words model. \n\n**Keep it up!**\n\n",
    "supporting": [
      "lab01_files"
    ],
    "filters": [],
    "includes": {}
  }
}