{
  "hash": "95d602875e56222e4a63210c05492bed",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ndate: 2020-10-24\ntitle: 'Modified Triplet Loss'\nsubtitle: \"Sequence Models\"\n#description: \"we cover Neural networks for deep learning, then build a tweet classifier that places tweets into positive or negative sentiment categories, using a DNN.\"\ncategories: \n  - NLP \n  - Coursera \n  - Lab\n  - Sequence Models\njupyter: python3\n---\n\n\n\n\n![course banner](/images/Course-Logo-3-3.webp){.column-margin .nolightbox} \n\nIn this notebook you'll see how to calculate the full triplet loss, step by step, including the mean negative and the closest negative. You'll also calculate the matrix of similarity scores.\n\n## Background\nThis is the original triplet loss function:\n\n$\\mathcal{L_\\mathrm{Original}} = \\max{(\\mathrm{s}(A,N) -\\mathrm{s}(A,P) +\\alpha, 0)}$\n\nIt can be improved by including the mean negative and the closest negative, to create a new full loss function. The inputs are the Anchor $\\mathrm{A}$, Positive $\\mathrm{P}$ and Negative $\\mathrm{N}$.\n\n$\\mathcal{L_\\mathrm{1}} = \\max{(mean\\_neg -\\mathrm{s}(A,P)  +\\alpha, 0)}$\n\n$\\mathcal{L_\\mathrm{2}} = \\max{(closest\\_neg -\\mathrm{s}(A,P)  +\\alpha, 0)}$\n\n$\\mathcal{L_\\mathrm{Full}} = \\mathcal{L_\\mathrm{1}} + \\mathcal{L_\\mathrm{2}}$\n\nLet me show you what that means exactly, and how to calculate each step.\n\n## Imports\n\n::: {#65ec5bf1 .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\n```\n:::\n\n\n## Similarity Scores\nThe first step is to calculate the matrix of similarity scores using cosine similarity so that you can look up $\\mathrm{s}(A,P)$, $\\mathrm{s}(A,N)$ as needed for the loss formulas.\n\n### Two Vectors\nFirst I'll show you how to calculate the similarity score, using cosine similarity, for 2 vectors.\n\n$\\mathrm{s}(v_1,v_2) = \\mathrm{cosine \\ similarity}(v_1,v_2) = \\frac{v_1 \\cdot v_2}{||v_1||~||v_2||}$\n* Try changing the values in the second vector to see how it changes the cosine similarity.\n\n::: {#a2031246 .cell tags='[]' execution_count=3}\n``` {.python .cell-code}\n# Two vector example\n# Input data\nprint(\"-- Inputs --\")\nv1 = np.array([1, 2, 3], dtype=float)\nv2 = np.array([1, 2, 3.5])  # notice the 3rd element is offset by 0.5\n### START CODE HERE ###\n# Try modifying the vector v2 to see how it impacts the cosine similarity\n# v2 = v1                   # identical vector\n# v2 = v1 * -1              # opposite vector\n# v2 = np.array([0,-42,1])  # random example\n### END CODE HERE ###\nprint(\"v1 :\", v1)\nprint(\"v2 :\", v2, \"\\n\")\n\n# Similarity score\ndef cosine_similarity(v1, v2):\n    numerator = np.dot(v1, v2)\n    denominator = np.sqrt(np.dot(v1, v1)) * np.sqrt(np.dot(v2, v2))\n    return numerator / denominator\n\nprint(\"-- Outputs --\")\nprint(\"cosine similarity :\", cosine_similarity(v1, v2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n-- Inputs --\nv1 : [1. 2. 3.]\nv2 : [1.  2.  3.5] \n\n-- Outputs --\ncosine similarity : 0.9974086507360697\n```\n:::\n:::\n\n\n### Two Batches of Vectors\nNow i'll show you how to calculate the similarity scores, using cosine similarity, for 2 batches of vectors. These are rows of individual vectors, just like in the example above, but stacked vertically into a matrix. They would look like the image below for a batch size (row count) of 4 and embedding size (column count) of 5.\n\nThe data is setup so that $v_{1\\_1}$ and $v_{2\\_1}$ represent duplicate inputs, but they are not duplicates with any other rows in the batch. This means $v_{1\\_1}$ and $v_{2\\_1}$ (green and green) have more similar vectors than say $v_{1\\_1}$ and $v_{2\\_2}$ (green and magenta).\n\nI'll show you two different methods for calculating the matrix of similarities from 2 batches of vectors.\n\n<img src = 'img/v1v2_stacked.png' width=\"width\" height=\"height\" style=\"height:250px;\"/>\n\n::: {#cc35d5e0 .cell tags='[]' execution_count=4}\n``` {.python .cell-code}\n# Two batches of vectors example\n# Input data\nprint(\"-- Inputs --\")\nv1_1 = np.array([1, 2, 3])\nv1_2 = np.array([9, 8, 7])\nv1_3 = np.array([-1, -4, -2])\nv1_4 = np.array([1, -7, 2])\nv1 = np.vstack([v1_1, v1_2, v1_3, v1_4])\nprint(\"v1 :\")\nprint(v1, \"\\n\")\nv2_1 = v1_1 + np.random.normal(0, 2, 3)  # add some noise to create approximate duplicate\nv2_2 = v1_2 + np.random.normal(0, 2, 3)\nv2_3 = v1_3 + np.random.normal(0, 2, 3)\nv2_4 = v1_4 + np.random.normal(0, 2, 3)\nv2 = np.vstack([v2_1, v2_2, v2_3, v2_4])\nprint(\"v2 :\")\nprint(v2, \"\\n\")\n\n# Batch sizes must match\nb = len(v1)\nprint(\"batch sizes match :\", b == len(v2), \"\\n\")\n\n# Similarity scores\nprint(\"-- Outputs --\")\n# Option 1 : nested loops and the cosine similarity function\nsim_1 = np.zeros([b, b])  # empty array to take similarity scores\n# Loop\nfor row in range(0, sim_1.shape[0]):\n    for col in range(0, sim_1.shape[1]):\n        sim_1[row, col] = cosine_similarity(v1[row], v2[col])\n\nprint(\"option 1 : loop\")\nprint(sim_1, \"\\n\")\n\n# Option 2 : vector normalization and dot product\ndef norm(x):\n    return x / np.sqrt(np.sum(x * x, axis=1, keepdims=True))\n\nsim_2 = np.dot(norm(v1), norm(v2).T)\n\nprint(\"option 2 : vec norm & dot product\")\nprint(sim_2, \"\\n\")\n\n# Check\nprint(\"outputs are the same :\", np.allclose(sim_1, sim_2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n-- Inputs --\nv1 :\n[[ 1  2  3]\n [ 9  8  7]\n [-1 -4 -2]\n [ 1 -7  2]] \n\nv2 :\n[[ 1.21645308  1.35713237  3.78355839]\n [ 8.85413947  7.24768163  5.2194917 ]\n [ 1.47675307  0.88025807 -1.17386222]\n [ 1.90001605 -7.2662659  -1.56921395]] \n\nbatch sizes match : True \n\n-- Outputs --\noption 1 : loop\n[[ 0.97249649  0.82895399 -0.03650184 -0.60400133]\n [ 0.82555472  0.99449163  0.41785609 -0.48671225]\n [-0.73847824 -0.83778628 -0.27779311  0.86184996]\n [-0.02321248 -0.34020103 -0.45973258  0.8801495 ]] \n\noption 2 : vec norm & dot product\n[[ 0.97249649  0.82895399 -0.03650184 -0.60400133]\n [ 0.82555472  0.99449163  0.41785609 -0.48671225]\n [-0.73847824 -0.83778628 -0.27779311  0.86184996]\n [-0.02321248 -0.34020103 -0.45973258  0.8801495 ]] \n\noutputs are the same : True\n```\n:::\n:::\n\n\n\n## Hard Negative Mining\n\nI'll now show you how to calculate the mean negative $mean\\_neg$ and the closest negative $close\\_neg$ used in calculating $\\mathcal{L_\\mathrm{1}}$ and $\\mathcal{L_\\mathrm{2}}$.\n\n\n$\\mathcal{L_\\mathrm{1}} = \\max{(mean\\_neg -\\mathrm{s}(A,P)  +\\alpha, 0)}$\n\n$\\mathcal{L_\\mathrm{2}} = \\max{(closest\\_neg -\\mathrm{s}(A,P)  +\\alpha, 0)}$\n\nYou'll do this using the matrix of similarity scores you already know how to make, like the example below for a batch size of 4. The diagonal of the matrix contains all the $\\mathrm{s}(A,P)$ values, similarities from duplicate question pairs (aka Positives). This is an important attribute for the calculations to follow.\n\n<img src = 'img/ss_matrix.png' width=\"width\" height=\"height\" style=\"height:250px;\"/>\n\n\n### Mean Negative\n\n$mean\\_neg$ is the average of the off diagonals, the $\\mathrm{s}(A,N)$ values, for each row.\n\n### Closest Negative\n\n$closest\\_neg$ is the largest off diagonal value, $\\mathrm{s}(A,N)$, that is smaller than the diagonal $\\mathrm{s}(A,P)$ for each row.\n* Try using a different matrix of similarity scores. \n\n::: {#6c223ffc .cell tags='[]' execution_count=5}\n``` {.python .cell-code}\n# Hardcoded matrix of similarity scores\nsim_hardcoded = np.array(\n    [\n        [0.9, -0.8, 0.3, -0.5],\n        [-0.4, 0.5, 0.1, -0.1],\n        [0.3, 0.1, -0.4, -0.8],\n        [-0.5, -0.2, -0.7, 0.5],\n    ]\n)\n\nsim = sim_hardcoded\n### START CODE HERE ###\n# Try using different values for the matrix of similarity scores\n# sim = 2 * np.random.random_sample((b,b)) -1   # random similarity scores between -1 and 1\n# sim = sim_2                                   # the matrix calculated previously\n### END CODE HERE ###\n\n# Batch size\nb = sim.shape[0]\n\nprint(\"-- Inputs --\")\nprint(\"sim :\")\nprint(sim)\nprint(\"shape :\", sim.shape, \"\\n\")\n\n# Positives\n# All the s(A,P) values : similarities from duplicate question pairs (aka Positives)\n# These are along the diagonal\nsim_ap = np.diag(sim)\nprint(\"sim_ap :\")\nprint(np.diag(sim_ap), \"\\n\")\n\n# Negatives\n# all the s(A,N) values : similarities the non duplicate question pairs (aka Negatives)\n# These are in the off diagonals\nsim_an = sim - np.diag(sim_ap)\nprint(\"sim_an :\")\nprint(sim_an, \"\\n\")\n\nprint(\"-- Outputs --\")\n# Mean negative\n# Average of the s(A,N) values for each row\nmean_neg = np.sum(sim_an, axis=1, keepdims=True) / (b - 1)\nprint(\"mean_neg :\")\nprint(mean_neg, \"\\n\")\n\n# Closest negative\n# Max s(A,N) that is <= s(A,P) for each row\nmask_1 = np.identity(b) == 1            # mask to exclude the diagonal\nmask_2 = sim_an > sim_ap.reshape(b, 1)  # mask to exclude sim_an > sim_ap\nmask = mask_1 | mask_2\nsim_an_masked = np.copy(sim_an)         # create a copy to preserve sim_an\nsim_an_masked[mask] = -2\n\nclosest_neg = np.max(sim_an_masked, axis=1, keepdims=True)\nprint(\"closest_neg :\")\nprint(closest_neg, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n-- Inputs --\nsim :\n[[ 0.9 -0.8  0.3 -0.5]\n [-0.4  0.5  0.1 -0.1]\n [ 0.3  0.1 -0.4 -0.8]\n [-0.5 -0.2 -0.7  0.5]]\nshape : (4, 4) \n\nsim_ap :\n[[ 0.9  0.   0.   0. ]\n [ 0.   0.5  0.   0. ]\n [ 0.   0.  -0.4  0. ]\n [ 0.   0.   0.   0.5]] \n\nsim_an :\n[[ 0.  -0.8  0.3 -0.5]\n [-0.4  0.   0.1 -0.1]\n [ 0.3  0.1  0.  -0.8]\n [-0.5 -0.2 -0.7  0. ]] \n\n-- Outputs --\nmean_neg :\n[[-0.33333333]\n [-0.13333333]\n [-0.13333333]\n [-0.46666667]] \n\nclosest_neg :\n[[ 0.3]\n [ 0.1]\n [-0.8]\n [-0.2]] \n\n```\n:::\n:::\n\n\n## The Loss Functions\n\nThe last step is to calculate the loss functions.\n\n$\\mathcal{L_\\mathrm{1}} = \\max{(mean\\_neg -\\mathrm{s}(A,P)  +\\alpha, 0)}$\n\n$\\mathcal{L_\\mathrm{2}} = \\max{(closest\\_neg -\\mathrm{s}(A,P)  +\\alpha, 0)}$\n\n$\\mathcal{L_\\mathrm{Full}} = \\mathcal{L_\\mathrm{1}} + \\mathcal{L_\\mathrm{2}}$\n\n::: {#a7fd97c0 .cell tags='[]' execution_count=6}\n``` {.python .cell-code}\n# Alpha margin\nalpha = 0.25\n\n# Modified triplet loss\n# Loss 1\nl_1 = np.maximum(mean_neg - sim_ap.reshape(b, 1) + alpha, 0)\n# Loss 2\nl_2 = np.maximum(closest_neg - sim_ap.reshape(b, 1) + alpha, 0)\n# Loss full\nl_full = l_1 + l_2\n# Cost\ncost = np.sum(l_full)\n\nprint(\"-- Outputs --\")\nprint(\"loss full :\")\nprint(l_full, \"\\n\")\nprint(\"cost :\", \"{:.3f}\".format(cost))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n-- Outputs --\nloss full :\n[[0.        ]\n [0.        ]\n [0.51666667]\n [0.        ]] \n\ncost : 0.517\n```\n:::\n:::\n\n\n## Summary\n\nThere were a lot of steps in there, so well done. You now know how to calculate a modified triplet loss, incorporating the mean negative and the closest negative. You also learned how to create a matrix of similarity scores based on cosine similarity.\n\n",
    "supporting": [
      "lab02_files"
    ],
    "filters": [],
    "includes": {}
  }
}