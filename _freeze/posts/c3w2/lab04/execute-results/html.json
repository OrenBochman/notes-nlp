{
  "hash": "3dea8e69982ea3a1f0d2bf2d0ba0c6ba",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ndate: 2020-10-24\ntitle: 'Creating a GRU model using Trax: Ungraded Lecture Notebook'\nsubtitle: \"Sequence Models\"\njupyter: python3\ncategories: \n  - NLP \n  - Coursera \n  - Lab\n  - Sequence Models\n---\n\n\n\n\n![course banner](/images/Course-Logo-3-3.webp){.column-margin .nolightbox} \n\n\nFor this lecture notebook you will be using Trax's layers. These are the building blocks for creating neural networks with Trax.\n\n::: {#af62c7ec .cell execution_count=2}\n``` {.python .cell-code}\nimport trax\nfrom trax import layers as tl\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n2025-02-05 18:41:32.506060: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1738773692.522541  546416 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1738773692.526991  546416 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n```\n:::\n:::\n\n\nTrax allows to define neural network architectures by stacking layers (similarly to other libraries such as Keras). For this the `Serial()` is often used as it is a combinator that allows to stack layers serially using function composition.\n\nNext you can see a simple vanilla NN architecture containing 1 hidden(dense) layer with 128 cells and output (dense) layer with 10 cells on which we apply the final layer of logsoftmax.\n\n::: {#c80e2545 .cell execution_count=3}\n``` {.python .cell-code}\nmlp = tl.Serial(\n  tl.Dense(128),\n  tl.Relu(),\n  tl.Dense(10),\n  tl.LogSoftmax()\n)\n```\n:::\n\n\nEach of the layers within the `Serial` combinator layer is considered a sublayer. Notice that unlike similar libraries, **in Trax the activation functions are considered layers.** To know more about the `Serial` layer check the docs [here](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Serial).\n\nYou can try printing this object:\n\n::: {#92f0eb4d .cell execution_count=4}\n``` {.python .cell-code}\nprint(mlp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSerial[\n  Dense_128\n  Serial[\n    Relu\n  ]\n  Dense_10\n  LogSoftmax\n]\n```\n:::\n:::\n\n\nPrinting the model gives you the exact same information as the model's definition itself.\n\nBy just looking at the definition you can clearly see what is going on inside the neural network. Trax is very straightforward in the way a network is defined, that is one of the things that makes it awesome! \n\n## GRU MODEL\n\nTo create a `GRU` model you will need to be familiar with the following layers (Documentation link attached with each layer name):\n   - [`ShiftRight`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.ShiftRight) Shifts the tensor to the right by padding on axis 1. The `mode` should be specified and it refers to the context in which the model is being used. Possible values are: 'train', 'eval' or 'predict', predict mode is for fast inference. Defaults to \"train\".\n   \n   - [`Embedding`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding) Maps discrete tokens to vectors. It will have shape `(vocabulary length X dimension of output vectors)`. The dimension of output vectors (also called `d_feature`) is the number of elements in the word embedding.\n   - [`GRU`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.GRU) The GRU layer. It leverages another Trax layer called [`GRUCell`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.GRUCell). The number of GRU units should be specified and should match the number of elements in the word embedding. If you want to stack two consecutive GRU layers, it can be done by using python's list comprehension.\n   - [`Dense`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense) Vanilla Dense layer.\n   - [`LogSoftMax`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.LogSoftmax) Log Softmax function.\n\nPutting everything together the GRU model will look like this:\n\n::: {#c8e63b76 .cell execution_count=5}\n``` {.python .cell-code}\nmode = 'train'\nvocab_size = 256\nmodel_dimension = 512\nn_layers = 2\n\nGRU = tl.Serial(\n      tl.ShiftRight(mode=mode), # Do remember to pass the mode parameter if you are using it for interence/test as default is train \n      tl.Embedding(vocab_size=vocab_size, d_feature=model_dimension),\n      [tl.GRU(n_units=model_dimension) for _ in range(n_layers)], # You can play around n_layers if you want to stack more GRU layers together\n      tl.Dense(n_units=vocab_size),\n      tl.LogSoftmax()\n    )\n```\n:::\n\n\nNext is a helper function that prints information for every layer (sublayer within `Serial`):\n\n_Try changing the parameters defined before the GRU model and see how it changes!_\n\n::: {#09a2c90c .cell execution_count=6}\n``` {.python .cell-code}\ndef show_layers(model, layer_prefix=\"Serial.sublayers\"):\n    print(f\"Total layers: {len(model.sublayers)}\\n\")\n    for i in range(len(model.sublayers)):\n        print('========')\n        print(f'{layer_prefix}_{i}: {model.sublayers[i]}\\n')\n        \nshow_layers(GRU)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTotal layers: 6\n\n========\nSerial.sublayers_0: Serial[\n  ShiftRight(1)\n]\n\n========\nSerial.sublayers_1: Embedding_256_512\n\n========\nSerial.sublayers_2: GRU_512\n\n========\nSerial.sublayers_3: GRU_512\n\n========\nSerial.sublayers_4: Dense_256\n\n========\nSerial.sublayers_5: LogSoftmax\n\n```\n:::\n:::\n\n\nHope you are now more familiarized with creating GRU models using Trax. \n\nYou will train this model in this week's assignment and see it in action. \n\n\n**GRU and the trax minions will return, in this week's endgame.**\n\n",
    "supporting": [
      "lab04_files"
    ],
    "filters": [],
    "includes": {}
  }
}