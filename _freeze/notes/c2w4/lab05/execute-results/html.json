{
  "hash": "1a31eecae8f50c33e76644e396ba82a1",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ndate: 2020-11-04\ntitle: 'Word Embeddings: Hands On'\ncategories: \n  - NLP \n  - Coursera \n  - Lab\n  - Probabilistic Models\njupyter: python3\n# execute: \n#     error: true\n---\n\n\n\n\n::: {.column-margin .nolightbox}\n![course banner](/images/Course-Logo-2-3.webp)\n:::\n\n\nIn previous lecture notebooks we saw all the steps needed to train the CBOW model. This notebook will walk we through how to extract the word embedding vectors from a model.\n\nLet's dive into it!\n\n::: {#d90e8d32 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nfrom utils2 import get_dict\n```\n:::\n\n\nBefore moving on, we will be provided with some variables needed for further procedures, which should be familiar by now. Also a trained CBOW model will be simulated, the corresponding weights and biases are provided: \n\n::: {#800d1e9c .cell execution_count=2}\n``` {.python .cell-code}\n# Define the tokenized version of the corpus\nwords = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n\n# Define V. Remember this is the size of the vocabulary\nV = 5\n\n# Get 'word2Ind' and 'Ind2word' dictionaries for the tokenized corpus\nword2Ind, Ind2word = get_dict(words)\n\n\n# Define first matrix of weights\nW1 = np.array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n               [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n               [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n\n# Define second matrix of weights\nW2 = np.array([[-0.22182064, -0.43008631,  0.13310965],\n               [ 0.08476603,  0.08123194,  0.1772054 ],\n               [ 0.1871551 , -0.06107263, -0.1790735 ],\n               [ 0.07055222, -0.02015138,  0.36107434],\n               [ 0.33480474, -0.39423389, -0.43959196]])\n\n# Define first vector of biases\nb1 = np.array([[ 0.09688219],\n               [ 0.29239497],\n               [-0.27364426]])\n\n# Define second vector of biases\nb2 = np.array([[ 0.0352008 ],\n               [-0.36393384],\n               [-0.12775555],\n               [-0.34802326],\n               [-0.07017815]])\n```\n:::\n\n\n## Extracting word embedding vectors\n\nOnce we have finished training the neural network, we have three options to get word embedding vectors for the words of your vocabulary, based on the weight matrices $\\mathbf{W_1}$ and/or $\\mathbf{W_2}$.\n\n### Option 1: extract embedding vectors from $\\mathbf{W_1}$\n\nThe first option is to take the columns of $\\mathbf{W_1}$ as the embedding vectors of the words of the vocabulary, using the same order of the words as for the input and output vectors.\n\n> Note: in this practice notebooks the values of the word embedding vectors are meaningless after a single iteration with just one training example, but here's how we would proceed after the training process is complete.\n\nFor example $\\mathbf{W_1}$ is this matrix:\n\n::: {#f35ba55c .cell execution_count=3}\n``` {.python .cell-code}\n# Print W1\nW1\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\narray([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n       [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n       [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n```\n:::\n:::\n\n\nThe first column, which is a 3-element vector, is the embedding vector of the first word of your vocabulary. The second column is the word embedding vector for the second word, and so on.\n\nThe first, second, etc. words are ordered as follows.\n\n::: {#6302be25 .cell execution_count=4}\n``` {.python .cell-code}\n# Print corresponding word for each index within vocabulary's range\nfor i in range(V):\n    print(Ind2word[i])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nam\nbecause\nhappy\ni\nlearning\n```\n:::\n:::\n\n\nSo the word embedding vectors corresponding to each word are:\n\n::: {#449d4080 .cell execution_count=5}\n``` {.python .cell-code}\n# Loop through each word of the vocabulary\nfor word in word2Ind:\n    # Extract the column corresponding to the index of the word in the vocabulary\n    word_embedding_vector = W1[:, word2Ind[word]]\n    # Print word alongside word embedding vector\n    print(f'{word}: {word_embedding_vector}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nam: [0.41687358 0.32735501 0.26637602]\nbecause: [ 0.08854191  0.22795148 -0.23846886]\nhappy: [-0.23495225 -0.23951958 -0.37770863]\ni: [ 0.28320538  0.4117634  -0.11399446]\nlearning: [ 0.41800106 -0.23924344  0.34008124]\n```\n:::\n:::\n\n\n### Option 2: extract embedding vectors from $\\mathbf{W_2}$\n\nThe second option is to take $\\mathbf{W_2}$ transposed, and take its columns as the word embedding vectors just like we did for $\\mathbf{W_1}$.\n\n::: {#fa1e4d88 .cell execution_count=6}\n``` {.python .cell-code}\n# Print transposed W2\nW2.T\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\narray([[-0.22182064,  0.08476603,  0.1871551 ,  0.07055222,  0.33480474],\n       [-0.43008631,  0.08123194, -0.06107263, -0.02015138, -0.39423389],\n       [ 0.13310965,  0.1772054 , -0.1790735 ,  0.36107434, -0.43959196]])\n```\n:::\n:::\n\n\n::: {#840b0cb1 .cell execution_count=7}\n``` {.python .cell-code}\n# Loop through each word of the vocabulary\nfor word in word2Ind:\n    # Extract the column corresponding to the index of the word in the vocabulary\n    word_embedding_vector = W2.T[:, word2Ind[word]]\n    # Print word alongside word embedding vector\n    print(f'{word}: {word_embedding_vector}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nam: [-0.22182064 -0.43008631  0.13310965]\nbecause: [0.08476603 0.08123194 0.1772054 ]\nhappy: [ 0.1871551  -0.06107263 -0.1790735 ]\ni: [ 0.07055222 -0.02015138  0.36107434]\nlearning: [ 0.33480474 -0.39423389 -0.43959196]\n```\n:::\n:::\n\n\n### Option 3: extract embedding vectors from $\\mathbf{W_1}$ and $\\mathbf{W_2}$\n\nThe third option, which is the one we will use in this week's assignment, uses the average of $\\mathbf{W_1}$ and $\\mathbf{W_2^\\top}$.\n\n**Calculate the average of $\\mathbf{W_1}$ and $\\mathbf{W_2^\\top}$, and store the result in `W3`.**\n\n::: {#f44fc462 .cell execution_count=8}\n``` {.python .cell-code}\n# Compute W3 as the average of W1 and W2 transposed\nW3 = (W1+W2.T)/2\n\n# Print W3\nW3\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\narray([[ 0.09752647,  0.08665397, -0.02389858,  0.1768788 ,  0.3764029 ],\n       [-0.05136565,  0.15459171, -0.15029611,  0.19580601, -0.31673866],\n       [ 0.19974284, -0.03063173, -0.27839106,  0.12353994, -0.04975536]])\n```\n:::\n:::\n\n\nExpected output:\n\n    array([[ 0.09752647,  0.08665397, -0.02389858,  0.1768788 ,  0.3764029 ],\n           [-0.05136565,  0.15459171, -0.15029611,  0.19580601, -0.31673866],\n           [ 0.19974284, -0.03063173, -0.27839106,  0.12353994, -0.04975536]])\n\nExtracting the word embedding vectors works just like the two previous options, by taking the columns of the matrix you've just created.\n\n::: {#fea34318 .cell execution_count=9}\n``` {.python .cell-code}\n# Loop through each word of the vocabulary\nfor word in word2Ind:\n    # Extract the column corresponding to the index of the word in the vocabulary\n    word_embedding_vector = W3[:, word2Ind[word]]\n    # Print word alongside word embedding vector\n    print(f'{word}: {word_embedding_vector}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nam: [ 0.09752647 -0.05136565  0.19974284]\nbecause: [ 0.08665397  0.15459171 -0.03063173]\nhappy: [-0.02389858 -0.15029611 -0.27839106]\ni: [0.1768788  0.19580601 0.12353994]\nlearning: [ 0.3764029  -0.31673866 -0.04975536]\n```\n:::\n:::\n\n\nNow we know 3 different options to get the word embedding vectors from a model! \n\n### How this practice relates to and differs from the upcoming graded assignment\n\n- After extracting the word embedding vectors, we will use principal component analysis (PCA) to visualize the vectors, which will enable we to perform an intrinsic evaluation of the quality of the vectors, as explained in the lecture.\n\n**Congratulations on finishing all lecture notebooks for this week!** \n\nYou're now ready to take on this week's assignment!\n\n**Keep it up!**\n\n",
    "supporting": [
      "lab05_files"
    ],
    "filters": [],
    "includes": {}
  }
}