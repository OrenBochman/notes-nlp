{
  "hash": "66db05f338f0c9a001a3c668fc8847c1",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ndate: 2020-11-05\ntitle: 'Assignment 4: Word Embeddings'\ncategories: \n  - NLP \n  - Coursera \n  - Lab\n  - Probabilistic Models\njupyter: python3\nexecute: \n    error: true\ndraft: true\n---\n\n::: {#fig-00 .column-margin .nolightbox}\n![course banner](/images/Course-Logo-2-3.webp)\n:::\n\n\n::: {.callout}\n## Honor code alert\n\nDue to the Coursera Honor Code, I cannot provide the solutions to the assignments. \n\n- This notebook is the original notebook provided by the course\n- It is setup to run without stopping for errors. \n- It is also likely to be out of date as the course has had some updates since I took it.\n- Although I aced the course this assignment was the most time consuming.\n- Good luck with the assignment it should make we a better programmer.\n- It is also a good idea to go over it a few times until we can do it easily.\n:::\n\nWelcome to the fourth (and last) programming assignment of Course 2! \n\nIn this assignment, we will practice how to compute word embeddings and use them for sentiment analysis.\n- To implement sentiment analysis, we can go beyond counting the number of positive words and negative words. \n- We can find a way to represent each word numerically, by a vector. \n- The vector could then represent syntactic (i.e. parts of speech) and semantic (i.e. meaning) structures. \n\nIn this assignment, we will explore a classic way of generating word embeddings or representations.\n- We will implement a famous model called the continuous bag of words (CBOW) model. \n\nBy completing this assignment we will:\n\n- Train word vectors from scratch.\n- Learn how to create batches of data.\n- Understand how backpropagation works.\n- Plot and visualize your learned word vectors.\n\nKnowing how to train these models will give we a better understanding of word vectors, which are building blocks to many applications in natural language processing.\n\n\n## Outline\n\n- [1 The Continuous bag of words model](#1)\n- [2 Training the Model](#2)\n    - [2.0 Initialize the model](#2)\n        - [Exercise 01](#ex-01)\n    - [2.1 Softmax Function](#2.1)\n        - [Exercise 02](#ex-02)\n    - [2.2 Forward Propagation](#2.2)\n        - [Exercise 03](#ex-03)\n    - [2.3 Cost Function](#2.3)\n    - [2.4 Backproagation](#2.4)\n        - [Exercise 04](#ex-04)\n    - [2.5 Gradient Descent](#2.5)\n        - [Exercise 05](#ex-05)\n- [3 Visualizing the word vectors](#3)\n\n<a name='1'></a>\n# 1. The Continuous bag of words model\n\nLet's take a look at the following sentence: \n>**'I am happy because I am learning'**. \n\n- In continuous bag of words (CBOW) modeling, we try to predict the center word given a few context words (the words around the center word).\n- For example, if we were to choose a context half-size of say $C = 2$, then we would try to predict the word **happy** given the context that includes 2 words before and 2 words after the center word:\n\n> $C$ words before: [I, am] \n\n> $C$ words after: [because, I] \n\n- In other words:\n\n$$context = [I,am, because, I]$$\n$$target = happy$$\n\nThe structure of your model will look like this:\n\n::: {#fig-01 .column-margin}\n![course banner](img/word2.png){style=\"width:600px;height:250px;\" group=\"slides\"}\n\nFigure 1\n:::\n\nWhere $\\bar x$ is the average of all the one hot vectors of the context words. \n\n::: {#fig-00 .column-margin}\n![course banner](img/mean_vec2.png){ style=\"width:600px;height:250px;\" group=\"slides\"}\n\nFigure 2\n:::\n\nOnce we have encoded all the context words, we can use $\\bar x$ as the input to your model. \n\nThe architecture we will be implementing is as follows:\n\n\\begin{align}\n h &= W_1 \\  X + b_1  \\tag{1} \\\\\n a &= ReLU(h)  \\tag{2} \\\\\n z &= W_2 \\  a + b_2   \\tag{3} \\\\\n \\hat y &= softmax(z)   \\tag{4} \\\\\n\\end{align}\n\n::: {#07ca9539 .cell execution_count=1}\n``` {.python .cell-code}\n# Import Python libraries and helper functions (in utils2) \nimport nltk\nfrom nltk.tokenize import word_tokenize\nimport numpy as np\nfrom collections import Counter\nfrom utils2 import sigmoid, get_batches, compute_pca, get_dict\n```\n:::\n\n\n::: {#686edf90 .cell execution_count=2}\n``` {.python .cell-code}\n# Download sentence tokenizer\nnltk.data.path.append('.')\n```\n:::\n\n\n::: {#be7f8996 .cell execution_count=3}\n``` {.python .cell-code}\n# Load, tokenize and process the data\nimport re                                                           #  Load the Regex-modul\nwith open('shakespeare.txt') as f:\n    data = f.read()                                                 #  Read in the data\ndata = re.sub(r'[,!?;-]', '.',data)                                 #  Punktuations are replaced by .\ndata = nltk.word_tokenize(data)                                     #  Tokenize string to words\ndata = [ ch.lower() for ch in data if ch.isalpha() or ch == '.']    #  Lower case and drop non-alphabetical tokens\nprint(\"Number of tokens:\", len(data),'\\n', data[:15])               #  print data sample\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of tokens: 61011 \n ['o', 'for', 'a', 'muse', 'of', 'fire', '.', 'that', 'would', 'ascend', 'the', 'brightest', 'heaven', 'of', 'invention']\n```\n:::\n:::\n\n\n::: {#3ea2729c .cell execution_count=4}\n``` {.python .cell-code}\n# Compute the frequency distribution of the words in the dataset (vocabulary)\nfdist = nltk.FreqDist(word for word in data)\nprint(\"Size of vocabulary: \",len(fdist) )\nprint(\"Most frequent tokens: \",fdist.most_common(20) ) # print the 20 most frequent words and their freq.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSize of vocabulary:  5784\nMost frequent tokens:  [('.', 9630), ('the', 1521), ('and', 1394), ('i', 1259), ('to', 1159), ('of', 1093), ('my', 857), ('that', 781), ('in', 771), ('a', 752), ('you', 748), ('is', 630), ('not', 559), ('for', 467), ('it', 460), ('with', 441), ('his', 434), ('but', 417), ('me', 417), ('your', 397)]\n```\n:::\n:::\n\n\n#### Mapping words to indices and indices to words\nWe provide a helper function to create a dictionary that maps words to indices and indices to words.\n\n::: {#704353a4 .cell execution_count=5}\n``` {.python .cell-code}\n# get_dict creates two dictionaries, converting words to indices and viceversa.\nword2Ind, Ind2word = get_dict(data)\nV = len(word2Ind)\nprint(\"Size of vocabulary: \", V)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSize of vocabulary:  5784\n```\n:::\n:::\n\n\n::: {#730d7977 .cell execution_count=6}\n``` {.python .cell-code}\n# example of word to index mapping\nprint(\"Index of the word 'king' :  \",word2Ind['king'] )\nprint(\"Word which has index 2743:  \",Ind2word[2743] )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIndex of the word 'king' :   2747\nWord which has index 2743:   kind\n```\n:::\n:::\n\n\n<a name='2'></a>\n# 2 Training the Model\n\n###  Initializing the model\n\nWe will now initialize two matrices and two vectors. \n- The first matrix ($W_1$) is of dimension $N \\times V$, where $V$ is the number of words in your vocabulary and $N$ is the dimension of your word vector.\n- The second matrix ($W_2$) is of dimension $V \\times N$. \n- Vector $b_1$ has dimensions $N\\times 1$\n- Vector $b_2$ has dimensions  $V\\times 1$. \n- $b_1$ and $b_2$ are the bias vectors of the linear layers from matrices $W_1$ and $W_2$.\n\nThe overall structure of the model will look as in Figure 1, but at this stage we are just initializing the parameters. \n\n<a name='ex-01'></a>\n### Exercise 01\nPlease use [numpy.random.rand](https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html) to generate matrices that are initialized with random values from a uniform distribution, ranging between 0 and 1.\n\n**Note:** In the next cell we will encounter a random seed. Please **DO NOT** modify this seed so your solution can be tested correctly.\n\n::: {#a3188be1 .cell execution_count=7}\n``` {.python .cell-code}\n# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: initialize_model\ndef initialize_model(N,V, random_seed=1):\n    '''\n    Inputs: \n        N:  dimension of hidden vector \n        V:  dimension of vocabulary\n        random_seed: random seed for consistent results in the unit tests\n     Outputs: \n        W1, W2, b1, b2: initialized weights and biases\n    '''\n    \n    np.random.seed(random_seed)\n    \n    ### START CODE HERE (Replace instances of 'None' with your code) ###\n    # W1 has shape (N,V)\n    W1 = None\n    # W2 has shape (V,N)\n    W2 = None\n    # b1 has shape (N,1)\n    b1 = None\n    # b2 has shape (V,1)\n    b2 = None\n    ### END CODE HERE ###\n\n    return W1, W2, b1, b2\n```\n:::\n\n\n::: {#748d1060 .cell execution_count=8}\n``` {.python .cell-code}\n# Test your function example.\ntmp_N = 4\ntmp_V = 10\ntmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\nassert tmp_W1.shape == ((tmp_N,tmp_V))\nassert tmp_W2.shape == ((tmp_V,tmp_N))\nprint(f\"tmp_W1.shape: {tmp_W1.shape}\")\nprint(f\"tmp_W2.shape: {tmp_W2.shape}\")\nprint(f\"tmp_b1.shape: {tmp_b1.shape}\")\nprint(f\"tmp_b2.shape: {tmp_b2.shape}\")\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AttributeError</span>                            Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[8], line 5</span>\n<span class=\"ansi-green-fg ansi-bold\">      3</span> tmp_V <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(98,98,98)\">10</span>\n<span class=\"ansi-green-fg ansi-bold\">      4</span> tmp_W1, tmp_W2, tmp_b1, tmp_b2 <span style=\"color:rgb(98,98,98)\">=</span> initialize_model(tmp_N,tmp_V)\n<span class=\"ansi-green-fg\">----&gt; 5</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">assert</span> <span class=\"ansi-yellow-bg\">tmp_W1</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">shape</span> <span style=\"color:rgb(98,98,98)\">==</span> ((tmp_N,tmp_V))\n<span class=\"ansi-green-fg ansi-bold\">      6</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">assert</span> tmp_W2<span style=\"color:rgb(98,98,98)\">.</span>shape <span style=\"color:rgb(98,98,98)\">==</span> ((tmp_V,tmp_N))\n<span class=\"ansi-green-fg ansi-bold\">      7</span> <span style=\"color:rgb(0,135,0)\">print</span>(<span style=\"color:rgb(175,0,0)\">f</span><span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">tmp_W1.shape: </span><span style=\"font-weight:bold;color:rgb(175,95,135)\">{</span>tmp_W1<span style=\"color:rgb(98,98,98)\">.</span>shape<span style=\"font-weight:bold;color:rgb(175,95,135)\">}</span><span style=\"color:rgb(175,0,0)\">\"</span>)\n\n<span class=\"ansi-red-fg\">AttributeError</span>: 'NoneType' object has no attribute 'shape'</pre>\n```\n:::\n\n:::\n:::\n\n\n##### Expected Output \n\n```CPP\ntmp_W1.shape: (4, 10)\ntmp_W2.shape: (10, 4)\ntmp_b1.shape: (4, 1)\ntmp_b2.shape: (10, 1)\n```\n\n<a name='2.1'></a>\n### 2.1 Softmax\nBefore we can start training the model, we need to implement the softmax function as defined in equation 5:  \n\n<br>\n$$ \\text{softmax}(z_i) = \\frac{e^{z_i} }{\\sum_{i=0}^{V-1} e^{z_i} }  \\tag{5} $$\n\n- Array indexing in code starts at 0.\n- $V$ is the number of words in the vocabulary (which is also the number of rows of $z$).\n- $i$ goes from 0 to |V| - 1.\n\n\n<a name='ex-02'></a>\n### Exercise 02\n**Instructions**: Implement the softmax function below. \n\n- Assume that the input $z$ to `softmax` is a 2D array\n- Each training example is represented by a column of shape (V, 1) in this 2D array.\n- There may be more than one column, in the 2D array, because we can put in a batch of examples to increase efficiency.  Let's call the batch size lowercase $m$, so the $z$ array has shape (V, m)\n- When taking the sum from $i=1 \\cdots V-1$, take the sum for each column (each example) separately.\n\nPlease use\n- numpy.exp\n- numpy.sum (set the axis so that we take the sum of each column in z)\n\n::: {#53558a6f .cell execution_count=9}\n``` {.python .cell-code}\n# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: softmax\ndef softmax(z):\n    '''\n    Inputs: \n        z: output scores from the hidden layer\n    Outputs: \n        yhat: prediction (estimate of y)\n    '''\n    \n    ### START CODE HERE (Replace instances of 'None' with your own code) ###\n    \n    # Calculate yhat (softmax)\n    yhat = None\n    \n    ### END CODE HERE ###\n    \n    return yhat\n```\n:::\n\n\n::: {#4c8e6e77 .cell execution_count=10}\n``` {.python .cell-code}\n# Test the function\ntmp = np.array([[1,2,3],\n                [1,1,1]\n               ])\ntmp_sm = softmax(tmp)\ndisplay(tmp_sm)\n```\n\n::: {.cell-output .cell-output-display}\n```\nNone\n```\n:::\n:::\n\n\n##### Expected Ouput\n\n```CPP\narray([[0.5       , 0.73105858, 0.88079708],\n       [0.5       , 0.26894142, 0.11920292]])\n```\n\n<a name='2.2'></a>\n### 2.2 Forward propagation\n\n<a name='ex-03'></a>\n### Exercise 03\nImplement the forward propagation $z$ according to equations (1) to (3). <br>\n\n\\begin{align}\n h &= W_1 \\  X + b_1  \\tag{1} \\\\\n a &= ReLU(h)  \\tag{2} \\\\\n z &= W_2 \\  a + b_2   \\tag{3} \\\\\n\\end{align}\n\nFor that, we will use as activation the Rectified Linear Unit (ReLU) given by:\n\n$$f(h)=\\max (0,h) \\tag{6}$$\n\n<details>    \n<summary>\n    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n</summary>\n<p>\n<ul>\n    <li>We can use numpy.maximum(x1,x2) to get the maximum of two values</li>\n    <li>Use numpy.dot(A,B) to matrix multiply A and B</li>\n</ul>\n</p>\n</details>\n\n::: {#cfab175c .cell execution_count=11}\n``` {.python .cell-code}\n# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: forward_prop\ndef forward_prop(x, W1, W2, b1, b2):\n    '''\n    Inputs: \n        x:  average one hot vector for the context \n        W1, W2, b1, b2:  matrices and biases to be learned\n     Outputs: \n        z:  output score vector\n    '''\n    \n    ### START CODE HERE (Replace instances of 'None' with your own code) ###\n    \n    # Calculate h\n    h = None\n    \n    # Apply the relu on h (store result in h)\n    h = None\n    \n    # Calculate z\n    z = None\n    \n    ### END CODE HERE ###\n\n    return z, h\n```\n:::\n\n\n::: {#5b3d2765 .cell execution_count=12}\n``` {.python .cell-code}\n# Test the function\n\n# Create some inputs\ntmp_N = 2\ntmp_V = 3\ntmp_x = np.array([[0,1,0]]).T\ntmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(N=tmp_N,V=tmp_V, random_seed=1)\n\nprint(f\"x has shape {tmp_x.shape}\")\nprint(f\"N is {tmp_N} and vocabulary size V is {tmp_V}\")\n\n# call function\ntmp_z, tmp_h = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\nprint(\"call forward_prop\")\nprint()\n# Look at output\nprint(f\"z has shape {tmp_z.shape}\")\nprint(\"z has values:\")\nprint(tmp_z)\n\nprint()\n\nprint(f\"h has shape {tmp_h.shape}\")\nprint(\"h has values:\")\nprint(tmp_h)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nx has shape (3, 1)\nN is 2 and vocabulary size V is 3\ncall forward_prop\n\n```\n:::\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AttributeError</span>                            Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[12], line 17</span>\n<span class=\"ansi-green-fg ansi-bold\">     15</span> <span style=\"color:rgb(0,135,0)\">print</span>()\n<span class=\"ansi-green-fg ansi-bold\">     16</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Look at output</span>\n<span class=\"ansi-green-fg\">---&gt; 17</span> <span style=\"color:rgb(0,135,0)\">print</span>(<span style=\"color:rgb(175,0,0)\">f</span><span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">z has shape </span><span style=\"font-weight:bold;color:rgb(175,95,135)\">{</span><span class=\"ansi-yellow-bg\">tmp_z</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">shape</span><span style=\"font-weight:bold;color:rgb(175,95,135)\">}</span><span style=\"color:rgb(175,0,0)\">\"</span>)\n<span class=\"ansi-green-fg ansi-bold\">     18</span> <span style=\"color:rgb(0,135,0)\">print</span>(<span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">z has values:</span><span style=\"color:rgb(175,0,0)\">\"</span>)\n<span class=\"ansi-green-fg ansi-bold\">     19</span> <span style=\"color:rgb(0,135,0)\">print</span>(tmp_z)\n\n<span class=\"ansi-red-fg\">AttributeError</span>: 'NoneType' object has no attribute 'shape'</pre>\n```\n:::\n\n:::\n:::\n\n\n##### Expected output\n```CPP\nx has shape (3, 1)\nN is 2 and vocabulary size V is 3\ncall forward_prop\n\nz has shape (3, 1)\nz has values:\n[[0.55379268]\n [1.58960774]\n [1.50722933]]\n\nh has shape (2, 1)\nh has values:\n[[0.92477674]\n [1.02487333]]\n```\n\n<a name='2.3'></a>\n## 2.3 Cost function\n\n- We have implemented the *cross-entropy* cost function for you.\n\n::: {#4303154c .cell execution_count=13}\n``` {.python .cell-code}\n# compute_cost: cross-entropy cost functioN\ndef compute_cost(y, yhat, batch_size):\n    # cost function \n    logprobs = np.multiply(np.log(yhat),y) + np.multiply(np.log(1 - yhat), 1 - y)\n    cost = - 1/batch_size * np.sum(logprobs)\n    cost = np.squeeze(cost)\n    return cost\n```\n:::\n\n\n::: {#3544a892 .cell execution_count=14}\n``` {.python .cell-code}\n# Test the function\ntmp_C = 2\ntmp_N = 50\ntmp_batch_size = 4\ntmp_word2Ind, tmp_Ind2word = get_dict(data)\ntmp_V = len(word2Ind)\n\ntmp_x, tmp_y = next(get_batches(data, tmp_word2Ind, tmp_V,tmp_C, tmp_batch_size))\n        \nprint(f\"tmp_x.shape {tmp_x.shape}\")\nprint(f\"tmp_y.shape {tmp_y.shape}\")\n\ntmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n\nprint(f\"tmp_W1.shape {tmp_W1.shape}\")\nprint(f\"tmp_W2.shape {tmp_W2.shape}\")\nprint(f\"tmp_b1.shape {tmp_b1.shape}\")\nprint(f\"tmp_b2.shape {tmp_b2.shape}\")\n\ntmp_z, tmp_h = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\nprint(f\"tmp_z.shape: {tmp_z.shape}\")\nprint(f\"tmp_h.shape: {tmp_h.shape}\")\n\ntmp_yhat = softmax(tmp_z)\nprint(f\"tmp_yhat.shape: {tmp_yhat.shape}\")\n\ntmp_cost = compute_cost(tmp_y, tmp_yhat, tmp_batch_size)\nprint(\"call compute_cost\")\nprint(f\"tmp_cost {tmp_cost:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntmp_x.shape (5784, 4)\ntmp_y.shape (5784, 4)\n```\n:::\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AttributeError</span>                            Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[14], line 15</span>\n<span class=\"ansi-green-fg ansi-bold\">     11</span> <span style=\"color:rgb(0,135,0)\">print</span>(<span style=\"color:rgb(175,0,0)\">f</span><span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">tmp_y.shape </span><span style=\"font-weight:bold;color:rgb(175,95,135)\">{</span>tmp_y<span style=\"color:rgb(98,98,98)\">.</span>shape<span style=\"font-weight:bold;color:rgb(175,95,135)\">}</span><span style=\"color:rgb(175,0,0)\">\"</span>)\n<span class=\"ansi-green-fg ansi-bold\">     13</span> tmp_W1, tmp_W2, tmp_b1, tmp_b2 <span style=\"color:rgb(98,98,98)\">=</span> initialize_model(tmp_N,tmp_V)\n<span class=\"ansi-green-fg\">---&gt; 15</span> <span style=\"color:rgb(0,135,0)\">print</span>(<span style=\"color:rgb(175,0,0)\">f</span><span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">tmp_W1.shape </span><span style=\"font-weight:bold;color:rgb(175,95,135)\">{</span><span class=\"ansi-yellow-bg\">tmp_W1</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">shape</span><span style=\"font-weight:bold;color:rgb(175,95,135)\">}</span><span style=\"color:rgb(175,0,0)\">\"</span>)\n<span class=\"ansi-green-fg ansi-bold\">     16</span> <span style=\"color:rgb(0,135,0)\">print</span>(<span style=\"color:rgb(175,0,0)\">f</span><span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">tmp_W2.shape </span><span style=\"font-weight:bold;color:rgb(175,95,135)\">{</span>tmp_W2<span style=\"color:rgb(98,98,98)\">.</span>shape<span style=\"font-weight:bold;color:rgb(175,95,135)\">}</span><span style=\"color:rgb(175,0,0)\">\"</span>)\n<span class=\"ansi-green-fg ansi-bold\">     17</span> <span style=\"color:rgb(0,135,0)\">print</span>(<span style=\"color:rgb(175,0,0)\">f</span><span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">tmp_b1.shape </span><span style=\"font-weight:bold;color:rgb(175,95,135)\">{</span>tmp_b1<span style=\"color:rgb(98,98,98)\">.</span>shape<span style=\"font-weight:bold;color:rgb(175,95,135)\">}</span><span style=\"color:rgb(175,0,0)\">\"</span>)\n\n<span class=\"ansi-red-fg\">AttributeError</span>: 'NoneType' object has no attribute 'shape'</pre>\n```\n:::\n\n:::\n:::\n\n\n##### Expected output\n\n```CPP\ntmp_x.shape (5778, 4)\ntmp_y.shape (5778, 4)\ntmp_W1.shape (50, 5778)\ntmp_W2.shape (5778, 50)\ntmp_b1.shape (50, 1)\ntmp_b2.shape (5778, 1)\ntmp_z.shape: (5778, 4)\ntmp_h.shape: (50, 4)\ntmp_yhat.shape: (5778, 4)\ncall compute_cost\ntmp_cost 9.9560\n```\n\n<a name='2.4'></a>\n## 2.4 Training the Model - Backpropagation\n\n<a name='ex-04'></a>\n### Exercise 04\nNow that we have understood how the CBOW model works, we will train it. <br>\nWe created a function for the forward propagation. Now we will implement a function that computes the gradients to backpropagate the errors.\n\n::: {#66f63671 .cell execution_count=15}\n``` {.python .cell-code}\n# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: back_prop\ndef back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size):\n    '''\n    Inputs: \n        x:  average one hot vector for the context \n        yhat: prediction (estimate of y)\n        y:  target vector\n        h:  hidden vector (see eq. 1)\n        W1, W2, b1, b2:  matrices and biases  \n        batch_size: batch size \n     Outputs: \n        grad_W1, grad_W2, grad_b1, grad_b2:  gradients of matrices and biases   \n    '''\n    ### START CODE HERE (Replace instanes of 'None' with your code) ###\n    \n    # Compute l1 as W2^T (Yhat - Y)\n    # Re-use it whenever we see W2^T (Yhat - Y) used to compute a gradient\n    l1 = None\n    # Apply relu to l1\n    l1 = None\n    # Compute the gradient of W1\n    grad_W1 = None\n    # Compute the gradient of W2\n    grad_W2 = None\n    # Compute the gradient of b1\n    grad_b1 = None\n    # Compute the gradient of b2\n    grad_b2 = None\n    ### END CODE HERE ###\n    \n    return grad_W1, grad_W2, grad_b1, grad_b2\n```\n:::\n\n\n::: {#a6ab8cc2 .cell execution_count=16}\n``` {.python .cell-code}\n# Test the function\ntmp_C = 2\ntmp_N = 50\ntmp_batch_size = 4\ntmp_word2Ind, tmp_Ind2word = get_dict(data)\ntmp_V = len(word2Ind)\n\n# get a batch of data\ntmp_x, tmp_y = next(get_batches(data, tmp_word2Ind, tmp_V,tmp_C, tmp_batch_size))\n\nprint(\"get a batch of data\")\nprint(f\"tmp_x.shape {tmp_x.shape}\")\nprint(f\"tmp_y.shape {tmp_y.shape}\")\n\nprint()\nprint(\"Initialize weights and biases\")\ntmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n\nprint(f\"tmp_W1.shape {tmp_W1.shape}\")\nprint(f\"tmp_W2.shape {tmp_W2.shape}\")\nprint(f\"tmp_b1.shape {tmp_b1.shape}\")\nprint(f\"tmp_b2.shape {tmp_b2.shape}\")\n\nprint()\nprint(\"Forwad prop to get z and h\")\ntmp_z, tmp_h = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\nprint(f\"tmp_z.shape: {tmp_z.shape}\")\nprint(f\"tmp_h.shape: {tmp_h.shape}\")\n\nprint()\nprint(\"Get yhat by calling softmax\")\ntmp_yhat = softmax(tmp_z)\nprint(f\"tmp_yhat.shape: {tmp_yhat.shape}\")\n\ntmp_m = (2*tmp_C)\ntmp_grad_W1, tmp_grad_W2, tmp_grad_b1, tmp_grad_b2 = back_prop(tmp_x, tmp_yhat, tmp_y, tmp_h, tmp_W1, tmp_W2, tmp_b1, tmp_b2, tmp_batch_size)\n\nprint()\nprint(\"call back_prop\")\nprint(f\"tmp_grad_W1.shape {tmp_grad_W1.shape}\")\nprint(f\"tmp_grad_W2.shape {tmp_grad_W2.shape}\")\nprint(f\"tmp_grad_b1.shape {tmp_grad_b1.shape}\")\nprint(f\"tmp_grad_b2.shape {tmp_grad_b2.shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nget a batch of data\ntmp_x.shape (5784, 4)\ntmp_y.shape (5784, 4)\n\nInitialize weights and biases\n```\n:::\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AttributeError</span>                            Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[16], line 19</span>\n<span class=\"ansi-green-fg ansi-bold\">     16</span> <span style=\"color:rgb(0,135,0)\">print</span>(<span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">Initialize weights and biases</span><span style=\"color:rgb(175,0,0)\">\"</span>)\n<span class=\"ansi-green-fg ansi-bold\">     17</span> tmp_W1, tmp_W2, tmp_b1, tmp_b2 <span style=\"color:rgb(98,98,98)\">=</span> initialize_model(tmp_N,tmp_V)\n<span class=\"ansi-green-fg\">---&gt; 19</span> <span style=\"color:rgb(0,135,0)\">print</span>(<span style=\"color:rgb(175,0,0)\">f</span><span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">tmp_W1.shape </span><span style=\"font-weight:bold;color:rgb(175,95,135)\">{</span><span class=\"ansi-yellow-bg\">tmp_W1</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">shape</span><span style=\"font-weight:bold;color:rgb(175,95,135)\">}</span><span style=\"color:rgb(175,0,0)\">\"</span>)\n<span class=\"ansi-green-fg ansi-bold\">     20</span> <span style=\"color:rgb(0,135,0)\">print</span>(<span style=\"color:rgb(175,0,0)\">f</span><span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">tmp_W2.shape </span><span style=\"font-weight:bold;color:rgb(175,95,135)\">{</span>tmp_W2<span style=\"color:rgb(98,98,98)\">.</span>shape<span style=\"font-weight:bold;color:rgb(175,95,135)\">}</span><span style=\"color:rgb(175,0,0)\">\"</span>)\n<span class=\"ansi-green-fg ansi-bold\">     21</span> <span style=\"color:rgb(0,135,0)\">print</span>(<span style=\"color:rgb(175,0,0)\">f</span><span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">tmp_b1.shape </span><span style=\"font-weight:bold;color:rgb(175,95,135)\">{</span>tmp_b1<span style=\"color:rgb(98,98,98)\">.</span>shape<span style=\"font-weight:bold;color:rgb(175,95,135)\">}</span><span style=\"color:rgb(175,0,0)\">\"</span>)\n\n<span class=\"ansi-red-fg\">AttributeError</span>: 'NoneType' object has no attribute 'shape'</pre>\n```\n:::\n\n:::\n:::\n\n\n##### Expected output\n\n```CPP\nget a batch of data\ntmp_x.shape (5778, 4)\ntmp_y.shape (5778, 4)\n\nInitialize weights and biases\ntmp_W1.shape (50, 5778)\ntmp_W2.shape (5778, 50)\ntmp_b1.shape (50, 1)\ntmp_b2.shape (5778, 1)\n\nForwad prop to get z and h\ntmp_z.shape: (5778, 4)\ntmp_h.shape: (50, 4)\n\nGet yhat by calling softmax\ntmp_yhat.shape: (5778, 4)\n\ncall back_prop\ntmp_grad_W1.shape (50, 5778)\ntmp_grad_W2.shape (5778, 50)\ntmp_grad_b1.shape (50, 1)\ntmp_grad_b2.shape (5778, 1)\n```\n\n<a name='2.5'></a>\n## Gradient Descent\n\n<a name='ex-05'></a>\n### Exercise 05\nNow that we have implemented a function to compute the gradients, we will implement batch gradient descent over your training set. \n\n**Hint:** For that, we will use `initialize_model` and the `back_prop` functions which we just created (and the `compute_cost` function). We can also use the provided `get_batches` helper function:\n\n```for x, y in get_batches(data, word2Ind, V, C, batch_size):```\n\n```...```\n\nAlso: print the cost after each batch is processed (use batch size = 128)\n\n::: {#c84e2648 .cell execution_count=17}\n``` {.python .cell-code}\n# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: gradient_descent\ndef gradient_descent(data, word2Ind, N, V, num_iters, alpha=0.03):\n    \n    '''\n    This is the gradient_descent function\n    \n      Inputs: \n        data:      text\n        word2Ind:  words to Indices\n        N:         dimension of hidden vector  \n        V:         dimension of vocabulary \n        num_iters: number of iterations  \n     Outputs: \n        W1, W2, b1, b2:  updated matrices and biases   \n\n    '''\n    W1, W2, b1, b2 = initialize_model(N,V, random_seed=282)\n    batch_size = 128\n    iters = 0\n    C = 2\n    for x, y in get_batches(data, word2Ind, V, C, batch_size):\n        ### START CODE HERE (Replace instances of 'None' with your own code) ###\n        # Get z and h\n        z, h = None\n        # Get yhat\n        yhat = None\n        # Get cost\n        cost = None\n        if ( (iters+1) % 10 == 0):\n            print(f\"iters: {iters + 1} cost: {cost:.6f}\")\n        # Get gradients\n        grad_W1, grad_W2, grad_b1, grad_b2 = None\n        \n        # Update weights and biases\n        W1 = None \n        W2 = None\n        b1 = None\n        b2 = None\n        \n        ### END CODE HERE ###\n        \n        iters += 1 \n        if iters == num_iters: \n            break\n        if iters % 100 == 0:\n            alpha *= 0.66\n            \n    return W1, W2, b1, b2\n```\n:::\n\n\n::: {#ee3340fa .cell execution_count=18}\n``` {.python .cell-code}\n# test your function\nC = 2\nN = 50\nword2Ind, Ind2word = get_dict(data)\nV = len(word2Ind)\nnum_iters = 150\nprint(\"Call gradient_descent\")\nW1, W2, b1, b2 = gradient_descent(data, word2Ind, N, V, num_iters)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCall gradient_descent\n```\n:::\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">TypeError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[18], line 8</span>\n<span class=\"ansi-green-fg ansi-bold\">      6</span> num_iters <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(98,98,98)\">150</span>\n<span class=\"ansi-green-fg ansi-bold\">      7</span> <span style=\"color:rgb(0,135,0)\">print</span>(<span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">Call gradient_descent</span><span style=\"color:rgb(175,0,0)\">\"</span>)\n<span class=\"ansi-green-fg\">----&gt; 8</span> W1, W2, b1, b2 <span style=\"color:rgb(98,98,98)\">=</span> <span class=\"ansi-yellow-bg\">gradient_descent</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">data</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">word2Ind</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">N</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">V</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">num_iters</span><span class=\"ansi-yellow-bg\">)</span>\n\nCell <span class=\"ansi-green-fg\">In[17], line 25</span>, in <span class=\"ansi-cyan-fg\">gradient_descent</span><span class=\"ansi-blue-fg\">(data, word2Ind, N, V, num_iters, alpha)</span>\n<span class=\"ansi-green-fg ansi-bold\">     21</span> C <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(98,98,98)\">2</span>\n<span class=\"ansi-green-fg ansi-bold\">     22</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">for</span> x, y <span style=\"font-weight:bold;color:rgb(175,0,255)\">in</span> get_batches(data, word2Ind, V, C, batch_size):\n<span class=\"ansi-green-fg ansi-bold\">     23</span>     <span style=\"font-style:italic;color:rgb(95,135,135)\">### START CODE HERE (Replace instances of 'None' with your own code) ###</span>\n<span class=\"ansi-green-fg ansi-bold\">     24</span>     <span style=\"font-style:italic;color:rgb(95,135,135)\"># Get z and h</span>\n<span class=\"ansi-green-fg\">---&gt; 25</span>     z, h <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>\n<span class=\"ansi-green-fg ansi-bold\">     26</span>     <span style=\"font-style:italic;color:rgb(95,135,135)\"># Get yhat</span>\n<span class=\"ansi-green-fg ansi-bold\">     27</span>     yhat <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>\n\n<span class=\"ansi-red-fg\">TypeError</span>: cannot unpack non-iterable NoneType object</pre>\n```\n:::\n\n:::\n:::\n\n\n##### Expected Output\n\n\n```CPP\niters: 10 cost: 0.789141\niters: 20 cost: 0.105543\niters: 30 cost: 0.056008\niters: 40 cost: 0.038101\niters: 50 cost: 0.028868\niters: 60 cost: 0.023237\niters: 70 cost: 0.019444\niters: 80 cost: 0.016716\niters: 90 cost: 0.014660\niters: 100 cost: 0.013054\niters: 110 cost: 0.012133\niters: 120 cost: 0.011370\niters: 130 cost: 0.010698\niters: 140 cost: 0.010100\niters: 150 cost: 0.009566\n```\n\nYour numbers may differ a bit depending on which version of Python you're using.\n\n<a name='3'></a>\n## 3.0 Visualizing the word vectors\n\nIn this part we will visualize the word vectors trained using the function we just coded above. \n\n::: {#83ca4e0c .cell execution_count=19}\n``` {.python .cell-code}\n# visualizing the word vectors here\nfrom matplotlib import pyplot\n%config InlineBackend.figure_format = 'svg'\nwords = ['king', 'queen','lord','man', 'woman','dog','wolf',\n         'rich','happy','sad']\n\nembs = (W1.T + W2)/2.0\n \n# given a list of words and the embeddings, it returns a matrix with all the embeddings\nidx = [word2Ind[word] for word in words]\nX = embs[idx, :]\nprint(X.shape, idx)  # X.shape:  Number of words of dimension N each \n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[19], line 7</span>\n<span class=\"ansi-green-fg ansi-bold\">      3</span> get_ipython()<span style=\"color:rgb(98,98,98)\">.</span>run_line_magic(<span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">config</span><span style=\"color:rgb(175,0,0)\">'</span>, <span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">InlineBackend.figure_format = </span><span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">svg</span><span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">\"</span>)\n<span class=\"ansi-green-fg ansi-bold\">      4</span> words <span style=\"color:rgb(98,98,98)\">=</span> [<span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">king</span><span style=\"color:rgb(175,0,0)\">'</span>, <span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">queen</span><span style=\"color:rgb(175,0,0)\">'</span>,<span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">lord</span><span style=\"color:rgb(175,0,0)\">'</span>,<span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">man</span><span style=\"color:rgb(175,0,0)\">'</span>, <span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">woman</span><span style=\"color:rgb(175,0,0)\">'</span>,<span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">dog</span><span style=\"color:rgb(175,0,0)\">'</span>,<span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">wolf</span><span style=\"color:rgb(175,0,0)\">'</span>,\n<span class=\"ansi-green-fg ansi-bold\">      5</span>          <span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">rich</span><span style=\"color:rgb(175,0,0)\">'</span>,<span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">happy</span><span style=\"color:rgb(175,0,0)\">'</span>,<span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">sad</span><span style=\"color:rgb(175,0,0)\">'</span>]\n<span class=\"ansi-green-fg\">----&gt; 7</span> embs <span style=\"color:rgb(98,98,98)\">=</span> (<span class=\"ansi-yellow-bg\">W1</span><span style=\"color:rgb(98,98,98)\">.</span>T <span style=\"color:rgb(98,98,98)\">+</span> W2)<span style=\"color:rgb(98,98,98)\">/</span><span style=\"color:rgb(98,98,98)\">2.0</span>\n<span class=\"ansi-green-fg ansi-bold\">      9</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># given a list of words and the embeddings, it returns a matrix with all the embeddings</span>\n<span class=\"ansi-green-fg ansi-bold\">     10</span> idx <span style=\"color:rgb(98,98,98)\">=</span> [word2Ind[word] <span style=\"font-weight:bold;color:rgb(0,135,0)\">for</span> word <span style=\"font-weight:bold;color:rgb(175,0,255)\">in</span> words]\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'W1' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n::: {#c664078d .cell execution_count=20}\n``` {.python .cell-code}\nresult= compute_pca(X, 2)\npyplot.scatter(result[:, 0], result[:, 1])\nfor i, word in enumerate(words):\n    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\npyplot.show()\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[20], line 1</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span> result<span style=\"color:rgb(98,98,98)\">=</span> compute_pca(<span class=\"ansi-yellow-bg\">X</span>, <span style=\"color:rgb(98,98,98)\">2</span>)\n<span class=\"ansi-green-fg ansi-bold\">      2</span> pyplot<span style=\"color:rgb(98,98,98)\">.</span>scatter(result[:, <span style=\"color:rgb(98,98,98)\">0</span>], result[:, <span style=\"color:rgb(98,98,98)\">1</span>])\n<span class=\"ansi-green-fg ansi-bold\">      3</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">for</span> i, word <span style=\"font-weight:bold;color:rgb(175,0,255)\">in</span> <span style=\"color:rgb(0,135,0)\">enumerate</span>(words):\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'X' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\nWe can see that man and king are next to each other. However, we have to be careful with the interpretation of this projected word vectors, since the PCA depends on the projection -- as shown in the following illustration.\n\n::: {#5aad3506 .cell execution_count=21}\n``` {.python .cell-code}\nresult= compute_pca(X, 4)\npyplot.scatter(result[:, 3], result[:, 1])\nfor i, word in enumerate(words):\n    pyplot.annotate(word, xy=(result[i, 3], result[i, 1]))\npyplot.show()\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[21], line 1</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span> result<span style=\"color:rgb(98,98,98)\">=</span> compute_pca(<span class=\"ansi-yellow-bg\">X</span>, <span style=\"color:rgb(98,98,98)\">4</span>)\n<span class=\"ansi-green-fg ansi-bold\">      2</span> pyplot<span style=\"color:rgb(98,98,98)\">.</span>scatter(result[:, <span style=\"color:rgb(98,98,98)\">3</span>], result[:, <span style=\"color:rgb(98,98,98)\">1</span>])\n<span class=\"ansi-green-fg ansi-bold\">      3</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">for</span> i, word <span style=\"font-weight:bold;color:rgb(175,0,255)\">in</span> <span style=\"color:rgb(0,135,0)\">enumerate</span>(words):\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'X' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n",
    "supporting": [
      "assignment_files"
    ],
    "filters": [],
    "includes": {}
  }
}