{
  "hash": "73be5ae5e48d0ad724f6908b2eb1d577",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ndate: 2020-11-03\ntitle: 'Word Embeddings: Ungraded Practice Notebook'\ncategories: \n  - NLP \n  - Coursera \n  - Lab\n  - Probabilistic Models\njupyter: python3\n# execute: \n#     error: true\n---\n\n::: {.column-margin .nolightbox}\n![course banner](/images/Course-Logo-2-3.webp)\n:::\n\n\n\nIn this ungraded notebook, you'll try out all the individual techniques that we learned about in the lecture. Practicing on small examples will prepare we for the graded assignment, where we will combine the techniques in more advanced ways to create word embeddings from a real-life corpus.\n\nThis notebook is made of two main parts: \n\n- data preparation, and \n- the continuous bag-of-words (CBOW) model.\n\nTo get started, import and initialize all the libraries we will need.\n\n::: {#c3f2d165 .cell execution_count=1}\n``` {.python .cell-code}\nimport sys\n!{sys.executable} -m pip install emoji\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRequirement already satisfied: emoji in /home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages (1.4.1)\r\n```\n:::\n:::\n\n\n::: {#ac12e531 .cell execution_count=2}\n``` {.python .cell-code}\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize\nimport emoji\nimport numpy as np\n\nfrom utils2 import get_dict\n\nnltk.download('punkt')  # download pre-trained Punkt tokenizer for English\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n[nltk_data] Downloading package punkt to /home/oren/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nTrue\n```\n:::\n:::\n\n\n# Data preparation\n\nIn the data preparation phase, starting with a corpus of text, we will:\n\n- Clean and tokenize the corpus.\n\n- Extract the pairs of context words and center word that will make up the training data set for the CBOW model. The context words are the features that will be fed into the model, and the center words are the target values that the model will learn to predict.\n\n- Create simple vector representations of the context words (features) and center words (targets) that can be used by the neural network of the CBOW model.\n\n## Cleaning and tokenization\n\nTo demonstrate the cleaning and tokenization process, consider a corpus that contains emojis and various punctuation signs.\n\n::: {#e959abb2 .cell execution_count=3}\n``` {.python .cell-code}\ncorpus = 'Who ❤️ \"word embeddings\" in 2020? I do!!!'\n```\n:::\n\n\nFirst, replace all interrupting punctuation signs — such as commas and exclamation marks — with periods.\n\n::: {#ee2a5317 .cell execution_count=4}\n``` {.python .cell-code}\nprint(f'Corpus:  {corpus}')\ndata = re.sub(r'[,!?;-]+', '.', corpus)\nprint(f'After cleaning punctuation:  {data}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorpus:  Who ❤️ \"word embeddings\" in 2020? I do!!!\nAfter cleaning punctuation:  Who ❤️ \"word embeddings\" in 2020. I do.\n```\n:::\n:::\n\n\nNext, use NLTK's tokenization engine to split the corpus into individual tokens.\n\n::: {#be986458 .cell execution_count=5}\n``` {.python .cell-code}\nprint(f'Initial string:  {data}')\ndata = nltk.word_tokenize(data)\nprint(f'After tokenization:  {data}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nInitial string:  Who ❤️ \"word embeddings\" in 2020. I do.\nAfter tokenization:  ['Who', '❤️', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '.']\n```\n:::\n:::\n\n\nFinally, as we saw in the lecture, get rid of numbers and punctuation other than periods, and convert all the remaining tokens to lowercase.\n\n::: {#c7dc1aff .cell execution_count=6}\n``` {.python .cell-code}\nprint(f'Initial list of tokens:  {data}')\ndata = [ ch.lower() for ch in data\n         if ch.isalpha()\n         or ch == '.'\n         or emoji.get_emoji_regexp().search(ch)\n       ]\nprint(f'After cleaning:  {data}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nInitial list of tokens:  ['Who', '❤️', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '.']\nAfter cleaning:  ['who', '❤️', 'word', 'embeddings', 'in', '.', 'i', 'do', '.']\n```\n:::\n:::\n\n\nNote that the heart emoji is considered as a token just like any normal word.\n\nNow let's streamline the cleaning and tokenization process by wrapping the previous steps in a function.\n\n::: {#dc5c5c39 .cell execution_count=7}\n``` {.python .cell-code}\ndef tokenize(corpus):\n    data = re.sub(r'[,!?;-]+', '.', corpus)\n    data = nltk.word_tokenize(data)  # tokenize string to words\n    data = [ ch.lower() for ch in data\n             if ch.isalpha()\n             or ch == '.'\n             or emoji.get_emoji_regexp().search(ch)\n           ]\n    return data\n```\n:::\n\n\nApply this function to the corpus that you'll be working on in the rest of this notebook: \"I am happy because I am learning\"\n\n::: {#9bf6f996 .cell execution_count=8}\n``` {.python .cell-code}\ncorpus = 'I am happy because I am learning'\nprint(f'Corpus:  {corpus}')\nwords = tokenize(corpus)\nprint(f'Words (tokens):  {words}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorpus:  I am happy because I am learning\nWords (tokens):  ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n```\n:::\n:::\n\n\n**Now try it out yourself with your own sentence.**\n\n::: {#6722d890 .cell execution_count=9}\n``` {.python .cell-code}\ntokenize(\"Now it's your turn: try with your own sentence!\")\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n['now', 'it', 'your', 'turn', 'try', 'with', 'your', 'own', 'sentence', '.']\n```\n:::\n:::\n\n\n## Sliding window of words\n\nNow that we have transformed the corpus into a list of clean tokens, we can slide a window of words across this list. For each window we can extract a center word and the context words.\n\nThe `get_windows` function in the next cell was introduced in the lecture.\n\n::: {#22e0e71e .cell execution_count=10}\n``` {.python .cell-code}\ndef get_windows(words, C):\n    i = C\n    while i < len(words) - C:\n        center_word = words[i]\n        context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n        yield context_words, center_word\n        i += 1\n```\n:::\n\n\nThe first argument of this function is a list of words (or tokens). The second argument, `C`, is the context half-size. Recall that for a given center word, the context words are made of `C` words to the left and `C` words to the right of the center word.\n\nHere is how we can use this function to extract context words and center words from a list of tokens. These context and center words will make up the training set that we will use to train the CBOW model.\n\n::: {#dc1c42d9 .cell execution_count=11}\n``` {.python .cell-code}\nfor x, y in get_windows(\n            ['i', 'am', 'happy', 'because', 'i', 'am', 'learning'],\n            2\n        ):\n    print(f'{x}\\t{y}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['i', 'am', 'because', 'i']\thappy\n['am', 'happy', 'i', 'am']\tbecause\n['happy', 'because', 'am', 'learning']\ti\n```\n:::\n:::\n\n\nThe first example of the training set is made of:\n\n- the context words \"i\", \"am\", \"because\", \"i\",\n\n- and the center word to be predicted: \"happy\".\n\n**Now try it out yourself. In the next cell, we can change both the sentence and the context half-size.**\n\n::: {#9e8a967d .cell execution_count=12}\n``` {.python .cell-code}\nfor x, y in get_windows(tokenize(\"Now it's your turn: try with your own sentence!\"), 1):\n    print(f'{x}\\t{y}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['now', 'your']\tit\n['it', 'turn']\tyour\n['your', 'try']\tturn\n['turn', 'with']\ttry\n['try', 'your']\twith\n['with', 'own']\tyour\n['your', 'sentence']\town\n['own', '.']\tsentence\n```\n:::\n:::\n\n\n## Transforming words into vectors for the training set\n\nTo finish preparing the training set, we need to transform the context words and center words into vectors.\n\n### Mapping words to indices and indices to words\n\nThe center words will be represented as one-hot vectors, and the vectors that represent context words are also based on one-hot vectors.\n\nTo create one-hot word vectors, we can start by mapping each unique word to a unique integer (or index). We have provided a helper function, `get_dict`, that creates a Python dictionary that maps words to integers and back.\n\n::: {#39ed1ccb .cell execution_count=13}\n``` {.python .cell-code}\nword2Ind, Ind2word = get_dict(words)\n```\n:::\n\n\nHere's the dictionary that maps words to numeric indices.\n\n::: {#51d9c459 .cell execution_count=14}\n``` {.python .cell-code}\nword2Ind\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\n{'am': 0, 'because': 1, 'happy': 2, 'i': 3, 'learning': 4}\n```\n:::\n:::\n\n\nWe can use this dictionary to get the index of a word.\n\n::: {#3d902712 .cell execution_count=15}\n``` {.python .cell-code}\nprint(\"Index of the word 'i':  \",word2Ind['i'])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIndex of the word 'i':   3\n```\n:::\n:::\n\n\nAnd conversely, here's the dictionary that maps indices to words.\n\n::: {#d88a9c6b .cell execution_count=16}\n``` {.python .cell-code}\nInd2word\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\n{0: 'am', 1: 'because', 2: 'happy', 3: 'i', 4: 'learning'}\n```\n:::\n:::\n\n\n::: {#d624ae09 .cell execution_count=17}\n``` {.python .cell-code}\nprint(\"Word which has index 2:  \",Ind2word[2] )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWord which has index 2:   happy\n```\n:::\n:::\n\n\nFinally, get the length of either of these dictionaries to get the size of the vocabulary of your corpus, in other words the number of different words making up the corpus.\n\n::: {#1c7ebef5 .cell execution_count=18}\n``` {.python .cell-code}\nV = len(word2Ind)\nprint(\"Size of vocabulary: \", V)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSize of vocabulary:  5\n```\n:::\n:::\n\n\n### Getting one-hot word vectors\n\nRecall from the lecture that we can easily convert an integer, $n$, into a one-hot vector.\n\nConsider the word \"happy\". First, retrieve its numeric index.\n\n::: {#3db10837 .cell execution_count=19}\n``` {.python .cell-code}\nn = word2Ind['happy']\nn\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\n2\n```\n:::\n:::\n\n\nNow create a vector with the size of the vocabulary, and fill it with zeros.\n\n::: {#96eb7633 .cell execution_count=20}\n``` {.python .cell-code}\ncenter_word_vector = np.zeros(V)\ncenter_word_vector\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\narray([0., 0., 0., 0., 0.])\n```\n:::\n:::\n\n\nWe can confirm that the vector has the right size.\n\n::: {#d8bb5bff .cell execution_count=21}\n``` {.python .cell-code}\nlen(center_word_vector) == V\n```\n\n::: {.cell-output .cell-output-display execution_count=21}\n```\nTrue\n```\n:::\n:::\n\n\nNext, replace the 0 of the $n$-th element with a 1.\n\n::: {#044f61e7 .cell execution_count=22}\n``` {.python .cell-code}\ncenter_word_vector[n] = 1\n```\n:::\n\n\nAnd we have your one-hot word vector.\n\n::: {#a7f4e2c2 .cell execution_count=23}\n``` {.python .cell-code}\ncenter_word_vector\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```\narray([0., 0., 1., 0., 0.])\n```\n:::\n:::\n\n\n**We can now group all of these steps in a convenient function, which takes as parameters: a word to be encoded, a dictionary that maps words to indices, and the size of the vocabulary.**\n\n::: {#48f702a7 .cell execution_count=24}\n``` {.python .cell-code}\ndef word_to_one_hot_vector(word, word2Ind, V):\n    # BEGIN your code here\n    one_hot_vector = np.zeros(V)\n    one_hot_vector[word2Ind[word]] = 1\n    # END your code here\n    return one_hot_vector\n```\n:::\n\n\nCheck that it works as intended.\n\n::: {#5d1ce644 .cell execution_count=25}\n``` {.python .cell-code}\nword_to_one_hot_vector('happy', word2Ind, V)\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```\narray([0., 0., 1., 0., 0.])\n```\n:::\n:::\n\n\n**What is the word vector for \"learning\"?**\n\n::: {#d3ecba64 .cell execution_count=26}\n``` {.python .cell-code}\n# BEGIN your code here\nword_to_one_hot_vector('learning', word2Ind, V)\n# END your code here\n```\n\n::: {.cell-output .cell-output-display execution_count=26}\n```\narray([0., 0., 0., 0., 1.])\n```\n:::\n:::\n\n\nExpected output:\n\n    array([0., 0., 0., 0., 1.])\n\n### Getting context word vectors\n\nTo create the vectors that represent context words, we will calculate the average of the one-hot vectors representing the individual words.\n\nLet's start with a list of context words.\n\n::: {#bc60e033 .cell execution_count=27}\n``` {.python .cell-code}\ncontext_words = ['i', 'am', 'because', 'i']\n```\n:::\n\n\nUsing Python's list comprehension construct and the `word_to_one_hot_vector` function that we created in the previous section, we can create a list of one-hot vectors representing each of the context words.\n\n::: {#4fb6db7a .cell execution_count=28}\n``` {.python .cell-code}\ncontext_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\ncontext_words_vectors\n```\n\n::: {.cell-output .cell-output-display execution_count=28}\n```\n[array([0., 0., 0., 1., 0.]),\n array([1., 0., 0., 0., 0.]),\n array([0., 1., 0., 0., 0.]),\n array([0., 0., 0., 1., 0.])]\n```\n:::\n:::\n\n\nAnd we can now simply get the average of these vectors using numpy's `mean` function, to get the vector representation of the context words.\n\n::: {#25c373e6 .cell execution_count=29}\n``` {.python .cell-code}\nnp.mean(context_words_vectors, axis=0)\n```\n\n::: {.cell-output .cell-output-display execution_count=29}\n```\narray([0.25, 0.25, 0.  , 0.5 , 0.  ])\n```\n:::\n:::\n\n\nNote the `axis=0` parameter that tells `mean` to calculate the average of the rows (if we had wanted the average of the columns, we would have used `axis=1`).\n\n**Now create the `context_words_to_vector` function that takes in a list of context words, a word-to-index dictionary, and a vocabulary size, and outputs the vector representation of the context words.**\n\n::: {#cb31128a .cell execution_count=30}\n``` {.python .cell-code}\ndef context_words_to_vector(context_words, word2Ind, V):\n    # BEGIN your code here\n    context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n    context_words_vectors = np.mean(context_words_vectors, axis=0)\n    # END your code here\n    return context_words_vectors\n```\n:::\n\n\nAnd check that we obtain the same output as the manual approach above.\n\n::: {#f9cfd510 .cell execution_count=31}\n``` {.python .cell-code}\ncontext_words_to_vector(['i', 'am', 'because', 'i'], word2Ind, V)\n```\n\n::: {.cell-output .cell-output-display execution_count=31}\n```\narray([0.25, 0.25, 0.  , 0.5 , 0.  ])\n```\n:::\n:::\n\n\n**What is the vector representation of the context words \"am happy i am\"?**\n\n::: {#1e86126e .cell execution_count=32}\n``` {.python .cell-code}\n# BEGIN your code here\ncontext_words_to_vector(['am', 'happy', 'i', 'am'], word2Ind, V)\n# END your code here\n```\n\n::: {.cell-output .cell-output-display execution_count=32}\n```\narray([0.5 , 0.  , 0.25, 0.25, 0.  ])\n```\n:::\n:::\n\n\nExpected output:\n\n    array([0.5 , 0.  , 0.25, 0.25, 0.  ])\n\n\n## Building the training set\n\nWe can now combine the functions that we created in the previous sections, to build a training set for the CBOW model, starting from the following tokenized corpus.\n\n::: {#adbd4ecb .cell execution_count=33}\n``` {.python .cell-code}\nwords\n```\n\n::: {.cell-output .cell-output-display execution_count=33}\n```\n['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n```\n:::\n:::\n\n\nTo do this we need to use the sliding window function (`get_windows`) to extract the context words and center words, and we then convert these sets of words into a basic vector representation using `word_to_one_hot_vector` and `context_words_to_vector`.\n\n::: {#94c18813 .cell execution_count=34}\n``` {.python .cell-code}\nfor context_words, center_word in get_windows(words, 2):  # reminder: 2 is the context half-size\n    print(f'Context words:  {context_words} -> {context_words_to_vector(context_words, word2Ind, V)}')\n    print(f'Center word:  {center_word} -> {word_to_one_hot_vector(center_word, word2Ind, V)}')\n    print()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nContext words:  ['i', 'am', 'because', 'i'] -> [0.25 0.25 0.   0.5  0.  ]\nCenter word:  happy -> [0. 0. 1. 0. 0.]\n\nContext words:  ['am', 'happy', 'i', 'am'] -> [0.5  0.   0.25 0.25 0.  ]\nCenter word:  because -> [0. 1. 0. 0. 0.]\n\nContext words:  ['happy', 'because', 'am', 'learning'] -> [0.25 0.25 0.25 0.   0.25]\nCenter word:  i -> [0. 0. 0. 1. 0.]\n\n```\n:::\n:::\n\n\nIn this practice notebook you'll be performing a single iteration of training using a single example, but in this week's assignment you'll train the CBOW model using several iterations and batches of example.\nHere is how we would use a Python generator function (remember the `yield` keyword from the lecture?) to make it easier to iterate over a set of examples.\n\n::: {#c0e1546e .cell execution_count=35}\n``` {.python .cell-code}\ndef get_training_example(words, C, word2Ind, V):\n    for context_words, center_word in get_windows(words, C):\n        yield context_words_to_vector(context_words, word2Ind, V), word_to_one_hot_vector(center_word, word2Ind, V)\n```\n:::\n\n\nThe output of this function can be iterated on to get successive context word vectors and center word vectors, as demonstrated in the next cell.\n\n::: {#00f9703d .cell execution_count=36}\n``` {.python .cell-code}\nfor context_words_vector, center_word_vector in get_training_example(words, 2, word2Ind, V):\n    print(f'Context words vector:  {context_words_vector}')\n    print(f'Center word vector:  {center_word_vector}')\n    print()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nContext words vector:  [0.25 0.25 0.   0.5  0.  ]\nCenter word vector:  [0. 0. 1. 0. 0.]\n\nContext words vector:  [0.5  0.   0.25 0.25 0.  ]\nCenter word vector:  [0. 1. 0. 0. 0.]\n\nContext words vector:  [0.25 0.25 0.25 0.   0.25]\nCenter word vector:  [0. 0. 0. 1. 0.]\n\n```\n:::\n:::\n\n\nYour training set is ready, we can now move on to the CBOW model itself.\n\n# The continuous bag-of-words model\n\nThe CBOW model is based on a neural network, the architecture of which looks like the figure below, as you'll recall from the lecture.\n\n<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='cbow_model_architecture.png?1' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:917;height:337;\" /> Figure 1 </div>\n\nThis part of the notebook will walk we through:\n\n- The two activation functions used in the neural network.\n\n- Forward propagation.\n\n- Cross-entropy loss.\n\n- Backpropagation.\n\n- Gradient descent.\n\n- Extracting the word embedding vectors from the weight matrices once the neural network has been trained.\n\n## Activation functions\n\nLet's start by implementing the activation functions, ReLU and softmax.\n\n### ReLU\n\nReLU is used to calculate the values of the hidden layer, in the following formulas:\n\n\\begin{align}\n \\mathbf{z_1} &= \\mathbf{W_1}\\mathbf{x} + \\mathbf{b_1}  \\tag{1} \\\\\n \\mathbf{h} &= \\mathrm{ReLU}(\\mathbf{z_1})  \\tag{2} \\\\\n\\end{align}\n\nLet's fix a value for $\\mathbf{z_1}$ as a working example.\n\n::: {#079a12c9 .cell execution_count=37}\n``` {.python .cell-code}\nnp.random.seed(10)\nz_1 = 10*np.random.rand(5, 1)-5\nz_1\n```\n\n::: {.cell-output .cell-output-display execution_count=37}\n```\narray([[ 2.71320643],\n       [-4.79248051],\n       [ 1.33648235],\n       [ 2.48803883],\n       [-0.01492988]])\n```\n:::\n:::\n\n\nTo get the ReLU of this vector, we want all the negative values to become zeros.\n\nFirst create a copy of this vector.\n\n::: {#b0c1b7eb .cell execution_count=38}\n``` {.python .cell-code}\nh = z_1.copy()\n```\n:::\n\n\nNow determine which of its values are negative.\n\n::: {#ae6ecba7 .cell execution_count=39}\n``` {.python .cell-code}\nh < 0\n```\n\n::: {.cell-output .cell-output-display execution_count=39}\n```\narray([[False],\n       [ True],\n       [False],\n       [False],\n       [ True]])\n```\n:::\n:::\n\n\nWe can now simply set all of the values which are negative to 0.\n\n::: {#ae21a9f3 .cell execution_count=40}\n``` {.python .cell-code}\nh[h < 0] = 0\n```\n:::\n\n\nAnd that's it: we have the ReLU of $\\mathbf{z_1}$!\n\n::: {#974dcf3b .cell execution_count=41}\n``` {.python .cell-code}\nh\n```\n\n::: {.cell-output .cell-output-display execution_count=41}\n```\narray([[2.71320643],\n       [0.        ],\n       [1.33648235],\n       [2.48803883],\n       [0.        ]])\n```\n:::\n:::\n\n\n**Now implement ReLU as a function.**\n\n::: {#63ba1a0b .cell execution_count=42}\n``` {.python .cell-code}\ndef relu(z):\n    # BEGIN your code here\n    result = z.copy()\n    result[result < 0] = 0\n    # END your code here\n    \n    return result\n```\n:::\n\n\n**And check that it's working.**\n\n::: {#c24ac88c .cell execution_count=43}\n``` {.python .cell-code}\nz = np.array([[-1.25459881], [ 4.50714306], [ 2.31993942], [ 0.98658484], [-3.4398136 ]])\nrelu(z)\n```\n\n::: {.cell-output .cell-output-display execution_count=43}\n```\narray([[0.        ],\n       [4.50714306],\n       [2.31993942],\n       [0.98658484],\n       [0.        ]])\n```\n:::\n:::\n\n\nExpected output:\n\n    array([[0.        ],\n           [4.50714306],\n           [2.31993942],\n           [0.98658484],\n           [0.        ]])\n\n### Softmax\n\nThe second activation function that we need is softmax. This function is used to calculate the values of the output layer of the neural network, using the following formulas:\n\n\\begin{align}\n \\mathbf{z_2} &= \\mathbf{W_2}\\mathbf{h} + \\mathbf{b_2}   \\tag{3} \\\\\n \\mathbf{\\hat y} &= \\mathrm{softmax}(\\mathbf{z_2})   \\tag{4} \\\\\n\\end{align}\n\nTo calculate softmax of a vector $\\mathbf{z}$, the $i$-th component of the resulting vector is given by:\n\n$$ \\textrm{softmax}(\\textbf{z})_i = \\frac{e^{z_i} }{\\sum\\limits_{j=1}^{V} e^{z_j} }  \\tag{5} $$\n\nLet's work through an example.\n\n::: {#f54665c7 .cell execution_count=44}\n``` {.python .cell-code}\nz = np.array([9, 8, 11, 10, 8.5])\nz\n```\n\n::: {.cell-output .cell-output-display execution_count=44}\n```\narray([ 9. ,  8. , 11. , 10. ,  8.5])\n```\n:::\n:::\n\n\nYou'll need to calculate the exponentials of each element, both for the numerator and for the denominator.\n\n::: {#c520511b .cell execution_count=45}\n``` {.python .cell-code}\ne_z = np.exp(z)\ne_z\n```\n\n::: {.cell-output .cell-output-display execution_count=45}\n```\narray([ 8103.08392758,  2980.95798704, 59874.1417152 , 22026.46579481,\n        4914.7688403 ])\n```\n:::\n:::\n\n\nThe denominator is equal to the sum of these exponentials.\n\n::: {#58715f08 .cell execution_count=46}\n``` {.python .cell-code}\nsum_e_z = np.sum(e_z)\nsum_e_z\n```\n\n::: {.cell-output .cell-output-display execution_count=46}\n```\nnp.float64(97899.41826492078)\n```\n:::\n:::\n\n\nAnd the value of the first element of $\\textrm{softmax}(\\textbf{z})$ is given by:\n\n::: {#998076c0 .cell execution_count=47}\n``` {.python .cell-code}\ne_z[0]/sum_e_z\n```\n\n::: {.cell-output .cell-output-display execution_count=47}\n```\nnp.float64(0.08276947985173956)\n```\n:::\n:::\n\n\nThis is for one element. We can use numpy's vectorized operations to calculate the values of all the elements of the $\\textrm{softmax}(\\textbf{z})$ vector in one go.\n\n**Implement the softmax function.**\n\n::: {#5ee015f8 .cell execution_count=48}\n``` {.python .cell-code}\ndef softmax(z):\n    # BEGIN your code here\n    e_z = np.exp(z)\n    sum_e_z = np.sum(e_z)\n    return e_z / sum_e_z\n    # END your code here\n```\n:::\n\n\n**Now check that it works.**\n\n::: {#5d34c454 .cell execution_count=49}\n``` {.python .cell-code}\nsoftmax([9, 8, 11, 10, 8.5])\n```\n\n::: {.cell-output .cell-output-display execution_count=49}\n```\narray([0.08276948, 0.03044919, 0.61158833, 0.22499077, 0.05020223])\n```\n:::\n:::\n\n\nExpected output:\n\n    array([0.08276948, 0.03044919, 0.61158833, 0.22499077, 0.05020223])\n\n## Dimensions: 1-D arrays vs 2-D column vectors\n\nBefore moving on to implement forward propagation, backpropagation, and gradient descent, let's have a look at the dimensions of the vectors you've been handling until now.\n\nCreate a vector of length $V$ filled with zeros.\n\n::: {#d3944f2a .cell execution_count=50}\n``` {.python .cell-code}\nx_array = np.zeros(V)\nx_array\n```\n\n::: {.cell-output .cell-output-display execution_count=50}\n```\narray([0., 0., 0., 0., 0.])\n```\n:::\n:::\n\n\nThis is a 1-dimensional array, as revealed by the `.shape` property of the array.\n\n::: {#0d19cb59 .cell execution_count=51}\n``` {.python .cell-code}\nx_array.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=51}\n```\n(5,)\n```\n:::\n:::\n\n\nTo perform matrix multiplication in the next steps, we actually need your column vectors to be represented as a matrix with one column. In numpy, this matrix is represented as a 2-dimensional array.\n\nThe easiest way to convert a 1D vector to a 2D column matrix is to set its `.shape` property to the number of rows and one column, as shown in the next cell.\n\n::: {#2b18a06b .cell execution_count=52}\n``` {.python .cell-code}\nx_column_vector = x_array.copy()\nx_column_vector.shape = (V, 1)  # alternatively ... = (x_array.shape[0], 1)\nx_column_vector\n```\n\n::: {.cell-output .cell-output-display execution_count=52}\n```\narray([[0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.]])\n```\n:::\n:::\n\n\nThe shape of the resulting \"vector\" is:\n\n::: {#c939ef44 .cell execution_count=53}\n``` {.python .cell-code}\nx_column_vector.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=53}\n```\n(5, 1)\n```\n:::\n:::\n\n\nSo we now have a 5x1 matrix that we can use to perform standard matrix multiplication.\n\n## Forward propagation\n\nLet's dive into the neural network itself, which is shown below with all the dimensions and formulas you'll need.\n\n<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='cbow_model_dimensions_single_input.png?2' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:839;height:349;\" /> Figure 2 </div>\n\nSet $N$ equal to 3. Remember that $N$ is a hyperparameter of the CBOW model that represents the size of the word embedding vectors, as well as the size of the hidden layer.\n\n::: {#c72c2fad .cell execution_count=54}\n``` {.python .cell-code}\nN = 3\n```\n:::\n\n\n### Initialization of the weights and biases\n\nBefore we start training the neural network, we need to initialize the weight matrices and bias vectors with random values.\n\nIn the assignment we will implement a function to do this yourself using `numpy.random.rand`. In this notebook, we've pre-populated these matrices and vectors for you.\n\n::: {#38efc00e .cell execution_count=55}\n``` {.python .cell-code}\nW1 = np.array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n               [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n               [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n\nW2 = np.array([[-0.22182064, -0.43008631,  0.13310965],\n               [ 0.08476603,  0.08123194,  0.1772054 ],\n               [ 0.1871551 , -0.06107263, -0.1790735 ],\n               [ 0.07055222, -0.02015138,  0.36107434],\n               [ 0.33480474, -0.39423389, -0.43959196]])\n\nb1 = np.array([[ 0.09688219],\n               [ 0.29239497],\n               [-0.27364426]])\n\nb2 = np.array([[ 0.0352008 ],\n               [-0.36393384],\n               [-0.12775555],\n               [-0.34802326],\n               [-0.07017815]])\n```\n:::\n\n\n**Check that the dimensions of these matrices match those shown in the figure above.**\n\n::: {#1b0a3518 .cell execution_count=56}\n``` {.python .cell-code}\n# BEGIN your code here\nprint(f'V (vocabulary size): {V}')\nprint(f'N (embedding size / size of the hidden layer): {N}')\nprint(f'size of W1: {W1.shape} (NxV)')\nprint(f'size of b1: {b1.shape} (Nx1)')\nprint(f'size of W2: {W1.shape} (VxN)')\nprint(f'size of b2: {b2.shape} (Vx1)')\n# END your code here\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nV (vocabulary size): 5\nN (embedding size / size of the hidden layer): 3\nsize of W1: (3, 5) (NxV)\nsize of b1: (3, 1) (Nx1)\nsize of W2: (3, 5) (VxN)\nsize of b2: (5, 1) (Vx1)\n```\n:::\n:::\n\n\n### Training example\n\nRun the next cells to get the first training example, made of the vector representing the context words \"i am because i\", and the target which is the one-hot vector representing the center word \"happy\".\n\n> We don't need to worry about the Python syntax, but there are some explanations below if we want to know what's happening behind the scenes.\n\n::: {#4e600538 .cell execution_count=57}\n``` {.python .cell-code}\ntraining_examples = get_training_example(words, 2, word2Ind, V)\n```\n:::\n\n\n> `get_training_examples`, which uses the `yield` keyword, is known as a generator. When run, it builds an iterator, which is a special type of object that... we can iterate on (using a `for` loop for instance), to retrieve the successive values that the function generates.\n>\n> In this case `get_training_examples` `yield`s training examples, and iterating on `training_examples` will return the successive training examples.\n\n::: {#52657aae .cell execution_count=58}\n``` {.python .cell-code}\nx_array, y_array = next(training_examples)\n```\n:::\n\n\n> `next` is another special keyword, which gets the next available value from an iterator. Here, you'll get the very first value, which is the first training example. If we run this cell again, you'll get the next value, and so on until the iterator runs out of values to return.\n>\n> In this notebook `next` is used because we will only be performing one iteration of training. In this week's assignment with the full training over several iterations you'll use regular `for` loops with the iterator that supplies the training examples.\n\nThe vector representing the context words, which will be fed into the neural network, is:\n\n::: {#de8ee87f .cell execution_count=59}\n``` {.python .cell-code}\nx_array\n```\n\n::: {.cell-output .cell-output-display execution_count=59}\n```\narray([0.25, 0.25, 0.  , 0.5 , 0.  ])\n```\n:::\n:::\n\n\nThe one-hot vector representing the center word to be predicted is:\n\n::: {#d420eda0 .cell execution_count=60}\n``` {.python .cell-code}\ny_array\n```\n\n::: {.cell-output .cell-output-display execution_count=60}\n```\narray([0., 0., 1., 0., 0.])\n```\n:::\n:::\n\n\nNow convert these vectors into matrices (or 2D arrays) to be able to perform matrix multiplication on the right types of objects, as explained above.\n\n::: {#e7786ea7 .cell execution_count=61}\n``` {.python .cell-code}\nx = x_array.copy()\nx.shape = (V, 1)\nprint('x')\nprint(x)\nprint()\n\ny = y_array.copy()\ny.shape = (V, 1)\nprint('y')\nprint(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nx\n[[0.25]\n [0.25]\n [0.  ]\n [0.5 ]\n [0.  ]]\n\ny\n[[0.]\n [0.]\n [1.]\n [0.]\n [0.]]\n```\n:::\n:::\n\n\n### Values of the hidden layer\n\nNow that we have initialized all the variables that we need for forward propagation, we can calculate the values of the hidden layer using the following formulas:\n\n\\begin{align}\n \\mathbf{z_1} = \\mathbf{W_1}\\mathbf{x} + \\mathbf{b_1}  \\tag{1} \\\\\n \\mathbf{h} = \\mathrm{ReLU}(\\mathbf{z_1})  \\tag{2} \\\\\n\\end{align}\n\nFirst, we can calculate the value of $\\mathbf{z_1}$.\n\n::: {#7097af81 .cell execution_count=62}\n``` {.python .cell-code}\nz1 = np.dot(W1, x) + b1\n```\n:::\n\n\n> `np.dot` is numpy's function for matrix multiplication.\n\nAs expected we get an $N$ by 1 matrix, or column vector with $N$ elements, where $N$ is equal to the embedding size, which is 3 in this example.\n\n::: {#01421ebb .cell execution_count=63}\n``` {.python .cell-code}\nz1\n```\n\n::: {.cell-output .cell-output-display execution_count=63}\n```\narray([[ 0.36483875],\n       [ 0.63710329],\n       [-0.3236647 ]])\n```\n:::\n:::\n\n\nWe can now take the ReLU of $\\mathbf{z_1}$ to get $\\mathbf{h}$, the vector with the values of the hidden layer.\n\n::: {#2b3d7755 .cell execution_count=64}\n``` {.python .cell-code}\nh = relu(z1)\nh\n```\n\n::: {.cell-output .cell-output-display execution_count=64}\n```\narray([[0.36483875],\n       [0.63710329],\n       [0.        ]])\n```\n:::\n:::\n\n\nApplying ReLU means that the negative element of $\\mathbf{z_1}$ has been replaced with a zero.\n\n### Values of the output layer\n\nHere are the formulas we need to calculate the values of the output layer, represented by the vector $\\mathbf{\\hat y}$:\n\n\\begin{align}\n \\mathbf{z_2} &= \\mathbf{W_2}\\mathbf{h} + \\mathbf{b_2}   \\tag{3} \\\\\n \\mathbf{\\hat y} &= \\mathrm{softmax}(\\mathbf{z_2})   \\tag{4} \\\\\n\\end{align}\n\n**First, calculate $\\mathbf{z_2}$.**\n\n::: {#a77d3463 .cell execution_count=65}\n``` {.python .cell-code}\n# BEGIN your code here\nz2 = np.dot(W2, h) + b2\n# END your code here\n\nz2\n```\n\n::: {.cell-output .cell-output-display execution_count=65}\n```\narray([[-0.31973737],\n       [-0.28125477],\n       [-0.09838369],\n       [-0.33512159],\n       [-0.19919612]])\n```\n:::\n:::\n\n\nExpected output:\n\n    array([[-0.31973737],\n           [-0.28125477],\n           [-0.09838369],\n           [-0.33512159],\n           [-0.19919612]])\n\nThis is a $V$ by 1 matrix, where $V$ is the size of the vocabulary, which is 5 in this example.\n\n**Now calculate the value of $\\mathbf{\\hat y}$.**\n\n::: {#659c5b23 .cell execution_count=66}\n``` {.python .cell-code}\n# BEGIN your code here\ny_hat = softmax(z2)\n# END your code here\n\ny_hat\n```\n\n::: {.cell-output .cell-output-display execution_count=66}\n```\narray([[0.18519074],\n       [0.19245626],\n       [0.23107446],\n       [0.18236353],\n       [0.20891502]])\n```\n:::\n:::\n\n\nExpected output:\n\n    array([[0.18519074],\n           [0.19245626],\n           [0.23107446],\n           [0.18236353],\n           [0.20891502]])\n\nAs you've performed the calculations with random matrices and vectors (apart from the input vector), the output of the neural network is essentially random at this point. The learning process will adjust the weights and biases to match the actual targets better.\n\n**That being said, what word did the neural network predict?**\n\n<details>    \n<summary>\n    <font size=\"3\" color=\"darkgreen\"><b>Solution</b></font>\n</summary>\n<p>The neural network predicted the word \"happy\": the largest element of $\\mathbf{\\hat y}$ is the third one, and the third word of the vocabulary is \"happy\".</p>\n<p>Here's how we could implement this in Python:</p>\n<p><code>print(Ind2word[np.argmax(y_hat)])</code></p>\n</details>\n\nWell done, you've completed the forward propagation phase!\n\n## Cross-entropy loss\n\nNow that we have the network's prediction, we can calculate the cross-entropy loss to determine how accurate the prediction was compared to the actual target.\n\n> Remember that we are working on a single training example, not on a batch of examples, which is why we are using *loss* and not *cost*, which is the generalized form of loss.\n\nFirst let's recall what the prediction was.\n\n::: {#dc0df036 .cell execution_count=67}\n``` {.python .cell-code}\ny_hat\n```\n\n::: {.cell-output .cell-output-display execution_count=67}\n```\narray([[0.18519074],\n       [0.19245626],\n       [0.23107446],\n       [0.18236353],\n       [0.20891502]])\n```\n:::\n:::\n\n\nAnd the actual target value is:\n\n::: {#ba2cbe85 .cell execution_count=68}\n``` {.python .cell-code}\ny\n```\n\n::: {.cell-output .cell-output-display execution_count=68}\n```\narray([[0.],\n       [0.],\n       [1.],\n       [0.],\n       [0.]])\n```\n:::\n:::\n\n\nThe formula for cross-entropy loss is:\n\n$$ J=-\\sum\\limits_{k=1}^{V}y_k\\log{\\hat{y}_k} \\tag{6}$$\n\n**Implement the cross-entropy loss function.**\n\nHere are a some hints if you're stuck.\n\n<details>    \n<summary>\n    <font size=\"3\" color=\"darkgreen\"><b>Hint 1</b></font>\n</summary>\n    <p>To multiply two numpy matrices (such as <code>y</code> and <code>y_hat</code>) element-wise, we can simply use the <code>*</code> operator.</p>\n</details>\n\n<details>    \n<summary>\n    <font size=\"3\" color=\"darkgreen\"><b>Hint 2</b></font>\n</summary>\n<p>Once we have a vector equal to the element-wise multiplication of <code>y</code> and <code>y_hat</code>, we can use <code>np.sum</code> to calculate the sum of the elements of this vector.</p>\n</details>\n\n::: {#ca276b74 .cell execution_count=69}\n``` {.python .cell-code}\ndef cross_entropy_loss(y_predicted, y_actual):\n    # BEGIN your code here\n    loss = np.sum(-np.log(y_hat)*y)\n    # END your code here\n    return loss\n```\n:::\n\n\n**Now use this function to calculate the loss with the actual values of $\\mathbf{y}$ and $\\mathbf{\\hat y}$.**\n\n::: {#71ec3060 .cell execution_count=70}\n``` {.python .cell-code}\ncross_entropy_loss(y_hat, y)\n```\n\n::: {.cell-output .cell-output-display execution_count=70}\n```\nnp.float64(1.4650152923611108)\n```\n:::\n:::\n\n\nExpected output:\n\n    1.4650152923611106\n\nThis value is neither good nor bad, which is expected as the neural network hasn't learned anything yet.\n\nThe actual learning will start during the next phase: backpropagation.\n\n## Backpropagation\n\nThe formulas that we will implement for backpropagation are the following.\n\n\\begin{align}\n \\frac{\\partial J}{\\partial \\mathbf{W_1}} &= \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right )\\mathbf{x}^\\top \\tag{7}\\\\\n \\frac{\\partial J}{\\partial \\mathbf{W_2}} &= (\\mathbf{\\hat{y}} - \\mathbf{y})\\mathbf{h^\\top} \\tag{8}\\\\\n \\frac{\\partial J}{\\partial \\mathbf{b_1}} &= \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right ) \\tag{9}\\\\\n \\frac{\\partial J}{\\partial \\mathbf{b_2}} &= \\mathbf{\\hat{y}} - \\mathbf{y} \\tag{10}\n\\end{align}\n\n> Note: these formulas are slightly simplified compared to the ones in the lecture as you're working on a single training example, whereas the lecture provided the formulas for a batch of examples. In the assignment you'll be implementing the latter.\n\nLet's start with an easy one.\n\n**Calculate the partial derivative of the loss function with respect to $\\mathbf{b_2}$, and store the result in `grad_b2`.**\n\n$$\\frac{\\partial J}{\\partial \\mathbf{b_2}} = \\mathbf{\\hat{y}} - \\mathbf{y} \\tag{10}$$\n\n::: {#df89c90a .cell execution_count=71}\n``` {.python .cell-code}\n# BEGIN your code here\ngrad_b2 = y_hat - y\n# END your code here\n\ngrad_b2\n```\n\n::: {.cell-output .cell-output-display execution_count=71}\n```\narray([[ 0.18519074],\n       [ 0.19245626],\n       [-0.76892554],\n       [ 0.18236353],\n       [ 0.20891502]])\n```\n:::\n:::\n\n\nExpected output:\n\n    array([[ 0.18519074],\n           [ 0.19245626],\n           [-0.76892554],\n           [ 0.18236353],\n           [ 0.20891502]])\n\n**Next, calculate the partial derivative of the loss function with respect to $\\mathbf{W_2}$, and store the result in `grad_W2`.**\n\n$$\\frac{\\partial J}{\\partial \\mathbf{W_2}} = (\\mathbf{\\hat{y}} - \\mathbf{y})\\mathbf{h^\\top} \\tag{8}$$\n\n> Hint: use `.T` to get a transposed matrix, e.g. `h.T` returns $\\mathbf{h^\\top}$.\n\n::: {#644c7d3f .cell execution_count=72}\n``` {.python .cell-code}\n# BEGIN your code here\ngrad_W2 = np.dot(y_hat - y, h.T)\n# END your code here\n\ngrad_W2\n```\n\n::: {.cell-output .cell-output-display execution_count=72}\n```\narray([[ 0.06756476,  0.11798563,  0.        ],\n       [ 0.0702155 ,  0.12261452,  0.        ],\n       [-0.28053384, -0.48988499,  0.        ],\n       [ 0.06653328,  0.1161844 ,  0.        ],\n       [ 0.07622029,  0.13310045,  0.        ]])\n```\n:::\n:::\n\n\nExpected output:\n\n    array([[ 0.06756476,  0.11798563,  0.        ],\n           [ 0.0702155 ,  0.12261452,  0.        ],\n           [-0.28053384, -0.48988499,  0.        ],\n           [ 0.06653328,  0.1161844 ,  0.        ],\n           [ 0.07622029,  0.13310045,  0.        ]])\n\n**Now calculate the partial derivative with respect to $\\mathbf{b_1}$ and store the result in `grad_b1`.**\n\n$$\\frac{\\partial J}{\\partial \\mathbf{b_1}} = \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right ) \\tag{9}$$\n\n::: {#da359994 .cell execution_count=73}\n``` {.python .cell-code}\n# BEGIN your code here\ngrad_b1 = relu(np.dot(W2.T, y_hat - y))\n# END your code here\n\ngrad_b1\n```\n\n::: {.cell-output .cell-output-display execution_count=73}\n```\narray([[0.        ],\n       [0.        ],\n       [0.17045858]])\n```\n:::\n:::\n\n\nExpected output:\n\n    array([[0.        ],\n           [0.        ],\n           [0.17045858]])\n\n**Finally, calculate the partial derivative of the loss with respect to $\\mathbf{W_1}$, and store it in `grad_W1`.**\n\n$$\\frac{\\partial J}{\\partial \\mathbf{W_1}} = \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right )\\mathbf{x}^\\top \\tag{7}$$\n\n::: {#aaa1e97d .cell execution_count=74}\n``` {.python .cell-code}\n# BEGIN your code here\ngrad_W1 = np.dot(relu(np.dot(W2.T, y_hat - y)), x.T)\n# END your code here\n\ngrad_W1\n```\n\n::: {.cell-output .cell-output-display execution_count=74}\n```\narray([[0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.04261464, 0.04261464, 0.        , 0.08522929, 0.        ]])\n```\n:::\n:::\n\n\nExpected output:\n\n    array([[0.        , 0.        , 0.        , 0.        , 0.        ],\n           [0.        , 0.        , 0.        , 0.        , 0.        ],\n           [0.04261464, 0.04261464, 0.        , 0.08522929, 0.        ]])\n\nBefore moving on to gradient descent, double-check that all the matrices have the expected dimensions.\n\n::: {#1e7f8c71 .cell execution_count=75}\n``` {.python .cell-code}\n# BEGIN your code here\nprint(f'V (vocabulary size): {V}')\nprint(f'N (embedding size / size of the hidden layer): {N}')\nprint(f'size of grad_W1: {grad_W1.shape} (NxV)')\nprint(f'size of grad_b1: {grad_b1.shape} (Nx1)')\nprint(f'size of grad_W2: {grad_W1.shape} (VxN)')\nprint(f'size of grad_b2: {grad_b2.shape} (Vx1)')\n# END your code here\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nV (vocabulary size): 5\nN (embedding size / size of the hidden layer): 3\nsize of grad_W1: (3, 5) (NxV)\nsize of grad_b1: (3, 1) (Nx1)\nsize of grad_W2: (3, 5) (VxN)\nsize of grad_b2: (5, 1) (Vx1)\n```\n:::\n:::\n\n\n## Gradient descent\n\nDuring the gradient descent phase, we will update the weights and biases by subtracting $\\alpha$ times the gradient from the original matrices and vectors, using the following formulas.\n\n\\begin{align}\n \\mathbf{W_1} &:= \\mathbf{W_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_1}} \\tag{11}\\\\\n \\mathbf{W_2} &:= \\mathbf{W_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_2}} \\tag{12}\\\\\n \\mathbf{b_1} &:= \\mathbf{b_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_1}} \\tag{13}\\\\\n \\mathbf{b_2} &:= \\mathbf{b_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_2}} \\tag{14}\\\\\n\\end{align}\n\nFirst, let set a value for $\\alpha$.\n\n::: {#73768f08 .cell execution_count=76}\n``` {.python .cell-code}\nalpha = 0.03\n```\n:::\n\n\nThe updated weight matrix $\\mathbf{W_1}$ will be:\n\n::: {#189a08df .cell execution_count=77}\n``` {.python .cell-code}\nW1_new = W1 - alpha * grad_W1\n```\n:::\n\n\nLet's compare the previous and new values of $\\mathbf{W_1}$:\n\n::: {#b5964336 .cell execution_count=78}\n``` {.python .cell-code}\nprint('old value of W1:')\nprint(W1)\nprint()\nprint('new value of W1:')\nprint(W1_new)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nold value of W1:\n[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]\n [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]\n [ 0.26637602 -0.23846886 -0.37770863 -0.11399446  0.34008124]]\n\nnew value of W1:\n[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]\n [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]\n [ 0.26509758 -0.2397473  -0.37770863 -0.11655134  0.34008124]]\n```\n:::\n:::\n\n\nThe difference is very subtle (hint: take a closer look at the last row), which is why it takes a fair amount of iterations to train the neural network until it reaches optimal weights and biases starting from random values.\n\n**Now calculate the new values of $\\mathbf{W_2}$ (to be stored in `W2_new`), $\\mathbf{b_1}$ (in `b1_new`), and $\\mathbf{b_2}$ (in `b2_new`).**\n\n\\begin{align}\n \\mathbf{W_2} &:= \\mathbf{W_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_2}} \\tag{12}\\\\\n \\mathbf{b_1} &:= \\mathbf{b_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_1}} \\tag{13}\\\\\n \\mathbf{b_2} &:= \\mathbf{b_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_2}} \\tag{14}\\\\\n\\end{align}\n\n::: {#b5909a13 .cell execution_count=79}\n``` {.python .cell-code}\n# BEGIN your code here\nW2_new = W2 - alpha * grad_W2\nb1_new = b1 - alpha * grad_b1\nb2_new = b2 - alpha * grad_b2\n# END your code here\n\nprint('W2_new')\nprint(W2_new)\nprint()\nprint('b1_new')\nprint(b1_new)\nprint()\nprint('b2_new')\nprint(b2_new)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nW2_new\n[[-0.22384758 -0.43362588  0.13310965]\n [ 0.08265956  0.0775535   0.1772054 ]\n [ 0.19557112 -0.04637608 -0.1790735 ]\n [ 0.06855622 -0.02363691  0.36107434]\n [ 0.33251813 -0.3982269  -0.43959196]]\n\nb1_new\n[[ 0.09688219]\n [ 0.29239497]\n [-0.27875802]]\n\nb2_new\n[[ 0.02964508]\n [-0.36970753]\n [-0.10468778]\n [-0.35349417]\n [-0.0764456 ]]\n```\n:::\n:::\n\n\nExpected output:\n\n    W2_new\n    [[-0.22384758 -0.43362588  0.13310965]\n     [ 0.08265956  0.0775535   0.1772054 ]\n     [ 0.19557112 -0.04637608 -0.1790735 ]\n     [ 0.06855622 -0.02363691  0.36107434]\n     [ 0.33251813 -0.3982269  -0.43959196]]\n\n    b1_new\n    [[ 0.09688219]\n     [ 0.29239497]\n     [-0.27875802]]\n\n    b2_new\n    [[ 0.02964508]\n     [-0.36970753]\n     [-0.10468778]\n     [-0.35349417]\n     [-0.0764456 ]]\n\nCongratulations, we have completed one iteration of training using one training example!\n\nYou'll need many more iterations to fully train the neural network, and we can optimize the learning process by training on batches of examples, as described in the lecture. We will get to do this during this week's assignment.\n\n## Extracting word embedding vectors\n\nOnce we have finished training the neural network, we have three options to get word embedding vectors for the words of your vocabulary, based on the weight matrices $\\mathbf{W_1}$ and/or $\\mathbf{W_2}$.\n\n### Option 1: extract embedding vectors from $\\mathbf{W_1}$\n\nThe first option is to take the columns of $\\mathbf{W_1}$ as the embedding vectors of the words of the vocabulary, using the same order of the words as for the input and output vectors.\n\n> Note: in this practice notebook the values of the word embedding vectors are meaningless after a single iteration with just one training example, but here's how we would proceed after the training process is complete.\n\nFor example $\\mathbf{W_1}$ is this matrix:\n\n::: {#4472ef0d .cell execution_count=80}\n``` {.python .cell-code}\nW1\n```\n\n::: {.cell-output .cell-output-display execution_count=80}\n```\narray([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n       [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n       [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n```\n:::\n:::\n\n\nThe first column, which is a 3-element vector, is the embedding vector of the first word of your vocabulary. The second column is the word embedding vector for the second word, and so on.\n\nThe first, second, etc. words are ordered as follows.\n\n::: {#0b1785ab .cell execution_count=81}\n``` {.python .cell-code}\nfor i in range(V):\n    print(Ind2word[i])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nam\nbecause\nhappy\ni\nlearning\n```\n:::\n:::\n\n\nSo the word embedding vectors corresponding to each word are:\n\n::: {#af56a4c3 .cell execution_count=82}\n``` {.python .cell-code}\n# loop through each word of the vocabulary\nfor word in word2Ind:\n    # extract the column corresponding to the index of the word in the vocabulary\n    word_embedding_vector = W1[:, word2Ind[word]]\n    \n    print(f'{word}: {word_embedding_vector}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nam: [0.41687358 0.32735501 0.26637602]\nbecause: [ 0.08854191  0.22795148 -0.23846886]\nhappy: [-0.23495225 -0.23951958 -0.37770863]\ni: [ 0.28320538  0.4117634  -0.11399446]\nlearning: [ 0.41800106 -0.23924344  0.34008124]\n```\n:::\n:::\n\n\n### Option 2: extract embedding vectors from $\\mathbf{W_2}$\n\nThe second option is to take $\\mathbf{W_2}$ transposed, and take its columns as the word embedding vectors just like we did for $\\mathbf{W_1}$.\n\n::: {#d72fea0c .cell execution_count=83}\n``` {.python .cell-code}\nW2.T\n```\n\n::: {.cell-output .cell-output-display execution_count=83}\n```\narray([[-0.22182064,  0.08476603,  0.1871551 ,  0.07055222,  0.33480474],\n       [-0.43008631,  0.08123194, -0.06107263, -0.02015138, -0.39423389],\n       [ 0.13310965,  0.1772054 , -0.1790735 ,  0.36107434, -0.43959196]])\n```\n:::\n:::\n\n\n::: {#7bcfcbc7 .cell execution_count=84}\n``` {.python .cell-code}\n# loop through each word of the vocabulary\nfor word in word2Ind:\n    # extract the column corresponding to the index of the word in the vocabulary\n    word_embedding_vector = W2.T[:, word2Ind[word]]\n    \n    print(f'{word}: {word_embedding_vector}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nam: [-0.22182064 -0.43008631  0.13310965]\nbecause: [0.08476603 0.08123194 0.1772054 ]\nhappy: [ 0.1871551  -0.06107263 -0.1790735 ]\ni: [ 0.07055222 -0.02015138  0.36107434]\nlearning: [ 0.33480474 -0.39423389 -0.43959196]\n```\n:::\n:::\n\n\n### Option 3: extract embedding vectors from $\\mathbf{W_1}$ and $\\mathbf{W_2}$\n\nThe third option, which is the one we will use in this week's assignment, uses the average of $\\mathbf{W_1}$ and $\\mathbf{W_2^\\top}$.\n\n**Calculate the average of $\\mathbf{W_1}$ and $\\mathbf{W_2^\\top}$, and store the result in `W3`.**\n\n::: {#9c90db50 .cell execution_count=85}\n``` {.python .cell-code}\n# BEGIN your code here\nW3 = (W1+W2.T)/2\n# END your code here\n\nW3\n```\n\n::: {.cell-output .cell-output-display execution_count=85}\n```\narray([[ 0.09752647,  0.08665397, -0.02389858,  0.1768788 ,  0.3764029 ],\n       [-0.05136565,  0.15459171, -0.15029611,  0.19580601, -0.31673866],\n       [ 0.19974284, -0.03063173, -0.27839106,  0.12353994, -0.04975536]])\n```\n:::\n:::\n\n\nExpected output:\n\n    array([[ 0.09752647,  0.08665397, -0.02389858,  0.1768788 ,  0.3764029 ],\n           [-0.05136565,  0.15459171, -0.15029611,  0.19580601, -0.31673866],\n           [ 0.19974284, -0.03063173, -0.27839106,  0.12353994, -0.04975536]])\n\nExtracting the word embedding vectors works just like the two previous options, by taking the columns of the matrix you've just created.\n\n::: {#962f6080 .cell execution_count=86}\n``` {.python .cell-code}\n# loop through each word of the vocabulary\nfor word in word2Ind:\n    # extract the column corresponding to the index of the word in the vocabulary\n    word_embedding_vector = W3[:, word2Ind[word]]\n    \n    print(f'{word}: {word_embedding_vector}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nam: [ 0.09752647 -0.05136565  0.19974284]\nbecause: [ 0.08665397  0.15459171 -0.03063173]\nhappy: [-0.02389858 -0.15029611 -0.27839106]\ni: [0.1768788  0.19580601 0.12353994]\nlearning: [ 0.3764029  -0.31673866 -0.04975536]\n```\n:::\n:::\n\n\nYou're now ready to take on this week's assignment!\n\n### How this practice relates to and differs from the upcoming graded assignment\n\n- In the assignment, for each iteration of training we will use batches of examples instead of a single example. The formulas for forward propagation and backpropagation will be modified accordingly, and we will use cross-entropy cost instead of cross-entropy loss.\n- We will also complete several iterations of training, until we reach an acceptably low cross-entropy cost, at which point we can extract good word embeddings from the weight matrices.\n- After extracting the word embedding vectors, we will use principal component analysis (PCA) to visualize the vectors, which will enable we to perform an intrinsic evaluation of the quality of the vectors, as explained in the lecture.\n\n",
    "supporting": [
      "lab04_files"
    ],
    "filters": [],
    "includes": {}
  }
}