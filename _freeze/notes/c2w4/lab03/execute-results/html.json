{
  "hash": "75591426e87b258d0acf5912fe964a04",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ndate: 2020-11-02\ntitle: 'Word Embeddings: Training the CBOW model'\ncategories: \n  - NLP \n  - Coursera \n  - Lab\n  - Probabilistic Models\njupyter: python3\n# execute: \n#     error: true\n---\n\n\n\n\n::: {.column-margin .nolightbox}\n![course banner](/images/Course-Logo-2-3.webp)\n:::\n\nIn previous lecture notebooks we saw \n\n1. how to prepare data before feeding it to a continuous bag-of-words model, \n2. the model itself, its architecture and activation functions. \n\nThis notebook will walk we through:\n\n- Forward propagation.\n- Cross-entropy loss.\n- Backpropagation.\n- Gradient descent.\n\nWhich are concepts necessary to understand how the training of the model works.\n\nLet's dive into it!\n\n::: {#0936f0ad .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nfrom utils2 import get_dict\n```\n:::\n\n\n## Forward propagation\n\nLet's dive into the neural network itself, which is shown below with all the dimensions and formulas you'll need.\n\n![](img/cbow_model_dimensions_single_input.png){#fig-02  .column-margin   style=\"width:839;height:349;\" }\n\nSet $N$ equal to 3. Remember that $N$ is a hyperparameter of the CBOW model that represents the size of the word embedding vectors, as well as the size of the hidden layer.\n\nAlso set $V$ equal to 5, which is the size of the vocabulary we have used so far.\n\n::: {#bf253a02 .cell execution_count=2}\n``` {.python .cell-code}\n# Define the size of the word embedding vectors and save it in the variable 'N'\nN = 3\n\n# Define V. Remember this was the size of the vocabulary in the previous lecture notebooks\nV = 5\n```\n:::\n\n\n### Initialization of the weights and biases\n\nBefore we start training the neural network, we need to initialize the weight matrices and bias vectors with random values.\n\nIn the assignment we will implement a function to do this yourself using `numpy.random.rand`. In this notebook, we've pre-populated these matrices and vectors for you.\n\n::: {#16ec59a9 .cell execution_count=3}\n``` {.python .cell-code}\n# Define first matrix of weights\nW1 = np.array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n               [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n               [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n\n# Define second matrix of weights\nW2 = np.array([[-0.22182064, -0.43008631,  0.13310965],\n               [ 0.08476603,  0.08123194,  0.1772054 ],\n               [ 0.1871551 , -0.06107263, -0.1790735 ],\n               [ 0.07055222, -0.02015138,  0.36107434],\n               [ 0.33480474, -0.39423389, -0.43959196]])\n\n# Define first vector of biases\nb1 = np.array([[ 0.09688219],\n               [ 0.29239497],\n               [-0.27364426]])\n\n# Define second vector of biases\nb2 = np.array([[ 0.0352008 ],\n               [-0.36393384],\n               [-0.12775555],\n               [-0.34802326],\n               [-0.07017815]])\n```\n:::\n\n\n**Check that the dimensions of these matrices match those shown in the figure above.**\n\n::: {#246f93e6 .cell execution_count=4}\n``` {.python .cell-code}\nprint(f'V (vocabulary size): {V}')\nprint(f'N (embedding size / size of the hidden layer): {N}')\nprint(f'size of W1: {W1.shape} (NxV)')\nprint(f'size of b1: {b1.shape} (Nx1)')\nprint(f'size of W2: {W2.shape} (VxN)')\nprint(f'size of b2: {b2.shape} (Vx1)')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nV (vocabulary size): 5\nN (embedding size / size of the hidden layer): 3\nsize of W1: (3, 5) (NxV)\nsize of b1: (3, 1) (Nx1)\nsize of W2: (5, 3) (VxN)\nsize of b2: (5, 1) (Vx1)\n```\n:::\n:::\n\n\nBefore moving forward, we will need some functions and variables defined in previous notebooks. They can be found next. Be sure we understand everything that is going on in the next cell, if not consider doing a refresh of the first lecture notebook.\n\n::: {#dcf8e704 .cell execution_count=5}\n``` {.python .cell-code}\n# Define the tokenized version of the corpus\nwords = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n\n# Get 'word2Ind' and 'Ind2word' dictionaries for the tokenized corpus\nword2Ind, Ind2word = get_dict(words)\n\n# Define the 'get_windows' function as seen in a previous notebook\ndef get_windows(words, C):\n    i = C\n    while i < len(words) - C:\n        center_word = words[i]\n        context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n        yield context_words, center_word\n        i += 1\n\n# Define the 'word_to_one_hot_vector' function as seen in a previous notebook\ndef word_to_one_hot_vector(word, word2Ind, V):\n    one_hot_vector = np.zeros(V)\n    one_hot_vector[word2Ind[word]] = 1\n    return one_hot_vector\n\n# Define the 'context_words_to_vector' function as seen in a previous notebook\ndef context_words_to_vector(context_words, word2Ind, V):\n    context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n    context_words_vectors = np.mean(context_words_vectors, axis=0)\n    return context_words_vectors\n\n# Define the generator function 'get_training_example' as seen in a previous notebook\ndef get_training_example(words, C, word2Ind, V):\n    for context_words, center_word in get_windows(words, C):\n        yield context_words_to_vector(context_words, word2Ind, V), word_to_one_hot_vector(center_word, word2Ind, V)\n```\n:::\n\n\n### Training example\n\nRun the next cells to get the first training example, made of the vector representing the context words \"i am because i\", and the target which is the one-hot vector representing the center word \"happy\".\n\n> We don't need to worry about the Python syntax, but there are some explanations below if we want to know what's happening behind the scenes.\n\n::: {#e95fecb8 .cell execution_count=6}\n``` {.python .cell-code}\n# Save generator object in the 'training_examples' variable with the desired arguments\ntraining_examples = get_training_example(words, 2, word2Ind, V)\n```\n:::\n\n\n> `get_training_examples`, which uses the `yield` keyword, is known as a generator. When run, it builds an iterator, which is a special type of object that... we can iterate on (using a `for` loop for instance), to retrieve the successive values that the function generates.\n>\n> In this case `get_training_examples` `yield`s training examples, and iterating on `training_examples` will return the successive training examples.\n\n::: {#0cd19c58 .cell execution_count=7}\n``` {.python .cell-code}\n# Get first values from generator\nx_array, y_array = next(training_examples)\n```\n:::\n\n\n> `next` is another special keyword, which gets the next available value from an iterator. Here, you'll get the very first value, which is the first training example. If we run this cell again, you'll get the next value, and so on until the iterator runs out of values to return.\n>\n> In this notebook `next` is used because we will only be performing one iteration of training. In this week's assignment with the full training over several iterations you'll use regular `for` loops with the iterator that supplies the training examples.\n\nThe vector representing the context words, which will be fed into the neural network, is:\n\n::: {#58e4ea99 .cell execution_count=8}\n``` {.python .cell-code}\n# Print context words vector\nx_array\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\narray([0.25, 0.25, 0.  , 0.5 , 0.  ])\n```\n:::\n:::\n\n\nThe one-hot vector representing the center word to be predicted is:\n\n::: {#0c785b09 .cell execution_count=9}\n``` {.python .cell-code}\n# Print one hot vector of center word\ny_array\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\narray([0., 0., 1., 0., 0.])\n```\n:::\n:::\n\n\nNow convert these vectors into matrices (or 2D arrays) to be able to perform matrix multiplication on the right types of objects, as explained in a previous notebook.\n\n::: {#0102e2a0 .cell execution_count=10}\n``` {.python .cell-code}\n# Copy vector\nx = x_array.copy()\n\n# Reshape it\nx.shape = (V, 1)\n\n# Print it\nprint(f'x:\\n{x}\\n')\n\n# Copy vector\ny = y_array.copy()\n\n# Reshape it\ny.shape = (V, 1)\n\n# Print it\nprint(f'y:\\n{y}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nx:\n[[0.25]\n [0.25]\n [0.  ]\n [0.5 ]\n [0.  ]]\n\ny:\n[[0.]\n [0.]\n [1.]\n [0.]\n [0.]]\n```\n:::\n:::\n\n\nNow we will need the activation functions seen before. Again, if this feel unfamiliar consider checking the previous lecture notebook. \n\n::: {#7e1817cc .cell execution_count=11}\n``` {.python .cell-code}\n# Define the 'relu' function as seen in the previous lecture notebook\ndef relu(z):\n    result = z.copy()\n    result[result < 0] = 0\n    return result\n\n# Define the 'softmax' function as seen in the previous lecture notebook\ndef softmax(z):\n    e_z = np.exp(z)\n    sum_e_z = np.sum(e_z)\n    return e_z / sum_e_z\n```\n:::\n\n\n### Values of the hidden layer\n\nNow that we have initialized all the variables that we need for forward propagation, we can calculate the values of the hidden layer using the following formulas:\n\n$$\n\\begin{align}\n \\mathbf{z_1} = \\mathbf{W_1}\\mathbf{x} + \\mathbf{b_1}  \\tag{1} \\\\\n \\mathbf{h} = \\mathrm{ReLU}(\\mathbf{z_1})  \\tag{2} \\\\\n\\end{align}\n$$\n\nFirst, we can calculate the value of $\\mathbf{z_1}$.\n\n::: {#e75aae01 .cell execution_count=12}\n``` {.python .cell-code}\n# Compute z1 (values of first hidden layer before applying the ReLU function)\nz1 = np.dot(W1, x) + b1\n```\n:::\n\n\n>Â `np.dot` is numpy's function for matrix multiplication.\n\nAs expected we get an $N$ by 1 matrix, or column vector with $N$ elements, where $N$ is equal to the embedding size, which is 3 in this example.\n\n::: {#bd100f7f .cell execution_count=13}\n``` {.python .cell-code}\n# Print z1\nz1\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\narray([[ 0.36483875],\n       [ 0.63710329],\n       [-0.3236647 ]])\n```\n:::\n:::\n\n\nWe can now take the ReLU of $\\mathbf{z_1}$ to get $\\mathbf{h}$, the vector with the values of the hidden layer.\n\n::: {#bae7cd9c .cell execution_count=14}\n``` {.python .cell-code}\n# Compute h (z1 after applying ReLU function)\nh = relu(z1)\n\n# Print h\nh\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\narray([[0.36483875],\n       [0.63710329],\n       [0.        ]])\n```\n:::\n:::\n\n\nApplying ReLU means that the negative element of $\\mathbf{z_1}$ has been replaced with a zero.\n\n### Values of the output layer\n\nHere are the formulas we need to calculate the values of the output layer, represented by the vector $\\mathbf{\\hat y}$:\n\n$$\n\\begin{align}\n \\mathbf{z_2} &= \\mathbf{W_2}\\mathbf{h} + \\mathbf{b_2}   \\tag{3} \\\\\n \\mathbf{\\hat y} &= \\mathrm{softmax}(\\mathbf{z_2})   \\tag{4} \\\\\n\\end{align}\n$$\n\n**First, calculate $\\mathbf{z_2}$.**\n\n::: {#ea583524 .cell execution_count=15}\n``` {.python .cell-code}\n# Compute z2 (values of the output layer before applying the softmax function)\nz2 = np.dot(W2, h) + b2\n\n# Print z2\nz2\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\narray([[-0.31973737],\n       [-0.28125477],\n       [-0.09838369],\n       [-0.33512159],\n       [-0.19919612]])\n```\n:::\n:::\n\n\nExpected output:\n\n    array([[-0.31973737],\n           [-0.28125477],\n           [-0.09838369],\n           [-0.33512159],\n           [-0.19919612]])\n\nThis is a $V$ by 1 matrix, where $V$ is the size of the vocabulary, which is 5 in this example.\n\n**Now calculate the value of $\\mathbf{\\hat y}$.**\n\n::: {#1b7f0a45 .cell execution_count=16}\n``` {.python .cell-code}\n# Compute y_hat (z2 after applying softmax function)\ny_hat = softmax(z2)\n\n# Print y_hat\ny_hat\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\narray([[0.18519074],\n       [0.19245626],\n       [0.23107446],\n       [0.18236353],\n       [0.20891502]])\n```\n:::\n:::\n\n\nExpected output:\n\n    array([[0.18519074],\n           [0.19245626],\n           [0.23107446],\n           [0.18236353],\n           [0.20891502]])\n\nAs you've performed the calculations with random matrices and vectors (apart from the input vector), the output of the neural network is essentially random at this point. The learning process will adjust the weights and biases to match the actual targets better.\n\n**That being said, what word did the neural network predict?**\n\n<details>    \n<summary>\n    <font size=\"3\" color=\"darkgreen\"><b>Solution</b></font>\n</summary>\n<p>The neural network predicted the word \"happy\": the largest element of $\\mathbf{\\hat y}$ is the third one, and the third word of the vocabulary is \"happy\".</p>\n<p>Here's how we could implement this in Python:</p>\n<p><code>print(Ind2word[np.argmax(y_hat)])</code></p>\n</details>\n\nWell done, you've completed the forward propagation phase!\n\n## Cross-entropy loss\n\nNow that we have the network's prediction, we can calculate the cross-entropy loss to determine how accurate the prediction was compared to the actual target.\n\n> Remember that we are working on a single training example, not on a batch of examples, which is why we are using *loss* and not *cost*, which is the generalized form of loss.\n\nFirst let's recall what the prediction was.\n\n::: {#c6225e2e .cell execution_count=17}\n``` {.python .cell-code}\n# Print prediction\ny_hat\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\narray([[0.18519074],\n       [0.19245626],\n       [0.23107446],\n       [0.18236353],\n       [0.20891502]])\n```\n:::\n:::\n\n\nAnd the actual target value is:\n\n::: {#4b5a1cfd .cell execution_count=18}\n``` {.python .cell-code}\n# Print target value\ny\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\narray([[0.],\n       [0.],\n       [1.],\n       [0.],\n       [0.]])\n```\n:::\n:::\n\n\nThe formula for cross-entropy loss is:\n\n$$ J=-\\sum\\limits_{k=1}^{V}y_k\\log{\\hat{y}_k} \\tag{6}$$\n\n**Try implementing the cross-entropy loss function so we get more familiar working with numpy**\n\nHere are a some hints if you're stuck.\n\n::: {#4f5be5f5 .cell execution_count=19}\n``` {.python .cell-code}\ndef cross_entropy_loss(y_predicted, y_actual):\n    # Fill the loss variable with your code\n    loss = np.sum(-np.log(y_predicted)*y_actual)\n    return loss\n```\n:::\n\n\n<details>    \n<summary>\n    <font size=\"3\" color=\"darkgreen\"><b>Hint 1</b></font>\n</summary>\n    <p>To multiply two numpy matrices (such as <code>y</code> and <code>y_hat</code>) element-wise, we can simply use the <code>*</code> operator.</p>\n</details>\n\n\n<details>    \n<summary>\n    <font size=\"3\" color=\"darkgreen\"><b>Hint 2</b></font>\n</summary>\n<p>Once we have a vector equal to the element-wise multiplication of <code>y</code> and <code>y_hat</code>, we can use <code>np.sum</code> to calculate the sum of the elements of this vector.</p>\n</details>\n\n\n<details>    \n<summary>\n    <font size=\"3\" color=\"darkgreen\"><b>Solution</b></font>\n</summary>\n<p><code>loss = np.sum(-np.log(y_hat)*y)</code></p>\n</details>\n\n\nDon't forget to run the cell containing the `cross_entropy_loss` function once it is solved.\n\n**Now use this function to calculate the loss with the actual values of $\\mathbf{y}$ and $\\mathbf{\\hat y}$.**\n\n::: {#0c3941f8 .cell execution_count=20}\n``` {.python .cell-code}\n# Print value of cross entropy loss for prediction and target value\ncross_entropy_loss(y_hat, y)\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\nnp.float64(1.4650152923611108)\n```\n:::\n:::\n\n\nExpected output:\n\n    1.4650152923611106\n\nThis value is neither good nor bad, which is expected as the neural network hasn't learned anything yet.\n\nThe actual learning will start during the next phase: backpropagation.\n\n## Backpropagation\n\nThe formulas that we will implement for backpropagation are the following.\n\n$$\n\\begin{align}\n \\frac{\\partial J}{\\partial \\mathbf{W_1}} &= \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right )\\mathbf{x}^\\top \\tag{7}\\\\\n \\frac{\\partial J}{\\partial \\mathbf{W_2}} &= (\\mathbf{\\hat{y}} - \\mathbf{y})\\mathbf{h^\\top} \\tag{8}\\\\\n \\frac{\\partial J}{\\partial \\mathbf{b_1}} &= \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right ) \\tag{9}\\\\\n \\frac{\\partial J}{\\partial \\mathbf{b_2}} &= \\mathbf{\\hat{y}} - \\mathbf{y} \\tag{10}\n\\end{align}\n$$\n\n> Note: these formulas are slightly simplified compared to the ones in the lecture as you're working on a single training example, whereas the lecture provided the formulas for a batch of examples. In the assignment you'll be implementing the latter.\n\nLet's start with an easy one.\n\n**Calculate the partial derivative of the loss function with respect to $\\mathbf{b_2}$, and store the result in `grad_b2`.**\n\n$$\n\\frac{\\partial J}{\\partial \\mathbf{b_2}} = \\mathbf{\\hat{y}} - \\mathbf{y} \\tag{10}\n$$\n\n::: {#5840129c .cell execution_count=21}\n``` {.python .cell-code}\n# Compute vector with partial derivatives of loss function with respect to b2\ngrad_b2 = y_hat - y\n\n# Print this vector\ngrad_b2\n```\n\n::: {.cell-output .cell-output-display execution_count=21}\n```\narray([[ 0.18519074],\n       [ 0.19245626],\n       [-0.76892554],\n       [ 0.18236353],\n       [ 0.20891502]])\n```\n:::\n:::\n\n\nExpected output:\n\n    array([[ 0.18519074],\n           [ 0.19245626],\n           [-0.76892554],\n           [ 0.18236353],\n           [ 0.20891502]])\n\n**Next, calculate the partial derivative of the loss function with respect to $\\mathbf{W_2}$, and store the result in `grad_W2`.**\n\n$$\n\\frac{\\partial J}{\\partial \\mathbf{W_2}} = (\\mathbf{\\hat{y}} - \\mathbf{y})\\mathbf{h^\\top} \\tag{8}\n$$\n\n> Hint: use `.T` to get a transposed matrix, e.g. `h.T` returns $\\mathbf{h^\\top}$.\n\n::: {#9bcd9b62 .cell execution_count=22}\n``` {.python .cell-code}\n# Compute matrix with partial derivatives of loss function with respect to W2\ngrad_W2 = np.dot(y_hat - y, h.T)\n\n# Print matrix\ngrad_W2\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\narray([[ 0.06756476,  0.11798563,  0.        ],\n       [ 0.0702155 ,  0.12261452,  0.        ],\n       [-0.28053384, -0.48988499,  0.        ],\n       [ 0.06653328,  0.1161844 ,  0.        ],\n       [ 0.07622029,  0.13310045,  0.        ]])\n```\n:::\n:::\n\n\nExpected output:\n\n    array([[ 0.06756476,  0.11798563,  0.        ],\n           [ 0.0702155 ,  0.12261452,  0.        ],\n           [-0.28053384, -0.48988499,  0.        ],\n           [ 0.06653328,  0.1161844 ,  0.        ],\n           [ 0.07622029,  0.13310045,  0.        ]])\n\n**Now calculate the partial derivative with respect to $\\mathbf{b_1}$ and store the result in `grad_b1`.**\n\n$$\\frac{\\partial J}{\\partial \\mathbf{b_1}} = \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right ) \\tag{9}$$\n\n::: {#783f875d .cell execution_count=23}\n``` {.python .cell-code}\n# Compute vector with partial derivatives of loss function with respect to b1\ngrad_b1 = relu(np.dot(W2.T, y_hat - y))\n\n# Print vector\ngrad_b1\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```\narray([[0.        ],\n       [0.        ],\n       [0.17045858]])\n```\n:::\n:::\n\n\nExpected output:\n\n    array([[0.        ],\n           [0.        ],\n           [0.17045858]])\n\n**Finally, calculate the partial derivative of the loss with respect to $\\mathbf{W_1}$, and store it in `grad_W1`.**\n\n$$\n\\frac{\\partial J}{\\partial \\mathbf{W_1}} = \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right )\\mathbf{x}^\\top \\tag{7}\n$$\n\n::: {#0f27e511 .cell execution_count=24}\n``` {.python .cell-code}\n# Compute matrix with partial derivatives of loss function with respect to W1\ngrad_W1 = np.dot(relu(np.dot(W2.T, y_hat - y)), x.T)\n\n# Print matrix\ngrad_W1\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```\narray([[0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.04261464, 0.04261464, 0.        , 0.08522929, 0.        ]])\n```\n:::\n:::\n\n\nExpected output:\n\n    array([[0.        , 0.        , 0.        , 0.        , 0.        ],\n           [0.        , 0.        , 0.        , 0.        , 0.        ],\n           [0.04261464, 0.04261464, 0.        , 0.08522929, 0.        ]])\n\nBefore moving on to gradient descent, double-check that all the matrices have the expected dimensions.\n\n::: {#e4a09b1e .cell execution_count=25}\n``` {.python .cell-code}\nprint(f'V (vocabulary size): {V}')\nprint(f'N (embedding size / size of the hidden layer): {N}')\nprint(f'size of grad_W1: {grad_W1.shape} (NxV)')\nprint(f'size of grad_b1: {grad_b1.shape} (Nx1)')\nprint(f'size of grad_W2: {grad_W2.shape} (VxN)')\nprint(f'size of grad_b2: {grad_b2.shape} (Vx1)')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nV (vocabulary size): 5\nN (embedding size / size of the hidden layer): 3\nsize of grad_W1: (3, 5) (NxV)\nsize of grad_b1: (3, 1) (Nx1)\nsize of grad_W2: (5, 3) (VxN)\nsize of grad_b2: (5, 1) (Vx1)\n```\n:::\n:::\n\n\n## Gradient descent\n\nDuring the gradient descent phase, we will update the weights and biases by subtracting $\\alpha$ times the gradient from the original matrices and vectors, using the following formulas.\n\n$$\n\\begin{align}\n \\mathbf{W_1} &:= \\mathbf{W_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_1}} \\tag{11}\\\\\n \\mathbf{W_2} &:= \\mathbf{W_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_2}} \\tag{12}\\\\\n \\mathbf{b_1} &:= \\mathbf{b_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_1}} \\tag{13}\\\\\n \\mathbf{b_2} &:= \\mathbf{b_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_2}} \\tag{14}\\\\\n\\end{align}\n$$\n\nFirst, let set a value for $\\alpha$.\n\n::: {#34e825b2 .cell execution_count=26}\n``` {.python .cell-code}\n# Define alpha\nalpha = 0.03\n```\n:::\n\n\nThe updated weight matrix $\\mathbf{W_1}$ will be:\n\n::: {#1820b7ff .cell execution_count=27}\n``` {.python .cell-code}\n# Compute updated W1\nW1_new = W1 - alpha * grad_W1\n```\n:::\n\n\nLet's compare the previous and new values of $\\mathbf{W_1}$:\n\n::: {#96cf4421 .cell execution_count=28}\n``` {.python .cell-code}\nprint('old value of W1:')\nprint(W1)\nprint()\nprint('new value of W1:')\nprint(W1_new)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nold value of W1:\n[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]\n [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]\n [ 0.26637602 -0.23846886 -0.37770863 -0.11399446  0.34008124]]\n\nnew value of W1:\n[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]\n [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]\n [ 0.26509758 -0.2397473  -0.37770863 -0.11655134  0.34008124]]\n```\n:::\n:::\n\n\nThe difference is very subtle (hint: take a closer look at the last row), which is why it takes a fair amount of iterations to train the neural network until it reaches optimal weights and biases starting from random values.\n\n**Now calculate the new values of $\\mathbf{W_2}$ (to be stored in `W2_new`), $\\mathbf{b_1}$ (in `b1_new`), and $\\mathbf{b_2}$ (in `b2_new`).**\n\n\\begin{align}\n \\mathbf{W_2} &:= \\mathbf{W_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_2}} \\tag{12}\\\\\n \\mathbf{b_1} &:= \\mathbf{b_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_1}} \\tag{13}\\\\\n \\mathbf{b_2} &:= \\mathbf{b_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_2}} \\tag{14}\\\\\n\\end{align}\n\n::: {#807955fa .cell execution_count=29}\n``` {.python .cell-code}\n# Compute updated W2\nW2_new = W2 - alpha * grad_W2\n\n# Compute updated b1\nb1_new = b1 - alpha * grad_b1\n\n# Compute updated b2\nb2_new = b2 - alpha * grad_b2\n\n\nprint('W2_new')\nprint(W2_new)\nprint()\nprint('b1_new')\nprint(b1_new)\nprint()\nprint('b2_new')\nprint(b2_new)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nW2_new\n[[-0.22384758 -0.43362588  0.13310965]\n [ 0.08265956  0.0775535   0.1772054 ]\n [ 0.19557112 -0.04637608 -0.1790735 ]\n [ 0.06855622 -0.02363691  0.36107434]\n [ 0.33251813 -0.3982269  -0.43959196]]\n\nb1_new\n[[ 0.09688219]\n [ 0.29239497]\n [-0.27875802]]\n\nb2_new\n[[ 0.02964508]\n [-0.36970753]\n [-0.10468778]\n [-0.35349417]\n [-0.0764456 ]]\n```\n:::\n:::\n\n\nExpected output:\n\n    W2_new\n    [[-0.22384758 -0.43362588  0.13310965]\n     [ 0.08265956  0.0775535   0.1772054 ]\n     [ 0.19557112 -0.04637608 -0.1790735 ]\n     [ 0.06855622 -0.02363691  0.36107434]\n     [ 0.33251813 -0.3982269  -0.43959196]]\n\n    b1_new\n    [[ 0.09688219]\n     [ 0.29239497]\n     [-0.27875802]]\n\n    b2_new\n    [[ 0.02964508]\n     [-0.36970753]\n     [-0.10468778]\n     [-0.35349417]\n     [-0.0764456 ]]\n\nCongratulations, we have completed one iteration of training using one training example!\n\nYou'll need many more iterations to fully train the neural network, and we can optimize the learning process by training on batches of examples, as described in the lecture. We will get to do this during this week's assignment.\n\n### How this practice relates to and differs from the upcoming graded assignment\n\n- In the assignment, for each iteration of training we will use batches of examples instead of a single example. The formulas for forward propagation and backpropagation will be modified accordingly, and we will use cross-entropy cost instead of cross-entropy loss.\n- We will also complete several iterations of training, until we reach an acceptably low cross-entropy cost, at which point we can extract good word embeddings from the weight matrices.\n\n",
    "supporting": [
      "lab03_files"
    ],
    "filters": [],
    "includes": {}
  }
}