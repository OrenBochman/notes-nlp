{
  "hash": "480492e6e6fb05dc529438210f94900f",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ndate: 2020-11-01\ntitle: 'Word Embeddings: Intro to CBOW model, activation functions and working with Numpy'\ncategories: \n  - NLP \n  - Coursera \n  - Lab\n  - Probabilistic Models\njupyter: python3\n# execute: \n#     error: true\n---\n\n\n\n::: {.column-margin .nolightbox}\n![course banner](/images/Course-Logo-2-3.webp)\n:::\n\n\n\nIn this lecture notebook we will be given an introduction to the continuous bag-of-words model, its activation functions and some considerations when working with Numpy. \n\nLet's dive into it!\n\n::: {#2344c629 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\n```\n:::\n\n\n# The continuous bag-of-words model\n\nThe CBOW model is based on a neural network, the architecture of which looks like the figure below, as you'll recall from the lecture.\n\n<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='cbow_model_architecture.png' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:917;height:337;\" /> Figure 1 </div>\n\n## Activation functions\n\nLet's start by implementing the activation functions, ReLU and softmax.\n\n### ReLU\n\nReLU is used to calculate the values of the hidden layer, in the following formulas:\n\n\\begin{align}\n \\mathbf{z_1} &= \\mathbf{W_1}\\mathbf{x} + \\mathbf{b_1}  \\tag{1} \\\\\n \\mathbf{h} &= \\mathrm{ReLU}(\\mathbf{z_1})  \\tag{2} \\\\\n\\end{align}\n\nLet's fix a value for $\\mathbf{z_1}$ as a working example.\n\n::: {#4514e4cc .cell execution_count=2}\n``` {.python .cell-code}\n# Define a random seed so all random outcomes can be reproduced\nnp.random.seed(10)\n\n# Define a 5X1 column vector using numpy\nz_1 = 10*np.random.rand(5, 1)-5\n\n# Print the vector\nz_1\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\narray([[ 2.71320643],\n       [-4.79248051],\n       [ 1.33648235],\n       [ 2.48803883],\n       [-0.01492988]])\n```\n:::\n:::\n\n\nNotice that using numpy's `random.rand` function returns a numpy array filled with values taken from a uniform distribution over [0, 1). Numpy allows vectorization so each value is multiplied by 10 and then substracted 5.\n\nTo get the ReLU of this vector, we want all the negative values to become zeros.\n\nFirst create a copy of this vector.\n\n::: {#b078a590 .cell execution_count=3}\n``` {.python .cell-code}\n# Create copy of vector and save it in the 'h' variable\nh = z_1.copy()\n```\n:::\n\n\nNow determine which of its values are negative.\n\n::: {#f89727ae .cell execution_count=4}\n``` {.python .cell-code}\n# Determine which values met the criteria (this is possible because of vectorization)\nh < 0\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\narray([[False],\n       [ True],\n       [False],\n       [False],\n       [ True]])\n```\n:::\n:::\n\n\nWe can now simply set all of the values which are negative to 0.\n\n::: {#5720ab9b .cell execution_count=5}\n``` {.python .cell-code}\n# Slice the array or vector. This is the same as applying ReLU to it\nh[h < 0] = 0\n```\n:::\n\n\nAnd that's it: we have the ReLU of $\\mathbf{z_1}$!\n\n::: {#559a2808 .cell execution_count=6}\n``` {.python .cell-code}\n# Print the vector after ReLU\nh\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\narray([[2.71320643],\n       [0.        ],\n       [1.33648235],\n       [2.48803883],\n       [0.        ]])\n```\n:::\n:::\n\n\n**Now implement ReLU as a function.**\n\n::: {#7a3e7490 .cell execution_count=7}\n``` {.python .cell-code}\n# Define the 'relu' function that will include the steps previously seen\ndef relu(z):\n    result = z.copy()\n    result[result < 0] = 0\n    return result\n```\n:::\n\n\n**And check that it's working.**\n\n::: {#94cf09b3 .cell execution_count=8}\n``` {.python .cell-code}\n# Define a new vector and save it in the 'z' variable\nz = np.array([[-1.25459881], [ 4.50714306], [ 2.31993942], [ 0.98658484], [-3.4398136 ]])\n\n# Apply ReLU to it\nrelu(z)\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\narray([[0.        ],\n       [4.50714306],\n       [2.31993942],\n       [0.98658484],\n       [0.        ]])\n```\n:::\n:::\n\n\nExpected output:\n\n    array([[0.        ],\n           [4.50714306],\n           [2.31993942],\n           [0.98658484],\n           [0.        ]])\n\n### Softmax\n\nThe second activation function that we need is softmax. This function is used to calculate the values of the output layer of the neural network, using the following formulas:\n\n\\begin{align}\n \\mathbf{z_2} &= \\mathbf{W_2}\\mathbf{h} + \\mathbf{b_2}   \\tag{3} \\\\\n \\mathbf{\\hat y} &= \\mathrm{softmax}(\\mathbf{z_2})   \\tag{4} \\\\\n\\end{align}\n\nTo calculate softmax of a vector $\\mathbf{z}$, the $i$-th component of the resulting vector is given by:\n\n$$ \\textrm{softmax}(\\textbf{z})_i = \\frac{e^{z_i} }{\\sum\\limits_{j=1}^{V} e^{z_j} }  \\tag{5} $$\n\nLet's work through an example.\n\n::: {#fe7c344a .cell execution_count=9}\n``` {.python .cell-code}\n# Define a new vector and save it in the 'z' variable\nz = np.array([9, 8, 11, 10, 8.5])\n\n# Print the vector\nz\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\narray([ 9. ,  8. , 11. , 10. ,  8.5])\n```\n:::\n:::\n\n\nYou'll need to calculate the exponentials of each element, both for the numerator and for the denominator.\n\n::: {#4d5a5312 .cell execution_count=10}\n``` {.python .cell-code}\n# Save exponentials of the values in a new vector\ne_z = np.exp(z)\n\n# Print the vector with the exponential values\ne_z\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\narray([ 8103.08392758,  2980.95798704, 59874.1417152 , 22026.46579481,\n        4914.7688403 ])\n```\n:::\n:::\n\n\nThe denominator is equal to the sum of these exponentials.\n\n::: {#dca3d506 .cell execution_count=11}\n``` {.python .cell-code}\n# Save the sum of the exponentials\nsum_e_z = np.sum(e_z)\n\n# Print sum of exponentials\nsum_e_z\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\nnp.float64(97899.41826492078)\n```\n:::\n:::\n\n\nAnd the value of the first element of $\\textrm{softmax}(\\textbf{z})$ is given by:\n\n::: {#9b8011dc .cell execution_count=12}\n``` {.python .cell-code}\n# Print softmax value of the first element in the original vector\ne_z[0]/sum_e_z\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\nnp.float64(0.08276947985173956)\n```\n:::\n:::\n\n\nThis is for one element. We can use numpy's vectorized operations to calculate the values of all the elements of the $\\textrm{softmax}(\\textbf{z})$ vector in one go.\n\n**Implement the softmax function.**\n\n::: {#8a4f83c4 .cell execution_count=13}\n``` {.python .cell-code}\n# Define the 'softmax' function that will include the steps previously seen\ndef softmax(z):\n    e_z = np.exp(z)\n    sum_e_z = np.sum(e_z)\n    return e_z / sum_e_z\n```\n:::\n\n\n**Now check that it works.**\n\n::: {#0aa5a921 .cell execution_count=14}\n``` {.python .cell-code}\n# Print softmax values for original vector\nsoftmax([9, 8, 11, 10, 8.5])\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\narray([0.08276948, 0.03044919, 0.61158833, 0.22499077, 0.05020223])\n```\n:::\n:::\n\n\nExpected output:\n\n    array([0.08276948, 0.03044919, 0.61158833, 0.22499077, 0.05020223])\n\nNotice that the sum of all these values is equal to 1.\n\n::: {#42a4f6a2 .cell execution_count=15}\n``` {.python .cell-code}\n# Assert that the sum of the softmax values is equal to 1\nnp.sum(softmax([9, 8, 11, 10, 8.5])) == 1\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\nnp.True_\n```\n:::\n:::\n\n\n## Dimensions: 1-D arrays vs 2-D column vectors\n\nBefore moving on to implement forward propagation, backpropagation, and gradient descent in the next lecture notebook, let's have a look at the dimensions of the vectors you've been handling until now.\n\nCreate a vector of length $V$ filled with zeros.\n\n::: {#0e52e504 .cell execution_count=16}\n``` {.python .cell-code}\n# Define V. Remember this was the size of the vocabulary in the previous lecture notebook\nV = 5\n\n# Define vector of length V filled with zeros\nx_array = np.zeros(V)\n\n# Print vector\nx_array\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\narray([0., 0., 0., 0., 0.])\n```\n:::\n:::\n\n\nThis is a 1-dimensional array, as revealed by the `.shape` property of the array.\n\n::: {#82562884 .cell execution_count=17}\n``` {.python .cell-code}\n# Print vector's shape\nx_array.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\n(5,)\n```\n:::\n:::\n\n\nTo perform matrix multiplication in the next steps, we actually need your column vectors to be represented as a matrix with one column. In numpy, this matrix is represented as a 2-dimensional array.\n\nThe easiest way to convert a 1D vector to a 2D column matrix is to set its `.shape` property to the number of rows and one column, as shown in the next cell.\n\n::: {#9e4c4fdb .cell execution_count=18}\n``` {.python .cell-code}\n# Copy vector\nx_column_vector = x_array.copy()\n\n# Reshape copy of vector\nx_column_vector.shape = (V, 1)  # alternatively ... = (x_array.shape[0], 1)\n\n# Print vector\nx_column_vector\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\narray([[0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.]])\n```\n:::\n:::\n\n\nThe shape of the resulting \"vector\" is:\n\n::: {#503aa3a0 .cell execution_count=19}\n``` {.python .cell-code}\n# Print vector's shape\nx_column_vector.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\n(5, 1)\n```\n:::\n:::\n\n\nSo we now have a 5x1 matrix that we can use to perform standard matrix multiplication.\n\n**Congratulations on finishing this lecture notebook!** Hopefully we now have a better understanding of the activation functions used in the continuous bag-of-words model, as well as a clearer idea of how to leverage Numpy's power for these types of mathematical computations.\n\nIn the next lecture notebook we will get a comprehensive dive into:\n\n- Forward propagation.\n- Cross-entropy loss.\n- Backpropagation.\n- Gradient descent.\n\n**See we next time!**\n\n",
    "supporting": [
      "lab02_files"
    ],
    "filters": [],
    "includes": {}
  }
}