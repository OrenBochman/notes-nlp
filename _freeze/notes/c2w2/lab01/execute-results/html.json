{
  "hash": "f61eaa6f37398388a0627f1ede109a7c",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ndate: 2020-10-21\ntitle: 'Parts-of-Speech Tagging - First Steps: Working with text files, Creating a Vocabulary and Handling Unknown Words'\nsubtitle: \"Probabilistic Models\"\ndescription: \"Extract features from text into numerical vectors, then build a binary classifier for tweets using logistic regression!\"\ncategories: \n  - NLP \n  - Coursera \n  - lab\n  - Probabilistic Models\njupyter: python3\n---\n\n\n::: {#fig-00 .column-margin .nolightbox}\n![course banner](/images/Course-Logo-2-3.webp)\n:::\n\n\nIn this lecture notebook we will create a vocabulary from a tagged dataset and learn how to deal with words that are not present in this vocabulary when working with other text sources. \n\nAside from this we will also learn how to:\n \n- read text files\n- work with defaultdict\n- work with string data\n\n::: {#b79c4555 .cell execution_count=2}\n``` {.python .cell-code}\nimport string\nfrom collections import defaultdict\n```\n:::\n\n\n### Read Text Data\n\nA tagged dataset taken from the Wall Street Journal is provided in the file `WSJ_02-21.pos`. \n\nTo read this file we can use Python's context manager by using the `with` keyword and specifying the name of the file we wish to read. To actually save the contents of the file into memory we will need to use the `readlines()` method and store its return value in a variable. \n\nPython's context managers are great because we don't need to explicitly close the connection to the file, this is done under the hood:\n\n::: {#2033e387 .cell execution_count=3}\n``` {.python .cell-code}\n# Read lines from 'WSJ_02-21.pos' file and save them into the 'lines' variable\nwith open(\"WSJ_02-21.pos\", 'r') as f:\n    lines = f.readlines()\n```\n:::\n\n\nTo check the contents of the dataset we can print the first 5 lines:\n\n::: {#a78a23da .cell execution_count=4}\n``` {.python .cell-code}\n# Print columns for reference\nprint(\"\\t\\tWord\", \"\\tTag\\n\")\n\n# Print first five lines of the dataset\nfor i in range(5):\n    print(f'line number {i+1}: {lines[i]}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\t\tWord \tTag\n\nline number 1: In\tIN\n\nline number 2: an\tDT\n\nline number 3: Oct.\tNNP\n\nline number 4: 19\tCD\n\nline number 5: review\tNN\n\n```\n:::\n:::\n\n\nEach line within the dataset has a word followed by its corresponding tag. However since the printing was done using a formatted string it can be inferred that the **word** and the **tag** are separated by a tab (or some spaces) and there is a newline at the end of each line (notice that there is a space between each line). \n\nIf we want to understand the meaning of these tags we can take a look [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).\n\nTo better understand how the information is structured in the dataset it is recommended to print an unformatted version of it:\n\n::: {#f6990189 .cell execution_count=5}\n``` {.python .cell-code}\n# Print first line (unformatted)\nlines[0]\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n'In\\tIN\\n'\n```\n:::\n:::\n\n\nIndeed there is a tab between the word and the tag and a newline at the end of each line.\n\n### Creating a vocabulary\n\nNow that we understand how the dataset is structured, we will create a vocabulary out of it. A vocabulary is made up of every word that appeared at least 2 times in the dataset. \nFor this, follow these steps:\n- Get only the words from the dataset\n- Use a defaultdict to count the number of times each word appears\n- Filter the dict to only include words that appeared at least 2 times\n- Create a list out of the filtered dict\n- Sort the list\n\nFor step 1 we can use the fact that every word and tag are separated by a tab and that words always come first. Using list comprehension the words list can be created like this:\n\n::: {#5d166561 .cell execution_count=6}\n``` {.python .cell-code}\n# Get the words from each line in the dataset\nwords = [line.split('\\t')[0] for line in lines]\n```\n:::\n\n\nStep 2 can be done easily by leveraging `defaultdict`. In case we aren't familiar with defaultdicts they are a special kind of dictionaries that **return the \"zero\" value of a type if we try to access a key that does not exist**. Since we want the frequencies of words, we should define the defaultdict with a type of `int`. \n\nNow we don't need to worry about the case when the word is not present within the dictionary because getting the value for that key will simply return a zero. Isn't that cool?\n\n::: {#313a7718 .cell execution_count=7}\n``` {.python .cell-code}\n# Define defaultdict of type 'int'\nfreq = defaultdict(int)\n\n# Count frequency of ocurrence for each word in the dataset\nfor word in words:\n    freq[word] += 1\n```\n:::\n\n\nFiltering the `freq` dictionary can be done using list comprehensions again (aren't they handy?). We should filter out words that appeared only once and also words that are just a newline character:\n\n::: {#84396b81 .cell execution_count=8}\n``` {.python .cell-code}\n# Create the vocabulary by filtering the 'freq' dictionary\nvocab = [k for k, v in freq.items() if (v > 1 and k != '\\n')]\n```\n:::\n\n\nFinally, the `sort` method will take care of the final step. Notice that it changes the list directly so we don't need to reassign the `vocab` variable:\n\n::: {#c4c4fd6a .cell execution_count=9}\n``` {.python .cell-code}\n# Sort the vocabulary\nvocab.sort()\n\n# Print some random values of the vocabulary\nfor i in range(4000, 4005):\n    print(vocab[i])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEarly\nEarnings\nEarth\nEarthquake\nEast\n```\n:::\n:::\n\n\nNow we have successfully created a vocabulary from the dataset. **Great job!** The vocabulary is quite extensive so it is not printed out but we can still do so by creating a cell and running something like `print(vocab)`. \n\nAt this point we will usually write the vocabulary into a file for future use, but that is out of the scope of this notebook. If we are curious it is very similar to how we read the file at the beginning of this notebook.\n\n## Processing new text sources\n\n### Dealing with unknown words\n\nNow that we have a vocabulary, we will use it when processing new text sources. **A new text will have words that do not appear in the current vocabulary**. To tackle this, we can simply classify each new word as an unknown one, but we can do better by creating a function that tries to classify the type of each unknown word and assign it a corresponding `unknown token`. \n\nThis function will do the following checks and return an appropriate token:\n\n   - Check if the unknown word contains any character that is a digit \n       - return `--unk_digit--`\n   - Check if the unknown word contains any punctuation character \n       - return `--unk_punct--`\n   - Check if the unknown word contains any upper-case character \n       - return `--unk_upper--`\n   - Check if the unknown word ends with a suffix that could indicate it is a noun, verb, adjective or adverb \n        - return `--unk_noun--`, `--unk_verb--`, `--unk_adj--`, `--unk_adv--` respectively\n\nIf a word fails to fall under any condition then its token will be a plain `--unk--`. The conditions will be evaluated in the same order as listed here. So if a word contains a punctuation character but does not contain digits, it will fall under the second condition. To achieve this behaviour some if/elif statements can be used along with early returns. \n\nThis function is implemented next. Notice that the `any()` function is being heavily used. It returns `True` if at least one of the cases it evaluates is `True`.\n\n::: {#0c2c41a9 .cell execution_count=10}\n``` {.python .cell-code}\ndef assign_unk(word):\n    \"\"\"\n    Assign tokens to unknown words\n    \"\"\"\n    \n    # Punctuation characters\n    # Try printing them out in a new cell!\n    punct = set(string.punctuation)\n    \n    # Suffixes\n    noun_suffix = [\"action\", \"age\", \"ance\", \"cy\", \"dom\", \"ee\", \"ence\", \"er\", \"hood\", \"ion\", \"ism\", \"ist\", \"ity\", \"ling\", \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ship\", \"ty\"]\n    verb_suffix = [\"ate\", \"ify\", \"ise\", \"ize\"]\n    adj_suffix = [\"able\", \"ese\", \"ful\", \"i\", \"ian\", \"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ly\", \"ous\"]\n    adv_suffix = [\"ward\", \"wards\", \"wise\"]\n\n    # Loop the characters in the word, check if any is a digit\n    if any(char.isdigit() for char in word):\n        return \"--unk_digit--\"\n\n    # Loop the characters in the word, check if any is a punctuation character\n    elif any(char in punct for char in word):\n        return \"--unk_punct--\"\n\n    # Loop the characters in the word, check if any is an upper case character\n    elif any(char.isupper() for char in word):\n        return \"--unk_upper--\"\n\n    # Check if word ends with any noun suffix\n    elif any(word.endswith(suffix) for suffix in noun_suffix):\n        return \"--unk_noun--\"\n\n    # Check if word ends with any verb suffix\n    elif any(word.endswith(suffix) for suffix in verb_suffix):\n        return \"--unk_verb--\"\n\n    # Check if word ends with any adjective suffix\n    elif any(word.endswith(suffix) for suffix in adj_suffix):\n        return \"--unk_adj--\"\n\n    # Check if word ends with any adverb suffix\n    elif any(word.endswith(suffix) for suffix in adv_suffix):\n        return \"--unk_adv--\"\n    \n    # If none of the previous criteria is met, return plain unknown\n    return \"--unk--\"\n```\n:::\n\n\nA POS tagger will always encounter words that are not within the vocabulary that is being used. By augmenting the dataset to include these `unknown word tokens` we are helping the tagger to have a better idea of the appropriate tag for these words. \n\n### Getting the correct tag for a word\n\nAll that is left is to implement a function that will get the correct tag for a particular word taking special considerations for unknown words. Since the dataset provides each word and tag within the same line and a word being known depends on the vocabulary used, these two elements should be arguments to this function.\n\nThis function should check if a line is empty and if so, it should return a placeholder word and tag, `--n--` and `--s--` respectively. \n\nIf not, it should process the line to return the correct word and tag pair, considering if a word is unknown in which scenario the function `assign_unk()` should be used.\n\nThe function is implemented next. Notice That the `split()` method can be used without specifying the delimiter, in which case it will default to any whitespace.\n\n::: {#dcdf08ab .cell execution_count=11}\n``` {.python .cell-code}\ndef get_word_tag(line, vocab):\n    # If line is empty return placeholders for word and tag\n    if not line.split():\n        word = \"--n--\"\n        tag = \"--s--\"\n    else:\n        # Split line to separate word and tag\n        word, tag = line.split()\n        # Check if word is not in vocabulary\n        if word not in vocab: \n            # Handle unknown word\n            word = assign_unk(word)\n    return word, tag\n```\n:::\n\n\nNow we can try this function with some examples to test that it is working as intended:\n\n::: {#f79d1ce3 .cell execution_count=12}\n``` {.python .cell-code}\nget_word_tag('\\n', vocab)\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\n('--n--', '--s--')\n```\n:::\n:::\n\n\nSince this line only includes a newline character it returns a placeholder word and tag.\n\n::: {#70d4751d .cell execution_count=13}\n``` {.python .cell-code}\nget_word_tag('In\\tIN\\n', vocab)\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\n('In', 'IN')\n```\n:::\n:::\n\n\nThis one is a valid line and the function does a fair job at returning the correct (word, tag) pair.\n\n::: {#b4c82781 .cell execution_count=14}\n``` {.python .cell-code}\nget_word_tag('tardigrade\\tNN\\n', vocab)\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\n('--unk--', 'NN')\n```\n:::\n:::\n\n\nThis line includes a noun that is not present in the vocabulary. \n\nThe `assign_unk` function fails to detect that it is a noun so it returns an `unknown token`.\n\n::: {#afb893bb .cell execution_count=15}\n``` {.python .cell-code}\nget_word_tag('scrutinize\\tVB\\n', vocab)\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\n('--unk_verb--', 'VB')\n```\n:::\n:::\n\n\nThis line includes a verb that is not present in the vocabulary. \n\nIn this case the `assign_unk` is able to detect that it is a verb so it returns an `unknown verb token`.\n\n**Congratulations on finishing this lecture notebook!** Now we should be more familiar with working with text data and have a better understanding of how a basic POS tagger works.\n\n**Keep it up!**\n\n",
    "supporting": [
      "lab01_files"
    ],
    "filters": [],
    "includes": {}
  }
}