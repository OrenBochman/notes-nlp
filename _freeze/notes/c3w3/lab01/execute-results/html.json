{
  "hash": "4ffc3e76f969fdbc2700d1eb7e5dfbb7",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ndate: 2020-11-17\ntitle: 'Vanishing Gradients'\nsubtitle: \"Sequence Models\"\ncategories: \n  - NLP \n  - Coursera \n  - Lab\n  - Sequence Models\njupyter: python3\nexecute: \n    error: true\n---\n\n\n![course banner](/images/Course-Logo-3-3.webp){.column-margin .nolightbox} \n\n\nIn this notebook you'll take another look at vanishing gradients, from an intuitive standpoint.\n\n## Background\n\nAdding layers to a neural network introduces multiplicative effects in both forward and backward propagation. The back prop in particular presents a problem as the gradient of activation functions can be very small. Multiplied together across many layers, their product can be vanishingly small! This results in weights not being updated in the front layers and training not progressing.\n\n<br/><br/>\nGradients of the sigmoid function, for example, are in the range 0 to 0.25. To calculate gradients for the front layers of a neural network the chain rule is used. This means that these tiny values are multiplied starting at the last layer, working backwards to the first layer, with the gradients shrinking exponentially at each step.\n## Imports\n\n::: {#9ab47392 .cell tags='[]' execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n```\n:::\n\n\n## Data, Activation & Gradient\n\n### Data\n\nI'll start be creating some data, nothing special going on here. Just some values spread across the interval -5 to 5.\n\n- Try changing the range of values in the data to see how it impacts the plots that follow.\n\n### Activation\n\nThe example here is sigmoid() to squish the data x into the interval 0 to 1.\n\n### Gradient\n\nThis is the derivative of the sigmoid() activation function. It has a maximum of 0.25 at x = 0, the steepest point on the sigmoid plot.\n\n- Try changing the x value for finding the tangent line in the plot.\n\n<img src = 'img/sigmoid_tangent.png' width=\"width\" height=\"height\" style=\"height:250px;\"/>\n\n::: {#988fb4e5 .cell execution_count=3}\n``` {.python .cell-code}\n# Data\n# Interval [-5, 5]\n### START CODE HERE ###\nx = np.linspace(-5, 5, 100)  # try changing the range of values in the data. eg: (-100,100,1000)\n### END CODE HERE ###\n# Activation\n# Interval [0, 1]\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nactivations = sigmoid(x)\n\n# Gradient\n# Interval [0, 0.25]\ndef sigmoid_gradient(x):\n    return (x) * (1 - x)\n\ngradients = sigmoid_gradient(activations)\n\n# Plot sigmoid with tangent line\nplt.plot(x, activations)\nplt.title(\"Sigmoid Steepest Point\")\nplt.xlabel(\"x input data\")\nplt.ylabel(\"sigmoid(x)\")\n\n# Add the tangent line\n### START CODE HERE ###\nx_tan = 0   # x value to find the tangent. try different values within x declared above. eg: 2  \n### END CODE HERE ###\ny_tan = sigmoid(x_tan)  # y value\nspan = 1.7              # line span along x axis\ndata_tan = np.linspace(x_tan - span, x_tan + span)  # x values to plot\ngradient_tan = sigmoid_gradient(sigmoid(x_tan))     # gradient of the tangent\ntan = y_tan + gradient_tan * (data_tan - x_tan)     # y values to plot\nplt.plot(x_tan, y_tan, marker=\"o\", color=\"orange\", label=True)  # marker\nplt.plot(data_tan, tan, linestyle=\"--\", color=\"orange\")         # line\nplt.show()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nText(0.5, 1.0, 'Sigmoid Steepest Point')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nText(0.5, 0, 'x input data')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nText(0, 0.5, 'sigmoid(x)')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](lab01_files/figure-html/cell-3-output-4.png){width=589 height=449}\n:::\n:::\n\n\n## Plots\n\n### Sub Plots\n\nData values along the x-axis of the plots on the interval chosen for x, -5 to 5. Subplots:\n- x vs x\n- sigmoid of x\n- gradient of sigmoid\n\nNotice how the y axis keeps compressing from the left plot to the right plot. The interval range has shrunk from 10 to 1 to 0.25. How did this happen? As |x| gets larger the sigmoid approaches asymptotes at 0 and 1, and the sigmoid gradient shrinks towards 0.\n* Try changing the range of values in the code block above to see how it impacts the plots.\n\n::: {#45a9e49e .cell execution_count=4}\n``` {.python .cell-code}\n# Sub plots\nfig, axs = plt.subplots(1, 3, figsize=(15, 4), sharex=True)\n\n# X values\naxs[0].plot(x, x)\naxs[0].set_title(\"x values\")\naxs[0].set_ylabel(\"y=x\")\naxs[0].set_xlabel(\"x input data\")\n\n# Sigmoid\naxs[1].plot(x, activations)\naxs[1].set_title(\"sigmoid\")\naxs[1].set_ylabel(\"sigmoid\")\naxs[1].set_xlabel(\"x input data\")\n\n# Sigmoid gradient\naxs[2].plot(x, gradients)\naxs[2].set_title(\"sigmoid gradient\")\naxs[2].set_ylabel(\"gradient\")\naxs[2].set_xlabel(\"x input data\")\n\nfig.show()\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nText(0.5, 1.0, 'x values')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nText(0, 0.5, 'y=x')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nText(0.5, 0, 'x input data')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nText(0.5, 1.0, 'sigmoid')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nText(0, 0.5, 'sigmoid')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nText(0.5, 0, 'x input data')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nText(0.5, 1.0, 'sigmoid gradient')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nText(0, 0.5, 'gradient')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nText(0.5, 0, 'x input data')\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/tmp/ipykernel_120031/2272590038.py:22: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  fig.show()\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](lab01_files/figure-html/cell-4-output-11.png){width=1182 height=376}\n:::\n:::\n\n\n### Single Plo\nt\nPutting all 3 series on a single plot can help visualize the compression. Notice how hard it is to interpret because sigmoid and sigmoid gradient are so small compared to the scale of the input data x.\n\n* Trying changing the plot ylim to zoom in.\n\n::: {#5d09ad20 .cell tags='[]' execution_count=5}\n``` {.python .cell-code}\n# Single plot\nplt.plot(x, x, label=\"data\")\nplt.plot(x, activations, label=\"sigmoid\")\nplt.plot(x, gradients, label=\"sigmoid gradient\")\nplt.legend(loc=\"upper left\")\nplt.title(\"Visualizing Compression\")\nplt.xlabel(\"x input data\")\nplt.ylabel(\"range\")\n### START CODE HERE ###\n# plt.ylim(-.5, 1.5)    # try shrinking the y axis limit for better visualization. eg: uncomment this line\n### END CODE HERE ###\nplt.show()\n\n# Max, Min of each array\nprint(\"\")\nprint(\"Max of x data :\", np.max(x))\nprint(\"Min of x data :\", np.min(x), \"\\n\")\nprint(\"Max of sigmoid :\", \"{:.3f}\".format(np.max(activations)))\nprint(\"Min of sigmoid :\", \"{:.3f}\".format(np.min(activations)), \"\\n\")\nprint(\"Max of gradients :\", \"{:.3f}\".format(np.max(gradients)))\nprint(\"Min of gradients :\", \"{:.3f}\".format(np.min(gradients)))\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\nText(0.5, 1.0, 'Visualizing Compression')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\nText(0.5, 0, 'x input data')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\nText(0, 0.5, 'range')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](lab01_files/figure-html/cell-5-output-4.png){width=587 height=449}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nMax of x data : 5.0\nMin of x data : -5.0 \n\nMax of sigmoid : 0.993\nMin of sigmoid : 0.007 \n\nMax of gradients : 0.250\nMin of gradients : 0.007\n```\n:::\n:::\n\n\n## Numerical Impact\n\n###  Multiplication & Decay\n\nMultiplying numbers smaller than 1 results in smaller and smaller numbers. Below is an example that finds the gradient for an input x = 0 and multiplies it over n steps. Look how quickly it 'Vanishes' to almost zero. Yet sigmoid(x=0)=0.5 which has a sigmoid gradient of 0.25 and that happens to be the largest sigmoid gradient possible!\n<br/><br/>\n(Note: This is NOT an implementation of back propagation.)\n\n- Try changing the number of steps n.\n- Try changing the input value x. Consider the impact on sigmoid and sigmoid gradient.\n\n::: {#09e360ee .cell tags='[]' execution_count=6}\n``` {.python .cell-code}\n# Simulate decay\n# Inputs\n### START CODE HERE ###\nn = 6  # number of steps : try changing this\nx = 0  # value for input x : try changing this\n### END CODE HERE ###\ngrad = sigmoid_gradient(sigmoid(x))\nsteps = np.arange(1, n + 1)\nprint(\"-- Inputs --\")\nprint(\"steps :\", n)\nprint(\"x value :\", x)\nprint(\"sigmoid :\", \"{:.5f}\".format(sigmoid(x)))\nprint(\"gradient :\", \"{:.5f}\".format(grad), \"\\n\")\n\n# Loop to calculate cumulative total\nprint(\"-- Loop --\")\nvals = []\ntotal_grad = 1  # initialize to 1 to satisfy first loop below\nfor s in steps:\n    total_grad = total_grad * grad\n    vals.append(total_grad)\n    print(\"step\", s, \":\", total_grad)\n\nprint(\"\")\n\n# Plot\nplt.plot(steps, vals)\nplt.xticks(steps)\nplt.title(\"Multiplying Small Numbers\")\nplt.xlabel(\"Steps\")\nplt.ylabel(\"Cumulative Gradient\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n-- Inputs --\nsteps : 6\nx value : 0\nsigmoid : 0.50000\ngradient : 0.25000 \n\n-- Loop --\nstep 1 : 0.25\nstep 2 : 0.0625\nstep 3 : 0.015625\nstep 4 : 0.00390625\nstep 5 : 0.0009765625\nstep 6 : 0.000244140625\n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n([<matplotlib.axis.XTick at 0x7edb9179aa10>,\n  <matplotlib.axis.XTick at 0x7edb917989d0>,\n  <matplotlib.axis.XTick at 0x7edb917ba920>,\n  <matplotlib.axis.XTick at 0x7edb917bb610>,\n  <matplotlib.axis.XTick at 0x7edb917e0340>,\n  <matplotlib.axis.XTick at 0x7edb917ba170>],\n [Text(1, 0, '1'),\n  Text(2, 0, '2'),\n  Text(3, 0, '3'),\n  Text(4, 0, '4'),\n  Text(5, 0, '5'),\n  Text(6, 0, '6')])\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\nText(0.5, 1.0, 'Multiplying Small Numbers')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\nText(0.5, 0, 'Steps')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\nText(0, 0.5, 'Cumulative Gradient')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](lab01_files/figure-html/cell-6-output-6.png){width=597 height=449}\n:::\n:::\n\n\n## Solution\n\nOne solution is to use activation functions that don't have tiny gradients. Other solutions involve more sophisticated model design. But they're both discussions for another time.\n\n",
    "supporting": [
      "lab01_files"
    ],
    "filters": [],
    "includes": {}
  }
}