{
  "hash": "01a412c93daa299fb281d5ab6ab29322",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ndate: 2020-10-07\ntitle: \"Vector Space Models\"\nsubtitle: \"Classification & Vector Spaces\"\ndescription: \"Vector space models capture semantic meaning and relationships between words. You'll learn how to create word vectors that capture dependencies between words, then visualize their relationships in two dimensions using PCA.\"\ncategories: \n  - NLP \n  - Coursera \n  - Notes\n  - Classification & Vector Spaces\n---\n\n\n\n![course banner](/images/Course-Logo-1-3.webp){#fig-00 .column-margin .nolightbox}\n\n::: {#fig-slide-deck .column-margin}\n![This week's slides](slides.pdf){width=\"420px\" height=\"340px\" style=\"@page {size: 16in 9in;  margin: 0;  group=\"slides\"}\"}\n:::\n\n### Vector Space Models\n\n::: {#fig-slide-01 .column-margin}\n![linear representation](img/slide01.png)\n\nUnderstanding Vector Space\n:::\n\nVector spaces are fundamental in many applications in NLP.\nIf we were to represent a word, document, tweet, or any form of text, we will probably be encoding it as a vector.\nThese vectors are important in tasks like information extraction, machine translation, and chatbots.\nVector spaces could also be used to help we identify relationships between words as follows:\n\n-   We eat **cereal** from a **bowl**\n-   We **buy** something and someone else sells **it**\n\n\n{{< pagebreak >}}\n\n\n::: {#fig-slide-firth .column-margin}\n![Distributed Representation](img/slide00.png)\n\nDistributed Representation is A fundamental concept in Linguistics\n:::\n\n> \"We shall know a word by the company it keeps\".\n> -- J.R.\n> Firth\n\nWhen we are learning these vectors, we usually make use of the neighboring words to extract meaning and information about the center word.\nIf we were to cluster these vectors together, we will see that adjectives, nouns, verbs, etc. tend to be near one another.\nAnother aspect of the vector space representation, is that synonyms and antonyms are also very close to one another.\nThis is because we can easily interchange them in a sentence and they tend to have similar neighboring words!\n\n::: {#fig-slide-02 .column-margin}\n![linear representation](img/slide02.png)\n\nSome applications of the vector space model in NLP\n:::\n\n\n{{< pagebreak >}}\n\n\n### Word-by-Word and Word By Document\n\nWe now delve into constructing vector word representations, based off a co-occurrence matrix.\nSpecifically, depending on the task we are trying to solve, we can choose from several possible designs.\n\n#### Word by word Design\n\nWe will start by exploring the **word by word** design.\nAssume that we are trying to come up with a vector that will represent a certain word.\nOne possible design would be to [create a matrix where each row and column corresponds to a word in your vocabulary. Then we can iterate over a document and see the number of times each word shows up next each other word]{.mark}.\nWe can keep track of the number in the matrix.\nIn the video I spoke about a parameter $K$.\n\nWe can think of $K$ as the window size for the context which determines if two words are considered adjacent according to Firth's law.\n\n::: {#fig-slide-03 .column-margin}\n![Word by word design](img/slide03.png)\n\nWord Level aggregation: Creating a word by word vector space using a word by word design\n:::\n\nIn the example above, we can see how we are keeping track of the number of times words occur together within a certain distance k.\nAt the end, we can represent the word data, as a vector $v=[2,1,1,0].$\n\n\n{{< pagebreak >}}\n\n\n#### Word by Document Design\n\nWe can now apply the same concept and map words to documents.\n[The rows could correspond to words and the columns to documents. The numbers in the matrix correspond to the number of times each word showed up in the document]{.mark}.\n\n::: {#fig-slide-04 .column-margin}\n![Word by document](img/slide04.png){.column-margin group=\"slides\"}\n\nDocument Level aggregation: Creating a document by word vector space using a word by word design\n:::\n\nIn the example above, we can see how we are keeping track of the number of times words occur together within a certain distance k.\n\nAt the end, we can represent the word data, as a vector $v = [2, 1, 1, 0]$.\n\n::: {#fig-slide-05 .column-margin}\n![linear representation](img/slide05.png)\n\nIn this slide we see the vector space matrix at the top left, and on the right a plot of the it geometry of two words in three document categories.\nWe can use the angle and the distance between these vectors to understand the similarity between the vectors.\n:::\n\n### Euclidean Distance\n\nLet us assume that we want to compute the distance between two points: A,B.\nTo do so, we can use the euclidean distance defined as\n\n$$\nd(B,A) = \\sqrt{\\sum_{i=1}^{n} (B_i - A_i)^2}\n$$ {#eq-euclidean-distance}\n\n::: {#fig-slide-06 .column-margin}\n![Euclidean distance](img/slide06.png)\n\nIn this slide we see the vector space matrix at the top left, and on the right a plot of the it geometry of two words in three document categories.\nWe can use the angle and the distance between these vectors to understand the similarity between the vectors.\n:::\n\n::: {#fig-slide-07 .column-margin}\n![Distance calculation](img/slide07.png)\n\nCalculate the distance between two vectors of dimensional size three.\n:::\n\n## LAB: Linear algebra in Python with `numpy`\n\n[The Numpy lab](lab01.qmd)\n\n### Cosine Similarity Intuition\n\nOne of the issues with euclidean distance is that it is not always accurate and sometimes we are not looking for that type of similarity metric.\nFor example, when comparing large documents to smaller ones with euclidean distance one could get an inaccurate result.\n\nLook at the [diagram](@fig-slide-08):\n\n::: {#fig-slide-08 .column-margin}\n![Cosine Similarity: Intuition](img/slide08.png)\n\nThe cosine similarity is the cosine of the angle between two vectors.\n:::\n\nNormally the **food** corpus and the **agriculture** corpus are more similar because they have the same proportion of words.\nHowever the food corpus is much smaller than the agriculture corpus.\nTo further clarify, although the history corpus and the agriculture corpus are different, they have a smaller euclidean distance.\nHence $d_2<d_1$.\n\nTo solve this problem, we look at the cosine between the vectors.\nThis allows us to compare $B$ and $Î±$.\n\n### Background\n\nBefore getting into the cosine similarity function remember that the norm of a vector is defined as:\n\n### Norm of a Vector\n\n$$\n||\\vec{A}|| = \\sqrt{\\sum_{i=1}^{n} a_i^2}\n$$ {#eq-norm-vector}\n\n### Dot-product of Two Vectors\n\nThe dot product is then defined as:\n\n$$\n\\vec{A} \\cdot \\vec{B} = \\sum_{i=1}^{n} a_i \\cdot b_i\n$$ {#eq-dot-product}\n\n::: {#fig-slide-09 .column-margin}\n![Cosine Similarity](img/slide09.png)\n\nThe cosine similarity is the cosine of the angle between two vectors.\n:::\n\nThe following cosine similarity equation makes sense:\n\n$$\n\\cos(\\theta) = \\frac{\\vec{v} \\cdot \\vec{w}}{||\\vec{v}|| \\cdot ||\\vec{w}||}\n$$ {#eq-cosine-similarity}\n\n## Implementation\n\nWhen $\\vec{v}$ and $\\vec{u}$ are parallel the numerator is equal to the denominator so $cos(\\beta)=1$ thus $\\angle \\beta=0$.\n\nOn the other hand, the dot product of two orthogonal (perpendicular) vectors is $0$.\nThat takes place when $\\angle \\beta=90$.\n\n::: {#fig-slide-10 .column-margin}\n![Cosine Similarity Examples](img/slide10.png)\n\nExamples of cosine similarity between similar and dissimilar vectors.\n:::\n\n# Word Manipulation in Vector Spaces\n\nWe can use word vectors to actually extract patterns and identify certain structures in your text.\nFor example:\n\n::: {#fig-slide-11 .column-margin}\n![Word analogy conceptual](img/slide11.png)\n\nWord analogy have a geometric interpretation in vector spaces.\n:::\n\nWe can use the word vector for Russia, USA, and DC to actually compute a vector that would be very similar to that of Moscow.\nWe can then use cosine similarity of the vector with all the other word vectors we have and we can see that the vector of Moscow is the closest.\n\nIsn't that goofy?\n\n::: {#fig-slide-12 .column-margin}\n![Word analogy](img/slide12.png)\n\nSome word analogies\n:::\n\nNote that the distance (and direction) between a country and its capital is relatively the same.\nHence manipulating word vectors allows we identify patterns in the text.\n\n## Lab on Manipulating Word Vectors\n\n[The lab](lab02.qmd)\n\n# PCA\n\n## Visualization of Word Vectors\n\nPrincipal component analysis is an unsupervised learning algorithm which can be used to reduce the dimension of your data.\nAs a result, it allows we to visualize your data.\nIt tries to combine variances across features.\nHere is a concrete example of PCA:\n\n::: {#fig-slide-13 .column-margin}\n![Word analogy](img/slide13.png)\n\nSome word analogies\n:::\n\n::: {#fig-slide-14 .column-margin}\n![Word analogy](img/slide14.png)\n\nSome word analogies\n:::\n\nThose are the results of plotting a couple of vectors in two dimensions.\nNote that words with similar part of speech (POS) tags are next to one another.\nThis is because many of the training algorithms learn words by identifying the neighboring words.\nThus, words with similar POS tags tend to be found in similar locations.\nAn interesting insight is that synonyms and antonyms tend to be found next to each other in the plot.\nWhy is that the case?\n\n## Implementation\n\n## PCA algorithm\n\nPCA is commonly used to reduce the dimension of your data.\nIntuitively the model collapses the data across principal components.\nWe can think of the first principal component (in a 2D dataset) as the line where there is the most amount of variance.\nWe can then collapse the data points on that line.\nHence we went from 2D to 1D.\nWe can generalize this intuition to several dimensions.\n\n::: {#fig-slide-15 .column-margin}\n![PCA](img/slide15.png)\n\nSome word analogies\n:::\n\nEigenvector\n\n:   the resulting vectors, also known as the uncorrelated features of your data\n\nEigenvalue\n\n:   the amount of information retained by each new feature.\n    We can think of it as the variance in the eigenvector.\n\nAlso each **eigenvalue** has a corresponding eigenvector.\nThe eigenvalue tells we how much variance there is in the eigenvector.\nHere are the steps required to compute PCA:\n\n::: {#fig-slide-16 .column-margin}\n![PCA](img/slide16.png)\n\nSome word analogies\n:::\n\n### Steps to Compute PCA:\n\n-   Mean normalize your data\n-   Compute the covariance matrix\n-   Compute SVD on your covariance matrix. This returns $[USV]=svd(Î£)$ . The three matrices $U$, $S$, $V$ are drawn above. $U$ is labelled with eigenvectors, and $S$ is labelled with eigenvalues.\n-   We can then use the first n columns of vector $U$, to get your new data by multiplying $XU[:,0:n]$.\n\n## Putting It Together with Code\n\n::: {#d098a7e0 .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np \n\ndef PCA(X , num_components):\n  # center data around the mean\n  X_meaned = X - np.mean(X , axis = 0) \n  # calculate the covariance matrix   \n  cov_mat = np.cov(X_meaned , rowvar = False) \n  # compute an uncorrelated feature basis (eigen vectors) \n  eigen_values , eigen_vectors = np.linalg.eigh(cov_mat)\n  # sort the new basis by decreasing eigen values (variance) \n  sorted_index = np.argsort(eigen_values)[::-1] \n  sorted_eigenvalue = eigen_values[sorted_index] \n  sorted_eigenvectors = eigen_vectors[:,sorted_index] \n  # by subseting the most leading features  \n  eigenvector_subset = sorted_eigenvectors[:,0:num_components] \n  #Step-6 \n  X_reduced = np.dot(eigenvector_subset.transpose() ,     \n                   X_meaned.transpose() ).transpose() \n  return X_reduced \n```\n:::\n\n\n## Lab on PCA\n\n[PCA lab](assignment.qmd)\n\n# Document As a Vector\n\n## Resources\n\n-   Alex Williams - [Everything we did and didn't know about PCA](https://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/)\n-   Udell et al. (2015).â¯[Generalized Low-Rank Models](http://arxiv.org/abs/1410.0342) â¯arxiv preprint\n-   Tipping & Bishop (1999).â¯[Probabilistic principal component analysis](http://dx.doi.org/10.1111/1467-9868.00196) Journal of the Royal Statistical Society: Series B\n-   Ilin & Raiko (2010) [Practical Approaches to Principal Component Analysis in the Presence of Missing Values](http://www.jmlr.org/papers/volume11/ilin10a/ilin10a.pdf) Journal of Machine Learning Research\n-   Gordon (2002).â¯[Generalized^2^ Linear^2^â¯Models](http://www.cs.cmu.edu/~ggordon/ggllm.pdf)â¯NIPS\n-   Cunningham & Ghahramani (2015)â¯ [Linear dimensionality reduction: survey, insights, and generalizations](http://jmlr.org/papers/volume16/cunningham15a/cunningham15a.pdf)â¯Journal of Machine Learning Research\n-   Burges (2009).â¯[Dimension Reduction: A Guided Tour](http://dx.doi.org/10.1561/2200000002)â¯Foundations varia Trends in Machine Learning\n-   M. Gavish and D. L. Donoho, [The Optimal Hard Threshold for Singular Values is $\\frac{4}{\\sqrt{3}}$](https://ieeexplore.ieee.org/document/6846297) in IEEE Transactions on Information Theory, vol.\n    60, no. 8, pp. 5040-5053, Aug. 2014, doi: 10.1109/TIT.2014.2323359.\n-   Thomas P. Minka [Automatic choice of dimensionality for PCA](http://hd.media.mit.edu/tech-reports/TR-514.pdf) Dec. 2000\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}