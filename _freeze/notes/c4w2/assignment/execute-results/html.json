{
  "hash": "cab42fd4014e661fae21a8bc47f40629",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ndate: 2021-04-04\ntitle: 'Transformer Summarizer'\nsubtitle: \"Sequence Models\"\ncategories: \n  - NLP \n  - Coursera \n  - Lab\n  - Sequence Models\njupyter: python3\nexecute: \n    error: true\ndraft: true\n---\n\n\n![course banner](/images/Course-Logo-3-3.webp){.column-margin .nolightbox} \n\n::: {.callout}\n## Honor code alert\n\nDue to the Coursera Honor Code, I cannot provide the solutions to the assignments. \n\n- This notebook is the original notebook provided by the course\n- It is setup to run without stopping for errors. \n- It is also likely to be out of date as the course has had some updates since I took it.\n- Although I aced the course this assignment was the most time consuming.\n- Good luck with the assignment it should make we a better programmer.\n- It is also a good idea to go over it a few times until we can do it easily.\n:::\n\n\nWelcome to the second assignment of course 4. In this assignment you will explore summarization using the transformer model. Yes, you will implement the transformer decoder from scratch, but we will slowly walk you through it. There are many hints in this notebook so feel free to use them as needed. \n\n<img src = \"transformerNews.png\">\n\n\n## Outline\n\n- [Introduction](#0)\n- [Part 1: Importing the dataset](#1)\n    - [1.1 Encode & Decode helper functions](#1.1)\n    - [1.2 Defining parameters](#1.2)\n    - [1.3 Exploring the data](#1.3)\n- [Part 2: Summarization with transformer](#2)\n    - [2.1 Dot product attention](#2.1)\n        - [Exercise 01](#ex01)\n    - [2.2 Causal Attention](#2.2)\n        - [Exercise 02](#ex02)\n    - [2.3 Transformer decoder block](#2.3)\n        - [Exercise 03](#ex03)\n    - [2.4 Transformer Language model](#2.4)\n        - [Exercise 04](#ex04)\n- [Part 3: Training](#3)\n    - [3.1 Training the model](#3.1)\n        - [Exercise 05](#ex05)\n- [Part 4: Evaluation](#4)\n    - [4.1 Loading in a trained model](#4.1)\n- [Part 5: Testing with your own input](#5) \n    - [Exercise 6](#ex06)\n    - [5.1 Greedy decoding](#5.1)\n        - [Exercise 07](#ex07)\n\n<a name='0'></a>\n\n### Introduction\n\nSummarization is an important task in natural language processing and could be useful for a consumer enterprise. For example, bots can be used to scrape articles, summarize them, and then you can use sentiment analysis to identify the sentiment about certain stocks. Anyways who wants to read an article or a long email today, when you can build a transformer to summarize text for you. Let's get started, by completing this assignment you will learn to:  \n\n- Use built-in functions to preprocess your data\n- Implement DotProductAttention\n- Implement Causal Attention\n- Understand how attention works\n- Build the transformer model\n- Evaluate your model\n- Summarize an article\n\nAs you can tell, this model is slightly different than the ones you have already implemented. This is heavily based on attention and does not rely on sequences, which allows for parallel computing. \n\n::: {#e43653e6 .cell execution_count=1}\n``` {.python .cell-code}\nimport sys\nimport os\n\nimport numpy as np\n\nimport textwrap\nwrapper = textwrap.TextWrapper(width=70)\n\nimport trax\nfrom trax import layers as tl\nfrom trax.fastmath import numpy as jnp\n\n# to print the entire np array\nnp.set_printoptions(threshold=sys.maxsize)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n2025-02-10 16:54:41.681393: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1739199281.693983  122882 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1739199281.697996  122882 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n```\n:::\n:::\n\n\n<a name='1'></a>\n## Part 1: Importing the dataset\n\nTrax makes it easy to work with Tensorflow's datasets:\n\n::: {#c936b196 .cell execution_count=2}\n``` {.python .cell-code}\n# This will download the dataset if no data_dir is specified.\n# Downloading and processing can take bit of time,\n# so we have the data already in 'data/' for you\n\n# Importing CNN/DailyMail articles dataset\ntrain_stream_fn = trax.data.TFDS('cnn_dailymail',\n                                 data_dir='data/',\n                                 keys=('article', 'highlights'),\n                                 train=True)\n\n# This should be much faster as the data is downloaded already.\neval_stream_fn = trax.data.TFDS('cnn_dailymail',\n                                data_dir='data/',\n                                keys=('article', 'highlights'),\n                                train=False)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/jax/_src/xla_bridge.py:1234: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n  warnings.warn(\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nDownloading and preparing dataset 558.32 MiB (download: 558.32 MiB, generated: 1.29 GiB, total: 1.84 GiB) to data/cnn_dailymail/3.4.0...\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<script type=\"application/vnd.jupyter.widget-view+json\">\n{\"model_id\":\"3e0ac4c092ac40ec8f65796a33c86800\",\"version_major\":2,\"version_minor\":0,\"quarto_mimetype\":\"application/vnd.jupyter.widget-view+json\"}\n</script>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<script type=\"application/vnd.jupyter.widget-view+json\">\n{\"model_id\":\"0744284963d74d7ba3cd511880beefae\",\"version_major\":2,\"version_minor\":0,\"quarto_mimetype\":\"application/vnd.jupyter.widget-view+json\"}\n</script>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<script type=\"application/vnd.jupyter.widget-view+json\">\n{\"model_id\":\"6fc9a39e1c4d44d89338565a46b4d8f8\",\"version_major\":2,\"version_minor\":0,\"quarto_mimetype\":\"application/vnd.jupyter.widget-view+json\"}\n</script>\n```\n:::\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NonMatchingChecksumError</span>                  Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[2], line 6</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># This will download the dataset if no data_dir is specified.</span>\n<span class=\"ansi-green-fg ansi-bold\">      2</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Downloading and processing can take bit of time,</span>\n<span class=\"ansi-green-fg ansi-bold\">      3</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># so we have the data already in 'data/' for you</span>\n<span class=\"ansi-green-fg ansi-bold\">      4</span> \n<span class=\"ansi-green-fg ansi-bold\">      5</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Importing CNN/DailyMail articles dataset</span>\n<span class=\"ansi-green-fg\">----&gt; 6</span> train_stream_fn <span style=\"color:rgb(98,98,98)\">=</span> <span class=\"ansi-yellow-bg\">trax</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">data</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">TFDS</span><span class=\"ansi-yellow-bg\">(</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">'</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">cnn_dailymail</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">'</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">      7</span> <span class=\"ansi-yellow-bg\">                                 </span><span class=\"ansi-yellow-bg\">data_dir</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">'</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">data/</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">'</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">      8</span> <span class=\"ansi-yellow-bg\">                                 </span><span class=\"ansi-yellow-bg\">keys</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-bg\">(</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">'</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">article</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">'</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">'</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">highlights</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">'</span><span class=\"ansi-yellow-bg\">)</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">      9</span> <span class=\"ansi-yellow-bg\">                                 </span><span class=\"ansi-yellow-bg\">train</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span style=\"font-weight:bold;color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">True</span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">     11</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># This should be much faster as the data is downloaded already.</span>\n<span class=\"ansi-green-fg ansi-bold\">     12</span> eval_stream_fn <span style=\"color:rgb(98,98,98)\">=</span> trax<span style=\"color:rgb(98,98,98)\">.</span>data<span style=\"color:rgb(98,98,98)\">.</span>TFDS(<span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">cnn_dailymail</span><span style=\"color:rgb(175,0,0)\">'</span>,\n<span class=\"ansi-green-fg ansi-bold\">     13</span>                                 data_dir<span style=\"color:rgb(98,98,98)\">=</span><span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">data/</span><span style=\"color:rgb(175,0,0)\">'</span>,\n<span class=\"ansi-green-fg ansi-bold\">     14</span>                                 keys<span style=\"color:rgb(98,98,98)\">=</span>(<span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">article</span><span style=\"color:rgb(175,0,0)\">'</span>, <span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">highlights</span><span style=\"color:rgb(175,0,0)\">'</span>),\n<span class=\"ansi-green-fg ansi-bold\">     15</span>                                 train<span style=\"color:rgb(98,98,98)\">=</span><span style=\"font-weight:bold;color:rgb(0,135,0)\">False</span>)\n\nFile <span class=\"ansi-green-fg\">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/gin/config.py:1605</span>, in <span class=\"ansi-cyan-fg\">_make_gin_wrapper.&lt;locals&gt;.gin_wrapper</span><span class=\"ansi-blue-fg\">(*args, **kwargs)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1603</span> scope_info <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\"> in scope </span><span style=\"color:rgb(175,0,0)\">'</span><span style=\"font-weight:bold;color:rgb(175,95,135)\">{}</span><span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(98,98,98)\">.</span>format(scope_str) <span style=\"font-weight:bold;color:rgb(0,135,0)\">if</span> scope_str <span style=\"font-weight:bold;color:rgb(0,135,0)\">else</span> <span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">'</span>\n<span class=\"ansi-green-fg ansi-bold\">   1604</span> err_str <span style=\"color:rgb(98,98,98)\">=</span> err_str<span style=\"color:rgb(98,98,98)\">.</span>format(name, fn_or_cls, scope_info)\n<span class=\"ansi-green-fg\">-&gt; 1605</span> <span class=\"ansi-yellow-bg\">utils</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">augment_exception_message_and_reraise</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">e</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">err_str</span><span class=\"ansi-yellow-bg\">)</span>\n\nFile <span class=\"ansi-green-fg\">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/gin/utils.py:41</span>, in <span class=\"ansi-cyan-fg\">augment_exception_message_and_reraise</span><span class=\"ansi-blue-fg\">(exception, message)</span>\n<span class=\"ansi-green-fg ansi-bold\">     39</span> proxy <span style=\"color:rgb(98,98,98)\">=</span> ExceptionProxy()\n<span class=\"ansi-green-fg ansi-bold\">     40</span> ExceptionProxy<span style=\"color:rgb(98,98,98)\">.</span><span style=\"color:rgb(0,0,135)\">__qualname__</span> <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(0,135,0)\">type</span>(exception)<span style=\"color:rgb(98,98,98)\">.</span><span style=\"color:rgb(0,0,135)\">__qualname__</span>\n<span class=\"ansi-green-fg\">---&gt; 41</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">raise</span> proxy<span style=\"color:rgb(98,98,98)\">.</span>with_traceback(exception<span style=\"color:rgb(98,98,98)\">.</span>__traceback__) <span style=\"font-weight:bold;color:rgb(0,135,0)\">from</span><span style=\"color:rgb(188,188,188)\"> </span><span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>\n\nFile <span class=\"ansi-green-fg\">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/gin/config.py:1582</span>, in <span class=\"ansi-cyan-fg\">_make_gin_wrapper.&lt;locals&gt;.gin_wrapper</span><span class=\"ansi-blue-fg\">(*args, **kwargs)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1579</span> new_kwargs<span style=\"color:rgb(98,98,98)\">.</span>update(kwargs)\n<span class=\"ansi-green-fg ansi-bold\">   1581</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">try</span>:\n<span class=\"ansi-green-fg\">-&gt; 1582</span>   <span style=\"font-weight:bold;color:rgb(0,135,0)\">return</span> <span class=\"ansi-yellow-bg\">fn</span><span class=\"ansi-yellow-bg\">(</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span class=\"ansi-yellow-bg\">new_args</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span class=\"ansi-yellow-bg\">new_kwargs</span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1583</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">except</span> <span style=\"font-weight:bold;color:rgb(215,95,95)\">Exception</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">as</span> e:  <span style=\"font-style:italic;color:rgb(95,135,135)\"># pylint: disable=broad-except</span>\n<span class=\"ansi-green-fg ansi-bold\">   1584</span>   err_str <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">'</span>\n\nFile <span class=\"ansi-green-fg\">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/data/tf_inputs.py:336</span>, in <span class=\"ansi-cyan-fg\">TFDS</span><span class=\"ansi-blue-fg\">(dataset_name, data_dir, tfds_preprocess_fn, keys, train, use_alt_eval, shuffle_train, host_id, n_hosts, eval_holdout_size)</span>\n<span class=\"ansi-green-fg ansi-bold\">    333</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">else</span>:\n<span class=\"ansi-green-fg ansi-bold\">    334</span>   subsplit <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>\n<span class=\"ansi-green-fg ansi-bold\">    335</span> train_data, eval_data, _ <span style=\"color:rgb(98,98,98)\">=</span> (\n<span class=\"ansi-green-fg\">--&gt; 336</span>     <span class=\"ansi-yellow-bg\">_train_and_eval_dataset</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">dataset_name</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">    337</span> <span class=\"ansi-yellow-bg\">                            </span><span class=\"ansi-yellow-bg\">data_dir</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">    338</span> <span class=\"ansi-yellow-bg\">                            </span><span class=\"ansi-yellow-bg\">eval_holdout_size</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">    339</span> <span class=\"ansi-yellow-bg\">                            </span><span class=\"ansi-yellow-bg\">train_shuffle_files</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-bg\">shuffle_train</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">    340</span> <span class=\"ansi-yellow-bg\">                            </span><span class=\"ansi-yellow-bg\">use_alt_eval</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-bg\">use_alt_eval</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">    341</span> <span class=\"ansi-yellow-bg\">                            </span><span class=\"ansi-yellow-bg\">subsplit</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-bg\">subsplit</span><span class=\"ansi-yellow-bg\">)</span>)\n<span class=\"ansi-green-fg ansi-bold\">    342</span> dataset <span style=\"color:rgb(98,98,98)\">=</span> train_data <span style=\"font-weight:bold;color:rgb(0,135,0)\">if</span> train <span style=\"font-weight:bold;color:rgb(0,135,0)\">else</span> eval_data\n<span class=\"ansi-green-fg ansi-bold\">    343</span> dataset <span style=\"color:rgb(98,98,98)\">=</span> dataset <span style=\"font-weight:bold;color:rgb(0,135,0)\">if</span> tfds_preprocess_fn <span style=\"font-weight:bold;color:rgb(175,0,255)\">is</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">else</span> tfds_preprocess_fn(\n<span class=\"ansi-green-fg ansi-bold\">    344</span>     dataset)\n\nFile <span class=\"ansi-green-fg\">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/data/tf_inputs.py:270</span>, in <span class=\"ansi-cyan-fg\">_train_and_eval_dataset</span><span class=\"ansi-blue-fg\">(dataset_name, data_dir, eval_holdout_size, train_shuffle_files, eval_shuffle_files, use_alt_eval, subsplit)</span>\n<span class=\"ansi-green-fg ansi-bold\">    267</span>   <span style=\"font-weight:bold;color:rgb(0,135,0)\">if</span> tfds<span style=\"color:rgb(98,98,98)\">.</span>Split<span style=\"color:rgb(98,98,98)\">.</span>VALIDATION <span style=\"font-weight:bold;color:rgb(175,0,255)\">not</span> <span style=\"font-weight:bold;color:rgb(175,0,255)\">in</span> splits:\n<span class=\"ansi-green-fg ansi-bold\">    268</span>     eval_split <span style=\"color:rgb(98,98,98)\">=</span> tfds<span style=\"color:rgb(98,98,98)\">.</span>Split<span style=\"color:rgb(98,98,98)\">.</span>TEST\n<span class=\"ansi-green-fg\">--&gt; 270</span> train <span style=\"color:rgb(98,98,98)\">=</span> <span class=\"ansi-yellow-bg\">tfds</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">load</span><span class=\"ansi-yellow-bg\">(</span>\n<span class=\"ansi-green-fg ansi-bold\">    271</span> <span class=\"ansi-yellow-bg\">    </span><span class=\"ansi-yellow-bg\">name</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-bg\">dataset_name</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">    272</span> <span class=\"ansi-yellow-bg\">    </span><span class=\"ansi-yellow-bg\">split</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-bg\">train_split</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">    273</span> <span class=\"ansi-yellow-bg\">    </span><span class=\"ansi-yellow-bg\">data_dir</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-bg\">data_dir</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">    274</span> <span class=\"ansi-yellow-bg\">    </span><span class=\"ansi-yellow-bg\">shuffle_files</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-bg\">train_shuffle_files</span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">    275</span> valid <span style=\"color:rgb(98,98,98)\">=</span> tfds<span style=\"color:rgb(98,98,98)\">.</span>load(\n<span class=\"ansi-green-fg ansi-bold\">    276</span>     name<span style=\"color:rgb(98,98,98)\">=</span>dataset_name,\n<span class=\"ansi-green-fg ansi-bold\">    277</span>     split<span style=\"color:rgb(98,98,98)\">=</span>eval_split,\n<span class=\"ansi-green-fg ansi-bold\">    278</span>     data_dir<span style=\"color:rgb(98,98,98)\">=</span>data_dir,\n<span class=\"ansi-green-fg ansi-bold\">    279</span>     shuffle_files<span style=\"color:rgb(98,98,98)\">=</span>eval_shuffle_files)\n<span class=\"ansi-green-fg ansi-bold\">    280</span> keys <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>\n\nFile <span class=\"ansi-green-fg\">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tensorflow_datasets/core/logging/__init__.py:166</span>, in <span class=\"ansi-cyan-fg\">_FunctionDecorator.__call__</span><span class=\"ansi-blue-fg\">(self, function, instance, args, kwargs)</span>\n<span class=\"ansi-green-fg ansi-bold\">    164</span> metadata <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>_start_call()\n<span class=\"ansi-green-fg ansi-bold\">    165</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">try</span>:\n<span class=\"ansi-green-fg\">--&gt; 166</span>   <span style=\"font-weight:bold;color:rgb(0,135,0)\">return</span> <span class=\"ansi-yellow-bg\">function</span><span class=\"ansi-yellow-bg\">(</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span class=\"ansi-yellow-bg\">args</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span class=\"ansi-yellow-bg\">kwargs</span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">    167</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">except</span> <span style=\"font-weight:bold;color:rgb(215,95,95)\">Exception</span>:\n<span class=\"ansi-green-fg ansi-bold\">    168</span>   metadata<span style=\"color:rgb(98,98,98)\">.</span>mark_error()\n\nFile <span class=\"ansi-green-fg\">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tensorflow_datasets/core/load.py:639</span>, in <span class=\"ansi-cyan-fg\">load</span><span class=\"ansi-blue-fg\">(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)</span>\n<span class=\"ansi-green-fg ansi-bold\">    520</span> <span style=\"font-style:italic;color:rgb(175,0,0)\">\"\"\"Loads the named dataset into a `tf.data.Dataset`.</span>\n<span class=\"ansi-green-fg ansi-bold\">    521</span> \n<span class=\"ansi-green-fg ansi-bold\">    522</span> <span style=\"font-style:italic;color:rgb(175,0,0)\">`tfds.load` is a convenience method that:</span>\n<span class=\"ansi-green-fg\">   (...)</span>\n<span class=\"ansi-green-fg ansi-bold\">    631</span> <span style=\"font-style:italic;color:rgb(175,0,0)\">    Split-specific information is available in `ds_info.splits`.</span>\n<span class=\"ansi-green-fg ansi-bold\">    632</span> <span style=\"font-style:italic;color:rgb(175,0,0)\">\"\"\"</span>\n<span class=\"ansi-green-fg ansi-bold\">    633</span> dbuilder <span style=\"color:rgb(98,98,98)\">=</span> _fetch_builder(\n<span class=\"ansi-green-fg ansi-bold\">    634</span>     name,\n<span class=\"ansi-green-fg ansi-bold\">    635</span>     data_dir,\n<span class=\"ansi-green-fg ansi-bold\">    636</span>     builder_kwargs,\n<span class=\"ansi-green-fg ansi-bold\">    637</span>     try_gcs,\n<span class=\"ansi-green-fg ansi-bold\">    638</span> )\n<span class=\"ansi-green-fg\">--&gt; 639</span> <span class=\"ansi-yellow-bg\">_download_and_prepare_builder</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">dbuilder</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">download</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">download_and_prepare_kwargs</span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">    641</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">if</span> as_dataset_kwargs <span style=\"font-weight:bold;color:rgb(175,0,255)\">is</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>:\n<span class=\"ansi-green-fg ansi-bold\">    642</span>   as_dataset_kwargs <span style=\"color:rgb(98,98,98)\">=</span> {}\n\nFile <span class=\"ansi-green-fg\">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tensorflow_datasets/core/load.py:498</span>, in <span class=\"ansi-cyan-fg\">_download_and_prepare_builder</span><span class=\"ansi-blue-fg\">(dbuilder, download, download_and_prepare_kwargs)</span>\n<span class=\"ansi-green-fg ansi-bold\">    496</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">if</span> download:\n<span class=\"ansi-green-fg ansi-bold\">    497</span>   download_and_prepare_kwargs <span style=\"color:rgb(98,98,98)\">=</span> download_and_prepare_kwargs <span style=\"font-weight:bold;color:rgb(175,0,255)\">or</span> {}\n<span class=\"ansi-green-fg\">--&gt; 498</span>   <span class=\"ansi-yellow-bg\">dbuilder</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">download_and_prepare</span><span class=\"ansi-yellow-bg\">(</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span class=\"ansi-yellow-bg\">download_and_prepare_kwargs</span><span class=\"ansi-yellow-bg\">)</span>\n\nFile <span class=\"ansi-green-fg\">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tensorflow_datasets/core/logging/__init__.py:166</span>, in <span class=\"ansi-cyan-fg\">_FunctionDecorator.__call__</span><span class=\"ansi-blue-fg\">(self, function, instance, args, kwargs)</span>\n<span class=\"ansi-green-fg ansi-bold\">    164</span> metadata <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>_start_call()\n<span class=\"ansi-green-fg ansi-bold\">    165</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">try</span>:\n<span class=\"ansi-green-fg\">--&gt; 166</span>   <span style=\"font-weight:bold;color:rgb(0,135,0)\">return</span> <span class=\"ansi-yellow-bg\">function</span><span class=\"ansi-yellow-bg\">(</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span class=\"ansi-yellow-bg\">args</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span class=\"ansi-yellow-bg\">kwargs</span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">    167</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">except</span> <span style=\"font-weight:bold;color:rgb(215,95,95)\">Exception</span>:\n<span class=\"ansi-green-fg ansi-bold\">    168</span>   metadata<span style=\"color:rgb(98,98,98)\">.</span>mark_error()\n\nFile <span class=\"ansi-green-fg\">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tensorflow_datasets/core/dataset_builder.py:691</span>, in <span class=\"ansi-cyan-fg\">DatasetBuilder.download_and_prepare</span><span class=\"ansi-blue-fg\">(self, download_dir, download_config, file_format)</span>\n<span class=\"ansi-green-fg ansi-bold\">    689</span>   <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>info<span style=\"color:rgb(98,98,98)\">.</span>read_from_directory(<span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>data_dir)\n<span class=\"ansi-green-fg ansi-bold\">    690</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">else</span>:\n<span class=\"ansi-green-fg\">--&gt; 691</span>   <span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">self</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">_download_and_prepare</span><span class=\"ansi-yellow-bg\">(</span>\n<span class=\"ansi-green-fg ansi-bold\">    692</span> <span class=\"ansi-yellow-bg\">      </span><span class=\"ansi-yellow-bg\">dl_manager</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-bg\">dl_manager</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">    693</span> <span class=\"ansi-yellow-bg\">      </span><span class=\"ansi-yellow-bg\">download_config</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-bg\">download_config</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">    694</span> <span class=\"ansi-yellow-bg\">  </span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">    696</span>   <span style=\"font-style:italic;color:rgb(95,135,135)\"># NOTE: If modifying the lines below to put additional information in</span>\n<span class=\"ansi-green-fg ansi-bold\">    697</span>   <span style=\"font-style:italic;color:rgb(95,135,135)\"># DatasetInfo, you'll likely also want to update</span>\n<span class=\"ansi-green-fg ansi-bold\">    698</span>   <span style=\"font-style:italic;color:rgb(95,135,135)\"># DatasetInfo.read_from_directory to possibly restore these attributes</span>\n<span class=\"ansi-green-fg ansi-bold\">    699</span>   <span style=\"font-style:italic;color:rgb(95,135,135)\"># when reading from package data.</span>\n<span class=\"ansi-green-fg ansi-bold\">    700</span>   <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>info<span style=\"color:rgb(98,98,98)\">.</span>download_size <span style=\"color:rgb(98,98,98)\">=</span> dl_manager<span style=\"color:rgb(98,98,98)\">.</span>downloaded_size\n\nFile <span class=\"ansi-green-fg\">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tensorflow_datasets/core/dataset_builder.py:1546</span>, in <span class=\"ansi-cyan-fg\">GeneratorBasedBuilder._download_and_prepare</span><span class=\"ansi-blue-fg\">(self, dl_manager, download_config)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1544</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">else</span>:\n<span class=\"ansi-green-fg ansi-bold\">   1545</span>   optional_pipeline_kwargs <span style=\"color:rgb(98,98,98)\">=</span> {}\n<span class=\"ansi-green-fg\">-&gt; 1546</span> split_generators <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">self</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">_split_generators</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">  </span><span style=\"font-style:italic;color:rgb(95,135,135)\" class=\"ansi-yellow-bg\"># pylint: disable=unexpected-keyword-arg</span>\n<span class=\"ansi-green-fg ansi-bold\">   1547</span> <span class=\"ansi-yellow-bg\">    </span><span class=\"ansi-yellow-bg\">dl_manager</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span class=\"ansi-yellow-bg\">optional_pipeline_kwargs</span>\n<span class=\"ansi-green-fg ansi-bold\">   1548</span> <span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1549</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># TODO(tfds): Could be removed once all datasets are migrated.</span>\n<span class=\"ansi-green-fg ansi-bold\">   1550</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># https://github.com/tensorflow/datasets/issues/2537</span>\n<span class=\"ansi-green-fg ansi-bold\">   1551</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Legacy mode (eventually convert list[SplitGeneratorLegacy] -&gt; dict)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1552</span> split_generators <span style=\"color:rgb(98,98,98)\">=</span> split_builder<span style=\"color:rgb(98,98,98)\">.</span>normalize_legacy_split_generators(\n<span class=\"ansi-green-fg ansi-bold\">   1553</span>     split_generators<span style=\"color:rgb(98,98,98)\">=</span>split_generators,\n<span class=\"ansi-green-fg ansi-bold\">   1554</span>     generator_fn<span style=\"color:rgb(98,98,98)\">=</span><span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>_generate_examples,\n<span class=\"ansi-green-fg ansi-bold\">   1555</span>     is_beam<span style=\"color:rgb(98,98,98)\">=</span><span style=\"color:rgb(0,135,0)\">isinstance</span>(<span style=\"color:rgb(0,135,0)\">self</span>, BeamBasedBuilder),\n<span class=\"ansi-green-fg ansi-bold\">   1556</span> )\n\nFile <span class=\"ansi-green-fg\">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tensorflow_datasets/summarization/cnn_dailymail.py:238</span>, in <span class=\"ansi-cyan-fg\">CnnDailymail._split_generators</span><span class=\"ansi-blue-fg\">(self, dl_manager)</span>\n<span class=\"ansi-green-fg ansi-bold\">    237</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">def</span><span style=\"color:rgb(188,188,188)\"> </span><span style=\"color:rgb(0,0,255)\">_split_generators</span>(<span style=\"color:rgb(0,135,0)\">self</span>, dl_manager):\n<span class=\"ansi-green-fg\">--&gt; 238</span>   dl_paths <span style=\"color:rgb(98,98,98)\">=</span> <span class=\"ansi-yellow-bg\">dl_manager</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">download_and_extract</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">_DL_URLS</span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">    240</span>   <span style=\"font-weight:bold;color:rgb(0,135,0)\">return</span> [\n<span class=\"ansi-green-fg ansi-bold\">    241</span>       tfds<span style=\"color:rgb(98,98,98)\">.</span>core<span style=\"color:rgb(98,98,98)\">.</span>SplitGenerator(\n<span class=\"ansi-green-fg ansi-bold\">    242</span>           name<span style=\"color:rgb(98,98,98)\">=</span>tfds<span style=\"color:rgb(98,98,98)\">.</span>Split<span style=\"color:rgb(98,98,98)\">.</span>TRAIN,\n<span class=\"ansi-green-fg\">   (...)</span>\n<span class=\"ansi-green-fg ansi-bold\">    254</span>       ),\n<span class=\"ansi-green-fg ansi-bold\">    255</span>   ]\n\nFile <span class=\"ansi-green-fg\">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tensorflow_datasets/core/download/download_manager.py:688</span>, in <span class=\"ansi-cyan-fg\">DownloadManager.download_and_extract</span><span class=\"ansi-blue-fg\">(self, url_or_urls)</span>\n<span class=\"ansi-green-fg ansi-bold\">    686</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">with</span> <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>_downloader<span style=\"color:rgb(98,98,98)\">.</span>tqdm():\n<span class=\"ansi-green-fg ansi-bold\">    687</span>   <span style=\"font-weight:bold;color:rgb(0,135,0)\">with</span> <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>_extractor<span style=\"color:rgb(98,98,98)\">.</span>tqdm():\n<span class=\"ansi-green-fg\">--&gt; 688</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">return</span> <span class=\"ansi-yellow-bg\">_map_promise</span><span class=\"ansi-yellow-bg\">(</span><span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">self</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">_download_extract</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">url_or_urls</span><span class=\"ansi-yellow-bg\">)</span>\n\nFile <span class=\"ansi-green-fg\">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tensorflow_datasets/core/download/download_manager.py:831</span>, in <span class=\"ansi-cyan-fg\">_map_promise</span><span class=\"ansi-blue-fg\">(map_fn, all_inputs)</span>\n<span class=\"ansi-green-fg ansi-bold\">    827</span> <span style=\"font-style:italic;color:rgb(175,0,0)\">\"\"\"Map the function into each element and resolve the promise.\"\"\"</span>\n<span class=\"ansi-green-fg ansi-bold\">    828</span> all_promises <span style=\"color:rgb(98,98,98)\">=</span> tree_utils<span style=\"color:rgb(98,98,98)\">.</span>map_structure(\n<span class=\"ansi-green-fg ansi-bold\">    829</span>     map_fn, all_inputs\n<span class=\"ansi-green-fg ansi-bold\">    830</span> )  <span style=\"font-style:italic;color:rgb(95,135,135)\"># Apply the function</span>\n<span class=\"ansi-green-fg\">--&gt; 831</span> res <span style=\"color:rgb(98,98,98)\">=</span> <span class=\"ansi-yellow-bg\">tree_utils</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">map_structure</span><span class=\"ansi-yellow-bg\">(</span>\n<span class=\"ansi-green-fg ansi-bold\">    832</span> <span class=\"ansi-yellow-bg\">    </span><span style=\"font-weight:bold;color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">lambda</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">p</span><span class=\"ansi-yellow-bg\">:</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">p</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">get</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">)</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">all_promises</span>\n<span class=\"ansi-green-fg ansi-bold\">    833</span> <span class=\"ansi-yellow-bg\">)</span>  <span style=\"font-style:italic;color:rgb(95,135,135)\"># Wait promises</span>\n<span class=\"ansi-green-fg ansi-bold\">    834</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">return</span> res\n\nFile <span class=\"ansi-green-fg\">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tree/__init__.py:435</span>, in <span class=\"ansi-cyan-fg\">map_structure</span><span class=\"ansi-blue-fg\">(func, *structures, **kwargs)</span>\n<span class=\"ansi-green-fg ansi-bold\">    432</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">for</span> other <span style=\"font-weight:bold;color:rgb(175,0,255)\">in</span> structures[<span style=\"color:rgb(98,98,98)\">1</span>:]:\n<span class=\"ansi-green-fg ansi-bold\">    433</span>   assert_same_structure(structures[<span style=\"color:rgb(98,98,98)\">0</span>], other, check_types<span style=\"color:rgb(98,98,98)\">=</span>check_types)\n<span class=\"ansi-green-fg ansi-bold\">    434</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">return</span> unflatten_as(structures[<span style=\"color:rgb(98,98,98)\">0</span>],\n<span class=\"ansi-green-fg\">--&gt; 435</span>                     [func(<span style=\"color:rgb(98,98,98)\">*</span>args) <span style=\"font-weight:bold;color:rgb(0,135,0)\">for</span> args <span style=\"font-weight:bold;color:rgb(175,0,255)\">in</span> <span style=\"color:rgb(0,135,0)\">zip</span>(<span style=\"color:rgb(98,98,98)\">*</span><span style=\"color:rgb(0,135,0)\">map</span>(flatten, structures))])\n\nFile <span class=\"ansi-green-fg\">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tree/__init__.py:435</span>, in <span class=\"ansi-cyan-fg\">&lt;listcomp&gt;</span><span class=\"ansi-blue-fg\">(.0)</span>\n<span class=\"ansi-green-fg ansi-bold\">    432</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">for</span> other <span style=\"font-weight:bold;color:rgb(175,0,255)\">in</span> structures[<span style=\"color:rgb(98,98,98)\">1</span>:]:\n<span class=\"ansi-green-fg ansi-bold\">    433</span>   assert_same_structure(structures[<span style=\"color:rgb(98,98,98)\">0</span>], other, check_types<span style=\"color:rgb(98,98,98)\">=</span>check_types)\n<span class=\"ansi-green-fg ansi-bold\">    434</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">return</span> unflatten_as(structures[<span style=\"color:rgb(98,98,98)\">0</span>],\n<span class=\"ansi-green-fg\">--&gt; 435</span>                     [<span class=\"ansi-yellow-bg\">func</span><span class=\"ansi-yellow-bg\">(</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span class=\"ansi-yellow-bg\">args</span><span class=\"ansi-yellow-bg\">)</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">for</span> args <span style=\"font-weight:bold;color:rgb(175,0,255)\">in</span> <span style=\"color:rgb(0,135,0)\">zip</span>(<span style=\"color:rgb(98,98,98)\">*</span><span style=\"color:rgb(0,135,0)\">map</span>(flatten, structures))])\n\nFile <span class=\"ansi-green-fg\">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tensorflow_datasets/core/download/download_manager.py:832</span>, in <span class=\"ansi-cyan-fg\">_map_promise.&lt;locals&gt;.&lt;lambda&gt;</span><span class=\"ansi-blue-fg\">(p)</span>\n<span class=\"ansi-green-fg ansi-bold\">    827</span> <span style=\"font-style:italic;color:rgb(175,0,0)\">\"\"\"Map the function into each element and resolve the promise.\"\"\"</span>\n<span class=\"ansi-green-fg ansi-bold\">    828</span> all_promises <span style=\"color:rgb(98,98,98)\">=</span> tree_utils<span style=\"color:rgb(98,98,98)\">.</span>map_structure(\n<span class=\"ansi-green-fg ansi-bold\">    829</span>     map_fn, all_inputs\n<span class=\"ansi-green-fg ansi-bold\">    830</span> )  <span style=\"font-style:italic;color:rgb(95,135,135)\"># Apply the function</span>\n<span class=\"ansi-green-fg ansi-bold\">    831</span> res <span style=\"color:rgb(98,98,98)\">=</span> tree_utils<span style=\"color:rgb(98,98,98)\">.</span>map_structure(\n<span class=\"ansi-green-fg\">--&gt; 832</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">lambda</span> p: <span class=\"ansi-yellow-bg\">p</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">get</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">)</span>, all_promises\n<span class=\"ansi-green-fg ansi-bold\">    833</span> )  <span style=\"font-style:italic;color:rgb(95,135,135)\"># Wait promises</span>\n<span class=\"ansi-green-fg ansi-bold\">    834</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">return</span> res\n\nFile <span class=\"ansi-green-fg\">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/promise/promise.py:512</span>, in <span class=\"ansi-cyan-fg\">Promise.get</span><span class=\"ansi-blue-fg\">(self, timeout)</span>\n<span class=\"ansi-green-fg ansi-bold\">    510</span> target <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>_target()\n<span class=\"ansi-green-fg ansi-bold\">    511</span> <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>_wait(timeout <span style=\"font-weight:bold;color:rgb(175,0,255)\">or</span> DEFAULT_TIMEOUT)\n<span class=\"ansi-green-fg\">--&gt; 512</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">return</span> <span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">self</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">_target_settled_value</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">_raise</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span style=\"font-weight:bold;color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">True</span><span class=\"ansi-yellow-bg\">)</span>\n\nFile <span class=\"ansi-green-fg\">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/promise/promise.py:516</span>, in <span class=\"ansi-cyan-fg\">Promise._target_settled_value</span><span class=\"ansi-blue-fg\">(self, _raise)</span>\n<span class=\"ansi-green-fg ansi-bold\">    514</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">def</span><span style=\"color:rgb(188,188,188)\"> </span><span style=\"color:rgb(0,0,255)\">_target_settled_value</span>(<span style=\"color:rgb(0,135,0)\">self</span>, _raise<span style=\"color:rgb(98,98,98)\">=</span><span style=\"font-weight:bold;color:rgb(0,135,0)\">False</span>):\n<span class=\"ansi-green-fg ansi-bold\">    515</span>     <span style=\"font-style:italic;color:rgb(95,135,135)\"># type: (bool) -&gt; Any</span>\n<span class=\"ansi-green-fg\">--&gt; 516</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">return</span> <span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">self</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">_target</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">)</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">_settled_value</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">_raise</span><span class=\"ansi-yellow-bg\">)</span>\n\nFile <span class=\"ansi-green-fg\">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/promise/promise.py:226</span>, in <span class=\"ansi-cyan-fg\">Promise._settled_value</span><span class=\"ansi-blue-fg\">(self, _raise)</span>\n<span class=\"ansi-green-fg ansi-bold\">    224</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">if</span> _raise:\n<span class=\"ansi-green-fg ansi-bold\">    225</span>     raise_val <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>_fulfillment_handler0\n<span class=\"ansi-green-fg\">--&gt; 226</span>     <span class=\"ansi-yellow-bg\">reraise</span><span class=\"ansi-yellow-bg\">(</span><span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">type</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">raise_val</span><span class=\"ansi-yellow-bg\">)</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">raise_val</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">self</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">_traceback</span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">    227</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">return</span> <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>_fulfillment_handler0\n\nFile <span class=\"ansi-green-fg\">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/six.py:724</span>, in <span class=\"ansi-cyan-fg\">reraise</span><span class=\"ansi-blue-fg\">(tp, value, tb)</span>\n<span class=\"ansi-green-fg ansi-bold\">    722</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">if</span> value<span style=\"color:rgb(98,98,98)\">.</span>__traceback__ <span style=\"font-weight:bold;color:rgb(175,0,255)\">is</span> <span style=\"font-weight:bold;color:rgb(175,0,255)\">not</span> tb:\n<span class=\"ansi-green-fg ansi-bold\">    723</span>         <span style=\"font-weight:bold;color:rgb(0,135,0)\">raise</span> value<span style=\"color:rgb(98,98,98)\">.</span>with_traceback(tb)\n<span class=\"ansi-green-fg\">--&gt; 724</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">raise</span> value\n<span class=\"ansi-green-fg ansi-bold\">    725</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">finally</span>:\n<span class=\"ansi-green-fg ansi-bold\">    726</span>     value <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>\n\nFile <span class=\"ansi-green-fg\">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/promise/promise.py:87</span>, in <span class=\"ansi-cyan-fg\">try_catch</span><span class=\"ansi-blue-fg\">(handler, *args, **kwargs)</span>\n<span class=\"ansi-green-fg ansi-bold\">     84</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">def</span><span style=\"color:rgb(188,188,188)\"> </span><span style=\"color:rgb(0,0,255)\">try_catch</span>(handler, <span style=\"color:rgb(98,98,98)\">*</span>args, <span style=\"color:rgb(98,98,98)\">*</span><span style=\"color:rgb(98,98,98)\">*</span>kwargs):\n<span class=\"ansi-green-fg ansi-bold\">     85</span>     <span style=\"font-style:italic;color:rgb(95,135,135)\"># type: (Callable, Any, Any) -&gt; Union[Tuple[Any, None], Tuple[None, Tuple[Exception, Optional[TracebackType]]]]</span>\n<span class=\"ansi-green-fg ansi-bold\">     86</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">try</span>:\n<span class=\"ansi-green-fg\">---&gt; 87</span>         <span style=\"font-weight:bold;color:rgb(0,135,0)\">return</span> (<span class=\"ansi-yellow-bg\">handler</span><span class=\"ansi-yellow-bg\">(</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span class=\"ansi-yellow-bg\">args</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span class=\"ansi-yellow-bg\">kwargs</span><span class=\"ansi-yellow-bg\">)</span>, <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>)\n<span class=\"ansi-green-fg ansi-bold\">     88</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">except</span> <span style=\"font-weight:bold;color:rgb(215,95,95)\">Exception</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">as</span> e:\n<span class=\"ansi-green-fg ansi-bold\">     89</span>         tb <span style=\"color:rgb(98,98,98)\">=</span> exc_info()[<span style=\"color:rgb(98,98,98)\">2</span>]\n\nFile <span class=\"ansi-green-fg\">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tensorflow_datasets/core/download/download_manager.py:408</span>, in <span class=\"ansi-cyan-fg\">DownloadManager._download.&lt;locals&gt;.&lt;lambda&gt;</span><span class=\"ansi-blue-fg\">(dl_result)</span>\n<span class=\"ansi-green-fg ansi-bold\">    402</span>   future <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>_downloader<span style=\"color:rgb(98,98,98)\">.</span>download(\n<span class=\"ansi-green-fg ansi-bold\">    403</span>       url, download_tmp_dir, verify<span style=\"color:rgb(98,98,98)\">=</span><span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>_verify_ssl\n<span class=\"ansi-green-fg ansi-bold\">    404</span>   )\n<span class=\"ansi-green-fg ansi-bold\">    406</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Post-process the result</span>\n<span class=\"ansi-green-fg ansi-bold\">    407</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">return</span> future<span style=\"color:rgb(98,98,98)\">.</span>then(\n<span class=\"ansi-green-fg\">--&gt; 408</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">lambda</span> dl_result: <span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">self</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">_register_or_validate_checksums</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">  </span><span style=\"font-style:italic;color:rgb(95,135,135)\" class=\"ansi-yellow-bg\"># pylint: disable=g-long-lambda</span>\n<span class=\"ansi-green-fg ansi-bold\">    409</span> <span class=\"ansi-yellow-bg\">        </span><span class=\"ansi-yellow-bg\">url</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-bg\">url</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">    410</span> <span class=\"ansi-yellow-bg\">        </span><span class=\"ansi-yellow-bg\">path</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-bg\">dl_result</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">path</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">    411</span> <span class=\"ansi-yellow-bg\">        </span><span class=\"ansi-yellow-bg\">computed_url_info</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-bg\">dl_result</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">url_info</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">    412</span> <span class=\"ansi-yellow-bg\">        </span><span class=\"ansi-yellow-bg\">expected_url_info</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-bg\">expected_url_info</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">    413</span> <span class=\"ansi-yellow-bg\">        </span><span class=\"ansi-yellow-bg\">checksum_path</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-bg\">checksum_path</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">    414</span> <span class=\"ansi-yellow-bg\">        </span><span class=\"ansi-yellow-bg\">url_path</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-bg\">url_path</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">    415</span> <span class=\"ansi-yellow-bg\">    </span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">    416</span> )\n\nFile <span class=\"ansi-green-fg\">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tensorflow_datasets/core/download/download_manager.py:465</span>, in <span class=\"ansi-cyan-fg\">DownloadManager._register_or_validate_checksums</span><span class=\"ansi-blue-fg\">(self, path, url, expected_url_info, computed_url_info, checksum_path, url_path)</span>\n<span class=\"ansi-green-fg ansi-bold\">    454</span>   checksum_path <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>_get_dl_path(url, computed_url_info<span style=\"color:rgb(98,98,98)\">.</span>checksum)\n<span class=\"ansi-green-fg ansi-bold\">    455</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">else</span>:\n<span class=\"ansi-green-fg ansi-bold\">    456</span>   <span style=\"font-style:italic;color:rgb(95,135,135)\"># Eventually validate checksums</span>\n<span class=\"ansi-green-fg ansi-bold\">    457</span>   <span style=\"font-style:italic;color:rgb(95,135,135)\"># Note:</span>\n<span class=\"ansi-green-fg\">   (...)</span>\n<span class=\"ansi-green-fg ansi-bold\">    463</span>   <span style=\"font-style:italic;color:rgb(95,135,135)\">#   download). This is expected as it might mean the downloaded file</span>\n<span class=\"ansi-green-fg ansi-bold\">    464</span>   <span style=\"font-style:italic;color:rgb(95,135,135)\">#   was corrupted. Note: The tmp file isn't deleted to allow inspection.</span>\n<span class=\"ansi-green-fg\">--&gt; 465</span>   <span class=\"ansi-yellow-bg\">_validate_checksums</span><span class=\"ansi-yellow-bg\">(</span>\n<span class=\"ansi-green-fg ansi-bold\">    466</span> <span class=\"ansi-yellow-bg\">      </span><span class=\"ansi-yellow-bg\">url</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-bg\">url</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">    467</span> <span class=\"ansi-yellow-bg\">      </span><span class=\"ansi-yellow-bg\">path</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-bg\">path</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">    468</span> <span class=\"ansi-yellow-bg\">      </span><span class=\"ansi-yellow-bg\">expected_url_info</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-bg\">expected_url_info</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">    469</span> <span class=\"ansi-yellow-bg\">      </span><span class=\"ansi-yellow-bg\">computed_url_info</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-bg\">computed_url_info</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">    470</span> <span class=\"ansi-yellow-bg\">      </span><span class=\"ansi-yellow-bg\">force_checksums_validation</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">self</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">_force_checksums_validation</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">    471</span> <span class=\"ansi-yellow-bg\">  </span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">    473</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">return</span> <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>_rename_and_get_final_dl_path(\n<span class=\"ansi-green-fg ansi-bold\">    474</span>     url<span style=\"color:rgb(98,98,98)\">=</span>url,\n<span class=\"ansi-green-fg ansi-bold\">    475</span>     path<span style=\"color:rgb(98,98,98)\">=</span>path,\n<span class=\"ansi-green-fg\">   (...)</span>\n<span class=\"ansi-green-fg ansi-bold\">    479</span>     url_path<span style=\"color:rgb(98,98,98)\">=</span>url_path,\n<span class=\"ansi-green-fg ansi-bold\">    480</span> )\n\nFile <span class=\"ansi-green-fg\">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tensorflow_datasets/core/download/download_manager.py:809</span>, in <span class=\"ansi-cyan-fg\">_validate_checksums</span><span class=\"ansi-blue-fg\">(url, path, computed_url_info, expected_url_info, force_checksums_validation)</span>\n<span class=\"ansi-green-fg ansi-bold\">    797</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">if</span> (\n<span class=\"ansi-green-fg ansi-bold\">    798</span>     expected_url_info\n<span class=\"ansi-green-fg ansi-bold\">    799</span>     <span style=\"font-weight:bold;color:rgb(175,0,255)\">and</span> computed_url_info\n<span class=\"ansi-green-fg ansi-bold\">    800</span>     <span style=\"font-weight:bold;color:rgb(175,0,255)\">and</span> expected_url_info <span style=\"color:rgb(98,98,98)\">!=</span> computed_url_info\n<span class=\"ansi-green-fg ansi-bold\">    801</span> ):\n<span class=\"ansi-green-fg ansi-bold\">    802</span>   msg <span style=\"color:rgb(98,98,98)\">=</span> (\n<span class=\"ansi-green-fg ansi-bold\">    803</span>       <span style=\"color:rgb(175,0,0)\">f</span><span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">Artifact </span><span style=\"font-weight:bold;color:rgb(175,95,135)\">{</span>url<span style=\"font-weight:bold;color:rgb(175,95,135)\">}</span><span style=\"color:rgb(175,0,0)\">, downloaded to </span><span style=\"font-weight:bold;color:rgb(175,95,135)\">{</span>path<span style=\"font-weight:bold;color:rgb(175,95,135)\">}</span><span style=\"color:rgb(175,0,0)\">, has wrong checksum:</span><span style=\"font-weight:bold;color:rgb(175,95,0)\">\\n</span><span style=\"color:rgb(175,0,0)\">'</span>\n<span class=\"ansi-green-fg ansi-bold\">    804</span>       <span style=\"color:rgb(175,0,0)\">f</span><span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">* Expected: </span><span style=\"font-weight:bold;color:rgb(175,95,135)\">{</span>expected_url_info<span style=\"font-weight:bold;color:rgb(175,95,135)\">}</span><span style=\"font-weight:bold;color:rgb(175,95,0)\">\\n</span><span style=\"color:rgb(175,0,0)\">'</span>\n<span class=\"ansi-green-fg\">   (...)</span>\n<span class=\"ansi-green-fg ansi-bold\">    807</span>       <span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">https://www.tensorflow.org/datasets/overview#fixing_nonmatchingchecksumerror</span><span style=\"color:rgb(175,0,0)\">'</span>\n<span class=\"ansi-green-fg ansi-bold\">    808</span>   )\n<span class=\"ansi-green-fg\">--&gt; 809</span>   <span style=\"font-weight:bold;color:rgb(0,135,0)\">raise</span> NonMatchingChecksumError(msg)\n\n<span class=\"ansi-red-fg\">NonMatchingChecksumError</span>: Artifact https://drive.google.com/uc?export=download&amp;id=0BwmD_VLjROrfTHk4NFg2SndKcjQ, downloaded to data/downloads/ucexport_download_id_0BwmD_VLjROrfTHk4NFg2SndKG8BdJPpt2iRo6Dpzz23CByJuAePEilB-pxbcBCHaWDs.tmp.0a9bdc0e7618468383b40cf1b79e3e37/download, has wrong checksum:\n* Expected: UrlInfo(size=151.23 MiB, checksum='e8fbc0027e54e0a916abd9c969eb35f708ed1467d7ef4e3b17a56739d65cb200', filename='cnn_stories.tgz')\n* Got: UrlInfo(size=2.36 KiB, checksum='82f743d167fd9c076191ff53c96e626e0b870f77ef221509a2c10e11d00a0396', filename='download')\nTo debug, see: https://www.tensorflow.org/datasets/overview#fixing_nonmatchingchecksumerror\n  In call to configurable 'TFDS' (&lt;function TFDS at 0x7650cc6652d0&gt;)</pre>\n```\n:::\n\n:::\n:::\n\n\n<a name='1.1'></a>\n## 1.1 Tokenize & Detokenize helper functions\n\nJust like in the previous assignment, the cell above loads in the encoder for you. Given any data set, you have to be able to map words to their indices, and indices to their words. The inputs and outputs to your [Trax](https://github.com/google/trax) models are usually tensors of numbers where each number corresponds to a word. If you were to process your data manually, you would have to make use of the following: \n\n- <span style='color:blue'> word2Ind: </span> a dictionary mapping the word to its index.\n- <span style='color:blue'> ind2Word:</span> a dictionary mapping the index to its word.\n- <span style='color:blue'> word2Count:</span> a dictionary mapping the word to the number of times it appears. \n- <span style='color:blue'> num_words:</span> total number of words that have appeared. \n\nSince you have already implemented these in previous assignments of the specialization, we will provide you with helper functions that will do this for you. Run the cell below to get the following functions:\n\n- <span style='color:blue'> tokenize: </span> converts a text sentence to its corresponding token list (i.e. list of indices). Also converts words to subwords.\n- <span style='color:blue'> detokenize: </span> converts a token list to its corresponding sentence (i.e. string).\n\n::: {#8e90bc17 .cell execution_count=3}\n``` {.python .cell-code}\ndef tokenize(input_str, EOS=1):\n    \"\"\"Input str to features dict, ready for inference\"\"\"\n  \n    # Use the trax.data.tokenize method. It takes streams and returns streams,\n    # we get around it by making a 1-element stream with `iter`.\n    inputs =  next(trax.data.tokenize(iter([input_str]),\n                                      vocab_dir='vocab_dir/',\n                                      vocab_file='summarize32k.subword.subwords'))\n    \n    # Mark the end of the sentence with EOS\n    return list(inputs) + [EOS]\n\ndef detokenize(integers):\n    \"\"\"List of ints to str\"\"\"\n  \n    s = trax.data.detokenize(integers,\n                             vocab_dir='vocab_dir/',\n                             vocab_file='summarize32k.subword.subwords')\n    \n    return wrapper.fill(s)\n```\n:::\n\n\n<a name='1.2'></a>\n\n## 1.2 Preprocessing for Language Models: Concatenate It!\n\nThis week you will use a language model -- Transformer Decoder -- to solve\nan input-output problem. As you know, language models only predict the next\nword, they have no notion of inputs. To create a single input suitable for\na language model, we concatenate inputs with targets putting a separator\nin between. We also need to create a mask -- with 0s at inputs and 1s at targets -- so that the model is not penalized for mis-predicting the article and only focuses on the summary. See the preprocess function below for how this is done.\n\n::: {#34e10778 .cell execution_count=4}\n``` {.python .cell-code}\n# Special tokens\nSEP = 0 # Padding or separator token\nEOS = 1 # End of sentence token\n\n# Concatenate tokenized inputs and targets using 0 as separator.\ndef preprocess(stream):\n    for (article, summary) in stream:\n        joint = np.array(list(article) + [EOS, SEP] + list(summary) + [EOS])\n        mask = [0] * (len(list(article)) + 2) + [1] * (len(list(summary)) + 1) # Accounting for EOS and SEP\n        yield joint, joint, np.array(mask)\n\n# You can combine a few data preprocessing steps into a pipeline like this.\ninput_pipeline = trax.data.Serial(\n    # Tokenizes\n    trax.data.Tokenize(vocab_dir='vocab_dir/',\n                       vocab_file='summarize32k.subword.subwords'),\n    # Uses function defined above\n    preprocess,\n    # Filters out examples longer than 2048\n    trax.data.FilterByLength(2048)\n)\n\n# Apply preprocessing to data streams.\ntrain_stream = input_pipeline(train_stream_fn())\neval_stream = input_pipeline(eval_stream_fn())\n\ntrain_input, train_target, train_mask = next(train_stream)\n\nassert sum((train_input - train_target)**2) == 0  # They are the same in Language Model (LM).\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[4], line 24</span>\n<span class=\"ansi-green-fg ansi-bold\">     13</span> input_pipeline <span style=\"color:rgb(98,98,98)\">=</span> trax<span style=\"color:rgb(98,98,98)\">.</span>data<span style=\"color:rgb(98,98,98)\">.</span>Serial(\n<span class=\"ansi-green-fg ansi-bold\">     14</span>     <span style=\"font-style:italic;color:rgb(95,135,135)\"># Tokenizes</span>\n<span class=\"ansi-green-fg ansi-bold\">     15</span>     trax<span style=\"color:rgb(98,98,98)\">.</span>data<span style=\"color:rgb(98,98,98)\">.</span>Tokenize(vocab_dir<span style=\"color:rgb(98,98,98)\">=</span><span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">vocab_dir/</span><span style=\"color:rgb(175,0,0)\">'</span>,\n<span class=\"ansi-green-fg\">   (...)</span>\n<span class=\"ansi-green-fg ansi-bold\">     20</span>     trax<span style=\"color:rgb(98,98,98)\">.</span>data<span style=\"color:rgb(98,98,98)\">.</span>FilterByLength(<span style=\"color:rgb(98,98,98)\">2048</span>)\n<span class=\"ansi-green-fg ansi-bold\">     21</span> )\n<span class=\"ansi-green-fg ansi-bold\">     23</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Apply preprocessing to data streams.</span>\n<span class=\"ansi-green-fg\">---&gt; 24</span> train_stream <span style=\"color:rgb(98,98,98)\">=</span> input_pipeline(<span class=\"ansi-yellow-bg\">train_stream_fn</span>())\n<span class=\"ansi-green-fg ansi-bold\">     25</span> eval_stream <span style=\"color:rgb(98,98,98)\">=</span> input_pipeline(eval_stream_fn())\n<span class=\"ansi-green-fg ansi-bold\">     27</span> train_input, train_target, train_mask <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(0,135,0)\">next</span>(train_stream)\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'train_stream_fn' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n::: {#de124bed .cell execution_count=5}\n``` {.python .cell-code}\n# prints mask, 0s on article, 1s on summary\nprint(f'Single example mask:\\n\\n {train_mask}')\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[5], line 2</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># prints mask, 0s on article, 1s on summary</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span> <span style=\"color:rgb(0,135,0)\">print</span>(<span style=\"color:rgb(175,0,0)\">f</span><span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">Single example mask:</span><span style=\"font-weight:bold;color:rgb(175,95,0)\">\\n</span><span style=\"font-weight:bold;color:rgb(175,95,0)\">\\n</span><span style=\"color:rgb(175,0,0)\"> </span><span style=\"font-weight:bold;color:rgb(175,95,135)\">{</span><span class=\"ansi-yellow-bg\">train_mask</span><span style=\"font-weight:bold;color:rgb(175,95,135)\">}</span><span style=\"color:rgb(175,0,0)\">'</span>)\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'train_mask' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n::: {#5eb7729e .cell execution_count=6}\n``` {.python .cell-code}\n# prints: [Example][<EOS>][<pad>][Example Summary][<EOS>]\nprint(f'Single example:\\n\\n {detokenize(train_input)}')\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[6], line 2</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># prints: [Example][&lt;EOS&gt;][&lt;pad&gt;][Example Summary][&lt;EOS&gt;]</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span> <span style=\"color:rgb(0,135,0)\">print</span>(<span style=\"color:rgb(175,0,0)\">f</span><span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">Single example:</span><span style=\"font-weight:bold;color:rgb(175,95,0)\">\\n</span><span style=\"font-weight:bold;color:rgb(175,95,0)\">\\n</span><span style=\"color:rgb(175,0,0)\"> </span><span style=\"font-weight:bold;color:rgb(175,95,135)\">{</span>detokenize(<span class=\"ansi-yellow-bg\">train_input</span>)<span style=\"font-weight:bold;color:rgb(175,95,135)\">}</span><span style=\"color:rgb(175,0,0)\">'</span>)\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'train_input' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n<a name='1.3'></a>\n\n## 1.3 Batching with bucketing\n\nAs in the previous week, we use bucketing to create batches of data.\n\n::: {#42bc12f1 .cell execution_count=7}\n``` {.python .cell-code}\n# Bucketing to create batched generators.\n\n# Buckets are defined in terms of boundaries and batch sizes.\n# Batch_sizes[i] determines the batch size for items with length < boundaries[i]\n# So below, we'll take a batch of 16 sentences of length < 128 , 8 of length < 256,\n# 4 of length < 512. And so on. \nboundaries =  [128, 256,  512, 1024]\nbatch_sizes = [16,    8,    4,    2, 1]\n\n# Create the streams.\ntrain_batch_stream = trax.data.BucketByLength(\n    boundaries, batch_sizes)(train_stream)\n\neval_batch_stream = trax.data.BucketByLength(\n    boundaries, batch_sizes)(eval_stream)\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[7], line 12</span>\n<span class=\"ansi-green-fg ansi-bold\">      8</span> batch_sizes <span style=\"color:rgb(98,98,98)\">=</span> [<span style=\"color:rgb(98,98,98)\">16</span>,    <span style=\"color:rgb(98,98,98)\">8</span>,    <span style=\"color:rgb(98,98,98)\">4</span>,    <span style=\"color:rgb(98,98,98)\">2</span>, <span style=\"color:rgb(98,98,98)\">1</span>]\n<span class=\"ansi-green-fg ansi-bold\">     10</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Create the streams.</span>\n<span class=\"ansi-green-fg ansi-bold\">     11</span> train_batch_stream <span style=\"color:rgb(98,98,98)\">=</span> trax<span style=\"color:rgb(98,98,98)\">.</span>data<span style=\"color:rgb(98,98,98)\">.</span>BucketByLength(\n<span class=\"ansi-green-fg\">---&gt; 12</span>     boundaries, batch_sizes)(<span class=\"ansi-yellow-bg\">train_stream</span>)\n<span class=\"ansi-green-fg ansi-bold\">     14</span> eval_batch_stream <span style=\"color:rgb(98,98,98)\">=</span> trax<span style=\"color:rgb(98,98,98)\">.</span>data<span style=\"color:rgb(98,98,98)\">.</span>BucketByLength(\n<span class=\"ansi-green-fg ansi-bold\">     15</span>     boundaries, batch_sizes)(eval_stream)\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'train_stream' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n::: {#ea44a6dc .cell execution_count=8}\n``` {.python .cell-code}\n# Every execution will result in generation of a different article\n# Try running this cell multiple times to see how the length of the examples affects the batch size\ninput_batch, _, mask_batch = next(train_batch_stream)\n\n# Shape of the input_batch\ninput_batch.shape\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[8], line 3</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Every execution will result in generation of a different article</span>\n<span class=\"ansi-green-fg ansi-bold\">      2</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Try running this cell multiple times to see how the length of the examples affects the batch size</span>\n<span class=\"ansi-green-fg\">----&gt; 3</span> input_batch, _, mask_batch <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(0,135,0)\">next</span>(<span class=\"ansi-yellow-bg\">train_batch_stream</span>)\n<span class=\"ansi-green-fg ansi-bold\">      5</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Shape of the input_batch</span>\n<span class=\"ansi-green-fg ansi-bold\">      6</span> input_batch<span style=\"color:rgb(98,98,98)\">.</span>shape\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'train_batch_stream' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n::: {#73cc6b5b .cell execution_count=9}\n``` {.python .cell-code}\n# print corresponding integer values\nprint(input_batch[0])\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[9], line 2</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># print corresponding integer values</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span> <span style=\"color:rgb(0,135,0)\">print</span>(<span class=\"ansi-yellow-bg\">input_batch</span>[<span style=\"color:rgb(98,98,98)\">0</span>])\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'input_batch' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\nThings to notice:\n - First we see the corresponding values of the words.\n - The first 1, which represents the `<EOS>` tag of the article.\n - Followed by a 0, which represents a `<pad>` tag.\n - After the first 0 (`<pad>` tag) the corresponding values are of the words that are used for the summary of the article.\n - The second 1 represents the `<EOS>` tag for the summary.\n - All the trailing 0s represent `<pad>` tags which are appended to maintain consistent length (If you don't see them then it would mean it is already of max length)\n\n::: {#b5fd620c .cell execution_count=10}\n``` {.python .cell-code}\n# print the article and its summary\nprint('Article:\\n\\n', detokenize(input_batch[0]))\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[10], line 2</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># print the article and its summary</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span> <span style=\"color:rgb(0,135,0)\">print</span>(<span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">Article:</span><span style=\"font-weight:bold;color:rgb(175,95,0)\">\\n</span><span style=\"font-weight:bold;color:rgb(175,95,0)\">\\n</span><span style=\"color:rgb(175,0,0)\">'</span>, detokenize(<span class=\"ansi-yellow-bg\">input_batch</span>[<span style=\"color:rgb(98,98,98)\">0</span>]))\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'input_batch' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\nYou can see that the data has the following structure:\n- <span style='color:blue'> [Article] </span> -> `<EOS>` -> `<pad>` -> <span style='color:blue'> [Article Summary] </span> -> `<EOS>` -> (possibly) multiple `<pad>`\n\nThe loss is taken only on the summary using cross_entropy as loss function. \n\n<a name='2'></a>\n# Part 2: Summarization with transformer\n\nNow that we have given you the data generator and have handled the preprocessing for you, it is time for you to build your own model. We saved you some time because we know you have already preprocessed data before in this specialization, so we would rather you spend your time doing the next steps. \n\nYou will be implementing the attention from scratch and then using it in your transformer model. Concretely, you will understand how attention works, how you use it to connect the encoder and the decoder.\n\n<img src=\"transformer_decoder_zoomin.png\">\n\n<a name='2.1'></a>\n## 2.1 Dot product attention \n\nNow you will implement dot product attention which takes in a query, key, value, and a mask. It returns the output. \n\n<img src =\"dotproduct.png\">\n\n\nHere are some helper functions that will help you create tensors and display useful information:\n   - `create_tensor`  creates a `jax numpy array` from a list of lists.\n   - `display_tensor` prints out the shape and the actual tensor.\n\n::: {#f655edd3 .cell execution_count=11}\n``` {.python .cell-code}\ndef create_tensor(t):\n    \"\"\"Create tensor from list of lists\"\"\"\n    return jnp.array(t)\n\n\ndef display_tensor(t, name):\n    \"\"\"Display shape and tensor\"\"\"\n    print(f'{name} shape: {t.shape}\\n')\n    print(f'{t}\\n')\n```\n:::\n\n\nBefore implementing it yourself, you can play around with a toy example of `dot product attention` without the softmax  operation. Technically it would not be `dot product attention` without the softmax but this is done to avoid giving away too much of the answer and the idea is to display these tensors to give you a sense of how they look like.\n\nThe formula for attention is this one:\n\n$$\n\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}+{M}\\right) V\\tag{1}\\\n$$\n\n$d_{k}$ stands for the dimension of queries and keys.\n\nThe `query`, `key`, `value` and `mask` vectors are provided for this example.\n\nNotice that the masking is done using very negative values that will yield a similar effect to using $-\\infty $. \n\n::: {#14a5bca7 .cell execution_count=12}\n``` {.python .cell-code}\nq = create_tensor([[1, 0, 0], [0, 1, 0]])\ndisplay_tensor(q, 'query')\nk = create_tensor([[1, 2, 3], [4, 5, 6]])\ndisplay_tensor(k, 'key')\nv = create_tensor([[0, 1, 0], [1, 0, 1]])\ndisplay_tensor(v, 'value')\nm = create_tensor([[0, 0], [-1e9, 0]])\ndisplay_tensor(m, 'mask')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nquery shape: (2, 3)\n\n[[1 0 0]\n [0 1 0]]\n\nkey shape: (2, 3)\n\n[[1 2 3]\n [4 5 6]]\n\nvalue shape: (2, 3)\n\n[[0 1 0]\n [1 0 1]]\n\nmask shape: (2, 2)\n\n[[ 0.e+00  0.e+00]\n [-1.e+09  0.e+00]]\n\n```\n:::\n:::\n\n\n**Expected Output:**\n```CPP\nquery shape: (2, 3)\n\n[[1 0 0]\n [0 1 0]]\n\nkey shape: (2, 3)\n\n[[1 2 3]\n [4 5 6]]\n\nvalue shape: (2, 3)\n\n[[0 1 0]\n [1 0 1]]\n\nmask shape: (2, 2)\n\n[[ 0.e+00  0.e+00]\n [-1.e+09  0.e+00]]\n\n```\n\n::: {#1d094a3e .cell execution_count=13}\n``` {.python .cell-code}\nq_dot_k = q @ k.T / jnp.sqrt(3)\ndisplay_tensor(q_dot_k, 'query dot key')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nquery dot key shape: (2, 2)\n\n[[0.57735026 2.309401  ]\n [1.1547005  2.8867512 ]]\n\n```\n:::\n:::\n\n\n**Expected Output:**\n```CPP\nquery dot key shape: (2, 2)\n\n[[0.57735026 2.309401  ]\n [1.1547005  2.8867514 ]]\n```\n\n::: {#95ea75a8 .cell execution_count=14}\n``` {.python .cell-code}\nmasked = q_dot_k + m\ndisplay_tensor(masked, 'masked query dot key')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nmasked query dot key shape: (2, 2)\n\n[[ 5.7735026e-01  2.3094010e+00]\n [-1.0000000e+09  2.8867512e+00]]\n\n```\n:::\n:::\n\n\n**Expected Output:**\n```CPP\nmasked query dot key shape: (2, 2)\n\n[[ 5.7735026e-01  2.3094010e+00]\n [-1.0000000e+09  2.8867514e+00]]\n```\n\n::: {#0a255184 .cell execution_count=15}\n``` {.python .cell-code}\ndisplay_tensor(masked @ v, 'masked query dot key dot value')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nmasked query dot key dot value shape: (2, 3)\n\n[[ 2.3094010e+00  5.7735026e-01  2.3094010e+00]\n [ 2.8867512e+00 -1.0000000e+09  2.8867512e+00]]\n\n```\n:::\n:::\n\n\n**Expected Output:**\n```CPP\nmasked query dot key dot value shape: (2, 3)\n\n[[ 2.3094010e+00  5.7735026e-01  2.3094010e+00]\n [ 2.8867514e+00 -1.0000000e+09  2.8867514e+00]]\n```\n\nIn order to use the previous dummy tensors to test some of the graded functions, a batch dimension should be added to them so they mimic the shape of real-life examples. The mask is also replaced by a version of it that resembles the one that is used by trax:\n\n::: {#58c914d1 .cell execution_count=16}\n``` {.python .cell-code}\nq_with_batch = q[None,:]\ndisplay_tensor(q_with_batch, 'query with batch dim')\nk_with_batch = k[None,:]\ndisplay_tensor(k_with_batch, 'key with batch dim')\nv_with_batch = v[None,:]\ndisplay_tensor(v_with_batch, 'value with batch dim')\nm_bool = create_tensor([[True, True], [False, True]])\ndisplay_tensor(m_bool, 'boolean mask')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nquery with batch dim shape: (1, 2, 3)\n\n[[[1 0 0]\n  [0 1 0]]]\n\nkey with batch dim shape: (1, 2, 3)\n\n[[[1 2 3]\n  [4 5 6]]]\n\nvalue with batch dim shape: (1, 2, 3)\n\n[[[0 1 0]\n  [1 0 1]]]\n\nboolean mask shape: (2, 2)\n\n[[ True  True]\n [False  True]]\n\n```\n:::\n:::\n\n\n**Expected Output:**\n```CPP\nquery with batch dim shape: (1, 2, 3)\n\n[[[1 0 0]\n  [0 1 0]]]\n\nkey with batch dim shape: (1, 2, 3)\n\n[[[1 2 3]\n  [4 5 6]]]\n\nvalue with batch dim shape: (1, 2, 3)\n\n[[[0 1 0]\n  [1 0 1]]]\n\nboolean mask shape: (2, 2)\n\n[[ True  True]\n [False  True]]\n```\n\n<a name='ex01'></a>\n### Exercise 01\n\n**Instructions:** Implement the dot product attention. Concretely, implement the following equation\n\n\n$$\n\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}+{M}\\right) V\\tag{1}\\\n$$\n\n$Q$ - query, \n$K$ - key, \n$V$ - values, \n$M$ - mask, \n${d_k}$ - depth/dimension of the queries and keys (used for scaling down)\n\nYou can implement this formula either by `trax` numpy (trax.math.numpy) or regular `numpy` but it is recommended to use `jnp`.\n\nSomething to take into consideration is that within trax, the masks are tensors of `True/False` values not 0's and $-\\infty$ as in the previous example. Within the graded function don't think of applying the mask by summing up matrices, instead use `jnp.where()` and treat the **mask as a tensor of boolean values with `False` for values that need to be masked and True for the ones that don't.**\n\nAlso take into account that the real tensors are far more complex than the toy ones you just played with. Because of this avoid using shortened operations such as `@` for dot product or `.T` for transposing. Use `jnp.matmul()` and `jnp.swapaxes()` instead.\n\nThis is the self-attention block for the transformer decoder. Good luck!  \n\n::: {#cc240a4b .cell execution_count=17}\n``` {.python .cell-code}\n# UNQ_C1\n# GRADED FUNCTION: DotProductAttention\ndef DotProductAttention(query, key, value, mask):\n    \"\"\"Dot product self-attention.\n    Args:\n        query (jax.interpreters.xla.DeviceArray): array of query representations with shape (L_q by d)\n        key (jax.interpreters.xla.DeviceArray): array of key representations with shape (L_k by d)\n        value (jax.interpreters.xla.DeviceArray): array of value representations with shape (L_k by d) where L_v = L_k\n        mask (jax.interpreters.xla.DeviceArray): attention-mask, gates attention with shape (L_q by L_k)\n\n    Returns:\n        jax.interpreters.xla.DeviceArray: Self-attention array for q, k, v arrays. (L_q by L_k)\n    \"\"\"\n\n    assert query.shape[-1] == key.shape[-1] == value.shape[-1], \"Embedding dimensions of q, k, v aren't all the same\"\n\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    # Save depth/dimension of the query embedding for scaling down the dot product\n    depth = None\n\n    # Calculate scaled query key dot product according to formula above\n    dots = jnp.matmul(None, jnp.swapaxes(None, None, None)) / jnp.sqrt(None)\n    \n    # Apply the mask\n    if mask is not None: # The 'None' in this line does not need to be replaced\n        dots = jnp.where(None, None, jnp.full_like(None, -1e9))\n    \n    # Softmax formula implementation\n    # Use trax.fastmath.logsumexp of dots to avoid underflow by division by large numbers\n    # Hint: Last axis should be used and keepdims should be True\n    # Note: softmax = e^(dots - logsumexp(dots)) = E^dots / sumexp(dots)\n    logsumexp = None\n\n    # Take exponential of dots minus logsumexp to get softmax\n    # Use jnp.exp()\n    dots = None\n\n    # Multiply dots by value to get self-attention\n    # Use jnp.matmul()\n    attention = None\n\n    ## END CODE HERE ###\n    \n    return attention\n```\n:::\n\n\n::: {#3f939377 .cell execution_count=18}\n``` {.python .cell-code}\nDotProductAttention(q_with_batch, k_with_batch, v_with_batch, m_bool)\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">TypeError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[18], line 1</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span> <span class=\"ansi-yellow-bg\">DotProductAttention</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">q_with_batch</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">k_with_batch</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">v_with_batch</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">m_bool</span><span class=\"ansi-yellow-bg\">)</span>\n\nCell <span class=\"ansi-green-fg\">In[17], line 22</span>, in <span class=\"ansi-cyan-fg\">DotProductAttention</span><span class=\"ansi-blue-fg\">(query, key, value, mask)</span>\n<span class=\"ansi-green-fg ansi-bold\">     19</span> depth <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>\n<span class=\"ansi-green-fg ansi-bold\">     21</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Calculate scaled query key dot product according to formula above</span>\n<span class=\"ansi-green-fg\">---&gt; 22</span> dots <span style=\"color:rgb(98,98,98)\">=</span> jnp<span style=\"color:rgb(98,98,98)\">.</span>matmul(<span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>, <span class=\"ansi-yellow-bg\">jnp</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">swapaxes</span><span class=\"ansi-yellow-bg\">(</span><span style=\"font-weight:bold;color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">None</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span style=\"font-weight:bold;color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">None</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span style=\"font-weight:bold;color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">None</span><span class=\"ansi-yellow-bg\">)</span>) <span style=\"color:rgb(98,98,98)\">/</span> jnp<span style=\"color:rgb(98,98,98)\">.</span>sqrt(<span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>)\n<span class=\"ansi-green-fg ansi-bold\">     24</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Apply the mask</span>\n<span class=\"ansi-green-fg ansi-bold\">     25</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">if</span> mask <span style=\"font-weight:bold;color:rgb(175,0,255)\">is</span> <span style=\"font-weight:bold;color:rgb(175,0,255)\">not</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>: <span style=\"font-style:italic;color:rgb(95,135,135)\"># The 'None' in this line does not need to be replaced</span>\n\n    <span class=\"ansi-red-fg\">[... skipping hidden 15 frame]</span>\n\nFile <span class=\"ansi-green-fg\">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:2610</span>, in <span class=\"ansi-cyan-fg\">swapaxes</span><span class=\"ansi-blue-fg\">(a, axis1, axis2)</span>\n<span class=\"ansi-green-fg ansi-bold\">   2568</span> <span style=\"color:rgb(175,0,255)\">@export</span>\n<span class=\"ansi-green-fg ansi-bold\">   2569</span> <span style=\"color:rgb(175,0,255)\">@partial</span>(jit, static_argnames<span style=\"color:rgb(98,98,98)\">=</span>(<span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">axis1</span><span style=\"color:rgb(175,0,0)\">'</span>, <span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">axis2</span><span style=\"color:rgb(175,0,0)\">'</span>), inline<span style=\"color:rgb(98,98,98)\">=</span><span style=\"font-weight:bold;color:rgb(0,135,0)\">True</span>)\n<span class=\"ansi-green-fg ansi-bold\">   2570</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">def</span><span style=\"color:rgb(188,188,188)\"> </span><span style=\"color:rgb(0,0,255)\">swapaxes</span>(a: ArrayLike, axis1: <span style=\"color:rgb(0,135,0)\">int</span>, axis2: <span style=\"color:rgb(0,135,0)\">int</span>) <span style=\"color:rgb(98,98,98)\">-</span><span style=\"color:rgb(98,98,98)\">&gt;</span> Array:\n<span class=\"ansi-green-fg ansi-bold\">   2571</span> <span style=\"color:rgb(188,188,188)\">  </span><span style=\"font-style:italic;color:rgb(175,0,0)\">\"\"\"Swap two axes of an array.</span>\n<span class=\"ansi-green-fg ansi-bold\">   2572</span> \n<span class=\"ansi-green-fg ansi-bold\">   2573</span> <span style=\"font-style:italic;color:rgb(175,0,0)\">  JAX implementation of :func:`numpy.swapaxes`, implemented in terms of</span>\n<span class=\"ansi-green-fg\">   (...)</span>\n<span class=\"ansi-green-fg ansi-bold\">   2608</span> <span style=\"font-style:italic;color:rgb(175,0,0)\">    (2, 5, 4, 3)</span>\n<span class=\"ansi-green-fg ansi-bold\">   2609</span> <span style=\"font-style:italic;color:rgb(175,0,0)\">  \"\"\"</span>\n<span class=\"ansi-green-fg\">-&gt; 2610</span>   <span class=\"ansi-yellow-bg\">util</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">check_arraylike</span><span class=\"ansi-yellow-bg\">(</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">\"</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">swapaxes</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">\"</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">a</span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">   2611</span>   perm <span style=\"color:rgb(98,98,98)\">=</span> np<span style=\"color:rgb(98,98,98)\">.</span>arange(ndim(a))\n<span class=\"ansi-green-fg ansi-bold\">   2612</span>   perm[axis1], perm[axis2] <span style=\"color:rgb(98,98,98)\">=</span> perm[axis2], perm[axis1]\n\nFile <span class=\"ansi-green-fg\">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/jax/_src/numpy/util.py:147</span>, in <span class=\"ansi-cyan-fg\">check_arraylike</span><span class=\"ansi-blue-fg\">(fun_name, emit_warning, stacklevel, *args)</span>\n<span class=\"ansi-green-fg ansi-bold\">    144</span>   warnings<span style=\"color:rgb(98,98,98)\">.</span>warn(msg <span style=\"color:rgb(98,98,98)\">+</span> <span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\"> In a future JAX release this will be an error.</span><span style=\"color:rgb(175,0,0)\">\"</span>,\n<span class=\"ansi-green-fg ansi-bold\">    145</span>                 category<span style=\"color:rgb(98,98,98)\">=</span><span style=\"font-weight:bold;color:rgb(215,95,95)\">DeprecationWarning</span>, stacklevel<span style=\"color:rgb(98,98,98)\">=</span>stacklevel)\n<span class=\"ansi-green-fg ansi-bold\">    146</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">else</span>:\n<span class=\"ansi-green-fg\">--&gt; 147</span>   <span style=\"font-weight:bold;color:rgb(0,135,0)\">raise</span> <span style=\"font-weight:bold;color:rgb(215,95,95)\">TypeError</span>(msg<span style=\"color:rgb(98,98,98)\">.</span>format(fun_name, <span style=\"color:rgb(0,135,0)\">type</span>(arg), pos))\n\n<span class=\"ansi-red-fg\">TypeError</span>: swapaxes requires ndarray or scalar arguments, got &lt;class 'NoneType'&gt; at position 0.</pre>\n```\n:::\n\n:::\n:::\n\n\n**Expected Output:**\n```CPP\nDeviceArray([[[0.8496746 , 0.15032545, 0.8496746 ],\n              [1.        , 0.        , 1.        ]]], dtype=float32)\n```    \n\n<a name='2.2'></a>\n\n## 2.2 Causal Attention\n\nNow you are going to implement causal attention: multi-headed attention with a mask to attend only to words that occurred before. \n\n<img src = \"causal.png\">\n\nIn the image above, a word can see everything that is before it, but not what is after it. To implement causal attention, you will have to transform vectors and do many reshapes. You will need to implement the functions below.\n\n\n<a name='ex02'></a>\n### Exercise 02\n\nImplement the following functions that will be needed for Causal Attention:\n\n- <span style='color:blue'> compute_attention_heads </span>: Gets an input $x$ of dimension (batch_size, seqlen, n_heads $\\times$ d_head) and splits the last (depth) dimension and stacks it to the zeroth dimension to allow matrix multiplication (batch_size $\\times$ n_heads, seqlen, d_head).\n- <span style='color:blue'> dot_product_self_attention </span>: Creates a mask matrix with `False` values above the diagonal and `True` values below and calls DotProductAttention which implements dot product self attention.\n- <span style='color:blue'> compute_attention_output </span>: Undoes compute_attention_heads by splitting first (vertical) dimension and stacking in the last (depth) dimension (batch_size, seqlen, n_heads $\\times$ d_head). These operations concatenate (stack/merge) the heads. \n\nNext there are some toy tensors which may serve to give you an idea of the data shapes and opperations involved in Causal Attention. They are also useful to test out your functions! \n\n::: {#df4ec2fd .cell execution_count=19}\n``` {.python .cell-code}\ntensor2d = create_tensor(q)\ndisplay_tensor(tensor2d, 'query matrix (2D tensor)')\n\ntensor4d2b = create_tensor([[q, q], [q, q]])\ndisplay_tensor(tensor4d2b, 'batch of two (multi-head) collections of query matrices (4D tensor)')\n\ntensor3dc = create_tensor([jnp.concatenate([q, q], axis = -1)])\ndisplay_tensor(tensor3dc, 'one batch of concatenated heads of query matrices (3d tensor)')\n\ntensor3dc3b = create_tensor([jnp.concatenate([q, q], axis = -1), jnp.concatenate([q, q], axis = -1), jnp.concatenate([q, q], axis = -1)])\ndisplay_tensor(tensor3dc3b, 'three batches of concatenated heads of query matrices (3d tensor)')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nquery matrix (2D tensor) shape: (2, 3)\n\n[[1 0 0]\n [0 1 0]]\n\nbatch of two (multi-head) collections of query matrices (4D tensor) shape: (2, 2, 2, 3)\n\n[[[[1 0 0]\n   [0 1 0]]\n\n  [[1 0 0]\n   [0 1 0]]]\n\n\n [[[1 0 0]\n   [0 1 0]]\n\n  [[1 0 0]\n   [0 1 0]]]]\n\none batch of concatenated heads of query matrices (3d tensor) shape: (1, 2, 6)\n\n[[[1 0 0 1 0 0]\n  [0 1 0 0 1 0]]]\n\nthree batches of concatenated heads of query matrices (3d tensor) shape: (3, 2, 6)\n\n[[[1 0 0 1 0 0]\n  [0 1 0 0 1 0]]\n\n [[1 0 0 1 0 0]\n  [0 1 0 0 1 0]]\n\n [[1 0 0 1 0 0]\n  [0 1 0 0 1 0]]]\n\n```\n:::\n:::\n\n\nIt is important to know that the following 3 functions would normally be defined within the `CausalAttention` function further below. \n\nHowever this makes these functions harder to test. Because of this, these functions are shown individually using a `closure` (when necessary) that simulates them being inside of the `CausalAttention` function. This is done because they rely on some variables that can be accessed from within `CausalAttention`.\n\n### Support Functions\n\n<span style='color:blue'> compute_attention_heads </span>: Gets an input $x$ of dimension (batch_size, seqlen, n_heads $\\times$ d_head) and splits the last (depth) dimension and stacks it to the zeroth dimension to allow matrix multiplication (batch_size $\\times$ n_heads, seqlen, d_head).\n\n**For the closures you only have to fill the inner function.**\n\n::: {#c794f919 .cell execution_count=20}\n``` {.python .cell-code}\n# UNQ_C2\n# GRADED FUNCTION: compute_attention_heads_closure\ndef compute_attention_heads_closure(n_heads, d_head):\n    \"\"\" Function that simulates environment inside CausalAttention function.\n    Args:\n        d_head (int):  dimensionality of heads.\n        n_heads (int): number of attention heads.\n    Returns:\n        function: compute_attention_heads function\n    \"\"\"\n\n    def compute_attention_heads(x):\n        \"\"\" Compute the attention heads.\n        Args:\n            x (jax.interpreters.xla.DeviceArray): tensor with shape (batch_size, seqlen, n_heads X d_head).\n        Returns:\n            jax.interpreters.xla.DeviceArray: reshaped tensor with shape (batch_size X n_heads, seqlen, d_head).\n        \"\"\"\n        ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n        \n        # Size of the x's batch dimension\n        batch_size = None\n        # Length of the sequence\n        # Should be size of x's first dimension without counting the batch dim\n        seqlen = None\n        # Reshape x using jnp.reshape()\n        # batch_size, seqlen, n_heads*d_head -> batch_size, seqlen, n_heads, d_head\n        x = None\n        # Transpose x using jnp.transpose()\n        # batch_size, seqlen, n_heads, d_head -> batch_size, n_heads, seqlen, d_head\n        # Note that the values within the tuple are the indexes of the dimensions of x and you must rearrange them\n        x = jnp.transpose(x, (None, None, None, None))\n        # Reshape x using jnp.reshape()\n        # batch_size, n_heads, seqlen, d_head -> batch_size*n_heads, seqlen, d_head\n        x = None\n        \n        ### END CODE HERE ###\n        \n        return x\n    \n    return compute_attention_heads\n```\n:::\n\n\n::: {#02ef0e10 .cell execution_count=21}\n``` {.python .cell-code}\ndisplay_tensor(tensor3dc3b, \"input tensor\")\nresult_cah = compute_attention_heads_closure(2,3)(tensor3dc3b)\ndisplay_tensor(result_cah, \"output tensor\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ninput tensor shape: (3, 2, 6)\n\n[[[1 0 0 1 0 0]\n  [0 1 0 0 1 0]]\n\n [[1 0 0 1 0 0]\n  [0 1 0 0 1 0]]\n\n [[1 0 0 1 0 0]\n  [0 1 0 0 1 0]]]\n\n```\n:::\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">TypeError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[21], line 2</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> display_tensor(tensor3dc3b, <span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">input tensor</span><span style=\"color:rgb(175,0,0)\">\"</span>)\n<span class=\"ansi-green-fg\">----&gt; 2</span> result_cah <span style=\"color:rgb(98,98,98)\">=</span> <span class=\"ansi-yellow-bg\">compute_attention_heads_closure</span><span class=\"ansi-yellow-bg\">(</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">2</span><span class=\"ansi-yellow-bg\">,</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">3</span><span class=\"ansi-yellow-bg\">)</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">tensor3dc3b</span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">      3</span> display_tensor(result_cah, <span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">output tensor</span><span style=\"color:rgb(175,0,0)\">\"</span>)\n\nCell <span class=\"ansi-green-fg\">In[20], line 32</span>, in <span class=\"ansi-cyan-fg\">compute_attention_heads_closure.&lt;locals&gt;.compute_attention_heads</span><span class=\"ansi-blue-fg\">(x)</span>\n<span class=\"ansi-green-fg ansi-bold\">     28</span> x <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>\n<span class=\"ansi-green-fg ansi-bold\">     29</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Transpose x using jnp.transpose()</span>\n<span class=\"ansi-green-fg ansi-bold\">     30</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># batch_size, seqlen, n_heads, d_head -&gt; batch_size, n_heads, seqlen, d_head</span>\n<span class=\"ansi-green-fg ansi-bold\">     31</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Note that the values within the tuple are the indexes of the dimensions of x and you must rearrange them</span>\n<span class=\"ansi-green-fg\">---&gt; 32</span> x <span style=\"color:rgb(98,98,98)\">=</span> <span class=\"ansi-yellow-bg\">jnp</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">transpose</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">x</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">(</span><span style=\"font-weight:bold;color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">None</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span style=\"font-weight:bold;color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">None</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span style=\"font-weight:bold;color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">None</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span style=\"font-weight:bold;color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">None</span><span class=\"ansi-yellow-bg\">)</span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">     33</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Reshape x using jnp.reshape()</span>\n<span class=\"ansi-green-fg ansi-bold\">     34</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># batch_size, n_heads, seqlen, d_head -&gt; batch_size*n_heads, seqlen, d_head</span>\n<span class=\"ansi-green-fg ansi-bold\">     35</span> x <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>\n\nFile <span class=\"ansi-green-fg\">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:1328</span>, in <span class=\"ansi-cyan-fg\">transpose</span><span class=\"ansi-blue-fg\">(a, axes)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1255</span> <span style=\"color:rgb(175,0,255)\">@export</span>\n<span class=\"ansi-green-fg ansi-bold\">   1256</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">def</span><span style=\"color:rgb(188,188,188)\"> </span><span style=\"color:rgb(0,0,255)\">transpose</span>(a: ArrayLike, axes: Sequence[<span style=\"color:rgb(0,135,0)\">int</span>] <span style=\"color:rgb(98,98,98)\">|</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span> <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>) <span style=\"color:rgb(98,98,98)\">-</span><span style=\"color:rgb(98,98,98)\">&gt;</span> Array:\n<span class=\"ansi-green-fg ansi-bold\">   1257</span> <span style=\"color:rgb(188,188,188)\">  </span><span style=\"font-style:italic;color:rgb(175,0,0)\">\"\"\"Return a transposed version of an N-dimensional array.</span>\n<span class=\"ansi-green-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-fg ansi-bold\">   1259</span> <span style=\"font-style:italic;color:rgb(175,0,0)\">  JAX implementation of :func:`numpy.transpose`, implemented in terms of</span>\n<span class=\"ansi-green-fg\">   (...)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1326</span> <span style=\"font-style:italic;color:rgb(175,0,0)\">           [2, 4]], dtype=int32)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1327</span> <span style=\"font-style:italic;color:rgb(175,0,0)\">  \"\"\"</span>\n<span class=\"ansi-green-fg\">-&gt; 1328</span>   <span class=\"ansi-yellow-bg\">util</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">check_arraylike</span><span class=\"ansi-yellow-bg\">(</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">\"</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">transpose</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">\"</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">a</span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1329</span>   axes_ <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(0,135,0)\">list</span>(<span style=\"color:rgb(0,135,0)\">range</span>(ndim(a))[::<span style=\"color:rgb(98,98,98)\">-</span><span style=\"color:rgb(98,98,98)\">1</span>]) <span style=\"font-weight:bold;color:rgb(0,135,0)\">if</span> axes <span style=\"font-weight:bold;color:rgb(175,0,255)\">is</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">else</span> axes\n<span class=\"ansi-green-fg ansi-bold\">   1330</span>   axes_ <span style=\"color:rgb(98,98,98)\">=</span> [_canonicalize_axis(i, ndim(a)) <span style=\"font-weight:bold;color:rgb(0,135,0)\">for</span> i <span style=\"font-weight:bold;color:rgb(175,0,255)\">in</span> axes_]\n\nFile <span class=\"ansi-green-fg\">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/jax/_src/numpy/util.py:147</span>, in <span class=\"ansi-cyan-fg\">check_arraylike</span><span class=\"ansi-blue-fg\">(fun_name, emit_warning, stacklevel, *args)</span>\n<span class=\"ansi-green-fg ansi-bold\">    144</span>   warnings<span style=\"color:rgb(98,98,98)\">.</span>warn(msg <span style=\"color:rgb(98,98,98)\">+</span> <span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\"> In a future JAX release this will be an error.</span><span style=\"color:rgb(175,0,0)\">\"</span>,\n<span class=\"ansi-green-fg ansi-bold\">    145</span>                 category<span style=\"color:rgb(98,98,98)\">=</span><span style=\"font-weight:bold;color:rgb(215,95,95)\">DeprecationWarning</span>, stacklevel<span style=\"color:rgb(98,98,98)\">=</span>stacklevel)\n<span class=\"ansi-green-fg ansi-bold\">    146</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">else</span>:\n<span class=\"ansi-green-fg\">--&gt; 147</span>   <span style=\"font-weight:bold;color:rgb(0,135,0)\">raise</span> <span style=\"font-weight:bold;color:rgb(215,95,95)\">TypeError</span>(msg<span style=\"color:rgb(98,98,98)\">.</span>format(fun_name, <span style=\"color:rgb(0,135,0)\">type</span>(arg), pos))\n\n<span class=\"ansi-red-fg\">TypeError</span>: transpose requires ndarray or scalar arguments, got &lt;class 'NoneType'&gt; at position 0.</pre>\n```\n:::\n\n:::\n:::\n\n\n**Expected Output:**\n```CPP\ninput tensor shape: (3, 2, 6)\n\n[[[1 0 0 1 0 0]\n  [0 1 0 0 1 0]]\n\n [[1 0 0 1 0 0]\n  [0 1 0 0 1 0]]\n\n [[1 0 0 1 0 0]\n  [0 1 0 0 1 0]]]\n\noutput tensor shape: (6, 2, 3)\n\n[[[1 0 0]\n  [0 1 0]]\n\n [[1 0 0]\n  [0 1 0]]\n\n [[1 0 0]\n  [0 1 0]]\n\n [[1 0 0]\n  [0 1 0]]\n\n [[1 0 0]\n  [0 1 0]]\n\n [[1 0 0]\n  [0 1 0]]]\n```\n\n<span style='color:blue'> dot_product_self_attention </span>: Creates a mask matrix with `False` values above the diagonal and `True` values below and calls DotProductAttention which implements dot product self attention.\n\n::: {#0202ad77 .cell execution_count=22}\n``` {.python .cell-code}\n# UNQ_C3\n# GRADED FUNCTION: dot_product_self_attention\ndef dot_product_self_attention(q, k, v):\n    \"\"\" Masked dot product self attention.\n    Args:\n        q (jax.interpreters.xla.DeviceArray): queries.\n        k (jax.interpreters.xla.DeviceArray): keys.\n        v (jax.interpreters.xla.DeviceArray): values.\n    Returns:\n        jax.interpreters.xla.DeviceArray: masked dot product self attention tensor.\n    \"\"\"\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    \n    # Hint: mask size should be equal to L_q. Remember that q has shape (batch_size, L_q, d)\n    # NOTE: there is a revision underway with the autograder to tolerate better indexing. \n    # Until then, please index q.shape using negative values (this is equivalent to counting from right to left)\n    mask_size = None\n\n    # Creates a matrix with ones below the diagonal and 0s above. It should have shape (1, mask_size, mask_size)\n    # Notice that 1's and 0's get casted to True/False by setting dtype to jnp.bool_\n    # Use jnp.tril() - Lower triangle of an array and jnp.ones()\n    mask = jnp.tril(jnp.ones((None, None, None), dtype=jnp.bool_), k=0)\n    \n    ### END CODE HERE ###\n    \n    return DotProductAttention(q, k, v, mask)\n```\n:::\n\n\n::: {#3d112ba2 .cell execution_count=23}\n``` {.python .cell-code}\ndot_product_self_attention(q_with_batch, k_with_batch, v_with_batch)\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">TypeError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[23], line 1</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span> <span class=\"ansi-yellow-bg\">dot_product_self_attention</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">q_with_batch</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">k_with_batch</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">v_with_batch</span><span class=\"ansi-yellow-bg\">)</span>\n\nCell <span class=\"ansi-green-fg\">In[22], line 22</span>, in <span class=\"ansi-cyan-fg\">dot_product_self_attention</span><span class=\"ansi-blue-fg\">(q, k, v)</span>\n<span class=\"ansi-green-fg ansi-bold\">     17</span> mask_size <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>\n<span class=\"ansi-green-fg ansi-bold\">     19</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Creates a matrix with ones below the diagonal and 0s above. It should have shape (1, mask_size, mask_size)</span>\n<span class=\"ansi-green-fg ansi-bold\">     20</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Notice that 1's and 0's get casted to True/False by setting dtype to jnp.bool_</span>\n<span class=\"ansi-green-fg ansi-bold\">     21</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Use jnp.tril() - Lower triangle of an array and jnp.ones()</span>\n<span class=\"ansi-green-fg\">---&gt; 22</span> mask <span style=\"color:rgb(98,98,98)\">=</span> jnp<span style=\"color:rgb(98,98,98)\">.</span>tril(<span class=\"ansi-yellow-bg\">jnp</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">ones</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">(</span><span style=\"font-weight:bold;color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">None</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span style=\"font-weight:bold;color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">None</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span style=\"font-weight:bold;color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">None</span><span class=\"ansi-yellow-bg\">)</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">dtype</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-bg\">jnp</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">bool_</span><span class=\"ansi-yellow-bg\">)</span>, k<span style=\"color:rgb(98,98,98)\">=</span><span style=\"color:rgb(98,98,98)\">0</span>)\n<span class=\"ansi-green-fg ansi-bold\">     24</span> <span style=\"font-style:italic;color:rgb(95,135,135)\">### END CODE HERE ###</span>\n<span class=\"ansi-green-fg ansi-bold\">     26</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">return</span> DotProductAttention(q, k, v, mask)\n\nFile <span class=\"ansi-green-fg\">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:6183</span>, in <span class=\"ansi-cyan-fg\">ones</span><span class=\"ansi-blue-fg\">(shape, dtype, device)</span>\n<span class=\"ansi-green-fg ansi-bold\">   6181</span>   <span style=\"font-weight:bold;color:rgb(0,135,0)\">raise</span> <span style=\"font-weight:bold;color:rgb(215,95,95)\">TypeError</span>(<span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">expected sequence object with len &gt;= 0 or a single integer</span><span style=\"color:rgb(175,0,0)\">\"</span>)\n<span class=\"ansi-green-fg ansi-bold\">   6182</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">if</span> (m <span style=\"color:rgb(98,98,98)\">:=</span> _check_forgot_shape_tuple(<span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">ones</span><span style=\"color:rgb(175,0,0)\">\"</span>, shape, dtype)): <span style=\"font-weight:bold;color:rgb(0,135,0)\">raise</span> <span style=\"font-weight:bold;color:rgb(215,95,95)\">TypeError</span>(m)\n<span class=\"ansi-green-fg\">-&gt; 6183</span> shape <span style=\"color:rgb(98,98,98)\">=</span> <span class=\"ansi-yellow-bg\">canonicalize_shape</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">shape</span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">   6184</span> dtypes<span style=\"color:rgb(98,98,98)\">.</span>check_user_dtype_supported(dtype, <span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">ones</span><span style=\"color:rgb(175,0,0)\">\"</span>)\n<span class=\"ansi-green-fg ansi-bold\">   6185</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">return</span> lax<span style=\"color:rgb(98,98,98)\">.</span>full(shape, <span style=\"color:rgb(98,98,98)\">1</span>, _jnp_dtype(dtype), sharding<span style=\"color:rgb(98,98,98)\">=</span>_normalize_to_sharding(device))\n\nFile <span class=\"ansi-green-fg\">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:102</span>, in <span class=\"ansi-cyan-fg\">canonicalize_shape</span><span class=\"ansi-blue-fg\">(shape, context)</span>\n<span class=\"ansi-green-fg ansi-bold\">    100</span>   <span style=\"font-weight:bold;color:rgb(0,135,0)\">return</span> core<span style=\"color:rgb(98,98,98)\">.</span>canonicalize_shape((shape,), context)\n<span class=\"ansi-green-fg ansi-bold\">    101</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">else</span>:\n<span class=\"ansi-green-fg\">--&gt; 102</span>   <span style=\"font-weight:bold;color:rgb(0,135,0)\">return</span> <span class=\"ansi-yellow-bg\">core</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">canonicalize_shape</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">shape</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">context</span><span class=\"ansi-yellow-bg\">)</span>\n\nFile <span class=\"ansi-green-fg\">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/jax/_src/core.py:1643</span>, in <span class=\"ansi-cyan-fg\">canonicalize_shape</span><span class=\"ansi-blue-fg\">(shape, context)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1641</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">except</span> <span style=\"font-weight:bold;color:rgb(215,95,95)\">TypeError</span>:\n<span class=\"ansi-green-fg ansi-bold\">   1642</span>   <span style=\"font-weight:bold;color:rgb(0,135,0)\">pass</span>\n<span class=\"ansi-green-fg\">-&gt; 1643</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">raise</span> _invalid_shape_error(shape, context)\n\n<span class=\"ansi-red-fg\">TypeError</span>: Shapes must be 1D sequences of concrete values of integer type, got (None, None, None).</pre>\n```\n:::\n\n:::\n:::\n\n\n**Expected Output:**\n```CPP\nDeviceArray([[[0.        , 1.        , 0.        ],\n              [0.8496746 , 0.15032543, 0.8496746 ]]], dtype=float32)\n```\n\n<span style='color:blue'> compute_attention_output </span>: Undoes compute_attention_heads by splitting first (vertical) dimension and stacking in the last (depth) dimension (batch_size, seqlen, n_heads $\\times$ d_head). These operations concatenate (stack/merge) the heads. \n\n::: {#3397ed64 .cell execution_count=24}\n``` {.python .cell-code}\n# UNQ_C4\n# GRADED FUNCTION: compute_attention_output_closure\ndef compute_attention_output_closure(n_heads, d_head):\n    \"\"\" Function that simulates environment inside CausalAttention function.\n    Args:\n        d_head (int):  dimensionality of heads.\n        n_heads (int): number of attention heads.\n    Returns:\n        function: compute_attention_output function\n    \"\"\"\n    \n    def compute_attention_output(x):\n        \"\"\" Compute the attention output.\n        Args:\n            x (jax.interpreters.xla.DeviceArray): tensor with shape (batch_size X n_heads, seqlen, d_head).\n        Returns:\n            jax.interpreters.xla.DeviceArray: reshaped tensor with shape (batch_size, seqlen, n_heads X d_head).\n        \"\"\"\n        ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n        \n        # Length of the sequence\n        # Should be size of x's first dimension without counting the batch dim\n        seqlen = None\n        # Reshape x using jnp.reshape() to shape (batch_size, n_heads, seqlen, d_head)\n        x = None\n        # Transpose x using jnp.transpose() to shape (batch_size, seqlen, n_heads, d_head)\n        x = None\n        \n        ### END CODE HERE ###\n        \n        # Reshape to allow to concatenate the heads\n        return jnp.reshape(x, (-1, seqlen, n_heads * d_head))\n    \n    return compute_attention_output\n```\n:::\n\n\n::: {#8055ec26 .cell execution_count=25}\n``` {.python .cell-code}\ndisplay_tensor(result_cah, \"input tensor\")\nresult_cao = compute_attention_output_closure(2,3)(result_cah)\ndisplay_tensor(result_cao, \"output tensor\")\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[25], line 1</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span> display_tensor(<span class=\"ansi-yellow-bg\">result_cah</span>, <span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">input tensor</span><span style=\"color:rgb(175,0,0)\">\"</span>)\n<span class=\"ansi-green-fg ansi-bold\">      2</span> result_cao <span style=\"color:rgb(98,98,98)\">=</span> compute_attention_output_closure(<span style=\"color:rgb(98,98,98)\">2</span>,<span style=\"color:rgb(98,98,98)\">3</span>)(result_cah)\n<span class=\"ansi-green-fg ansi-bold\">      3</span> display_tensor(result_cao, <span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">output tensor</span><span style=\"color:rgb(175,0,0)\">\"</span>)\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'result_cah' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n**Expected Output:**\n```CPP\ninput tensor shape: (6, 2, 3)\n\n[[[1 0 0]\n  [0 1 0]]\n\n [[1 0 0]\n  [0 1 0]]\n\n [[1 0 0]\n  [0 1 0]]\n\n [[1 0 0]\n  [0 1 0]]\n\n [[1 0 0]\n  [0 1 0]]\n\n [[1 0 0]\n  [0 1 0]]]\n\noutput tensor shape: (3, 2, 6)\n\n[[[1 0 0 1 0 0]\n  [0 1 0 0 1 0]]\n\n [[1 0 0 1 0 0]\n  [0 1 0 0 1 0]]\n\n [[1 0 0 1 0 0]\n  [0 1 0 0 1 0]]]\n```\n\n### Causal Attention Function\n\nNow it is time for you to put everything together within the `CausalAttention` or Masked multi-head attention function:\n\n<img src = \"masked-attention.png\"> \n\n**Instructions:** Implement the causal attention.\nYour model returns the causal attention through a $tl.Serial$ with the following:\n\n- <span style='color:blue'> [tl.Branch](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Branch) </span>: consisting of 3 [tl.Dense(d_feature), ComputeAttentionHeads] to account for the queries, keys, and values.\n- <span style='color:blue'> [tl.Fn](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn)</span>: Takes in dot_product_self_attention function and uses it to compute the dot product using $Q$, $K$, $V$.\n- <span style='color:blue'> [tl.Fn](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn)</span>: Takes in restack_attention_heads to allow for parallel computing.\n- <span style='color:blue'> [tl.Dense](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense)</span>: Final Dense layer, with dimension `d_feature`.\n\nRemember that in order for trax to properly handle the functions you just defined, they need to be added as layers using the [`tl.Fn()`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn) function. \n\n::: {#8e2f157f .cell execution_count=26}\n``` {.python .cell-code}\n# UNQ_C5\n# GRADED FUNCTION: CausalAttention\ndef CausalAttention(d_feature, \n                    n_heads, \n                    compute_attention_heads_closure=compute_attention_heads_closure,\n                    dot_product_self_attention=dot_product_self_attention,\n                    compute_attention_output_closure=compute_attention_output_closure,\n                    mode='train'):\n    \"\"\"Transformer-style multi-headed causal attention.\n\n    Args:\n        d_feature (int):  dimensionality of feature embedding.\n        n_heads (int): number of attention heads.\n        compute_attention_heads_closure (function): Closure around compute_attention heads.\n        dot_product_self_attention (function): dot_product_self_attention function. \n        compute_attention_output_closure (function): Closure around compute_attention_output. \n        mode (str): 'train' or 'eval'.\n\n    Returns:\n        trax.layers.combinators.Serial: Multi-headed self-attention model.\n    \"\"\"\n    \n    assert d_feature % n_heads == 0\n    d_head = d_feature // n_heads\n\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    \n    # HINT: The second argument to tl.Fn() is an uncalled function (without the parentheses)\n    # Since you are dealing with closures you might need to call the outer \n    # function with the correct parameters to get the actual uncalled function.\n    ComputeAttentionHeads = tl.Fn('AttnHeads', None, n_out=1)\n        \n\n    return tl.Serial(\n        tl.Branch( # creates three towers for one input, takes activations and creates queries keys and values\n            [None, None], # queries\n            [None, None], # keys\n            [None, None], # values\n        ),\n        \n        tl.Fn('DotProductAttn', None, n_out=1), # takes QKV\n        # HINT: The second argument to tl.Fn() is an uncalled function\n        # Since you are dealing with closures you might need to call the outer \n        # function with the correct parameters to get the actual uncalled function.\n        tl.Fn('AttnOutput', None, n_out=1), # to allow for parallel\n        None # Final dense layer\n    )\n\n    ### END CODE HERE ###\n```\n:::\n\n\n::: {#44126b88 .cell execution_count=27}\n``` {.python .cell-code}\n# Take a look at the causal attention model\nprint(CausalAttention(d_feature=512, n_heads=8))\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">TypeError</span>                                 Traceback (most recent call last)\nFile <span class=\"ansi-green-fg\">/usr/lib/python3.10/inspect.py:1277</span>, in <span class=\"ansi-cyan-fg\">getfullargspec</span><span class=\"ansi-blue-fg\">(func)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1260</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">try</span>:\n<span class=\"ansi-green-fg ansi-bold\">   1261</span>     <span style=\"font-style:italic;color:rgb(95,135,135)\"># Re: `skip_bound_arg=False`</span>\n<span class=\"ansi-green-fg ansi-bold\">   1262</span>     <span style=\"font-style:italic;color:rgb(95,135,135)\">#</span>\n<span class=\"ansi-green-fg\">   (...)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1274</span>     <span style=\"font-style:italic;color:rgb(95,135,135)\"># getfullargspec() historically ignored __wrapped__ attributes,</span>\n<span class=\"ansi-green-fg ansi-bold\">   1275</span>     <span style=\"font-style:italic;color:rgb(95,135,135)\"># so we ensure that remains the case in 3.3+</span>\n<span class=\"ansi-green-fg\">-&gt; 1277</span>     sig <span style=\"color:rgb(98,98,98)\">=</span> <span class=\"ansi-yellow-bg\">_signature_from_callable</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">func</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">   1278</span> <span class=\"ansi-yellow-bg\">                                   </span><span class=\"ansi-yellow-bg\">follow_wrapper_chains</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span style=\"font-weight:bold;color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">False</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">   1279</span> <span class=\"ansi-yellow-bg\">                                   </span><span class=\"ansi-yellow-bg\">skip_bound_arg</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span style=\"font-weight:bold;color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">False</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">   1280</span> <span class=\"ansi-yellow-bg\">                                   </span><span class=\"ansi-yellow-bg\">sigcls</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-bg\">Signature</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">   1281</span> <span class=\"ansi-yellow-bg\">                                   </span><span class=\"ansi-yellow-bg\">eval_str</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span style=\"font-weight:bold;color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">False</span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1282</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">except</span> <span style=\"font-weight:bold;color:rgb(215,95,95)\">Exception</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">as</span> ex:\n<span class=\"ansi-green-fg ansi-bold\">   1283</span>     <span style=\"font-style:italic;color:rgb(95,135,135)\"># Most of the times 'signature' will raise ValueError.</span>\n<span class=\"ansi-green-fg ansi-bold\">   1284</span>     <span style=\"font-style:italic;color:rgb(95,135,135)\"># But, it can also raise AttributeError, and, maybe something</span>\n<span class=\"ansi-green-fg ansi-bold\">   1285</span>     <span style=\"font-style:italic;color:rgb(95,135,135)\"># else. So to be fully backwards compatible, we catch all</span>\n<span class=\"ansi-green-fg ansi-bold\">   1286</span>     <span style=\"font-style:italic;color:rgb(95,135,135)\"># possible exceptions here, and reraise a TypeError.</span>\n\nFile <span class=\"ansi-green-fg\">/usr/lib/python3.10/inspect.py:2396</span>, in <span class=\"ansi-cyan-fg\">_signature_from_callable</span><span class=\"ansi-blue-fg\">(obj, follow_wrapper_chains, skip_bound_arg, globals, locals, eval_str, sigcls)</span>\n<span class=\"ansi-green-fg ansi-bold\">   2395</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">if</span> <span style=\"font-weight:bold;color:rgb(175,0,255)\">not</span> <span style=\"color:rgb(0,135,0)\">callable</span>(obj):\n<span class=\"ansi-green-fg\">-&gt; 2396</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">raise</span> <span style=\"font-weight:bold;color:rgb(215,95,95)\">TypeError</span>(<span style=\"color:rgb(175,0,0)\">'</span><span style=\"font-weight:bold;color:rgb(175,95,135)\">{!r}</span><span style=\"color:rgb(175,0,0)\"> is not a callable object</span><span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(98,98,98)\">.</span>format(obj))\n<span class=\"ansi-green-fg ansi-bold\">   2398</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">if</span> <span style=\"color:rgb(0,135,0)\">isinstance</span>(obj, types<span style=\"color:rgb(98,98,98)\">.</span>MethodType):\n<span class=\"ansi-green-fg ansi-bold\">   2399</span>     <span style=\"font-style:italic;color:rgb(95,135,135)\"># In this case we skip the first parameter of the underlying</span>\n<span class=\"ansi-green-fg ansi-bold\">   2400</span>     <span style=\"font-style:italic;color:rgb(95,135,135)\"># function (usually `self` or `cls`).</span>\n\n<span class=\"ansi-red-fg\">TypeError</span>: None is not a callable object\n\nThe above exception was the direct cause of the following exception:\n\n<span class=\"ansi-red-fg\">TypeError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[27], line 2</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Take a look at the causal attention model</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span> <span style=\"color:rgb(0,135,0)\">print</span>(<span class=\"ansi-yellow-bg\">CausalAttention</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">d_feature</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">512</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">n_heads</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">8</span><span class=\"ansi-yellow-bg\">)</span>)\n\nCell <span class=\"ansi-green-fg\">In[26], line 31</span>, in <span class=\"ansi-cyan-fg\">CausalAttention</span><span class=\"ansi-blue-fg\">(d_feature, n_heads, compute_attention_heads_closure, dot_product_self_attention, compute_attention_output_closure, mode)</span>\n<span class=\"ansi-green-fg ansi-bold\">     24</span> d_head <span style=\"color:rgb(98,98,98)\">=</span> d_feature <span style=\"color:rgb(98,98,98)\">/</span><span style=\"color:rgb(98,98,98)\">/</span> n_heads\n<span class=\"ansi-green-fg ansi-bold\">     26</span> <span style=\"font-style:italic;color:rgb(95,135,135)\">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span>\n<span class=\"ansi-green-fg ansi-bold\">     27</span> \n<span class=\"ansi-green-fg ansi-bold\">     28</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># HINT: The second argument to tl.Fn() is an uncalled function (without the parentheses)</span>\n<span class=\"ansi-green-fg ansi-bold\">     29</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Since you are dealing with closures you might need to call the outer </span>\n<span class=\"ansi-green-fg ansi-bold\">     30</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># function with the correct parameters to get the actual uncalled function.</span>\n<span class=\"ansi-green-fg\">---&gt; 31</span> ComputeAttentionHeads <span style=\"color:rgb(98,98,98)\">=</span> <span class=\"ansi-yellow-bg\">tl</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">Fn</span><span class=\"ansi-yellow-bg\">(</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">'</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">AttnHeads</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">'</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span style=\"font-weight:bold;color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">None</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">n_out</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">1</span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">     34</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">return</span> tl<span style=\"color:rgb(98,98,98)\">.</span>Serial(\n<span class=\"ansi-green-fg ansi-bold\">     35</span>     tl<span style=\"color:rgb(98,98,98)\">.</span>Branch( <span style=\"font-style:italic;color:rgb(95,135,135)\"># creates three towers for one input, takes activations and creates queries keys and values</span>\n<span class=\"ansi-green-fg ansi-bold\">     36</span>         [<span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>, <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>], <span style=\"font-style:italic;color:rgb(95,135,135)\"># queries</span>\n<span class=\"ansi-green-fg\">   (...)</span>\n<span class=\"ansi-green-fg ansi-bold\">     46</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Final dense layer</span>\n<span class=\"ansi-green-fg ansi-bold\">     47</span> )\n\nFile <span class=\"ansi-green-fg\">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/base.py:773</span>, in <span class=\"ansi-cyan-fg\">Fn</span><span class=\"ansi-blue-fg\">(name, f, n_out)</span>\n<span class=\"ansi-green-fg ansi-bold\">    748</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">def</span><span style=\"color:rgb(188,188,188)\"> </span><span style=\"color:rgb(0,0,255)\">Fn</span>(name, f, n_out<span style=\"color:rgb(98,98,98)\">=</span><span style=\"color:rgb(98,98,98)\">1</span>):  <span style=\"font-style:italic;color:rgb(95,135,135)\"># pylint: disable=invalid-name</span>\n<span class=\"ansi-green-fg ansi-bold\">    749</span> <span style=\"color:rgb(188,188,188)\">  </span><span style=\"font-style:italic;color:rgb(175,0,0)\">\"\"\"Returns a layer with no weights that applies the function `f`.</span>\n<span class=\"ansi-green-fg ansi-bold\">    750</span> \n<span class=\"ansi-green-fg ansi-bold\">    751</span> <span style=\"font-style:italic;color:rgb(175,0,0)\">  `f` can take and return any number of arguments, and takes only positional</span>\n<span class=\"ansi-green-fg\">   (...)</span>\n<span class=\"ansi-green-fg ansi-bold\">    771</span> <span style=\"font-style:italic;color:rgb(175,0,0)\">    Layer executing the function `f`.</span>\n<span class=\"ansi-green-fg ansi-bold\">    772</span> <span style=\"font-style:italic;color:rgb(175,0,0)\">  \"\"\"</span>\n<span class=\"ansi-green-fg\">--&gt; 773</span>   argspec <span style=\"color:rgb(98,98,98)\">=</span> <span class=\"ansi-yellow-bg\">inspect</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">getfullargspec</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">f</span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">    774</span>   <span style=\"font-weight:bold;color:rgb(0,135,0)\">if</span> argspec<span style=\"color:rgb(98,98,98)\">.</span>defaults <span style=\"font-weight:bold;color:rgb(175,0,255)\">is</span> <span style=\"font-weight:bold;color:rgb(175,0,255)\">not</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>:\n<span class=\"ansi-green-fg ansi-bold\">    775</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">raise</span> <span style=\"font-weight:bold;color:rgb(215,95,95)\">ValueError</span>(<span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">Function has default arguments (not allowed).</span><span style=\"color:rgb(175,0,0)\">'</span>)\n\nFile <span class=\"ansi-green-fg\">/usr/lib/python3.10/inspect.py:1287</span>, in <span class=\"ansi-cyan-fg\">getfullargspec</span><span class=\"ansi-blue-fg\">(func)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1277</span>     sig <span style=\"color:rgb(98,98,98)\">=</span> _signature_from_callable(func,\n<span class=\"ansi-green-fg ansi-bold\">   1278</span>                                    follow_wrapper_chains<span style=\"color:rgb(98,98,98)\">=</span><span style=\"font-weight:bold;color:rgb(0,135,0)\">False</span>,\n<span class=\"ansi-green-fg ansi-bold\">   1279</span>                                    skip_bound_arg<span style=\"color:rgb(98,98,98)\">=</span><span style=\"font-weight:bold;color:rgb(0,135,0)\">False</span>,\n<span class=\"ansi-green-fg ansi-bold\">   1280</span>                                    sigcls<span style=\"color:rgb(98,98,98)\">=</span>Signature,\n<span class=\"ansi-green-fg ansi-bold\">   1281</span>                                    eval_str<span style=\"color:rgb(98,98,98)\">=</span><span style=\"font-weight:bold;color:rgb(0,135,0)\">False</span>)\n<span class=\"ansi-green-fg ansi-bold\">   1282</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">except</span> <span style=\"font-weight:bold;color:rgb(215,95,95)\">Exception</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">as</span> ex:\n<span class=\"ansi-green-fg ansi-bold\">   1283</span>     <span style=\"font-style:italic;color:rgb(95,135,135)\"># Most of the times 'signature' will raise ValueError.</span>\n<span class=\"ansi-green-fg ansi-bold\">   1284</span>     <span style=\"font-style:italic;color:rgb(95,135,135)\"># But, it can also raise AttributeError, and, maybe something</span>\n<span class=\"ansi-green-fg ansi-bold\">   1285</span>     <span style=\"font-style:italic;color:rgb(95,135,135)\"># else. So to be fully backwards compatible, we catch all</span>\n<span class=\"ansi-green-fg ansi-bold\">   1286</span>     <span style=\"font-style:italic;color:rgb(95,135,135)\"># possible exceptions here, and reraise a TypeError.</span>\n<span class=\"ansi-green-fg\">-&gt; 1287</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">raise</span> <span style=\"font-weight:bold;color:rgb(215,95,95)\">TypeError</span>(<span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">unsupported callable</span><span style=\"color:rgb(175,0,0)\">'</span>) <span style=\"font-weight:bold;color:rgb(0,135,0)\">from</span><span style=\"color:rgb(188,188,188)\"> </span><span style=\"font-weight:bold;color:rgb(0,0,255)\">ex</span>\n<span class=\"ansi-green-fg ansi-bold\">   1289</span> args <span style=\"color:rgb(98,98,98)\">=</span> []\n<span class=\"ansi-green-fg ansi-bold\">   1290</span> varargs <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>\n\n<span class=\"ansi-red-fg\">TypeError</span>: unsupported callable</pre>\n```\n:::\n\n:::\n:::\n\n\n**Expected Output:**\n```CPP\nSerial[\n  Branch_out3[\n    [Dense_512, AttnHeads]\n    [Dense_512, AttnHeads]\n    [Dense_512, AttnHeads]\n  ]\n  DotProductAttn_in3\n  AttnOutput\n  Dense_512\n]\n```\n\n<a name='2.3'></a>\n\n## 2.3 Transformer decoder block\n\nNow that you have implemented the causal part of the transformer, you will implement the transformer decoder block. Concretely you will be implementing this image now.\n\n<img src = \"transformer_decoder_1.png\" style = \"height:300px\"> \n\nTo implement this function, you will have to call the `CausalAttention` or Masked multi-head attention function you implemented above. You will have to add a feedforward which consists of: \n\n- <span style='color:blue'> [tl.LayerNorm](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.normalization.LayerNorm) </span>: used to layer normalize\n- <span style='color:blue'> [tl.Dense](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense) </span>: the dense layer\n- <span style='color:blue'> [ff_activation](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.activation_fns.Relu) </span>: feed forward activation (we use ReLu) here.\n- <span style='color:blue'> [tl.Dropout](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dropout) </span>: dropout layer\n- <span style='color:blue'> [tl.Dense](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense) </span>: dense layer\n- <span style='color:blue'> [tl.Dropout](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dropout) </span>: dropout layer\n\nFinally once you implement the feedforward, you can go ahead and implement the entire block using: \n\n- <span style='color:blue'> [tl.Residual](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Residual) </span>: takes in the tl.LayerNorm(), causal attention block, tl.dropout. \n\n- <span style='color:blue'> [tl.Residual](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Residual) </span>: takes in the feedforward block you will implement. \n\n<a name='ex03'></a>\n### Exercise 03\n**Instructions:** Implement the transformer decoder block. Good luck!\n\n::: {#aeea4633 .cell execution_count=28}\n``` {.python .cell-code}\n# UNQ_C6\n# GRADED FUNCTION: DecoderBlock\ndef DecoderBlock(d_model, d_ff, n_heads,\n                 dropout, mode, ff_activation):\n    \"\"\"Returns a list of layers that implements a Transformer decoder block.\n\n    The input is an activation tensor.\n\n    Args:\n        d_model (int):  depth of embedding.\n        d_ff (int): depth of feed-forward layer.\n        n_heads (int): number of attention heads.\n        dropout (float): dropout rate (how much to drop out).\n        mode (str): 'train' or 'eval'.\n        ff_activation (function): the non-linearity in feed-forward layer.\n\n    Returns:\n        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\n    \"\"\"\n    \n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    \n    # Create masked multi-head attention block using CausalAttention function\n    causal_attention = CausalAttention( \n                        None,\n                        n_heads=None,\n                        mode=None\n                        )\n\n    # Create feed-forward block (list) with two dense layers with dropout and input normalized\n    feed_forward = [ \n        # Normalize layer inputs\n        None,\n        # Add first feed forward (dense) layer (don't forget to set the correct value for n_units)\n        None,\n        # Add activation function passed in as a parameter (you need to call it!)\n        None, # Generally ReLU\n        # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\n        None,\n        # Add second feed forward layer (don't forget to set the correct value for n_units)\n        None,\n        # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\n        None\n    ]\n\n    # Add list of two Residual blocks: the attention with normalization and dropout and feed-forward blocks\n    return [\n      tl.Residual(\n          # Normalize layer input\n          None,\n          # Add causal attention block previously defined (without parentheses)\n          None,\n          # Add dropout with rate and mode specified\n          None\n        ),\n      tl.Residual(\n          # Add feed forward block (without parentheses)\n          None\n        ),\n      ]\n    ### END CODE HERE ###\n```\n:::\n\n\n::: {#500449f8 .cell execution_count=29}\n``` {.python .cell-code}\n# Take a look at the decoder block\nprint(DecoderBlock(d_model=512, d_ff=2048, n_heads=8, dropout=0.1, mode='train', ff_activation=tl.Relu))\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">TypeError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[29], line 2</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Take a look at the decoder block</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span> <span style=\"color:rgb(0,135,0)\">print</span>(<span class=\"ansi-yellow-bg\">DecoderBlock</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">d_model</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">512</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">d_ff</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">2048</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">n_heads</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">8</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">dropout</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">0.1</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">mode</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">'</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">train</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">'</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">ff_activation</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-bg\">tl</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">Relu</span><span class=\"ansi-yellow-bg\">)</span>)\n\nCell <span class=\"ansi-green-fg\">In[28], line 24</span>, in <span class=\"ansi-cyan-fg\">DecoderBlock</span><span class=\"ansi-blue-fg\">(d_model, d_ff, n_heads, dropout, mode, ff_activation)</span>\n<span class=\"ansi-green-fg ansi-bold\">      5</span> <span style=\"font-style:italic;color:rgb(175,0,0)\">\"\"\"Returns a list of layers that implements a Transformer decoder block.</span>\n<span class=\"ansi-green-fg ansi-bold\">      6</span> \n<span class=\"ansi-green-fg ansi-bold\">      7</span> <span style=\"font-style:italic;color:rgb(175,0,0)\">The input is an activation tensor.</span>\n<span class=\"ansi-green-fg\">   (...)</span>\n<span class=\"ansi-green-fg ansi-bold\">     18</span> <span style=\"font-style:italic;color:rgb(175,0,0)\">    list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.</span>\n<span class=\"ansi-green-fg ansi-bold\">     19</span> <span style=\"font-style:italic;color:rgb(175,0,0)\">\"\"\"</span>\n<span class=\"ansi-green-fg ansi-bold\">     21</span> <span style=\"font-style:italic;color:rgb(95,135,135)\">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span>\n<span class=\"ansi-green-fg ansi-bold\">     22</span> \n<span class=\"ansi-green-fg ansi-bold\">     23</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Create masked multi-head attention block using CausalAttention function</span>\n<span class=\"ansi-green-fg\">---&gt; 24</span> causal_attention <span style=\"color:rgb(98,98,98)\">=</span> <span class=\"ansi-yellow-bg\">CausalAttention</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\"> </span>\n<span class=\"ansi-green-fg ansi-bold\">     25</span> <span class=\"ansi-yellow-bg\">                    </span><span style=\"font-weight:bold;color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">None</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">     26</span> <span class=\"ansi-yellow-bg\">                    </span><span class=\"ansi-yellow-bg\">n_heads</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span style=\"font-weight:bold;color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">None</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">     27</span> <span class=\"ansi-yellow-bg\">                    </span><span class=\"ansi-yellow-bg\">mode</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span style=\"font-weight:bold;color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">None</span>\n<span class=\"ansi-green-fg ansi-bold\">     28</span> <span class=\"ansi-yellow-bg\">                    </span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">     30</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Create feed-forward block (list) with two dense layers with dropout and input normalized</span>\n<span class=\"ansi-green-fg ansi-bold\">     31</span> feed_forward <span style=\"color:rgb(98,98,98)\">=</span> [ \n<span class=\"ansi-green-fg ansi-bold\">     32</span>     <span style=\"font-style:italic;color:rgb(95,135,135)\"># Normalize layer inputs</span>\n<span class=\"ansi-green-fg ansi-bold\">     33</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>,\n<span class=\"ansi-green-fg\">   (...)</span>\n<span class=\"ansi-green-fg ansi-bold\">     43</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>\n<span class=\"ansi-green-fg ansi-bold\">     44</span> ]\n\nCell <span class=\"ansi-green-fg\">In[26], line 23</span>, in <span class=\"ansi-cyan-fg\">CausalAttention</span><span class=\"ansi-blue-fg\">(d_feature, n_heads, compute_attention_heads_closure, dot_product_self_attention, compute_attention_output_closure, mode)</span>\n<span class=\"ansi-green-fg ansi-bold\">      3</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">def</span><span style=\"color:rgb(188,188,188)\"> </span><span style=\"color:rgb(0,0,255)\">CausalAttention</span>(d_feature, \n<span class=\"ansi-green-fg ansi-bold\">      4</span>                     n_heads, \n<span class=\"ansi-green-fg ansi-bold\">      5</span>                     compute_attention_heads_closure<span style=\"color:rgb(98,98,98)\">=</span>compute_attention_heads_closure,\n<span class=\"ansi-green-fg ansi-bold\">      6</span>                     dot_product_self_attention<span style=\"color:rgb(98,98,98)\">=</span>dot_product_self_attention,\n<span class=\"ansi-green-fg ansi-bold\">      7</span>                     compute_attention_output_closure<span style=\"color:rgb(98,98,98)\">=</span>compute_attention_output_closure,\n<span class=\"ansi-green-fg ansi-bold\">      8</span>                     mode<span style=\"color:rgb(98,98,98)\">=</span><span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">train</span><span style=\"color:rgb(175,0,0)\">'</span>):\n<span class=\"ansi-green-fg ansi-bold\">      9</span> <span style=\"color:rgb(188,188,188)\">    </span><span style=\"font-style:italic;color:rgb(175,0,0)\">\"\"\"Transformer-style multi-headed causal attention.</span>\n<span class=\"ansi-green-fg ansi-bold\">     10</span> \n<span class=\"ansi-green-fg ansi-bold\">     11</span> <span style=\"font-style:italic;color:rgb(175,0,0)\">    Args:</span>\n<span class=\"ansi-green-fg\">   (...)</span>\n<span class=\"ansi-green-fg ansi-bold\">     20</span> <span style=\"font-style:italic;color:rgb(175,0,0)\">        trax.layers.combinators.Serial: Multi-headed self-attention model.</span>\n<span class=\"ansi-green-fg ansi-bold\">     21</span> <span style=\"font-style:italic;color:rgb(175,0,0)\">    \"\"\"</span>\n<span class=\"ansi-green-fg\">---&gt; 23</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">assert</span> <span class=\"ansi-yellow-bg\">d_feature</span><span class=\"ansi-yellow-bg\"> </span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">%</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">n_heads</span> <span style=\"color:rgb(98,98,98)\">==</span> <span style=\"color:rgb(98,98,98)\">0</span>\n<span class=\"ansi-green-fg ansi-bold\">     24</span>     d_head <span style=\"color:rgb(98,98,98)\">=</span> d_feature <span style=\"color:rgb(98,98,98)\">/</span><span style=\"color:rgb(98,98,98)\">/</span> n_heads\n<span class=\"ansi-green-fg ansi-bold\">     26</span>     <span style=\"font-style:italic;color:rgb(95,135,135)\">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span>\n<span class=\"ansi-green-fg ansi-bold\">     27</span>     \n<span class=\"ansi-green-fg ansi-bold\">     28</span>     <span style=\"font-style:italic;color:rgb(95,135,135)\"># HINT: The second argument to tl.Fn() is an uncalled function (without the parentheses)</span>\n<span class=\"ansi-green-fg ansi-bold\">     29</span>     <span style=\"font-style:italic;color:rgb(95,135,135)\"># Since you are dealing with closures you might need to call the outer </span>\n<span class=\"ansi-green-fg ansi-bold\">     30</span>     <span style=\"font-style:italic;color:rgb(95,135,135)\"># function with the correct parameters to get the actual uncalled function.</span>\n\n<span class=\"ansi-red-fg\">TypeError</span>: unsupported operand type(s) for %: 'NoneType' and 'NoneType'</pre>\n```\n:::\n\n:::\n:::\n\n\n**Expected Output:**\n```CPP\n[Serial[\n  Branch_out2[\n    None\n    Serial[\n      LayerNorm\n      Serial[\n        Branch_out3[\n          [Dense_512, AttnHeads]\n          [Dense_512, AttnHeads]\n          [Dense_512, AttnHeads]\n        ]\n        DotProductAttn_in3\n        AttnOutput\n        Dense_512\n      ]\n      Dropout\n    ]\n  ]\n  Add_in2\n], Serial[\n  Branch_out2[\n    None\n    Serial[\n      LayerNorm\n      Dense_2048\n      Relu\n      Dropout\n      Dense_512\n      Dropout\n    ]\n  ]\n  Add_in2\n]]\n```\n\n<a name='2.4'></a>\n## 2.4 Transformer Language Model\n\nYou will now bring it all together. In this part you will use all the subcomponents you previously built to make the final model. Concretely, here is the image you will be implementing. \n<img src = \"transformer_decoder.png\" style = \"height:400px\">\n\n    \n<a name='ex04'></a>\n### Exercise 04\n**Instructions:** Previously you coded the decoder block. Now you will code the transformer language model. Here is what you will need. \n\n- <span style=\"color:blue\"> positional_enconder </span>- a list containing the following layers:\n    - <span style=\"color:blue\"> [tl.Embedding](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding)\n    - <span style=\"color:blue\"> [tl.Dropout](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dropout)\n    - <span style=\"color:blue\"> [tl.PositionalEncoding](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.PositionalEncoding)\n\n- A list of `n_layers` <span style=\"color:blue\"> decoder blocks</span>.\n- <span style=\"color:blue\"> [tl.Serial](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Serial): </span> takes in the following layers or lists of layers:\n    - <span style=\"color:blue\"> [tl.ShiftRight](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.ShiftRight): </span>: shift the tensor to the right by padding on axis 1.\n    - <span style=\"color:blue\"> positional_encoder </span>: encodes the text positions.\n    - <span style=\"color:blue\"> decoder_blocks </span>: the ones you created.\n    - <span style=\"color:blue\"> [tl.LayerNorm](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.normalization.LayerNorm) </span>: a layer norm.\n    - <span style=\"color:blue\"> [tl.Dense](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense) </span>: takes in the vocab_size.\n    - <span style=\"color:blue\"> [tl.LogSoftmax](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.LogSoftmax) </span>: to predict.\n    \nGo go go!! You can do it :)\n\n::: {#0432484e .cell execution_count=30}\n``` {.python .cell-code}\n# UNQ_C7\n# GRADED FUNCTION: TransformerLM\ndef TransformerLM(vocab_size=33300,\n                  d_model=512,\n                  d_ff=2048,\n                  n_layers=6,\n                  n_heads=8,\n                  dropout=0.1,\n                  max_len=4096,\n                  mode='train',\n                  ff_activation=tl.Relu):\n    \"\"\"Returns a Transformer language model.\n\n    The input to the model is a tensor of tokens. (This model uses only the\n    decoder part of the overall Transformer.)\n\n    Args:\n        vocab_size (int): vocab size.\n        d_model (int):  depth of embedding.\n        d_ff (int): depth of feed-forward layer.\n        n_layers (int): number of decoder layers.\n        n_heads (int): number of attention heads.\n        dropout (float): dropout rate (how much to drop out).\n        max_len (int): maximum symbol length for positional encoding.\n        mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.\n        ff_activation (function): the non-linearity in feed-forward layer.\n\n    Returns:\n        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens\n        to activations over a vocab set.\n    \"\"\"\n    \n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    \n    # Embedding inputs and positional encoder\n    positional_encoder = [ \n        # Add embedding layer of dimension (vocab_size, d_model)\n        None,\n        # Use dropout with rate and mode specified\n        None,\n        # Add positional encoding layer with maximum input length and mode specified\n        None]\n\n    # Create stack (list) of decoder blocks with n_layers with necessary parameters\n    decoder_blocks = [ \n        None for _ in range(None)]\n\n    # Create the complete model as written in the figure\n    return tl.Serial(\n        # Use teacher forcing (feed output of previous step to current step)\n        None, # Specify the mode!\n        # Add positional encoder\n        None,\n        # Add decoder blocks\n        None,\n        # Normalize layer\n        None,\n\n        # Add dense layer of vocab_size (since need to select a word to translate to)\n        # (a.k.a., logits layer. Note: activation already set by ff_activation)\n        None,\n        # Get probabilities with Logsoftmax\n        None\n    )\n\n    ### END CODE HERE ###\n```\n:::\n\n\n::: {#063fa8f4 .cell execution_count=31}\n``` {.python .cell-code}\n# Take a look at the Transformer\nprint(TransformerLM(n_layers=1))\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">TypeError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[31], line 2</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Take a look at the Transformer</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span> <span style=\"color:rgb(0,135,0)\">print</span>(<span class=\"ansi-yellow-bg\">TransformerLM</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">n_layers</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">1</span><span class=\"ansi-yellow-bg\">)</span>)\n\nCell <span class=\"ansi-green-fg\">In[30], line 46</span>, in <span class=\"ansi-cyan-fg\">TransformerLM</span><span class=\"ansi-blue-fg\">(vocab_size, d_model, d_ff, n_layers, n_heads, dropout, max_len, mode, ff_activation)</span>\n<span class=\"ansi-green-fg ansi-bold\">     36</span> positional_encoder <span style=\"color:rgb(98,98,98)\">=</span> [ \n<span class=\"ansi-green-fg ansi-bold\">     37</span>     <span style=\"font-style:italic;color:rgb(95,135,135)\"># Add embedding layer of dimension (vocab_size, d_model)</span>\n<span class=\"ansi-green-fg ansi-bold\">     38</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>,\n<span class=\"ansi-green-fg\">   (...)</span>\n<span class=\"ansi-green-fg ansi-bold\">     41</span>     <span style=\"font-style:italic;color:rgb(95,135,135)\"># Add positional encoding layer with maximum input length and mode specified</span>\n<span class=\"ansi-green-fg ansi-bold\">     42</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>]\n<span class=\"ansi-green-fg ansi-bold\">     44</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Create stack (list) of decoder blocks with n_layers with necessary parameters</span>\n<span class=\"ansi-green-fg ansi-bold\">     45</span> decoder_blocks <span style=\"color:rgb(98,98,98)\">=</span> [ \n<span class=\"ansi-green-fg\">---&gt; 46</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">for</span> _ <span style=\"font-weight:bold;color:rgb(175,0,255)\">in</span> <span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">range</span><span class=\"ansi-yellow-bg\">(</span><span style=\"font-weight:bold;color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">None</span><span class=\"ansi-yellow-bg\">)</span>]\n<span class=\"ansi-green-fg ansi-bold\">     48</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Create the complete model as written in the figure</span>\n<span class=\"ansi-green-fg ansi-bold\">     49</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">return</span> tl<span style=\"color:rgb(98,98,98)\">.</span>Serial(\n<span class=\"ansi-green-fg ansi-bold\">     50</span>     <span style=\"font-style:italic;color:rgb(95,135,135)\"># Use teacher forcing (feed output of previous step to current step)</span>\n<span class=\"ansi-green-fg ansi-bold\">     51</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>, <span style=\"font-style:italic;color:rgb(95,135,135)\"># Specify the mode!</span>\n<span class=\"ansi-green-fg\">   (...)</span>\n<span class=\"ansi-green-fg ansi-bold\">     63</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>\n<span class=\"ansi-green-fg ansi-bold\">     64</span> )\n\n<span class=\"ansi-red-fg\">TypeError</span>: 'NoneType' object cannot be interpreted as an integer</pre>\n```\n:::\n\n:::\n:::\n\n\n**Expected Output:**\n```CPP\nSerial[\n  ShiftRight(1)\n  Embedding_33300_512\n  Dropout\n  PositionalEncoding\n  Serial[\n    Branch_out2[\n      None\n      Serial[\n        LayerNorm\n        Serial[\n          Branch_out3[\n            [Dense_512, AttnHeads]\n            [Dense_512, AttnHeads]\n            [Dense_512, AttnHeads]\n          ]\n          DotProductAttn_in3\n          AttnOutput\n          Dense_512\n        ]\n        Dropout\n      ]\n    ]\n    Add_in2\n  ]\n  Serial[\n    Branch_out2[\n      None\n      Serial[\n        LayerNorm\n        Dense_2048\n        Relu\n        Dropout\n        Dense_512\n        Dropout\n      ]\n    ]\n    Add_in2\n  ]\n  LayerNorm\n  Dense_33300\n  LogSoftmax\n]\n```\n\n<a name='3'></a>\n# Part 3: Training\n\nNow you are going to train your model. As usual, you have to define the cost function, the optimizer, and decide whether you will be training it on a `gpu` or `cpu`. In this case, you will train your model on a cpu for a few steps and we will load in a pre-trained model that you can use to predict with your own words.\n\n<a name='3.1'></a>\n### 3.1 Training the model\n\nYou will now write a function that takes in your model and trains it. To train your model you have to decide how many times you want to iterate over the entire data set. Each iteration is defined as an `epoch`. For each epoch, you have to go over all the data, using your training iterator.\n\n<a name='ex05'></a>\n### Exercise 05\n**Instructions:** Implement the `train_model` program below to train the neural network above. Here is a list of things you should do:\n\n- Create the train task by calling [`trax.supervised.training.TrainTask`](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.TrainTask) and pass in the following: \n    - <span style='color:blue'> labeled_data </span> = train_gen\n    - <span style='color:blue'> loss_fn </span> = [tl.CrossEntropyLoss()](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.CrossEntropyLoss)\n    - <span style='color:blue'> optimizer </span> = [trax.optimizers.Adam(0.01)](https://trax-ml.readthedocs.io/en/latest/trax.optimizers.html#trax.optimizers.adam.Adam)\n    - <span style='color:blue'> lr_schedule </span> = [lr_schedule](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.lr_schedules.warmup_and_rsqrt_decay)\n\n\n- Create the eval task by calling [`trax.supervised.training.EvalTask`](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.EvalTask) and pass in the following: \n    - <span style='color:blue'> labeled_data </span> = eval_gen\n    - <span style='color:blue'> metrics </span> = tl.CrossEntropyLoss() and [tl.Accuracy()](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.Accuracy)\n    \n    \n- Create the training loop by calling [`trax.supervised.Training.Loop`](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.Loop) and pass in the following: \n    - <span style='color:blue'> TransformerLM </span> \n    - <span style='color:blue'> train_task </span> \n    - <span style='color:blue'> eval_task </span> = [eval_task]\n    - <span style='color:blue'> output_dir</span> = output_dir\n    \nYou will be using a cross entropy loss, with Adam optimizer. Please read the [Trax](https://trax-ml.readthedocs.io/en/latest/index.html) documentation to get a full understanding. \n\nThe training loop that this function returns can be runned using the `run()` method by passing in the desired number of steps.\n\n::: {#a39af205 .cell execution_count=32}\n``` {.python .cell-code}\nfrom trax.supervised import training\n\n# UNQ_C8\n# GRADED FUNCTION: train_model\ndef training_loop(TransformerLM, train_gen, eval_gen, output_dir = \"~/model\"):\n    '''\n    Input:\n        TransformerLM (trax.layers.combinators.Serial): The model you are building.\n        train_gen (generator): Training stream of data.\n        eval_gen (generator): Evaluation stream of data.\n        output_dir (str): folder to save your file.\n        \n    Returns:\n        trax.supervised.training.Loop: Training loop.\n    '''\n    output_dir = os.path.expanduser(output_dir)  # trainer is an object\n    lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=1000, max_value=0.01)\n\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    train_task = training.TrainTask( \n      labeled_data=None, # The training generator\n      loss_layer=None, # Loss function \n      optimizer=None, # Optimizer (Don't forget to set LR to 0.01)\n      lr_schedule=None,\n      n_steps_per_checkpoint=10\n    )\n\n    eval_task = training.EvalTask( \n      labeled_data=None, # The evaluation generator\n      metrics=[None, None] # CrossEntropyLoss and Accuracy\n    )\n\n    ### END CODE HERE ###\n\n    loop = training.Loop(TransformerLM(d_model=4,\n                                       d_ff=16,\n                                       n_layers=1,\n                                       n_heads=2,\n                                       mode='train'),\n                         train_task,\n                         eval_tasks=[eval_task],\n                         output_dir=output_dir)\n    \n    return loop\n```\n:::\n\n\nNotice that the model will be trained for only 10 steps. \n\nEven with this constraint the model with the original default arguments took a very long time to finish. Because of this some parameters are changed when defining the model that is fed into the training loop in the function above.\n\n::: {#6c0f86f9 .cell execution_count=33}\n``` {.python .cell-code}\n# Should take around 1.5 minutes\n!rm -f ~/model/model.pkl.gz\nloop = training_loop(TransformerLM, train_batch_stream, eval_batch_stream)\nloop.run(10)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n```\n:::\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[33], line 3</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Should take around 1.5 minutes</span>\n<span class=\"ansi-green-fg ansi-bold\">      2</span> get_ipython()<span style=\"color:rgb(98,98,98)\">.</span>system(<span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">rm -f ~/model/model.pkl.gz</span><span style=\"color:rgb(175,0,0)\">'</span>)\n<span class=\"ansi-green-fg\">----&gt; 3</span> loop <span style=\"color:rgb(98,98,98)\">=</span> training_loop(TransformerLM, <span class=\"ansi-yellow-bg\">train_batch_stream</span>, eval_batch_stream)\n<span class=\"ansi-green-fg ansi-bold\">      4</span> loop<span style=\"color:rgb(98,98,98)\">.</span>run(<span style=\"color:rgb(98,98,98)\">10</span>)\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'train_batch_stream' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n <a name='4'></a>\n # Part 4:  Evaluation  \n\n<a name='4.1'></a>\n### 4.1 Loading in a trained model\n\nIn this part you will evaluate by loading in an almost exact version of the model you coded, but we trained it for you to save you time. Please run the cell below to load in the model.\n\nAs you may have already noticed the model that you trained and the pretrained model share the same overall architecture but they have different values for some of the parameters:\n\n    \n   `Original (pretrained) model: `                                 \n                                       \n    TransformerLM(vocab_size=33300, d_model=512, d_ff=2048, n_layers=6, n_heads=8, \n                   dropout=0.1, max_len=4096, ff_activation=tl.Relu)\n                   \n   `Your model:`\n   \n    TransformerLM(d_model=4, d_ff=16, n_layers=1, n_heads=2)\n   \n   **Only the parameters shown for your model were changed. The others stayed the same.**\n\n::: {#e1874e86 .cell execution_count=34}\n``` {.python .cell-code}\n# Get the model architecture\nmodel = TransformerLM(mode='eval')\n\n# Load the pre-trained weights\nmodel.init_from_file('model.pkl.gz', weights_only=True)\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">TypeError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[34], line 2</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Get the model architecture</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span> model <span style=\"color:rgb(98,98,98)\">=</span> <span class=\"ansi-yellow-bg\">TransformerLM</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">mode</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">'</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">eval</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">'</span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">      4</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Load the pre-trained weights</span>\n<span class=\"ansi-green-fg ansi-bold\">      5</span> model<span style=\"color:rgb(98,98,98)\">.</span>init_from_file(<span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">model.pkl.gz</span><span style=\"color:rgb(175,0,0)\">'</span>, weights_only<span style=\"color:rgb(98,98,98)\">=</span><span style=\"font-weight:bold;color:rgb(0,135,0)\">True</span>)\n\nCell <span class=\"ansi-green-fg\">In[30], line 46</span>, in <span class=\"ansi-cyan-fg\">TransformerLM</span><span class=\"ansi-blue-fg\">(vocab_size, d_model, d_ff, n_layers, n_heads, dropout, max_len, mode, ff_activation)</span>\n<span class=\"ansi-green-fg ansi-bold\">     36</span> positional_encoder <span style=\"color:rgb(98,98,98)\">=</span> [ \n<span class=\"ansi-green-fg ansi-bold\">     37</span>     <span style=\"font-style:italic;color:rgb(95,135,135)\"># Add embedding layer of dimension (vocab_size, d_model)</span>\n<span class=\"ansi-green-fg ansi-bold\">     38</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>,\n<span class=\"ansi-green-fg\">   (...)</span>\n<span class=\"ansi-green-fg ansi-bold\">     41</span>     <span style=\"font-style:italic;color:rgb(95,135,135)\"># Add positional encoding layer with maximum input length and mode specified</span>\n<span class=\"ansi-green-fg ansi-bold\">     42</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>]\n<span class=\"ansi-green-fg ansi-bold\">     44</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Create stack (list) of decoder blocks with n_layers with necessary parameters</span>\n<span class=\"ansi-green-fg ansi-bold\">     45</span> decoder_blocks <span style=\"color:rgb(98,98,98)\">=</span> [ \n<span class=\"ansi-green-fg\">---&gt; 46</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">for</span> _ <span style=\"font-weight:bold;color:rgb(175,0,255)\">in</span> <span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">range</span><span class=\"ansi-yellow-bg\">(</span><span style=\"font-weight:bold;color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">None</span><span class=\"ansi-yellow-bg\">)</span>]\n<span class=\"ansi-green-fg ansi-bold\">     48</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Create the complete model as written in the figure</span>\n<span class=\"ansi-green-fg ansi-bold\">     49</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">return</span> tl<span style=\"color:rgb(98,98,98)\">.</span>Serial(\n<span class=\"ansi-green-fg ansi-bold\">     50</span>     <span style=\"font-style:italic;color:rgb(95,135,135)\"># Use teacher forcing (feed output of previous step to current step)</span>\n<span class=\"ansi-green-fg ansi-bold\">     51</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>, <span style=\"font-style:italic;color:rgb(95,135,135)\"># Specify the mode!</span>\n<span class=\"ansi-green-fg\">   (...)</span>\n<span class=\"ansi-green-fg ansi-bold\">     63</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>\n<span class=\"ansi-green-fg ansi-bold\">     64</span> )\n\n<span class=\"ansi-red-fg\">TypeError</span>: 'NoneType' object cannot be interpreted as an integer</pre>\n```\n:::\n\n:::\n:::\n\n\n<a name='5'></a>\n# Part 5: Testing with your own input\n\nYou will now test your input. You are going to implement greedy decoding. This consists of two functions. The first one allows you to identify the next symbol. It gets the argmax of the output of your model and then returns that index. \n\n<a name='ex06'></a>\n### Exercise 06\n**Instructions:** Implement the next symbol function that takes in the cur_output_tokens and the trained model to return the index of the next word. \n\n::: {#fb0dec4d .cell execution_count=35}\n``` {.python .cell-code}\n# UNQ_C9\ndef next_symbol(cur_output_tokens, model):\n    \"\"\"Returns the next symbol for a given sentence.\n\n    Args:\n        cur_output_tokens (list): tokenized sentence with EOS and PAD tokens at the end.\n        model (trax.layers.combinators.Serial): The transformer model.\n\n    Returns:\n        int: tokenized symbol.\n    \"\"\"\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    \n    # current output tokens length\n    token_length = None\n    # calculate the minimum power of 2 big enough to store token_length\n    # HINT: use np.ceil() and np.log2()\n    # add 1 to token_length so np.log2() doesn't receive 0 when token_length is 0\n    padded_length = None**int(np.ceil(np.log2(None + None)))\n\n    # Fill cur_output_tokens with 0's until it reaches padded_length\n    padded = None + [0] * (None - None)\n    padded_with_batch = np.array(padded)[None, :] # Don't replace this 'None'! This is a way of setting the batch dim\n\n    # model expects a tuple containing two padded tensors (with batch)\n    output, _ = model((None, None)) \n    # HINT: output has shape (1, padded_length, vocab_size)\n    # To get log_probs you need to index output with 0 in the first dim\n    # token_length in the second dim and all of the entries for the last dim.\n    log_probs = output[None, None, None]\n    \n    ### END CODE HERE ###\n    \n    return int(np.argmax(log_probs))\n```\n:::\n\n\n::: {#2e9ba875 .cell execution_count=36}\n``` {.python .cell-code}\n# Test it out!\nsentence_test_nxt_symbl = \"I want to fly in the sky.\"\ndetokenize([next_symbol(tokenize(sentence_test_nxt_symbl)+[0], model)])\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[36], line 3</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Test it out!</span>\n<span class=\"ansi-green-fg ansi-bold\">      2</span> sentence_test_nxt_symbl <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">I want to fly in the sky.</span><span style=\"color:rgb(175,0,0)\">\"</span>\n<span class=\"ansi-green-fg\">----&gt; 3</span> detokenize([next_symbol(tokenize(sentence_test_nxt_symbl)<span style=\"color:rgb(98,98,98)\">+</span>[<span style=\"color:rgb(98,98,98)\">0</span>], <span class=\"ansi-yellow-bg\">model</span>)])\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'model' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n**Expected Output:**\n```CPP\n'The'\n```\n\n<a name='5.1'></a>\n### 5.1 Greedy decoding\n\nNow you will implement the greedy_decode algorithm that will call the `next_symbol` function. It takes in the input_sentence, the trained model and returns the decoded sentence. \n\n<a name='ex07'></a>\n### Exercise 07\n\n**Instructions**: Implement the greedy_decode algorithm. \n\n::: {#d7b35045 .cell execution_count=37}\n``` {.python .cell-code}\n# UNQ_C10\n# Decoding functions.\ndef greedy_decode(input_sentence, model):\n    \"\"\"Greedy decode function.\n\n    Args:\n        input_sentence (string): a sentence or article.\n        model (trax.layers.combinators.Serial): Transformer model.\n\n    Returns:\n        string: summary of the input.\n    \"\"\"\n    \n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    # Use tokenize()\n    cur_output_tokens = None + [0]\n    generated_output = [] \n    cur_output = 0 \n    EOS = 1 \n    \n    while cur_output != EOS:\n        # Get next symbol\n        cur_output = next_symbol(None, None)\n        # Append next symbol to original sentence\n        cur_output_tokens.append(None)\n        # Append next symbol to generated sentence\n        generated_output.append(None)\n        print(detokenize(generated_output))\n    \n    ### END CODE HERE ###\n    \n    return detokenize(generated_output)\n```\n:::\n\n\n::: {#1f77886e .cell execution_count=38}\n``` {.python .cell-code}\n# Test it out on a sentence!\ntest_sentence = \"It was a sunny day when I went to the market to buy some flowers. But I only found roses, not tulips.\"\nprint(wrapper.fill(test_sentence), '\\n')\nprint(greedy_decode(test_sentence, model))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIt was a sunny day when I went to the market to buy some flowers. But\nI only found roses, not tulips. \n\n```\n:::\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[38], line 4</span>\n<span class=\"ansi-green-fg ansi-bold\">      2</span> test_sentence <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">It was a sunny day when I went to the market to buy some flowers. But I only found roses, not tulips.</span><span style=\"color:rgb(175,0,0)\">\"</span>\n<span class=\"ansi-green-fg ansi-bold\">      3</span> <span style=\"color:rgb(0,135,0)\">print</span>(wrapper<span style=\"color:rgb(98,98,98)\">.</span>fill(test_sentence), <span style=\"color:rgb(175,0,0)\">'</span><span style=\"font-weight:bold;color:rgb(175,95,0)\">\\n</span><span style=\"color:rgb(175,0,0)\">'</span>)\n<span class=\"ansi-green-fg\">----&gt; 4</span> <span style=\"color:rgb(0,135,0)\">print</span>(greedy_decode(test_sentence, <span class=\"ansi-yellow-bg\">model</span>))\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'model' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n**Expected Output:**\n```CPP\n:\n: I\n: I just\n: I just found\n: I just found ros\n: I just found roses\n: I just found roses,\n: I just found roses, not\n: I just found roses, not tu\n: I just found roses, not tulips\n: I just found roses, not tulips\n: I just found roses, not tulips.\n: I just found roses, not tulips.<EOS>\n: I just found roses, not tulips.<EOS>\n```\n\n::: {#90a9f1db .cell execution_count=39}\n``` {.python .cell-code}\n# Test it out with a whole article!\narticle = \"It’s the posing craze sweeping the U.S. after being brought to fame by skier Lindsey Vonn, soccer star Omar Cummings, baseball player Albert Pujols - and even Republican politician Rick Perry. But now four students at Riverhead High School on Long Island, New York, have been suspended for dropping to a knee and taking up a prayer pose to mimic Denver Broncos quarterback Tim Tebow. Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were all suspended for one day because the ‘Tebowing’ craze was blocking the hallway and presenting a safety hazard to students. Scroll down for video. Banned: Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll (all pictured left) were all suspended for one day by Riverhead High School on Long Island, New York, for their tribute to Broncos quarterback Tim Tebow. Issue: Four of the pupils were suspended for one day because they allegedly did not heed to warnings that the 'Tebowing' craze at the school was blocking the hallway and presenting a safety hazard to students.\"\nprint(wrapper.fill(article), '\\n')\nprint(greedy_decode(article, model))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIt’s the posing craze sweeping the U.S. after being brought to fame by\nskier Lindsey Vonn, soccer star Omar Cummings, baseball player Albert\nPujols - and even Republican politician Rick Perry. But now four\nstudents at Riverhead High School on Long Island, New York, have been\nsuspended for dropping to a knee and taking up a prayer pose to mimic\nDenver Broncos quarterback Tim Tebow. Jordan Fulcoly, Wayne Drexel,\nTyler Carroll and Connor Carroll were all suspended for one day\nbecause the ‘Tebowing’ craze was blocking the hallway and presenting a\nsafety hazard to students. Scroll down for video. Banned: Jordan\nFulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll (all pictured\nleft) were all suspended for one day by Riverhead High School on Long\nIsland, New York, for their tribute to Broncos quarterback Tim Tebow.\nIssue: Four of the pupils were suspended for one day because they\nallegedly did not heed to warnings that the 'Tebowing' craze at the\nschool was blocking the hallway and presenting a safety hazard to\nstudents. \n\n```\n:::\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[39], line 4</span>\n<span class=\"ansi-green-fg ansi-bold\">      2</span> article <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">It’s the posing craze sweeping the U.S. after being brought to fame by skier Lindsey Vonn, soccer star Omar Cummings, baseball player Albert Pujols - and even Republican politician Rick Perry. But now four students at Riverhead High School on Long Island, New York, have been suspended for dropping to a knee and taking up a prayer pose to mimic Denver Broncos quarterback Tim Tebow. Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were all suspended for one day because the ‘Tebowing’ craze was blocking the hallway and presenting a safety hazard to students. Scroll down for video. Banned: Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll (all pictured left) were all suspended for one day by Riverhead High School on Long Island, New York, for their tribute to Broncos quarterback Tim Tebow. Issue: Four of the pupils were suspended for one day because they allegedly did not heed to warnings that the </span><span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">Tebowing</span><span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\"> craze at the school was blocking the hallway and presenting a safety hazard to students.</span><span style=\"color:rgb(175,0,0)\">\"</span>\n<span class=\"ansi-green-fg ansi-bold\">      3</span> <span style=\"color:rgb(0,135,0)\">print</span>(wrapper<span style=\"color:rgb(98,98,98)\">.</span>fill(article), <span style=\"color:rgb(175,0,0)\">'</span><span style=\"font-weight:bold;color:rgb(175,95,0)\">\\n</span><span style=\"color:rgb(175,0,0)\">'</span>)\n<span class=\"ansi-green-fg\">----&gt; 4</span> <span style=\"color:rgb(0,135,0)\">print</span>(greedy_decode(article, <span class=\"ansi-yellow-bg\">model</span>))\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'model' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n**Expected Output:**\n```CPP\nJordan\nJordan Ful\nJordan Fulcol\nJordan Fulcoly\nJordan Fulcoly,\nJordan Fulcoly, Wayne\nJordan Fulcoly, Wayne Dre\nJordan Fulcoly, Wayne Drexe\nJordan Fulcoly, Wayne Drexel\nJordan Fulcoly, Wayne Drexel,\n.\n.\n.\n\nFinal summary:\n\nJordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\nsuspended for one day. Four students were suspended for one day\nbecause they allegedly did not heed to warnings that the 'Tebowing'\ncraze was blocking the hallway and presenting a safety hazard to\nstudents.<EOS>\n```\n\n**Congratulations on finishing this week's assignment!** You did a lot of work and now you should have a better understanding of the encoder part of Transformers and how Transformers can be used for text summarization.\n\n**Keep it up!**\n\n",
    "supporting": [
      "assignment_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n<script src=\"https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js\" crossorigin=\"anonymous\"></script>\n"
      ],
      "include-after-body": [
        "<script type=application/vnd.jupyter.widget-state+json>\n{\"state\":{\"03201dd6411549d6af2102500b36ecbb\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"FloatProgressModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"FloatProgressModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"ProgressView\",\"bar_style\":\"success\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_f7e8002cdbf0426d99660e1bc6e9cc71\",\"max\":1,\"min\":0,\"orientation\":\"horizontal\",\"style\":\"IPY_MODEL_dfb1d49edbfc4adfb6a2a83b83afc997\",\"tabbable\":null,\"tooltip\":null,\"value\":1}},\"0744284963d74d7ba3cd511880beefae\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HBoxModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HBoxModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HBoxView\",\"box_style\":\"\",\"children\":[\"IPY_MODEL_c46fa04aa7af41de9e14a66e85741fa7\",\"IPY_MODEL_a4a2f9b8a26043dfa57eeb7e17e779fa\",\"IPY_MODEL_772090781e2d4164bd0015b498ff3b91\"],\"layout\":\"IPY_MODEL_9877d006a2ca4ad5a53ed2872d5e9190\",\"tabbable\":null,\"tooltip\":null}},\"12d9174c4db74c04a9ddfd57ffa5b4d3\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"background\":null,\"description_width\":\"\",\"font_size\":null,\"text_color\":null}},\"15853bfbe1624c52b8a6d3bb4357017c\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"background\":null,\"description_width\":\"\",\"font_size\":null,\"text_color\":null}},\"1e1f49ab429642d5b3c1c6ed2372f86d\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"3e0ac4c092ac40ec8f65796a33c86800\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HBoxModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HBoxModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HBoxView\",\"box_style\":\"\",\"children\":[\"IPY_MODEL_bd853bb8eb1041958f57f811f7ccb61a\",\"IPY_MODEL_03201dd6411549d6af2102500b36ecbb\",\"IPY_MODEL_f0e48d8ff48042c9be4f3f502545c723\"],\"layout\":\"IPY_MODEL_1e1f49ab429642d5b3c1c6ed2372f86d\",\"tabbable\":null,\"tooltip\":null}},\"424077e84838400ab2a29f943bb997f7\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_bafe0df4af6a4f729879d0ad8eb63e05\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_5e7926972fcc45298127fe67e8d94ac6\",\"tabbable\":null,\"tooltip\":null,\"value\":\" 0/0 [00:01&lt;?, ? file/s]\"}},\"50cd8424d99d4187a1349291c8b0ab93\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":\"20px\"}},\"583267fccb6d4f6da510df6b099b570e\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"5e7926972fcc45298127fe67e8d94ac6\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"background\":null,\"description_width\":\"\",\"font_size\":null,\"text_color\":null}},\"677fd4cbb8684f22870f30981df602fa\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"6bc697a36db846a59f7e915c220123da\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"ProgressStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"ProgressStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"bar_color\":null,\"description_width\":\"\"}},\"6fc9a39e1c4d44d89338565a46b4d8f8\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HBoxModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HBoxModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HBoxView\",\"box_style\":\"\",\"children\":[\"IPY_MODEL_84c8b6f9792743d39ae2dfd05bdb070d\",\"IPY_MODEL_8680d3ee72d2474b9051c531d6d50ebb\",\"IPY_MODEL_424077e84838400ab2a29f943bb997f7\"],\"layout\":\"IPY_MODEL_d6826170892e4c6e886cc8cc50eece9b\",\"tabbable\":null,\"tooltip\":null}},\"75476aa6753c4ebbad7c6037cc5da250\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"772090781e2d4164bd0015b498ff3b91\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_583267fccb6d4f6da510df6b099b570e\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_d4ae4602a5aa4195be88cccc34e8518f\",\"tabbable\":null,\"tooltip\":null,\"value\":\" 50967909/50967909 [00:01&lt;00:00, 43337198.85 MiB/s]\"}},\"7992d657f8204e37ad159bbbd55056fb\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"ProgressStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"ProgressStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"bar_color\":null,\"description_width\":\"\"}},\"84c8b6f9792743d39ae2dfd05bdb070d\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_677fd4cbb8684f22870f30981df602fa\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_12d9174c4db74c04a9ddfd57ffa5b4d3\",\"tabbable\":null,\"tooltip\":null,\"value\":\"Extraction completed...: \"}},\"8680d3ee72d2474b9051c531d6d50ebb\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"FloatProgressModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"FloatProgressModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"ProgressView\",\"bar_style\":\"success\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_50cd8424d99d4187a1349291c8b0ab93\",\"max\":1,\"min\":0,\"orientation\":\"horizontal\",\"style\":\"IPY_MODEL_6bc697a36db846a59f7e915c220123da\",\"tabbable\":null,\"tooltip\":null,\"value\":0}},\"9877d006a2ca4ad5a53ed2872d5e9190\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"9b5ae83545864442a72a3f5d95925acc\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"a188e8ee251d4a669cc8be2b4a50a12c\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"background\":null,\"description_width\":\"\",\"font_size\":null,\"text_color\":null}},\"a4a2f9b8a26043dfa57eeb7e17e779fa\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"FloatProgressModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"FloatProgressModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"ProgressView\",\"bar_style\":\"success\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_ad855613cfe646d0ab941c1be35821d9\",\"max\":1,\"min\":0,\"orientation\":\"horizontal\",\"style\":\"IPY_MODEL_7992d657f8204e37ad159bbbd55056fb\",\"tabbable\":null,\"tooltip\":null,\"value\":1}},\"ad855613cfe646d0ab941c1be35821d9\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":\"20px\"}},\"bafe0df4af6a4f729879d0ad8eb63e05\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"bd853bb8eb1041958f57f811f7ccb61a\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_d85033445370479488d99887d3d49ce2\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_c3456c937bff4537b2eca8edd06d6779\",\"tabbable\":null,\"tooltip\":null,\"value\":\"Dl Completed...: 100%\"}},\"c3456c937bff4537b2eca8edd06d6779\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"background\":null,\"description_width\":\"\",\"font_size\":null,\"text_color\":null}},\"c46fa04aa7af41de9e14a66e85741fa7\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_75476aa6753c4ebbad7c6037cc5da250\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_a188e8ee251d4a669cc8be2b4a50a12c\",\"tabbable\":null,\"tooltip\":null,\"value\":\"Dl Size...: 100%\"}},\"d4ae4602a5aa4195be88cccc34e8518f\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"background\":null,\"description_width\":\"\",\"font_size\":null,\"text_color\":null}},\"d6826170892e4c6e886cc8cc50eece9b\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"d85033445370479488d99887d3d49ce2\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"dfb1d49edbfc4adfb6a2a83b83afc997\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"ProgressStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"ProgressStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"bar_color\":null,\"description_width\":\"\"}},\"f0e48d8ff48042c9be4f3f502545c723\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_9b5ae83545864442a72a3f5d95925acc\",\"placeholder\":\"​\",\"style\":\"IPY_MODEL_15853bfbe1624c52b8a6d3bb4357017c\",\"tabbable\":null,\"tooltip\":null,\"value\":\" 5/5 [00:01&lt;00:00,  3.46 url/s]\"}},\"f7e8002cdbf0426d99660e1bc6e9cc71\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":\"20px\"}}},\"version_major\":2,\"version_minor\":0}\n</script>\n"
      ]
    }
  }
}