<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Oren Bochman">

<title>Assignment 1: Neural Machine Translation – NLP Specialization</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-d166b450ba5a8e9f7a0ab969bf6592c1.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-1372234da3246ce8e868649689ba5ed0.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-d99a2a2a191b5c7f2a9a83135e7f0803.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-sidebar floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">NLP Specialization</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">NLP with Attention Models</li><li class="breadcrumb-item"><a href="../../posts/c4w1/index.html">Neural Machine Translation</a></li><li class="breadcrumb-item"><a href="../../posts/c4w1/assignment.html">A4 - NMT with Attention</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">NLP with Attention Models</li><li class="breadcrumb-item"><a href="../../posts/c4w1/index.html">Neural Machine Translation</a></li><li class="breadcrumb-item"><a href="../../posts/c4w1/assignment.html">A4 - NMT with Attention</a></li></ol></nav>
      <h1 class="title">Assignment 1: Neural Machine Translation</h1>
            <p class="subtitle lead">Sequence Models</p>
                                <div class="quarto-categories">
                <div class="quarto-category">NLP</div>
                <div class="quarto-category">Coursera</div>
                <div class="quarto-category">Lab</div>
                <div class="quarto-category">Sequence Models</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta column-body">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Oren Bochman </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">Thursday, February 6, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Classification &amp; Vector Spaces</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Logistic Regression</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c1w1/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c1w1/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Preprocessing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c1w1/lab02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - Frequencies</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c1w1/lab03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L3 - Visualizing tweets</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c1w1/assignment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A1 - Logistic Regression</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Probability &amp; Bayes Rule</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c1w2/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c1w2/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Visualizing Naive Bayes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c1w2/assignment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A2 - Logistic Regression</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false">
 <span class="menu-text">Vector Space Models &amp; PCA</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c1w3/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c1w3/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Linear algebra with NumPy</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c1w3/lab02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - Manipulating word embeddings</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c1w3/assignment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A3 - Hello Vectors</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">MT &amp; Document Search via KNN</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c1w4/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c1w3/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Vector manipulation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c1w3/lab02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - Hash functions and multiplanes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c1w3/assignment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A4 - Naive Machine Translation and LSH</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Probabilistic Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false">
 <span class="menu-text">Autocorrect &amp; Dynamic Programming</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c2w1/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c2w1/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Building the vocabulary</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c2w1/lab02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - Candidates from String Edits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c2w1/assignment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A1 - Auto Correct</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false">
 <span class="menu-text">POS tagging &amp; HMMS</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c2w2/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c2w2/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Vocabulary with unknowns</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c2w2/lab02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - Working with tags and Numpy</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c2w2/assignment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A2 - POS tagging</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false">
 <span class="menu-text">Autocomplete &amp; Language Models</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c2w3/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c2w3/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - N-grams Corpus preprocessing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c2w3/lab02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - Building the language model</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c2w3/lab03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L3 - Out of vocabulary words</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c2w3/assignment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A3 - Auto-Complete</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false">
 <span class="menu-text">Word embeddings with neural networks</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c2w4/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c2w4/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Data preparation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c2w4/lab02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - Intro to CBOW</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c2w4/lab03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L3 - Training the CBOW</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c2w4/assignment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A4 - Word embeddings</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true">
 <span class="menu-text">Sequence Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="false">
 <span class="menu-text">Neural Networks for Sentiment Analysis</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c3w1/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c3w1/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Introduction to Trax</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c3w1/lab02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - Classes and Subclasses</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c3w1/lab03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L3 - Data Generators</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c3w1/assignment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A1 - Sentiment with Deep Neural Networks</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="false">
 <span class="menu-text">RNN for Language Modeling</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c3w2/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c3w2/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Hidden State Activation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c3w2/lab02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - Calculating Perplexity</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c3w2/lab03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L3 - Vanilla RNNs, GRUs and the scan function</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c3w2/lab04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L4 - Creating a GRU model using Trax</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c3w2/assignment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A2 - Deep N-grams</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="false">
 <span class="menu-text">LSTMs and Named Entity Recognition</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-14" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c3w3/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c3w3/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Vanishing Gradients</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c3w3/assignment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A3 - NER</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="false">
 <span class="menu-text">Siamese Networks</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-15" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c3w4/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c3w4/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Creating a Siamese Model using Trax</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c3w4/lab02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - Modified Triplet Loss</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c3w4/lab03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L3 - Evaluate a Siamese Model</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c3w4/assignment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A4 - Question duplicates</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-16" role="navigation" aria-expanded="true">
 <span class="menu-text">NLP with Attention Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-16" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-16" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-17" role="navigation" aria-expanded="true">
 <span class="menu-text">Neural Machine Translation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-17" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-17" class="collapse list-unstyled sidebar-section depth2 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c4w1/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c4w1/lab01.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Stack Semantics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c4w1/lab02.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - BLEU Score</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c4w1/assignment.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">A4 - NMT with Attention</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-18" role="navigation" aria-expanded="false">
 <span class="menu-text">Text Summarization</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-18" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-18" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c4w2/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c4w2/lab01.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Attention</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c4w2/lab02.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - The Transformer Decoder</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c4w2/assignment.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A4 - Transformer Summarizer</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-19" role="navigation" aria-expanded="false">
 <span class="menu-text">Question Answering</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-19" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-19" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c4w3/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c4w3/lab01.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - SentencePiece and BPE</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c4w3/lab02.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - BERT Loss</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c4w3/lab03.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L3 - T5</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c4w3/assignment.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A4 - Question Answering</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-20" role="navigation" aria-expanded="false">
 <span class="menu-text">Chat Bots</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-20" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-20" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c4w4/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c4w4/lab01.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Reformer LSH</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c4w4/lab02.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - Revnet</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c4w4/assignment.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A4 - Chatbot</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#input-encoder" id="toc-input-encoder" class="nav-link active" data-scroll-target="#input-encoder">2.2.1 Input encoder</a></li>
  <li><a href="#pre-attention-decoder" id="toc-pre-attention-decoder" class="nav-link" data-scroll-target="#pre-attention-decoder">2.2.2 Pre-attention decoder</a></li>
  <li><a href="#preparing-the-attention-input" id="toc-preparing-the-attention-input" class="nav-link" data-scroll-target="#preparing-the-attention-input">2.2.3 Preparing the attention input</a></li>
  <li><a href="#comparing-overlaps" id="toc-comparing-overlaps" class="nav-link" data-scroll-target="#comparing-overlaps">4.2.2 Comparing overlaps</a></li>
  <li><a href="#overall-score" id="toc-overall-score" class="nav-link" data-scroll-target="#overall-score">4.2.3 Overall score</a></li>
  <li><a href="#putting-it-all-together" id="toc-putting-it-all-together" class="nav-link" data-scroll-target="#putting-it-all-together">4.2.4 Putting it all together</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/OrenBochman/notes-nlp/edit/main/posts/c4w1/assignment.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/OrenBochman/notes-nlp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full column-body" id="quarto-document-content">






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/Course-Logo-3-3.webp" class="nolightbox img-fluid figure-img"></p>
<figcaption>course banner</figcaption>
</figure>
</div></div><div class="callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Honor code alert
</div>
</div>
<div class="callout-body-container callout-body">
<p>Due to the Coursera Honor Code, I cannot provide the solutions to the assignments.</p>
<ul>
<li>This notebook is the original notebook provided by the course</li>
<li>It is setup to run without stopping for errors.</li>
<li>It is also likely to be out of date as the course has had some updates since I took it.</li>
<li>Although I aced the course this assignment was the most time consuming.</li>
<li>Good luck with the assignment it should make we a better programmer.</li>
<li>It is also a good idea to go over it a few times until we can do it easily.</li>
</ul>
</div>
</div>
<p>Welcome to the first assignment of Course 4. Here, you will build an English-to-German neural machine translation (NMT) model using Long Short-Term Memory (LSTM) networks with attention. Machine translation is an important task in natural language processing and could be useful not only for translating one language to another but also for word sense disambiguation (e.g.&nbsp;determining whether the word “bank” refers to the financial bank, or the land alongside a river). Implementing this using just a Recurrent Neural Network (RNN) with LSTMs can work for short to medium length sentences but can result in vanishing gradients for very long sequences. To solve this, you will be adding an attention mechanism to allow the decoder to access all relevant parts of the input sentence regardless of its length. By completing this assignment, you will:</p>
<ul>
<li>learn how to preprocess your training and evaluation data</li>
<li>implement an encoder-decoder system with attention</li>
<li>understand how attention works</li>
<li>build the NMT model from scratch using Trax</li>
<li>generate translations using greedy and Minimum Bayes Risk (MBR) decoding ## Outline</li>
<li><a href="#1">Part 1: Data Preparation</a>
<ul>
<li><a href="#1.1">1.1 Importing the Data</a></li>
<li><a href="#1.2">1.2 Tokenization and Formatting</a></li>
<li><a href="#1.3">1.3 tokenize &amp; detokenize helper functions</a></li>
<li><a href="#1.4">1.4 Bucketing</a></li>
<li><a href="#1.5">1.5 Exploring the data</a></li>
</ul></li>
<li><a href="#2">Part 2: Neural Machine Translation with Attention</a>
<ul>
<li><a href="#2.1">2.1 Attention Overview</a></li>
<li><a href="#2.2">2.2 Helper functions</a>
<ul>
<li><a href="#ex01">Exercise 01</a></li>
<li><a href="#ex02">Exercise 02</a></li>
<li><a href="#ex03">Exercise 03</a></li>
</ul></li>
<li><a href="#2.3">2.3 Implementation Overview</a>
<ul>
<li><a href="#ex04">Exercise 04</a></li>
</ul></li>
</ul></li>
<li><a href="#3">Part 3: Training</a>
<ul>
<li><a href="#3.1">3.1 TrainTask</a>
<ul>
<li><a href="#ex05">Exercise 05</a></li>
</ul></li>
<li><a href="#3.2">3.2 EvalTask</a></li>
<li><a href="#3.3">3.3 Loop</a></li>
</ul></li>
<li><a href="#4">Part 4: Testing</a>
<ul>
<li><a href="#4.1">4.1 Decoding</a>
<ul>
<li><a href="#ex06">Exercise 06</a></li>
<li><a href="#ex07">Exercise 07</a></li>
</ul></li>
<li><a href="#4.2">4.2 Minimum Bayes-Risk Decoding</a>
<ul>
<li><a href="#ex08">Exercise 08</a></li>
<li><a href="#ex09">Exercise 09</a></li>
<li><a href="#ex10">Exercise 10</a></li>
</ul></li>
</ul></li>
</ul>
<p><a name="1"></a> # Part 1: Data Preparation</p>
<p><a name="1.1"></a> ## 1.1 Importing the Data</p>
<p>We will first start by importing the packages we will use in this assignment. As in the previous course of this specialization, we will use the <a href="https://github.com/google/trax">Trax</a> library created and maintained by the <a href="https://research.google/teams/brain/">Google Brain team</a> to do most of the heavy lifting. It provides submodules to fetch and process the datasets, as well as build and train the model.</p>
<div id="bf7b293b" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> termcolor <span class="im">import</span> colored</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> trax</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> trax <span class="im">import</span> layers <span class="im">as</span> tl</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> trax.fastmath <span class="im">import</span> numpy <span class="im">as</span> fastnp</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> trax.supervised <span class="im">import</span> training</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip <span class="bu">list</span> <span class="op">|</span> grep trax</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>2025-02-06 21:04:21.065529: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1738868661.082575 1093516 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1738868661.088469 1093516 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>trax                         1.4.1</code></pre>
</div>
</div>
<p>Next, we will import the dataset we will use to train the model. To meet the storage constraints in this lab environment, we will just use a small dataset from <a href="http://opus.nlpl.eu/">Opus</a>, a growing collection of translated texts from the web. Particularly, we will get an English to German translation subset specified as <code>opus/medical</code> which has medical related texts. If storage is not an issue, you can opt to get a larger corpus such as the English to German translation dataset from <a href="https://paracrawl.eu/">ParaCrawl</a>, a large multi-lingual translation dataset created by the European Union. Both of these datasets are available via <a href="https://www.tensorflow.org/datasets">Tensorflow Datasets (TFDS)</a> and you can browse through the other available datasets <a href="https://www.tensorflow.org/datasets/catalog/overview">here</a>. We have downloaded the data for you in the <code>data/</code> directory of your workspace. As you’ll see below, you can easily access this dataset from TFDS with <code>trax.data.TFDS</code>. The result is a python generator function yielding tuples. Use the <code>keys</code> argument to select what appears at which position in the tuple. For example, <code>keys=('en', 'de')</code> below will return pairs as (English sentence, German sentence).</p>
<div id="95825275" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get generator function for the training set</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co"># This will download the train dataset if no data_dir is specified.</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>train_stream_fn <span class="op">=</span> trax.data.TFDS(<span class="st">'opus/medical'</span>,</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>                                 data_dir<span class="op">=</span><span class="st">'./data/'</span>,</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>                                 keys<span class="op">=</span>(<span class="st">'en'</span>, <span class="st">'de'</span>),</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>                                 eval_holdout_size<span class="op">=</span><span class="fl">0.01</span>, <span class="co"># 1% for eval</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>                                 train<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Get generator function for the eval set</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>eval_stream_fn <span class="op">=</span> trax.data.TFDS(<span class="st">'opus/medical'</span>,</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>                                data_dir<span class="op">=</span><span class="st">'./data/'</span>,</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>                                keys<span class="op">=</span>(<span class="st">'en'</span>, <span class="st">'de'</span>),</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>                                eval_holdout_size<span class="op">=</span><span class="fl">0.01</span>, <span class="co"># 1% for eval</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>                                train<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/jax/_src/xla_bridge.py:1234: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.
  warnings.warn(
I0000 00:00:1738868712.221304 1093516 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5870 MB memory:  -&gt; device: 0, name: NVIDIA GeForce RTX 3070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6</code></pre>
</div>
</div>
<p>Notice that TFDS returns a generator <em>function</em>, not a generator. This is because in Python, you cannot reset generators so you cannot go back to a previously yielded value. During deep learning training, you use Stochastic Gradient Descent and don’t actually need to go back – but it is sometimes good to be able to do that, and that’s where the functions come in. It is actually very common to use generator functions in Python – e.g., <code>zip</code> is a generator function. You can read more about <a href="https://book.pythontips.com/en/latest/generators.html">Python generators</a> to understand why we use them. Let’s print a a sample pair from our train and eval data. Notice that the raw ouput is represented in bytes (denoted by the <code>b'</code> prefix) and these will be converted to strings internally in the next steps.</p>
<div id="0e77d54a" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>train_stream <span class="op">=</span> train_stream_fn()</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(colored(<span class="st">'train data (en, de) tuple:'</span>, <span class="st">'red'</span>), <span class="bu">next</span>(train_stream))</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>eval_stream <span class="op">=</span> eval_stream_fn()</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(colored(<span class="st">'eval data (en, de) tuple:'</span>, <span class="st">'red'</span>), <span class="bu">next</span>(eval_stream))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NotFoundError</span>                             Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[3], line 2</span>
<span class="ansi-green-fg ansi-bold">      1</span> train_stream <span style="color:rgb(98,98,98)">=</span> train_stream_fn()
<span class="ansi-green-fg">----&gt; 2</span> <span style="color:rgb(0,135,0)">print</span>(colored(<span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">train data (en, de) tuple:</span><span style="color:rgb(175,0,0)">'</span>, <span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">red</span><span style="color:rgb(175,0,0)">'</span>), <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">next</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">train_stream</span><span class="ansi-yellow-bg">)</span>)
<span class="ansi-green-fg ansi-bold">      3</span> <span style="color:rgb(0,135,0)">print</span>()
<span class="ansi-green-fg ansi-bold">      5</span> eval_stream <span style="color:rgb(98,98,98)">=</span> eval_stream_fn()

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/data/tf_inputs.py:354</span>, in <span class="ansi-cyan-fg">TFDS.&lt;locals&gt;.gen</span><span class="ansi-blue-fg">(***failed resolving arguments***)</span>
<span class="ansi-green-fg ansi-bold">    352</span> <span style="font-weight:bold;color:rgb(0,135,0)">def</span><span style="color:rgb(188,188,188)"> </span><span style="color:rgb(0,0,255)">gen</span>(generator<span style="color:rgb(98,98,98)">=</span><span style="font-weight:bold;color:rgb(0,135,0)">None</span>):
<span class="ansi-green-fg ansi-bold">    353</span>   <span style="font-weight:bold;color:rgb(0,135,0)">del</span> generator
<span class="ansi-green-fg">--&gt; 354</span>   <span style="font-weight:bold;color:rgb(0,135,0)">for</span> example <span style="font-weight:bold;color:rgb(175,0,255)">in</span> fastmath<span style="color:rgb(98,98,98)">.</span>dataset_as_numpy(dataset):
<span class="ansi-green-fg ansi-bold">    355</span>     <span style="font-weight:bold;color:rgb(0,135,0)">yield</span> example

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/fastmath/jax.py:154</span>, in <span class="ansi-cyan-fg">_dataset_as_numpy</span><span class="ansi-blue-fg">(ds, batch_size)</span>
<span class="ansi-green-fg ansi-bold">    151</span>       <span style="font-weight:bold;color:rgb(0,135,0)">yield</span> single_example
<span class="ansi-green-fg ansi-bold">    152</span> <span style="font-weight:bold;color:rgb(0,135,0)">except</span> <span style="font-weight:bold;color:rgb(215,95,95)">AttributeError</span>:
<span class="ansi-green-fg ansi-bold">    153</span>   <span style="font-style:italic;color:rgb(95,135,135)"># In TF 1.X there is not dense_to_ragged_batch: fallback.</span>
<span class="ansi-green-fg">--&gt; 154</span>   <span style="font-weight:bold;color:rgb(0,135,0)">for</span> example <span style="font-weight:bold;color:rgb(175,0,255)">in</span> tfds<span style="color:rgb(98,98,98)">.</span>as_numpy(ds):
<span class="ansi-green-fg ansi-bold">    155</span>     <span style="font-weight:bold;color:rgb(0,135,0)">yield</span> example

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tensorflow_datasets/core/dataset_utils.py:82</span>, in <span class="ansi-cyan-fg">_eager_dataset_iterator</span><span class="ansi-blue-fg">(ds)</span>
<span class="ansi-green-fg ansi-bold">     81</span> <span style="font-weight:bold;color:rgb(0,135,0)">def</span><span style="color:rgb(188,188,188)"> </span><span style="color:rgb(0,0,255)">_eager_dataset_iterator</span>(ds: tf<span style="color:rgb(98,98,98)">.</span>data<span style="color:rgb(98,98,98)">.</span>Dataset) <span style="color:rgb(98,98,98)">-</span><span style="color:rgb(98,98,98)">&gt;</span> Iterator[NumpyElem]:
<span class="ansi-green-fg">---&gt; 82</span>   <span style="font-weight:bold;color:rgb(0,135,0)">for</span> elem <span style="font-weight:bold;color:rgb(175,0,255)">in</span> ds:
<span class="ansi-green-fg ansi-bold">     83</span>     <span style="font-weight:bold;color:rgb(0,135,0)">yield</span> tree_utils<span style="color:rgb(98,98,98)">.</span>map_structure(_elem_to_numpy_eager, elem)

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:826</span>, in <span class="ansi-cyan-fg">OwnedIterator.__next__</span><span class="ansi-blue-fg">(self)</span>
<span class="ansi-green-fg ansi-bold">    824</span> <span style="font-weight:bold;color:rgb(0,135,0)">def</span><span style="color:rgb(188,188,188)"> </span><span style="color:rgb(0,0,255)">__next__</span>(<span style="color:rgb(0,135,0)">self</span>):
<span class="ansi-green-fg ansi-bold">    825</span>   <span style="font-weight:bold;color:rgb(0,135,0)">try</span>:
<span class="ansi-green-fg">--&gt; 826</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">_next_internal</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">    827</span>   <span style="font-weight:bold;color:rgb(0,135,0)">except</span> errors<span style="color:rgb(98,98,98)">.</span>OutOfRangeError:
<span class="ansi-green-fg ansi-bold">    828</span>     <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> <span style="font-weight:bold;color:rgb(215,95,95)">StopIteration</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:776</span>, in <span class="ansi-cyan-fg">OwnedIterator._next_internal</span><span class="ansi-blue-fg">(self)</span>
<span class="ansi-green-fg ansi-bold">    773</span> <span style="font-style:italic;color:rgb(95,135,135)"># TODO(b/77291417): This runs in sync mode as iterators use an error status</span>
<span class="ansi-green-fg ansi-bold">    774</span> <span style="font-style:italic;color:rgb(95,135,135)"># to communicate that there is no more data to iterate over.</span>
<span class="ansi-green-fg ansi-bold">    775</span> <span style="font-weight:bold;color:rgb(0,135,0)">with</span> context<span style="color:rgb(98,98,98)">.</span>execution_mode(context<span style="color:rgb(98,98,98)">.</span>SYNC):
<span class="ansi-green-fg">--&gt; 776</span>   ret <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">gen_dataset_ops</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">iterator_get_next</span><span class="ansi-yellow-bg">(</span>
<span class="ansi-green-fg ansi-bold">    777</span> <span class="ansi-yellow-bg">      </span><span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">_iterator_resource</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">    778</span> <span class="ansi-yellow-bg">      </span><span class="ansi-yellow-bg">output_types</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">_flat_output_types</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">    779</span> <span class="ansi-yellow-bg">      </span><span class="ansi-yellow-bg">output_shapes</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">_flat_output_shapes</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">    781</span>   <span style="font-weight:bold;color:rgb(0,135,0)">try</span>:
<span class="ansi-green-fg ansi-bold">    782</span>     <span style="font-style:italic;color:rgb(95,135,135)"># Fast path for the case `self._structure` is not a nested structure.</span>
<span class="ansi-green-fg ansi-bold">    783</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_element_spec<span style="color:rgb(98,98,98)">.</span>_from_compatible_tensor_list(ret)  <span style="font-style:italic;color:rgb(95,135,135)"># pylint: disable=protected-access</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3086</span>, in <span class="ansi-cyan-fg">iterator_get_next</span><span class="ansi-blue-fg">(iterator, output_types, output_shapes, name)</span>
<span class="ansi-green-fg ansi-bold">   3084</span>   <span style="font-weight:bold;color:rgb(0,135,0)">return</span> _result
<span class="ansi-green-fg ansi-bold">   3085</span> <span style="font-weight:bold;color:rgb(0,135,0)">except</span> _core<span style="color:rgb(98,98,98)">.</span>_NotOkStatusException <span style="font-weight:bold;color:rgb(0,135,0)">as</span> e:
<span class="ansi-green-fg">-&gt; 3086</span>   <span class="ansi-yellow-bg">_ops</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">raise_from_not_ok_status</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">e</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">name</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">   3087</span> <span style="font-weight:bold;color:rgb(0,135,0)">except</span> _core<span style="color:rgb(98,98,98)">.</span>_FallbackException:
<span class="ansi-green-fg ansi-bold">   3088</span>   <span style="font-weight:bold;color:rgb(0,135,0)">pass</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:6002</span>, in <span class="ansi-cyan-fg">raise_from_not_ok_status</span><span class="ansi-blue-fg">(e, name)</span>
<span class="ansi-green-fg ansi-bold">   6000</span> <span style="font-weight:bold;color:rgb(0,135,0)">def</span><span style="color:rgb(188,188,188)"> </span><span style="color:rgb(0,0,255)">raise_from_not_ok_status</span>(e, name) <span style="color:rgb(98,98,98)">-</span><span style="color:rgb(98,98,98)">&gt;</span> NoReturn:
<span class="ansi-green-fg ansi-bold">   6001</span>   e<span style="color:rgb(98,98,98)">.</span>message <span style="color:rgb(98,98,98)">+</span><span style="color:rgb(98,98,98)">=</span> (<span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)"> name: </span><span style="color:rgb(175,0,0)">"</span> <span style="color:rgb(98,98,98)">+</span> <span style="color:rgb(0,135,0)">str</span>(name <span style="font-weight:bold;color:rgb(0,135,0)">if</span> name <span style="font-weight:bold;color:rgb(175,0,255)">is</span> <span style="font-weight:bold;color:rgb(175,0,255)">not</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span> <span style="font-weight:bold;color:rgb(0,135,0)">else</span> <span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">"</span>))
<span class="ansi-green-fg">-&gt; 6002</span>   <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> core<span style="color:rgb(98,98,98)">.</span>_status_to_exception(e) <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">None</span>

<span class="ansi-red-fg">NotFoundError</span>: {{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} data/opus/medical/0.1.0/opus-train.tfrecord-00000-of-00002; No such file or directory [Op:IteratorGetNext] name: </pre>
</div>
</div>
</div>
<p><a name="1.2"></a> ## 1.2 Tokenization and Formatting</p>
<p>Now that we have imported our corpus, we will be preprocessing the sentences into a format that our model can accept. This will be composed of several steps:</p>
<p><strong>Tokenizing the sentences using subword representations:</strong> As you’ve learned in the earlier courses of this specialization, we want to represent each sentence as an array of integers instead of strings. For our application, we will use <em>subword</em> representations to tokenize our sentences. This is a common technique to avoid out-of-vocabulary words by allowing parts of words to be represented separately. For example, instead of having separate entries in your vocabulary for –“fear”, “fearless”, “fearsome”, “some”, and “less”–, you can simply store –“fear”, “some”, and “less”– then allow your tokenizer to combine these subwords when needed. This allows it to be more flexible so you won’t have to save uncommon words explicitly in your vocabulary (e.g.&nbsp;<em>stylebender</em>, <em>nonce</em>, etc). Tokenizing is done with the <code>trax.data.Tokenize()</code> command and we have provided you the combined subword vocabulary for English and German (i.e.&nbsp;<code>ende_32k.subword</code>) saved in the <code>data</code> directory. Feel free to open this file to see how the subwords look like.</p>
<div id="29f1a7ae" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># global variables that state the filename and directory of the vocabulary file</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>VOCAB_FILE <span class="op">=</span> <span class="st">'ende_32k.subword'</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>VOCAB_DIR <span class="op">=</span> <span class="st">'data/'</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenize the dataset.</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>tokenized_train_stream <span class="op">=</span> trax.data.Tokenize(vocab_file<span class="op">=</span>VOCAB_FILE, vocab_dir<span class="op">=</span>VOCAB_DIR)(train_stream)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>tokenized_eval_stream <span class="op">=</span> trax.data.Tokenize(vocab_file<span class="op">=</span>VOCAB_FILE, vocab_dir<span class="op">=</span>VOCAB_DIR)(eval_stream)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[4], line 7</span>
<span class="ansi-green-fg ansi-bold">      5</span> <span style="font-style:italic;color:rgb(95,135,135)"># Tokenize the dataset.</span>
<span class="ansi-green-fg ansi-bold">      6</span> tokenized_train_stream <span style="color:rgb(98,98,98)">=</span> trax<span style="color:rgb(98,98,98)">.</span>data<span style="color:rgb(98,98,98)">.</span>Tokenize(vocab_file<span style="color:rgb(98,98,98)">=</span>VOCAB_FILE, vocab_dir<span style="color:rgb(98,98,98)">=</span>VOCAB_DIR)(train_stream)
<span class="ansi-green-fg">----&gt; 7</span> tokenized_eval_stream <span style="color:rgb(98,98,98)">=</span> trax<span style="color:rgb(98,98,98)">.</span>data<span style="color:rgb(98,98,98)">.</span>Tokenize(vocab_file<span style="color:rgb(98,98,98)">=</span>VOCAB_FILE, vocab_dir<span style="color:rgb(98,98,98)">=</span>VOCAB_DIR)(<span class="ansi-yellow-bg">eval_stream</span>)

<span class="ansi-red-fg">NameError</span>: name 'eval_stream' is not defined</pre>
</div>
</div>
</div>
<p><strong>Append an end-of-sentence token to each sentence:</strong> We will assign a token (i.e.&nbsp;in this case <code>1</code>) to mark the end of a sentence. This will be useful in inference/prediction so we’ll know that the model has completed the translation.</p>
<div id="0cb6aed9" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Append EOS at the end of each sentence.</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Integer assigned as end-of-sentence (EOS)</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>EOS <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co"># generator helper function to append EOS to each sentence</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> append_eos(stream):</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (inputs, targets) <span class="kw">in</span> stream:</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>        inputs_with_eos <span class="op">=</span> <span class="bu">list</span>(inputs) <span class="op">+</span> [EOS]</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>        targets_with_eos <span class="op">=</span> <span class="bu">list</span>(targets) <span class="op">+</span> [EOS]</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">yield</span> np.array(inputs_with_eos), np.array(targets_with_eos)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="co"># append EOS to the train data</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>tokenized_train_stream <span class="op">=</span> append_eos(tokenized_train_stream)</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a><span class="co"># append EOS to the eval data</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>tokenized_eval_stream <span class="op">=</span> append_eos(tokenized_eval_stream)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[5], line 17</span>
<span class="ansi-green-fg ansi-bold">     14</span> tokenized_train_stream <span style="color:rgb(98,98,98)">=</span> append_eos(tokenized_train_stream)
<span class="ansi-green-fg ansi-bold">     16</span> <span style="font-style:italic;color:rgb(95,135,135)"># append EOS to the eval data</span>
<span class="ansi-green-fg">---&gt; 17</span> tokenized_eval_stream <span style="color:rgb(98,98,98)">=</span> append_eos(<span class="ansi-yellow-bg">tokenized_eval_stream</span>)

<span class="ansi-red-fg">NameError</span>: name 'tokenized_eval_stream' is not defined</pre>
</div>
</div>
</div>
<p><strong>Filter long sentences:</strong> We will place a limit on the number of tokens per sentence to ensure we won’t run out of memory. This is done with the <code>trax.data.FilterByLength()</code> method and you can see its syntax below.</p>
<div id="e3dcc071" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Filter too long sentences to not run out of memory.</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="co"># length_keys=[0, 1] means we filter both English and German sentences, so</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co"># both much be not longer that 256 tokens for training / 512 for eval.</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>filtered_train_stream <span class="op">=</span> trax.data.FilterByLength(</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    max_length<span class="op">=</span><span class="dv">256</span>, length_keys<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">1</span>])(tokenized_train_stream)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>filtered_eval_stream <span class="op">=</span> trax.data.FilterByLength(</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    max_length<span class="op">=</span><span class="dv">512</span>, length_keys<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">1</span>])(tokenized_eval_stream)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="co"># print a sample input-target pair of tokenized sentences</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>train_input, train_target <span class="op">=</span> <span class="bu">next</span>(filtered_train_stream)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(colored(<span class="ss">f'Single tokenized example input:'</span>, <span class="st">'red'</span> ), train_input)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(colored(<span class="ss">f'Single tokenized example target:'</span>, <span class="st">'red'</span>), train_target)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[6], line 7</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># Filter too long sentences to not run out of memory.</span>
<span class="ansi-green-fg ansi-bold">      2</span> <span style="font-style:italic;color:rgb(95,135,135)"># length_keys=[0, 1] means we filter both English and German sentences, so</span>
<span class="ansi-green-fg ansi-bold">      3</span> <span style="font-style:italic;color:rgb(95,135,135)"># both much be not longer that 256 tokens for training / 512 for eval.</span>
<span class="ansi-green-fg ansi-bold">      4</span> filtered_train_stream <span style="color:rgb(98,98,98)">=</span> trax<span style="color:rgb(98,98,98)">.</span>data<span style="color:rgb(98,98,98)">.</span>FilterByLength(
<span class="ansi-green-fg ansi-bold">      5</span>     max_length<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">256</span>, length_keys<span style="color:rgb(98,98,98)">=</span>[<span style="color:rgb(98,98,98)">0</span>, <span style="color:rgb(98,98,98)">1</span>])(tokenized_train_stream)
<span class="ansi-green-fg ansi-bold">      6</span> filtered_eval_stream <span style="color:rgb(98,98,98)">=</span> trax<span style="color:rgb(98,98,98)">.</span>data<span style="color:rgb(98,98,98)">.</span>FilterByLength(
<span class="ansi-green-fg">----&gt; 7</span>     max_length<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">512</span>, length_keys<span style="color:rgb(98,98,98)">=</span>[<span style="color:rgb(98,98,98)">0</span>, <span style="color:rgb(98,98,98)">1</span>])(<span class="ansi-yellow-bg">tokenized_eval_stream</span>)
<span class="ansi-green-fg ansi-bold">      9</span> <span style="font-style:italic;color:rgb(95,135,135)"># print a sample input-target pair of tokenized sentences</span>
<span class="ansi-green-fg ansi-bold">     10</span> train_input, train_target <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)">next</span>(filtered_train_stream)

<span class="ansi-red-fg">NameError</span>: name 'tokenized_eval_stream' is not defined</pre>
</div>
</div>
</div>
<p><a name="1.3"></a> ## 1.3 tokenize &amp; detokenize helper functions</p>
<p>Given any data set, you have to be able to map words to their indices, and indices to their words. The inputs and outputs to your trax models are usually tensors of numbers where each number corresponds to a word. If you were to process your data manually, you would have to make use of the following:</p>
<ul>
<li><span style="color:blue"> word2Ind: </span> a dictionary mapping the word to its index.</li>
<li><span style="color:blue"> ind2Word:</span> a dictionary mapping the index to its word.</li>
<li><span style="color:blue"> word2Count:</span> a dictionary mapping the word to the number of times it appears.</li>
<li><span style="color:blue"> num_words:</span> total number of words that have appeared.</li>
</ul>
<p>Since you have already implemented these in previous assignments of the specialization, we will provide you with helper functions that will do this for you. Run the cell below to get the following functions:</p>
<ul>
<li><span style="color:blue"> tokenize(): </span> converts a text sentence to its corresponding token list (i.e.&nbsp;list of indices). Also converts words to subwords (parts of words).</li>
<li><span style="color:blue"> detokenize(): </span> converts a token list to its corresponding sentence (i.e.&nbsp;string).</li>
</ul>
<div id="ffb79b74" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Setup helper functions for tokenizing and detokenizing sentences</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize(input_str, vocab_file<span class="op">=</span><span class="va">None</span>, vocab_dir<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Encodes a string to an array of integers</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co">        input_str (str): human-readable string to encode</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="co">        vocab_file (str): filename of the vocabulary text file</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="co">        vocab_dir (str): path to the vocabulary file</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="co">  </span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="co">        numpy.ndarray: tokenized version of the input string</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Set the encoding of the "end of sentence" as 1</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>    EOS <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use the trax.data.tokenize method. It takes streams and returns streams,</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># we get around it by making a 1-element stream with `iter`.</span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span>  <span class="bu">next</span>(trax.data.tokenize(<span class="bu">iter</span>([input_str]),</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>                                      vocab_file<span class="op">=</span>vocab_file, vocab_dir<span class="op">=</span>vocab_dir))</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mark the end of the sentence with EOS</span></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> <span class="bu">list</span>(inputs) <span class="op">+</span> [EOS]</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Adding the batch dimension to the front of the shape</span></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>    batch_inputs <span class="op">=</span> np.reshape(np.array(inputs), [<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> batch_inputs</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> detokenize(integers, vocab_file<span class="op">=</span><span class="va">None</span>, vocab_dir<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Decodes an array of integers to a human readable string</span></span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a><span class="co">        integers (numpy.ndarray): array of integers to decode</span></span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a><span class="co">        vocab_file (str): filename of the vocabulary text file</span></span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a><span class="co">        vocab_dir (str): path to the vocabulary file</span></span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a><span class="co">  </span></span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a><span class="co">        str: the decoded sentence.</span></span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Remove the dimensions of size 1</span></span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a>    integers <span class="op">=</span> <span class="bu">list</span>(np.squeeze(integers))</span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Set the encoding of the "end of sentence" as 1</span></span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a>    EOS <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Remove the EOS to decode only the original tokens</span></span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> EOS <span class="kw">in</span> integers:</span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a>        integers <span class="op">=</span> integers[:integers.index(EOS)] </span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> trax.data.detokenize(integers, vocab_file<span class="op">=</span>vocab_file, vocab_dir<span class="op">=</span>vocab_dir)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s see how we might use these functions:</p>
<div id="5bb3dfca" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># As declared earlier:</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="co"># VOCAB_FILE = 'ende_32k.subword'</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co"># VOCAB_DIR = 'data/'</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Detokenize an input-target pair of tokenized sentences</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(colored(<span class="ss">f'Single detokenized example input:'</span>, <span class="st">'red'</span>), detokenize(train_input, vocab_file<span class="op">=</span>VOCAB_FILE, vocab_dir<span class="op">=</span>VOCAB_DIR))</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(colored(<span class="ss">f'Single detokenized example target:'</span>, <span class="st">'red'</span>), detokenize(train_target, vocab_file<span class="op">=</span>VOCAB_FILE, vocab_dir<span class="op">=</span>VOCAB_DIR))</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenize and detokenize a word that is not explicitly saved in the vocabulary file.</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="co"># See how it combines the subwords -- 'hell' and 'o'-- to form the word 'hello'.</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(colored(<span class="ss">f"tokenize('hello'): "</span>, <span class="st">'green'</span>), tokenize(<span class="st">'hello'</span>, vocab_file<span class="op">=</span>VOCAB_FILE, vocab_dir<span class="op">=</span>VOCAB_DIR))</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(colored(<span class="ss">f"detokenize([17332, 140, 1]): "</span>, <span class="st">'green'</span>), detokenize([<span class="dv">17332</span>, <span class="dv">140</span>, <span class="dv">1</span>], vocab_file<span class="op">=</span>VOCAB_FILE, vocab_dir<span class="op">=</span>VOCAB_DIR))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[8], line 6</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># As declared earlier:</span>
<span class="ansi-green-fg ansi-bold">      2</span> <span style="font-style:italic;color:rgb(95,135,135)"># VOCAB_FILE = 'ende_32k.subword'</span>
<span class="ansi-green-fg ansi-bold">      3</span> <span style="font-style:italic;color:rgb(95,135,135)"># VOCAB_DIR = 'data/'</span>
<span class="ansi-green-fg ansi-bold">      4</span> 
<span class="ansi-green-fg ansi-bold">      5</span> <span style="font-style:italic;color:rgb(95,135,135)"># Detokenize an input-target pair of tokenized sentences</span>
<span class="ansi-green-fg">----&gt; 6</span> <span style="color:rgb(0,135,0)">print</span>(colored(<span style="color:rgb(175,0,0)">f</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">Single detokenized example input:</span><span style="color:rgb(175,0,0)">'</span>, <span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">red</span><span style="color:rgb(175,0,0)">'</span>), detokenize(<span class="ansi-yellow-bg">train_input</span>, vocab_file<span style="color:rgb(98,98,98)">=</span>VOCAB_FILE, vocab_dir<span style="color:rgb(98,98,98)">=</span>VOCAB_DIR))
<span class="ansi-green-fg ansi-bold">      7</span> <span style="color:rgb(0,135,0)">print</span>(colored(<span style="color:rgb(175,0,0)">f</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">Single detokenized example target:</span><span style="color:rgb(175,0,0)">'</span>, <span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">red</span><span style="color:rgb(175,0,0)">'</span>), detokenize(train_target, vocab_file<span style="color:rgb(98,98,98)">=</span>VOCAB_FILE, vocab_dir<span style="color:rgb(98,98,98)">=</span>VOCAB_DIR))
<span class="ansi-green-fg ansi-bold">      8</span> <span style="color:rgb(0,135,0)">print</span>()

<span class="ansi-red-fg">NameError</span>: name 'train_input' is not defined</pre>
</div>
</div>
</div>
<p><a name="1.4"></a> ## 1.4 Bucketing</p>
<p>Bucketing the tokenized sentences is an important technique used to speed up training in NLP. Here is a <a href="https://medium.com/@rashmi.margani/how-to-speed-up-the-training-of-the-sequence-model-using-bucketing-techniques-9e302b0fd976">nice article describing it in detail</a> but the gist is very simple. Our inputs have variable lengths and you want to make these the same when batching groups of sentences together. One way to do that is to pad each sentence to the length of the longest sentence in the dataset. This might lead to some wasted computation though. For example, if there are multiple short sentences with just two tokens, do we want to pad these when the longest sentence is composed of a 100 tokens? Instead of padding with 0s to the maximum length of a sentence each time, we can group our tokenized sentences by length and bucket, as on this image (from the article above):</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://miro.medium.com/max/700/1*hcGuja_d5Z_rFcgwe9dPow.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="alt text"><img src="https://miro.medium.com/max/700/1*hcGuja_d5Z_rFcgwe9dPow.png" class="img-fluid figure-img" alt="alt text"></a></p>
<figcaption>alt text</figcaption>
</figure>
</div>
<p>We batch the sentences with similar length together (e.g.&nbsp;the blue sentences in the image above) and only add minimal padding to make them have equal length (usually up to the nearest power of two). This allows to waste less computation when processing padded sequences. In Trax, it is implemented in the <a href="https://github.com/google/trax/blob/5fb8aa8c5cb86dabb2338938c745996d5d87d996/trax/supervised/inputs.py#L378">bucket_by_length</a> function.</p>
<div id="ac946b26" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Bucketing to create streams of batches.</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Buckets are defined in terms of boundaries and batch sizes.</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Batch_sizes[i] determines the batch size for items with length &lt; boundaries[i]</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="co"># So below, we'll take a batch of 256 sentences of length &lt; 8, 128 if length is</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="co"># between 8 and 16, and so on -- and only 2 if length is over 512.</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>boundaries <span class="op">=</span>  [<span class="dv">8</span>,   <span class="dv">16</span>,  <span class="dv">32</span>, <span class="dv">64</span>, <span class="dv">128</span>, <span class="dv">256</span>, <span class="dv">512</span>]</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>batch_sizes <span class="op">=</span> [<span class="dv">256</span>, <span class="dv">128</span>, <span class="dv">64</span>, <span class="dv">32</span>, <span class="dv">16</span>,    <span class="dv">8</span>,   <span class="dv">4</span>,  <span class="dv">2</span>]</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the generators.</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>train_batch_stream <span class="op">=</span> trax.data.BucketByLength(</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    boundaries, batch_sizes,</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    length_keys<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">1</span>]  <span class="co"># As before: count inputs and targets to length.</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>)(filtered_train_stream)</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>eval_batch_stream <span class="op">=</span> trax.data.BucketByLength(</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>    boundaries, batch_sizes,</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>    length_keys<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">1</span>]  <span class="co"># As before: count inputs and targets to length.</span></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>)(filtered_eval_stream)</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Add masking for the padding (0s).</span></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>train_batch_stream <span class="op">=</span> trax.data.AddLossWeights(id_to_mask<span class="op">=</span><span class="dv">0</span>)(train_batch_stream)</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>eval_batch_stream <span class="op">=</span> trax.data.AddLossWeights(id_to_mask<span class="op">=</span><span class="dv">0</span>)(eval_batch_stream)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[9], line 19</span>
<span class="ansi-green-fg ansi-bold">     10</span> <span style="font-style:italic;color:rgb(95,135,135)"># Create the generators.</span>
<span class="ansi-green-fg ansi-bold">     11</span> train_batch_stream <span style="color:rgb(98,98,98)">=</span> trax<span style="color:rgb(98,98,98)">.</span>data<span style="color:rgb(98,98,98)">.</span>BucketByLength(
<span class="ansi-green-fg ansi-bold">     12</span>     boundaries, batch_sizes,
<span class="ansi-green-fg ansi-bold">     13</span>     length_keys<span style="color:rgb(98,98,98)">=</span>[<span style="color:rgb(98,98,98)">0</span>, <span style="color:rgb(98,98,98)">1</span>]  <span style="font-style:italic;color:rgb(95,135,135)"># As before: count inputs and targets to length.</span>
<span class="ansi-green-fg ansi-bold">     14</span> )(filtered_train_stream)
<span class="ansi-green-fg ansi-bold">     16</span> eval_batch_stream <span style="color:rgb(98,98,98)">=</span> trax<span style="color:rgb(98,98,98)">.</span>data<span style="color:rgb(98,98,98)">.</span>BucketByLength(
<span class="ansi-green-fg ansi-bold">     17</span>     boundaries, batch_sizes,
<span class="ansi-green-fg ansi-bold">     18</span>     length_keys<span style="color:rgb(98,98,98)">=</span>[<span style="color:rgb(98,98,98)">0</span>, <span style="color:rgb(98,98,98)">1</span>]  <span style="font-style:italic;color:rgb(95,135,135)"># As before: count inputs and targets to length.</span>
<span class="ansi-green-fg">---&gt; 19</span> )(<span class="ansi-yellow-bg">filtered_eval_stream</span>)
<span class="ansi-green-fg ansi-bold">     21</span> <span style="font-style:italic;color:rgb(95,135,135)"># Add masking for the padding (0s).</span>
<span class="ansi-green-fg ansi-bold">     22</span> train_batch_stream <span style="color:rgb(98,98,98)">=</span> trax<span style="color:rgb(98,98,98)">.</span>data<span style="color:rgb(98,98,98)">.</span>AddLossWeights(id_to_mask<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">0</span>)(train_batch_stream)

<span class="ansi-red-fg">NameError</span>: name 'filtered_eval_stream' is not defined</pre>
</div>
</div>
</div>
<p><a name="1.5"></a> ## 1.5 Exploring the data</p>
<p>We will now be displaying some of our data. You will see that the functions defined above (i.e.&nbsp;<code>tokenize()</code> and <code>detokenize()</code>) do the same things you have been doing again and again throughout the specialization. We gave these so you can focus more on building the model from scratch. Let us first get the data generator and get one batch of the data.</p>
<div id="a4521477" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>input_batch, target_batch, mask_batch <span class="op">=</span> <span class="bu">next</span>(train_batch_stream)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co"># let's see the data type of a batch</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"input_batch data type: "</span>, <span class="bu">type</span>(input_batch))</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"target_batch data type: "</span>, <span class="bu">type</span>(target_batch))</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co"># let's see the shape of this particular batch (batch length, sentence length)</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"input_batch shape: "</span>, input_batch.shape)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"target_batch shape: "</span>, target_batch.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">StopIteration</span>                             Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[10], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> input_batch, target_batch, mask_batch <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">next</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">train_batch_stream</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">      3</span> <span style="font-style:italic;color:rgb(95,135,135)"># let's see the data type of a batch</span>
<span class="ansi-green-fg ansi-bold">      4</span> <span style="color:rgb(0,135,0)">print</span>(<span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">input_batch data type: </span><span style="color:rgb(175,0,0)">"</span>, <span style="color:rgb(0,135,0)">type</span>(input_batch))

<span class="ansi-red-fg">StopIteration</span>: </pre>
</div>
</div>
</div>
<p>The <code>input_batch</code> and <code>target_batch</code> are Numpy arrays consisting of tokenized English sentences and German sentences respectively. These tokens will later be used to produce embedding vectors for each word in the sentence (so the embedding for a sentence will be a matrix). The number of sentences in each batch is usually a power of 2 for optimal computer memory usage.</p>
<p>We can now visually inspect some of the data. You can run the cell below several times to shuffle through the sentences. Just to note, while this is a standard data set that is used widely, it does have some known wrong translations. With that, let’s pick a random sentence and print its tokenized representation.</p>
<div id="1f6033cb" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pick a random index less than the batch size.</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>index <span class="op">=</span> random.randrange(<span class="bu">len</span>(input_batch))</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co"># use the index to grab an entry from the input and target batch</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(colored(<span class="st">'THIS IS THE ENGLISH SENTENCE: </span><span class="ch">\n</span><span class="st">'</span>, <span class="st">'red'</span>), detokenize(input_batch[index], vocab_file<span class="op">=</span>VOCAB_FILE, vocab_dir<span class="op">=</span>VOCAB_DIR), <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(colored(<span class="st">'THIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: </span><span class="ch">\n</span><span class="st"> '</span>, <span class="st">'red'</span>), input_batch[index], <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(colored(<span class="st">'THIS IS THE GERMAN TRANSLATION: </span><span class="ch">\n</span><span class="st">'</span>, <span class="st">'red'</span>), detokenize(target_batch[index], vocab_file<span class="op">=</span>VOCAB_FILE, vocab_dir<span class="op">=</span>VOCAB_DIR), <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(colored(<span class="st">'THIS IS THE TOKENIZED VERSION OF THE GERMAN TRANSLATION: </span><span class="ch">\n</span><span class="st">'</span>, <span class="st">'red'</span>), target_batch[index], <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[11], line 2</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># pick a random index less than the batch size.</span>
<span class="ansi-green-fg">----&gt; 2</span> index <span style="color:rgb(98,98,98)">=</span> random<span style="color:rgb(98,98,98)">.</span>randrange(<span style="color:rgb(0,135,0)">len</span>(<span class="ansi-yellow-bg">input_batch</span>))
<span class="ansi-green-fg ansi-bold">      4</span> <span style="font-style:italic;color:rgb(95,135,135)"># use the index to grab an entry from the input and target batch</span>
<span class="ansi-green-fg ansi-bold">      5</span> <span style="color:rgb(0,135,0)">print</span>(colored(<span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">THIS IS THE ENGLISH SENTENCE: </span><span style="font-weight:bold;color:rgb(175,95,0)">\n</span><span style="color:rgb(175,0,0)">'</span>, <span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">red</span><span style="color:rgb(175,0,0)">'</span>), detokenize(input_batch[index], vocab_file<span style="color:rgb(98,98,98)">=</span>VOCAB_FILE, vocab_dir<span style="color:rgb(98,98,98)">=</span>VOCAB_DIR), <span style="color:rgb(175,0,0)">'</span><span style="font-weight:bold;color:rgb(175,95,0)">\n</span><span style="color:rgb(175,0,0)">'</span>)

<span class="ansi-red-fg">NameError</span>: name 'input_batch' is not defined</pre>
</div>
</div>
</div>
<p><a name="2"></a> # Part 2: Neural Machine Translation with Attention</p>
<p>Now that you have the data generators and have handled the preprocessing, it is time for you to build the model. You will be implementing a neural machine translation model from scratch with attention.</p>
<p><a name="2.1"></a> ## 2.1 Attention Overview</p>
<p>The model we will be building uses an encoder-decoder architecture. This Recurrent Neural Network (RNN) will take in a tokenized version of a sentence in its encoder, then passes it on to the decoder for translation. As mentioned in the lectures, just using a a regular sequence-to-sequence model with LSTMs will work effectively for short to medium sentences but will start to degrade for longer ones. You can picture it like the figure below where all of the context of the input sentence is compressed into one vector that is passed into the decoder block. You can see how this will be an issue for very long sentences (e.g.&nbsp;100 tokens or more) because the context of the first parts of the input will have very little effect on the final vector passed to the decoder.</p>
<p><img src="plain_rnn.png"></p>
<p>Adding an attention layer to this model avoids this problem by giving the decoder access to all parts of the input sentence. To illustrate, let’s just use a 4-word input sentence as shown below. Remember that a hidden state is produced at each timestep of the encoder (represented by the orange rectangles). These are all passed to the attention layer and each are given a score given the current activation (i.e.&nbsp;hidden state) of the decoder. For instance, let’s consider the figure below where the first prediction “Wie” is already made. To produce the next prediction, the attention layer will first receive all the encoder hidden states (i.e.&nbsp;orange rectangles) as well as the decoder hidden state when producing the word “Wie” (i.e.&nbsp;first green rectangle). Given these information, it will score each of the encoder hidden states to know which one the decoder should focus on to produce the next word. The result of the model training might have learned that it should align to the second encoder hidden state and subsequently assigns a high probability to the word “geht”. If we are using greedy decoding, we will output the said word as the next symbol, then restart the process to produce the next word until we reach an end-of-sentence prediction.</p>
<p><img src="attention_overview.png"></p>
<p>There are different ways to implement attention and the one we’ll use for this assignment is the Scaled Dot Product Attention which has the form:</p>
<p><span class="math display">Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V</span></p>
<p>You will dive deeper into this equation in the next week but for now, you can think of it as computing scores using queries (Q) and keys (K), followed by a multiplication of values (V) to get a context vector at a particular timestep of the decoder. This context vector is fed to the decoder RNN to get a set of probabilities for the next predicted word. The division by square root of the keys dimensionality (<span class="math inline">\sqrt{d_k}</span>) is for improving model performance and you’ll also learn more about it next week. For our machine translation application, the encoder activations (i.e.&nbsp;encoder hidden states) will be the keys and values, while the decoder activations (i.e.&nbsp;decoder hidden states) will be the queries.</p>
<p>You will see in the upcoming sections that this complex architecture and mechanism can be implemented with just a few lines of code. Let’s get started!</p>
<p><a name="2.2"></a> ## 2.2 Helper functions</p>
<p>We will first implement a few functions that we will use later on. These will be for the input encoder, pre-attention decoder, and preparation of the queries, keys, values, and mask.</p>
<section id="input-encoder" class="level3">
<h3 class="anchored" data-anchor-id="input-encoder">2.2.1 Input encoder</h3>
<p>The input encoder runs on the input tokens, creates its embeddings, and feeds it to an LSTM network. This outputs the activations that will be the keys and values for attention. It is a <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Serial">Serial</a> network which uses:</p>
<ul>
<li><p><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding">tl.Embedding</a>: Converts each token to its vector representation. In this case, it is the the size of the vocabulary by the dimension of the model: <code>tl.Embedding(vocab_size, d_model)</code>. <code>vocab_size</code> is the number of entries in the given vocabulary. <code>d_model</code> is the number of elements in the word embedding.</p></li>
<li><p><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.LSTM">tl.LSTM</a>: LSTM layer of size <code>d_model</code>. We want to be able to configure how many encoder layers we have so remember to create LSTM layers equal to the number of the <code>n_encoder_layers</code> parameter.</p></li>
</ul>
<p><img src="input_encoder.png"></p>
<p><a name="ex01"></a> ### Exercise 01</p>
<p><strong>Instructions:</strong> Implement the <code>input_encoder_fn</code> function.</p>
<div id="30b99d0d" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># UNQ_C1</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="co"># GRADED FUNCTION</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> input_encoder_fn(input_vocab_size, d_model, n_encoder_layers):</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" Input encoder runs on the input sentence and creates</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="co">    activations that will be the keys and values for attention.</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="co">        input_vocab_size: int: vocab size of the input</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="co">        d_model: int:  depth of embedding (n_units in the LSTM cell)</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="co">        n_encoder_layers: int: number of LSTM layers in the encoder</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="co">        tl.Serial: The input encoder</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># create a serial network</span></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>    input_encoder <span class="op">=</span> tl.Serial( </span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>        <span class="co">### START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) </span><span class="al">###</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># create an embedding layer to convert tokens to vectors</span></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">None</span>,</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># feed the embeddings to the LSTM layers. It is a stack of n_encoder_layers LSTM layers</span></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>        [<span class="va">None</span> <span class="cf">for</span> _ <span class="kw">in</span> <span class="va">None</span>]</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>        <span class="co">### </span><span class="re">END</span><span class="co"> CODE HERE </span><span class="al">###</span></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> input_encoder</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><em>Note: To make this notebook more neat, we moved the unit tests to a separate file called <code>w1_unittest.py</code>. Feel free to open it from your workspace if needed. We have placed comments in that file to indicate which functions are testing which part of the assignment (e.g.&nbsp;<code>test_input_encoder_fn()</code> has the unit tests for UNQ_C1).</em></p>
<div id="349d0f80" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="re">BEGIN</span><span class="co"> UNIT </span><span class="al">TEST</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> w1_unittest</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>w1_unittest.test_input_encoder_fn(input_encoder_fn)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="re">END</span><span class="co"> UNIT </span><span class="al">TEST</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">TypeError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[13], line 4</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># BEGIN UNIT TEST</span>
<span class="ansi-green-fg ansi-bold">      2</span> <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">w1_unittest</span>
<span class="ansi-green-fg">----&gt; 4</span> <span class="ansi-yellow-bg">w1_unittest</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">test_input_encoder_fn</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">input_encoder_fn</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">      5</span> <span style="font-style:italic;color:rgb(95,135,135)"># END UNIT TEST</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/posts/c4w1/w1_unittest.py:98</span>, in <span class="ansi-cyan-fg">test_input_encoder_fn</span><span class="ansi-blue-fg">(input_encoder_fn)</span>
<span class="ansi-green-fg ansi-bold">     95</span> d_model <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(98,98,98)">2</span>
<span class="ansi-green-fg ansi-bold">     96</span> n_encoder_layers <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(98,98,98)">6</span>
<span class="ansi-green-fg">---&gt; 98</span> encoder <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">target</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">input_vocab_size</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">d_model</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">n_encoder_layers</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">    100</span> lstms <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(175,0,0)">"</span><span style="font-weight:bold;color:rgb(175,95,0)">\n</span><span style="color:rgb(175,0,0)">"</span><span style="color:rgb(98,98,98)">.</span>join([<span style="color:rgb(175,0,0)">f</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">  LSTM_</span><span style="font-weight:bold;color:rgb(175,95,135)">{</span>d_model<span style="font-weight:bold;color:rgb(175,95,135)">}</span><span style="color:rgb(175,0,0)">'</span>] <span style="color:rgb(98,98,98)">*</span> n_encoder_layers)
<span class="ansi-green-fg ansi-bold">    102</span> expected <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(175,0,0)">f</span><span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">Serial[</span><span style="font-weight:bold;color:rgb(175,95,0)">\n</span><span style="color:rgb(175,0,0)">  Embedding_</span><span style="font-weight:bold;color:rgb(175,95,135)">{</span>input_vocab_size<span style="font-weight:bold;color:rgb(175,95,135)">}</span><span style="color:rgb(175,0,0)">_</span><span style="font-weight:bold;color:rgb(175,95,135)">{</span>d_model<span style="font-weight:bold;color:rgb(175,95,135)">}</span><span style="font-weight:bold;color:rgb(175,95,0)">\n</span><span style="font-weight:bold;color:rgb(175,95,135)">{</span>lstms<span style="font-weight:bold;color:rgb(175,95,135)">}</span><span style="font-weight:bold;color:rgb(175,95,0)">\n</span><span style="color:rgb(175,0,0)">]</span><span style="color:rgb(175,0,0)">"</span>

Cell <span class="ansi-green-fg">In[12], line 23</span>, in <span class="ansi-cyan-fg">input_encoder_fn</span><span class="ansi-blue-fg">(input_vocab_size, d_model, n_encoder_layers)</span>
<span class="ansi-green-fg ansi-bold">      4</span> <span style="font-style:italic;color:rgb(175,0,0)">""" Input encoder runs on the input sentence and creates</span>
<span class="ansi-green-fg ansi-bold">      5</span> <span style="font-style:italic;color:rgb(175,0,0)">activations that will be the keys and values for attention.</span>
<span class="ansi-green-fg ansi-bold">      6</span> 
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-fg ansi-bold">     12</span> <span style="font-style:italic;color:rgb(175,0,0)">    tl.Serial: The input encoder</span>
<span class="ansi-green-fg ansi-bold">     13</span> <span style="font-style:italic;color:rgb(175,0,0)">"""</span>
<span class="ansi-green-fg ansi-bold">     15</span> <span style="font-style:italic;color:rgb(95,135,135)"># create a serial network</span>
<span class="ansi-green-fg ansi-bold">     16</span> input_encoder <span style="color:rgb(98,98,98)">=</span> tl<span style="color:rgb(98,98,98)">.</span>Serial( 
<span class="ansi-green-fg ansi-bold">     17</span>     
<span class="ansi-green-fg ansi-bold">     18</span>     <span style="font-style:italic;color:rgb(95,135,135)">### START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) ###</span>
<span class="ansi-green-fg ansi-bold">     19</span>     <span style="font-style:italic;color:rgb(95,135,135)"># create an embedding layer to convert tokens to vectors</span>
<span class="ansi-green-fg ansi-bold">     20</span>     <span style="font-weight:bold;color:rgb(0,135,0)">None</span>,
<span class="ansi-green-fg ansi-bold">     21</span>     
<span class="ansi-green-fg ansi-bold">     22</span>     <span style="font-style:italic;color:rgb(95,135,135)"># feed the embeddings to the LSTM layers. It is a stack of n_encoder_layers LSTM layers</span>
<span class="ansi-green-fg">---&gt; 23</span>     [<span style="font-weight:bold;color:rgb(0,135,0)">None</span> <span style="font-weight:bold;color:rgb(0,135,0)">for</span> _ <span style="font-weight:bold;color:rgb(175,0,255)">in</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span>]
<span class="ansi-green-fg ansi-bold">     24</span>     <span style="font-style:italic;color:rgb(95,135,135)">### END CODE HERE ###</span>
<span class="ansi-green-fg ansi-bold">     25</span> )
<span class="ansi-green-fg ansi-bold">     27</span> <span style="font-weight:bold;color:rgb(0,135,0)">return</span> input_encoder

<span class="ansi-red-fg">TypeError</span>: 'NoneType' object is not iterable</pre>
</div>
</div>
</div>
</section>
<section id="pre-attention-decoder" class="level3">
<h3 class="anchored" data-anchor-id="pre-attention-decoder">2.2.2 Pre-attention decoder</h3>
<p>The pre-attention decoder runs on the targets and creates activations that are used as queries in attention. This is a Serial network which is composed of the following:</p>
<ul>
<li><p><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.ShiftRight">tl.ShiftRight</a>: This pads a token to the beginning of your target tokens (e.g.&nbsp;<code>[8, 34, 12]</code> shifted right is <code>[0, 8, 34, 12]</code>). This will act like a start-of-sentence token that will be the first input to the decoder. During training, this shift also allows the target tokens to be passed as input to do teacher forcing.</p></li>
<li><p><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding">tl.Embedding</a>: Like in the previous function, this converts each token to its vector representation. In this case, it is the the size of the vocabulary by the dimension of the model: <code>tl.Embedding(vocab_size, d_model)</code>. <code>vocab_size</code> is the number of entries in the given vocabulary. <code>d_model</code> is the number of elements in the word embedding.</p></li>
<li><p><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.LSTM">tl.LSTM</a>: LSTM layer of size <code>d_model</code>.</p></li>
</ul>
<p><img src="pre_attention_decoder.png"></p>
<p><a name="ex02"></a> ### Exercise 02</p>
<p><strong>Instructions:</strong> Implement the <code>pre_attention_decoder_fn</code> function.</p>
<div id="55467e15" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># UNQ_C2</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="co"># GRADED FUNCTION</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> pre_attention_decoder_fn(mode, target_vocab_size, d_model):</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" Pre-attention decoder runs on the targets and creates</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="co">    activations that are used as queries in attention.</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="co">        mode: str: 'train' or 'eval'</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="co">        target_vocab_size: int: vocab size of the target</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="co">        d_model: int:  depth of embedding (n_units in the LSTM cell)</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="co">        tl.Serial: The pre-attention decoder</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># create a serial network</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>    pre_attention_decoder <span class="op">=</span> tl.Serial(</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>        <span class="co">### START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) </span><span class="al">###</span></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># shift right to insert start-of-sentence token and implement</span></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># teacher forcing during training</span></span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">None</span>,</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># run an embedding layer to convert tokens to vectors</span></span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">None</span>,</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># feed to an LSTM layer</span></span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">None</span></span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>        <span class="co">### </span><span class="re">END</span><span class="co"> CODE HERE </span><span class="al">###</span></span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pre_attention_decoder</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="67ae9c38" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="re">BEGIN</span><span class="co"> UNIT </span><span class="al">TEST</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>w1_unittest.test_pre_attention_decoder_fn(pre_attention_decoder_fn)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="re">END</span><span class="co"> UNIT </span><span class="al">TEST</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/combinators.py:437: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if self._mode == 'predict' and self._state[1] is not ():  # pylint: disable=literal-comparison
/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/combinators.py:910: SyntaxWarning: "is" with a literal. Did you mean "=="?
  if state[0] is ():  # pylint: disable=literal-comparison
/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/combinators.py:437: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if self._mode == 'predict' and self._state[1] is not ():  # pylint: disable=literal-comparison
/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/combinators.py:910: SyntaxWarning: "is" with a literal. Did you mean "=="?
  if state[0] is ():  # pylint: disable=literal-comparison</code></pre>
</div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">ValueError</span>                                Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[15], line 3</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># BEGIN UNIT TEST</span>
<span class="ansi-green-fg">----&gt; 3</span> <span class="ansi-yellow-bg">w1_unittest</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">test_pre_attention_decoder_fn</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">pre_attention_decoder_fn</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">      5</span> <span style="font-style:italic;color:rgb(95,135,135)"># END UNIT TEST</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/posts/c4w1/w1_unittest.py:148</span>, in <span class="ansi-cyan-fg">test_pre_attention_decoder_fn</span><span class="ansi-blue-fg">(pre_attention_decoder_fn)</span>
<span class="ansi-green-fg ansi-bold">    145</span> target_vocab_size <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(98,98,98)">10</span>
<span class="ansi-green-fg ansi-bold">    146</span> d_model <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(98,98,98)">2</span>
<span class="ansi-green-fg">--&gt; 148</span> decoder <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">target</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">mode</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">target_vocab_size</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">d_model</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">    150</span> expected <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(175,0,0)">f</span><span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">Serial[</span><span style="font-weight:bold;color:rgb(175,95,0)">\n</span><span style="color:rgb(175,0,0)">  ShiftRight(1)</span><span style="font-weight:bold;color:rgb(175,95,0)">\n</span><span style="color:rgb(175,0,0)">  Embedding_</span><span style="font-weight:bold;color:rgb(175,95,135)">{</span>target_vocab_size<span style="font-weight:bold;color:rgb(175,95,135)">}</span><span style="color:rgb(175,0,0)">_</span><span style="font-weight:bold;color:rgb(175,95,135)">{</span>d_model<span style="font-weight:bold;color:rgb(175,95,135)">}</span><span style="font-weight:bold;color:rgb(175,95,0)">\n</span><span style="color:rgb(175,0,0)">  LSTM_</span><span style="font-weight:bold;color:rgb(175,95,135)">{</span>d_model<span style="font-weight:bold;color:rgb(175,95,135)">}</span><span style="font-weight:bold;color:rgb(175,95,0)">\n</span><span style="color:rgb(175,0,0)">]</span><span style="color:rgb(175,0,0)">"</span>
<span class="ansi-green-fg ansi-bold">    152</span> proposed <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)">str</span>(decoder)

Cell <span class="ansi-green-fg">In[14], line 16</span>, in <span class="ansi-cyan-fg">pre_attention_decoder_fn</span><span class="ansi-blue-fg">(mode, target_vocab_size, d_model)</span>
<span class="ansi-green-fg ansi-bold">      4</span> <span style="font-style:italic;color:rgb(175,0,0)">""" Pre-attention decoder runs on the targets and creates</span>
<span class="ansi-green-fg ansi-bold">      5</span> <span style="font-style:italic;color:rgb(175,0,0)">activations that are used as queries in attention.</span>
<span class="ansi-green-fg ansi-bold">      6</span> 
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-fg ansi-bold">     12</span> <span style="font-style:italic;color:rgb(175,0,0)">    tl.Serial: The pre-attention decoder</span>
<span class="ansi-green-fg ansi-bold">     13</span> <span style="font-style:italic;color:rgb(175,0,0)">"""</span>
<span class="ansi-green-fg ansi-bold">     15</span> <span style="font-style:italic;color:rgb(95,135,135)"># create a serial network</span>
<span class="ansi-green-fg">---&gt; 16</span> pre_attention_decoder <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">tl</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">Serial</span><span class="ansi-yellow-bg">(</span>
<span class="ansi-green-fg ansi-bold">     17</span> <span class="ansi-yellow-bg">    </span>
<span class="ansi-green-fg ansi-bold">     18</span> <span class="ansi-yellow-bg">    </span><span style="font-style:italic;color:rgb(95,135,135)" class="ansi-yellow-bg">### START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) ###</span>
<span class="ansi-green-fg ansi-bold">     19</span> <span class="ansi-yellow-bg">    </span><span style="font-style:italic;color:rgb(95,135,135)" class="ansi-yellow-bg"># shift right to insert start-of-sentence token and implement</span>
<span class="ansi-green-fg ansi-bold">     20</span> <span class="ansi-yellow-bg">    </span><span style="font-style:italic;color:rgb(95,135,135)" class="ansi-yellow-bg"># teacher forcing during training</span>
<span class="ansi-green-fg ansi-bold">     21</span> <span class="ansi-yellow-bg">    </span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">None</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">     22</span> 
<span class="ansi-green-fg ansi-bold">     23</span> <span class="ansi-yellow-bg">    </span><span style="font-style:italic;color:rgb(95,135,135)" class="ansi-yellow-bg"># run an embedding layer to convert tokens to vectors</span>
<span class="ansi-green-fg ansi-bold">     24</span> <span class="ansi-yellow-bg">    </span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">None</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">     25</span> 
<span class="ansi-green-fg ansi-bold">     26</span> <span class="ansi-yellow-bg">    </span><span style="font-style:italic;color:rgb(95,135,135)" class="ansi-yellow-bg"># feed to an LSTM layer</span>
<span class="ansi-green-fg ansi-bold">     27</span> <span class="ansi-yellow-bg">    </span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">None</span>
<span class="ansi-green-fg ansi-bold">     28</span> <span class="ansi-yellow-bg">    </span><span style="font-style:italic;color:rgb(95,135,135)" class="ansi-yellow-bg">### END CODE HERE ###</span>
<span class="ansi-green-fg ansi-bold">     29</span> <span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">     31</span> <span style="font-weight:bold;color:rgb(0,135,0)">return</span> pre_attention_decoder

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/combinators.py:59</span>, in <span class="ansi-cyan-fg">Serial.__init__</span><span class="ansi-blue-fg">(self, name, sublayers_to_print, *sublayers)</span>
<span class="ansi-green-fg ansi-bold">     55</span> <span style="font-weight:bold;color:rgb(0,135,0)">def</span><span style="color:rgb(188,188,188)"> </span><span style="color:rgb(0,0,255)">__init__</span>(<span style="color:rgb(0,135,0)">self</span>, <span style="color:rgb(98,98,98)">*</span>sublayers, name<span style="color:rgb(98,98,98)">=</span><span style="font-weight:bold;color:rgb(0,135,0)">None</span>, sublayers_to_print<span style="color:rgb(98,98,98)">=</span><span style="font-weight:bold;color:rgb(0,135,0)">None</span>):
<span class="ansi-green-fg ansi-bold">     56</span>   <span style="color:rgb(0,135,0)">super</span>()<span style="color:rgb(98,98,98)">.</span><span style="color:rgb(0,0,255)">__init__</span>(
<span class="ansi-green-fg ansi-bold">     57</span>       name<span style="color:rgb(98,98,98)">=</span>name, sublayers_to_print<span style="color:rgb(98,98,98)">=</span>sublayers_to_print)
<span class="ansi-green-fg">---&gt; 59</span>   sublayers <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">_ensure_flat</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">sublayers</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">     60</span>   <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_sublayers <span style="color:rgb(98,98,98)">=</span> sublayers
<span class="ansi-green-fg ansi-bold">     61</span>   <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_n_layers <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)">len</span>(sublayers)

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/combinators.py:1110</span>, in <span class="ansi-cyan-fg">_ensure_flat</span><span class="ansi-blue-fg">(layers)</span>
<span class="ansi-green-fg ansi-bold">   1108</span> <span style="font-weight:bold;color:rgb(0,135,0)">for</span> obj <span style="font-weight:bold;color:rgb(175,0,255)">in</span> layers:
<span class="ansi-green-fg ansi-bold">   1109</span>   <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="font-weight:bold;color:rgb(175,0,255)">not</span> <span style="color:rgb(0,135,0)">isinstance</span>(obj, base<span style="color:rgb(98,98,98)">.</span>Layer):
<span class="ansi-green-fg">-&gt; 1110</span>     <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> <span style="font-weight:bold;color:rgb(215,95,95)">ValueError</span>(
<span class="ansi-green-fg ansi-bold">   1111</span>         <span style="color:rgb(175,0,0)">f</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">Found nonlayer object (</span><span style="font-weight:bold;color:rgb(175,95,135)">{</span>obj<span style="font-weight:bold;color:rgb(175,95,135)">}</span><span style="color:rgb(175,0,0)">) in layers: </span><span style="font-weight:bold;color:rgb(175,95,135)">{</span>layers<span style="font-weight:bold;color:rgb(175,95,135)">}</span><span style="color:rgb(175,0,0)">'</span>)
<span class="ansi-green-fg ansi-bold">   1112</span> <span style="font-weight:bold;color:rgb(0,135,0)">return</span> layers

<span class="ansi-red-fg">ValueError</span>: Found nonlayer object (None) in layers: [None, None, None]</pre>
</div>
</div>
</div>
</section>
<section id="preparing-the-attention-input" class="level3">
<h3 class="anchored" data-anchor-id="preparing-the-attention-input">2.2.3 Preparing the attention input</h3>
<p>This function will prepare the inputs to the attention layer. We want to take in the encoder and pre-attention decoder activations and assign it to the queries, keys, and values. In addition, another output here will be the mask to distinguish real tokens from padding tokens. This mask will be used internally by Trax when computing the softmax so padding tokens will not have an effect on the computated probabilities. From the data preparation steps in Section 1 of this assignment, you should know which tokens in the input correspond to padding.</p>
<p>We have filled the last two lines in composing the mask for you because it includes a concept that will be discussed further next week. This is related to <em>multiheaded attention</em> which you can think of right now as computing the attention multiple times to improve the model’s predictions. It is required to consider this additional axis in the output so we’ve included it already but you don’t need to analyze it just yet. What’s important now is for you to know which should be the queries, keys, and values, as well as to initialize the mask.</p>
<p><a name="ex03"></a> ### Exercise 03</p>
<p><strong>Instructions:</strong> Implement the <code>prepare_attention_input</code> function</p>
<div id="78e6be08" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># UNQ_C3</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="co"># GRADED FUNCTION</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> prepare_attention_input(encoder_activations, decoder_activations, inputs):</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Prepare queries, keys, values and mask for attention.</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="co">        encoder_activations fastnp.array(batch_size, padded_input_length, d_model): output from the input encoder</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="co">        decoder_activations fastnp.array(batch_size, padded_input_length, d_model): output from the pre-attention decoder</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="co">        inputs fastnp.array(batch_size, padded_input_length): padded input tokens</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a><span class="co">        queries, keys, values and mask for attention.</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>    <span class="co">### START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) </span><span class="al">###</span></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># set the keys and values to the encoder activations</span></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>    keys <span class="op">=</span> <span class="va">None</span></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>    values <span class="op">=</span> <span class="va">None</span></span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># set the queries to the decoder activations</span></span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>    queries <span class="op">=</span> <span class="va">None</span></span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># generate the mask to distinguish real tokens from padding</span></span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># hint: inputs is 1 for real tokens and 0 where they are padding</span></span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> <span class="va">None</span></span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>    <span class="co">### </span><span class="re">END</span><span class="co"> CODE HERE </span><span class="al">###</span></span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># add axes to the mask for attention heads and decoder length.</span></span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> fastnp.reshape(mask, (mask.shape[<span class="dv">0</span>], <span class="dv">1</span>, <span class="dv">1</span>, mask.shape[<span class="dv">1</span>]))</span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># broadcast so mask shape is [batch size, attention heads, decoder-len, encoder-len].</span></span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># note: for this assignment, attention heads is set to 1.</span></span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> mask <span class="op">+</span> fastnp.zeros((<span class="dv">1</span>, <span class="dv">1</span>, decoder_activations.shape[<span class="dv">1</span>], <span class="dv">1</span>))</span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> queries, keys, values, mask</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="f336b32a" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="re">BEGIN</span><span class="co"> UNIT </span><span class="al">TEST</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>w1_unittest.test_prepare_attention_input(prepare_attention_input)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="re">END</span><span class="co"> UNIT </span><span class="al">TEST</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">AttributeError</span>                            Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[17], line 2</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># BEGIN UNIT TEST</span>
<span class="ansi-green-fg">----&gt; 2</span> <span class="ansi-yellow-bg">w1_unittest</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">test_prepare_attention_input</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">prepare_attention_input</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">      3</span> <span style="font-style:italic;color:rgb(95,135,135)"># END UNIT TEST</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/posts/c4w1/w1_unittest.py:205</span>, in <span class="ansi-cyan-fg">test_prepare_attention_input</span><span class="ansi-blue-fg">(prepare_attention_input)</span>
<span class="ansi-green-fg ansi-bold">    200</span> exp_mask <span style="color:rgb(98,98,98)">=</span> fastnp<span style="color:rgb(98,98,98)">.</span>array([[[[<span style="color:rgb(98,98,98)">1.</span>, <span style="color:rgb(98,98,98)">1.</span>, <span style="color:rgb(98,98,98)">1.</span>], [<span style="color:rgb(98,98,98)">1.</span>, <span style="color:rgb(98,98,98)">1.</span>, <span style="color:rgb(98,98,98)">1.</span>], [<span style="color:rgb(98,98,98)">1.</span>, <span style="color:rgb(98,98,98)">1.</span>, <span style="color:rgb(98,98,98)">1.</span>]]], 
<span class="ansi-green-fg ansi-bold">    201</span>                          [[[<span style="color:rgb(98,98,98)">1.</span>, <span style="color:rgb(98,98,98)">1.</span>, <span style="color:rgb(98,98,98)">0.</span>], [<span style="color:rgb(98,98,98)">1.</span>, <span style="color:rgb(98,98,98)">1.</span>, <span style="color:rgb(98,98,98)">0.</span>], [<span style="color:rgb(98,98,98)">1.</span>, <span style="color:rgb(98,98,98)">1.</span>, <span style="color:rgb(98,98,98)">0.</span>]]]])
<span class="ansi-green-fg ansi-bold">    203</span> exp_type <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)">type</span>(enc_act)
<span class="ansi-green-fg">--&gt; 205</span> queries, keys, values, mask <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">target</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">enc_act</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">dec_act</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">inputs</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">    207</span> <span style="font-weight:bold;color:rgb(0,135,0)">try</span>:
<span class="ansi-green-fg ansi-bold">    208</span>     <span style="font-weight:bold;color:rgb(0,135,0)">assert</span>(fastnp<span style="color:rgb(98,98,98)">.</span>allclose(queries, dec_act))

Cell <span class="ansi-green-fg">In[16], line 32</span>, in <span class="ansi-cyan-fg">prepare_attention_input</span><span class="ansi-blue-fg">(encoder_activations, decoder_activations, inputs)</span>
<span class="ansi-green-fg ansi-bold">     27</span> mask <span style="color:rgb(98,98,98)">=</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span>
<span class="ansi-green-fg ansi-bold">     29</span> <span style="font-style:italic;color:rgb(95,135,135)">### END CODE HERE ###</span>
<span class="ansi-green-fg ansi-bold">     30</span> 
<span class="ansi-green-fg ansi-bold">     31</span> <span style="font-style:italic;color:rgb(95,135,135)"># add axes to the mask for attention heads and decoder length.</span>
<span class="ansi-green-fg">---&gt; 32</span> mask <span style="color:rgb(98,98,98)">=</span> fastnp<span style="color:rgb(98,98,98)">.</span>reshape(mask, (<span class="ansi-yellow-bg">mask</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">shape</span>[<span style="color:rgb(98,98,98)">0</span>], <span style="color:rgb(98,98,98)">1</span>, <span style="color:rgb(98,98,98)">1</span>, mask<span style="color:rgb(98,98,98)">.</span>shape[<span style="color:rgb(98,98,98)">1</span>]))
<span class="ansi-green-fg ansi-bold">     34</span> <span style="font-style:italic;color:rgb(95,135,135)"># broadcast so mask shape is [batch size, attention heads, decoder-len, encoder-len].</span>
<span class="ansi-green-fg ansi-bold">     35</span> <span style="font-style:italic;color:rgb(95,135,135)"># note: for this assignment, attention heads is set to 1.</span>
<span class="ansi-green-fg ansi-bold">     36</span> mask <span style="color:rgb(98,98,98)">=</span> mask <span style="color:rgb(98,98,98)">+</span> fastnp<span style="color:rgb(98,98,98)">.</span>zeros((<span style="color:rgb(98,98,98)">1</span>, <span style="color:rgb(98,98,98)">1</span>, decoder_activations<span style="color:rgb(98,98,98)">.</span>shape[<span style="color:rgb(98,98,98)">1</span>], <span style="color:rgb(98,98,98)">1</span>))

<span class="ansi-red-fg">AttributeError</span>: 'NoneType' object has no attribute 'shape'</pre>
</div>
</div>
</div>
<p><a name="2.3"></a> ## 2.3 Implementation Overview</p>
<p>We are now ready to implement our sequence-to-sequence model with attention. This will be a Serial network and is illustrated in the diagram below. It shows the layers you’ll be using in Trax and you’ll see that each step can be implemented quite easily with one line commands. We’ve placed several links to the documentation for each relevant layer in the discussion after the figure below.</p>
<p><img src="NMTModel.png"></p>
<p><a name="ex04"></a> ### Exercise 04 <strong>Instructions:</strong> Implement the <code>NMTAttn</code> function below to define your machine translation model which uses attention. We have left hyperlinks below pointing to the Trax documentation of the relevant layers. Remember to consult it to get tips on what parameters to pass.</p>
<p><strong>Step 0:</strong> Prepare the input encoder and pre-attention decoder branches. You have already defined this earlier as helper functions so it’s just a matter of calling those functions and assigning it to variables.</p>
<p><strong>Step 1:</strong> Create a Serial network. This will stack the layers in the next steps one after the other. Like the earlier exercises, you can use <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Serial">tl.Serial</a>.</p>
<p><strong>Step 2:</strong> Make a copy of the input and target tokens. As you see in the diagram above, the input and target tokens will be fed into different layers of the model. You can use <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Select">tl.Select</a> layer to create copies of these tokens. Arrange them as <code>[input tokens, target tokens, input tokens, target tokens]</code>.</p>
<p><strong>Step 3:</strong> Create a parallel branch to feed the input tokens to the <code>input_encoder</code> and the target tokens to the <code>pre_attention_decoder</code>. You can use <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Parallel">tl.Parallel</a> to create these sublayers in parallel. Remember to pass the variables you defined in Step 0 as parameters to this layer.</p>
<p><strong>Step 4:</strong> Next, call the <code>prepare_attention_input</code> function to convert the encoder and pre-attention decoder activations to a format that the attention layer will accept. You can use <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn">tl.Fn</a> to call this function. Note: Pass the <code>prepare_attention_input</code> function as the <code>f</code> parameter in <code>tl.Fn</code> without any arguments or parenthesis.</p>
<p><strong>Step 5:</strong> We will now feed the (queries, keys, values, and mask) to the <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.AttentionQKV">tl.AttentionQKV</a> layer. This computes the scaled dot product attention and outputs the attention weights and mask. Take note that although it is a one liner, this layer is actually composed of a deep network made up of several branches. We’ll show the implementation taken <a href="https://github.com/google/trax/blob/master/trax/layers/attention.py#L61">here</a> to see the different layers used.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> AttentionQKV(d_feature, n_heads<span class="op">=</span><span class="dv">1</span>, dropout<span class="op">=</span><span class="fl">0.0</span>, mode<span class="op">=</span><span class="st">'train'</span>):</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>  <span class="co">"""Returns a layer that maps (q, k, v, mask) to (activations, mask).</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="co">  See `Attention` above for further context/details.</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="co">  Args:</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="co">    d_feature: Depth/dimensionality of feature embedding.</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="co">    n_heads: Number of attention heads.</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="co">    dropout: Probababilistic rate for internal dropout applied to attention</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a><span class="co">        activations (based on query-key pairs) before dotting them with values.</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a><span class="co">    mode: Either 'train' or 'eval'.</span></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a><span class="co">  """</span></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> cb.Serial(</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>      cb.Parallel(</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>          core.Dense(d_feature),</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>          core.Dense(d_feature),</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>          core.Dense(d_feature),</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>      ),</span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>      PureAttention(  <span class="co"># pylint: disable=no-value-for-parameter</span></span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>          n_heads<span class="op">=</span>n_heads, dropout<span class="op">=</span>dropout, mode<span class="op">=</span>mode),</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>      core.Dense(d_feature),</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Having deep layers pose the risk of vanishing gradients during training and we would want to mitigate that. To improve the ability of the network to learn, we can insert a <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Residual">tl.Residual</a> layer to add the output of AttentionQKV with the <code>queries</code> input. You can do this in trax by simply nesting the <code>AttentionQKV</code> layer inside the <code>Residual</code> layer. The library will take care of branching and adding for you.</p>
<p><strong>Step 6:</strong> We will not need the mask for the model we’re building so we can safely drop it. At this point in the network, the signal stack currently has <code>[attention activations, mask, target tokens]</code> and you can use <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Select">tl.Select</a> to output just <code>[attention activations, target tokens]</code>.</p>
<p><strong>Step 7:</strong> We can now feed the attention weighted output to the LSTM decoder. We can stack multiple <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.LSTM">tl.LSTM</a> layers to improve the output so remember to append LSTMs equal to the number defined by <code>n_decoder_layers</code> parameter to the model.</p>
<p><strong>Step 8:</strong> We want to determine the probabilities of each subword in the vocabulary and you can set this up easily with a <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense">tl.Dense</a> layer by making its size equal to the size of our vocabulary.</p>
<p><strong>Step 9:</strong> Normalize the output to log probabilities by passing the activations in Step 8 to a <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.LogSoftmax">tl.LogSoftmax</a> layer.</p>
<div id="74a19992" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># UNQ_C4</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="co"># GRADED FUNCTION</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> NMTAttn(input_vocab_size<span class="op">=</span><span class="dv">33300</span>,</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>            target_vocab_size<span class="op">=</span><span class="dv">33300</span>,</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>            d_model<span class="op">=</span><span class="dv">1024</span>,</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>            n_encoder_layers<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>            n_decoder_layers<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>            n_attention_heads<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>            attention_dropout<span class="op">=</span><span class="fl">0.0</span>,</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>            mode<span class="op">=</span><span class="st">'train'</span>):</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Returns an LSTM sequence-to-sequence model with attention.</span></span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a><span class="co">    The input to the model is a pair (input tokens, target tokens), e.g.,</span></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a><span class="co">    an English sentence (tokenized) and its translation into German (tokenized).</span></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a><span class="co">    input_vocab_size: int: vocab size of the input</span></span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a><span class="co">    target_vocab_size: int: vocab size of the target</span></span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a><span class="co">    d_model: int:  depth of embedding (n_units in the LSTM cell)</span></span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a><span class="co">    n_encoder_layers: int: number of LSTM layers in the encoder</span></span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a><span class="co">    n_decoder_layers: int: number of LSTM layers in the decoder after attention</span></span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a><span class="co">    n_attention_heads: int: number of attention heads</span></span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a><span class="co">    attention_dropout: float, dropout for the attention layer</span></span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a><span class="co">    mode: str: 'train', 'eval' or 'predict', predict mode is for fast inference</span></span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a><span class="co">    A LSTM sequence-to-sequence model with attention.</span></span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a>    <span class="co">### START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) </span><span class="al">###</span></span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 0: call the helper function to create layers for the input encoder</span></span>
<span id="cb23-33"><a href="#cb23-33" aria-hidden="true" tabindex="-1"></a>    input_encoder <span class="op">=</span> <span class="va">None</span></span>
<span id="cb23-34"><a href="#cb23-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-35"><a href="#cb23-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 0: call the helper function to create layers for the pre-attention decoder</span></span>
<span id="cb23-36"><a href="#cb23-36" aria-hidden="true" tabindex="-1"></a>    pre_attention_decoder <span class="op">=</span> <span class="va">None</span></span>
<span id="cb23-37"><a href="#cb23-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-38"><a href="#cb23-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: create a serial network</span></span>
<span id="cb23-39"><a href="#cb23-39" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> tl.Serial( </span>
<span id="cb23-40"><a href="#cb23-40" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-41"><a href="#cb23-41" aria-hidden="true" tabindex="-1"></a>      <span class="co"># Step 2: copy input tokens and target tokens as they will be needed later.</span></span>
<span id="cb23-42"><a href="#cb23-42" aria-hidden="true" tabindex="-1"></a>      <span class="va">None</span>,</span>
<span id="cb23-43"><a href="#cb23-43" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-44"><a href="#cb23-44" aria-hidden="true" tabindex="-1"></a>      <span class="co"># Step 3: run input encoder on the input and pre-attention decoder the target.</span></span>
<span id="cb23-45"><a href="#cb23-45" aria-hidden="true" tabindex="-1"></a>      <span class="va">None</span>(<span class="va">None</span>, <span class="va">None</span>),</span>
<span id="cb23-46"><a href="#cb23-46" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-47"><a href="#cb23-47" aria-hidden="true" tabindex="-1"></a>      <span class="co"># Step 4: prepare queries, keys, values and mask for attention.</span></span>
<span id="cb23-48"><a href="#cb23-48" aria-hidden="true" tabindex="-1"></a>      <span class="va">None</span>(<span class="st">'PrepareAttentionInput'</span>, <span class="va">None</span>, n_out<span class="op">=</span><span class="dv">4</span>),</span>
<span id="cb23-49"><a href="#cb23-49" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-50"><a href="#cb23-50" aria-hidden="true" tabindex="-1"></a>      <span class="co"># Step 5: run the AttentionQKV layer</span></span>
<span id="cb23-51"><a href="#cb23-51" aria-hidden="true" tabindex="-1"></a>      <span class="co"># nest it inside a Residual layer to add to the pre-attention decoder activations(i.e. queries)</span></span>
<span id="cb23-52"><a href="#cb23-52" aria-hidden="true" tabindex="-1"></a>      tl.Residual(tl.AttentionQKV(<span class="va">None</span>, n_heads<span class="op">=</span>n_attention_heads, dropout<span class="op">=</span>attention_dropout, mode<span class="op">=</span><span class="va">None</span>)),</span>
<span id="cb23-53"><a href="#cb23-53" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb23-54"><a href="#cb23-54" aria-hidden="true" tabindex="-1"></a>      <span class="co"># Step 6: drop attention mask (i.e. index = None</span></span>
<span id="cb23-55"><a href="#cb23-55" aria-hidden="true" tabindex="-1"></a>      <span class="va">None</span>,</span>
<span id="cb23-56"><a href="#cb23-56" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-57"><a href="#cb23-57" aria-hidden="true" tabindex="-1"></a>      <span class="co"># Step 7: run the rest of the RNN decoder</span></span>
<span id="cb23-58"><a href="#cb23-58" aria-hidden="true" tabindex="-1"></a>      [<span class="va">None</span> <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="va">None</span>)],</span>
<span id="cb23-59"><a href="#cb23-59" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-60"><a href="#cb23-60" aria-hidden="true" tabindex="-1"></a>      <span class="co"># Step 8: prepare output by making it the right size</span></span>
<span id="cb23-61"><a href="#cb23-61" aria-hidden="true" tabindex="-1"></a>      <span class="va">None</span>(<span class="va">None</span>),</span>
<span id="cb23-62"><a href="#cb23-62" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb23-63"><a href="#cb23-63" aria-hidden="true" tabindex="-1"></a>      <span class="co"># Step 9: Log-softmax for output</span></span>
<span id="cb23-64"><a href="#cb23-64" aria-hidden="true" tabindex="-1"></a>      <span class="va">None</span></span>
<span id="cb23-65"><a href="#cb23-65" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb23-66"><a href="#cb23-66" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-67"><a href="#cb23-67" aria-hidden="true" tabindex="-1"></a>    <span class="co">### </span><span class="re">END</span><span class="co"> CODE HERE</span></span>
<span id="cb23-68"><a href="#cb23-68" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-69"><a href="#cb23-69" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>&lt;&gt;:45: SyntaxWarning: 'NoneType' object is not callable; perhaps you missed a comma?
&lt;&gt;:48: SyntaxWarning: 'NoneType' object is not callable; perhaps you missed a comma?
&lt;&gt;:61: SyntaxWarning: 'NoneType' object is not callable; perhaps you missed a comma?
&lt;&gt;:45: SyntaxWarning: 'NoneType' object is not callable; perhaps you missed a comma?
&lt;&gt;:48: SyntaxWarning: 'NoneType' object is not callable; perhaps you missed a comma?
&lt;&gt;:61: SyntaxWarning: 'NoneType' object is not callable; perhaps you missed a comma?
/tmp/ipykernel_1093516/554596082.py:45: SyntaxWarning: 'NoneType' object is not callable; perhaps you missed a comma?
  None(None, None),
/tmp/ipykernel_1093516/554596082.py:48: SyntaxWarning: 'NoneType' object is not callable; perhaps you missed a comma?
  None('PrepareAttentionInput', None, n_out=4),
/tmp/ipykernel_1093516/554596082.py:61: SyntaxWarning: 'NoneType' object is not callable; perhaps you missed a comma?
  None(None),</code></pre>
</div>
</div>
<div id="26569cd9" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="re">BEGIN</span><span class="co"> UNIT </span><span class="al">TEST</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>w1_unittest.test_NMTAttn(NMTAttn)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="re">END</span><span class="co"> UNIT </span><span class="al">TEST</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The NMTAttn is not defined properly.
We found {} layers in your model. It should be 9.
Check the LSTM stack before the dense layer
Look at your selection layers.
 0  Tests passed
 3  Tests failed</code></pre>
</div>
</div>
<div id="d034a0cd" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># print your model</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> NMTAttn()</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_1093516/554596082.py:45: SyntaxWarning: 'NoneType' object is not callable; perhaps you missed a comma?
  None(None, None),
/tmp/ipykernel_1093516/554596082.py:48: SyntaxWarning: 'NoneType' object is not callable; perhaps you missed a comma?
  None('PrepareAttentionInput', None, n_out=4),
/tmp/ipykernel_1093516/554596082.py:61: SyntaxWarning: 'NoneType' object is not callable; perhaps you missed a comma?
  None(None),
/tmp/ipykernel_1093516/554596082.py:45: SyntaxWarning: 'NoneType' object is not callable; perhaps you missed a comma?
  None(None, None),
/tmp/ipykernel_1093516/554596082.py:48: SyntaxWarning: 'NoneType' object is not callable; perhaps you missed a comma?
  None('PrepareAttentionInput', None, n_out=4),
/tmp/ipykernel_1093516/554596082.py:61: SyntaxWarning: 'NoneType' object is not callable; perhaps you missed a comma?
  None(None),
/tmp/ipykernel_1093516/554596082.py:45: SyntaxWarning: 'NoneType' object is not callable; perhaps you missed a comma?
  None(None, None),
/tmp/ipykernel_1093516/554596082.py:48: SyntaxWarning: 'NoneType' object is not callable; perhaps you missed a comma?
  None('PrepareAttentionInput', None, n_out=4),
/tmp/ipykernel_1093516/554596082.py:61: SyntaxWarning: 'NoneType' object is not callable; perhaps you missed a comma?
  None(None),
/tmp/ipykernel_1093516/554596082.py:45: SyntaxWarning: 'NoneType' object is not callable; perhaps you missed a comma?
  None(None, None),
/tmp/ipykernel_1093516/554596082.py:48: SyntaxWarning: 'NoneType' object is not callable; perhaps you missed a comma?
  None('PrepareAttentionInput', None, n_out=4),
/tmp/ipykernel_1093516/554596082.py:61: SyntaxWarning: 'NoneType' object is not callable; perhaps you missed a comma?
  None(None),
/tmp/ipykernel_1093516/554596082.py:45: SyntaxWarning: 'NoneType' object is not callable; perhaps you missed a comma?
  None(None, None),
/tmp/ipykernel_1093516/554596082.py:48: SyntaxWarning: 'NoneType' object is not callable; perhaps you missed a comma?
  None('PrepareAttentionInput', None, n_out=4),
/tmp/ipykernel_1093516/554596082.py:61: SyntaxWarning: 'NoneType' object is not callable; perhaps you missed a comma?
  None(None),
/tmp/ipykernel_1093516/554596082.py:45: SyntaxWarning: 'NoneType' object is not callable; perhaps you missed a comma?
  None(None, None),
/tmp/ipykernel_1093516/554596082.py:48: SyntaxWarning: 'NoneType' object is not callable; perhaps you missed a comma?
  None('PrepareAttentionInput', None, n_out=4),
/tmp/ipykernel_1093516/554596082.py:61: SyntaxWarning: 'NoneType' object is not callable; perhaps you missed a comma?
  None(None),
/tmp/ipykernel_1093516/554596082.py:45: SyntaxWarning: 'NoneType' object is not callable; perhaps you missed a comma?
  None(None, None),
/tmp/ipykernel_1093516/554596082.py:48: SyntaxWarning: 'NoneType' object is not callable; perhaps you missed a comma?
  None('PrepareAttentionInput', None, n_out=4),
/tmp/ipykernel_1093516/554596082.py:61: SyntaxWarning: 'NoneType' object is not callable; perhaps you missed a comma?
  None(None),
/tmp/ipykernel_1093516/554596082.py:45: SyntaxWarning: 'NoneType' object is not callable; perhaps you missed a comma?
  None(None, None),
/tmp/ipykernel_1093516/554596082.py:48: SyntaxWarning: 'NoneType' object is not callable; perhaps you missed a comma?
  None('PrepareAttentionInput', None, n_out=4),
/tmp/ipykernel_1093516/554596082.py:61: SyntaxWarning: 'NoneType' object is not callable; perhaps you missed a comma?
  None(None),</code></pre>
</div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">TypeError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[20], line 2</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># print your model</span>
<span class="ansi-green-fg">----&gt; 2</span> model <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">NMTAttn</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">      3</span> <span style="color:rgb(0,135,0)">print</span>(model)

Cell <span class="ansi-green-fg">In[18], line 45</span>, in <span class="ansi-cyan-fg">NMTAttn</span><span class="ansi-blue-fg">(input_vocab_size, target_vocab_size, d_model, n_encoder_layers, n_decoder_layers, n_attention_heads, attention_dropout, mode)</span>
<span class="ansi-green-fg ansi-bold">     36</span> pre_attention_decoder <span style="color:rgb(98,98,98)">=</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span>
<span class="ansi-green-fg ansi-bold">     38</span> <span style="font-style:italic;color:rgb(95,135,135)"># Step 1: create a serial network</span>
<span class="ansi-green-fg ansi-bold">     39</span> model <span style="color:rgb(98,98,98)">=</span> tl<span style="color:rgb(98,98,98)">.</span>Serial( 
<span class="ansi-green-fg ansi-bold">     40</span>     
<span class="ansi-green-fg ansi-bold">     41</span>   <span style="font-style:italic;color:rgb(95,135,135)"># Step 2: copy input tokens and target tokens as they will be needed later.</span>
<span class="ansi-green-fg ansi-bold">     42</span>   <span style="font-weight:bold;color:rgb(0,135,0)">None</span>,
<span class="ansi-green-fg ansi-bold">     43</span>     
<span class="ansi-green-fg ansi-bold">     44</span>   <span style="font-style:italic;color:rgb(95,135,135)"># Step 3: run input encoder on the input and pre-attention decoder the target.</span>
<span class="ansi-green-fg">---&gt; 45</span>   <span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">None</span><span class="ansi-yellow-bg">(</span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">None</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">None</span><span class="ansi-yellow-bg">)</span>,
<span class="ansi-green-fg ansi-bold">     46</span>     
<span class="ansi-green-fg ansi-bold">     47</span>   <span style="font-style:italic;color:rgb(95,135,135)"># Step 4: prepare queries, keys, values and mask for attention.</span>
<span class="ansi-green-fg ansi-bold">     48</span>   <span style="font-weight:bold;color:rgb(0,135,0)">None</span>(<span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">PrepareAttentionInput</span><span style="color:rgb(175,0,0)">'</span>, <span style="font-weight:bold;color:rgb(0,135,0)">None</span>, n_out<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">4</span>),
<span class="ansi-green-fg ansi-bold">     49</span>     
<span class="ansi-green-fg ansi-bold">     50</span>   <span style="font-style:italic;color:rgb(95,135,135)"># Step 5: run the AttentionQKV layer</span>
<span class="ansi-green-fg ansi-bold">     51</span>   <span style="font-style:italic;color:rgb(95,135,135)"># nest it inside a Residual layer to add to the pre-attention decoder activations(i.e. queries)</span>
<span class="ansi-green-fg ansi-bold">     52</span>   tl<span style="color:rgb(98,98,98)">.</span>Residual(tl<span style="color:rgb(98,98,98)">.</span>AttentionQKV(<span style="font-weight:bold;color:rgb(0,135,0)">None</span>, n_heads<span style="color:rgb(98,98,98)">=</span>n_attention_heads, dropout<span style="color:rgb(98,98,98)">=</span>attention_dropout, mode<span style="color:rgb(98,98,98)">=</span><span style="font-weight:bold;color:rgb(0,135,0)">None</span>)),
<span class="ansi-green-fg ansi-bold">     53</span>   
<span class="ansi-green-fg ansi-bold">     54</span>   <span style="font-style:italic;color:rgb(95,135,135)"># Step 6: drop attention mask (i.e. index = None</span>
<span class="ansi-green-fg ansi-bold">     55</span>   <span style="font-weight:bold;color:rgb(0,135,0)">None</span>,
<span class="ansi-green-fg ansi-bold">     56</span>     
<span class="ansi-green-fg ansi-bold">     57</span>   <span style="font-style:italic;color:rgb(95,135,135)"># Step 7: run the rest of the RNN decoder</span>
<span class="ansi-green-fg ansi-bold">     58</span>   [<span style="font-weight:bold;color:rgb(0,135,0)">None</span> <span style="font-weight:bold;color:rgb(0,135,0)">for</span> _ <span style="font-weight:bold;color:rgb(175,0,255)">in</span> <span style="color:rgb(0,135,0)">range</span>(<span style="font-weight:bold;color:rgb(0,135,0)">None</span>)],
<span class="ansi-green-fg ansi-bold">     59</span>     
<span class="ansi-green-fg ansi-bold">     60</span>   <span style="font-style:italic;color:rgb(95,135,135)"># Step 8: prepare output by making it the right size</span>
<span class="ansi-green-fg ansi-bold">     61</span>   <span style="font-weight:bold;color:rgb(0,135,0)">None</span>(<span style="font-weight:bold;color:rgb(0,135,0)">None</span>),
<span class="ansi-green-fg ansi-bold">     62</span>     
<span class="ansi-green-fg ansi-bold">     63</span>   <span style="font-style:italic;color:rgb(95,135,135)"># Step 9: Log-softmax for output</span>
<span class="ansi-green-fg ansi-bold">     64</span>   <span style="font-weight:bold;color:rgb(0,135,0)">None</span>
<span class="ansi-green-fg ansi-bold">     65</span> )
<span class="ansi-green-fg ansi-bold">     67</span> <span style="font-style:italic;color:rgb(95,135,135)">### END CODE HERE</span>
<span class="ansi-green-fg ansi-bold">     69</span> <span style="font-weight:bold;color:rgb(0,135,0)">return</span> model

<span class="ansi-red-fg">TypeError</span>: 'NoneType' object is not callable</pre>
</div>
</div>
</div>
<p><strong>Expected Output:</strong></p>
<pre><code>Serial_in2_out2[
  Select[0,1,0,1]_in2_out4
  Parallel_in2_out2[
    Serial[
      Embedding_33300_1024
      LSTM_1024
      LSTM_1024
    ]
    Serial[
      ShiftRight(1)
      Embedding_33300_1024
      LSTM_1024
    ]
  ]
  PrepareAttentionInput_in3_out4
  Serial_in4_out2[
    Branch_in4_out3[
      None
      Serial_in4_out2[
        Parallel_in3_out3[
          Dense_1024
          Dense_1024
          Dense_1024
        ]
        PureAttention_in4_out2
        Dense_1024
      ]
    ]
    Add_in2
  ]
  Select[0,2]_in3_out2
  LSTM_1024
  LSTM_1024
  Dense_33300
  LogSoftmax
]</code></pre>
<p><a name="3"></a> # Part 3: Training</p>
<p>We will now be training our model in this section. Doing supervised training in Trax is pretty straightforward (short example <a href="https://trax-ml.readthedocs.io/en/latest/notebooks/trax_intro.html#Supervised-training">here</a>). We will be instantiating three classes for this: <code>TrainTask</code>, <code>EvalTask</code>, and <code>Loop</code>. Let’s take a closer look at each of these in the sections below.</p>
<p><a name="3.1"></a> ## 3.1 TrainTask</p>
<p>The <a href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.TrainTask">TrainTask</a> class allows us to define the labeled data to use for training and the feedback mechanisms to compute the loss and update the weights.</p>
<p><a name="ex05"></a> ### Exercise 05</p>
<p><strong>Instructions:</strong> Instantiate a train task.</p>
<div id="20940929" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># UNQ_C5</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="co"># GRADED </span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>train_task <span class="op">=</span> training.TrainTask(</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">### START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) </span><span class="al">###</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># use the train batch stream as labeled data</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>    labeled_data<span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># use the cross entropy loss</span></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>    loss_layer<span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># use the Adam optimizer with learning rate of 0.01</span></span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>    optimizer<span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># use the `trax.lr.warmup_and_rsqrt_decay` as the learning rate schedule</span></span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># have 1000 warmup steps with a max value of 0.01</span></span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a>    lr_schedule<span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># have a checkpoint every 10 steps</span></span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a>    n_steps_per_checkpoint<span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a>    <span class="co">### </span><span class="re">END</span><span class="co"> CODE HERE </span><span class="al">###</span></span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/supervised/training.py:1388: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  return [f for f in flat if f is not None and f is not ()]  # pylint: disable=literal-comparison
/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/supervised/training.py:1388: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  return [f for f in flat if f is not None and f is not ()]  # pylint: disable=literal-comparison</code></pre>
</div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">TypeError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[21], line 3</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># UNQ_C5</span>
<span class="ansi-green-fg ansi-bold">      2</span> <span style="font-style:italic;color:rgb(95,135,135)"># GRADED </span>
<span class="ansi-green-fg">----&gt; 3</span> train_task <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">training</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">TrainTask</span><span class="ansi-yellow-bg">(</span>
<span class="ansi-green-fg ansi-bold">      4</span> <span class="ansi-yellow-bg">    </span>
<span class="ansi-green-fg ansi-bold">      5</span> <span class="ansi-yellow-bg">    </span><span style="font-style:italic;color:rgb(95,135,135)" class="ansi-yellow-bg">### START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) ###</span>
<span class="ansi-green-fg ansi-bold">      6</span> <span class="ansi-yellow-bg">    </span>
<span class="ansi-green-fg ansi-bold">      7</span> <span class="ansi-yellow-bg">    </span><span style="font-style:italic;color:rgb(95,135,135)" class="ansi-yellow-bg"># use the train batch stream as labeled data</span>
<span class="ansi-green-fg ansi-bold">      8</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">labeled_data</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg"> </span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">None</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">      9</span> <span class="ansi-yellow-bg">    </span>
<span class="ansi-green-fg ansi-bold">     10</span> <span class="ansi-yellow-bg">    </span><span style="font-style:italic;color:rgb(95,135,135)" class="ansi-yellow-bg"># use the cross entropy loss</span>
<span class="ansi-green-fg ansi-bold">     11</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">loss_layer</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg"> </span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">None</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">     12</span> <span class="ansi-yellow-bg">    </span>
<span class="ansi-green-fg ansi-bold">     13</span> <span class="ansi-yellow-bg">    </span><span style="font-style:italic;color:rgb(95,135,135)" class="ansi-yellow-bg"># use the Adam optimizer with learning rate of 0.01</span>
<span class="ansi-green-fg ansi-bold">     14</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">optimizer</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg"> </span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">None</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">     15</span> <span class="ansi-yellow-bg">    </span>
<span class="ansi-green-fg ansi-bold">     16</span> <span class="ansi-yellow-bg">    </span><span style="font-style:italic;color:rgb(95,135,135)" class="ansi-yellow-bg"># use the `trax.lr.warmup_and_rsqrt_decay` as the learning rate schedule</span>
<span class="ansi-green-fg ansi-bold">     17</span> <span class="ansi-yellow-bg">    </span><span style="font-style:italic;color:rgb(95,135,135)" class="ansi-yellow-bg"># have 1000 warmup steps with a max value of 0.01</span>
<span class="ansi-green-fg ansi-bold">     18</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">lr_schedule</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg"> </span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">None</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">     19</span> <span class="ansi-yellow-bg">    </span>
<span class="ansi-green-fg ansi-bold">     20</span> <span class="ansi-yellow-bg">    </span><span style="font-style:italic;color:rgb(95,135,135)" class="ansi-yellow-bg"># have a checkpoint every 10 steps</span>
<span class="ansi-green-fg ansi-bold">     21</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">n_steps_per_checkpoint</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg"> </span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">None</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">     22</span> <span class="ansi-yellow-bg">    </span>
<span class="ansi-green-fg ansi-bold">     23</span> <span class="ansi-yellow-bg">    </span><span style="font-style:italic;color:rgb(95,135,135)" class="ansi-yellow-bg">### END CODE HERE ###</span>
<span class="ansi-green-fg ansi-bold">     24</span> <span class="ansi-yellow-bg">)</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/gin/config.py:1605</span>, in <span class="ansi-cyan-fg">_make_gin_wrapper.&lt;locals&gt;.gin_wrapper</span><span class="ansi-blue-fg">(*args, **kwargs)</span>
<span class="ansi-green-fg ansi-bold">   1603</span> scope_info <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)"> in scope </span><span style="color:rgb(175,0,0)">'</span><span style="font-weight:bold;color:rgb(175,95,135)">{}</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">"</span><span style="color:rgb(98,98,98)">.</span>format(scope_str) <span style="font-weight:bold;color:rgb(0,135,0)">if</span> scope_str <span style="font-weight:bold;color:rgb(0,135,0)">else</span> <span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">'</span>
<span class="ansi-green-fg ansi-bold">   1604</span> err_str <span style="color:rgb(98,98,98)">=</span> err_str<span style="color:rgb(98,98,98)">.</span>format(name, fn_or_cls, scope_info)
<span class="ansi-green-fg">-&gt; 1605</span> <span class="ansi-yellow-bg">utils</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">augment_exception_message_and_reraise</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">e</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">err_str</span><span class="ansi-yellow-bg">)</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/gin/utils.py:41</span>, in <span class="ansi-cyan-fg">augment_exception_message_and_reraise</span><span class="ansi-blue-fg">(exception, message)</span>
<span class="ansi-green-fg ansi-bold">     39</span> proxy <span style="color:rgb(98,98,98)">=</span> ExceptionProxy()
<span class="ansi-green-fg ansi-bold">     40</span> ExceptionProxy<span style="color:rgb(98,98,98)">.</span><span style="color:rgb(0,0,135)">__qualname__</span> <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)">type</span>(exception)<span style="color:rgb(98,98,98)">.</span><span style="color:rgb(0,0,135)">__qualname__</span>
<span class="ansi-green-fg">---&gt; 41</span> <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> proxy<span style="color:rgb(98,98,98)">.</span>with_traceback(exception<span style="color:rgb(98,98,98)">.</span>__traceback__) <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">None</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/gin/config.py:1582</span>, in <span class="ansi-cyan-fg">_make_gin_wrapper.&lt;locals&gt;.gin_wrapper</span><span class="ansi-blue-fg">(*args, **kwargs)</span>
<span class="ansi-green-fg ansi-bold">   1579</span> new_kwargs<span style="color:rgb(98,98,98)">.</span>update(kwargs)
<span class="ansi-green-fg ansi-bold">   1581</span> <span style="font-weight:bold;color:rgb(0,135,0)">try</span>:
<span class="ansi-green-fg">-&gt; 1582</span>   <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span class="ansi-yellow-bg">fn</span><span class="ansi-yellow-bg">(</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">new_args</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">new_kwargs</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">   1583</span> <span style="font-weight:bold;color:rgb(0,135,0)">except</span> <span style="font-weight:bold;color:rgb(215,95,95)">Exception</span> <span style="font-weight:bold;color:rgb(0,135,0)">as</span> e:  <span style="font-style:italic;color:rgb(95,135,135)"># pylint: disable=broad-except</span>
<span class="ansi-green-fg ansi-bold">   1584</span>   err_str <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">'</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/supervised/training.py:1086</span>, in <span class="ansi-cyan-fg">TrainTask.__init__</span><span class="ansi-blue-fg">(self, labeled_data, loss_layer, optimizer, lr_schedule, n_steps_per_checkpoint, n_steps_per_permanent_checkpoint, loss_name, sample_batch, export_prefix)</span>
<span class="ansi-green-fg ansi-bold">   1084</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_optimizer <span style="color:rgb(98,98,98)">=</span> optimizer
<span class="ansi-green-fg ansi-bold">   1085</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_lr_schedule <span style="color:rgb(98,98,98)">=</span> lr_schedule
<span class="ansi-green-fg">-&gt; 1086</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_sample_batch <span style="color:rgb(98,98,98)">=</span> sample_batch <span style="font-weight:bold;color:rgb(175,0,255)">or</span> <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">next</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">labeled_data</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">   1087</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_n_steps_per_checkpoint <span style="color:rgb(98,98,98)">=</span> n_steps_per_checkpoint
<span class="ansi-green-fg ansi-bold">   1088</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_n_steps_per_permanent_checkpoint <span style="color:rgb(98,98,98)">=</span> n_steps_per_permanent_checkpoint

<span class="ansi-red-fg">TypeError</span>: 'NoneType' object is not an iterator
  In call to configurable 'TrainTask' (&lt;class 'trax.supervised.training.TrainTask'&gt;)</pre>
</div>
</div>
</div>
<div id="ff9387d9" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="re">BEGIN</span><span class="co"> UNIT </span><span class="al">TEST</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>w1_unittest.test_train_task(train_task)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="re">END</span><span class="co"> UNIT </span><span class="al">TEST</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[22], line 2</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># BEGIN UNIT TEST</span>
<span class="ansi-green-fg">----&gt; 2</span> w1_unittest<span style="color:rgb(98,98,98)">.</span>test_train_task(<span class="ansi-yellow-bg">train_task</span>)
<span class="ansi-green-fg ansi-bold">      3</span> <span style="font-style:italic;color:rgb(95,135,135)"># END UNIT TEST</span>

<span class="ansi-red-fg">NameError</span>: name 'train_task' is not defined</pre>
</div>
</div>
</div>
<p><a name="3.2"></a> ## 3.2 EvalTask</p>
<p>The <a href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.EvalTask">EvalTask</a> on the other hand allows us to see how the model is doing while training. For our application, we want it to report the cross entropy loss and accuracy.</p>
<div id="89310007" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>eval_task <span class="op">=</span> training.EvalTask(</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">## use the eval batch stream as labeled data</span></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>    labeled_data<span class="op">=</span>eval_batch_stream,</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">## use the cross entropy loss and accuracy as metrics</span></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>    metrics<span class="op">=</span>[tl.CrossEntropyLoss(), tl.Accuracy()],</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[23], line 4</span>
<span class="ansi-green-fg ansi-bold">      1</span> eval_task <span style="color:rgb(98,98,98)">=</span> training<span style="color:rgb(98,98,98)">.</span>EvalTask(
<span class="ansi-green-fg ansi-bold">      2</span>     
<span class="ansi-green-fg ansi-bold">      3</span>     <span style="font-style:italic;color:rgb(95,135,135)">## use the eval batch stream as labeled data</span>
<span class="ansi-green-fg">----&gt; 4</span>     labeled_data<span style="color:rgb(98,98,98)">=</span><span class="ansi-yellow-bg">eval_batch_stream</span>,
<span class="ansi-green-fg ansi-bold">      5</span>     
<span class="ansi-green-fg ansi-bold">      6</span>     <span style="font-style:italic;color:rgb(95,135,135)">## use the cross entropy loss and accuracy as metrics</span>
<span class="ansi-green-fg ansi-bold">      7</span>     metrics<span style="color:rgb(98,98,98)">=</span>[tl<span style="color:rgb(98,98,98)">.</span>CrossEntropyLoss(), tl<span style="color:rgb(98,98,98)">.</span>Accuracy()],
<span class="ansi-green-fg ansi-bold">      8</span> )

<span class="ansi-red-fg">NameError</span>: name 'eval_batch_stream' is not defined</pre>
</div>
</div>
</div>
<p><a name="3.3"></a> ## 3.3 Loop</p>
<p>The <a href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.Loop">Loop</a> class defines the model we will train as well as the train and eval tasks to execute. Its <code>run()</code> method allows us to execute the training for a specified number of steps.</p>
<div id="42f9ac3f" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># define the output directory</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>output_dir <span class="op">=</span> <span class="st">'output_dir/'</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="co"># remove old model if it exists. restarts training.</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>rm <span class="op">-</span>f <span class="op">~/</span>output_dir<span class="op">/</span>model.pkl.gz  </span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="co"># define the training loop</span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>training_loop <span class="op">=</span> training.Loop(NMTAttn(mode<span class="op">=</span><span class="st">'train'</span>),</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>                              train_task,</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>                              eval_tasks<span class="op">=</span>[eval_task],</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>                              output_dir<span class="op">=</span>output_dir)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.
  pid, fd = os.forkpty()</code></pre>
</div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">TypeError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[24], line 8</span>
<span class="ansi-green-fg ansi-bold">      5</span> get_ipython()<span style="color:rgb(98,98,98)">.</span>system(<span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">rm -f ~/output_dir/model.pkl.gz</span><span style="color:rgb(175,0,0)">'</span>)
<span class="ansi-green-fg ansi-bold">      7</span> <span style="font-style:italic;color:rgb(95,135,135)"># define the training loop</span>
<span class="ansi-green-fg">----&gt; 8</span> training_loop <span style="color:rgb(98,98,98)">=</span> training<span style="color:rgb(98,98,98)">.</span>Loop(<span class="ansi-yellow-bg">NMTAttn</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">mode</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">'</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">train</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">'</span><span class="ansi-yellow-bg">)</span>,
<span class="ansi-green-fg ansi-bold">      9</span>                               train_task,
<span class="ansi-green-fg ansi-bold">     10</span>                               eval_tasks<span style="color:rgb(98,98,98)">=</span>[eval_task],
<span class="ansi-green-fg ansi-bold">     11</span>                               output_dir<span style="color:rgb(98,98,98)">=</span>output_dir)

Cell <span class="ansi-green-fg">In[18], line 45</span>, in <span class="ansi-cyan-fg">NMTAttn</span><span class="ansi-blue-fg">(input_vocab_size, target_vocab_size, d_model, n_encoder_layers, n_decoder_layers, n_attention_heads, attention_dropout, mode)</span>
<span class="ansi-green-fg ansi-bold">     36</span> pre_attention_decoder <span style="color:rgb(98,98,98)">=</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span>
<span class="ansi-green-fg ansi-bold">     38</span> <span style="font-style:italic;color:rgb(95,135,135)"># Step 1: create a serial network</span>
<span class="ansi-green-fg ansi-bold">     39</span> model <span style="color:rgb(98,98,98)">=</span> tl<span style="color:rgb(98,98,98)">.</span>Serial( 
<span class="ansi-green-fg ansi-bold">     40</span>     
<span class="ansi-green-fg ansi-bold">     41</span>   <span style="font-style:italic;color:rgb(95,135,135)"># Step 2: copy input tokens and target tokens as they will be needed later.</span>
<span class="ansi-green-fg ansi-bold">     42</span>   <span style="font-weight:bold;color:rgb(0,135,0)">None</span>,
<span class="ansi-green-fg ansi-bold">     43</span>     
<span class="ansi-green-fg ansi-bold">     44</span>   <span style="font-style:italic;color:rgb(95,135,135)"># Step 3: run input encoder on the input and pre-attention decoder the target.</span>
<span class="ansi-green-fg">---&gt; 45</span>   <span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">None</span><span class="ansi-yellow-bg">(</span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">None</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">None</span><span class="ansi-yellow-bg">)</span>,
<span class="ansi-green-fg ansi-bold">     46</span>     
<span class="ansi-green-fg ansi-bold">     47</span>   <span style="font-style:italic;color:rgb(95,135,135)"># Step 4: prepare queries, keys, values and mask for attention.</span>
<span class="ansi-green-fg ansi-bold">     48</span>   <span style="font-weight:bold;color:rgb(0,135,0)">None</span>(<span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">PrepareAttentionInput</span><span style="color:rgb(175,0,0)">'</span>, <span style="font-weight:bold;color:rgb(0,135,0)">None</span>, n_out<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">4</span>),
<span class="ansi-green-fg ansi-bold">     49</span>     
<span class="ansi-green-fg ansi-bold">     50</span>   <span style="font-style:italic;color:rgb(95,135,135)"># Step 5: run the AttentionQKV layer</span>
<span class="ansi-green-fg ansi-bold">     51</span>   <span style="font-style:italic;color:rgb(95,135,135)"># nest it inside a Residual layer to add to the pre-attention decoder activations(i.e. queries)</span>
<span class="ansi-green-fg ansi-bold">     52</span>   tl<span style="color:rgb(98,98,98)">.</span>Residual(tl<span style="color:rgb(98,98,98)">.</span>AttentionQKV(<span style="font-weight:bold;color:rgb(0,135,0)">None</span>, n_heads<span style="color:rgb(98,98,98)">=</span>n_attention_heads, dropout<span style="color:rgb(98,98,98)">=</span>attention_dropout, mode<span style="color:rgb(98,98,98)">=</span><span style="font-weight:bold;color:rgb(0,135,0)">None</span>)),
<span class="ansi-green-fg ansi-bold">     53</span>   
<span class="ansi-green-fg ansi-bold">     54</span>   <span style="font-style:italic;color:rgb(95,135,135)"># Step 6: drop attention mask (i.e. index = None</span>
<span class="ansi-green-fg ansi-bold">     55</span>   <span style="font-weight:bold;color:rgb(0,135,0)">None</span>,
<span class="ansi-green-fg ansi-bold">     56</span>     
<span class="ansi-green-fg ansi-bold">     57</span>   <span style="font-style:italic;color:rgb(95,135,135)"># Step 7: run the rest of the RNN decoder</span>
<span class="ansi-green-fg ansi-bold">     58</span>   [<span style="font-weight:bold;color:rgb(0,135,0)">None</span> <span style="font-weight:bold;color:rgb(0,135,0)">for</span> _ <span style="font-weight:bold;color:rgb(175,0,255)">in</span> <span style="color:rgb(0,135,0)">range</span>(<span style="font-weight:bold;color:rgb(0,135,0)">None</span>)],
<span class="ansi-green-fg ansi-bold">     59</span>     
<span class="ansi-green-fg ansi-bold">     60</span>   <span style="font-style:italic;color:rgb(95,135,135)"># Step 8: prepare output by making it the right size</span>
<span class="ansi-green-fg ansi-bold">     61</span>   <span style="font-weight:bold;color:rgb(0,135,0)">None</span>(<span style="font-weight:bold;color:rgb(0,135,0)">None</span>),
<span class="ansi-green-fg ansi-bold">     62</span>     
<span class="ansi-green-fg ansi-bold">     63</span>   <span style="font-style:italic;color:rgb(95,135,135)"># Step 9: Log-softmax for output</span>
<span class="ansi-green-fg ansi-bold">     64</span>   <span style="font-weight:bold;color:rgb(0,135,0)">None</span>
<span class="ansi-green-fg ansi-bold">     65</span> )
<span class="ansi-green-fg ansi-bold">     67</span> <span style="font-style:italic;color:rgb(95,135,135)">### END CODE HERE</span>
<span class="ansi-green-fg ansi-bold">     69</span> <span style="font-weight:bold;color:rgb(0,135,0)">return</span> model

<span class="ansi-red-fg">TypeError</span>: 'NoneType' object is not callable</pre>
</div>
</div>
</div>
<div id="99c37a1a" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">NOTE</span><span class="co">: Execute the training loop. This will take around 8 minutes to complete.</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>training_loop.run(<span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[25], line 2</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># NOTE: Execute the training loop. This will take around 8 minutes to complete.</span>
<span class="ansi-green-fg">----&gt; 2</span> <span class="ansi-yellow-bg">training_loop</span><span style="color:rgb(98,98,98)">.</span>run(<span style="color:rgb(98,98,98)">10</span>)

<span class="ansi-red-fg">NameError</span>: name 'training_loop' is not defined</pre>
</div>
</div>
</div>
<p><a name="4"></a> # Part 4: Testing</p>
<p>We will now be using the model you just trained to translate English sentences to German. We will implement this with two functions: The first allows you to identify the next symbol (i.e.&nbsp;output token). The second one takes care of combining the entire translated string.</p>
<p>We will start by first loading in a pre-trained copy of the model you just coded. Please run the cell below to do just that.</p>
<div id="f1e85ad2" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># instantiate the model we built in eval mode</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> NMTAttn(mode<span class="op">=</span><span class="st">'eval'</span>)</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize weights from a pre-trained model</span></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>model.init_from_file(<span class="st">"model.pkl.gz"</span>, weights_only<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> tl.Accelerate(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">TypeError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[26], line 2</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># instantiate the model we built in eval mode</span>
<span class="ansi-green-fg">----&gt; 2</span> model <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">NMTAttn</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">mode</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">'</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">eval</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">'</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">      4</span> <span style="font-style:italic;color:rgb(95,135,135)"># initialize weights from a pre-trained model</span>
<span class="ansi-green-fg ansi-bold">      5</span> model<span style="color:rgb(98,98,98)">.</span>init_from_file(<span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">model.pkl.gz</span><span style="color:rgb(175,0,0)">"</span>, weights_only<span style="color:rgb(98,98,98)">=</span><span style="font-weight:bold;color:rgb(0,135,0)">True</span>)

Cell <span class="ansi-green-fg">In[18], line 45</span>, in <span class="ansi-cyan-fg">NMTAttn</span><span class="ansi-blue-fg">(input_vocab_size, target_vocab_size, d_model, n_encoder_layers, n_decoder_layers, n_attention_heads, attention_dropout, mode)</span>
<span class="ansi-green-fg ansi-bold">     36</span> pre_attention_decoder <span style="color:rgb(98,98,98)">=</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span>
<span class="ansi-green-fg ansi-bold">     38</span> <span style="font-style:italic;color:rgb(95,135,135)"># Step 1: create a serial network</span>
<span class="ansi-green-fg ansi-bold">     39</span> model <span style="color:rgb(98,98,98)">=</span> tl<span style="color:rgb(98,98,98)">.</span>Serial( 
<span class="ansi-green-fg ansi-bold">     40</span>     
<span class="ansi-green-fg ansi-bold">     41</span>   <span style="font-style:italic;color:rgb(95,135,135)"># Step 2: copy input tokens and target tokens as they will be needed later.</span>
<span class="ansi-green-fg ansi-bold">     42</span>   <span style="font-weight:bold;color:rgb(0,135,0)">None</span>,
<span class="ansi-green-fg ansi-bold">     43</span>     
<span class="ansi-green-fg ansi-bold">     44</span>   <span style="font-style:italic;color:rgb(95,135,135)"># Step 3: run input encoder on the input and pre-attention decoder the target.</span>
<span class="ansi-green-fg">---&gt; 45</span>   <span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">None</span><span class="ansi-yellow-bg">(</span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">None</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">None</span><span class="ansi-yellow-bg">)</span>,
<span class="ansi-green-fg ansi-bold">     46</span>     
<span class="ansi-green-fg ansi-bold">     47</span>   <span style="font-style:italic;color:rgb(95,135,135)"># Step 4: prepare queries, keys, values and mask for attention.</span>
<span class="ansi-green-fg ansi-bold">     48</span>   <span style="font-weight:bold;color:rgb(0,135,0)">None</span>(<span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">PrepareAttentionInput</span><span style="color:rgb(175,0,0)">'</span>, <span style="font-weight:bold;color:rgb(0,135,0)">None</span>, n_out<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">4</span>),
<span class="ansi-green-fg ansi-bold">     49</span>     
<span class="ansi-green-fg ansi-bold">     50</span>   <span style="font-style:italic;color:rgb(95,135,135)"># Step 5: run the AttentionQKV layer</span>
<span class="ansi-green-fg ansi-bold">     51</span>   <span style="font-style:italic;color:rgb(95,135,135)"># nest it inside a Residual layer to add to the pre-attention decoder activations(i.e. queries)</span>
<span class="ansi-green-fg ansi-bold">     52</span>   tl<span style="color:rgb(98,98,98)">.</span>Residual(tl<span style="color:rgb(98,98,98)">.</span>AttentionQKV(<span style="font-weight:bold;color:rgb(0,135,0)">None</span>, n_heads<span style="color:rgb(98,98,98)">=</span>n_attention_heads, dropout<span style="color:rgb(98,98,98)">=</span>attention_dropout, mode<span style="color:rgb(98,98,98)">=</span><span style="font-weight:bold;color:rgb(0,135,0)">None</span>)),
<span class="ansi-green-fg ansi-bold">     53</span>   
<span class="ansi-green-fg ansi-bold">     54</span>   <span style="font-style:italic;color:rgb(95,135,135)"># Step 6: drop attention mask (i.e. index = None</span>
<span class="ansi-green-fg ansi-bold">     55</span>   <span style="font-weight:bold;color:rgb(0,135,0)">None</span>,
<span class="ansi-green-fg ansi-bold">     56</span>     
<span class="ansi-green-fg ansi-bold">     57</span>   <span style="font-style:italic;color:rgb(95,135,135)"># Step 7: run the rest of the RNN decoder</span>
<span class="ansi-green-fg ansi-bold">     58</span>   [<span style="font-weight:bold;color:rgb(0,135,0)">None</span> <span style="font-weight:bold;color:rgb(0,135,0)">for</span> _ <span style="font-weight:bold;color:rgb(175,0,255)">in</span> <span style="color:rgb(0,135,0)">range</span>(<span style="font-weight:bold;color:rgb(0,135,0)">None</span>)],
<span class="ansi-green-fg ansi-bold">     59</span>     
<span class="ansi-green-fg ansi-bold">     60</span>   <span style="font-style:italic;color:rgb(95,135,135)"># Step 8: prepare output by making it the right size</span>
<span class="ansi-green-fg ansi-bold">     61</span>   <span style="font-weight:bold;color:rgb(0,135,0)">None</span>(<span style="font-weight:bold;color:rgb(0,135,0)">None</span>),
<span class="ansi-green-fg ansi-bold">     62</span>     
<span class="ansi-green-fg ansi-bold">     63</span>   <span style="font-style:italic;color:rgb(95,135,135)"># Step 9: Log-softmax for output</span>
<span class="ansi-green-fg ansi-bold">     64</span>   <span style="font-weight:bold;color:rgb(0,135,0)">None</span>
<span class="ansi-green-fg ansi-bold">     65</span> )
<span class="ansi-green-fg ansi-bold">     67</span> <span style="font-style:italic;color:rgb(95,135,135)">### END CODE HERE</span>
<span class="ansi-green-fg ansi-bold">     69</span> <span style="font-weight:bold;color:rgb(0,135,0)">return</span> model

<span class="ansi-red-fg">TypeError</span>: 'NoneType' object is not callable</pre>
</div>
</div>
</div>
<p><a name="4.1"></a> ## 4.1 Decoding</p>
<p>As discussed in the lectures, there are several ways to get the next token when translating a sentence. For instance, we can just get the most probable token at each step (i.e.&nbsp;greedy decoding) or get a sample from a distribution. We can generalize the implementation of these two approaches by using the <code>tl.logsoftmax_sample()</code> method. Let’s briefly look at its implementation:</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> logsoftmax_sample(log_probs, temperature<span class="op">=</span><span class="fl">1.0</span>):  <span class="co"># pylint: disable=invalid-name</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>  <span class="co">"""Returns a sample from a log-softmax output, with temperature.</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="co">  Args:</span></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="co">    log_probs: Logarithms of probabilities (often coming from LogSofmax)</span></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a><span class="co">    temperature: For scaling before sampling (1.0 = default, 0.0 = pick argmax)</span></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a><span class="co">  """</span></span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># This is equivalent to sampling from a softmax with temperature.</span></span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>  u <span class="op">=</span> np.random.uniform(low<span class="op">=</span><span class="fl">1e-6</span>, high<span class="op">=</span><span class="fl">1.0</span> <span class="op">-</span> <span class="fl">1e-6</span>, size<span class="op">=</span>log_probs.shape)</span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>  g <span class="op">=</span> <span class="op">-</span>np.log(<span class="op">-</span>np.log(u))</span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> np.argmax(log_probs <span class="op">+</span> g <span class="op">*</span> temperature, axis<span class="op">=-</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The key things to take away here are: 1. it gets random samples with the same shape as your input (i.e.&nbsp;<code>log_probs</code>), and 2. the amount of “noise” added to the input by these random samples is scaled by a <code>temperature</code> setting. You’ll notice that setting it to <code>0</code> will just make the return statement equal to getting the argmax of <code>log_probs</code>. This will come in handy later.</p>
<p><a name="ex06"></a> ### Exercise 06</p>
<p><strong>Instructions:</strong> Implement the <code>next_symbol()</code> function that takes in the <code>input_tokens</code> and the <code>cur_output_tokens</code>, then return the index of the next word. You can click below for hints in completing this exercise.</p>
<details>
<summary>
<font size="3" color="darkgreen"><b>Click Here for Hints</b></font>
</summary>
<p>
</p><ul>
<li>
To get the next power of two, you can compute <i>2^log_2(token_length + 1)</i> . We add 1 to avoid <i>log(0).</i>
</li>
<li>
You can use <i>np.ceil()</i> to get the ceiling of a float.
</li>
<li>
<i>np.log2()</i> will get the logarithm base 2 of a value
</li>
<li>
<i>int()</i> will cast a value into an integer type
</li>
<li>
From the model diagram in part 2, you know that it takes two inputs. You can feed these with this syntax to get the model outputs: <i>model((input1, input2))</i>. It’s up to you to determine which variables below to substitute for input1 and input2. Remember also from the diagram that the output has two elements: [log probabilities, target tokens]. You won’t need the target tokens so we assigned it to _ below for you.
</li>
<li>
The log probabilities output will have the shape: (batch size, decoder length, vocab size). It will contain log probabilities for each token in the <i>cur_output_tokens</i> plus 1 for the start symbol introduced by the ShiftRight in the preattention decoder. For example, if cur_output_tokens is [1, 2, 5], the model will output an array of log probabilities each for tokens 0 (start symbol), 1, 2, and 5. To generate the next symbol, you just want to get the log probabilities associated with the last token (i.e.&nbsp;token 5 at index 3). You can slice the model output at [0, 3, :] to get this. It will be up to you to generalize this for any length of cur_output_tokens
</li>
</ul>
<div id="022367dd" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># UNQ_C6</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="co"># GRADED FUNCTION</span></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> next_symbol(NMTAttn, input_tokens, cur_output_tokens, temperature):</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Returns the index of the next token.</span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a><span class="co">        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.</span></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a><span class="co">        input_tokens (np.ndarray 1 x n_tokens): tokenized representation of the input sentence</span></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a><span class="co">        cur_output_tokens (list): tokenized representation of previously translated words</span></span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a><span class="co">        temperature (float): parameter for sampling ranging from 0.0 to 1.0.</span></span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a><span class="co">            0.0: same as argmax, always pick the most probable token</span></span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a><span class="co">            1.0: sampling from the distribution (can sometimes say random things)</span></span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a><span class="co">        int: index of the next token in the translated sentence</span></span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a><span class="co">        float: log probability of the next symbol</span></span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a>    <span class="co">### START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) </span><span class="al">###</span></span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># set the length of the current output tokens</span></span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a>    token_length <span class="op">=</span> <span class="va">None</span></span>
<span id="cb39-23"><a href="#cb39-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-24"><a href="#cb39-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># calculate next power of 2 for padding length </span></span>
<span id="cb39-25"><a href="#cb39-25" aria-hidden="true" tabindex="-1"></a>    padded_length <span class="op">=</span> <span class="va">None</span></span>
<span id="cb39-26"><a href="#cb39-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-27"><a href="#cb39-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># pad cur_output_tokens up to the padded_length</span></span>
<span id="cb39-28"><a href="#cb39-28" aria-hidden="true" tabindex="-1"></a>    padded <span class="op">=</span> cur_output_tokens <span class="op">+</span> <span class="va">None</span></span>
<span id="cb39-29"><a href="#cb39-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb39-30"><a href="#cb39-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># model expects the output to have an axis for the batch size in front so</span></span>
<span id="cb39-31"><a href="#cb39-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># convert `padded` list to a numpy array with shape (None, &lt;padded_length&gt;) where</span></span>
<span id="cb39-32"><a href="#cb39-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># None is a placeholder for the batch size</span></span>
<span id="cb39-33"><a href="#cb39-33" aria-hidden="true" tabindex="-1"></a>    padded_with_batch <span class="op">=</span> <span class="va">None</span></span>
<span id="cb39-34"><a href="#cb39-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-35"><a href="#cb39-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># get the model prediction (remember to use the `NMAttn` argument defined above)</span></span>
<span id="cb39-36"><a href="#cb39-36" aria-hidden="true" tabindex="-1"></a>    output, _ <span class="op">=</span> <span class="va">None</span></span>
<span id="cb39-37"><a href="#cb39-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb39-38"><a href="#cb39-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># get log probabilities from the last token output</span></span>
<span id="cb39-39"><a href="#cb39-39" aria-hidden="true" tabindex="-1"></a>    log_probs <span class="op">=</span> output[<span class="va">None</span>]</span>
<span id="cb39-40"><a href="#cb39-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-41"><a href="#cb39-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># get the next symbol by getting a logsoftmax sample (*hint: cast to an int)</span></span>
<span id="cb39-42"><a href="#cb39-42" aria-hidden="true" tabindex="-1"></a>    symbol <span class="op">=</span> <span class="va">None</span></span>
<span id="cb39-43"><a href="#cb39-43" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb39-44"><a href="#cb39-44" aria-hidden="true" tabindex="-1"></a>    <span class="co">### </span><span class="re">END</span><span class="co"> CODE HERE </span><span class="al">###</span></span>
<span id="cb39-45"><a href="#cb39-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-46"><a href="#cb39-46" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> symbol, <span class="bu">float</span>(log_probs[symbol])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="d60e5b1a" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="re">BEGIN</span><span class="co"> UNIT </span><span class="al">TEST</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>w1_unittest.test_next_symbol(next_symbol, model)</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="re">END</span><span class="co"> UNIT </span><span class="al">TEST</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[28], line 2</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># BEGIN UNIT TEST</span>
<span class="ansi-green-fg">----&gt; 2</span> w1_unittest<span style="color:rgb(98,98,98)">.</span>test_next_symbol(next_symbol, <span class="ansi-yellow-bg">model</span>)
<span class="ansi-green-fg ansi-bold">      3</span> <span style="font-style:italic;color:rgb(95,135,135)"># END UNIT TEST</span>

<span class="ansi-red-fg">NameError</span>: name 'model' is not defined</pre>
</div>
</div>
</div>
<p>Now you will implement the <code>sampling_decode()</code> function. This will call the <code>next_symbol()</code> function above several times until the next output is the end-of-sentence token (i.e.&nbsp;<code>EOS</code>). It takes in an input string and returns the translated version of that string.</p>
<p><a name="ex07"></a> ### Exercise 07</p>
<p><strong>Instructions</strong>: Implement the <code>sampling_decode()</code> function.</p>
<div id="4fd76fc6" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># UNQ_C7</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="co"># GRADED FUNCTION</span></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sampling_decode(input_sentence, NMTAttn <span class="op">=</span> <span class="va">None</span>, temperature<span class="op">=</span><span class="fl">0.0</span>, vocab_file<span class="op">=</span><span class="va">None</span>, vocab_dir<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Returns the translated sentence.</span></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a><span class="co">        input_sentence (str): sentence to translate.</span></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a><span class="co">        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.</span></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a><span class="co">        temperature (float): parameter for sampling ranging from 0.0 to 1.0.</span></span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a><span class="co">            0.0: same as argmax, always pick the most probable token</span></span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a><span class="co">            1.0: sampling from the distribution (can sometimes say random things)</span></span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a><span class="co">        vocab_file (str): filename of the vocabulary</span></span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a><span class="co">        vocab_dir (str): path to the vocabulary file</span></span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a><span class="co">        tuple: (list, str, float)</span></span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a><span class="co">            list of int: tokenized version of the translated sentence</span></span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a><span class="co">            float: log probability of the translated sentence</span></span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a><span class="co">            str: the translated sentence</span></span>
<span id="cb41-20"><a href="#cb41-20" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb41-21"><a href="#cb41-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb41-22"><a href="#cb41-22" aria-hidden="true" tabindex="-1"></a>    <span class="co">### START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) </span><span class="al">###</span></span>
<span id="cb41-23"><a href="#cb41-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb41-24"><a href="#cb41-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># encode the input sentence</span></span>
<span id="cb41-25"><a href="#cb41-25" aria-hidden="true" tabindex="-1"></a>    input_tokens <span class="op">=</span> <span class="va">None</span></span>
<span id="cb41-26"><a href="#cb41-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb41-27"><a href="#cb41-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># initialize the list of output tokens</span></span>
<span id="cb41-28"><a href="#cb41-28" aria-hidden="true" tabindex="-1"></a>    cur_output_tokens <span class="op">=</span> <span class="va">None</span></span>
<span id="cb41-29"><a href="#cb41-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb41-30"><a href="#cb41-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># initialize an integer that represents the current output index</span></span>
<span id="cb41-31"><a href="#cb41-31" aria-hidden="true" tabindex="-1"></a>    cur_output <span class="op">=</span> <span class="va">None</span></span>
<span id="cb41-32"><a href="#cb41-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb41-33"><a href="#cb41-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Set the encoding of the "end of sentence" as 1</span></span>
<span id="cb41-34"><a href="#cb41-34" aria-hidden="true" tabindex="-1"></a>    EOS <span class="op">=</span> <span class="va">None</span></span>
<span id="cb41-35"><a href="#cb41-35" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb41-36"><a href="#cb41-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># check that the current output is not the end of sentence token</span></span>
<span id="cb41-37"><a href="#cb41-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> cur_output <span class="op">!=</span> EOS:</span>
<span id="cb41-38"><a href="#cb41-38" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb41-39"><a href="#cb41-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># update the current output token by getting the index of the next word (hint: use next_symbol)</span></span>
<span id="cb41-40"><a href="#cb41-40" aria-hidden="true" tabindex="-1"></a>        cur_output, log_prob <span class="op">=</span> <span class="va">None</span></span>
<span id="cb41-41"><a href="#cb41-41" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb41-42"><a href="#cb41-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># append the current output token to the list of output tokens</span></span>
<span id="cb41-43"><a href="#cb41-43" aria-hidden="true" tabindex="-1"></a>        cur_output_tokens.append(cur_output)</span>
<span id="cb41-44"><a href="#cb41-44" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb41-45"><a href="#cb41-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># detokenize the output tokens</span></span>
<span id="cb41-46"><a href="#cb41-46" aria-hidden="true" tabindex="-1"></a>    sentence <span class="op">=</span> <span class="va">None</span></span>
<span id="cb41-47"><a href="#cb41-47" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb41-48"><a href="#cb41-48" aria-hidden="true" tabindex="-1"></a>    <span class="co">### </span><span class="re">END</span><span class="co"> CODE HERE </span><span class="al">###</span></span>
<span id="cb41-49"><a href="#cb41-49" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb41-50"><a href="#cb41-50" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> cur_output_tokens, log_prob, sentence</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="5f215414" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Test the function above. Try varying the temperature setting with values from 0 to 1.</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Run it several times with each setting and see how often the output changes.</span></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>sampling_decode(<span class="st">"I love languages."</span>, model, temperature<span class="op">=</span><span class="fl">0.0</span>, vocab_file<span class="op">=</span>VOCAB_FILE, vocab_dir<span class="op">=</span>VOCAB_DIR)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[30], line 3</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># Test the function above. Try varying the temperature setting with values from 0 to 1.</span>
<span class="ansi-green-fg ansi-bold">      2</span> <span style="font-style:italic;color:rgb(95,135,135)"># Run it several times with each setting and see how often the output changes.</span>
<span class="ansi-green-fg">----&gt; 3</span> sampling_decode(<span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">I love languages.</span><span style="color:rgb(175,0,0)">"</span>, <span class="ansi-yellow-bg">model</span>, temperature<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">0.0</span>, vocab_file<span style="color:rgb(98,98,98)">=</span>VOCAB_FILE, vocab_dir<span style="color:rgb(98,98,98)">=</span>VOCAB_DIR)

<span class="ansi-red-fg">NameError</span>: name 'model' is not defined</pre>
</div>
</div>
</div>
<div id="5b1e6f56" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="re">BEGIN</span><span class="co"> UNIT </span><span class="al">TEST</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>w1_unittest.test_sampling_decode(sampling_decode, model)</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="re">END</span><span class="co"> UNIT </span><span class="al">TEST</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[31], line 2</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># BEGIN UNIT TEST</span>
<span class="ansi-green-fg">----&gt; 2</span> w1_unittest<span style="color:rgb(98,98,98)">.</span>test_sampling_decode(sampling_decode, <span class="ansi-yellow-bg">model</span>)
<span class="ansi-green-fg ansi-bold">      3</span> <span style="font-style:italic;color:rgb(95,135,135)"># END UNIT TEST</span>

<span class="ansi-red-fg">NameError</span>: name 'model' is not defined</pre>
</div>
</div>
</div>
<p>We have set a default value of <code>0</code> to the temperature setting in our implementation of <code>sampling_decode()</code> above. As you may have noticed in the <code>logsoftmax_sample()</code> method, this setting will ultimately result in greedy decoding. As mentioned in the lectures, this algorithm generates the translation by getting the most probable word at each step. It gets the argmax of the output array of your model and then returns that index. See the testing function and sample inputs below. You’ll notice that the output will remain the same each time you run it.</p>
<div id="099aaf87" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> greedy_decode_test(sentence, NMTAttn<span class="op">=</span><span class="va">None</span>, vocab_file<span class="op">=</span><span class="va">None</span>, vocab_dir<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Prints the input and output of our NMTAttn model using greedy decode</span></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="co">        sentence (str): a custom string.</span></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a><span class="co">        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.</span></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a><span class="co">        vocab_file (str): filename of the vocabulary</span></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a><span class="co">        vocab_dir (str): path to the vocabulary file</span></span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a><span class="co">        str: the translated sentence</span></span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a>    _,_, translated_sentence <span class="op">=</span> sampling_decode(sentence, NMTAttn, vocab_file<span class="op">=</span>vocab_file, vocab_dir<span class="op">=</span>vocab_dir)</span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"English: "</span>, sentence)</span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"German: "</span>, translated_sentence)</span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> translated_sentence</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cc428988" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># put a custom string here</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>your_sentence <span class="op">=</span> <span class="st">'I love languages.'</span></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>greedy_decode_test(your_sentence, model, vocab_file<span class="op">=</span>VOCAB_FILE, vocab_dir<span class="op">=</span>VOCAB_DIR)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[33], line 4</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># put a custom string here</span>
<span class="ansi-green-fg ansi-bold">      2</span> your_sentence <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">I love languages.</span><span style="color:rgb(175,0,0)">'</span>
<span class="ansi-green-fg">----&gt; 4</span> greedy_decode_test(your_sentence, <span class="ansi-yellow-bg">model</span>, vocab_file<span style="color:rgb(98,98,98)">=</span>VOCAB_FILE, vocab_dir<span style="color:rgb(98,98,98)">=</span>VOCAB_DIR);

<span class="ansi-red-fg">NameError</span>: name 'model' is not defined</pre>
</div>
</div>
</div>
<div id="45be894f" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>greedy_decode_test(<span class="st">'You are almost done with the assignment!'</span>, model, vocab_file<span class="op">=</span>VOCAB_FILE, vocab_dir<span class="op">=</span>VOCAB_DIR)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[34], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> greedy_decode_test(<span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">You are almost done with the assignment!</span><span style="color:rgb(175,0,0)">'</span>, <span class="ansi-yellow-bg">model</span>, vocab_file<span style="color:rgb(98,98,98)">=</span>VOCAB_FILE, vocab_dir<span style="color:rgb(98,98,98)">=</span>VOCAB_DIR);

<span class="ansi-red-fg">NameError</span>: name 'model' is not defined</pre>
</div>
</div>
</div>
<p><a name="4.2"></a> ## 4.2 Minimum Bayes-Risk Decoding</p>
<p>As mentioned in the lectures, getting the most probable token at each step may not necessarily produce the best results. Another approach is to do Minimum Bayes Risk Decoding or MBR. The general steps to implement this are:</p>
<ol type="1">
<li>take several random samples</li>
<li>score each sample against all other samples</li>
<li>select the one with the highest score</li>
</ol>
<p>You will be building helper functions for these steps in the following sections.</p>
<p><a name="4.2.1"></a> ### 4.2.1 Generating samples</p>
<p>First, let’s build a function to generate several samples. You can use the <code>sampling_decode()</code> function you developed earlier to do this easily. We want to record the token list and log probability for each sample as these will be needed in the next step.</p>
<div id="fc885d1c" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_samples(sentence, n_samples, NMTAttn<span class="op">=</span><span class="va">None</span>, temperature<span class="op">=</span><span class="fl">0.6</span>, vocab_file<span class="op">=</span><span class="va">None</span>, vocab_dir<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Generates samples using sampling_decode()</span></span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a><span class="co">        sentence (str): sentence to translate.</span></span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a><span class="co">        n_samples (int): number of samples to generate</span></span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a><span class="co">        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.</span></span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a><span class="co">        temperature (float): parameter for sampling ranging from 0.0 to 1.0.</span></span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a><span class="co">            0.0: same as argmax, always pick the most probable token</span></span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a><span class="co">            1.0: sampling from the distribution (can sometimes say random things)</span></span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a><span class="co">        vocab_file (str): filename of the vocabulary</span></span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a><span class="co">        vocab_dir (str): path to the vocabulary file</span></span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a><span class="co">        tuple: (list, list)</span></span>
<span id="cb47-16"><a href="#cb47-16" aria-hidden="true" tabindex="-1"></a><span class="co">            list of lists: token list per sample</span></span>
<span id="cb47-17"><a href="#cb47-17" aria-hidden="true" tabindex="-1"></a><span class="co">            list of floats: log probability per sample</span></span>
<span id="cb47-18"><a href="#cb47-18" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb47-19"><a href="#cb47-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># define lists to contain samples and probabilities</span></span>
<span id="cb47-20"><a href="#cb47-20" aria-hidden="true" tabindex="-1"></a>    samples, log_probs <span class="op">=</span> [], []</span>
<span id="cb47-21"><a href="#cb47-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-22"><a href="#cb47-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># run a for loop to generate n samples</span></span>
<span id="cb47-23"><a href="#cb47-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_samples):</span>
<span id="cb47-24"><a href="#cb47-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb47-25"><a href="#cb47-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># get a sample using the sampling_decode() function</span></span>
<span id="cb47-26"><a href="#cb47-26" aria-hidden="true" tabindex="-1"></a>        sample, logp, _ <span class="op">=</span> sampling_decode(sentence, NMTAttn, temperature, vocab_file<span class="op">=</span>vocab_file, vocab_dir<span class="op">=</span>vocab_dir)</span>
<span id="cb47-27"><a href="#cb47-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb47-28"><a href="#cb47-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># append the token list to the samples list</span></span>
<span id="cb47-29"><a href="#cb47-29" aria-hidden="true" tabindex="-1"></a>        samples.append(sample)</span>
<span id="cb47-30"><a href="#cb47-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb47-31"><a href="#cb47-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># append the log probability to the log_probs list</span></span>
<span id="cb47-32"><a href="#cb47-32" aria-hidden="true" tabindex="-1"></a>        log_probs.append(logp)</span>
<span id="cb47-33"><a href="#cb47-33" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb47-34"><a href="#cb47-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> samples, log_probs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="bfa70b46" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># generate 4 samples with the default temperature (0.6)</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>generate_samples(<span class="st">'I love languages.'</span>, <span class="dv">4</span>, model, vocab_file<span class="op">=</span>VOCAB_FILE, vocab_dir<span class="op">=</span>VOCAB_DIR)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[36], line 2</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># generate 4 samples with the default temperature (0.6)</span>
<span class="ansi-green-fg">----&gt; 2</span> generate_samples(<span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">I love languages.</span><span style="color:rgb(175,0,0)">'</span>, <span style="color:rgb(98,98,98)">4</span>, <span class="ansi-yellow-bg">model</span>, vocab_file<span style="color:rgb(98,98,98)">=</span>VOCAB_FILE, vocab_dir<span style="color:rgb(98,98,98)">=</span>VOCAB_DIR)

<span class="ansi-red-fg">NameError</span>: name 'model' is not defined</pre>
</div>
</div>
</div>
</details></section>
<section id="comparing-overlaps" class="level3">
<h3 class="anchored" data-anchor-id="comparing-overlaps">4.2.2 Comparing overlaps</h3>
<p>Let us now build our functions to compare a sample against another. There are several metrics available as shown in the lectures and you can try experimenting with any one of these. For this assignment, we will be calculating scores for unigram overlaps. One of the more simple metrics is the <a href="https://en.wikipedia.org/wiki/Jaccard_index">Jaccard similarity</a> which gets the intersection over union of two sets. We’ve already implemented it below for your perusal.</p>
<div id="473b8fd8" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> jaccard_similarity(candidate, reference):</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Returns the Jaccard similarity between two token lists</span></span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a><span class="co">        candidate (list of int): tokenized version of the candidate translation</span></span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a><span class="co">        reference (list of int): tokenized version of the reference translation</span></span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a><span class="co">        float: overlap between the two token lists</span></span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># convert the lists to a set to get the unique tokens</span></span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a>    can_unigram_set, ref_unigram_set <span class="op">=</span> <span class="bu">set</span>(candidate), <span class="bu">set</span>(reference)  </span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb49-15"><a href="#cb49-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># get the set of tokens common to both candidate and reference</span></span>
<span id="cb49-16"><a href="#cb49-16" aria-hidden="true" tabindex="-1"></a>    joint_elems <span class="op">=</span> can_unigram_set.intersection(ref_unigram_set)</span>
<span id="cb49-17"><a href="#cb49-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb49-18"><a href="#cb49-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># get the set of all tokens found in either candidate or reference</span></span>
<span id="cb49-19"><a href="#cb49-19" aria-hidden="true" tabindex="-1"></a>    all_elems <span class="op">=</span> can_unigram_set.union(ref_unigram_set)</span>
<span id="cb49-20"><a href="#cb49-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb49-21"><a href="#cb49-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># divide the number of joint elements by the number of all elements</span></span>
<span id="cb49-22"><a href="#cb49-22" aria-hidden="true" tabindex="-1"></a>    overlap <span class="op">=</span> <span class="bu">len</span>(joint_elems) <span class="op">/</span> <span class="bu">len</span>(all_elems)</span>
<span id="cb49-23"><a href="#cb49-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb49-24"><a href="#cb49-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> overlap</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="6bda8021" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="co"># let's try using the function. remember the result here and compare with the next function below.</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>jaccard_similarity([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="38">
<pre><code>0.75</code></pre>
</div>
</div>
<p>One of the more commonly used metrics in machine translation is the ROUGE score. For unigrams, this is called ROUGE-1 and as shown in class, you can output the scores for both precision and recall when comparing two samples. To get the final score, you will want to compute the F1-score as given by:</p>
<p><span class="math display">score = 2* \frac{(precision * recall)}{(precision + recall)}</span></p>
<p><a name="ex08"></a> ### Exercise 08</p>
<p><strong>Instructions</strong>: Implement the <code>rouge1_similarity()</code> function.</p>
<div id="0c94bb0e" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="co"># UNQ_C8</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="co"># GRADED FUNCTION</span></span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a><span class="co"># for making a frequency table easily</span></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rouge1_similarity(system, reference):</span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Returns the ROUGE-1 score between two token lists</span></span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a><span class="co">        system (list of int): tokenized version of the system translation</span></span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a><span class="co">        reference (list of int): tokenized version of the reference translation</span></span>
<span id="cb52-13"><a href="#cb52-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-14"><a href="#cb52-14" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb52-15"><a href="#cb52-15" aria-hidden="true" tabindex="-1"></a><span class="co">        float: overlap between the two token lists</span></span>
<span id="cb52-16"><a href="#cb52-16" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span>    </span>
<span id="cb52-17"><a href="#cb52-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb52-18"><a href="#cb52-18" aria-hidden="true" tabindex="-1"></a>    <span class="co">### START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) </span><span class="al">###</span></span>
<span id="cb52-19"><a href="#cb52-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb52-20"><a href="#cb52-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># make a frequency table of the system tokens (hint: use the Counter class)</span></span>
<span id="cb52-21"><a href="#cb52-21" aria-hidden="true" tabindex="-1"></a>    sys_counter <span class="op">=</span> <span class="va">None</span></span>
<span id="cb52-22"><a href="#cb52-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb52-23"><a href="#cb52-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># make a frequency table of the reference tokens (hint: use the Counter class)</span></span>
<span id="cb52-24"><a href="#cb52-24" aria-hidden="true" tabindex="-1"></a>    ref_counter <span class="op">=</span> <span class="va">None</span></span>
<span id="cb52-25"><a href="#cb52-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb52-26"><a href="#cb52-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># initialize overlap to 0</span></span>
<span id="cb52-27"><a href="#cb52-27" aria-hidden="true" tabindex="-1"></a>    overlap <span class="op">=</span> <span class="va">None</span></span>
<span id="cb52-28"><a href="#cb52-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb52-29"><a href="#cb52-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># run a for loop over the sys_counter object (can be treated as a dictionary)</span></span>
<span id="cb52-30"><a href="#cb52-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> token <span class="kw">in</span> sys_counter:</span>
<span id="cb52-31"><a href="#cb52-31" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb52-32"><a href="#cb52-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># lookup the value of the token in the sys_counter dictionary (hint: use the get() method)</span></span>
<span id="cb52-33"><a href="#cb52-33" aria-hidden="true" tabindex="-1"></a>        token_count_sys <span class="op">=</span> <span class="va">None</span></span>
<span id="cb52-34"><a href="#cb52-34" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb52-35"><a href="#cb52-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># lookup the value of the token in the ref_counter dictionary (hint: use the get() method)</span></span>
<span id="cb52-36"><a href="#cb52-36" aria-hidden="true" tabindex="-1"></a>        token_count_ref <span class="op">=</span> <span class="va">None</span></span>
<span id="cb52-37"><a href="#cb52-37" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb52-38"><a href="#cb52-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># update the overlap by getting the smaller number between the two token counts above</span></span>
<span id="cb52-39"><a href="#cb52-39" aria-hidden="true" tabindex="-1"></a>        overlap <span class="op">+=</span> <span class="va">None</span></span>
<span id="cb52-40"><a href="#cb52-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb52-41"><a href="#cb52-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># get the precision (i.e. number of overlapping tokens / number of system tokens)</span></span>
<span id="cb52-42"><a href="#cb52-42" aria-hidden="true" tabindex="-1"></a>    precision <span class="op">=</span> <span class="va">None</span></span>
<span id="cb52-43"><a href="#cb52-43" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb52-44"><a href="#cb52-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># get the recall (i.e. number of overlapping tokens / number of reference tokens)</span></span>
<span id="cb52-45"><a href="#cb52-45" aria-hidden="true" tabindex="-1"></a>    recall <span class="op">=</span> <span class="va">None</span></span>
<span id="cb52-46"><a href="#cb52-46" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb52-47"><a href="#cb52-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> precision <span class="op">+</span> recall <span class="op">!=</span> <span class="dv">0</span>:</span>
<span id="cb52-48"><a href="#cb52-48" aria-hidden="true" tabindex="-1"></a>        <span class="co"># compute the f1-score</span></span>
<span id="cb52-49"><a href="#cb52-49" aria-hidden="true" tabindex="-1"></a>        rouge1_score <span class="op">=</span> <span class="va">None</span></span>
<span id="cb52-50"><a href="#cb52-50" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb52-51"><a href="#cb52-51" aria-hidden="true" tabindex="-1"></a>        rouge1_score <span class="op">=</span> <span class="dv">0</span> </span>
<span id="cb52-52"><a href="#cb52-52" aria-hidden="true" tabindex="-1"></a>    <span class="co">### </span><span class="re">END</span><span class="co"> CODE HERE </span><span class="al">###</span></span>
<span id="cb52-53"><a href="#cb52-53" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb52-54"><a href="#cb52-54" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> rouge1_score</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="296939c5" class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="co"># notice that this produces a different value from the jaccard similarity earlier</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>rouge1_similarity([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">TypeError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[40], line 2</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># notice that this produces a different value from the jaccard similarity earlier</span>
<span class="ansi-green-fg">----&gt; 2</span> <span class="ansi-yellow-bg">rouge1_similarity</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">[</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">1</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">2</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">3</span><span class="ansi-yellow-bg">]</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">[</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">1</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">2</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">3</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">4</span><span class="ansi-yellow-bg">]</span><span class="ansi-yellow-bg">)</span>

Cell <span class="ansi-green-fg">In[39], line 30</span>, in <span class="ansi-cyan-fg">rouge1_similarity</span><span class="ansi-blue-fg">(system, reference)</span>
<span class="ansi-green-fg ansi-bold">     27</span> overlap <span style="color:rgb(98,98,98)">=</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span>
<span class="ansi-green-fg ansi-bold">     29</span> <span style="font-style:italic;color:rgb(95,135,135)"># run a for loop over the sys_counter object (can be treated as a dictionary)</span>
<span class="ansi-green-fg">---&gt; 30</span> <span style="font-weight:bold;color:rgb(0,135,0)">for</span> token <span style="font-weight:bold;color:rgb(175,0,255)">in</span> sys_counter:
<span class="ansi-green-fg ansi-bold">     31</span>     
<span class="ansi-green-fg ansi-bold">     32</span>     <span style="font-style:italic;color:rgb(95,135,135)"># lookup the value of the token in the sys_counter dictionary (hint: use the get() method)</span>
<span class="ansi-green-fg ansi-bold">     33</span>     token_count_sys <span style="color:rgb(98,98,98)">=</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span>
<span class="ansi-green-fg ansi-bold">     35</span>     <span style="font-style:italic;color:rgb(95,135,135)"># lookup the value of the token in the ref_counter dictionary (hint: use the get() method)</span>

<span class="ansi-red-fg">TypeError</span>: 'NoneType' object is not iterable</pre>
</div>
</div>
</div>
<div id="5200bc60" class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="re">BEGIN</span><span class="co"> UNIT </span><span class="al">TEST</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>w1_unittest.test_rouge1_similarity(rouge1_similarity)</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="re">END</span><span class="co"> UNIT </span><span class="al">TEST</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Expected similarity: 0.8571428571428571
Expected similarity: 0.5
Expected similarity: 0
Expected similarity: 0.5
 0  Tests passed
 4  Tests failed</code></pre>
</div>
</div>
</section>
<section id="overall-score" class="level3">
<h3 class="anchored" data-anchor-id="overall-score">4.2.3 Overall score</h3>
<p>We will now build a function to generate the overall score for a particular sample. As mentioned earlier, we need to compare each sample with all other samples. For instance, if we generated 30 sentences, we will need to compare sentence 1 to sentences 2 to 30. Then, we compare sentence 2 to sentences 1 and 3 to 30, and so forth. At each step, we get the average score of all comparisons to get the overall score for a particular sample. To illustrate, these will be the steps to generate the scores of a 4-sample list.</p>
<ol type="1">
<li>Get similarity score between sample 1 and sample 2</li>
<li>Get similarity score between sample 1 and sample 3</li>
<li>Get similarity score between sample 1 and sample 4</li>
<li>Get average score of the first 3 steps. This will be the overall score of sample 1.</li>
<li>Iterate and repeat until samples 1 to 4 have overall scores.</li>
</ol>
<p>We will be storing the results in a dictionary for easy lookups.</p>
<p><a name="ex09"></a> ### Exercise 09</p>
<p><strong>Instructions</strong>: Implement the <code>average_overlap()</code> function.</p>
<div id="34e8fe7b" class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co"># UNQ_C9</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="co"># GRADED FUNCTION</span></span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> average_overlap(similarity_fn, samples, <span class="op">*</span>ignore_params):</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Returns the arithmetic mean of each candidate sentence in the samples</span></span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a><span class="co">        similarity_fn (function): similarity function used to compute the overlap</span></span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a><span class="co">        samples (list of lists): tokenized version of the translated sentences</span></span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a><span class="co">        *ignore_params: additional parameters will be ignored</span></span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a><span class="co">        dict: scores of each sample</span></span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a><span class="co">            key: index of the sample</span></span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true" tabindex="-1"></a><span class="co">            value: score of the sample</span></span>
<span id="cb56-15"><a href="#cb56-15" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span>  </span>
<span id="cb56-16"><a href="#cb56-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb56-17"><a href="#cb56-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># initialize dictionary</span></span>
<span id="cb56-18"><a href="#cb56-18" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> {}</span>
<span id="cb56-19"><a href="#cb56-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb56-20"><a href="#cb56-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># run a for loop for each sample</span></span>
<span id="cb56-21"><a href="#cb56-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> index_candidate, candidate <span class="kw">in</span> <span class="bu">enumerate</span>(samples):    </span>
<span id="cb56-22"><a href="#cb56-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb56-23"><a href="#cb56-23" aria-hidden="true" tabindex="-1"></a>        <span class="co">### START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) </span><span class="al">###</span></span>
<span id="cb56-24"><a href="#cb56-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb56-25"><a href="#cb56-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># initialize overlap to 0.0</span></span>
<span id="cb56-26"><a href="#cb56-26" aria-hidden="true" tabindex="-1"></a>        overlap <span class="op">=</span> <span class="va">None</span></span>
<span id="cb56-27"><a href="#cb56-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb56-28"><a href="#cb56-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># run a for loop for each sample</span></span>
<span id="cb56-29"><a href="#cb56-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> index_sample, sample <span class="kw">in</span> <span class="bu">enumerate</span>(samples): </span>
<span id="cb56-30"><a href="#cb56-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-31"><a href="#cb56-31" aria-hidden="true" tabindex="-1"></a>            <span class="co"># skip if the candidate index is the same as the sample index</span></span>
<span id="cb56-32"><a href="#cb56-32" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> index_candidate <span class="op">==</span> index_sample:</span>
<span id="cb56-33"><a href="#cb56-33" aria-hidden="true" tabindex="-1"></a>                <span class="cf">continue</span></span>
<span id="cb56-34"><a href="#cb56-34" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb56-35"><a href="#cb56-35" aria-hidden="true" tabindex="-1"></a>            <span class="co"># get the overlap between candidate and sample using the similarity function</span></span>
<span id="cb56-36"><a href="#cb56-36" aria-hidden="true" tabindex="-1"></a>            sample_overlap <span class="op">=</span> <span class="va">None</span></span>
<span id="cb56-37"><a href="#cb56-37" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb56-38"><a href="#cb56-38" aria-hidden="true" tabindex="-1"></a>            <span class="co"># add the sample overlap to the total overlap</span></span>
<span id="cb56-39"><a href="#cb56-39" aria-hidden="true" tabindex="-1"></a>            overlap <span class="op">+=</span> <span class="va">None</span></span>
<span id="cb56-40"><a href="#cb56-40" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb56-41"><a href="#cb56-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># get the score for the candidate by computing the average</span></span>
<span id="cb56-42"><a href="#cb56-42" aria-hidden="true" tabindex="-1"></a>        score <span class="op">=</span> <span class="va">None</span></span>
<span id="cb56-43"><a href="#cb56-43" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb56-44"><a href="#cb56-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># save the score in the dictionary. use index as the key.</span></span>
<span id="cb56-45"><a href="#cb56-45" aria-hidden="true" tabindex="-1"></a>        scores[index_candidate] <span class="op">=</span> <span class="va">None</span></span>
<span id="cb56-46"><a href="#cb56-46" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb56-47"><a href="#cb56-47" aria-hidden="true" tabindex="-1"></a>        <span class="co">### </span><span class="re">END</span><span class="co"> CODE HERE </span><span class="al">###</span></span>
<span id="cb56-48"><a href="#cb56-48" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> scores</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="46ea5659" class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>average_overlap(jaccard_similarity, [[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>], [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">5</span>]], [<span class="fl">0.4</span>, <span class="fl">0.2</span>, <span class="fl">0.5</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">TypeError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[43], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> <span class="ansi-yellow-bg">average_overlap</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">jaccard_similarity</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">[</span><span class="ansi-yellow-bg">[</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">1</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">2</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">3</span><span class="ansi-yellow-bg">]</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">[</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">1</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">2</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">4</span><span class="ansi-yellow-bg">]</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">[</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">1</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">2</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">4</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">5</span><span class="ansi-yellow-bg">]</span><span class="ansi-yellow-bg">]</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">[</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">0.4</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">0.2</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">0.5</span><span class="ansi-yellow-bg">]</span><span class="ansi-yellow-bg">)</span>

Cell <span class="ansi-green-fg">In[42], line 39</span>, in <span class="ansi-cyan-fg">average_overlap</span><span class="ansi-blue-fg">(similarity_fn, samples, *ignore_params)</span>
<span class="ansi-green-fg ansi-bold">     36</span>     sample_overlap <span style="color:rgb(98,98,98)">=</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span>
<span class="ansi-green-fg ansi-bold">     38</span>     <span style="font-style:italic;color:rgb(95,135,135)"># add the sample overlap to the total overlap</span>
<span class="ansi-green-fg">---&gt; 39</span>     overlap <span style="color:rgb(98,98,98)">+</span><span style="color:rgb(98,98,98)">=</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span>
<span class="ansi-green-fg ansi-bold">     41</span> <span style="font-style:italic;color:rgb(95,135,135)"># get the score for the candidate by computing the average</span>
<span class="ansi-green-fg ansi-bold">     42</span> score <span style="color:rgb(98,98,98)">=</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span>

<span class="ansi-red-fg">TypeError</span>: unsupported operand type(s) for +=: 'NoneType' and 'NoneType'</pre>
</div>
</div>
</div>
<div id="6f2e3991" class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="re">BEGIN</span><span class="co"> UNIT </span><span class="al">TEST</span></span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>w1_unittest.test_average_overlap(average_overlap)</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="re">END</span><span class="co"> UNIT </span><span class="al">TEST</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Expected output does not match
Expected output does not match
 0  Tests passed
 2  Tests failed</code></pre>
</div>
</div>
<p>In practice, it is also common to see the weighted mean being used to calculate the overall score instead of just the arithmetic mean. We have implemented it below and you can use it in your experiements to see which one will give better results.</p>
<div id="f8073234" class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> weighted_avg_overlap(similarity_fn, samples, log_probs):</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Returns the weighted mean of each candidate sentence in the samples</span></span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a><span class="co">        samples (list of lists): tokenized version of the translated sentences</span></span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a><span class="co">        log_probs (list of float): log probability of the translated sentences</span></span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a><span class="co">        dict: scores of each sample</span></span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a><span class="co">            key: index of the sample</span></span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a><span class="co">            value: score of the sample</span></span>
<span id="cb60-12"><a href="#cb60-12" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb60-13"><a href="#cb60-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb60-14"><a href="#cb60-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># initialize dictionary</span></span>
<span id="cb60-15"><a href="#cb60-15" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> {}</span>
<span id="cb60-16"><a href="#cb60-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb60-17"><a href="#cb60-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># run a for loop for each sample</span></span>
<span id="cb60-18"><a href="#cb60-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> index_candidate, candidate <span class="kw">in</span> <span class="bu">enumerate</span>(samples):    </span>
<span id="cb60-19"><a href="#cb60-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb60-20"><a href="#cb60-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># initialize overlap and weighted sum</span></span>
<span id="cb60-21"><a href="#cb60-21" aria-hidden="true" tabindex="-1"></a>        overlap, weight_sum <span class="op">=</span> <span class="fl">0.0</span>, <span class="fl">0.0</span></span>
<span id="cb60-22"><a href="#cb60-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb60-23"><a href="#cb60-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># run a for loop for each sample</span></span>
<span id="cb60-24"><a href="#cb60-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> index_sample, (sample, logp) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(samples, log_probs)):</span>
<span id="cb60-25"><a href="#cb60-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-26"><a href="#cb60-26" aria-hidden="true" tabindex="-1"></a>            <span class="co"># skip if the candidate index is the same as the sample index            </span></span>
<span id="cb60-27"><a href="#cb60-27" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> index_candidate <span class="op">==</span> index_sample:</span>
<span id="cb60-28"><a href="#cb60-28" aria-hidden="true" tabindex="-1"></a>                <span class="cf">continue</span></span>
<span id="cb60-29"><a href="#cb60-29" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb60-30"><a href="#cb60-30" aria-hidden="true" tabindex="-1"></a>            <span class="co"># convert log probability to linear scale</span></span>
<span id="cb60-31"><a href="#cb60-31" aria-hidden="true" tabindex="-1"></a>            sample_p <span class="op">=</span> <span class="bu">float</span>(np.exp(logp))</span>
<span id="cb60-32"><a href="#cb60-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-33"><a href="#cb60-33" aria-hidden="true" tabindex="-1"></a>            <span class="co"># update the weighted sum</span></span>
<span id="cb60-34"><a href="#cb60-34" aria-hidden="true" tabindex="-1"></a>            weight_sum <span class="op">+=</span> sample_p</span>
<span id="cb60-35"><a href="#cb60-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-36"><a href="#cb60-36" aria-hidden="true" tabindex="-1"></a>            <span class="co"># get the unigram overlap between candidate and sample</span></span>
<span id="cb60-37"><a href="#cb60-37" aria-hidden="true" tabindex="-1"></a>            sample_overlap <span class="op">=</span> similarity_fn(candidate, sample)</span>
<span id="cb60-38"><a href="#cb60-38" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb60-39"><a href="#cb60-39" aria-hidden="true" tabindex="-1"></a>            <span class="co"># update the overlap</span></span>
<span id="cb60-40"><a href="#cb60-40" aria-hidden="true" tabindex="-1"></a>            overlap <span class="op">+=</span> sample_p <span class="op">*</span> sample_overlap</span>
<span id="cb60-41"><a href="#cb60-41" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb60-42"><a href="#cb60-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># get the score for the candidate</span></span>
<span id="cb60-43"><a href="#cb60-43" aria-hidden="true" tabindex="-1"></a>        score <span class="op">=</span> overlap <span class="op">/</span> weight_sum</span>
<span id="cb60-44"><a href="#cb60-44" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb60-45"><a href="#cb60-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># save the score in the dictionary. use index as the key.</span></span>
<span id="cb60-46"><a href="#cb60-46" aria-hidden="true" tabindex="-1"></a>        scores[index_candidate] <span class="op">=</span> score</span>
<span id="cb60-47"><a href="#cb60-47" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb60-48"><a href="#cb60-48" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> scores</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="1d6bcb06" class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>weighted_avg_overlap(jaccard_similarity, [[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>], [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">5</span>]], [<span class="fl">0.4</span>, <span class="fl">0.2</span>, <span class="fl">0.5</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="46">
<pre><code>{0: 0.44255574831883415, 1: 0.631244796869735, 2: 0.5575581009406329}</code></pre>
</div>
</div>
</section>
<section id="putting-it-all-together" class="level3">
<h3 class="anchored" data-anchor-id="putting-it-all-together">4.2.4 Putting it all together</h3>
<p>We will now put everything together and develop the <code>mbr_decode()</code> function. Please use the helper functions you just developed to complete this. You will want to generate samples, get the score for each sample, get the highest score among all samples, then detokenize this sample to get the translated sentence.</p>
<p><a name="ex10"></a> ### Exercise 10</p>
<p><strong>Instructions</strong>: Implement the <code>mbr_overlap()</code> function.</p>
<div id="f1117265" class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="co"># UNQ_C10</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a><span class="co"># GRADED FUNCTION</span></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mbr_decode(sentence, n_samples, score_fn, similarity_fn, NMTAttn<span class="op">=</span><span class="va">None</span>, temperature<span class="op">=</span><span class="fl">0.6</span>, vocab_file<span class="op">=</span><span class="va">None</span>, vocab_dir<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Returns the translated sentence using Minimum Bayes Risk decoding</span></span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a><span class="co">        sentence (str): sentence to translate.</span></span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a><span class="co">        n_samples (int): number of samples to generate</span></span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a><span class="co">        score_fn (function): function that generates the score for each sample</span></span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a><span class="co">        similarity_fn (function): function used to compute the overlap between a pair of samples</span></span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a><span class="co">        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.</span></span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a><span class="co">        temperature (float): parameter for sampling ranging from 0.0 to 1.0.</span></span>
<span id="cb63-13"><a href="#cb63-13" aria-hidden="true" tabindex="-1"></a><span class="co">            0.0: same as argmax, always pick the most probable token</span></span>
<span id="cb63-14"><a href="#cb63-14" aria-hidden="true" tabindex="-1"></a><span class="co">            1.0: sampling from the distribution (can sometimes say random things)</span></span>
<span id="cb63-15"><a href="#cb63-15" aria-hidden="true" tabindex="-1"></a><span class="co">        vocab_file (str): filename of the vocabulary</span></span>
<span id="cb63-16"><a href="#cb63-16" aria-hidden="true" tabindex="-1"></a><span class="co">        vocab_dir (str): path to the vocabulary file</span></span>
<span id="cb63-17"><a href="#cb63-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-18"><a href="#cb63-18" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb63-19"><a href="#cb63-19" aria-hidden="true" tabindex="-1"></a><span class="co">        str: the translated sentence</span></span>
<span id="cb63-20"><a href="#cb63-20" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb63-21"><a href="#cb63-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb63-22"><a href="#cb63-22" aria-hidden="true" tabindex="-1"></a>    <span class="co">### START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) </span><span class="al">###</span></span>
<span id="cb63-23"><a href="#cb63-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># generate samples</span></span>
<span id="cb63-24"><a href="#cb63-24" aria-hidden="true" tabindex="-1"></a>    samples, log_probs <span class="op">=</span> <span class="va">None</span></span>
<span id="cb63-25"><a href="#cb63-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb63-26"><a href="#cb63-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># use the scoring function to get a dictionary of scores</span></span>
<span id="cb63-27"><a href="#cb63-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># pass in the relevant parameters as shown in the function definition of </span></span>
<span id="cb63-28"><a href="#cb63-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the mean methods you developed earlier</span></span>
<span id="cb63-29"><a href="#cb63-29" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> <span class="va">None</span></span>
<span id="cb63-30"><a href="#cb63-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb63-31"><a href="#cb63-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># find the key with the highest score</span></span>
<span id="cb63-32"><a href="#cb63-32" aria-hidden="true" tabindex="-1"></a>    max_index <span class="op">=</span> <span class="va">None</span></span>
<span id="cb63-33"><a href="#cb63-33" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb63-34"><a href="#cb63-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># detokenize the token list associated with the max_index</span></span>
<span id="cb63-35"><a href="#cb63-35" aria-hidden="true" tabindex="-1"></a>    translated_sentence <span class="op">=</span> <span class="va">None</span></span>
<span id="cb63-36"><a href="#cb63-36" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb63-37"><a href="#cb63-37" aria-hidden="true" tabindex="-1"></a>    <span class="co">### </span><span class="re">END</span><span class="co"> CODE HERE </span><span class="al">###</span></span>
<span id="cb63-38"><a href="#cb63-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (translated_sentence, max_index, scores)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="4adb9ef0" class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>TEMPERATURE <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a><span class="co"># put a custom string here</span></span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>your_sentence <span class="op">=</span> <span class="st">'She speaks English and German.'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="0cd9ca70" class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>mbr_decode(your_sentence, <span class="dv">4</span>, weighted_avg_overlap, jaccard_similarity, model, TEMPERATURE, vocab_file<span class="op">=</span>VOCAB_FILE, vocab_dir<span class="op">=</span>VOCAB_DIR)[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[49], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> mbr_decode(your_sentence, <span style="color:rgb(98,98,98)">4</span>, weighted_avg_overlap, jaccard_similarity, <span class="ansi-yellow-bg">model</span>, TEMPERATURE, vocab_file<span style="color:rgb(98,98,98)">=</span>VOCAB_FILE, vocab_dir<span style="color:rgb(98,98,98)">=</span>VOCAB_DIR)[<span style="color:rgb(98,98,98)">0</span>]

<span class="ansi-red-fg">NameError</span>: name 'model' is not defined</pre>
</div>
</div>
</div>
<div id="fd9c1c9c" class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>mbr_decode(<span class="st">'Congratulations!'</span>, <span class="dv">4</span>, average_overlap, rouge1_similarity, model, TEMPERATURE, vocab_file<span class="op">=</span>VOCAB_FILE, vocab_dir<span class="op">=</span>VOCAB_DIR)[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[50], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> mbr_decode(<span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">Congratulations!</span><span style="color:rgb(175,0,0)">'</span>, <span style="color:rgb(98,98,98)">4</span>, average_overlap, rouge1_similarity, <span class="ansi-yellow-bg">model</span>, TEMPERATURE, vocab_file<span style="color:rgb(98,98,98)">=</span>VOCAB_FILE, vocab_dir<span style="color:rgb(98,98,98)">=</span>VOCAB_DIR)[<span style="color:rgb(98,98,98)">0</span>]

<span class="ansi-red-fg">NameError</span>: name 'model' is not defined</pre>
</div>
</div>
</div>
<div id="a81447b0" class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>mbr_decode(<span class="st">'You have completed the assignment!'</span>, <span class="dv">4</span>, average_overlap, rouge1_similarity, model, TEMPERATURE, vocab_file<span class="op">=</span>VOCAB_FILE, vocab_dir<span class="op">=</span>VOCAB_DIR)[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[51], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> mbr_decode(<span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">You have completed the assignment!</span><span style="color:rgb(175,0,0)">'</span>, <span style="color:rgb(98,98,98)">4</span>, average_overlap, rouge1_similarity, <span class="ansi-yellow-bg">model</span>, TEMPERATURE, vocab_file<span style="color:rgb(98,98,98)">=</span>VOCAB_FILE, vocab_dir<span style="color:rgb(98,98,98)">=</span>VOCAB_DIR)[<span style="color:rgb(98,98,98)">0</span>]

<span class="ansi-red-fg">NameError</span>: name 'model' is not defined</pre>
</div>
</div>
</div>
<p><strong>This unit test take a while to run. Please be patient</strong></p>
<div id="a3911a99" class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="re">BEGIN</span><span class="co"> UNIT </span><span class="al">TEST</span></span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>w1_unittest.test_mbr_decode(mbr_decode, model)</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="re">END</span><span class="co"> UNIT </span><span class="al">TEST</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[52], line 2</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># BEGIN UNIT TEST</span>
<span class="ansi-green-fg">----&gt; 2</span> w1_unittest<span style="color:rgb(98,98,98)">.</span>test_mbr_decode(mbr_decode, <span class="ansi-yellow-bg">model</span>)
<span class="ansi-green-fg ansi-bold">      3</span> <span style="font-style:italic;color:rgb(95,135,135)"># END UNIT TEST</span>

<span class="ansi-red-fg">NameError</span>: name 'model' is not defined</pre>
</div>
</div>
</div>
<section id="congratulations-next-week-youll-dive-deeper-into-attention-models-and-study-the-transformer-architecture.-you-will-build-another-network-but-without-the-recurrent-part.-it-will-show-that-attention-is-all-you-need-it-should-be-fun" class="level4">
<h4 class="anchored" data-anchor-id="congratulations-next-week-youll-dive-deeper-into-attention-models-and-study-the-transformer-architecture.-you-will-build-another-network-but-without-the-recurrent-part.-it-will-show-that-attention-is-all-you-need-it-should-be-fun">Congratulations! Next week, you’ll dive deeper into attention models and study the Transformer architecture. You will build another network but without the recurrent part. It will show that attention is all you need! It should be fun!</h4>


</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Assignment 1: {Neural} {Machine} {Translation}},
  date = {2025-02-06},
  url = {https://orenbochman.github.io/notes-nlp/posts/c4w1/assignment.html},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas" role="listitem">
Bochman, Oren. 2025. <span>“Assignment 1: Neural Machine
Translation.”</span> February 6, 2025. <a href="https://orenbochman.github.io/notes-nlp/posts/c4w1/assignment.html">https://orenbochman.github.io/notes-nlp/posts/c4w1/assignment.html</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("^(?:http:|https:)\/\/www\.quarto\.org\/custom");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
          // default icon
          link.classList.add("external");
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© Copyright 2023-2025, Oren Bochman</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/OrenBochman/notes-nlp/edit/main/posts/c4w1/assignment.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/OrenBochman/notes-nlp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This page is built with 💛 and <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"fade","descPosition":"right","loop":true,"openEffect":"fade","selector":".lightbox","skin":"my-css-class"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>