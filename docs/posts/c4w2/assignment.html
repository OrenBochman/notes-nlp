<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Oren Bochman">

<title>Transformer Summarizer – NLP Specialization</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-d166b450ba5a8e9f7a0ab969bf6592c1.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-1372234da3246ce8e868649689ba5ed0.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-d99a2a2a191b5c7f2a9a83135e7f0803.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-sidebar floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">NLP Specialization</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">NLP with Attention Models</li><li class="breadcrumb-item"><a href="../../posts/c4w2/index.html">Text Summarization</a></li><li class="breadcrumb-item"><a href="../../posts/c4w2/assignment.html">A4 - Transformer Summarizer</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">NLP with Attention Models</li><li class="breadcrumb-item"><a href="../../posts/c4w2/index.html">Text Summarization</a></li><li class="breadcrumb-item"><a href="../../posts/c4w2/assignment.html">A4 - Transformer Summarizer</a></li></ol></nav>
      <h1 class="title">Transformer Summarizer</h1>
            <p class="subtitle lead">Sequence Models</p>
                                <div class="quarto-categories">
                <div class="quarto-category">NLP</div>
                <div class="quarto-category">Coursera</div>
                <div class="quarto-category">Lab</div>
                <div class="quarto-category">Sequence Models</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta column-body">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Oren Bochman </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">Thursday, February 6, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Classification &amp; Vector Spaces</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Logistic Regression</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c1w1/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c1w1/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Preprocessing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c1w1/lab02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - Frequencies</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c1w1/lab03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L3 - Visualizing tweets</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c1w1/assignment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A1 - Logistic Regression</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Probability &amp; Bayes Rule</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c1w2/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c1w2/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Visualizing Naive Bayes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c1w2/assignment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A2 - Logistic Regression</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false">
 <span class="menu-text">Vector Space Models &amp; PCA</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c1w3/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c1w3/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Linear algebra with NumPy</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c1w3/lab02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - Manipulating word embeddings</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c1w3/assignment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A3 - Hello Vectors</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">MT &amp; Document Search via KNN</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c1w4/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c1w3/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Vector manipulation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c1w3/lab02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - Hash functions and multiplanes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c1w3/assignment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A4 - Naive Machine Translation and LSH</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Probabilistic Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false">
 <span class="menu-text">Autocorrect &amp; Dynamic Programming</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c2w1/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c2w1/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Building the vocabulary</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c2w1/lab02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - Candidates from String Edits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c2w1/assignment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A1 - Auto Correct</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false">
 <span class="menu-text">POS tagging &amp; HMMS</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c2w2/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c2w2/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Vocabulary with unknowns</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c2w2/lab02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - Working with tags and Numpy</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c2w2/assignment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A2 - POS tagging</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false">
 <span class="menu-text">Autocomplete &amp; Language Models</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c2w3/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c2w3/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - N-grams Corpus preprocessing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c2w3/lab02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - Building the language model</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c2w3/lab03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L3 - Out of vocabulary words</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c2w3/assignment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A3 - Auto-Complete</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false">
 <span class="menu-text">Word embeddings with neural networks</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c2w4/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c2w4/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Data preparation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c2w4/lab02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - Intro to CBOW</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c2w4/lab03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L3 - Training the CBOW</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c2w4/assignment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A4 - Word embeddings</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true">
 <span class="menu-text">Sequence Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="false">
 <span class="menu-text">Neural Networks for Sentiment Analysis</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c3w1/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c3w1/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Introduction to Trax</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c3w1/lab02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - Classes and Subclasses</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c3w1/lab03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L3 - Data Generators</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c3w1/assignment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A1 - Sentiment with Deep Neural Networks</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="false">
 <span class="menu-text">RNN for Language Modeling</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c3w2/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c3w2/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Hidden State Activation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c3w2/lab02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - Calculating Perplexity</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c3w2/lab03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L3 - Vanilla RNNs, GRUs and the scan function</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c3w2/lab04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L4 - Creating a GRU model using Trax</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c3w2/assignment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A2 - Deep N-grams</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="false">
 <span class="menu-text">LSTMs and Named Entity Recognition</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-14" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c3w3/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c3w3/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Vanishing Gradients</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c3w3/assignment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A3 - NER</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="false">
 <span class="menu-text">Siamese Networks</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-15" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c3w4/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c3w4/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Creating a Siamese Model using Trax</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c3w4/lab02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - Modified Triplet Loss</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c3w4/lab03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L3 - Evaluate a Siamese Model</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c3w4/assignment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A4 - Question duplicates</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-16" role="navigation" aria-expanded="true">
 <span class="menu-text">NLP with Attention Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-16" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-16" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-17" role="navigation" aria-expanded="false">
 <span class="menu-text">Neural Machine Translation</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-17" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-17" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c4w1/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c4w1/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Stack Semantics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c4w1/lab02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - BLEU Score</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c4w1/assignment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A4 - NMT with Attention</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-18" role="navigation" aria-expanded="true">
 <span class="menu-text">Text Summarization</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-18" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-18" class="collapse list-unstyled sidebar-section depth2 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c4w2/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c4w2/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Attention</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c4w2/lab02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - The Transformer Decoder</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c4w2/assignment.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">A4 - Transformer Summarizer</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-19" role="navigation" aria-expanded="false">
 <span class="menu-text">Question Answering</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-19" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-19" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c4w3/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c4w3/lab01.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - SentencePiece and BPE</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c4w3/lab02.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - BERT Loss</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c4w3/lab03.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L3 - T5</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c4w3/assignment.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A4 - Question Answering</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-20" role="navigation" aria-expanded="false">
 <span class="menu-text">Chat Bots</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-20" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-20" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c4w4/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c4w4/lab01.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Reformer LSH</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c4w4/lab02.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - Revnet</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/c4w4/assignment.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">A4 - Chatbot</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#outline" id="toc-outline" class="nav-link active" data-scroll-target="#outline">Outline</a>
  <ul class="collapse">
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a></li>
  </ul></li>
  <li><a href="#preprocessing-for-language-models-concatenate-it" id="toc-preprocessing-for-language-models-concatenate-it" class="nav-link" data-scroll-target="#preprocessing-for-language-models-concatenate-it">1.2 Preprocessing for Language Models: Concatenate It!</a></li>
  <li><a href="#batching-with-bucketing" id="toc-batching-with-bucketing" class="nav-link" data-scroll-target="#batching-with-bucketing">1.3 Batching with bucketing</a></li>
  <li><a href="#causal-attention" id="toc-causal-attention" class="nav-link" data-scroll-target="#causal-attention">2.2 Causal Attention</a>
  <ul class="collapse">
  <li><a href="#support-functions" id="toc-support-functions" class="nav-link" data-scroll-target="#support-functions">Support Functions</a></li>
  <li><a href="#causal-attention-function" id="toc-causal-attention-function" class="nav-link" data-scroll-target="#causal-attention-function">Causal Attention Function</a></li>
  </ul></li>
  <li><a href="#transformer-decoder-block" id="toc-transformer-decoder-block" class="nav-link" data-scroll-target="#transformer-decoder-block">2.3 Transformer decoder block</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/OrenBochman/notes-nlp/edit/main/posts/c4w2/assignment.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/OrenBochman/notes-nlp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full column-body" id="quarto-document-content">






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/Course-Logo-3-3.webp" class="nolightbox img-fluid figure-img"></p>
<figcaption>course banner</figcaption>
</figure>
</div></div><div class="callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Honor code alert
</div>
</div>
<div class="callout-body-container callout-body">
<p>Due to the Coursera Honor Code, I cannot provide the solutions to the assignments.</p>
<ul>
<li>This notebook is the original notebook provided by the course</li>
<li>It is setup to run without stopping for errors.</li>
<li>It is also likely to be out of date as the course has had some updates since I took it.</li>
<li>Although I aced the course this assignment was the most time consuming.</li>
<li>Good luck with the assignment it should make we a better programmer.</li>
<li>It is also a good idea to go over it a few times until we can do it easily.</li>
</ul>
</div>
</div>
<p>Welcome to the second assignment of course 4. In this assignment you will explore summarization using the transformer model. Yes, you will implement the transformer decoder from scratch, but we will slowly walk you through it. There are many hints in this notebook so feel free to use them as needed.</p>
<p><img src="transformerNews.png"></p>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li><a href="#0">Introduction</a></li>
<li><a href="#1">Part 1: Importing the dataset</a>
<ul>
<li><a href="#1.1">1.1 Encode &amp; Decode helper functions</a></li>
<li><a href="#1.2">1.2 Defining parameters</a></li>
<li><a href="#1.3">1.3 Exploring the data</a></li>
</ul></li>
<li><a href="#2">Part 2: Summarization with transformer</a>
<ul>
<li><a href="#2.1">2.1 Dot product attention</a>
<ul>
<li><a href="#ex01">Exercise 01</a></li>
</ul></li>
<li><a href="#2.2">2.2 Causal Attention</a>
<ul>
<li><a href="#ex02">Exercise 02</a></li>
</ul></li>
<li><a href="#2.3">2.3 Transformer decoder block</a>
<ul>
<li><a href="#ex03">Exercise 03</a></li>
</ul></li>
<li><a href="#2.4">2.4 Transformer Language model</a>
<ul>
<li><a href="#ex04">Exercise 04</a></li>
</ul></li>
</ul></li>
<li><a href="#3">Part 3: Training</a>
<ul>
<li><a href="#3.1">3.1 Training the model</a>
<ul>
<li><a href="#ex05">Exercise 05</a></li>
</ul></li>
</ul></li>
<li><a href="#4">Part 4: Evaluation</a>
<ul>
<li><a href="#4.1">4.1 Loading in a trained model</a></li>
</ul></li>
<li><a href="#5">Part 5: Testing with your own input</a>
<ul>
<li><a href="#ex06">Exercise 6</a></li>
<li><a href="#5.1">5.1 Greedy decoding</a>
<ul>
<li><a href="#ex07">Exercise 07</a></li>
</ul></li>
</ul></li>
</ul>
<p><a name="0"></a></p>
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<p>Summarization is an important task in natural language processing and could be useful for a consumer enterprise. For example, bots can be used to scrape articles, summarize them, and then you can use sentiment analysis to identify the sentiment about certain stocks. Anyways who wants to read an article or a long email today, when you can build a transformer to summarize text for you. Let’s get started, by completing this assignment you will learn to:</p>
<ul>
<li>Use built-in functions to preprocess your data</li>
<li>Implement DotProductAttention</li>
<li>Implement Causal Attention</li>
<li>Understand how attention works</li>
<li>Build the transformer model</li>
<li>Evaluate your model</li>
<li>Summarize an article</li>
</ul>
<p>As you can tell, this model is slightly different than the ones you have already implemented. This is heavily based on attention and does not rely on sequences, which allows for parallel computing.</p>
<div id="40572652" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> textwrap</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>wrapper <span class="op">=</span> textwrap.TextWrapper(width<span class="op">=</span><span class="dv">70</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> trax</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> trax <span class="im">import</span> layers <span class="im">as</span> tl</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> trax.fastmath <span class="im">import</span> numpy <span class="im">as</span> jnp</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># to print the entire np array</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>np.set_printoptions(threshold<span class="op">=</span>sys.maxsize)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>2025-02-06 21:20:11.967017: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1738869611.988632 1103379 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1738869611.993503 1103379 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered</code></pre>
</div>
</div>
<p><a name="1"></a> ## Part 1: Importing the dataset</p>
<p>Trax makes it easy to work with Tensorflow’s datasets:</p>
<div id="7b174446" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># This will download the dataset if no data_dir is specified.</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Downloading and processing can take bit of time,</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># so we have the data already in 'data/' for you</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing CNN/DailyMail articles dataset</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>train_stream_fn <span class="op">=</span> trax.data.TFDS(<span class="st">'cnn_dailymail'</span>,</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>                                 data_dir<span class="op">=</span><span class="st">'data/'</span>,</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>                                 keys<span class="op">=</span>(<span class="st">'article'</span>, <span class="st">'highlights'</span>),</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>                                 train<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co"># This should be much faster as the data is downloaded already.</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>eval_stream_fn <span class="op">=</span> trax.data.TFDS(<span class="st">'cnn_dailymail'</span>,</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>                                data_dir<span class="op">=</span><span class="st">'data/'</span>,</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>                                keys<span class="op">=</span>(<span class="st">'article'</span>, <span class="st">'highlights'</span>),</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>                                train<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/jax/_src/xla_bridge.py:1234: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.
  warnings.warn(</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Downloading and preparing dataset 558.32 MiB (download: 558.32 MiB, generated: 1.29 GiB, total: 1.84 GiB) to data/cnn_dailymail/3.4.0...</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"7f1ff6dbc48f4c46a19b6c597f935cf8","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"d1d6acacb275498e8e4eb8d749499823","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"a879bbe9f17a4432b23aa66552082189","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NonMatchingChecksumError</span>                  Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[2], line 6</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># This will download the dataset if no data_dir is specified.</span>
<span class="ansi-green-fg ansi-bold">      2</span> <span style="font-style:italic;color:rgb(95,135,135)"># Downloading and processing can take bit of time,</span>
<span class="ansi-green-fg ansi-bold">      3</span> <span style="font-style:italic;color:rgb(95,135,135)"># so we have the data already in 'data/' for you</span>
<span class="ansi-green-fg ansi-bold">      4</span> 
<span class="ansi-green-fg ansi-bold">      5</span> <span style="font-style:italic;color:rgb(95,135,135)"># Importing CNN/DailyMail articles dataset</span>
<span class="ansi-green-fg">----&gt; 6</span> train_stream_fn <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">trax</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">data</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">TFDS</span><span class="ansi-yellow-bg">(</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">'</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">cnn_dailymail</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">'</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">      7</span> <span class="ansi-yellow-bg">                                 </span><span class="ansi-yellow-bg">data_dir</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">'</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">data/</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">'</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">      8</span> <span class="ansi-yellow-bg">                                 </span><span class="ansi-yellow-bg">keys</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">(</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">'</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">article</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">'</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">'</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">highlights</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">'</span><span class="ansi-yellow-bg">)</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">      9</span> <span class="ansi-yellow-bg">                                 </span><span class="ansi-yellow-bg">train</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">True</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">     11</span> <span style="font-style:italic;color:rgb(95,135,135)"># This should be much faster as the data is downloaded already.</span>
<span class="ansi-green-fg ansi-bold">     12</span> eval_stream_fn <span style="color:rgb(98,98,98)">=</span> trax<span style="color:rgb(98,98,98)">.</span>data<span style="color:rgb(98,98,98)">.</span>TFDS(<span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">cnn_dailymail</span><span style="color:rgb(175,0,0)">'</span>,
<span class="ansi-green-fg ansi-bold">     13</span>                                 data_dir<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">data/</span><span style="color:rgb(175,0,0)">'</span>,
<span class="ansi-green-fg ansi-bold">     14</span>                                 keys<span style="color:rgb(98,98,98)">=</span>(<span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">article</span><span style="color:rgb(175,0,0)">'</span>, <span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">highlights</span><span style="color:rgb(175,0,0)">'</span>),
<span class="ansi-green-fg ansi-bold">     15</span>                                 train<span style="color:rgb(98,98,98)">=</span><span style="font-weight:bold;color:rgb(0,135,0)">False</span>)

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/gin/config.py:1605</span>, in <span class="ansi-cyan-fg">_make_gin_wrapper.&lt;locals&gt;.gin_wrapper</span><span class="ansi-blue-fg">(*args, **kwargs)</span>
<span class="ansi-green-fg ansi-bold">   1603</span> scope_info <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)"> in scope </span><span style="color:rgb(175,0,0)">'</span><span style="font-weight:bold;color:rgb(175,95,135)">{}</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">"</span><span style="color:rgb(98,98,98)">.</span>format(scope_str) <span style="font-weight:bold;color:rgb(0,135,0)">if</span> scope_str <span style="font-weight:bold;color:rgb(0,135,0)">else</span> <span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">'</span>
<span class="ansi-green-fg ansi-bold">   1604</span> err_str <span style="color:rgb(98,98,98)">=</span> err_str<span style="color:rgb(98,98,98)">.</span>format(name, fn_or_cls, scope_info)
<span class="ansi-green-fg">-&gt; 1605</span> <span class="ansi-yellow-bg">utils</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">augment_exception_message_and_reraise</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">e</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">err_str</span><span class="ansi-yellow-bg">)</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/gin/utils.py:41</span>, in <span class="ansi-cyan-fg">augment_exception_message_and_reraise</span><span class="ansi-blue-fg">(exception, message)</span>
<span class="ansi-green-fg ansi-bold">     39</span> proxy <span style="color:rgb(98,98,98)">=</span> ExceptionProxy()
<span class="ansi-green-fg ansi-bold">     40</span> ExceptionProxy<span style="color:rgb(98,98,98)">.</span><span style="color:rgb(0,0,135)">__qualname__</span> <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)">type</span>(exception)<span style="color:rgb(98,98,98)">.</span><span style="color:rgb(0,0,135)">__qualname__</span>
<span class="ansi-green-fg">---&gt; 41</span> <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> proxy<span style="color:rgb(98,98,98)">.</span>with_traceback(exception<span style="color:rgb(98,98,98)">.</span>__traceback__) <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">None</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/gin/config.py:1582</span>, in <span class="ansi-cyan-fg">_make_gin_wrapper.&lt;locals&gt;.gin_wrapper</span><span class="ansi-blue-fg">(*args, **kwargs)</span>
<span class="ansi-green-fg ansi-bold">   1579</span> new_kwargs<span style="color:rgb(98,98,98)">.</span>update(kwargs)
<span class="ansi-green-fg ansi-bold">   1581</span> <span style="font-weight:bold;color:rgb(0,135,0)">try</span>:
<span class="ansi-green-fg">-&gt; 1582</span>   <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span class="ansi-yellow-bg">fn</span><span class="ansi-yellow-bg">(</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">new_args</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">new_kwargs</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">   1583</span> <span style="font-weight:bold;color:rgb(0,135,0)">except</span> <span style="font-weight:bold;color:rgb(215,95,95)">Exception</span> <span style="font-weight:bold;color:rgb(0,135,0)">as</span> e:  <span style="font-style:italic;color:rgb(95,135,135)"># pylint: disable=broad-except</span>
<span class="ansi-green-fg ansi-bold">   1584</span>   err_str <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">'</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/data/tf_inputs.py:336</span>, in <span class="ansi-cyan-fg">TFDS</span><span class="ansi-blue-fg">(dataset_name, data_dir, tfds_preprocess_fn, keys, train, use_alt_eval, shuffle_train, host_id, n_hosts, eval_holdout_size)</span>
<span class="ansi-green-fg ansi-bold">    333</span> <span style="font-weight:bold;color:rgb(0,135,0)">else</span>:
<span class="ansi-green-fg ansi-bold">    334</span>   subsplit <span style="color:rgb(98,98,98)">=</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span>
<span class="ansi-green-fg ansi-bold">    335</span> train_data, eval_data, _ <span style="color:rgb(98,98,98)">=</span> (
<span class="ansi-green-fg">--&gt; 336</span>     <span class="ansi-yellow-bg">_train_and_eval_dataset</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">dataset_name</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">    337</span> <span class="ansi-yellow-bg">                            </span><span class="ansi-yellow-bg">data_dir</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">    338</span> <span class="ansi-yellow-bg">                            </span><span class="ansi-yellow-bg">eval_holdout_size</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">    339</span> <span class="ansi-yellow-bg">                            </span><span class="ansi-yellow-bg">train_shuffle_files</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">shuffle_train</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">    340</span> <span class="ansi-yellow-bg">                            </span><span class="ansi-yellow-bg">use_alt_eval</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">use_alt_eval</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">    341</span> <span class="ansi-yellow-bg">                            </span><span class="ansi-yellow-bg">subsplit</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">subsplit</span><span class="ansi-yellow-bg">)</span>)
<span class="ansi-green-fg ansi-bold">    342</span> dataset <span style="color:rgb(98,98,98)">=</span> train_data <span style="font-weight:bold;color:rgb(0,135,0)">if</span> train <span style="font-weight:bold;color:rgb(0,135,0)">else</span> eval_data
<span class="ansi-green-fg ansi-bold">    343</span> dataset <span style="color:rgb(98,98,98)">=</span> dataset <span style="font-weight:bold;color:rgb(0,135,0)">if</span> tfds_preprocess_fn <span style="font-weight:bold;color:rgb(175,0,255)">is</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span> <span style="font-weight:bold;color:rgb(0,135,0)">else</span> tfds_preprocess_fn(
<span class="ansi-green-fg ansi-bold">    344</span>     dataset)

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/data/tf_inputs.py:270</span>, in <span class="ansi-cyan-fg">_train_and_eval_dataset</span><span class="ansi-blue-fg">(dataset_name, data_dir, eval_holdout_size, train_shuffle_files, eval_shuffle_files, use_alt_eval, subsplit)</span>
<span class="ansi-green-fg ansi-bold">    267</span>   <span style="font-weight:bold;color:rgb(0,135,0)">if</span> tfds<span style="color:rgb(98,98,98)">.</span>Split<span style="color:rgb(98,98,98)">.</span>VALIDATION <span style="font-weight:bold;color:rgb(175,0,255)">not</span> <span style="font-weight:bold;color:rgb(175,0,255)">in</span> splits:
<span class="ansi-green-fg ansi-bold">    268</span>     eval_split <span style="color:rgb(98,98,98)">=</span> tfds<span style="color:rgb(98,98,98)">.</span>Split<span style="color:rgb(98,98,98)">.</span>TEST
<span class="ansi-green-fg">--&gt; 270</span> train <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">tfds</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">load</span><span class="ansi-yellow-bg">(</span>
<span class="ansi-green-fg ansi-bold">    271</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">name</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">dataset_name</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">    272</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">split</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">train_split</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">    273</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">data_dir</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">data_dir</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">    274</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">shuffle_files</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">train_shuffle_files</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">    275</span> valid <span style="color:rgb(98,98,98)">=</span> tfds<span style="color:rgb(98,98,98)">.</span>load(
<span class="ansi-green-fg ansi-bold">    276</span>     name<span style="color:rgb(98,98,98)">=</span>dataset_name,
<span class="ansi-green-fg ansi-bold">    277</span>     split<span style="color:rgb(98,98,98)">=</span>eval_split,
<span class="ansi-green-fg ansi-bold">    278</span>     data_dir<span style="color:rgb(98,98,98)">=</span>data_dir,
<span class="ansi-green-fg ansi-bold">    279</span>     shuffle_files<span style="color:rgb(98,98,98)">=</span>eval_shuffle_files)
<span class="ansi-green-fg ansi-bold">    280</span> keys <span style="color:rgb(98,98,98)">=</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tensorflow_datasets/core/logging/__init__.py:166</span>, in <span class="ansi-cyan-fg">_FunctionDecorator.__call__</span><span class="ansi-blue-fg">(self, function, instance, args, kwargs)</span>
<span class="ansi-green-fg ansi-bold">    164</span> metadata <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_start_call()
<span class="ansi-green-fg ansi-bold">    165</span> <span style="font-weight:bold;color:rgb(0,135,0)">try</span>:
<span class="ansi-green-fg">--&gt; 166</span>   <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span class="ansi-yellow-bg">function</span><span class="ansi-yellow-bg">(</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">args</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">kwargs</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">    167</span> <span style="font-weight:bold;color:rgb(0,135,0)">except</span> <span style="font-weight:bold;color:rgb(215,95,95)">Exception</span>:
<span class="ansi-green-fg ansi-bold">    168</span>   metadata<span style="color:rgb(98,98,98)">.</span>mark_error()

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tensorflow_datasets/core/load.py:639</span>, in <span class="ansi-cyan-fg">load</span><span class="ansi-blue-fg">(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)</span>
<span class="ansi-green-fg ansi-bold">    520</span> <span style="font-style:italic;color:rgb(175,0,0)">"""Loads the named dataset into a `tf.data.Dataset`.</span>
<span class="ansi-green-fg ansi-bold">    521</span> 
<span class="ansi-green-fg ansi-bold">    522</span> <span style="font-style:italic;color:rgb(175,0,0)">`tfds.load` is a convenience method that:</span>
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-fg ansi-bold">    631</span> <span style="font-style:italic;color:rgb(175,0,0)">    Split-specific information is available in `ds_info.splits`.</span>
<span class="ansi-green-fg ansi-bold">    632</span> <span style="font-style:italic;color:rgb(175,0,0)">"""</span>
<span class="ansi-green-fg ansi-bold">    633</span> dbuilder <span style="color:rgb(98,98,98)">=</span> _fetch_builder(
<span class="ansi-green-fg ansi-bold">    634</span>     name,
<span class="ansi-green-fg ansi-bold">    635</span>     data_dir,
<span class="ansi-green-fg ansi-bold">    636</span>     builder_kwargs,
<span class="ansi-green-fg ansi-bold">    637</span>     try_gcs,
<span class="ansi-green-fg ansi-bold">    638</span> )
<span class="ansi-green-fg">--&gt; 639</span> <span class="ansi-yellow-bg">_download_and_prepare_builder</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">dbuilder</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">download</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">download_and_prepare_kwargs</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">    641</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> as_dataset_kwargs <span style="font-weight:bold;color:rgb(175,0,255)">is</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span>:
<span class="ansi-green-fg ansi-bold">    642</span>   as_dataset_kwargs <span style="color:rgb(98,98,98)">=</span> {}

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tensorflow_datasets/core/load.py:498</span>, in <span class="ansi-cyan-fg">_download_and_prepare_builder</span><span class="ansi-blue-fg">(dbuilder, download, download_and_prepare_kwargs)</span>
<span class="ansi-green-fg ansi-bold">    496</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> download:
<span class="ansi-green-fg ansi-bold">    497</span>   download_and_prepare_kwargs <span style="color:rgb(98,98,98)">=</span> download_and_prepare_kwargs <span style="font-weight:bold;color:rgb(175,0,255)">or</span> {}
<span class="ansi-green-fg">--&gt; 498</span>   <span class="ansi-yellow-bg">dbuilder</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">download_and_prepare</span><span class="ansi-yellow-bg">(</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">download_and_prepare_kwargs</span><span class="ansi-yellow-bg">)</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tensorflow_datasets/core/logging/__init__.py:166</span>, in <span class="ansi-cyan-fg">_FunctionDecorator.__call__</span><span class="ansi-blue-fg">(self, function, instance, args, kwargs)</span>
<span class="ansi-green-fg ansi-bold">    164</span> metadata <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_start_call()
<span class="ansi-green-fg ansi-bold">    165</span> <span style="font-weight:bold;color:rgb(0,135,0)">try</span>:
<span class="ansi-green-fg">--&gt; 166</span>   <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span class="ansi-yellow-bg">function</span><span class="ansi-yellow-bg">(</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">args</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">kwargs</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">    167</span> <span style="font-weight:bold;color:rgb(0,135,0)">except</span> <span style="font-weight:bold;color:rgb(215,95,95)">Exception</span>:
<span class="ansi-green-fg ansi-bold">    168</span>   metadata<span style="color:rgb(98,98,98)">.</span>mark_error()

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tensorflow_datasets/core/dataset_builder.py:691</span>, in <span class="ansi-cyan-fg">DatasetBuilder.download_and_prepare</span><span class="ansi-blue-fg">(self, download_dir, download_config, file_format)</span>
<span class="ansi-green-fg ansi-bold">    689</span>   <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>info<span style="color:rgb(98,98,98)">.</span>read_from_directory(<span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>data_dir)
<span class="ansi-green-fg ansi-bold">    690</span> <span style="font-weight:bold;color:rgb(0,135,0)">else</span>:
<span class="ansi-green-fg">--&gt; 691</span>   <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">_download_and_prepare</span><span class="ansi-yellow-bg">(</span>
<span class="ansi-green-fg ansi-bold">    692</span> <span class="ansi-yellow-bg">      </span><span class="ansi-yellow-bg">dl_manager</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">dl_manager</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">    693</span> <span class="ansi-yellow-bg">      </span><span class="ansi-yellow-bg">download_config</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">download_config</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">    694</span> <span class="ansi-yellow-bg">  </span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">    696</span>   <span style="font-style:italic;color:rgb(95,135,135)"># NOTE: If modifying the lines below to put additional information in</span>
<span class="ansi-green-fg ansi-bold">    697</span>   <span style="font-style:italic;color:rgb(95,135,135)"># DatasetInfo, you'll likely also want to update</span>
<span class="ansi-green-fg ansi-bold">    698</span>   <span style="font-style:italic;color:rgb(95,135,135)"># DatasetInfo.read_from_directory to possibly restore these attributes</span>
<span class="ansi-green-fg ansi-bold">    699</span>   <span style="font-style:italic;color:rgb(95,135,135)"># when reading from package data.</span>
<span class="ansi-green-fg ansi-bold">    700</span>   <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>info<span style="color:rgb(98,98,98)">.</span>download_size <span style="color:rgb(98,98,98)">=</span> dl_manager<span style="color:rgb(98,98,98)">.</span>downloaded_size

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tensorflow_datasets/core/dataset_builder.py:1546</span>, in <span class="ansi-cyan-fg">GeneratorBasedBuilder._download_and_prepare</span><span class="ansi-blue-fg">(self, dl_manager, download_config)</span>
<span class="ansi-green-fg ansi-bold">   1544</span> <span style="font-weight:bold;color:rgb(0,135,0)">else</span>:
<span class="ansi-green-fg ansi-bold">   1545</span>   optional_pipeline_kwargs <span style="color:rgb(98,98,98)">=</span> {}
<span class="ansi-green-fg">-&gt; 1546</span> split_generators <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">_split_generators</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">  </span><span style="font-style:italic;color:rgb(95,135,135)" class="ansi-yellow-bg"># pylint: disable=unexpected-keyword-arg</span>
<span class="ansi-green-fg ansi-bold">   1547</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">dl_manager</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">optional_pipeline_kwargs</span>
<span class="ansi-green-fg ansi-bold">   1548</span> <span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">   1549</span> <span style="font-style:italic;color:rgb(95,135,135)"># TODO(tfds): Could be removed once all datasets are migrated.</span>
<span class="ansi-green-fg ansi-bold">   1550</span> <span style="font-style:italic;color:rgb(95,135,135)"># https://github.com/tensorflow/datasets/issues/2537</span>
<span class="ansi-green-fg ansi-bold">   1551</span> <span style="font-style:italic;color:rgb(95,135,135)"># Legacy mode (eventually convert list[SplitGeneratorLegacy] -&gt; dict)</span>
<span class="ansi-green-fg ansi-bold">   1552</span> split_generators <span style="color:rgb(98,98,98)">=</span> split_builder<span style="color:rgb(98,98,98)">.</span>normalize_legacy_split_generators(
<span class="ansi-green-fg ansi-bold">   1553</span>     split_generators<span style="color:rgb(98,98,98)">=</span>split_generators,
<span class="ansi-green-fg ansi-bold">   1554</span>     generator_fn<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_generate_examples,
<span class="ansi-green-fg ansi-bold">   1555</span>     is_beam<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(0,135,0)">isinstance</span>(<span style="color:rgb(0,135,0)">self</span>, BeamBasedBuilder),
<span class="ansi-green-fg ansi-bold">   1556</span> )

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tensorflow_datasets/summarization/cnn_dailymail.py:238</span>, in <span class="ansi-cyan-fg">CnnDailymail._split_generators</span><span class="ansi-blue-fg">(self, dl_manager)</span>
<span class="ansi-green-fg ansi-bold">    237</span> <span style="font-weight:bold;color:rgb(0,135,0)">def</span><span style="color:rgb(188,188,188)"> </span><span style="color:rgb(0,0,255)">_split_generators</span>(<span style="color:rgb(0,135,0)">self</span>, dl_manager):
<span class="ansi-green-fg">--&gt; 238</span>   dl_paths <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">dl_manager</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">download_and_extract</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">_DL_URLS</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">    240</span>   <span style="font-weight:bold;color:rgb(0,135,0)">return</span> [
<span class="ansi-green-fg ansi-bold">    241</span>       tfds<span style="color:rgb(98,98,98)">.</span>core<span style="color:rgb(98,98,98)">.</span>SplitGenerator(
<span class="ansi-green-fg ansi-bold">    242</span>           name<span style="color:rgb(98,98,98)">=</span>tfds<span style="color:rgb(98,98,98)">.</span>Split<span style="color:rgb(98,98,98)">.</span>TRAIN,
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-fg ansi-bold">    254</span>       ),
<span class="ansi-green-fg ansi-bold">    255</span>   ]

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tensorflow_datasets/core/download/download_manager.py:688</span>, in <span class="ansi-cyan-fg">DownloadManager.download_and_extract</span><span class="ansi-blue-fg">(self, url_or_urls)</span>
<span class="ansi-green-fg ansi-bold">    686</span> <span style="font-weight:bold;color:rgb(0,135,0)">with</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_downloader<span style="color:rgb(98,98,98)">.</span>tqdm():
<span class="ansi-green-fg ansi-bold">    687</span>   <span style="font-weight:bold;color:rgb(0,135,0)">with</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_extractor<span style="color:rgb(98,98,98)">.</span>tqdm():
<span class="ansi-green-fg">--&gt; 688</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span class="ansi-yellow-bg">_map_promise</span><span class="ansi-yellow-bg">(</span><span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">_download_extract</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">url_or_urls</span><span class="ansi-yellow-bg">)</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tensorflow_datasets/core/download/download_manager.py:831</span>, in <span class="ansi-cyan-fg">_map_promise</span><span class="ansi-blue-fg">(map_fn, all_inputs)</span>
<span class="ansi-green-fg ansi-bold">    827</span> <span style="font-style:italic;color:rgb(175,0,0)">"""Map the function into each element and resolve the promise."""</span>
<span class="ansi-green-fg ansi-bold">    828</span> all_promises <span style="color:rgb(98,98,98)">=</span> tree_utils<span style="color:rgb(98,98,98)">.</span>map_structure(
<span class="ansi-green-fg ansi-bold">    829</span>     map_fn, all_inputs
<span class="ansi-green-fg ansi-bold">    830</span> )  <span style="font-style:italic;color:rgb(95,135,135)"># Apply the function</span>
<span class="ansi-green-fg">--&gt; 831</span> res <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">tree_utils</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">map_structure</span><span class="ansi-yellow-bg">(</span>
<span class="ansi-green-fg ansi-bold">    832</span> <span class="ansi-yellow-bg">    </span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">lambda</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">p</span><span class="ansi-yellow-bg">:</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">p</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">get</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">)</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">all_promises</span>
<span class="ansi-green-fg ansi-bold">    833</span> <span class="ansi-yellow-bg">)</span>  <span style="font-style:italic;color:rgb(95,135,135)"># Wait promises</span>
<span class="ansi-green-fg ansi-bold">    834</span> <span style="font-weight:bold;color:rgb(0,135,0)">return</span> res

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tree/__init__.py:435</span>, in <span class="ansi-cyan-fg">map_structure</span><span class="ansi-blue-fg">(func, *structures, **kwargs)</span>
<span class="ansi-green-fg ansi-bold">    432</span> <span style="font-weight:bold;color:rgb(0,135,0)">for</span> other <span style="font-weight:bold;color:rgb(175,0,255)">in</span> structures[<span style="color:rgb(98,98,98)">1</span>:]:
<span class="ansi-green-fg ansi-bold">    433</span>   assert_same_structure(structures[<span style="color:rgb(98,98,98)">0</span>], other, check_types<span style="color:rgb(98,98,98)">=</span>check_types)
<span class="ansi-green-fg ansi-bold">    434</span> <span style="font-weight:bold;color:rgb(0,135,0)">return</span> unflatten_as(structures[<span style="color:rgb(98,98,98)">0</span>],
<span class="ansi-green-fg">--&gt; 435</span>                     [func(<span style="color:rgb(98,98,98)">*</span>args) <span style="font-weight:bold;color:rgb(0,135,0)">for</span> args <span style="font-weight:bold;color:rgb(175,0,255)">in</span> <span style="color:rgb(0,135,0)">zip</span>(<span style="color:rgb(98,98,98)">*</span><span style="color:rgb(0,135,0)">map</span>(flatten, structures))])

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tree/__init__.py:435</span>, in <span class="ansi-cyan-fg">&lt;listcomp&gt;</span><span class="ansi-blue-fg">(.0)</span>
<span class="ansi-green-fg ansi-bold">    432</span> <span style="font-weight:bold;color:rgb(0,135,0)">for</span> other <span style="font-weight:bold;color:rgb(175,0,255)">in</span> structures[<span style="color:rgb(98,98,98)">1</span>:]:
<span class="ansi-green-fg ansi-bold">    433</span>   assert_same_structure(structures[<span style="color:rgb(98,98,98)">0</span>], other, check_types<span style="color:rgb(98,98,98)">=</span>check_types)
<span class="ansi-green-fg ansi-bold">    434</span> <span style="font-weight:bold;color:rgb(0,135,0)">return</span> unflatten_as(structures[<span style="color:rgb(98,98,98)">0</span>],
<span class="ansi-green-fg">--&gt; 435</span>                     [<span class="ansi-yellow-bg">func</span><span class="ansi-yellow-bg">(</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">args</span><span class="ansi-yellow-bg">)</span> <span style="font-weight:bold;color:rgb(0,135,0)">for</span> args <span style="font-weight:bold;color:rgb(175,0,255)">in</span> <span style="color:rgb(0,135,0)">zip</span>(<span style="color:rgb(98,98,98)">*</span><span style="color:rgb(0,135,0)">map</span>(flatten, structures))])

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tensorflow_datasets/core/download/download_manager.py:832</span>, in <span class="ansi-cyan-fg">_map_promise.&lt;locals&gt;.&lt;lambda&gt;</span><span class="ansi-blue-fg">(p)</span>
<span class="ansi-green-fg ansi-bold">    827</span> <span style="font-style:italic;color:rgb(175,0,0)">"""Map the function into each element and resolve the promise."""</span>
<span class="ansi-green-fg ansi-bold">    828</span> all_promises <span style="color:rgb(98,98,98)">=</span> tree_utils<span style="color:rgb(98,98,98)">.</span>map_structure(
<span class="ansi-green-fg ansi-bold">    829</span>     map_fn, all_inputs
<span class="ansi-green-fg ansi-bold">    830</span> )  <span style="font-style:italic;color:rgb(95,135,135)"># Apply the function</span>
<span class="ansi-green-fg ansi-bold">    831</span> res <span style="color:rgb(98,98,98)">=</span> tree_utils<span style="color:rgb(98,98,98)">.</span>map_structure(
<span class="ansi-green-fg">--&gt; 832</span>     <span style="font-weight:bold;color:rgb(0,135,0)">lambda</span> p: <span class="ansi-yellow-bg">p</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">get</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">)</span>, all_promises
<span class="ansi-green-fg ansi-bold">    833</span> )  <span style="font-style:italic;color:rgb(95,135,135)"># Wait promises</span>
<span class="ansi-green-fg ansi-bold">    834</span> <span style="font-weight:bold;color:rgb(0,135,0)">return</span> res

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/promise/promise.py:512</span>, in <span class="ansi-cyan-fg">Promise.get</span><span class="ansi-blue-fg">(self, timeout)</span>
<span class="ansi-green-fg ansi-bold">    510</span> target <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_target()
<span class="ansi-green-fg ansi-bold">    511</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_wait(timeout <span style="font-weight:bold;color:rgb(175,0,255)">or</span> DEFAULT_TIMEOUT)
<span class="ansi-green-fg">--&gt; 512</span> <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">_target_settled_value</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">_raise</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">True</span><span class="ansi-yellow-bg">)</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/promise/promise.py:516</span>, in <span class="ansi-cyan-fg">Promise._target_settled_value</span><span class="ansi-blue-fg">(self, _raise)</span>
<span class="ansi-green-fg ansi-bold">    514</span> <span style="font-weight:bold;color:rgb(0,135,0)">def</span><span style="color:rgb(188,188,188)"> </span><span style="color:rgb(0,0,255)">_target_settled_value</span>(<span style="color:rgb(0,135,0)">self</span>, _raise<span style="color:rgb(98,98,98)">=</span><span style="font-weight:bold;color:rgb(0,135,0)">False</span>):
<span class="ansi-green-fg ansi-bold">    515</span>     <span style="font-style:italic;color:rgb(95,135,135)"># type: (bool) -&gt; Any</span>
<span class="ansi-green-fg">--&gt; 516</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">_target</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">)</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">_settled_value</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">_raise</span><span class="ansi-yellow-bg">)</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/promise/promise.py:226</span>, in <span class="ansi-cyan-fg">Promise._settled_value</span><span class="ansi-blue-fg">(self, _raise)</span>
<span class="ansi-green-fg ansi-bold">    224</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> _raise:
<span class="ansi-green-fg ansi-bold">    225</span>     raise_val <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_fulfillment_handler0
<span class="ansi-green-fg">--&gt; 226</span>     <span class="ansi-yellow-bg">reraise</span><span class="ansi-yellow-bg">(</span><span style="color:rgb(0,135,0)" class="ansi-yellow-bg">type</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">raise_val</span><span class="ansi-yellow-bg">)</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">raise_val</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">_traceback</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">    227</span> <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_fulfillment_handler0

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/six.py:724</span>, in <span class="ansi-cyan-fg">reraise</span><span class="ansi-blue-fg">(tp, value, tb)</span>
<span class="ansi-green-fg ansi-bold">    722</span>     <span style="font-weight:bold;color:rgb(0,135,0)">if</span> value<span style="color:rgb(98,98,98)">.</span>__traceback__ <span style="font-weight:bold;color:rgb(175,0,255)">is</span> <span style="font-weight:bold;color:rgb(175,0,255)">not</span> tb:
<span class="ansi-green-fg ansi-bold">    723</span>         <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> value<span style="color:rgb(98,98,98)">.</span>with_traceback(tb)
<span class="ansi-green-fg">--&gt; 724</span>     <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> value
<span class="ansi-green-fg ansi-bold">    725</span> <span style="font-weight:bold;color:rgb(0,135,0)">finally</span>:
<span class="ansi-green-fg ansi-bold">    726</span>     value <span style="color:rgb(98,98,98)">=</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/promise/promise.py:87</span>, in <span class="ansi-cyan-fg">try_catch</span><span class="ansi-blue-fg">(handler, *args, **kwargs)</span>
<span class="ansi-green-fg ansi-bold">     84</span> <span style="font-weight:bold;color:rgb(0,135,0)">def</span><span style="color:rgb(188,188,188)"> </span><span style="color:rgb(0,0,255)">try_catch</span>(handler, <span style="color:rgb(98,98,98)">*</span>args, <span style="color:rgb(98,98,98)">*</span><span style="color:rgb(98,98,98)">*</span>kwargs):
<span class="ansi-green-fg ansi-bold">     85</span>     <span style="font-style:italic;color:rgb(95,135,135)"># type: (Callable, Any, Any) -&gt; Union[Tuple[Any, None], Tuple[None, Tuple[Exception, Optional[TracebackType]]]]</span>
<span class="ansi-green-fg ansi-bold">     86</span>     <span style="font-weight:bold;color:rgb(0,135,0)">try</span>:
<span class="ansi-green-fg">---&gt; 87</span>         <span style="font-weight:bold;color:rgb(0,135,0)">return</span> (<span class="ansi-yellow-bg">handler</span><span class="ansi-yellow-bg">(</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">args</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">kwargs</span><span class="ansi-yellow-bg">)</span>, <span style="font-weight:bold;color:rgb(0,135,0)">None</span>)
<span class="ansi-green-fg ansi-bold">     88</span>     <span style="font-weight:bold;color:rgb(0,135,0)">except</span> <span style="font-weight:bold;color:rgb(215,95,95)">Exception</span> <span style="font-weight:bold;color:rgb(0,135,0)">as</span> e:
<span class="ansi-green-fg ansi-bold">     89</span>         tb <span style="color:rgb(98,98,98)">=</span> exc_info()[<span style="color:rgb(98,98,98)">2</span>]

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tensorflow_datasets/core/download/download_manager.py:408</span>, in <span class="ansi-cyan-fg">DownloadManager._download.&lt;locals&gt;.&lt;lambda&gt;</span><span class="ansi-blue-fg">(dl_result)</span>
<span class="ansi-green-fg ansi-bold">    402</span>   future <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_downloader<span style="color:rgb(98,98,98)">.</span>download(
<span class="ansi-green-fg ansi-bold">    403</span>       url, download_tmp_dir, verify<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_verify_ssl
<span class="ansi-green-fg ansi-bold">    404</span>   )
<span class="ansi-green-fg ansi-bold">    406</span> <span style="font-style:italic;color:rgb(95,135,135)"># Post-process the result</span>
<span class="ansi-green-fg ansi-bold">    407</span> <span style="font-weight:bold;color:rgb(0,135,0)">return</span> future<span style="color:rgb(98,98,98)">.</span>then(
<span class="ansi-green-fg">--&gt; 408</span>     <span style="font-weight:bold;color:rgb(0,135,0)">lambda</span> dl_result: <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">_register_or_validate_checksums</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">  </span><span style="font-style:italic;color:rgb(95,135,135)" class="ansi-yellow-bg"># pylint: disable=g-long-lambda</span>
<span class="ansi-green-fg ansi-bold">    409</span> <span class="ansi-yellow-bg">        </span><span class="ansi-yellow-bg">url</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">url</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">    410</span> <span class="ansi-yellow-bg">        </span><span class="ansi-yellow-bg">path</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">dl_result</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">path</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">    411</span> <span class="ansi-yellow-bg">        </span><span class="ansi-yellow-bg">computed_url_info</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">dl_result</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">url_info</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">    412</span> <span class="ansi-yellow-bg">        </span><span class="ansi-yellow-bg">expected_url_info</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">expected_url_info</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">    413</span> <span class="ansi-yellow-bg">        </span><span class="ansi-yellow-bg">checksum_path</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">checksum_path</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">    414</span> <span class="ansi-yellow-bg">        </span><span class="ansi-yellow-bg">url_path</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">url_path</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">    415</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">    416</span> )

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tensorflow_datasets/core/download/download_manager.py:465</span>, in <span class="ansi-cyan-fg">DownloadManager._register_or_validate_checksums</span><span class="ansi-blue-fg">(self, path, url, expected_url_info, computed_url_info, checksum_path, url_path)</span>
<span class="ansi-green-fg ansi-bold">    454</span>   checksum_path <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_get_dl_path(url, computed_url_info<span style="color:rgb(98,98,98)">.</span>checksum)
<span class="ansi-green-fg ansi-bold">    455</span> <span style="font-weight:bold;color:rgb(0,135,0)">else</span>:
<span class="ansi-green-fg ansi-bold">    456</span>   <span style="font-style:italic;color:rgb(95,135,135)"># Eventually validate checksums</span>
<span class="ansi-green-fg ansi-bold">    457</span>   <span style="font-style:italic;color:rgb(95,135,135)"># Note:</span>
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-fg ansi-bold">    463</span>   <span style="font-style:italic;color:rgb(95,135,135)">#   download). This is expected as it might mean the downloaded file</span>
<span class="ansi-green-fg ansi-bold">    464</span>   <span style="font-style:italic;color:rgb(95,135,135)">#   was corrupted. Note: The tmp file isn't deleted to allow inspection.</span>
<span class="ansi-green-fg">--&gt; 465</span>   <span class="ansi-yellow-bg">_validate_checksums</span><span class="ansi-yellow-bg">(</span>
<span class="ansi-green-fg ansi-bold">    466</span> <span class="ansi-yellow-bg">      </span><span class="ansi-yellow-bg">url</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">url</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">    467</span> <span class="ansi-yellow-bg">      </span><span class="ansi-yellow-bg">path</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">path</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">    468</span> <span class="ansi-yellow-bg">      </span><span class="ansi-yellow-bg">expected_url_info</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">expected_url_info</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">    469</span> <span class="ansi-yellow-bg">      </span><span class="ansi-yellow-bg">computed_url_info</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">computed_url_info</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">    470</span> <span class="ansi-yellow-bg">      </span><span class="ansi-yellow-bg">force_checksums_validation</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">_force_checksums_validation</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">    471</span> <span class="ansi-yellow-bg">  </span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">    473</span> <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_rename_and_get_final_dl_path(
<span class="ansi-green-fg ansi-bold">    474</span>     url<span style="color:rgb(98,98,98)">=</span>url,
<span class="ansi-green-fg ansi-bold">    475</span>     path<span style="color:rgb(98,98,98)">=</span>path,
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-fg ansi-bold">    479</span>     url_path<span style="color:rgb(98,98,98)">=</span>url_path,
<span class="ansi-green-fg ansi-bold">    480</span> )

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tensorflow_datasets/core/download/download_manager.py:809</span>, in <span class="ansi-cyan-fg">_validate_checksums</span><span class="ansi-blue-fg">(url, path, computed_url_info, expected_url_info, force_checksums_validation)</span>
<span class="ansi-green-fg ansi-bold">    797</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> (
<span class="ansi-green-fg ansi-bold">    798</span>     expected_url_info
<span class="ansi-green-fg ansi-bold">    799</span>     <span style="font-weight:bold;color:rgb(175,0,255)">and</span> computed_url_info
<span class="ansi-green-fg ansi-bold">    800</span>     <span style="font-weight:bold;color:rgb(175,0,255)">and</span> expected_url_info <span style="color:rgb(98,98,98)">!=</span> computed_url_info
<span class="ansi-green-fg ansi-bold">    801</span> ):
<span class="ansi-green-fg ansi-bold">    802</span>   msg <span style="color:rgb(98,98,98)">=</span> (
<span class="ansi-green-fg ansi-bold">    803</span>       <span style="color:rgb(175,0,0)">f</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">Artifact </span><span style="font-weight:bold;color:rgb(175,95,135)">{</span>url<span style="font-weight:bold;color:rgb(175,95,135)">}</span><span style="color:rgb(175,0,0)">, downloaded to </span><span style="font-weight:bold;color:rgb(175,95,135)">{</span>path<span style="font-weight:bold;color:rgb(175,95,135)">}</span><span style="color:rgb(175,0,0)">, has wrong checksum:</span><span style="font-weight:bold;color:rgb(175,95,0)">\n</span><span style="color:rgb(175,0,0)">'</span>
<span class="ansi-green-fg ansi-bold">    804</span>       <span style="color:rgb(175,0,0)">f</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">* Expected: </span><span style="font-weight:bold;color:rgb(175,95,135)">{</span>expected_url_info<span style="font-weight:bold;color:rgb(175,95,135)">}</span><span style="font-weight:bold;color:rgb(175,95,0)">\n</span><span style="color:rgb(175,0,0)">'</span>
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-fg ansi-bold">    807</span>       <span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">https://www.tensorflow.org/datasets/overview#fixing_nonmatchingchecksumerror</span><span style="color:rgb(175,0,0)">'</span>
<span class="ansi-green-fg ansi-bold">    808</span>   )
<span class="ansi-green-fg">--&gt; 809</span>   <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> NonMatchingChecksumError(msg)

<span class="ansi-red-fg">NonMatchingChecksumError</span>: Artifact https://drive.google.com/uc?export=download&amp;id=0BwmD_VLjROrfTHk4NFg2SndKcjQ, downloaded to data/downloads/ucexport_download_id_0BwmD_VLjROrfTHk4NFg2SndKG8BdJPpt2iRo6Dpzz23CByJuAePEilB-pxbcBCHaWDs.tmp.ed2fa2e94c19499a823b79f7703895a8/download, has wrong checksum:
* Expected: UrlInfo(size=151.23 MiB, checksum='e8fbc0027e54e0a916abd9c969eb35f708ed1467d7ef4e3b17a56739d65cb200', filename='cnn_stories.tgz')
* Got: UrlInfo(size=2.36 KiB, checksum='a87c219b971ac34a93adfa34db40b9a1586fcb58bccf291e0d1e61343742c7a4', filename='download')
To debug, see: https://www.tensorflow.org/datasets/overview#fixing_nonmatchingchecksumerror
  In call to configurable 'TFDS' (&lt;function TFDS at 0x763d3aa81090&gt;)</pre>
</div>
</div>
</div>
<p><a name="1.1"></a> ## 1.1 Tokenize &amp; Detokenize helper functions</p>
<p>Just like in the previous assignment, the cell above loads in the encoder for you. Given any data set, you have to be able to map words to their indices, and indices to their words. The inputs and outputs to your <a href="https://github.com/google/trax">Trax</a> models are usually tensors of numbers where each number corresponds to a word. If you were to process your data manually, you would have to make use of the following:</p>
<ul>
<li><span style="color:blue"> word2Ind: </span> a dictionary mapping the word to its index.</li>
<li><span style="color:blue"> ind2Word:</span> a dictionary mapping the index to its word.</li>
<li><span style="color:blue"> word2Count:</span> a dictionary mapping the word to the number of times it appears.</li>
<li><span style="color:blue"> num_words:</span> total number of words that have appeared.</li>
</ul>
<p>Since you have already implemented these in previous assignments of the specialization, we will provide you with helper functions that will do this for you. Run the cell below to get the following functions:</p>
<ul>
<li><span style="color:blue"> tokenize: </span> converts a text sentence to its corresponding token list (i.e.&nbsp;list of indices). Also converts words to subwords.</li>
<li><span style="color:blue"> detokenize: </span> converts a token list to its corresponding sentence (i.e.&nbsp;string).</li>
</ul>
<div id="8eb9e98f" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize(input_str, EOS<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Input str to features dict, ready for inference"""</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use the trax.data.tokenize method. It takes streams and returns streams,</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># we get around it by making a 1-element stream with `iter`.</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span>  <span class="bu">next</span>(trax.data.tokenize(<span class="bu">iter</span>([input_str]),</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>                                      vocab_dir<span class="op">=</span><span class="st">'vocab_dir/'</span>,</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>                                      vocab_file<span class="op">=</span><span class="st">'summarize32k.subword.subwords'</span>))</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mark the end of the sentence with EOS</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">list</span>(inputs) <span class="op">+</span> [EOS]</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> detokenize(integers):</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""List of ints to str"""</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    s <span class="op">=</span> trax.data.detokenize(integers,</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>                             vocab_dir<span class="op">=</span><span class="st">'vocab_dir/'</span>,</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>                             vocab_file<span class="op">=</span><span class="st">'summarize32k.subword.subwords'</span>)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> wrapper.fill(s)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><a name="1.2"></a></p>
</section>
</section>
<section id="preprocessing-for-language-models-concatenate-it" class="level2">
<h2 class="anchored" data-anchor-id="preprocessing-for-language-models-concatenate-it">1.2 Preprocessing for Language Models: Concatenate It!</h2>
<p>This week you will use a language model – Transformer Decoder – to solve an input-output problem. As you know, language models only predict the next word, they have no notion of inputs. To create a single input suitable for a language model, we concatenate inputs with targets putting a separator in between. We also need to create a mask – with 0s at inputs and 1s at targets – so that the model is not penalized for mis-predicting the article and only focuses on the summary. See the preprocess function below for how this is done.</p>
<div id="08c7766e" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Special tokens</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>SEP <span class="op">=</span> <span class="dv">0</span> <span class="co"># Padding or separator token</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>EOS <span class="op">=</span> <span class="dv">1</span> <span class="co"># End of sentence token</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Concatenate tokenized inputs and targets using 0 as separator.</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> preprocess(stream):</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (article, summary) <span class="kw">in</span> stream:</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>        joint <span class="op">=</span> np.array(<span class="bu">list</span>(article) <span class="op">+</span> [EOS, SEP] <span class="op">+</span> <span class="bu">list</span>(summary) <span class="op">+</span> [EOS])</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        mask <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> (<span class="bu">len</span>(<span class="bu">list</span>(article)) <span class="op">+</span> <span class="dv">2</span>) <span class="op">+</span> [<span class="dv">1</span>] <span class="op">*</span> (<span class="bu">len</span>(<span class="bu">list</span>(summary)) <span class="op">+</span> <span class="dv">1</span>) <span class="co"># Accounting for EOS and SEP</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">yield</span> joint, joint, np.array(mask)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="co"># You can combine a few data preprocessing steps into a pipeline like this.</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>input_pipeline <span class="op">=</span> trax.data.Serial(</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Tokenizes</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    trax.data.Tokenize(vocab_dir<span class="op">=</span><span class="st">'vocab_dir/'</span>,</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>                       vocab_file<span class="op">=</span><span class="st">'summarize32k.subword.subwords'</span>),</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Uses function defined above</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>    preprocess,</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Filters out examples longer than 2048</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    trax.data.FilterByLength(<span class="dv">2048</span>)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply preprocessing to data streams.</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>train_stream <span class="op">=</span> input_pipeline(train_stream_fn())</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>eval_stream <span class="op">=</span> input_pipeline(eval_stream_fn())</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>train_input, train_target, train_mask <span class="op">=</span> <span class="bu">next</span>(train_stream)</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> <span class="bu">sum</span>((train_input <span class="op">-</span> train_target)<span class="op">**</span><span class="dv">2</span>) <span class="op">==</span> <span class="dv">0</span>  <span class="co"># They are the same in Language Model (LM).</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[4], line 24</span>
<span class="ansi-green-fg ansi-bold">     13</span> input_pipeline <span style="color:rgb(98,98,98)">=</span> trax<span style="color:rgb(98,98,98)">.</span>data<span style="color:rgb(98,98,98)">.</span>Serial(
<span class="ansi-green-fg ansi-bold">     14</span>     <span style="font-style:italic;color:rgb(95,135,135)"># Tokenizes</span>
<span class="ansi-green-fg ansi-bold">     15</span>     trax<span style="color:rgb(98,98,98)">.</span>data<span style="color:rgb(98,98,98)">.</span>Tokenize(vocab_dir<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">vocab_dir/</span><span style="color:rgb(175,0,0)">'</span>,
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-fg ansi-bold">     20</span>     trax<span style="color:rgb(98,98,98)">.</span>data<span style="color:rgb(98,98,98)">.</span>FilterByLength(<span style="color:rgb(98,98,98)">2048</span>)
<span class="ansi-green-fg ansi-bold">     21</span> )
<span class="ansi-green-fg ansi-bold">     23</span> <span style="font-style:italic;color:rgb(95,135,135)"># Apply preprocessing to data streams.</span>
<span class="ansi-green-fg">---&gt; 24</span> train_stream <span style="color:rgb(98,98,98)">=</span> input_pipeline(<span class="ansi-yellow-bg">train_stream_fn</span>())
<span class="ansi-green-fg ansi-bold">     25</span> eval_stream <span style="color:rgb(98,98,98)">=</span> input_pipeline(eval_stream_fn())
<span class="ansi-green-fg ansi-bold">     27</span> train_input, train_target, train_mask <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)">next</span>(train_stream)

<span class="ansi-red-fg">NameError</span>: name 'train_stream_fn' is not defined</pre>
</div>
</div>
</div>
<div id="00a413d4" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># prints mask, 0s on article, 1s on summary</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Single example mask:</span><span class="ch">\n\n</span><span class="ss"> </span><span class="sc">{</span>train_mask<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[5], line 2</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># prints mask, 0s on article, 1s on summary</span>
<span class="ansi-green-fg">----&gt; 2</span> <span style="color:rgb(0,135,0)">print</span>(<span style="color:rgb(175,0,0)">f</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">Single example mask:</span><span style="font-weight:bold;color:rgb(175,95,0)">\n</span><span style="font-weight:bold;color:rgb(175,95,0)">\n</span><span style="color:rgb(175,0,0)"> </span><span style="font-weight:bold;color:rgb(175,95,135)">{</span><span class="ansi-yellow-bg">train_mask</span><span style="font-weight:bold;color:rgb(175,95,135)">}</span><span style="color:rgb(175,0,0)">'</span>)

<span class="ansi-red-fg">NameError</span>: name 'train_mask' is not defined</pre>
</div>
</div>
</div>
<div id="4850e885" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># prints: [Example][&lt;EOS&gt;][&lt;pad&gt;][Example Summary][&lt;EOS&gt;]</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Single example:</span><span class="ch">\n\n</span><span class="ss"> </span><span class="sc">{</span>detokenize(train_input)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[6], line 2</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># prints: [Example][&lt;EOS&gt;][&lt;pad&gt;][Example Summary][&lt;EOS&gt;]</span>
<span class="ansi-green-fg">----&gt; 2</span> <span style="color:rgb(0,135,0)">print</span>(<span style="color:rgb(175,0,0)">f</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">Single example:</span><span style="font-weight:bold;color:rgb(175,95,0)">\n</span><span style="font-weight:bold;color:rgb(175,95,0)">\n</span><span style="color:rgb(175,0,0)"> </span><span style="font-weight:bold;color:rgb(175,95,135)">{</span>detokenize(<span class="ansi-yellow-bg">train_input</span>)<span style="font-weight:bold;color:rgb(175,95,135)">}</span><span style="color:rgb(175,0,0)">'</span>)

<span class="ansi-red-fg">NameError</span>: name 'train_input' is not defined</pre>
</div>
</div>
</div>
<p><a name="1.3"></a></p>
</section>
<section id="batching-with-bucketing" class="level2">
<h2 class="anchored" data-anchor-id="batching-with-bucketing">1.3 Batching with bucketing</h2>
<p>As in the previous week, we use bucketing to create batches of data.</p>
<div id="dfb53d07" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Bucketing to create batched generators.</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Buckets are defined in terms of boundaries and batch sizes.</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Batch_sizes[i] determines the batch size for items with length &lt; boundaries[i]</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co"># So below, we'll take a batch of 16 sentences of length &lt; 128 , 8 of length &lt; 256,</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 4 of length &lt; 512. And so on. </span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>boundaries <span class="op">=</span>  [<span class="dv">128</span>, <span class="dv">256</span>,  <span class="dv">512</span>, <span class="dv">1024</span>]</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>batch_sizes <span class="op">=</span> [<span class="dv">16</span>,    <span class="dv">8</span>,    <span class="dv">4</span>,    <span class="dv">2</span>, <span class="dv">1</span>]</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the streams.</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>train_batch_stream <span class="op">=</span> trax.data.BucketByLength(</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    boundaries, batch_sizes)(train_stream)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>eval_batch_stream <span class="op">=</span> trax.data.BucketByLength(</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    boundaries, batch_sizes)(eval_stream)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[7], line 12</span>
<span class="ansi-green-fg ansi-bold">      8</span> batch_sizes <span style="color:rgb(98,98,98)">=</span> [<span style="color:rgb(98,98,98)">16</span>,    <span style="color:rgb(98,98,98)">8</span>,    <span style="color:rgb(98,98,98)">4</span>,    <span style="color:rgb(98,98,98)">2</span>, <span style="color:rgb(98,98,98)">1</span>]
<span class="ansi-green-fg ansi-bold">     10</span> <span style="font-style:italic;color:rgb(95,135,135)"># Create the streams.</span>
<span class="ansi-green-fg ansi-bold">     11</span> train_batch_stream <span style="color:rgb(98,98,98)">=</span> trax<span style="color:rgb(98,98,98)">.</span>data<span style="color:rgb(98,98,98)">.</span>BucketByLength(
<span class="ansi-green-fg">---&gt; 12</span>     boundaries, batch_sizes)(<span class="ansi-yellow-bg">train_stream</span>)
<span class="ansi-green-fg ansi-bold">     14</span> eval_batch_stream <span style="color:rgb(98,98,98)">=</span> trax<span style="color:rgb(98,98,98)">.</span>data<span style="color:rgb(98,98,98)">.</span>BucketByLength(
<span class="ansi-green-fg ansi-bold">     15</span>     boundaries, batch_sizes)(eval_stream)

<span class="ansi-red-fg">NameError</span>: name 'train_stream' is not defined</pre>
</div>
</div>
</div>
<div id="2d4a0a75" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Every execution will result in generation of a different article</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Try running this cell multiple times to see how the length of the examples affects the batch size</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>input_batch, _, mask_batch <span class="op">=</span> <span class="bu">next</span>(train_batch_stream)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Shape of the input_batch</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>input_batch.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[8], line 3</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># Every execution will result in generation of a different article</span>
<span class="ansi-green-fg ansi-bold">      2</span> <span style="font-style:italic;color:rgb(95,135,135)"># Try running this cell multiple times to see how the length of the examples affects the batch size</span>
<span class="ansi-green-fg">----&gt; 3</span> input_batch, _, mask_batch <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)">next</span>(<span class="ansi-yellow-bg">train_batch_stream</span>)
<span class="ansi-green-fg ansi-bold">      5</span> <span style="font-style:italic;color:rgb(95,135,135)"># Shape of the input_batch</span>
<span class="ansi-green-fg ansi-bold">      6</span> input_batch<span style="color:rgb(98,98,98)">.</span>shape

<span class="ansi-red-fg">NameError</span>: name 'train_batch_stream' is not defined</pre>
</div>
</div>
</div>
<div id="b63add10" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># print corresponding integer values</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(input_batch[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[9], line 2</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># print corresponding integer values</span>
<span class="ansi-green-fg">----&gt; 2</span> <span style="color:rgb(0,135,0)">print</span>(<span class="ansi-yellow-bg">input_batch</span>[<span style="color:rgb(98,98,98)">0</span>])

<span class="ansi-red-fg">NameError</span>: name 'input_batch' is not defined</pre>
</div>
</div>
</div>
<p>Things to notice: - First we see the corresponding values of the words. - The first 1, which represents the <code>&lt;EOS&gt;</code> tag of the article. - Followed by a 0, which represents a <code>&lt;pad&gt;</code> tag. - After the first 0 (<code>&lt;pad&gt;</code> tag) the corresponding values are of the words that are used for the summary of the article. - The second 1 represents the <code>&lt;EOS&gt;</code> tag for the summary. - All the trailing 0s represent <code>&lt;pad&gt;</code> tags which are appended to maintain consistent length (If you don’t see them then it would mean it is already of max length)</p>
<div id="370edc32" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># print the article and its summary</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Article:</span><span class="ch">\n\n</span><span class="st">'</span>, detokenize(input_batch[<span class="dv">0</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[10], line 2</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># print the article and its summary</span>
<span class="ansi-green-fg">----&gt; 2</span> <span style="color:rgb(0,135,0)">print</span>(<span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">Article:</span><span style="font-weight:bold;color:rgb(175,95,0)">\n</span><span style="font-weight:bold;color:rgb(175,95,0)">\n</span><span style="color:rgb(175,0,0)">'</span>, detokenize(<span class="ansi-yellow-bg">input_batch</span>[<span style="color:rgb(98,98,98)">0</span>]))

<span class="ansi-red-fg">NameError</span>: name 'input_batch' is not defined</pre>
</div>
</div>
</div>
<p>You can see that the data has the following structure: - <span style="color:blue"> [Article] </span> -&gt; <code>&lt;EOS&gt;</code> -&gt; <code>&lt;pad&gt;</code> -&gt; <span style="color:blue"> [Article Summary] </span> -&gt; <code>&lt;EOS&gt;</code> -&gt; (possibly) multiple <code>&lt;pad&gt;</code></p>
<p>The loss is taken only on the summary using cross_entropy as loss function.</p>
<p><a name="2"></a> # Part 2: Summarization with transformer</p>
<p>Now that we have given you the data generator and have handled the preprocessing for you, it is time for you to build your own model. We saved you some time because we know you have already preprocessed data before in this specialization, so we would rather you spend your time doing the next steps.</p>
<p>You will be implementing the attention from scratch and then using it in your transformer model. Concretely, you will understand how attention works, how you use it to connect the encoder and the decoder.</p>
<p><img src="transformer_decoder_zoomin.png"></p>
<p><a name="2.1"></a> ## 2.1 Dot product attention</p>
<p>Now you will implement dot product attention which takes in a query, key, value, and a mask. It returns the output.</p>
<p><img src="dotproduct.png"></p>
<p>Here are some helper functions that will help you create tensors and display useful information: - <code>create_tensor</code> creates a <code>jax numpy array</code> from a list of lists. - <code>display_tensor</code> prints out the shape and the actual tensor.</p>
<div id="395a1b26" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_tensor(t):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Create tensor from list of lists"""</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.array(t)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> display_tensor(t, name):</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Display shape and tensor"""</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>name<span class="sc">}</span><span class="ss"> shape: </span><span class="sc">{</span>t<span class="sc">.</span>shape<span class="sc">}</span><span class="ch">\n</span><span class="ss">'</span>)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>t<span class="sc">}</span><span class="ch">\n</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Before implementing it yourself, you can play around with a toy example of <code>dot product attention</code> without the softmax operation. Technically it would not be <code>dot product attention</code> without the softmax but this is done to avoid giving away too much of the answer and the idea is to display these tensors to give you a sense of how they look like.</p>
<p>The formula for attention is this one:</p>
<p><span class="math display">
\text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}+{M}\right) V\tag{1}\
</span></p>
<p><span class="math inline">d_{k}</span> stands for the dimension of queries and keys.</p>
<p>The <code>query</code>, <code>key</code>, <code>value</code> and <code>mask</code> vectors are provided for this example.</p>
<p>Notice that the masking is done using very negative values that will yield a similar effect to using $-$.</p>
<div id="f0b956ab" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> create_tensor([[<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>]])</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>display_tensor(q, <span class="st">'query'</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> create_tensor([[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], [<span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>]])</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>display_tensor(k, <span class="st">'key'</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> create_tensor([[<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>]])</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>display_tensor(v, <span class="st">'value'</span>)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> create_tensor([[<span class="dv">0</span>, <span class="dv">0</span>], [<span class="op">-</span><span class="fl">1e9</span>, <span class="dv">0</span>]])</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>display_tensor(m, <span class="st">'mask'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>query shape: (2, 3)

[[1 0 0]
 [0 1 0]]

key shape: (2, 3)

[[1 2 3]
 [4 5 6]]

value shape: (2, 3)

[[0 1 0]
 [1 0 1]]

mask shape: (2, 2)

[[ 0.e+00  0.e+00]
 [-1.e+09  0.e+00]]
</code></pre>
</div>
</div>
<p><strong>Expected Output:</strong></p>
<div class="sourceCode" id="cb17"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>query shape<span class="op">:</span> <span class="op">(</span><span class="dv">2</span><span class="op">,</span> <span class="dv">3</span><span class="op">)</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="op">[[</span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">0</span><span class="op">]</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="at"> </span><span class="op">[</span><span class="dv">0</span><span class="at"> </span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="op">]]</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>key shape<span class="op">:</span> <span class="op">(</span><span class="dv">2</span><span class="op">,</span> <span class="dv">3</span><span class="op">)</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="op">[[</span><span class="dv">1</span><span class="at"> </span><span class="dv">2</span><span class="at"> </span><span class="dv">3</span><span class="op">]</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="at"> </span><span class="op">[</span><span class="dv">4</span><span class="at"> </span><span class="dv">5</span><span class="at"> </span><span class="dv">6</span><span class="op">]]</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>value shape<span class="op">:</span> <span class="op">(</span><span class="dv">2</span><span class="op">,</span> <span class="dv">3</span><span class="op">)</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="op">[[</span><span class="dv">0</span><span class="at"> </span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="op">]</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="at"> </span><span class="op">[</span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">1</span><span class="op">]]</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>mask shape<span class="op">:</span> <span class="op">(</span><span class="dv">2</span><span class="op">,</span> <span class="dv">2</span><span class="op">)</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a><span class="op">[[</span><span class="at"> </span><span class="dv">0</span><span class="er">.e</span><span class="op">+</span><span class="bn">00</span><span class="at">  </span><span class="dv">0</span><span class="er">.e</span><span class="op">+</span><span class="bn">00</span><span class="op">]</span></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a><span class="at"> </span><span class="op">[-</span><span class="dv">1</span><span class="er">.e</span><span class="op">+</span><span class="dv">0</span><span class="er">9</span><span class="at">  </span><span class="dv">0</span><span class="er">.e</span><span class="op">+</span><span class="bn">00</span><span class="op">]]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="b44650cf" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>q_dot_k <span class="op">=</span> q <span class="op">@</span> k.T <span class="op">/</span> jnp.sqrt(<span class="dv">3</span>)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>display_tensor(q_dot_k, <span class="st">'query dot key'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>query dot key shape: (2, 2)

[[0.57735026 2.309401  ]
 [1.1547005  2.8867512 ]]
</code></pre>
</div>
</div>
<p><strong>Expected Output:</strong></p>
<div class="sourceCode" id="cb20"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>query dot key shape<span class="op">:</span> <span class="op">(</span><span class="dv">2</span><span class="op">,</span> <span class="dv">2</span><span class="op">)</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="op">[[</span><span class="dv">0</span><span class="er">.57735026</span><span class="at"> </span><span class="dv">2</span><span class="er">.309401</span><span class="at">  </span><span class="op">]</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="at"> </span><span class="op">[</span><span class="dv">1</span><span class="er">.1547005</span><span class="at">  </span><span class="dv">2</span><span class="er">.8867514</span><span class="at"> </span><span class="op">]]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="4cba3f84" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>masked <span class="op">=</span> q_dot_k <span class="op">+</span> m</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>display_tensor(masked, <span class="st">'masked query dot key'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>masked query dot key shape: (2, 2)

[[ 5.7735026e-01  2.3094010e+00]
 [-1.0000000e+09  2.8867512e+00]]
</code></pre>
</div>
</div>
<p><strong>Expected Output:</strong></p>
<div class="sourceCode" id="cb23"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>masked query dot key shape<span class="op">:</span> <span class="op">(</span><span class="dv">2</span><span class="op">,</span> <span class="dv">2</span><span class="op">)</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="op">[[</span><span class="at"> </span><span class="dv">5</span><span class="er">.7735026e</span><span class="op">-</span><span class="bn">01</span><span class="at">  </span><span class="dv">2</span><span class="er">.3094010e</span><span class="op">+</span><span class="bn">00</span><span class="op">]</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="at"> </span><span class="op">[-</span><span class="dv">1</span><span class="er">.0000000e</span><span class="op">+</span><span class="dv">0</span><span class="er">9</span><span class="at">  </span><span class="dv">2</span><span class="er">.8867514e</span><span class="op">+</span><span class="bn">00</span><span class="op">]]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="edf65a4e" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>display_tensor(masked <span class="op">@</span> v, <span class="st">'masked query dot key dot value'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>masked query dot key dot value shape: (2, 3)

[[ 2.3094010e+00  5.7735026e-01  2.3094010e+00]
 [ 2.8867512e+00 -1.0000000e+09  2.8867512e+00]]
</code></pre>
</div>
</div>
<p><strong>Expected Output:</strong></p>
<div class="sourceCode" id="cb26"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>masked query dot key dot value shape<span class="op">:</span> <span class="op">(</span><span class="dv">2</span><span class="op">,</span> <span class="dv">3</span><span class="op">)</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="op">[[</span><span class="at"> </span><span class="dv">2</span><span class="er">.3094010e</span><span class="op">+</span><span class="bn">00</span><span class="at">  </span><span class="dv">5</span><span class="er">.7735026e</span><span class="op">-</span><span class="bn">01</span><span class="at">  </span><span class="dv">2</span><span class="er">.3094010e</span><span class="op">+</span><span class="bn">00</span><span class="op">]</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="at"> </span><span class="op">[</span><span class="at"> </span><span class="dv">2</span><span class="er">.8867514e</span><span class="op">+</span><span class="bn">00</span><span class="at"> </span><span class="op">-</span><span class="dv">1</span><span class="er">.0000000e</span><span class="op">+</span><span class="dv">0</span><span class="er">9</span><span class="at">  </span><span class="dv">2</span><span class="er">.8867514e</span><span class="op">+</span><span class="bn">00</span><span class="op">]]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In order to use the previous dummy tensors to test some of the graded functions, a batch dimension should be added to them so they mimic the shape of real-life examples. The mask is also replaced by a version of it that resembles the one that is used by trax:</p>
<div id="609f88c7" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>q_with_batch <span class="op">=</span> q[<span class="va">None</span>,:]</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>display_tensor(q_with_batch, <span class="st">'query with batch dim'</span>)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>k_with_batch <span class="op">=</span> k[<span class="va">None</span>,:]</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>display_tensor(k_with_batch, <span class="st">'key with batch dim'</span>)</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>v_with_batch <span class="op">=</span> v[<span class="va">None</span>,:]</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>display_tensor(v_with_batch, <span class="st">'value with batch dim'</span>)</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>m_bool <span class="op">=</span> create_tensor([[<span class="va">True</span>, <span class="va">True</span>], [<span class="va">False</span>, <span class="va">True</span>]])</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>display_tensor(m_bool, <span class="st">'boolean mask'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>query with batch dim shape: (1, 2, 3)

[[[1 0 0]
  [0 1 0]]]

key with batch dim shape: (1, 2, 3)

[[[1 2 3]
  [4 5 6]]]

value with batch dim shape: (1, 2, 3)

[[[0 1 0]
  [1 0 1]]]

boolean mask shape: (2, 2)

[[ True  True]
 [False  True]]
</code></pre>
</div>
</div>
<p><strong>Expected Output:</strong></p>
<div class="sourceCode" id="cb29"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>query with batch dim shape<span class="op">:</span> <span class="op">(</span><span class="dv">1</span><span class="op">,</span> <span class="dv">2</span><span class="op">,</span> <span class="dv">3</span><span class="op">)</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="op">[[[</span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">0</span><span class="op">]</span></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="op">[</span><span class="dv">0</span><span class="at"> </span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="op">]]]</span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>key with batch dim shape<span class="op">:</span> <span class="op">(</span><span class="dv">1</span><span class="op">,</span> <span class="dv">2</span><span class="op">,</span> <span class="dv">3</span><span class="op">)</span></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a><span class="op">[[[</span><span class="dv">1</span><span class="at"> </span><span class="dv">2</span><span class="at"> </span><span class="dv">3</span><span class="op">]</span></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="op">[</span><span class="dv">4</span><span class="at"> </span><span class="dv">5</span><span class="at"> </span><span class="dv">6</span><span class="op">]]]</span></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>value with batch dim shape<span class="op">:</span> <span class="op">(</span><span class="dv">1</span><span class="op">,</span> <span class="dv">2</span><span class="op">,</span> <span class="dv">3</span><span class="op">)</span></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a><span class="op">[[[</span><span class="dv">0</span><span class="at"> </span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="op">]</span></span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="op">[</span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">1</span><span class="op">]]]</span></span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>boolean mask shape<span class="op">:</span> <span class="op">(</span><span class="dv">2</span><span class="op">,</span> <span class="dv">2</span><span class="op">)</span></span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a><span class="op">[[</span><span class="at"> True  True</span><span class="op">]</span></span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a><span class="at"> </span><span class="op">[</span><span class="at">False  True</span><span class="op">]]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><a name="ex01"></a> ### Exercise 01</p>
<p><strong>Instructions:</strong> Implement the dot product attention. Concretely, implement the following equation</p>
<p><span class="math display">
\text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}+{M}\right) V\tag{1}\
</span></p>
<p><span class="math inline">Q</span> - query, <span class="math inline">K</span> - key, <span class="math inline">V</span> - values, <span class="math inline">M</span> - mask, <span class="math inline">{d_k}</span> - depth/dimension of the queries and keys (used for scaling down)</p>
<p>You can implement this formula either by <code>trax</code> numpy (trax.math.numpy) or regular <code>numpy</code> but it is recommended to use <code>jnp</code>.</p>
<p>Something to take into consideration is that within trax, the masks are tensors of <code>True/False</code> values not 0’s and <span class="math inline">-\infty</span> as in the previous example. Within the graded function don’t think of applying the mask by summing up matrices, instead use <code>jnp.where()</code> and treat the <strong>mask as a tensor of boolean values with <code>False</code> for values that need to be masked and True for the ones that don’t.</strong></p>
<p>Also take into account that the real tensors are far more complex than the toy ones you just played with. Because of this avoid using shortened operations such as <code>@</code> for dot product or <code>.T</code> for transposing. Use <code>jnp.matmul()</code> and <code>jnp.swapaxes()</code> instead.</p>
<p>This is the self-attention block for the transformer decoder. Good luck!</p>
<div id="243062eb" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># UNQ_C1</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="co"># GRADED FUNCTION: DotProductAttention</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> DotProductAttention(query, key, value, mask):</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Dot product self-attention.</span></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="co">        query (jax.interpreters.xla.DeviceArray): array of query representations with shape (L_q by d)</span></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a><span class="co">        key (jax.interpreters.xla.DeviceArray): array of key representations with shape (L_k by d)</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a><span class="co">        value (jax.interpreters.xla.DeviceArray): array of value representations with shape (L_k by d) where L_v = L_k</span></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a><span class="co">        mask (jax.interpreters.xla.DeviceArray): attention-mask, gates attention with shape (L_q by L_k)</span></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a><span class="co">        jax.interpreters.xla.DeviceArray: Self-attention array for q, k, v arrays. (L_q by L_k)</span></span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> query.shape[<span class="op">-</span><span class="dv">1</span>] <span class="op">==</span> key.shape[<span class="op">-</span><span class="dv">1</span>] <span class="op">==</span> value.shape[<span class="op">-</span><span class="dv">1</span>], <span class="st">"Embedding dimensions of q, k, v aren't all the same"</span></span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a>    <span class="co">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) </span><span class="al">###</span></span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Save depth/dimension of the query embedding for scaling down the dot product</span></span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>    depth <span class="op">=</span> <span class="va">None</span></span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate scaled query key dot product according to formula above</span></span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a>    dots <span class="op">=</span> jnp.matmul(<span class="va">None</span>, jnp.swapaxes(<span class="va">None</span>, <span class="va">None</span>, <span class="va">None</span>)) <span class="op">/</span> jnp.sqrt(<span class="va">None</span>)</span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply the mask</span></span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>: <span class="co"># The 'None' in this line does not need to be replaced</span></span>
<span id="cb30-26"><a href="#cb30-26" aria-hidden="true" tabindex="-1"></a>        dots <span class="op">=</span> jnp.where(<span class="va">None</span>, <span class="va">None</span>, jnp.full_like(<span class="va">None</span>, <span class="op">-</span><span class="fl">1e9</span>))</span>
<span id="cb30-27"><a href="#cb30-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb30-28"><a href="#cb30-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Softmax formula implementation</span></span>
<span id="cb30-29"><a href="#cb30-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use trax.fastmath.logsumexp of dots to avoid underflow by division by large numbers</span></span>
<span id="cb30-30"><a href="#cb30-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Hint: Last axis should be used and keepdims should be True</span></span>
<span id="cb30-31"><a href="#cb30-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Note: softmax = e^(dots - logsumexp(dots)) = E^dots / sumexp(dots)</span></span>
<span id="cb30-32"><a href="#cb30-32" aria-hidden="true" tabindex="-1"></a>    logsumexp <span class="op">=</span> <span class="va">None</span></span>
<span id="cb30-33"><a href="#cb30-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-34"><a href="#cb30-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Take exponential of dots minus logsumexp to get softmax</span></span>
<span id="cb30-35"><a href="#cb30-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use jnp.exp()</span></span>
<span id="cb30-36"><a href="#cb30-36" aria-hidden="true" tabindex="-1"></a>    dots <span class="op">=</span> <span class="va">None</span></span>
<span id="cb30-37"><a href="#cb30-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-38"><a href="#cb30-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Multiply dots by value to get self-attention</span></span>
<span id="cb30-39"><a href="#cb30-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use jnp.matmul()</span></span>
<span id="cb30-40"><a href="#cb30-40" aria-hidden="true" tabindex="-1"></a>    attention <span class="op">=</span> <span class="va">None</span></span>
<span id="cb30-41"><a href="#cb30-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-42"><a href="#cb30-42" aria-hidden="true" tabindex="-1"></a>    <span class="co">## </span><span class="re">END</span><span class="co"> CODE HERE </span><span class="al">###</span></span>
<span id="cb30-43"><a href="#cb30-43" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb30-44"><a href="#cb30-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> attention</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="184ff942" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>DotProductAttention(q_with_batch, k_with_batch, v_with_batch, m_bool)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">TypeError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[18], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> <span class="ansi-yellow-bg">DotProductAttention</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">q_with_batch</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">k_with_batch</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">v_with_batch</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">m_bool</span><span class="ansi-yellow-bg">)</span>

Cell <span class="ansi-green-fg">In[17], line 22</span>, in <span class="ansi-cyan-fg">DotProductAttention</span><span class="ansi-blue-fg">(query, key, value, mask)</span>
<span class="ansi-green-fg ansi-bold">     19</span> depth <span style="color:rgb(98,98,98)">=</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span>
<span class="ansi-green-fg ansi-bold">     21</span> <span style="font-style:italic;color:rgb(95,135,135)"># Calculate scaled query key dot product according to formula above</span>
<span class="ansi-green-fg">---&gt; 22</span> dots <span style="color:rgb(98,98,98)">=</span> jnp<span style="color:rgb(98,98,98)">.</span>matmul(<span style="font-weight:bold;color:rgb(0,135,0)">None</span>, <span class="ansi-yellow-bg">jnp</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">swapaxes</span><span class="ansi-yellow-bg">(</span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">None</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">None</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">None</span><span class="ansi-yellow-bg">)</span>) <span style="color:rgb(98,98,98)">/</span> jnp<span style="color:rgb(98,98,98)">.</span>sqrt(<span style="font-weight:bold;color:rgb(0,135,0)">None</span>)
<span class="ansi-green-fg ansi-bold">     24</span> <span style="font-style:italic;color:rgb(95,135,135)"># Apply the mask</span>
<span class="ansi-green-fg ansi-bold">     25</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> mask <span style="font-weight:bold;color:rgb(175,0,255)">is</span> <span style="font-weight:bold;color:rgb(175,0,255)">not</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span>: <span style="font-style:italic;color:rgb(95,135,135)"># The 'None' in this line does not need to be replaced</span>

    <span class="ansi-red-fg">[... skipping hidden 15 frame]</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:2610</span>, in <span class="ansi-cyan-fg">swapaxes</span><span class="ansi-blue-fg">(a, axis1, axis2)</span>
<span class="ansi-green-fg ansi-bold">   2568</span> <span style="color:rgb(175,0,255)">@export</span>
<span class="ansi-green-fg ansi-bold">   2569</span> <span style="color:rgb(175,0,255)">@partial</span>(jit, static_argnames<span style="color:rgb(98,98,98)">=</span>(<span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">axis1</span><span style="color:rgb(175,0,0)">'</span>, <span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">axis2</span><span style="color:rgb(175,0,0)">'</span>), inline<span style="color:rgb(98,98,98)">=</span><span style="font-weight:bold;color:rgb(0,135,0)">True</span>)
<span class="ansi-green-fg ansi-bold">   2570</span> <span style="font-weight:bold;color:rgb(0,135,0)">def</span><span style="color:rgb(188,188,188)"> </span><span style="color:rgb(0,0,255)">swapaxes</span>(a: ArrayLike, axis1: <span style="color:rgb(0,135,0)">int</span>, axis2: <span style="color:rgb(0,135,0)">int</span>) <span style="color:rgb(98,98,98)">-</span><span style="color:rgb(98,98,98)">&gt;</span> Array:
<span class="ansi-green-fg ansi-bold">   2571</span> <span style="color:rgb(188,188,188)">  </span><span style="font-style:italic;color:rgb(175,0,0)">"""Swap two axes of an array.</span>
<span class="ansi-green-fg ansi-bold">   2572</span> 
<span class="ansi-green-fg ansi-bold">   2573</span> <span style="font-style:italic;color:rgb(175,0,0)">  JAX implementation of :func:`numpy.swapaxes`, implemented in terms of</span>
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-fg ansi-bold">   2608</span> <span style="font-style:italic;color:rgb(175,0,0)">    (2, 5, 4, 3)</span>
<span class="ansi-green-fg ansi-bold">   2609</span> <span style="font-style:italic;color:rgb(175,0,0)">  """</span>
<span class="ansi-green-fg">-&gt; 2610</span>   <span class="ansi-yellow-bg">util</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">check_arraylike</span><span class="ansi-yellow-bg">(</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">"</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">swapaxes</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">"</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">a</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">   2611</span>   perm <span style="color:rgb(98,98,98)">=</span> np<span style="color:rgb(98,98,98)">.</span>arange(ndim(a))
<span class="ansi-green-fg ansi-bold">   2612</span>   perm[axis1], perm[axis2] <span style="color:rgb(98,98,98)">=</span> perm[axis2], perm[axis1]

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/jax/_src/numpy/util.py:147</span>, in <span class="ansi-cyan-fg">check_arraylike</span><span class="ansi-blue-fg">(fun_name, emit_warning, stacklevel, *args)</span>
<span class="ansi-green-fg ansi-bold">    144</span>   warnings<span style="color:rgb(98,98,98)">.</span>warn(msg <span style="color:rgb(98,98,98)">+</span> <span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)"> In a future JAX release this will be an error.</span><span style="color:rgb(175,0,0)">"</span>,
<span class="ansi-green-fg ansi-bold">    145</span>                 category<span style="color:rgb(98,98,98)">=</span><span style="font-weight:bold;color:rgb(215,95,95)">DeprecationWarning</span>, stacklevel<span style="color:rgb(98,98,98)">=</span>stacklevel)
<span class="ansi-green-fg ansi-bold">    146</span> <span style="font-weight:bold;color:rgb(0,135,0)">else</span>:
<span class="ansi-green-fg">--&gt; 147</span>   <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> <span style="font-weight:bold;color:rgb(215,95,95)">TypeError</span>(msg<span style="color:rgb(98,98,98)">.</span>format(fun_name, <span style="color:rgb(0,135,0)">type</span>(arg), pos))

<span class="ansi-red-fg">TypeError</span>: swapaxes requires ndarray or scalar arguments, got &lt;class 'NoneType'&gt; at position 0.</pre>
</div>
</div>
</div>
<p><strong>Expected Output:</strong></p>
<div class="sourceCode" id="cb32"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>DeviceArray<span class="op">([[[</span><span class="dv">0</span><span class="er">.8496746</span><span class="at"> </span><span class="op">,</span><span class="at"> </span><span class="dv">0</span><span class="er">.15032545</span><span class="op">,</span><span class="at"> </span><span class="dv">0</span><span class="er">.8496746</span><span class="at"> </span><span class="op">],</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="op">[</span><span class="dv">1</span><span class="er">.</span><span class="at">        </span><span class="op">,</span><span class="at"> </span><span class="dv">0</span><span class="er">.</span><span class="at">        </span><span class="op">,</span><span class="at"> </span><span class="dv">1</span><span class="er">.</span><span class="at">        </span><span class="op">]]],</span> dtype<span class="op">=</span>float32<span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><a name="2.2"></a></p>
</section>
<section id="causal-attention" class="level2">
<h2 class="anchored" data-anchor-id="causal-attention">2.2 Causal Attention</h2>
<p>Now you are going to implement causal attention: multi-headed attention with a mask to attend only to words that occurred before.</p>
<p><img src="causal.png"></p>
<p>In the image above, a word can see everything that is before it, but not what is after it. To implement causal attention, you will have to transform vectors and do many reshapes. You will need to implement the functions below.</p>
<p><a name="ex02"></a> ### Exercise 02</p>
<p>Implement the following functions that will be needed for Causal Attention:</p>
<ul>
<li><span style="color:blue"> compute_attention_heads </span>: Gets an input <span class="math inline">x</span> of dimension (batch_size, seqlen, n_heads <span class="math inline">\times</span> d_head) and splits the last (depth) dimension and stacks it to the zeroth dimension to allow matrix multiplication (batch_size <span class="math inline">\times</span> n_heads, seqlen, d_head).</li>
<li><span style="color:blue"> dot_product_self_attention </span>: Creates a mask matrix with <code>False</code> values above the diagonal and <code>True</code> values below and calls DotProductAttention which implements dot product self attention.</li>
<li><span style="color:blue"> compute_attention_output </span>: Undoes compute_attention_heads by splitting first (vertical) dimension and stacking in the last (depth) dimension (batch_size, seqlen, n_heads <span class="math inline">\times</span> d_head). These operations concatenate (stack/merge) the heads.</li>
</ul>
<p>Next there are some toy tensors which may serve to give you an idea of the data shapes and opperations involved in Causal Attention. They are also useful to test out your functions!</p>
<div id="fa3ae132" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>tensor2d <span class="op">=</span> create_tensor(q)</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>display_tensor(tensor2d, <span class="st">'query matrix (2D tensor)'</span>)</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>tensor4d2b <span class="op">=</span> create_tensor([[q, q], [q, q]])</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>display_tensor(tensor4d2b, <span class="st">'batch of two (multi-head) collections of query matrices (4D tensor)'</span>)</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>tensor3dc <span class="op">=</span> create_tensor([jnp.concatenate([q, q], axis <span class="op">=</span> <span class="op">-</span><span class="dv">1</span>)])</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>display_tensor(tensor3dc, <span class="st">'one batch of concatenated heads of query matrices (3d tensor)'</span>)</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>tensor3dc3b <span class="op">=</span> create_tensor([jnp.concatenate([q, q], axis <span class="op">=</span> <span class="op">-</span><span class="dv">1</span>), jnp.concatenate([q, q], axis <span class="op">=</span> <span class="op">-</span><span class="dv">1</span>), jnp.concatenate([q, q], axis <span class="op">=</span> <span class="op">-</span><span class="dv">1</span>)])</span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>display_tensor(tensor3dc3b, <span class="st">'three batches of concatenated heads of query matrices (3d tensor)'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>query matrix (2D tensor) shape: (2, 3)

[[1 0 0]
 [0 1 0]]

batch of two (multi-head) collections of query matrices (4D tensor) shape: (2, 2, 2, 3)

[[[[1 0 0]
   [0 1 0]]

  [[1 0 0]
   [0 1 0]]]


 [[[1 0 0]
   [0 1 0]]

  [[1 0 0]
   [0 1 0]]]]

one batch of concatenated heads of query matrices (3d tensor) shape: (1, 2, 6)

[[[1 0 0 1 0 0]
  [0 1 0 0 1 0]]]

three batches of concatenated heads of query matrices (3d tensor) shape: (3, 2, 6)

[[[1 0 0 1 0 0]
  [0 1 0 0 1 0]]

 [[1 0 0 1 0 0]
  [0 1 0 0 1 0]]

 [[1 0 0 1 0 0]
  [0 1 0 0 1 0]]]
</code></pre>
</div>
</div>
<p>It is important to know that the following 3 functions would normally be defined within the <code>CausalAttention</code> function further below.</p>
<p>However this makes these functions harder to test. Because of this, these functions are shown individually using a <code>closure</code> (when necessary) that simulates them being inside of the <code>CausalAttention</code> function. This is done because they rely on some variables that can be accessed from within <code>CausalAttention</code>.</p>
<section id="support-functions" class="level3">
<h3 class="anchored" data-anchor-id="support-functions">Support Functions</h3>
<p><span style="color:blue"> compute_attention_heads </span>: Gets an input <span class="math inline">x</span> of dimension (batch_size, seqlen, n_heads <span class="math inline">\times</span> d_head) and splits the last (depth) dimension and stacks it to the zeroth dimension to allow matrix multiplication (batch_size <span class="math inline">\times</span> n_heads, seqlen, d_head).</p>
<p><strong>For the closures you only have to fill the inner function.</strong></p>
<div id="18e694fe" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># UNQ_C2</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="co"># GRADED FUNCTION: compute_attention_heads_closure</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_attention_heads_closure(n_heads, d_head):</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" Function that simulates environment inside CausalAttention function.</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a><span class="co">        d_head (int):  dimensionality of heads.</span></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a><span class="co">        n_heads (int): number of attention heads.</span></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a><span class="co">        function: compute_attention_heads function</span></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> compute_attention_heads(x):</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>        <span class="co">""" Compute the attention heads.</span></span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a><span class="co">            x (jax.interpreters.xla.DeviceArray): tensor with shape (batch_size, seqlen, n_heads X d_head).</span></span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a><span class="co">            jax.interpreters.xla.DeviceArray: reshaped tensor with shape (batch_size X n_heads, seqlen, d_head).</span></span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a>        <span class="co">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) </span><span class="al">###</span></span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Size of the x's batch dimension</span></span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> <span class="va">None</span></span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Length of the sequence</span></span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Should be size of x's first dimension without counting the batch dim</span></span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a>        seqlen <span class="op">=</span> <span class="va">None</span></span>
<span id="cb35-26"><a href="#cb35-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Reshape x using jnp.reshape()</span></span>
<span id="cb35-27"><a href="#cb35-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># batch_size, seqlen, n_heads*d_head -&gt; batch_size, seqlen, n_heads, d_head</span></span>
<span id="cb35-28"><a href="#cb35-28" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">None</span></span>
<span id="cb35-29"><a href="#cb35-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Transpose x using jnp.transpose()</span></span>
<span id="cb35-30"><a href="#cb35-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># batch_size, seqlen, n_heads, d_head -&gt; batch_size, n_heads, seqlen, d_head</span></span>
<span id="cb35-31"><a href="#cb35-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Note that the values within the tuple are the indexes of the dimensions of x and you must rearrange them</span></span>
<span id="cb35-32"><a href="#cb35-32" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> jnp.transpose(x, (<span class="va">None</span>, <span class="va">None</span>, <span class="va">None</span>, <span class="va">None</span>))</span>
<span id="cb35-33"><a href="#cb35-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Reshape x using jnp.reshape()</span></span>
<span id="cb35-34"><a href="#cb35-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># batch_size, n_heads, seqlen, d_head -&gt; batch_size*n_heads, seqlen, d_head</span></span>
<span id="cb35-35"><a href="#cb35-35" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">None</span></span>
<span id="cb35-36"><a href="#cb35-36" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb35-37"><a href="#cb35-37" aria-hidden="true" tabindex="-1"></a>        <span class="co">### </span><span class="re">END</span><span class="co"> CODE HERE </span><span class="al">###</span></span>
<span id="cb35-38"><a href="#cb35-38" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb35-39"><a href="#cb35-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb35-40"><a href="#cb35-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb35-41"><a href="#cb35-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> compute_attention_heads</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="bdf30aca" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>display_tensor(tensor3dc3b, <span class="st">"input tensor"</span>)</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>result_cah <span class="op">=</span> compute_attention_heads_closure(<span class="dv">2</span>,<span class="dv">3</span>)(tensor3dc3b)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>display_tensor(result_cah, <span class="st">"output tensor"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>input tensor shape: (3, 2, 6)

[[[1 0 0 1 0 0]
  [0 1 0 0 1 0]]

 [[1 0 0 1 0 0]
  [0 1 0 0 1 0]]

 [[1 0 0 1 0 0]
  [0 1 0 0 1 0]]]
</code></pre>
</div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">TypeError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[21], line 2</span>
<span class="ansi-green-fg ansi-bold">      1</span> display_tensor(tensor3dc3b, <span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">input tensor</span><span style="color:rgb(175,0,0)">"</span>)
<span class="ansi-green-fg">----&gt; 2</span> result_cah <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">compute_attention_heads_closure</span><span class="ansi-yellow-bg">(</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">2</span><span class="ansi-yellow-bg">,</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">3</span><span class="ansi-yellow-bg">)</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">tensor3dc3b</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">      3</span> display_tensor(result_cah, <span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">output tensor</span><span style="color:rgb(175,0,0)">"</span>)

Cell <span class="ansi-green-fg">In[20], line 32</span>, in <span class="ansi-cyan-fg">compute_attention_heads_closure.&lt;locals&gt;.compute_attention_heads</span><span class="ansi-blue-fg">(x)</span>
<span class="ansi-green-fg ansi-bold">     28</span> x <span style="color:rgb(98,98,98)">=</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span>
<span class="ansi-green-fg ansi-bold">     29</span> <span style="font-style:italic;color:rgb(95,135,135)"># Transpose x using jnp.transpose()</span>
<span class="ansi-green-fg ansi-bold">     30</span> <span style="font-style:italic;color:rgb(95,135,135)"># batch_size, seqlen, n_heads, d_head -&gt; batch_size, n_heads, seqlen, d_head</span>
<span class="ansi-green-fg ansi-bold">     31</span> <span style="font-style:italic;color:rgb(95,135,135)"># Note that the values within the tuple are the indexes of the dimensions of x and you must rearrange them</span>
<span class="ansi-green-fg">---&gt; 32</span> x <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">jnp</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">transpose</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">x</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">(</span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">None</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">None</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">None</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">None</span><span class="ansi-yellow-bg">)</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">     33</span> <span style="font-style:italic;color:rgb(95,135,135)"># Reshape x using jnp.reshape()</span>
<span class="ansi-green-fg ansi-bold">     34</span> <span style="font-style:italic;color:rgb(95,135,135)"># batch_size, n_heads, seqlen, d_head -&gt; batch_size*n_heads, seqlen, d_head</span>
<span class="ansi-green-fg ansi-bold">     35</span> x <span style="color:rgb(98,98,98)">=</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:1328</span>, in <span class="ansi-cyan-fg">transpose</span><span class="ansi-blue-fg">(a, axes)</span>
<span class="ansi-green-fg ansi-bold">   1255</span> <span style="color:rgb(175,0,255)">@export</span>
<span class="ansi-green-fg ansi-bold">   1256</span> <span style="font-weight:bold;color:rgb(0,135,0)">def</span><span style="color:rgb(188,188,188)"> </span><span style="color:rgb(0,0,255)">transpose</span>(a: ArrayLike, axes: Sequence[<span style="color:rgb(0,135,0)">int</span>] <span style="color:rgb(98,98,98)">|</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span> <span style="color:rgb(98,98,98)">=</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span>) <span style="color:rgb(98,98,98)">-</span><span style="color:rgb(98,98,98)">&gt;</span> Array:
<span class="ansi-green-fg ansi-bold">   1257</span> <span style="color:rgb(188,188,188)">  </span><span style="font-style:italic;color:rgb(175,0,0)">"""Return a transposed version of an N-dimensional array.</span>
<span class="ansi-green-fg ansi-bold">   1258</span> 
<span class="ansi-green-fg ansi-bold">   1259</span> <span style="font-style:italic;color:rgb(175,0,0)">  JAX implementation of :func:`numpy.transpose`, implemented in terms of</span>
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-fg ansi-bold">   1326</span> <span style="font-style:italic;color:rgb(175,0,0)">           [2, 4]], dtype=int32)</span>
<span class="ansi-green-fg ansi-bold">   1327</span> <span style="font-style:italic;color:rgb(175,0,0)">  """</span>
<span class="ansi-green-fg">-&gt; 1328</span>   <span class="ansi-yellow-bg">util</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">check_arraylike</span><span class="ansi-yellow-bg">(</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">"</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">transpose</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">"</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">a</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">   1329</span>   axes_ <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)">list</span>(<span style="color:rgb(0,135,0)">range</span>(ndim(a))[::<span style="color:rgb(98,98,98)">-</span><span style="color:rgb(98,98,98)">1</span>]) <span style="font-weight:bold;color:rgb(0,135,0)">if</span> axes <span style="font-weight:bold;color:rgb(175,0,255)">is</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span> <span style="font-weight:bold;color:rgb(0,135,0)">else</span> axes
<span class="ansi-green-fg ansi-bold">   1330</span>   axes_ <span style="color:rgb(98,98,98)">=</span> [_canonicalize_axis(i, ndim(a)) <span style="font-weight:bold;color:rgb(0,135,0)">for</span> i <span style="font-weight:bold;color:rgb(175,0,255)">in</span> axes_]

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/jax/_src/numpy/util.py:147</span>, in <span class="ansi-cyan-fg">check_arraylike</span><span class="ansi-blue-fg">(fun_name, emit_warning, stacklevel, *args)</span>
<span class="ansi-green-fg ansi-bold">    144</span>   warnings<span style="color:rgb(98,98,98)">.</span>warn(msg <span style="color:rgb(98,98,98)">+</span> <span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)"> In a future JAX release this will be an error.</span><span style="color:rgb(175,0,0)">"</span>,
<span class="ansi-green-fg ansi-bold">    145</span>                 category<span style="color:rgb(98,98,98)">=</span><span style="font-weight:bold;color:rgb(215,95,95)">DeprecationWarning</span>, stacklevel<span style="color:rgb(98,98,98)">=</span>stacklevel)
<span class="ansi-green-fg ansi-bold">    146</span> <span style="font-weight:bold;color:rgb(0,135,0)">else</span>:
<span class="ansi-green-fg">--&gt; 147</span>   <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> <span style="font-weight:bold;color:rgb(215,95,95)">TypeError</span>(msg<span style="color:rgb(98,98,98)">.</span>format(fun_name, <span style="color:rgb(0,135,0)">type</span>(arg), pos))

<span class="ansi-red-fg">TypeError</span>: transpose requires ndarray or scalar arguments, got &lt;class 'NoneType'&gt; at position 0.</pre>
</div>
</div>
</div>
<p><strong>Expected Output:</strong></p>
<div class="sourceCode" id="cb38"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>input tensor shape<span class="op">:</span> <span class="op">(</span><span class="dv">3</span><span class="op">,</span> <span class="dv">2</span><span class="op">,</span> <span class="dv">6</span><span class="op">)</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="op">[[[</span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">0</span><span class="op">]</span></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="op">[</span><span class="dv">0</span><span class="at"> </span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="op">]]</span></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a> <span class="op">[[</span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">0</span><span class="op">]</span></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="op">[</span><span class="dv">0</span><span class="at"> </span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="op">]]</span></span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a> <span class="op">[[</span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">0</span><span class="op">]</span></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="op">[</span><span class="dv">0</span><span class="at"> </span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="op">]]]</span></span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a>output tensor shape<span class="op">:</span> <span class="op">(</span><span class="dv">6</span><span class="op">,</span> <span class="dv">2</span><span class="op">,</span> <span class="dv">3</span><span class="op">)</span></span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a><span class="op">[[[</span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">0</span><span class="op">]</span></span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="op">[</span><span class="dv">0</span><span class="at"> </span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="op">]]</span></span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a> <span class="op">[[</span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">0</span><span class="op">]</span></span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="op">[</span><span class="dv">0</span><span class="at"> </span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="op">]]</span></span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a> <span class="op">[[</span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">0</span><span class="op">]</span></span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="op">[</span><span class="dv">0</span><span class="at"> </span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="op">]]</span></span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-23"><a href="#cb38-23" aria-hidden="true" tabindex="-1"></a> <span class="op">[[</span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">0</span><span class="op">]</span></span>
<span id="cb38-24"><a href="#cb38-24" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="op">[</span><span class="dv">0</span><span class="at"> </span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="op">]]</span></span>
<span id="cb38-25"><a href="#cb38-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-26"><a href="#cb38-26" aria-hidden="true" tabindex="-1"></a> <span class="op">[[</span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">0</span><span class="op">]</span></span>
<span id="cb38-27"><a href="#cb38-27" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="op">[</span><span class="dv">0</span><span class="at"> </span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="op">]]</span></span>
<span id="cb38-28"><a href="#cb38-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-29"><a href="#cb38-29" aria-hidden="true" tabindex="-1"></a> <span class="op">[[</span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">0</span><span class="op">]</span></span>
<span id="cb38-30"><a href="#cb38-30" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="op">[</span><span class="dv">0</span><span class="at"> </span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="op">]]]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><span style="color:blue"> dot_product_self_attention </span>: Creates a mask matrix with <code>False</code> values above the diagonal and <code>True</code> values below and calls DotProductAttention which implements dot product self attention.</p>
<div id="510ecb49" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># UNQ_C3</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="co"># GRADED FUNCTION: dot_product_self_attention</span></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dot_product_self_attention(q, k, v):</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" Masked dot product self attention.</span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a><span class="co">        q (jax.interpreters.xla.DeviceArray): queries.</span></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a><span class="co">        k (jax.interpreters.xla.DeviceArray): keys.</span></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a><span class="co">        v (jax.interpreters.xla.DeviceArray): values.</span></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a><span class="co">        jax.interpreters.xla.DeviceArray: masked dot product self attention tensor.</span></span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) </span><span class="al">###</span></span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Hint: mask size should be equal to L_q. Remember that q has shape (batch_size, L_q, d)</span></span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="al">NOTE</span><span class="co">: there is a revision underway with the autograder to tolerate better indexing. </span></span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Until then, please index q.shape using negative values (this is equivalent to counting from right to left)</span></span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a>    mask_size <span class="op">=</span> <span class="va">None</span></span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Creates a matrix with ones below the diagonal and 0s above. It should have shape (1, mask_size, mask_size)</span></span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Notice that 1's and 0's get casted to True/False by setting dtype to jnp.bool_</span></span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use jnp.tril() - Lower triangle of an array and jnp.ones()</span></span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> jnp.tril(jnp.ones((<span class="va">None</span>, <span class="va">None</span>, <span class="va">None</span>), dtype<span class="op">=</span>jnp.bool_), k<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb39-23"><a href="#cb39-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb39-24"><a href="#cb39-24" aria-hidden="true" tabindex="-1"></a>    <span class="co">### </span><span class="re">END</span><span class="co"> CODE HERE </span><span class="al">###</span></span>
<span id="cb39-25"><a href="#cb39-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb39-26"><a href="#cb39-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> DotProductAttention(q, k, v, mask)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="361311fe" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>dot_product_self_attention(q_with_batch, k_with_batch, v_with_batch)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">TypeError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[23], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> <span class="ansi-yellow-bg">dot_product_self_attention</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">q_with_batch</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">k_with_batch</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">v_with_batch</span><span class="ansi-yellow-bg">)</span>

Cell <span class="ansi-green-fg">In[22], line 22</span>, in <span class="ansi-cyan-fg">dot_product_self_attention</span><span class="ansi-blue-fg">(q, k, v)</span>
<span class="ansi-green-fg ansi-bold">     17</span> mask_size <span style="color:rgb(98,98,98)">=</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span>
<span class="ansi-green-fg ansi-bold">     19</span> <span style="font-style:italic;color:rgb(95,135,135)"># Creates a matrix with ones below the diagonal and 0s above. It should have shape (1, mask_size, mask_size)</span>
<span class="ansi-green-fg ansi-bold">     20</span> <span style="font-style:italic;color:rgb(95,135,135)"># Notice that 1's and 0's get casted to True/False by setting dtype to jnp.bool_</span>
<span class="ansi-green-fg ansi-bold">     21</span> <span style="font-style:italic;color:rgb(95,135,135)"># Use jnp.tril() - Lower triangle of an array and jnp.ones()</span>
<span class="ansi-green-fg">---&gt; 22</span> mask <span style="color:rgb(98,98,98)">=</span> jnp<span style="color:rgb(98,98,98)">.</span>tril(<span class="ansi-yellow-bg">jnp</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">ones</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">(</span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">None</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">None</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">None</span><span class="ansi-yellow-bg">)</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">dtype</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">jnp</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">bool_</span><span class="ansi-yellow-bg">)</span>, k<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">0</span>)
<span class="ansi-green-fg ansi-bold">     24</span> <span style="font-style:italic;color:rgb(95,135,135)">### END CODE HERE ###</span>
<span class="ansi-green-fg ansi-bold">     26</span> <span style="font-weight:bold;color:rgb(0,135,0)">return</span> DotProductAttention(q, k, v, mask)

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:6183</span>, in <span class="ansi-cyan-fg">ones</span><span class="ansi-blue-fg">(shape, dtype, device)</span>
<span class="ansi-green-fg ansi-bold">   6181</span>   <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> <span style="font-weight:bold;color:rgb(215,95,95)">TypeError</span>(<span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">expected sequence object with len &gt;= 0 or a single integer</span><span style="color:rgb(175,0,0)">"</span>)
<span class="ansi-green-fg ansi-bold">   6182</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> (m <span style="color:rgb(98,98,98)">:=</span> _check_forgot_shape_tuple(<span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">ones</span><span style="color:rgb(175,0,0)">"</span>, shape, dtype)): <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> <span style="font-weight:bold;color:rgb(215,95,95)">TypeError</span>(m)
<span class="ansi-green-fg">-&gt; 6183</span> shape <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">canonicalize_shape</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">shape</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">   6184</span> dtypes<span style="color:rgb(98,98,98)">.</span>check_user_dtype_supported(dtype, <span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">ones</span><span style="color:rgb(175,0,0)">"</span>)
<span class="ansi-green-fg ansi-bold">   6185</span> <span style="font-weight:bold;color:rgb(0,135,0)">return</span> lax<span style="color:rgb(98,98,98)">.</span>full(shape, <span style="color:rgb(98,98,98)">1</span>, _jnp_dtype(dtype), sharding<span style="color:rgb(98,98,98)">=</span>_normalize_to_sharding(device))

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:102</span>, in <span class="ansi-cyan-fg">canonicalize_shape</span><span class="ansi-blue-fg">(shape, context)</span>
<span class="ansi-green-fg ansi-bold">    100</span>   <span style="font-weight:bold;color:rgb(0,135,0)">return</span> core<span style="color:rgb(98,98,98)">.</span>canonicalize_shape((shape,), context)
<span class="ansi-green-fg ansi-bold">    101</span> <span style="font-weight:bold;color:rgb(0,135,0)">else</span>:
<span class="ansi-green-fg">--&gt; 102</span>   <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span class="ansi-yellow-bg">core</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">canonicalize_shape</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">shape</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">context</span><span class="ansi-yellow-bg">)</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/jax/_src/core.py:1643</span>, in <span class="ansi-cyan-fg">canonicalize_shape</span><span class="ansi-blue-fg">(shape, context)</span>
<span class="ansi-green-fg ansi-bold">   1641</span> <span style="font-weight:bold;color:rgb(0,135,0)">except</span> <span style="font-weight:bold;color:rgb(215,95,95)">TypeError</span>:
<span class="ansi-green-fg ansi-bold">   1642</span>   <span style="font-weight:bold;color:rgb(0,135,0)">pass</span>
<span class="ansi-green-fg">-&gt; 1643</span> <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> _invalid_shape_error(shape, context)

<span class="ansi-red-fg">TypeError</span>: Shapes must be 1D sequences of concrete values of integer type, got (None, None, None).</pre>
</div>
</div>
</div>
<p><strong>Expected Output:</strong></p>
<div class="sourceCode" id="cb41"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>DeviceArray<span class="op">([[[</span><span class="dv">0</span><span class="er">.</span><span class="at">        </span><span class="op">,</span><span class="at"> </span><span class="dv">1</span><span class="er">.</span><span class="at">        </span><span class="op">,</span><span class="at"> </span><span class="dv">0</span><span class="er">.</span><span class="at">        </span><span class="op">],</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="op">[</span><span class="dv">0</span><span class="er">.8496746</span><span class="at"> </span><span class="op">,</span><span class="at"> </span><span class="dv">0</span><span class="er">.15032543</span><span class="op">,</span><span class="at"> </span><span class="dv">0</span><span class="er">.8496746</span><span class="at"> </span><span class="op">]]],</span> dtype<span class="op">=</span>float32<span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><span style="color:blue"> compute_attention_output </span>: Undoes compute_attention_heads by splitting first (vertical) dimension and stacking in the last (depth) dimension (batch_size, seqlen, n_heads <span class="math inline">\times</span> d_head). These operations concatenate (stack/merge) the heads.</p>
<div id="ca3a7a2d" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># UNQ_C4</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="co"># GRADED FUNCTION: compute_attention_output_closure</span></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_attention_output_closure(n_heads, d_head):</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" Function that simulates environment inside CausalAttention function.</span></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a><span class="co">        d_head (int):  dimensionality of heads.</span></span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a><span class="co">        n_heads (int): number of attention heads.</span></span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a><span class="co">        function: compute_attention_output function</span></span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> compute_attention_output(x):</span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>        <span class="co">""" Compute the attention output.</span></span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a><span class="co">            x (jax.interpreters.xla.DeviceArray): tensor with shape (batch_size X n_heads, seqlen, d_head).</span></span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a><span class="co">            jax.interpreters.xla.DeviceArray: reshaped tensor with shape (batch_size, seqlen, n_heads X d_head).</span></span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb42-19"><a href="#cb42-19" aria-hidden="true" tabindex="-1"></a>        <span class="co">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) </span><span class="al">###</span></span>
<span id="cb42-20"><a href="#cb42-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb42-21"><a href="#cb42-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Length of the sequence</span></span>
<span id="cb42-22"><a href="#cb42-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Should be size of x's first dimension without counting the batch dim</span></span>
<span id="cb42-23"><a href="#cb42-23" aria-hidden="true" tabindex="-1"></a>        seqlen <span class="op">=</span> <span class="va">None</span></span>
<span id="cb42-24"><a href="#cb42-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Reshape x using jnp.reshape() to shape (batch_size, n_heads, seqlen, d_head)</span></span>
<span id="cb42-25"><a href="#cb42-25" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">None</span></span>
<span id="cb42-26"><a href="#cb42-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Transpose x using jnp.transpose() to shape (batch_size, seqlen, n_heads, d_head)</span></span>
<span id="cb42-27"><a href="#cb42-27" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">None</span></span>
<span id="cb42-28"><a href="#cb42-28" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb42-29"><a href="#cb42-29" aria-hidden="true" tabindex="-1"></a>        <span class="co">### </span><span class="re">END</span><span class="co"> CODE HERE </span><span class="al">###</span></span>
<span id="cb42-30"><a href="#cb42-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb42-31"><a href="#cb42-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Reshape to allow to concatenate the heads</span></span>
<span id="cb42-32"><a href="#cb42-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> jnp.reshape(x, (<span class="op">-</span><span class="dv">1</span>, seqlen, n_heads <span class="op">*</span> d_head))</span>
<span id="cb42-33"><a href="#cb42-33" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb42-34"><a href="#cb42-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> compute_attention_output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="6bc2b5f8" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>display_tensor(result_cah, <span class="st">"input tensor"</span>)</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>result_cao <span class="op">=</span> compute_attention_output_closure(<span class="dv">2</span>,<span class="dv">3</span>)(result_cah)</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>display_tensor(result_cao, <span class="st">"output tensor"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[25], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> display_tensor(<span class="ansi-yellow-bg">result_cah</span>, <span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">input tensor</span><span style="color:rgb(175,0,0)">"</span>)
<span class="ansi-green-fg ansi-bold">      2</span> result_cao <span style="color:rgb(98,98,98)">=</span> compute_attention_output_closure(<span style="color:rgb(98,98,98)">2</span>,<span style="color:rgb(98,98,98)">3</span>)(result_cah)
<span class="ansi-green-fg ansi-bold">      3</span> display_tensor(result_cao, <span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">output tensor</span><span style="color:rgb(175,0,0)">"</span>)

<span class="ansi-red-fg">NameError</span>: name 'result_cah' is not defined</pre>
</div>
</div>
</div>
<p><strong>Expected Output:</strong></p>
<div class="sourceCode" id="cb44"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>input tensor shape<span class="op">:</span> <span class="op">(</span><span class="dv">6</span><span class="op">,</span> <span class="dv">2</span><span class="op">,</span> <span class="dv">3</span><span class="op">)</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="op">[[[</span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">0</span><span class="op">]</span></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="op">[</span><span class="dv">0</span><span class="at"> </span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="op">]]</span></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a> <span class="op">[[</span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">0</span><span class="op">]</span></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="op">[</span><span class="dv">0</span><span class="at"> </span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="op">]]</span></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a> <span class="op">[[</span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">0</span><span class="op">]</span></span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="op">[</span><span class="dv">0</span><span class="at"> </span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="op">]]</span></span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a> <span class="op">[[</span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">0</span><span class="op">]</span></span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="op">[</span><span class="dv">0</span><span class="at"> </span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="op">]]</span></span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a> <span class="op">[[</span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">0</span><span class="op">]</span></span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="op">[</span><span class="dv">0</span><span class="at"> </span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="op">]]</span></span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a> <span class="op">[[</span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">0</span><span class="op">]</span></span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="op">[</span><span class="dv">0</span><span class="at"> </span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="op">]]]</span></span>
<span id="cb44-20"><a href="#cb44-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-21"><a href="#cb44-21" aria-hidden="true" tabindex="-1"></a>output tensor shape<span class="op">:</span> <span class="op">(</span><span class="dv">3</span><span class="op">,</span> <span class="dv">2</span><span class="op">,</span> <span class="dv">6</span><span class="op">)</span></span>
<span id="cb44-22"><a href="#cb44-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-23"><a href="#cb44-23" aria-hidden="true" tabindex="-1"></a><span class="op">[[[</span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">0</span><span class="op">]</span></span>
<span id="cb44-24"><a href="#cb44-24" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="op">[</span><span class="dv">0</span><span class="at"> </span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="op">]]</span></span>
<span id="cb44-25"><a href="#cb44-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-26"><a href="#cb44-26" aria-hidden="true" tabindex="-1"></a> <span class="op">[[</span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">0</span><span class="op">]</span></span>
<span id="cb44-27"><a href="#cb44-27" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="op">[</span><span class="dv">0</span><span class="at"> </span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="op">]]</span></span>
<span id="cb44-28"><a href="#cb44-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-29"><a href="#cb44-29" aria-hidden="true" tabindex="-1"></a> <span class="op">[[</span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">0</span><span class="op">]</span></span>
<span id="cb44-30"><a href="#cb44-30" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="op">[</span><span class="dv">0</span><span class="at"> </span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">0</span><span class="at"> </span><span class="dv">1</span><span class="at"> </span><span class="dv">0</span><span class="op">]]]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="causal-attention-function" class="level3">
<h3 class="anchored" data-anchor-id="causal-attention-function">Causal Attention Function</h3>
<p>Now it is time for you to put everything together within the <code>CausalAttention</code> or Masked multi-head attention function:</p>
<p><img src="masked-attention.png"></p>
<p><strong>Instructions:</strong> Implement the causal attention. Your model returns the causal attention through a <span class="math inline">tl.Serial</span> with the following:</p>
<ul>
<li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Branch">tl.Branch</a> </span>: consisting of 3 [tl.Dense(d_feature), ComputeAttentionHeads] to account for the queries, keys, and values.</li>
<li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn">tl.Fn</a></span>: Takes in dot_product_self_attention function and uses it to compute the dot product using <span class="math inline">Q</span>, <span class="math inline">K</span>, <span class="math inline">V</span>.</li>
<li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn">tl.Fn</a></span>: Takes in restack_attention_heads to allow for parallel computing.</li>
<li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense">tl.Dense</a></span>: Final Dense layer, with dimension <code>d_feature</code>.</li>
</ul>
<p>Remember that in order for trax to properly handle the functions you just defined, they need to be added as layers using the <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn"><code>tl.Fn()</code></a> function.</p>
<div id="2aeb545d" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># UNQ_C5</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="co"># GRADED FUNCTION: CausalAttention</span></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> CausalAttention(d_feature, </span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>                    n_heads, </span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>                    compute_attention_heads_closure<span class="op">=</span>compute_attention_heads_closure,</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>                    dot_product_self_attention<span class="op">=</span>dot_product_self_attention,</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>                    compute_attention_output_closure<span class="op">=</span>compute_attention_output_closure,</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>                    mode<span class="op">=</span><span class="st">'train'</span>):</span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Transformer-style multi-headed causal attention.</span></span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a><span class="co">        d_feature (int):  dimensionality of feature embedding.</span></span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a><span class="co">        n_heads (int): number of attention heads.</span></span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a><span class="co">        compute_attention_heads_closure (function): Closure around compute_attention heads.</span></span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a><span class="co">        dot_product_self_attention (function): dot_product_self_attention function. </span></span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true" tabindex="-1"></a><span class="co">        compute_attention_output_closure (function): Closure around compute_attention_output. </span></span>
<span id="cb45-17"><a href="#cb45-17" aria-hidden="true" tabindex="-1"></a><span class="co">        mode (str): 'train' or 'eval'.</span></span>
<span id="cb45-18"><a href="#cb45-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-19"><a href="#cb45-19" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb45-20"><a href="#cb45-20" aria-hidden="true" tabindex="-1"></a><span class="co">        trax.layers.combinators.Serial: Multi-headed self-attention model.</span></span>
<span id="cb45-21"><a href="#cb45-21" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb45-22"><a href="#cb45-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb45-23"><a href="#cb45-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> d_feature <span class="op">%</span> n_heads <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb45-24"><a href="#cb45-24" aria-hidden="true" tabindex="-1"></a>    d_head <span class="op">=</span> d_feature <span class="op">//</span> n_heads</span>
<span id="cb45-25"><a href="#cb45-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-26"><a href="#cb45-26" aria-hidden="true" tabindex="-1"></a>    <span class="co">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) </span><span class="al">###</span></span>
<span id="cb45-27"><a href="#cb45-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb45-28"><a href="#cb45-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># HINT: The second argument to tl.Fn() is an uncalled function (without the parentheses)</span></span>
<span id="cb45-29"><a href="#cb45-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Since you are dealing with closures you might need to call the outer </span></span>
<span id="cb45-30"><a href="#cb45-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># function with the correct parameters to get the actual uncalled function.</span></span>
<span id="cb45-31"><a href="#cb45-31" aria-hidden="true" tabindex="-1"></a>    ComputeAttentionHeads <span class="op">=</span> tl.Fn(<span class="st">'AttnHeads'</span>, <span class="va">None</span>, n_out<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb45-32"><a href="#cb45-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb45-33"><a href="#cb45-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-34"><a href="#cb45-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tl.Serial(</span>
<span id="cb45-35"><a href="#cb45-35" aria-hidden="true" tabindex="-1"></a>        tl.Branch( <span class="co"># creates three towers for one input, takes activations and creates queries keys and values</span></span>
<span id="cb45-36"><a href="#cb45-36" aria-hidden="true" tabindex="-1"></a>            [<span class="va">None</span>, <span class="va">None</span>], <span class="co"># queries</span></span>
<span id="cb45-37"><a href="#cb45-37" aria-hidden="true" tabindex="-1"></a>            [<span class="va">None</span>, <span class="va">None</span>], <span class="co"># keys</span></span>
<span id="cb45-38"><a href="#cb45-38" aria-hidden="true" tabindex="-1"></a>            [<span class="va">None</span>, <span class="va">None</span>], <span class="co"># values</span></span>
<span id="cb45-39"><a href="#cb45-39" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb45-40"><a href="#cb45-40" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb45-41"><a href="#cb45-41" aria-hidden="true" tabindex="-1"></a>        tl.Fn(<span class="st">'DotProductAttn'</span>, <span class="va">None</span>, n_out<span class="op">=</span><span class="dv">1</span>), <span class="co"># takes QKV</span></span>
<span id="cb45-42"><a href="#cb45-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># HINT: The second argument to tl.Fn() is an uncalled function</span></span>
<span id="cb45-43"><a href="#cb45-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Since you are dealing with closures you might need to call the outer </span></span>
<span id="cb45-44"><a href="#cb45-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># function with the correct parameters to get the actual uncalled function.</span></span>
<span id="cb45-45"><a href="#cb45-45" aria-hidden="true" tabindex="-1"></a>        tl.Fn(<span class="st">'AttnOutput'</span>, <span class="va">None</span>, n_out<span class="op">=</span><span class="dv">1</span>), <span class="co"># to allow for parallel</span></span>
<span id="cb45-46"><a href="#cb45-46" aria-hidden="true" tabindex="-1"></a>        <span class="va">None</span> <span class="co"># Final dense layer</span></span>
<span id="cb45-47"><a href="#cb45-47" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb45-48"><a href="#cb45-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-49"><a href="#cb45-49" aria-hidden="true" tabindex="-1"></a>    <span class="co">### </span><span class="re">END</span><span class="co"> CODE HERE </span><span class="al">###</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="c7e3fbde" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Take a look at the causal attention model</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(CausalAttention(d_feature<span class="op">=</span><span class="dv">512</span>, n_heads<span class="op">=</span><span class="dv">8</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">TypeError</span>                                 Traceback (most recent call last)
File <span class="ansi-green-fg">/usr/lib/python3.10/inspect.py:1277</span>, in <span class="ansi-cyan-fg">getfullargspec</span><span class="ansi-blue-fg">(func)</span>
<span class="ansi-green-fg ansi-bold">   1260</span> <span style="font-weight:bold;color:rgb(0,135,0)">try</span>:
<span class="ansi-green-fg ansi-bold">   1261</span>     <span style="font-style:italic;color:rgb(95,135,135)"># Re: `skip_bound_arg=False`</span>
<span class="ansi-green-fg ansi-bold">   1262</span>     <span style="font-style:italic;color:rgb(95,135,135)">#</span>
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-fg ansi-bold">   1274</span>     <span style="font-style:italic;color:rgb(95,135,135)"># getfullargspec() historically ignored __wrapped__ attributes,</span>
<span class="ansi-green-fg ansi-bold">   1275</span>     <span style="font-style:italic;color:rgb(95,135,135)"># so we ensure that remains the case in 3.3+</span>
<span class="ansi-green-fg">-&gt; 1277</span>     sig <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">_signature_from_callable</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">func</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">   1278</span> <span class="ansi-yellow-bg">                                   </span><span class="ansi-yellow-bg">follow_wrapper_chains</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">False</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">   1279</span> <span class="ansi-yellow-bg">                                   </span><span class="ansi-yellow-bg">skip_bound_arg</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">False</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">   1280</span> <span class="ansi-yellow-bg">                                   </span><span class="ansi-yellow-bg">sigcls</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">Signature</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">   1281</span> <span class="ansi-yellow-bg">                                   </span><span class="ansi-yellow-bg">eval_str</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">False</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">   1282</span> <span style="font-weight:bold;color:rgb(0,135,0)">except</span> <span style="font-weight:bold;color:rgb(215,95,95)">Exception</span> <span style="font-weight:bold;color:rgb(0,135,0)">as</span> ex:
<span class="ansi-green-fg ansi-bold">   1283</span>     <span style="font-style:italic;color:rgb(95,135,135)"># Most of the times 'signature' will raise ValueError.</span>
<span class="ansi-green-fg ansi-bold">   1284</span>     <span style="font-style:italic;color:rgb(95,135,135)"># But, it can also raise AttributeError, and, maybe something</span>
<span class="ansi-green-fg ansi-bold">   1285</span>     <span style="font-style:italic;color:rgb(95,135,135)"># else. So to be fully backwards compatible, we catch all</span>
<span class="ansi-green-fg ansi-bold">   1286</span>     <span style="font-style:italic;color:rgb(95,135,135)"># possible exceptions here, and reraise a TypeError.</span>

File <span class="ansi-green-fg">/usr/lib/python3.10/inspect.py:2396</span>, in <span class="ansi-cyan-fg">_signature_from_callable</span><span class="ansi-blue-fg">(obj, follow_wrapper_chains, skip_bound_arg, globals, locals, eval_str, sigcls)</span>
<span class="ansi-green-fg ansi-bold">   2395</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="font-weight:bold;color:rgb(175,0,255)">not</span> <span style="color:rgb(0,135,0)">callable</span>(obj):
<span class="ansi-green-fg">-&gt; 2396</span>     <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> <span style="font-weight:bold;color:rgb(215,95,95)">TypeError</span>(<span style="color:rgb(175,0,0)">'</span><span style="font-weight:bold;color:rgb(175,95,135)">{!r}</span><span style="color:rgb(175,0,0)"> is not a callable object</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(98,98,98)">.</span>format(obj))
<span class="ansi-green-fg ansi-bold">   2398</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="color:rgb(0,135,0)">isinstance</span>(obj, types<span style="color:rgb(98,98,98)">.</span>MethodType):
<span class="ansi-green-fg ansi-bold">   2399</span>     <span style="font-style:italic;color:rgb(95,135,135)"># In this case we skip the first parameter of the underlying</span>
<span class="ansi-green-fg ansi-bold">   2400</span>     <span style="font-style:italic;color:rgb(95,135,135)"># function (usually `self` or `cls`).</span>

<span class="ansi-red-fg">TypeError</span>: None is not a callable object

The above exception was the direct cause of the following exception:

<span class="ansi-red-fg">TypeError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[27], line 2</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># Take a look at the causal attention model</span>
<span class="ansi-green-fg">----&gt; 2</span> <span style="color:rgb(0,135,0)">print</span>(<span class="ansi-yellow-bg">CausalAttention</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">d_feature</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">512</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">n_heads</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">8</span><span class="ansi-yellow-bg">)</span>)

Cell <span class="ansi-green-fg">In[26], line 31</span>, in <span class="ansi-cyan-fg">CausalAttention</span><span class="ansi-blue-fg">(d_feature, n_heads, compute_attention_heads_closure, dot_product_self_attention, compute_attention_output_closure, mode)</span>
<span class="ansi-green-fg ansi-bold">     24</span> d_head <span style="color:rgb(98,98,98)">=</span> d_feature <span style="color:rgb(98,98,98)">/</span><span style="color:rgb(98,98,98)">/</span> n_heads
<span class="ansi-green-fg ansi-bold">     26</span> <span style="font-style:italic;color:rgb(95,135,135)">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span>
<span class="ansi-green-fg ansi-bold">     27</span> 
<span class="ansi-green-fg ansi-bold">     28</span> <span style="font-style:italic;color:rgb(95,135,135)"># HINT: The second argument to tl.Fn() is an uncalled function (without the parentheses)</span>
<span class="ansi-green-fg ansi-bold">     29</span> <span style="font-style:italic;color:rgb(95,135,135)"># Since you are dealing with closures you might need to call the outer </span>
<span class="ansi-green-fg ansi-bold">     30</span> <span style="font-style:italic;color:rgb(95,135,135)"># function with the correct parameters to get the actual uncalled function.</span>
<span class="ansi-green-fg">---&gt; 31</span> ComputeAttentionHeads <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">tl</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">Fn</span><span class="ansi-yellow-bg">(</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">'</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">AttnHeads</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">'</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">None</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">n_out</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">1</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">     34</span> <span style="font-weight:bold;color:rgb(0,135,0)">return</span> tl<span style="color:rgb(98,98,98)">.</span>Serial(
<span class="ansi-green-fg ansi-bold">     35</span>     tl<span style="color:rgb(98,98,98)">.</span>Branch( <span style="font-style:italic;color:rgb(95,135,135)"># creates three towers for one input, takes activations and creates queries keys and values</span>
<span class="ansi-green-fg ansi-bold">     36</span>         [<span style="font-weight:bold;color:rgb(0,135,0)">None</span>, <span style="font-weight:bold;color:rgb(0,135,0)">None</span>], <span style="font-style:italic;color:rgb(95,135,135)"># queries</span>
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-fg ansi-bold">     46</span>     <span style="font-weight:bold;color:rgb(0,135,0)">None</span> <span style="font-style:italic;color:rgb(95,135,135)"># Final dense layer</span>
<span class="ansi-green-fg ansi-bold">     47</span> )

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/base.py:773</span>, in <span class="ansi-cyan-fg">Fn</span><span class="ansi-blue-fg">(name, f, n_out)</span>
<span class="ansi-green-fg ansi-bold">    748</span> <span style="font-weight:bold;color:rgb(0,135,0)">def</span><span style="color:rgb(188,188,188)"> </span><span style="color:rgb(0,0,255)">Fn</span>(name, f, n_out<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">1</span>):  <span style="font-style:italic;color:rgb(95,135,135)"># pylint: disable=invalid-name</span>
<span class="ansi-green-fg ansi-bold">    749</span> <span style="color:rgb(188,188,188)">  </span><span style="font-style:italic;color:rgb(175,0,0)">"""Returns a layer with no weights that applies the function `f`.</span>
<span class="ansi-green-fg ansi-bold">    750</span> 
<span class="ansi-green-fg ansi-bold">    751</span> <span style="font-style:italic;color:rgb(175,0,0)">  `f` can take and return any number of arguments, and takes only positional</span>
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-fg ansi-bold">    771</span> <span style="font-style:italic;color:rgb(175,0,0)">    Layer executing the function `f`.</span>
<span class="ansi-green-fg ansi-bold">    772</span> <span style="font-style:italic;color:rgb(175,0,0)">  """</span>
<span class="ansi-green-fg">--&gt; 773</span>   argspec <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">inspect</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">getfullargspec</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">f</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">    774</span>   <span style="font-weight:bold;color:rgb(0,135,0)">if</span> argspec<span style="color:rgb(98,98,98)">.</span>defaults <span style="font-weight:bold;color:rgb(175,0,255)">is</span> <span style="font-weight:bold;color:rgb(175,0,255)">not</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span>:
<span class="ansi-green-fg ansi-bold">    775</span>     <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> <span style="font-weight:bold;color:rgb(215,95,95)">ValueError</span>(<span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">Function has default arguments (not allowed).</span><span style="color:rgb(175,0,0)">'</span>)

File <span class="ansi-green-fg">/usr/lib/python3.10/inspect.py:1287</span>, in <span class="ansi-cyan-fg">getfullargspec</span><span class="ansi-blue-fg">(func)</span>
<span class="ansi-green-fg ansi-bold">   1277</span>     sig <span style="color:rgb(98,98,98)">=</span> _signature_from_callable(func,
<span class="ansi-green-fg ansi-bold">   1278</span>                                    follow_wrapper_chains<span style="color:rgb(98,98,98)">=</span><span style="font-weight:bold;color:rgb(0,135,0)">False</span>,
<span class="ansi-green-fg ansi-bold">   1279</span>                                    skip_bound_arg<span style="color:rgb(98,98,98)">=</span><span style="font-weight:bold;color:rgb(0,135,0)">False</span>,
<span class="ansi-green-fg ansi-bold">   1280</span>                                    sigcls<span style="color:rgb(98,98,98)">=</span>Signature,
<span class="ansi-green-fg ansi-bold">   1281</span>                                    eval_str<span style="color:rgb(98,98,98)">=</span><span style="font-weight:bold;color:rgb(0,135,0)">False</span>)
<span class="ansi-green-fg ansi-bold">   1282</span> <span style="font-weight:bold;color:rgb(0,135,0)">except</span> <span style="font-weight:bold;color:rgb(215,95,95)">Exception</span> <span style="font-weight:bold;color:rgb(0,135,0)">as</span> ex:
<span class="ansi-green-fg ansi-bold">   1283</span>     <span style="font-style:italic;color:rgb(95,135,135)"># Most of the times 'signature' will raise ValueError.</span>
<span class="ansi-green-fg ansi-bold">   1284</span>     <span style="font-style:italic;color:rgb(95,135,135)"># But, it can also raise AttributeError, and, maybe something</span>
<span class="ansi-green-fg ansi-bold">   1285</span>     <span style="font-style:italic;color:rgb(95,135,135)"># else. So to be fully backwards compatible, we catch all</span>
<span class="ansi-green-fg ansi-bold">   1286</span>     <span style="font-style:italic;color:rgb(95,135,135)"># possible exceptions here, and reraise a TypeError.</span>
<span class="ansi-green-fg">-&gt; 1287</span>     <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> <span style="font-weight:bold;color:rgb(215,95,95)">TypeError</span>(<span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">unsupported callable</span><span style="color:rgb(175,0,0)">'</span>) <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">ex</span>
<span class="ansi-green-fg ansi-bold">   1289</span> args <span style="color:rgb(98,98,98)">=</span> []
<span class="ansi-green-fg ansi-bold">   1290</span> varargs <span style="color:rgb(98,98,98)">=</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span>

<span class="ansi-red-fg">TypeError</span>: unsupported callable</pre>
</div>
</div>
</div>
<p><strong>Expected Output:</strong></p>
<div class="sourceCode" id="cb47"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>Serial<span class="op">[</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>  Branch_out3<span class="op">[</span></span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>    <span class="op">[</span>Dense_512<span class="op">,</span> AttnHeads<span class="op">]</span></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>    <span class="op">[</span>Dense_512<span class="op">,</span> AttnHeads<span class="op">]</span></span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>    <span class="op">[</span>Dense_512<span class="op">,</span> AttnHeads<span class="op">]</span></span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>  <span class="op">]</span></span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>  DotProductAttn_in3</span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>  AttnOutput</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>  Dense_512</span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><a name="2.3"></a></p>
</section>
</section>
<section id="transformer-decoder-block" class="level2">
<h2 class="anchored" data-anchor-id="transformer-decoder-block">2.3 Transformer decoder block</h2>
<p>Now that you have implemented the causal part of the transformer, you will implement the transformer decoder block. Concretely you will be implementing this image now.</p>
<p><img src="transformer_decoder_1.png" style="height:300px"></p>
<p>To implement this function, you will have to call the <code>CausalAttention</code> or Masked multi-head attention function you implemented above. You will have to add a feedforward which consists of:</p>
<ul>
<li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.normalization.LayerNorm">tl.LayerNorm</a> </span>: used to layer normalize</li>
<li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense">tl.Dense</a> </span>: the dense layer</li>
<li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.activation_fns.Relu">ff_activation</a> </span>: feed forward activation (we use ReLu) here.</li>
<li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dropout">tl.Dropout</a> </span>: dropout layer</li>
<li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense">tl.Dense</a> </span>: dense layer</li>
<li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dropout">tl.Dropout</a> </span>: dropout layer</li>
</ul>
<p>Finally once you implement the feedforward, you can go ahead and implement the entire block using:</p>
<ul>
<li><p><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Residual">tl.Residual</a> </span>: takes in the tl.LayerNorm(), causal attention block, tl.dropout.</p></li>
<li><p><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Residual">tl.Residual</a> </span>: takes in the feedforward block you will implement.</p></li>
</ul>
<p><a name="ex03"></a> ### Exercise 03 <strong>Instructions:</strong> Implement the transformer decoder block. Good luck!</p>
<div id="98cf5192" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># UNQ_C6</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="co"># GRADED FUNCTION: DecoderBlock</span></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> DecoderBlock(d_model, d_ff, n_heads,</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>                 dropout, mode, ff_activation):</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Returns a list of layers that implements a Transformer decoder block.</span></span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a><span class="co">    The input is an activation tensor.</span></span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a><span class="co">        d_model (int):  depth of embedding.</span></span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a><span class="co">        d_ff (int): depth of feed-forward layer.</span></span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a><span class="co">        n_heads (int): number of attention heads.</span></span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a><span class="co">        dropout (float): dropout rate (how much to drop out).</span></span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a><span class="co">        mode (str): 'train' or 'eval'.</span></span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a><span class="co">        ff_activation (function): the non-linearity in feed-forward layer.</span></span>
<span id="cb48-16"><a href="#cb48-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-17"><a href="#cb48-17" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb48-18"><a href="#cb48-18" aria-hidden="true" tabindex="-1"></a><span class="co">        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.</span></span>
<span id="cb48-19"><a href="#cb48-19" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb48-20"><a href="#cb48-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb48-21"><a href="#cb48-21" aria-hidden="true" tabindex="-1"></a>    <span class="co">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) </span><span class="al">###</span></span>
<span id="cb48-22"><a href="#cb48-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb48-23"><a href="#cb48-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create masked multi-head attention block using CausalAttention function</span></span>
<span id="cb48-24"><a href="#cb48-24" aria-hidden="true" tabindex="-1"></a>    causal_attention <span class="op">=</span> CausalAttention( </span>
<span id="cb48-25"><a href="#cb48-25" aria-hidden="true" tabindex="-1"></a>                        <span class="va">None</span>,</span>
<span id="cb48-26"><a href="#cb48-26" aria-hidden="true" tabindex="-1"></a>                        n_heads<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb48-27"><a href="#cb48-27" aria-hidden="true" tabindex="-1"></a>                        mode<span class="op">=</span><span class="va">None</span></span>
<span id="cb48-28"><a href="#cb48-28" aria-hidden="true" tabindex="-1"></a>                        )</span>
<span id="cb48-29"><a href="#cb48-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-30"><a href="#cb48-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create feed-forward block (list) with two dense layers with dropout and input normalized</span></span>
<span id="cb48-31"><a href="#cb48-31" aria-hidden="true" tabindex="-1"></a>    feed_forward <span class="op">=</span> [ </span>
<span id="cb48-32"><a href="#cb48-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Normalize layer inputs</span></span>
<span id="cb48-33"><a href="#cb48-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">None</span>,</span>
<span id="cb48-34"><a href="#cb48-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add first feed forward (dense) layer (don't forget to set the correct value for n_units)</span></span>
<span id="cb48-35"><a href="#cb48-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">None</span>,</span>
<span id="cb48-36"><a href="#cb48-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add activation function passed in as a parameter (you need to call it!)</span></span>
<span id="cb48-37"><a href="#cb48-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">None</span>, <span class="co"># Generally ReLU</span></span>
<span id="cb48-38"><a href="#cb48-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)</span></span>
<span id="cb48-39"><a href="#cb48-39" aria-hidden="true" tabindex="-1"></a>        <span class="va">None</span>,</span>
<span id="cb48-40"><a href="#cb48-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add second feed forward layer (don't forget to set the correct value for n_units)</span></span>
<span id="cb48-41"><a href="#cb48-41" aria-hidden="true" tabindex="-1"></a>        <span class="va">None</span>,</span>
<span id="cb48-42"><a href="#cb48-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)</span></span>
<span id="cb48-43"><a href="#cb48-43" aria-hidden="true" tabindex="-1"></a>        <span class="va">None</span></span>
<span id="cb48-44"><a href="#cb48-44" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb48-45"><a href="#cb48-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-46"><a href="#cb48-46" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add list of two Residual blocks: the attention with normalization and dropout and feed-forward blocks</span></span>
<span id="cb48-47"><a href="#cb48-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [</span>
<span id="cb48-48"><a href="#cb48-48" aria-hidden="true" tabindex="-1"></a>      tl.Residual(</span>
<span id="cb48-49"><a href="#cb48-49" aria-hidden="true" tabindex="-1"></a>          <span class="co"># Normalize layer input</span></span>
<span id="cb48-50"><a href="#cb48-50" aria-hidden="true" tabindex="-1"></a>          <span class="va">None</span>,</span>
<span id="cb48-51"><a href="#cb48-51" aria-hidden="true" tabindex="-1"></a>          <span class="co"># Add causal attention block previously defined (without parentheses)</span></span>
<span id="cb48-52"><a href="#cb48-52" aria-hidden="true" tabindex="-1"></a>          <span class="va">None</span>,</span>
<span id="cb48-53"><a href="#cb48-53" aria-hidden="true" tabindex="-1"></a>          <span class="co"># Add dropout with rate and mode specified</span></span>
<span id="cb48-54"><a href="#cb48-54" aria-hidden="true" tabindex="-1"></a>          <span class="va">None</span></span>
<span id="cb48-55"><a href="#cb48-55" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb48-56"><a href="#cb48-56" aria-hidden="true" tabindex="-1"></a>      tl.Residual(</span>
<span id="cb48-57"><a href="#cb48-57" aria-hidden="true" tabindex="-1"></a>          <span class="co"># Add feed forward block (without parentheses)</span></span>
<span id="cb48-58"><a href="#cb48-58" aria-hidden="true" tabindex="-1"></a>          <span class="va">None</span></span>
<span id="cb48-59"><a href="#cb48-59" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb48-60"><a href="#cb48-60" aria-hidden="true" tabindex="-1"></a>      ]</span>
<span id="cb48-61"><a href="#cb48-61" aria-hidden="true" tabindex="-1"></a>    <span class="co">### </span><span class="re">END</span><span class="co"> CODE HERE </span><span class="al">###</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="b97a1e47" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Take a look at the decoder block</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(DecoderBlock(d_model<span class="op">=</span><span class="dv">512</span>, d_ff<span class="op">=</span><span class="dv">2048</span>, n_heads<span class="op">=</span><span class="dv">8</span>, dropout<span class="op">=</span><span class="fl">0.1</span>, mode<span class="op">=</span><span class="st">'train'</span>, ff_activation<span class="op">=</span>tl.Relu))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">TypeError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[29], line 2</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># Take a look at the decoder block</span>
<span class="ansi-green-fg">----&gt; 2</span> <span style="color:rgb(0,135,0)">print</span>(<span class="ansi-yellow-bg">DecoderBlock</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">d_model</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">512</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">d_ff</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">2048</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">n_heads</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">8</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">dropout</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">0.1</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">mode</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">'</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">train</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">'</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">ff_activation</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">tl</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">Relu</span><span class="ansi-yellow-bg">)</span>)

Cell <span class="ansi-green-fg">In[28], line 24</span>, in <span class="ansi-cyan-fg">DecoderBlock</span><span class="ansi-blue-fg">(d_model, d_ff, n_heads, dropout, mode, ff_activation)</span>
<span class="ansi-green-fg ansi-bold">      5</span> <span style="font-style:italic;color:rgb(175,0,0)">"""Returns a list of layers that implements a Transformer decoder block.</span>
<span class="ansi-green-fg ansi-bold">      6</span> 
<span class="ansi-green-fg ansi-bold">      7</span> <span style="font-style:italic;color:rgb(175,0,0)">The input is an activation tensor.</span>
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-fg ansi-bold">     18</span> <span style="font-style:italic;color:rgb(175,0,0)">    list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.</span>
<span class="ansi-green-fg ansi-bold">     19</span> <span style="font-style:italic;color:rgb(175,0,0)">"""</span>
<span class="ansi-green-fg ansi-bold">     21</span> <span style="font-style:italic;color:rgb(95,135,135)">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span>
<span class="ansi-green-fg ansi-bold">     22</span> 
<span class="ansi-green-fg ansi-bold">     23</span> <span style="font-style:italic;color:rgb(95,135,135)"># Create masked multi-head attention block using CausalAttention function</span>
<span class="ansi-green-fg">---&gt; 24</span> causal_attention <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">CausalAttention</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg"> </span>
<span class="ansi-green-fg ansi-bold">     25</span> <span class="ansi-yellow-bg">                    </span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">None</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">     26</span> <span class="ansi-yellow-bg">                    </span><span class="ansi-yellow-bg">n_heads</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">None</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">     27</span> <span class="ansi-yellow-bg">                    </span><span class="ansi-yellow-bg">mode</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">None</span>
<span class="ansi-green-fg ansi-bold">     28</span> <span class="ansi-yellow-bg">                    </span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">     30</span> <span style="font-style:italic;color:rgb(95,135,135)"># Create feed-forward block (list) with two dense layers with dropout and input normalized</span>
<span class="ansi-green-fg ansi-bold">     31</span> feed_forward <span style="color:rgb(98,98,98)">=</span> [ 
<span class="ansi-green-fg ansi-bold">     32</span>     <span style="font-style:italic;color:rgb(95,135,135)"># Normalize layer inputs</span>
<span class="ansi-green-fg ansi-bold">     33</span>     <span style="font-weight:bold;color:rgb(0,135,0)">None</span>,
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-fg ansi-bold">     43</span>     <span style="font-weight:bold;color:rgb(0,135,0)">None</span>
<span class="ansi-green-fg ansi-bold">     44</span> ]

Cell <span class="ansi-green-fg">In[26], line 23</span>, in <span class="ansi-cyan-fg">CausalAttention</span><span class="ansi-blue-fg">(d_feature, n_heads, compute_attention_heads_closure, dot_product_self_attention, compute_attention_output_closure, mode)</span>
<span class="ansi-green-fg ansi-bold">      3</span> <span style="font-weight:bold;color:rgb(0,135,0)">def</span><span style="color:rgb(188,188,188)"> </span><span style="color:rgb(0,0,255)">CausalAttention</span>(d_feature, 
<span class="ansi-green-fg ansi-bold">      4</span>                     n_heads, 
<span class="ansi-green-fg ansi-bold">      5</span>                     compute_attention_heads_closure<span style="color:rgb(98,98,98)">=</span>compute_attention_heads_closure,
<span class="ansi-green-fg ansi-bold">      6</span>                     dot_product_self_attention<span style="color:rgb(98,98,98)">=</span>dot_product_self_attention,
<span class="ansi-green-fg ansi-bold">      7</span>                     compute_attention_output_closure<span style="color:rgb(98,98,98)">=</span>compute_attention_output_closure,
<span class="ansi-green-fg ansi-bold">      8</span>                     mode<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">train</span><span style="color:rgb(175,0,0)">'</span>):
<span class="ansi-green-fg ansi-bold">      9</span> <span style="color:rgb(188,188,188)">    </span><span style="font-style:italic;color:rgb(175,0,0)">"""Transformer-style multi-headed causal attention.</span>
<span class="ansi-green-fg ansi-bold">     10</span> 
<span class="ansi-green-fg ansi-bold">     11</span> <span style="font-style:italic;color:rgb(175,0,0)">    Args:</span>
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-fg ansi-bold">     20</span> <span style="font-style:italic;color:rgb(175,0,0)">        trax.layers.combinators.Serial: Multi-headed self-attention model.</span>
<span class="ansi-green-fg ansi-bold">     21</span> <span style="font-style:italic;color:rgb(175,0,0)">    """</span>
<span class="ansi-green-fg">---&gt; 23</span>     <span style="font-weight:bold;color:rgb(0,135,0)">assert</span> <span class="ansi-yellow-bg">d_feature</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">%</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">n_heads</span> <span style="color:rgb(98,98,98)">==</span> <span style="color:rgb(98,98,98)">0</span>
<span class="ansi-green-fg ansi-bold">     24</span>     d_head <span style="color:rgb(98,98,98)">=</span> d_feature <span style="color:rgb(98,98,98)">/</span><span style="color:rgb(98,98,98)">/</span> n_heads
<span class="ansi-green-fg ansi-bold">     26</span>     <span style="font-style:italic;color:rgb(95,135,135)">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span>
<span class="ansi-green-fg ansi-bold">     27</span>     
<span class="ansi-green-fg ansi-bold">     28</span>     <span style="font-style:italic;color:rgb(95,135,135)"># HINT: The second argument to tl.Fn() is an uncalled function (without the parentheses)</span>
<span class="ansi-green-fg ansi-bold">     29</span>     <span style="font-style:italic;color:rgb(95,135,135)"># Since you are dealing with closures you might need to call the outer </span>
<span class="ansi-green-fg ansi-bold">     30</span>     <span style="font-style:italic;color:rgb(95,135,135)"># function with the correct parameters to get the actual uncalled function.</span>

<span class="ansi-red-fg">TypeError</span>: unsupported operand type(s) for %: 'NoneType' and 'NoneType'</pre>
</div>
</div>
</div>
<p><strong>Expected Output:</strong></p>
<div class="sourceCode" id="cb50"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="op">[</span>Serial<span class="op">[</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>  Branch_out2<span class="op">[</span></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>    None</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>    Serial<span class="op">[</span></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>      LayerNorm</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>      Serial<span class="op">[</span></span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>        Branch_out3<span class="op">[</span></span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>          <span class="op">[</span>Dense_512<span class="op">,</span> AttnHeads<span class="op">]</span></span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>          <span class="op">[</span>Dense_512<span class="op">,</span> AttnHeads<span class="op">]</span></span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a>          <span class="op">[</span>Dense_512<span class="op">,</span> AttnHeads<span class="op">]</span></span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a>        <span class="op">]</span></span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a>        DotProductAttn_in3</span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a>        AttnOutput</span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a>        Dense_512</span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a>      <span class="op">]</span></span>
<span id="cb50-16"><a href="#cb50-16" aria-hidden="true" tabindex="-1"></a>      Dropout</span>
<span id="cb50-17"><a href="#cb50-17" aria-hidden="true" tabindex="-1"></a>    <span class="op">]</span></span>
<span id="cb50-18"><a href="#cb50-18" aria-hidden="true" tabindex="-1"></a>  <span class="op">]</span></span>
<span id="cb50-19"><a href="#cb50-19" aria-hidden="true" tabindex="-1"></a>  Add_in2</span>
<span id="cb50-20"><a href="#cb50-20" aria-hidden="true" tabindex="-1"></a><span class="op">],</span> Serial<span class="op">[</span></span>
<span id="cb50-21"><a href="#cb50-21" aria-hidden="true" tabindex="-1"></a>  Branch_out2<span class="op">[</span></span>
<span id="cb50-22"><a href="#cb50-22" aria-hidden="true" tabindex="-1"></a>    None</span>
<span id="cb50-23"><a href="#cb50-23" aria-hidden="true" tabindex="-1"></a>    Serial<span class="op">[</span></span>
<span id="cb50-24"><a href="#cb50-24" aria-hidden="true" tabindex="-1"></a>      LayerNorm</span>
<span id="cb50-25"><a href="#cb50-25" aria-hidden="true" tabindex="-1"></a>      Dense_2048</span>
<span id="cb50-26"><a href="#cb50-26" aria-hidden="true" tabindex="-1"></a>      Relu</span>
<span id="cb50-27"><a href="#cb50-27" aria-hidden="true" tabindex="-1"></a>      Dropout</span>
<span id="cb50-28"><a href="#cb50-28" aria-hidden="true" tabindex="-1"></a>      Dense_512</span>
<span id="cb50-29"><a href="#cb50-29" aria-hidden="true" tabindex="-1"></a>      Dropout</span>
<span id="cb50-30"><a href="#cb50-30" aria-hidden="true" tabindex="-1"></a>    <span class="op">]</span></span>
<span id="cb50-31"><a href="#cb50-31" aria-hidden="true" tabindex="-1"></a>  <span class="op">]</span></span>
<span id="cb50-32"><a href="#cb50-32" aria-hidden="true" tabindex="-1"></a>  Add_in2</span>
<span id="cb50-33"><a href="#cb50-33" aria-hidden="true" tabindex="-1"></a><span class="op">]]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><a name="2.4"></a> ## 2.4 Transformer Language Model</p>
<p>You will now bring it all together. In this part you will use all the subcomponents you previously built to make the final model. Concretely, here is the image you will be implementing. <img src="transformer_decoder.png" style="height:400px"></p>
<p><a name="ex04"></a> ### Exercise 04 <strong>Instructions:</strong> Previously you coded the decoder block. Now you will code the transformer language model. Here is what you will need.</p>
<ul>
<li><span style="color:blue"> positional_enconder </span>- a list containing the following layers:
<ul>
<li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding">tl.Embedding</a></span></li>
<li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dropout">tl.Dropout</a></span></li>
<li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.PositionalEncoding">tl.PositionalEncoding</a></span></li>
</ul></li>
<li>A list of <code>n_layers</code> <span style="color:blue"> decoder blocks</span>.</li>
<li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Serial">tl.Serial</a>: </span> takes in the following layers or lists of layers:
<ul>
<li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.ShiftRight">tl.ShiftRight</a>: </span>: shift the tensor to the right by padding on axis 1.</li>
<li><span style="color:blue"> positional_encoder </span>: encodes the text positions.</li>
<li><span style="color:blue"> decoder_blocks </span>: the ones you created.</li>
<li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.normalization.LayerNorm">tl.LayerNorm</a> </span>: a layer norm.</li>
<li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense">tl.Dense</a> </span>: takes in the vocab_size.</li>
<li><span style="color:blue"> <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.LogSoftmax">tl.LogSoftmax</a> </span>: to predict.</li>
</ul></li>
</ul>
<p>Go go go!! You can do it :)</p>
<div id="38b4f85c" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># UNQ_C7</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a><span class="co"># GRADED FUNCTION: TransformerLM</span></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> TransformerLM(vocab_size<span class="op">=</span><span class="dv">33300</span>,</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>                  d_model<span class="op">=</span><span class="dv">512</span>,</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>                  d_ff<span class="op">=</span><span class="dv">2048</span>,</span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>                  n_layers<span class="op">=</span><span class="dv">6</span>,</span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>                  n_heads<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>                  dropout<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>                  max_len<span class="op">=</span><span class="dv">4096</span>,</span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>                  mode<span class="op">=</span><span class="st">'train'</span>,</span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a>                  ff_activation<span class="op">=</span>tl.Relu):</span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Returns a Transformer language model.</span></span>
<span id="cb51-13"><a href="#cb51-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-14"><a href="#cb51-14" aria-hidden="true" tabindex="-1"></a><span class="co">    The input to the model is a tensor of tokens. (This model uses only the</span></span>
<span id="cb51-15"><a href="#cb51-15" aria-hidden="true" tabindex="-1"></a><span class="co">    decoder part of the overall Transformer.)</span></span>
<span id="cb51-16"><a href="#cb51-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-17"><a href="#cb51-17" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb51-18"><a href="#cb51-18" aria-hidden="true" tabindex="-1"></a><span class="co">        vocab_size (int): vocab size.</span></span>
<span id="cb51-19"><a href="#cb51-19" aria-hidden="true" tabindex="-1"></a><span class="co">        d_model (int):  depth of embedding.</span></span>
<span id="cb51-20"><a href="#cb51-20" aria-hidden="true" tabindex="-1"></a><span class="co">        d_ff (int): depth of feed-forward layer.</span></span>
<span id="cb51-21"><a href="#cb51-21" aria-hidden="true" tabindex="-1"></a><span class="co">        n_layers (int): number of decoder layers.</span></span>
<span id="cb51-22"><a href="#cb51-22" aria-hidden="true" tabindex="-1"></a><span class="co">        n_heads (int): number of attention heads.</span></span>
<span id="cb51-23"><a href="#cb51-23" aria-hidden="true" tabindex="-1"></a><span class="co">        dropout (float): dropout rate (how much to drop out).</span></span>
<span id="cb51-24"><a href="#cb51-24" aria-hidden="true" tabindex="-1"></a><span class="co">        max_len (int): maximum symbol length for positional encoding.</span></span>
<span id="cb51-25"><a href="#cb51-25" aria-hidden="true" tabindex="-1"></a><span class="co">        mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.</span></span>
<span id="cb51-26"><a href="#cb51-26" aria-hidden="true" tabindex="-1"></a><span class="co">        ff_activation (function): the non-linearity in feed-forward layer.</span></span>
<span id="cb51-27"><a href="#cb51-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-28"><a href="#cb51-28" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb51-29"><a href="#cb51-29" aria-hidden="true" tabindex="-1"></a><span class="co">        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens</span></span>
<span id="cb51-30"><a href="#cb51-30" aria-hidden="true" tabindex="-1"></a><span class="co">        to activations over a vocab set.</span></span>
<span id="cb51-31"><a href="#cb51-31" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb51-32"><a href="#cb51-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb51-33"><a href="#cb51-33" aria-hidden="true" tabindex="-1"></a>    <span class="co">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) </span><span class="al">###</span></span>
<span id="cb51-34"><a href="#cb51-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb51-35"><a href="#cb51-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Embedding inputs and positional encoder</span></span>
<span id="cb51-36"><a href="#cb51-36" aria-hidden="true" tabindex="-1"></a>    positional_encoder <span class="op">=</span> [ </span>
<span id="cb51-37"><a href="#cb51-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add embedding layer of dimension (vocab_size, d_model)</span></span>
<span id="cb51-38"><a href="#cb51-38" aria-hidden="true" tabindex="-1"></a>        <span class="va">None</span>,</span>
<span id="cb51-39"><a href="#cb51-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Use dropout with rate and mode specified</span></span>
<span id="cb51-40"><a href="#cb51-40" aria-hidden="true" tabindex="-1"></a>        <span class="va">None</span>,</span>
<span id="cb51-41"><a href="#cb51-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add positional encoding layer with maximum input length and mode specified</span></span>
<span id="cb51-42"><a href="#cb51-42" aria-hidden="true" tabindex="-1"></a>        <span class="va">None</span>]</span>
<span id="cb51-43"><a href="#cb51-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-44"><a href="#cb51-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create stack (list) of decoder blocks with n_layers with necessary parameters</span></span>
<span id="cb51-45"><a href="#cb51-45" aria-hidden="true" tabindex="-1"></a>    decoder_blocks <span class="op">=</span> [ </span>
<span id="cb51-46"><a href="#cb51-46" aria-hidden="true" tabindex="-1"></a>        <span class="va">None</span> <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="va">None</span>)]</span>
<span id="cb51-47"><a href="#cb51-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-48"><a href="#cb51-48" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create the complete model as written in the figure</span></span>
<span id="cb51-49"><a href="#cb51-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tl.Serial(</span>
<span id="cb51-50"><a href="#cb51-50" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Use teacher forcing (feed output of previous step to current step)</span></span>
<span id="cb51-51"><a href="#cb51-51" aria-hidden="true" tabindex="-1"></a>        <span class="va">None</span>, <span class="co"># Specify the mode!</span></span>
<span id="cb51-52"><a href="#cb51-52" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add positional encoder</span></span>
<span id="cb51-53"><a href="#cb51-53" aria-hidden="true" tabindex="-1"></a>        <span class="va">None</span>,</span>
<span id="cb51-54"><a href="#cb51-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add decoder blocks</span></span>
<span id="cb51-55"><a href="#cb51-55" aria-hidden="true" tabindex="-1"></a>        <span class="va">None</span>,</span>
<span id="cb51-56"><a href="#cb51-56" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Normalize layer</span></span>
<span id="cb51-57"><a href="#cb51-57" aria-hidden="true" tabindex="-1"></a>        <span class="va">None</span>,</span>
<span id="cb51-58"><a href="#cb51-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-59"><a href="#cb51-59" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add dense layer of vocab_size (since need to select a word to translate to)</span></span>
<span id="cb51-60"><a href="#cb51-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (a.k.a., logits layer. Note: activation already set by ff_activation)</span></span>
<span id="cb51-61"><a href="#cb51-61" aria-hidden="true" tabindex="-1"></a>        <span class="va">None</span>,</span>
<span id="cb51-62"><a href="#cb51-62" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get probabilities with Logsoftmax</span></span>
<span id="cb51-63"><a href="#cb51-63" aria-hidden="true" tabindex="-1"></a>        <span class="va">None</span></span>
<span id="cb51-64"><a href="#cb51-64" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb51-65"><a href="#cb51-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-66"><a href="#cb51-66" aria-hidden="true" tabindex="-1"></a>    <span class="co">### </span><span class="re">END</span><span class="co"> CODE HERE </span><span class="al">###</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="8210d173" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Take a look at the Transformer</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(TransformerLM(n_layers<span class="op">=</span><span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">TypeError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[31], line 2</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># Take a look at the Transformer</span>
<span class="ansi-green-fg">----&gt; 2</span> <span style="color:rgb(0,135,0)">print</span>(<span class="ansi-yellow-bg">TransformerLM</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">n_layers</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">1</span><span class="ansi-yellow-bg">)</span>)

Cell <span class="ansi-green-fg">In[30], line 46</span>, in <span class="ansi-cyan-fg">TransformerLM</span><span class="ansi-blue-fg">(vocab_size, d_model, d_ff, n_layers, n_heads, dropout, max_len, mode, ff_activation)</span>
<span class="ansi-green-fg ansi-bold">     36</span> positional_encoder <span style="color:rgb(98,98,98)">=</span> [ 
<span class="ansi-green-fg ansi-bold">     37</span>     <span style="font-style:italic;color:rgb(95,135,135)"># Add embedding layer of dimension (vocab_size, d_model)</span>
<span class="ansi-green-fg ansi-bold">     38</span>     <span style="font-weight:bold;color:rgb(0,135,0)">None</span>,
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-fg ansi-bold">     41</span>     <span style="font-style:italic;color:rgb(95,135,135)"># Add positional encoding layer with maximum input length and mode specified</span>
<span class="ansi-green-fg ansi-bold">     42</span>     <span style="font-weight:bold;color:rgb(0,135,0)">None</span>]
<span class="ansi-green-fg ansi-bold">     44</span> <span style="font-style:italic;color:rgb(95,135,135)"># Create stack (list) of decoder blocks with n_layers with necessary parameters</span>
<span class="ansi-green-fg ansi-bold">     45</span> decoder_blocks <span style="color:rgb(98,98,98)">=</span> [ 
<span class="ansi-green-fg">---&gt; 46</span>     <span style="font-weight:bold;color:rgb(0,135,0)">None</span> <span style="font-weight:bold;color:rgb(0,135,0)">for</span> _ <span style="font-weight:bold;color:rgb(175,0,255)">in</span> <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">range</span><span class="ansi-yellow-bg">(</span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">None</span><span class="ansi-yellow-bg">)</span>]
<span class="ansi-green-fg ansi-bold">     48</span> <span style="font-style:italic;color:rgb(95,135,135)"># Create the complete model as written in the figure</span>
<span class="ansi-green-fg ansi-bold">     49</span> <span style="font-weight:bold;color:rgb(0,135,0)">return</span> tl<span style="color:rgb(98,98,98)">.</span>Serial(
<span class="ansi-green-fg ansi-bold">     50</span>     <span style="font-style:italic;color:rgb(95,135,135)"># Use teacher forcing (feed output of previous step to current step)</span>
<span class="ansi-green-fg ansi-bold">     51</span>     <span style="font-weight:bold;color:rgb(0,135,0)">None</span>, <span style="font-style:italic;color:rgb(95,135,135)"># Specify the mode!</span>
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-fg ansi-bold">     63</span>     <span style="font-weight:bold;color:rgb(0,135,0)">None</span>
<span class="ansi-green-fg ansi-bold">     64</span> )

<span class="ansi-red-fg">TypeError</span>: 'NoneType' object cannot be interpreted as an integer</pre>
</div>
</div>
</div>
<p><strong>Expected Output:</strong></p>
<div class="sourceCode" id="cb53"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>Serial<span class="op">[</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>  ShiftRight<span class="op">(</span><span class="dv">1</span><span class="op">)</span></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>  Embedding_33300_512</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>  Dropout</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>  PositionalEncoding</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>  Serial<span class="op">[</span></span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>    Branch_out2<span class="op">[</span></span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>      None</span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>      Serial<span class="op">[</span></span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a>        LayerNorm</span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a>        Serial<span class="op">[</span></span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a>          Branch_out3<span class="op">[</span></span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a>            <span class="op">[</span>Dense_512<span class="op">,</span> AttnHeads<span class="op">]</span></span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true" tabindex="-1"></a>            <span class="op">[</span>Dense_512<span class="op">,</span> AttnHeads<span class="op">]</span></span>
<span id="cb53-15"><a href="#cb53-15" aria-hidden="true" tabindex="-1"></a>            <span class="op">[</span>Dense_512<span class="op">,</span> AttnHeads<span class="op">]</span></span>
<span id="cb53-16"><a href="#cb53-16" aria-hidden="true" tabindex="-1"></a>          <span class="op">]</span></span>
<span id="cb53-17"><a href="#cb53-17" aria-hidden="true" tabindex="-1"></a>          DotProductAttn_in3</span>
<span id="cb53-18"><a href="#cb53-18" aria-hidden="true" tabindex="-1"></a>          AttnOutput</span>
<span id="cb53-19"><a href="#cb53-19" aria-hidden="true" tabindex="-1"></a>          Dense_512</span>
<span id="cb53-20"><a href="#cb53-20" aria-hidden="true" tabindex="-1"></a>        <span class="op">]</span></span>
<span id="cb53-21"><a href="#cb53-21" aria-hidden="true" tabindex="-1"></a>        Dropout</span>
<span id="cb53-22"><a href="#cb53-22" aria-hidden="true" tabindex="-1"></a>      <span class="op">]</span></span>
<span id="cb53-23"><a href="#cb53-23" aria-hidden="true" tabindex="-1"></a>    <span class="op">]</span></span>
<span id="cb53-24"><a href="#cb53-24" aria-hidden="true" tabindex="-1"></a>    Add_in2</span>
<span id="cb53-25"><a href="#cb53-25" aria-hidden="true" tabindex="-1"></a>  <span class="op">]</span></span>
<span id="cb53-26"><a href="#cb53-26" aria-hidden="true" tabindex="-1"></a>  Serial<span class="op">[</span></span>
<span id="cb53-27"><a href="#cb53-27" aria-hidden="true" tabindex="-1"></a>    Branch_out2<span class="op">[</span></span>
<span id="cb53-28"><a href="#cb53-28" aria-hidden="true" tabindex="-1"></a>      None</span>
<span id="cb53-29"><a href="#cb53-29" aria-hidden="true" tabindex="-1"></a>      Serial<span class="op">[</span></span>
<span id="cb53-30"><a href="#cb53-30" aria-hidden="true" tabindex="-1"></a>        LayerNorm</span>
<span id="cb53-31"><a href="#cb53-31" aria-hidden="true" tabindex="-1"></a>        Dense_2048</span>
<span id="cb53-32"><a href="#cb53-32" aria-hidden="true" tabindex="-1"></a>        Relu</span>
<span id="cb53-33"><a href="#cb53-33" aria-hidden="true" tabindex="-1"></a>        Dropout</span>
<span id="cb53-34"><a href="#cb53-34" aria-hidden="true" tabindex="-1"></a>        Dense_512</span>
<span id="cb53-35"><a href="#cb53-35" aria-hidden="true" tabindex="-1"></a>        Dropout</span>
<span id="cb53-36"><a href="#cb53-36" aria-hidden="true" tabindex="-1"></a>      <span class="op">]</span></span>
<span id="cb53-37"><a href="#cb53-37" aria-hidden="true" tabindex="-1"></a>    <span class="op">]</span></span>
<span id="cb53-38"><a href="#cb53-38" aria-hidden="true" tabindex="-1"></a>    Add_in2</span>
<span id="cb53-39"><a href="#cb53-39" aria-hidden="true" tabindex="-1"></a>  <span class="op">]</span></span>
<span id="cb53-40"><a href="#cb53-40" aria-hidden="true" tabindex="-1"></a>  LayerNorm</span>
<span id="cb53-41"><a href="#cb53-41" aria-hidden="true" tabindex="-1"></a>  Dense_33300</span>
<span id="cb53-42"><a href="#cb53-42" aria-hidden="true" tabindex="-1"></a>  LogSoftmax</span>
<span id="cb53-43"><a href="#cb53-43" aria-hidden="true" tabindex="-1"></a><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><a name="3"></a> # Part 3: Training</p>
<p>Now you are going to train your model. As usual, you have to define the cost function, the optimizer, and decide whether you will be training it on a <code>gpu</code> or <code>cpu</code>. In this case, you will train your model on a cpu for a few steps and we will load in a pre-trained model that you can use to predict with your own words.</p>
<p><a name="3.1"></a> ### 3.1 Training the model</p>
<p>You will now write a function that takes in your model and trains it. To train your model you have to decide how many times you want to iterate over the entire data set. Each iteration is defined as an <code>epoch</code>. For each epoch, you have to go over all the data, using your training iterator.</p>
<p><a name="ex05"></a> ### Exercise 05 <strong>Instructions:</strong> Implement the <code>train_model</code> program below to train the neural network above. Here is a list of things you should do:</p>
<ul>
<li>Create the train task by calling <a href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.TrainTask"><code>trax.supervised.training.TrainTask</code></a> and pass in the following:
<ul>
<li><span style="color:blue"> labeled_data </span> = train_gen</li>
<li><span style="color:blue"> loss_fn </span> = <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.CrossEntropyLoss">tl.CrossEntropyLoss()</a></li>
<li><span style="color:blue"> optimizer </span> = <a href="https://trax-ml.readthedocs.io/en/latest/trax.optimizers.html#trax.optimizers.adam.Adam">trax.optimizers.Adam(0.01)</a></li>
<li><span style="color:blue"> lr_schedule </span> = <a href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.lr_schedules.warmup_and_rsqrt_decay">lr_schedule</a></li>
</ul></li>
<li>Create the eval task by calling <a href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.EvalTask"><code>trax.supervised.training.EvalTask</code></a> and pass in the following:
<ul>
<li><span style="color:blue"> labeled_data </span> = eval_gen</li>
<li><span style="color:blue"> metrics </span> = tl.CrossEntropyLoss() and <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.Accuracy">tl.Accuracy()</a></li>
</ul></li>
<li>Create the training loop by calling <a href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.Loop"><code>trax.supervised.Training.Loop</code></a> and pass in the following:
<ul>
<li><span style="color:blue"> TransformerLM </span></li>
<li><span style="color:blue"> train_task </span></li>
<li><span style="color:blue"> eval_task </span> = [eval_task]</li>
<li><span style="color:blue"> output_dir</span> = output_dir</li>
</ul></li>
</ul>
<p>You will be using a cross entropy loss, with Adam optimizer. Please read the <a href="https://trax-ml.readthedocs.io/en/latest/index.html">Trax</a> documentation to get a full understanding.</p>
<p>The training loop that this function returns can be runned using the <code>run()</code> method by passing in the desired number of steps.</p>
<div id="518f044e" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> trax.supervised <span class="im">import</span> training</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a><span class="co"># UNQ_C8</span></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a><span class="co"># GRADED FUNCTION: train_model</span></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> training_loop(TransformerLM, train_gen, eval_gen, output_dir <span class="op">=</span> <span class="st">"~/model"</span>):</span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''</span></span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Input:</span></span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a><span class="co">        TransformerLM (trax.layers.combinators.Serial): The model you are building.</span></span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a><span class="co">        train_gen (generator): Training stream of data.</span></span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a><span class="co">        eval_gen (generator): Evaluation stream of data.</span></span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a><span class="co">        output_dir (str): folder to save your file.</span></span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb54-14"><a href="#cb54-14" aria-hidden="true" tabindex="-1"></a><span class="co">        trax.supervised.training.Loop: Training loop.</span></span>
<span id="cb54-15"><a href="#cb54-15" aria-hidden="true" tabindex="-1"></a><span class="co">    '''</span></span>
<span id="cb54-16"><a href="#cb54-16" aria-hidden="true" tabindex="-1"></a>    output_dir <span class="op">=</span> os.path.expanduser(output_dir)  <span class="co"># trainer is an object</span></span>
<span id="cb54-17"><a href="#cb54-17" aria-hidden="true" tabindex="-1"></a>    lr_schedule <span class="op">=</span> trax.lr.warmup_and_rsqrt_decay(n_warmup_steps<span class="op">=</span><span class="dv">1000</span>, max_value<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb54-18"><a href="#cb54-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-19"><a href="#cb54-19" aria-hidden="true" tabindex="-1"></a>    <span class="co">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) </span><span class="al">###</span></span>
<span id="cb54-20"><a href="#cb54-20" aria-hidden="true" tabindex="-1"></a>    train_task <span class="op">=</span> training.TrainTask( </span>
<span id="cb54-21"><a href="#cb54-21" aria-hidden="true" tabindex="-1"></a>      labeled_data<span class="op">=</span><span class="va">None</span>, <span class="co"># The training generator</span></span>
<span id="cb54-22"><a href="#cb54-22" aria-hidden="true" tabindex="-1"></a>      loss_layer<span class="op">=</span><span class="va">None</span>, <span class="co"># Loss function </span></span>
<span id="cb54-23"><a href="#cb54-23" aria-hidden="true" tabindex="-1"></a>      optimizer<span class="op">=</span><span class="va">None</span>, <span class="co"># Optimizer (Don't forget to set LR to 0.01)</span></span>
<span id="cb54-24"><a href="#cb54-24" aria-hidden="true" tabindex="-1"></a>      lr_schedule<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb54-25"><a href="#cb54-25" aria-hidden="true" tabindex="-1"></a>      n_steps_per_checkpoint<span class="op">=</span><span class="dv">10</span></span>
<span id="cb54-26"><a href="#cb54-26" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb54-27"><a href="#cb54-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-28"><a href="#cb54-28" aria-hidden="true" tabindex="-1"></a>    eval_task <span class="op">=</span> training.EvalTask( </span>
<span id="cb54-29"><a href="#cb54-29" aria-hidden="true" tabindex="-1"></a>      labeled_data<span class="op">=</span><span class="va">None</span>, <span class="co"># The evaluation generator</span></span>
<span id="cb54-30"><a href="#cb54-30" aria-hidden="true" tabindex="-1"></a>      metrics<span class="op">=</span>[<span class="va">None</span>, <span class="va">None</span>] <span class="co"># CrossEntropyLoss and Accuracy</span></span>
<span id="cb54-31"><a href="#cb54-31" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb54-32"><a href="#cb54-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-33"><a href="#cb54-33" aria-hidden="true" tabindex="-1"></a>    <span class="co">### </span><span class="re">END</span><span class="co"> CODE HERE </span><span class="al">###</span></span>
<span id="cb54-34"><a href="#cb54-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-35"><a href="#cb54-35" aria-hidden="true" tabindex="-1"></a>    loop <span class="op">=</span> training.Loop(TransformerLM(d_model<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb54-36"><a href="#cb54-36" aria-hidden="true" tabindex="-1"></a>                                       d_ff<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb54-37"><a href="#cb54-37" aria-hidden="true" tabindex="-1"></a>                                       n_layers<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb54-38"><a href="#cb54-38" aria-hidden="true" tabindex="-1"></a>                                       n_heads<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb54-39"><a href="#cb54-39" aria-hidden="true" tabindex="-1"></a>                                       mode<span class="op">=</span><span class="st">'train'</span>),</span>
<span id="cb54-40"><a href="#cb54-40" aria-hidden="true" tabindex="-1"></a>                         train_task,</span>
<span id="cb54-41"><a href="#cb54-41" aria-hidden="true" tabindex="-1"></a>                         eval_tasks<span class="op">=</span>[eval_task],</span>
<span id="cb54-42"><a href="#cb54-42" aria-hidden="true" tabindex="-1"></a>                         output_dir<span class="op">=</span>output_dir)</span>
<span id="cb54-43"><a href="#cb54-43" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb54-44"><a href="#cb54-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loop</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Notice that the model will be trained for only 10 steps.</p>
<p>Even with this constraint the model with the original default arguments took a very long time to finish. Because of this some parameters are changed when defining the model that is fed into the training loop in the function above.</p>
<div id="6c90577d" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Should take around 1.5 minutes</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>rm <span class="op">-</span>f <span class="op">~/</span>model<span class="op">/</span>model.pkl.gz</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>loop <span class="op">=</span> training_loop(TransformerLM, train_batch_stream, eval_batch_stream)</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>loop.run(<span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.
  pid, fd = os.forkpty()</code></pre>
</div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[33], line 3</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># Should take around 1.5 minutes</span>
<span class="ansi-green-fg ansi-bold">      2</span> get_ipython()<span style="color:rgb(98,98,98)">.</span>system(<span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">rm -f ~/model/model.pkl.gz</span><span style="color:rgb(175,0,0)">'</span>)
<span class="ansi-green-fg">----&gt; 3</span> loop <span style="color:rgb(98,98,98)">=</span> training_loop(TransformerLM, <span class="ansi-yellow-bg">train_batch_stream</span>, eval_batch_stream)
<span class="ansi-green-fg ansi-bold">      4</span> loop<span style="color:rgb(98,98,98)">.</span>run(<span style="color:rgb(98,98,98)">10</span>)

<span class="ansi-red-fg">NameError</span>: name 'train_batch_stream' is not defined</pre>
</div>
</div>
</div>
<p><a name="4"></a> # Part 4: Evaluation</p>
<p><a name="4.1"></a> ### 4.1 Loading in a trained model</p>
<p>In this part you will evaluate by loading in an almost exact version of the model you coded, but we trained it for you to save you time. Please run the cell below to load in the model.</p>
<p>As you may have already noticed the model that you trained and the pretrained model share the same overall architecture but they have different values for some of the parameters:</p>
<p><code>Original (pretrained) model:</code></p>
<pre><code>TransformerLM(vocab_size=33300, d_model=512, d_ff=2048, n_layers=6, n_heads=8, 
               dropout=0.1, max_len=4096, ff_activation=tl.Relu)
               </code></pre>
<p><code>Your model:</code></p>
<pre><code>TransformerLM(d_model=4, d_ff=16, n_layers=1, n_heads=2)</code></pre>
<p><strong>Only the parameters shown for your model were changed. The others stayed the same.</strong></p>
<div id="b7dc01a8" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the model architecture</span></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> TransformerLM(mode<span class="op">=</span><span class="st">'eval'</span>)</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the pre-trained weights</span></span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>model.init_from_file(<span class="st">'model.pkl.gz'</span>, weights_only<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">TypeError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[34], line 2</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># Get the model architecture</span>
<span class="ansi-green-fg">----&gt; 2</span> model <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">TransformerLM</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">mode</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">'</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">eval</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">'</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">      4</span> <span style="font-style:italic;color:rgb(95,135,135)"># Load the pre-trained weights</span>
<span class="ansi-green-fg ansi-bold">      5</span> model<span style="color:rgb(98,98,98)">.</span>init_from_file(<span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">model.pkl.gz</span><span style="color:rgb(175,0,0)">'</span>, weights_only<span style="color:rgb(98,98,98)">=</span><span style="font-weight:bold;color:rgb(0,135,0)">True</span>)

Cell <span class="ansi-green-fg">In[30], line 46</span>, in <span class="ansi-cyan-fg">TransformerLM</span><span class="ansi-blue-fg">(vocab_size, d_model, d_ff, n_layers, n_heads, dropout, max_len, mode, ff_activation)</span>
<span class="ansi-green-fg ansi-bold">     36</span> positional_encoder <span style="color:rgb(98,98,98)">=</span> [ 
<span class="ansi-green-fg ansi-bold">     37</span>     <span style="font-style:italic;color:rgb(95,135,135)"># Add embedding layer of dimension (vocab_size, d_model)</span>
<span class="ansi-green-fg ansi-bold">     38</span>     <span style="font-weight:bold;color:rgb(0,135,0)">None</span>,
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-fg ansi-bold">     41</span>     <span style="font-style:italic;color:rgb(95,135,135)"># Add positional encoding layer with maximum input length and mode specified</span>
<span class="ansi-green-fg ansi-bold">     42</span>     <span style="font-weight:bold;color:rgb(0,135,0)">None</span>]
<span class="ansi-green-fg ansi-bold">     44</span> <span style="font-style:italic;color:rgb(95,135,135)"># Create stack (list) of decoder blocks with n_layers with necessary parameters</span>
<span class="ansi-green-fg ansi-bold">     45</span> decoder_blocks <span style="color:rgb(98,98,98)">=</span> [ 
<span class="ansi-green-fg">---&gt; 46</span>     <span style="font-weight:bold;color:rgb(0,135,0)">None</span> <span style="font-weight:bold;color:rgb(0,135,0)">for</span> _ <span style="font-weight:bold;color:rgb(175,0,255)">in</span> <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">range</span><span class="ansi-yellow-bg">(</span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">None</span><span class="ansi-yellow-bg">)</span>]
<span class="ansi-green-fg ansi-bold">     48</span> <span style="font-style:italic;color:rgb(95,135,135)"># Create the complete model as written in the figure</span>
<span class="ansi-green-fg ansi-bold">     49</span> <span style="font-weight:bold;color:rgb(0,135,0)">return</span> tl<span style="color:rgb(98,98,98)">.</span>Serial(
<span class="ansi-green-fg ansi-bold">     50</span>     <span style="font-style:italic;color:rgb(95,135,135)"># Use teacher forcing (feed output of previous step to current step)</span>
<span class="ansi-green-fg ansi-bold">     51</span>     <span style="font-weight:bold;color:rgb(0,135,0)">None</span>, <span style="font-style:italic;color:rgb(95,135,135)"># Specify the mode!</span>
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-fg ansi-bold">     63</span>     <span style="font-weight:bold;color:rgb(0,135,0)">None</span>
<span class="ansi-green-fg ansi-bold">     64</span> )

<span class="ansi-red-fg">TypeError</span>: 'NoneType' object cannot be interpreted as an integer</pre>
</div>
</div>
</div>
<p><a name="5"></a> # Part 5: Testing with your own input</p>
<p>You will now test your input. You are going to implement greedy decoding. This consists of two functions. The first one allows you to identify the next symbol. It gets the argmax of the output of your model and then returns that index.</p>
<p><a name="ex06"></a> ### Exercise 06 <strong>Instructions:</strong> Implement the next symbol function that takes in the cur_output_tokens and the trained model to return the index of the next word.</p>
<div id="42545722" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="co"># UNQ_C9</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> next_symbol(cur_output_tokens, model):</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Returns the next symbol for a given sentence.</span></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a><span class="co">        cur_output_tokens (list): tokenized sentence with EOS and PAD tokens at the end.</span></span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a><span class="co">        model (trax.layers.combinators.Serial): The transformer model.</span></span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a><span class="co">        int: tokenized symbol.</span></span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb60-12"><a href="#cb60-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) </span><span class="al">###</span></span>
<span id="cb60-13"><a href="#cb60-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb60-14"><a href="#cb60-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># current output tokens length</span></span>
<span id="cb60-15"><a href="#cb60-15" aria-hidden="true" tabindex="-1"></a>    token_length <span class="op">=</span> <span class="va">None</span></span>
<span id="cb60-16"><a href="#cb60-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># calculate the minimum power of 2 big enough to store token_length</span></span>
<span id="cb60-17"><a href="#cb60-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># HINT: use np.ceil() and np.log2()</span></span>
<span id="cb60-18"><a href="#cb60-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># add 1 to token_length so np.log2() doesn't receive 0 when token_length is 0</span></span>
<span id="cb60-19"><a href="#cb60-19" aria-hidden="true" tabindex="-1"></a>    padded_length <span class="op">=</span> <span class="va">None</span><span class="op">**</span><span class="bu">int</span>(np.ceil(np.log2(<span class="va">None</span> <span class="op">+</span> <span class="va">None</span>)))</span>
<span id="cb60-20"><a href="#cb60-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-21"><a href="#cb60-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Fill cur_output_tokens with 0's until it reaches padded_length</span></span>
<span id="cb60-22"><a href="#cb60-22" aria-hidden="true" tabindex="-1"></a>    padded <span class="op">=</span> <span class="va">None</span> <span class="op">+</span> [<span class="dv">0</span>] <span class="op">*</span> (<span class="va">None</span> <span class="op">-</span> <span class="va">None</span>)</span>
<span id="cb60-23"><a href="#cb60-23" aria-hidden="true" tabindex="-1"></a>    padded_with_batch <span class="op">=</span> np.array(padded)[<span class="va">None</span>, :] <span class="co"># Don't replace this 'None'! This is a way of setting the batch dim</span></span>
<span id="cb60-24"><a href="#cb60-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-25"><a href="#cb60-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># model expects a tuple containing two padded tensors (with batch)</span></span>
<span id="cb60-26"><a href="#cb60-26" aria-hidden="true" tabindex="-1"></a>    output, _ <span class="op">=</span> model((<span class="va">None</span>, <span class="va">None</span>)) </span>
<span id="cb60-27"><a href="#cb60-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># HINT: output has shape (1, padded_length, vocab_size)</span></span>
<span id="cb60-28"><a href="#cb60-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># To get log_probs you need to index output with 0 in the first dim</span></span>
<span id="cb60-29"><a href="#cb60-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># token_length in the second dim and all of the entries for the last dim.</span></span>
<span id="cb60-30"><a href="#cb60-30" aria-hidden="true" tabindex="-1"></a>    log_probs <span class="op">=</span> output[<span class="va">None</span>, <span class="va">None</span>, <span class="va">None</span>]</span>
<span id="cb60-31"><a href="#cb60-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb60-32"><a href="#cb60-32" aria-hidden="true" tabindex="-1"></a>    <span class="co">### </span><span class="re">END</span><span class="co"> CODE HERE </span><span class="al">###</span></span>
<span id="cb60-33"><a href="#cb60-33" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb60-34"><a href="#cb60-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">int</span>(np.argmax(log_probs))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="b04c85f8" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Test it out!</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>sentence_test_nxt_symbl <span class="op">=</span> <span class="st">"I want to fly in the sky."</span></span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>detokenize([next_symbol(tokenize(sentence_test_nxt_symbl)<span class="op">+</span>[<span class="dv">0</span>], model)])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[36], line 3</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># Test it out!</span>
<span class="ansi-green-fg ansi-bold">      2</span> sentence_test_nxt_symbl <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">I want to fly in the sky.</span><span style="color:rgb(175,0,0)">"</span>
<span class="ansi-green-fg">----&gt; 3</span> detokenize([next_symbol(tokenize(sentence_test_nxt_symbl)<span style="color:rgb(98,98,98)">+</span>[<span style="color:rgb(98,98,98)">0</span>], <span class="ansi-yellow-bg">model</span>)])

<span class="ansi-red-fg">NameError</span>: name 'model' is not defined</pre>
</div>
</div>
</div>
<p><strong>Expected Output:</strong></p>
<div class="sourceCode" id="cb62"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="ch">'T</span><span class="er">he</span><span class="ch">'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><a name="5.1"></a> ### 5.1 Greedy decoding</p>
<p>Now you will implement the greedy_decode algorithm that will call the <code>next_symbol</code> function. It takes in the input_sentence, the trained model and returns the decoded sentence.</p>
<p><a name="ex07"></a> ### Exercise 07</p>
<p><strong>Instructions</strong>: Implement the greedy_decode algorithm.</p>
<div id="a344d6b6" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="co"># UNQ_C10</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Decoding functions.</span></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> greedy_decode(input_sentence, model):</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Greedy decode function.</span></span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a><span class="co">        input_sentence (string): a sentence or article.</span></span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a><span class="co">        model (trax.layers.combinators.Serial): Transformer model.</span></span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a><span class="co">        string: summary of the input.</span></span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb63-13"><a href="#cb63-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb63-14"><a href="#cb63-14" aria-hidden="true" tabindex="-1"></a>    <span class="co">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) </span><span class="al">###</span></span>
<span id="cb63-15"><a href="#cb63-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use tokenize()</span></span>
<span id="cb63-16"><a href="#cb63-16" aria-hidden="true" tabindex="-1"></a>    cur_output_tokens <span class="op">=</span> <span class="va">None</span> <span class="op">+</span> [<span class="dv">0</span>]</span>
<span id="cb63-17"><a href="#cb63-17" aria-hidden="true" tabindex="-1"></a>    generated_output <span class="op">=</span> [] </span>
<span id="cb63-18"><a href="#cb63-18" aria-hidden="true" tabindex="-1"></a>    cur_output <span class="op">=</span> <span class="dv">0</span> </span>
<span id="cb63-19"><a href="#cb63-19" aria-hidden="true" tabindex="-1"></a>    EOS <span class="op">=</span> <span class="dv">1</span> </span>
<span id="cb63-20"><a href="#cb63-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb63-21"><a href="#cb63-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> cur_output <span class="op">!=</span> EOS:</span>
<span id="cb63-22"><a href="#cb63-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get next symbol</span></span>
<span id="cb63-23"><a href="#cb63-23" aria-hidden="true" tabindex="-1"></a>        cur_output <span class="op">=</span> next_symbol(<span class="va">None</span>, <span class="va">None</span>)</span>
<span id="cb63-24"><a href="#cb63-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Append next symbol to original sentence</span></span>
<span id="cb63-25"><a href="#cb63-25" aria-hidden="true" tabindex="-1"></a>        cur_output_tokens.append(<span class="va">None</span>)</span>
<span id="cb63-26"><a href="#cb63-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Append next symbol to generated sentence</span></span>
<span id="cb63-27"><a href="#cb63-27" aria-hidden="true" tabindex="-1"></a>        generated_output.append(<span class="va">None</span>)</span>
<span id="cb63-28"><a href="#cb63-28" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(detokenize(generated_output))</span>
<span id="cb63-29"><a href="#cb63-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb63-30"><a href="#cb63-30" aria-hidden="true" tabindex="-1"></a>    <span class="co">### </span><span class="re">END</span><span class="co"> CODE HERE </span><span class="al">###</span></span>
<span id="cb63-31"><a href="#cb63-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb63-32"><a href="#cb63-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> detokenize(generated_output)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="76bdc944" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Test it out on a sentence!</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>test_sentence <span class="op">=</span> <span class="st">"It was a sunny day when I went to the market to buy some flowers. But I only found roses, not tulips."</span></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(wrapper.fill(test_sentence), <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(greedy_decode(test_sentence, model))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>It was a sunny day when I went to the market to buy some flowers. But
I only found roses, not tulips. 
</code></pre>
</div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[38], line 4</span>
<span class="ansi-green-fg ansi-bold">      2</span> test_sentence <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">It was a sunny day when I went to the market to buy some flowers. But I only found roses, not tulips.</span><span style="color:rgb(175,0,0)">"</span>
<span class="ansi-green-fg ansi-bold">      3</span> <span style="color:rgb(0,135,0)">print</span>(wrapper<span style="color:rgb(98,98,98)">.</span>fill(test_sentence), <span style="color:rgb(175,0,0)">'</span><span style="font-weight:bold;color:rgb(175,95,0)">\n</span><span style="color:rgb(175,0,0)">'</span>)
<span class="ansi-green-fg">----&gt; 4</span> <span style="color:rgb(0,135,0)">print</span>(greedy_decode(test_sentence, <span class="ansi-yellow-bg">model</span>))

<span class="ansi-red-fg">NameError</span>: name 'model' is not defined</pre>
</div>
</div>
</div>
<p><strong>Expected Output:</strong></p>
<div class="sourceCode" id="cb66"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="op">:</span></span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a><span class="op">:</span> I</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a><span class="op">:</span> I just</span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a><span class="op">:</span> I just found</span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a><span class="op">:</span> I just found ros</span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a><span class="op">:</span> I just found roses</span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a><span class="op">:</span> I just found roses<span class="op">,</span></span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true" tabindex="-1"></a><span class="op">:</span> I just found roses<span class="op">,</span> <span class="kw">not</span></span>
<span id="cb66-9"><a href="#cb66-9" aria-hidden="true" tabindex="-1"></a><span class="op">:</span> I just found roses<span class="op">,</span> <span class="kw">not</span> tu</span>
<span id="cb66-10"><a href="#cb66-10" aria-hidden="true" tabindex="-1"></a><span class="op">:</span> I just found roses<span class="op">,</span> <span class="kw">not</span> tulips</span>
<span id="cb66-11"><a href="#cb66-11" aria-hidden="true" tabindex="-1"></a><span class="op">:</span> I just found roses<span class="op">,</span> <span class="kw">not</span> tulips</span>
<span id="cb66-12"><a href="#cb66-12" aria-hidden="true" tabindex="-1"></a><span class="op">:</span> I just found roses<span class="op">,</span> <span class="kw">not</span> tulips<span class="op">.</span></span>
<span id="cb66-13"><a href="#cb66-13" aria-hidden="true" tabindex="-1"></a><span class="op">:</span> I just found roses<span class="op">,</span> <span class="kw">not</span> tulips<span class="op">.&lt;</span>EOS<span class="op">&gt;</span></span>
<span id="cb66-14"><a href="#cb66-14" aria-hidden="true" tabindex="-1"></a><span class="op">:</span> I just found roses<span class="op">,</span> <span class="kw">not</span> tulips<span class="op">.&lt;</span>EOS<span class="op">&gt;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="b873793f" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Test it out with a whole article!</span></span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>article <span class="op">=</span> <span class="st">"It’s the posing craze sweeping the U.S. after being brought to fame by skier Lindsey Vonn, soccer star Omar Cummings, baseball player Albert Pujols - and even Republican politician Rick Perry. But now four students at Riverhead High School on Long Island, New York, have been suspended for dropping to a knee and taking up a prayer pose to mimic Denver Broncos quarterback Tim Tebow. Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were all suspended for one day because the ‘Tebowing’ craze was blocking the hallway and presenting a safety hazard to students. Scroll down for video. Banned: Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll (all pictured left) were all suspended for one day by Riverhead High School on Long Island, New York, for their tribute to Broncos quarterback Tim Tebow. Issue: Four of the pupils were suspended for one day because they allegedly did not heed to warnings that the 'Tebowing' craze at the school was blocking the hallway and presenting a safety hazard to students."</span></span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(wrapper.fill(article), <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(greedy_decode(article, model))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>It’s the posing craze sweeping the U.S. after being brought to fame by
skier Lindsey Vonn, soccer star Omar Cummings, baseball player Albert
Pujols - and even Republican politician Rick Perry. But now four
students at Riverhead High School on Long Island, New York, have been
suspended for dropping to a knee and taking up a prayer pose to mimic
Denver Broncos quarterback Tim Tebow. Jordan Fulcoly, Wayne Drexel,
Tyler Carroll and Connor Carroll were all suspended for one day
because the ‘Tebowing’ craze was blocking the hallway and presenting a
safety hazard to students. Scroll down for video. Banned: Jordan
Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll (all pictured
left) were all suspended for one day by Riverhead High School on Long
Island, New York, for their tribute to Broncos quarterback Tim Tebow.
Issue: Four of the pupils were suspended for one day because they
allegedly did not heed to warnings that the 'Tebowing' craze at the
school was blocking the hallway and presenting a safety hazard to
students. 
</code></pre>
</div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[39], line 4</span>
<span class="ansi-green-fg ansi-bold">      2</span> article <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">It’s the posing craze sweeping the U.S. after being brought to fame by skier Lindsey Vonn, soccer star Omar Cummings, baseball player Albert Pujols - and even Republican politician Rick Perry. But now four students at Riverhead High School on Long Island, New York, have been suspended for dropping to a knee and taking up a prayer pose to mimic Denver Broncos quarterback Tim Tebow. Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were all suspended for one day because the ‘Tebowing’ craze was blocking the hallway and presenting a safety hazard to students. Scroll down for video. Banned: Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll (all pictured left) were all suspended for one day by Riverhead High School on Long Island, New York, for their tribute to Broncos quarterback Tim Tebow. Issue: Four of the pupils were suspended for one day because they allegedly did not heed to warnings that the </span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">Tebowing</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)"> craze at the school was blocking the hallway and presenting a safety hazard to students.</span><span style="color:rgb(175,0,0)">"</span>
<span class="ansi-green-fg ansi-bold">      3</span> <span style="color:rgb(0,135,0)">print</span>(wrapper<span style="color:rgb(98,98,98)">.</span>fill(article), <span style="color:rgb(175,0,0)">'</span><span style="font-weight:bold;color:rgb(175,95,0)">\n</span><span style="color:rgb(175,0,0)">'</span>)
<span class="ansi-green-fg">----&gt; 4</span> <span style="color:rgb(0,135,0)">print</span>(greedy_decode(article, <span class="ansi-yellow-bg">model</span>))

<span class="ansi-red-fg">NameError</span>: name 'model' is not defined</pre>
</div>
</div>
</div>
<p><strong>Expected Output:</strong></p>
<div class="sourceCode" id="cb69"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>Jordan</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>Jordan Ful</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>Jordan Fulcol</span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>Jordan Fulcoly</span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a>Jordan Fulcoly<span class="op">,</span></span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a>Jordan Fulcoly<span class="op">,</span> Wayne</span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a>Jordan Fulcoly<span class="op">,</span> Wayne Dre</span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a>Jordan Fulcoly<span class="op">,</span> Wayne Drexe</span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a>Jordan Fulcoly<span class="op">,</span> Wayne Drexel</span>
<span id="cb69-10"><a href="#cb69-10" aria-hidden="true" tabindex="-1"></a>Jordan Fulcoly<span class="op">,</span> Wayne Drexel<span class="op">,</span></span>
<span id="cb69-11"><a href="#cb69-11" aria-hidden="true" tabindex="-1"></a><span class="op">.</span></span>
<span id="cb69-12"><a href="#cb69-12" aria-hidden="true" tabindex="-1"></a><span class="op">.</span></span>
<span id="cb69-13"><a href="#cb69-13" aria-hidden="true" tabindex="-1"></a><span class="op">.</span></span>
<span id="cb69-14"><a href="#cb69-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-15"><a href="#cb69-15" aria-hidden="true" tabindex="-1"></a>Final summary<span class="op">:</span></span>
<span id="cb69-16"><a href="#cb69-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-17"><a href="#cb69-17" aria-hidden="true" tabindex="-1"></a>Jordan Fulcoly<span class="op">,</span> Wayne Drexel<span class="op">,</span> Tyler Carroll <span class="kw">and</span> Connor Carroll were</span>
<span id="cb69-18"><a href="#cb69-18" aria-hidden="true" tabindex="-1"></a>suspended <span class="cf">for</span> one day<span class="op">.</span> Four students were suspended <span class="cf">for</span> one day</span>
<span id="cb69-19"><a href="#cb69-19" aria-hidden="true" tabindex="-1"></a>because they allegedly did <span class="kw">not</span> heed to warnings that the <span class="ch">'T</span><span class="er">ebowing</span><span class="ch">'</span></span>
<span id="cb69-20"><a href="#cb69-20" aria-hidden="true" tabindex="-1"></a>craze was blocking the hallway <span class="kw">and</span> presenting a safety hazard to</span>
<span id="cb69-21"><a href="#cb69-21" aria-hidden="true" tabindex="-1"></a>students<span class="op">.&lt;</span>EOS<span class="op">&gt;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Congratulations on finishing this week’s assignment!</strong> You did a lot of work and now you should have a better understanding of the encoder part of Transformers and how Transformers can be used for text summarization.</p>
<p><strong>Keep it up!</strong></p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Transformer {Summarizer}},
  date = {2025-02-06},
  url = {https://orenbochman.github.io/notes-nlp/posts/c4w2/assignment.html},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas" role="listitem">
Bochman, Oren. 2025. <span>“Transformer Summarizer.”</span> February 6,
2025. <a href="https://orenbochman.github.io/notes-nlp/posts/c4w2/assignment.html">https://orenbochman.github.io/notes-nlp/posts/c4w2/assignment.html</a>.
</div></div></section></div></main> <!-- /main -->
<script type="application/vnd.jupyter.widget-state+json">
{"state":{"201e7a3b725f4eb6a4ba520681e1611e":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"372e5cf2c9c44a78b845a69ae8da132b":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_allow_html":false,"layout":"IPY_MODEL_fcc83cd5c2ab4f94a85b31d41bfab840","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6d1fe4751f4c43158f05c23a8838818d","tabbable":null,"tooltip":null,"value":1}},"378ebb547ad14e5ea482685ec0120b09":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_e25ea7955f654af29967368272924bfe","placeholder":"​","style":"IPY_MODEL_ee392bd28bf94239afb6ffedddadc816","tabbable":null,"tooltip":null,"value":" 4/5 [00:01&lt;00:00,  2.83 url/s]"}},"4e1eab6e3f834bcca8e3a4ae34537416":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_201e7a3b725f4eb6a4ba520681e1611e","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6cef33a9c39941809fe34ba139c9f5ee","tabbable":null,"tooltip":null,"value":0}},"4f9ea3bfb8ce4942a0f36d9d8e47774f":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"51be7d94413c423a9431aea5e9fe2c94":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_7c781a08009e48138a87046242e55c9b","placeholder":"​","style":"IPY_MODEL_a0843f86263b4345917f481838044828","tabbable":null,"tooltip":null,"value":"Extraction completed...: "}},"5de866cdb0a948509947a21e6ea907fe":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6345b9f898ef418bba937b45c85d1245":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"6cef33a9c39941809fe34ba139c9f5ee":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6d1fe4751f4c43158f05c23a8838818d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6d53a136df794445b3a384e76dd9167d":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"7c781a08009e48138a87046242e55c9b":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f1ff6dbc48f4c46a19b6c597f935cf8":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c92eabc12934478b8c1787e09d9dfa1e","IPY_MODEL_372e5cf2c9c44a78b845a69ae8da132b","IPY_MODEL_378ebb547ad14e5ea482685ec0120b09"],"layout":"IPY_MODEL_815e878e9ae14da09ccb814fa918c939","tabbable":null,"tooltip":null}},"815e878e9ae14da09ccb814fa918c939":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"853d16d73e0d458d8f07f14d7b431ed6":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_bc7e2ebc34cd4546a510e9fbd8bef61c","placeholder":"​","style":"IPY_MODEL_f78250e6eee845769652552990f1b1e9","tabbable":null,"tooltip":null,"value":"Dl Size...: 100%"}},"888db66a31804953af7bfb937e5343b0":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"a0843f86263b4345917f481838044828":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"a7eeb3b0e677440a9fd26d3d425816fe":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a879bbe9f17a4432b23aa66552082189":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_51be7d94413c423a9431aea5e9fe2c94","IPY_MODEL_4e1eab6e3f834bcca8e3a4ae34537416","IPY_MODEL_ddcdf10f1c1f466c93fd1020b98bc748"],"layout":"IPY_MODEL_5de866cdb0a948509947a21e6ea907fe","tabbable":null,"tooltip":null}},"abe67a3a07124582bff31c961d74e0c8":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_ac20e83266cc45d4b586dbe2e636133b","placeholder":"​","style":"IPY_MODEL_d399628028f04c81b2ed794bedfabc9f","tabbable":null,"tooltip":null,"value":" 4543222/4543232 [00:01&lt;00:00, 5051461.69 MiB/s]"}},"ac20e83266cc45d4b586dbe2e636133b":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc7e2ebc34cd4546a510e9fbd8bef61c":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c92eabc12934478b8c1787e09d9dfa1e":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_ec784187a2a04ac6a1d8986857c8ffdf","placeholder":"​","style":"IPY_MODEL_6345b9f898ef418bba937b45c85d1245","tabbable":null,"tooltip":null,"value":"Dl Completed...:  80%"}},"d1d6acacb275498e8e4eb8d749499823":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_853d16d73e0d458d8f07f14d7b431ed6","IPY_MODEL_f9e2e71d37ff430399b3084d1b2ddf2e","IPY_MODEL_abe67a3a07124582bff31c961d74e0c8"],"layout":"IPY_MODEL_a7eeb3b0e677440a9fd26d3d425816fe","tabbable":null,"tooltip":null}},"d2c9e72c707b467191ec6ffbe00103b0":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d399628028f04c81b2ed794bedfabc9f":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"ddcdf10f1c1f466c93fd1020b98bc748":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_d2c9e72c707b467191ec6ffbe00103b0","placeholder":"​","style":"IPY_MODEL_888db66a31804953af7bfb937e5343b0","tabbable":null,"tooltip":null,"value":" 0/0 [00:01&lt;?, ? file/s]"}},"e25ea7955f654af29967368272924bfe":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec784187a2a04ac6a1d8986857c8ffdf":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee392bd28bf94239afb6ffedddadc816":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"f78250e6eee845769652552990f1b1e9":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"f9e2e71d37ff430399b3084d1b2ddf2e":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_allow_html":false,"layout":"IPY_MODEL_6d53a136df794445b3a384e76dd9167d","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4f9ea3bfb8ce4942a0f36d9d8e47774f","tabbable":null,"tooltip":null,"value":1}},"fcc83cd5c2ab4f94a85b31d41bfab840":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}}},"version_major":2,"version_minor":0}
</script>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("^(?:http:|https:)\/\/www\.quarto\.org\/custom");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
          // default icon
          link.classList.add("external");
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© Copyright 2023-2025, Oren Bochman</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/OrenBochman/notes-nlp/edit/main/posts/c4w2/assignment.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/OrenBochman/notes-nlp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This page is built with 💛 and <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>