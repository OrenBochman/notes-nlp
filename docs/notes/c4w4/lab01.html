<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Oren Bochman">

<title>Reformer Efficient Attention: Ungraded Lab – NLP Course Notes &amp; Research</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-d166b450ba5a8e9f7a0ab969bf6592c1.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-1372234da3246ce8e868649689ba5ed0.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-d99a2a2a191b5c7f2a9a83135e7f0803.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-V0T6EFS8LY"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-V0T6EFS8LY', { 'anonymize_ip': true});
</script>
<style>html{ scroll-behavior: smooth; }</style>
<style>

      .quarto-title-block .quarto-title-banner {
        background: images/banner_deep.jpg;
      }
</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Reformer Efficient Attention: Ungraded Lab – NLP Course Notes &amp; Research">
<meta property="og:description" content="Course and Research notes">
<meta property="og:image" content="https://orenbochman.github.io/notes-nlp/images/nlp-brain-wordcloud.jpg">
<meta property="og:site_name" content="NLP Course Notes &amp; Research">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">NLP Course Notes &amp; Research</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">NLP with Attention Models</li><li class="breadcrumb-item"><a href="../../notes/c4w4/index.html">Chat Bots</a></li><li class="breadcrumb-item"><a href="../../notes/c4w4/lab01.html">L1 - Reformer LSH</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">NLP with Attention Models</li><li class="breadcrumb-item"><a href="../../notes/c4w4/index.html">Chat Bots</a></li><li class="breadcrumb-item"><a href="../../notes/c4w4/lab01.html">L1 - Reformer LSH</a></li></ol></nav>
      <h1 class="title">Reformer Efficient Attention: Ungraded Lab</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta column-body">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Oren Bochman </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">Wednesday, April 28, 2021</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Classification &amp; Vector Spaces</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Logistic Regression</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c1w1/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c1w1/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Preprocessing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c1w1/lab02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - Frequencies</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c1w1/lab03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L3 - Visualizing tweets</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Probability &amp; Bayes Rule</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c1w2/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c1w2/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Visualizing Naive Bayes</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false">
 <span class="menu-text">Vector Space Models &amp; PCA</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c1w3/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c1w3/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Linear algebra with NumPy</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c1w3/lab02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - Manipulating word embeddings</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">MT &amp; Document Search via KNN</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c1w4/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c1w3/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Vector manipulation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c1w3/lab02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - Hash functions and multiplanes</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Probabilistic Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false">
 <span class="menu-text">Autocorrect &amp; Dynamic Programming</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c2w1/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c2w1/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Building the vocabulary</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c2w1/lab02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - Candidates from String Edits</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false">
 <span class="menu-text">POS tagging &amp; HMMS</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c2w2/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c2w2/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Vocabulary with unknowns</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c2w2/lab02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - Working with tags and Numpy</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false">
 <span class="menu-text">Autocomplete &amp; Language Models</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c2w3/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c2w3/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - N-grams Corpus preprocessing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c2w3/lab02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - Building the language model</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c2w3/lab03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L3 - Out of vocabulary words</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false">
 <span class="menu-text">Word embeddings with neural networks</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c2w4/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c2w4/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Data preparation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c2w4/lab02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - Intro to CBOW</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c2w4/lab03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L3 - Training the CBOW</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true">
 <span class="menu-text">Sequence Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="false">
 <span class="menu-text">Neural Networks for Sentiment Analysis</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c3w1/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c3w1/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Introduction to Trax</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c3w1/lab02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - Classes and Subclasses</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c3w1/lab03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L3 - Data Generators</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="false">
 <span class="menu-text">RNN for Language Modeling</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c3w2/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c3w2/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Hidden State Activation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c3w2/lab02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - Calculating Perplexity</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c3w2/lab03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L3 - Vanilla RNNs, GRUs and the scan function</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c3w2/lab04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L4 - Creating a GRU model using Trax</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="false">
 <span class="menu-text">LSTMs and Named Entity Recognition</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-14" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c3w3/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c3w3/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Vanishing Gradients</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="false">
 <span class="menu-text">Siamese Networks</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-15" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c3w4/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c3w4/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Creating a Siamese Model using Trax</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c3w4/lab02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - Modified Triplet Loss</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c3w4/lab03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L3 - Evaluate a Siamese Model</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-16" role="navigation" aria-expanded="true">
 <span class="menu-text">NLP with Attention Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-16" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-16" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-17" role="navigation" aria-expanded="false">
 <span class="menu-text">Neural Machine Translation</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-17" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-17" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c4w1/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c4w1/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Stack Semantics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c4w1/lab02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - BLEU Score</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-18" role="navigation" aria-expanded="false">
 <span class="menu-text">Text Summarization</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-18" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-18" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c4w2/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c4w2/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - Attention</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c4w2/lab02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - The Transformer Decoder</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-19" role="navigation" aria-expanded="false">
 <span class="menu-text">Question Answering</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-19" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-19" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c4w3/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c4w3/lab01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L1 - SentencePiece and BPE</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c4w3/lab02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - BERT Loss</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c4w3/lab03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L3 - T5</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-20" role="navigation" aria-expanded="true">
 <span class="menu-text">Chat Bots</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-20" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-20" class="collapse list-unstyled sidebar-section depth2 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c4w4/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c4w4/lab01.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">L1 - Reformer LSH</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/c4w4/lab02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">L2 - Revnet</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-21" role="navigation" aria-expanded="true">
 <span class="menu-text">Papers</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-21" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-21" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../reviews/paper/2014-NMT-by-jointly-learning-to-align-and-translate/" class="sidebar-item-text sidebar-link">
 <span class="menu-text">NMT by Jointly Learning to Align and Translate</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../reviews/paper/2015-LSH/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Practical &amp; Optimal LSH for Angular Distance</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../reviews/paper/2015-effective-approaches-to-attention-based-NMT/" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Effective Approaches to Attention-based NMT</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../reviews/paper/2016-coverage-embedding-models-for-NMT/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Coverage Embedding Models for NMT</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../reviews/paper/2016-neural-morphological-segmentation/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Neural Morphological Analysis Encoding-Decoding Canonical Segments</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../reviews/paper/2017-attention-is-all-you-need/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Attention is all you need</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../reviews/paper/2017-data-augmentation-low-resource-NMT/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data augmentation for low-resource NMT</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../reviews/paper/2018-ELMo/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ELMO</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../reviews/paper/2018-BERT/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">BERT</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../reviews/paper/2018-PTWM-NMT/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">PTWM NMT</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../reviews/paper/2019-morphological-embeddings/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Morphological embeddings</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../reviews/paper/2022-nakdimon/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">nakdimon</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../reviews/paper/2023-exposing-glitches/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">exposing glitches</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../reviews/paper/2023-MBR-all-the-way-down/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">MBR all the way down</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../reviews/paper/2024-MBR-decoding/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">MBR decoding</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../reviews/paper/2025-xLSTM/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">xLSTM</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#outline" id="toc-outline" class="nav-link active" data-scroll-target="#outline">Outline</a></li>
  <li><a href="#1" id="toc-1" class="nav-link" data-scroll-target="#1">Part 1.0 Trax Efficient Attention classes</a></li>
  <li><a href="#1.2" id="toc-1.2" class="nav-link" data-scroll-target="#1.2">Part 1.2 Trax Details</a></li>
  <li><a href="#2" id="toc-2" class="nav-link" data-scroll-target="#2">Part 2 Full Dot-Product Self Attention</a>
  <ul class="collapse">
  <li><a href="#2.1" id="toc-2.1" class="nav-link" data-scroll-target="#2.1">Part 2.1 Description</a></li>
  <li><a href="#2.1.1" id="toc-2.1.1" class="nav-link" data-scroll-target="#2.1.1">Part 2.1.1 our_softmax</a></li>
  <li><a href="#2.2" id="toc-2.2" class="nav-link" data-scroll-target="#2.2">Part 2.2 our_simple_attend</a></li>
  </ul></li>
  <li><a href="#2.3" id="toc-2.3" class="nav-link" data-scroll-target="#2.3">Part 2.3 Class OurSelfAttention</a></li>
  <li><a href="#since-this-notebook-is-ungraded-the-completed-code-is-provided-here-for-reference" id="toc-since-this-notebook-is-ungraded-the-completed-code-is-provided-here-for-reference" class="nav-link" data-scroll-target="#since-this-notebook-is-ungraded-the-completed-code-is-provided-here-for-reference">since this notebook is ungraded the completed code is provided here for reference</a>
  <ul class="collapse">
  <li><a href="#3.5" id="toc-3.5" class="nav-link" data-scroll-target="#3.5">Part 3.5 OurLSHSelfAttention</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/OrenBochman/notes-nlp/edit/main/notes/c4w4/lab01.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/OrenBochman/notes-nlp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block column-body" id="quarto-document-content">





<p>The videos describe two ‘reforms’ made to the Transformer to make it more memory and compute efficient. The <em>Reversible Layers</em> reduce memory and <em>Locality Sensitive Hashing(LSH)</em> reduces the cost of the Dot Product attention for large input sizes. This ungraded lab will look more closely at LSH and how it is used in the Reformer model.</p>
<p>Specifically, the notebook has 3 goals</p>
<ul>
<li>review dot-product self attention for reference</li>
<li>examine LSH based self attention</li>
<li>extend our understanding and familiarity with Trax infrastructure</li>
</ul>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li><a href="#1">Part 1: Trax Efficient Attention classes</a></li>
<li><a href="#2">Part 2: Full Dot Product Self Attention</a>
<ul>
<li><a href="#2.1">2.1 Description</a>
<ul>
<li><a href="#2.1.1">2.1.1 our_softmax</a></li>
</ul></li>
<li><a href="#2.2">2.2 our simple attend</a></li>
<li><a href="#2.3">2.3 Class OurSelfAttention</a></li>
</ul></li>
<li><a href="#3">Part 3: Trax LSHSelfAttention</a>
<ul>
<li><a href="#3.1">3.1 Description</a></li>
<li><a href="#3.2">3.2 our_hash_vectors</a></li>
<li><a href="#3.3">3.3 Sorting Buckets</a></li>
<li><a href="#3.4">3.4 Chunked dot product attention</a></li>
<li><a href="#3.5">3.5 OurLSHSelfAttention</a></li>
</ul></li>
</ul>
</section>
<section id="1" class="level2">
<h2 class="anchored" data-anchor-id="1">Part 1.0 Trax Efficient Attention classes</h2>
<p>Trax is similar to other popular NN development platforms such as Keras (now integrated into Tensorflow) and Pytorch in that it uses ‘layers’ as a useful level of abstraction. Layers are often represented as <em>classes</em>. We’re going to improve our understanding of Trax by locally extending the classes used in the attention layers. We will extend only the ‘forward’ functions and utilize the existing attention layers as parent classes. The original code can be found at <a href="https://github.com/google/trax/blob/v1.3.4/trax/layers/research/efficient_attention.py">github:trax/layers/Research/Efficient_attention</a>. This link references release 1.3.4 but note that this is under the ‘research’ directory as this is an area of active research. When accessing the code on Github for review on this assignment, be sure you select the 1.3.4 release tag, the master copy may have new changes.:</p>
<div id="fig-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/C4W4_LN2_image11.PNG" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: Reference Tag 1.3.4 on github"><img src="img/C4W4_LN2_image11.PNG" width="250" height="250" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Reference Tag 1.3.4 on github
</figcaption>
</figure>
</div>
<p>While Trax uses classes liberally, we have not built many classes in the course so far. Let’s spend a few moments reviewing the classes we will be using.</p>
<div id="fig-02" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/C4W4_LN2_image1.PNG" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;2: Classes from Trax/layers/Research/Efficient_Attention.py that we will be utilizing"><img src="img/C4W4_LN2_image1.PNG" width="1561" height="788" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Classes from Trax/layers/Research/Efficient_Attention.py that we will be utilizing
</figcaption>
</figure>
</div>
<p>Starting on the right in the diagram below you see EfficientAttentionBase. The parent to this class is the base.layer which has the routines used by all layers. EfficientAttentionBase leaves many routines to be overridden by child classes - but it has an important feature in the <em>Forward</em> routine. It supports a <code>use_reference_code</code> capability that selects implementations that limit some of the complexities to provide a more easily understood version of the algorithms. In particular, it implements a nested loop that treats each <em>‘example, head’</em> independently. This simplifies our work as we need only worry about matrix operations on one <em>‘example, head’</em> at a time. This loop calls <em>forward_unbatched</em>, which is the child process that we will be overriding.</p>
<p>On the top left are the outlines of the two child classes we will be using. The SelfAttention layer is a ‘traditional’ implementation of the dot product attention. We will be implementing the <em>forward_unbatched</em> version of this to highlight the differences between this and the LSH implementation.</p>
<p>Below that is the LSHSelfAttention. This is the routine used in the Reformer architecture. We will override the <em>forward_unbatched</em> section of this and some of the utility functions it uses to explore its implementation in more detail.</p>
<p>The code we will be working with is from the Trax source, and as such has implementation details that will make it a bit harder to follow. However, it will allow use of the results along with the rest of the Trax infrastructure. I will try to briefly describe these as they arise. The <a href="https://trax-ml.readthedocs.io/en/latest/">Trax documentation</a> can also be referenced.</p>
</section>
<section id="1.2" class="level2">
<h2 class="anchored" data-anchor-id="1.2">Part 1.2 Trax Details</h2>
<p>The goal in this notebook is to override a few routines in the Trax classes with our own versions. To maintain their functionality in a full Trax environment, many of the details we might ignore in example version of routines will be maintained in this code. Here are some of the considerations that may impact our code:</p>
<ul>
<li>Trax operates with multiple back-end libraries, we will see special cases that will utilize unique features.</li>
<li>‘Fancy’ numpy indexing is not supported in all backend environments and must be emulated in other ways.</li>
<li>Some operations don’t have gradients for backprop and must be ignored or include forced re-evaluation.</li>
</ul>
<p>Here are some of the functions we may see:</p>
<ul>
<li>Abstracted as <code>fastmath</code>, Trax supports multiple backend’s such as <a href="https://github.com/google/jax">Jax</a> and <a href="https://github.com/tensorflow/tensorflow">Tensorflow2</a></li>
<li><a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.tie_in.html">tie_in</a>: Some non-numeric operations must be invoked during backpropagation. Normally, the gradient compute graph would determine invocation but these functions are not included. To force re-evaluation, they are ‘tied’ to other numeric operations using tie_in.</li>
<li><a href="https://trax-ml.readthedocs.io/en/latest/trax.fastmath.html">stop_gradient</a>: Some operations are intentionally excluded from backprop gradient calculations by setting their gradients to zero.</li>
<li>Below we will execute <code>from trax.fastmath import numpy as np</code>, this uses accelerated forms of numpy functions. This is, however a <em>subset</em> of numpy</li>
</ul>
<div id="06f08848" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> trax</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> trax <span class="im">import</span> layers <span class="im">as</span> tl  <span class="co"># core building block</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> trax <span class="im">import</span> fastmath  <span class="co"># uses jax, offers numpy on steroids</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># fastmath.use_backend('tensorflow-numpy')</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> functools</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> trax.fastmath <span class="im">import</span> numpy <span class="im">as</span> np  <span class="co"># note, using fastmath subset of numpy!</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> trax.layers <span class="im">import</span> (</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    tie_in,</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    length_normalized,</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    apply_broadcasted_dropout,</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    look_adjacent,</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    permute_via_gather,</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    permute_via_sort,</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># def tie_in(x, y):</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co">#   if fastmath.backend_name() == 'jax':</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co">#     return jax.lax.tie_in(x, y)</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co">#   return y</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>2025-02-23 13:21:25.257698: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1740309685.270437 1538144 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1740309685.274438 1538144 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered</code></pre>
</div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">ImportError</span>                               Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[1], line 10</span>
<span class="ansi-green-fg ansi-bold">      8</span> <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">functools</span>
<span class="ansi-green-fg ansi-bold">      9</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">trax</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">fastmath</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> numpy <span style="font-weight:bold;color:rgb(0,135,0)">as</span> np  <span style="font-style:italic;color:rgb(95,135,135)"># note, using fastmath subset of numpy!</span>
<span class="ansi-green-fg">---&gt; 10</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">trax</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">layers</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> (
<span class="ansi-green-fg ansi-bold">     11</span>     tie_in,
<span class="ansi-green-fg ansi-bold">     12</span>     length_normalized,
<span class="ansi-green-fg ansi-bold">     13</span>     apply_broadcasted_dropout,
<span class="ansi-green-fg ansi-bold">     14</span>     look_adjacent,
<span class="ansi-green-fg ansi-bold">     15</span>     permute_via_gather,
<span class="ansi-green-fg ansi-bold">     16</span>     permute_via_sort,
<span class="ansi-green-fg ansi-bold">     17</span> )
<span class="ansi-green-fg ansi-bold">     20</span> <span style="font-style:italic;color:rgb(95,135,135)"># def tie_in(x, y):</span>
<span class="ansi-green-fg ansi-bold">     21</span> <span style="font-style:italic;color:rgb(95,135,135)">#   if fastmath.backend_name() == 'jax':</span>
<span class="ansi-green-fg ansi-bold">     22</span> <span style="font-style:italic;color:rgb(95,135,135)">#     return jax.lax.tie_in(x, y)</span>
<span class="ansi-green-fg ansi-bold">     23</span> <span style="font-style:italic;color:rgb(95,135,135)">#   return y</span>

<span class="ansi-red-fg">ImportError</span>: cannot import name 'tie_in' from 'trax.layers' (/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/__init__.py)</pre>
</div>
</div>
</div>
</section>
<section id="2" class="level2">
<h2 class="anchored" data-anchor-id="2">Part 2 Full Dot-Product Self Attention</h2>
<section id="2.1" class="level3">
<h3 class="anchored" data-anchor-id="2.1">Part 2.1 Description</h3>
<div id="fig-03" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/C4W4_LN2_image2.PNG" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;3: Project datapath and primary data structures and where they are implemented"><img src="img/C4W4_LN2_image2.PNG" width="600" height="200" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Project datapath and primary data structures and where they are implemented
</figcaption>
</figure>
</div>
<p>The diagram above shows many of the familiar data structures and operations related to attention and describes the routines in which they are implemented. We will start by working on <em>our_simple_attend</em> or our simpler version of the original <em>attend</em> function. We will review the steps in performing dot-product attention with more focus on the details of the operations and their significance. This is useful when comparing to LSH attention. Note we will be discussing a single example/head unless otherwise specified.</p>
<div id="fig-04" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-04-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/C4W4_LN2_image3.PNG" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;4: Dot-product of Query and Key"><img src="img/C4W4_LN2_image3.PNG" width="700" height="250" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-04-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Dot-product of Query and Key
</figcaption>
</figure>
</div>
<p>The <em>attend</em> function receives <em>Query</em> and <em>Key</em>. As a reminder, they are produced by a matrix multiply of all the inputs with a single set of weights. We will describe the inputs as <em>embeddings</em> assuming an NLP application, however, this is not required. This matrix multiply very much like a convolutional network where a set of weights (a filter) slide across the input vectors leaving behind a map of the similarity of the input to the filter. In this case, the filters are the weight matrices <span class="math inline">W^Q</span> and <span class="math inline">W^K</span>. The resulting maps are Q and K. Q and K have the dimensions of (n_seq, n_q) where n_seq is the number input embeddings and n_q or n_k is the selected size of the Q or K vectors. Note the shading of Q and K, this reflects the fact that each entry is associated with a particular input embedding. You will note later in the code that K is optional. Apparently, similar results can be achieved using Query alone saving the compute and storage associated with K. In that case, the dot-product in <em>attend</em> is matmul(q,q). Note the resulting dot-product (<em>Dot</em>) entries describe a complete (n_seq,n_seq) map of the similarity of all entries of q vs all entries of k. This is reflected in the notation in the dot-product boxes of <span class="math inline">w_n</span>,<span class="math inline">w_m</span> representing word_n, word_m. Note that each row of <em>Dot</em> describes the relationship of an input embedding, say <span class="math inline">w_0</span>, with every other input.</p>
<p>In some applications some values are masked. This can be used, for example to exclude results that occur later in time (causal) or to mask padding or other inputs.</p>
<div id="fig-05" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-05-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/C4W4_LN2_image4.PNG" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure&nbsp;5: Masking"><img src="img/C4W4_LN2_image4.PNG" width="900" height="300" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-05-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Masking
</figcaption>
</figure>
</div>
<p>The routine below <em>mask_self_attention</em> implements a flexible masking capability. The masking is controlled by the information in q_info and kv_info.</p>
<div id="ac62ee54" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mask_self_attention(</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    dots, q_info, kv_info, causal<span class="op">=</span><span class="va">True</span>, exclude_self<span class="op">=</span><span class="va">True</span>, masked<span class="op">=</span><span class="va">False</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Performs masking for self-attention."""</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> causal:</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        mask <span class="op">=</span> fastmath.lt(q_info, kv_info).astype(np.float32)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>        dots <span class="op">=</span> dots <span class="op">-</span> <span class="fl">1e9</span> <span class="op">*</span> mask</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> exclude_self:</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        mask <span class="op">=</span> np.equal(q_info, kv_info).astype(np.float32)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        dots <span class="op">=</span> dots <span class="op">-</span> <span class="fl">1e5</span> <span class="op">*</span> mask</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> masked:</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        zeros_like_kv_info <span class="op">=</span> tie_in(kv_info, np.zeros_like(kv_info))</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        mask <span class="op">=</span> fastmath.lt(kv_info, zeros_like_kv_info).astype(np.float32)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>        dots <span class="op">=</span> dots <span class="op">-</span> <span class="fl">1e9</span> <span class="op">*</span> mask</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> dots</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>A SoftMax is applied per row of the <em>Dot</em> matrix to scale the values in the row between 0 and 1.</p>
<div id="fig-06" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-06-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/C4W4_LN2_image5.PNG" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure&nbsp;6: SoftMax per row of Dot"><img src="img/C4W4_LN2_image5.PNG" width="900" height="300" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-06-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: SoftMax per row of Dot
</figcaption>
</figure>
</div>
</section>
<section id="2.1.1" class="level3">
<h3 class="anchored" data-anchor-id="2.1.1">Part 2.1.1 our_softmax</h3>
<p>This code uses a separable form of the softmax calculation. Recall the softmax: <span class="math display">
softmax(x_i)=\frac{\exp(x_i)}{\sum_j \exp(x_j)}\tag{1}
</span></p>
<p>This can be alternately implemented as: <span class="math display">
logsumexp(x)=\log{({\sum_j \exp(x_j)})}\tag{2}
</span></p>
<p><span class="math display">
softmax(x_i)=\exp({x_i - logsumexp(x)})\tag{3}
</span></p>
<p>The work below will maintain a copy of the logsumexp allowing the softmax to be completed in sections. You will see how this is useful later in the LSHSelfAttention class.</p>
<p>We’ll create a routine to implement that here with the addition of a passthrough. The matrix operations we will be working on below are easier to follow if we can maintain integer values. So, for tests, we will skip the softmax in some cases.</p>
<div id="0134ada9" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> our_softmax(x, passthrough<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" softmax with passthrough"""</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    logsumexp <span class="op">=</span> fastmath.logsumexp(x, axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    o <span class="op">=</span> np.exp(x <span class="op">-</span> logsumexp)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> passthrough:</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (x, np.zeros_like(logsumexp))</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (o, logsumexp)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s check our implementation.</p>
<div id="4b078fdb" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co">## compare softmax(a) using both methods</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> np.array([<span class="fl">1.0</span>, <span class="fl">2.0</span>, <span class="fl">3.0</span>, <span class="fl">4.0</span>])</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>sma <span class="op">=</span> np.exp(a) <span class="op">/</span> <span class="bu">sum</span>(np.exp(a))</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sma)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>sma2, a_logsumexp <span class="op">=</span> our_softmax(a)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sma2)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(a_logsumexp)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0.0320586  0.08714432 0.23688282 0.6439142 ]
[0.0320586  0.0871443  0.23688279 0.64391416]
[4.44019]</code></pre>
</div>
</div>
<p>The purpose of the dot-product is to ‘focus attention’ on some of the inputs. Dot now has entries appropriately scaled to enhance some values and reduce others. These are now applied to the <span class="math inline">V</span> entries.</p>
<div id="fig-07" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-07-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/C4W4_LN2_image6.PNG" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Figure&nbsp;7: Applying Attention to V"><img src="img/C4W4_LN2_image6.PNG" width="900" height="300" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-07-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Applying Attention to V
</figcaption>
</figure>
</div>
<p><span class="math inline">V</span> is of size (n_seq,n_v). Note the shading in the diagram. This is to draw attention to the operation of the matrix multiplication. This is detailed below.</p>
<div id="fig-08" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-08-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/C4W4_LN2_image7.PNG" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Figure&nbsp;8: Matrix Multiply"><img src="img/C4W4_LN2_image7.PNG" width="900" height="300" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-08-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Matrix Multiply
</figcaption>
</figure>
</div>
<p><span class="math inline">V</span> is formed by a matrix multiply of the input embedding with the weight matrix <span class="math inline">W^v</span> whose values were set by backpropagation. The row entries of <span class="math inline">V</span> are then related to the corresponding input embedding. The matrix multiply weights first column of V, representing a section of each of the input embeddings, with the first row of Dot, representing the similarity of <span class="math inline">W_0</span> and each word of the input embedding and deposits the value in <span class="math inline">Z</span></p>
</section>
<section id="2.2" class="level3">
<h3 class="anchored" data-anchor-id="2.2">Part 2.2 our_simple_attend</h3>
<p>In this section we’ll work on an implementation of <em>attend</em> whose operations you can see in figure 3. It is a slightly simplified version of the routine in <a href="https://github.com/google/trax/blob/v1.3.4/trax/layers/research/efficient_attention.py">efficient_attention.py</a>. We will fill in a few lines of code. The main goal is to become familiar with the routine. You have implemented similar functionality in a previous assignment.</p>
<p><strong>Instructions</strong> <strong>Step 1:</strong> matrix multiply (np.matmul) q and the k ‘transpose’ kr. <strong>Step 2:</strong> use our_softmax() to perform a softmax on masked output of the dot product, dots. <strong>Step 3:</strong> matrix multiply (np.matmul) dots and v.</p>
<div id="75754f08" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> our_simple_attend(</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    q, k<span class="op">=</span><span class="va">None</span>, v<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    mask_fn<span class="op">=</span><span class="va">None</span>, q_info<span class="op">=</span><span class="va">None</span>, kv_info<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    dropout<span class="op">=</span><span class="fl">0.0</span>, rng<span class="op">=</span><span class="va">None</span>, verbose<span class="op">=</span><span class="va">False</span>, passthrough<span class="op">=</span><span class="va">False</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>  <span class="co">"""Dot-product attention,  with masking, without optional chunking and/or.</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co">  Args:</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="co">    q: Query vectors, shape [q_len, d_qk]</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="co">    k: Key vectors, shape [kv_len, d_qk]; or None</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="co">    v: Value vectors, shape [kv_len, d_v]</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="co">    mask_fn: a function reference that implements masking (e.g. mask_self_attention)</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="co">    q_info: Query-associated metadata for masking</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="co">    kv_info: Key-associated metadata for masking</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="co">    dropout: Dropout rate</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="co">    rng: RNG for dropout</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="co">  Returns:</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a><span class="co">    A tuple (output, dots_logsumexp). The output has shape [q_len, d_v], and</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a><span class="co">    dots_logsumexp has shape [q_len]. The logsumexp of the attention</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a><span class="co">    probabilities is useful for combining multiple rounds of attention (as in</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="co">    LSH attention).</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a><span class="co">  """</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>  <span class="cf">assert</span> v <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>  share_qk <span class="op">=</span> (k <span class="kw">is</span> <span class="va">None</span>)</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> share_qk:</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>    k <span class="op">=</span> q</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> kv_info <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>      kv_info <span class="op">=</span> q_info</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> share_qk:</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>    k <span class="op">=</span> length_normalized(k)</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>  k <span class="op">=</span> k <span class="op">/</span> np.sqrt(k.shape[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Dot-product attention.</span></span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>  kr <span class="op">=</span> np.swapaxes(k, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">2</span>)  <span class="co"># note the fancy transpose for later..</span></span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a><span class="co">## Step 1  ##</span></span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>  dots <span class="op">=</span> np.matmul(q, kr )</span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> verbose: <span class="bu">print</span>(<span class="st">"Our attend dots"</span>, dots.shape)</span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Masking</span></span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> mask_fn <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a>    dots <span class="op">=</span> mask_fn(dots, q_info[..., :, <span class="va">None</span>], kv_info[..., <span class="va">None</span>, :])</span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Softmax.</span></span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a>  <span class="co">#dots_logsumexp = fastmath.logsumexp(dots, axis=-1, keepdims=True)  #original</span></span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a>  <span class="co">#dots = np.exp(dots - dots_logsumexp)  #original</span></span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a><span class="co">## Step 2  ##</span></span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a>  <span class="co">#replace with our_softmax()</span></span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a>  dots, dots_logsumexp <span class="op">=</span> our_softmax(dots, passthrough<span class="op">=</span>passthrough)</span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> verbose: <span class="bu">print</span>(<span class="st">"Our attend dots post softmax"</span>, dots.shape, dots_logsumexp.shape)</span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> dropout <span class="op">&gt;</span> <span class="fl">0.0</span>:</span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> rng <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span></span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Dropout is broadcast across the bin dimension</span></span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a>    dropout_shape <span class="op">=</span> (dots.shape[<span class="op">-</span><span class="dv">2</span>], dots.shape[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a>    keep_prob <span class="op">=</span> tie_in(dots, <span class="fl">1.0</span> <span class="op">-</span> dropout)</span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a>    keep <span class="op">=</span> fastmath.random.bernoulli(rng, keep_prob, dropout_shape)</span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a>    multiplier <span class="op">=</span> keep.astype(dots.dtype) <span class="op">/</span> tie_in(keep, keep_prob)</span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a>    dots <span class="op">=</span> dots <span class="op">*</span> multiplier</span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a><span class="co">## Step 3  ##</span></span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a><span class="co"># The softmax normalizer (dots_logsumexp) is used by multi-round LSH attn.</span></span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a>  out <span class="op">=</span> np.matmul(dots, v)</span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> verbose: <span class="bu">print</span>(<span class="st">"Our attend out1"</span>, out.shape)</span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a>  out <span class="op">=</span> np.reshape(out, (<span class="op">-</span><span class="dv">1</span>, out.shape[<span class="op">-</span><span class="dv">1</span>]))</span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> verbose: <span class="bu">print</span>(<span class="st">"Our attend out2"</span>, out.shape)</span>
<span id="cb7-70"><a href="#cb7-70" aria-hidden="true" tabindex="-1"></a>  dots_logsumexp <span class="op">=</span> np.reshape(dots_logsumexp, (<span class="op">-</span><span class="dv">1</span>,))</span>
<span id="cb7-71"><a href="#cb7-71" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> out, dots_logsumexp</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="b7f45f36" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>seq_len <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>emb_len <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>d_qk <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>d_v <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> fastmath.use_backend(<span class="st">"jax"</span>):  <span class="co"># specify the backend for consistency</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    rng_attend <span class="op">=</span> fastmath.random.get_prng(<span class="dv">1</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    q <span class="op">=</span> k <span class="op">=</span> jax.random.uniform(rng_attend, (seq_len, d_qk), dtype<span class="op">=</span>np.float32)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    v <span class="op">=</span> jax.random.uniform(rng_attend, (seq_len, d_v), dtype<span class="op">=</span>np.float32)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    o, logits <span class="op">=</span> our_simple_attend(</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>        q,</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>        k,</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>        v,</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>        mask_fn<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>        q_info<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>        kv_info<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>        dropout<span class="op">=</span><span class="fl">0.0</span>,</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>        rng<span class="op">=</span>rng_attend,</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>        verbose<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(o, <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>, logits)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Our attend dots (8, 8)
Our attend dots post softmax (8, 8) (8, 1)
Our attend out1 (8, 4)
Our attend out2 (8, 4)
[[0.5455444  0.4232705  0.62970716 0.45504814]
 [0.5558777  0.4169514  0.6260488  0.45763403]
 [0.5502556  0.42250413 0.6107501  0.4532582 ]
 [0.53680766 0.43004778 0.63048995 0.4492887 ]
 [0.5546176  0.41898918 0.62778664 0.44567773]
 [0.54741716 0.4229177  0.6060424  0.46433902]
 [0.53192824 0.43415833 0.63327026 0.44313937]
 [0.538871   0.42285213 0.6527077  0.44843906]] 
 [2.5345023 2.6896586 2.8266857 2.4992957 2.861424  2.6235857 2.5204637
 2.3627536]</code></pre>
</div>
</div>
<details>
<summary>
<font size="3"><b> Expected Output </b></font>
</summary>
</details>
<p><strong>Expected Output</strong></p>
<pre><code>Our attend dots (8, 8)
Our attend dots post softmax (8, 8) (8, 1)
Our attend out1 (8, 4)
Our attend out2 (8, 4)
[[0.5606324  0.7290605  0.5251243  0.47101074]
 [0.5713517  0.71991956 0.5033342  0.46975708]
 [0.5622886  0.7288458  0.52172124 0.46318397]
 [0.5568317  0.72234154 0.542236   0.4699722 ]
 [0.56504494 0.72274375 0.5204978  0.47231334]
 [0.56175965 0.7216782  0.53293145 0.48003793]
 [0.56753993 0.72232544 0.5141734  0.46625748]
 [0.57100445 0.70785505 0.5325362  0.4590797 ]]
 [2.6512175 2.1914332 2.6630518 2.7792363 2.4583826 2.5421977 2.4145055
 2.5111294]</code></pre>
<details>
<summary>
<font size="3"><b> completed code for reference </b></font>
</summary>
<pre><code>This notebook is ungraded, so for reference, the completed code follows:</code></pre>
</details>
<pre><code>def our_simple_attend(
    q, k=None, v=None,
    mask_fn=None, q_info=None, kv_info=None,
    dropout=0.0, rng=None, verbose=False, passthrough=False
    ):
  """Dot-product attention,  with masking, without optional chunking and/or.

  Args:
    q: Query vectors, shape [q_len, d_qk]
    k: Key vectors, shape [kv_len, d_qk]; or None
    v: Value vectors, shape [kv_len, d_v]
    mask_fn: a function reference that implements masking (e.g. mask_self_attention)
    q_info: Query-associated metadata for masking
    kv_info: Key-associated metadata for masking
    dropout: Dropout rate
    rng: RNG for dropout

  Returns:
    A tuple (output, dots_logsumexp). The output has shape [q_len, d_v], and
    dots_logsumexp has shape [q_len]. The logsumexp of the attention
    probabilities is useful for combining multiple rounds of attention (as in
    LSH attention).
  """
  assert v is not None
  share_qk = (k is None)
  if share_qk:
    k = q
    if kv_info is None:
      kv_info = q_info

  if share_qk:
    k = length_normalized(k)
  k = k / np.sqrt(k.shape[-1])

  # Dot-product attention.
  kr = np.swapaxes(k, -1, -2)  #note the fancy transpose for later..

## Step 1  ##
  dots = np.matmul(q, kr )
  if verbose: print("Our attend dots", dots.shape)

  # Masking
  if mask_fn is not None:
    dots = mask_fn(dots, q_info[..., :, None], kv_info[..., None, :])

  # Softmax.
  #dots_logsumexp = fastmath.logsumexp(dots, axis=-1, keepdims=True)  #original
  #dots = np.exp(dots - dots_logsumexp)  #original
## Step 2  ##
  #replace with our_softmax()
  dots, dots_logsumexp = our_softmax(dots, passthrough=passthrough)
  if verbose: print("Our attend dots post softmax", dots.shape, dots_logsumexp.shape)

  if dropout &gt; 0.0:
    assert rng is not None
    # Dropout is broadcast across the bin dimension
    dropout_shape = (dots.shape[-2], dots.shape[-1])
    keep_prob = tie_in(dots, 1.0 - dropout)
    keep = fastmath.random.bernoulli(rng, keep_prob, dropout_shape)
    multiplier = keep.astype(dots.dtype) / tie_in(keep, keep_prob)
    dots = dots * multiplier

## Step 3  ##
# The softmax normalizer (dots_logsumexp) is used by multi-round LSH attn.
  out = np.matmul(dots, v)
  if verbose: print("Our attend out1", out.shape)
  out = np.reshape(out, (-1, out.shape[-1]))
  if verbose: print("Our attend out2", out.shape)
  dots_logsumexp = np.reshape(dots_logsumexp, (-1,))
  return out, dots_logsumexp</code></pre>
</section>
</section>
<section id="2.3" class="level2">
<h2 class="anchored" data-anchor-id="2.3">Part 2.3 Class OurSelfAttention</h2>
<p>Here we create our own self attention layer by creating a class <code>OurSelfAttention</code>. The parent class will be the tl.SelfAttention layer in Trax. We will only override the <code>forward_unbatched</code> routine.</p>
<p>We’re not asking you to modify anything in this routine. There are some comments to draw your attention to a few lines.</p>
<div id="3b2bfbdb" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> OurSelfAttention(tl.SelfAttention):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Our self-attention. Just the Forward Function."""</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward_unbatched(</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>, x, mask<span class="op">=</span><span class="va">None</span>, <span class="op">*</span>, weights, state, rng, update_state, verbose<span class="op">=</span><span class="va">False</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"ourSelfAttention:forward_unbatched"</span>)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>        <span class="kw">del</span> update_state</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>        attend_rng, output_rng <span class="op">=</span> fastmath.random.split(rng)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.bias:</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.share_qk:</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>                w_q, w_v, w_o, b_q, b_v <span class="op">=</span> weights</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>                w_q, w_k, w_v, w_o, b_q, b_k, b_v <span class="op">=</span> weights</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.share_qk:</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>                w_q, w_v, w_o <span class="op">=</span> weights</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>                w_q, w_k, w_v, w_o <span class="op">=</span> weights</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"x.shape,w_q.shape"</span>, x.shape, w_q.shape)</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>        q <span class="op">=</span> np.matmul(x, w_q)</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>        k <span class="op">=</span> <span class="va">None</span></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.share_qk:</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>            k <span class="op">=</span> np.matmul(x, w_k)</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> np.matmul(x, w_v)</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.bias:</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>            q <span class="op">=</span> q <span class="op">+</span> b_q</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.share_qk:</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>                k <span class="op">=</span> k <span class="op">+</span> b_k</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>            v <span class="op">=</span> v <span class="op">+</span> b_v</span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>        mask_fn <span class="op">=</span> functools.partial(</span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>            mask_self_attention,</span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>            causal<span class="op">=</span><span class="va">self</span>.causal,</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>            exclude_self<span class="op">=</span><span class="va">self</span>.share_qk,</span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>            masked<span class="op">=</span><span class="va">self</span>.masked,</span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>        q_info <span class="op">=</span> kv_info <span class="op">=</span> tie_in(x, np.arange(q.shape[<span class="op">-</span><span class="dv">2</span>], dtype<span class="op">=</span>np.int32))</span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> (mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>) <span class="op">==</span> <span class="va">self</span>.masked</span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.masked:</span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a>            <span class="co"># mask is a boolean array (True means "is valid token")</span></span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a>            ones_like_mask <span class="op">=</span> tie_in(x, np.ones_like(mask, dtype<span class="op">=</span>np.int32))</span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a>            kv_info <span class="op">=</span> kv_info <span class="op">*</span> np.where(mask, ones_like_mask, <span class="op">-</span>ones_like_mask)</span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Notice, we are callout our vesion of attend</span></span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a>        o, _ <span class="op">=</span> our_simple_attend(</span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a>            q,</span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a>            k,</span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a>            v,</span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a>            mask_fn<span class="op">=</span>mask_fn,</span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a>            q_info<span class="op">=</span>q_info,</span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a>            kv_info<span class="op">=</span>kv_info,</span>
<span id="cb13-56"><a href="#cb13-56" aria-hidden="true" tabindex="-1"></a>            dropout<span class="op">=</span><span class="va">self</span>.attention_dropout,</span>
<span id="cb13-57"><a href="#cb13-57" aria-hidden="true" tabindex="-1"></a>            rng<span class="op">=</span>attend_rng,</span>
<span id="cb13-58"><a href="#cb13-58" aria-hidden="true" tabindex="-1"></a>            verbose<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb13-59"><a href="#cb13-59" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb13-60"><a href="#cb13-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-61"><a href="#cb13-61" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Notice, wo weight matrix applied to output of attend in forward_unbatched</span></span>
<span id="cb13-62"><a href="#cb13-62" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> np.matmul(o, w_o)</span>
<span id="cb13-63"><a href="#cb13-63" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> apply_broadcasted_dropout(out, <span class="va">self</span>.output_dropout, output_rng)</span>
<span id="cb13-64"><a href="#cb13-64" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out, state</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="acfae321" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>causal <span class="op">=</span> <span class="va">False</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>masked <span class="op">=</span> <span class="va">False</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> <span class="va">None</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>attention_dropout <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>n_heads <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>d_qk <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>d_v <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>seq_len <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>emb_len <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>osa <span class="op">=</span> OurSelfAttention(</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>    n_heads<span class="op">=</span>n_heads,</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>    d_qk<span class="op">=</span>d_qk,</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>    d_v<span class="op">=</span>d_v,</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    causal<span class="op">=</span>causal,</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>    use_reference_code<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>    attention_dropout<span class="op">=</span>attention_dropout,</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>    mode<span class="op">=</span><span class="st">"train"</span>,</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>rng_osa <span class="op">=</span> fastmath.random.get_prng(<span class="dv">1</span>)</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> jax.random.uniform(</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>    jax.random.PRNGKey(<span class="dv">0</span>), (batch_size, seq_len, emb_len), dtype<span class="op">=</span>np.float32</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>_, _ <span class="op">=</span> osa.init(tl.shapes.signature(x), rng<span class="op">=</span>rng_osa)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="a1d162d5" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>osa(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>ourSelfAttention:forward_unbatched</code></pre>
</div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">LayerError</span>                                Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[9], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> <span class="ansi-yellow-bg">osa</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">x</span><span class="ansi-yellow-bg">)</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/base.py:197</span>, in <span class="ansi-cyan-fg">Layer.__call__</span><span class="ansi-blue-fg">(self, x, weights, state, rng)</span>
<span class="ansi-green-fg ansi-bold">    195</span>   <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>state <span style="color:rgb(98,98,98)">=</span> state  <span style="font-style:italic;color:rgb(95,135,135)"># Needed if the model wasn't fully initialized.</span>
<span class="ansi-green-fg ansi-bold">    196</span> state <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>state
<span class="ansi-green-fg">--&gt; 197</span> outputs, new_state <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">pure_fn</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">x</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">weights</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">state</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">rng</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">    198</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>state <span style="color:rgb(98,98,98)">=</span> new_state
<span class="ansi-green-fg ansi-bold">    199</span> <span style="font-weight:bold;color:rgb(0,135,0)">return</span> outputs

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/base.py:605</span>, in <span class="ansi-cyan-fg">Layer.pure_fn</span><span class="ansi-blue-fg">(self, x, weights, state, rng, use_cache)</span>
<span class="ansi-green-fg ansi-bold">    602</span> <span style="font-weight:bold;color:rgb(0,135,0)">except</span> <span style="font-weight:bold;color:rgb(215,95,95)">Exception</span>:
<span class="ansi-green-fg ansi-bold">    603</span>   <span style="font-style:italic;color:rgb(95,135,135)"># Skipping 3 lines as it's always the uninteresting internal call.</span>
<span class="ansi-green-fg ansi-bold">    604</span>   name, trace <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_name, _short_traceback(skip<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">3</span>)
<span class="ansi-green-fg">--&gt; 605</span>   <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> LayerError(name, <span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">pure_fn</span><span style="color:rgb(175,0,0)">'</span>,
<span class="ansi-green-fg ansi-bold">    606</span>                    <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_caller, signature(x), trace) <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">None</span>

<span class="ansi-red-fg">LayerError</span>: Exception passing through layer OurSelfAttention (in pure_fn):
  layer created in file [...]/layers/research/efficient_attention.py, line 1014
  layer input shapes: ShapeDtype{shape:(1, 8, 5), dtype:float32}

  File [...]/layers/research/efficient_attention.py, line 1323, in forward
    single_out, single_new_state = self.forward_unbatched(

  File [...]/tmp/ipykernel_1538144/1155828790.py, line 10, in forward_unbatched
    if self.bias:

AttributeError: 'OurSelfAttention' object has no attribute 'bias'. Did you mean: '_bias'?</pre>
</div>
</div>
</div>
<details>
<summary>
<font size="3"><b> Expected Output </b></font>
</summary>
<p><strong>Expected Output</strong> Notice a few things:</p>
<ul>
<li>the w_q (and w_k) matrices are applied to each row or each embedding on the input. This is similar to the filter operation in convolution</li>
<li>forward_unbatched is called 3 times. This is because we have 3 heads in this example.</li>
</ul>
<pre><code>ourSelfAttention:forward_unbatched
x.shape,w_q.shape (8, 5) (5, 3)
Our attend dots (8, 8)
Our attend dots post softmax (8, 8) (8, 1)
Our attend out1 (8, 4)
Our attend out2 (8, 4)
ourSelfAttention:forward_unbatched
x.shape,w_q.shape (8, 5) (5, 3)
Our attend dots (8, 8)
Our attend dots post softmax (8, 8) (8, 1)
Our attend out1 (8, 4)
Our attend out2 (8, 4)
ourSelfAttention:forward_unbatched
x.shape,w_q.shape (8, 5) (5, 3)
Our attend dots (8, 8)
Our attend dots post softmax (8, 8) (8, 1)
Our attend out1 (8, 4)
Our attend out2 (8, 4)
DeviceArray([[[ 6.70414209e-01, -1.04319841e-01, -5.33822298e-01,
                1.92711830e-01, -4.54187393e-05],
              [ 6.64090097e-01, -1.01875424e-01, -5.35733163e-01,
                1.88311756e-01, -6.30629063e-03],
              [ 6.73380017e-01, -1.06952369e-01, -5.31989932e-01,
                1.90056816e-01,  1.30271912e-03],
              [ 6.84564888e-01, -1.13240272e-01, -5.50182462e-01,
                1.95673436e-01,  5.47635555e-03],
              [ 6.81435883e-01, -1.11068964e-01, -5.32343209e-01,
                1.91912338e-01,  5.69400191e-03],
              [ 6.80724978e-01, -1.08496904e-01, -5.34994125e-01,
                1.96332246e-01,  5.89773059e-03],
              [ 6.80933356e-01, -1.14087075e-01, -5.18659890e-01,
                1.90674081e-01,  1.14096403e-02],
              [ 6.80265009e-01, -1.09031796e-01, -5.38248718e-01,
                1.94203183e-01,  4.23943996e-03]]], dtype=float32)</code></pre>
<p><a name="3"></a> ## Part 3.0 Trax LSHSelfAttention <a name="3.1"></a> ## Part 3.1 Description The larger the matrix multiply in the previous section is, the more context can be taken into account when making the next decision. However, the self attention dot product grows as the size of the input squared. For example, if one wished to have an input size of 1024, that would result in <span class="math inline">1024^2</span> or over a million dot products for each head! As a result, there has been significant research related to reducing the compute requirements. One such approach is Locality Sensitive Hashing(LSH) Self Attention.</p>
<p>You may recall, earlier in the course you utilized LSH to find similar tweets without resorting to calculating cosine similarity for each pair of embeddings. We will use a similar approach here. It may be best described with an example. <img src="C4W4_LN2_image8.PNG" height="400" width="750"></p>
<center>
<b>Figure 9: Example of LSH Self Attention</b>
</center>
<p>LSH Self attention uses Queries only, no Keys. Attention then generates a metric of the similarity of each value of Q relative to all the other values in Q. An earlier assignment demonstrated that values which hash to the same bucket are likely to be similar. Further, multiple random hashes can improve the chances of finding entries which are similar. This is the approach taken here, though the hash is implemented a bit differently. The values of Q are hashed into buckets using a randomly generated set of hash vectors. Multiple sets of hash vectors are used, generating multiple hash tables. In the figure above, we have 3 hash tables with 4 buckets in each table. Notionally, following the hash, the values of Q have been replicated 3 times and distributed to their appropriate bucket in each of the 3 tables. To find similarity then, one generates dot-products only between members of the buckets. The result of this operation provides information on which entries are similar. As the operation has been distributed over multiple hash tables, these results need to be combined to form a complete picture and this can be used to generate a reduced dot-product attention array. Its clear that because we do not do a compare of every value vs every other value, the size of <em>Dots</em> will be reduced.</p>
<p>The challenge in this approach is getting it to operate efficiently. You may recall from the earlier assignments the buckets were lists of entries and had varying length. This will operate poorly on a vector processing machine such as a GPU or TPU. Ideally, operations are done in large blocks with uniform sizes. While it is straightforward to implement the hash algorithm this way, it is challenging to managed buckets and variable sized dot-products. This will be discussed further below. For now, we will examine and implement the hash function.</p>
<p><a name="3.2"></a> ## Part 3.2 our_hash_vectors</p>
<p><em>our_hash_vectors</em>, is a reimplementation of Trax <em>hashvector</em>. It takes in an array of vectors, hashes the entries and returns and array assigning each input vector to n_hash buckets. Hashing is described as creating <em>random rotations</em>, see <a href="https://arxiv.org/pdf/1509.02897.pdf">Practical and Optimal LSH for Angular Distance</a>.</p>
<img src="C4W4_LN2_image9.PNG" height="400" width="750"> <img src="C4W4_LN2_image10.PNG" height="400" width="750">
<center>
<b>Figure 10: Processing steps in our_hash_vectors </b>
</center>
<p>Note, in the diagram, sizes relate to our expected input <span class="math inline">Q</span> while our_hash_vectors is written assuming a generic input vector</p>
<p><strong>Instructions</strong> <strong>Step 1</strong> create an array of random normal vectors which will be our hash vectors. Each vector will be hashed into a hash table and into <code>rot_size//2</code> buckets. We use <code>rot_size//2</code> to reduce computation. Later in the routine we will form the negative rotations with a simple negation and concatenate to get a full <code>rot_size</code> number of rotations. * use fastmath.random.normal and create an array of random vectors of shape (vec.shape[-1],n_hashes, rot_size//2)</p>
<p><strong>Step 2</strong> In this step we simply do the matrix multiply. <code>jax</code> has an accelerated version of <a href="https://numpy.org/doc/stable/reference/generated/numpy.einsum.html">einsum</a>. Here we will utilize more conventional routines.</p>
<p><strong>Step 2x</strong> * 2a: np.reshape random_rotations into a 2 dimensional array ([-1, n_hashes * (rot_size // 2)]) * 2b: np.dot vecs and random_rotations forming our rotated_vecs * 2c: back to 3 dimension with np.reshape [-1, n_hashes, rot_size//2] * 2d: prepare for concatenating by swapping dimensions np.transpose (1, 0, 2) <strong>Step 3</strong> Here we concatenate our rotation vectors getting a fullrot_size number of buckets (note, n_buckets = rotsize) * use np.concatenate, [rotated_vecs, -rotated_vecs], axis=-1 <strong>Step 4</strong> <strong>This is the exciting step!</strong> You have no doubt been wondering how we will turn these vectors into bucket indexes. By performing np.argmax over the rotations for a given entry, you get the index to the best match! We will use this as a bucket index. * np.argmax(…).astype(np.int32); be sure to use the correct axis! <strong>Step 5</strong> In this style of hashing, items which land in bucket 0 of hash table 0 are not necessarily similar to those landing in bucket 0 of hash table 1, so we keep them separate. We do this by offsetting the bucket numbers by ‘n_buckets’. * add buckets and offsets and reshape into a one dimensional array This will return a 1D array of size n_hashes * vec.shape[0].</p>
<div id="8d116184" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> our_hash_vectors(vecs, rng, n_buckets, n_hashes, mask<span class="op">=</span><span class="va">None</span>, verbose<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="co">  Args:</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="co">    vecs: tensor of at least 2 dimension,</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="co">    rng: random number generator</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="co">    n_buckets: number of buckets in each hash table</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="co">    n_hashes: the number of hash tables</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="co">    mask: None indicating no mask or a 1D boolean array of length vecs.shape[0], containing the location of padding value</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="co">    verbose: controls prints for debug</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="co">  Returns:</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a><span class="co">    A vector of size n_hashes * vecs.shape[0] containing the buckets associated with each input vector per hash table.</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># check for even, integer bucket sizes</span></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="bu">isinstance</span>(n_buckets, <span class="bu">int</span>) <span class="kw">and</span> n_buckets <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>    rng <span class="op">=</span> fastmath.stop_gradient(tie_in(vecs, rng))</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>    rot_size <span class="op">=</span> n_buckets</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>    <span class="co">### Start Code Here</span></span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>    <span class="co">### Step 1 </span><span class="al">###</span></span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>    rotations_shape <span class="op">=</span> (vecs.shape[<span class="op">-</span><span class="dv">1</span>], n_hashes, rot_size <span class="op">//</span> <span class="dv">2</span>)</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>    random_rotations <span class="op">=</span> fastmath.random.normal(rng, rotations_shape).astype(</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>        np.float32)</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> verbose: <span class="bu">print</span>(<span class="st">"random.rotations.shape"</span>, random_rotations.shape)</span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>    <span class="co">### Step 2 </span><span class="al">###</span></span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> fastmath.backend_name() <span class="op">==</span> <span class="st">'jax'</span>:</span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>      rotated_vecs <span class="op">=</span> np.einsum(<span class="st">'tf,fhb-&gt;htb'</span>, vecs, random_rotations)</span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> verbose: <span class="bu">print</span>(<span class="st">"using jax"</span>)</span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a>      <span class="co">#Step 2a</span></span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a>      random_rotations <span class="op">=</span> np.reshape(random_rotations,</span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a>                                    [<span class="op">-</span><span class="dv">1</span>, n_hashes <span class="op">*</span> (rot_size <span class="op">//</span> <span class="dv">2</span>)])</span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> verbose: <span class="bu">print</span>(<span class="st">"random_rotations reshaped"</span>, random_rotations.shape)</span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a>      <span class="co">#Step 2b</span></span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a>      rotated_vecs <span class="op">=</span> np.dot(vecs, random_rotations)</span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> verbose: <span class="bu">print</span>(<span class="st">"rotated_vecs1"</span>, rotated_vecs.shape)</span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a>      <span class="co">#Step 2c</span></span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a>      rotated_vecs <span class="op">=</span> np.reshape(rotated_vecs, [<span class="op">-</span><span class="dv">1</span>, n_hashes, rot_size<span class="op">//</span><span class="dv">2</span>])</span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> verbose: <span class="bu">print</span>(<span class="st">"rotated_vecs2"</span>, rotated_vecs.shape)</span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true" tabindex="-1"></a>      <span class="co">#Step 2d</span></span>
<span id="cb18-44"><a href="#cb18-44" aria-hidden="true" tabindex="-1"></a>      rotated_vecs <span class="op">=</span> np.transpose(rotated_vecs, (<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">2</span>))</span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> verbose: <span class="bu">print</span>(<span class="st">"rotated_vecs3"</span>, rotated_vecs.shape)</span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-47"><a href="#cb18-47" aria-hidden="true" tabindex="-1"></a>    <span class="co">### Step 3 </span><span class="al">###</span></span>
<span id="cb18-48"><a href="#cb18-48" aria-hidden="true" tabindex="-1"></a>    rotated_vecs <span class="op">=</span> np.concatenate([rotated_vecs, <span class="op">-</span>rotated_vecs], axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb18-49"><a href="#cb18-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> verbose: <span class="bu">print</span>(<span class="st">"rotated_vecs.shape"</span>, rotated_vecs.shape)</span>
<span id="cb18-50"><a href="#cb18-50" aria-hidden="true" tabindex="-1"></a>    <span class="co">### Step 4 </span><span class="al">###</span></span>
<span id="cb18-51"><a href="#cb18-51" aria-hidden="true" tabindex="-1"></a>    buckets <span class="op">=</span> np.argmax(rotated_vecs, axis<span class="op">=-</span><span class="dv">1</span>).astype(np.int32)</span>
<span id="cb18-52"><a href="#cb18-52" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> verbose: <span class="bu">print</span>(<span class="st">"buckets.shape"</span>, buckets.shape)</span>
<span id="cb18-53"><a href="#cb18-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> verbose: <span class="bu">print</span>(<span class="st">"buckets"</span>, buckets)</span>
<span id="cb18-54"><a href="#cb18-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-55"><a href="#cb18-55" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb18-56"><a href="#cb18-56" aria-hidden="true" tabindex="-1"></a>      n_buckets <span class="op">+=</span> <span class="dv">1</span>  <span class="co"># Create an extra bucket for padding tokens only</span></span>
<span id="cb18-57"><a href="#cb18-57" aria-hidden="true" tabindex="-1"></a>      buckets <span class="op">=</span> np.where(mask[<span class="va">None</span>, :], buckets, n_buckets <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb18-58"><a href="#cb18-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-59"><a href="#cb18-59" aria-hidden="true" tabindex="-1"></a>    <span class="co"># buckets is now (n_hashes, seqlen). Next we add offsets so that</span></span>
<span id="cb18-60"><a href="#cb18-60" aria-hidden="true" tabindex="-1"></a>    <span class="co"># bucket numbers from different hashing rounds don't overlap.</span></span>
<span id="cb18-61"><a href="#cb18-61" aria-hidden="true" tabindex="-1"></a>    offsets <span class="op">=</span> tie_in(buckets, np.arange(n_hashes, dtype<span class="op">=</span>np.int32))</span>
<span id="cb18-62"><a href="#cb18-62" aria-hidden="true" tabindex="-1"></a>    offsets <span class="op">=</span> np.reshape(offsets <span class="op">*</span> n_buckets, (<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb18-63"><a href="#cb18-63" aria-hidden="true" tabindex="-1"></a>    <span class="co">### Step 5 </span><span class="al">###</span></span>
<span id="cb18-64"><a href="#cb18-64" aria-hidden="true" tabindex="-1"></a>    buckets <span class="op">=</span> np.reshape(buckets <span class="op">+</span> offsets, (<span class="op">-</span><span class="dv">1</span>,))</span>
<span id="cb18-65"><a href="#cb18-65" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> verbose: <span class="bu">print</span>(<span class="st">"buckets with offsets"</span>, buckets.shape, <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>, buckets)</span>
<span id="cb18-66"><a href="#cb18-66" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> buckets</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="4a9c57ad" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># example code. Note for reference, the sizes in this example match the values in the diagram above.</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>ohv_q <span class="op">=</span> np.ones((<span class="dv">8</span>, <span class="dv">5</span>))  <span class="co"># (seq_len=8, n_q=5)</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>ohv_n_buckets <span class="op">=</span> <span class="dv">4</span>  <span class="co"># even number</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>ohv_n_hashes <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> fastmath.use_backend(<span class="st">"tf"</span>):</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    ohv_rng <span class="op">=</span> fastmath.random.get_prng(<span class="dv">1</span>)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>    ohv <span class="op">=</span> our_hash_vectors(</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>        ohv_q, ohv_rng, ohv_n_buckets, ohv_n_hashes, mask<span class="op">=</span><span class="va">None</span>, verbose<span class="op">=</span><span class="va">True</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"ohv shape"</span>, ohv.shape, <span class="st">"</span><span class="ch">\n</span><span class="st">ohv"</span>, ohv)  <span class="co"># (ohv_n_hashes * ohv_n_buckets)</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="co"># note the random number generators do not produce the same results with different backends</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> fastmath.use_backend(<span class="st">"jax"</span>):</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>    ohv_rng <span class="op">=</span> fastmath.random.get_prng(<span class="dv">1</span>)</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>    ohv <span class="op">=</span> our_hash_vectors(ohv_q, ohv_rng, ohv_n_buckets, ohv_n_hashes, mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"ohv shape"</span>, ohv.shape, <span class="st">"</span><span class="ch">\n</span><span class="st">ohv"</span>, ohv)  <span class="co"># (ohv_n_hashes * ohv_n_buckets)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">ValueError</span>                                Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[11], line 5</span>
<span class="ansi-green-fg ansi-bold">      3</span> ohv_n_buckets <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(98,98,98)">4</span>  <span style="font-style:italic;color:rgb(95,135,135)"># even number</span>
<span class="ansi-green-fg ansi-bold">      4</span> ohv_n_hashes <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(98,98,98)">3</span>
<span class="ansi-green-fg">----&gt; 5</span> <span style="font-weight:bold;color:rgb(0,135,0)">with</span> fastmath<span style="color:rgb(98,98,98)">.</span>use_backend(<span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">tf</span><span style="color:rgb(175,0,0)">"</span>):
<span class="ansi-green-fg ansi-bold">      6</span>     ohv_rng <span style="color:rgb(98,98,98)">=</span> fastmath<span style="color:rgb(98,98,98)">.</span>random<span style="color:rgb(98,98,98)">.</span>get_prng(<span style="color:rgb(98,98,98)">1</span>)
<span class="ansi-green-fg ansi-bold">      7</span>     ohv <span style="color:rgb(98,98,98)">=</span> our_hash_vectors(
<span class="ansi-green-fg ansi-bold">      8</span>         ohv_q, ohv_rng, ohv_n_buckets, ohv_n_hashes, mask<span style="color:rgb(98,98,98)">=</span><span style="font-weight:bold;color:rgb(0,135,0)">None</span>, verbose<span style="color:rgb(98,98,98)">=</span><span style="font-weight:bold;color:rgb(0,135,0)">True</span>
<span class="ansi-green-fg ansi-bold">      9</span>     )

File <span class="ansi-green-fg">/usr/lib/python3.10/contextlib.py:135</span>, in <span class="ansi-cyan-fg">_GeneratorContextManager.__enter__</span><span class="ansi-blue-fg">(self)</span>
<span class="ansi-green-fg ansi-bold">    133</span> <span style="font-weight:bold;color:rgb(0,135,0)">del</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>args, <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>kwds, <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>func
<span class="ansi-green-fg ansi-bold">    134</span> <span style="font-weight:bold;color:rgb(0,135,0)">try</span>:
<span class="ansi-green-fg">--&gt; 135</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">next</span><span class="ansi-yellow-bg">(</span><span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">gen</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">    136</span> <span style="font-weight:bold;color:rgb(0,135,0)">except</span> <span style="font-weight:bold;color:rgb(215,95,95)">StopIteration</span>:
<span class="ansi-green-fg ansi-bold">    137</span>     <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> <span style="font-weight:bold;color:rgb(215,95,95)">RuntimeError</span>(<span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">generator didn</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">t yield</span><span style="color:rgb(175,0,0)">"</span>) <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">None</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/fastmath/ops.py:428</span>, in <span class="ansi-cyan-fg">use_backend</span><span class="ansi-blue-fg">(name)</span>
<span class="ansi-green-fg ansi-bold">    425</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="color:rgb(0,135,0)">isinstance</span>(name, Backend):
<span class="ansi-green-fg ansi-bold">    426</span>   name <span style="color:rgb(98,98,98)">=</span> name<span style="color:rgb(98,98,98)">.</span>value
<span class="ansi-green-fg">--&gt; 428</span> <span class="ansi-yellow-bg">_assert_valid_backend_name</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">name</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">    429</span> <span style="font-weight:bold;color:rgb(0,135,0)">global</span> override_backend
<span class="ansi-green-fg ansi-bold">    430</span> prev_name_or_backend <span style="color:rgb(98,98,98)">=</span> override_backend

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/fastmath/ops.py:387</span>, in <span class="ansi-cyan-fg">_assert_valid_backend_name</span><span class="ansi-blue-fg">(name)</span>
<span class="ansi-green-fg ansi-bold">    385</span>   <span style="font-weight:bold;color:rgb(0,135,0)">if</span> backend_<span style="color:rgb(98,98,98)">.</span>value <span style="color:rgb(98,98,98)">==</span> name:
<span class="ansi-green-fg ansi-bold">    386</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span>
<span class="ansi-green-fg">--&gt; 387</span> <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> <span style="font-weight:bold;color:rgb(215,95,95)">ValueError</span>(<span style="color:rgb(175,0,0)">f</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">No backend with name </span><span style="font-weight:bold;color:rgb(175,95,135)">{</span>name<span style="font-weight:bold;color:rgb(175,95,135)">}</span><span style="color:rgb(175,0,0)">'</span>)

<span class="ansi-red-fg">ValueError</span>: No backend with name tf</pre>
</div>
</div>
</div>
<details>
<summary>
<font size="3"><b> Expected Output </b></font>
</summary>
<p><strong>Expected Values</strong></p>
<pre><code>random.rotations.shape (5, 3, 2)
random_rotations reshaped (5, 6)
rotated_vecs1 (8, 6)
rotated_vecs2 (8, 3, 2)
rotated_vecs3 (3, 8, 2)
rotated_vecs.shape (3, 8, 4)
buckets.shape (3, 8)
buckets ndarray&lt;tf.Tensor(
[[3 3 3 3 3 3 3 3]
 [3 3 3 3 3 3 3 3]
 [3 3 3 3 3 3 3 3]], shape=(3, 8), dtype=int32)&gt;
buckets with offsets (24,)
 ndarray&lt;tf.Tensor([ 3  3  3  3  3  3  3  3  7  7  7  7  7  7  7  7 11 11 11 11 11 11 11 11], shape=(24,), dtype=int32)&gt;
ohv shape (24,)
ohv ndarray&lt;tf.Tensor([ 3  3  3  3  3  3  3  3  7  7  7  7  7  7  7  7 11 11 11 11 11 11 11 11], shape=(24,), dtype=int32)&gt;
using jax
ohv shape (24,)
ohv [ 3  3  3  3  3  3  3  3  5  5  5  5  5  5  5  5 11 11 11 11 11 11 11 11]```

&lt;details&gt;
&lt;summary&gt;
    &lt;font size="3" &gt;&lt;b&gt;Completed code for reference &lt;/b&gt;&lt;/font&gt;
&lt;/summary&gt;
</code></pre>
</details></details></section>
<section id="since-this-notebook-is-ungraded-the-completed-code-is-provided-here-for-reference" class="level1">
<h1>since this notebook is ungraded the completed code is provided here for reference</h1>
<p>def our_hash_vectors(vecs, rng, n_buckets, n_hashes, mask=None, verbose=False): ““” Args: vecs: tensor of at least 2 dimension, rng: random number generator n_buckets: number of buckets in each hash table n_hashes: the number of hash tables mask: None indicating no mask or a 1D boolean array of length vecs.shape[0], containing the location of padding value verbose: controls prints for debug Returns: A vector of size n_hashes * vecs.shape[0] containing the buckets associated with each input vector per hash table.</p>
<pre><code>"""

# check for even, integer bucket sizes
assert isinstance(n_buckets, int) and n_buckets % 2 == 0

rng = fastmath.stop_gradient(tie_in(vecs, rng))
rot_size = n_buckets
### Start Code Here

### Step 1 ###
rotations_shape = (vecs.shape[-1], n_hashes, rot_size // 2)
random_rotations = fastmath.random.normal(rng, rotations_shape).astype(
    np.float32)
if verbose: print("random.rotations.shape", random_rotations.shape)

### Step 2 ###
if fastmath.backend_name() == 'jax':
  rotated_vecs = np.einsum('tf,fhb-&gt;htb', vecs, random_rotations)
  if verbose: print("using jax")
else:
  #Step 2a
  random_rotations = np.reshape(random_rotations,
                                [-1, n_hashes * (rot_size // 2)])
  if verbose: print("random_rotations reshaped", random_rotations.shape)
  #Step 2b
  rotated_vecs = np.dot(vecs, random_rotations)
  if verbose: print("rotated_vecs1", rotated_vecs.shape)
  #Step 2c
  rotated_vecs = np.reshape(rotated_vecs, [-1, n_hashes, rot_size//2])
  if verbose: print("rotated_vecs2", rotated_vecs.shape)
  #Step 2d
  rotated_vecs = np.transpose(rotated_vecs, (1, 0, 2))
  if verbose: print("rotated_vecs3", rotated_vecs.shape)

### Step 3 ###
rotated_vecs = np.concatenate([rotated_vecs, -rotated_vecs], axis=-1)
if verbose: print("rotated_vecs.shape", rotated_vecs.shape)
### Step 4 ###
buckets = np.argmax(rotated_vecs, axis=-1).astype(np.int32)
if verbose: print("buckets.shape", buckets.shape)
if verbose: print("buckets", buckets)

if mask is not None:
  n_buckets += 1  # Create an extra bucket for padding tokens only
  buckets = np.where(mask[None, :], buckets, n_buckets - 1)

# buckets is now (n_hashes, seqlen). Next we add offsets so that
# bucket numbers from different hashing rounds don't overlap.
offsets = tie_in(buckets, np.arange(n_hashes, dtype=np.int32))
offsets = np.reshape(offsets * n_buckets, (-1, 1))
### Step 5 ###
buckets = np.reshape(buckets + offsets, (-1,))
if verbose: print("buckets with offsets", buckets.shape, "\n", buckets)
return buckets```</code></pre>
<p><a name="3.3"></a> ## Part 3.3 Sorting Buckets</p>
<p>Great! Now that we have a hash function, we can work on sorting our buckets and performing our matrix operations. We’ll walk through this algorithm in small steps: * sort_buckets - we’ll perform the sort * softmax * dotandv - do the matrix math to form the dotproduct and output These routines will demonstrate a simplified version of the algorithm. We won’t address masking and variable bucket sizes but will consider how they would be handled.</p>
<p><strong>sort_buckets</strong></p>
<p>At this point, we have called the hash function and were returned the associated buckets. For example, if we started with <code>q[n_seq,n_q]</code>, with <code>n_hash = 2; n_buckets = 4; n_seq = 8</code> we might be returned: <code>bucket = [0,1,2,3,0,1,2,3, 4,5,6,7,4,5,6,7]</code> Note that it is n_hash*n_seq long and that the bucket values for each hash have been offset by n_hash so the numbers do not overlap. Going forward, we going to sort this array of buckets to group together members of the same (hash,bucket) pair.</p>
<p><strong>Instructions</strong> <strong>Step 1</strong> Our goal is to sort <span class="math inline">q</span> rather than the bucket list, so we will need to track the association of the buckets to their elements in <span class="math inline">q</span>. * using np.arange, create <code>ticker</code>, just a sequence of numbers (0..n_hashed * seqlen) associating members of q with their bucket.</p>
<p><strong>Step 2</strong> This step is provided to you as it is a bit difficult to describe. We want to disambiguate elements that map to the same bucket. When a sorting routine encounters a situation where multiple entries have the same value, it can correctly choose any entry to go first. This makes testing ambiguous. This prevents that. We multiply all the buckets by <code>seqlen</code> and then add <code>ticker % seqlen</code></p>
<p><strong>Step 3</strong> Here we are! Ready to sort. This is the exciting part. * Utilize <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.sort_key_val.html#jax.lax.sort_key_val">fastmath.sort_key_val</a> and sort <code>buckets_and_t</code> and <code>ticker</code>.</p>
<p><strong>Step 4</strong> We need to be able to undo the sort at the end to get things back into their correct locations * sort <code>sticker</code> and <code>ticker</code> to for the reverse map</p>
<p><strong>Step 5</strong> create our sorted q and sorted v * use <a href="https://numpy.org/doc/stable/reference/generated/numpy.take.html">np.take</a> and <code>st</code> to grab correct values in <code>q</code> for the sorted values, <code>sq</code>. Use axis=0.</p>
<p>Use the example code below the routine to check and help debug your results.</p>
<div id="41511bc4" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sort_buckets(buckets, q, v, n_buckets, n_hashes, seqlen, verbose<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="co">  Args:</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="co">    buckets: tensor of at least 2 dimension,</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="co">    n_buckets: number of buckets in each hash table</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="co">    n_hashes: the number of hash tables</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> verbose: <span class="bu">print</span>(<span class="st">"---sort_buckets--"</span>)</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>    <span class="co">## Step 1</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>    ticker <span class="op">=</span> np.arange(n_hashes <span class="op">*</span> seqlen)</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> verbose: <span class="bu">print</span>(<span class="st">"ticker"</span>,ticker.shape, ticker)</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">## Step 2</span></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>    buckets_and_t <span class="op">=</span> seqlen <span class="op">*</span> buckets <span class="op">+</span> (ticker <span class="op">%</span> seqlen)</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> verbose: <span class="bu">print</span>(<span class="st">"buckets_and_t"</span>,buckets_and_t.shape, buckets_and_t)</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Hash-based sort ("s" at the start of variable names means "sorted")</span></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Step 3</span></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>    sbuckets_and_t, sticker <span class="op">=</span> fastmath.sort_key_val(</span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>    buckets_and_t, ticker, dimension<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> verbose: <span class="bu">print</span>(<span class="st">"sbuckets_and_t"</span>,sbuckets_and_t.shape, sbuckets_and_t)</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> verbose: <span class="bu">print</span>(<span class="st">"sticker"</span>,sticker.shape, sticker)</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Step 4</span></span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>    _, undo_sort <span class="op">=</span> fastmath.sort_key_val(sticker, ticker, dimension<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> verbose: <span class="bu">print</span>(<span class="st">"undo_sort"</span>,undo_sort.shape, undo_sort)</span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Step 4</span></span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a>    st <span class="op">=</span> (sticker <span class="op">%</span> seqlen)</span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a>    sq <span class="op">=</span> np.take(q, st, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>    sv <span class="op">=</span> np.take(v, st, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sq, sv, sticker, undo_sort</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="056c925f" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>t_n_hashes <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>t_n_buckets <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>t_n_seq <span class="op">=</span> t_seqlen <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>t_n_q <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>n_v <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>t_q <span class="op">=</span> (np.array([(j <span class="op">%</span> t_n_buckets) <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(t_n_seq)]) <span class="op">*</span> np.ones((t_n_q, <span class="dv">1</span>))).T</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>t_v <span class="op">=</span> np.ones((t_n_seq, n_v))</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>t_buckets <span class="op">=</span> np.array(</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>    [</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>        (j <span class="op">%</span> t_n_buckets) <span class="op">+</span> t_n_buckets <span class="op">*</span> i</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(t_n_hashes)</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(t_n_seq)</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"q</span><span class="ch">\n</span><span class="st">"</span>, t_q)</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"t_buckets: "</span>, t_buckets)</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>t_sq, t_sv, t_sticker, t_undo_sort <span class="op">=</span> sort_buckets(</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>    t_buckets, t_q, t_v, t_n_buckets, t_n_hashes, t_seqlen, verbose<span class="op">=</span><span class="va">True</span></span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"sq.shape"</span>, t_sq.shape, <span class="st">"sv.shape"</span>, t_sv.shape)</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"sq</span><span class="ch">\n</span><span class="st">"</span>, t_sq)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>q
 [[0. 0. 0.]
 [1. 1. 1.]
 [2. 2. 2.]
 [3. 3. 3.]
 [0. 0. 0.]
 [1. 1. 1.]
 [2. 2. 2.]
 [3. 3. 3.]]
t_buckets:  [0 1 2 3 0 1 2 3 4 5 6 7 4 5 6 7]
---sort_buckets--
ticker (16,) [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]
buckets_and_t (16,) [ 0  9 18 27  4 13 22 31 32 41 50 59 36 45 54 63]
sbuckets_and_t (16,) [ 0  4  9 13 18 22 27 31 32 36 41 45 50 54 59 63]
sticker (16,) [ 0  4  1  5  2  6  3  7  8 12  9 13 10 14 11 15]
undo_sort (16,) [ 0  2  4  6  1  3  5  7  8 10 12 14  9 11 13 15]
sq.shape (16, 3) sv.shape (16, 5)
sq
 [[0. 0. 0.]
 [0. 0. 0.]
 [1. 1. 1.]
 [1. 1. 1.]
 [2. 2. 2.]
 [2. 2. 2.]
 [3. 3. 3.]
 [3. 3. 3.]
 [0. 0. 0.]
 [0. 0. 0.]
 [1. 1. 1.]
 [1. 1. 1.]
 [2. 2. 2.]
 [2. 2. 2.]
 [3. 3. 3.]
 [3. 3. 3.]]</code></pre>
</div>
</div>
<details>
<summary>
<font size="3"><b> Expected Output </b></font>
</summary>
<p><strong>Expected Values</strong></p>
<pre><code>q
 [[0. 0. 0.]
 [1. 1. 1.]
 [2. 2. 2.]
 [3. 3. 3.]
 [0. 0. 0.]
 [1. 1. 1.]
 [2. 2. 2.]
 [3. 3. 3.]]
t_buckets:  [0 1 2 3 0 1 2 3 4 5 6 7 4 5 6 7]
---sort_buckets--
ticker (16,) [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]
buckets_and_t (16,) [ 0  9 18 27  4 13 22 31 32 41 50 59 36 45 54 63]
sbuckets_and_t (16,) [ 0  4  9 13 18 22 27 31 32 36 41 45 50 54 59 63]
sticker (16,) [ 0  4  1  5  2  6  3  7  8 12  9 13 10 14 11 15]
undo_sort (16,) [ 0  2  4  6  1  3  5  7  8 10 12 14  9 11 13 15]
sq.shape (16, 3) sv.shape (16, 5)
sq
 [[0. 0. 0.]
 [0. 0. 0.]
 [1. 1. 1.]
 [1. 1. 1.]
 [2. 2. 2.]
 [2. 2. 2.]
 [3. 3. 3.]
 [3. 3. 3.]
 [0. 0. 0.]
 [0. 0. 0.]
 [1. 1. 1.]
 [1. 1. 1.]
 [2. 2. 2.]
 [2. 2. 2.]
 [3. 3. 3.]
 [3. 3. 3.]]
</code></pre>
<details>
<summary>
<font size="3"><b>Completed code for reference </b></font>
</summary>
<pre><code># since this notebook is ungraded the completed code is provided here for reference
def sort_buckets(buckets, q, v, n_buckets, n_hashes, seqlen, verbose=True):
    """
  Args:
    buckets: tensor of at least 2 dimension,
    n_buckets: number of buckets in each hash table
    n_hashes: the number of hash tables
    """
    if verbose: print("---sort_buckets--")
    ## Step 1
    ticker = np.arange(n_hashes * seqlen)
    if verbose: print("ticker",ticker.shape, ticker)
    ## Step 2
    buckets_and_t = seqlen * buckets + (ticker % seqlen)
    if verbose: print("buckets_and_t",buckets_and_t.shape, buckets_and_t)

    # Hash-based sort ("s" at the start of variable names means "sorted")
    #Step 3
    sbuckets_and_t, sticker = fastmath.sort_key_val(
    buckets_and_t, ticker, dimension=-1)
    if verbose: print("sbuckets_and_t",sbuckets_and_t.shape, sbuckets_and_t)
    if verbose: print("sticker",sticker.shape, sticker)
    #Step 4
    _, undo_sort = fastmath.sort_key_val(sticker, ticker, dimension=-1)
    if verbose: print("undo_sort",undo_sort.shape, undo_sort)

    #Step 4
    st = (sticker % seqlen)
    sq = np.take(q, st, axis=0)
    sv = np.take(v, st, axis=0)
    return sq, sv, sticker, undo_sort</code></pre>
<p><a name="3.4"></a> ## Part 3.4 Chunked dot product attention</p>
<p>Now let’s create the dot product attention. We have sorted <span class="math inline">Q</span> so that elements that the hash has determined are likely to be similar are adjacent to each other. We now want to perform the dot-product within those limited regions - in ‘chunks’.</p>
<img src="C4W4_LN2_image12.PNG" height="400" width="750">
<center>
<b>Figure 11: Performing dot product in ‘chunks’ </b>
</center>
<p>The example we have been working on is shown above, with sequences of 8, 2 hashes, 4 buckets and, conveniently, the content of Q was such that when sorted, there were 2 entries in each bucket. If we reshape Q into a (8,2,n_q), we can use numpy matmul to perform the operation. Numpy <a href="https://numpy.org/doc/stable/reference/generated/numpy.matmul.html">matmul</a> will treat the inputs as a stack of matrices residing in the last two indexes. This will allow us to matrix multiply Q with itself in <em>chunks</em> and later can also be used to perform the matrix multiply with v.</p>
<p>We will perform a softmax on the output of the dot product of Q and Q, but in this case, there is a bit more to the story. Recall the output of the hash had multiple hash tables. We will perform softmax on those separately and then must combine them. This is where the form of softmax we defined at the top of the notebook comes into play. The routines below will utilize the logsumexp values that the <code>our_softmax</code> routine calculates.</p>
<p>There is a good deal of <a href="https://numpy.org/doc/stable/reference/generated/numpy.reshape.html">reshaping</a> to get things into the right formats. The code has many print statements that match the expected values below. You can use those to check your work as you go along. If you don’t do a lot of 3-dimensional matrix multiplications in your daily life, it might be worthwhile to open a spare cell and practice a few simple examples to get the hang of it! Here is one to start with:</p>
<div id="4411522c" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> np.arange(<span class="dv">16</span> <span class="op">*</span> <span class="dv">3</span>).reshape((<span class="dv">16</span>, <span class="dv">3</span>))</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>chunksize <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>ar <span class="op">=</span> np.reshape(</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>    a, (<span class="op">-</span><span class="dv">1</span>, chunksize, a.shape[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>)  <span class="co"># the -1 usage is very handy, see numpy reshape</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(ar.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(8, 2, 3)</code></pre>
</div>
</div>
<p><strong>Instructions</strong> <strong>Step 1</strong> Reshaping Q * np.reshape <code>sq</code> (sorted q) to be 3 dimensions. The middle dimension is the size of the ‘chunk’ specified by <code>kv_chunk_len</code> * np.swapaxes to perform a ‘transpose’ on the reshaped <code>sq</code>, <em>but only on the last two dimension</em> * np.matmul the two values.</p>
<p><strong>Step 2</strong> * use our_softmax to perform the softmax on the dot product. Don’t forget <code>passthrough</code></p>
<p><strong>Step 3</strong> * np.reshape <code>sv</code>. Like <code>sq</code>, the middle dimension is the size of the ‘chunk’ specified by <code>kv_chunk_len</code> * np.matmul dotlike and the reshaped <code>sv</code> * np.reshape so to a two dimensional array with the last dimension stays the same (<code>so.shape[-1]</code>) * <code>logits</code> also needs reshaping, we’ll do that.</p>
<p><strong>Step 4</strong> Now we can undo the sort. * use <a href="https://numpy.org/doc/stable/reference/generated/numpy.take.html">np.take</a> and <code>undo_sort</code> and axis = 0 to unsort so * do the same with <code>slogits</code>.</p>
<p><strong>Step 5</strong> This step combines the results of multiple hashes. Recall, the softmax was only over the values in one hash, this extends it to all the hashes. Read through it, the code is provided. Note this is taking place <em>after</em> the matrix multiply with v while the softmax output is used before the multiply. How does this achieve the correct result?</p>
<div id="c7f65ebd" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dotandv(sq, sv, undo_sort, kv_chunk_len, n_hashes, seqlen, passthrough, verbose<span class="op">=</span><span class="va">False</span> ):</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>    rsq <span class="op">=</span> np.reshape(sq,(<span class="op">-</span><span class="dv">1</span>, kv_chunk_len, sq.shape[<span class="op">-</span><span class="dv">1</span>]))</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    rsqt <span class="op">=</span>  np.swapaxes(rsq, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">2</span>)</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> verbose: <span class="bu">print</span>(<span class="st">"rsq.shape,rsqt.shape: "</span>, rsq.shape,rsqt.shape)</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>    dotlike <span class="op">=</span> np.matmul(rsq, rsqt)</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> verbose: <span class="bu">print</span>(<span class="st">"dotlike</span><span class="ch">\n</span><span class="st">"</span>, dotlike)</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Step 2</span></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>    dotlike, slogits <span class="op">=</span> our_softmax(dotlike, passthrough)</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> verbose: <span class="bu">print</span>(<span class="st">"dotlike post softmax</span><span class="ch">\n</span><span class="st">"</span>, dotlike)</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Step 3</span></span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>    vr <span class="op">=</span> np.reshape(sv, (<span class="op">-</span><span class="dv">1</span>, kv_chunk_len, sv.shape[<span class="op">-</span><span class="dv">1</span>]))</span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> verbose:  <span class="bu">print</span>(<span class="st">"dotlike.shape, vr.shape:"</span>, dotlike.shape, vr.shape)</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>    so <span class="op">=</span> np.matmul(dotlike, vr)</span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> verbose: <span class="bu">print</span>(<span class="st">"so.shape:"</span>, so.shape)</span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>    so <span class="op">=</span> np.reshape(so, (<span class="op">-</span><span class="dv">1</span>, so.shape[<span class="op">-</span><span class="dv">1</span>]))</span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>    slogits <span class="op">=</span> np.reshape(slogits, (<span class="op">-</span><span class="dv">1</span>,))  <span class="co"># provided</span></span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> verbose: <span class="bu">print</span>(<span class="st">"so.shape,slogits.shape"</span>, so.shape, slogits.shape)</span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Step 4</span></span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a>    o <span class="op">=</span> np.take(so, undo_sort, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> np.take(slogits, undo_sort, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb29-25"><a href="#cb29-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> verbose: <span class="bu">print</span>(<span class="st">"o.shape,o"</span>, o.shape, o)</span>
<span id="cb29-26"><a href="#cb29-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> verbose: <span class="bu">print</span>(<span class="st">"logits.shape, logits"</span>, logits.shape, logits)</span>
<span id="cb29-27"><a href="#cb29-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-28"><a href="#cb29-28" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Step 5 (Provided)</span></span>
<span id="cb29-29"><a href="#cb29-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> n_hashes <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb29-30"><a href="#cb29-30" aria-hidden="true" tabindex="-1"></a>      o <span class="op">=</span> np.reshape(o, (n_hashes, seqlen, o.shape[<span class="op">-</span><span class="dv">1</span>]))</span>
<span id="cb29-31"><a href="#cb29-31" aria-hidden="true" tabindex="-1"></a>      logits <span class="op">=</span> np.reshape(logits, (n_hashes, seqlen, <span class="dv">1</span>))</span>
<span id="cb29-32"><a href="#cb29-32" aria-hidden="true" tabindex="-1"></a>      probs <span class="op">=</span> np.exp(logits <span class="op">-</span> fastmath.logsumexp(logits, axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb29-33"><a href="#cb29-33" aria-hidden="true" tabindex="-1"></a>      o <span class="op">=</span> np.<span class="bu">sum</span>(o <span class="op">*</span> probs, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb29-34"><a href="#cb29-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-35"><a href="#cb29-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>(o)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="d4b565e3" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>t_kv_chunk_len <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> dotandv(</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>    t_sq,</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>    t_sv,</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>    t_undo_sort,</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>    t_kv_chunk_len,</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>    t_n_hashes,</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>    t_seqlen,</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>    passthrough<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"out</span><span class="ch">\n</span><span class="st">"</span>, out)</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">-----With softmax enabled----</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> dotandv(</span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>    t_sq,</span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>    t_sv,</span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a>    t_undo_sort,</span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a>    t_kv_chunk_len,</span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>    t_n_hashes,</span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>    t_seqlen,</span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a>    passthrough<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"out</span><span class="ch">\n</span><span class="st">"</span>, out)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>rsq.shape,rsqt.shape:  (8, 2, 3) (8, 3, 2)
dotlike
 [[[ 0.  0.]
  [ 0.  0.]]

 [[ 3.  3.]
  [ 3.  3.]]

 [[12. 12.]
  [12. 12.]]

 [[27. 27.]
  [27. 27.]]

 [[ 0.  0.]
  [ 0.  0.]]

 [[ 3.  3.]
  [ 3.  3.]]

 [[12. 12.]
  [12. 12.]]

 [[27. 27.]
  [27. 27.]]]
dotlike post softmax
 [[[ 0.  0.]
  [ 0.  0.]]

 [[ 3.  3.]
  [ 3.  3.]]

 [[12. 12.]
  [12. 12.]]

 [[27. 27.]
  [27. 27.]]

 [[ 0.  0.]
  [ 0.  0.]]

 [[ 3.  3.]
  [ 3.  3.]]

 [[12. 12.]
  [12. 12.]]

 [[27. 27.]
  [27. 27.]]]
dotlike.shape, vr.shape: (8, 2, 2) (8, 2, 5)
so.shape: (8, 2, 5)
so.shape,slogits.shape (16, 5) (16,)
o.shape,o (16, 5) [[ 0.  0.  0.  0.  0.]
 [ 6.  6.  6.  6.  6.]
 [24. 24. 24. 24. 24.]
 [54. 54. 54. 54. 54.]
 [ 0.  0.  0.  0.  0.]
 [ 6.  6.  6.  6.  6.]
 [24. 24. 24. 24. 24.]
 [54. 54. 54. 54. 54.]
 [ 0.  0.  0.  0.  0.]
 [ 6.  6.  6.  6.  6.]
 [24. 24. 24. 24. 24.]
 [54. 54. 54. 54. 54.]
 [ 0.  0.  0.  0.  0.]
 [ 6.  6.  6.  6.  6.]
 [24. 24. 24. 24. 24.]
 [54. 54. 54. 54. 54.]]
logits.shape, logits (16,) [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
out
 [[ 0.  0.  0.  0.  0.]
 [ 6.  6.  6.  6.  6.]
 [24. 24. 24. 24. 24.]
 [54. 54. 54. 54. 54.]
 [ 0.  0.  0.  0.  0.]
 [ 6.  6.  6.  6.  6.]
 [24. 24. 24. 24. 24.]
 [54. 54. 54. 54. 54.]]

-----With softmax enabled----

rsq.shape,rsqt.shape:  (8, 2, 3) (8, 3, 2)
dotlike
 [[[ 0.  0.]
  [ 0.  0.]]

 [[ 3.  3.]
  [ 3.  3.]]

 [[12. 12.]
  [12. 12.]]

 [[27. 27.]
  [27. 27.]]

 [[ 0.  0.]
  [ 0.  0.]]

 [[ 3.  3.]
  [ 3.  3.]]

 [[12. 12.]
  [12. 12.]]

 [[27. 27.]
  [27. 27.]]]
dotlike post softmax
 [[[0.5        0.5       ]
  [0.5        0.5       ]]

 [[0.5        0.5       ]
  [0.5        0.5       ]]

 [[0.49999976 0.49999976]
  [0.49999976 0.49999976]]

 [[0.49999976 0.49999976]
  [0.49999976 0.49999976]]

 [[0.5        0.5       ]
  [0.5        0.5       ]]

 [[0.5        0.5       ]
  [0.5        0.5       ]]

 [[0.49999976 0.49999976]
  [0.49999976 0.49999976]]

 [[0.49999976 0.49999976]
  [0.49999976 0.49999976]]]
dotlike.shape, vr.shape: (8, 2, 2) (8, 2, 5)
so.shape: (8, 2, 5)
so.shape,slogits.shape (16, 5) (16,)
o.shape,o (16, 5) [[1.        1.        1.        1.        1.       ]
 [1.        1.        1.        1.        1.       ]
 [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]
 [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]
 [1.        1.        1.        1.        1.       ]
 [1.        1.        1.        1.        1.       ]
 [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]
 [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]
 [1.        1.        1.        1.        1.       ]
 [1.        1.        1.        1.        1.       ]
 [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]
 [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]
 [1.        1.        1.        1.        1.       ]
 [1.        1.        1.        1.        1.       ]
 [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]
 [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]]
logits.shape, logits (16,) [ 0.6931472  3.6931472 12.693148  27.693148   0.6931472  3.6931472
 12.693148  27.693148   0.6931472  3.6931472 12.693148  27.693148
  0.6931472  3.6931472 12.693148  27.693148 ]
out
 [[1.         1.         1.         1.         1.        ]
 [1.         1.         1.         1.         1.        ]
 [0.99999905 0.99999905 0.99999905 0.99999905 0.99999905]
 [0.99999905 0.99999905 0.99999905 0.99999905 0.99999905]
 [1.         1.         1.         1.         1.        ]
 [1.         1.         1.         1.         1.        ]
 [0.99999905 0.99999905 0.99999905 0.99999905 0.99999905]
 [0.99999905 0.99999905 0.99999905 0.99999905 0.99999905]]</code></pre>
</div>
</div>
<details>
<summary>
<font size="3"><b> Expected Output </b></font>
</summary>
<p><strong>Expected Values</strong></p>
<pre><code>rsq.shape,rsqt.shape:  (8, 2, 3) (8, 3, 2)
dotlike
 [[[ 0.  0.]
  [ 0.  0.]]

 [[ 3.  3.]
  [ 3.  3.]]

 [[12. 12.]
  [12. 12.]]

 [[27. 27.]
  [27. 27.]]

 [[ 0.  0.]
  [ 0.  0.]]

 [[ 3.  3.]
  [ 3.  3.]]

 [[12. 12.]
  [12. 12.]]

 [[27. 27.]
  [27. 27.]]]
dotlike post softmax
 [[[ 0.  0.]
  [ 0.  0.]]

 [[ 3.  3.]
  [ 3.  3.]]

 [[12. 12.]
  [12. 12.]]

 [[27. 27.]
  [27. 27.]]

 [[ 0.  0.]
  [ 0.  0.]]

 [[ 3.  3.]
  [ 3.  3.]]

 [[12. 12.]
  [12. 12.]]

 [[27. 27.]
  [27. 27.]]]
dotlike.shape, vr.shape: (8, 2, 2) (8, 2, 5)
so.shape: (8, 2, 5)
so.shape,slogits.shape (16, 5) (16,)
o.shape,o (16, 5) [[ 0.  0.  0.  0.  0.]
 [ 6.  6.  6.  6.  6.]
 [24. 24. 24. 24. 24.]
 [54. 54. 54. 54. 54.]
 [ 0.  0.  0.  0.  0.]
 [ 6.  6.  6.  6.  6.]
 [24. 24. 24. 24. 24.]
 [54. 54. 54. 54. 54.]
 [ 0.  0.  0.  0.  0.]
 [ 6.  6.  6.  6.  6.]
 [24. 24. 24. 24. 24.]
 [54. 54. 54. 54. 54.]
 [ 0.  0.  0.  0.  0.]
 [ 6.  6.  6.  6.  6.]
 [24. 24. 24. 24. 24.]
 [54. 54. 54. 54. 54.]]
logits.shape, logits (16,) [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
out
 [[ 0.  0.  0.  0.  0.]
 [ 6.  6.  6.  6.  6.]
 [24. 24. 24. 24. 24.]
 [54. 54. 54. 54. 54.]
 [ 0.  0.  0.  0.  0.]
 [ 6.  6.  6.  6.  6.]
 [24. 24. 24. 24. 24.]
 [54. 54. 54. 54. 54.]]

-----With softmax enabled----

rsq.shape,rsqt.shape:  (8, 2, 3) (8, 3, 2)
dotlike
 [[[ 0.  0.]
  [ 0.  0.]]

 [[ 3.  3.]
  [ 3.  3.]]

 [[12. 12.]
  [12. 12.]]

 [[27. 27.]
  [27. 27.]]

 [[ 0.  0.]
  [ 0.  0.]]

 [[ 3.  3.]
  [ 3.  3.]]

 [[12. 12.]
  [12. 12.]]

 [[27. 27.]
  [27. 27.]]]
dotlike post softmax
 [[[0.5        0.5       ]
  [0.5        0.5       ]]

 [[0.5        0.5       ]
  [0.5        0.5       ]]

 [[0.49999976 0.49999976]
  [0.49999976 0.49999976]]

 [[0.49999976 0.49999976]
  [0.49999976 0.49999976]]

 [[0.5        0.5       ]
  [0.5        0.5       ]]

 [[0.5        0.5       ]
  [0.5        0.5       ]]

 [[0.49999976 0.49999976]
  [0.49999976 0.49999976]]

 [[0.49999976 0.49999976]
  [0.49999976 0.49999976]]]
dotlike.shape, vr.shape: (8, 2, 2) (8, 2, 5)
so.shape: (8, 2, 5)
so.shape,slogits.shape (16, 5) (16,)
o.shape,o (16, 5) [[1.        1.        1.        1.        1.       ]
 [1.        1.        1.        1.        1.       ]
 [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]
 [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]
 [1.        1.        1.        1.        1.       ]
 [1.        1.        1.        1.        1.       ]
 [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]
 [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]
 [1.        1.        1.        1.        1.       ]
 [1.        1.        1.        1.        1.       ]
 [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]
 [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]
 [1.        1.        1.        1.        1.       ]
 [1.        1.        1.        1.        1.       ]
 [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]
 [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]]
logits.shape, logits (16,) [ 0.6931472  3.6931472 12.693148  27.693148   0.6931472  3.6931472
 12.693148  27.693148   0.6931472  3.6931472 12.693148  27.693148
  0.6931472  3.6931472 12.693148  27.693148 ]
out
 [[1.         1.         1.         1.         1.        ]
 [1.         1.         1.         1.         1.        ]
 [0.99999905 0.99999905 0.99999905 0.99999905 0.99999905]
 [0.99999905 0.99999905 0.99999905 0.99999905 0.99999905]
 [1.         1.         1.         1.         1.        ]
 [1.         1.         1.         1.         1.        ]
 [0.99999905 0.99999905 0.99999905 0.99999905 0.99999905]
 [0.99999905 0.99999905 0.99999905 0.99999905 0.99999905]]</code></pre>
<details>
<summary>
<font size="3"><b>Completed code for reference </b></font>
</summary>
<pre><code># since this notebook is ungraded the completed code is provided here for reference
def dotandv(sq, sv, undo_sort, kv_chunk_len, n_hashes, seqlen, passthrough, verbose=False ):
    # Step 1
    rsq = np.reshape(sq,(-1, kv_chunk_len, sq.shape[-1]))
    rsqt =  np.swapaxes(rsq, -1, -2)
    if verbose: print("rsq.shape,rsqt.shape: ", rsq.shape,rsqt.shape)
    dotlike = np.matmul(rsq, rsqt)
    if verbose: print("dotlike\n", dotlike)

    #Step 2
    dotlike, slogits = our_softmax(dotlike, passthrough)
    if verbose: print("dotlike post softmax\n", dotlike)

    #Step 3
    vr = np.reshape(sv, (-1, kv_chunk_len, sv.shape[-1]))
    if verbose:  print("dotlike.shape, vr.shape:", dotlike.shape, vr.shape)
    so = np.matmul(dotlike, vr)
    if verbose: print("so.shape:", so.shape)
    so = np.reshape(so, (-1, so.shape[-1]))
    slogits = np.reshape(slogits, (-1,))  # provided
    if verbose: print("so.shape,slogits.shape", so.shape, slogits.shape)

    #Step 4
    o = np.take(so, undo_sort, axis=0)
    logits = np.take(slogits, undo_sort, axis=0)
    if verbose: print("o.shape,o", o.shape, o)
    if verbose: print("logits.shape, logits", logits.shape, logits)

    #Step 5 (Provided)
    if n_hashes &gt; 1:
      o = np.reshape(o, (n_hashes, seqlen, o.shape[-1]))
      logits = np.reshape(logits, (n_hashes, seqlen, 1))
      probs = np.exp(logits - fastmath.logsumexp(logits, axis=0, keepdims=True))
      o = np.sum(o * probs, axis=0)

    return(o)</code></pre>
</details>
<p>Great! You have now done examples code for most of the operation that are unique to the LSH version of self-attention. I’m sure at this point you are wondering what happens if the number of entries in a bucket is not evenly distributed the way our example is. It is possible, for example for all of the <code>seqlen</code> entries to land in one bucket. Further, since the buckets are not aligned, our ‘chunks’ may be misaligned with the start of the bucket. The implementation addresses this by attending to adjacent chunks as was described in the lecture:</p>
<div id="fig-12" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-12-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/C4W4_LN2_image13.PNG" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Figure&nbsp;9: Misaligned Access, looking before and after"><img src="img/C4W4_LN2_image13.PNG" width="750" height="400" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-12-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Misaligned Access, looking before and after
</figcaption>
</figure>
</div>
<p>Hopefully, having implemented parts of this, you will appreciate this diagram more fully.</p>
<section id="3.5" class="level2">
<h2 class="anchored" data-anchor-id="3.5">Part 3.5 OurLSHSelfAttention</h2>
<p>You can examine the full implementations below. Area’s we did not ‘attend to’ in our implementations above include variable bucket sizes and masking. We will instantiate a layer of the full implementation below. We tried to use the same variable names above to make it easier to decipher the full version. Note that some of the functionality we implemented in our routines is split between <code>attend</code> and <code>forward_unbatched</code>. We’ve inserted our version of hash below, but use the original version of <code>attend</code>.</p>
<div id="7e08f952" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># original version from trax 1.3.4</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> attend(</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>    q,</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>    k<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>    v<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>    q_chunk_len<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>    kv_chunk_len<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>    n_chunks_before<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>    n_chunks_after<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>    mask_fn<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>    q_info<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>    kv_info<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>    dropout<span class="op">=</span><span class="fl">0.0</span>,</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>    rng<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Dot-product attention, with optional chunking and/or masking.</span></span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a><span class="co">  Args:</span></span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a><span class="co">    q: Query vectors, shape [q_len, d_qk]</span></span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a><span class="co">    k: Key vectors, shape [kv_len, d_qk]; or None</span></span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a><span class="co">    v: Value vectors, shape [kv_len, d_v]</span></span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a><span class="co">    q_chunk_len: Set to non-zero to enable chunking for query vectors</span></span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a><span class="co">    kv_chunk_len: Set to non-zero to enable chunking for key/value vectors</span></span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a><span class="co">    n_chunks_before: Number of adjacent previous chunks to attend to</span></span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a><span class="co">    n_chunks_after: Number of adjacent subsequent chunks to attend to</span></span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a><span class="co">    mask_fn: </span><span class="al">TODO</span><span class="co">(kitaev) doc</span></span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a><span class="co">    q_info: Query-associated metadata for masking</span></span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a><span class="co">    kv_info: Key-associated metadata for masking</span></span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a><span class="co">    dropout: Dropout rate</span></span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a><span class="co">    rng: RNG for dropout</span></span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a><span class="co">  Returns:</span></span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a><span class="co">    A tuple (output, dots_logsumexp). The output has shape [q_len, d_v], and</span></span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a><span class="co">    dots_logsumexp has shape [q_len]. The logsumexp of the attention</span></span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a><span class="co">    probabilities is useful for combining multiple rounds of attention (as in</span></span>
<span id="cb34-36"><a href="#cb34-36" aria-hidden="true" tabindex="-1"></a><span class="co">    LSH attention).</span></span>
<span id="cb34-37"><a href="#cb34-37" aria-hidden="true" tabindex="-1"></a><span class="co">  """</span></span>
<span id="cb34-38"><a href="#cb34-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> v <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span></span>
<span id="cb34-39"><a href="#cb34-39" aria-hidden="true" tabindex="-1"></a>    share_qk <span class="op">=</span> k <span class="kw">is</span> <span class="va">None</span></span>
<span id="cb34-40"><a href="#cb34-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-41"><a href="#cb34-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> q_info <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb34-42"><a href="#cb34-42" aria-hidden="true" tabindex="-1"></a>        q_info <span class="op">=</span> np.arange(q.shape[<span class="op">-</span><span class="dv">2</span>], dtype<span class="op">=</span>np.int32)</span>
<span id="cb34-43"><a href="#cb34-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-44"><a href="#cb34-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> kv_info <span class="kw">is</span> <span class="va">None</span> <span class="kw">and</span> <span class="kw">not</span> share_qk:</span>
<span id="cb34-45"><a href="#cb34-45" aria-hidden="true" tabindex="-1"></a>        kv_info <span class="op">=</span> np.arange(v.shape[<span class="op">-</span><span class="dv">2</span>], dtype<span class="op">=</span>np.int32)</span>
<span id="cb34-46"><a href="#cb34-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-47"><a href="#cb34-47" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Split q/k/v into chunks along the time axis, if desired.</span></span>
<span id="cb34-48"><a href="#cb34-48" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> q_chunk_len <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb34-49"><a href="#cb34-49" aria-hidden="true" tabindex="-1"></a>        q <span class="op">=</span> np.reshape(q, (<span class="op">-</span><span class="dv">1</span>, q_chunk_len, q.shape[<span class="op">-</span><span class="dv">1</span>]))</span>
<span id="cb34-50"><a href="#cb34-50" aria-hidden="true" tabindex="-1"></a>        q_info <span class="op">=</span> np.reshape(q_info, (<span class="op">-</span><span class="dv">1</span>, q_chunk_len))</span>
<span id="cb34-51"><a href="#cb34-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-52"><a href="#cb34-52" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> share_qk:</span>
<span id="cb34-53"><a href="#cb34-53" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> kv_chunk_len <span class="kw">is</span> <span class="va">None</span> <span class="kw">or</span> kv_chunk_len <span class="op">==</span> q_chunk_len</span>
<span id="cb34-54"><a href="#cb34-54" aria-hidden="true" tabindex="-1"></a>        k <span class="op">=</span> q</span>
<span id="cb34-55"><a href="#cb34-55" aria-hidden="true" tabindex="-1"></a>        kv_chunk_len <span class="op">=</span> q_chunk_len</span>
<span id="cb34-56"><a href="#cb34-56" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> kv_info <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb34-57"><a href="#cb34-57" aria-hidden="true" tabindex="-1"></a>            kv_info <span class="op">=</span> q_info</span>
<span id="cb34-58"><a href="#cb34-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> kv_chunk_len <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb34-59"><a href="#cb34-59" aria-hidden="true" tabindex="-1"></a>            <span class="co"># kv_info is not None, but reshape as required.</span></span>
<span id="cb34-60"><a href="#cb34-60" aria-hidden="true" tabindex="-1"></a>            kv_info <span class="op">=</span> np.reshape(kv_info, (<span class="op">-</span><span class="dv">1</span>, kv_chunk_len))</span>
<span id="cb34-61"><a href="#cb34-61" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> kv_chunk_len <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb34-62"><a href="#cb34-62" aria-hidden="true" tabindex="-1"></a>        k <span class="op">=</span> np.reshape(k, (<span class="op">-</span><span class="dv">1</span>, kv_chunk_len, k.shape[<span class="op">-</span><span class="dv">1</span>]))</span>
<span id="cb34-63"><a href="#cb34-63" aria-hidden="true" tabindex="-1"></a>        kv_info <span class="op">=</span> np.reshape(kv_info, (<span class="op">-</span><span class="dv">1</span>, kv_chunk_len))</span>
<span id="cb34-64"><a href="#cb34-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-65"><a href="#cb34-65" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> kv_chunk_len <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb34-66"><a href="#cb34-66" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> np.reshape(v, (<span class="op">-</span><span class="dv">1</span>, kv_chunk_len, v.shape[<span class="op">-</span><span class="dv">1</span>]))</span>
<span id="cb34-67"><a href="#cb34-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-68"><a href="#cb34-68" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> share_qk:</span>
<span id="cb34-69"><a href="#cb34-69" aria-hidden="true" tabindex="-1"></a>        k <span class="op">=</span> length_normalized(k)</span>
<span id="cb34-70"><a href="#cb34-70" aria-hidden="true" tabindex="-1"></a>    k <span class="op">=</span> k <span class="op">/</span> np.sqrt(k.shape[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb34-71"><a href="#cb34-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-72"><a href="#cb34-72" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Optionally include adjacent chunks.</span></span>
<span id="cb34-73"><a href="#cb34-73" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> q_chunk_len <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">or</span> kv_chunk_len <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb34-74"><a href="#cb34-74" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> q_chunk_len <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> kv_chunk_len <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span></span>
<span id="cb34-75"><a href="#cb34-75" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb34-76"><a href="#cb34-76" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> n_chunks_before <span class="op">==</span> <span class="dv">0</span> <span class="kw">and</span> n_chunks_after <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb34-77"><a href="#cb34-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-78"><a href="#cb34-78" aria-hidden="true" tabindex="-1"></a>    k <span class="op">=</span> look_adjacent(k, n_chunks_before, n_chunks_after)</span>
<span id="cb34-79"><a href="#cb34-79" aria-hidden="true" tabindex="-1"></a>    v <span class="op">=</span> look_adjacent(v, n_chunks_before, n_chunks_after)</span>
<span id="cb34-80"><a href="#cb34-80" aria-hidden="true" tabindex="-1"></a>    kv_info <span class="op">=</span> look_adjacent(kv_info, n_chunks_before, n_chunks_after)</span>
<span id="cb34-81"><a href="#cb34-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-82"><a href="#cb34-82" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Dot-product attention.</span></span>
<span id="cb34-83"><a href="#cb34-83" aria-hidden="true" tabindex="-1"></a>    dots <span class="op">=</span> np.matmul(q, np.swapaxes(k, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">2</span>))</span>
<span id="cb34-84"><a href="#cb34-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-85"><a href="#cb34-85" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Masking</span></span>
<span id="cb34-86"><a href="#cb34-86" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> mask_fn <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb34-87"><a href="#cb34-87" aria-hidden="true" tabindex="-1"></a>        dots <span class="op">=</span> mask_fn(dots, q_info[..., :, <span class="va">None</span>], kv_info[..., <span class="va">None</span>, :])</span>
<span id="cb34-88"><a href="#cb34-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-89"><a href="#cb34-89" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Softmax.</span></span>
<span id="cb34-90"><a href="#cb34-90" aria-hidden="true" tabindex="-1"></a>    dots_logsumexp <span class="op">=</span> fastmath.logsumexp(dots, axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb34-91"><a href="#cb34-91" aria-hidden="true" tabindex="-1"></a>    dots <span class="op">=</span> np.exp(dots <span class="op">-</span> dots_logsumexp)</span>
<span id="cb34-92"><a href="#cb34-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-93"><a href="#cb34-93" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> dropout <span class="op">&gt;</span> <span class="fl">0.0</span>:</span>
<span id="cb34-94"><a href="#cb34-94" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> rng <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span></span>
<span id="cb34-95"><a href="#cb34-95" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Dropout is broadcast across the bin dimension</span></span>
<span id="cb34-96"><a href="#cb34-96" aria-hidden="true" tabindex="-1"></a>        dropout_shape <span class="op">=</span> (dots.shape[<span class="op">-</span><span class="dv">2</span>], dots.shape[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb34-97"><a href="#cb34-97" aria-hidden="true" tabindex="-1"></a>        <span class="co">#</span></span>
<span id="cb34-98"><a href="#cb34-98" aria-hidden="true" tabindex="-1"></a>        keep_prob <span class="op">=</span> tie_in(dots, <span class="fl">1.0</span> <span class="op">-</span> dropout)</span>
<span id="cb34-99"><a href="#cb34-99" aria-hidden="true" tabindex="-1"></a>        keep <span class="op">=</span> fastmath.random.bernoulli(rng, keep_prob, dropout_shape)</span>
<span id="cb34-100"><a href="#cb34-100" aria-hidden="true" tabindex="-1"></a>        multiplier <span class="op">=</span> keep.astype(dots.dtype) <span class="op">/</span> tie_in(keep, keep_prob)</span>
<span id="cb34-101"><a href="#cb34-101" aria-hidden="true" tabindex="-1"></a>        dots <span class="op">=</span> dots <span class="op">*</span> multiplier</span>
<span id="cb34-102"><a href="#cb34-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-103"><a href="#cb34-103" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The softmax normalizer (dots_logsumexp) is used by multi-round LSH attn.</span></span>
<span id="cb34-104"><a href="#cb34-104" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> np.matmul(dots, v)</span>
<span id="cb34-105"><a href="#cb34-105" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> np.reshape(out, (<span class="op">-</span><span class="dv">1</span>, out.shape[<span class="op">-</span><span class="dv">1</span>]))</span>
<span id="cb34-106"><a href="#cb34-106" aria-hidden="true" tabindex="-1"></a>    dots_logsumexp <span class="op">=</span> np.reshape(dots_logsumexp, (<span class="op">-</span><span class="dv">1</span>,))</span>
<span id="cb34-107"><a href="#cb34-107" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> out, dots_logsumexp</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="dfb79e10" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> OurLSHSelfAttention(tl.LSHSelfAttention):</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Our simplified LSH self-attention """</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward_unbatched(<span class="va">self</span>, x, mask<span class="op">=</span><span class="va">None</span>, <span class="op">*</span>, weights, state, rng, update_state):</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>        attend_rng, output_rng <span class="op">=</span> fastmath.random.split(rng)</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>        w_q, w_v, w_o <span class="op">=</span> weights</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>        q <span class="op">=</span> np.matmul(x, w_q)</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> np.matmul(x, w_v)</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> update_state:</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>            _, old_hash_rng <span class="op">=</span> state</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>            hash_rng, hash_subrng <span class="op">=</span> fastmath.random.split(old_hash_rng)</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>            <span class="co">#      buckets = self.hash_vectors(q, hash_subrng, mask)  #  original</span></span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>            <span class="co">## use our version of hash</span></span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a>            buckets <span class="op">=</span> our_hash_vectors(</span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a>                q, hash_subrng, <span class="va">self</span>.n_buckets, <span class="va">self</span>.n_hashes, mask<span class="op">=</span>mask</span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a>            s_buckets <span class="op">=</span> buckets</span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>._max_length_for_buckets:</span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a>                length <span class="op">=</span> <span class="va">self</span>.n_hashes <span class="op">*</span> <span class="va">self</span>._max_length_for_buckets</span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> buckets.shape[<span class="dv">0</span>] <span class="op">&lt;</span> length:</span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a>                    s_buckets <span class="op">=</span> np.concatenate(</span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a>                        [buckets, np.zeros(length <span class="op">-</span> buckets.shape[<span class="dv">0</span>], dtype<span class="op">=</span>np.int32)],</span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a>                        axis<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb35-26"><a href="#cb35-26" aria-hidden="true" tabindex="-1"></a>                    )</span>
<span id="cb35-27"><a href="#cb35-27" aria-hidden="true" tabindex="-1"></a>            state <span class="op">=</span> (s_buckets, hash_rng)</span>
<span id="cb35-28"><a href="#cb35-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb35-29"><a href="#cb35-29" aria-hidden="true" tabindex="-1"></a>            buckets, _ <span class="op">=</span> state</span>
<span id="cb35-30"><a href="#cb35-30" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>._max_length_for_buckets:</span>
<span id="cb35-31"><a href="#cb35-31" aria-hidden="true" tabindex="-1"></a>                buckets <span class="op">=</span> buckets[: <span class="va">self</span>.n_hashes <span class="op">*</span> x.shape[<span class="dv">0</span>]]</span>
<span id="cb35-32"><a href="#cb35-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-33"><a href="#cb35-33" aria-hidden="true" tabindex="-1"></a>        seqlen <span class="op">=</span> x.shape[<span class="dv">0</span>]</span>
<span id="cb35-34"><a href="#cb35-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> <span class="bu">int</span>(buckets.shape[<span class="dv">0</span>]) <span class="op">==</span> <span class="va">self</span>.n_hashes <span class="op">*</span> seqlen</span>
<span id="cb35-35"><a href="#cb35-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-36"><a href="#cb35-36" aria-hidden="true" tabindex="-1"></a>        ticker <span class="op">=</span> tie_in(x, np.arange(<span class="va">self</span>.n_hashes <span class="op">*</span> seqlen, dtype<span class="op">=</span>np.int32))</span>
<span id="cb35-37"><a href="#cb35-37" aria-hidden="true" tabindex="-1"></a>        buckets_and_t <span class="op">=</span> seqlen <span class="op">*</span> buckets <span class="op">+</span> (ticker <span class="op">%</span> seqlen)</span>
<span id="cb35-38"><a href="#cb35-38" aria-hidden="true" tabindex="-1"></a>        buckets_and_t <span class="op">=</span> fastmath.stop_gradient(buckets_and_t)</span>
<span id="cb35-39"><a href="#cb35-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-40"><a href="#cb35-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hash-based sort ("s" at the start of variable names means "sorted")</span></span>
<span id="cb35-41"><a href="#cb35-41" aria-hidden="true" tabindex="-1"></a>        sbuckets_and_t, sticker <span class="op">=</span> fastmath.sort_key_val(</span>
<span id="cb35-42"><a href="#cb35-42" aria-hidden="true" tabindex="-1"></a>            buckets_and_t, ticker, dimension<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb35-43"><a href="#cb35-43" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb35-44"><a href="#cb35-44" aria-hidden="true" tabindex="-1"></a>        _, undo_sort <span class="op">=</span> fastmath.sort_key_val(sticker, ticker, dimension<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb35-45"><a href="#cb35-45" aria-hidden="true" tabindex="-1"></a>        sbuckets_and_t <span class="op">=</span> fastmath.stop_gradient(sbuckets_and_t)</span>
<span id="cb35-46"><a href="#cb35-46" aria-hidden="true" tabindex="-1"></a>        sticker <span class="op">=</span> fastmath.stop_gradient(sticker)</span>
<span id="cb35-47"><a href="#cb35-47" aria-hidden="true" tabindex="-1"></a>        undo_sort <span class="op">=</span> fastmath.stop_gradient(undo_sort)</span>
<span id="cb35-48"><a href="#cb35-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-49"><a href="#cb35-49" aria-hidden="true" tabindex="-1"></a>        st <span class="op">=</span> sticker <span class="op">%</span> seqlen</span>
<span id="cb35-50"><a href="#cb35-50" aria-hidden="true" tabindex="-1"></a>        sq <span class="op">=</span> np.take(q, st, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb35-51"><a href="#cb35-51" aria-hidden="true" tabindex="-1"></a>        sv <span class="op">=</span> np.take(v, st, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb35-52"><a href="#cb35-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-53"><a href="#cb35-53" aria-hidden="true" tabindex="-1"></a>        mask_fn <span class="op">=</span> functools.partial(</span>
<span id="cb35-54"><a href="#cb35-54" aria-hidden="true" tabindex="-1"></a>            mask_self_attention,</span>
<span id="cb35-55"><a href="#cb35-55" aria-hidden="true" tabindex="-1"></a>            causal<span class="op">=</span><span class="va">self</span>.causal,</span>
<span id="cb35-56"><a href="#cb35-56" aria-hidden="true" tabindex="-1"></a>            exclude_self<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb35-57"><a href="#cb35-57" aria-hidden="true" tabindex="-1"></a>            masked<span class="op">=</span><span class="va">self</span>.masked,</span>
<span id="cb35-58"><a href="#cb35-58" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb35-59"><a href="#cb35-59" aria-hidden="true" tabindex="-1"></a>        q_info <span class="op">=</span> st</span>
<span id="cb35-60"><a href="#cb35-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-61"><a href="#cb35-61" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> (mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>) <span class="op">==</span> <span class="va">self</span>.masked</span>
<span id="cb35-62"><a href="#cb35-62" aria-hidden="true" tabindex="-1"></a>        kv_info <span class="op">=</span> <span class="va">None</span></span>
<span id="cb35-63"><a href="#cb35-63" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.masked:</span>
<span id="cb35-64"><a href="#cb35-64" aria-hidden="true" tabindex="-1"></a>            <span class="co"># mask is a boolean array (True means "is valid token")</span></span>
<span id="cb35-65"><a href="#cb35-65" aria-hidden="true" tabindex="-1"></a>            smask <span class="op">=</span> np.take(mask, st, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb35-66"><a href="#cb35-66" aria-hidden="true" tabindex="-1"></a>            ones_like_mask <span class="op">=</span> tie_in(x, np.ones_like(smask, dtype<span class="op">=</span>np.int32))</span>
<span id="cb35-67"><a href="#cb35-67" aria-hidden="true" tabindex="-1"></a>            kv_info <span class="op">=</span> q_info <span class="op">*</span> np.where(smask, ones_like_mask, <span class="op">-</span>ones_like_mask)</span>
<span id="cb35-68"><a href="#cb35-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-69"><a href="#cb35-69" aria-hidden="true" tabindex="-1"></a>        <span class="co">## use original version of attend (could use ours but lacks masks and masking)</span></span>
<span id="cb35-70"><a href="#cb35-70" aria-hidden="true" tabindex="-1"></a>        so, slogits <span class="op">=</span> attend(</span>
<span id="cb35-71"><a href="#cb35-71" aria-hidden="true" tabindex="-1"></a>            sq,</span>
<span id="cb35-72"><a href="#cb35-72" aria-hidden="true" tabindex="-1"></a>            k<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb35-73"><a href="#cb35-73" aria-hidden="true" tabindex="-1"></a>            v<span class="op">=</span>sv,</span>
<span id="cb35-74"><a href="#cb35-74" aria-hidden="true" tabindex="-1"></a>            q_chunk_len<span class="op">=</span><span class="va">self</span>.chunk_len,</span>
<span id="cb35-75"><a href="#cb35-75" aria-hidden="true" tabindex="-1"></a>            n_chunks_before<span class="op">=</span><span class="va">self</span>.n_chunks_before,</span>
<span id="cb35-76"><a href="#cb35-76" aria-hidden="true" tabindex="-1"></a>            n_chunks_after<span class="op">=</span><span class="va">self</span>.n_chunks_after,</span>
<span id="cb35-77"><a href="#cb35-77" aria-hidden="true" tabindex="-1"></a>            mask_fn<span class="op">=</span>mask_fn,</span>
<span id="cb35-78"><a href="#cb35-78" aria-hidden="true" tabindex="-1"></a>            q_info<span class="op">=</span>q_info,</span>
<span id="cb35-79"><a href="#cb35-79" aria-hidden="true" tabindex="-1"></a>            kv_info<span class="op">=</span>kv_info,</span>
<span id="cb35-80"><a href="#cb35-80" aria-hidden="true" tabindex="-1"></a>            dropout<span class="op">=</span><span class="va">self</span>.attention_dropout,</span>
<span id="cb35-81"><a href="#cb35-81" aria-hidden="true" tabindex="-1"></a>            rng<span class="op">=</span>attend_rng,</span>
<span id="cb35-82"><a href="#cb35-82" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb35-83"><a href="#cb35-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-84"><a href="#cb35-84" aria-hidden="true" tabindex="-1"></a>        <span class="co"># np.take(so, undo_sort, axis=0); np.take(slogits, undo_sort, axis=0) would</span></span>
<span id="cb35-85"><a href="#cb35-85" aria-hidden="true" tabindex="-1"></a>        <span class="co"># also work, but these helpers include performance optimizations for TPU.</span></span>
<span id="cb35-86"><a href="#cb35-86" aria-hidden="true" tabindex="-1"></a>        o <span class="op">=</span> permute_via_gather(so, undo_sort, sticker, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb35-87"><a href="#cb35-87" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> permute_via_sort(slogits, sticker, buckets_and_t, axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb35-88"><a href="#cb35-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-89"><a href="#cb35-89" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.n_hashes <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb35-90"><a href="#cb35-90" aria-hidden="true" tabindex="-1"></a>            o <span class="op">=</span> np.reshape(o, (<span class="va">self</span>.n_hashes, seqlen, o.shape[<span class="op">-</span><span class="dv">1</span>]))</span>
<span id="cb35-91"><a href="#cb35-91" aria-hidden="true" tabindex="-1"></a>            logits <span class="op">=</span> np.reshape(logits, (<span class="va">self</span>.n_hashes, seqlen, <span class="dv">1</span>))</span>
<span id="cb35-92"><a href="#cb35-92" aria-hidden="true" tabindex="-1"></a>            probs <span class="op">=</span> np.exp(logits <span class="op">-</span> fastmath.logsumexp(logits, axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb35-93"><a href="#cb35-93" aria-hidden="true" tabindex="-1"></a>            o <span class="op">=</span> np.<span class="bu">sum</span>(o <span class="op">*</span> probs, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb35-94"><a href="#cb35-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-95"><a href="#cb35-95" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> o.shape <span class="op">==</span> (seqlen, w_v.shape[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb35-96"><a href="#cb35-96" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> np.matmul(o, w_o)</span>
<span id="cb35-97"><a href="#cb35-97" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> apply_broadcasted_dropout(out, <span class="va">self</span>.output_dropout, output_rng)</span>
<span id="cb35-98"><a href="#cb35-98" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out, state</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="4d77402f" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Here we're going to try out our LSHSelfAttention</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>n_heads <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>causal <span class="op">=</span> <span class="va">False</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>masked <span class="op">=</span> <span class="va">False</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> <span class="va">None</span></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>chunk_len <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>n_chunks_before <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>n_chunks_after <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>attention_dropout <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>n_hashes <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>n_buckets <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>seq_len <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>emb_len <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>al <span class="op">=</span> OurLSHSelfAttention(</span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>    n_heads<span class="op">=</span>n_heads,</span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a>    d_qk<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a>    d_v<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a>    causal<span class="op">=</span>causal,</span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a>    chunk_len<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a>    n_chunks_before<span class="op">=</span>n_chunks_before,</span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a>    n_chunks_after<span class="op">=</span>n_chunks_after,</span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a>    n_hashes<span class="op">=</span>n_hashes,</span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a>    n_buckets<span class="op">=</span>n_buckets,</span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a>    use_reference_code<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a>    attention_dropout<span class="op">=</span>attention_dropout,</span>
<span id="cb36-26"><a href="#cb36-26" aria-hidden="true" tabindex="-1"></a>    mode<span class="op">=</span><span class="st">"train"</span>,</span>
<span id="cb36-27"><a href="#cb36-27" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb36-28"><a href="#cb36-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-29"><a href="#cb36-29" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> jax.random.uniform(jax.random.PRNGKey(<span class="dv">0</span>), (<span class="dv">1</span>, seq_len, emb_len), dtype<span class="op">=</span>np.float32)</span>
<span id="cb36-30"><a href="#cb36-30" aria-hidden="true" tabindex="-1"></a>al_osa <span class="op">=</span> fastmath.random.get_prng(<span class="dv">1</span>)</span>
<span id="cb36-31"><a href="#cb36-31" aria-hidden="true" tabindex="-1"></a>_, _ <span class="op">=</span> al.init(tl.shapes.signature(x), rng<span class="op">=</span>al_osa)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="f2b6d4c9" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>al(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">LayerError</span>                                Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[20], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> <span class="ansi-yellow-bg">al</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">x</span><span class="ansi-yellow-bg">)</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/base.py:197</span>, in <span class="ansi-cyan-fg">Layer.__call__</span><span class="ansi-blue-fg">(self, x, weights, state, rng)</span>
<span class="ansi-green-fg ansi-bold">    195</span>   <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>state <span style="color:rgb(98,98,98)">=</span> state  <span style="font-style:italic;color:rgb(95,135,135)"># Needed if the model wasn't fully initialized.</span>
<span class="ansi-green-fg ansi-bold">    196</span> state <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>state
<span class="ansi-green-fg">--&gt; 197</span> outputs, new_state <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">pure_fn</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">x</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">weights</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">state</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">rng</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">    198</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>state <span style="color:rgb(98,98,98)">=</span> new_state
<span class="ansi-green-fg ansi-bold">    199</span> <span style="font-weight:bold;color:rgb(0,135,0)">return</span> outputs

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/base.py:605</span>, in <span class="ansi-cyan-fg">Layer.pure_fn</span><span class="ansi-blue-fg">(self, x, weights, state, rng, use_cache)</span>
<span class="ansi-green-fg ansi-bold">    602</span> <span style="font-weight:bold;color:rgb(0,135,0)">except</span> <span style="font-weight:bold;color:rgb(215,95,95)">Exception</span>:
<span class="ansi-green-fg ansi-bold">    603</span>   <span style="font-style:italic;color:rgb(95,135,135)"># Skipping 3 lines as it's always the uninteresting internal call.</span>
<span class="ansi-green-fg ansi-bold">    604</span>   name, trace <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_name, _short_traceback(skip<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">3</span>)
<span class="ansi-green-fg">--&gt; 605</span>   <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> LayerError(name, <span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">pure_fn</span><span style="color:rgb(175,0,0)">'</span>,
<span class="ansi-green-fg ansi-bold">    606</span>                    <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_caller, signature(x), trace) <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">None</span>

<span class="ansi-red-fg">LayerError</span>: Exception passing through layer OurLSHSelfAttention (in pure_fn):
  layer created in file [...]/layers/research/efficient_attention.py, line 1751
  layer input shapes: ShapeDtype{shape:(1, 8, 5), dtype:float32}

  File [...]/layers/research/efficient_attention.py, line 2158, in forward
    single_out, single_new_state = self.forward_unbatched(

  File [...]/tmp/ipykernel_1538144/2615489615.py, line 17, in forward_unbatched
    q, hash_subrng, self.n_buckets, self.n_hashes, mask=mask

AttributeError: 'OurLSHSelfAttention' object has no attribute 'n_buckets'. Did you mean: '_n_buckets'?</pre>
</div>
</div>
</div>
<details>
<summary>
<font size="3"><b> Expected Output </b></font>
</summary>
<p><strong>Expected Values</strong></p>
<pre><code>using jax
using jax
using jax
DeviceArray([[[ 6.6842824e-01, -1.1364323e-01, -5.4430610e-01,
                2.1126242e-01, -1.0988623e-02],
              [ 7.0949769e-01, -1.5455185e-01, -5.9923315e-01,
                2.2719440e-01,  1.3833776e-02],
              [ 7.1442688e-01, -1.2046628e-01, -5.3956544e-01,
                1.7320301e-01, -1.6552269e-02],
              [ 6.7178929e-01, -7.6611102e-02, -5.9399861e-01,
                2.1236290e-01,  7.9482794e-04],
              [ 7.1518433e-01, -1.1359170e-01, -5.7821894e-01,
                2.1304411e-01,  3.0598268e-02],
              [ 6.8235350e-01, -9.3979925e-02, -5.5341840e-01,
                2.1608177e-01, -6.6673756e-04],
              [ 6.1286640e-01, -8.1027031e-02, -4.8148823e-01,
                1.9373313e-01,  3.1555295e-02],
              [ 7.2203505e-01, -1.0199660e-01, -5.5215168e-01,
                1.7872262e-01, -2.2289157e-02]]], dtype=float32)</code></pre>
</details>
<p><strong>Congratuations!</strong> you have created a custom layer and have become familiar with LSHSelfAttention.</p>


</section>
</details></details></details></section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2021,
  author = {Bochman, Oren},
  title = {Reformer {Efficient} {Attention:} {Ungraded} {Lab}},
  date = {2021-04-28},
  url = {https://orenbochman.github.io/notes-nlp/notes/c4w4/lab01.html},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2021" class="csl-entry quarto-appendix-citeas" role="listitem">
Bochman, Oren. 2021. <span>“Reformer Efficient Attention: Ungraded
Lab.”</span> April 28, 2021. <a href="https://orenbochman.github.io/notes-nlp/notes/c4w4/lab01.html">https://orenbochman.github.io/notes-nlp/notes/c4w4/lab01.html</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("^(?:http:|https:)\/\/www\.quarto\.org\/custom");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
          // default icon
          link.classList.add("external");
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© Copyright 2023-2025, Oren Bochman</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/OrenBochman/notes-nlp/edit/main/notes/c4w4/lab01.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/OrenBochman/notes-nlp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This page is built with 💛 and <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"fade","descPosition":"right","loop":true,"openEffect":"fade","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>