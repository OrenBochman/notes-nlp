<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>NLP Specialization</title>
<link>https://orenbochman.github.io/notes-nlp/</link>
<atom:link href="https://orenbochman.github.io/notes-nlp/index.xml" rel="self" type="application/rss+xml"/>
<description>Course and Research notes</description>
<image>
<url>https://orenbochman.github.io/notes-nlp/images/nlp-brain-wordcloud.jpg</url>
<title>NLP Specialization</title>
<link>https://orenbochman.github.io/notes-nlp/</link>
</image>
<generator>quarto-1.6.39</generator>
<lastBuildDate>Mon, 10 Feb 2025 21:42:50 GMT</lastBuildDate>
<item>
  <title>Floating Constraints in Lexical Choice</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/floating-contraints/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/literature-review-open-book.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Litrature review"><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="img-fluid figure-img" alt="Litrature review"></a></p>
<figcaption>Litrature review</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/DffGdrfY9gI" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid quarto-uncaptioned" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1
</figcaption>
</figure>
</div></div>
<section id="context" class="level2">
<h2 class="anchored" data-anchor-id="context">Context</h2>
<p>This is one of the three paper mentioned by Kathleen McKeown in her heroes of NLP interview by Andrew NG. The paper is about the floating constraints in lexical choice.</p>
<p>Looking quickly over it I noticed a couple of things:</p>
<p><span class="citation" data-cites="article">(<strong>article?</strong>)</span>{mckeown1997floating, title={Floating constraints in lexical choice}, author={McKeown, Kathleen and Elhadad, Michael and Robin, Jacques}, year={1997} }</p>
<ol type="1">
<li>I was familiar with Elhadad’s work on generation. I had read his work on FUF Functional Unification Formalism in the 90s which introduced me to the paradigm of unification before I even knew about Prolog or logic programming.</li>
<li>I found it fascinating that McKeown and Elhadad had worked together on this paper and even a bit curious about what they were looking into here.</li>
<li>Since McKeown brought it up, it might intimate that there is something worth looking into here. And it is discusing aspects of generative language models.</li>
</ol>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>Lexical choice is a computationally complex task, requiring a generation system to consider a potentially large number of mappings between concepts and words. Constraints that aid in determining which word is best come from a wide variety of sources, including syntax, semantics, pragmatics, the lexicon, and the underlying domain. Furthermore, in some situations, different constraints come into play early on, while in others, they apply much later. This makes it difficult to determine a systematic ordering in which to apply constraints. In this paper, we present a general approach to lexical choice that can handle multiple, interacting constraints. We focus on the problem of floating constraints, semantic or pragmatic constraints that float, appearing at a variety of different syntactic ranks, often merged with other semantic constraints. This means that multiple content units can be realized by a single surface element, and conversely, that a single content unit can be realized by a variety of surface elements. Our approach uses the Functional Unification Formalism (FuF) to represent a generation lexicon, allowing for declarative and compositional representation of individual constraints. - <span class="citation" data-cites="mckeown1997floating">(<strong>mckeown1997floating?</strong>)</span></p>
</blockquote>
</section>
<section id="outline" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li>Introduction
<ul>
<li>Mentions the complexity of lexical choice in language generation.</li>
<li>Notes the diverse sources of constraints on lexical choice, including syntax, semantics, lexicon, domain, and pragmatics.</li>
<li>Highlights the challenges posed by floating constraints, which are semantic or pragmatic constraints that can appear at various syntactic ranks and be merged with other semantic constraints.</li>
<li>Presents examples of floating constraints in sentences.</li>
</ul></li>
<li>An Architecture for Lexical Choice
<ul>
<li>Discusses the role of lexical choice within a typical language generation architecture.</li>
<li>Argues for placing the lexical choice module between the content planner and the surface sentence generator.</li>
<li>Describes the input and output of the lexical choice module, emphasizing the need for language-independent conceptual input.</li>
<li>Presents the two tasks of lexical choice: syntagmatic<sup>1</sup> decisions (determining thematic structure) and paradigmatic<sup>2</sup> decisions (choosing specific words).</li>
<li>Introduces the FUF/SURGE package as the implementation environment.</li>
</ul></li>
<li>Phrase Planning in Lexical Choice
<ul>
<li>Describes the need for phrase planning to articulate multiple semantic relations into a coherent linguistic structure.</li>
<li>Explains how the Lexical Chooser selects the head relation and builds its argument structure based on focus and perspective.</li>
<li>Details how remaining relations are attached as modifiers, considering options like embedding, subordination, and syntactic forms of modifiers.</li>
</ul></li>
<li>Cross-ranking and Merged Realizations
<ul>
<li>Discusses the non-isomorphism between semantic and syntactic structures, necessitating merging and cross-ranking realizations.</li>
<li>Provides examples of merging (multiple semantic elements realized by a single word) and cross-ranking (single semantic element realized at different syntactic ranks).</li>
<li>Emphasizes the need for linguistically neutral input to support this flexibility.</li>
<li>Explains the implementation of merging and cross-ranking using FUF, highlighting the challenges of search and the use of bk-class for efficient backtracking.</li>
</ul></li>
<li>Interlexical Constraints
<ul>
<li>Defines interlexical constraints as arising from alternative sets of collocations for realizing pairs of content units.</li>
<li>Presents an example of interlexical constraints with verbs and nouns in the basketball domain.</li>
<li>Discusses different strategies for encoding interlexical constraints in the Lexical Chooser.</li>
<li>Introduces the :wait mechanism in FUF to delay the choice of one collocate until the other is chosen, preserving modularity and avoiding backtracking.</li>
</ul></li>
<li>Other Approaches to Lexical Choice
<ul>
<li>Reviews other systems using FUF for lexical choice, comparing their architectures and features to ADVISOR-II.</li>
<li>Discusses non-FUF-based systems, categorizing them by their positioning of lexical choice in the generation process and analyzing their handling of various constraints.</li>
</ul></li>
<li>Conclusion
<ul>
<li>Summarizes the contributions of the paper, highlighting the ability to handle a wide range of constraints, the use of FUF for declarative representation and interaction, and the algorithm for lexical selection and syntactic structure building.</li>
<li>Emphasizes the importance of syntagmatic choice and perspective selection for handling floating constraints.</li>
<li>Notes the use of lazy evaluation and dependency-directed backtracking for computational efficiency.</li>
<li>Mentions future directions, such as developing domain-independent lexicon modules and methods for organizing large-scale lexica.</li>
</ul></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;of or denoting the relationship between two or more linguistic units used sequentially to make well-formed structures. The combination of ‘that handsome man + ate + some chicken’ forms a syntagmatic relationship. If the word position is changed, it also changes the meaning of the sentence, eg ’Some chicken ate the handsome man</p></div><div id="fn2"><p><sup>2</sup>&nbsp;Paradigmatic relation describes the relationship between words that can be substituted for words with the same word class (eg replacing a noun with another noun)</p></div></div></section>
<section id="reflection" class="level2">
<h2 class="anchored" data-anchor-id="reflection">Reflection</h2>
<ul>
<li>In unification-based approaches, any word is a candidate unless it is ruled out by a constraint. With an LLM a word is selected based on its probability.</li>
<li>The idea of constraints is deeply embedded in Linguistics. The two type that immediately come to mind are:
<ul>
<li>Subcategorization constraints which are rules that govern the selection of complements and adjuncts. These are syntactic constraints.</li>
<li>Selectional constraints which are rules that govern the selection of arguments. These are semantic constraints</li>
</ul></li>
<li>Other constraints that I did not have to deal with as often are pragmatics and these are resolved from the state of the world or common sense knowledge.</li>
<li>In FUF there are many more constraints and if care isn’t taken, the system can become over constrained and no words will be selected.</li>
<li>One Neat aspect of the approach though is that FUF language could be merged with a query to generate a text.</li>
<li>Unfortunately as fat as I recall the FUF had to be specified by hand. The only people who could do this were the people who designed the system. They needed to be expert at linguistics and logic programming. Even I was pretty sharp at the time the linguistics being referred was far too challenging. I learned that in reality every computational linguist worth their salt had their own theory of language and their own version of grammar and that FUF was just one of many.</li>
<li>It seems that today attention mechanisms within LSTMs or Transformers are capable of learning a pragmatic formalism without a linguist stepping in to specify it.</li>
</ul>
<p>Why this is still interesting:</p>
<p>Imagine we want to build a low resource capable language model.</p>
<p>We would really like it to be good with grammar Also we would like it to have a lexicon that is as rich as the person it is interacting with. And we would also like to make available to it the common sense knowledge that it would need to use in the context of current and possibly a profile based on past conversations…. On the other hand we don’t want to require a 70 billion parameter model to do this. So what do we do?</p>
<p>Let imagine we start by training on Wikipedia, after all wikipedia’s mission is to become the sum of all human knowledge. By do we realy need all of the information in Wikipedia in out edge model?</p>
<p>The answer is no but the real question is how can we partition the training information and the wights etc so that we have a powerful model with a basic grammar and lexicon but which can load a knowledge graph, extra lexicons and common sense knowledge as needed.</p>
<p>So I had this idea from lookin at a <span class="citation" data-cites="ouhalla1999introducing">Ouhalla (1999)</span>. It pointed out there were phrase boundries and that if the sentence was transformed in a number of ways t would still make sense if we did this with respect to a phrase but if we used cucnks that were too small or too big the transofrmation would create malformed sentences. The operations of intrerest were movement and deletion.</p>
<p>and that can still have a model that is both small but able to access this extra material.</p>
<p>One direction seems to me is that we want to apply queries to all the training material as we train the model. Based on the queries that we retrieve each clause or phrase we can decide where we want to learn it and if we want to learn it at all.</p>
<p>(If it is common sense knowledge we may already know it. If it is semi structured we may want to put it into wikidata. If it is neither we may want to avoid learning it at all. We may still want to use it for improving the grammar and lexicon.) However we may want to store it in a speclised lexicon for domains like medicinee or law.</p>
<p>We could also decide if and how to eliminate it from we want to The queries should match each bit of information in the sentence. These are our queries. While the facts may change form</p>
<ol type="1">
<li>say we want to decompose a wikipedia article</li>
</ol>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-ouhalla1999introducing" class="csl-entry">
Ouhalla, J. 1999. <em>Introducing Transformational Grammar: From Principles and Parameters to Minimalism</em>. A Hodder Arnold Publication. Arnold. <a href="https://www.google.com/books?id=ZP-ZQ0lKI9QC">https://www.google.com/books?id=ZP-ZQ0lKI9QC</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Floating {Constraints} in {Lexical} {Choice}},
  date = {2025-02-10},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/floating-contraints/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Floating Constraints in Lexical
Choice.”</span> February 10, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/floating-contraints/">https://orenbochman.github.io/notes-nlp/reviews/paper/floating-contraints/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/floating-contraints/</guid>
  <pubDate>Mon, 10 Feb 2025 21:42:50 GMT</pubDate>
</item>
<item>
  <title>Summarization Task</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/posts/2025-02-10-summerizer/</link>
  <description><![CDATA[ 





<section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p>In the Deeplearning.ai NLP specialization, we implemented both <strong>Q&amp;A</strong> and <strong>Summarization tasks</strong>. However, we only looked at these tasks in terms of the attention models. It is now time to dive deeper into the summarization task. In this article I will review a talk covering a body of research on summarization, which has many ideas about features. Consider some of these and see what aspects are relevant to a modern task implementation.</p>
<p>Some of the tasks here can be done with a simple LSTM or Transformer. However it seems that RL may be useful if we want to plan the summary in a non-linear fashion or to make use of a dynamic programming approach. For instance to identify the narrative structure might require many more steps then we typically use in a summerization task. For a speeches we may want to capture a gist of the rhetoric.</p>
</section>
<section id="ideas" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="ideas">Ideas</h2>
<p>Several ideas surfaced while working on the assignment for building a hybrid generative/abstractive summarizer based on GPT2<sup>1</sup>. Many ideas came from prior work developing search engines. I had noticed that some issues were anathema to summarization from its inception.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;a Generative Pre-training Transformer or a decoder transformer</p></div></div><section id="coverage-ranking-content-by-importance" class="level3">
<h3 class="anchored" data-anchor-id="coverage-ranking-content-by-importance">Coverage ranking content by importance</h3>
<p>The first issue is <strong>coverage</strong>, which is how much of the original document to include. Coverage is a much more difficult problem than it appears. For example, technical documents, like patents, headings, academic writing cues, and even IR statistics, may serve as features that we may feed into a regression that can guide us in discarding the chuff from the grain. On the other hand, for movie scripts or novels, we can’t use all that, and we need to decide what is essential based on the narrative structure, which requires sophisticated processing and extensive domain knowledge to extract. So, When we want to create a synopsis of a book like War and Peace, specific details are part of the narrative structure that stands out as essential to us, and I can say that even in 2025, LLM is not good at picking these out of a large document. For attentive readers with a suitable background, if we double the length of the text, they might pick additional plot points and then less significant subplots. If again we double we would, they would include more details concerning characters, their motivations, etc.</p>
<p>To conclude, different documents can have radically different structures and cues. These suggest different strategies for selecting and ranking the material to be included in a summary of a given length. More so, when we consider summaries of different lengths, one can see that we are talking about a hierarchal structure.</p>
</section>
</section>
<section id="avoiding-repetition" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="avoiding-repetition">Avoiding repetition</h2>
<p>A second challenge that is pervasive is avoiding <strong>repetition</strong>. In the generative settings we have a tougher challenge since sentences that are generated may well be equivalent to sentences we have already generated, and we want the model to redirect it attention from material already covered. Luckily we have learned to detect similar sentences in the Q&amp;A task<sup>2</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;using Siamese networks for oneshot similarity detection.</p></div></div><p>check if it is similar to any sentence in the summary</p>
<p>Alg1: similarity detection</p>
<pre><code>summary = []
tokenized_doc = tokenize(doc)
embeddings_doc= embed(tokenized_doc)
while len(summary) &lt; doc_tokens * summary_ratio: 
    a = gen_a_sentence(embeddings_doc,summary)
    for s in summary:
        if sim(a,s) &gt; threshold:
            continue
    else:
        summary.append(a)
s -&gt; gen a sentence,</code></pre>
</section>
<section id="context-window-constraints" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="context-window-constraints">Context window constraints</h2>
<p>The third challenge is technical and is related to the <strong>length of the context window</strong>. as transformer models have a limit on the number of tokens that they can process in a context window. A second issue here is that the summary itself needs to also fit in the context window. If the structure is linear and incremental we can chunk it into many smaller pieced say using sections. However in reality we are often dealing with trees of graphs structures and chunking can erode the model’s ability to inspect said hierarchy of the structure it is tasked with summarizing. In Q&amp;A or IR tasks since we can process each chunk separately and then combine the results.</p>
<p>If we are not using a generative model one imagines a tree data structure that can efficiently track what what part of the document has been summarized. If we can use that to work with the attention mechanism we may be able to reduce the effective window size as we progress with the summary. A more nuanced idea would come from <strong>bayesian search</strong> where we may want to keep moving the probability mass for the attention mechanism to regions that are significant but have not been covered.</p>
<p>Another challenge is that we may want to grow our summary in a non-linear fashion. We may consider the main narrative structure then add in the subplots and then the character motivations. This suggests two approaches - a top down <strong>planning</strong> based approach<sup>3</sup> and a bottom up approach where we start with the most important details, then add in the less important ones. In the second case we may want to revise the initial sentences to efficiently incorporate more details as we progress.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;using rl or dynamic programming</p></div><div id="fn4"><p><sup>4</sup>&nbsp;reformer layers</p></div></div><p>I also interviewed with a company that told me they often worked with patents and that these documents were frequently in excess of a hundred pages and they were having lots of problems with the summarization task. This suggest that an efficient transformer<sup>4</sup> would be needed if we are to support context windows that can span hundreds of pages. But that besides the context window we need some good way to ensure that the summary is balanced and that it is not too repetitive.</p>
</section>
<section id="compassion" class="level2">
<h2 class="anchored" data-anchor-id="compassion">Compassion</h2>
<p>My experience with summarization by hand is that most texts can be compressed by 5-10% without losing any information simply by rephrasing with more concise language. So another idea that should be obvious is that the generated summary should be <strong>compressive</strong>, i.e., once we generate a good sentence we should consider if we can make it shorter. This should be fairly easy as we are reducing the length of a single sentence. We may however be able to do even better by considering a section and trying to see if we can merge two sentences or if two sentences have partial overlap that can be merged or referenced via an anaphora. This is another area where we diverge from Q&amp;A, where we do not have a length constraint and may often want to include all relevant information. Implementation of this idea cam be easily embodied in a loss function that penalizes summaries for each token or character. This same idea can also be used in a RL summarizer.</p>
</section>
<section id="grounding" class="level2">
<h2 class="anchored" data-anchor-id="grounding">Grounding</h2>
<p>Generative summarization is a case where we can require that all generated text be grounded in the original document. In reality generative models have many glitches, due to tokenization issues, or failures of the attention mechanism, or out of distribution generation, due to bias learned from the corpus, due to negative recency bias and and due to emergent contradictions (where a contradiction is introduced into the context window and cannot be removed.) And even if we can avoid all these there is still nothing in the model that makes it prefer truthy generations. So it seems essential that the generated text is grounded in the original document. This seems to be verifiable by visually inspecting the using the attention mechanism. In reality there may be multiple heads and multiple layers. This means we need to access the attention mechanism programmatically and store annotations from the source document for each generated token. We may also want to make abstracting summarization explicit. c.f. [Extrinsic Hallucinations in LLMs in ](https://lilianweng.github.io/posts/2024-07-07-hallucination/) by Lilian Weng</p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Summarization {Task}},
  date = {2025-02-10},
  url = {https://orenbochman.github.io/notes-nlp/posts/2025-02-10-summerizer/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Summarization Task.”</span> February 10,
2025. <a href="https://orenbochman.github.io/notes-nlp/posts/2025-02-10-summerizer/">https://orenbochman.github.io/notes-nlp/posts/2025-02-10-summerizer/</a>.
</div></div></section></div> ]]></description>
  <category>NLP</category>
  <category>Notes</category>
  <category>Literature review</category>
  <category>Summarization task</category>
  <category>Relevance</category>
  <category>Conference talk</category>
  <guid>https://orenbochman.github.io/notes-nlp/posts/2025-02-10-summerizer/</guid>
  <pubDate>Sun, 09 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Morphological Word Embeddings</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/morphological-embeddings/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./cover.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Litrature review"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/morphological-embeddings/cover.jpg" class="img-fluid figure-img" alt="Litrature review"></a></p>
<figcaption>Litrature review</figcaption>
</figure>
</div></div><p>This is a mentioned in the tokenization lab in course four week 3</p>
<p>Time permitting I will try and dive deeper into this paper.</p>
<section id="podcast" class="level2">
<h2 class="anchored" data-anchor-id="podcast">Podcast</h2>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>Linguistic similarity is multi-faceted. For instance, two words may be similar with respect to semantics, syntax, or morphology inter alia. Continuous word-embeddings have been shown to capture most of these shades of similarity to some degree. This work considers guiding word-embeddings with morphologically annotated data, a form of semisupervised learning, encouraging the vectors to encode a word’s morphology, i.e., words close in the embedded space share morphological features. We extend the log-bilinear model to this end and show that indeed our learned embeddings achieve this, using German as a case study. – <span class="citation" data-cites="cotterell2019morphological">(Cotterell and Schütze 2019)</span></p>
</blockquote>
<!-- TODO: add review and podcast for this paper-->
<!-- TODO: link here from LSH lessons-->
</section>
<section id="terminology" class="level2">
<h2 class="anchored" data-anchor-id="terminology">Terminology</h2>
<p>Here are some terms and concepts discussed in the sources, with explanations:</p>
<dl>
<dt>Word embeddings</dt>
<dd>
These are vector representations of words in a continuous space, where similar words are located close to each other in the vector space. The goal is to capture linguistic similarity, but the definition of “similarity” can vary (semantic, syntactic, morphological). Word embeddings are typically trained to produce representations that capture linguistic similarity.
</dd>
<dt>Log-bilinear model (LBL)</dt>
<dd>
This is a language model that learns features along with weights, as opposed to using hand-crafted features. It uses context to predict the next word, and word embeddings fall out as low dimensional representations of the context. The LBL is a generalization of the log-linear model.
</dd>
<dt>Morphology</dt>
<dd>
The study of word structure, including how words are formed from morphemes (the smallest units of meaning).
</dd>
<dt>Morphological tags</dt>
<dd>
These are annotations that describe the morphological properties of a word, such as case, gender, number, tense, etc.. They can be very detailed for morphologically rich languages. For example, in German, a word might be tagged to indicate its part of speech, case, gender, and number.
</dd>
<dt>Morphologically rich languages</dt>
<dd>
Languages with a high morpheme-per-word ratio, where word-internal structure is important for processing. German and Czech are cited as examples.
</dd>
<dt>Morphologically impoverished languages</dt>
<dd>
Languages with a low morpheme-per-word ratio, such as English.
</dd>
<dt>Multi-task objective</dt>
<dd>
Training a model to perform multiple tasks simultaneously. In this case, the model is trained to predict both the next word and its morphological tag.
</dd>
<dt>Semi-supervised learning</dt>
<dd>
Training a model on a partially annotated corpus, using both labeled and unlabeled data. This approach is useful when large amounts of unannotated text are available.
</dd>
<dt>Contextual signature</dt>
<dd>
The words surrounding a given word. The context in which a word appears can be used to determine the word’s meaning and morphological properties.
</dd>
<dt>Hamming distance</dt>
<dd>
A measure of the difference between two binary strings, calculated as the number of positions at which the corresponding bits are different. In this context, it is used to compare morphological tags, which are represented as bit vectors.
</dd>
<dt>MORPHOSIM</dt>
<dd>
A metric for evaluating morphologically-driven embeddings that measures how morphologically similar a word is to its nearest neighbors in the embedding space. It is calculated as the average Hamming distance between morphological tags of a word and its neighbors. Lower values are better, as they indicate that the nearest neighbors of each word are closer morphologically.
</dd>
</dl>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li>Introduction
<ul>
<li>Presents the importance of capturing word morphology, especially for morphologically-rich languages.</li>
<li>Highlights the multifaceted nature of linguistic similarity (semantics, syntax, morphology).</li>
<li>Discusses the goal of the paper: to develop word embeddings that specifically encode morphological relationships.</li>
</ul></li>
<li>Related Work
<ul>
<li>Discusses previous integration of morphology into language models, including factored language models and neural network-based approaches.</li>
<li>Notes the role of morphology in computational morphology, particularly in morphological tagging for inflectionally-rich languages.</li>
<li>Highlights the significance of distributional similarity in morphological analysis.</li>
</ul></li>
<li>Log-Bilinear Model
<ul>
<li>Describes the log-bilinear model (LBL), a generalization of the log-linear model with learned features.</li>
<li>Presents the LBL’s energy function and probability distribution in the context of language modeling.</li>
</ul></li>
<li>Morph-LBL
<ul>
<li>Proposes a multi-task objective that jointly predicts the next word and its morphological tag.</li>
<li>Describes the model’s joint probability distribution, incorporating morphological tag features.</li>
<li>Discusses the use of semi-supervised learning, allowing training on partially annotated corpora</li>
</ul></li>
<li>Evaluation
<ul>
<li>Mentions the qualitative evaluation using t-SNE, showing clusters reflecting morphological and POS relationships.</li>
<li>Introduces MorphoSim, a novel quantitative metric to assess the extent to which similar embeddings are morphologically related.</li>
</ul></li>
<li>Experiments and Results
<ul>
<li>Presents experiments on the German TIGER corpus, comparing Morph-LBL with the original LBL and Word2Vec.</li>
<li>Describes two experiments:
<ul>
<li>Experiment 1: Evaluates the morphological information encoded in embeddings using a k-NN classifier.</li>
<li>Experiment 2: Compares models using the MorphoSim metric to measure morphological similarity among nearest neighbors.</li>
</ul></li>
<li>Discusses the results, highlighting Morph-LBL’s superior performance in capturing morphological relationships, even without observing all tags during training.</li>
</ul></li>
<li>Conclusion and Future Work
<ul>
<li>Summarizes the contributions of the paper: introducing Morph-LBL for inducing morphologically guided embeddings.</li>
<li>Notes the model’s success in leveraging distributional signatures to capture morphology.</li>
<li>Discusses future work on integrating orthographic features for further improvement.</li>
<li>Mentions potential applications in morphological tagging and other NLP tasks.</li>
</ul></li>
</ul>
</section>
<section id="log-bilinear-model" class="level2">
<h2 class="anchored" data-anchor-id="log-bilinear-model">Log-Bilinear Model</h2>
<p><img src="https://latex.codecogs.com/png.latex?p(w%5Cmid%20h)%20=%0A%5Cfrac%7B%5Cexp%5Cleft(s_%5Ctheta(w,h)%5Cright)%7D%7B%5Csum_%7Bw'%7D%0A%5Cexp%5Cleft(s_%5Ctheta(w',h)%5Cright)%7D%20%5Cqquad%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?w"> is a word, <img src="https://latex.codecogs.com/png.latex?h"> is a history and <img src="https://latex.codecogs.com/png.latex?s_%5Ctheta"> is an energy function. Following the notation of , in the LBL we define <img src="https://latex.codecogs.com/png.latex?%0As_%5Ctheta(w,h)%20=%20%5Cleft(%5Csum_%7Bi=1%7D%5E%7Bn-1%7D%20C_i%0Ar_%7Bh_i%7D%5Cright)%5ET%20q_w%20+%20b_w%20%5Cqquad%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?n-1"> is the history length</p>
</section>
<section id="morph-lbl" class="level2">
<h2 class="anchored" data-anchor-id="morph-lbl">Morph-LBL</h2>
<p><img src="https://latex.codecogs.com/png.latex?%0A%20%20p(w,%20t%20%5Cmid%20h)%20%5Cpropto%20%5Cexp((%20f_t%5ET%20S%20%20+%20%5Csum_%7Bi=1%7D%5E%7Bn-1%7DC_i%20r_%7Bh_i%7D)%5ET%20q_w%20+%20b_%7Bw%7D%20)%20%5Cqquad%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?f_t"> is a hand-crafted feature vector for a morphological tag <img src="https://latex.codecogs.com/png.latex?t"> and <img src="https://latex.codecogs.com/png.latex?S"> is an additional weight matrix.</p>
<p>Upon inspection, we see that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap(t%20%5Cmid%20w,h)%20%5Cpropto%20%5Cexp(S%5ET%20f_t%20q_w)%20%5Cqquad%0A"></p>
<p>Hence given a fixed embedding <img src="https://latex.codecogs.com/png.latex?q_w"> for word <img src="https://latex.codecogs.com/png.latex?w">, we can interpret <img src="https://latex.codecogs.com/png.latex?S"> as the weights of a conditional log-linear model used to predict the tag <img src="https://latex.codecogs.com/png.latex?t">.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<ul>
<li><a href="http://www.eva.mpg.de/lingua/resources/glossing-rules.php">Leipzig Glossing Rules</a> which provides a standard way to explain morphological features by examples</li>
<li><a href="https://www.youtube.com/watch?v=y9sVFrmGu0w&amp;ab_channel=GrahamNeubig">CMU Multilingual NLP 2020 (17): Morphological Analysis and Inflection</a> <!--
- [nb-lm](https://notebooklm.google.com/notebook/f594ff01-19f2-49b0-a0ac-84176fb22667?_gl=1*1rba7bx*_ga*MzAyOTc3ODMwLjE3Mzg1MDQ5Njc.*_ga_W0LDH41ZCB*MTczODkyMzY5Ni41LjAuMTczODkyMzY5Ni42MC4wLjA.)
--></li>
<li><span class="citation" data-cites="kann-etal-2016-neural">Kann, Cotterell, and Schütze (2016)</span> <a href="https://github.com/ryancotterell/neural-canonical-segmentation">code</a></li>
<li><a href="https://unimorph.github.io/">unimorph</a> univorsal morphological database</li>
</ul>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-cotterell2019morphological" class="csl-entry">
Cotterell, Ryan, and Hinrich Schütze. 2019. <span>“Morphological Word Embeddings.”</span> <em>arXiv Preprint arXiv:1907.02423</em>.
</div>
<div id="ref-kann-etal-2016-neural" class="csl-entry">
Kann, Katharina, Ryan Cotterell, and Hinrich Schütze. 2016. <span>“Neural Morphological Analysis: Encoding-Decoding Canonical Segments.”</span> In <em>Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</em>, edited by Jian Su, Kevin Duh, and Xavier Carreras, 961–67. Austin, Texas: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/D16-1097">https://doi.org/10.18653/v1/D16-1097</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Morphological {Word} {Embeddings}},
  date = {2025-02-07},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/morphological-embeddings/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Morphological Word Embeddings.”</span>
February 7, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/morphological-embeddings/">https://orenbochman.github.io/notes-nlp/reviews/paper/morphological-embeddings/</a>.
</div></div></section></div> ]]></description>
  <category>Paper</category>
  <category>Review</category>
  <category>Podcast</category>
  <category>NLP</category>
  <category>Morphology</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/morphological-embeddings/</guid>
  <pubDate>Thu, 06 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Practical and Optimal LSH for Angular Distance</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/2015-LSH/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./cover.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Litrature review"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2015-LSH/cover.jpg" class="img-fluid figure-img" alt="Litrature review"></a></p>
<figcaption>Litrature review</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/t_8SpFV0l7A" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Locality-Sensitive Hashing and Beyond
</figcaption>
</figure>
</div><div id="vid-02" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/Dkomk2wPaoc" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;2: Beyond Locality Sensitive Hashing; Alexandr Andoni
</figcaption>
</figure>
</div></div>

<p>In the NLP specialization we have covered and used LSH a number of times in at least two courses. <!-- TODO: link to the notes so as to make them more useful as this is a stub!!!! --> In one sense I have an understanding of what is going on when we implement this. In another sense this seems to be quite confusing. So I don’t fully understand some aspects of LSH.</p>
<p>One way to do better is to try and explain it to someone else. This is what I am trying to do here by going back to the source and trying to understand the paper, problems, motivation and finally isolate what it is that is hard to grasp.</p>
<p>This paper is a good introduction to LSH for the angular distance.</p>
<p>In <span class="citation" data-cites="andoni2015practical">(Andoni et al. 2015)</span> the authors delve into Locality-Sensitive Hashing (LSH) for the angular distance. They show the existence of an LSH family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal running time exponent. Unlike earlier algorithms with this property (e.g., Spherical LSH), their algorithm is also practical, improving upon the well-studied hyperplane LSH in practice.</p>
<p>For now the two videos which explain in some depth this an the related paper - <a href="https://arxiv.org/abs/1501.01062">Optimal Data-Dependent Hashing for Approximate Near Neighbors</a> will have to do.</p>
<p>Time permitting I will try and dive deeper into this paper.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR - Location Sensitive Hashing
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/in_a_nutshell.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Location Sensitive Hashing in a Nutshell"><img src="https://orenbochman.github.io/notes-nlp/images/in_a_nutshell.jpg" class="img-fluid figure-img" alt="Location Sensitive Hashing in a Nutshell"></a></p>
<figcaption>Location Sensitive Hashing in a Nutshell</figcaption>
</figure>
</div>
<ul>
<li>The paper introduces a Locality-Sensitive Hashing (LSH) family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal running time exponent.</li>
<li>The algorithm is also practical, improving upon the well-studied hyperplane LSH in practice.</li>
<li>The authors also introduce a multiprobe version of this algorithm, and conduct experimental evaluation on real and synthetic data sets.</li>
</ul>
</div>
</div>
<section id="podcast" class="level2">
<h2 class="anchored" data-anchor-id="podcast">Podcast</h2>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>We show the existence of a Locality-Sensitive Hashing (LSH) family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal running time exponent. Unlike earlier algorithms with this property (e.g., Spherical LSH [1 , 2]), our algorithm is also practical, improving upon the well-studied hyperplane LSH [3] in practice.</p>
<p>We also introduce a <strong>multiprobe</strong> version of this algorithm, and conduct experimental evaluation on real and synthetic data sets.</p>
<p>We complement the above positive results with a fine-grained lower bound for the quality of any LSH family for angular distance. Our lower bound implies that the above LSH family exhibits a trade-off between evaluation time and quality that is close to optimal for a natural class of LSH functions. – <span class="citation" data-cites="andoni2015practical">(Andoni et al. 2015)</span></p>
</blockquote>
<!-- TODO: add outline, reflection, and terminology and for this paper-->
<!-- TODO: link here from LSH lessons-->
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-andoni2015practical" class="csl-entry">
Andoni, Alexandr, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn, and Ludwig Schmidt. 2015. <span>“Practical and Optimal LSH for Angular Distance.”</span> <em>Advances in Neural Information Processing Systems</em> 28.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Practical and {Optimal} {LSH} for {Angular} {Distance}},
  date = {2025-02-05},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/2015-LSH/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Practical and Optimal LSH for Angular
Distance.”</span> February 5, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/2015-LSH/">https://orenbochman.github.io/notes-nlp/reviews/paper/2015-LSH/</a>.
</div></div></section></div> ]]></description>
  <category>Paper</category>
  <category>Review</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/2015-LSH/</guid>
  <pubDate>Tue, 04 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Finetuning Pretrained Transformers into RNNs</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/transforer-to-rnn/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./cover.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Litrature review"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/transforer-to-rnn/cover.jpg" class="img-fluid figure-img" alt="Litrature review"></a></p>
<figcaption>Litrature review</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/JMeYGYANEqU" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Transformer to RNN (T2RNN) Part-1 by Dr.&nbsp;Niraj Kumar
</figcaption>
</figure>
</div><div id="vid-02" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/UHgy2faOD_M" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;2: Transformer to RNN (T2RNN) Part-2 by Dr.&nbsp;Niraj Kumar
</figcaption>
</figure>
</div></div>

<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>Transformers have outperformed recurrent neural networks (RNNs) in natural language generation. But this comes with a significant computational cost, as the attention mechanism’s complexity scales quadratically with sequence length. Efficient transformer variants have received increasing interest in recent works. Among them, a linear-complexity recurrent variant has proven well suited for autoregressive generation. It approximates the softmax attention with randomized or heuristic feature maps, but can be difficult to train and may yield suboptimal accuracy. This work aims to convert a pretrained transformer into its efficient recurrent counterpart, improving efficiency while maintaining accuracy. Specifically, we propose a swap-then-finetune procedure: in an off-the-shelf pretrained transformer, we replace the softmax attention with its linear-complexity recurrent alternative and then finetune. With a learned feature map, our approach provides an improved tradeoff between efficiency and accuracy over the standard transformer and other recurrent variants. We also show that the finetuning process has lower training cost relative to training these recurrent variants from scratch. As many models for natural language tasks are increasingly dependent on large-scale pretrained transformers, this work presents a viable approach to improving inference efficiency without repeating the expensive pretraining process. – []</p>
</blockquote>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Finetuning {Pretrained} {Transformers} into {RNNs}},
  date = {2025-02-03},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/transforer-to-rnn/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Finetuning Pretrained Transformers into
RNNs.”</span> February 3, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/transforer-to-rnn/">https://orenbochman.github.io/notes-nlp/reviews/paper/transforer-to-rnn/</a>.
</div></div></section></div> ]]></description>
  <category>Paper</category>
  <category>Review</category>
  <category>LSTM</category>
  <category>RNN</category>
  <category>Transformer</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/transforer-to-rnn/</guid>
  <pubDate>Sun, 02 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>xLSTM: Extended Long Short-Term Memory</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="cover.jpg" class="lightbox" data-gallery="slides" title="Literature Review"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/cover.jpg" class="img-fluid figure-img" alt="Literature Review"></a></p>
<figcaption>Literature Review</figcaption>
</figure>
</div><div id="vid-02" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/0aWGTNS03PU?t=3" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Review by AI Bites
</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/0OaEv1a5jUM" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;2: Review by Yannic Kilcher
</figcaption>
</figure>
</div></div>

<blockquote class="blockquote">
<p>Whate’er the theme, the Maiden sang<br>
<mark>As if her song could have no ending;</mark><br>
I saw her singing at her work,<br>
And o’er the sickle bending;—<br>
I listened, motionless and still;<br>
And, as I mounted up the hill,<br>
<mark>The music in my heart I bore,<br>
Long after it was heard no more.</mark> &nbsp;</p>
<p>– The Solitary Reaper by William Wordsworth.</p>
</blockquote>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR – Scaling LSTMs to Billions of Parameters with <strong>xLSTM</strong>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/in_a_nutshell.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="LSTMs in a nutshell"><img src="https://orenbochman.github.io/notes-nlp/images/in_a_nutshell.jpg" class="img-fluid figure-img" alt="LSTMs in a nutshell"></a></p>
<figcaption>LSTMs in a nutshell</figcaption>
</figure>
</div>
<ul>
<li>Just when we think that the transformer rules supreme the RNN makes a comeback with a refreshing new look at the LSTMs.</li>
<li>This paper brings new architectures to the LSTM that are fully parallelizable and can scale to billions of parameters.</li>
<li>They demonstrate on synthetic tasks and language modeling benchmarks that these new xLSTMs can outperform state-of-the-art Transformers and State Space Models.</li>
</ul>
</div>
</div>
</div>
<section id="introduction" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Motivation
</div>
</div>
<div class="callout-body-container callout-body">
<p>So this is my first review of the RNN papers. I found that since the courses covering LSTM seemed to be over simplified and confusing, I should go back to the source. Since time is limited, it helps to review the papers in reverse chronological order. Each new papers explains the limitations that are generally not apparent when the paper is first published. The authors of the later papers also have the benefit of hindsight which they share in the obligatory literature review wherein they provide lots of context we cannot get by reading the original papers. Given all that there is also a benefit to reading the original paper - the real innovators while harder to understand are remarkable thinkers and often reveal motivations and deep insights that elude later interlocutors.</p>
</div>
</div>
<section id="sec-podcast" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-podcast">Podcast &amp; Other Reviews</h3>
<p>This paper is a bit heavy on the technical side. But if we are still here, we may enjoy this podcast reviewing the paper. Though AI generated I found it quite useful to get oriented with the paper and understand that the experiments the authors run to validate this new architectures.</p>
<p>We also cover something I found absent from the Course on sequence models . The limitations of LSTMs as viewed by the authors when considered in the context of transformers and Mamba.</p>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
<p>Although this paper is recent there are a number of other people who cover it.</p>
<ul>
<li>In Video&nbsp;2 Yannic Kilcher covers the paper in his youtube channel. He does a great job of explaining the paper in a way that is easy to understand. Kilcher is all too human, he makes many mistakes, stresses opinions over facts. If we don’t mind all that he is a great resource for understanding the paper. I find he rubs me the wrong way but at least I find this motivating to do better in my own reviews!</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
What can we expect from xLSTM?
</div>
</div>
<div class="callout-body-container callout-body">

<blockquote class="blockquote">
<p>xLSTM has the potential to considerably impact other fields like Reinforcement Learning, Time Series Prediction, or the modeling of physical systems. – <span class="citation" data-cites="beck2024xlstm">(Beck et al. 2024)</span></p>
</blockquote>
<p>In his book Understanding Media <span class="citation" data-cites="mcluhan1988understanding">(McLuhan 1988)</span> Marshal McLuhan introduced his <strong>Tetrad</strong>. <mark>The <strong>Tetrad</strong> is a mental model for understanding how a technological innovation like the xLSTM might disrupt society</mark>. To work the model we ask the following four questions that can be applied to any new technology to understand its impact. The four questions in this instance are:</p>
<ol type="1">
<li>What does the xLSTM enhance or amplify?</li>
</ol>
<blockquote class="blockquote">
<p>The xLSTM enhances the LSTM by introducing exponential gating and novel memory structures, boosting its capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models.</p>
</blockquote>
<ol start="2" type="1">
<li>What does the xLSTM make obsolete or replace?</li>
</ol>
<blockquote class="blockquote">
<p>The xLSTM makes the traditional LSTM obsolete by addressing its limitations, such as the inability to revise storage decisions, limited storage capacities, and lack of parallelizability.</p>
</blockquote>
<ol start="3" type="1">
<li>What does the xLSTM retrieve that was previously obsolesced?</li>
</ol>
<blockquote class="blockquote">
<p>The xLSTM retrieves the LSTM’s relevance in large-scale language modeling by scaling it to billions of parameters and integrating modern LLM techniques while mitigating known limitations. 4. What does the xLSTM reverse or flip into when pushed to its limits?</p>
</blockquote>
<blockquote class="blockquote">
<p>The xLSTM, when pushed to its limits, can compete with current Large Language Models based on Transformer technology, offering a competitive alternative for sequence modeling tasks.</p>
</blockquote>
<p>The xLSTM paper by <span class="citation" data-cites="beck2024xlstm">(Beck et al. 2024)</span> is a great example of the tetrad in action. The authors take the LSTM, a technology that has stood the test of time, and enhance it with exponential gating and novel memory structures. They address the limitations of traditional LSTMs and push the technology to new limits, creating a new architecture that can compete with state-of-the-art Transformers and State Space Models. Recent work by <span class="citation" data-cites="chen2024computationallimitsstatespacemodels">(Chen et al. 2024)</span> suggest that Transformers and The State Space Models are actually limited in their own ways.</p>
</div>
</div>
<div class="no-row-height column-margin column-container"><div id="sup-01" class="quarto-float quarto-figure quarto-figure-center anchored callout-margin-content">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="MediaTetrad.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Tetrad"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/MediaTetrad.svg" class="img-fluid figure-img"></a></p>
<figcaption>Tetrad</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div></section>
</section>
<section id="abstract" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining:</p>
<ol type="1">
<li>sLSTM with a scalar memory, a scalar update, and new memory mixing,<br>
</li>
<li>mLSTM that is fully parallelizable with a matrix memory and a covariance update rule.<br>
</li>
</ol>
<p>Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling. — <span class="citation" data-cites="beck2024xlstm">(Beck et al. 2024)</span></p>
</blockquote>

<div class="no-row-height column-margin column-container"><div id="fig-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="fig01.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="architecture"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/fig01.png" class="img-fluid figure-img"></a></p>
<figcaption>architecture</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: The extended LSTM (xLSTM) family. From left to right:<br>
1. The original LSTM memory cell with constant error carousel and gating.<br>
2. New <strong>sLSTM</strong> and <strong>mLSTM</strong> memory cells that introduce exponential gating. sLSTM offers a new memory mixing technique. <strong>mLSTM</strong> is fully parallelizable with a novel matrix memory cell state and new covariance update rule.<br>
3. mLSTM and sLSTM in residual blocks yield xLSTM blocks.<br>
4. Stacked xLSTM blocks give an xLSTM
</figcaption>
</figure>
</div><div id="fig-02" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="fig02.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="blocks"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/fig02.png" class="img-fluid figure-img"></a></p>
<figcaption>blocks</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: LSTM limitations.<br>
- Left: Nearest Neighbor Search problem in terms of mean squared error (MSE). Given a reference vector, a sequence is scanned sequentially for the most similar vector with the objective to return its attached value at sequence end. LSTM struggles to revise a stored value when a more similar vector is found. Our new xLSTM overcomes this limitation by exponential gating. - Right: Rare Token Prediction. The perplexity (PPL) of token prediction on Wikitext-103, in partitions of token frequency. LSTM performs worse on predicting rare tokens because of its limited storage capacities, whereas our new xLSTM solves this problem via a matrix memory.
</figcaption>
</figure>
</div><div id="fig-03" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="fig03.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="blocks"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/fig03.png" class="img-fluid figure-img"></a></p>
<figcaption>blocks</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: xLSTM blocks.<br>
Left: A residual sLSTM block with post up-projection (like Transformers): The input is fed into an sLSTM — with an optional convolution — followed by a gated MLP.<br>
Right: A residual mLSTM block with pre up-projection (like State Space models): mLSTM is wrapped inside two MLPs, via a convolution, a learnable skip connection, and an output gate that acts componentwise.
</figcaption>
</figure>
</div><div id="fig-04" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-04-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="fig04.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="blocks"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/fig04.png" class="img-fluid figure-img"></a></p>
<figcaption>blocks</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-04-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: xLSTM blocks.<br>
Left: A residual sLSTM block with post up-projection (like Transformers): The input is fed into an sLSTM — with an optional convolution — followed by a gated MLP.<br>
Right: A residual mLSTM block with pre up-projection (like State Space models): mLSTM is wrapped inside two MLPs, via a convolution, a learnable skip connection, and an output gate that acts componentwise.
</figcaption>
</figure>
</div><div id="fig-05" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-05-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="fig05.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="blocks"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/fig05.png" class="img-fluid figure-img"></a></p>
<figcaption>blocks</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-05-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: xLSTM blocks.<br>
Left: A residual sLSTM block with post up-projection (like Transformers): The input is fed into an sLSTM — with an optional convolution — followed by a gated MLP.<br>
Right: A residual mLSTM block with pre up-projection (like State Space models): mLSTM is wrapped inside two MLPs, via a convolution, a learnable skip connection, and an output gate that acts componentwise.
</figcaption>
</figure>
</div></div>



</section>
<section id="sec-outline" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-outline">Paper Outline</h2>
<ul>
<li>Introduction
<ul>
<li>Describes the constant error carousel and gating mechanisms of Long Short-Term Memory (LSTM). &gt; “In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories…”</li>
<li>Discusses three main limitations of LSTMs:
<ol type="1">
<li>The inability to revise storage decisions.</li>
<li>Limited storage capacities.</li>
<li>Lack of parallelizability due to memory mixing.</li>
</ol></li>
<li>Highlights <mark>the emergence of Transformers in language modeling due to these limitations</mark>. &gt; Transformer Era: The rise of Transformers with parallelizable self-attention has largely overshadowed LSTMs due to their superior scalability.</li>
</ul></li>
<li>Extended Long Short-Term Memory
<ul>
<li>Introduces Extended Long Short-Term Memory (xLSTM), which incorporates exponential gating and novel memory structures.</li>
<li>Presents two new LSTM variants:
<ol type="1">
<li>sLSTM with a scalar memory, a scalar update, and new memory mixing.</li>
<li>mLSTM with a matrix memory and a covariance update rule, which is fully parallelizable.</li>
</ol></li>
<li>Describes the integration of these LSTM variants into residual block modules, resulting in xLSTM blocks.</li>
<li>Presents xLSTM architectures that residually stack xLSTM blocks.</li>
</ul></li>
<li>Related Work:
<ul>
<li>Mentions various linear attention methods to overcome the quadratic complexity of Transformer attention.</li>
<li>Notes the popularity of State Space Models (SSMs) for language modeling.</li>
<li>Highlights the use of Recurrent Neural Networks (RNNs) as a replacement for Transformers.</li>
<li>Mentions the use of <strong>gating</strong> in recent RNN and SSM approaches.</li>
<li>Notes the use of <strong>covariance update rules</strong><sup>1</sup> to enhance storage capacities in various models.</li>
</ul></li>
<li>Experiments
<ul>
<li>Presents the experimental evaluation of xLSTM compared to existing methods with a focus on language modeling.</li>
<li>Discusses the effectiveness of xLSTM on synthetic tasks, including:
<ul>
<li>Formal languages.</li>
<li>Multi-Query Associative Recall.</li>
<li>Long Range Arena tasks.</li>
</ul></li>
<li>Presents a method comparison and ablation study, where different models are trained on 15 billion tokens from the SlimPajama dataset.</li>
<li>Assesses the scaling behavior of different methods based on validation perplexity.</li>
<li>Conducts a large-scale language modeling experiment:
<ul>
<li>Training different model sizes on 300B tokens from SlimPajama.</li>
<li>Evaluating models on length extrapolation.</li>
<li>Assessing models on validation perplexity and performance on downstream tasks.</li>
<li>Evaluating models on 571 text domains of the PALOMA language benchmark dataset.</li>
<li>Assessing the scaling behavior with increased training data.</li>
</ul></li>
</ul></li>
<li>Limitations
<ul>
<li>Highlights limitations of xLSTM, including:
<ul>
<li>Lack of parallelizability for sLSTM due to memory mixing.</li>
<li>Unoptimized CUDA kernels for mLSTM.</li>
<li>High computational complexity of mLSTM’s matrix memory.</li>
<li>Memory overload for longer contexts due to the independence of mLSTM’s memory from sequence length.</li>
</ul></li>
</ul></li>
<li>Conclusion
<ul>
<li>Concludes that xLSTM performs favorably in language modeling compared to Transformers and State Space Models.</li>
<li>Suggests that larger xLSTM models will be competitive with current Large Language Models based on Transformer technology.</li>
<li>Notes the potential impact of xLSTM on other deep learning fields such as reinforcement learning, time series prediction, and physical system modeling.</li>
</ul></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;explain covariance</p></div></div></section>
<section id="briefing-xlstm---extended-long-short-term-memory" class="level2">
<h2 class="anchored" data-anchor-id="briefing-xlstm---extended-long-short-term-memory">Briefing : xLSTM - Extended Long Short-Term Memory</h2>
<section id="introduction-and-motivation" class="level3">
<h3 class="anchored" data-anchor-id="introduction-and-motivation">Introduction and Motivation:</h3>
<p><strong>LSTM’s Legacy</strong>: The Long Short-Term Memory (LSTM), introduced in the 1990s, has been a cornerstone of deep learning, especially in sequence modeling and early Large Language Models (LLMs). LSTMs are known for their “constant error carousel” and gating mechanisms, designed to combat vanishing gradients in recurrent neural networks.</p>
<blockquote class="blockquote">
<p>“In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories…”</p>
</blockquote>
<p>Transformer Era: The rise of Transformers with parallelizable self-attention has largely overshadowed LSTMs due to their superior scalability.</p>
<blockquote class="blockquote">
<p>“However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale.” <strong>xLSTM Question</strong>: This research aims to explore the capabilities of LSTMs when scaled to billions of parameters, by integrating modern LLM techniques while addressing known limitations.</p>
</blockquote>
<blockquote class="blockquote">
<p>“We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs?”</p>
</blockquote>
</section>
<section id="key-limitations-of-traditional-lstms" class="level3">
<h3 class="anchored" data-anchor-id="key-limitations-of-traditional-lstms">Key Limitations of Traditional LSTMs:</h3>
<p>The paper identifies three major limitations of traditional LSTMs:</p>
<ol type="1">
<li>Inability to Revise Storage Decisions: LSTMs struggle to update previously stored information when more relevant information arrives later in the sequence. The paper uses a “Nearest Neighbor Search” task to demonstrate this.</li>
</ol>
<blockquote class="blockquote">
<p>“LSTM struggles to revise a stored value when a more similar vector is found…”</p>
</blockquote>
<ol start="2" type="1">
<li>Limited Storage Capacity: LSTMs store information in scalar cell states, which can lead to information compression and loss. This impacts performance on tasks requiring detailed memory, like predicting rare tokens.</li>
</ol>
<blockquote class="blockquote">
<p>“LSTM performs worse on predicting rare tokens because of its limited storage capacities…”</p>
</blockquote>
<ol start="3" type="1">
<li>Lack of Parallelizability: LSTMs process data sequentially due to the recurrent connections between hidden states, hindering parallel computation.</li>
</ol>
<blockquote class="blockquote">
<p>“Lack of parallelizability due to memory mixing, i.e., the hidden-hidden connections between hidden states from one time step to the next, which enforce sequential processing.”</p>
</blockquote>
<ol start="3" type="1">
<li>xLSTM Innovations:</li>
</ol>
<p>The paper introduces Extended Long Short-Term Memory (xLSTM), designed to address the above limitations, by modifying the original LSTM using two main approaches:</p>
<p><strong>Exponential Gating</strong>:Implemented to allow for better revision of storage decisions. Normalization and stabilization techniques are used to avoid overflows in calculations due to the exponential function.</p>
<blockquote class="blockquote">
<p>“To empower LSTMs with the ability to revise storage decisions, we introduce exponential gates (red) together with normalization and stabilization.” Novel Memory Structures: Two new LSTM variants are created with these structures:</p>
</blockquote>
<p><strong>sLSTM</strong>: A scalar memory LSTM with a scalar update and new memory mixing technique (within heads, not across). Features multiple memory cells and multiple heads, with memory mixing within each head.</p>
<blockquote class="blockquote">
<p>“The new sLSTM (see Section 2.2) with a scalar memory, a scalar update, and memory mixing…”</p>
</blockquote>
<p><strong>mLSTM</strong>: A matrix memory LSTM, fully parallelizable, with a covariance update rule for storing key-value pairs. Does not have memory mixing between cells or heads.</p>
<blockquote class="blockquote">
<p>“the new mLSTM (see Section 2.3) with a matrix memory and a covariance (outer product) update rule, which is fully parallelizable.”</p>
</blockquote>
<ol start="4" type="1">
<li>sLSTM Details:</li>
</ol>
<ul>
<li><strong>Exponential Gates</strong>: Introduces exponential activation functions for input and forget gates with normalization and a stabilizer state to prevent overflow.</li>
<li><strong>Normalizer State</strong>: A state used to normalize the hidden state output by accumulating a weighted sum of input and forget gates.</li>
<li><strong>Memory Mixing</strong>: sLSTM allows memory mixing between cells, and also introduces a new form of memory mixing through multiple heads, mixing between cells within a head, but not across heads.</li>
</ul>
<blockquote class="blockquote">
<p>“The new sLSTM can have multiple heads with memory mixing within each head but not across heads. The introduction of heads for sLSTM together with exponential gating establishes a new way of memory mixing.”</p>
</blockquote>
<ol start="5" type="1">
<li>mLSTM Details:</li>
</ol>
<p><strong>Matrix Memory</strong>: Replaces the scalar cell state with a matrix, increasing storage capacity.</p>
<p><strong>Key-Value Storage</strong>: The mLSTM memory is based around a key-value storage system inspired by Bidirectional Associative Memories (BAMs) using covariance updates, where keys and values are updated via an outer product.</p>
<blockquote class="blockquote">
<p>“At time <img src="https://latex.codecogs.com/png.latex?t">, we want to store a pair of vectors, the key <img src="https://latex.codecogs.com/png.latex?k_t%20%E2%88%88%20R%5Ed"> and the value <img src="https://latex.codecogs.com/png.latex?v_t%20%E2%88%88%20R%5Ed">… The covariance update rule… for storing a key-value pair…”</p>
</blockquote>
<p>Parallelization: mLSTM is fully parallelizable due to abandoning hidden-hidden recurrent connections and memory mixing. Computations can be performed efficiently on GPUs using standard matrix operations.</p>
<blockquote class="blockquote">
<p>“…the mLSTM… which is fully parallelizable.”</p>
</blockquote>
<ol start="6" type="1">
<li>xLSTM Architecture:</li>
</ol>
<p><strong>xLSTM Blocks</strong>: The sLSTM and mLSTM variants are integrated into residual blocks.</p>
<p><strong>sLSTM blocks</strong> use post up-projection (like Transformers).</p>
<p><strong>mLSTM blocks</strong> use pre up-projection (like State Space Models).</p>
<blockquote class="blockquote">
<p>“Integrating these new LSTM variants into residual block modules results in xLSTM blocks…”</p>
</blockquote>
<p><strong>Residual Stacking</strong>: xLSTM architectures are created by residually stacking these blocks, using pre-LayerNorm backbones similar to current LLMs.</p>
<blockquote class="blockquote">
<p>“An xLSTM architecture is constructed by residually stacking build-ing blocks…” <strong>Cover’s Theorem</strong>: Used to justify mapping information into a higher-dimensional space for better separation of histories or contexts.</p>
</blockquote>
<blockquote class="blockquote">
<p>“We resort to Cover’s Theorem (Cover, 1965), which states that in a higher dimensional space non-linearly embedded patterns can more likely be linearly separated than in the original space.”</p>
</blockquote>
<ol start="7" type="1">
<li>Performance and Scaling:</li>
</ol>
<p><strong>Linear Computation &amp; Constant Memory</strong>: Unlike Transformers, xLSTM networks have linear computation and constant memory complexity with respect to sequence length.</p>
<blockquote class="blockquote">
<p>“Contrary to Transformers, xLSTM networks have a linear computation and a constant memory complexity with respect to the sequence length.”</p>
</blockquote>
<p><strong>Synthetic Tasks</strong>: xLSTM showed improved performance over regular LSTMs on “Nearest Neighbor Search” and “Rare Token Prediction” synthetic tasks.</p>
<p><strong>SlimPajama Experiments</strong>: The xLSTM models were trained on 15B and 300B tokens from the SlimPajama dataset for language modelling evaluation.</p>
<p><strong>Competitive Performance</strong>: xLSTMs demonstrate competitive performance against state-of-the-art Transformers and State Space Models on language modelling benchmarks, both in terms of next token prediction perplexity and downstream tasks.</p>
<blockquote class="blockquote">
<p>“Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.”</p>
</blockquote>
<p>Ablation studies show importance of gating techniques.</p>
<ol start="8" type="1">
<li>Memory &amp; Speed:</li>
</ol>
<p><strong>sLSTM</strong>: Not parallelizable due to memory mixing, although the authors have created a fast CUDA implementation (less than twice as slow as mLSTM).</p>
<blockquote class="blockquote">
<p>“While mLSTM is parallelizable analog to FlashAttention… sLSTM is not parallelizable due to the memory mixing…”</p>
</blockquote>
<p><strong>mLSTM</strong>: Parallelizable, but currently slower than FlashAttention or Mamba due to non-optimized CUDA kernels. Matrix memory is computationally complex but does not require parameters and can be parallelized.</p>
<blockquote class="blockquote">
<p>“The memory of mLSTM does not require parameters but is computationally expensive through its d×d matrix memory and d× d update… the computations can be done in parallel on GPUs, therefore these computations have only a minor effect on the wall clock time.”</p>
</blockquote>
<ol start="9" type="1">
<li>Limitations:</li>
</ol>
<p><strong>sLSTM Parallelization</strong>: sLSTM’s memory mixing is non-parallelizable.</p>
<blockquote class="blockquote">
<p>“(i) In contrast to mLSTM, memory mixing of the sLSTM prohibits parallelizable operations, and therefore does not allow a fast parallel implementation.”</p>
</blockquote>
<p>mLSTM CUDA Optimization: The mLSTM CUDA kernels are not fully optimized.</p>
<blockquote class="blockquote">
<p>“(ii) The CUDA kernels for mLSTM are not optimized, and therefore the current implementation is about 4 times slower than FlashAttention or the scan used in Mamba.”</p>
</blockquote>
<p><strong>mLSTM Matrix Memory</strong>: High computational complexity for mLSTM due to matrix memory operations.</p>
<p><strong>Forget Gate Initialization</strong>: Careful initialization of the forget gates is needed.</p>
<p><strong>Long Context Memory</strong>: The matrix memory is independent of sequence length, and might overload memory for long context sizes.</p>
<blockquote class="blockquote">
<p>“(v) Since the matrix memory is independent of the sequence length, increasing the sequence length might overload the memory for longer context sizes.”</p>
</blockquote>
<p><strong>Hyperparameter Optimization</strong>: xLSTM is not yet fully optimized due to computational load, indicating further performance gains are possible.</p>
<blockquote class="blockquote">
<p>“(vi) Due to the expensive computational load for large language experiments, we did neither fully optimize the architecture nor the hyperparameters, especially for larger xLSTM architectures.”</p>
</blockquote>
<ol start="10" type="1">
<li>Related Work:</li>
</ol>
<p>The paper highlights connections of its ideas with the following areas:</p>
<p><strong>Gating</strong>: Many recent models such as HGRN, GLA, Gated State Space models, Mamba, and others also utilize gating mechanisms. Covariance Update Rule: The covariance update rule used in mLSTM is related to other methods such as Fast Weight Programmers, Retention, and Linear Transformers.</p>
<ol start="11" type="1">
<li>Conclusion:</li>
</ol>
<p>The xLSTM presents a compelling alternative to Transformers in sequence modeling, especially in contexts where memory efficiency and longer sequences are required. By combining insights from the original LSTM with exponential gating and novel memory structures, the paper shows that LSTMs can be competitive at scale. While limitations exist in terms of parallelization and hyperparameter tuning, the research shows promising potential for further development. The authors posit further research and improvements could allow xLSTM to reach its full potential.</p>
</section>
</section>
<section id="my-thoughts" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="my-thoughts">My Thoughts</h2>

<div class="no-row-height column-margin column-container"><div id="vid-sigmoid" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-sigmoid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/TPqr8t919YM" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-sigmoid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;3: Review of the sigmoid function
</figcaption>
</figure>
</div></div><div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Research questions
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li>How does the constant error carousel mitigate the vanishing gradient problem in the LSTM?
<ul>
<li>The constant error carousel is a way to keep the error constant over time. This helps to prevent the vanishing gradient problem in the LSTM.</li>
</ul></li>
<li>How are the gates in the original LSTM binary?
<ul>
<li>Sigmoid, saturation and a threshold at 0.5</li>
</ul></li>
<li>What is the long term memory in the LSTM?
<ul>
<li>the cell state <img src="https://latex.codecogs.com/png.latex?c_%7Bt-1%7D"></li>
</ul></li>
<li>What is the short term memory in the LSTM?
<ul>
<li>the hidden state <img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D"></li>
</ul></li>
<li>How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs</li>
</ol>
</div>
</div>

<div class="no-row-height column-margin column-container"><div id="sup-02" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="sigmoid-limits.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Supplementary Figure&nbsp;2: limits of the sigmoid function"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/sigmoid-limits.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup" id="sup-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;2: limits of the sigmoid function
</figcaption>
</figure>
</div><div id="sup-03" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="sigmoid-values.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="bias"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/sigmoid-values.png" class="img-fluid figure-img"></a></p>
<figcaption>bias</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup" id="sup-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;3: The inductive bias of the sigmoid decision function.
</figcaption>
</figure>
</div></div>
<section id="binary-nature-of-non-exponential-gating-mechanisms" class="level3">
<h3 class="anchored" data-anchor-id="binary-nature-of-non-exponential-gating-mechanisms">Binary nature of non exponential Gating Mechanisms</h3>
<p>If we try to understand why are the gating mechanisms in the original LSTM is described here as binary?</p>
<p>It helps to the properties of the sigmoid functions that are explained in Video&nbsp;3.</p>
<ol type="1">
<li><p>Supplementary Figure&nbsp;1 shows that The sigmoid function has a domain of <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D"> and a range of <img src="https://latex.codecogs.com/png.latex?(0,1)">. This means that the sigmoid function can only output values between 0 and 1.</p></li>
<li><p>It has a very sharp gradient around 0 if the bias is set to 0. This means that the sigmoid function acts as a binary function around 0.</p></li>
<li><p>The Sigmoid function tends to saturate. Now sigmoid function around 0 isn’t quite quite binary but outside of (-6,+6) it becomes saturated and acts very much as a binary function.</p></li>
<li><p>If we use it as a loss function or a decision function, the sigmoid function is used with a threshold of 0.5. This means that if the output of the sigmoid function is greater than 0.5, the decision is 1, and if it is less than 0.5, the decision is 0. That is how the sigmoid function can become binary.</p></li>
<li><p>Note that we can use thresholding during inference to make the decision binary and we can keep them as continuous values during training so we can have a smooth gradient for back-propagation.</p></li>
<li><p>Even without thresholding the sigmoid function can become binary this is due to the saturation of the sigmoid function. I recall that Hinton in his Coursera course on Neural Networks and Deep Learning mentioned that activation function that become saturated are problematic. I can say that at last I think I understand what he meant:<br>
I see that that sigmoid functions can fall off the manifold of the data and this can be a problem. This is why the ReLU function is used in the Gated Recurrent Unit (GRU) and the Transformer. <strong>Falling off the manifold of the data</strong> means that the sigmoid function becomes saturated (above 6 and below -6). At this point the back-propagation steps are no longer able to change the weight or bias since the gradient is effectively so learning stops.</p></li>
</ol>
<p>This becomes a is a problem in two senses. The sigmoid unit is likely to be stuck in a setting that only representative of part of the training data. The rest of the network which is not stuck now has to learn to compensate for this.</p>
<p>This issues may well be the root cause LSTM limitation number 1 - the inability to revise storage decisions. It seems that the authors decided to use the exponential gating function to address this issues.</p>
<p>This is why the gating mechanisms in the original LSTM are binary. The sigmoid function is used to control the flow of information in the LSTM by deciding which information to keep and which to discard. The sigmoid function is used to decide how much of the new input information should be added to the cell state and how much of the old cell state should be retained. This binary nature of the sigmoid function is what allows the LSTM to control the flow of information and decide what to remember and what to forget.</p>
</section>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li><p><a href="https://openreview.net/forum?id=ARAxPPIAhq&amp;noteId=gra7vHnb0q">The paper on open review</a> has some additional insights from the authors</p></li>
<li><p><a href="https://www.ai-bites.net/xlstm-extended-long-short-term-memory-networks/">XLSTM — Extended Long Short-Term Memory Networks</a> By Shrinivasan Sankar — May 20, 2024</p></li>
</ul>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-beck2024xlstm" class="csl-entry">
Beck, Maximilian, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, Günter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. 2024. <span>“xLSTM: Extended Long Short-Term Memory.”</span> <em>arXiv Preprint arXiv:2405.04517</em>.
</div>
<div id="ref-chen2024computationallimitsstatespacemodels" class="csl-entry">
Chen, Yifang, Xiaoyu Li, Yingyu Liang, Zhenmei Shi, and Zhao Song. 2024. <span>“The Computational Limits of State-Space Models and Mamba via the Lens of Circuit Complexity.”</span> <a href="https://arxiv.org/abs/2412.06148">https://arxiv.org/abs/2412.06148</a>.
</div>
<div id="ref-mcluhan1988understanding" class="csl-entry">
McLuhan, Marshall. 1988. <em>Understanding Media : The Extensions of Man</em>. New York: New American Library. <a href="http://www.worldcat.org/search?qt=worldcat_org_all&amp;q=0451624963">http://www.worldcat.org/search?qt=worldcat_org_all&amp;q=0451624963</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {xLSTM: {Extended} {Long} {Short-Term} {Memory}},
  date = {2025-02-03},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“xLSTM: Extended Long Short-Term
Memory.”</span> February 3, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/">https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/</a>.
</div></div></section></div> ]]></description>
  <category>Paper</category>
  <category>Review</category>
  <category>NLP</category>
  <category>LSTM</category>
  <category>Seq2Seqs</category>
  <category>RNN</category>
  <category>Podcast</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/</guid>
  <pubDate>Sun, 02 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>The Secret Life of Pronouns What Our Words Say About Us</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/book/seret-life-of-pronouns/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="cover.jpg" class="lightbox" data-gallery="slides" title="cover"><img src="https://orenbochman.github.io/notes-nlp/reviews/book/seret-life-of-pronouns/cover.jpg" class="img-fluid figure-img" alt="cover"></a></p>
<figcaption>cover</figcaption>
</figure>
</div></div><div class="page-columns page-full"><blockquote class="blockquote">
<p>To grunt and sweat under a weary life, But that the dread of something after death, <mark>The undiscovered country from whose bourn No traveler returns, puzzles the will, And makes us rather bear those ills we have, Than fly to others that we know not of</mark>?<sup>1</sup> — Hamlet Act 3, Scene 1 by William Shakespeare.</p>
</blockquote><div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Pronouns oft fall prey to the stop word filter, yet they hold the keys to unlocking the depth of intimate meaning</p></div></div></div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR – ML, NLP and the secret life of pronouns
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/in_a_nutshell.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Pronouns in a nutshell"><img src="https://orenbochman.github.io/notes-nlp/images/in_a_nutshell.jpg" class="img-fluid figure-img" alt="Pronouns in a nutshell"></a></p>
<figcaption>Pronouns in a nutshell</figcaption>
</figure>
</div>
<ul>
<li>When I got started with NLP I was coding search engines and conventional wisdom was <mark>pronouns don’t pass the <strong>stop word filter</strong></mark></li>
<li>One of the gems I learned on that job was that <mark>everything in the corpus can be provide invaluable information if we only know how to index it</mark>.</li>
<li>After reading this book and I started to unlocked the power of the pronouns.</li>
<li>At Hungarian School I discovered how pronouns combine with case ending to create a vast vistas of untapped NLP resource.</li>
<li>Cognitive AI I noted that pronouns and particles are key in extending the primitive verb system via thematic roles to get a very rich semantic systems in the lexicon with little costs in learning.</li>
</ul>
</div>
</div>
</div>
<section id="podcast" class="level2">
<h2 class="anchored" data-anchor-id="podcast">Podcast</h2>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The secret life of pronouns in context
</div>
</div>
<div class="callout-body-container callout-body">
<p>I got interested in this book when I was deep into wikipedia research. I was in a place where I wanted to be able to attribute content to the authors. I had read some disturbing articles with prior work on the topic by some researchers from my Alma Mater. However, I eventually came across this book and it seemed to put the ideas in a better form. Less about ML and more about the type of writers and their personalities.</p>
</div>
</div>
<p>In <span class="citation" data-cites="pennebaker2013secret">(Pennebaker 2013)</span> “The Secret Life of Pronouns: What Our Words Say About Us,” the author explores how the seemingly innocuous function words we use—pronouns, articles, prepositions, and auxiliary verbs—can reveal a great deal about our personalities, emotions, social connections, and psychological states. James W. Pennebaker, the author and a social psychologist, uses computer-based text analysis methods to uncover these hidden patterns.</p>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li><p><strong>Key Questions and Themes:</strong></p>
<ul>
<li><strong>Can language reveal psychological states?</strong> The author is primarily interested in what people’s words say about their psychological states. The book investigates whether language features can reveal how people are thinking and whether this information can be used to change their thinking.</li>
<li><strong>How do function words differ from content words?</strong> The book distinguishes between content and style of speech, arguing that function words, which reflect language style, reveal more about a person than content words. Function words are tied to the relationship between speaker and listener.</li>
<li><strong>Do men and women use words differently?</strong> The book explores gender differences in language use, noting that women are typically more attentive to the thoughts and feelings of others, while men are less so. It also investigates whether different writing styles reflect different personalities.</li>
<li><strong>Can language predict behavior?</strong> The book examines whether language use can predict behaviors such as success in college, risk of suicide, leadership styles, and likelihood of declaring war.</li>
<li><strong>How can language be used as a tool for change?</strong> The author examines whether analyzing words can lead to improvements in relationships, teaching, and leadership. Additionally, the book looks at how writing about emotional upheavals can improve mental and physical health, and whether using certain types of words can influence the effectiveness of that writing.</li>
<li><strong>Can language reveal deception?</strong> The book explores linguistic markers of deception, such as the use of first-person singular pronouns and the mention of other individuals.</li>
<li><strong>Can language analysis help identify authors?</strong> The book presents methods for identifying authors using function words, punctuation, and obscure words.</li>
</ul></li>
<li><p><strong>Main Examples and Studies:</strong></p>
<ul>
<li><strong>Expressive Writing:</strong> Early research focused on the benefits of writing about traumatic experiences, leading to the development of the LIWC (Linguistic Inquiry and Word Count) program. This program helps analyze word categories associated with healthy writing such as positive emotion words, cognitive words, and the ability to construct a story.</li>
<li><strong>The Bottle and the Two People Pictures:</strong> Participants described pictures, and their descriptions were analyzed for function word use and content. For example, people who focused on shadows in the bottle picture tended to be more creative. The second picture revealed common themes such as social connections, clothing, and accessories.</li>
<li><strong>Thinking Styles:</strong> The book describes how people’s function words can be statistically grouped into categories reflecting thinking styles, such as formal, analytic, and narrative. These thinking styles can predict how people relate to others and organize their worlds.</li>
<li><strong>9/11 Blog Analysis:</strong> The language used in blog posts was analyzed before and after the 9/11 attacks. Researchers noted an increase in negative emotion words, a decrease in positive emotion words, and a temporary increase in cognitive words.</li>
<li><strong>College Admissions Essays:</strong> The study examined whether the writing style in college admissions essays could predict college grades.</li>
<li><strong>The Federalist Papers:</strong> The book discusses the use of function words to determine the authorship of the anonymous Federalist Papers.</li>
<li><strong>Language Style Matching (LSM):</strong> LSM is a technique to determine how similarly two people use function words. It is used in studies of couples, speed dating, and even the writings of historical figures to analyze the relationship between the authors.</li>
<li><strong>Obama’s Pronoun Use</strong>: The book analyzes Barack Obama’s use of first-person singular pronouns. His relatively low use of I-words is interpreted as a sign of self-confidence.</li>
</ul></li>
<li><p><strong>Additional Insights:</strong></p>
<ul>
<li><strong>Stealth Words:</strong> The book emphasizes the importance of function words, often called “stealth words,” which are often overlooked.</li>
<li><strong>The Role of Computers:</strong> Computer technology has revolutionized the study of language, allowing researchers to analyze large datasets and uncover hidden patterns that would be impossible to see with manual analysis.</li>
<li><strong>Language as a Tool:</strong> Language can be used not only as a window into the inner workings of people, but also as a tool for guiding thinking and promoting change.</li>
<li><strong>Interdisciplinary Approach</strong>: The author’s work draws on multiple disciplines including psychology, linguistics, computer science, history, and political science.</li>
</ul></li>
</ul>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-pennebaker2013secret" class="csl-entry">
Pennebaker, J. W. 2013. <em>The Secret Life of Pronouns: What Our Words Say about Us</em>. Bloomsbury USA. <a href="https://books.google.co.il/books?id=p9KmCAAAQBAJ">https://books.google.co.il/books?id=p9KmCAAAQBAJ</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {The {Secret} {Life} of {Pronouns} {What} {Our} {Words} {Say}
    {About} {Us}},
  date = {2025-01-30},
  url = {https://orenbochman.github.io/notes-nlp/reviews/book/seret-life-of-pronouns/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“The Secret Life of Pronouns What Our Words
Say About Us.”</span> January 30, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/book/seret-life-of-pronouns/">https://orenbochman.github.io/notes-nlp/reviews/book/seret-life-of-pronouns/</a>.
</div></div></section></div> ]]></description>
  <category>Review</category>
  <category>Book</category>
  <category>NLP</category>
  <category>Sentiment Analysis</category>
  <category>Sentiment Analysis</category>
  <category>Podcast</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/book/seret-life-of-pronouns/</guid>
  <pubDate>Wed, 29 Jan 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>It’s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/mbr-all-the-way-dowm/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/literature-review-open-book.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Litrature review"><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="img-fluid figure-img" alt="Litrature review"></a></p>
<figcaption>Litrature review</figcaption>
</figure>
</div></div><section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<p>Minimum Bayes Risk (MBR) decoding is a method for choosing the outputs of a machine learning system based not on the output with the highest probability, but the output with the lowest risk (expected error) among multiple candidates. It is a simple but powerful method: for an additional cost at inference time, MBR provides reliable several-point improvements across metrics for a wide variety of tasks without any additional data or training. Despite this, MBR is not frequently applied in NLP works, and knowledge of the method itself is limited. We first provide an introduction to the method and the recent literature. We show that several recent methods that do not reference MBR can be written as special cases of MBR; this reformulation provides additional theoretical justification for the performance of these methods, explaining some results that were previously only empirical. We provide theoretical and empirical results about the effectiveness of various MBR variants and make concrete recommendations for the application of MBR in NLP models, including future directions in this area. – <span class="citation" data-cites="bertsch2023itsmbrwaydown">(Bertsch et al. 2023)</span></p>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li>Abstract
<ul>
<li>Describes Minimum Bayes Risk (MBR) decoding.</li>
<li>Presents MBR as a widely applicable method that often improves performance over beam search and single-sample decoding.</li>
<li>Shows that several recent generation methods can be framed as special cases of MBR, thereby providing theoretical grounding for their empirical success.</li>
</ul></li>
<li>Introduction
<ul>
<li>Presents Minimum Bayes Risk (MBR) decoding as a simple yet powerful decoding method.</li>
<li>Discusses how recent methods in natural language processing (NLP) unknowingly replicate aspects of MBR.</li>
</ul></li>
<li>Formalization
<ul>
<li>Standard decoding
<ul>
<li>Describes the standard decoding methods for autoregressive models such as greedy decoding, sampling, and beam search.</li>
</ul></li>
<li>Minimum Bayes Risk decoding
<ul>
<li>Defines the theoretical foundation of Minimum Bayes Risk (MBR) decoding, including the concept of risk based on expected error.</li>
<li>Explains how risk computation is typically approximated using Monte Carlo methods due to the intractability of calculating the full expectation.</li>
</ul></li>
</ul></li>
<li>Taxonomy of MBR
<ul>
<li>Notes four key design considerations for implementing an MBR method: the hypothesis set, the evidence set, the gain/error function, and the evidence distribution.</li>
<li>Sampling a hypothesis set
<ul>
<li>Highlights several works that show improvements in MBR by filtering the hypothesis set to contain only higher-quality candidate outputs.</li>
</ul></li>
<li>Sampling an evidence set
<ul>
<li>Briefly discusses various sampling strategies, with most work focusing on drawing unbiased samples from the model distribution.</li>
</ul></li>
<li>What metric do we want to maximize?
<ul>
<li>Explores the impact of different gain (or error) functions, noting that using a specific metric as a gain function in MBR tends to lead to improved performance on that metric.</li>
</ul></li>
<li>What probability distribution should we use to estimate risk?
<ul>
<li>Briefly discusses the choice of the distribution used to estimate risk in MBR, with most methods using the model’s score distribution over outputs, and some work using alternative distributions like a human or true distribution.</li>
</ul></li>
</ul></li>
<li>MBR as a frame for other methods
<ul>
<li>Highlights the framing of self-consistency, range voting, output ensembling, and density estimation as special cases of MBR.</li>
<li>Self-consistency as MBR
<ul>
<li>Shows how self-consistency, a method where the most frequent answer from multiple model generations is selected, can be formulated as MBR.</li>
<li>Explains that the best performing sampling strategies for self-consistency are those closest to ancestral sampling due to its unbiased estimation properties.</li>
</ul></li>
<li>Output Ensembling as MBR
<ul>
<li>Presents output ensembling, where a set of models is used to generate outputs and a combined output is selected, as a form of MBR with a mixture distribution.</li>
</ul></li>
<li>MBR as Density Estimation
<ul>
<li>Establishes the connection between MBR and kernel density estimation, noting that both can be seen as mode-seeking methods.</li>
</ul></li>
<li>Range Voting as MBR
<ul>
<li>Shows that range voting, where candidates are assigned scores by voters, can be formulated as MBR by treating candidates as hypotheses and voters as evidence.</li>
</ul></li>
</ul></li>
<li>Design Decisions Impact MBR Performance
<ul>
<li>Examines cases where the choices made in designing an MBR method significantly affect its performance.</li>
<li>Experimental Details
<ul>
<li>Presents the datasets and models used for evaluating MBR in abstractive summarization and machine translation tasks.</li>
</ul></li>
<li>The MBR metric matters–but perhaps not as much as the hypothesis set
<ul>
<li>Demonstrates that MBR using different gain functions (ROUGE-1, BEER, BERTScore) improves abstractive summarization performance.</li>
<li>Notes that the choice of hypothesis set has a more significant impact than the choice of gain function.</li>
</ul></li>
<li>Varying the risk distribution: lessons from beam search don’t translate to MBR
<ul>
<li>Investigates the effects of correcting for length bias in the evidence distribution used for estimating risk in MBR.</li>
<li>Finds that while length correction benefits beam search, it hurts MBR performance, possibly due to a high-variance estimator of risk.</li>
</ul></li>
</ul></li>
<li>MBR applications in NLP
<ul>
<li>Presents a historical overview of MBR applications in NLP, from its early use in statistical models to its recent resurgence in neural models.</li>
<li>Historical context
<ul>
<li>Traces the roots of MBR to Bayesian decision theory and its use in parsing, speech recognition, and machine translation since the 1990s.</li>
<li>Explains how early MBR applications were constrained by graph-based model structures, requiring complex algorithms for exact decoding.</li>
</ul></li>
<li>Recent usage
<ul>
<li>Discusses the revival of MBR in neural text generation tasks, with much of the recent work focusing on machine translation.</li>
<li>Notes the decline in the explicit use of the term “MBR” in favor of newer terminologies like “self-consistency.”</li>
</ul></li>
</ul></li>
<li>Conclusion
<ul>
<li>Discusses the terminology drift in NLP that leads to the renaming of MBR as different methods.</li>
<li>Reemphasizes the importance of connecting modern techniques with their historical roots for a better understanding of why they work.</li>
</ul></li>
<li>Appendix A: More details on importance sampling for MBR
<ul>
<li>Provides a detailed explanation of importance sampling and its application to MBR, specifically when estimating risk under a length-corrected distribution.</li>
</ul></li>
<li>Appendix B: Contextualizing this work within philosophy of science
<ul>
<li>Explores the broader implications of the work within the context of meta-analysis of scientific research.</li>
<li>Discusses the phenomena of citational amnesia and terminology drift in scientific literature and their possible consequences.</li>
</ul></li>
</ul>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-bertsch2023itsmbrwaydown" class="csl-entry">
Bertsch, Amanda, Alex Xie, Graham Neubig, and Matthew R. Gormley. 2023. <span>“It’s MBR All the Way down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk.”</span> <a href="https://arxiv.org/abs/2310.01387">https://arxiv.org/abs/2310.01387</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2021,
  author = {Bochman, Oren},
  title = {It’s {MBR} {All} the {Way} {Down:} {Modern} {Generation}
    {Techniques} {Through} the {Lens} of {Minimum} {Bayes} {Risk}},
  date = {2021-05-10},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/mbr-all-the-way-dowm/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2021" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2021. <span>“It’s MBR All the Way Down: Modern Generation
Techniques Through the Lens of Minimum Bayes Risk.”</span> May 10, 2021.
<a href="https://orenbochman.github.io/notes-nlp/reviews/paper/mbr-all-the-way-dowm/">https://orenbochman.github.io/notes-nlp/reviews/paper/mbr-all-the-way-dowm/</a>.
</div></div></section></div> ]]></description>
  <category>NLP</category>
  <category>Paper</category>
  <category>Review</category>
  <category>MBR</category>
  <category>Minimum Bayes Risk</category>
  <category>ASR task</category>
  <category>Automatic speech recognition</category>
  <category>MT task</category>
  <category>Syntactic Parsing task</category>
  <category>AMR parsing task</category>
  <category>Question answering task</category>
  <category>Summarization task</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/mbr-all-the-way-dowm/</guid>
  <pubDate>Sun, 09 May 2021 21:00:00 GMT</pubDate>
</item>
<item>
  <title>Generating Diverse and High-Quality Texts by Minimum Bayes Risk Decoding</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/mbr-decoding/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/literature-review-open-book.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Litrature review"><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="img-fluid figure-img" alt="Litrature review"></a></p>
<figcaption>Litrature review</figcaption>
</figure>
</div></div><section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>One of the most important challenges in text generation systems is to produce outputs that are not only correct but also diverse. Recently, Minimum Bayes-Risk (MBR) decoding has gained prominence for generating sentences of the highest quality among the decoding algorithms. However, existing algorithms proposed to generate diverse outputs are predominantly based on beam search or random sampling, thus their output quality is capped by these underlying decoding algorithms. In this paper, we investigate an alternative approach – we develop diversity-promoting decoding algorithms by enforcing diversity objectives to MBR decoding. We propose two variants of MBR</p>
<ol type="i">
<li><p>Diverse MBR (DMBR) that adds a diversity penalty to the decoding objective and</p></li>
<li><p>k-medoids MBR (KMBR) that reformulates the decoding task as a clustering problem.</p></li>
</ol>
<p>We evaluate DMBR and KMBR on a variety of directed text generation tasks using encoder-decoder models and a language model with prompting. The experimental results show that the proposed method achieves a better trade-off than the diverse beam search and sampling algorithms overall – <span class="citation" data-cites="jinnai-etal-2024-generating">(Jinnai et al. 2024)</span></p>
</blockquote>
<ul>
<li><a href="https://github.com/CyberAgentAILab/diverse-mbr/">github code repo</a></li>
</ul>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li>Introduction
<ul>
<li><del>Describes the importance of generating diverse and high-quality texts in various natural language processing tasks</del>.</li>
<li>Highlights the limitations of existing approaches for <strong>diverse text generation</strong>, which are typically based on <strong>beam search</strong> or <strong>random sampling</strong>.</li>
<li>Presents the paper’s focus on developing diversity-promoting algorithms using Minimum Bayes Risk (MBR) decoding.</li>
<li>Notes the advantages of MBR decoding over traditional methods like beam search and random sampling.</li>
<li>Introduces:
<ul>
<li><strong>Diverse MBR (DMBR)</strong> and</li>
<li><strong>k-medoids MBR (KMBR)</strong>.</li>
</ul></li>
</ul></li>
<li>Background
<ul>
<li>Defines the sequence-to-sequence generation task and the goal of decoding in finding the hypothesis that aligns with human preference.</li>
<li>Discusses the concept of set decoding, where the aim is to generate a set of sentences that maximizes both human preference and diversity.</li>
<li>Reviews existing diversity-aware decoding algorithms, including random sampling techniques and diverse beam search.</li>
<li>Explains the principle of Minimum Bayes Risk (MBR) decoding, focusing on its expected utility maximization approach and contrasting it with MAP decoding.</li>
</ul></li>
<li>Minimum Bayes Risk Decoding with Diversity
<ul>
<li>Introduces the set decoding problem with diversity objective using MBR decoding.</li>
<li>Presents a naive approach for generating k sentences using MBR and notes its tendency to produce similar sentences.</li>
<li>Proposes Diverse MBR (DMBR) decoding as a solution by adding a diversity penalty to the objective function.</li>
<li>Explains the formulation of DMBR, including the quality objective and diversity objective, and discusses the use of pairwise similarity as a diversity metric.</li>
<li>Highlights the computational complexity of DMBR and the deployment of a greedy heuristic algorithm for approximation.</li>
<li>Proposes k-medoids MBR (KMBR) as an alternative method for diversity promotion in MBR decoding.</li>
<li>Explains how KMBR leverages the k-medoids clustering algorithm to select a set of diverse and high-quality hypotheses.</li>
<li>Notes the computational challenges of KMBR and the use of the Partition Around Medoids (PAM) algorithm for approximate computation.</li>
</ul></li>
<li>Experiments
<ul>
<li>Describes the experimental setup, including the tasks (machine translation, image captioning, question generation, common sense reasoning, text summarization), datasets, evaluation metrics, and the choice of BERTScore as the utility function for MBR.</li>
<li>Mentions the use of Huggingface’s Transformers library and other tools for the experiments.</li>
<li>Summarizes the key results, highlighting that DMBR and KMBR generally achieve better trade-offs between quality and diversity than diverse beam search and sampling algorithms across the tasks.</li>
</ul></li>
<li>Machine Translation
<ul>
<li>Explains the use of the WMT’19 dataset for machine translation experiments, focusing on German-English and Russian-English translation tasks.</li>
<li>Discusses the experimental settings, including the number of outputs, comparison baselines (sampling algorithms, diverse beam search), sample size for MBR, and diversity penalty parameters.</li>
<li>Presents the key findings, emphasizing DMBR’s achievement of higher diversity, flexibility in the quality-diversity trade-off, and higher Oracle scores compared to baselines.</li>
</ul></li>
<li>Image Captioning using BLIP-2
<ul>
<li>Describes the use of the MS COCO dataset and the BLIP-2 model for evaluating performance on image captioning.</li>
<li>Briefly notes the experimental settings, including the output size, baselines, and diversity penalty parameters.</li>
<li>Highlights DMBR’s effectiveness in achieving lower P-BLEU, higher distinct-2, and better semantic diversity as measured by P-SentBERT.</li>
</ul></li>
<li>Question Generation using Language Model
<ul>
<li>Presents the evaluation of decoding algorithms for question generation using the SQuADv2 dataset and a language model (Zephyr-7B β) with prompting.</li>
<li>Mentions the experimental settings, including the number of outputs, baselines, sample size for MBR, and diversity penalty parameters.</li>
<li>Discusses the findings, noting that DMBR demonstrates advantages in distinct-2 and P-SentBERT but underperforms slightly in P-BLEU compared to DBS.</li>
</ul></li>
<li>Generative Common Sense Reasoning using Language Model
<ul>
<li>Explains the use of the CommonGen task and the Zephyr-7B β language model for evaluating common sense reasoning abilities.</li>
<li>Notes the use of prompting and the experimental settings, including the number of outputs, baselines, sample size, and diversity penalty.</li>
<li>Briefly mentions DMBR’s performance, achieving better distinct-2 but slightly worse P-BLEU compared to DBS, and discusses the low coverage of input concepts in the generations.</li>
</ul></li>
<li>Text Summarization
<ul>
<li>Describes the evaluation of text summarization using the XSum dataset and a BART model pre-trained on XSum.</li>
<li>Mentions the experimental settings, including the output size, baselines, sample size for MBR, and diversity penalty parameters.</li>
<li>Briefly presents the results, emphasizing DMBR’s better diversity compared to DBS, as measured by P-BLEU and distinct-n.</li>
</ul></li>
<li>Conclusions
<ul>
<li>Summarizes the research, highlighting the development and evaluation of DMBR and KMBR for generating diverse and high-quality texts.</li>
<li>Reiterates the better quality-diversity trade-off achieved by these methods compared to diverse beam search, and notes the higher Oracle scores attained by DMBR and KMBR over vanilla MBR.</li>
<li>Discusses potential future research directions, including applying the methods to open-ended text generation, conducting human evaluation of diversity, and reducing the inference time of DMBR and KMBR.</li>
</ul></li>
<li>Limitations
<ul>
<li>Discusses the limitations of the research, including the focus on directed text generation tasks.</li>
<li>Notes the reliance on automatic evaluation metrics and the need for human evaluation.</li>
<li>Highlights the slower inference time of DMBR and KMBR compared to DBS, suggesting further research on computational efficiency.</li>
<li>Mentions the use of a simple greedy algorithm for DMBR and the potential for more sophisticated approximation algorithms.</li>
</ul></li>
<li>Appendices
<ul>
<li>Appendix A: Provides a proof of submodularity.</li>
<li>Appendix B: Evaluates the coverage of input concepts for CommonGen, noting the low coverage in the experiments.</li>
<li>Appendix C: Shows examples of generations from various decoding algorithms across different tasks.</li>
<li>Appendix D: Evaluates the oversampling strategy as a baseline, comparing its performance to DMBR.</li>
<li>Appendix E: Presents additional figures and tables summarizing the experimental results.</li>
<li>Appendix F: Lists the pre-trained models and codes used in the experiments.</li>
</ul></li>
</ul>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-jinnai-etal-2024-generating" class="csl-entry">
Jinnai, Yuu, Ukyo Honda, Tetsuro Morimura, and Peinan Zhang. 2024. <span>“Generating Diverse and High-Quality Texts by Minimum <span>B</span>ayes Risk Decoding.”</span> In <em>Findings of the Association for Computational Linguistics: ACL 2024</em>, edited by Lun-Wei Ku, Andre Martins, and Vivek Srikumar, 8494–8525. Bangkok, Thailand: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2024.findings-acl.503">https://doi.org/10.18653/v1/2024.findings-acl.503</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2021,
  author = {Bochman, Oren},
  title = {Generating {Diverse} and {High-Quality} {Texts} by {Minimum}
    {Bayes} {Risk} {Decoding}},
  date = {2021-05-10},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/mbr-decoding/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2021" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2021. <span>“Generating Diverse and High-Quality Texts by
Minimum Bayes Risk Decoding.”</span> May 10, 2021. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/mbr-decoding/">https://orenbochman.github.io/notes-nlp/reviews/paper/mbr-decoding/</a>.
</div></div></section></div> ]]></description>
  <category>NLP</category>
  <category>Paper</category>
  <category>Review</category>
  <category>MBR</category>
  <category>Minimum Bayes Risk</category>
  <category>Text Generation</category>
  <category>Decoding Algorithms</category>
  <category>Encoder-Decoder Models</category>
  <category>Language Models</category>
  <category>Machine Translation</category>
  <category>Image Captioning</category>
  <category>Question Generation</category>
  <category>Common Sense Reasoning</category>
  <category>Text Summarization</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/mbr-decoding/</guid>
  <pubDate>Sun, 09 May 2021 21:00:00 GMT</pubDate>
</item>
<item>
  <title>ELMo - Deep contextualized word representations</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/ELMo/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/literature-review-open-book.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Litrature review"><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="img-fluid figure-img" alt="Litrature review"></a></p>
<figcaption>Litrature review</figcaption>
</figure>
</div></div><section id="podcast" class="level2">
<h2 class="anchored" data-anchor-id="podcast">Podcast</h2>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>We introduce a new type of deep contextualized word representation that models both</p>
<ol type="1">
<li><p>complex characteristics of word use (e.g., syntax and semantics), and</p></li>
<li><p>how these uses vary across linguistic contexts (i.e., to model polysemy).</p></li>
</ol>
<p>Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.</p>
</blockquote>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li><strong>I. Introduction</strong>
<ul>
<li>Ideally, word representations should model complex characteristics of word use, such as syntax and semantics, and how these uses vary across linguistic contexts, to model polysemy.</li>
<li>The authors introduces a new type of <strong>deep contextualized word representation</strong> (ELMo) that addresses these challenges, integrates easily into existing models, and improves state-of-the-art results.</li>
</ul></li>
<li><strong>II. ELMo (Embeddings from Language Models)</strong>
<ul>
<li><strong>ELMo representations are functions of the entire input sentence, not just individual tokens</strong>.</li>
<li>They are computed using a <strong>bidirectional LSTM</strong> (biLM) trained on a large text corpus with a language model objective.</li>
<li>ELMo representations are <strong>deep</strong>, in the sense they are a function of all internal layers of the biLM.</li>
<li>A linear combination of the vectors stacked above each input word is learned for each end task.</li>
<li>Internal states are combined to create rich word representations</li>
<li>Higher-level LSTM states capture context-dependent aspects of word meaning (semantics),</li>
<li>Lower-level states model aspects of syntax.</li>
<li>Exposing all of these signals allows learned models to select the most useful types of semi-supervision for each end task.</li>
</ul></li>
<li><strong>III. Bidirectional Language Models (biLM)</strong>
<ul>
<li>A forward language model predicts the next token given the history of previous tokens.</li>
<li>A backward language model predicts the previous token given the future context.</li>
<li>A biLM combines both forward and backward LMs, maximizing the log-likelihood of both directions.</li>
<li>The biLM uses tied parameters for token representations and the Softmax layer in both directions, but maintains separate parameters for LSTMs in each direction.</li>
</ul></li>
<li><strong>IV. ELMo Specifics</strong>
<ul>
<li>For each token, an L-layer biLM computes a set of 2L+1 representations.</li>
<li>ELMo collapses all biLM layers into a single vector using a task-specific weighting of all layers.</li>
<li>A scalar parameter scales the entire ELMo vector.</li>
<li>Layer normalization can be applied to each biLM layer before weighting.</li>
</ul></li>
<li><strong>V. Integrating ELMo into Supervised NLP Tasks</strong>
<ul>
<li>The weights of the pre-trained biLM are frozen, and then the ELMo vector is concatenated with the existing token representation before being passed into the task’s RNN.</li>
<li>For some tasks, ELMo is also included at the output of the task RNN.</li>
<li>Dropout is added to ELMo, and sometimes the ELMo weights are regularized.</li>
</ul></li>
<li><strong>VI. Pre-trained biLM Architecture</strong>
<ul>
<li>The biLMs are similar to previous architectures but modified to support joint training of both directions and add a residual connection between LSTM layers.</li>
<li>The model uses 2 biLSTM layers with 4096 units and 512-dimension projections, with a residual connection.</li>
<li>The context-insensitive type representation uses character n-gram convolutional filters and highway layers.</li>
<li>The biLM provides three layers of representation for each input token.</li>
</ul></li>
<li><strong>VII. Evaluation</strong>
<ul>
<li>ELMo was evaluated on six benchmark NLP tasks, including question answering, textual entailment, and sentiment analysis.</li>
<li><strong>Adding ELMo significantly improves the state-of-the-art in every case</strong>.</li>
<li>For tasks where direct comparisons are possible, ELMo outperforms CoVe.</li>
<li>Deep representations outperform those derived from just the top layer of an LSTM.</li>
</ul></li>
<li><strong>VIII. Task-Specific Results</strong>
<ul>
<li><strong>Question Answering (SQuAD):</strong> ELMo significantly improved the F1 score.</li>
<li><strong>Textual Entailment (SNLI):</strong> ELMo improved accuracy.</li>
<li><strong>Semantic Role Labeling (SRL):</strong> ELMo improved the F1 score.</li>
<li><strong>Coreference Resolution:</strong> ELMo improved the average F1 score.</li>
<li><strong>Named Entity Extraction (NER):</strong> ELMo enhanced biLSTM-CRF achieved a new state-of-the-art F1 score.</li>
<li><strong>Sentiment Analysis (SST-5):</strong> ELMo improved accuracy over the prior state-of-the-art.</li>
</ul></li>
<li><strong>IX. Analysis</strong>
<ul>
<li><mark>Using deep contextual representations improves performance compared to just using the top layer.</mark></li>
<li>ELMo provides better overall performance than representations from a machine translation encoder like CoVe.</li>
<li>Syntactic information is better represented at lower layers, while semantic information is better captured at higher layers.</li>
<li><strong>Including ELMo at both the input and output layers of the supervised model can improve performance for some tasks</strong>.</li>
<li>ELMo increases sample efficiency, requiring fewer training updates and less data to reach the same level of performance.</li>
<li>The contextual information captured by ELMo is more important than the sub-word information.</li>
<li>Pre-trained word vectors provide a marginal improvement when used with ELMo.</li>
</ul></li>
<li><strong>X. Key Findings</strong>
<ul>
<li><strong>ELMo efficiently encodes different types of syntactic and semantic information about words in context</strong>.</li>
<li>Using all layers of the biLM improves overall task performance.</li>
<li><strong>ELMo provides a general approach for learning high-quality, deep, context-dependent representations</strong>.</li>
</ul></li>
</ul>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2021,
  author = {Bochman, Oren},
  title = {ELMo - {Deep} Contextualized Word Representations},
  date = {2021-05-09},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/ELMo/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2021" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2021. <span>“ELMo - Deep Contextualized Word
Representations.”</span> May 9, 2021. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/ELMo/">https://orenbochman.github.io/notes-nlp/reviews/paper/ELMo/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Bidirectional LSTM</category>
  <category>Deep learning</category>
  <category>Embeddings</category>
  <category>LLM</category>
  <category>NLP</category>
  <category>Paper</category>
  <category>Review</category>
  <category>Stub</category>
  <category>Podcast</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/ELMo/</guid>
  <pubDate>Sat, 08 May 2021 21:00:00 GMT</pubDate>
</item>
<item>
  <title>Exposing Attention Glitches with Flip-Flop Language Modeling</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/exposing-glitches/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/literature-review-open-book.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Litrature review"><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="img-fluid figure-img" alt="Litrature review"></a></p>
<figcaption>Litrature review</figcaption>
</figure>
</div></div><section id="podcast" class="level2">
<h2 class="anchored" data-anchor-id="podcast">Podcast</h2>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>Why do large language models sometimes output factual inaccuracies and exhibit erroneous reasoning? The brittleness of these models, particularly when executing long chains of reasoning, currently seems to be an inevitable price to pay for their advanced capabilities of coherently synthesizing knowledge, pragmatics, and abstract thought. Towards making sense of this fundamentally unsolved problem, this work identifies and analyzes the phenomenon of attention glitches, in which the Transformer architecture’s inductive biases intermittently fail to capture robust reasoning. To isolate the issue, we introduce flip-flop language modeling (FFLM), a parametric family of synthetic benchmarks designed to probe the extrapolative behavior of neural language models. This simple generative task requires a model to copy binary symbols over long-range dependencies, ignoring the tokens in between. We find that Transformer FFLMs suffer from a long tail of sporadic reasoning errors, some of which we can eliminate using various regularization techniques. Our preliminary mechanistic analyses show why the remaining errors may be very difficult to diagnose and resolve. We hypothesize that attention glitches account for (some of) the closed-domain hallucinations in natural LLMs. – <span class="citation" data-cites="liu2023exposingattentionglitchesflipflop">(Liu et al. 2023)</span></p>
</blockquote>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li>Introduction
<ul>
<li>Describes the problem of factual inaccuracies and erroneous reasoning in large language models (LLMs), particularly in long chains of reasoning.</li>
<li>Presents integer addition problems as a simple example of algorithmic reasoning where LLMs exhibit sporadic errors, highlighting both their capabilities and limitations.</li>
<li>Introduces the concept of “attention glitches” as a potential explanation for these errors, suggesting that the Transformer architecture’s inductive biases may intermittently fail to capture robust reasoning.</li>
</ul></li>
<li>Flip-flop Automata and the FFLM Task
<ul>
<li>Defines flip-flop strings and flip-flop languages, focusing on a canonical family parameterized by the probabilities of write, read, and ignore instructions.</li>
<li>Introduces the flip-flop language modeling (FFLM) task, which involves training language models to generate or predict continuations of flip-flop strings, emphasizing the importance of perfect read operation accuracy.</li>
<li>Discusses the rationale for focusing on flip-flops, highlighting their role as fundamental building blocks of memory and their relevance to various reasoning tasks.</li>
</ul></li>
<li>Attention Glitches: A Long Tail of Errors for Transformer FFLMs
<ul>
<li>Presents the main empirical result: Transformer models trained on FFLM exhibit a long tail of unpredictable reasoning errors (attention glitches), even on simple tasks like remembering one bit.</li>
<li>Highlights the contrast between Transformers and LSTMs, showing that LSTMs achieve perfect accuracy on FFLM with significantly fewer resources.</li>
<li>Notes that similar attention glitches are observed in real LLMs when prompted to complete natural language embeddings of flip-flop tasks.</li>
<li>Discusses multiple potential mechanisms for attention glitches, including implicit n-gram models, Lipschitz limitations of soft attention, and the difficulty of non-commutative tiebreaking.</li>
</ul></li>
<li>Mitigations for Attention Glitches
<ul>
<li>Investigates various approaches to eliminate attention glitches in Transformer FFLMs, using a 6-layer 19M-parameter model as a canonical baseline.</li>
<li>Discusses the effects of training data and scale, showing that training on rare sequences significantly reduces errors, while resource scaling provides weaker improvements.</li>
<li>Explores indirect algorithmic controls, including standard regularization techniques and attention-sharpening regularizers, finding that some choices improve extrapolation but none completely eliminate glitches.</li>
<li>Presents a preliminary mechanistic study of trained networks, showing that attention-sharpening promotes hard attention but errors persist due to the complexity and redundancy of attention patterns.</li>
</ul></li>
<li>Conclusion and Future Challenges
<ul>
<li>Summarizes the findings, emphasizing that attention glitches represent a systematic architectural flaw in Transformers that may contribute to closed-domain hallucinations in natural LLMs.</li>
<li>Discusses the challenges of confirming or refuting the hypothesis that attention glitches cause hallucinations in natural LLMs, highlighting the need for further research.</li>
<li>Suggests potential paths to hallucination-free Transformers, including data diversity, scale, regularization, and architectural innovations inspired by recurrent models.</li>
<li>Mentions the broader impacts and limitations of the work, emphasizing its foundational nature and the potential for unintended consequences of improved factual reliability in LLMs.</li>
</ul></li>
<li>Appendix
<ul>
<li>Provides deferred background information on flip-flop terminology and history, including the definition of the flip-flop automaton and its transformation monoid.</li>
<li>Discusses additional related work on hallucinations, long-range dependencies, explicit memory mechanisms, and Transformers’ performance on algorithmic tasks.</li>
<li>Explains the rationale for the specific flip-flop language used in the study, highlighting its compatibility with standard language modeling and its parsimonious encoding.</li>
<li>Elaborates on the hypothesis that attention glitches cause hallucinations in natural LLMs, discussing the challenges of formalizing and testing this hypothesis.</li>
<li>Presents full experimental results, including details for LLM addition prompts, extrapolation failures of standard Transformers, effects of training data and scale, indirect algorithmic controls, and preliminary mechanistic studies.</li>
<li>Provides proofs for propositions related to the realizability of FFL by small Transformers, the failure of soft attention due to attention dilution, and the failure of hard attention due to bad margins for positional embeddings.</li>
<li>Notes the software, compute infrastructure, and resource costs associated with the experiments.</li>
</ul></li>
</ul>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-liu2023exposingattentionglitchesflipflop" class="csl-entry">
Liu, Bingbin, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. 2023. <span>“Exposing Attention Glitches with Flip-Flop Language Modeling.”</span> <a href="https://arxiv.org/abs/2306.00946">https://arxiv.org/abs/2306.00946</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2021,
  author = {Bochman, Oren},
  title = {Exposing {Attention} {Glitches} with {Flip-Flop} {Language}
    {Modeling}},
  date = {2021-05-09},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/exposing-glitches/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2021" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2021. <span>“Exposing Attention Glitches with Flip-Flop
Language Modeling.”</span> May 9, 2021. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/exposing-glitches/">https://orenbochman.github.io/notes-nlp/reviews/paper/exposing-glitches/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>LSTM</category>
  <category>Deep learning</category>
  <category>LLM</category>
  <category>NLP</category>
  <category>Paper</category>
  <category>Podcast</category>
  <category>Review</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/exposing-glitches/</guid>
  <pubDate>Sat, 08 May 2021 21:00:00 GMT</pubDate>
</item>
<item>
  <title>Attention Is All You Need</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/2017-attention-is-all-you-need/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/literature-review-open-book.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Litrature review"><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="img-fluid figure-img" alt="Litrature review"></a></p>
<figcaption>Litrature review</figcaption>
</figure>
</div></div><section id="attention-is-all-you-need" class="level1">
<h1>Attention Is All You Need</h1>
<p>Attention Is All You Need is a paper by Vaswani et al, 2017. It is a seminal paper that introduced the transformer architecture, which has since become the backbone of many state-of-the-art models in natural language processing (NLP). The transformer architecture is known for its parallelism, scalability, and ability to capture long-range dependencies in sequences. The key innovation in the transformer is the self-attention mechanism, which allows the model to weigh the importance of different parts of the input sequence when making predictions. This mechanism has proven to be highly effective in capturing complex patterns in language data and has led to significant improvements in a wide range of NLP tasks.</p>
</section>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data –<span class="citation" data-cites="vaswani2023attentionneed">(Vaswani et al. 2023)</span></p>
</blockquote>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-vaswani2023attentionneed" class="csl-entry">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. <span>“Attention Is All You Need.”</span> <a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2021,
  author = {Bochman, Oren},
  title = {Attention {Is} {All} {You} {Need}},
  date = {2021-05-08},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/2017-attention-is-all-you-need/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2021" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2021. <span>“Attention Is All You Need.”</span> May 8,
2021. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/2017-attention-is-all-you-need/">https://orenbochman.github.io/notes-nlp/reviews/paper/2017-attention-is-all-you-need/</a>.
</div></div></section></div> ]]></description>
  <category>NLP</category>
  <category>Paper</category>
  <category>Attention</category>
  <category>Deep learning</category>
  <category>Review</category>
  <category>Stub</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/2017-attention-is-all-you-need/</guid>
  <pubDate>Fri, 07 May 2021 21:00:00 GMT</pubDate>
</item>
<item>
  <title>Effective Approaches to Attention-based Neural Machine Translation</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/effective/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/literature-review-open-book.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Literature review"><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="img-fluid figure-img" alt="Literature review"></a></p>
<figcaption>Literature review</figcaption>
</figure>
</div></div><p>This review is a bit of a mess, it has gone through at least three editions of this blog and I have learned much about writing and structuring a review since then. I think it needs a bit of an overhaul. This latest version is a step in the right direction.</p>
<p>There are many other review of this paper but I think that covering this paper is highly relevant followup to the assignments for <a href="http://localhost:7780/notes/c4w1/">NMT</a> as well as the earlier assignment in for <a href="http://localhost:7780/notes/c1w4/">MT with KNN</a> in the Classification and Vector Space Models course.</p>

<div class="no-row-height column-margin column-container"><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/IxQtK2SjWWM?t=7" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Attention models by Christopher D. Manning
</figcaption>
</figure>
</div><div id="vid-02" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/R05UzD8SQLE?t=6" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;2: Paper Explained by Professor. Maziar Raissi
</figcaption>
</figure>
</div><div id="vid-03" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video"><video id="video_shortcode_videojs_video1" class="video-js vjs-default-skin vjs-fluid" controls="" preload="auto" data-setup="{}" title=""><source src="https://aclanthology.org/D15-1166.mp4"></video></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;3: Presentation in Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing
</figcaption>
</figure>
</div></div>

<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/in_a_nutshell.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Attention in a Nutshell"><img src="https://orenbochman.github.io/notes-nlp/images/in_a_nutshell.jpg" class="img-fluid figure-img" alt="Attention in a Nutshell"></a></p>
<figcaption>Attention in a Nutshell</figcaption>
</figure>
</div>
<p>This is an attention paper for machine translation.</p>
<p>The earlier work in <span class="citation" data-cites="bahdanau2016neuralmachinetranslationjointly">Bahdanau, Cho, and Bengio (2016)</span> the authors used a bidirectional RNN with (GRUs) as the encoder and a unidirectional RNN with GRUs as the decoder. The attention mechanism dynamically aligned source words with the target words during decoding.</p>
<p>In this paper the authors used stacked LSTMs for both the encoder and decoder. The paper proposed two simple and effective classes of attention mechanisms: a global approach that always attends to all source words and a local approach that only looks at a subset of source words at a time. The paper demonstrated the effectiveness of both approaches on the WMT translation tasks between English and German in both directions. The paper also introduced the concept of input-feeding approach, which feeds attentional vectors as inputs to the next time steps to inform the model about past alignment decisions.</p>
<p>The paper is a must-read for anyone interested in neural machine translation and attention mechanisms in NLP. Not long after the transformer architecture was introduced in 2017 with attention becoming the backbone of the model.</p>
</div>
</div>
<section id="podcast" class="level2">
<h2 class="anchored" data-anchor-id="podcast">Podcast</h2>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
</section>
<section id="effective-approaches-to-attention-based-neural-machine-translation" class="level2">
<h2 class="anchored" data-anchor-id="effective-approaches-to-attention-based-neural-machine-translation">Effective Approaches to Attention-based Neural Machine Translation</h2>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches on the WMT translation tasks between English and German in both directions.</p>
</blockquote>
<blockquote class="blockquote">
<p>With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout. Our ensemble model using different attention architectures yields a new state-of-the-art result in the WMT’15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker. – <span class="citation" data-cites="luong2015effectiveapproachesattentionbasedneural">(Luong, Pham, and Manning 2015)</span></p>
</blockquote>
</section>
<section id="dot-product-attention" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="dot-product-attention">Dot-Product Attention</h2>
<p><strong>Dot-Product attention</strong> is the first of three attention mechanisms covered in the course and the simplest covered in this paper. Dot-Product Attention is a good fit, in an engineering sense, for a encoder-decoder architecture with tasks where the source source sequence is fully available at the start and the tasks is mapping or transformation the source sequence to an output sequence like in alignment, or translation.</p>
<div id="fig-01" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/fig-01.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;1: Neural machine translation – a stacking recurrent architecture for translating a source sequence A B C D into a target sequence X Y Z. Here, <eos> marks the end of a sentence."><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/effective/img/fig-01.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Neural machine translation – a stacking recurrent architecture for translating a source sequence A B C D into a target sequence X Y Z. Here, <code>&lt;eos&gt;</code> marks the end of a sentence.
</figcaption>
</figure>
</div>
<p>The first assignment in the course using encoder decoder LSTM model with attention is so similar to the setup disused in this paper, I would not be surprised if it may well have inspired it.</p>
<hr>
<p>This is a review of the paper in which scaled dot product attention was introduced in 2015 by <em>Minh-Thang Luong, Hieu Pham, Christopher D. Manning</em> in <a href="https://arxiv.org/pdf/1508.04025v5.pdf">Effective Approaches to Attention-based Neural Machine Translation</a> which is available at <a href="https://paperswithcode.com/paper/effective-approaches-to-attention-based">papers with code</a>. In this paper they tried to take the attention mechanism being used in other tasks and to distill it to its essence and at the same time to also find a more general form.</p>
<div class="columns">
<div class="column" style="width:45%;">
<div id="fig-02" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="img/fig-02.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Global Attention"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/effective/img/fig-02.png" class="img-fluid figure-img"></a></p>
<figcaption>Global Attention</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Global attentional model – at each time step t, the model infers a variable-length alignment weight vector at based on the current target state ht and all source states ̄hs. A global contextvector ct is then computed as the weighted average, according to at, over all the source states.
</figcaption>
</figure>
</div>
</div><div class="column" style="width:10%;">
<!-- empty column to create gap -->
</div><div class="column" style="width:45%;">
<div id="fig-03" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="img/fig-03.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Local attention"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/effective/img/fig-03.png" class="img-fluid figure-img"></a></p>
<figcaption>Local attention</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Local attention model – the model first predicts a single aligned position <img src="https://latex.codecogs.com/png.latex?p_t"> or the current target word. A window centered around the source position <img src="https://latex.codecogs.com/png.latex?p_t"> is then used to compute a context vector <img src="https://latex.codecogs.com/png.latex?c_t">, a weighted average of the source hidden states in the window. The weights at are inferred from the current target state <img src="https://latex.codecogs.com/png.latex?h_t"> and those source states <img src="https://latex.codecogs.com/png.latex?%5Cbar%7Bh%7D_s"> in the window.
</figcaption>
</figure>
</div>
</div>
</div>
<div id="fig-04" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-04-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="img/fig-04.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Input-feeding approach"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/effective/img/fig-04.png" class="img-fluid figure-img"></a></p>
<figcaption>Input-feeding approach</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-04-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Input-feeding approach – Attentional vectors <img src="https://latex.codecogs.com/png.latex?%5Cbar%7Bh%7D_s"> are fed as inputs to the next time steps to inform the model about past alignment decisions
</figcaption>
</figure>
</div>
<div id="fig-05" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-05-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="img/fig-05.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Learning curves"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/effective/img/fig-05.png" class="img-fluid figure-img"></a></p>
<figcaption>Learning curves</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-05-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Learning curves – test cost (ln perplexity) on newstest2014 for English-German NMTs as training progresses.
</figcaption>
</figure>
</div>
<div id="fig-06" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-06-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="img/fig-06.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Length Analysis"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/effective/img/fig-06.png" class="img-fluid figure-img"></a></p>
<figcaption>Length Analysis</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-06-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Length Analysis – translation qualities of different systems as sentences become longer
</figcaption>
</figure>
</div>
<p>They also came up with a interesting way to visualize the alignment’s attention mechanism.</p>
<div id="fig-07" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-07-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="img/fig-07.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="alignment-visulization"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/effective/img/fig-07.png" class="img-fluid figure-img"></a></p>
<figcaption>alignment-visulization</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-07-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Alignment visualizations – shown are images of the attention weights learned by various models: (top left) global, (top right) local-m, and (bottom left) local-p.&nbsp;The gold alignments are displayed at the bottom right corner.
</figcaption>
</figure>
</div>
<p>So to recap: Luong et all were focused on alignment problem in NMT. When they try to tackle it using attention as function of the content and a function of its location. They came up with a number of ways to distill and generalize the attention mechanism.</p>
<hr>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="img/page01.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="page 1"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/effective/img/page01.png" class="img-fluid figure-img" alt="page 1"></a></p>
<figcaption>page 1</figcaption>
</figure>
</div></div><p>Attention was just another engineering technique to improve alignment and it had not yet taken center stage in the models, as it would in <a href="img/https://arxiv.org/abs/1706.03762">Attention Is All We Need</a> (<span class="citation" data-cites="vaswani2023attentionneed">Vaswani et al. (2023)</span>).I find it useful to garner the concepts and intuition which inspired these researchers to adapt attention and how they come up with this form of attention.</p>
<p>The abstract begins with:</p>
<blockquote class="blockquote">
<p>“An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation.”</p>
</blockquote>
<p>which was covered in last lesson. The abstract continues with:</p>
<blockquote class="blockquote">
<p>“This paper examines two simple and effective classes of attentional mechanism: a <strong>global</strong> approach which always attends to <strong>all</strong> source words and a <strong>local</strong> one that only looks at a <strong>subset</strong> of source words at a time.”</p>
</blockquote>
<p>talks about</p>
<hr>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="img/page02.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="page 2"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/effective/img/page02.png" class="img-fluid figure-img" alt="page 2"></a></p>
<figcaption>page 2</figcaption>
</figure>
</div></div></section>
<section id="neural-machine-translation" class="level2">
<h2 class="anchored" data-anchor-id="neural-machine-translation">§2 Neural Machine Translation:</h2>
<p>This section provides a summary of the the NMT task using 4 equations: In particular they note that in the decoder the conditional probability of the target given the source is of the form: <img src="https://latex.codecogs.com/png.latex?%0Alog%20%5Cspace%20p(y%20%5Cvert%20x)%20=%20%5Csum_%7Bj=1%7D%5Em%20log%20%5Cspace%20p%20(y_j%20%5Cvert%20y_%7B%3Cj%7D%20,%20s)%0A"></p>
<p>Where <img src="https://latex.codecogs.com/png.latex?x_i"> are the <em>source</em> sentence and <img src="https://latex.codecogs.com/png.latex?y_i"> are the <em>target</em> sentence. <img src="https://latex.codecogs.com/png.latex?%0Ap%20(y_j%20%5Cvert%20y%7B%3Cj%7D%20,%20s)%20=%20softmax%20(g(h_j))%0A"></p>
<p>Here, <img src="https://latex.codecogs.com/png.latex?h_j"> is the RNN hidden unit, abstractly computed as: <img src="https://latex.codecogs.com/png.latex?%0Ah_j%20=%20f(h_%7Bj-1%7D,s)%0A"></p>
<p>Our training objective is formulated as follows <img src="https://latex.codecogs.com/png.latex?%0AJ_t=%5Csum_%7B(x,y)%5Cin%20D%7D%20-log%20%5Cspace%20p(x%20%5Cvert%20y)%0A"></p>
<p>With D being our parallel training corpus.</p>
<hr>
</section>
<section id="overview-of-attention" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="overview-of-attention">§3 Overview of attention</h2>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="img/page03.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12" title="page 3"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/effective/img/page03.png" class="img-fluid figure-img" alt="page 3"></a></p>
<figcaption>page 3</figcaption>
</figure>
</div></div><p>Next they provide a recap of the attention mechanism to set their starting point: &gt;Specifically, given the target hidden state <img src="https://latex.codecogs.com/png.latex?h_t"> and the source-side context vector <img src="https://latex.codecogs.com/png.latex?c_t">, we employ a simple concatenation layer to combine the information from both vectors to produce an attentional hidden state as follows:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbar%7Bh%7D_t%20=%20tanh(W_c%5Bc_t;h_t%5D)%0A"></p>
<blockquote class="blockquote">
<p>The attentional vector <img src="https://latex.codecogs.com/png.latex?%5Cbar%7Bh%7D_t"> is then fed through the softmax layer to produce the predictive distribution formulated as:</p>
</blockquote>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap(y_t%7Cy%7B%3Ct%7D,%20x)%20=%20softmax(W_s%5Cbar%7Bh%7D_t)%0A"></p>
</section>
<section id="global-attention" class="level2">
<h2 class="anchored" data-anchor-id="global-attention">§3.1 Global attention</h2>
<p>This is defined in §3.1 of the paper as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%20%20%20a_t(s)%20&amp;%20=%20align(h_t,%5Cbar%7Bh%7D_s)%20%20%5Cnewline%0A%20%20%20&amp;%20=%20%5Cfrac%7B%20e%5E%7Bscore(h_t,%5Cbar%7Bh%7D_s)%7D%20%7D%7B%20%5Csum_%7Bs'%7D%20e%5E%7Bscore(h_t,%5Cbar%7Bh%7D_s)%7D%20%7D%20%5Cnewline%0A%20%20%20&amp;%20=%20softmax(score(h_t,%5Cbar%7Bh%7D_s))%0A%5Cend%7Balign%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?h_t"> and <img src="https://latex.codecogs.com/png.latex?h_s"> are the target and source sequences and <img src="https://latex.codecogs.com/png.latex?score()"> which is referred to as a <em>content-based</em> function as one of three alternative forms provided:</p>
<section id="dot-product-attention-1" class="level3">
<h3 class="anchored" data-anchor-id="dot-product-attention-1">Dot product attention:</h3>
<p><img src="https://latex.codecogs.com/png.latex?%0Ascore(h_t,%5Cbar%7Bh%7D_s)=h_t%5ET%5Cbar%7Bh%7D_s%0A"> This form combines the source and target using a dot product. Geometrically this essentially a projection operation.</p>
</section>
<section id="general-attention" class="level3">
<h3 class="anchored" data-anchor-id="general-attention">General attention:</h3>
<p><img src="https://latex.codecogs.com/png.latex?%0Ascore(h_t,%5Cbar%7Bh%7D_s)=h_t%5ETW_a%5Cbar%7Bh%7D_s%0A"></p>
<p>this form combines the source and target using a dot product after applying a learned attention weights to the source. Geometrically this is a projection of the target on a linear transformation of the source or <strong>scaled dot product attention</strong> as it is now known</p>
</section>
<section id="concatenative-attention" class="level3">
<h3 class="anchored" data-anchor-id="concatenative-attention">Concatenative attention:</h3>
<p><img src="https://latex.codecogs.com/png.latex?%0Ascore(h_t,%5Cbar%7Bh%7D_s)=v_a%5ET%20tanh(W_a%20%5Bh_t;%5Cbar%7Bh%7D_s%5D)%0A"></p>
<p>This is a little puzzling <img src="https://latex.codecogs.com/png.latex?v_a%5ET"> is not accounted for and seems to be a learned attention vector which is projected onto the linearly weighted combination of the hidden states of the encoder and decoder. they also mention having considered using a <em>location based function</em> location :</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Aa_t%20=%20softmax(W_a%20h_t)%0A"></p>
which is just a linear transform of the hidden target state <img src="https://latex.codecogs.com/png.latex?h_t">
<hr>
</section>
</section>
<section id="local-attention" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="local-attention">§3.2 Local Attention</h2>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="img/page04.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13" title="page 4"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/effective/img/page04.png" class="img-fluid figure-img" alt="page 4"></a></p>
<figcaption>page 4</figcaption>
</figure>
</div></div><p>in §3.2 they consider a local attention mechanism. This is a resource saving modification of global attention using the simple concept of applying the mechanism within a fixed sized window.</p>
<blockquote class="blockquote">
<p>We propose a local attentional mechanism that chooses to focus only on a small subset of the source positions per target word. This model takes inspiration from the tradeoff between the soft and hard attentional models proposed by Xu et al.&nbsp;(2015) to tackle the image caption generation task.</p>
</blockquote>
<blockquote class="blockquote">
<p>Our local attention mechanism selectively focuses on a small window of context and is differentiable. … In concrete details, the model first generates an aligned position <img src="https://latex.codecogs.com/png.latex?p_t"> for each target word at time <img src="https://latex.codecogs.com/png.latex?t">. The context vector <img src="https://latex.codecogs.com/png.latex?c_t"></p>
</blockquote>
<p>is then derived as a weighted average over the set of source hidden states within the window <img src="https://latex.codecogs.com/png.latex?%5Bp_t%E2%88%92D,%20p_t+D%5D">; Where <img src="https://latex.codecogs.com/png.latex?D"> is empirically selected. The <em>big idea</em> here is to use a fixed window size for this step to conserve resources when translating paragraphs or documents - a laudable notion for times where LSTM gobbled up resources in proportion to the sequence length…</p>
<p>They also talk about <em>monotonic alignment</em> where <img src="https://latex.codecogs.com/png.latex?p_t=t"> and <em>predictive alignment</em></p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap_t=S%5Ccdot%20sigmoid(v_p%5ETtanh(W_ph_t))%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0Aa_t(s)=align(h_t,%5Cbar%7Bh%7D_s)e%5E%7B(-%5Cfrac%7B(s-p_t)%5E2%7D%7Bs%5Csigma%5E2%7D)%7D%0A"></p>
<p>with align() as defined above and</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Csigma=%5Cfrac%7BD%7D%7B2%7D%0A"></p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="img/page05.png" class="lightbox" data-gallery="quarto-lightbox-gallery-14" title="page 5"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/effective/img/page05.png" class="img-fluid figure-img" alt="page 5"></a></p>
<figcaption>page 5</figcaption>
</figure>
</div></div><p>I found the rest of the paper lesser interest</p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="img/page06.png" class="lightbox" data-gallery="quarto-lightbox-gallery-15" title="page 6"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/effective/img/page06.png" class="img-fluid figure-img" alt="page 6"></a></p>
<figcaption>page 6</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="img/page07.png" class="lightbox" data-gallery="quarto-lightbox-gallery-16" title="page 7"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/effective/img/page07.png" class="img-fluid figure-img" alt="page 7"></a></p>
<figcaption>page 7</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="img/page08.png" class="lightbox" data-gallery="quarto-lightbox-gallery-17" title="page 8"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/effective/img/page08.png" class="img-fluid figure-img" alt="page 8"></a></p>
<figcaption>page 8</figcaption>
</figure>
</div></div>

<p>In §5.4 In alignment quality</p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="img/page09.png" class="lightbox" data-gallery="quarto-lightbox-gallery-18" title="page 9"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/effective/img/page09.png" class="img-fluid figure-img" alt="page 9"></a></p>
<figcaption>page 9</figcaption>
</figure>
</div></div><p>some sample translations</p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="img/page10.png" class="lightbox" data-gallery="quarto-lightbox-gallery-19" title="page 10"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/effective/img/page10.png" class="img-fluid figure-img" alt="page 10"></a></p>
<figcaption>page 10</figcaption>
</figure>
</div></div><p>the references</p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="img/page11.png" class="lightbox" data-gallery="quarto-lightbox-gallery-20" title="page 11"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/effective/img/page11.png" class="img-fluid figure-img" alt="page 11"></a></p>
<figcaption>page 11</figcaption>
</figure>
</div></div><p>This is appendix A which shows the visualization of alignment weights.</p>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<!-- mark with strikeout supperplous and obvious entries -->
<ul>
<li>Introduction
<ul>
<li>Lists the advantages of neural machine translation (NMT)
<ul>
<li>Minimal domain knowledge</li>
<li>Conceptual simplicity</li>
<li>Ability to generalize well to long sequences</li>
<li>Small memory footprint</li>
<li>Easy implementation of decoders</li>
</ul></li>
<li>Discusses the concept of attention in neural networks</li>
<li>Mentions different applications of attention in different tasks</li>
<li>Highlights the application of attention mechanism in NMT by Bahdanau et al.&nbsp;(2015) and the lack of further exploration</li>
<li>Presents the purpose of the paper, which is to design two novel types of attention-based models
<ul>
<li>A global approach</li>
<li>A local approach</li>
</ul></li>
<li>Presents the experimental evaluation of the proposed approaches on WMT translation tasks and its analysis</li>
</ul></li>
<li>Neural Machine Translation
<ul>
<li>Describes the conditional probability of translating a source sentence to a target sentence</li>
<li>Presents the two components of a basic NMT system an Encoder and a Decoder</li>
<li>Discusses the use of recurrent neural network (RNN) architectures in NMT</li>
<li>Presents the parameterization of the probability of decoding each word in the target sentence</li>
<li>Presents the training objective used in NMT</li>
</ul></li>
<li>Attention-based Models
<ul>
<li>Classifies the various attention-based models into two broad categories
<ul>
<li>Global attention</li>
<li>Local attention</li>
</ul></li>
<li>Presents the common process followed by both global and local attention models for deriving the context vector</li>
<li>Describes the concatenation of target hidden state and source-side context vector for prediction</li>
</ul></li>
<li>Global Attention
<ul>
<li>Describes the concept of global attention model</li>
<li>Presents the derivation of a variable-length alignment vector by comparing the current target hidden state with each source hidden state</li>
<li>Presents three different alternatives for the content-based function used in calculating the alignment vector</li>
<li>Describes the location-based function used in early attempts to build attention-based models</li>
<li>Describes the calculation of the context vector as the weighted average over all the source hidden states</li>
<li>Presents a comparison of the proposed global attention approach to the model by Bahdanau et al.&nbsp;(2015)
<ul>
<li>Simpler architecture</li>
<li>Simpler computation path</li>
<li>Use of multiple alignment functions</li>
</ul></li>
</ul></li>
<li>Local Attention
<ul>
<li>Describes the concept of local attention model</li>
<li>Mentions the inspiration from the soft and hard attentional models</li>
<li>Describes the selection of a small window of context and its advantages over soft and hard attention models</li>
<li>Presents two variants of the local attention model
<ul>
<li>Monotonic alignment</li>
<li>Predictive alignment</li>
</ul></li>
<li>Describes the comparison to the selective attention mechanism by Gregor et al.&nbsp;(2015)</li>
</ul></li>
<li>Input-feeding Approach
<ul>
<li>Describes the suboptimal nature of making independent attentional decisions in the global and local approaches</li>
<li>Discusses the need for joint alignment decisions taking into account past alignment information</li>
<li>Presents the input-feeding approach</li>
<li>Mentions the effects of input-feeding approach
<ul>
<li>Makes the model fully aware of previous alignment choices</li>
<li>Creates a deep network spanning both horizontally and vertically</li>
</ul></li>
<li>Presents the comparison to other related works
<ul>
<li>Use of context vectors by Bahdanau et al.&nbsp;(2015)</li>
<li>Doubly attentional approach by Xu et al.&nbsp;(2015)</li>
</ul></li>
</ul></li>
<li>Experiments
<ul>
<li>Describes the evaluation setup and datasets used
<ul>
<li>newstest2013 as development set</li>
<li>newstest2014 and newstest2015 as test sets</li>
</ul></li>
<li>Mentions the use of case-sensitive BLEU for reporting translation performances</li>
<li>Describes the two types of BLEU used
<ul>
<li>Tokenized BLEU</li>
<li>NIST BLEU</li>
</ul></li>
</ul></li>
<li>Training Details
<ul>
<li>Describes the data used for training NMT systems
<ul>
<li>WMT’14 training data</li>
</ul></li>
<li>Presents the details of vocabulary size and filtering criteria used</li>
<li>Discusses the architecture of the LSTM models and training settings</li>
<li>Mentions the training speed and time</li>
</ul></li>
<li>English-German Results
<ul>
<li>Discusses the different systems used for comparison</li>
<li>Presents the progressive improvements achieved by
<ul>
<li>Reversing the source sentence</li>
<li>Using dropout</li>
<li>Using global attention approach</li>
<li>Using input-feeding approach</li>
<li>Using local attention model with predictive alignments</li>
</ul></li>
<li>Notes the correlation between perplexity and translation quality</li>
<li>Mentions the use of unknown replacement technique and the achievement of new SOTA result by ensembling 8 different models</li>
<li>Describes the results of testing the models on newstest2015 and the establishment of new SOTA performance</li>
</ul></li>
<li>German-English Results
<ul>
<li>Mentions the evaluation setup for the German-English translation task</li>
<li>Presents the results highlighting the effectiveness of
<ul>
<li>Attentional mechanism</li>
<li>Input-feeding approach</li>
<li>Content-based dot product function with dropout</li>
<li>Unknown word replacement technique</li>
</ul></li>
</ul></li>
<li>Analysis
<ul>
<li>Describes the purpose of conducting extensive analysis
<ul>
<li>Understanding of the learning process</li>
<li>Ability to handle long sentences</li>
<li>Choice of attentional architectures</li>
<li>Alignment quality</li>
</ul></li>
</ul></li>
<li>Learning Curves
<ul>
<li>Presents the analysis of the learning curves for different models</li>
<li>Notes the separation between non-attentional and attentional models</li>
<li>Briefly mentions the effectiveness of input-feeding approach and local attention models</li>
</ul></li>
<li>Effects of Translating Long Sentences
<ul>
<li>Briefly discusses the grouping of sentences based on lengths and computation of BLEU score per group</li>
<li>Mentions the effectiveness of attentional models in handling long sentences</li>
<li>Notes the superior performance of the best model across all sentence length buckets</li>
</ul></li>
<li>Choices of Attentional Architectures
<ul>
<li>Presents the analysis of different attention models and alignment functions</li>
<li>Highlights the poor performance of the location-based function</li>
<li>Briefly mentions the performance of content-based functions
<ul>
<li>Good performance of dot function for global attention</li>
<li>Better performance of general function for local attention</li>
</ul></li>
<li>Notes the best performance of local attention model with predictive alignments</li>
</ul></li>
<li>Alignment Quality
<ul>
<li>Briefly discusses the use of alignment error rate (AER) metric to evaluate the alignment quality</li>
<li>Mentions the data used for evaluating alignment quality and the process of extracting one-to-one alignments</li>
<li>Presents the results of AER evaluation and the comparison to Berkeley aligner</li>
<li>Notes the better performance of local attention models compared to the global one</li>
<li>Briefly discusses the AER of the ensemble</li>
</ul></li>
</ul>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-bahdanau2016neuralmachinetranslationjointly" class="csl-entry">
Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2016. <span>“Neural Machine Translation by Jointly Learning to Align and Translate.”</span> <a href="https://arxiv.org/abs/1409.0473">https://arxiv.org/abs/1409.0473</a>.
</div>
<div id="ref-luong2015effectiveapproachesattentionbasedneural" class="csl-entry">
Luong, Minh-Thang, Hieu Pham, and Christopher D. Manning. 2015. <span>“Effective Approaches to Attention-Based Neural Machine Translation.”</span> <a href="https://arxiv.org/abs/1508.04025">https://arxiv.org/abs/1508.04025</a>.
</div>
<div id="ref-vaswani2023attentionneed" class="csl-entry">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. <span>“Attention Is All You Need.”</span> <a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2021,
  author = {Bochman, Oren},
  title = {Effective {Approaches} to {Attention-based} {Neural}
    {Machine} {Translation}},
  date = {2021-05-08},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/effective/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2021" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2021. <span>“Effective Approaches to Attention-Based
Neural Machine Translation.”</span> May 8, 2021. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/effective/">https://orenbochman.github.io/notes-nlp/reviews/paper/effective/</a>.
</div></div></section></div> ]]></description>
  <category>NLP</category>
  <category>Paper</category>
  <category>Attention</category>
  <category>Deep learning</category>
  <category>Review</category>
  <category>Podcast</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/effective/</guid>
  <pubDate>Fri, 07 May 2021 21:00:00 GMT</pubDate>
</item>
<item>
  <title>Putting the “Re” in Reformer: Ungraded Lab</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/c4w4/lab02.html</link>
  <description><![CDATA[ 





<p>This ungraded lab will explore Reversible Residual Networks. You will use these networks in this week’s assignment that utilizes the Reformer model. It is based on on the Transformer model you already know, but with two unique features. * Locality Sensitive Hashing (LSH) Attention to reduce the compute cost of the dot product attention and * Reversible Residual Networks (RevNets) organization to reduce the storage requirements when doing backpropagation in training.</p>
<p>In this ungraded lab we’ll start with a quick review of Residual Networks and their implementation in Trax. Then we will discuss the Revnet architecture and its use in Reformer.</p>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li>Part 1: Residual Networks
<ul>
<li>1.1 Branch</li>
<li>1.2 Residual Model</li>
</ul></li>
<li>Part 2: Reversible Residual Networks
<ul>
<li>2.1 Trax Reversible Layers</li>
<li>2.2 Residual Model</li>
</ul></li>
</ul>
<div id="da34cee5" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> trax</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> trax <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> layers <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> tl               <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># core building block</span></span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np                          <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># regular ol' numpy</span></span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> trax.models.reformer.reformer <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> (</span>
<span id="cb1-5">    ReversibleHalfResidualV2 <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> ReversibleHalfResidual,</span>
<span id="cb1-6">)                                           <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># unique spot</span></span>
<span id="cb1-7"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> trax <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> fastmath                   <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># uses jax, offers numpy on steroids</span></span>
<span id="cb1-8"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> trax <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> shapes                     <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># data signatures: dimensionality and type</span></span>
<span id="cb1-9"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> trax.fastmath <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> jnp      <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># For use in defining new layer types.</span></span>
<span id="cb1-10"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> trax.shapes <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> ShapeDtype</span>
<span id="cb1-11"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> trax.shapes <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> signature</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>2025-02-10 16:54:01.601593: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1739199241.613582  121997 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1739199241.617462  121997 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered</code></pre>
</div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">ImportError</span>                               Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[1], line 4</span>
<span class="ansi-green-fg ansi-bold">      2</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">trax</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> layers <span style="font-weight:bold;color:rgb(0,135,0)">as</span> tl               <span style="font-style:italic;color:rgb(95,135,135)"># core building block</span>
<span class="ansi-green-fg ansi-bold">      3</span> <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">numpy</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">as</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">np</span>                          <span style="font-style:italic;color:rgb(95,135,135)"># regular ol' numpy</span>
<span class="ansi-green-fg">----&gt; 4</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">trax</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">models</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">reformer</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">reformer</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> (
<span class="ansi-green-fg ansi-bold">      5</span>     ReversibleHalfResidualV2 <span style="font-weight:bold;color:rgb(0,135,0)">as</span> ReversibleHalfResidual,
<span class="ansi-green-fg ansi-bold">      6</span> )                                           <span style="font-style:italic;color:rgb(95,135,135)"># unique spot</span>
<span class="ansi-green-fg ansi-bold">      7</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">trax</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> fastmath                   <span style="font-style:italic;color:rgb(95,135,135)"># uses jax, offers numpy on steroids</span>
<span class="ansi-green-fg ansi-bold">      8</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">trax</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> shapes                     <span style="font-style:italic;color:rgb(95,135,135)"># data signatures: dimensionality and type</span>

<span class="ansi-red-fg">ImportError</span>: cannot import name 'ReversibleHalfResidualV2' from 'trax.models.reformer.reformer' (/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/models/reformer/reformer.py)</pre>
</div>
</div>
</div>
</section>
<section id="1" class="level2">
<h2 class="anchored" data-anchor-id="1">Part 1.0 Residual Networks</h2>
<p><a href="https://arxiv.org/abs/1512.03385">Deep Residual Networks</a> (Resnets) were introduced to improve convergence in deep networks. Residual Networks introduce a shortcut connection around one or more layers in a deep network as shown in the diagram below from the original paper.</p>
<div id="fig-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/Revnet7.PNG" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: Residual Network diagram from original paper"><img src="https://orenbochman.github.io/notes-nlp/notes/c4w4/img/Revnet7.PNG" width="250" height="250" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Residual Network diagram from original paper
</figcaption>
</figure>
</div>
<p>The <a href="https://trax-ml.readthedocs.io/en/latest/notebooks/layers_intro.html#2.-Inputs-and-Outputs">Trax documentation</a> describes an implementation of Resnets using <code>branch</code>. We’ll explore that here by implementing a simple resnet built from simple function based layers. Specifically, we’ll build a 4 layer network based on two functions, ‘F’ and ‘G’.</p>
<div id="fig-02" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/Revnet8.PNG" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;2: 4 stage Residual network"><img src="https://orenbochman.github.io/notes-nlp/notes/c4w4/img/Revnet8.PNG" width="1400" height="200" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: 4 stage Residual network
</figcaption>
</figure>
</div>
<p>Don’t worry about the lengthy equations. Those are simply there to be referenced later in the notebook.</p>
<section id="1.1" class="level3">
<h3 class="anchored" data-anchor-id="1.1">Part 1.1 Branch</h3>
<p>Trax <code>branch</code> figures prominently in the residual network layer so we will first examine it. You can see from the figure above that we will need a function that will copy an input and send it down multiple paths. This is accomplished with a <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#module-trax.layers.combinators">branch layer</a>, one of the Trax ‘combinators’. Branch is a combinator that applies a list of layers in parallel to copies of inputs. Lets try it out! First we will need some layers to play with. Let’s build some from functions.</p>
<div id="27b97c7b" class="cell" data-tags="[]" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># simple function taking one input and one output</span></span>
<span id="cb3-2">bl_add1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tl.Fn(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"add1"</span>, <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x0: (x0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>), n_out<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb3-3">bl_add2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tl.Fn(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"add2"</span>, <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x0: (x0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>), n_out<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb3-4">bl_add3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tl.Fn(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"add3"</span>, <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x0: (x0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>), n_out<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb3-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># try them out</span></span>
<span id="cb3-6">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb3-7"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(bl_add1(x), bl_add2(x), bl_add3(x))</span>
<span id="cb3-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># some information about our new layers</span></span>
<span id="cb3-9"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(</span>
<span id="cb3-10">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"name:"</span>,</span>
<span id="cb3-11">    bl_add1.name,</span>
<span id="cb3-12">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"number of inputs:"</span>,</span>
<span id="cb3-13">    bl_add1.n_in,</span>
<span id="cb3-14">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"number of outputs:"</span>,</span>
<span id="cb3-15">    bl_add1.n_out,</span>
<span id="cb3-16">)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[2] [3] [4]
name: add1 number of inputs: 1 number of outputs: 1</code></pre>
</div>
</div>
<div id="bd3d4338" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">bl_3add1s <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tl.Branch(bl_add1, bl_add2, bl_add3)</span>
<span id="cb5-2">bl_3add1s</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>Branch_out3[
  add1
  add2
  add3
]</code></pre>
</div>
</div>
Trax uses the concept of a ‘stack’ to transfer data between layers. For Branch, for each of its layer arguments, it copies the <code>n_in</code> inputs from the stack and provides them to the layer, tracking the max_n_in, or the largest n_in required. It then pops the max_n_in elements from the stack. <img src="https://orenbochman.github.io/notes-nlp/notes/c4w4/branch1.PNG" height="260" width="600">
<center>
<b>Figure 3: One in, one out Branch</b>
</center>
<p>On output, each layer, in succession pushes its results onto the stack. Note that the push/pull operations impact the top of the stack. Elements that are not part of the operation (n, and m in the diagram) remain intact.</p>
<div id="6d8e400b" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># n_in = 1, Each bl_addx pushes n_out = 1 elements onto the stack</span></span>
<span id="cb7-2">bl_3add1s(x)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>(array([2]), array([3]), array([4]))</code></pre>
</div>
</div>
<div id="7b580cb6" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># n = np.array([10]); m = np.array([20])  # n, m will remain on the stack</span></span>
<span id="cb9-2">n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"n"</span></span>
<span id="cb9-3">m <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"m"</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># n, m will remain on the stack</span></span>
<span id="cb9-4">bl_3add1s([x, n, m]) </span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>(array([2]), array([3]), array([4]), 'n', 'm')</code></pre>
</div>
</div>
<p>Each layer in the input list copies as many inputs from the stack as it needs, and their outputs are successively combined on stack. Put another way, each element of the branch can have differing numbers of inputs and outputs. Let’s try a more complex example.</p>
<div id="dc26777a" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">bl_addab <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tl.Fn(</span>
<span id="cb11-2">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"addab"</span>, <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x0, x1: (x0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> x1), n_out<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb11-3">)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Trax figures out how many inputs there are</span></span>
<span id="cb11-4">bl_rep3x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tl.Fn(</span>
<span id="cb11-5">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"add2x"</span>, <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x0: (x0, x0, x0), n_out<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span></span>
<span id="cb11-6">)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># but you have to tell it how many outputs there are</span></span>
<span id="cb11-7">bl_3ops <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tl.Branch(bl_add1, bl_addab, bl_rep3x)</span></code></pre></div>
</div>
In this case, the number if inputs being copied from the stack varies with the layer <img src="https://orenbochman.github.io/notes-nlp/notes/c4w4/branch2.PNG" height="260" width="600">
<center>
<b>Figure 4: variable in, variable out Branch</b>
</center>
<p>The stack when the operation is finished is 5 entries reflecting the total from each layer.</p>
<div id="c469d76c" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Before Running this cell, what is the output you are expecting?</span></span>
<span id="cb12-2">y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>])</span>
<span id="cb12-3">bl_3ops([x, y, n, m])</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>(array([2]), array([4]), array([1]), array([1]), array([1]), 'n', 'm')</code></pre>
</div>
</div>
Branch has a special feature to support Residual Network. If an argument is ‘None’, it will pull the top of stack and push it (at its location in the sequence) onto the output stack <img src="https://orenbochman.github.io/notes-nlp/notes/c4w4/branch3.PNG" height="260" width="600">
<center>
<b>Figure 5: Branch for Residual</b>
</center>
<div id="6c3487df" class="cell" data-tags="[]" data-execution_count="8">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">bl_2ops <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tl.Branch(bl_add1, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>)</span>
<span id="cb14-2">bl_2ops([x, n, m])</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>(array([2]), array([1]), 'n', 'm')</code></pre>
</div>
</div>
<p><a name="1.2"></a> ### Part 1.2 Residual Model OK, your turn. Write a function ‘MyResidual’, that uses <code>tl.Branch</code> and <code>tl.Add</code> to build a residual layer. If you are curious about the Trax implementation, you can see the code <a href="https://github.com/google/trax/blob/190ec6c3d941d8a9f30422f27ef0c95dc16d2ab1/trax/layers/combinators.py">here</a>.</p>
<div id="118ac2e1" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> MyResidual(layer):</span>
<span id="cb16-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> tl.Serial(</span>
<span id="cb16-3">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">### START CODE HERE </span><span class="al" style="color: #AD0000;
background-color: null;
font-style: inherit;">###</span></span>
<span id="cb16-4">        tl.Branch(layer, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>),</span>
<span id="cb16-5">        tl.Add(),</span>
<span id="cb16-6">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">### </span><span class="re">END</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> CODE HERE </span><span class="al" style="color: #AD0000;
background-color: null;
font-style: inherit;">###</span></span>
<span id="cb16-7">    )</span></code></pre></div>
</div>
<div id="01efb0db" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Lets Try it</span></span>
<span id="cb17-2">mr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> MyResidual(bl_add1)</span>
<span id="cb17-3">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb17-4">mr([x, n, m])</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>(array([3]), 'n', 'm')</code></pre>
</div>
</div>
<p><strong>Expected Result</strong> (array([3]), ‘n’, ‘m’)</p>
<p>Great! Now, let’s build the 4 layer residual Network in Figure 2. You can use <code>MyResidual</code>, or if you prefer, the tl.Residual in Trax, or a combination!</p>
<div id="ce4fc11c" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">Fl <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tl.Fn(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"F"</span>, <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x0: (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> x0), n_out<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb19-2">Gl <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tl.Fn(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"G"</span>, <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x0: (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> x0), n_out<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb19-3">x1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span></code></pre></div>
</div>
<div id="46f3e974" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1">resfg <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tl.Serial(</span>
<span id="cb20-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">### START CODE HERE </span><span class="al" style="color: #AD0000;
background-color: null;
font-style: inherit;">###</span></span>
<span id="cb20-3">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># None,  #Fl    # x + F(x)</span></span>
<span id="cb20-4">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># None,  #Gl    # x + F(x) + G(x + F(x)) etc</span></span>
<span id="cb20-5">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># None,  #Fl</span></span>
<span id="cb20-6">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># None,  #Gl</span></span>
<span id="cb20-7">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">### </span><span class="re">END</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> CODE HERE </span><span class="al" style="color: #AD0000;
background-color: null;
font-style: inherit;">###</span></span>
<span id="cb20-8">)</span></code></pre></div>
</div>
<div id="5680ccd7" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Lets try it</span></span>
<span id="cb21-2">resfg([x1, n, m])</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>[array([1]), 'n', 'm']</code></pre>
</div>
</div>
<p><strong>Expected Results</strong> (array([1089]), ‘n’, ‘m’)</p>
<a name="2"></a> ## Part 2.0 Reversible Residual Networks The Reformer utilized RevNets to reduce the storage requirements for performing backpropagation. <img src="https://orenbochman.github.io/notes-nlp/notes/c4w4/Reversible2.PNG" height="260" width="600">
<center>
<b>Figure 6: Reversible Residual Networks </b>
</center>
<p>The standard approach on the left above requires one to store the outputs of each stage for use during backprop. By using the organization to the right, one need only store the outputs of the last stage, y1, y2 in the diagram. Using those values and running the algorithm in reverse, one can reproduce the values required for backprop. This trades additional computation for memory space which is at a premium with the current generation of GPU’s/TPU’s.</p>
One thing to note is that the forward functions produced by two networks are similar, but they are not equivalent. Note for example the asymmetry in the output equations after two stages of operation. <img src="https://orenbochman.github.io/notes-nlp/notes/c4w4/Revnet1.PNG" height="340" width="1100">
<center>
<b>Figure 7: ‘Normal’ Residual network (Top) vs REversible Residual Network </b>
</center>
</section>
<section id="part-2.1-trax-reversible-layers" class="level3">
<h3 class="anchored" data-anchor-id="part-2.1-trax-reversible-layers">Part 2.1 Trax Reversible Layers</h3>
<p>Let’s take a look at how this is used in the Reformer.</p>
<div id="63c19451" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1">refm <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> trax.models.reformer.ReformerLM(</span>
<span id="cb23-2">    vocab_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">33000</span>, n_layers<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, mode<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"train"</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Add more options.</span></span>
<span id="cb23-3">)</span>
<span id="cb23-4">refm</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>Serial[
  Serial[
    ShiftRight(1)
  ]
  Embedding_33000_512
  Dropout
  Serial[
    PositionalEncoding
  ]
  Dup_out2
  ReversibleSerial_in2_out2[
    ReversibleHalfResidualDecoderAttn_in2_out2[
      Serial[
        LayerNorm
      ]
      SelfAttention
    ]
    ReversibleSwap_in2_out2
    ReversibleHalfResidualDecoderFF_in2_out2[
      Serial[
        LayerNorm
        Dense_2048
        Dropout
        Serial[
          FastGelu
        ]
        Dense_512
        Dropout
      ]
    ]
    ReversibleSwap_in2_out2
    ReversibleHalfResidualDecoderAttn_in2_out2[
      Serial[
        LayerNorm
      ]
      SelfAttention
    ]
    ReversibleSwap_in2_out2
    ReversibleHalfResidualDecoderFF_in2_out2[
      Serial[
        LayerNorm
        Dense_2048
        Dropout
        Serial[
          FastGelu
        ]
        Dense_512
        Dropout
      ]
    ]
    ReversibleSwap_in2_out2
  ]
  Concatenate_in2
  LayerNorm
  Dropout
  Serial[
    Dense_33000
  ]
]</code></pre>
</div>
</div>
Eliminating some of the detail, we can see the structure of the network. <img src="https://orenbochman.github.io/notes-nlp/notes/c4w4/Revnet2.PNG" height="300" width="350">
<center>
<b>Figure 8: Key Structure of Reformer Reversible Network Layers in Trax </b>
</center>
We’ll review the Trax layers used to implement the Reversible section of the Reformer. First we can note that not all of the reformer is reversible. Only the section in the ReversibleSerial layer is reversible. In a large Reformer model, that section is repeated many times making up the majority of the model. <img src="https://orenbochman.github.io/notes-nlp/notes/c4w4/Revnet3.PNG" height="650" width="1600">
<center>
<b>Figure 9: Functional Diagram of Trax elements in Reformer </b>
</center>
<p>The implementation starts by duplicating the input to allow the two paths that are part of the reversible residual organization with <a href="https://github.com/google/trax/blob/190ec6c3d941d8a9f30422f27ef0c95dc16d2ab1/trax/layers/combinators.py#L666">Dup</a>. Note that this is accomplished by copying the top of stack and pushing two copies of it onto the stack. This then feeds into the ReversibleHalfResidual layer which we’ll review in more detail below. This is followed by <a href="https://github.com/google/trax/blob/190ec6c3d941d8a9f30422f27ef0c95dc16d2ab1/trax/layers/reversible.py#L83">ReversibleSwap</a>. As the name implies, this performs a swap, in this case, the two topmost entries in the stack. This pattern is repeated until we reach the end of the ReversibleSerial section. At that point, the topmost 2 entries of the stack represent the two paths through the network. These are concatenated and pushed onto the stack. The result is an entry that is twice the size of the non-reversible version.</p>
Let’s look more closely at the <a href="https://github.com/google/trax/blob/190ec6c3d941d8a9f30422f27ef0c95dc16d2ab1/trax/layers/reversible.py#L154">ReversibleHalfResidual</a>. This layer is responsible for executing the layer or layers provided as arguments and adding the output of those layers, the ‘residual’, to the top of the stack. Below is the ‘forward’ routine which implements this. <img src="https://orenbochman.github.io/notes-nlp/notes/c4w4/Revnet4.PNG" height="650" width="1600">
<center>
<b>Figure 10: ReversibleHalfResidual code and diagram </b>
</center>
<p>Unlike the previous residual function, the value that is added is from the second path rather than the input to the set of sublayers in this layer. Note that the Layers called by the ReversibleHalfResidual forward function are not modified to support reverse functionality. This layer provides them a ‘normal’ view of the stack and takes care of reverse operation.</p>
<p>Let’s try out some of these layers! We’ll start with the ones that just operate on the stack, Dup() and Swap().</p>
<div id="326c0808" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1">x1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb25-2">x2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>])</span>
<span id="cb25-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Dup() duplicates the Top of Stack and returns the stack</span></span>
<span id="cb25-4">dl <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tl.Dup()</span>
<span id="cb25-5">dl(x1)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>(array([1]), array([1]))</code></pre>
</div>
</div>
<div id="41310c81" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ReversibleSwap() duplicates the Top of Stack and returns the stack</span></span>
<span id="cb27-2">sl <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tl.ReversibleSwap()</span>
<span id="cb27-3">sl([x1, x2])</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>(array([5]), array([1]))</code></pre>
</div>
</div>
You are no doubt wondering “How is ReversibleSwap different from Swap?”. Good question! Lets look: <img src="https://orenbochman.github.io/notes-nlp/notes/c4w4/Revnet5.PNG" height="389" width="1000">
<center>
<b>Figure 11: Two versions of Swap() </b>
</center>
<p>The ReverseXYZ functions include a “reverse” compliment to their “forward” function that provides the functionality to run in reverse when doing backpropagation. It can also be run in reverse by simply calling ‘reverse’.</p>
<div id="8c8e897f" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Demonstrate reverse swap</span></span>
<span id="cb29-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(x1, x2, sl.reverse([x1, x2]))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] [5] (array([5]), array([1]))</code></pre>
</div>
</div>
<p>Let’s try ReversibleHalfResidual, First we’ll need some layers..</p>
<div id="1011c6f0" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1">Fl <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tl.Fn(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"F"</span>, <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x0: (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> x0), n_out<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb31-2">Gl <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tl.Fn(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"G"</span>, <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x0: (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> x0), n_out<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span></code></pre></div>
</div>
<p>Just a note about ReversibleHalfResidual. As this is written, it resides in the Reformer model and is a layer. It is invoked a bit differently that other layers. Rather than tl.XYZ, it is just ReversibleHalfResidual(layers..) as shown below. This may change in the future.</p>
<div id="0f89f722" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb32" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1">half_res_F <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ReversibleHalfResidual(Fl)</span>
<span id="cb32-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">type</span>(half_res_F), <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, half_res_F)</span></code></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[19], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> half_res_F <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">ReversibleHalfResidual</span>(Fl)
<span class="ansi-green-fg ansi-bold">      2</span> <span style="color:rgb(0,135,0)">print</span>(<span style="color:rgb(0,135,0)">type</span>(half_res_F), <span style="color:rgb(175,0,0)">"</span><span style="font-weight:bold;color:rgb(175,95,0)">\n</span><span style="color:rgb(175,0,0)">"</span>, half_res_F)

<span class="ansi-red-fg">NameError</span>: name 'ReversibleHalfResidual' is not defined</pre>
</div>
</div>
</div>
<div id="57dd7781" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb33" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1">half_res_F([x1, x1])  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># this is going to produce an error - why?</span></span></code></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[20], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> <span class="ansi-yellow-bg">half_res_F</span>([x1, x1])  <span style="font-style:italic;color:rgb(95,135,135)"># this is going to produce an error - why?</span>

<span class="ansi-red-fg">NameError</span>: name 'half_res_F' is not defined</pre>
</div>
</div>
</div>
<div id="e5a0be0f" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb34" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># we have to initialize the ReversibleHalfResidual layer to let it know what the input is going to look like</span></span>
<span id="cb34-2">half_res_F.init(shapes.signature([x1, x1]))</span>
<span id="cb34-3">half_res_F([x1, x1])</span></code></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[21], line 2</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># we have to initialize the ReversibleHalfResidual layer to let it know what the input is going to look like</span>
<span class="ansi-green-fg">----&gt; 2</span> <span class="ansi-yellow-bg">half_res_F</span><span style="color:rgb(98,98,98)">.</span>init(shapes<span style="color:rgb(98,98,98)">.</span>signature([x1, x1]))
<span class="ansi-green-fg ansi-bold">      3</span> half_res_F([x1, x1])

<span class="ansi-red-fg">NameError</span>: name 'half_res_F' is not defined</pre>
</div>
</div>
</div>
<p>Notice the output: (DeviceArray([3], dtype=int32), array([1])). The first value, (DeviceArray([3], dtype=int32) is the output of the “Fl” layer and has been converted to a ‘Jax’ DeviceArray. The second array([1]) is just passed through (recall the diagram of ReversibleHalfResidual above).</p>
<p>The final layer we need is the ReversibleSerial Layer. This is the reversible equivalent of the Serial layer and is used in the same manner to build a sequence of layers.</p>
<a name="2.2"></a> ### Part 2.2 Build a reversible model We now have all the layers we need to build the model shown below. Let’s build it in two parts. First we’ll build ‘blk’ and then a list of blk’s. And then ‘mod’.
<center>
<img src="https://orenbochman.github.io/notes-nlp/notes/c4w4/Revnet6.PNG" height="800" width="1600">
</center>
<center>
<b>Figure 12: Reversible Model we will build using Trax components </b>
</center>
<div id="81c0cdbe" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb35" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1">blk <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># a list of the 4 layers shown above</span></span>
<span id="cb35-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">### START CODE HERE </span><span class="al" style="color: #AD0000;
background-color: null;
font-style: inherit;">###</span></span>
<span id="cb35-3">    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>,</span>
<span id="cb35-4">    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>,</span>
<span id="cb35-5">    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>,</span>
<span id="cb35-6">    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>,</span>
<span id="cb35-7">]</span>
<span id="cb35-8">blks <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>]</span>
<span id="cb35-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">### </span><span class="re">END</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> CODE HERE </span><span class="al" style="color: #AD0000;
background-color: null;
font-style: inherit;">###</span></span></code></pre></div>
</div>
<div id="6d670941" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb36" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1">mod <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tl.Serial(</span>
<span id="cb36-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">### START CODE HERE </span><span class="al" style="color: #AD0000;
background-color: null;
font-style: inherit;">###</span></span>
<span id="cb36-3">    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>,</span>
<span id="cb36-4">    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>,</span>
<span id="cb36-5">    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>,</span>
<span id="cb36-6">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">### </span><span class="re">END</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"> CODE HERE </span><span class="al" style="color: #AD0000;
background-color: null;
font-style: inherit;">###</span></span>
<span id="cb36-7">)</span>
<span id="cb36-8">mod</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/combinators.py:437: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if self._mode == 'predict' and self._state[1] is not ():  # pylint: disable=literal-comparison
/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/combinators.py:910: SyntaxWarning: "is" with a literal. Did you mean "=="?
  if state[0] is ():  # pylint: disable=literal-comparison
/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/combinators.py:437: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if self._mode == 'predict' and self._state[1] is not ():  # pylint: disable=literal-comparison
/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/combinators.py:910: SyntaxWarning: "is" with a literal. Did you mean "=="?
  if state[0] is ():  # pylint: disable=literal-comparison</code></pre>
</div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">ValueError</span>                                Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[23], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> mod <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">tl</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">Serial</span><span class="ansi-yellow-bg">(</span>
<span class="ansi-green-fg ansi-bold">      2</span> <span class="ansi-yellow-bg">    </span><span style="font-style:italic;color:rgb(95,135,135)" class="ansi-yellow-bg">### START CODE HERE ###</span>
<span class="ansi-green-fg ansi-bold">      3</span> <span class="ansi-yellow-bg">    </span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">None</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">      4</span> <span class="ansi-yellow-bg">    </span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">None</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">      5</span> <span class="ansi-yellow-bg">    </span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">None</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">      6</span> <span class="ansi-yellow-bg">    </span><span style="font-style:italic;color:rgb(95,135,135)" class="ansi-yellow-bg">### END CODE HERE ###</span>
<span class="ansi-green-fg ansi-bold">      7</span> <span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">      8</span> mod

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/combinators.py:59</span>, in <span class="ansi-cyan-fg">Serial.__init__</span><span class="ansi-blue-fg">(self, name, sublayers_to_print, *sublayers)</span>
<span class="ansi-green-fg ansi-bold">     55</span> <span style="font-weight:bold;color:rgb(0,135,0)">def</span><span style="color:rgb(188,188,188)"> </span><span style="color:rgb(0,0,255)">__init__</span>(<span style="color:rgb(0,135,0)">self</span>, <span style="color:rgb(98,98,98)">*</span>sublayers, name<span style="color:rgb(98,98,98)">=</span><span style="font-weight:bold;color:rgb(0,135,0)">None</span>, sublayers_to_print<span style="color:rgb(98,98,98)">=</span><span style="font-weight:bold;color:rgb(0,135,0)">None</span>):
<span class="ansi-green-fg ansi-bold">     56</span>   <span style="color:rgb(0,135,0)">super</span>()<span style="color:rgb(98,98,98)">.</span><span style="color:rgb(0,0,255)">__init__</span>(
<span class="ansi-green-fg ansi-bold">     57</span>       name<span style="color:rgb(98,98,98)">=</span>name, sublayers_to_print<span style="color:rgb(98,98,98)">=</span>sublayers_to_print)
<span class="ansi-green-fg">---&gt; 59</span>   sublayers <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">_ensure_flat</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">sublayers</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">     60</span>   <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_sublayers <span style="color:rgb(98,98,98)">=</span> sublayers
<span class="ansi-green-fg ansi-bold">     61</span>   <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_n_layers <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)">len</span>(sublayers)

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/combinators.py:1110</span>, in <span class="ansi-cyan-fg">_ensure_flat</span><span class="ansi-blue-fg">(layers)</span>
<span class="ansi-green-fg ansi-bold">   1108</span> <span style="font-weight:bold;color:rgb(0,135,0)">for</span> obj <span style="font-weight:bold;color:rgb(175,0,255)">in</span> layers:
<span class="ansi-green-fg ansi-bold">   1109</span>   <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="font-weight:bold;color:rgb(175,0,255)">not</span> <span style="color:rgb(0,135,0)">isinstance</span>(obj, base<span style="color:rgb(98,98,98)">.</span>Layer):
<span class="ansi-green-fg">-&gt; 1110</span>     <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> <span style="font-weight:bold;color:rgb(215,95,95)">ValueError</span>(
<span class="ansi-green-fg ansi-bold">   1111</span>         <span style="color:rgb(175,0,0)">f</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">Found nonlayer object (</span><span style="font-weight:bold;color:rgb(175,95,135)">{</span>obj<span style="font-weight:bold;color:rgb(175,95,135)">}</span><span style="color:rgb(175,0,0)">) in layers: </span><span style="font-weight:bold;color:rgb(175,95,135)">{</span>layers<span style="font-weight:bold;color:rgb(175,95,135)">}</span><span style="color:rgb(175,0,0)">'</span>)
<span class="ansi-green-fg ansi-bold">   1112</span> <span style="font-weight:bold;color:rgb(0,135,0)">return</span> layers

<span class="ansi-red-fg">ValueError</span>: Found nonlayer object (None) in layers: [None, None, None]</pre>
</div>
</div>
</div>
<p><strong>Expected Output</strong></p>
<pre><code>Serial[
  Dup_out2
  ReversibleSerial_in2_out2[
    ReversibleHalfResidualV2_in2_out2[
      Serial[
        F
      ]
    ]
    ReversibleSwap_in2_out2
    ReversibleHalfResidualV2_in2_out2[
      Serial[
        G
      ]
    ]
    ReversibleSwap_in2_out2
    ReversibleHalfResidualV2_in2_out2[
      Serial[
        F
      ]
    ]
    ReversibleSwap_in2_out2
    ReversibleHalfResidualV2_in2_out2[
      Serial[
        G
      ]
    ]
    ReversibleSwap_in2_out2
  ]
  Concatenate_in2
]</code></pre>
<div id="faaacbbc" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb39" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1">mod.init(shapes.signature(x1))</span>
<span id="cb39-2">out <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mod(x1)</span>
<span id="cb39-3">out</span></code></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[24], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> <span class="ansi-yellow-bg">mod</span><span style="color:rgb(98,98,98)">.</span>init(shapes<span style="color:rgb(98,98,98)">.</span>signature(x1))
<span class="ansi-green-fg ansi-bold">      2</span> out <span style="color:rgb(98,98,98)">=</span> mod(x1)
<span class="ansi-green-fg ansi-bold">      3</span> out

<span class="ansi-red-fg">NameError</span>: name 'mod' is not defined</pre>
</div>
</div>
</div>
<p><strong>Expected Result</strong> DeviceArray([ 65, 681], dtype=int32)</p>
<p>OK, now you have had a chance to try all the ‘Reversible’ functions in Trax. On to the Assignment!</p>


</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2021,
  author = {Bochman, Oren},
  title = {Putting the “{Re}” in {Reformer:} {Ungraded} {Lab}},
  date = {2021-04-29},
  url = {https://orenbochman.github.io/notes-nlp/notes/c4w4/lab02.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2021" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2021. <span>“Putting the <span>‘Re’</span> in Reformer:
Ungraded Lab.”</span> April 29, 2021. <a href="https://orenbochman.github.io/notes-nlp/notes/c4w4/lab02.html">https://orenbochman.github.io/notes-nlp/notes/c4w4/lab02.html</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/notes-nlp/notes/c4w4/lab02.html</guid>
  <pubDate>Wed, 28 Apr 2021 21:00:00 GMT</pubDate>
</item>
<item>
  <title>Reformer Efficient Attention: Ungraded Lab</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/c4w4/lab01.html</link>
  <description><![CDATA[ 





<p>The videos describe two ‘reforms’ made to the Transformer to make it more memory and compute efficient. The <em>Reversible Layers</em> reduce memory and <em>Locality Sensitive Hashing(LSH)</em> reduces the cost of the Dot Product attention for large input sizes. This ungraded lab will look more closely at LSH and how it is used in the Reformer model.</p>
<p>Specifically, the notebook has 3 goals</p>
<ul>
<li>review dot-product self attention for reference</li>
<li>examine LSH based self attention</li>
<li>extend our understanding and familiarity with Trax infrastructure</li>
</ul>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li>Part 1: Trax Efficient Attention classes</li>
<li>Part 2: Full Dot Product Self Attention
<ul>
<li>2.1 Description
<ul>
<li>2.1.1 our_softmax</li>
</ul></li>
<li>2.2 our simple attend</li>
<li>2.3 Class OurSelfAttention</li>
</ul></li>
<li>Part 3: Trax LSHSelfAttention
<ul>
<li>3.1 Description</li>
<li>3.2 our_hash_vectors</li>
<li>3.3 Sorting Buckets</li>
<li>3.4 Chunked dot product attention</li>
<li>3.5 OurLSHSelfAttention</li>
</ul></li>
</ul>
</section>
<section id="1" class="level2">
<h2 class="anchored" data-anchor-id="1">Part 1.0 Trax Efficient Attention classes</h2>
<p>Trax is similar to other popular NN development platforms such as Keras (now integrated into Tensorflow) and Pytorch in that it uses ‘layers’ as a useful level of abstraction. Layers are often represented as <em>classes</em>. We’re going to improve our understanding of Trax by locally extending the classes used in the attention layers. We will extend only the ‘forward’ functions and utilize the existing attention layers as parent classes. The original code can be found at <a href="https://github.com/google/trax/blob/v1.3.4/trax/layers/research/efficient_attention.py">github:trax/layers/Research/Efficient_attention</a>. This link references release 1.3.4 but note that this is under the ‘research’ directory as this is an area of active research. When accessing the code on Github for review on this assignment, be sure you select the 1.3.4 release tag, the master copy may have new changes.:</p>
<div id="fig-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/C4W4_LN2_image11.PNG" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: Reference Tag 1.3.4 on github"><img src="https://orenbochman.github.io/notes-nlp/notes/c4w4/img/C4W4_LN2_image11.PNG" width="250" height="250" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Reference Tag 1.3.4 on github
</figcaption>
</figure>
</div>
<p>While Trax uses classes liberally, we have not built many classes in the course so far. Let’s spend a few moments reviewing the classes we will be using.</p>
<div id="fig-02" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/C4W4_LN2_image1.PNG" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;2: Classes from Trax/layers/Research/Efficient_Attention.py that we will be utilizing"><img src="https://orenbochman.github.io/notes-nlp/notes/c4w4/img/C4W4_LN2_image1.PNG" width="1561" height="788" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Classes from Trax/layers/Research/Efficient_Attention.py that we will be utilizing
</figcaption>
</figure>
</div>
<p>Starting on the right in the diagram below you see EfficientAttentionBase. The parent to this class is the base.layer which has the routines used by all layers. EfficientAttentionBase leaves many routines to be overridden by child classes - but it has an important feature in the <em>Forward</em> routine. It supports a <code>use_reference_code</code> capability that selects implementations that limit some of the complexities to provide a more easily understood version of the algorithms. In particular, it implements a nested loop that treats each <em>‘example, head’</em> independently. This simplifies our work as we need only worry about matrix operations on one <em>‘example, head’</em> at a time. This loop calls <em>forward_unbatched</em>, which is the child process that we will be overriding.</p>
<p>On the top left are the outlines of the two child classes we will be using. The SelfAttention layer is a ‘traditional’ implementation of the dot product attention. We will be implementing the <em>forward_unbatched</em> version of this to highlight the differences between this and the LSH implementation.</p>
<p>Below that is the LSHSelfAttention. This is the routine used in the Reformer architecture. We will override the <em>forward_unbatched</em> section of this and some of the utility functions it uses to explore its implementation in more detail.</p>
<p>The code we will be working with is from the Trax source, and as such has implementation details that will make it a bit harder to follow. However, it will allow use of the results along with the rest of the Trax infrastructure. I will try to briefly describe these as they arise. The <a href="https://trax-ml.readthedocs.io/en/latest/">Trax documentation</a> can also be referenced.</p>
</section>
<section id="1.2" class="level2">
<h2 class="anchored" data-anchor-id="1.2">Part 1.2 Trax Details</h2>
<p>The goal in this notebook is to override a few routines in the Trax classes with our own versions. To maintain their functionality in a full Trax environment, many of the details we might ignore in example version of routines will be maintained in this code. Here are some of the considerations that may impact our code:</p>
<ul>
<li>Trax operates with multiple back-end libraries, we will see special cases that will utilize unique features.</li>
<li>‘Fancy’ numpy indexing is not supported in all backend environments and must be emulated in other ways.</li>
<li>Some operations don’t have gradients for backprop and must be ignored or include forced re-evaluation.</li>
</ul>
<p>Here are some of the functions we may see:</p>
<ul>
<li>Abstracted as <code>fastmath</code>, Trax supports multiple backend’s such as <a href="https://github.com/google/jax">Jax</a> and <a href="https://github.com/tensorflow/tensorflow">Tensorflow2</a></li>
<li><a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.tie_in.html">tie_in</a>: Some non-numeric operations must be invoked during backpropagation. Normally, the gradient compute graph would determine invocation but these functions are not included. To force re-evaluation, they are ‘tied’ to other numeric operations using tie_in.</li>
<li><a href="https://trax-ml.readthedocs.io/en/latest/trax.fastmath.html">stop_gradient</a>: Some operations are intentionally excluded from backprop gradient calculations by setting their gradients to zero.</li>
<li>Below we will execute <code>from trax.fastmath import numpy as np</code>, this uses accelerated forms of numpy functions. This is, however a <em>subset</em> of numpy</li>
</ul>
<div id="b0fb71ce" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> os</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> trax</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> trax <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> layers <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> tl  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># core building block</span></span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> jax</span>
<span id="cb1-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> trax <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> fastmath  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># uses jax, offers numpy on steroids</span></span>
<span id="cb1-6"></span>
<span id="cb1-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># fastmath.use_backend('tensorflow-numpy')</span></span>
<span id="cb1-8"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> functools</span>
<span id="cb1-9"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> trax.fastmath <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># note, using fastmath subset of numpy!</span></span>
<span id="cb1-10"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> trax.layers <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> (</span>
<span id="cb1-11">    tie_in,</span>
<span id="cb1-12">    length_normalized,</span>
<span id="cb1-13">    apply_broadcasted_dropout,</span>
<span id="cb1-14">    look_adjacent,</span>
<span id="cb1-15">    permute_via_gather,</span>
<span id="cb1-16">    permute_via_sort,</span>
<span id="cb1-17">)</span>
<span id="cb1-18"></span>
<span id="cb1-19"></span>
<span id="cb1-20"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># def tie_in(x, y):</span></span>
<span id="cb1-21"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#   if fastmath.backend_name() == 'jax':</span></span>
<span id="cb1-22"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#     return jax.lax.tie_in(x, y)</span></span>
<span id="cb1-23"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#   return y</span></span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>2025-02-10 16:53:34.595009: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1739199214.607869  121487 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1739199214.611988  121487 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered</code></pre>
</div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">ImportError</span>                               Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[1], line 10</span>
<span class="ansi-green-fg ansi-bold">      8</span> <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">functools</span>
<span class="ansi-green-fg ansi-bold">      9</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">trax</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">fastmath</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> numpy <span style="font-weight:bold;color:rgb(0,135,0)">as</span> np  <span style="font-style:italic;color:rgb(95,135,135)"># note, using fastmath subset of numpy!</span>
<span class="ansi-green-fg">---&gt; 10</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">trax</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">layers</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> (
<span class="ansi-green-fg ansi-bold">     11</span>     tie_in,
<span class="ansi-green-fg ansi-bold">     12</span>     length_normalized,
<span class="ansi-green-fg ansi-bold">     13</span>     apply_broadcasted_dropout,
<span class="ansi-green-fg ansi-bold">     14</span>     look_adjacent,
<span class="ansi-green-fg ansi-bold">     15</span>     permute_via_gather,
<span class="ansi-green-fg ansi-bold">     16</span>     permute_via_sort,
<span class="ansi-green-fg ansi-bold">     17</span> )
<span class="ansi-green-fg ansi-bold">     20</span> <span style="font-style:italic;color:rgb(95,135,135)"># def tie_in(x, y):</span>
<span class="ansi-green-fg ansi-bold">     21</span> <span style="font-style:italic;color:rgb(95,135,135)">#   if fastmath.backend_name() == 'jax':</span>
<span class="ansi-green-fg ansi-bold">     22</span> <span style="font-style:italic;color:rgb(95,135,135)">#     return jax.lax.tie_in(x, y)</span>
<span class="ansi-green-fg ansi-bold">     23</span> <span style="font-style:italic;color:rgb(95,135,135)">#   return y</span>

<span class="ansi-red-fg">ImportError</span>: cannot import name 'tie_in' from 'trax.layers' (/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/__init__.py)</pre>
</div>
</div>
</div>
</section>
<section id="2" class="level2">
<h2 class="anchored" data-anchor-id="2">Part 2 Full Dot-Product Self Attention</h2>
<section id="2.1" class="level3">
<h3 class="anchored" data-anchor-id="2.1">Part 2.1 Description</h3>
<div id="fig-03" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/C4W4_LN2_image2.PNG" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;3: Project datapath and primary data structures and where they are implemented"><img src="https://orenbochman.github.io/notes-nlp/notes/c4w4/img/C4W4_LN2_image2.PNG" width="600" height="200" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Project datapath and primary data structures and where they are implemented
</figcaption>
</figure>
</div>
<p>The diagram above shows many of the familiar data structures and operations related to attention and describes the routines in which they are implemented. We will start by working on <em>our_simple_attend</em> or our simpler version of the original <em>attend</em> function. We will review the steps in performing dot-product attention with more focus on the details of the operations and their significance. This is useful when comparing to LSH attention. Note we will be discussing a single example/head unless otherwise specified.</p>
<div id="fig-04" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-04-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/C4W4_LN2_image3.PNG" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;4: Dot-product of Query and Key"><img src="https://orenbochman.github.io/notes-nlp/notes/c4w4/img/C4W4_LN2_image3.PNG" width="700" height="250" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-04-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Dot-product of Query and Key
</figcaption>
</figure>
</div>
<p>The <em>attend</em> function receives <em>Query</em> and <em>Key</em>. As a reminder, they are produced by a matrix multiply of all the inputs with a single set of weights. We will describe the inputs as <em>embeddings</em> assuming an NLP application, however, this is not required. This matrix multiply very much like a convolutional network where a set of weights (a filter) slide across the input vectors leaving behind a map of the similarity of the input to the filter. In this case, the filters are the weight matrices <img src="https://latex.codecogs.com/png.latex?W%5EQ"> and <img src="https://latex.codecogs.com/png.latex?W%5EK">. The resulting maps are Q and K. Q and K have the dimensions of (n_seq, n_q) where n_seq is the number input embeddings and n_q or n_k is the selected size of the Q or K vectors. Note the shading of Q and K, this reflects the fact that each entry is associated with a particular input embedding. You will note later in the code that K is optional. Apparently, similar results can be achieved using Query alone saving the compute and storage associated with K. In that case, the dot-product in <em>attend</em> is matmul(q,q). Note the resulting dot-product (<em>Dot</em>) entries describe a complete (n_seq,n_seq) map of the similarity of all entries of q vs all entries of k. This is reflected in the notation in the dot-product boxes of <img src="https://latex.codecogs.com/png.latex?w_n">,<img src="https://latex.codecogs.com/png.latex?w_m"> representing word_n, word_m. Note that each row of <em>Dot</em> describes the relationship of an input embedding, say <img src="https://latex.codecogs.com/png.latex?w_0">, with every other input.</p>
<p>In some applications some values are masked. This can be used, for example to exclude results that occur later in time (causal) or to mask padding or other inputs.</p>
<div id="fig-05" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-05-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/C4W4_LN2_image4.PNG" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure&nbsp;5: Masking"><img src="https://orenbochman.github.io/notes-nlp/notes/c4w4/img/C4W4_LN2_image4.PNG" width="900" height="300" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-05-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Masking
</figcaption>
</figure>
</div>
<p>The routine below <em>mask_self_attention</em> implements a flexible masking capability. The masking is controlled by the information in q_info and kv_info.</p>
<div id="8a1e86ea" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> mask_self_attention(</span>
<span id="cb3-2">    dots, q_info, kv_info, causal<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, exclude_self<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, masked<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="cb3-3">):</span>
<span id="cb3-4">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""Performs masking for self-attention."""</span></span>
<span id="cb3-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> causal:</span>
<span id="cb3-6">        mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fastmath.lt(q_info, kv_info).astype(np.float32)</span>
<span id="cb3-7">        dots <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dots <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e9</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> mask</span>
<span id="cb3-8">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> exclude_self:</span>
<span id="cb3-9">        mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.equal(q_info, kv_info).astype(np.float32)</span>
<span id="cb3-10">        dots <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dots <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e5</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> mask</span>
<span id="cb3-11">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> masked:</span>
<span id="cb3-12">        zeros_like_kv_info <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tie_in(kv_info, np.zeros_like(kv_info))</span>
<span id="cb3-13">        mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fastmath.lt(kv_info, zeros_like_kv_info).astype(np.float32)</span>
<span id="cb3-14">        dots <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dots <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e9</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> mask</span>
<span id="cb3-15">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> dots</span></code></pre></div>
</div>
<p>A SoftMax is applied per row of the <em>Dot</em> matrix to scale the values in the row between 0 and 1.</p>
<div id="fig-06" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-06-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/C4W4_LN2_image5.PNG" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure&nbsp;6: SoftMax per row of Dot"><img src="https://orenbochman.github.io/notes-nlp/notes/c4w4/img/C4W4_LN2_image5.PNG" width="900" height="300" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-06-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: SoftMax per row of Dot
</figcaption>
</figure>
</div>
</section>
<section id="2.1.1" class="level3">
<h3 class="anchored" data-anchor-id="2.1.1">Part 2.1.1 our_softmax</h3>
<p>This code uses a separable form of the softmax calculation. Recall the softmax: <img src="https://latex.codecogs.com/png.latex?%0Asoftmax(x_i)=%5Cfrac%7B%5Cexp(x_i)%7D%7B%5Csum_j%20%5Cexp(x_j)%7D%5Ctag%7B1%7D%0A"></p>
<p>This can be alternately implemented as: <img src="https://latex.codecogs.com/png.latex?%0Alogsumexp(x)=%5Clog%7B(%7B%5Csum_j%20%5Cexp(x_j)%7D)%7D%5Ctag%7B2%7D%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0Asoftmax(x_i)=%5Cexp(%7Bx_i%20-%20logsumexp(x)%7D)%5Ctag%7B3%7D%0A"></p>
<p>The work below will maintain a copy of the logsumexp allowing the softmax to be completed in sections. You will see how this is useful later in the LSHSelfAttention class.</p>
<p>We’ll create a routine to implement that here with the addition of a passthrough. The matrix operations we will be working on below are easier to follow if we can maintain integer values. So, for tests, we will skip the softmax in some cases.</p>
<div id="451cc81b" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> our_softmax(x, passthrough<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>):</span>
<span id="cb4-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">""" softmax with passthrough"""</span></span>
<span id="cb4-3">    logsumexp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fastmath.logsumexp(x, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, keepdims<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb4-4">    o <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.exp(x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> logsumexp)</span>
<span id="cb4-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> passthrough:</span>
<span id="cb4-6">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> (x, np.zeros_like(logsumexp))</span>
<span id="cb4-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb4-8">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> (o, logsumexp)</span></code></pre></div>
</div>
<p>Let’s check our implementation.</p>
<div id="8a74d136" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">## compare softmax(a) using both methods</span></span>
<span id="cb5-2">a <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">2.0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">3.0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">4.0</span>])</span>
<span id="cb5-3">sma <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.exp(a) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(np.exp(a))</span>
<span id="cb5-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(sma)</span>
<span id="cb5-5">sma2, a_logsumexp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> our_softmax(a)</span>
<span id="cb5-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(sma2)</span>
<span id="cb5-7"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(a_logsumexp)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0.0320586  0.08714432 0.23688282 0.6439142 ]
[0.0320586  0.0871443  0.23688279 0.64391416]
[4.44019]</code></pre>
</div>
</div>
<p>The purpose of the dot-product is to ‘focus attention’ on some of the inputs. Dot now has entries appropriately scaled to enhance some values and reduce others. These are now applied to the <img src="https://latex.codecogs.com/png.latex?V"> entries.</p>
<div id="fig-07" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-07-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/C4W4_LN2_image6.PNG" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Figure&nbsp;7: Applying Attention to V"><img src="https://orenbochman.github.io/notes-nlp/notes/c4w4/img/C4W4_LN2_image6.PNG" width="900" height="300" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-07-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Applying Attention to V
</figcaption>
</figure>
</div>
<p><img src="https://latex.codecogs.com/png.latex?V"> is of size (n_seq,n_v). Note the shading in the diagram. This is to draw attention to the operation of the matrix multiplication. This is detailed below.</p>
<div id="fig-08" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-08-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/C4W4_LN2_image7.PNG" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Figure&nbsp;8: Matrix Multiply"><img src="https://orenbochman.github.io/notes-nlp/notes/c4w4/img/C4W4_LN2_image7.PNG" width="900" height="300" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-08-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Matrix Multiply
</figcaption>
</figure>
</div>
<p><img src="https://latex.codecogs.com/png.latex?V"> is formed by a matrix multiply of the input embedding with the weight matrix <img src="https://latex.codecogs.com/png.latex?W%5Ev"> whose values were set by backpropagation. The row entries of <img src="https://latex.codecogs.com/png.latex?V"> are then related to the corresponding input embedding. The matrix multiply weights first column of V, representing a section of each of the input embeddings, with the first row of Dot, representing the similarity of <img src="https://latex.codecogs.com/png.latex?W_0"> and each word of the input embedding and deposits the value in <img src="https://latex.codecogs.com/png.latex?Z"></p>
</section>
<section id="2.2" class="level3">
<h3 class="anchored" data-anchor-id="2.2">Part 2.2 our_simple_attend</h3>
<p>In this section we’ll work on an implementation of <em>attend</em> whose operations you can see in figure 3. It is a slightly simplified version of the routine in <a href="https://github.com/google/trax/blob/v1.3.4/trax/layers/research/efficient_attention.py">efficient_attention.py</a>. We will fill in a few lines of code. The main goal is to become familiar with the routine. You have implemented similar functionality in a previous assignment.</p>
<p><strong>Instructions</strong> <strong>Step 1:</strong> matrix multiply (np.matmul) q and the k ‘transpose’ kr. <strong>Step 2:</strong> use our_softmax() to perform a softmax on masked output of the dot product, dots. <strong>Step 3:</strong> matrix multiply (np.matmul) dots and v.</p>
<div id="46e393aa" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> our_simple_attend(</span>
<span id="cb7-2">    q, k<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, v<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>,</span>
<span id="cb7-3">    mask_fn<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, q_info<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, kv_info<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>,</span>
<span id="cb7-4">    dropout<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span>, rng<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, passthrough<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="cb7-5">    ):</span>
<span id="cb7-6">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""Dot-product attention,  with masking, without optional chunking and/or.</span></span>
<span id="cb7-7"></span>
<span id="cb7-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">  Args:</span></span>
<span id="cb7-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    q: Query vectors, shape [q_len, d_qk]</span></span>
<span id="cb7-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    k: Key vectors, shape [kv_len, d_qk]; or None</span></span>
<span id="cb7-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    v: Value vectors, shape [kv_len, d_v]</span></span>
<span id="cb7-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    mask_fn: a function reference that implements masking (e.g. mask_self_attention)</span></span>
<span id="cb7-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    q_info: Query-associated metadata for masking</span></span>
<span id="cb7-14"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    kv_info: Key-associated metadata for masking</span></span>
<span id="cb7-15"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    dropout: Dropout rate</span></span>
<span id="cb7-16"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    rng: RNG for dropout</span></span>
<span id="cb7-17"></span>
<span id="cb7-18"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">  Returns:</span></span>
<span id="cb7-19"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    A tuple (output, dots_logsumexp). The output has shape [q_len, d_v], and</span></span>
<span id="cb7-20"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    dots_logsumexp has shape [q_len]. The logsumexp of the attention</span></span>
<span id="cb7-21"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    probabilities is useful for combining multiple rounds of attention (as in</span></span>
<span id="cb7-22"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    LSH attention).</span></span>
<span id="cb7-23"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">  """</span></span>
<span id="cb7-24">  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">assert</span> v <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb7-25">  share_qk <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (k <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>)</span>
<span id="cb7-26">  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> share_qk:</span>
<span id="cb7-27">    k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> q</span>
<span id="cb7-28">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> kv_info <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb7-29">      kv_info <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> q_info</span>
<span id="cb7-30"></span>
<span id="cb7-31">  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> share_qk:</span>
<span id="cb7-32">    k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> length_normalized(k)</span>
<span id="cb7-33">  k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> np.sqrt(k.shape[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb7-34"></span>
<span id="cb7-35">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Dot-product attention.</span></span>
<span id="cb7-36">  kr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.swapaxes(k, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># note the fancy transpose for later..</span></span>
<span id="cb7-37"></span>
<span id="cb7-38"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">## Step 1  ##</span></span>
<span id="cb7-39">  dots <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.matmul(q, kr )</span>
<span id="cb7-40">  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> verbose: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Our attend dots"</span>, dots.shape)</span>
<span id="cb7-41"></span>
<span id="cb7-42">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Masking</span></span>
<span id="cb7-43">  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> mask_fn <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb7-44">    dots <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mask_fn(dots, q_info[..., :, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>], kv_info[..., <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, :])</span>
<span id="cb7-45"></span>
<span id="cb7-46">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Softmax.</span></span>
<span id="cb7-47">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#dots_logsumexp = fastmath.logsumexp(dots, axis=-1, keepdims=True)  #original</span></span>
<span id="cb7-48">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#dots = np.exp(dots - dots_logsumexp)  #original</span></span>
<span id="cb7-49">  </span>
<span id="cb7-50"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">## Step 2  ##</span></span>
<span id="cb7-51">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#replace with our_softmax()</span></span>
<span id="cb7-52">  dots, dots_logsumexp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> our_softmax(dots, passthrough<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>passthrough)</span>
<span id="cb7-53">  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> verbose: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Our attend dots post softmax"</span>, dots.shape, dots_logsumexp.shape)</span>
<span id="cb7-54"></span>
<span id="cb7-55">  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> dropout <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span>:</span>
<span id="cb7-56">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">assert</span> rng <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb7-57">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Dropout is broadcast across the bin dimension</span></span>
<span id="cb7-58">    dropout_shape <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (dots.shape[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>], dots.shape[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb7-59">    keep_prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tie_in(dots, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> dropout)</span>
<span id="cb7-60">    keep <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fastmath.random.bernoulli(rng, keep_prob, dropout_shape)</span>
<span id="cb7-61">    multiplier <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> keep.astype(dots.dtype) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> tie_in(keep, keep_prob)</span>
<span id="cb7-62">    dots <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dots <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> multiplier</span>
<span id="cb7-63"></span>
<span id="cb7-64"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">## Step 3  ##</span></span>
<span id="cb7-65"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># The softmax normalizer (dots_logsumexp) is used by multi-round LSH attn.</span></span>
<span id="cb7-66">  out <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.matmul(dots, v)</span>
<span id="cb7-67">  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> verbose: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Our attend out1"</span>, out.shape)</span>
<span id="cb7-68">  out <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.reshape(out, (<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, out.shape[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]))</span>
<span id="cb7-69">  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> verbose: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Our attend out2"</span>, out.shape)</span>
<span id="cb7-70">  dots_logsumexp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.reshape(dots_logsumexp, (<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,))</span>
<span id="cb7-71">  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> out, dots_logsumexp</span></code></pre></div>
</div>
<div id="9cd341bd" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">seq_len <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span></span>
<span id="cb8-2">emb_len <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span></span>
<span id="cb8-3">d_qk <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span></span>
<span id="cb8-4">d_v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span></span>
<span id="cb8-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> fastmath.use_backend(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"jax"</span>):  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># specify the backend for consistency</span></span>
<span id="cb8-6">    rng_attend <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fastmath.random.get_prng(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb8-7">    q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> jax.random.uniform(rng_attend, (seq_len, d_qk), dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.float32)</span>
<span id="cb8-8">    v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> jax.random.uniform(rng_attend, (seq_len, d_v), dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.float32)</span>
<span id="cb8-9">    o, logits <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> our_simple_attend(</span>
<span id="cb8-10">        q,</span>
<span id="cb8-11">        k,</span>
<span id="cb8-12">        v,</span>
<span id="cb8-13">        mask_fn<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>,</span>
<span id="cb8-14">        q_info<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>,</span>
<span id="cb8-15">        kv_info<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>,</span>
<span id="cb8-16">        dropout<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span>,</span>
<span id="cb8-17">        rng<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>rng_attend,</span>
<span id="cb8-18">        verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb8-19">    )</span>
<span id="cb8-20"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(o, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, logits)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Our attend dots (8, 8)
Our attend dots post softmax (8, 8) (8, 1)
Our attend out1 (8, 4)
Our attend out2 (8, 4)
[[0.5455444  0.4232705  0.62970716 0.45504814]
 [0.5558777  0.4169514  0.6260488  0.45763403]
 [0.5502556  0.42250413 0.6107501  0.4532582 ]
 [0.53680766 0.43004778 0.63048995 0.4492887 ]
 [0.5546176  0.41898918 0.62778664 0.44567773]
 [0.54741716 0.4229177  0.6060424  0.46433902]
 [0.53192824 0.43415833 0.63327026 0.44313937]
 [0.538871   0.42285213 0.6527077  0.44843906]] 
 [2.5345023 2.6896586 2.8266857 2.4992957 2.861424  2.6235857 2.5204637
 2.3627536]</code></pre>
</div>
</div>
<details>
<summary>
<font size="3"><b> Expected Output </b></font>
</summary>
</details>
<p><strong>Expected Output</strong></p>
<pre><code>Our attend dots (8, 8)
Our attend dots post softmax (8, 8) (8, 1)
Our attend out1 (8, 4)
Our attend out2 (8, 4)
[[0.5606324  0.7290605  0.5251243  0.47101074]
 [0.5713517  0.71991956 0.5033342  0.46975708]
 [0.5622886  0.7288458  0.52172124 0.46318397]
 [0.5568317  0.72234154 0.542236   0.4699722 ]
 [0.56504494 0.72274375 0.5204978  0.47231334]
 [0.56175965 0.7216782  0.53293145 0.48003793]
 [0.56753993 0.72232544 0.5141734  0.46625748]
 [0.57100445 0.70785505 0.5325362  0.4590797 ]]
 [2.6512175 2.1914332 2.6630518 2.7792363 2.4583826 2.5421977 2.4145055
 2.5111294]</code></pre>
<details>
<summary>
<font size="3"><b> completed code for reference </b></font>
</summary>
<pre><code>This notebook is ungraded, so for reference, the completed code follows:</code></pre>
</details>
<pre><code>def our_simple_attend(
    q, k=None, v=None,
    mask_fn=None, q_info=None, kv_info=None,
    dropout=0.0, rng=None, verbose=False, passthrough=False
    ):
  """Dot-product attention,  with masking, without optional chunking and/or.

  Args:
    q: Query vectors, shape [q_len, d_qk]
    k: Key vectors, shape [kv_len, d_qk]; or None
    v: Value vectors, shape [kv_len, d_v]
    mask_fn: a function reference that implements masking (e.g. mask_self_attention)
    q_info: Query-associated metadata for masking
    kv_info: Key-associated metadata for masking
    dropout: Dropout rate
    rng: RNG for dropout

  Returns:
    A tuple (output, dots_logsumexp). The output has shape [q_len, d_v], and
    dots_logsumexp has shape [q_len]. The logsumexp of the attention
    probabilities is useful for combining multiple rounds of attention (as in
    LSH attention).
  """
  assert v is not None
  share_qk = (k is None)
  if share_qk:
    k = q
    if kv_info is None:
      kv_info = q_info

  if share_qk:
    k = length_normalized(k)
  k = k / np.sqrt(k.shape[-1])

  # Dot-product attention.
  kr = np.swapaxes(k, -1, -2)  #note the fancy transpose for later..

## Step 1  ##
  dots = np.matmul(q, kr )
  if verbose: print("Our attend dots", dots.shape)

  # Masking
  if mask_fn is not None:
    dots = mask_fn(dots, q_info[..., :, None], kv_info[..., None, :])

  # Softmax.
  #dots_logsumexp = fastmath.logsumexp(dots, axis=-1, keepdims=True)  #original
  #dots = np.exp(dots - dots_logsumexp)  #original
## Step 2  ##
  #replace with our_softmax()
  dots, dots_logsumexp = our_softmax(dots, passthrough=passthrough)
  if verbose: print("Our attend dots post softmax", dots.shape, dots_logsumexp.shape)

  if dropout &gt; 0.0:
    assert rng is not None
    # Dropout is broadcast across the bin dimension
    dropout_shape = (dots.shape[-2], dots.shape[-1])
    keep_prob = tie_in(dots, 1.0 - dropout)
    keep = fastmath.random.bernoulli(rng, keep_prob, dropout_shape)
    multiplier = keep.astype(dots.dtype) / tie_in(keep, keep_prob)
    dots = dots * multiplier

## Step 3  ##
# The softmax normalizer (dots_logsumexp) is used by multi-round LSH attn.
  out = np.matmul(dots, v)
  if verbose: print("Our attend out1", out.shape)
  out = np.reshape(out, (-1, out.shape[-1]))
  if verbose: print("Our attend out2", out.shape)
  dots_logsumexp = np.reshape(dots_logsumexp, (-1,))
  return out, dots_logsumexp</code></pre>
</section>
</section>
<section id="2.3" class="level2">
<h2 class="anchored" data-anchor-id="2.3">Part 2.3 Class OurSelfAttention</h2>
<p>Here we create our own self attention layer by creating a class <code>OurSelfAttention</code>. The parent class will be the tl.SelfAttention layer in Trax. We will only override the <code>forward_unbatched</code> routine.</p>
<p>We’re not asking you to modify anything in this routine. There are some comments to draw your attention to a few lines.</p>
<div id="7be8a7a2" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> OurSelfAttention(tl.SelfAttention):</span>
<span id="cb13-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""Our self-attention. Just the Forward Function."""</span></span>
<span id="cb13-3"></span>
<span id="cb13-4">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> forward_unbatched(</span>
<span id="cb13-5">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x, mask<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>, weights, state, rng, update_state, verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="cb13-6">    ):</span>
<span id="cb13-7">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ourSelfAttention:forward_unbatched"</span>)</span>
<span id="cb13-8">        <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">del</span> update_state</span>
<span id="cb13-9">        attend_rng, output_rng <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fastmath.random.split(rng)</span>
<span id="cb13-10">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.bias:</span>
<span id="cb13-11">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.share_qk:</span>
<span id="cb13-12">                w_q, w_v, w_o, b_q, b_v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> weights</span>
<span id="cb13-13">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb13-14">                w_q, w_k, w_v, w_o, b_q, b_k, b_v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> weights</span>
<span id="cb13-15">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb13-16">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.share_qk:</span>
<span id="cb13-17">                w_q, w_v, w_o <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> weights</span>
<span id="cb13-18">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb13-19">                w_q, w_k, w_v, w_o <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> weights</span>
<span id="cb13-20"></span>
<span id="cb13-21">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"x.shape,w_q.shape"</span>, x.shape, w_q.shape)</span>
<span id="cb13-22">        q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.matmul(x, w_q)</span>
<span id="cb13-23">        k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb13-24">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.share_qk:</span>
<span id="cb13-25">            k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.matmul(x, w_k)</span>
<span id="cb13-26">        v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.matmul(x, w_v)</span>
<span id="cb13-27"></span>
<span id="cb13-28">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.bias:</span>
<span id="cb13-29">            q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> b_q</span>
<span id="cb13-30">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.share_qk:</span>
<span id="cb13-31">                k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> b_k</span>
<span id="cb13-32">            v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> b_v</span>
<span id="cb13-33"></span>
<span id="cb13-34">        mask_fn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> functools.partial(</span>
<span id="cb13-35">            mask_self_attention,</span>
<span id="cb13-36">            causal<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.causal,</span>
<span id="cb13-37">            exclude_self<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.share_qk,</span>
<span id="cb13-38">            masked<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.masked,</span>
<span id="cb13-39">        )</span>
<span id="cb13-40">        q_info <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> kv_info <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tie_in(x, np.arange(q.shape[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>], dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.int32))</span>
<span id="cb13-41"></span>
<span id="cb13-42">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">assert</span> (mask <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.masked</span>
<span id="cb13-43">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.masked:</span>
<span id="cb13-44">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># mask is a boolean array (True means "is valid token")</span></span>
<span id="cb13-45">            ones_like_mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tie_in(x, np.ones_like(mask, dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.int32))</span>
<span id="cb13-46">            kv_info <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> kv_info <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.where(mask, ones_like_mask, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>ones_like_mask)</span>
<span id="cb13-47"></span>
<span id="cb13-48">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Notice, we are callout our vesion of attend</span></span>
<span id="cb13-49">        o, _ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> our_simple_attend(</span>
<span id="cb13-50">            q,</span>
<span id="cb13-51">            k,</span>
<span id="cb13-52">            v,</span>
<span id="cb13-53">            mask_fn<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>mask_fn,</span>
<span id="cb13-54">            q_info<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>q_info,</span>
<span id="cb13-55">            kv_info<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>kv_info,</span>
<span id="cb13-56">            dropout<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.attention_dropout,</span>
<span id="cb13-57">            rng<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>attend_rng,</span>
<span id="cb13-58">            verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb13-59">        )</span>
<span id="cb13-60"></span>
<span id="cb13-61">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Notice, wo weight matrix applied to output of attend in forward_unbatched</span></span>
<span id="cb13-62">        out <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.matmul(o, w_o)</span>
<span id="cb13-63">        out <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> apply_broadcasted_dropout(out, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.output_dropout, output_rng)</span>
<span id="cb13-64">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> out, state</span></code></pre></div>
</div>
<div id="ec17728e" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">causal <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="cb14-2">masked <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="cb14-3">mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb14-4">attention_dropout <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span></span>
<span id="cb14-5">n_heads <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span></span>
<span id="cb14-6">d_qk <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span></span>
<span id="cb14-7">d_v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span></span>
<span id="cb14-8">seq_len <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span></span>
<span id="cb14-9">emb_len <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span></span>
<span id="cb14-10">batch_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb14-11"></span>
<span id="cb14-12">osa <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> OurSelfAttention(</span>
<span id="cb14-13">    n_heads<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>n_heads,</span>
<span id="cb14-14">    d_qk<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>d_qk,</span>
<span id="cb14-15">    d_v<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>d_v,</span>
<span id="cb14-16">    causal<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>causal,</span>
<span id="cb14-17">    use_reference_code<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb14-18">    attention_dropout<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>attention_dropout,</span>
<span id="cb14-19">    mode<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"train"</span>,</span>
<span id="cb14-20">)</span>
<span id="cb14-21"></span>
<span id="cb14-22">rng_osa <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fastmath.random.get_prng(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb14-23">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> jax.random.uniform(</span>
<span id="cb14-24">    jax.random.PRNGKey(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>), (batch_size, seq_len, emb_len), dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.float32</span>
<span id="cb14-25">)</span>
<span id="cb14-26">_, _ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> osa.init(tl.shapes.signature(x), rng<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>rng_osa)</span></code></pre></div>
</div>
<div id="8d9e4c5a" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">osa(x)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>ourSelfAttention:forward_unbatched</code></pre>
</div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">LayerError</span>                                Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[9], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> <span class="ansi-yellow-bg">osa</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">x</span><span class="ansi-yellow-bg">)</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/base.py:197</span>, in <span class="ansi-cyan-fg">Layer.__call__</span><span class="ansi-blue-fg">(self, x, weights, state, rng)</span>
<span class="ansi-green-fg ansi-bold">    195</span>   <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>state <span style="color:rgb(98,98,98)">=</span> state  <span style="font-style:italic;color:rgb(95,135,135)"># Needed if the model wasn't fully initialized.</span>
<span class="ansi-green-fg ansi-bold">    196</span> state <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>state
<span class="ansi-green-fg">--&gt; 197</span> outputs, new_state <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">pure_fn</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">x</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">weights</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">state</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">rng</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">    198</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>state <span style="color:rgb(98,98,98)">=</span> new_state
<span class="ansi-green-fg ansi-bold">    199</span> <span style="font-weight:bold;color:rgb(0,135,0)">return</span> outputs

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/base.py:605</span>, in <span class="ansi-cyan-fg">Layer.pure_fn</span><span class="ansi-blue-fg">(self, x, weights, state, rng, use_cache)</span>
<span class="ansi-green-fg ansi-bold">    602</span> <span style="font-weight:bold;color:rgb(0,135,0)">except</span> <span style="font-weight:bold;color:rgb(215,95,95)">Exception</span>:
<span class="ansi-green-fg ansi-bold">    603</span>   <span style="font-style:italic;color:rgb(95,135,135)"># Skipping 3 lines as it's always the uninteresting internal call.</span>
<span class="ansi-green-fg ansi-bold">    604</span>   name, trace <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_name, _short_traceback(skip<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">3</span>)
<span class="ansi-green-fg">--&gt; 605</span>   <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> LayerError(name, <span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">pure_fn</span><span style="color:rgb(175,0,0)">'</span>,
<span class="ansi-green-fg ansi-bold">    606</span>                    <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_caller, signature(x), trace) <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">None</span>

<span class="ansi-red-fg">LayerError</span>: Exception passing through layer OurSelfAttention (in pure_fn):
  layer created in file [...]/layers/research/efficient_attention.py, line 1014
  layer input shapes: ShapeDtype{shape:(1, 8, 5), dtype:float32}

  File [...]/layers/research/efficient_attention.py, line 1323, in forward
    single_out, single_new_state = self.forward_unbatched(

  File [...]/tmp/ipykernel_121487/1155828790.py, line 10, in forward_unbatched
    if self.bias:

AttributeError: 'OurSelfAttention' object has no attribute 'bias'. Did you mean: '_bias'?</pre>
</div>
</div>
</div>
<details>
<summary>
<font size="3"><b> Expected Output </b></font>
</summary>
<p><strong>Expected Output</strong> Notice a few things:</p>
<ul>
<li>the w_q (and w_k) matrices are applied to each row or each embedding on the input. This is similar to the filter operation in convolution</li>
<li>forward_unbatched is called 3 times. This is because we have 3 heads in this example.</li>
</ul>
<pre><code>ourSelfAttention:forward_unbatched
x.shape,w_q.shape (8, 5) (5, 3)
Our attend dots (8, 8)
Our attend dots post softmax (8, 8) (8, 1)
Our attend out1 (8, 4)
Our attend out2 (8, 4)
ourSelfAttention:forward_unbatched
x.shape,w_q.shape (8, 5) (5, 3)
Our attend dots (8, 8)
Our attend dots post softmax (8, 8) (8, 1)
Our attend out1 (8, 4)
Our attend out2 (8, 4)
ourSelfAttention:forward_unbatched
x.shape,w_q.shape (8, 5) (5, 3)
Our attend dots (8, 8)
Our attend dots post softmax (8, 8) (8, 1)
Our attend out1 (8, 4)
Our attend out2 (8, 4)
DeviceArray([[[ 6.70414209e-01, -1.04319841e-01, -5.33822298e-01,
                1.92711830e-01, -4.54187393e-05],
              [ 6.64090097e-01, -1.01875424e-01, -5.35733163e-01,
                1.88311756e-01, -6.30629063e-03],
              [ 6.73380017e-01, -1.06952369e-01, -5.31989932e-01,
                1.90056816e-01,  1.30271912e-03],
              [ 6.84564888e-01, -1.13240272e-01, -5.50182462e-01,
                1.95673436e-01,  5.47635555e-03],
              [ 6.81435883e-01, -1.11068964e-01, -5.32343209e-01,
                1.91912338e-01,  5.69400191e-03],
              [ 6.80724978e-01, -1.08496904e-01, -5.34994125e-01,
                1.96332246e-01,  5.89773059e-03],
              [ 6.80933356e-01, -1.14087075e-01, -5.18659890e-01,
                1.90674081e-01,  1.14096403e-02],
              [ 6.80265009e-01, -1.09031796e-01, -5.38248718e-01,
                1.94203183e-01,  4.23943996e-03]]], dtype=float32)</code></pre>
<p><a name="3"></a> ## Part 3.0 Trax LSHSelfAttention <a name="3.1"></a> ## Part 3.1 Description The larger the matrix multiply in the previous section is, the more context can be taken into account when making the next decision. However, the self attention dot product grows as the size of the input squared. For example, if one wished to have an input size of 1024, that would result in <img src="https://latex.codecogs.com/png.latex?1024%5E2"> or over a million dot products for each head! As a result, there has been significant research related to reducing the compute requirements. One such approach is Locality Sensitive Hashing(LSH) Self Attention.</p>
<p>You may recall, earlier in the course you utilized LSH to find similar tweets without resorting to calculating cosine similarity for each pair of embeddings. We will use a similar approach here. It may be best described with an example. <img src="https://orenbochman.github.io/notes-nlp/notes/c4w4/C4W4_LN2_image8.PNG" height="400" width="750"></p>
<center>
<b>Figure 9: Example of LSH Self Attention</b>
</center>
<p>LSH Self attention uses Queries only, no Keys. Attention then generates a metric of the similarity of each value of Q relative to all the other values in Q. An earlier assignment demonstrated that values which hash to the same bucket are likely to be similar. Further, multiple random hashes can improve the chances of finding entries which are similar. This is the approach taken here, though the hash is implemented a bit differently. The values of Q are hashed into buckets using a randomly generated set of hash vectors. Multiple sets of hash vectors are used, generating multiple hash tables. In the figure above, we have 3 hash tables with 4 buckets in each table. Notionally, following the hash, the values of Q have been replicated 3 times and distributed to their appropriate bucket in each of the 3 tables. To find similarity then, one generates dot-products only between members of the buckets. The result of this operation provides information on which entries are similar. As the operation has been distributed over multiple hash tables, these results need to be combined to form a complete picture and this can be used to generate a reduced dot-product attention array. Its clear that because we do not do a compare of every value vs every other value, the size of <em>Dots</em> will be reduced.</p>
<p>The challenge in this approach is getting it to operate efficiently. You may recall from the earlier assignments the buckets were lists of entries and had varying length. This will operate poorly on a vector processing machine such as a GPU or TPU. Ideally, operations are done in large blocks with uniform sizes. While it is straightforward to implement the hash algorithm this way, it is challenging to managed buckets and variable sized dot-products. This will be discussed further below. For now, we will examine and implement the hash function.</p>
<p><a name="3.2"></a> ## Part 3.2 our_hash_vectors</p>
<p><em>our_hash_vectors</em>, is a reimplementation of Trax <em>hashvector</em>. It takes in an array of vectors, hashes the entries and returns and array assigning each input vector to n_hash buckets. Hashing is described as creating <em>random rotations</em>, see <a href="https://arxiv.org/pdf/1509.02897.pdf">Practical and Optimal LSH for Angular Distance</a>.</p>
<img src="https://orenbochman.github.io/notes-nlp/notes/c4w4/C4W4_LN2_image9.PNG" height="400" width="750"> <img src="https://orenbochman.github.io/notes-nlp/notes/c4w4/C4W4_LN2_image10.PNG" height="400" width="750">
<center>
<b>Figure 10: Processing steps in our_hash_vectors </b>
</center>
<p>Note, in the diagram, sizes relate to our expected input <img src="https://latex.codecogs.com/png.latex?Q"> while our_hash_vectors is written assuming a generic input vector</p>
<p><strong>Instructions</strong> <strong>Step 1</strong> create an array of random normal vectors which will be our hash vectors. Each vector will be hashed into a hash table and into <code>rot_size//2</code> buckets. We use <code>rot_size//2</code> to reduce computation. Later in the routine we will form the negative rotations with a simple negation and concatenate to get a full <code>rot_size</code> number of rotations. * use fastmath.random.normal and create an array of random vectors of shape (vec.shape[-1],n_hashes, rot_size//2)</p>
<p><strong>Step 2</strong> In this step we simply do the matrix multiply. <code>jax</code> has an accelerated version of <a href="https://numpy.org/doc/stable/reference/generated/numpy.einsum.html">einsum</a>. Here we will utilize more conventional routines.</p>
<p><strong>Step 2x</strong> * 2a: np.reshape random_rotations into a 2 dimensional array ([-1, n_hashes * (rot_size // 2)]) * 2b: np.dot vecs and random_rotations forming our rotated_vecs * 2c: back to 3 dimension with np.reshape [-1, n_hashes, rot_size//2] * 2d: prepare for concatenating by swapping dimensions np.transpose (1, 0, 2) <strong>Step 3</strong> Here we concatenate our rotation vectors getting a fullrot_size number of buckets (note, n_buckets = rotsize) * use np.concatenate, [rotated_vecs, -rotated_vecs], axis=-1 <strong>Step 4</strong> <strong>This is the exciting step!</strong> You have no doubt been wondering how we will turn these vectors into bucket indexes. By performing np.argmax over the rotations for a given entry, you get the index to the best match! We will use this as a bucket index. * np.argmax(…).astype(np.int32); be sure to use the correct axis! <strong>Step 5</strong> In this style of hashing, items which land in bucket 0 of hash table 0 are not necessarily similar to those landing in bucket 0 of hash table 1, so we keep them separate. We do this by offsetting the bucket numbers by ‘n_buckets’. * add buckets and offsets and reshape into a one dimensional array This will return a 1D array of size n_hashes * vec.shape[0].</p>
<div id="05bb6c1d" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> our_hash_vectors(vecs, rng, n_buckets, n_hashes, mask<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>):</span>
<span id="cb18-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb18-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">  Args:</span></span>
<span id="cb18-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    vecs: tensor of at least 2 dimension,</span></span>
<span id="cb18-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    rng: random number generator</span></span>
<span id="cb18-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    n_buckets: number of buckets in each hash table</span></span>
<span id="cb18-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    n_hashes: the number of hash tables</span></span>
<span id="cb18-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    mask: None indicating no mask or a 1D boolean array of length vecs.shape[0], containing the location of padding value</span></span>
<span id="cb18-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    verbose: controls prints for debug</span></span>
<span id="cb18-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">  Returns:</span></span>
<span id="cb18-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    A vector of size n_hashes * vecs.shape[0] containing the buckets associated with each input vector per hash table.</span></span>
<span id="cb18-12"></span>
<span id="cb18-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb18-14"></span>
<span id="cb18-15">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># check for even, integer bucket sizes</span></span>
<span id="cb18-16">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">assert</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">isinstance</span>(n_buckets, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>) <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> n_buckets <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb18-17"></span>
<span id="cb18-18">    rng <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fastmath.stop_gradient(tie_in(vecs, rng))</span>
<span id="cb18-19">    rot_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> n_buckets</span>
<span id="cb18-20">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">### Start Code Here</span></span>
<span id="cb18-21"></span>
<span id="cb18-22">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">### Step 1 </span><span class="al" style="color: #AD0000;
background-color: null;
font-style: inherit;">###</span></span>
<span id="cb18-23">    rotations_shape <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (vecs.shape[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], n_hashes, rot_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb18-24">    random_rotations <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fastmath.random.normal(rng, rotations_shape).astype(</span>
<span id="cb18-25">        np.float32)</span>
<span id="cb18-26">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> verbose: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"random.rotations.shape"</span>, random_rotations.shape)</span>
<span id="cb18-27"></span>
<span id="cb18-28">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">### Step 2 </span><span class="al" style="color: #AD0000;
background-color: null;
font-style: inherit;">###</span></span>
<span id="cb18-29">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> fastmath.backend_name() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'jax'</span>:</span>
<span id="cb18-30">      rotated_vecs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.einsum(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'tf,fhb-&gt;htb'</span>, vecs, random_rotations)</span>
<span id="cb18-31">      <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> verbose: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"using jax"</span>)</span>
<span id="cb18-32">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb18-33">      <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#Step 2a</span></span>
<span id="cb18-34">      random_rotations <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.reshape(random_rotations,</span>
<span id="cb18-35">                                    [<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, n_hashes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (rot_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)])</span>
<span id="cb18-36">      <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> verbose: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"random_rotations reshaped"</span>, random_rotations.shape)</span>
<span id="cb18-37">      <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#Step 2b</span></span>
<span id="cb18-38">      rotated_vecs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.dot(vecs, random_rotations)</span>
<span id="cb18-39">      <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> verbose: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"rotated_vecs1"</span>, rotated_vecs.shape)</span>
<span id="cb18-40">      <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#Step 2c</span></span>
<span id="cb18-41">      rotated_vecs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.reshape(rotated_vecs, [<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, n_hashes, rot_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>])</span>
<span id="cb18-42">      <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> verbose: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"rotated_vecs2"</span>, rotated_vecs.shape)</span>
<span id="cb18-43">      <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#Step 2d</span></span>
<span id="cb18-44">      rotated_vecs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.transpose(rotated_vecs, (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>))</span>
<span id="cb18-45">      <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> verbose: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"rotated_vecs3"</span>, rotated_vecs.shape)</span>
<span id="cb18-46"></span>
<span id="cb18-47">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">### Step 3 </span><span class="al" style="color: #AD0000;
background-color: null;
font-style: inherit;">###</span></span>
<span id="cb18-48">    rotated_vecs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.concatenate([rotated_vecs, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>rotated_vecs], axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb18-49">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> verbose: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"rotated_vecs.shape"</span>, rotated_vecs.shape)</span>
<span id="cb18-50">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">### Step 4 </span><span class="al" style="color: #AD0000;
background-color: null;
font-style: inherit;">###</span></span>
<span id="cb18-51">    buckets <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.argmax(rotated_vecs, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>).astype(np.int32)</span>
<span id="cb18-52">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> verbose: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"buckets.shape"</span>, buckets.shape)</span>
<span id="cb18-53">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> verbose: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"buckets"</span>, buckets)</span>
<span id="cb18-54"></span>
<span id="cb18-55">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> mask <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb18-56">      n_buckets <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Create an extra bucket for padding tokens only</span></span>
<span id="cb18-57">      buckets <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.where(mask[<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, :], buckets, n_buckets <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb18-58"></span>
<span id="cb18-59">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># buckets is now (n_hashes, seqlen). Next we add offsets so that</span></span>
<span id="cb18-60">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># bucket numbers from different hashing rounds don't overlap.</span></span>
<span id="cb18-61">    offsets <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tie_in(buckets, np.arange(n_hashes, dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.int32))</span>
<span id="cb18-62">    offsets <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.reshape(offsets <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> n_buckets, (<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb18-63">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">### Step 5 </span><span class="al" style="color: #AD0000;
background-color: null;
font-style: inherit;">###</span></span>
<span id="cb18-64">    buckets <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.reshape(buckets <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> offsets, (<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,))</span>
<span id="cb18-65">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> verbose: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"buckets with offsets"</span>, buckets.shape, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, buckets)</span>
<span id="cb18-66">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> buckets</span></code></pre></div>
</div>
<div id="30e911a7" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># example code. Note for reference, the sizes in this example match the values in the diagram above.</span></span>
<span id="cb19-2">ohv_q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.ones((<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>))  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># (seq_len=8, n_q=5)</span></span>
<span id="cb19-3">ohv_n_buckets <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># even number</span></span>
<span id="cb19-4">ohv_n_hashes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span></span>
<span id="cb19-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> fastmath.use_backend(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"tf"</span>):</span>
<span id="cb19-6">    ohv_rng <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fastmath.random.get_prng(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb19-7">    ohv <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> our_hash_vectors(</span>
<span id="cb19-8">        ohv_q, ohv_rng, ohv_n_buckets, ohv_n_hashes, mask<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span></span>
<span id="cb19-9">    )</span>
<span id="cb19-10">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ohv shape"</span>, ohv.shape, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">ohv"</span>, ohv)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># (ohv_n_hashes * ohv_n_buckets)</span></span>
<span id="cb19-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># note the random number generators do not produce the same results with different backends</span></span>
<span id="cb19-12"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> fastmath.use_backend(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"jax"</span>):</span>
<span id="cb19-13">    ohv_rng <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fastmath.random.get_prng(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb19-14">    ohv <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> our_hash_vectors(ohv_q, ohv_rng, ohv_n_buckets, ohv_n_hashes, mask<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>)</span>
<span id="cb19-15">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ohv shape"</span>, ohv.shape, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">ohv"</span>, ohv)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># (ohv_n_hashes * ohv_n_buckets)</span></span></code></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">ValueError</span>                                Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[11], line 5</span>
<span class="ansi-green-fg ansi-bold">      3</span> ohv_n_buckets <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(98,98,98)">4</span>  <span style="font-style:italic;color:rgb(95,135,135)"># even number</span>
<span class="ansi-green-fg ansi-bold">      4</span> ohv_n_hashes <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(98,98,98)">3</span>
<span class="ansi-green-fg">----&gt; 5</span> <span style="font-weight:bold;color:rgb(0,135,0)">with</span> fastmath<span style="color:rgb(98,98,98)">.</span>use_backend(<span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">tf</span><span style="color:rgb(175,0,0)">"</span>):
<span class="ansi-green-fg ansi-bold">      6</span>     ohv_rng <span style="color:rgb(98,98,98)">=</span> fastmath<span style="color:rgb(98,98,98)">.</span>random<span style="color:rgb(98,98,98)">.</span>get_prng(<span style="color:rgb(98,98,98)">1</span>)
<span class="ansi-green-fg ansi-bold">      7</span>     ohv <span style="color:rgb(98,98,98)">=</span> our_hash_vectors(
<span class="ansi-green-fg ansi-bold">      8</span>         ohv_q, ohv_rng, ohv_n_buckets, ohv_n_hashes, mask<span style="color:rgb(98,98,98)">=</span><span style="font-weight:bold;color:rgb(0,135,0)">None</span>, verbose<span style="color:rgb(98,98,98)">=</span><span style="font-weight:bold;color:rgb(0,135,0)">True</span>
<span class="ansi-green-fg ansi-bold">      9</span>     )

File <span class="ansi-green-fg">/usr/lib/python3.10/contextlib.py:135</span>, in <span class="ansi-cyan-fg">_GeneratorContextManager.__enter__</span><span class="ansi-blue-fg">(self)</span>
<span class="ansi-green-fg ansi-bold">    133</span> <span style="font-weight:bold;color:rgb(0,135,0)">del</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>args, <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>kwds, <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>func
<span class="ansi-green-fg ansi-bold">    134</span> <span style="font-weight:bold;color:rgb(0,135,0)">try</span>:
<span class="ansi-green-fg">--&gt; 135</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">next</span><span class="ansi-yellow-bg">(</span><span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">gen</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">    136</span> <span style="font-weight:bold;color:rgb(0,135,0)">except</span> <span style="font-weight:bold;color:rgb(215,95,95)">StopIteration</span>:
<span class="ansi-green-fg ansi-bold">    137</span>     <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> <span style="font-weight:bold;color:rgb(215,95,95)">RuntimeError</span>(<span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">generator didn</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">t yield</span><span style="color:rgb(175,0,0)">"</span>) <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">None</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/fastmath/ops.py:428</span>, in <span class="ansi-cyan-fg">use_backend</span><span class="ansi-blue-fg">(name)</span>
<span class="ansi-green-fg ansi-bold">    425</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="color:rgb(0,135,0)">isinstance</span>(name, Backend):
<span class="ansi-green-fg ansi-bold">    426</span>   name <span style="color:rgb(98,98,98)">=</span> name<span style="color:rgb(98,98,98)">.</span>value
<span class="ansi-green-fg">--&gt; 428</span> <span class="ansi-yellow-bg">_assert_valid_backend_name</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">name</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">    429</span> <span style="font-weight:bold;color:rgb(0,135,0)">global</span> override_backend
<span class="ansi-green-fg ansi-bold">    430</span> prev_name_or_backend <span style="color:rgb(98,98,98)">=</span> override_backend

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/fastmath/ops.py:387</span>, in <span class="ansi-cyan-fg">_assert_valid_backend_name</span><span class="ansi-blue-fg">(name)</span>
<span class="ansi-green-fg ansi-bold">    385</span>   <span style="font-weight:bold;color:rgb(0,135,0)">if</span> backend_<span style="color:rgb(98,98,98)">.</span>value <span style="color:rgb(98,98,98)">==</span> name:
<span class="ansi-green-fg ansi-bold">    386</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span>
<span class="ansi-green-fg">--&gt; 387</span> <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> <span style="font-weight:bold;color:rgb(215,95,95)">ValueError</span>(<span style="color:rgb(175,0,0)">f</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">No backend with name </span><span style="font-weight:bold;color:rgb(175,95,135)">{</span>name<span style="font-weight:bold;color:rgb(175,95,135)">}</span><span style="color:rgb(175,0,0)">'</span>)

<span class="ansi-red-fg">ValueError</span>: No backend with name tf</pre>
</div>
</div>
</div>
<details>
<summary>
<font size="3"><b> Expected Output </b></font>
</summary>
<p><strong>Expected Values</strong></p>
<pre><code>random.rotations.shape (5, 3, 2)
random_rotations reshaped (5, 6)
rotated_vecs1 (8, 6)
rotated_vecs2 (8, 3, 2)
rotated_vecs3 (3, 8, 2)
rotated_vecs.shape (3, 8, 4)
buckets.shape (3, 8)
buckets ndarray&lt;tf.Tensor(
[[3 3 3 3 3 3 3 3]
 [3 3 3 3 3 3 3 3]
 [3 3 3 3 3 3 3 3]], shape=(3, 8), dtype=int32)&gt;
buckets with offsets (24,)
 ndarray&lt;tf.Tensor([ 3  3  3  3  3  3  3  3  7  7  7  7  7  7  7  7 11 11 11 11 11 11 11 11], shape=(24,), dtype=int32)&gt;
ohv shape (24,)
ohv ndarray&lt;tf.Tensor([ 3  3  3  3  3  3  3  3  7  7  7  7  7  7  7  7 11 11 11 11 11 11 11 11], shape=(24,), dtype=int32)&gt;
using jax
ohv shape (24,)
ohv [ 3  3  3  3  3  3  3  3  5  5  5  5  5  5  5  5 11 11 11 11 11 11 11 11]```

&lt;details&gt;
&lt;summary&gt;
    &lt;font size="3" &gt;&lt;b&gt;Completed code for reference &lt;/b&gt;&lt;/font&gt;
&lt;/summary&gt;
</code></pre>
</details></details></section>
<section id="since-this-notebook-is-ungraded-the-completed-code-is-provided-here-for-reference" class="level1">
<h1>since this notebook is ungraded the completed code is provided here for reference</h1>
<p>def our_hash_vectors(vecs, rng, n_buckets, n_hashes, mask=None, verbose=False): ““” Args: vecs: tensor of at least 2 dimension, rng: random number generator n_buckets: number of buckets in each hash table n_hashes: the number of hash tables mask: None indicating no mask or a 1D boolean array of length vecs.shape[0], containing the location of padding value verbose: controls prints for debug Returns: A vector of size n_hashes * vecs.shape[0] containing the buckets associated with each input vector per hash table.</p>
<pre><code>"""

# check for even, integer bucket sizes
assert isinstance(n_buckets, int) and n_buckets % 2 == 0

rng = fastmath.stop_gradient(tie_in(vecs, rng))
rot_size = n_buckets
### Start Code Here

### Step 1 ###
rotations_shape = (vecs.shape[-1], n_hashes, rot_size // 2)
random_rotations = fastmath.random.normal(rng, rotations_shape).astype(
    np.float32)
if verbose: print("random.rotations.shape", random_rotations.shape)

### Step 2 ###
if fastmath.backend_name() == 'jax':
  rotated_vecs = np.einsum('tf,fhb-&gt;htb', vecs, random_rotations)
  if verbose: print("using jax")
else:
  #Step 2a
  random_rotations = np.reshape(random_rotations,
                                [-1, n_hashes * (rot_size // 2)])
  if verbose: print("random_rotations reshaped", random_rotations.shape)
  #Step 2b
  rotated_vecs = np.dot(vecs, random_rotations)
  if verbose: print("rotated_vecs1", rotated_vecs.shape)
  #Step 2c
  rotated_vecs = np.reshape(rotated_vecs, [-1, n_hashes, rot_size//2])
  if verbose: print("rotated_vecs2", rotated_vecs.shape)
  #Step 2d
  rotated_vecs = np.transpose(rotated_vecs, (1, 0, 2))
  if verbose: print("rotated_vecs3", rotated_vecs.shape)

### Step 3 ###
rotated_vecs = np.concatenate([rotated_vecs, -rotated_vecs], axis=-1)
if verbose: print("rotated_vecs.shape", rotated_vecs.shape)
### Step 4 ###
buckets = np.argmax(rotated_vecs, axis=-1).astype(np.int32)
if verbose: print("buckets.shape", buckets.shape)
if verbose: print("buckets", buckets)

if mask is not None:
  n_buckets += 1  # Create an extra bucket for padding tokens only
  buckets = np.where(mask[None, :], buckets, n_buckets - 1)

# buckets is now (n_hashes, seqlen). Next we add offsets so that
# bucket numbers from different hashing rounds don't overlap.
offsets = tie_in(buckets, np.arange(n_hashes, dtype=np.int32))
offsets = np.reshape(offsets * n_buckets, (-1, 1))
### Step 5 ###
buckets = np.reshape(buckets + offsets, (-1,))
if verbose: print("buckets with offsets", buckets.shape, "\n", buckets)
return buckets```</code></pre>
<p><a name="3.3"></a> ## Part 3.3 Sorting Buckets</p>
<p>Great! Now that we have a hash function, we can work on sorting our buckets and performing our matrix operations. We’ll walk through this algorithm in small steps: * sort_buckets - we’ll perform the sort * softmax * dotandv - do the matrix math to form the dotproduct and output These routines will demonstrate a simplified version of the algorithm. We won’t address masking and variable bucket sizes but will consider how they would be handled.</p>
<p><strong>sort_buckets</strong></p>
<p>At this point, we have called the hash function and were returned the associated buckets. For example, if we started with <code>q[n_seq,n_q]</code>, with <code>n_hash = 2; n_buckets = 4; n_seq = 8</code> we might be returned: <code>bucket = [0,1,2,3,0,1,2,3, 4,5,6,7,4,5,6,7]</code> Note that it is n_hash*n_seq long and that the bucket values for each hash have been offset by n_hash so the numbers do not overlap. Going forward, we going to sort this array of buckets to group together members of the same (hash,bucket) pair.</p>
<p><strong>Instructions</strong> <strong>Step 1</strong> Our goal is to sort <img src="https://latex.codecogs.com/png.latex?q"> rather than the bucket list, so we will need to track the association of the buckets to their elements in <img src="https://latex.codecogs.com/png.latex?q">. * using np.arange, create <code>ticker</code>, just a sequence of numbers (0..n_hashed * seqlen) associating members of q with their bucket.</p>
<p><strong>Step 2</strong> This step is provided to you as it is a bit difficult to describe. We want to disambiguate elements that map to the same bucket. When a sorting routine encounters a situation where multiple entries have the same value, it can correctly choose any entry to go first. This makes testing ambiguous. This prevents that. We multiply all the buckets by <code>seqlen</code> and then add <code>ticker % seqlen</code></p>
<p><strong>Step 3</strong> Here we are! Ready to sort. This is the exciting part. * Utilize <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.sort_key_val.html#jax.lax.sort_key_val">fastmath.sort_key_val</a> and sort <code>buckets_and_t</code> and <code>ticker</code>.</p>
<p><strong>Step 4</strong> We need to be able to undo the sort at the end to get things back into their correct locations * sort <code>sticker</code> and <code>ticker</code> to for the reverse map</p>
<p><strong>Step 5</strong> create our sorted q and sorted v * use <a href="https://numpy.org/doc/stable/reference/generated/numpy.take.html">np.take</a> and <code>st</code> to grab correct values in <code>q</code> for the sorted values, <code>sq</code>. Use axis=0.</p>
<p>Use the example code below the routine to check and help debug your results.</p>
<div id="ca025645" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> sort_buckets(buckets, q, v, n_buckets, n_hashes, seqlen, verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>):</span>
<span id="cb22-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb22-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">  Args:</span></span>
<span id="cb22-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    buckets: tensor of at least 2 dimension,</span></span>
<span id="cb22-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    n_buckets: number of buckets in each hash table</span></span>
<span id="cb22-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    n_hashes: the number of hash tables</span></span>
<span id="cb22-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb22-8">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> verbose: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"---sort_buckets--"</span>)</span>
<span id="cb22-9">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">## Step 1</span></span>
<span id="cb22-10">    ticker <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.arange(n_hashes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> seqlen)</span>
<span id="cb22-11">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> verbose: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ticker"</span>,ticker.shape, ticker)</span>
<span id="cb22-12">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">## Step 2</span></span>
<span id="cb22-13">    buckets_and_t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> seqlen <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> buckets <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (ticker <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span> seqlen)</span>
<span id="cb22-14">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> verbose: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"buckets_and_t"</span>,buckets_and_t.shape, buckets_and_t)</span>
<span id="cb22-15"></span>
<span id="cb22-16">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Hash-based sort ("s" at the start of variable names means "sorted")</span></span>
<span id="cb22-17">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#Step 3</span></span>
<span id="cb22-18">    sbuckets_and_t, sticker <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fastmath.sort_key_val(</span>
<span id="cb22-19">    buckets_and_t, ticker, dimension<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb22-20">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> verbose: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sbuckets_and_t"</span>,sbuckets_and_t.shape, sbuckets_and_t)</span>
<span id="cb22-21">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> verbose: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sticker"</span>,sticker.shape, sticker)</span>
<span id="cb22-22">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#Step 4</span></span>
<span id="cb22-23">    _, undo_sort <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fastmath.sort_key_val(sticker, ticker, dimension<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb22-24">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> verbose: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"undo_sort"</span>,undo_sort.shape, undo_sort)</span>
<span id="cb22-25"></span>
<span id="cb22-26">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#Step 4</span></span>
<span id="cb22-27">    st <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (sticker <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span> seqlen)</span>
<span id="cb22-28">    sq <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.take(q, st, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb22-29">    sv <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.take(v, st, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb22-30">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> sq, sv, sticker, undo_sort</span></code></pre></div>
</div>
<div id="5e026315" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1">t_n_hashes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb23-2">t_n_buckets <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span></span>
<span id="cb23-3">t_n_seq <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> t_seqlen <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span></span>
<span id="cb23-4">t_n_q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span></span>
<span id="cb23-5">n_v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span></span>
<span id="cb23-6"></span>
<span id="cb23-7">t_q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (np.array([(j <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span> t_n_buckets) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(t_n_seq)]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.ones((t_n_q, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))).T</span>
<span id="cb23-8">t_v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.ones((t_n_seq, n_v))</span>
<span id="cb23-9">t_buckets <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array(</span>
<span id="cb23-10">    [</span>
<span id="cb23-11">        (j <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span> t_n_buckets) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> t_n_buckets <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> i</span>
<span id="cb23-12">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(t_n_hashes)</span>
<span id="cb23-13">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(t_n_seq)</span>
<span id="cb23-14">    ]</span>
<span id="cb23-15">)</span>
<span id="cb23-16"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"q</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, t_q)</span>
<span id="cb23-17"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"t_buckets: "</span>, t_buckets)</span>
<span id="cb23-18"></span>
<span id="cb23-19">t_sq, t_sv, t_sticker, t_undo_sort <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sort_buckets(</span>
<span id="cb23-20">    t_buckets, t_q, t_v, t_n_buckets, t_n_hashes, t_seqlen, verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span></span>
<span id="cb23-21">)</span>
<span id="cb23-22"></span>
<span id="cb23-23"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sq.shape"</span>, t_sq.shape, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sv.shape"</span>, t_sv.shape)</span>
<span id="cb23-24"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sq</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, t_sq)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>q
 [[0. 0. 0.]
 [1. 1. 1.]
 [2. 2. 2.]
 [3. 3. 3.]
 [0. 0. 0.]
 [1. 1. 1.]
 [2. 2. 2.]
 [3. 3. 3.]]
t_buckets:  [0 1 2 3 0 1 2 3 4 5 6 7 4 5 6 7]
---sort_buckets--
ticker (16,) [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]
buckets_and_t (16,) [ 0  9 18 27  4 13 22 31 32 41 50 59 36 45 54 63]
sbuckets_and_t (16,) [ 0  4  9 13 18 22 27 31 32 36 41 45 50 54 59 63]
sticker (16,) [ 0  4  1  5  2  6  3  7  8 12  9 13 10 14 11 15]
undo_sort (16,) [ 0  2  4  6  1  3  5  7  8 10 12 14  9 11 13 15]
sq.shape (16, 3) sv.shape (16, 5)
sq
 [[0. 0. 0.]
 [0. 0. 0.]
 [1. 1. 1.]
 [1. 1. 1.]
 [2. 2. 2.]
 [2. 2. 2.]
 [3. 3. 3.]
 [3. 3. 3.]
 [0. 0. 0.]
 [0. 0. 0.]
 [1. 1. 1.]
 [1. 1. 1.]
 [2. 2. 2.]
 [2. 2. 2.]
 [3. 3. 3.]
 [3. 3. 3.]]</code></pre>
</div>
</div>
<details>
<summary>
<font size="3"><b> Expected Output </b></font>
</summary>
<p><strong>Expected Values</strong></p>
<pre><code>q
 [[0. 0. 0.]
 [1. 1. 1.]
 [2. 2. 2.]
 [3. 3. 3.]
 [0. 0. 0.]
 [1. 1. 1.]
 [2. 2. 2.]
 [3. 3. 3.]]
t_buckets:  [0 1 2 3 0 1 2 3 4 5 6 7 4 5 6 7]
---sort_buckets--
ticker (16,) [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]
buckets_and_t (16,) [ 0  9 18 27  4 13 22 31 32 41 50 59 36 45 54 63]
sbuckets_and_t (16,) [ 0  4  9 13 18 22 27 31 32 36 41 45 50 54 59 63]
sticker (16,) [ 0  4  1  5  2  6  3  7  8 12  9 13 10 14 11 15]
undo_sort (16,) [ 0  2  4  6  1  3  5  7  8 10 12 14  9 11 13 15]
sq.shape (16, 3) sv.shape (16, 5)
sq
 [[0. 0. 0.]
 [0. 0. 0.]
 [1. 1. 1.]
 [1. 1. 1.]
 [2. 2. 2.]
 [2. 2. 2.]
 [3. 3. 3.]
 [3. 3. 3.]
 [0. 0. 0.]
 [0. 0. 0.]
 [1. 1. 1.]
 [1. 1. 1.]
 [2. 2. 2.]
 [2. 2. 2.]
 [3. 3. 3.]
 [3. 3. 3.]]
</code></pre>
<details>
<summary>
<font size="3"><b>Completed code for reference </b></font>
</summary>
<pre><code># since this notebook is ungraded the completed code is provided here for reference
def sort_buckets(buckets, q, v, n_buckets, n_hashes, seqlen, verbose=True):
    """
  Args:
    buckets: tensor of at least 2 dimension,
    n_buckets: number of buckets in each hash table
    n_hashes: the number of hash tables
    """
    if verbose: print("---sort_buckets--")
    ## Step 1
    ticker = np.arange(n_hashes * seqlen)
    if verbose: print("ticker",ticker.shape, ticker)
    ## Step 2
    buckets_and_t = seqlen * buckets + (ticker % seqlen)
    if verbose: print("buckets_and_t",buckets_and_t.shape, buckets_and_t)

    # Hash-based sort ("s" at the start of variable names means "sorted")
    #Step 3
    sbuckets_and_t, sticker = fastmath.sort_key_val(
    buckets_and_t, ticker, dimension=-1)
    if verbose: print("sbuckets_and_t",sbuckets_and_t.shape, sbuckets_and_t)
    if verbose: print("sticker",sticker.shape, sticker)
    #Step 4
    _, undo_sort = fastmath.sort_key_val(sticker, ticker, dimension=-1)
    if verbose: print("undo_sort",undo_sort.shape, undo_sort)

    #Step 4
    st = (sticker % seqlen)
    sq = np.take(q, st, axis=0)
    sv = np.take(v, st, axis=0)
    return sq, sv, sticker, undo_sort</code></pre>
<p><a name="3.4"></a> ## Part 3.4 Chunked dot product attention</p>
<p>Now let’s create the dot product attention. We have sorted <img src="https://latex.codecogs.com/png.latex?Q"> so that elements that the hash has determined are likely to be similar are adjacent to each other. We now want to perform the dot-product within those limited regions - in ‘chunks’.</p>
<img src="https://orenbochman.github.io/notes-nlp/notes/c4w4/C4W4_LN2_image12.PNG" height="400" width="750">
<center>
<b>Figure 11: Performing dot product in ‘chunks’ </b>
</center>
<p>The example we have been working on is shown above, with sequences of 8, 2 hashes, 4 buckets and, conveniently, the content of Q was such that when sorted, there were 2 entries in each bucket. If we reshape Q into a (8,2,n_q), we can use numpy matmul to perform the operation. Numpy <a href="https://numpy.org/doc/stable/reference/generated/numpy.matmul.html">matmul</a> will treat the inputs as a stack of matrices residing in the last two indexes. This will allow us to matrix multiply Q with itself in <em>chunks</em> and later can also be used to perform the matrix multiply with v.</p>
<p>We will perform a softmax on the output of the dot product of Q and Q, but in this case, there is a bit more to the story. Recall the output of the hash had multiple hash tables. We will perform softmax on those separately and then must combine them. This is where the form of softmax we defined at the top of the notebook comes into play. The routines below will utilize the logsumexp values that the <code>our_softmax</code> routine calculates.</p>
<p>There is a good deal of <a href="https://numpy.org/doc/stable/reference/generated/numpy.reshape.html">reshaping</a> to get things into the right formats. The code has many print statements that match the expected values below. You can use those to check your work as you go along. If you don’t do a lot of 3-dimensional matrix multiplications in your daily life, it might be worthwhile to open a spare cell and practice a few simple examples to get the hang of it! Here is one to start with:</p>
<div id="0c8f7ad8" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1">a <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.arange(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>).reshape((<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>))</span>
<span id="cb27-2">chunksize <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb27-3">ar <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.reshape(</span>
<span id="cb27-4">    a, (<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, chunksize, a.shape[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb27-5">)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># the -1 usage is very handy, see numpy reshape</span></span>
<span id="cb27-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(ar.shape)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(8, 2, 3)</code></pre>
</div>
</div>
<p><strong>Instructions</strong> <strong>Step 1</strong> Reshaping Q * np.reshape <code>sq</code> (sorted q) to be 3 dimensions. The middle dimension is the size of the ‘chunk’ specified by <code>kv_chunk_len</code> * np.swapaxes to perform a ‘transpose’ on the reshaped <code>sq</code>, <em>but only on the last two dimension</em> * np.matmul the two values.</p>
<p><strong>Step 2</strong> * use our_softmax to perform the softmax on the dot product. Don’t forget <code>passthrough</code></p>
<p><strong>Step 3</strong> * np.reshape <code>sv</code>. Like <code>sq</code>, the middle dimension is the size of the ‘chunk’ specified by <code>kv_chunk_len</code> * np.matmul dotlike and the reshaped <code>sv</code> * np.reshape so to a two dimensional array with the last dimension stays the same (<code>so.shape[-1]</code>) * <code>logits</code> also needs reshaping, we’ll do that.</p>
<p><strong>Step 4</strong> Now we can undo the sort. * use <a href="https://numpy.org/doc/stable/reference/generated/numpy.take.html">np.take</a> and <code>undo_sort</code> and axis = 0 to unsort so * do the same with <code>slogits</code>.</p>
<p><strong>Step 5</strong> This step combines the results of multiple hashes. Recall, the softmax was only over the values in one hash, this extends it to all the hashes. Read through it, the code is provided. Note this is taking place <em>after</em> the matrix multiply with v while the softmax output is used before the multiply. How does this achieve the correct result?</p>
<div id="fb8a1944" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> dotandv(sq, sv, undo_sort, kv_chunk_len, n_hashes, seqlen, passthrough, verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span> ):</span>
<span id="cb29-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Step 1</span></span>
<span id="cb29-3">    rsq <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.reshape(sq,(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, kv_chunk_len, sq.shape[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]))</span>
<span id="cb29-4">    rsqt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>  np.swapaxes(rsq, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb29-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> verbose: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"rsq.shape,rsqt.shape: "</span>, rsq.shape,rsqt.shape)</span>
<span id="cb29-6">    dotlike <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.matmul(rsq, rsqt)</span>
<span id="cb29-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> verbose: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"dotlike</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, dotlike)</span>
<span id="cb29-8"></span>
<span id="cb29-9">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#Step 2</span></span>
<span id="cb29-10">    dotlike, slogits <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> our_softmax(dotlike, passthrough)</span>
<span id="cb29-11">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> verbose: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"dotlike post softmax</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, dotlike)</span>
<span id="cb29-12"></span>
<span id="cb29-13">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#Step 3</span></span>
<span id="cb29-14">    vr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.reshape(sv, (<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, kv_chunk_len, sv.shape[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]))</span>
<span id="cb29-15">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> verbose:  <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"dotlike.shape, vr.shape:"</span>, dotlike.shape, vr.shape)</span>
<span id="cb29-16">    so <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.matmul(dotlike, vr)</span>
<span id="cb29-17">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> verbose: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"so.shape:"</span>, so.shape)</span>
<span id="cb29-18">    so <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.reshape(so, (<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, so.shape[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]))</span>
<span id="cb29-19">    slogits <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.reshape(slogits, (<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,))  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># provided</span></span>
<span id="cb29-20">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> verbose: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"so.shape,slogits.shape"</span>, so.shape, slogits.shape)</span>
<span id="cb29-21"></span>
<span id="cb29-22">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#Step 4</span></span>
<span id="cb29-23">    o <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.take(so, undo_sort, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb29-24">    logits <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.take(slogits, undo_sort, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb29-25">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> verbose: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"o.shape,o"</span>, o.shape, o)</span>
<span id="cb29-26">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> verbose: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"logits.shape, logits"</span>, logits.shape, logits)</span>
<span id="cb29-27"></span>
<span id="cb29-28">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#Step 5 (Provided)</span></span>
<span id="cb29-29">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> n_hashes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:</span>
<span id="cb29-30">      o <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.reshape(o, (n_hashes, seqlen, o.shape[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]))</span>
<span id="cb29-31">      logits <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.reshape(logits, (n_hashes, seqlen, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb29-32">      probs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.exp(logits <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> fastmath.logsumexp(logits, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, keepdims<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>))</span>
<span id="cb29-33">      o <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(o <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> probs, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb29-34"></span>
<span id="cb29-35">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span>(o)</span></code></pre></div>
</div>
<div id="edc469e7" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb30" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1">t_kv_chunk_len <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb30-2">out <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dotandv(</span>
<span id="cb30-3">    t_sq,</span>
<span id="cb30-4">    t_sv,</span>
<span id="cb30-5">    t_undo_sort,</span>
<span id="cb30-6">    t_kv_chunk_len,</span>
<span id="cb30-7">    t_n_hashes,</span>
<span id="cb30-8">    t_seqlen,</span>
<span id="cb30-9">    passthrough<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb30-10">    verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb30-11">)</span>
<span id="cb30-12"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"out</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, out)</span>
<span id="cb30-13"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">-----With softmax enabled----</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb30-14">out <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dotandv(</span>
<span id="cb30-15">    t_sq,</span>
<span id="cb30-16">    t_sv,</span>
<span id="cb30-17">    t_undo_sort,</span>
<span id="cb30-18">    t_kv_chunk_len,</span>
<span id="cb30-19">    t_n_hashes,</span>
<span id="cb30-20">    t_seqlen,</span>
<span id="cb30-21">    passthrough<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>,</span>
<span id="cb30-22">    verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb30-23">)</span>
<span id="cb30-24"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"out</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, out)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>rsq.shape,rsqt.shape:  (8, 2, 3) (8, 3, 2)
dotlike
 [[[ 0.  0.]
  [ 0.  0.]]

 [[ 3.  3.]
  [ 3.  3.]]

 [[12. 12.]
  [12. 12.]]

 [[27. 27.]
  [27. 27.]]

 [[ 0.  0.]
  [ 0.  0.]]

 [[ 3.  3.]
  [ 3.  3.]]

 [[12. 12.]
  [12. 12.]]

 [[27. 27.]
  [27. 27.]]]
dotlike post softmax
 [[[ 0.  0.]
  [ 0.  0.]]

 [[ 3.  3.]
  [ 3.  3.]]

 [[12. 12.]
  [12. 12.]]

 [[27. 27.]
  [27. 27.]]

 [[ 0.  0.]
  [ 0.  0.]]

 [[ 3.  3.]
  [ 3.  3.]]

 [[12. 12.]
  [12. 12.]]

 [[27. 27.]
  [27. 27.]]]
dotlike.shape, vr.shape: (8, 2, 2) (8, 2, 5)
so.shape: (8, 2, 5)
so.shape,slogits.shape (16, 5) (16,)
o.shape,o (16, 5) [[ 0.  0.  0.  0.  0.]
 [ 6.  6.  6.  6.  6.]
 [24. 24. 24. 24. 24.]
 [54. 54. 54. 54. 54.]
 [ 0.  0.  0.  0.  0.]
 [ 6.  6.  6.  6.  6.]
 [24. 24. 24. 24. 24.]
 [54. 54. 54. 54. 54.]
 [ 0.  0.  0.  0.  0.]
 [ 6.  6.  6.  6.  6.]
 [24. 24. 24. 24. 24.]
 [54. 54. 54. 54. 54.]
 [ 0.  0.  0.  0.  0.]
 [ 6.  6.  6.  6.  6.]
 [24. 24. 24. 24. 24.]
 [54. 54. 54. 54. 54.]]
logits.shape, logits (16,) [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
out
 [[ 0.  0.  0.  0.  0.]
 [ 6.  6.  6.  6.  6.]
 [24. 24. 24. 24. 24.]
 [54. 54. 54. 54. 54.]
 [ 0.  0.  0.  0.  0.]
 [ 6.  6.  6.  6.  6.]
 [24. 24. 24. 24. 24.]
 [54. 54. 54. 54. 54.]]

-----With softmax enabled----

rsq.shape,rsqt.shape:  (8, 2, 3) (8, 3, 2)
dotlike
 [[[ 0.  0.]
  [ 0.  0.]]

 [[ 3.  3.]
  [ 3.  3.]]

 [[12. 12.]
  [12. 12.]]

 [[27. 27.]
  [27. 27.]]

 [[ 0.  0.]
  [ 0.  0.]]

 [[ 3.  3.]
  [ 3.  3.]]

 [[12. 12.]
  [12. 12.]]

 [[27. 27.]
  [27. 27.]]]
dotlike post softmax
 [[[0.5        0.5       ]
  [0.5        0.5       ]]

 [[0.5        0.5       ]
  [0.5        0.5       ]]

 [[0.49999976 0.49999976]
  [0.49999976 0.49999976]]

 [[0.49999976 0.49999976]
  [0.49999976 0.49999976]]

 [[0.5        0.5       ]
  [0.5        0.5       ]]

 [[0.5        0.5       ]
  [0.5        0.5       ]]

 [[0.49999976 0.49999976]
  [0.49999976 0.49999976]]

 [[0.49999976 0.49999976]
  [0.49999976 0.49999976]]]
dotlike.shape, vr.shape: (8, 2, 2) (8, 2, 5)
so.shape: (8, 2, 5)
so.shape,slogits.shape (16, 5) (16,)
o.shape,o (16, 5) [[1.        1.        1.        1.        1.       ]
 [1.        1.        1.        1.        1.       ]
 [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]
 [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]
 [1.        1.        1.        1.        1.       ]
 [1.        1.        1.        1.        1.       ]
 [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]
 [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]
 [1.        1.        1.        1.        1.       ]
 [1.        1.        1.        1.        1.       ]
 [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]
 [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]
 [1.        1.        1.        1.        1.       ]
 [1.        1.        1.        1.        1.       ]
 [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]
 [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]]
logits.shape, logits (16,) [ 0.6931472  3.6931472 12.693148  27.693148   0.6931472  3.6931472
 12.693148  27.693148   0.6931472  3.6931472 12.693148  27.693148
  0.6931472  3.6931472 12.693148  27.693148 ]
out
 [[1.         1.         1.         1.         1.        ]
 [1.         1.         1.         1.         1.        ]
 [0.99999905 0.99999905 0.99999905 0.99999905 0.99999905]
 [0.99999905 0.99999905 0.99999905 0.99999905 0.99999905]
 [1.         1.         1.         1.         1.        ]
 [1.         1.         1.         1.         1.        ]
 [0.99999905 0.99999905 0.99999905 0.99999905 0.99999905]
 [0.99999905 0.99999905 0.99999905 0.99999905 0.99999905]]</code></pre>
</div>
</div>
<details>
<summary>
<font size="3"><b> Expected Output </b></font>
</summary>
<p><strong>Expected Values</strong></p>
<pre><code>rsq.shape,rsqt.shape:  (8, 2, 3) (8, 3, 2)
dotlike
 [[[ 0.  0.]
  [ 0.  0.]]

 [[ 3.  3.]
  [ 3.  3.]]

 [[12. 12.]
  [12. 12.]]

 [[27. 27.]
  [27. 27.]]

 [[ 0.  0.]
  [ 0.  0.]]

 [[ 3.  3.]
  [ 3.  3.]]

 [[12. 12.]
  [12. 12.]]

 [[27. 27.]
  [27. 27.]]]
dotlike post softmax
 [[[ 0.  0.]
  [ 0.  0.]]

 [[ 3.  3.]
  [ 3.  3.]]

 [[12. 12.]
  [12. 12.]]

 [[27. 27.]
  [27. 27.]]

 [[ 0.  0.]
  [ 0.  0.]]

 [[ 3.  3.]
  [ 3.  3.]]

 [[12. 12.]
  [12. 12.]]

 [[27. 27.]
  [27. 27.]]]
dotlike.shape, vr.shape: (8, 2, 2) (8, 2, 5)
so.shape: (8, 2, 5)
so.shape,slogits.shape (16, 5) (16,)
o.shape,o (16, 5) [[ 0.  0.  0.  0.  0.]
 [ 6.  6.  6.  6.  6.]
 [24. 24. 24. 24. 24.]
 [54. 54. 54. 54. 54.]
 [ 0.  0.  0.  0.  0.]
 [ 6.  6.  6.  6.  6.]
 [24. 24. 24. 24. 24.]
 [54. 54. 54. 54. 54.]
 [ 0.  0.  0.  0.  0.]
 [ 6.  6.  6.  6.  6.]
 [24. 24. 24. 24. 24.]
 [54. 54. 54. 54. 54.]
 [ 0.  0.  0.  0.  0.]
 [ 6.  6.  6.  6.  6.]
 [24. 24. 24. 24. 24.]
 [54. 54. 54. 54. 54.]]
logits.shape, logits (16,) [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
out
 [[ 0.  0.  0.  0.  0.]
 [ 6.  6.  6.  6.  6.]
 [24. 24. 24. 24. 24.]
 [54. 54. 54. 54. 54.]
 [ 0.  0.  0.  0.  0.]
 [ 6.  6.  6.  6.  6.]
 [24. 24. 24. 24. 24.]
 [54. 54. 54. 54. 54.]]

-----With softmax enabled----

rsq.shape,rsqt.shape:  (8, 2, 3) (8, 3, 2)
dotlike
 [[[ 0.  0.]
  [ 0.  0.]]

 [[ 3.  3.]
  [ 3.  3.]]

 [[12. 12.]
  [12. 12.]]

 [[27. 27.]
  [27. 27.]]

 [[ 0.  0.]
  [ 0.  0.]]

 [[ 3.  3.]
  [ 3.  3.]]

 [[12. 12.]
  [12. 12.]]

 [[27. 27.]
  [27. 27.]]]
dotlike post softmax
 [[[0.5        0.5       ]
  [0.5        0.5       ]]

 [[0.5        0.5       ]
  [0.5        0.5       ]]

 [[0.49999976 0.49999976]
  [0.49999976 0.49999976]]

 [[0.49999976 0.49999976]
  [0.49999976 0.49999976]]

 [[0.5        0.5       ]
  [0.5        0.5       ]]

 [[0.5        0.5       ]
  [0.5        0.5       ]]

 [[0.49999976 0.49999976]
  [0.49999976 0.49999976]]

 [[0.49999976 0.49999976]
  [0.49999976 0.49999976]]]
dotlike.shape, vr.shape: (8, 2, 2) (8, 2, 5)
so.shape: (8, 2, 5)
so.shape,slogits.shape (16, 5) (16,)
o.shape,o (16, 5) [[1.        1.        1.        1.        1.       ]
 [1.        1.        1.        1.        1.       ]
 [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]
 [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]
 [1.        1.        1.        1.        1.       ]
 [1.        1.        1.        1.        1.       ]
 [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]
 [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]
 [1.        1.        1.        1.        1.       ]
 [1.        1.        1.        1.        1.       ]
 [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]
 [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]
 [1.        1.        1.        1.        1.       ]
 [1.        1.        1.        1.        1.       ]
 [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]
 [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]]
logits.shape, logits (16,) [ 0.6931472  3.6931472 12.693148  27.693148   0.6931472  3.6931472
 12.693148  27.693148   0.6931472  3.6931472 12.693148  27.693148
  0.6931472  3.6931472 12.693148  27.693148 ]
out
 [[1.         1.         1.         1.         1.        ]
 [1.         1.         1.         1.         1.        ]
 [0.99999905 0.99999905 0.99999905 0.99999905 0.99999905]
 [0.99999905 0.99999905 0.99999905 0.99999905 0.99999905]
 [1.         1.         1.         1.         1.        ]
 [1.         1.         1.         1.         1.        ]
 [0.99999905 0.99999905 0.99999905 0.99999905 0.99999905]
 [0.99999905 0.99999905 0.99999905 0.99999905 0.99999905]]</code></pre>
<details>
<summary>
<font size="3"><b>Completed code for reference </b></font>
</summary>
<pre><code># since this notebook is ungraded the completed code is provided here for reference
def dotandv(sq, sv, undo_sort, kv_chunk_len, n_hashes, seqlen, passthrough, verbose=False ):
    # Step 1
    rsq = np.reshape(sq,(-1, kv_chunk_len, sq.shape[-1]))
    rsqt =  np.swapaxes(rsq, -1, -2)
    if verbose: print("rsq.shape,rsqt.shape: ", rsq.shape,rsqt.shape)
    dotlike = np.matmul(rsq, rsqt)
    if verbose: print("dotlike\n", dotlike)

    #Step 2
    dotlike, slogits = our_softmax(dotlike, passthrough)
    if verbose: print("dotlike post softmax\n", dotlike)

    #Step 3
    vr = np.reshape(sv, (-1, kv_chunk_len, sv.shape[-1]))
    if verbose:  print("dotlike.shape, vr.shape:", dotlike.shape, vr.shape)
    so = np.matmul(dotlike, vr)
    if verbose: print("so.shape:", so.shape)
    so = np.reshape(so, (-1, so.shape[-1]))
    slogits = np.reshape(slogits, (-1,))  # provided
    if verbose: print("so.shape,slogits.shape", so.shape, slogits.shape)

    #Step 4
    o = np.take(so, undo_sort, axis=0)
    logits = np.take(slogits, undo_sort, axis=0)
    if verbose: print("o.shape,o", o.shape, o)
    if verbose: print("logits.shape, logits", logits.shape, logits)

    #Step 5 (Provided)
    if n_hashes &gt; 1:
      o = np.reshape(o, (n_hashes, seqlen, o.shape[-1]))
      logits = np.reshape(logits, (n_hashes, seqlen, 1))
      probs = np.exp(logits - fastmath.logsumexp(logits, axis=0, keepdims=True))
      o = np.sum(o * probs, axis=0)

    return(o)</code></pre>
</details>
<p>Great! You have now done examples code for most of the operation that are unique to the LSH version of self-attention. I’m sure at this point you are wondering what happens if the number of entries in a bucket is not evenly distributed the way our example is. It is possible, for example for all of the <code>seqlen</code> entries to land in one bucket. Further, since the buckets are not aligned, our ‘chunks’ may be misaligned with the start of the bucket. The implementation addresses this by attending to adjacent chunks as was described in the lecture:</p>
<div id="fig-12" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-12-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/C4W4_LN2_image13.PNG" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Figure&nbsp;9: Misaligned Access, looking before and after"><img src="https://orenbochman.github.io/notes-nlp/notes/c4w4/img/C4W4_LN2_image13.PNG" width="750" height="400" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-12-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Misaligned Access, looking before and after
</figcaption>
</figure>
</div>
<p>Hopefully, having implemented parts of this, you will appreciate this diagram more fully.</p>
<section id="3.5" class="level2">
<h2 class="anchored" data-anchor-id="3.5">Part 3.5 OurLSHSelfAttention</h2>
<p>You can examine the full implementations below. Area’s we did not ‘attend to’ in our implementations above include variable bucket sizes and masking. We will instantiate a layer of the full implementation below. We tried to use the same variable names above to make it easier to decipher the full version. Note that some of the functionality we implemented in our routines is split between <code>attend</code> and <code>forward_unbatched</code>. We’ve inserted our version of hash below, but use the original version of <code>attend</code>.</p>
<div id="0500a12f" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb34" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># original version from trax 1.3.4</span></span>
<span id="cb34-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> attend(</span>
<span id="cb34-3">    q,</span>
<span id="cb34-4">    k<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>,</span>
<span id="cb34-5">    v<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>,</span>
<span id="cb34-6">    q_chunk_len<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>,</span>
<span id="cb34-7">    kv_chunk_len<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>,</span>
<span id="cb34-8">    n_chunks_before<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,</span>
<span id="cb34-9">    n_chunks_after<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,</span>
<span id="cb34-10">    mask_fn<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>,</span>
<span id="cb34-11">    q_info<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>,</span>
<span id="cb34-12">    kv_info<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>,</span>
<span id="cb34-13">    dropout<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span>,</span>
<span id="cb34-14">    rng<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>,</span>
<span id="cb34-15">):</span>
<span id="cb34-16">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""Dot-product attention, with optional chunking and/or masking.</span></span>
<span id="cb34-17"></span>
<span id="cb34-18"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">  Args:</span></span>
<span id="cb34-19"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    q: Query vectors, shape [q_len, d_qk]</span></span>
<span id="cb34-20"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    k: Key vectors, shape [kv_len, d_qk]; or None</span></span>
<span id="cb34-21"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    v: Value vectors, shape [kv_len, d_v]</span></span>
<span id="cb34-22"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    q_chunk_len: Set to non-zero to enable chunking for query vectors</span></span>
<span id="cb34-23"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    kv_chunk_len: Set to non-zero to enable chunking for key/value vectors</span></span>
<span id="cb34-24"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    n_chunks_before: Number of adjacent previous chunks to attend to</span></span>
<span id="cb34-25"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    n_chunks_after: Number of adjacent subsequent chunks to attend to</span></span>
<span id="cb34-26"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    mask_fn: </span><span class="al" style="color: #AD0000;
background-color: null;
font-style: inherit;">TODO</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(kitaev) doc</span></span>
<span id="cb34-27"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    q_info: Query-associated metadata for masking</span></span>
<span id="cb34-28"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    kv_info: Key-associated metadata for masking</span></span>
<span id="cb34-29"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    dropout: Dropout rate</span></span>
<span id="cb34-30"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    rng: RNG for dropout</span></span>
<span id="cb34-31"></span>
<span id="cb34-32"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">  Returns:</span></span>
<span id="cb34-33"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    A tuple (output, dots_logsumexp). The output has shape [q_len, d_v], and</span></span>
<span id="cb34-34"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    dots_logsumexp has shape [q_len]. The logsumexp of the attention</span></span>
<span id="cb34-35"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    probabilities is useful for combining multiple rounds of attention (as in</span></span>
<span id="cb34-36"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    LSH attention).</span></span>
<span id="cb34-37"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">  """</span></span>
<span id="cb34-38">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">assert</span> v <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb34-39">    share_qk <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> k <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb34-40"></span>
<span id="cb34-41">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> q_info <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb34-42">        q_info <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.arange(q.shape[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>], dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.int32)</span>
<span id="cb34-43"></span>
<span id="cb34-44">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> kv_info <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> share_qk:</span>
<span id="cb34-45">        kv_info <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.arange(v.shape[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>], dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.int32)</span>
<span id="cb34-46"></span>
<span id="cb34-47">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Split q/k/v into chunks along the time axis, if desired.</span></span>
<span id="cb34-48">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> q_chunk_len <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb34-49">        q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.reshape(q, (<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, q_chunk_len, q.shape[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]))</span>
<span id="cb34-50">        q_info <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.reshape(q_info, (<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, q_chunk_len))</span>
<span id="cb34-51"></span>
<span id="cb34-52">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> share_qk:</span>
<span id="cb34-53">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">assert</span> kv_chunk_len <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">or</span> kv_chunk_len <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> q_chunk_len</span>
<span id="cb34-54">        k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> q</span>
<span id="cb34-55">        kv_chunk_len <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> q_chunk_len</span>
<span id="cb34-56">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> kv_info <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb34-57">            kv_info <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> q_info</span>
<span id="cb34-58">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">elif</span> kv_chunk_len <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb34-59">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># kv_info is not None, but reshape as required.</span></span>
<span id="cb34-60">            kv_info <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.reshape(kv_info, (<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, kv_chunk_len))</span>
<span id="cb34-61">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">elif</span> kv_chunk_len <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb34-62">        k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.reshape(k, (<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, kv_chunk_len, k.shape[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]))</span>
<span id="cb34-63">        kv_info <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.reshape(kv_info, (<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, kv_chunk_len))</span>
<span id="cb34-64"></span>
<span id="cb34-65">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> kv_chunk_len <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb34-66">        v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.reshape(v, (<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, kv_chunk_len, v.shape[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]))</span>
<span id="cb34-67"></span>
<span id="cb34-68">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> share_qk:</span>
<span id="cb34-69">        k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> length_normalized(k)</span>
<span id="cb34-70">    k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> np.sqrt(k.shape[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb34-71"></span>
<span id="cb34-72">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Optionally include adjacent chunks.</span></span>
<span id="cb34-73">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> q_chunk_len <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">or</span> kv_chunk_len <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb34-74">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">assert</span> q_chunk_len <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> kv_chunk_len <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb34-75">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb34-76">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">assert</span> n_chunks_before <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> n_chunks_after <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb34-77"></span>
<span id="cb34-78">    k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> look_adjacent(k, n_chunks_before, n_chunks_after)</span>
<span id="cb34-79">    v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> look_adjacent(v, n_chunks_before, n_chunks_after)</span>
<span id="cb34-80">    kv_info <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> look_adjacent(kv_info, n_chunks_before, n_chunks_after)</span>
<span id="cb34-81"></span>
<span id="cb34-82">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Dot-product attention.</span></span>
<span id="cb34-83">    dots <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.matmul(q, np.swapaxes(k, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>))</span>
<span id="cb34-84"></span>
<span id="cb34-85">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Masking</span></span>
<span id="cb34-86">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> mask_fn <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb34-87">        dots <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mask_fn(dots, q_info[..., :, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>], kv_info[..., <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, :])</span>
<span id="cb34-88"></span>
<span id="cb34-89">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Softmax.</span></span>
<span id="cb34-90">    dots_logsumexp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fastmath.logsumexp(dots, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, keepdims<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb34-91">    dots <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.exp(dots <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> dots_logsumexp)</span>
<span id="cb34-92"></span>
<span id="cb34-93">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> dropout <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span>:</span>
<span id="cb34-94">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">assert</span> rng <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb34-95">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Dropout is broadcast across the bin dimension</span></span>
<span id="cb34-96">        dropout_shape <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (dots.shape[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>], dots.shape[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb34-97">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#</span></span>
<span id="cb34-98">        keep_prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tie_in(dots, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> dropout)</span>
<span id="cb34-99">        keep <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fastmath.random.bernoulli(rng, keep_prob, dropout_shape)</span>
<span id="cb34-100">        multiplier <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> keep.astype(dots.dtype) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> tie_in(keep, keep_prob)</span>
<span id="cb34-101">        dots <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dots <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> multiplier</span>
<span id="cb34-102"></span>
<span id="cb34-103">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># The softmax normalizer (dots_logsumexp) is used by multi-round LSH attn.</span></span>
<span id="cb34-104">    out <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.matmul(dots, v)</span>
<span id="cb34-105">    out <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.reshape(out, (<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, out.shape[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]))</span>
<span id="cb34-106">    dots_logsumexp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.reshape(dots_logsumexp, (<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,))</span>
<span id="cb34-107">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> out, dots_logsumexp</span></code></pre></div>
</div>
<div id="317665a8" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb35" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> OurLSHSelfAttention(tl.LSHSelfAttention):</span>
<span id="cb35-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""Our simplified LSH self-attention """</span></span>
<span id="cb35-3"></span>
<span id="cb35-4">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> forward_unbatched(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x, mask<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>, weights, state, rng, update_state):</span>
<span id="cb35-5">        attend_rng, output_rng <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fastmath.random.split(rng)</span>
<span id="cb35-6">        w_q, w_v, w_o <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> weights</span>
<span id="cb35-7"></span>
<span id="cb35-8">        q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.matmul(x, w_q)</span>
<span id="cb35-9">        v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.matmul(x, w_v)</span>
<span id="cb35-10"></span>
<span id="cb35-11">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> update_state:</span>
<span id="cb35-12">            _, old_hash_rng <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> state</span>
<span id="cb35-13">            hash_rng, hash_subrng <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fastmath.random.split(old_hash_rng)</span>
<span id="cb35-14">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#      buckets = self.hash_vectors(q, hash_subrng, mask)  #  original</span></span>
<span id="cb35-15">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">## use our version of hash</span></span>
<span id="cb35-16">            buckets <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> our_hash_vectors(</span>
<span id="cb35-17">                q, hash_subrng, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n_buckets, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n_hashes, mask<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>mask</span>
<span id="cb35-18">            )</span>
<span id="cb35-19">            s_buckets <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> buckets</span>
<span id="cb35-20">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._max_length_for_buckets:</span>
<span id="cb35-21">                length <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n_hashes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._max_length_for_buckets</span>
<span id="cb35-22">                <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> buckets.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> length:</span>
<span id="cb35-23">                    s_buckets <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.concatenate(</span>
<span id="cb35-24">                        [buckets, np.zeros(length <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> buckets.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.int32)],</span>
<span id="cb35-25">                        axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,</span>
<span id="cb35-26">                    )</span>
<span id="cb35-27">            state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (s_buckets, hash_rng)</span>
<span id="cb35-28">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb35-29">            buckets, _ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> state</span>
<span id="cb35-30">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._max_length_for_buckets:</span>
<span id="cb35-31">                buckets <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> buckets[: <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n_hashes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> x.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]]</span>
<span id="cb35-32"></span>
<span id="cb35-33">        seqlen <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb35-34">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">assert</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(buckets.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n_hashes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> seqlen</span>
<span id="cb35-35"></span>
<span id="cb35-36">        ticker <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tie_in(x, np.arange(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n_hashes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> seqlen, dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.int32))</span>
<span id="cb35-37">        buckets_and_t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> seqlen <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> buckets <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (ticker <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span> seqlen)</span>
<span id="cb35-38">        buckets_and_t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fastmath.stop_gradient(buckets_and_t)</span>
<span id="cb35-39"></span>
<span id="cb35-40">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Hash-based sort ("s" at the start of variable names means "sorted")</span></span>
<span id="cb35-41">        sbuckets_and_t, sticker <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fastmath.sort_key_val(</span>
<span id="cb35-42">            buckets_and_t, ticker, dimension<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb35-43">        )</span>
<span id="cb35-44">        _, undo_sort <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fastmath.sort_key_val(sticker, ticker, dimension<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb35-45">        sbuckets_and_t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fastmath.stop_gradient(sbuckets_and_t)</span>
<span id="cb35-46">        sticker <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fastmath.stop_gradient(sticker)</span>
<span id="cb35-47">        undo_sort <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fastmath.stop_gradient(undo_sort)</span>
<span id="cb35-48"></span>
<span id="cb35-49">        st <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sticker <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span> seqlen</span>
<span id="cb35-50">        sq <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.take(q, st, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb35-51">        sv <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.take(v, st, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb35-52"></span>
<span id="cb35-53">        mask_fn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> functools.partial(</span>
<span id="cb35-54">            mask_self_attention,</span>
<span id="cb35-55">            causal<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.causal,</span>
<span id="cb35-56">            exclude_self<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb35-57">            masked<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.masked,</span>
<span id="cb35-58">        )</span>
<span id="cb35-59">        q_info <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> st</span>
<span id="cb35-60"></span>
<span id="cb35-61">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">assert</span> (mask <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.masked</span>
<span id="cb35-62">        kv_info <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb35-63">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.masked:</span>
<span id="cb35-64">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># mask is a boolean array (True means "is valid token")</span></span>
<span id="cb35-65">            smask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.take(mask, st, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb35-66">            ones_like_mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tie_in(x, np.ones_like(smask, dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.int32))</span>
<span id="cb35-67">            kv_info <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> q_info <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> np.where(smask, ones_like_mask, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>ones_like_mask)</span>
<span id="cb35-68"></span>
<span id="cb35-69">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">## use original version of attend (could use ours but lacks masks and masking)</span></span>
<span id="cb35-70">        so, slogits <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> attend(</span>
<span id="cb35-71">            sq,</span>
<span id="cb35-72">            k<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>,</span>
<span id="cb35-73">            v<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>sv,</span>
<span id="cb35-74">            q_chunk_len<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.chunk_len,</span>
<span id="cb35-75">            n_chunks_before<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n_chunks_before,</span>
<span id="cb35-76">            n_chunks_after<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n_chunks_after,</span>
<span id="cb35-77">            mask_fn<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>mask_fn,</span>
<span id="cb35-78">            q_info<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>q_info,</span>
<span id="cb35-79">            kv_info<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>kv_info,</span>
<span id="cb35-80">            dropout<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.attention_dropout,</span>
<span id="cb35-81">            rng<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>attend_rng,</span>
<span id="cb35-82">        )</span>
<span id="cb35-83"></span>
<span id="cb35-84">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># np.take(so, undo_sort, axis=0); np.take(slogits, undo_sort, axis=0) would</span></span>
<span id="cb35-85">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># also work, but these helpers include performance optimizations for TPU.</span></span>
<span id="cb35-86">        o <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> permute_via_gather(so, undo_sort, sticker, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb35-87">        logits <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> permute_via_sort(slogits, sticker, buckets_and_t, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb35-88"></span>
<span id="cb35-89">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n_hashes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:</span>
<span id="cb35-90">            o <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.reshape(o, (<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n_hashes, seqlen, o.shape[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]))</span>
<span id="cb35-91">            logits <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.reshape(logits, (<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n_hashes, seqlen, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb35-92">            probs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.exp(logits <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> fastmath.logsumexp(logits, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, keepdims<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>))</span>
<span id="cb35-93">            o <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(o <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> probs, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb35-94"></span>
<span id="cb35-95">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">assert</span> o.shape <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> (seqlen, w_v.shape[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb35-96">        out <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.matmul(o, w_o)</span>
<span id="cb35-97">        out <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> apply_broadcasted_dropout(out, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.output_dropout, output_rng)</span>
<span id="cb35-98">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> out, state</span></code></pre></div>
</div>
<div id="983d7220" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb36" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Here we're going to try out our LSHSelfAttention</span></span>
<span id="cb36-2">n_heads <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span></span>
<span id="cb36-3">causal <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="cb36-4">masked <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="cb36-5">mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb36-6">chunk_len <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span></span>
<span id="cb36-7">n_chunks_before <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb36-8">n_chunks_after <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb36-9">attention_dropout <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span></span>
<span id="cb36-10">n_hashes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span></span>
<span id="cb36-11">n_buckets <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span></span>
<span id="cb36-12">seq_len <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span></span>
<span id="cb36-13">emb_len <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span></span>
<span id="cb36-14">al <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> OurLSHSelfAttention(</span>
<span id="cb36-15">    n_heads<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>n_heads,</span>
<span id="cb36-16">    d_qk<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>,</span>
<span id="cb36-17">    d_v<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>,</span>
<span id="cb36-18">    causal<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>causal,</span>
<span id="cb36-19">    chunk_len<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>,</span>
<span id="cb36-20">    n_chunks_before<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>n_chunks_before,</span>
<span id="cb36-21">    n_chunks_after<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>n_chunks_after,</span>
<span id="cb36-22">    n_hashes<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>n_hashes,</span>
<span id="cb36-23">    n_buckets<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>n_buckets,</span>
<span id="cb36-24">    use_reference_code<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb36-25">    attention_dropout<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>attention_dropout,</span>
<span id="cb36-26">    mode<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"train"</span>,</span>
<span id="cb36-27">)</span>
<span id="cb36-28"></span>
<span id="cb36-29">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> jax.random.uniform(jax.random.PRNGKey(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>), (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, seq_len, emb_len), dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.float32)</span>
<span id="cb36-30">al_osa <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fastmath.random.get_prng(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb36-31">_, _ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> al.init(tl.shapes.signature(x), rng<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>al_osa)</span></code></pre></div>
</div>
<div id="df0a35cb" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb37" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1">al(x)</span></code></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">LayerError</span>                                Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[20], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> <span class="ansi-yellow-bg">al</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">x</span><span class="ansi-yellow-bg">)</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/base.py:197</span>, in <span class="ansi-cyan-fg">Layer.__call__</span><span class="ansi-blue-fg">(self, x, weights, state, rng)</span>
<span class="ansi-green-fg ansi-bold">    195</span>   <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>state <span style="color:rgb(98,98,98)">=</span> state  <span style="font-style:italic;color:rgb(95,135,135)"># Needed if the model wasn't fully initialized.</span>
<span class="ansi-green-fg ansi-bold">    196</span> state <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>state
<span class="ansi-green-fg">--&gt; 197</span> outputs, new_state <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">pure_fn</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">x</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">weights</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">state</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">rng</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">    198</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>state <span style="color:rgb(98,98,98)">=</span> new_state
<span class="ansi-green-fg ansi-bold">    199</span> <span style="font-weight:bold;color:rgb(0,135,0)">return</span> outputs

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/base.py:605</span>, in <span class="ansi-cyan-fg">Layer.pure_fn</span><span class="ansi-blue-fg">(self, x, weights, state, rng, use_cache)</span>
<span class="ansi-green-fg ansi-bold">    602</span> <span style="font-weight:bold;color:rgb(0,135,0)">except</span> <span style="font-weight:bold;color:rgb(215,95,95)">Exception</span>:
<span class="ansi-green-fg ansi-bold">    603</span>   <span style="font-style:italic;color:rgb(95,135,135)"># Skipping 3 lines as it's always the uninteresting internal call.</span>
<span class="ansi-green-fg ansi-bold">    604</span>   name, trace <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_name, _short_traceback(skip<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">3</span>)
<span class="ansi-green-fg">--&gt; 605</span>   <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> LayerError(name, <span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">pure_fn</span><span style="color:rgb(175,0,0)">'</span>,
<span class="ansi-green-fg ansi-bold">    606</span>                    <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_caller, signature(x), trace) <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">None</span>

<span class="ansi-red-fg">LayerError</span>: Exception passing through layer OurLSHSelfAttention (in pure_fn):
  layer created in file [...]/layers/research/efficient_attention.py, line 1751
  layer input shapes: ShapeDtype{shape:(1, 8, 5), dtype:float32}

  File [...]/layers/research/efficient_attention.py, line 2158, in forward
    single_out, single_new_state = self.forward_unbatched(

  File [...]/tmp/ipykernel_121487/2615489615.py, line 17, in forward_unbatched
    q, hash_subrng, self.n_buckets, self.n_hashes, mask=mask

AttributeError: 'OurLSHSelfAttention' object has no attribute 'n_buckets'. Did you mean: '_n_buckets'?</pre>
</div>
</div>
</div>
<details>
<summary>
<font size="3"><b> Expected Output </b></font>
</summary>
<p><strong>Expected Values</strong></p>
<pre><code>using jax
using jax
using jax
DeviceArray([[[ 6.6842824e-01, -1.1364323e-01, -5.4430610e-01,
                2.1126242e-01, -1.0988623e-02],
              [ 7.0949769e-01, -1.5455185e-01, -5.9923315e-01,
                2.2719440e-01,  1.3833776e-02],
              [ 7.1442688e-01, -1.2046628e-01, -5.3956544e-01,
                1.7320301e-01, -1.6552269e-02],
              [ 6.7178929e-01, -7.6611102e-02, -5.9399861e-01,
                2.1236290e-01,  7.9482794e-04],
              [ 7.1518433e-01, -1.1359170e-01, -5.7821894e-01,
                2.1304411e-01,  3.0598268e-02],
              [ 6.8235350e-01, -9.3979925e-02, -5.5341840e-01,
                2.1608177e-01, -6.6673756e-04],
              [ 6.1286640e-01, -8.1027031e-02, -4.8148823e-01,
                1.9373313e-01,  3.1555295e-02],
              [ 7.2203505e-01, -1.0199660e-01, -5.5215168e-01,
                1.7872262e-01, -2.2289157e-02]]], dtype=float32)</code></pre>
</details>
<p><strong>Congratuations!</strong> you have created a custom layer and have become familiar with LSHSelfAttention.</p>


</section>
</details></details></details></section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2021,
  author = {Bochman, Oren},
  title = {Reformer {Efficient} {Attention:} {Ungraded} {Lab}},
  date = {2021-04-28},
  url = {https://orenbochman.github.io/notes-nlp/notes/c4w4/lab01.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2021" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2021. <span>“Reformer Efficient Attention: Ungraded
Lab.”</span> April 28, 2021. <a href="https://orenbochman.github.io/notes-nlp/notes/c4w4/lab01.html">https://orenbochman.github.io/notes-nlp/notes/c4w4/lab01.html</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/notes-nlp/notes/c4w4/lab01.html</guid>
  <pubDate>Tue, 27 Apr 2021 21:00:00 GMT</pubDate>
</item>
<item>
  <title>Chat Bots</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/c4w4/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/Course-Logo-4-1.webp" class="nolightbox img-fluid figure-img"></p>
<figcaption>course banner</figcaption>
</figure>
</div><div id="sup-slide-deck" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div>
<p>My notes for Week 4 of the <a href="https://www.coursera.org/learn/attention-models-in-nlp/home/info">Natural Language Processing with Attention Labels</a> Course in the Natural Language Processing Specialization Offered by <a href="DeepLearning.AI">DeepLearning.AI</a> on <a href="https://www.coursera.org/">Coursera</a></p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked="">Explain the motivation for reversible layers</label></li>
<li><label><input type="checkbox" checked="">Integrate locality sensitive hashing into attention layers</label></li>
<li><label><input type="checkbox" checked="">Describe the Reformer model</label></li>
</ul>
</div>
</div>
<p>Deep learning and A.I. researchers push the field forward by looking for new techniques as well as refinements of old ideas to get better performance on tasks. In this lesson <mark>we cover <em>reversible layers</em> which allow us to leverage a time memory tradeoff</mark> to process book length sequences and handle contexts over a conversation.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR Chatbots <span class="emoji" data-emoji="chart_with_upwards_trend">📈</span>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<!-- https://github.com/ikatyang/emoji-cheat-sheet/blob/master/README.md#smileys--emotion -->
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../images/in_a_nutshell.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Chatbots in a nutshell"><img src="https://orenbochman.github.io/notes-nlp/images/in_a_nutshell.jpg" class="img-fluid figure-img" alt="Chatbots in a nutshell"></a></p>
<figcaption>Chatbots in a nutshell</figcaption>
</figure>
</div>
<ul>
<li><strong>Chatbots</strong> are intelligent agents that can hold conversations with humans.</li>
<li><strong>Reversible layers</strong> allow us to trade memory for compute time.</li>
<li><strong>Reformer</strong> is an efficient transformer model that can handle long sequences.</li>
<li><strong>LSH Attention</strong> is a technique to reduce the memory requirements of transformers.</li>
</ul>
</div>
</div>
</div>
<section id="sec-task-long" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-task-long">Tasks with Long Sequences</h2>

<div class="no-row-height column-margin column-container"><div id="fig-01" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/v01-001.png" class="lightbox" data-gallery="slides" title="Figure&nbsp;1: Context Window"><img src="https://orenbochman.github.io/notes-nlp/notes/c4w4/img/v01-001.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Context Window
</figcaption>
</figure>
</div></div><p>This week we are going to learn about tasks that require processing longer sequences:</p>
<ul>
<li>Writing books</li>
<li>Storytelling and understanding</li>
<li>Building intelligent agents for conversations like chat-bots.</li>
</ul>
<p>More specifically we will understand how <em>re-former</em> model (AKA the reversible transformer) and <em>reversible layers</em> work.</p>
<p>This week we will learn about the bottlenecks in these larger transformer models, and solutions we can use to make them trainable for you. We will also learn about the. Here is what we will be building for your programming assignment: A chatbot!</p>
<p>In many ways a Chat bot is very similar to a Q&amp;A system which we built last week and that is also similar to query based summarization another task we covered a week before that. The new challenge is to manage what parts of the new and old context we keep around as the dialogue progresses. Chatbot are smart A.I. agents and much of the techniques developed under the umbrella of knowledge-based AI is also relevant in developing these. For instance carrying out actions on behalf of the user.</p>
<p>Chatbots can also get a very simple ui via the web or as an mobile app, which is another area I have some experience. However an even more powerful paradigm here is the ability to interact using voice which has many additional benefit for example supporting people with disabilities and operating in hands-free mode.</p>
<p>Here is a link to an <a href="https://play.aidungeon.io/main/landing">AI Storytelling system</a>.</p>
</section>
<section id="sec-transformer-complexity" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-transformer-complexity">Transformer Complexity</h2>

<div class="no-row-height column-margin column-container"><div id="fig-02" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/v02-002.png" class="lightbox" data-gallery="slides" title="Figure&nbsp;2: week-4"><img src="https://orenbochman.github.io/notes-nlp/notes/c4w4/img/v02-002.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: week-4
</figcaption>
</figure>
</div></div><p>One of the biggest issues with the transformers is that it takes time and a lot of memory when training. Concretely here are the numbers. If we have a sequence of length <img src="https://latex.codecogs.com/png.latex?L"> , then we need <img src="https://latex.codecogs.com/png.latex?L%5E2*N"> memory to handle the sequence. So if we have <img src="https://latex.codecogs.com/png.latex?N"> layers, that means your model will take <img src="https://latex.codecogs.com/png.latex?N"> times more time to complete. As <img src="https://latex.codecogs.com/png.latex?L"> gets larger, the memory and the time quickly increases.</p>
<p>Perhaps this is the reason people are looking into converting transformers into RNN after training.</p>

<div class="no-row-height column-margin column-container"><div id="fig-03" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/v02-003.png" class="lightbox" data-gallery="slides" title="Figure&nbsp;3: week-4"><img src="https://orenbochman.github.io/notes-nlp/notes/c4w4/img/v02-003.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: week-4
</figcaption>
</figure>
</div></div><p>When we are handling long sequences, we frequently don’t need to consider all L positions. We can just focus on an area of interest instead. For example, when translating a long text from one language to another, we don’t need to consider every word at once. We can instead focus on a single word being translated, and those immediately around it, by using attention.</p>
<p>To overcome the memory requirements we can recompute the activations. As long as we do it efficiently, we will be able to save a good amount of time and memory. We will learn this week how to do it. Instead of storing N layers, we will be able to recompute them when doing the back-propagation. That combined with local attention, will give we a much faster model that works at the same level as the transformer we learned about last week.</p>
<ul>
<li><p>one area where we can make headway is working with a subsequence of interest.</p></li>
<li><p>during training we need to keep the activations in memory for the back propagation task. Clearly for inference we may be able to save on memory.</p></li>
<li><p>the alternative is to discard the activations as we go along and recalculate later. This can allows trading memory for compute time. However with larger models compute time is also a bottleneck.</p></li>
</ul>

<div class="no-row-height column-margin column-container"><div id="fig-04" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-04-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/v03-004.png" class="lightbox" data-gallery="slides" title="Figure&nbsp;4: Approximate Nearest Neighbours"><img src="https://orenbochman.github.io/notes-nlp/notes/c4w4/img/v03-004.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-04-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Approximate Nearest Neighbours
</figcaption>
</figure>
</div></div></section>
<section id="sec-lsh-attention" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-lsh-attention">LSH Attention</h2>
<p>In Course 1, we covered how <em>locality sensitive hashing</em> (LSH) works. We learned about:</p>
<ul>
<li>KNN</li>
<li>Hash Tables and Hash Functions</li>
<li>Locality Sensitive Hashing</li>
<li>Multiple Planes</li>
</ul>
<p>Here are the steps to follow to compute LSH given some vectors, where the vectors may correspond to the transformed word embedding that your transformer outputs.</p>
<p>Attention is used to try which query (q) and key (k) are the most similar. To do so, we hash q and the keys. This will put similar vectors in the same bucket that we can use. The drawing above shows the lines that separate the buckets. Those could be seen as the planes.</p>
<p>First let’s recall how the standard attention mechanism is defined as follows:</p>
<p><span id="eq-attention"><img src="https://latex.codecogs.com/png.latex?%0AA(Q,K,V)%20=%20softmax(QK%5ET)V%0A%5Ctag%7B1%7D"></span></p>
<p>Once we hash <img src="https://latex.codecogs.com/png.latex?Q"> and <img src="https://latex.codecogs.com/png.latex?K"> we will then compute standard attention on the bins that we have created. We will repeat the same process several times to increase the probability of having the same key in the same bin as the query.</p>

<div class="no-row-height column-margin column-container"><div id="fig-05" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-05-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/v03-005.png" class="lightbox" data-gallery="slides" title="Figure&nbsp;5: week-4"><img src="https://orenbochman.github.io/notes-nlp/notes/c4w4/img/v03-005.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-05-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: week-4
</figcaption>
</figure>
</div></div><ul>
<li>Given the sequence of queries and keys, we hash them into buckets. Check out Course 1 Week 4 for a review of the hashing.</li>
<li>We will then sort them by bucket.</li>
<li>We split the buckets into chunks (this is a technical detail for parallel computing purposes).</li>
<li>We then compute the attention within the same bucket of the chunk we are looking at and the previous chunk.</li>
</ul>
<blockquote class="blockquote">
<p>Q. Why do we need to look at the previous chunk?</p>
</blockquote>
<p>We can see in the figure some buckets (both blue and yellow) have been split across two chunks. Looking at the previous chunk will let we attend to the full bucket.</p>
<p>In <em>Winograd schemas</em> the resolution of the ambiguous pronoun switches between the two variants of the sentence.</p>
<blockquote class="blockquote">
<p>the <strong>animal</strong> didn’t cross the street because <code>it</code> was too <strong>tired</strong> / the animal didn’t cross the <strong>street</strong> because <code>it</code> was too <strong>wide</strong> / The city <strong>councilmen</strong> refused the demonstrators a permit because <code>they</code> <strong>feared</strong> violence. / The city councilmen refused the <strong>demonstrators</strong> a permit because <code>they</code> <strong>advocated</strong> violence. /</p>
</blockquote>
</section>
<section id="reformer-lsh" class="level2">
<h2 class="anchored" data-anchor-id="reformer-lsh">Reformer LSH</h2>
<p><a href="../../notes/c4w4/lab01.html">Reformer LSH</a></p>
</section>
<section id="sec-reversible" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-reversible">Motivation for Reversible Layers: Memory!</h2>

<div class="no-row-height column-margin column-container"><div id="fig-06" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-06-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/v04-006.png" class="lightbox" data-gallery="slides" title="Figure&nbsp;6: Memory efficency"><img src="https://orenbochman.github.io/notes-nlp/notes/c4w4/img/v04-006.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-06-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Memory efficency
</figcaption>
</figure>
</div></div><p>For example in this model:</p>
<ul>
<li>2 GB for the input</li>
<li>2 GB are required to compute the Attention</li>
<li>2 GB for the feed forward. There are 12 attention layers 12 feed forward layers. That is equal to <img src="https://latex.codecogs.com/png.latex?12%20*%202%20+%2012*2%20+%202%20(for%20the%20input)%20=%2050%20GB">. That is a lot of memory.</li>
</ul>
<p>If N is the sequence length:</p>
<ul>
<li>Transformers need <img src="https://latex.codecogs.com/png.latex?O(N%5E2)"> memory.</li>
</ul>
<p>Each layer of a transformers has an Attention block and feed-forward block. If we want to process, for example to train a document of length 1 million token with 12 layers we will need 50 GB of ram. As we use residual architecture during prediction we only need the current layers input and the output for the next layer. But during training we need to keep all the copies so we can back-propagate the errors.</p>
</section>
<section id="sec-reversible-residual" class="level2">
<h2 class="anchored" data-anchor-id="sec-reversible-residual">Reversible Residual Layers</h2>
</section>
<section id="sec-reformer" class="level2">
<h2 class="anchored" data-anchor-id="sec-reformer">Reformer</h2>
<p>can run 1 million token in 16 gb</p>
</section>
<section id="sec-lab-2" class="level2">
<h2 class="anchored" data-anchor-id="sec-lab-2">Lab 2: Reversible layers</h2>
<p><a href="../../notes/c4w4/lab02.html"></a></p>
<p>From the <code>trax</code> <a href="https://trax-ml.readthedocs.io/en/latest/notebooks/layers_intro.html#2.-Inputs-and-Outputs">documents</a> a Residual, involves first a split and then a merge:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> Serial(</span>
<span id="cb1-2">    Branch(shortcut, layer), <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># split </span></span>
<span id="cb1-3">    Add(),                   <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># merge</span></span>
<span id="cb1-4">)</span></code></pre></div>
<p>where:</p>
<ul>
<li><code>Branch(shortcut, layers)</code>: makes two copies of the single incoming data stream, passes one copy via the shortcut (typically a no-op), and processes the other copy via the given layers (applied in series). <img src="https://latex.codecogs.com/png.latex?%5B%F0%9D%91%9B_%7B%F0%9D%91%96%F0%9D%91%9B%7D=1,%20%F0%9D%91%9B_%7B%F0%9D%91%9C%F0%9D%91%A2%F0%9D%91%A1%7D=2%5D"></li>
<li><code>Add()</code>: combines the two streams back into one by adding two tensors element-wise. <img src="https://latex.codecogs.com/png.latex?%5B%F0%9D%91%9B_%7B%F0%9D%91%96%F0%9D%91%9B%7D=2,%20%F0%9D%91%9B_%7B%F0%9D%91%9C%F0%9D%91%A2%F0%9D%91%A1%7D=1%5D"></li>
</ul>
<p>In the <code>Branch</code> operation each layer in the input list copies as many inputs from the stack as it needs, and their outputs are successively combined on stack. Put another way, each element of the branch can have differing numbers of inputs and outputs. Let’s try a more complex example. To work these operations modify the stack by replicating the input needed as well as pushing the outputs (as specified using th <code>out</code> parameters).</p>
</section>
<section id="sec-references" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-references">References</h2>
<ul>
<li><a href="https://arxiv.org/pdf/1509.02897.pdf">Practical and Optimal LSH for Angular Distance</a></li>
</ul>
<section id="tokenization" class="level3">
<h3 class="anchored" data-anchor-id="tokenization">Tokenization</h3>
<ol type="1">
<li><a href="https://www.aclweb.org/anthology/D18-2012.pdf">SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (Kudo &amp; Richardson 2018)</a> sub-word tokenization</li>
<li><a href="https://www.aclweb.org/anthology/P18-1007.pdf">Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates (Kudo 2018)</a> sub-word tokenization</li>
<li><a href="https://arxiv.org/pdf/1508.07909.pdf">Neural Machine Translation of Rare Words with Subword Units (Sennrich et all 2016)</a> sub-word tokenization</li>
<li><a href="https://www.tensorflow.org/tutorials/tensorflow_text/subwords_tokenizer">Subword tokenizers TF tutorial</a> sub-word tokenization</li>
<li>[https://blog.floydhub.com/tokenization-nlp/]</li>
<li><a href="https://arxiv.org/abs/1602.02215">Swivel: Improving Embeddings by Noticing What’s Missing (Shazeer, 2016)</a></li>
</ol>
</section>
<section id="transformers" class="level3">
<h3 class="anchored" data-anchor-id="transformers">Transformers</h3>
<ol type="1">
<li>[Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer] (Raffel et al, 2019)<br>
</li>
<li>[Reformer: The Efficient Transformer] (Kitaev et al, 2020)</li>
<li><a href="https://arxiv.org/abs/1706.03762">Attention Is All We Need</a> (Vaswani et al, 2017) <span class="citation" data-cites="vaswani2023attentionneed">Vaswani et al. (2023)</span></li>
<li>[Deep contextualized word representations] (Peters et al, 2018)</li>
<li>[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding] (Devlin et al, 2018)</li>
<li>[Finetuning Pretrained Transformers into RNNs] (Kasai et all 2021)</li>
<li>[The Illustrated Transformer] (Alammar, 2018)</li>
<li>[The Illustrated GPT-2] (Alammar, 2019)</li>
<li>[How GPT3 Works - Visualizations and Animations] (Alammar, 2020)</li>
<li>In <span class="citation" data-cites="weng2018attention">Weng (2018)</span> the author covers many attention mechanism <a href="https://lilianweng.github.io/posts/2018-06-24-attention/">Attention? Attention!</a></li>
<li>[The Transformer Family] (Lilian Weng, 2020)</li>
<li><a href="https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/">Teacher forcing for RNNs</a></li>
</ol>
</section>
<section id="question-answering-task" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="question-answering-task">Question Answering Task:</h3>
<ul>
<li>In <span class="citation" data-cites="rush2015neural">Rush (2015)</span> , a paper titled <a href="https://arxiv.org/pdf/1509.00685.pdf">A Neural Attention Model for Abstractive Sentence Summarization</a> the authors discuss the summarization task.</li>
</ul>
<p>The first two videos can be viewed on youtube.</p>

<div class="no-row-height column-margin column-container"><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/yIdF-17HwSk" title="Question Answering" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Christopher Manning in Stanford CS224N: NLP with Deep Learning | Winter 2019 | Lecture 10 On Question Answering.
</figcaption>
</figure>
</div><div id="vid-02" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/NcqfHa0_YmU?list=PLoROMvodv4rMFqRtEuo6SGjY4XbRIVRd4&amp;t=222" title="Question Answering" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;2: Christopher Manning and Danqi Chen in Stanford CS224N NLP with Deep Learning | Winter 2021 | Lecture 12 - Question Answering
</figcaption>
</figure>
</div><div id="vid-03" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/hAvtJ516Mw4" title="Swivel Embeddings" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid quarto-uncaptioned" id="vid-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;3
</figcaption>
</figure>
</div></div>

</section>
</section>
<section id="links" class="level2">
<h2 class="anchored" data-anchor-id="links">Links</h2>
<ul>
<li><p><a href="https://github.com/google/jax">Jax</a></p></li>
<li><p><a href="https://trax-ml.readthedocs.io/en/latest/index.html">Trax</a></p></li>
<li><p><a href="https://gitter.im/trax-ml/community">Trax community</a> on Gitter</p></li>
<li><p><a href="https://github.com/abisee/cnn-dailymail">CNN daily mail dataset</a></p></li>
<li><p><a href="https://leimao.github.io/">Lei Mao</a> Machine Learning, Artificial Intelligence, Computer Science.</p></li>
<li><p><a href="https://leimao.github.io/blog/Byte-Pair-Encoding/">Byte Pair Encoding (Lei Mao 2021)</a></p></li>
<li><p><a href="https://arxiv.org/abs/1910.10683">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a></p></li>
<li><p><a href="https://arxiv.org/abs/2001.04451">Reformer: The Efficient Transformer</a></p></li>
<li><p><a href="https://arxiv.org/abs/1706.03762">Attention Is All We Need</a></p></li>
<li><p><a href="https://arxiv.org/pdf/1802.05365.pdf">Deep contextualized word representations</a></p></li>
<li><p><a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></p></li>
<li><p><a href="http://jalammar.github.io/illustrated-gpt2/">The Illustrated GPT-2</a></p></li>
<li><p><a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p></li>
<li><p><a href="http://jalammar.github.io/how-gpt3-works-visualizations-animations/">How GPT3 Works - Visualizations and Animations</a></p></li>
<li><p><a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html">Attention? Attention!</a> (Lilian Weng, 2018)</p></li>
<li><p><a href="https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html">The Transformer Family</a> “(Lilian Weng, 2020)”</p></li>
<li><p><a href="https://arxiv.org/abs/2103.13076">Finetuning Pretrained Transformers into RNNs</a> “(Kasai et all 2021)”</p></li>
</ul>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-rush2015neural" class="csl-entry">
Rush, AM. 2015. <span>“A Neural Attention Model for Abstractive Sentence Summarization.”</span> <em>arXiv Preprint arXiv:1509.00685</em>.
</div>
<div id="ref-vaswani2023attentionneed" class="csl-entry">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. <span>“Attention Is All You Need.”</span> <a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>.
</div>
<div id="ref-weng2018attention" class="csl-entry">
Weng, Lilian. 2018. <span>“Attention? Attention!”</span> <em>Lilianweng.github.io</em>. <a href="https://lilianweng.github.io/posts/2018-06-24-attention/">https://lilianweng.github.io/posts/2018-06-24-attention/</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2021,
  author = {Bochman, Oren},
  title = {Chat {Bots}},
  date = {2021-04-27},
  url = {https://orenbochman.github.io/notes-nlp/notes/c4w4/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2021" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2021. <span>“Chat Bots.”</span> April 27, 2021. <a href="https://orenbochman.github.io/notes-nlp/notes/c4w4/">https://orenbochman.github.io/notes-nlp/notes/c4w4/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Chat bot development</category>
  <category>Coursera</category>
  <category>Intelligent agents</category>
  <category>Locality sensitive hashing</category>
  <category>NLP with Attention Models</category>
  <category>Neural Machine Translation</category>
  <category>NLP</category>
  <category>Notes</category>
  <category>Positional encoding</category>
  <category>Question answering task</category>
  <category>Reversible layers</category>
  <category>Teacher forcing</category>
  <category>Transformer</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/c4w4/</guid>
  <pubDate>Mon, 26 Apr 2021 21:00:00 GMT</pubDate>
</item>
<item>
  <title>Automatic Summarization Task</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/posts/2021-04-24-summerization/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="notes-formulas.jpg" class="lightbox" data-gallery="slides" title="notes"><img src="https://orenbochman.github.io/notes-nlp/posts/2021-04-24-summerization/notes-formulas.jpg" class="img-fluid figure-img" alt="notes"></a></p>
<figcaption>notes</figcaption>
</figure>
</div></div><p>This is one of my blogposts on NLP.</p>
<section id="motivation---building-a-good-summarizer." class="level2">
<h2 class="anchored" data-anchor-id="motivation---building-a-good-summarizer.">Motivation - Building a good summarizer.</h2>
<p>In the Deeplearning.ai NLP specialization, we implemented both <strong>Q&amp;A</strong> and <strong>Summarization tasks</strong>. However, we only looked at these tasks in terms of the attention models. It is now time to dive deeper into the summarization task. In this article I will review a talk covering a body of research on summarization, which has many ideas about features. Consider some of these and see what aspects are relevant to a modern task implementation.</p>
<p>When I looked for more information, I found the following video, which, together with a review paper, can provide a good intro to this subject. I also found links to the papers mentioned and extracted some of their abstracts.</p>
<p>Looking at all the algorithms critically, I found some new ideas for tackling problems beyond what I had come up with on my own.</p>
</section>
<section id="automatic-text-summarization-task" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="automatic-text-summarization-task">Automatic Text Summarization Task</h2>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR The summarization Task
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../images/in_a_nutshell.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Summarization in a nutshell"><img src="https://orenbochman.github.io/notes-nlp/images/in_a_nutshell.jpg" class="img-fluid figure-img" alt="Summarization in a nutshell"></a></p>
<figcaption>Summarization in a nutshell</figcaption>
</figure>
</div>
<p>This is a review of the Automatic Text Summarization Task by Masa Nekic. The talk provides a starter ontology, a review of algorithms, some evaluation methods, and some tools.</p>
</div>
</div>

<div class="no-row-height column-margin column-container"><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/_d0OXm0dRZ4" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Masa Nekic’s NDC Conferences talk on the Automatic Text Summarization Task
</figcaption>
</figure>
</div></div><p>Notes from the following lecture by <a href="https://www.crunchbase.com/person/masa-nekic">Masa Nekic</a> given at <a href="https://ndcoslo.com/">NDC Conferences</a>.</p>
<p>The talk provides:</p>
<ul>
<li>a starter ontology.</li>
<li>a review of algorithms.</li>
<li>some evaluation methods</li>
<li>some tools.</li>
</ul>
<hr>
</section>
<section id="concepts" class="level1 page-columns page-full">
<h1>Concepts</h1>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="s1-mindmap.png" class="lightbox" data-gallery="slides" title="mindmap"><img src="https://orenbochman.github.io/notes-nlp/posts/2021-04-24-summerization/s1-mindmap.png" class="img-fluid figure-img" alt="mindmap"></a></p>
<figcaption>mindmap</figcaption>
</figure>
</div></div><section id="ontological-mindmap" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="ontological-mindmap">Ontological Mindmap</h2>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">mindmap
  Root((Summarization&lt;br&gt;Task))
    id1[Input Based]
        id11(Single document) 
        id12(Multi document)
    id2[Contextal]
        id21[Generic]
        id22(Domain Specific)
        id23(Query)
           id231{{from IR}}
    id3[Output Based]
        id31(Extractive)
          id311{{Picks sentences from the text}}
        id32(Abstractive)
          id321{{Generates from scratch}}
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>Note: the Query based approach intersects with the NLP QA task.</p>
<hr>
<p class="page-columns page-full"><a href="s2-extractive-abstractive.png" class="lightbox page-columns page-full" data-gallery="slides"></a></p><div class="no-row-height column-margin column-container"><a href="s2-extractive-abstractive.png" class="lightbox page-columns page-full" data-gallery="slides"><img src="https://orenbochman.github.io/notes-nlp/posts/2021-04-24-summerization/s2-extractive-abstractive.png" class="img-fluid"></a></div><p></p>
</section>
<section id="extractive-vs.-abstractive" class="level2">
<h2 class="anchored" data-anchor-id="extractive-vs.-abstractive">Extractive vs.&nbsp;Abstractive</h2>
<p>The “Summarizing before exams” meme demonstrates the extractive approach. The “abridged classics” meme demonstrates the abstractive approach.</p>
<hr>
<section id="extractive-summaries-illustrated" class="level3">
<h3 class="anchored" data-anchor-id="extractive-summaries-illustrated">Extractive Summaries Illustrated</h3>
<p>Extractive algorithms locate and rank the content of a document.</p>
<blockquote class="blockquote">
<p><mark>Game of Thrones is an American fantasy drama television series created by David Benioff and D. B. Weiss for HBO. It is an adaptation of A Song of Ice and Fire, George R. R. Martin’s series of fantasy novels</mark>, the first of which is A Game of Thrones. The show was both produced and filmed in Belfast and elsewhere in the United Kingdom. Filming locations also included Canada, Croatia, Iceland, Malta, Morocco, and Spain. <mark>The series premiered on HBO in the United States on April 17, 2011, and concluded on May 19, 2019, with 73 episodes broadcast over eight seasons.</mark></p>
<p><mark>Set on the fictional continents of Westeros and Essos, Game of Thrones has several plots and a large ensemble cast and follows several story arcs</mark>. One arc is about the Iron Throne of the Seven Kingdoms and follows a web of alliances and conflicts among the noble dynasties either vying to claim the throne or fighting for independence from it. Another focuses on the last descendant of the realm’s deposed ruling dynasty, who has been exiled and is plotting a return to the throne, while another story arc follows the Night’s Watch, a brotherhood defending the realm against the fierce peoples and legendary creatures of the North.</p>
</blockquote>
<p>The Extractive Summary:</p>
<blockquote class="blockquote">
<p>Game of Thrones is an American fantasy drama television series created by David Benioff and D. B. Weiss for HBO. It is an adaptation of A Song of Ice and Fire, George R. R. Martin’s series of fantasy novels</p>
<p>Set on the fictional continents of Westeros and Essos, Game of Thrones has several plots and a large ensemble cast and follows several story arcs</p>
</blockquote>
<p>Extractive Summaries draw text verbatim from the source.</p>
<ol type="1">
<li>This was the more common approach in NLP</li>
<li>it is closely related to IR and Q&amp;A task.</li>
<li>Their main challenges of this approach are:
<ul>
<li>a <em>lack balance,</em> when some parts over represented while others under represented.</li>
<li>a <em>lack of cohesion,</em> as extracted text retains dangling pronouns etc.</li>
</ul></li>
</ol>
<hr>
</section>
</section>
<section id="abstractive-summaries-illustrated" class="level2">
<h2 class="anchored" data-anchor-id="abstractive-summaries-illustrated">Abstractive Summaries Illustrated</h2>
<p>Abstractive algorithms add generation of the extracted content.</p>
<blockquote class="blockquote">
<p>Game of Thrones is an American fantasy drama television series created by David Benioff and D. B. Weiss for HBO. It is an adaptation of A Song of Ice and fire, George R. R. Martin’s series of fantasy novels, the first of which is A Game of Thrones. The show was both produced and filmed in Belfast and elsewhere in the United Kingdom. Filming locations also included Canada, Croatia, Iceland, Malta, Morocco, and Spain, The series premiered on HBO in the United States on April 17, 2011, and concluded on May 19, 2019, with 73 episodes broadcast over eight seasons.</p>
<p>Set on the fictional continents of Westeros and Essos, Game of Thrones has several plots and a large ensemble cast and follows several story arcs. One arc is about the Iron Throne of the Seven Kingdoms and follows a web of alliances and conflicts among the noble dynasties either vying to claim the throne or fighting for independence from it. Another focuses on the last descendant of the realm’s deposed ruling dynasty, who has been exiled and is plotting a return to the throne, while another story arc follows the Night’s Watch, a brotherhood defending the realm against the fierce peoples and legendary creatures of the North.</p>
</blockquote>
<p>Summary (Abstractive):</p>
<blockquote class="blockquote">
<p>Game of Thrones is a TV show based on book series A Song of Ice and Fire, written by G. R. R. Martin. All eight seasons were filmed in many beautiful countries across <strong>three different continents</strong>. Game of Thrones has a very complex story with several plots and story arcs — from conflicts between Westeros nobility to claim the Iron Throne and rule over Seven Kingdoms to fight between brotherhood called Night’s watch and enemies from the North.</p>
</blockquote>
<ol type="1">
<li><strong>Abstractive</strong> Summaries are not constrained to using text drawn the source. They can draw on <code>common-sense</code> and <code>domain knowledge</code> external to the document.</li>
<li>This is the more challenging approach in NLP</li>
<li>Their main issues are:
<ul>
<li>good coverage.</li>
<li>avoiding repetition.</li>
<li>can provide better <em>compression</em>.</li>
</ul></li>
</ol>
<hr>
</section>
</section>
<section id="algorithms-methods" class="level1 page-columns page-full">
<h1>Algorithms &amp; Methods</h1>
<section id="positional-method" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="positional-method">Positional method</h2>
<ul>
<li>Introduced in <span class="citation" data-cites="Baxendale1958Machine">(Baxendale 1958)</span></li>
<li>200 paragraphs</li>
<li>First and last sentence of a paragraph are topic sentences (85% vs 7%)</li>
</ul>
<p>e.g.</p>
<blockquote class="blockquote">
<p><mark>Game of Thrones is an American fantasy drama television series created by David Benioff and D. B. Weiss for HBO</mark>. It is an adaptation of A Song of Ice and Fire, George R. R. Martinis series of fantasy novels, the first of which is A Game of Thrones. The show was both produced and filmed in Belfast and elsewhere in the United Kingdom. Filming locations also included Canada, Croatia, Iceland, Malta, Morocco, and Spain. <mark>The series premiered on HBO in the United States on April 17, 2011, and concluded on May 19, 2019, with 73 episodes broadcast over eight seasons</mark>.</p>
</blockquote>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
My insights:
</div>
</div>
<div class="callout-body-container callout-body">
<p>If we want to build a summerier, <mark>first and last sentences may be useful features</mark>. Note that in academic writing the penultimate sentence may often the most important.</p>
<p>So a simple extractive method might pick one of the sentences from each paragraph. It could have a prior that like the first last sentence of a paragraph. But it would need more features to break ties.</p>
</div>
</div>
<hr>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="s8-luhn-method-1958.png" class="lightbox" data-gallery="slides" title="s8-luhn-method-1958"><img src="https://orenbochman.github.io/notes-nlp/posts/2021-04-24-summerization/s8-luhn-method-1958.png" class="img-fluid figure-img" alt="s8-luhn-method-1958"></a></p>
<figcaption>s8-luhn-method-1958</figcaption>
</figure>
</div></div></section>
<section id="luhns-method" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="luhns-method">Luhn’s method</h2>
<ul>
<li>Introduced in <span class="citation" data-cites="luhn-58">(Luhn 1958)</span></li>
<li>Frequency of content terms</li>
<li>Data pre-processing
<ul>
<li>Stop words removal</li>
<li>Stemming (cats cat)</li>
</ul></li>
</ul>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="s9-luhn-method-formula.png" class="lightbox" data-gallery="slides" title="s9-luhn-method-formula"><img src="https://orenbochman.github.io/notes-nlp/posts/2021-04-24-summerization/s9-luhn-method-formula.png" class="img-fluid figure-img" alt="s9-luhn-method-formula"></a></p>
<figcaption>s9-luhn-method-formula</figcaption>
</figure>
</div></div><p>Select sentences with highest concentrations of salient content terms</p>
<p><img src="https://latex.codecogs.com/png.latex?%20Score%20=%20%5Cfrac%7B%5Ctext%7BSalient%20Words%7D%5E2%7D%7B%20%20%5Ctext%7BTerms%20in%20chunk%7D%20%7D"></p>
<hr>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="s10-edmundson-method.png" class="lightbox" data-gallery="slides" title="s10-edmundson-method"><img src="https://orenbochman.github.io/notes-nlp/posts/2021-04-24-summerization/s10-edmundson-method.png" class="img-fluid figure-img" alt="s10-edmundson-method"></a></p>
<figcaption>s10-edmundson-method</figcaption>
</figure>
</div></div></section>
<section id="edmundsons-method" class="level2">
<h2 class="anchored" data-anchor-id="edmundsons-method">Edmundson’s method</h2>
<p>Introduced in <span class="citation" data-cites="edmundson-1969">(Edmundson 1969)</span></p>
<ul>
<li>Position (P)</li>
<li>Word frequency (F)</li>
<li>Cue words (C)
<ul>
<li><strong>Bonus words</strong> — pointing to the important sentence</li>
<li><strong>Stigma words</strong> — negative effect on the sentence importance</li>
<li><strong>Null words</strong> — neutral or irrelevant to the importance of the sentence</li>
</ul></li>
<li>Document structure (S)</li>
</ul>
<p>Linear combination of these 4 features:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ascore%20=%20%5Calpha_1%20P%20+%20%5Calpha_2%20F%20+%20%5Calpha_3%20C%20+%20%5Calpha_4%20S%20%5Cqquad%0A"></p>
<blockquote class="blockquote">
<p>This paper describes new methods of automatically extracting documents for screening purposes, i.e.&nbsp;the computer selection of sentences having the greatest potential for conveying to the reader the substance of the document. While previous work has focused on one component of sentence significance, namely, the presence of high-frequency content words (key words), the methods described here also treat three additional components: pragmatic words (cue words); title and heading words; and structural indicators (sentence location). The research has resulted in an operating system and a research methodology. The extracting system is parameterized to control and vary the influence of the above four components. The research methodology includes procedures for the compilation of the required dictionaries, the setting of the control parameters, and the comparative evaluation of the automatic extracts with manually produced extracts. The results indicate that the three newly proposed components dominate the frequency component in the production of better extracts. – <span class="citation" data-cites="edmundson-1969">(Edmundson 1969)</span></p>
</blockquote>
<p>It may not be clear from the abstract that the documents were pre-processed manually. And that the outcome was used for screening purposes. Much of the work was done with punch cards and the system was run on a mainframe.</p>
<p>Edmundson points out some issues that may be relevant to modern summarization tasks:</p>
<ol type="1">
<li>If an extracted sentence is an anaphora, it may not be useful as the reader will be less likely to be understood without its antecedent.</li>
</ol>
<blockquote class="blockquote">
<p>In the composition of maximally coherent and meaningful target extracts, it was noticed that requirements of antecedents, deletions of text due to preediting, minimization of redundancy, and restrictions on the length parameter</p>
</blockquote>
<ol start="2" type="1">
<li><p>Ranking of sentences is of secondary importance to preprocessing and extraction. Also picking the length of the summary can have a big impact on the quality of the summary. The length parameter decided how many of the top ranked sentences are to be included in the summary.</p></li>
<li><p>They used a dictionary for the Corpus and a Document dictionary called a glossary. Cues words were drawn from a a Corpus level dictionary of words indicating relevance.</p></li>
</ol>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
My insights:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>While generative summaries have many advantages over extractive ones they may hallucinate for example if the model has not been trained on the vocabulary of the text.</p></li>
<li><p>It is thus best to ensure that the generation is well grounded in the text.</p></li>
<li><p>On the other hand, extractive summaries have their short comings too.</p></li>
<li><p>We may want to add a sentence level feature to classify sentences as anaphoric or not. Since the dependency may be on a sentence of sentences extraction may fail.</p>
<ul>
<li>Edmundson point out that these anaphoric sentences are often marked by certain words or phrases. We might do better identifying anaphoric sentences by looking at the dependency tree of the sentence.</li>
<li>We can consider co-reference resolution as a pre-processing step.</li>
</ul></li>
<li><p>Cue words are also an interesting feature but a vector space model may yield a better indication of the importance of a sentence. It seems though that the paper did collect information to estimate td/idf scores for each word.</p></li>
</ul>
<p>Another interesting idea is that once we are able to evaluate the words in the document by weights using them to picking the top ranked sentences is probably a bad idea as many may well be redundant, particularly since all the titles and headings are included in the summary. A better approach could be to pick the sentences that are a solution to a <a href="https://en.wikipedia.org/wiki/Knapsack_problem">knapsack problem</a> where we want to pick sentences with the greatest value in unique cue words. This should allow for a more balanced summary.</p>
<p>If we are not using cue words by TD/IDF or a similar information theoretic weighting scheme based on entropy, we may eavluate the knapsack using mutual information between the sentences and the document. If we have a distributional method we could use the KL divergence between the distribution of words and phrases in the document and the distribution of words in the knapsack. This would allow for a more balanced summary.</p>
</div>
</div>
<hr>
</section>
<section id="frump---fast-reading-understanding-and-memory-program" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="frump---fast-reading-understanding-and-memory-program">FRUMP - Fast Reading Understanding and Memory Program</h2>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="s12-FRUMP-demo.png" class="lightbox" data-gallery="slides" title="s12-FRUMP-demo"><img src="https://orenbochman.github.io/notes-nlp/posts/2021-04-24-summerization/s12-FRUMP-demo.png" class="img-fluid figure-img" alt="s12-FRUMP-demo"></a></p>
<figcaption>s12-FRUMP-demo</figcaption>
</figure>
</div></div><ul>
<li>Introduced in <span class="citation" data-cites="DeJong1979PredictionAS">(DeJong 1979)</span></li>
<li>knowledge-based summarization system.</li>
<li>Template filling approach based on UPI news stories.</li>
<li>First abstractive method.</li>
<li>50 sketchy scripts
<ul>
<li>Contain important events that are expected to occur in a specific situation</li>
<li>Summarizer looks for instances of salient events, filling in as many as possible.</li>
</ul></li>
<li>Issues - 50 scripts were not enough.</li>
</ul>
<blockquote class="blockquote">
<p>This paper describes a new approach to natural language processing which results in a very robust and efficient system. The approach taken is to integrate the parser with the rest of the system. This enables the parser to benefit from predictions that the rest of the system makes in the course of its processing. These predictions can be invaluable as guides to the parser in such difficult problem areas as resolving referents and selecting meanings of ambiguous words. A program, called FRUMP for Fast Reading Understanding and Memory Program, employs this approach to parsing. FRUMP skims articles rather than reading them for detail. The program works on the relatively unconstrained domain of news articles. It routinely understands stories it has never before seen. The program’s success is largely due to its radically different approach to parsing.</p>
</blockquote>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
My insights:
</div>
</div>
<div class="callout-body-container callout-body">
<p>This approach has two interesting ideas.</p>
<ul>
<li>KR using templates or frames.</li>
<li>KR using scripts is even more powerful method.</li>
</ul>
<ol type="1">
<li>A modern take on this might involve using a classifier to identify sentences as
<ul>
<li>Facts
<ul>
<li>general knowledge (simple)</li>
<li>domain knowledge (complex or technical)</li>
</ul></li>
<li>Opinions
<ul>
<li>general knowledge (similar to many documents)</li>
<li>domain expert. (similar to a few)</li>
</ul></li>
<li>Events (narrative structure)</li>
<li>Deductive (logic, inference, statistical, syllogism)</li>
<li>Others</li>
</ul></li>
<li>Using a generative approach would allow a deep model to generate its own KR features and templates. An adversarial approach might split this into two nets one to generate and another to test.</li>
<li>Analyzing existing summaries and clustering them might allow one to begin summarize using a preferred template rather than starting from scratch. Clustering, deleting and generalizing from existing summaries may be a means for improving abstractive work.</li>
<li>Putting a focus on the added value of
<ul>
<li>out of document facts and vocabulary</li>
<li>how humans/abstractive summaries differ from extractive ones.</li>
</ul></li>
</ol>
</div>
</div>
<hr>
</section>
<section id="naive-bayes-classification" class="level2">
<h2 class="anchored" data-anchor-id="naive-bayes-classification">Naive Bayes Classification</h2>
<ul>
<li>Introduced in <span class="citation" data-cites="Kupiec1995ATD">(Kupiec, Pedersen, and Chen 1995)</span></li>
<li>First trainable method</li>
<li>Training set: original documents and manually created extracts</li>
<li>Used <strong>Naive Bayes</strong> classifier:</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%20P%20(s%20%5Cin%20S%20%5Cvert%20F_1%20...%20F_k)%20=%20%5Cfrac%7BP%20(F_1%20...%20F_k%20%5Cvert%20s%20%5Cin%20S%20)%20P(s%20%5Cin%20S%20)%7D%20%7BP%20(F_1%20...%20F_k)%7D%20%20"></p>
<ul>
<li>By assuming statistical independence of the features it reduces to:</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%20%20P%20(s%20%5Cin%20S%20%5Cvert%20F_1%20...%20F_k)%20%20=%20%5Cfrac%7B%20%5Cdisplaystyle%20%5Cprod_%7Bj%20%5Cin%20J%7D%20P%20(F_j%20%5Cvert%20s%20%5Cin%20S%20)%20P(s%20%5Cin%20S%20)%7D%20%7B%20%5Cdisplaystyle%20%5Cprod_%7Bj%20%5Cin%20J%7D%20P%20(F_i)%7D%20"></p>
<section id="performance" class="level3">
<h3 class="anchored" data-anchor-id="performance">Performance:</h3>
<ul>
<li>For 25% extracts - 84% precision</li>
<li>For smaller summaries - 74% improvement over <em>lead summaries</em></li>
</ul>
<hr>
</section>
</section>
<section id="maximum-entropy-classification" class="level2">
<h2 class="anchored" data-anchor-id="maximum-entropy-classification">Maximum Entropy Classification</h2>
<ul>
<li>Introduced in <span class="citation" data-cites="Osborne2002UsingME">(Osborne 2002)</span><br>
</li>
<li>Maximum entropy models are performing better than Naive Bayes approach</li>
</ul>
<hr>
</section>
<section id="mmr" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="mmr">MMR</h2>
<ul>
<li>Introduced in <span class="citation" data-cites="Carbonell1998TheUO">(Carbonell and Goldstein-Stewart 1998)</span></li>
<li>Maximal Marginal Relevance</li>
<li>Query based summaries.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BMMR%7D%20=%20%5Carg%20%5Cmax%5B%5Clambda%20Sim_1(s_i,Q)-(1-%5Clambda)%20%5Cmax%20Sim_2(s_i,%20s_j)%5D%0A"></p>
<p>Where:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?Q"> - user query</li>
<li><img src="https://latex.codecogs.com/png.latex?R"> - ranked list of sentences</li>
<li><img src="https://latex.codecogs.com/png.latex?S"> - already retrieved sentences</li>
<li><img src="https://latex.codecogs.com/png.latex?Sim"> - similarity metrics</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Clambda"> - hyper-parameter controlling importance of query or other sentence.</li>
</ul>
<blockquote class="blockquote">
<p>This paper presents a method for combining <em>query-relevance</em> with <em>information-novelty</em> in the context of text retrieval and summarization. <strong>The Maximal Marginal Relevance</strong> (MMR) criterion strives to reduce redundancy while maintaining query relevance in re-ranking retrieved documents and in selecting appropriate passages for text summarization. Preliminary results indicate some benefits for MMR diversity ranking in document retrieval and in single document summarization… the clearest advantage is demonstrated in constructing non-redundant multi-document summaries, where MMR results are clearly superior to non-MMR passage selection. <img src="https://latex.codecogs.com/png.latex?-"> The Use of MMR (abstract)</p>
</blockquote>
<hr>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
My insights
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>MMR seems to have a binomial formulation.</li>
<li>By avoiding to pin down the metric it is possible to use embedding similarity with this formulation.</li>
<li>MMR offers a formal metric for measuring added value (utility) For Sentences in a summary.</li>
<li>It can work with or without a query.</li>
<li>It could be adapted as a regularization term in a summarizer loss function.</li>
<li>It could be used on a summary to weigh each sentence’s utility.</li>
<li>If one were able to generate multiple candidates for a factum MMR could be used to easily rank them.</li>
</ul>
</div>
</div>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="s16-Mead-Centroid.png" class="lightbox" data-gallery="slides" title="s16-Mead-Centroid"><img src="https://orenbochman.github.io/notes-nlp/posts/2021-04-24-summerization/s16-Mead-Centroid.png" class="img-fluid figure-img" alt="s16-Mead-Centroid"></a></p>
<figcaption>s16-Mead-Centroid</figcaption>
</figure>
</div></div></section>
<section id="mead" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="mead">Mead</h2>
<ul>
<li>Introduced in <span class="citation" data-cites="Radev2000CentroidbasedSO">(Radev, Jing, and Budzikowska 2000)</span></li>
<li>Centroid-based method</li>
<li>Single and multi document</li>
</ul>
<blockquote class="blockquote">
<p>We present a multi-document summarizer, called MEAD, which generates summaries using cluster centroids produced by a topic detection and tracking system. We also describe two new techniques, based on sentence utility and subsumption, which we have applied to the evaluation of both single and multiple document summaries. Finally, we describe two user studies that test our models of multi-document summarization. <img src="https://latex.codecogs.com/png.latex?-"> Centroid-based summarization of multiple documents (abstract)</p>
</blockquote>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
My insights:
</div>
</div>
<div class="callout-body-container callout-body">
<p>Clustering has its benefits:</p>
<ul>
<li>Each centroid corresponds a candidate topic.</li>
<li>Cluster size establishes a natural hierarchy for ranking topics.</li>
<li>Cluster centrality provides the a hierarchy for ranking sentence within topics.</li>
<li>The centroids may be used in a generative context, to bootstrap attention to each topic !?</li>
<li>A query similarity can used with the centroids to rank in response to a query (for Q&amp;A)</li>
</ul>
</div>
</div>
<hr>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="s17-lexrank.png" class="lightbox" data-gallery="slides" title="Lexrank"><img src="https://orenbochman.github.io/notes-nlp/posts/2021-04-24-summerization/s17-lexrank.png" class="img-fluid figure-img" alt="Lexrank"></a></p>
<figcaption>Lexrank</figcaption>
</figure>
</div></div></section>
<section id="lexrank" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="lexrank">LexRank</h2>
<ul>
<li>Introduced in <span class="citation" data-cites="Erkan2004LexRankGL">(Erkan and Radev 2004)</span> <sup>1</sup>(https://www.cs.cmu.edu/afs/cs/project/jair/pub/volume22/erkan04a-html/erkan04a.html)</li>
<li>Graph based method.</li>
<li>Lexical centrality.</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;page</p></div></div><hr>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="s18-lexrank-rank.png" class="lightbox" data-gallery="slides" title="lexrank rank"><img src="https://orenbochman.github.io/notes-nlp/posts/2021-04-24-summerization/s18-lexrank-rank.png" class="img-fluid figure-img" alt="lexrank rank"></a></p>
<figcaption>lexrank rank</figcaption>
</figure>
</div></div><blockquote class="blockquote">
<p>We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.</p>
</blockquote>
<hr>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="s19-lexrank-graph.png" class="lightbox" data-gallery="slides" title="lexrank graph"><img src="https://orenbochman.github.io/notes-nlp/posts/2021-04-24-summerization/s19-lexrank-graph.png" class="img-fluid figure-img" alt="lexrank graph"></a></p>
<figcaption>lexrank graph</figcaption>
</figure>
</div></div><p>Idea:</p>
<p>similar to page rank where pages vote for each other:</p>
<ul>
<li>Create an adjacency matrix using cosine similarity.</li>
<li>Representing sentences as nodes in the graph</li>
<li>Connecting nodes based on inter-sentence cosine similarity matrix</li>
<li>uses <a href="https://en.wikipedia.org/wiki/Eigenvector_centrality">eigenvector centrality</a> from this matrix.</li>
<li>the sentence with the highest rank would be linked to many other important sentences. Are they very similar or not ?</li>
<li>a threshold is used to determine how many connected components should are used.</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
My insights
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li>Algorithmically lexrank is a more sophisticated way of clustering like the MEAD algorithm. According to the paper, lexrank performed better.</li>
<li>Graph algorithms are computationally expensive for large graphs. This could mean that the approach would not scale.</li>
<li>To build the matrix they used a cosine similarity - but on using words. Replacing words with their embeddings should yield even better results with lower costs.</li>
<li>There are a number of centrality measures on graphs. A high eigenvector score means that a node is connected to many nodes who themselves have high scores. The paper looked at Degree, LexRank with threshold, and continuous LexRank. This is clearly a place where one may be able to do better.</li>
<li>TfiDf is another way to rank concepts.</li>
<li>a problem is that the underlying assumptions for creating the graphical models are difficult to justify. Building a graph from web pages using links seems natural while constructing a graph using similarity between sentences perhaps in different documents seems contrived. Sentences may capture several concepts and arguments may span several sentences. Similar sentences may have very different meaning and different sentences may have the same meaning.</li>
</ol>
</div>
</div>
<hr>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="s19-seq2seq.png" class="lightbox" data-gallery="slides" title="seq2seq"><img src="https://orenbochman.github.io/notes-nlp/posts/2021-04-24-summerization/s19-seq2seq.png" class="img-fluid figure-img" alt="seq2seq"></a></p>
<figcaption>seq2seq</figcaption>
</figure>
</div></div><hr>
</section>
</section>
<section id="evaluation" class="level1 page-columns page-full">
<h1>Evaluation</h1>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="s22-information-content.png" class="lightbox" data-gallery="slides" title="Information Content"><img src="https://orenbochman.github.io/notes-nlp/posts/2021-04-24-summerization/s22-information-content.png" class="img-fluid figure-img" alt="Information Content"></a></p>
<figcaption>Information Content</figcaption>
</figure>
</div></div><section id="what-makes-a-good-summary" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="what-makes-a-good-summary">What makes a good summary?</h2>
<ul>
<li>Goals:
<ul>
<li>Optimize topic coverage</li>
<li>Optimize readability</li>
</ul></li>
<li>Evaluation criteria:
<ul>
<li>Salience</li>
<li>Length</li>
<li>Structure and coherence</li>
<li>Balance</li>
<li>Grammar</li>
<li>Non-redundancy</li>
</ul></li>
<li>Types of evaluation methods
<ul>
<li>Extrinsic techniques
<ul>
<li>Task based</li>
<li>Can a person make the same decision with summary as with the entire document?</li>
</ul></li>
<li>Intrinsic techniques
<ul>
<li>Comparing summaries against gold standards</li>
</ul></li>
</ul></li>
</ul>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="s24-precision-recall.png" class="lightbox" data-gallery="slides" title="Precision &amp; Recall"><img src="https://orenbochman.github.io/notes-nlp/posts/2021-04-24-summerization/s24-precision-recall.png" class="img-fluid figure-img" alt="Precision &amp; Recall"></a></p>
<figcaption>Precision &amp; Recall</figcaption>
</figure>
</div></div></section>
<section id="precision-and-recall" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="precision-and-recall">Precision and Recall</h2>
<p>starting with a contingency matrix we can get to:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0APrecision%20=%5Cfrac%7BTrue_+%7D%7B%20False_+%20+%20True_+%7D%20%5Cqquad%0A"> {eq-precision}</p>
<p><img src="https://latex.codecogs.com/png.latex?%0ARecall%20=%20%5Cfrac%7BTrue_+%7D%7BTrue_+%20+%20False_-%7D%20%5Cqquad%0A"> {eq-recall}</p>
<p>these can also be combined into an f-score is a harmonic mean of precision and recall.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
My insights
</div>
</div>
<div class="callout-body-container callout-body">
<p>Precision and Recall make more sense for IR settings, i.e.&nbsp;when we have a query.</p>
</div>
</div>
<hr>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="s24-utiliity.png" class="lightbox" data-gallery="slides" title="s24-utility"><img src="https://orenbochman.github.io/notes-nlp/posts/2021-04-24-summerization/s24-utiliity.png" class="img-fluid figure-img" alt="s24-utility"></a></p>
<figcaption>s24-utility</figcaption>
</figure>
</div></div></section>
<section id="utility" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="utility">Utility</h2>
<ul>
<li>Utility is interesting from economic or game theoretic perspective. It indicates an option of applying RL</li>
<li>Utility is usually translated as a <strong>loss function</strong> in ML!</li>
</ul>
<hr>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="s25-pyramid.png" class="lightbox" data-gallery="slides" title="s25-pyramid"><img src="https://orenbochman.github.io/notes-nlp/posts/2021-04-24-summerization/s25-pyramid.png" class="img-fluid figure-img" alt="s25-pyramid"></a></p>
<figcaption>s25-pyramid</figcaption>
</figure>
</div></div></section>
<section id="pyramid-method" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="pyramid-method">Pyramid method</h2>
<ul>
<li>Based on semantic content units</li>
<li>Used for multi-document summarization</li>
</ul>
<hr>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="s25-rougue.png" class="lightbox" data-gallery="slides" title="s25-rougue"><img src="https://orenbochman.github.io/notes-nlp/posts/2021-04-24-summerization/s25-rougue.png" class="img-fluid figure-img" alt="s25-rougue"></a></p>
<figcaption>s25-rougue</figcaption>
</figure>
</div></div></section>
<section id="rouge-n" class="level2">
<h2 class="anchored" data-anchor-id="rouge-n">ROUGE-N</h2>
<ul>
<li>Based on Bleu (used for MT)</li>
<li>R stands for Recall (Recall-Oriented Understudy for Gisting Evaluation)</li>
<li>ROUGE-N metric compares an automatic summary with a set of reference summaries using the n-gram overlap between the documents</li>
</ul>
<p><span id="eq-rouge"><img src="https://latex.codecogs.com/png.latex?%0AROUGE_N%20-%20=%20%5Cfrac%7B%5Csum_%7Bs%5Cin%20S_H%7D%20%5Csum_%7Bg_n%20%5Cin%20S%7DC_m(g_n)%7D%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%7B%5Csum_%7Bs%5Cin%20S_H%7D%20%5Csum_%7Bg_n%20%5Cin%20S%7DC(g_n)%20%7D%20%5Cqquad%0A%5Ctag%7B1%7D"></span></p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?S_H"> is a set of manual summaries</li>
<li><img src="https://latex.codecogs.com/png.latex?S"> is an individual manual summary</li>
<li><img src="https://latex.codecogs.com/png.latex?g_n"> is a N-gram</li>
<li><img src="https://latex.codecogs.com/png.latex?C(g_n)"> is number of occurrences of gn in reference summaries</li>
<li><img src="https://latex.codecogs.com/png.latex?C_m(g_n)"> is number of co-occurrences of g_n in both reference and automatic summary</li>
</ul>
</section>
</section>
<section id="tools" class="level1">
<h1>Tools</h1>
<ul>
<li>the <a href="https://github.com/miso-belica/sumy">sumy</a> python library</li>
<li><a href="https://www.nltk.org/book/ch01.html">nltk summeriser</a></li>
<li><a href="https://spacy.io/universe/project/spacy-pytextrank/">spacy summeriser</a></li>
</ul>
</section>
<section id="data-sets" class="level1">
<h1>Data Sets</h1>
<ul>
<li><a href="https://catalog.ldc.upenn.edu/LDC2012T21">GIGAWORD dataset</a></li>
<li><a href="https://github.com/abisee/cnn-dailymail">CNN Daily</a></li>
<li><a href="http://kavita-ganesan.com/opinosis-opinion-dataset/#.XIgkSihKg2w">Opinions dataset</a></li>
</ul>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-Baxendale1958Machine" class="csl-entry">
Baxendale, P. B. 1958. <span>“Machine-Made Index for Technical Literature—an Experiment.”</span> <em>IBM Journal of Research and Development</em> 2 (4): 354–61. <a href="https://doi.org/10.1147/rd.24.0354">https://doi.org/10.1147/rd.24.0354</a>.
</div>
<div id="ref-Carbonell1998TheUO" class="csl-entry">
Carbonell, Jaime G., and Jade Goldstein-Stewart. 1998. <span>“The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries.”</span> In <em>Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</em>. <a href="https://www.cs.cmu.edu/~jgc/publication/The_Use_MMR_Diversity_Based_LTMIR_1998.pdf">https://www.cs.cmu.edu/~jgc/publication/The_Use_MMR_Diversity_Based_LTMIR_1998.pdf</a>.
</div>
<div id="ref-DeJong1979PredictionAS" class="csl-entry">
DeJong, Gerald. 1979. <span>“Prediction and Substantiation: A New Approach to Natural Language Processing.”</span> <em>Cogn. Sci.</em> 3: 251–73. <a href="https://api.semanticscholar.org/CorpusID:28841837">https://api.semanticscholar.org/CorpusID:28841837</a>.
</div>
<div id="ref-edmundson-1969" class="csl-entry">
Edmundson, H. P. 1969. <span>“New Methods in Automatic Extracting.”</span> <em>Journal of theACM</em> 16 (2): 264–85. <a href="https://courses.ischool.berkeley.edu/i256/f06/papers/edmonson69.pdf">https://courses.ischool.berkeley.edu/i256/f06/papers/edmonson69.pdf</a>.
</div>
<div id="ref-Erkan2004LexRankGL" class="csl-entry">
Erkan, Günes, and Dragomir R. Radev. 2004. <span>“LexRank: Graph-Based Lexical Centrality as Salience in Text Summarization.”</span> <em>ArXiv</em> abs/1109.2128. <a href="https://api.semanticscholar.org/CorpusID:506350">https://api.semanticscholar.org/CorpusID:506350</a>.
</div>
<div id="ref-Kupiec1995ATD" class="csl-entry">
Kupiec, Julian, Jan O. Pedersen, and Francine R. Chen. 1995. <span>“A Trainable Document Summarizer.”</span> In <em>Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</em>. <a href="https://courses.ischool.berkeley.edu/i256/f06/papers/kupiec95.pdf">https://courses.ischool.berkeley.edu/i256/f06/papers/kupiec95.pdf</a>.
</div>
<div id="ref-luhn-58" class="csl-entry">
Luhn, H. P. 1958. <span>“The Automatic Creation of Literature Abstracts.”</span> <em>IBM Journal of Research and Development</em> 2 (2): 159–65. <a href="https://doi.org/10.1147/rd.22.0159">https://doi.org/10.1147/rd.22.0159</a>.
</div>
<div id="ref-Osborne2002UsingME" class="csl-entry">
Osborne, Miles. 2002. <span>“Using Maximum Entropy for Sentence Extraction.”</span> In <em>Annual Meeting of the Association for Computational Linguistics</em>. <a href="https://aclanthology.org/W02-0401.pdf">https://aclanthology.org/W02-0401.pdf</a>.
</div>
<div id="ref-Radev2000CentroidbasedSO" class="csl-entry">
Radev, Dragomir R., Hongyan Jing, and Malgorzata Budzikowska. 2000. <span>“Centroid-Based Summarization of Multiple Documents: Sentence Extraction, Utility-Based Evaluation, and User Studies.”</span> <em>ArXiv</em> cs.CL/0005020. <a href="https://arxiv.org/pdf/cs/0005020.pdf">https://arxiv.org/pdf/cs/0005020.pdf</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2021,
  author = {Bochman, Oren},
  title = {Automatic {Summarization} {Task}},
  date = {2021-04-24},
  url = {https://orenbochman.github.io/notes-nlp/posts/2021-04-24-summerization/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2021" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2021. <span>“Automatic Summarization Task.”</span> April
24, 2021. <a href="https://orenbochman.github.io/notes-nlp/posts/2021-04-24-summerization/">https://orenbochman.github.io/notes-nlp/posts/2021-04-24-summerization/</a>.
</div></div></section></div> ]]></description>
  <category>NLP</category>
  <category>Notes</category>
  <category>Literature review</category>
  <category>Summarization task</category>
  <category>Relevance</category>
  <category>Conference talk</category>
  <guid>https://orenbochman.github.io/notes-nlp/posts/2021-04-24-summerization/</guid>
  <pubDate>Fri, 23 Apr 2021 21:00:00 GMT</pubDate>
</item>
<item>
  <title>Assignment 3 Ungraded Sections - Part 2: T5 SQuAD Model</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/c4w3/lab03.html</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/Course-Logo-4-1.webp" class="nolightbox img-fluid figure-img"></p>
<figcaption>course banner</figcaption>
</figure>
</div></div><p>Welcome to the part 2 of testing the models for this week’s assignment. This time we will perform decoding using the T5 SQuAD model. In this notebook we’ll perform Question Answering by providing a “Question”, its “Context” and see how well we get the “Target” answer.</p>
<section id="colab" class="level2">
<h2 class="anchored" data-anchor-id="colab">Colab</h2>
<p>Since this ungraded lab takes a lot of time to run on coursera, as an alternative we have a colab prepared for you.</p>
<p><a href="https://drive.google.com/file/d/1c-8KJkTySRGqCx_JjwjvXuRBTNTqEE0N/view?usp=sharing">T5 SQuAD Model Colab</a></p>
<ul>
<li>If you run into a page that looks similar to the one below, with the option <code>Open with</code>, this would mean you need to download the <code>Colaboratory</code> app. You can do so by <code>Open with -&gt; Connect more apps -&gt; in the search bar write "Colaboratory" -&gt; install</code></li>
</ul>
<p><a href="img/colab_help_1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img src="https://orenbochman.github.io/notes-nlp/notes/c4w3/img/colab_help_1.png" class="img-fluid"></a></p>
<ul>
<li>After installation it should look like this. Click on <code>Open with Google Colaboratory</code></li>
</ul>
<p><a href="img/colab_help_2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img src="https://orenbochman.github.io/notes-nlp/notes/c4w3/img/colab_help_2.png" class="img-fluid"></a></p>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li>Overview</li>
<li>Part 1: Resuming the assignment (T5 SQuAD Model)</li>
<li>Part 2: Fine-tuning on SQuAD
<ul>
<li>2.1 Loading in the data and preprocessing</li>
<li>2.2 Decoding from a fine-tuned model</li>
</ul></li>
</ul>
<section id="0" class="level3">
<h3 class="anchored" data-anchor-id="0">Overview</h3>
<p>In this notebook you will:</p>
<ul>
<li>Implement the Bidirectional Encoder Representation from Transformer (BERT) loss.</li>
<li>Use a pretrained version of the model you created in the assignment for inference.</li>
</ul>
</section>
</section>
<section id="1" class="level2">
<h2 class="anchored" data-anchor-id="1">Part 1: Getting ready</h2>
<p>Run the code cells below to import the necessary libraries and to define some functions which will be useful for decoding. The code and the functions are the same as the ones you previsouly ran on the graded assignment.</p>
<div id="aa368efc" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> string</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> t5</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> trax </span>
<span id="cb1-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> trax.supervised <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> decoding</span>
<span id="cb1-6"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> textwrap </span>
<span id="cb1-7"></span>
<span id="cb1-8">wrapper <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> textwrap.TextWrapper(width<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">70</span>)</span></code></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">ImportError</span>                               Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[1], line 2</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">string</span>
<span class="ansi-green-fg">----&gt; 2</span> <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">t5</span>
<span class="ansi-green-fg ansi-bold">      3</span> <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">numpy</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">as</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">np</span>
<span class="ansi-green-fg ansi-bold">      4</span> <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">trax</span> 

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/t5/__init__.py:17</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># Copyright 2023 The T5 Authors.</span>
<span class="ansi-green-fg ansi-bold">      2</span> <span style="font-style:italic;color:rgb(95,135,135)">#</span>
<span class="ansi-green-fg ansi-bold">      3</span> <span style="font-style:italic;color:rgb(95,135,135)"># Licensed under the Apache License, Version 2.0 (the "License");</span>
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-fg ansi-bold">     12</span> <span style="font-style:italic;color:rgb(95,135,135)"># See the License for the specific language governing permissions and</span>
<span class="ansi-green-fg ansi-bold">     13</span> <span style="font-style:italic;color:rgb(95,135,135)"># limitations under the License.</span>
<span class="ansi-green-fg ansi-bold">     15</span> <span style="font-style:italic;color:rgb(175,0,0)">"""Import API modules."""</span>
<span class="ansi-green-fg">---&gt; 17</span> <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">t5</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">data</span>
<span class="ansi-green-fg ansi-bold">     18</span> <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">t5</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">evaluation</span>
<span class="ansi-green-fg ansi-bold">     20</span> <span style="font-style:italic;color:rgb(95,135,135)"># Version number.</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/t5/data/__init__.py:17</span>
<span class="ansi-green-fg ansi-bold">     15</span> <span style="font-style:italic;color:rgb(175,0,0)">"""Import data modules."""</span>
<span class="ansi-green-fg ansi-bold">     16</span> <span style="font-style:italic;color:rgb(95,135,135)"># pylint:disable=wildcard-import,g-bad-import-order</span>
<span class="ansi-green-fg">---&gt; 17</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">t5</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">data</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">dataset_providers</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> <span style="color:rgb(98,98,98)">*</span>
<span class="ansi-green-fg ansi-bold">     18</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">t5</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">data</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">glue_utils</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> <span style="color:rgb(98,98,98)">*</span>
<span class="ansi-green-fg ansi-bold">     19</span> <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">t5</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">data</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">postprocessors</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/t5/data/dataset_providers.py:28</span>
<span class="ansi-green-fg ansi-bold">     25</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">collections</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">abc</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> Mapping
<span class="ansi-green-fg ansi-bold">     26</span> <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">re</span>
<span class="ansi-green-fg">---&gt; 28</span> <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">seqio</span>
<span class="ansi-green-fg ansi-bold">     29</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">t5</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">data</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> utils
<span class="ansi-green-fg ansi-bold">     30</span> <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">tensorflow</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">compat</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">v2</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">as</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">tf</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/seqio/__init__.py:19</span>
<span class="ansi-green-fg ansi-bold">     15</span> <span style="font-style:italic;color:rgb(175,0,0)">"""Import to top-level API."""</span>
<span class="ansi-green-fg ansi-bold">     17</span> <span style="font-style:italic;color:rgb(95,135,135)"># pylint:disable=wildcard-import,g-bad-import-order,g-import-not-at-top</span>
<span class="ansi-green-fg">---&gt; 19</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">seqio</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">dataset_providers</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> <span style="color:rgb(98,98,98)">*</span>
<span class="ansi-green-fg ansi-bold">     20</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">seqio</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> evaluation
<span class="ansi-green-fg ansi-bold">     21</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">seqio</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> experimental

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/seqio/dataset_providers.py:36</span>
<span class="ansi-green-fg ansi-bold">     33</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">typing</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> Any, Callable, Iterable, List, Mapping, MutableMapping, Optional, Sequence, Set, Tuple, Type, Union
<span class="ansi-green-fg ansi-bold">     35</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">absl</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> logging
<span class="ansi-green-fg">---&gt; 36</span> <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">clu</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">metrics</span>
<span class="ansi-green-fg ansi-bold">     37</span> <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">editdistance</span>
<span class="ansi-green-fg ansi-bold">     38</span> <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">numpy</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">as</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">np</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/clu/metrics.py:66</span>
<span class="ansi-green-fg ansi-bold">     64</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">clu</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">internal</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> utils
<span class="ansi-green-fg ansi-bold">     65</span> <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">clu</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">values</span>
<span class="ansi-green-fg">---&gt; 66</span> <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">flax</span>
<span class="ansi-green-fg ansi-bold">     67</span> <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">jax</span>
<span class="ansi-green-fg ansi-bold">     68</span> <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">jax</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">numpy</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">as</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">jnp</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/flax/__init__.py:19</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># Copyright 2022 The Flax Authors.</span>
<span class="ansi-green-fg ansi-bold">      2</span> <span style="font-style:italic;color:rgb(95,135,135)">#</span>
<span class="ansi-green-fg ansi-bold">      3</span> <span style="font-style:italic;color:rgb(95,135,135)"># Licensed under the Apache License, Version 2.0 (the "License");</span>
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-fg ansi-bold">     14</span> 
<span class="ansi-green-fg ansi-bold">     15</span> <span style="font-style:italic;color:rgb(95,135,135)"># Lint as: python 3</span>
<span class="ansi-green-fg ansi-bold">     17</span> <span style="font-style:italic;color:rgb(175,0,0)">"""Flax API."""</span>
<span class="ansi-green-fg">---&gt; 19</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> core
<span class="ansi-green-fg ansi-bold">     20</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> linen
<span class="ansi-green-fg ansi-bold">     21</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> optim

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/flax/core/__init__.py:15</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># Copyright 2022 The Flax Authors.</span>
<span class="ansi-green-fg ansi-bold">      2</span> <span style="font-style:italic;color:rgb(95,135,135)">#</span>
<span class="ansi-green-fg ansi-bold">      3</span> <span style="font-style:italic;color:rgb(95,135,135)"># Licensed under the Apache License, Version 2.0 (the "License");</span>
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-fg ansi-bold">     12</span> <span style="font-style:italic;color:rgb(95,135,135)"># See the License for the specific language governing permissions and</span>
<span class="ansi-green-fg ansi-bold">     13</span> <span style="font-style:italic;color:rgb(95,135,135)"># limitations under the License.</span>
<span class="ansi-green-fg">---&gt; 15</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">axes_scan</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> broadcast
<span class="ansi-green-fg ansi-bold">     16</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">frozen_dict</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> FrozenDict, freeze, unfreeze
<span class="ansi-green-fg ansi-bold">     17</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">tracers</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> current_trace, trace_level, check_trace_level

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/flax/core/axes_scan.py:22</span>
<span class="ansi-green-fg ansi-bold">     19</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">jax</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> lax
<span class="ansi-green-fg ansi-bold">     21</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">jax</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">interpreters</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> partial_eval <span style="font-weight:bold;color:rgb(0,135,0)">as</span> pe
<span class="ansi-green-fg">---&gt; 22</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">jax</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> linear_util <span style="font-weight:bold;color:rgb(0,135,0)">as</span> lu
<span class="ansi-green-fg ansi-bold">     24</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">typing</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> Union, Optional, Callable, Any
<span class="ansi-green-fg ansi-bold">     26</span> <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">numpy</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">as</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">np</span>

<span class="ansi-red-fg">ImportError</span>: cannot import name 'linear_util' from 'jax' (/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/jax/__init__.py)</pre>
</div>
</div>
</div>
<div id="4df10504" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">PAD, EOS, UNK <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb2-2"></span>
<span id="cb2-3"></span>
<span id="cb2-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> detokenize(np_array):</span>
<span id="cb2-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> trax.data.detokenize(</span>
<span id="cb2-6">        np_array,</span>
<span id="cb2-7">        vocab_type<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sentencepiece'</span>,</span>
<span id="cb2-8">        vocab_file<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sentencepiece.model'</span>,</span>
<span id="cb2-9">        vocab_dir<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'.'</span>)</span>
<span id="cb2-10"></span>
<span id="cb2-11"></span>
<span id="cb2-12"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> tokenize(s):</span>
<span id="cb2-13">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">next</span>(trax.data.tokenize(</span>
<span id="cb2-14">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">iter</span>([s]),</span>
<span id="cb2-15">        vocab_type<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sentencepiece'</span>,</span>
<span id="cb2-16">        vocab_file<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sentencepiece.model'</span>,</span>
<span id="cb2-17">        vocab_dir<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'.'</span>))</span>
<span id="cb2-18"> </span>
<span id="cb2-19">    </span>
<span id="cb2-20">vocab_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> trax.data.vocab_size(</span>
<span id="cb2-21">    vocab_type<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sentencepiece'</span>,</span>
<span id="cb2-22">    vocab_file<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sentencepiece.model'</span>,</span>
<span id="cb2-23">    vocab_dir<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'.'</span>)</span>
<span id="cb2-24"></span>
<span id="cb2-25"></span>
<span id="cb2-26"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_sentinels(vocab_size, display<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>):</span>
<span id="cb2-27">    sentinels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {}</span>
<span id="cb2-28">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, char <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">reversed</span>(string.ascii_letters), <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>):</span>
<span id="cb2-29">        decoded_text <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> detokenize([vocab_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> i]) </span>
<span id="cb2-30">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Sentinels, ex: &lt;Z&gt; - &lt;a&gt;</span></span>
<span id="cb2-31">        sentinels[decoded_text] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'&lt;</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>char<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">&gt;'</span>    </span>
<span id="cb2-32">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> display:</span>
<span id="cb2-33">            <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'The sentinel is &lt;</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>char<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">&gt; and the decoded token is:'</span>, decoded_text)</span>
<span id="cb2-34">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> sentinels</span>
<span id="cb2-35"></span>
<span id="cb2-36"></span>
<span id="cb2-37">sentinels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_sentinels(vocab_size, display<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)    </span>
<span id="cb2-38"></span>
<span id="cb2-39"></span>
<span id="cb2-40"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> pretty_decode(encoded_str_list, sentinels<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>sentinels):</span>
<span id="cb2-41">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># If already a string, just do the replacements.</span></span>
<span id="cb2-42">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">isinstance</span>(encoded_str_list, (<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">bytes</span>)):</span>
<span id="cb2-43">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> token, char <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> sentinels.items():</span>
<span id="cb2-44">            encoded_str_list <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> encoded_str_list.replace(token, char)</span>
<span id="cb2-45">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> encoded_str_list</span>
<span id="cb2-46">  </span>
<span id="cb2-47">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># We need to decode and then prettyfy it.</span></span>
<span id="cb2-48">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> pretty_decode(detokenize(encoded_str_list))</span></code></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[2], line 20</span>
<span class="ansi-green-fg ansi-bold">     12</span> <span style="font-weight:bold;color:rgb(0,135,0)">def</span><span style="color:rgb(188,188,188)"> </span><span style="color:rgb(0,0,255)">tokenize</span>(s):
<span class="ansi-green-fg ansi-bold">     13</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(0,135,0)">next</span>(trax<span style="color:rgb(98,98,98)">.</span>data<span style="color:rgb(98,98,98)">.</span>tokenize(
<span class="ansi-green-fg ansi-bold">     14</span>         <span style="color:rgb(0,135,0)">iter</span>([s]),
<span class="ansi-green-fg ansi-bold">     15</span>         vocab_type<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">sentencepiece</span><span style="color:rgb(175,0,0)">'</span>,
<span class="ansi-green-fg ansi-bold">     16</span>         vocab_file<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">sentencepiece.model</span><span style="color:rgb(175,0,0)">'</span>,
<span class="ansi-green-fg ansi-bold">     17</span>         vocab_dir<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">.</span><span style="color:rgb(175,0,0)">'</span>))
<span class="ansi-green-fg">---&gt; 20</span> vocab_size <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">trax</span><span style="color:rgb(98,98,98)">.</span>data<span style="color:rgb(98,98,98)">.</span>vocab_size(
<span class="ansi-green-fg ansi-bold">     21</span>     vocab_type<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">sentencepiece</span><span style="color:rgb(175,0,0)">'</span>,
<span class="ansi-green-fg ansi-bold">     22</span>     vocab_file<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">sentencepiece.model</span><span style="color:rgb(175,0,0)">'</span>,
<span class="ansi-green-fg ansi-bold">     23</span>     vocab_dir<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">.</span><span style="color:rgb(175,0,0)">'</span>)
<span class="ansi-green-fg ansi-bold">     26</span> <span style="font-weight:bold;color:rgb(0,135,0)">def</span><span style="color:rgb(188,188,188)"> </span><span style="color:rgb(0,0,255)">get_sentinels</span>(vocab_size, display<span style="color:rgb(98,98,98)">=</span><span style="font-weight:bold;color:rgb(0,135,0)">False</span>):
<span class="ansi-green-fg ansi-bold">     27</span>     sentinels <span style="color:rgb(98,98,98)">=</span> {}

<span class="ansi-red-fg">NameError</span>: name 'trax' is not defined</pre>
</div>
</div>
</div>
</section>
<section id="2" class="level2">
<h2 class="anchored" data-anchor-id="2">Part 2: Fine-tuning on SQuAD</h2>
<p>Now let’s try to fine tune on SQuAD and see what becomes of the model.For this, we need to write a function that will create and process the SQuAD <code>tf.data.Dataset</code>. Below is how T5 pre-processes SQuAD dataset as a text2text example. Before we jump in, we will have to first load in the data.</p>
<section id="2.1" class="level3">
<h3 class="anchored" data-anchor-id="2.1">2.1 Loading in the data and preprocessing</h3>
<p>You first start by loading in the dataset. The text2text example for a SQuAD example looks like:</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode json code-with-copy"><code class="sourceCode json"><span id="cb3-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb3-2">  <span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">'inputs'</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">'question:</span> <span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">&lt;question&gt;</span> <span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">context:</span> <span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">&lt;article&gt;'</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb3-3">  <span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">'targets'</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">'&lt;answer_</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span><span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">&gt;'</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb3-4"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">}</span></span></code></pre></div>
<p>The squad pre-processing function takes in the dataset and processes it using the sentencePiece vocabulary you have seen above. It generates the features from the vocab and encodes the string features. It takes on question, context, and answer, and returns “question: Q context: C” as input and “A” as target.</p>
<div id="5804babb" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Retrieve Question, C, A and return "question: Q context: C" as input and "A" as target.</span></span>
<span id="cb4-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> squad_preprocess_fn(dataset, mode<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'train'</span>):</span>
<span id="cb4-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> t5.data.preprocessors.squad(dataset)</span></code></pre></div>
</div>
<div id="83286088" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># train generator, this takes about 1 minute</span></span>
<span id="cb5-2">train_generator_fn, eval_generator_fn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> trax.data.tf_inputs.data_streams(</span>
<span id="cb5-3">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'squad/plain_text:1.0.0'</span>,</span>
<span id="cb5-4">  data_dir<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'data/'</span>,</span>
<span id="cb5-5">  bare_preprocess_fn<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>squad_preprocess_fn,</span>
<span id="cb5-6">  input_name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'inputs'</span>,</span>
<span id="cb5-7">  target_name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'targets'</span></span>
<span id="cb5-8">)</span>
<span id="cb5-9"></span>
<span id="cb5-10">train_generator <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_generator_fn()</span>
<span id="cb5-11"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">next</span>(train_generator)</span></code></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[4], line 2</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># train generator, this takes about 1 minute</span>
<span class="ansi-green-fg">----&gt; 2</span> train_generator_fn, eval_generator_fn <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">trax</span><span style="color:rgb(98,98,98)">.</span>data<span style="color:rgb(98,98,98)">.</span>tf_inputs<span style="color:rgb(98,98,98)">.</span>data_streams(
<span class="ansi-green-fg ansi-bold">      3</span>   <span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">squad/plain_text:1.0.0</span><span style="color:rgb(175,0,0)">'</span>,
<span class="ansi-green-fg ansi-bold">      4</span>   data_dir<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">data/</span><span style="color:rgb(175,0,0)">'</span>,
<span class="ansi-green-fg ansi-bold">      5</span>   bare_preprocess_fn<span style="color:rgb(98,98,98)">=</span>squad_preprocess_fn,
<span class="ansi-green-fg ansi-bold">      6</span>   input_name<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">inputs</span><span style="color:rgb(175,0,0)">'</span>,
<span class="ansi-green-fg ansi-bold">      7</span>   target_name<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">targets</span><span style="color:rgb(175,0,0)">'</span>
<span class="ansi-green-fg ansi-bold">      8</span> )
<span class="ansi-green-fg ansi-bold">     10</span> train_generator <span style="color:rgb(98,98,98)">=</span> train_generator_fn()
<span class="ansi-green-fg ansi-bold">     11</span> <span style="color:rgb(0,135,0)">next</span>(train_generator)

<span class="ansi-red-fg">NameError</span>: name 'trax' is not defined</pre>
</div>
</div>
</div>
<div id="f900869e" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#print example from train_generator</span></span>
<span id="cb6-2">(inp, out) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">next</span>(train_generator)</span>
<span id="cb6-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(inp.decode(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'utf8'</span>).split(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'context:'</span>)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb6-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>()</span>
<span id="cb6-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'context:'</span>, inp.decode(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'utf8'</span>).split(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'context:'</span>)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb6-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>()</span>
<span id="cb6-7"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target:'</span>, out.decode(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'utf8'</span>))</span></code></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[5], line 2</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)">#print example from train_generator</span>
<span class="ansi-green-fg">----&gt; 2</span> (inp, out) <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)">next</span>(<span class="ansi-yellow-bg">train_generator</span>)
<span class="ansi-green-fg ansi-bold">      3</span> <span style="color:rgb(0,135,0)">print</span>(inp<span style="color:rgb(98,98,98)">.</span>decode(<span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">utf8</span><span style="color:rgb(175,0,0)">'</span>)<span style="color:rgb(98,98,98)">.</span>split(<span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">context:</span><span style="color:rgb(175,0,0)">'</span>)[<span style="color:rgb(98,98,98)">0</span>])
<span class="ansi-green-fg ansi-bold">      4</span> <span style="color:rgb(0,135,0)">print</span>()

<span class="ansi-red-fg">NameError</span>: name 'train_generator' is not defined</pre>
</div>
</div>
</div>
</section>
<section id="2.2" class="level3">
<h3 class="anchored" data-anchor-id="2.2">2.2 Decoding from a fine-tuned model</h3>
<p>You will now use an existing model that we trained for you. You will initialize, then load in your model, and then try with your own input.</p>
<div id="748c8889" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Initialize the model </span></span>
<span id="cb7-2">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> trax.models.Transformer(</span>
<span id="cb7-3">    d_ff <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4096</span>,</span>
<span id="cb7-4">    d_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1024</span>,</span>
<span id="cb7-5">    max_len <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2048</span>,</span>
<span id="cb7-6">    n_heads <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span>,</span>
<span id="cb7-7">    dropout <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>,</span>
<span id="cb7-8">    input_vocab_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32000</span>,</span>
<span id="cb7-9">    n_encoder_layers <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">24</span>,</span>
<span id="cb7-10">    n_decoder_layers <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">24</span>,</span>
<span id="cb7-11">    mode<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'predict'</span>)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Change to 'eval' for slow decoding.</span></span></code></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[6], line 2</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># Initialize the model </span>
<span class="ansi-green-fg">----&gt; 2</span> model <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">trax</span><span style="color:rgb(98,98,98)">.</span>models<span style="color:rgb(98,98,98)">.</span>Transformer(
<span class="ansi-green-fg ansi-bold">      3</span>     d_ff <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(98,98,98)">4096</span>,
<span class="ansi-green-fg ansi-bold">      4</span>     d_model <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(98,98,98)">1024</span>,
<span class="ansi-green-fg ansi-bold">      5</span>     max_len <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(98,98,98)">2048</span>,
<span class="ansi-green-fg ansi-bold">      6</span>     n_heads <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(98,98,98)">16</span>,
<span class="ansi-green-fg ansi-bold">      7</span>     dropout <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(98,98,98)">0.1</span>,
<span class="ansi-green-fg ansi-bold">      8</span>     input_vocab_size <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(98,98,98)">32000</span>,
<span class="ansi-green-fg ansi-bold">      9</span>     n_encoder_layers <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(98,98,98)">24</span>,
<span class="ansi-green-fg ansi-bold">     10</span>     n_decoder_layers <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(98,98,98)">24</span>,
<span class="ansi-green-fg ansi-bold">     11</span>     mode<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">predict</span><span style="color:rgb(175,0,0)">'</span>)  <span style="font-style:italic;color:rgb(95,135,135)"># Change to 'eval' for slow decoding.</span>

<span class="ansi-red-fg">NameError</span>: name 'trax' is not defined</pre>
</div>
</div>
</div>
<div id="2fa06781" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># load in the model</span></span>
<span id="cb8-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># this will take a minute</span></span>
<span id="cb8-3">shape11 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> trax.shapes.ShapeDtype((<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>), dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.int32)</span>
<span id="cb8-4">model.init_from_file(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'model_squad.pkl.gz'</span>,</span>
<span id="cb8-5">                     weights_only<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, input_signature<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(shape11, shape11))</span></code></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[7], line 3</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># load in the model</span>
<span class="ansi-green-fg ansi-bold">      2</span> <span style="font-style:italic;color:rgb(95,135,135)"># this will take a minute</span>
<span class="ansi-green-fg">----&gt; 3</span> shape11 <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">trax</span><span style="color:rgb(98,98,98)">.</span>shapes<span style="color:rgb(98,98,98)">.</span>ShapeDtype((<span style="color:rgb(98,98,98)">1</span>, <span style="color:rgb(98,98,98)">1</span>), dtype<span style="color:rgb(98,98,98)">=</span>np<span style="color:rgb(98,98,98)">.</span>int32)
<span class="ansi-green-fg ansi-bold">      4</span> model<span style="color:rgb(98,98,98)">.</span>init_from_file(<span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">model_squad.pkl.gz</span><span style="color:rgb(175,0,0)">'</span>,
<span class="ansi-green-fg ansi-bold">      5</span>                      weights_only<span style="color:rgb(98,98,98)">=</span><span style="font-weight:bold;color:rgb(0,135,0)">True</span>, input_signature<span style="color:rgb(98,98,98)">=</span>(shape11, shape11))

<span class="ansi-red-fg">NameError</span>: name 'trax' is not defined</pre>
</div>
</div>
</div>
<div id="5b23cd0e" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># create inputs</span></span>
<span id="cb9-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># a simple example </span></span>
<span id="cb9-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># inputs = 'question: She asked him where is john? context: John was at the game'</span></span>
<span id="cb9-4"></span>
<span id="cb9-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># an extensive example</span></span>
<span id="cb9-6">inputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'question: What are some of the colours of a rose? context: A rose is a woody perennial flowering plant of the genus Rosa, in the family Rosaceae, or the flower it bears.There are over three hundred species and tens of thousands of cultivars. They form a group of plants that can be erect shrubs, climbing, or trailing, with stems that are often armed with sharp prickles. Flowers vary in size and shape and are usually large and showy, in colours ranging from white through yellows and reds. Most species are native to Asia, with smaller numbers native to Europe, North America, and northwestern Africa. Species, cultivars and hybrids are all widely grown for their beauty and often are fragrant.'</span></span></code></pre></div>
</div>
<div id="b9859905" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># tokenizing the input so we could feed it for decoding</span></span>
<span id="cb10-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(tokenize(inputs))</span>
<span id="cb10-3">test_inputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tokenize(inputs) </span></code></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[9], line 2</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># tokenizing the input so we could feed it for decoding</span>
<span class="ansi-green-fg">----&gt; 2</span> <span style="color:rgb(0,135,0)">print</span>(<span class="ansi-yellow-bg">tokenize</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">inputs</span><span class="ansi-yellow-bg">)</span>)
<span class="ansi-green-fg ansi-bold">      3</span> test_inputs <span style="color:rgb(98,98,98)">=</span> tokenize(inputs) 

Cell <span class="ansi-green-fg">In[2], line 13</span>, in <span class="ansi-cyan-fg">tokenize</span><span class="ansi-blue-fg">(s)</span>
<span class="ansi-green-fg ansi-bold">     12</span> <span style="font-weight:bold;color:rgb(0,135,0)">def</span><span style="color:rgb(188,188,188)"> </span><span style="color:rgb(0,0,255)">tokenize</span>(s):
<span class="ansi-green-fg">---&gt; 13</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(0,135,0)">next</span>(<span class="ansi-yellow-bg">trax</span><span style="color:rgb(98,98,98)">.</span>data<span style="color:rgb(98,98,98)">.</span>tokenize(
<span class="ansi-green-fg ansi-bold">     14</span>         <span style="color:rgb(0,135,0)">iter</span>([s]),
<span class="ansi-green-fg ansi-bold">     15</span>         vocab_type<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">sentencepiece</span><span style="color:rgb(175,0,0)">'</span>,
<span class="ansi-green-fg ansi-bold">     16</span>         vocab_file<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">sentencepiece.model</span><span style="color:rgb(175,0,0)">'</span>,
<span class="ansi-green-fg ansi-bold">     17</span>         vocab_dir<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">.</span><span style="color:rgb(175,0,0)">'</span>))

<span class="ansi-red-fg">NameError</span>: name 'trax' is not defined</pre>
</div>
</div>
</div>
<p>Run the cell below to decode.</p>
</section>
<section id="note-this-will-take-some-time-to-run" class="level3">
<h3 class="anchored" data-anchor-id="note-this-will-take-some-time-to-run">Note: This will take some time to run</h3>
<div id="81122138" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Temperature is a parameter for sampling.</span></span>
<span id="cb11-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#   # * 0.0: same as argmax, always pick the most probable token</span></span>
<span id="cb11-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#   # * 1.0: sampling from the distribution (can sometimes say random things)</span></span>
<span id="cb11-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#   # * values inbetween can trade off diversity and quality, try it out!</span></span>
<span id="cb11-5">output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> decoding.autoregressive_sample(model, inputs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.array(test_inputs)[<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, :],</span>
<span id="cb11-6">                                        temperature<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span>, max_length<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># originally max_length=10</span></span>
<span id="cb11-7"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(wrapper.fill(pretty_decode(output[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])))</span></code></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[10], line 5</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># Temperature is a parameter for sampling.</span>
<span class="ansi-green-fg ansi-bold">      2</span> <span style="font-style:italic;color:rgb(95,135,135)">#   # * 0.0: same as argmax, always pick the most probable token</span>
<span class="ansi-green-fg ansi-bold">      3</span> <span style="font-style:italic;color:rgb(95,135,135)">#   # * 1.0: sampling from the distribution (can sometimes say random things)</span>
<span class="ansi-green-fg ansi-bold">      4</span> <span style="font-style:italic;color:rgb(95,135,135)">#   # * values inbetween can trade off diversity and quality, try it out!</span>
<span class="ansi-green-fg">----&gt; 5</span> output <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">decoding</span><span style="color:rgb(98,98,98)">.</span>autoregressive_sample(model, inputs<span style="color:rgb(98,98,98)">=</span>np<span style="color:rgb(98,98,98)">.</span>array(test_inputs)[<span style="font-weight:bold;color:rgb(0,135,0)">None</span>, :],
<span class="ansi-green-fg ansi-bold">      6</span>                                         temperature<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">0.0</span>, max_length<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">5</span>) <span style="font-style:italic;color:rgb(95,135,135)"># originally max_length=10</span>
<span class="ansi-green-fg ansi-bold">      7</span> <span style="color:rgb(0,135,0)">print</span>(wrapper<span style="color:rgb(98,98,98)">.</span>fill(pretty_decode(output[<span style="color:rgb(98,98,98)">0</span>])))

<span class="ansi-red-fg">NameError</span>: name 'decoding' is not defined</pre>
</div>
</div>
</div>
<p>You should also be aware that the quality of the decoding is not very good because max_length was downsized from 10 to 5 so that this runs faster within this environment. The colab version uses the original max_length so check that one for the actual decoding.</p>


</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2021,
  author = {Bochman, Oren},
  title = {Assignment 3 {Ungraded} {Sections} - {Part} 2: {T5} {SQuAD}
    {Model}},
  date = {2021-04-13},
  url = {https://orenbochman.github.io/notes-nlp/notes/c4w3/lab03.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2021" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2021. <span>“Assignment 3 Ungraded Sections - Part 2: T5
SQuAD Model.”</span> April 13, 2021. <a href="https://orenbochman.github.io/notes-nlp/notes/c4w3/lab03.html">https://orenbochman.github.io/notes-nlp/notes/c4w3/lab03.html</a>.
</div></div></section></div> ]]></description>
  <category>NLP</category>
  <category>Coursera</category>
  <category>Lab</category>
  <category>NLP with Attention Models</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/c4w3/lab03.html</guid>
  <pubDate>Mon, 12 Apr 2021 21:00:00 GMT</pubDate>
</item>
<item>
  <title>Assignment 3 Ungraded Sections - Part 1: BERT Loss Model</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/c4w3/lab02.html</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/Course-Logo-4-1.webp" class="nolightbox img-fluid figure-img"></p>
<figcaption>course banner</figcaption>
</figure>
</div></div><p>Welcome to the part 1 of testing the models for this week’s assignment. We will perform decoding using the BERT Loss model. In this notebook we’ll use an input, mask (hide) random word(s) in it and see how well we get the “Target” answer(s).</p>
<section id="colab" class="level2">
<h2 class="anchored" data-anchor-id="colab">Colab</h2>
<p>Since this ungraded lab takes a lot of time to run on coursera, as an alternative we have a colab prepared for you.</p>
<p><a href="https://drive.google.com/file/d/1EHAbMnW6u-GqYWh5r3Z8uLbz4KNpKOAv/view?usp=sharing">BERT Loss Model Colab</a></p>
<ul>
<li>If you run into a page that looks similar to the one below, with the option <code>Open with</code>, this would mean you need to download the <code>Colaboratory</code> app. You can do so by <code>Open with -&gt; Connect more apps -&gt; in the search bar write "Colaboratory" -&gt; install</code></li>
</ul>
<p><a href="img/colab_help_1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img src="https://orenbochman.github.io/notes-nlp/notes/c4w3/img/colab_help_1.png" class="img-fluid"></a></p>
<ul>
<li>After installation it should look like this. Click on <code>Open with Google Colaboratory</code></li>
</ul>
<p><a href="img/colab_help_2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img src="https://orenbochman.github.io/notes-nlp/notes/c4w3/img/colab_help_2.png" class="img-fluid"></a></p>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li>Overview</li>
<li>Part 1: Getting ready</li>
<li>Part 2: BERT Loss
<ul>
<li>2.1 Decoding</li>
</ul></li>
</ul>
<section id="0" class="level3">
<h3 class="anchored" data-anchor-id="0">Overview</h3>
<p>In this notebook you will:</p>
<ul>
<li>Implement the Bidirectional Encoder Representation from Transformer (BERT) loss.</li>
<li>Use a pretrained version of the model you created in the assignment for inference.</li>
</ul>
</section>
</section>
<section id="1" class="level2">
<h2 class="anchored" data-anchor-id="1">Part 1: Getting ready</h2>
<p>Run the code cells below to import the necessary libraries and to define some functions which will be useful for decoding. The code and the functions are the same as the ones you previsouly ran on the graded assignment.</p>
<div id="9a65d6e8" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pickle</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> string</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> ast</span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> trax </span>
<span id="cb1-6"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> trax.supervised <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> decoding</span>
<span id="cb1-7"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> textwrap </span>
<span id="cb1-8"></span>
<span id="cb1-9">wrapper <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> textwrap.TextWrapper(width<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">70</span>)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>2025-02-10 16:51:29.192037: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1739199089.204440  119292 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1739199089.208671  119292 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered</code></pre>
</div>
</div>
<div id="1d47369e" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">example_jsons <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">map</span>(ast.literal_eval, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">open</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'data.txt'</span>)))</span>
<span id="cb3-2"></span>
<span id="cb3-3">natural_language_texts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [example_json[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'text'</span>] <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> example_json <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> example_jsons]</span>
<span id="cb3-4"></span>
<span id="cb3-5">PAD, EOS, UNK <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb3-6"></span>
<span id="cb3-7"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> detokenize(np_array):</span>
<span id="cb3-8">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> trax.data.detokenize(</span>
<span id="cb3-9">        np_array,</span>
<span id="cb3-10">        vocab_type<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sentencepiece'</span>,</span>
<span id="cb3-11">        vocab_file<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sentencepiece.model'</span>,</span>
<span id="cb3-12">        vocab_dir<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'.'</span>)</span>
<span id="cb3-13"></span>
<span id="cb3-14"></span>
<span id="cb3-15"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> tokenize(s):</span>
<span id="cb3-16">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">next</span>(trax.data.tokenize(</span>
<span id="cb3-17">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">iter</span>([s]),</span>
<span id="cb3-18">        vocab_type<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sentencepiece'</span>,</span>
<span id="cb3-19">        vocab_file<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sentencepiece.model'</span>,</span>
<span id="cb3-20">        vocab_dir<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'.'</span>))</span>
<span id="cb3-21"> </span>
<span id="cb3-22">    </span>
<span id="cb3-23">vocab_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> trax.data.vocab_size(</span>
<span id="cb3-24">    vocab_type<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sentencepiece'</span>,</span>
<span id="cb3-25">    vocab_file<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sentencepiece.model'</span>,</span>
<span id="cb3-26">    vocab_dir<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'.'</span>)</span>
<span id="cb3-27"></span>
<span id="cb3-28"></span>
<span id="cb3-29"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_sentinels(vocab_size, display<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>):</span>
<span id="cb3-30">    sentinels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {}</span>
<span id="cb3-31">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, char <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">reversed</span>(string.ascii_letters), <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>):</span>
<span id="cb3-32">        decoded_text <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> detokenize([vocab_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> i]) </span>
<span id="cb3-33">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Sentinels, ex: &lt;Z&gt; - &lt;a&gt;</span></span>
<span id="cb3-34">        sentinels[decoded_text] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'&lt;</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>char<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">&gt;'</span>    </span>
<span id="cb3-35">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> display:</span>
<span id="cb3-36">            <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'The sentinel is &lt;</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>char<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">&gt; and the decoded token is:'</span>, decoded_text)</span>
<span id="cb3-37">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> sentinels</span>
<span id="cb3-38"></span>
<span id="cb3-39"></span>
<span id="cb3-40">sentinels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_sentinels(vocab_size, display<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)    </span>
<span id="cb3-41"></span>
<span id="cb3-42"></span>
<span id="cb3-43"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> pretty_decode(encoded_str_list, sentinels<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>sentinels):</span>
<span id="cb3-44">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># If already a string, just do the replacements.</span></span>
<span id="cb3-45">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">isinstance</span>(encoded_str_list, (<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">bytes</span>)):</span>
<span id="cb3-46">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> token, char <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> sentinels.items():</span>
<span id="cb3-47">            encoded_str_list <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> encoded_str_list.replace(token, char)</span>
<span id="cb3-48">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> encoded_str_list</span>
<span id="cb3-49">  </span>
<span id="cb3-50">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># We need to decode and then prettyfy it.</span></span>
<span id="cb3-51">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> pretty_decode(detokenize(encoded_str_list))</span>
<span id="cb3-52"></span>
<span id="cb3-53"></span>
<span id="cb3-54">inputs_targets_pairs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb3-55"></span>
<span id="cb3-56"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># here you are reading already computed input/target pairs from a file</span></span>
<span id="cb3-57"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">open</span> (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'inputs_targets_pairs_file.txt'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rb'</span>) <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> fp:</span>
<span id="cb3-58">    inputs_targets_pairs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pickle.load(fp)  </span>
<span id="cb3-59"></span>
<span id="cb3-60"></span>
<span id="cb3-61"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> display_input_target_pairs(inputs_targets_pairs):</span>
<span id="cb3-62">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, inp_tgt_pair <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(inputs_targets_pairs, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>):</span>
<span id="cb3-63">        inps, tgts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> inp_tgt_pair</span>
<span id="cb3-64">        inps, tgts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pretty_decode(inps), pretty_decode(tgts)</span>
<span id="cb3-65">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'[</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">]</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span></span>
<span id="cb3-66">              <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'inputs:</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>wrapper<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>fill(text<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>inps)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n\n</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span></span>
<span id="cb3-67">              <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'targets:</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>wrapper<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>fill(text<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>tgts)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n\n\n\n</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb3-68">    </span>
<span id="cb3-69">display_input_target_pairs(inputs_targets_pairs)    </span></code></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">ImportError</span>                               Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[2], line 23</span>
<span class="ansi-green-fg ansi-bold">     15</span> <span style="font-weight:bold;color:rgb(0,135,0)">def</span><span style="color:rgb(188,188,188)"> </span><span style="color:rgb(0,0,255)">tokenize</span>(s):
<span class="ansi-green-fg ansi-bold">     16</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(0,135,0)">next</span>(trax<span style="color:rgb(98,98,98)">.</span>data<span style="color:rgb(98,98,98)">.</span>tokenize(
<span class="ansi-green-fg ansi-bold">     17</span>         <span style="color:rgb(0,135,0)">iter</span>([s]),
<span class="ansi-green-fg ansi-bold">     18</span>         vocab_type<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">sentencepiece</span><span style="color:rgb(175,0,0)">'</span>,
<span class="ansi-green-fg ansi-bold">     19</span>         vocab_file<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">sentencepiece.model</span><span style="color:rgb(175,0,0)">'</span>,
<span class="ansi-green-fg ansi-bold">     20</span>         vocab_dir<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">.</span><span style="color:rgb(175,0,0)">'</span>))
<span class="ansi-green-fg">---&gt; 23</span> vocab_size <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">trax</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">data</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">vocab_size</span><span class="ansi-yellow-bg">(</span>
<span class="ansi-green-fg ansi-bold">     24</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">vocab_type</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">'</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">sentencepiece</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">'</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">     25</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">vocab_file</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">'</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">sentencepiece.model</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">'</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">     26</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">vocab_dir</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">'</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">.</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">'</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">     29</span> <span style="font-weight:bold;color:rgb(0,135,0)">def</span><span style="color:rgb(188,188,188)"> </span><span style="color:rgb(0,0,255)">get_sentinels</span>(vocab_size, display<span style="color:rgb(98,98,98)">=</span><span style="font-weight:bold;color:rgb(0,135,0)">False</span>):
<span class="ansi-green-fg ansi-bold">     30</span>     sentinels <span style="color:rgb(98,98,98)">=</span> {}

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/data/tf_inputs.py:570</span>, in <span class="ansi-cyan-fg">vocab_size</span><span class="ansi-blue-fg">(vocab_type, vocab_file, vocab_dir, n_reserved_ids)</span>
<span class="ansi-green-fg ansi-bold">    550</span> <span style="font-weight:bold;color:rgb(0,135,0)">def</span><span style="color:rgb(188,188,188)"> </span><span style="color:rgb(0,0,255)">vocab_size</span>(vocab_type<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">subword</span><span style="color:rgb(175,0,0)">'</span>,
<span class="ansi-green-fg ansi-bold">    551</span>                vocab_file<span style="color:rgb(98,98,98)">=</span><span style="font-weight:bold;color:rgb(0,135,0)">None</span>,
<span class="ansi-green-fg ansi-bold">    552</span>                vocab_dir<span style="color:rgb(98,98,98)">=</span><span style="font-weight:bold;color:rgb(0,135,0)">None</span>,
<span class="ansi-green-fg ansi-bold">    553</span>                n_reserved_ids<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">0</span>):
<span class="ansi-green-fg ansi-bold">    554</span> <span style="color:rgb(188,188,188)">  </span><span style="font-style:italic;color:rgb(175,0,0)">"""Returns the size of the vocabulary (number of symbols used).</span>
<span class="ansi-green-fg ansi-bold">    555</span> 
<span class="ansi-green-fg ansi-bold">    556</span> <span style="font-style:italic;color:rgb(175,0,0)">  This function can be used to set the size of the final layers of a model that</span>
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-fg ansi-bold">    568</span> <span style="font-style:italic;color:rgb(175,0,0)">    An integer, the number of symbols used (including reserved IDs).</span>
<span class="ansi-green-fg ansi-bold">    569</span> <span style="font-style:italic;color:rgb(175,0,0)">  """</span>
<span class="ansi-green-fg">--&gt; 570</span>   vocab <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">_get_vocab</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">vocab_type</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">vocab_file</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">vocab_dir</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">    571</span>   <span style="font-weight:bold;color:rgb(0,135,0)">return</span> vocab<span style="color:rgb(98,98,98)">.</span>vocab_size <span style="color:rgb(98,98,98)">+</span> n_reserved_ids

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/data/tf_inputs.py:603</span>, in <span class="ansi-cyan-fg">_get_vocab</span><span class="ansi-blue-fg">(vocab_type, vocab_file, vocab_dir, extra_ids)</span>
<span class="ansi-green-fg ansi-bold">    600</span>   <span style="font-weight:bold;color:rgb(0,135,0)">return</span> text_encoder<span style="color:rgb(98,98,98)">.</span>BertEncoder(path, do_lower_case<span style="color:rgb(98,98,98)">=</span><span style="font-weight:bold;color:rgb(0,135,0)">True</span>)
<span class="ansi-green-fg ansi-bold">    602</span> <span style="font-weight:bold;color:rgb(0,135,0)">assert</span> vocab_type <span style="color:rgb(98,98,98)">==</span> <span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">sentencepiece</span><span style="color:rgb(175,0,0)">'</span>
<span class="ansi-green-fg">--&gt; 603</span> <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span class="ansi-yellow-bg">t5_data</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">)</span><span style="color:rgb(98,98,98)">.</span>SentencePieceVocabulary(sentencepiece_model_file<span style="color:rgb(98,98,98)">=</span>path,
<span class="ansi-green-fg ansi-bold">    604</span>                                          extra_ids<span style="color:rgb(98,98,98)">=</span>extra_ids)

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/data/tf_inputs.py:53</span>, in <span class="ansi-cyan-fg">t5_data</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg ansi-bold">     51</span> module <span style="color:rgb(98,98,98)">=</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span>
<span class="ansi-green-fg ansi-bold">     52</span> <span style="font-weight:bold;color:rgb(0,135,0)">try</span>:
<span class="ansi-green-fg">---&gt; 53</span>   <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">t5</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">data</span>  <span style="font-style:italic;color:rgb(95,135,135)"># pylint: disable=g-import-not-at-top</span>
<span class="ansi-green-fg ansi-bold">     54</span>   module <span style="color:rgb(98,98,98)">=</span> t5<span style="color:rgb(98,98,98)">.</span>data
<span class="ansi-green-fg ansi-bold">     55</span> <span style="font-weight:bold;color:rgb(0,135,0)">except</span> <span style="font-weight:bold;color:rgb(215,95,95)">AttributeError</span> <span style="font-weight:bold;color:rgb(0,135,0)">as</span> e:

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/t5/__init__.py:17</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># Copyright 2023 The T5 Authors.</span>
<span class="ansi-green-fg ansi-bold">      2</span> <span style="font-style:italic;color:rgb(95,135,135)">#</span>
<span class="ansi-green-fg ansi-bold">      3</span> <span style="font-style:italic;color:rgb(95,135,135)"># Licensed under the Apache License, Version 2.0 (the "License");</span>
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-fg ansi-bold">     12</span> <span style="font-style:italic;color:rgb(95,135,135)"># See the License for the specific language governing permissions and</span>
<span class="ansi-green-fg ansi-bold">     13</span> <span style="font-style:italic;color:rgb(95,135,135)"># limitations under the License.</span>
<span class="ansi-green-fg ansi-bold">     15</span> <span style="font-style:italic;color:rgb(175,0,0)">"""Import API modules."""</span>
<span class="ansi-green-fg">---&gt; 17</span> <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">t5</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">data</span>
<span class="ansi-green-fg ansi-bold">     18</span> <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">t5</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">evaluation</span>
<span class="ansi-green-fg ansi-bold">     20</span> <span style="font-style:italic;color:rgb(95,135,135)"># Version number.</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/t5/data/__init__.py:17</span>
<span class="ansi-green-fg ansi-bold">     15</span> <span style="font-style:italic;color:rgb(175,0,0)">"""Import data modules."""</span>
<span class="ansi-green-fg ansi-bold">     16</span> <span style="font-style:italic;color:rgb(95,135,135)"># pylint:disable=wildcard-import,g-bad-import-order</span>
<span class="ansi-green-fg">---&gt; 17</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">t5</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">data</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">dataset_providers</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> <span style="color:rgb(98,98,98)">*</span>
<span class="ansi-green-fg ansi-bold">     18</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">t5</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">data</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">glue_utils</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> <span style="color:rgb(98,98,98)">*</span>
<span class="ansi-green-fg ansi-bold">     19</span> <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">t5</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">data</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">postprocessors</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/t5/data/dataset_providers.py:28</span>
<span class="ansi-green-fg ansi-bold">     25</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">collections</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">abc</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> Mapping
<span class="ansi-green-fg ansi-bold">     26</span> <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">re</span>
<span class="ansi-green-fg">---&gt; 28</span> <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">seqio</span>
<span class="ansi-green-fg ansi-bold">     29</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">t5</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">data</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> utils
<span class="ansi-green-fg ansi-bold">     30</span> <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">tensorflow</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">compat</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">v2</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">as</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">tf</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/seqio/__init__.py:19</span>
<span class="ansi-green-fg ansi-bold">     15</span> <span style="font-style:italic;color:rgb(175,0,0)">"""Import to top-level API."""</span>
<span class="ansi-green-fg ansi-bold">     17</span> <span style="font-style:italic;color:rgb(95,135,135)"># pylint:disable=wildcard-import,g-bad-import-order,g-import-not-at-top</span>
<span class="ansi-green-fg">---&gt; 19</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">seqio</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">dataset_providers</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> <span style="color:rgb(98,98,98)">*</span>
<span class="ansi-green-fg ansi-bold">     20</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">seqio</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> evaluation
<span class="ansi-green-fg ansi-bold">     21</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">seqio</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> experimental

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/seqio/dataset_providers.py:36</span>
<span class="ansi-green-fg ansi-bold">     33</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">typing</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> Any, Callable, Iterable, List, Mapping, MutableMapping, Optional, Sequence, Set, Tuple, Type, Union
<span class="ansi-green-fg ansi-bold">     35</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">absl</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> logging
<span class="ansi-green-fg">---&gt; 36</span> <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">clu</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">metrics</span>
<span class="ansi-green-fg ansi-bold">     37</span> <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">editdistance</span>
<span class="ansi-green-fg ansi-bold">     38</span> <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">numpy</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">as</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">np</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/clu/metrics.py:66</span>
<span class="ansi-green-fg ansi-bold">     64</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">clu</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">internal</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> utils
<span class="ansi-green-fg ansi-bold">     65</span> <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">clu</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">values</span>
<span class="ansi-green-fg">---&gt; 66</span> <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">flax</span>
<span class="ansi-green-fg ansi-bold">     67</span> <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">jax</span>
<span class="ansi-green-fg ansi-bold">     68</span> <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">jax</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">numpy</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">as</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">jnp</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/flax/__init__.py:19</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># Copyright 2022 The Flax Authors.</span>
<span class="ansi-green-fg ansi-bold">      2</span> <span style="font-style:italic;color:rgb(95,135,135)">#</span>
<span class="ansi-green-fg ansi-bold">      3</span> <span style="font-style:italic;color:rgb(95,135,135)"># Licensed under the Apache License, Version 2.0 (the "License");</span>
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-fg ansi-bold">     14</span> 
<span class="ansi-green-fg ansi-bold">     15</span> <span style="font-style:italic;color:rgb(95,135,135)"># Lint as: python 3</span>
<span class="ansi-green-fg ansi-bold">     17</span> <span style="font-style:italic;color:rgb(175,0,0)">"""Flax API."""</span>
<span class="ansi-green-fg">---&gt; 19</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> core
<span class="ansi-green-fg ansi-bold">     20</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> linen
<span class="ansi-green-fg ansi-bold">     21</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> optim

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/flax/core/__init__.py:15</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># Copyright 2022 The Flax Authors.</span>
<span class="ansi-green-fg ansi-bold">      2</span> <span style="font-style:italic;color:rgb(95,135,135)">#</span>
<span class="ansi-green-fg ansi-bold">      3</span> <span style="font-style:italic;color:rgb(95,135,135)"># Licensed under the Apache License, Version 2.0 (the "License");</span>
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-fg ansi-bold">     12</span> <span style="font-style:italic;color:rgb(95,135,135)"># See the License for the specific language governing permissions and</span>
<span class="ansi-green-fg ansi-bold">     13</span> <span style="font-style:italic;color:rgb(95,135,135)"># limitations under the License.</span>
<span class="ansi-green-fg">---&gt; 15</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">axes_scan</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> broadcast
<span class="ansi-green-fg ansi-bold">     16</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">frozen_dict</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> FrozenDict, freeze, unfreeze
<span class="ansi-green-fg ansi-bold">     17</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">tracers</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> current_trace, trace_level, check_trace_level

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/flax/core/axes_scan.py:22</span>
<span class="ansi-green-fg ansi-bold">     19</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">jax</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> lax
<span class="ansi-green-fg ansi-bold">     21</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">jax</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">interpreters</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> partial_eval <span style="font-weight:bold;color:rgb(0,135,0)">as</span> pe
<span class="ansi-green-fg">---&gt; 22</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">jax</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> linear_util <span style="font-weight:bold;color:rgb(0,135,0)">as</span> lu
<span class="ansi-green-fg ansi-bold">     24</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">typing</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> Union, Optional, Callable, Any
<span class="ansi-green-fg ansi-bold">     26</span> <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">numpy</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">as</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">np</span>

<span class="ansi-red-fg">ImportError</span>: cannot import name 'linear_util' from 'jax' (/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/jax/__init__.py)</pre>
</div>
</div>
</div>
<p><a name="2"></a></p>
</section>
<section id="part-2-bert-loss" class="level2">
<h2 class="anchored" data-anchor-id="part-2-bert-loss">Part 2: BERT Loss</h2>
<p>Now that you created the encoder, we will not make you train it. Training it could easily cost you a few days depending on which GPUs/TPUs you are using. Very few people train the full transformer from scratch. Instead, what the majority of people do, they load in a pretrained model, and they fine tune it on a specific task. That is exactly what you are about to do. Let’s start by initializing and then loading in the model.</p>
<p>Initialize the model from the saved checkpoint.</p>
<div id="d59c4fd9" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Initializing the model</span></span>
<span id="cb4-2">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> trax.models.Transformer(</span>
<span id="cb4-3">    d_ff <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4096</span>,</span>
<span id="cb4-4">    d_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1024</span>,</span>
<span id="cb4-5">    max_len <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2048</span>,</span>
<span id="cb4-6">    n_heads <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span>,</span>
<span id="cb4-7">    dropout <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>,</span>
<span id="cb4-8">    input_vocab_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32000</span>,</span>
<span id="cb4-9">    n_encoder_layers <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">24</span>,</span>
<span id="cb4-10">    n_decoder_layers <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">24</span>,</span>
<span id="cb4-11">    mode<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'predict'</span>)</span></code></pre></div>
</div>
<div id="0865bde0" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Now load in the model</span></span>
<span id="cb5-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># this takes about 1 minute</span></span>
<span id="cb5-3">shape11 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> trax.shapes.ShapeDtype((<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>), dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.int32)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Needed in predict mode.</span></span>
<span id="cb5-4">model.init_from_file(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'model.pkl.gz'</span>,</span>
<span id="cb5-5">                     weights_only<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, input_signature<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(shape11, shape11))</span></code></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NotFoundError</span>                             Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[4], line 4</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># Now load in the model</span>
<span class="ansi-green-fg ansi-bold">      2</span> <span style="font-style:italic;color:rgb(95,135,135)"># this takes about 1 minute</span>
<span class="ansi-green-fg ansi-bold">      3</span> shape11 <span style="color:rgb(98,98,98)">=</span> trax<span style="color:rgb(98,98,98)">.</span>shapes<span style="color:rgb(98,98,98)">.</span>ShapeDtype((<span style="color:rgb(98,98,98)">1</span>, <span style="color:rgb(98,98,98)">1</span>), dtype<span style="color:rgb(98,98,98)">=</span>np<span style="color:rgb(98,98,98)">.</span>int32)  <span style="font-style:italic;color:rgb(95,135,135)"># Needed in predict mode.</span>
<span class="ansi-green-fg">----&gt; 4</span> <span class="ansi-yellow-bg">model</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">init_from_file</span><span class="ansi-yellow-bg">(</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">'</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">model.pkl.gz</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">'</span><span class="ansi-yellow-bg">,</span>
<span class="ansi-green-fg ansi-bold">      5</span> <span class="ansi-yellow-bg">                     </span><span class="ansi-yellow-bg">weights_only</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span style="font-weight:bold;color:rgb(0,135,0)" class="ansi-yellow-bg">True</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">input_signature</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">shape11</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">shape11</span><span class="ansi-yellow-bg">)</span><span class="ansi-yellow-bg">)</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/base.py:334</span>, in <span class="ansi-cyan-fg">Layer.init_from_file</span><span class="ansi-blue-fg">(self, file_name, weights_only, input_signature)</span>
<span class="ansi-green-fg ansi-bold">    332</span> <span style="font-weight:bold;color:rgb(0,135,0)">with</span> tf<span style="color:rgb(98,98,98)">.</span>io<span style="color:rgb(98,98,98)">.</span>gfile<span style="color:rgb(98,98,98)">.</span>GFile(file_name, <span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">rb</span><span style="color:rgb(175,0,0)">'</span>) <span style="font-weight:bold;color:rgb(0,135,0)">as</span> f:
<span class="ansi-green-fg ansi-bold">    333</span>   <span style="font-weight:bold;color:rgb(0,135,0)">with</span> gzip<span style="color:rgb(98,98,98)">.</span>GzipFile(fileobj<span style="color:rgb(98,98,98)">=</span>f, compresslevel<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">2</span>) <span style="font-weight:bold;color:rgb(0,135,0)">as</span> gzipf:
<span class="ansi-green-fg">--&gt; 334</span>     dictionary <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">pickle</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">load</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">gzipf</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">    335</span> <span style="font-style:italic;color:rgb(95,135,135)"># In the current checkpoint format, we store weights in a separate</span>
<span class="ansi-green-fg ansi-bold">    336</span> <span style="font-style:italic;color:rgb(95,135,135)"># non-pickled file with the same name but added ".npy".</span>
<span class="ansi-green-fg ansi-bold">    337</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="color:rgb(0,135,0)">isinstance</span>(dictionary[<span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">flat_weights</span><span style="color:rgb(175,0,0)">'</span>], <span style="color:rgb(0,135,0)">int</span>):

File <span class="ansi-green-fg">/usr/lib/python3.10/gzip.py:321</span>, in <span class="ansi-cyan-fg">GzipFile.peek</span><span class="ansi-blue-fg">(self, n)</span>
<span class="ansi-green-fg ansi-bold">    319</span>     <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">errno</span>
<span class="ansi-green-fg ansi-bold">    320</span>     <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> <span style="font-weight:bold;color:rgb(215,95,95)">OSError</span>(errno<span style="color:rgb(98,98,98)">.</span>EBADF, <span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">peek() on write-only GzipFile object</span><span style="color:rgb(175,0,0)">"</span>)
<span class="ansi-green-fg">--&gt; 321</span> <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">_buffer</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">peek</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">n</span><span class="ansi-yellow-bg">)</span>

File <span class="ansi-green-fg">/usr/lib/python3.10/_compression.py:68</span>, in <span class="ansi-cyan-fg">DecompressReader.readinto</span><span class="ansi-blue-fg">(self, b)</span>
<span class="ansi-green-fg ansi-bold">     66</span> <span style="font-weight:bold;color:rgb(0,135,0)">def</span><span style="color:rgb(188,188,188)"> </span><span style="color:rgb(0,0,255)">readinto</span>(<span style="color:rgb(0,135,0)">self</span>, b):
<span class="ansi-green-fg ansi-bold">     67</span>     <span style="font-weight:bold;color:rgb(0,135,0)">with</span> <span style="color:rgb(0,135,0)">memoryview</span>(b) <span style="font-weight:bold;color:rgb(0,135,0)">as</span> view, view<span style="color:rgb(98,98,98)">.</span>cast(<span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">B</span><span style="color:rgb(175,0,0)">"</span>) <span style="font-weight:bold;color:rgb(0,135,0)">as</span> byte_view:
<span class="ansi-green-fg">---&gt; 68</span>         data <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">read</span><span class="ansi-yellow-bg">(</span><span style="color:rgb(0,135,0)" class="ansi-yellow-bg">len</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">byte_view</span><span class="ansi-yellow-bg">)</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">     69</span>         byte_view[:<span style="color:rgb(0,135,0)">len</span>(data)] <span style="color:rgb(98,98,98)">=</span> data
<span class="ansi-green-fg ansi-bold">     70</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(0,135,0)">len</span>(data)

File <span class="ansi-green-fg">/usr/lib/python3.10/gzip.py:488</span>, in <span class="ansi-cyan-fg">_GzipReader.read</span><span class="ansi-blue-fg">(self, size)</span>
<span class="ansi-green-fg ansi-bold">    484</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_new_member:
<span class="ansi-green-fg ansi-bold">    485</span>     <span style="font-style:italic;color:rgb(95,135,135)"># If the _new_member flag is set, we have to</span>
<span class="ansi-green-fg ansi-bold">    486</span>     <span style="font-style:italic;color:rgb(95,135,135)"># jump to the next member, if there is one.</span>
<span class="ansi-green-fg ansi-bold">    487</span>     <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_init_read()
<span class="ansi-green-fg">--&gt; 488</span>     <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="font-weight:bold;color:rgb(175,0,255)">not</span> <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">_read_gzip_header</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">)</span>:
<span class="ansi-green-fg ansi-bold">    489</span>         <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_size <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_pos
<span class="ansi-green-fg ansi-bold">    490</span>         <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(175,0,0)">b</span><span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">"</span>

File <span class="ansi-green-fg">/usr/lib/python3.10/gzip.py:431</span>, in <span class="ansi-cyan-fg">_GzipReader._read_gzip_header</span><span class="ansi-blue-fg">(self)</span>
<span class="ansi-green-fg ansi-bold">    430</span> <span style="font-weight:bold;color:rgb(0,135,0)">def</span><span style="color:rgb(188,188,188)"> </span><span style="color:rgb(0,0,255)">_read_gzip_header</span>(<span style="color:rgb(0,135,0)">self</span>):
<span class="ansi-green-fg">--&gt; 431</span>     magic <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">_fp</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">read</span><span class="ansi-yellow-bg">(</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">2</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">    432</span>     <span style="font-weight:bold;color:rgb(0,135,0)">if</span> magic <span style="color:rgb(98,98,98)">==</span> <span style="color:rgb(175,0,0)">b</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">'</span>:
<span class="ansi-green-fg ansi-bold">    433</span>         <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="font-weight:bold;color:rgb(0,135,0)">False</span>

File <span class="ansi-green-fg">/usr/lib/python3.10/gzip.py:97</span>, in <span class="ansi-cyan-fg">_PaddedFile.read</span><span class="ansi-blue-fg">(self, size)</span>
<span class="ansi-green-fg ansi-bold">     94</span> read <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_read
<span class="ansi-green-fg ansi-bold">     95</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_read <span style="color:rgb(98,98,98)">=</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span>
<span class="ansi-green-fg ansi-bold">     96</span> <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_buffer[read:] <span style="color:rgb(98,98,98)">+</span> \
<span class="ansi-green-fg">---&gt; 97</span>        <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">file</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">read</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">size</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">-</span><span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">_length</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">+</span><span class="ansi-yellow-bg">read</span><span class="ansi-yellow-bg">)</span>

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tensorflow/python/lib/io/file_io.py:116</span>, in <span class="ansi-cyan-fg">FileIO.read</span><span class="ansi-blue-fg">(self, n)</span>
<span class="ansi-green-fg ansi-bold">    104</span> <span style="font-weight:bold;color:rgb(0,135,0)">def</span><span style="color:rgb(188,188,188)"> </span><span style="color:rgb(0,0,255)">read</span>(<span style="color:rgb(0,135,0)">self</span>, n<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">-</span><span style="color:rgb(98,98,98)">1</span>):
<span class="ansi-green-fg ansi-bold">    105</span> <span style="color:rgb(188,188,188)">  </span><span style="font-style:italic;color:rgb(175,0,0)">"""Returns the contents of a file as a string.</span>
<span class="ansi-green-fg ansi-bold">    106</span> 
<span class="ansi-green-fg ansi-bold">    107</span> <span style="font-style:italic;color:rgb(175,0,0)">  Starts reading from current position in file.</span>
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-fg ansi-bold">    114</span> <span style="font-style:italic;color:rgb(175,0,0)">    string if in string (regular) mode.</span>
<span class="ansi-green-fg ansi-bold">    115</span> <span style="font-style:italic;color:rgb(175,0,0)">  """</span>
<span class="ansi-green-fg">--&gt; 116</span>   <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">_preread_check</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">    117</span>   <span style="font-weight:bold;color:rgb(0,135,0)">if</span> n <span style="color:rgb(98,98,98)">==</span> <span style="color:rgb(98,98,98)">-</span><span style="color:rgb(98,98,98)">1</span>:
<span class="ansi-green-fg ansi-bold">    118</span>     length <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>size() <span style="color:rgb(98,98,98)">-</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>tell()

File <span class="ansi-green-fg">~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tensorflow/python/lib/io/file_io.py:77</span>, in <span class="ansi-cyan-fg">FileIO._preread_check</span><span class="ansi-blue-fg">(self)</span>
<span class="ansi-green-fg ansi-bold">     74</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="font-weight:bold;color:rgb(175,0,255)">not</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_read_check_passed:
<span class="ansi-green-fg ansi-bold">     75</span>   <span style="font-weight:bold;color:rgb(0,135,0)">raise</span> errors<span style="color:rgb(98,98,98)">.</span>PermissionDeniedError(<span style="font-weight:bold;color:rgb(0,135,0)">None</span>, <span style="font-weight:bold;color:rgb(0,135,0)">None</span>,
<span class="ansi-green-fg ansi-bold">     76</span>                                      <span style="color:rgb(175,0,0)">"</span><span style="color:rgb(175,0,0)">File isn</span><span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">t open for reading</span><span style="color:rgb(175,0,0)">"</span>)
<span class="ansi-green-fg">---&gt; 77</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_read_buf <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">_pywrap_file_io</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">BufferedInputStream</span><span class="ansi-yellow-bg">(</span>
<span class="ansi-green-fg ansi-bold">     78</span> <span class="ansi-yellow-bg">    </span><span class="ansi-yellow-bg">compat</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">path_to_str</span><span class="ansi-yellow-bg">(</span><span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">__name</span><span class="ansi-yellow-bg">)</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">1024</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">512</span><span class="ansi-yellow-bg">)</span>

<span class="ansi-red-fg">NotFoundError</span>: model.pkl.gz; No such file or directory</pre>
</div>
</div>
</div>
<p><a name="2.1"></a></p>
<section id="decoding" class="level3">
<h3 class="anchored" data-anchor-id="decoding">2.1 Decoding</h3>
<p>Now you will use one of the <code>inputs_targets_pairs</code> for input and as target. Next you will use the <code>pretty_decode</code> to output the input and target. The code to perform all of this has been provided below.</p>
<div id="604016c4" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># using the 3rd example</span></span>
<span id="cb6-2">c4_input <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> inputs_targets_pairs[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb6-3">c4_target <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> inputs_targets_pairs[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb6-4"></span>
<span id="cb6-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pretty_decoded input: </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>, pretty_decode(c4_input))</span>
<span id="cb6-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">pretty_decoded target: </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>, pretty_decode(c4_target))</span>
<span id="cb6-7"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">c4_input:</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>, c4_input)</span>
<span id="cb6-8"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">c4_target:</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>, c4_target)</span>
<span id="cb6-9"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(c4_target))</span>
<span id="cb6-10"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(pretty_decode(c4_target)))</span></code></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[5], line 2</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># using the 3rd example</span>
<span class="ansi-green-fg">----&gt; 2</span> c4_input <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">inputs_targets_pairs</span>[<span style="color:rgb(98,98,98)">2</span>][<span style="color:rgb(98,98,98)">0</span>]
<span class="ansi-green-fg ansi-bold">      3</span> c4_target <span style="color:rgb(98,98,98)">=</span> inputs_targets_pairs[<span style="color:rgb(98,98,98)">2</span>][<span style="color:rgb(98,98,98)">1</span>]
<span class="ansi-green-fg ansi-bold">      5</span> <span style="color:rgb(0,135,0)">print</span>(<span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">pretty_decoded input: </span><span style="font-weight:bold;color:rgb(175,95,0)">\n</span><span style="font-weight:bold;color:rgb(175,95,0)">\n</span><span style="color:rgb(175,0,0)">'</span>, pretty_decode(c4_input))

<span class="ansi-red-fg">NameError</span>: name 'inputs_targets_pairs' is not defined</pre>
</div>
</div>
</div>
<p>Run the cell below to decode.</p>
</section>
<section id="note-this-will-take-some-time-to-run" class="level3">
<h3 class="anchored" data-anchor-id="note-this-will-take-some-time-to-run">Note: This will take some time to run</h3>
<div id="3a8ce6a0" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Temperature is a parameter for sampling.</span></span>
<span id="cb7-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#   # * 0.0: same as argmax, always pick the most probable token</span></span>
<span id="cb7-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#   # * 1.0: sampling from the distribution (can sometimes say random things)</span></span>
<span id="cb7-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#   # * values inbetween can trade off diversity and quality, try it out!</span></span>
<span id="cb7-5">output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> decoding.autoregressive_sample(model, inputs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.array(c4_input)[<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, :],</span>
<span id="cb7-6">                                        temperature<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span>, max_length<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># originally max_length = 50</span></span>
<span id="cb7-7"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(wrapper.fill(pretty_decode(output[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])))</span></code></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[6], line 5</span>
<span class="ansi-green-fg ansi-bold">      1</span> <span style="font-style:italic;color:rgb(95,135,135)"># Temperature is a parameter for sampling.</span>
<span class="ansi-green-fg ansi-bold">      2</span> <span style="font-style:italic;color:rgb(95,135,135)">#   # * 0.0: same as argmax, always pick the most probable token</span>
<span class="ansi-green-fg ansi-bold">      3</span> <span style="font-style:italic;color:rgb(95,135,135)">#   # * 1.0: sampling from the distribution (can sometimes say random things)</span>
<span class="ansi-green-fg ansi-bold">      4</span> <span style="font-style:italic;color:rgb(95,135,135)">#   # * values inbetween can trade off diversity and quality, try it out!</span>
<span class="ansi-green-fg">----&gt; 5</span> output <span style="color:rgb(98,98,98)">=</span> decoding<span style="color:rgb(98,98,98)">.</span>autoregressive_sample(model, inputs<span style="color:rgb(98,98,98)">=</span>np<span style="color:rgb(98,98,98)">.</span>array(<span class="ansi-yellow-bg">c4_input</span>)[<span style="font-weight:bold;color:rgb(0,135,0)">None</span>, :],
<span class="ansi-green-fg ansi-bold">      6</span>                                         temperature<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">0.0</span>, max_length<span style="color:rgb(98,98,98)">=</span><span style="color:rgb(98,98,98)">5</span>) <span style="font-style:italic;color:rgb(95,135,135)"># originally max_length = 50</span>
<span class="ansi-green-fg ansi-bold">      7</span> <span style="color:rgb(0,135,0)">print</span>(wrapper<span style="color:rgb(98,98,98)">.</span>fill(pretty_decode(output[<span style="color:rgb(98,98,98)">0</span>])))

<span class="ansi-red-fg">NameError</span>: name 'c4_input' is not defined</pre>
</div>
</div>
</div>
<p>At this point the RAM is almost full, this happens because the model and the decoding is memory heavy. You can run decoding just once. Running it the second time with another example might give you an answer that makes no sense, or repetitive words. If that happens restart the runtime (see how to at the start of the notebook) and run all the cells again.</p>
<p>You should also be aware that the quality of the decoding is not very good because max_length was downsized from 50 to 5 so that this runs faster within this environment. The colab version uses the original max_length so check that one for the actual decoding.</p>


</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2021,
  author = {Bochman, Oren},
  title = {Assignment 3 {Ungraded} {Sections} - {Part} 1: {BERT} {Loss}
    {Model}},
  date = {2021-04-12},
  url = {https://orenbochman.github.io/notes-nlp/notes/c4w3/lab02.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2021" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2021. <span>“Assignment 3 Ungraded Sections - Part 1:
BERT Loss Model.”</span> April 12, 2021. <a href="https://orenbochman.github.io/notes-nlp/notes/c4w3/lab02.html">https://orenbochman.github.io/notes-nlp/notes/c4w3/lab02.html</a>.
</div></div></section></div> ]]></description>
  <category>NLP</category>
  <category>Coursera</category>
  <category>Lab</category>
  <category>NLP with Attention Models</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/c4w3/lab02.html</guid>
  <pubDate>Sun, 11 Apr 2021 21:00:00 GMT</pubDate>
</item>
<item>
  <title>SentencePiece and Byte Pair Encoding</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/c4w3/lab01.html</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/Course-Logo-4-1.webp" class="nolightbox img-fluid figure-img"></p>
<figcaption>course banner</figcaption>
</figure>
</div></div><section id="introduction-to-tokenization" class="level2">
<h2 class="anchored" data-anchor-id="introduction-to-tokenization">Introduction to Tokenization</h2>
<p>In order to process text in neural network models, it is first required to <strong>encode</strong> text as numbers with ids (such as the embedding vectors we’ve been using in the previous assignments), since the tensor operations act on numbers. Finally, if the output of the network are words, it is required to <strong>decode</strong> the predicted tokens ids back to text.</p>
<p>To encode text, the first decision that has to be made is to what level of granularity are we going to consider the text? Because ultimately, from these <strong>tokens</strong>, features are going to be created about them. Many different experiments have been carried out using <em>words</em>, <em>morphological units</em>, <em>phonemic units</em>, <em>characters</em>. For example,</p>
<ul>
<li>Tokens are tricky. (raw text)</li>
<li>Tokens are tricky . (<a href="https://arxiv.org/pdf/1301.3781">words</a>)</li>
<li>Token s _ are _ trick _ y . (<a href="https://arxiv.org/pdf/1907.02423.pdf">morphemes</a>)</li>
<li>t oʊ k ə n z _ ɑː _ ˈt r ɪ k i. (<a href="https://www.aclweb.org/anthology/W18-5812.pdf">phonemes</a>, for STT)</li>
<li>T o k e n s _ a r e _ t r i c k y . (<a href="https://www.aclweb.org/anthology/C18-1139/">character</a>)</li>
</ul>
<p>But how to identify these units, such as words, are largely determined by the language they come from. For example, in many European languages a space is used to separate words, while in some Asian languages there are no spaces between words. Compare English and Mandarin.</p>
<ul>
<li>Tokens are tricky. (original sentence)</li>
<li>令牌很棘手 (Mandarin)</li>
<li>Lìng pái hěn jí shǒu (pinyin)</li>
<li>令牌 很 棘手 (Mandarin with spaces)</li>
</ul>
<p>So, the ability to <strong>tokenize</strong>, i.e.&nbsp;split text into meaningful fundamental units is not always straight-forward.</p>
<p>Also, there are practical issues of how large our <em>vocabulary</em> of words, <code>vocab_size</code>, should be, considering memory limitations vs.&nbsp;coverage. A compromise between the finest-grained models employing characters which can be memory and more computationally efficient <em>subword</em> units such as <a href="https://arxiv.org/pdf/1712.09405">n-grams</a> or larger units need to be made.</p>
<p>In <a href="https://www.aclweb.org/anthology/D18-2012.pdf">SentencePiece</a> unicode characters are grouped together using either a <a href="https://www.aclweb.org/anthology/P18-1007.pdf">unigram language model</a> (used in this week’s assignment) or <a href="https://arxiv.org/pdf/1508.07909.pdf">BPE</a>, <strong>byte-pair encoding</strong>. We will discuss BPE, since BERT and many of its variant uses a modified version of BPE and its pseudocode is easy to implement and understand… hopefully!</p>
</section>
<section id="sentencepiece-preprocessing" class="level2">
<h2 class="anchored" data-anchor-id="sentencepiece-preprocessing">SentencePiece Preprocessing</h2>
<section id="nfkc-normalization" class="level3">
<h3 class="anchored" data-anchor-id="nfkc-normalization">NFKC Normalization</h3>
<p>Unsurprisingly, even using unicode to initially tokenize text can be ambiguous, e.g.,</p>
<div id="2888c5cc" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1">eaccent <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\u00E9</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span></span>
<span id="cb1-2">e_accent <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\u0065\u0301</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span></span>
<span id="cb1-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>eaccent<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> = </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>e_accent<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> : </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>eaccent <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> e_accent<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>é = é : False</code></pre>
</div>
</div>
<p>SentencePiece uses the Unicode standard Normalization form, <a href="https://en.wikipedia.org/wiki/Unicode_equivalence">NFKC</a>, so this isn’t an issue. Looking at our example from above again with normalization:</p>
<div id="abf315fd" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> unicodedata <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> normalize</span>
<span id="cb3-2"></span>
<span id="cb3-3">norm_eaccent <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> normalize(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'NFKC'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\u00E9</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb3-4">norm_e_accent <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> normalize(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'NFKC'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\u0065\u0301</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb3-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>norm_eaccent<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> = </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>norm_e_accent<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> : </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>norm_eaccent <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> norm_e_accent<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>é = é : True</code></pre>
</div>
</div>
<p>Normalization has actually changed the unicode code point (unicode unique id) for one of these two characters.</p>
<div id="497f8a12" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_hex_encoding(s):</span>
<span id="cb5-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">' '</span>.join(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">hex</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">ord</span>(c)) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> c <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> s)</span>
<span id="cb5-3"></span>
<span id="cb5-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> print_string_and_encoding(s):</span>
<span id="cb5-5">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>s<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> : </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>get_hex_encoding(s)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>) </span></code></pre></div>
</div>
<div id="9ecaeb21" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> s <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> [eaccent, e_accent, norm_eaccent, norm_e_accent]:</span>
<span id="cb6-2">    print_string_and_encoding(s)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>é : 0xe9
é : 0x65 0x301
é : 0xe9
é : 0xe9</code></pre>
</div>
</div>
<p>This normalization has other side effects which may be considered useful such as converting curly quotes “ to ” their ASCII equivalent. (Although we <em>now</em> lose directionality of the quote…)</p>
</section>
<section id="lossless-tokenization" class="level3">
<h3 class="anchored" data-anchor-id="lossless-tokenization">Lossless Tokenization<sup>*</sup></h3>
<p>SentencePiece also ensures that when you tokenize your data and detokenize your data the original position of white space is preserved. (However, tabs and newlines are converted to spaces, please try this experiment yourself later below.)</p>
<p>To ensure this <strong>lossless tokenization</strong> it replaces white space with _ (U+2581). So that a simple join of the replace underscores with spaces can restore the white space, even if there are consecutives symbols. But remember first to normalize and then replace spaces with _ (U+2581). As the following example shows.</p>
<div id="a2071d8b" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">s <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Tokenization is hard.'</span></span>
<span id="cb8-2">s_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> s.replace(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">' '</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\u2581</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb8-3">s_n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> normalize(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'NFKC'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Tokenization is hard.'</span>)</span></code></pre></div>
</div>
<div id="104d8e62" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(get_hex_encoding(s))</span>
<span id="cb9-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(get_hex_encoding(s_))</span>
<span id="cb9-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(get_hex_encoding(s_n))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x20 0x69 0x73 0x20 0x68 0x61 0x72 0x64 0x2e
0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x2581 0x69 0x73 0x2581 0x68 0x61 0x72 0x64 0x2e
0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x20 0x69 0x73 0x20 0x68 0x61 0x72 0x64 0x2e</code></pre>
</div>
</div>
<p>So the special unicode underscore was replaced by the ASCII unicode. Reversing the order, we see that the special unicode underscore was retained.</p>
<div id="c6bd9c1c" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">s <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Tokenization is hard.'</span></span>
<span id="cb11-2">sn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> normalize(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'NFKC'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Tokenization is hard.'</span>)</span>
<span id="cb11-3">sn_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> s.replace(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">' '</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\u2581</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
</div>
<div id="a345e9bb" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(get_hex_encoding(s))</span>
<span id="cb12-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(get_hex_encoding(sn))</span>
<span id="cb12-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(get_hex_encoding(sn_))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x20 0x69 0x73 0x20 0x68 0x61 0x72 0x64 0x2e
0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x20 0x69 0x73 0x20 0x68 0x61 0x72 0x64 0x2e
0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x2581 0x69 0x73 0x2581 0x68 0x61 0x72 0x64 0x2e</code></pre>
</div>
</div>
</section>
</section>
<section id="bpe-algorithm" class="level2">
<h2 class="anchored" data-anchor-id="bpe-algorithm">BPE Algorithm</h2>
<p>Now that we have discussed the preprocessing that SentencePiece performs we will go get our data, preprocess, and apply the BPE algorithm. We will show how this reproduces the tokenization produced by training SentencePiece on our example dataset (from this week’s assignment).</p>
<section id="preparing-our-data" class="level3">
<h3 class="anchored" data-anchor-id="preparing-our-data">Preparing our Data</h3>
<p>First, we get our Squad data and process as above.</p>
<div id="0654fb36" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> ast</span>
<span id="cb14-2"></span>
<span id="cb14-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> convert_json_examples_to_text(filepath):</span>
<span id="cb14-4">    example_jsons <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">map</span>(ast.literal_eval, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">open</span>(filepath))) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Read in the json from the example file</span></span>
<span id="cb14-5">    texts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [example_json[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'text'</span>].decode(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'utf-8'</span>) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> example_json <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> example_jsons] <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Decode the byte sequences</span></span>
<span id="cb14-6">    text <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>.join(texts)       <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Separate different articles by two newlines</span></span>
<span id="cb14-7">    text <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> normalize(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'NFKC'</span>, text)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Normalize the text</span></span>
<span id="cb14-8"></span>
<span id="cb14-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">open</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'example.txt'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'w'</span>) <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> fw:</span>
<span id="cb14-10">        fw.write(text)</span>
<span id="cb14-11">    </span>
<span id="cb14-12">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> text</span></code></pre></div>
</div>
<div id="95e23abc" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">text <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> convert_json_examples_to_text(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'data.txt'</span>)</span>
<span id="cb15-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(text[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">900</span>])</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Beginners BBQ Class Taking Place in Missoula!
Do you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.
He will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information.
The cost to be in the class is $35 per person, and for spectators it is free. Included in the cost will be either a t-shirt or apron and you will be tasting samples of each meat that is prepared.

Discussion in 'Mac OS X Lion (10.7)' started by axboi87, Jan 20, 2012.
I've got a 500gb internal drive and a 240gb SSD.
When trying to restore using di</code></pre>
</div>
</div>
<p>In the algorithm the <code>vocab</code> variable is actually a frequency dictionary of the words. Further, those words have been prepended with an <em>underscore</em> to indicate that they are the beginning of a word. Finally, the characters have been delimited by spaces so that the BPE algorithm can group the most common characters together in the dictionary in a greedy fashion. We will see how that is exactly done shortly.</p>
<div id="cbb7b1be" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> collections <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Counter</span>
<span id="cb17-2"></span>
<span id="cb17-3">vocab <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Counter([<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\u2581</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> word <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> word <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> text.split()])</span>
<span id="cb17-4">vocab <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">' '</span>.join([l <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> l <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> word]): freq <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> word, freq <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> vocab.items()}</span></code></pre></div>
</div>
<div id="5867ef78" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> show_vocab(vocab, end<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>, limit<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>):</span>
<span id="cb18-2">    shown <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb18-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> word, freq <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> vocab.items():</span>
<span id="cb18-4">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>word<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>freq<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>, end<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>end)</span>
<span id="cb18-5">        shown <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb18-6">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> shown <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> limit:</span>
<span id="cb18-7">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">break</span></span></code></pre></div>
</div>
<div id="53d3752a" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">show_vocab(vocab)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>▁ B e g i n n e r s: 1
▁ B B Q: 3
▁ C l a s s: 2
▁ T a k i n g: 1
▁ P l a c e: 1
▁ i n: 15
▁ M i s s o u l a !: 1
▁ D o: 1
▁ y o u: 13
▁ w a n t: 1
▁ t o: 33
▁ g e t: 2
▁ b e t t e r: 2
▁ a t: 1
▁ m a k i n g: 2
▁ d e l i c i o u s: 1
▁ B B Q ?: 1
▁ Y o u: 1
▁ w i l l: 6
▁ h a v e: 4
▁ t h e: 31</code></pre>
</div>
</div>
<p>We check the size of the vocabulary (frequency dictionary) because this is the one hyperparameter that BPE depends on crucially on how far it breaks up a word into SentencePieces. It turns out that for our trained model on our small dataset that 60% of 455 merges of the most frequent characters need to be done to reproduce the upperlimit of a 32K <code>vocab_size</code> over the entire corpus of examples.</p>
<div id="31fde157" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Total number of unique words: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(vocab)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb21-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Number of merges required to reproduce SentencePiece training on the whole corpus: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.60</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(vocab))<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Total number of unique words: 455
Number of merges required to reproduce SentencePiece training on the whole corpus: 273</code></pre>
</div>
</div>
</section>
<section id="bpe-algorithm-1" class="level3">
<h3 class="anchored" data-anchor-id="bpe-algorithm-1">BPE Algorithm</h3>
<p>Directly from the BPE paper we have the following algorithm.</p>
<p>To understand what’s going on first take a look at the third function <code>get_sentence_piece_vocab</code>. It takes in the current <code>vocab</code> word-frequency dictionary and the fraction of the total <code>vocab_size</code> to merge characters in the words of the dictionary, <code>num_merges</code> times. Then for each <em>merge</em> operation it <code>get_stats</code> on how many of each pair of character sequences there are. It gets the most frequent <em>pair</em> of symbols as the <code>best</code> pair. Then it merges those pair of symbols (removes the space between them) in each word in the <code>vocab</code> that contains this <code>best</code> (= <code>pair</code>). Consquently, <code>merge_vocab</code> creates a new <code>vocab</code>, <code>v_out</code>. This process is repeated <code>num_merges</code> times and the result is the set of SentencePieces (keys of the final <code>sp_vocab</code>).</p>
<p>Please feel free to skip the below if the above description was enough.</p>
<p>In a little more detail then, we can see in <code>get_stats</code> we initially create a list of bigram frequencies (two character sequence) from our vocabulary. Later, this may include (trigrams, quadgrams, etc.). Note that the key of the <code>pairs</code> frequency dictionary is actually a 2-tuple, which is just shorthand notation for a pair.</p>
<p>In <code>merge_vocab</code> we take in an individual <code>pair</code> (of character sequences, note this is the most frequency <code>best</code> pair) and the current <code>vocab</code> as <code>v_in</code>. We create a new <code>vocab</code>, <code>v_out</code>, from the old by joining together the characters in the pair (removing the space), if they are present in the a word of the dictionary. <a href="https://regex101.com/">Warning</a>: the expression <code>(?&lt;!\S)</code> means that either whitespace character follows before the <code>bigram</code> or there is nothing before (beginning of word) the bigram, similarly for <code>(?!\S)</code> for preceding whitespace or end of word.</p>
<div id="df18a858" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> re, collections</span>
<span id="cb23-2"></span>
<span id="cb23-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_stats(vocab):</span>
<span id="cb23-4">    pairs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> collections.defaultdict(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>)</span>
<span id="cb23-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> word, freq <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> vocab.items():</span>
<span id="cb23-6">        symbols <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> word.split()</span>
<span id="cb23-7">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(symbols) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>):</span>
<span id="cb23-8">            pairs[symbols[i], symbols[i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> freq</span>
<span id="cb23-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> pairs</span>
<span id="cb23-10"></span>
<span id="cb23-11"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> merge_vocab(pair, v_in):</span>
<span id="cb23-12">    v_out <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {}</span>
<span id="cb23-13">    bigram <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> re.escape(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">' '</span>.join(pair))</span>
<span id="cb23-14">    p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> re.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">compile</span>(<span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">r'(?&lt;!\S)'</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> bigram <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">r'(?!\S)'</span>)</span>
<span id="cb23-15">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> word <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> v_in:</span>
<span id="cb23-16">        w_out <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> p.sub(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">''</span>.join(pair), word)</span>
<span id="cb23-17">        v_out[w_out] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> v_in[word]</span>
<span id="cb23-18">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> v_out</span>
<span id="cb23-19"></span>
<span id="cb23-20"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_sentence_piece_vocab(vocab, frac_merges<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.60</span>):</span>
<span id="cb23-21">    sp_vocab <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> vocab.copy()</span>
<span id="cb23-22">    num_merges <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(sp_vocab)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>frac_merges)</span>
<span id="cb23-23">    </span>
<span id="cb23-24">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(num_merges):</span>
<span id="cb23-25">        pairs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_stats(sp_vocab)</span>
<span id="cb23-26">        best <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(pairs, key<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pairs.get)</span>
<span id="cb23-27">        sp_vocab <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> merge_vocab(best, sp_vocab)</span>
<span id="cb23-28"></span>
<span id="cb23-29">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> sp_vocab</span></code></pre></div>
</div>
<div id="df6e4758" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">sp_vocab <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_sentence_piece_vocab(vocab)</span>
<span id="cb24-2">show_vocab(sp_vocab) </span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>▁B e g in n ers: 1
▁BBQ: 3
▁Cl ass: 2
▁T ak ing: 1
▁P la ce: 1
▁in: 15
▁M is s ou la !: 1
▁D o: 1
▁you: 13
▁w an t: 1
▁to: 33
▁g et: 2
▁be t ter: 2
▁a t: 1
▁mak ing: 2
▁d e l ic i ou s: 1
▁BBQ ?: 1
▁ Y ou: 1
▁will: 6
▁have: 4
▁the: 31</code></pre>
</div>
</div>
</section>
</section>
<section id="train-sentencepiece-bpe-tokenizer-on-example-data" class="level2">
<h2 class="anchored" data-anchor-id="train-sentencepiece-bpe-tokenizer-on-example-data">Train SentencePiece BPE Tokenizer on Example Data</h2>
<section id="explore-sentencepiece-model" class="level3">
<h3 class="anchored" data-anchor-id="explore-sentencepiece-model">Explore SentencePiece Model</h3>
<p>First let us explore the SentencePiece model provided with this week’s assignment. Remember you can always use Python’s built in <code>help</code> command to see the documentation for any object or method.</p>
<div id="5dde03fa" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> sentencepiece <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> spm</span>
<span id="cb26-2">sp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> spm.SentencePieceProcessor(model_file<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sentencepiece.model'</span>)</span></code></pre></div>
</div>
<div id="03e39e12" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">help</span>(sp)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Help on SentencePieceProcessor in module sentencepiece object:

class SentencePieceProcessor(builtins.object)
 |  SentencePieceProcessor(model_file=None, model_proto=None, out_type=&lt;class 'int'&gt;, add_bos=False, add_eos=False, reverse=False, emit_unk_piece=False, enable_sampling=False, nbest_size=-1, alpha=0.1, num_threads=-1)
 |  
 |  Methods defined here:
 |  
 |  CalculateEntropy(self, input, alpha, num_threads=None)
 |      Calculate sentence entropy
 |  
 |  Decode(self, input, out_type=&lt;class 'str'&gt;, num_threads=None)
 |      Decode processed id or token sequences.
 |      
 |      Args:
 |        out_type: output type. str, bytes or 'serialized_proto' or 'immutable_proto' (Default = str)
 |        num_threads: the number of threads used in the batch processing (Default = -1).
 |  
 |  DecodeIds(self, input, out_type=&lt;class 'str'&gt;, **kwargs)
 |  
 |  DecodeIdsAsImmutableProto(self, input, out_type='immutable_proto', **kwargs)
 |  
 |  DecodeIdsAsSerializedProto(self, input, out_type='serialized_proto', **kwargs)
 |  
 |  DecodePieces(self, input, out_type=&lt;class 'str'&gt;, **kwargs)
 |  
 |  DecodePiecesAsImmutableProto(self, input, out_type='immutable_proto', **kwargs)
 |  
 |  DecodePiecesAsSerializedProto(self, input, out_type='serialized_proto', **kwargs)
 |  
 |  Detokenize = Decode(self, input, out_type=&lt;class 'str'&gt;, num_threads=None)
 |  
 |  Encode(self, input, out_type=None, add_bos=None, add_eos=None, reverse=None, emit_unk_piece=None, enable_sampling=None, nbest_size=None, alpha=None, num_threads=None)
 |      Encode text input to segmented ids or tokens.
 |      
 |      Args:
 |      input: input string. accepsts list of string.
 |      out_type: output type. int or str.
 |      add_bos: Add &lt;s&gt; to the result (Default = false)
 |      add_eos: Add &lt;/s&gt; to the result (Default = false) &lt;s&gt;/&lt;/s&gt; is added after
 |               reversing (if enabled).
 |      reverse: Reverses the tokenized sequence (Default = false)
 |      emit_unk_piece: Emits the unk literal string (Default = false)
 |      nbest_size: sampling parameters for unigram. Invalid in BPE-Dropout.
 |                  nbest_size = {0,1}: No sampling is performed.
 |                  nbest_size &gt; 1: samples from the nbest_size results.
 |                  nbest_size &lt; 0: assuming that nbest_size is infinite and samples
 |                  from the all hypothesis (lattice) using
 |                  forward-filtering-and-backward-sampling algorithm.
 |      alpha: Soothing parameter for unigram sampling, and merge probability for
 |             BPE-dropout (probablity 'p' in BPE-dropout paper).
 |      num_threads: the number of threads used in the batch processing (Default = -1).
 |  
 |  EncodeAsIds(self, input, **kwargs)
 |  
 |  EncodeAsImmutableProto(self, input, **kwargs)
 |  
 |  EncodeAsPieces(self, input, **kwargs)
 |  
 |  EncodeAsSerializedProto(self, input, **kwargs)
 |  
 |  GetPieceSize(self)
 |  
 |  GetScore = _batched_func(self, arg)
 |  
 |  IdToPiece = _batched_func(self, arg)
 |  
 |  Init(self, model_file=None, model_proto=None, out_type=&lt;class 'int'&gt;, add_bos=False, add_eos=False, reverse=False, emit_unk_piece=False, enable_sampling=False, nbest_size=-1, alpha=0.1, num_threads=-1)
 |      Initialzie sentencepieceProcessor.
 |      
 |      Args:
 |        model_file: The sentencepiece model file path.
 |        model_proto: The sentencepiece model serialized proto.
 |        out_type: output type. int or str.
 |        add_bos: Add &lt;s&gt; to the result (Default = false)
 |        add_eos: Add &lt;/s&gt; to the result (Default = false) &lt;s&gt;/&lt;/s&gt; is added after
 |          reversing (if enabled).
 |        reverse: Reverses the tokenized sequence (Default = false)
 |        emit_unk_piece: Emits the unk literal string (Default = false)
 |        nbest_size: sampling parameters for unigram. Invalid in BPE-Dropout.
 |                    nbest_size = {0,1}: No sampling is performed.
 |                    nbest_size &gt; 1: samples from the nbest_size results.
 |                    nbest_size &lt; 0: assuming that nbest_size is infinite and samples
 |                      from the all hypothesis (lattice) using
 |                      forward-filtering-and-backward-sampling algorithm.
 |        alpha: Soothing parameter for unigram sampling, and dropout probability of
 |               merge operations for BPE-dropout.
 |        num_threads: number of threads in batch processing (Default = -1, auto-detected)
 |  
 |  IsByte = _batched_func(self, arg)
 |  
 |  IsControl = _batched_func(self, arg)
 |  
 |  IsUnknown = _batched_func(self, arg)
 |  
 |  IsUnused = _batched_func(self, arg)
 |  
 |  Load(self, model_file=None, model_proto=None)
 |      Overwride SentencePieceProcessor.Load to support both model_file and model_proto.
 |      
 |      Args:
 |        model_file: The sentencepiece model file path.
 |        model_proto: The sentencepiece model serialized proto. Either `model_file`
 |          or `model_proto` must be set.
 |  
 |  LoadFromFile(self, arg)
 |  
 |  LoadFromSerializedProto(self, serialized)
 |  
 |  LoadVocabulary(self, filename, threshold)
 |  
 |  NBestEncode(self, input, out_type=None, add_bos=None, add_eos=None, reverse=None, emit_unk_piece=None, nbest_size=None)
 |      NBestEncode text input to segmented ids or tokens.
 |      
 |      Args:
 |      input: input string. accepsts list of string.
 |      out_type: output type. int or str.
 |      add_bos: Add &lt;s&gt; to the result (Default = false)
 |      add_eos: Add &lt;/s&gt; to the result (Default = false) &lt;s&gt;/&lt;/s&gt; is added after reversing (if enabled).
 |      reverse: Reverses the tokenized sequence (Default = false)
 |      emit_unk_piece: Emits the unk literal string (Default = false)
 |      nbest_size: nbest size
 |  
 |  NBestEncodeAsIds(self, input, nbest_size=None, **kwargs)
 |  
 |  NBestEncodeAsImmutableProto(self, input, nbest_size=None, **kwargs)
 |  
 |  NBestEncodeAsPieces(self, input, nbest_size=None, **kwargs)
 |  
 |  NBestEncodeAsSerializedProto(self, input, nbest_size=None, **kwargs)
 |  
 |  Normalize(self, input, with_offsets=None)
 |  
 |  OverrideNormalizerSpec(self, **kwargs)
 |  
 |  PieceToId = _batched_func(self, arg)
 |  
 |  ResetVocabulary(self)
 |  
 |  SampleEncodeAndScore(self, input, out_type=None, add_bos=None, add_eos=None, reverse=None, emit_unk_piece=None, num_samples=None, alpha=None, wor=None, include_best=None)
 |      SampleEncodeAndScore text input to segmented ids or tokens.
 |      
 |      Args:
 |      input: input string. accepsts list of string.
 |      out_type: output type. int or str or 'serialized_proto' or 'immutable_proto'
 |      add_bos: Add &lt;s&gt; to the result (Default = false)
 |      add_eos: Add &lt;/s&gt; to the result (Default = false) &lt;s&gt;/&lt;/s&gt; is added after reversing (if enabled).
 |      reverse: Reverses the tokenized sequence (Default = false)
 |      emit_unk_piece: Emits the unk literal string (Default = false)
 |      num_samples: How many samples to return (Default = 1)
 |      alpha: inverse temperature for sampling
 |      wor: whether to sample without replacement (Default = false)
 |      include_best: whether to include the best tokenization, requires wor=True (Default = false)
 |  
 |  SampleEncodeAndScoreAsIds(self, input, num_samples=None, alpha=None, **kwargs)
 |  
 |  SampleEncodeAndScoreAsImmutableProto(self, input, num_samples=None, alpha=None, **kwargs)
 |  
 |  SampleEncodeAndScoreAsPieces(self, input, num_samples=None, alpha=None, **kwargs)
 |  
 |  SampleEncodeAndScoreAsSerializedProto(self, input, num_samples=None, alpha=None, **kwargs)
 |  
 |  SampleEncodeAsIds(self, input, nbest_size=None, alpha=None, **kwargs)
 |  
 |  SampleEncodeAsImmutableProto(self, input, nbest_size=None, alpha=None, **kwargs)
 |  
 |  SampleEncodeAsPieces(self, input, nbest_size=None, alpha=None, **kwargs)
 |  
 |  SampleEncodeAsSerializedProto(self, input, nbest_size=None, alpha=None, **kwargs)
 |  
 |  SetDecodeExtraOptions(self, extra_option)
 |  
 |  SetEncodeExtraOptions(self, extra_option)
 |  
 |  SetVocabulary(self, valid_vocab)
 |  
 |  Tokenize = Encode(self, input, out_type=None, add_bos=None, add_eos=None, reverse=None, emit_unk_piece=None, enable_sampling=None, nbest_size=None, alpha=None, num_threads=None)
 |  
 |  __getitem__(self, piece)
 |  
 |  __getstate__(self)
 |  
 |  __init__ = Init(self, model_file=None, model_proto=None, out_type=&lt;class 'int'&gt;, add_bos=False, add_eos=False, reverse=False, emit_unk_piece=False, enable_sampling=False, nbest_size=-1, alpha=0.1, num_threads=-1)
 |  
 |  __len__(self)
 |  
 |  __repr__ = _swig_repr(self)
 |  
 |  __setstate__(self, serialized_model_proto)
 |  
 |  bos_id(self)
 |  
 |  calculate_entropy = CalculateEntropy(self, input, alpha, num_threads=None)
 |  
 |  decode = Decode(self, input, out_type=&lt;class 'str'&gt;, num_threads=None)
 |  
 |  decode_ids = DecodeIds(self, input, out_type=&lt;class 'str'&gt;, **kwargs)
 |  
 |  decode_ids_as_immutable_proto = DecodeIdsAsImmutableProto(self, input, out_type='immutable_proto', **kwargs)
 |  
 |  decode_ids_as_serialized_proto = DecodeIdsAsSerializedProto(self, input, out_type='serialized_proto', **kwargs)
 |  
 |  decode_pieces = DecodePieces(self, input, out_type=&lt;class 'str'&gt;, **kwargs)
 |  
 |  decode_pieces_as_immutable_proto = DecodePiecesAsImmutableProto(self, input, out_type='immutable_proto', **kwargs)
 |  
 |  decode_pieces_as_serialized_proto = DecodePiecesAsSerializedProto(self, input, out_type='serialized_proto', **kwargs)
 |  
 |  detokenize = Decode(self, input, out_type=&lt;class 'str'&gt;, num_threads=None)
 |  
 |  encode = Encode(self, input, out_type=None, add_bos=None, add_eos=None, reverse=None, emit_unk_piece=None, enable_sampling=None, nbest_size=None, alpha=None, num_threads=None)
 |  
 |  encode_as_ids = EncodeAsIds(self, input, **kwargs)
 |  
 |  encode_as_immutable_proto = EncodeAsImmutableProto(self, input, **kwargs)
 |  
 |  encode_as_pieces = EncodeAsPieces(self, input, **kwargs)
 |  
 |  encode_as_serialized_proto = EncodeAsSerializedProto(self, input, **kwargs)
 |  
 |  eos_id(self)
 |  
 |  get_piece_size = GetPieceSize(self)
 |  
 |  get_score = _batched_func(self, arg)
 |  
 |  id_to_piece = _batched_func(self, arg)
 |  
 |  init = Init(self, model_file=None, model_proto=None, out_type=&lt;class 'int'&gt;, add_bos=False, add_eos=False, reverse=False, emit_unk_piece=False, enable_sampling=False, nbest_size=-1, alpha=0.1, num_threads=-1)
 |  
 |  is_byte = _batched_func(self, arg)
 |  
 |  is_control = _batched_func(self, arg)
 |  
 |  is_unknown = _batched_func(self, arg)
 |  
 |  is_unused = _batched_func(self, arg)
 |  
 |  load = Load(self, model_file=None, model_proto=None)
 |  
 |  load_from_file = LoadFromFile(self, arg)
 |  
 |  load_from_serialized_proto = LoadFromSerializedProto(self, serialized)
 |  
 |  load_vocabulary = LoadVocabulary(self, filename, threshold)
 |  
 |  nbest_encode = NBestEncode(self, input, out_type=None, add_bos=None, add_eos=None, reverse=None, emit_unk_piece=None, nbest_size=None)
 |  
 |  nbest_encode_as_ids = NBestEncodeAsIds(self, input, nbest_size=None, **kwargs)
 |  
 |  nbest_encode_as_immutable_proto = NBestEncodeAsImmutableProto(self, input, nbest_size=None, **kwargs)
 |  
 |  nbest_encode_as_pieces = NBestEncodeAsPieces(self, input, nbest_size=None, **kwargs)
 |  
 |  nbest_encode_as_serialized_proto = NBestEncodeAsSerializedProto(self, input, nbest_size=None, **kwargs)
 |  
 |  normalize = Normalize(self, input, with_offsets=None)
 |  
 |  override_normalizer_spec = OverrideNormalizerSpec(self, **kwargs)
 |  
 |  pad_id(self)
 |  
 |  piece_size(self)
 |  
 |  piece_to_id = _batched_func(self, arg)
 |  
 |  reset_vocabulary = ResetVocabulary(self)
 |  
 |  sample_encode_and_score = SampleEncodeAndScore(self, input, out_type=None, add_bos=None, add_eos=None, reverse=None, emit_unk_piece=None, num_samples=None, alpha=None, wor=None, include_best=None)
 |  
 |  sample_encode_and_score_as_ids = SampleEncodeAndScoreAsIds(self, input, num_samples=None, alpha=None, **kwargs)
 |  
 |  sample_encode_and_score_as_immutable_proto = SampleEncodeAndScoreAsImmutableProto(self, input, num_samples=None, alpha=None, **kwargs)
 |  
 |  sample_encode_and_score_as_pieces = SampleEncodeAndScoreAsPieces(self, input, num_samples=None, alpha=None, **kwargs)
 |  
 |  sample_encode_and_score_as_serialized_proto = SampleEncodeAndScoreAsSerializedProto(self, input, num_samples=None, alpha=None, **kwargs)
 |  
 |  sample_encode_as_ids = SampleEncodeAsIds(self, input, nbest_size=None, alpha=None, **kwargs)
 |  
 |  sample_encode_as_immutable_proto = SampleEncodeAsImmutableProto(self, input, nbest_size=None, alpha=None, **kwargs)
 |  
 |  sample_encode_as_pieces = SampleEncodeAsPieces(self, input, nbest_size=None, alpha=None, **kwargs)
 |  
 |  sample_encode_as_serialized_proto = SampleEncodeAsSerializedProto(self, input, nbest_size=None, alpha=None, **kwargs)
 |  
 |  serialized_model_proto(self)
 |  
 |  set_decode_extra_options = SetDecodeExtraOptions(self, extra_option)
 |  
 |  set_encode_extra_options = SetEncodeExtraOptions(self, extra_option)
 |  
 |  set_vocabulary = SetVocabulary(self, valid_vocab)
 |  
 |  tokenize = Encode(self, input, out_type=None, add_bos=None, add_eos=None, reverse=None, emit_unk_piece=None, enable_sampling=None, nbest_size=None, alpha=None, num_threads=None)
 |  
 |  unk_id(self)
 |  
 |  vocab_size(self)
 |  
 |  ----------------------------------------------------------------------
 |  Static methods defined here:
 |  
 |  __swig_destroy__ = delete_SentencePieceProcessor(...)
 |  
 |  ----------------------------------------------------------------------
 |  Data descriptors defined here:
 |  
 |  __dict__
 |      dictionary for instance variables (if defined)
 |  
 |  __weakref__
 |      list of weak references to the object (if defined)
 |  
 |  thisown
 |      The membership flag
</code></pre>
</div>
</div>
<p>Let’s work with the first sentence of our example text.</p>
<div id="c5f42c14" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1">s0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Beginners BBQ Class Taking Place in Missoula!'</span></span></code></pre></div>
</div>
<div id="6be28bed" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb30" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># encode: text =&gt; id</span></span>
<span id="cb30-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(sp.encode_as_pieces(s0))</span>
<span id="cb30-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(sp.encode_as_ids(s0))</span>
<span id="cb30-4"></span>
<span id="cb30-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># decode: id =&gt; text</span></span>
<span id="cb30-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(sp.decode_pieces(sp.encode_as_pieces(s0)))</span>
<span id="cb30-7"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(sp.decode_ids([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12847</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">277</span>]))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>['▁Beginn', 'ers', '▁BBQ', '▁Class', '▁', 'Taking', '▁Place', '▁in', '▁Miss', 'oul', 'a', '!']
[12847, 277, 15068, 4501, 3, 12297, 3399, 16, 5964, 7115, 9, 55]
Beginners BBQ Class Taking Place in Missoula!
Beginners</code></pre>
</div>
</div>
<p>Notice how SentencePiece breaks the words into seemingly odd parts, but we’ve seen something similar from our work with BPE. But how close were we to this model trained on the whole corpus of examles with a <code>vocab_size</code> of 32,000 instead of 455? Here you can also test what happens to white space, like ‘’.</p>
<p>But first let us note that SentencePiece encodes the SentencePieces, the tokens, and has reserved some of the ids as can be seen in this week’s assignment.</p>
<div id="78440f0f" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb32" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1">uid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15068</span></span>
<span id="cb32-2">spiece <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\u2581</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">BBQ"</span></span>
<span id="cb32-3">unknown <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"__MUST_BE_UNKNOWN__"</span></span>
<span id="cb32-4"></span>
<span id="cb32-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># id &lt;=&gt; piece conversion</span></span>
<span id="cb32-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'SentencePiece for ID </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>uid<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>sp<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>id_to_piece(uid)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb32-7"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'ID for Sentence Piece </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>spiece<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>sp<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>piece_to_id(spiece)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb32-8"></span>
<span id="cb32-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># returns 0 for unknown tokens (we can change the id for UNK)</span></span>
<span id="cb32-10"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'ID for unknown text </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>unknown<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>sp<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>piece_to_id(unknown)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>SentencePiece for ID 15068: ▁BBQ
ID for Sentence Piece ▁BBQ: 15068
ID for unknown text __MUST_BE_UNKNOWN__: 2</code></pre>
</div>
</div>
<div id="d4e695f7" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb34" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Beginning of sentence id: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>sp<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>bos_id()<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb34-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Pad id: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>sp<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>pad_id()<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb34-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'End of sentence id: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>sp<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>eos_id()<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb34-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Unknown id: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>sp<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>unk_id()<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb34-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Vocab size: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>sp<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>vocab_size()<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Beginning of sentence id: -1
Pad id: 0
End of sentence id: 1
Unknown id: 2
Vocab size: 32000</code></pre>
</div>
</div>
<p>We can also check what are the ids for the first part and last part of the vocabulary.</p>
<div id="76b3d7f9" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb36" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Id</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\t</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">SentP</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\t</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Control?'</span>)</span>
<span id="cb36-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'------------------------'</span>)</span>
<span id="cb36-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># &lt;unk&gt;, &lt;s&gt;, &lt;/s&gt; are defined by default. Their ids are (0, 1, 2)</span></span>
<span id="cb36-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># &lt;s&gt; and &lt;/s&gt; are defined as 'control' symbol.</span></span>
<span id="cb36-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> uid <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>):</span>
<span id="cb36-6">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(uid, sp.id_to_piece(uid), sp.is_control(uid), sep<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\t</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb36-7">    </span>
<span id="cb36-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># for uid in range(sp.vocab_size()-10,sp.vocab_size()):</span></span>
<span id="cb36-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#     print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\t')</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Id  SentP   Control?
------------------------
0   &lt;pad&gt;   True
1   &lt;/s&gt;    True
2   &lt;unk&gt;   False
3   ▁   False
4   X   False
5   .   False
6   ,   False
7   s   False
8   ▁the    False
9   a   False</code></pre>
</div>
</div>
</section>
<section id="train-sentencepiece-bpe-model-with-our-example.txt" class="level3">
<h3 class="anchored" data-anchor-id="train-sentencepiece-bpe-model-with-our-example.txt">Train SentencePiece BPE model with our example.txt</h3>
<p>Finally, let’s train our own BPE model directly from the SentencePiece library and compare it to the results of our implemention of the algorithm from the BPE paper itself.</p>
<div id="ee5ecc24" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb38" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1">spm.SentencePieceTrainer.train(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'--input=example.txt --model_prefix=example_bpe --vocab_size=450 --model_type=bpe'</span>)</span>
<span id="cb38-2">sp_bpe <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> spm.SentencePieceProcessor()</span>
<span id="cb38-3">sp_bpe.load(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'example_bpe.model'</span>)</span>
<span id="cb38-4"></span>
<span id="cb38-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'*** BPE ***'</span>)</span>
<span id="cb38-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(sp_bpe.encode_as_pieces(s0))</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=example.txt --model_prefix=example_bpe --vocab_size=450 --model_type=bpe
sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : 
trainer_spec {
  input: example.txt
  input_format: 
  model_prefix: example_bpe
  model_type: BPE
  vocab_size: 450
  self_test_sample_size: 0
  character_coverage: 0.9995
  input_sentence_size: 0
  shuffle_input_sentence: 1
  seed_sentencepiece_size: 1000000
  shrinking_factor: 0.75
  max_sentence_length: 4192
  num_threads: 16
  num_sub_iterations: 2
  max_sentencepiece_length: 16
  split_by_unicode_script: 1
  split_by_number: 1
  split_by_whitespace: 1
  split_digits: 0
  pretokenization_delimiter: 
  treat_whitespace_as_suffix: 0
  allow_whitespace_only_pieces: 0
  required_chars: 
  byte_fallback: 0
  vocabulary_output_piece_score: 1
  train_extremely_large_corpus: 0
  seed_sentencepieces_file: 
  hard_vocab_limit: 1
  use_all_vocab: 0
  unk_id: 0
  bos_id: 1
  eos_id: 2
  pad_id: -1
  unk_piece: &lt;unk&gt;
  bos_piece: &lt;s&gt;
  eos_piece: &lt;/s&gt;
  pad_piece: &lt;pad&gt;
  unk_surface:  ⁇ 
  enable_differential_privacy: 0
  differential_privacy_noise_level: 0
  differential_privacy_clipping_threshold: 0
}
normalizer_spec {
  name: nmt_nfkc
  add_dummy_prefix: 1
  remove_extra_whitespaces: 1
  escape_whitespaces: 1
  normalization_rule_tsv: 
}
denormalizer_spec {}
trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.
trainer_interface.cc(185) LOG(INFO) Loading corpus: example.txt
trainer_interface.cc(409) LOG(INFO) Loaded all 26 sentences
trainer_interface.cc(425) LOG(INFO) Adding meta_piece: &lt;unk&gt;
trainer_interface.cc(425) LOG(INFO) Adding meta_piece: &lt;s&gt;
trainer_interface.cc(425) LOG(INFO) Adding meta_piece: &lt;/s&gt;
trainer_interface.cc(430) LOG(INFO) Normalizing sentences...
trainer_interface.cc(539) LOG(INFO) all chars count=4533
trainer_interface.cc(550) LOG(INFO) Done: 99.9559% characters are covered.
trainer_interface.cc(560) LOG(INFO) Alphabet size=73
trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999559
trainer_interface.cc(592) LOG(INFO) Done! preprocessed 26 sentences.
trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 26
trainer_interface.cc(609) LOG(INFO) Done! 455
bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=99 min_freq=1
bpe_model_trainer.cc(268) LOG(INFO) Added: freq=31 size=20 all=732 active=658 piece=▁w
bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=40 all=937 active=863 piece=ch
bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=60 all=1014 active=940 piece=▁u
bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=80 all=1110 active=1036 piece=me
bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=100 all=1166 active=1092 piece=la
bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=0
bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=120 all=1217 active=1042 piece=SD
bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=140 all=1272 active=1097 piece=▁bu
bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=160 all=1288 active=1113 piece=▁site
bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=180 all=1315 active=1140 piece=ter
bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=200 all=1330 active=1155 piece=asure
bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=0
bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=220 all=1339 active=1008 piece=ge
bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=240 all=1371 active=1040 piece=▁sh
bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=260 all=1384 active=1053 piece=▁cost
bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=280 all=1391 active=1060 piece=de
bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=300 all=1405 active=1074 piece=000
bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=0
bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=320 all=1427 active=1021 piece=▁GB
bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=340 all=1438 active=1032 piece=last
bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=360 all=1441 active=1035 piece=▁let
trainer_interface.cc(687) LOG(INFO) Saving model: example_bpe.model
trainer_interface.cc(699) LOG(INFO) Saving vocabs: example_bpe.vocab</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>True</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>*** BPE ***
['▁B', 'e', 'ginn', 'ers', '▁BBQ', '▁Cl', 'ass', '▁T', 'ak', 'ing', '▁P', 'la', 'ce', '▁in', '▁M', 'is', 's', 'ou', 'la', '!']</code></pre>
</div>
</div>
<div id="ffb4d6b1" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb42" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1">show_vocab(sp_vocab, end <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">', '</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>▁B e g in n ers: 1, ▁BBQ: 3, ▁Cl ass: 2, ▁T ak ing: 1, ▁P la ce: 1, ▁in: 15, ▁M is s ou la !: 1, ▁D o: 1, ▁you: 13, ▁w an t: 1, ▁to: 33, ▁g et: 2, ▁be t ter: 2, ▁a t: 1, ▁mak ing: 2, ▁d e l ic i ou s: 1, ▁BBQ ?: 1, ▁ Y ou: 1, ▁will: 6, ▁have: 4, ▁the: 31, </code></pre>
</div>
</div>
<p>Our implementation of BPE’s code from the paper matches up pretty well with the library itself! Difference are probably accounted for by the <code>vocab_size</code>. There is also another technical difference in that in the SentencePiece implementation of BPE a priority queue is used to more efficiently keep track of the <em>best pairs</em>. Actually, there is a priority queue in the Python standard library called <code>heapq</code> if you would like to give that a try below!</p>
<div id="dbdb6be9" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb44" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> heapq <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> heappush, heappop</span></code></pre></div>
</div>
<div id="d2d093b7" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb45" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> heapsort(iterable):</span>
<span id="cb45-2">    h <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb45-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> value <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> iterable:</span>
<span id="cb45-4">        heappush(h, value)</span>
<span id="cb45-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> [heappop(h) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(h))]</span></code></pre></div>
</div>
<div id="fbed2471" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb46" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1">a <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>]</span>
<span id="cb46-2">heapsort(a)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<pre><code>[1, 1, 1, 2, 2, 3, 3, 4, 4]</code></pre>
</div>
</div>
<p>For a more extensive example consider looking at the <a href="https://github.com/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb">SentencePiece repo</a>. The last section of this code is repurposed from that tutorial. Thanks for your participation!</p>


</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2021,
  author = {Bochman, Oren},
  title = {SentencePiece and {Byte} {Pair} {Encoding}},
  date = {2021-04-11},
  url = {https://orenbochman.github.io/notes-nlp/notes/c4w3/lab01.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2021" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2021. <span>“SentencePiece and Byte Pair
Encoding.”</span> April 11, 2021. <a href="https://orenbochman.github.io/notes-nlp/notes/c4w3/lab01.html">https://orenbochman.github.io/notes-nlp/notes/c4w3/lab01.html</a>.
</div></div></section></div> ]]></description>
  <category>NLP</category>
  <category>Coursera</category>
  <category>Lab</category>
  <category>NLP with Attention Models</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/c4w3/lab01.html</guid>
  <pubDate>Sat, 10 Apr 2021 21:00:00 GMT</pubDate>
</item>
</channel>
</rss>
