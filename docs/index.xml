<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>NLP Course Notes &amp; Research</title>
<link>https://orenbochman.github.io/notes-nlp/</link>
<atom:link href="https://orenbochman.github.io/notes-nlp/index.xml" rel="self" type="application/rss+xml"/>
<description>Course and Research notes</description>
<image>
<url>https://orenbochman.github.io/notes-nlp/images/nlp-brain-wordcloud.jpg</url>
<title>NLP Course Notes &amp; Research</title>
<link>https://orenbochman.github.io/notes-nlp/</link>
</image>
<generator>quarto-1.6.39</generator>
<lastBuildDate>Wed, 12 Feb 2025 16:19:59 GMT</lastBuildDate>
<item>
  <title>Floating Constraints in Lexical Choice</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/1997-floating-contraints-in-lexical-choice/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/literature-review-open-book.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Literature review"><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="img-fluid figure-img" alt="Literature review"></a></p>
<figcaption>Literature review</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/DffGdrfY9gI" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid quarto-uncaptioned" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1
</figcaption>
</figure>
</div></div>
<section id="context" class="level2">
<h2 class="anchored" data-anchor-id="context">Context</h2>
<p>This is one of the three paper mentioned by Kathleen McKeown in her heroes of NLP interview by Andrew NG. The paper is about the floating constraints in lexical choice.</p>
<p>Looking quickly over it I noticed a couple of things:</p>
<!--
@article{mckeown1997floating,
  title={Floating constraints in lexical choice},
  author={McKeown, Kathleen and Elhadad, Michael and Robin, Jacques},
  year={1997}
}
-->
<ol type="1">
<li>I was familiar with Elhadad’s work on generation. I had read his work on FUF Functional Unification Formalism in the 90s which introduced me to the paradigm of unification before I even knew about Prolog or logic programming.</li>
<li>I found it fascinating that McKeown and Elhadad had worked together on this paper and even a bit curious about what they were looking into here.</li>
<li>Since McKeown brought it up, it might intimate that there is something worth looking into here. And it is discusing aspects of generative language models.</li>
</ol>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>Lexical choice is a computationally complex task, requiring a generation system to consider a potentially large number of mappings between concepts and words. Constraints that aid in determining which word is best come from a wide variety of sources, including syntax, semantics, pragmatics, the lexicon, and the underlying domain. Furthermore, in some situations, different constraints come into play early on, while in others, they apply much later. This makes it difficult to determine a systematic ordering in which to apply constraints. In this paper, we present a general approach to lexical choice that can handle multiple, interacting constraints. We focus on the problem of floating constraints, semantic or pragmatic constraints that float, appearing at a variety of different syntactic ranks, often merged with other semantic constraints. This means that multiple content units can be realized by a single surface element, and conversely, that a single content unit can be realized by a variety of surface elements. Our approach uses the Functional Unification Formalism (FuF) to represent a generation lexicon, allowing for declarative and compositional representation of individual constraints. – <span class="citation" data-cites="mckeown1997floating">(McKeown, Elhadad, and Robin 1997)</span></p>
</blockquote>
</section>
<section id="outline" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li>Introduction
<ul>
<li>Mentions the complexity of lexical choice in language generation.</li>
<li>Notes the diverse sources of constraints on lexical choice, including syntax, semantics, lexicon, domain, and pragmatics.</li>
<li>Highlights the challenges posed by floating constraints, which are semantic or pragmatic constraints that can appear at various syntactic ranks and be merged with other semantic constraints.</li>
<li>Presents examples of floating constraints in sentences.</li>
</ul></li>
<li>An Architecture for Lexical Choice
<ul>
<li>Discusses the role of lexical choice within a typical language generation architecture.</li>
<li>Argues for placing the lexical choice module between the content planner and the surface sentence generator.</li>
<li>Describes the input and output of the lexical choice module, emphasizing the need for language-independent conceptual input.</li>
<li>Presents the two tasks of lexical choice: syntagmatic<sup>1</sup> decisions (determining thematic structure) and paradigmatic<sup>2</sup> decisions (choosing specific words).</li>
<li>Introduces the FUF/SURGE package as the implementation environment.</li>
</ul></li>
<li>Phrase Planning in Lexical Choice
<ul>
<li>Describes the need for phrase planning to articulate multiple semantic relations into a coherent linguistic structure.</li>
<li>Explains how the Lexical Chooser selects the head relation and builds its argument structure based on focus and perspective.</li>
<li>Details how remaining relations are attached as modifiers, considering options like embedding, subordination, and syntactic forms of modifiers.</li>
</ul></li>
<li>Cross-ranking and Merged Realizations
<ul>
<li>Discusses the non-isomorphism between semantic and syntactic structures, necessitating merging and cross-ranking realizations.</li>
<li>Provides examples of merging (multiple semantic elements realized by a single word) and cross-ranking (single semantic element realized at different syntactic ranks).</li>
<li>Emphasizes the need for linguistically neutral input to support this flexibility.</li>
<li>Explains the implementation of merging and cross-ranking using FUF, highlighting the challenges of search and the use of bk-class for efficient backtracking.</li>
</ul></li>
<li>Interlexical Constraints
<ul>
<li>Defines interlexical constraints as arising from alternative sets of collocations for realizing pairs of content units.</li>
<li>Presents an example of interlexical constraints with verbs and nouns in the basketball domain.</li>
<li>Discusses different strategies for encoding interlexical constraints in the Lexical Chooser.</li>
<li>Introduces the :wait mechanism in FUF to delay the choice of one collocate until the other is chosen, preserving modularity and avoiding backtracking.</li>
</ul></li>
<li>Other Approaches to Lexical Choice
<ul>
<li>Reviews other systems using FUF for lexical choice, comparing their architectures and features to ADVISOR-II.</li>
<li>Discusses non-FUF-based systems, categorizing them by their positioning of lexical choice in the generation process and analyzing their handling of various constraints.</li>
</ul></li>
<li>Conclusion
<ul>
<li>Summarizes the contributions of the paper, highlighting the ability to handle a wide range of constraints, the use of FUF for declarative representation and interaction, and the algorithm for lexical selection and syntactic structure building.</li>
<li>Emphasizes the importance of syntagmatic choice and perspective selection for handling floating constraints.</li>
<li>Notes the use of lazy evaluation and dependency-directed backtracking for computational efficiency.</li>
<li>Mentions future directions, such as developing domain-independent lexicon modules and methods for organizing large-scale lexica.</li>
</ul></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;of or denoting the relationship between two or more linguistic units used sequentially to make well-formed structures. The combination of ‘that handsome man + ate + some chicken’ forms a syntagmatic relationship. If the word position is changed, it also changes the meaning of the sentence, eg ’Some chicken ate the handsome man</p></div><div id="fn2"><p><sup>2</sup>&nbsp;Paradigmatic relation describes the relationship between words that can be substituted for words with the same word class (eg replacing a noun with another noun)</p></div></div></section>
<section id="reflection" class="level2">
<h2 class="anchored" data-anchor-id="reflection">Reflection</h2>
<ul>
<li>In unification-based approaches, any word is a candidate unless it is ruled out by a constraint. With an LLM a word is selected based on its probability.</li>
<li>The idea of constraints is deeply embedded in Linguistics. The two type that immediately come to mind are:
<ul>
<li>Subcategorization constraints which are rules that govern the selection of complements and adjuncts. These are syntactic constraints.</li>
<li>Selectional constraints which are rules that govern the selection of arguments. These are semantic constraints</li>
</ul></li>
<li>Other constraints that I did not have to deal with as often are pragmatics and these are resolved from the state of the world or common sense knowledge.</li>
<li>In FUF there are many more constraints and if care isn’t taken, the system can become over constrained and no words will be selected.</li>
<li>One Neat aspect of the approach though is that FUF language could be merged with a query to generate a text.</li>
<li>Unfortunately as fat as I recall the FUF had to be specified by hand. The only people who could do this were the people who designed the system. They needed to be expert at linguistics and logic programming. Even I was pretty sharp at the time the linguistics being referred was far too challenging. I learned that in reality every computational linguist worth their salt had their own theory of language and their own version of grammar and that FUF was just one of many.</li>
<li>It seems that today attention mechanisms within LSTMs or Transformers are capable of learning a pragmatic formalism without a linguist stepping in to specify it.</li>
</ul>
<p>Why this is still interesting:</p>
<p>Imagine we want to build a low resource capable language model.</p>
<p>We would really like it to be good with grammar Also we would like it to have a lexicon that is as rich as the person it is interacting with. And we would also like to make available to it the common sense knowledge that it would need to use in the context of current and possibly a profile based on past conversations…. On the other hand we don’t want to require a 70 billion parameter model to do this. So what do we do?</p>
<p>Let imagine we start by training on Wikipedia, after all wikipedia’s mission is to become the sum of all human knowledge. By do we realy need all of the information in Wikipedia in out edge model?</p>
<p>The answer is no but the real question is how can we partition the training information and the wights etc so that we have a powerful model with a basic grammar and lexicon but which can load a knowledge graph, extra lexicons and common sense knowledge as needed.</p>
<p>So I had this idea from lookin at a <span class="citation" data-cites="ouhalla1999introducing">Ouhalla (1999)</span>. It pointed out there were phrase boundries and that if the sentence was transformed in a number of ways t would still make sense if we did this with respect to a phrase but if we used cucnks that were too small or too big the transofrmation would create malformed sentences. The operations of intrerest were movement and deletion.</p>
<p>and that can still have a model that is both small but able to access this extra material.</p>
<p>One direction seems to me is that we want to apply queries to all the training material as we train the model. Based on the queries that we retrieve each clause or phrase we can decide where we want to learn it and if we want to learn it at all.</p>
<p>(If it is common sense knowledge we may already know it. If it is semi structured we may want to put it into wikidata. If it is neither we may want to avoid learning it at all. We may still want to use it for improving the grammar and lexicon.) However we may want to store it in a speclised lexicon for domains like medicinee or law.</p>
<p>We could also decide if and how to eliminate it from we want to The queries should match each bit of information in the sentence. These are our queries. While the facts may change form</p>
<ol type="1">
<li>say we want to decompose a wikipedia article</li>
</ol>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-mckeown1997floating" class="csl-entry">
McKeown, Kathleen, Michael Elhadad, and Jacques Robin. 1997. <span>“Floating Constraints in Lexical Choice.”</span>
</div>
<div id="ref-ouhalla1999introducing" class="csl-entry">
Ouhalla, J. 1999. <em>Introducing Transformational Grammar: From Principles and Parameters to Minimalism</em>. A Hodder Arnold Publication. Arnold. <a href="https://www.google.com/books?id=ZP-ZQ0lKI9QC">https://www.google.com/books?id=ZP-ZQ0lKI9QC</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Floating {Constraints} in {Lexical} {Choice}},
  date = {2025-02-12},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/1997-floating-contraints-in-lexical-choice/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Floating Constraints in Lexical
Choice.”</span> February 12, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/1997-floating-contraints-in-lexical-choice/">https://orenbochman.github.io/notes-nlp/reviews/paper/1997-floating-contraints-in-lexical-choice/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/1997-floating-contraints-in-lexical-choice/</guid>
  <pubDate>Wed, 12 Feb 2025 16:19:59 GMT</pubDate>
</item>
<item>
  <title>Summarization Task</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/posts/2025-02-10-summerizer/</link>
  <description><![CDATA[ 





<section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p>In the Deeplearning.ai NLP specialization, we implemented both <strong>Q&amp;A</strong> and <strong>Summarization tasks</strong>. However, we only looked at these tasks in terms of the attention models. It is now time to dive deeper into the summarization task. In this article I will review a talk covering a body of research on summarization, which has many ideas about features. Consider some of these and see what aspects are relevant to a modern task implementation.</p>
<p>Some of the tasks here can be done with a simple LSTM or Transformer. However it seems that RL may be useful if we want to plan the summary in a non-linear fashion or to make use of a dynamic programming approach. For instance to identify the narrative structure might require many more steps then we typically use in a summerization task. For a speeches we may want to capture a gist of the rhetoric.</p>
</section>
<section id="ideas" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="ideas">Ideas</h2>
<p>Several ideas surfaced while working on the assignment for building a hybrid generative/abstractive summarizer based on GPT2<sup>1</sup>. Many ideas came from prior work developing search engines. I had noticed that some issues were anathema to summarization from its inception.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;a Generative Pre-training Transformer or a decoder transformer</p></div></div><section id="coverage-ranking-content-by-importance" class="level3">
<h3 class="anchored" data-anchor-id="coverage-ranking-content-by-importance">Coverage ranking content by importance</h3>
<p>The first issue is <strong>coverage</strong>, which is how much of the original document to include. Coverage is a much more difficult problem than it appears.</p>
<p>Consider summerizing a long novel a single paragraph summary may provide a good idea of the main plot points of the story. On the other hand summerising a similar sized encyclopedia might require listing all the top level headings and subheadings i.e.&nbsp;a one paragraph summary would not be able to provide a good idea of the content of the document. On the other hand it should be much easier to generate a summary for the encyclopedia than for the novel as the encyclopedia has a highly structured document with clear headings and subheadings and writing conventions that provide cues to the reader about the importance of the content. Summerizing a novel is more challanging as we would actualy want to ommit alsmost all the details and so deciding which parts to keep is a major challange with a very sparse signal.</p>
<p>For example, technical documents, like patents, headings, academic writing cues, and even IR statistics, may serve as features that we may feed into a regression that can guide us in discarding the chuff from the grain. On the other hand, for movie scripts or novels, we can’t use all that, and we need to decide what is essential based on the narrative structure, which requires sophisticated processing and extensive domain knowledge to extract. So, When we want to create a synopsis of a book like War and Peace, specific details are part of the narrative structure that stands out as essential to us, and I can say that even in 2025, LLM is not good at picking these out of a large document. For attentive readers with a suitable background, if we double the length of the text, they might pick additional plot points and then less significant subplots. If again we double we would, they would include more details concerning characters, their motivations, etc.</p>
<p>To conclude, different documents can have radically different structures and cues. These suggest different strategies for selecting and ranking the material to be included in a summary of a given length. More so, when we consider summaries of different lengths, one can see that we are talking about a hierarchal structure.</p>
</section>
</section>
<section id="avoiding-repetition" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="avoiding-repetition">Avoiding repetition</h2>
<p>A second challenge that is pervasive is avoiding <strong>repetition</strong>. In the generative settings we have a tougher challenge since sentences that are generated may well be equivalent to sentences we have already generated, and we want the model to redirect it attention from material already covered. Luckily we have learned to detect similar sentences in the Q&amp;A task<sup>2</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;using Siamese networks for oneshot similarity detection.</p></div></div><p>check if it is similar to any sentence in the summary</p>
<p>Alg1: similarity detection</p>
<pre><code>summary = []
tokenized_doc = tokenize(doc)
embeddings_doc= embed(tokenized_doc)
while len(summary) &lt; doc_tokens * summary_ratio: 
    a = gen_a_sentence(embeddings_doc,summary)
    for s in summary:
        if sim(a,s) &gt; threshold:
            continue
    else:
        summary.append(a)
s -&gt; gen a sentence,</code></pre>
</section>
<section id="context-window-constraints" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="context-window-constraints">Context window constraints</h2>
<p>The third challenge is technical and is related to the <strong>length of the context window</strong>. as transformer models have a limit on the number of tokens that they can process in a context window. A second issue here is that the summary itself needs to also fit in the context window. If the structure is linear and incremental we can chunk it into many smaller pieced say using sections. However in reality we are often dealing with trees of graphs structures and chunking can erode the model’s ability to inspect said hierarchy of the structure it is tasked with summarizing. In Q&amp;A or IR tasks since we can process each chunk separately and then combine the results.</p>
<p>If we are not using a generative model one imagines a tree data structure that can efficiently track what what part of the document has been summarized. If we can use that to work with the attention mechanism we may be able to reduce the effective window size as we progress with the summary. A more nuanced idea would come from <strong>bayesian search</strong> where we may want to keep moving the probability mass for the attention mechanism to regions that are significant but have not been covered.</p>
<p>Another challenge is that we may want to grow our summary in a non-linear fashion. We may consider the main narrative structure then add in the subplots and then the character motivations. This suggests two approaches - a top down <strong>planning</strong> based approach<sup>3</sup> and a bottom up approach where we start with the most important details, then add in the less important ones. In the second case we may want to revise the initial sentences to efficiently incorporate more details as we progress.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;using rl or dynamic programming</p></div><div id="fn4"><p><sup>4</sup>&nbsp;reformer layers</p></div></div><p>I also interviewed with a company that told me they often worked with patents and that these documents were frequently in excess of a hundred pages and they were having lots of problems with the summarization task. This suggest that an efficient transformer<sup>4</sup> would be needed if we are to support context windows that can span hundreds of pages. But that besides the context window we need some good way to ensure that the summary is balanced and that it is not too repetitive.</p>
</section>
<section id="compassion" class="level2">
<h2 class="anchored" data-anchor-id="compassion">Compassion</h2>
<p>My experience with summarization by hand is that most texts can be compressed by 5-10% without losing any information simply by rephrasing with more concise language. So another idea that should be obvious is that the generated summary should be <strong>compressive</strong>, i.e., once we generate a good sentence we should consider if we can make it shorter. This should be fairly easy as we are reducing the length of a single sentence. We may however be able to do even better by considering a section and trying to see if we can merge two sentences or if two sentences have partial overlap that can be merged or referenced via an anaphora. This is another area where we diverge from Q&amp;A, where we do not have a length constraint and may often want to include all relevant information. Implementation of this idea cam be easily embodied in a loss function that penalizes summaries for each token or character. This same idea can also be used in a RL summarizer.</p>
</section>
<section id="grounding" class="level2">
<h2 class="anchored" data-anchor-id="grounding">Grounding</h2>
<p>Generative summarization is a case where we can require that all generated text be grounded in the original document. In reality generative models have many glitches, due to tokenization issues, or failures of the attention mechanism, or out of distribution generation, due to bias learned from the corpus, due to negative recency bias and and due to emergent contradictions (where a contradiction is introduced into the context window and cannot be removed.) And even if we can avoid all these there is still nothing in the model that makes it prefer truthy generations. So it seems essential that the generated text is grounded in the original document. This seems to be verifiable by visually inspecting the using the attention mechanism. In reality there may be multiple heads and multiple layers. This means we need to access the attention mechanism programmatically and store annotations from the source document for each generated token. We may also want to make abstracting summarization explicit. c.f. [Extrinsic Hallucinations in LLMs in ](https://lilianweng.github.io/posts/2024-07-07-hallucination/) by Lilian Weng</p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Summarization {Task}},
  date = {2025-02-10},
  url = {https://orenbochman.github.io/notes-nlp/posts/2025-02-10-summerizer/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Summarization Task.”</span> February 10,
2025. <a href="https://orenbochman.github.io/notes-nlp/posts/2025-02-10-summerizer/">https://orenbochman.github.io/notes-nlp/posts/2025-02-10-summerizer/</a>.
</div></div></section></div> ]]></description>
  <category>NLP</category>
  <category>Notes</category>
  <category>Literature review</category>
  <category>Summarization task</category>
  <category>Relevance</category>
  <guid>https://orenbochman.github.io/notes-nlp/posts/2025-02-10-summerizer/</guid>
  <pubDate>Sun, 09 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Morphological Word Embeddings</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/2019-morphological-embeddings/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/literature-review-open-book.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Litrature review"><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="img-fluid figure-img" alt="Litrature review"></a></p>
<figcaption>Litrature review</figcaption>
</figure>
</div></div><p>This is a mentioned in the tokenization lab in course four week 3</p>
<p>Time permitting I will try and dive deeper into this paper.</p>
<section id="podcast" class="level2">
<h2 class="anchored" data-anchor-id="podcast">Podcast</h2>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>Linguistic similarity is multi-faceted. For instance, two words may be similar with respect to semantics, syntax, or morphology inter alia. Continuous word-embeddings have been shown to capture most of these shades of similarity to some degree. This work considers guiding word-embeddings with morphologically annotated data, a form of semisupervised learning, encouraging the vectors to encode a word’s morphology, i.e., words close in the embedded space share morphological features. We extend the log-bilinear model to this end and show that indeed our learned embeddings achieve this, using German as a case study. – <span class="citation" data-cites="cotterell2019morphological">(Cotterell and Schütze 2019)</span></p>
</blockquote>
<!-- TODO: add review and podcast for this paper-->
<!-- TODO: link here from LSH lessons-->
</section>
<section id="terminology" class="level2">
<h2 class="anchored" data-anchor-id="terminology">Terminology</h2>
<p>Here are some terms and concepts discussed in the sources, with explanations:</p>
<dl>
<dt>Word embeddings</dt>
<dd>
These are vector representations of words in a continuous space, where similar words are located close to each other in the vector space. The goal is to capture linguistic similarity, but the definition of “similarity” can vary (semantic, syntactic, morphological). Word embeddings are typically trained to produce representations that capture linguistic similarity.
</dd>
<dt>Log-bilinear model (LBL)</dt>
<dd>
This is a language model that learns features along with weights, as opposed to using hand-crafted features. It uses context to predict the next word, and word embeddings fall out as low dimensional representations of the context. The LBL is a generalization of the log-linear model.
</dd>
<dt>Morphology</dt>
<dd>
The study of word structure, including how words are formed from morphemes (the smallest units of meaning).
</dd>
<dt>Morphological tags</dt>
<dd>
These are annotations that describe the morphological properties of a word, such as case, gender, number, tense, etc.. They can be very detailed for morphologically rich languages. For example, in German, a word might be tagged to indicate its part of speech, case, gender, and number.
</dd>
<dt>Morphologically rich languages</dt>
<dd>
Languages with a high morpheme-per-word ratio, where word-internal structure is important for processing. German and Czech are cited as examples.
</dd>
<dt>Morphologically impoverished languages</dt>
<dd>
Languages with a low morpheme-per-word ratio, such as English.
</dd>
<dt>Multi-task objective</dt>
<dd>
Training a model to perform multiple tasks simultaneously. In this case, the model is trained to predict both the next word and its morphological tag.
</dd>
<dt>Semi-supervised learning</dt>
<dd>
Training a model on a partially annotated corpus, using both labeled and unlabeled data. This approach is useful when large amounts of unannotated text are available.
</dd>
<dt>Contextual signature</dt>
<dd>
The words surrounding a given word. The context in which a word appears can be used to determine the word’s meaning and morphological properties.
</dd>
<dt>Hamming distance</dt>
<dd>
A measure of the difference between two binary strings, calculated as the number of positions at which the corresponding bits are different. In this context, it is used to compare morphological tags, which are represented as bit vectors.
</dd>
<dt>MORPHOSIM</dt>
<dd>
A metric for evaluating morphologically-driven embeddings that measures how morphologically similar a word is to its nearest neighbors in the embedding space. It is calculated as the average Hamming distance between morphological tags of a word and its neighbors. Lower values are better, as they indicate that the nearest neighbors of each word are closer morphologically.
</dd>
</dl>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li>Introduction
<ul>
<li>Presents the importance of capturing word morphology, especially for morphologically-rich languages.</li>
<li>Highlights the multifaceted nature of linguistic similarity (semantics, syntax, morphology).</li>
<li>Discusses the goal of the paper: to develop word embeddings that specifically encode morphological relationships.</li>
</ul></li>
<li>Related Work
<ul>
<li>Discusses previous integration of morphology into language models, including factored language models and neural network-based approaches.</li>
<li>Notes the role of morphology in computational morphology, particularly in morphological tagging for inflectionally-rich languages.</li>
<li>Highlights the significance of distributional similarity in morphological analysis.</li>
</ul></li>
<li>Log-Bilinear Model
<ul>
<li>Describes the log-bilinear model (LBL), a generalization of the log-linear model with learned features.</li>
<li>Presents the LBL’s energy function and probability distribution in the context of language modeling.</li>
</ul></li>
<li>Morph-LBL
<ul>
<li>Proposes a multi-task objective that jointly predicts the next word and its morphological tag.</li>
<li>Describes the model’s joint probability distribution, incorporating morphological tag features.</li>
<li>Discusses the use of semi-supervised learning, allowing training on partially annotated corpora</li>
</ul></li>
<li>Evaluation
<ul>
<li>Mentions the qualitative evaluation using t-SNE, showing clusters reflecting morphological and POS relationships.</li>
<li>Introduces MorphoSim, a novel quantitative metric to assess the extent to which similar embeddings are morphologically related.</li>
</ul></li>
<li>Experiments and Results
<ul>
<li>Presents experiments on the German TIGER corpus, comparing Morph-LBL with the original LBL and Word2Vec.</li>
<li>Describes two experiments:
<ul>
<li>Experiment 1: Evaluates the morphological information encoded in embeddings using a k-NN classifier.</li>
<li>Experiment 2: Compares models using the MorphoSim metric to measure morphological similarity among nearest neighbors.</li>
</ul></li>
<li>Discusses the results, highlighting Morph-LBL’s superior performance in capturing morphological relationships, even without observing all tags during training.</li>
</ul></li>
<li>Conclusion and Future Work
<ul>
<li>Summarizes the contributions of the paper: introducing Morph-LBL for inducing morphologically guided embeddings.</li>
<li>Notes the model’s success in leveraging distributional signatures to capture morphology.</li>
<li>Discusses future work on integrating orthographic features for further improvement.</li>
<li>Mentions potential applications in morphological tagging and other NLP tasks.</li>
</ul></li>
</ul>
</section>
<section id="log-bilinear-model" class="level2">
<h2 class="anchored" data-anchor-id="log-bilinear-model">Log-Bilinear Model</h2>
<p><img src="https://latex.codecogs.com/png.latex?p(w%5Cmid%20h)%20=%0A%5Cfrac%7B%5Cexp%5Cleft(s_%5Ctheta(w,h)%5Cright)%7D%7B%5Csum_%7Bw'%7D%0A%5Cexp%5Cleft(s_%5Ctheta(w',h)%5Cright)%7D%20%5Cqquad%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?w"> is a word, <img src="https://latex.codecogs.com/png.latex?h"> is a history and <img src="https://latex.codecogs.com/png.latex?s_%5Ctheta"> is an energy function. Following the notation of , in the LBL we define <img src="https://latex.codecogs.com/png.latex?%0As_%5Ctheta(w,h)%20=%20%5Cleft(%5Csum_%7Bi=1%7D%5E%7Bn-1%7D%20C_i%0Ar_%7Bh_i%7D%5Cright)%5ET%20q_w%20+%20b_w%20%5Cqquad%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?n-1"> is the history length</p>
</section>
<section id="morph-lbl" class="level2">
<h2 class="anchored" data-anchor-id="morph-lbl">Morph-LBL</h2>
<p><img src="https://latex.codecogs.com/png.latex?%0A%20%20p(w,%20t%20%5Cmid%20h)%20%5Cpropto%20%5Cexp((%20f_t%5ET%20S%20%20+%20%5Csum_%7Bi=1%7D%5E%7Bn-1%7DC_i%20r_%7Bh_i%7D)%5ET%20q_w%20+%20b_%7Bw%7D%20)%20%5Cqquad%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?f_t"> is a hand-crafted feature vector for a morphological tag <img src="https://latex.codecogs.com/png.latex?t"> and <img src="https://latex.codecogs.com/png.latex?S"> is an additional weight matrix.</p>
<p>Upon inspection, we see that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap(t%20%5Cmid%20w,h)%20%5Cpropto%20%5Cexp(S%5ET%20f_t%20q_w)%20%5Cqquad%0A"></p>
<p>Hence given a fixed embedding <img src="https://latex.codecogs.com/png.latex?q_w"> for word <img src="https://latex.codecogs.com/png.latex?w">, we can interpret <img src="https://latex.codecogs.com/png.latex?S"> as the weights of a conditional log-linear model used to predict the tag <img src="https://latex.codecogs.com/png.latex?t">.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<ul>
<li><a href="http://www.eva.mpg.de/lingua/resources/glossing-rules.php">Leipzig Glossing Rules</a> which provides a standard way to explain morphological features by examples</li>
<li><a href="https://www.youtube.com/watch?v=y9sVFrmGu0w&amp;ab_channel=GrahamNeubig">CMU Multilingual NLP 2020 (17): Morphological Analysis and Inflection</a> <!--
- [nb-lm](https://notebooklm.google.com/notebook/f594ff01-19f2-49b0-a0ac-84176fb22667?_gl=1*1rba7bx*_ga*MzAyOTc3ODMwLjE3Mzg1MDQ5Njc.*_ga_W0LDH41ZCB*MTczODkyMzY5Ni41LjAuMTczODkyMzY5Ni42MC4wLjA.)
--></li>
<li><span class="citation" data-cites="kann-etal-2016-neural">Kann, Cotterell, and Schütze (2016)</span> <a href="https://github.com/ryancotterell/neural-canonical-segmentation">code</a></li>
<li><a href="https://unimorph.github.io/">unimorph</a> univorsal morphological database</li>
</ul>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-cotterell2019morphological" class="csl-entry">
Cotterell, Ryan, and Hinrich Schütze. 2019. <span>“Morphological Word Embeddings.”</span> <em>arXiv Preprint arXiv:1907.02423</em>.
</div>
<div id="ref-kann-etal-2016-neural" class="csl-entry">
Kann, Katharina, Ryan Cotterell, and Hinrich Schütze. 2016. <span>“Neural Morphological Analysis: Encoding-Decoding Canonical Segments.”</span> In <em>Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</em>, edited by Jian Su, Kevin Duh, and Xavier Carreras, 961–67. Austin, Texas: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/D16-1097">https://doi.org/10.18653/v1/D16-1097</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Morphological {Word} {Embeddings}},
  date = {2025-02-07},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/2019-morphological-embeddings/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Morphological Word Embeddings.”</span>
February 7, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/2019-morphological-embeddings/">https://orenbochman.github.io/notes-nlp/reviews/paper/2019-morphological-embeddings/</a>.
</div></div></section></div> ]]></description>
  <category>Paper</category>
  <category>Review</category>
  <category>Podcast</category>
  <category>NLP</category>
  <category>Morphology</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/2019-morphological-embeddings/</guid>
  <pubDate>Thu, 06 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Practical and Optimal LSH for Angular Distance</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/2015-LSH/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/literature-review-open-book.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Literature review"><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="img-fluid figure-img" alt="Literature review"></a></p>
<figcaption>Literature review</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/t_8SpFV0l7A" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Locality-Sensitive Hashing and Beyond
</figcaption>
</figure>
</div><div id="vid-02" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/Dkomk2wPaoc" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;2: Beyond Locality Sensitive Hashing; Alexandr Andoni
</figcaption>
</figure>
</div></div>

<p>In the NLP specialization we have covered and used LSH a number of times in at least two courses. <!-- TODO: link to the notes so as to make them more useful as this is a stub!!!! --> In one sense I have an understanding of what is going on when we implement this. In another sense this seems to be quite confusing. So I don’t fully understand some aspects of LSH.</p>
<p>One way to do better is to try and explain it to someone else. This is what I am trying to do here by going back to the source and trying to understand the paper, problems, motivation and finally isolate what it is that is hard to grasp.</p>
<p>This paper is a good introduction to LSH for the angular distance.</p>
<p>In <span class="citation" data-cites="andoni2015practical">(Andoni et al. 2015)</span> the authors delve into Locality-Sensitive Hashing (LSH) for the angular distance. They show the existence of an LSH family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal running time exponent. Unlike earlier algorithms with this property (e.g., Spherical LSH), their algorithm is also practical, improving upon the well-studied hyperplane LSH in practice.</p>
<p>For now the two videos which explain in some depth this an the related paper - <a href="https://arxiv.org/abs/1501.01062">Optimal Data-Dependent Hashing for Approximate Near Neighbors</a> will have to do.</p>
<p>Time permitting I will try and dive deeper into this paper.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR - Location Sensitive Hashing
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/in_a_nutshell.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Location Sensitive Hashing in a Nutshell"><img src="https://orenbochman.github.io/notes-nlp/images/in_a_nutshell.jpg" class="img-fluid figure-img" alt="Location Sensitive Hashing in a Nutshell"></a></p>
<figcaption>Location Sensitive Hashing in a Nutshell</figcaption>
</figure>
</div>
<ul>
<li>The paper introduces a Locality-Sensitive Hashing (LSH) family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal running time exponent.</li>
<li>The algorithm is also practical, improving upon the well-studied hyperplane LSH in practice.</li>
<li>The authors also introduce a multiprobe version of this algorithm, and conduct experimental evaluation on real and synthetic data sets.</li>
</ul>
</div>
</div>
<section id="podcast" class="level2">
<h2 class="anchored" data-anchor-id="podcast">Podcast</h2>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>We show the existence of a Locality-Sensitive Hashing (LSH) family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal running time exponent. Unlike earlier algorithms with this property (e.g., Spherical LSH [1 , 2]), our algorithm is also practical, improving upon the well-studied hyperplane LSH [3] in practice.</p>
<p>We also introduce a <strong>multiprobe</strong> version of this algorithm, and conduct experimental evaluation on real and synthetic data sets.</p>
<p>We complement the above positive results with a fine-grained lower bound for the quality of any LSH family for angular distance. Our lower bound implies that the above LSH family exhibits a trade-off between evaluation time and quality that is close to optimal for a natural class of LSH functions. – <span class="citation" data-cites="andoni2015practical">(Andoni et al. 2015)</span></p>
</blockquote>
<!-- TODO: add outline, reflection, and terminology and for this paper-->
<!-- TODO: link here from LSH lessons-->
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-andoni2015practical" class="csl-entry">
Andoni, Alexandr, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn, and Ludwig Schmidt. 2015. <span>“Practical and Optimal LSH for Angular Distance.”</span> <em>Advances in Neural Information Processing Systems</em> 28.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Practical and {Optimal} {LSH} for {Angular} {Distance}},
  date = {2025-02-05},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/2015-LSH/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Practical and Optimal LSH for Angular
Distance.”</span> February 5, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/2015-LSH/">https://orenbochman.github.io/notes-nlp/reviews/paper/2015-LSH/</a>.
</div></div></section></div> ]]></description>
  <category>Paper</category>
  <category>Review</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/2015-LSH/</guid>
  <pubDate>Tue, 04 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Finetuning Pretrained Transformers into RNNs</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/transforer-to-rnn/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./cover.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Litrature review"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/transforer-to-rnn/cover.jpg" class="img-fluid figure-img" alt="Litrature review"></a></p>
<figcaption>Litrature review</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/JMeYGYANEqU" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Transformer to RNN (T2RNN) Part-1 by Dr.&nbsp;Niraj Kumar
</figcaption>
</figure>
</div><div id="vid-02" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/UHgy2faOD_M" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;2: Transformer to RNN (T2RNN) Part-2 by Dr.&nbsp;Niraj Kumar
</figcaption>
</figure>
</div></div>

<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>Transformers have outperformed recurrent neural networks (RNNs) in natural language generation. But this comes with a significant computational cost, as the attention mechanism’s complexity scales quadratically with sequence length. Efficient transformer variants have received increasing interest in recent works. Among them, a linear-complexity recurrent variant has proven well suited for autoregressive generation. It approximates the softmax attention with randomized or heuristic feature maps, but can be difficult to train and may yield suboptimal accuracy. This work aims to convert a pretrained transformer into its efficient recurrent counterpart, improving efficiency while maintaining accuracy. Specifically, we propose a swap-then-finetune procedure: in an off-the-shelf pretrained transformer, we replace the softmax attention with its linear-complexity recurrent alternative and then finetune. With a learned feature map, our approach provides an improved tradeoff between efficiency and accuracy over the standard transformer and other recurrent variants. We also show that the finetuning process has lower training cost relative to training these recurrent variants from scratch. As many models for natural language tasks are increasingly dependent on large-scale pretrained transformers, this work presents a viable approach to improving inference efficiency without repeating the expensive pretraining process. – <span class="citation" data-cites="kasai2021finetuningpretrainedtransformersrnns">(Kasai et al. 2021)</span></p>
</blockquote>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-kasai2021finetuningpretrainedtransformersrnns" class="csl-entry">
Kasai, Jungo, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, and Noah A. Smith. 2021. <span>“Finetuning Pretrained Transformers into RNNs.”</span> <a href="https://arxiv.org/abs/2103.13076">https://arxiv.org/abs/2103.13076</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Finetuning {Pretrained} {Transformers} into {RNNs}},
  date = {2025-02-03},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/transforer-to-rnn/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Finetuning Pretrained Transformers into
RNNs.”</span> February 3, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/transforer-to-rnn/">https://orenbochman.github.io/notes-nlp/reviews/paper/transforer-to-rnn/</a>.
</div></div></section></div> ]]></description>
  <category>Paper</category>
  <category>Review</category>
  <category>LSTM</category>
  <category>RNN</category>
  <category>Transformer</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/transforer-to-rnn/</guid>
  <pubDate>Sun, 02 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>xLSTM: Extended Long Short-Term Memory</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="cover.jpg" class="lightbox" data-gallery="slides" title="Literature Review"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/cover.jpg" class="img-fluid figure-img" alt="Literature Review"></a></p>
<figcaption>Literature Review</figcaption>
</figure>
</div><div id="vid-02" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/0aWGTNS03PU?t=3" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Review by AI Bites
</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/0OaEv1a5jUM" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;2: Review by Yannic Kilcher
</figcaption>
</figure>
</div><div id="vid-03" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/8u2pW2zZLCs" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;3: LSTM: The Comeback Story? Talk with Sepp Hochreiter after the xLSTM paper
</figcaption>
</figure>
</div></div>


<blockquote class="blockquote">
<p>Whate’er the theme, the Maiden sang<br>
<mark>As if her song could have no ending;</mark><br>
I saw her singing at her work,<br>
And o’er the sickle bending;—<br>
I listened, motionless and still;<br>
And, as I mounted up the hill,<br>
<mark>The music in my heart I bore,<br>
Long after it was heard no more.</mark> &nbsp;</p>
<p>– The Solitary Reaper by William Wordsworth.</p>
</blockquote>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR – Scaling LSTMs to Billions of Parameters with <strong>xLSTM</strong>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/in_a_nutshell.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="LSTMs in a nutshell"><img src="https://orenbochman.github.io/notes-nlp/images/in_a_nutshell.jpg" class="img-fluid figure-img" alt="LSTMs in a nutshell"></a></p>
<figcaption>LSTMs in a nutshell</figcaption>
</figure>
</div>
<ul>
<li>Just when we think that the transformer rules supreme the RNN makes a comeback with a refreshing new look at the LSTMs.</li>
<li>This paper brings new architectures to the LSTM that are fully parallelizable and can scale to billions of parameters.</li>
<li>They demonstrate on synthetic tasks and language modeling benchmarks that these new xLSTMs can outperform state-of-the-art Transformers and State Space Models.</li>
</ul>
</div>
</div>
</div>
<section id="introduction" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Motivation
</div>
</div>
<div class="callout-body-container callout-body">
<p>So this is my first review of the RNN papers. I found that since the courses covering LSTM seemed to be over simplified and confusing, I should go back to the source. Since time is limited, it helps to review the papers in reverse chronological order. Each new papers explains the limitations that are generally not apparent when the paper is first published. The authors of the later papers also have the benefit of hindsight which they share in the obligatory literature review wherein they provide lots of context we cannot get by reading the original papers. Given all that there is also a benefit to reading the original paper - the real innovators while harder to understand are remarkable thinkers and often reveal motivations and deep insights that elude later interlocutors.</p>
</div>
</div>
<section id="sec-podcast" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-podcast">Podcast &amp; Other Reviews</h3>
<p>This paper is a bit heavy on the technical side. But if we are still here, we may enjoy this podcast reviewing the paper. Though AI generated I found it quite useful to get oriented with the paper and understand that the experiments the authors run to validate this new architectures.</p>
<p>We also cover something I found absent from the Course on sequence models . The limitations of LSTMs as viewed by the authors when considered in the context of transformers and Mamba.</p>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
<p>Although this paper is recent there are a number of other people who cover it.</p>
<ul>
<li>In Video&nbsp;2 Yannic Kilcher covers the paper in his youtube channel. He does a great job of explaining the paper in a way that is easy to understand. Kilcher is all too human, he makes many mistakes, stresses opinions over facts. If we don’t mind all that he is a great resource for understanding the paper. I find he rubs me the wrong way but at least I find this motivating to do better in my own reviews!</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
What can we expect from xLSTM?
</div>
</div>
<div class="callout-body-container callout-body">

<blockquote class="blockquote">
<p>xLSTM has the potential to considerably impact other fields like Reinforcement Learning, Time Series Prediction, or the modeling of physical systems. – <span class="citation" data-cites="beck2024xlstm">(Beck et al. 2024)</span></p>
</blockquote>
<p>In his book Understanding Media <span class="citation" data-cites="mcluhan1988understanding">(McLuhan 1988)</span> Marshal McLuhan introduced his <strong>Tetrad</strong>. <mark>The <strong>Tetrad</strong> is a mental model for understanding how a technological innovation like the xLSTM might disrupt society</mark>. To work the model we ask the following four questions that can be applied to any new technology to understand its impact. The four questions in this instance are:</p>
<ol type="1">
<li>What does the xLSTM enhance or amplify?</li>
</ol>
<blockquote class="blockquote">
<p>The xLSTM enhances the LSTM by introducing exponential gating and novel memory structures, boosting its capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models.</p>
</blockquote>
<ol start="2" type="1">
<li>What does the xLSTM make obsolete or replace?</li>
</ol>
<blockquote class="blockquote">
<p>The xLSTM makes the traditional LSTM obsolete by addressing its limitations, such as the inability to revise storage decisions, limited storage capacities, and lack of parallelizability.</p>
</blockquote>
<ol start="3" type="1">
<li>What does the xLSTM retrieve that was previously obsolesced?</li>
</ol>
<blockquote class="blockquote">
<p>The xLSTM retrieves the LSTM’s relevance in large-scale language modeling by scaling it to billions of parameters and integrating modern LLM techniques while mitigating known limitations. 4. What does the xLSTM reverse or flip into when pushed to its limits?</p>
</blockquote>
<blockquote class="blockquote">
<p>The xLSTM, when pushed to its limits, can compete with current Large Language Models based on Transformer technology, offering a competitive alternative for sequence modeling tasks.</p>
</blockquote>
<p>The xLSTM paper by <span class="citation" data-cites="beck2024xlstm">(Beck et al. 2024)</span> is a great example of the tetrad in action. The authors take the LSTM, a technology that has stood the test of time, and enhance it with exponential gating and novel memory structures. They address the limitations of traditional LSTMs and push the technology to new limits, creating a new architecture that can compete with state-of-the-art Transformers and State Space Models. Recent work by <span class="citation" data-cites="chen2024computationallimitsstatespacemodels">(Chen et al. 2024)</span> suggest that Transformers and The State Space Models are actually limited in their own ways.</p>
</div>
</div>
<div class="no-row-height column-margin column-container"><div id="sup-01" class="quarto-float quarto-figure quarto-figure-center anchored callout-margin-content">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="MediaTetrad.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Tetrad"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/MediaTetrad.svg" class="img-fluid figure-img"></a></p>
<figcaption>Tetrad</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div></section>
</section>
<section id="abstract" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining:</p>
<ol type="1">
<li>sLSTM with a scalar memory, a scalar update, and new memory mixing,<br>
</li>
<li>mLSTM that is fully parallelizable with a matrix memory and a covariance update rule.<br>
</li>
</ol>
<p>Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling. — <span class="citation" data-cites="beck2024xlstm">(Beck et al. 2024)</span></p>
</blockquote>

<div class="no-row-height column-margin column-container"><div id="fig-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="fig01.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="architecture"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/fig01.png" class="img-fluid figure-img"></a></p>
<figcaption>architecture</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: The extended LSTM (xLSTM) family. From left to right:<br>
1. The original LSTM memory cell with constant error carousel and gating.<br>
2. New <strong>sLSTM</strong> and <strong>mLSTM</strong> memory cells that introduce exponential gating. sLSTM offers a new memory mixing technique. <strong>mLSTM</strong> is fully parallelizable with a novel matrix memory cell state and new covariance update rule.<br>
3. mLSTM and sLSTM in residual blocks yield xLSTM blocks.<br>
4. Stacked xLSTM blocks give an xLSTM
</figcaption>
</figure>
</div><div id="fig-02" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="fig02.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="blocks"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/fig02.png" class="img-fluid figure-img"></a></p>
<figcaption>blocks</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: LSTM limitations.<br>
- Left: Nearest Neighbor Search problem in terms of mean squared error (MSE). Given a reference vector, a sequence is scanned sequentially for the most similar vector with the objective to return its attached value at sequence end. LSTM struggles to revise a stored value when a more similar vector is found. Our new xLSTM overcomes this limitation by exponential gating. - Right: Rare Token Prediction. The perplexity (PPL) of token prediction on Wikitext-103, in partitions of token frequency. LSTM performs worse on predicting rare tokens because of its limited storage capacities, whereas our new xLSTM solves this problem via a matrix memory.
</figcaption>
</figure>
</div><div id="fig-03" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="fig03.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="blocks"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/fig03.png" class="img-fluid figure-img"></a></p>
<figcaption>blocks</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: xLSTM blocks.<br>
Left: A residual sLSTM block with post up-projection (like Transformers): The input is fed into an sLSTM — with an optional convolution — followed by a gated MLP.<br>
Right: A residual mLSTM block with pre up-projection (like State Space models): mLSTM is wrapped inside two MLPs, via a convolution, a learnable skip connection, and an output gate that acts componentwise.
</figcaption>
</figure>
</div><div id="fig-04" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-04-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="fig04.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="blocks"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/fig04.png" class="img-fluid figure-img"></a></p>
<figcaption>blocks</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-04-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: xLSTM blocks.<br>
Left: A residual sLSTM block with post up-projection (like Transformers): The input is fed into an sLSTM — with an optional convolution — followed by a gated MLP.<br>
Right: A residual mLSTM block with pre up-projection (like State Space models): mLSTM is wrapped inside two MLPs, via a convolution, a learnable skip connection, and an output gate that acts componentwise.
</figcaption>
</figure>
</div><div id="fig-05" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-05-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="fig05.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="blocks"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/fig05.png" class="img-fluid figure-img"></a></p>
<figcaption>blocks</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-05-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: xLSTM blocks.<br>
Left: A residual sLSTM block with post up-projection (like Transformers): The input is fed into an sLSTM — with an optional convolution — followed by a gated MLP.<br>
Right: A residual mLSTM block with pre up-projection (like State Space models): mLSTM is wrapped inside two MLPs, via a convolution, a learnable skip connection, and an output gate that acts componentwise.
</figcaption>
</figure>
</div></div>



</section>
<section id="sec-outline" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-outline">Paper Outline</h2>
<ul>
<li>Introduction
<ul>
<li>Describes the constant error carousel and gating mechanisms of Long Short-Term Memory (LSTM). &gt; “In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories…”</li>
<li>Discusses three main limitations of LSTMs:
<ol type="1">
<li>The inability to revise storage decisions.</li>
<li>Limited storage capacities.</li>
<li>Lack of parallelizability due to memory mixing.</li>
</ol></li>
<li>Highlights <mark>the emergence of Transformers in language modeling due to these limitations</mark>. &gt; Transformer Era: The rise of Transformers with parallelizable self-attention has largely overshadowed LSTMs due to their superior scalability.</li>
</ul></li>
<li>Extended Long Short-Term Memory
<ul>
<li>Introduces Extended Long Short-Term Memory (xLSTM), which incorporates exponential gating and novel memory structures.</li>
<li>Presents two new LSTM variants:
<ol type="1">
<li>sLSTM with a scalar memory, a scalar update, and new memory mixing.</li>
<li>mLSTM with a matrix memory and a covariance update rule, which is fully parallelizable.</li>
</ol></li>
<li>Describes the integration of these LSTM variants into residual block modules, resulting in xLSTM blocks.</li>
<li>Presents xLSTM architectures that residually stack xLSTM blocks.</li>
</ul></li>
<li>Related Work:
<ul>
<li>Mentions various linear attention methods to overcome the quadratic complexity of Transformer attention.</li>
<li>Notes the popularity of State Space Models (SSMs) for language modeling.</li>
<li>Highlights the use of Recurrent Neural Networks (RNNs) as a replacement for Transformers.</li>
<li>Mentions the use of <strong>gating</strong> in recent RNN and SSM approaches.</li>
<li>Notes the use of <strong>covariance update rules</strong><sup>1</sup> to enhance storage capacities in various models.</li>
</ul></li>
<li>Experiments
<ul>
<li>Presents the experimental evaluation of xLSTM compared to existing methods with a focus on language modeling.</li>
<li>Discusses the effectiveness of xLSTM on synthetic tasks, including:
<ul>
<li>Formal languages.</li>
<li>Multi-Query Associative Recall.</li>
<li>Long Range Arena tasks.</li>
</ul></li>
<li>Presents a method comparison and ablation study, where different models are trained on 15 billion tokens from the SlimPajama dataset.</li>
<li>Assesses the scaling behavior of different methods based on validation perplexity.</li>
<li>Conducts a large-scale language modeling experiment:
<ul>
<li>Training different model sizes on 300B tokens from SlimPajama.</li>
<li>Evaluating models on length extrapolation.</li>
<li>Assessing models on validation perplexity and performance on downstream tasks.</li>
<li>Evaluating models on 571 text domains of the PALOMA language benchmark dataset.</li>
<li>Assessing the scaling behavior with increased training data.</li>
</ul></li>
</ul></li>
<li>Limitations
<ul>
<li>Highlights limitations of xLSTM, including:
<ul>
<li>Lack of parallelizability for sLSTM due to memory mixing.</li>
<li>Unoptimized CUDA kernels for mLSTM.</li>
<li>High computational complexity of mLSTM’s matrix memory.</li>
<li>Memory overload for longer contexts due to the independence of mLSTM’s memory from sequence length.</li>
</ul></li>
</ul></li>
<li>Conclusion
<ul>
<li>Concludes that xLSTM performs favorably in language modeling compared to Transformers and State Space Models.</li>
<li>Suggests that larger xLSTM models will be competitive with current Large Language Models based on Transformer technology.</li>
<li>Notes the potential impact of xLSTM on other deep learning fields such as reinforcement learning, time series prediction, and physical system modeling.</li>
</ul></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;explain covariance</p></div></div></section>
<section id="briefing-xlstm---extended-long-short-term-memory" class="level2">
<h2 class="anchored" data-anchor-id="briefing-xlstm---extended-long-short-term-memory">Briefing : xLSTM - Extended Long Short-Term Memory</h2>
<section id="introduction-and-motivation" class="level3">
<h3 class="anchored" data-anchor-id="introduction-and-motivation">Introduction and Motivation:</h3>
<p><strong>LSTM’s Legacy</strong>: The Long Short-Term Memory (LSTM), introduced in the 1990s, has been a cornerstone of deep learning, especially in sequence modeling and early Large Language Models (LLMs). LSTMs are known for their “constant error carousel” and gating mechanisms, designed to combat vanishing gradients in recurrent neural networks.</p>
<blockquote class="blockquote">
<p>“In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories…”</p>
</blockquote>
<p>Transformer Era: The rise of Transformers with parallelizable self-attention has largely overshadowed LSTMs due to their superior scalability.</p>
<blockquote class="blockquote">
<p>“However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale.” <strong>xLSTM Question</strong>: This research aims to explore the capabilities of LSTMs when scaled to billions of parameters, by integrating modern LLM techniques while addressing known limitations.</p>
</blockquote>
<blockquote class="blockquote">
<p>“We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs?”</p>
</blockquote>
</section>
<section id="key-limitations-of-traditional-lstms" class="level3">
<h3 class="anchored" data-anchor-id="key-limitations-of-traditional-lstms">Key Limitations of Traditional LSTMs:</h3>
<p>The paper identifies three major limitations of traditional LSTMs:</p>
<ol type="1">
<li>Inability to Revise Storage Decisions: LSTMs struggle to update previously stored information when more relevant information arrives later in the sequence. The paper uses a “Nearest Neighbor Search” task to demonstrate this.</li>
</ol>
<blockquote class="blockquote">
<p>“LSTM struggles to revise a stored value when a more similar vector is found…”</p>
</blockquote>
<ol start="2" type="1">
<li>Limited Storage Capacity: LSTMs store information in scalar cell states, which can lead to information compression and loss. This impacts performance on tasks requiring detailed memory, like predicting rare tokens.</li>
</ol>
<blockquote class="blockquote">
<p>“LSTM performs worse on predicting rare tokens because of its limited storage capacities…”</p>
</blockquote>
<ol start="3" type="1">
<li>Lack of Parallelizability: LSTMs process data sequentially due to the recurrent connections between hidden states, hindering parallel computation.</li>
</ol>
<blockquote class="blockquote">
<p>“Lack of parallelizability due to memory mixing, i.e., the hidden-hidden connections between hidden states from one time step to the next, which enforce sequential processing.”</p>
</blockquote>
<ol start="3" type="1">
<li>xLSTM Innovations:</li>
</ol>
<p>The paper introduces Extended Long Short-Term Memory (xLSTM), designed to address the above limitations, by modifying the original LSTM using two main approaches:</p>
<p><strong>Exponential Gating</strong>:Implemented to allow for better revision of storage decisions. Normalization and stabilization techniques are used to avoid overflows in calculations due to the exponential function.</p>
<blockquote class="blockquote">
<p>“To empower LSTMs with the ability to revise storage decisions, we introduce exponential gates (red) together with normalization and stabilization.” Novel Memory Structures: Two new LSTM variants are created with these structures:</p>
</blockquote>
<p><strong>sLSTM</strong>: A scalar memory LSTM with a scalar update and new memory mixing technique (within heads, not across). Features multiple memory cells and multiple heads, with memory mixing within each head.</p>
<blockquote class="blockquote">
<p>“The new sLSTM (see Section 2.2) with a scalar memory, a scalar update, and memory mixing…”</p>
</blockquote>
<p><strong>mLSTM</strong>: A matrix memory LSTM, fully parallelizable, with a covariance update rule for storing key-value pairs. Does not have memory mixing between cells or heads.</p>
<blockquote class="blockquote">
<p>“the new mLSTM (see Section 2.3) with a matrix memory and a covariance (outer product) update rule, which is fully parallelizable.”</p>
</blockquote>
<ol start="4" type="1">
<li>sLSTM Details:</li>
</ol>
<ul>
<li><strong>Exponential Gates</strong>: Introduces exponential activation functions for input and forget gates with normalization and a stabilizer state to prevent overflow.</li>
<li><strong>Normalizer State</strong>: A state used to normalize the hidden state output by accumulating a weighted sum of input and forget gates.</li>
<li><strong>Memory Mixing</strong>: sLSTM allows memory mixing between cells, and also introduces a new form of memory mixing through multiple heads, mixing between cells within a head, but not across heads.</li>
</ul>
<blockquote class="blockquote">
<p>“The new sLSTM can have multiple heads with memory mixing within each head but not across heads. The introduction of heads for sLSTM together with exponential gating establishes a new way of memory mixing.”</p>
</blockquote>
<ol start="5" type="1">
<li>mLSTM Details:</li>
</ol>
<p><strong>Matrix Memory</strong>: Replaces the scalar cell state with a matrix, increasing storage capacity.</p>
<p><strong>Key-Value Storage</strong>: The mLSTM memory is based around a key-value storage system inspired by Bidirectional Associative Memories (BAMs) using covariance updates, where keys and values are updated via an outer product.</p>
<blockquote class="blockquote">
<p>“At time <img src="https://latex.codecogs.com/png.latex?t">, we want to store a pair of vectors, the key <img src="https://latex.codecogs.com/png.latex?k_t%20%E2%88%88%20R%5Ed"> and the value <img src="https://latex.codecogs.com/png.latex?v_t%20%E2%88%88%20R%5Ed">… The covariance update rule… for storing a key-value pair…”</p>
</blockquote>
<p>Parallelization: mLSTM is fully parallelizable due to abandoning hidden-hidden recurrent connections and memory mixing. Computations can be performed efficiently on GPUs using standard matrix operations.</p>
<blockquote class="blockquote">
<p>“…the mLSTM… which is fully parallelizable.”</p>
</blockquote>
<ol start="6" type="1">
<li>xLSTM Architecture:</li>
</ol>
<p><strong>xLSTM Blocks</strong>: The sLSTM and mLSTM variants are integrated into residual blocks.</p>
<p><strong>sLSTM blocks</strong> use post up-projection (like Transformers).</p>
<p><strong>mLSTM blocks</strong> use pre up-projection (like State Space Models).</p>
<blockquote class="blockquote">
<p>“Integrating these new LSTM variants into residual block modules results in xLSTM blocks…”</p>
</blockquote>
<p><strong>Residual Stacking</strong>: xLSTM architectures are created by residually stacking these blocks, using pre-LayerNorm backbones similar to current LLMs.</p>
<blockquote class="blockquote">
<p>“An xLSTM architecture is constructed by residually stacking build-ing blocks…” <strong>Cover’s Theorem</strong>: Used to justify mapping information into a higher-dimensional space for better separation of histories or contexts.</p>
</blockquote>
<blockquote class="blockquote">
<p>“We resort to Cover’s Theorem (Cover, 1965), which states that in a higher dimensional space non-linearly embedded patterns can more likely be linearly separated than in the original space.”</p>
</blockquote>
<ol start="7" type="1">
<li>Performance and Scaling:</li>
</ol>
<p><strong>Linear Computation &amp; Constant Memory</strong>: Unlike Transformers, xLSTM networks have linear computation and constant memory complexity with respect to sequence length.</p>
<blockquote class="blockquote">
<p>“Contrary to Transformers, xLSTM networks have a linear computation and a constant memory complexity with respect to the sequence length.”</p>
</blockquote>
<p><strong>Synthetic Tasks</strong>: xLSTM showed improved performance over regular LSTMs on “Nearest Neighbor Search” and “Rare Token Prediction” synthetic tasks.</p>
<p><strong>SlimPajama Experiments</strong>: The xLSTM models were trained on 15B and 300B tokens from the SlimPajama dataset for language modelling evaluation.</p>
<p><strong>Competitive Performance</strong>: xLSTMs demonstrate competitive performance against state-of-the-art Transformers and State Space Models on language modelling benchmarks, both in terms of next token prediction perplexity and downstream tasks.</p>
<blockquote class="blockquote">
<p>“Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.”</p>
</blockquote>
<p>Ablation studies show importance of gating techniques.</p>
<ol start="8" type="1">
<li>Memory &amp; Speed:</li>
</ol>
<p><strong>sLSTM</strong>: Not parallelizable due to memory mixing, although the authors have created a fast CUDA implementation (less than twice as slow as mLSTM).</p>
<blockquote class="blockquote">
<p>“While mLSTM is parallelizable analog to FlashAttention… sLSTM is not parallelizable due to the memory mixing…”</p>
</blockquote>
<p><strong>mLSTM</strong>: Parallelizable, but currently slower than FlashAttention or Mamba due to non-optimized CUDA kernels. Matrix memory is computationally complex but does not require parameters and can be parallelized.</p>
<blockquote class="blockquote">
<p>“The memory of mLSTM does not require parameters but is computationally expensive through its d×d matrix memory and d× d update… the computations can be done in parallel on GPUs, therefore these computations have only a minor effect on the wall clock time.”</p>
</blockquote>
<ol start="9" type="1">
<li>Limitations:</li>
</ol>
<p><strong>sLSTM Parallelization</strong>: sLSTM’s memory mixing is non-parallelizable.</p>
<blockquote class="blockquote">
<p>“(i) In contrast to mLSTM, memory mixing of the sLSTM prohibits parallelizable operations, and therefore does not allow a fast parallel implementation.”</p>
</blockquote>
<p>mLSTM CUDA Optimization: The mLSTM CUDA kernels are not fully optimized.</p>
<blockquote class="blockquote">
<p>“(ii) The CUDA kernels for mLSTM are not optimized, and therefore the current implementation is about 4 times slower than FlashAttention or the scan used in Mamba.”</p>
</blockquote>
<p><strong>mLSTM Matrix Memory</strong>: High computational complexity for mLSTM due to matrix memory operations.</p>
<p><strong>Forget Gate Initialization</strong>: Careful initialization of the forget gates is needed.</p>
<p><strong>Long Context Memory</strong>: The matrix memory is independent of sequence length, and might overload memory for long context sizes.</p>
<blockquote class="blockquote">
<p>“(v) Since the matrix memory is independent of the sequence length, increasing the sequence length might overload the memory for longer context sizes.”</p>
</blockquote>
<p><strong>Hyperparameter Optimization</strong>: xLSTM is not yet fully optimized due to computational load, indicating further performance gains are possible.</p>
<blockquote class="blockquote">
<p>“(vi) Due to the expensive computational load for large language experiments, we did neither fully optimize the architecture nor the hyperparameters, especially for larger xLSTM architectures.”</p>
</blockquote>
<ol start="10" type="1">
<li>Related Work:</li>
</ol>
<p>The paper highlights connections of its ideas with the following areas:</p>
<p><strong>Gating</strong>: Many recent models such as HGRN, GLA, Gated State Space models, Mamba, and others also utilize gating mechanisms. Covariance Update Rule: The covariance update rule used in mLSTM is related to other methods such as Fast Weight Programmers, Retention, and Linear Transformers.</p>
<ol start="11" type="1">
<li>Conclusion:</li>
</ol>
<p>The xLSTM presents a compelling alternative to Transformers in sequence modeling, especially in contexts where memory efficiency and longer sequences are required. By combining insights from the original LSTM with exponential gating and novel memory structures, the paper shows that LSTMs can be competitive at scale. While limitations exist in terms of parallelization and hyperparameter tuning, the research shows promising potential for further development. The authors posit further research and improvements could allow xLSTM to reach its full potential.</p>
</section>
</section>
<section id="my-thoughts" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="my-thoughts">My Thoughts</h2>

<div class="no-row-height column-margin column-container"><div id="vid-sigmoid" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-sigmoid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/TPqr8t919YM" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-sigmoid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;4: Review of the sigmoid function
</figcaption>
</figure>
</div></div><div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Research questions
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li>How does the constant error carousel mitigate the vanishing gradient problem in the LSTM?
<ul>
<li>The constant error carousel is a way to keep the error constant over time. This helps to prevent the vanishing gradient problem in the LSTM.</li>
</ul></li>
<li>How are the gates in the original LSTM binary?
<ul>
<li>Sigmoid, saturation and a threshold at 0.5</li>
</ul></li>
<li>What is the long term memory in the LSTM?
<ul>
<li>the cell state <img src="https://latex.codecogs.com/png.latex?c_%7Bt-1%7D"></li>
</ul></li>
<li>What is the short term memory in the LSTM?
<ul>
<li>the hidden state <img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D"></li>
</ul></li>
<li>How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs</li>
</ol>
</div>
</div>

<div class="no-row-height column-margin column-container"><div id="sup-02" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="sigmoid-limits.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Supplementary Figure&nbsp;2: limits of the sigmoid function"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/sigmoid-limits.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup" id="sup-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;2: limits of the sigmoid function
</figcaption>
</figure>
</div><div id="sup-03" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="sigmoid-values.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="bias"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/sigmoid-values.png" class="img-fluid figure-img"></a></p>
<figcaption>bias</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup" id="sup-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;3: The inductive bias of the sigmoid decision function.
</figcaption>
</figure>
</div></div>
<section id="binary-nature-of-non-exponential-gating-mechanisms" class="level3">
<h3 class="anchored" data-anchor-id="binary-nature-of-non-exponential-gating-mechanisms">Binary nature of non exponential Gating Mechanisms</h3>
<p>If we try to understand why are the gating mechanisms in the original LSTM is described here as binary?</p>
<p>It helps to the properties of the sigmoid functions that are explained in Video&nbsp;4.</p>
<ol type="1">
<li><p>Supplementary Figure&nbsp;1 shows that The sigmoid function has a domain of <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D"> and a range of <img src="https://latex.codecogs.com/png.latex?(0,1)">. This means that the sigmoid function can only output values between 0 and 1.</p></li>
<li><p>It has a very sharp gradient around 0 if the bias is set to 0. This means that the sigmoid function acts as a binary function around 0.</p></li>
<li><p>The Sigmoid function tends to saturate. Now sigmoid function around 0 isn’t quite quite binary but outside of (-6,+6) it becomes saturated and acts very much as a binary function.</p></li>
<li><p>If we use it as a loss function or a decision function, the sigmoid function is used with a threshold of 0.5. This means that if the output of the sigmoid function is greater than 0.5, the decision is 1, and if it is less than 0.5, the decision is 0. That is how the sigmoid function can become binary.</p></li>
<li><p>Note that we can use thresholding during inference to make the decision binary and we can keep them as continuous values during training so we can have a smooth gradient for back-propagation.</p></li>
<li><p>Even without thresholding the sigmoid function can become binary this is due to the saturation of the sigmoid function. I recall that Hinton in his Coursera course on Neural Networks and Deep Learning mentioned that activation function that become saturated are problematic. I can say that at last I think I understand what he meant:<br>
I see that that sigmoid functions can fall off the manifold of the data and this can be a problem. This is why the ReLU function is used in the Gated Recurrent Unit (GRU) and the Transformer. <strong>Falling off the manifold of the data</strong> means that the sigmoid function becomes saturated (above 6 and below -6). At this point the back-propagation steps are no longer able to change the weight or bias since the gradient is effectively so learning stops.</p></li>
</ol>
<p>This becomes a is a problem in two senses. The sigmoid unit is likely to be stuck in a setting that only representative of part of the training data. The rest of the network which is not stuck now has to learn to compensate for this.</p>
<p>This issues may well be the root cause LSTM limitation number 1 - the inability to revise storage decisions. It seems that the authors decided to use the exponential gating function to address this issues.</p>
<p>This is why the gating mechanisms in the original LSTM are binary. The sigmoid function is used to control the flow of information in the LSTM by deciding which information to keep and which to discard. The sigmoid function is used to decide how much of the new input information should be added to the cell state and how much of the old cell state should be retained. This binary nature of the sigmoid function is what allows the LSTM to control the flow of information and decide what to remember and what to forget.</p>
</section>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li><p><a href="https://openreview.net/forum?id=ARAxPPIAhq&amp;noteId=gra7vHnb0q">The paper on open review</a> has some additional insights from the authors</p></li>
<li><p><a href="https://www.ai-bites.net/xlstm-extended-long-short-term-memory-networks/">XLSTM — Extended Long Short-Term Memory Networks</a> By Shrinivasan Sankar — May 20, 2024</p></li>
</ul>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-beck2024xlstm" class="csl-entry">
Beck, Maximilian, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, Günter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. 2024. <span>“xLSTM: Extended Long Short-Term Memory.”</span> <em>arXiv Preprint arXiv:2405.04517</em>.
</div>
<div id="ref-chen2024computationallimitsstatespacemodels" class="csl-entry">
Chen, Yifang, Xiaoyu Li, Yingyu Liang, Zhenmei Shi, and Zhao Song. 2024. <span>“The Computational Limits of State-Space Models and Mamba via the Lens of Circuit Complexity.”</span> <a href="https://arxiv.org/abs/2412.06148">https://arxiv.org/abs/2412.06148</a>.
</div>
<div id="ref-mcluhan1988understanding" class="csl-entry">
McLuhan, Marshall. 1988. <em>Understanding Media : The Extensions of Man</em>. New York: New American Library. <a href="http://www.worldcat.org/search?qt=worldcat_org_all&amp;q=0451624963">http://www.worldcat.org/search?qt=worldcat_org_all&amp;q=0451624963</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {xLSTM: {Extended} {Long} {Short-Term} {Memory}},
  date = {2025-02-03},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“xLSTM: Extended Long Short-Term
Memory.”</span> February 3, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/">https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/</a>.
</div></div></section></div> ]]></description>
  <category>Paper</category>
  <category>Review</category>
  <category>NLP</category>
  <category>LSTM</category>
  <category>Seq2Seqs</category>
  <category>RNN</category>
  <category>Podcast</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/</guid>
  <pubDate>Sun, 02 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>The Secret Life of Pronouns What Our Words Say About Us</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/book/seret-life-of-pronouns/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="cover.jpg" class="lightbox" data-gallery="slides" title="cover"><img src="https://orenbochman.github.io/notes-nlp/reviews/book/seret-life-of-pronouns/cover.jpg" class="img-fluid figure-img" alt="cover"></a></p>
<figcaption>cover</figcaption>
</figure>
</div></div><div class="page-columns page-full"><blockquote class="blockquote">
<p>To grunt and sweat under a weary life, But that the dread of something after death, <mark>The undiscovered country from whose bourn No traveler returns, puzzles the will, And makes us rather bear those ills we have, Than fly to others that we know not of</mark>?<sup>1</sup> — Hamlet Act 3, Scene 1 by William Shakespeare.</p>
</blockquote><div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Pronouns oft fall prey to the stop word filter, yet they hold the keys to unlocking the depth of intimate meaning</p></div></div></div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR – ML, NLP and the secret life of pronouns
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/in_a_nutshell.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Pronouns in a nutshell"><img src="https://orenbochman.github.io/notes-nlp/images/in_a_nutshell.jpg" class="img-fluid figure-img" alt="Pronouns in a nutshell"></a></p>
<figcaption>Pronouns in a nutshell</figcaption>
</figure>
</div>
<ul>
<li>When I got started with NLP I was coding search engines and conventional wisdom was <mark>pronouns don’t pass the <strong>stop word filter</strong></mark></li>
<li>One of the gems I learned on that job was that <mark>everything in the corpus can be provide invaluable information if we only know how to index it</mark>.</li>
<li>After reading this book and I started to unlocked the power of the pronouns.</li>
<li>At Hungarian School I discovered how pronouns combine with case ending to create a vast vistas of untapped NLP resource.</li>
<li>Cognitive AI I noted that pronouns and particles are key in extending the primitive verb system via thematic roles to get a very rich semantic systems in the lexicon with little costs in learning.</li>
</ul>
</div>
</div>
</div>
<section id="podcast" class="level2">
<h2 class="anchored" data-anchor-id="podcast">Podcast</h2>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The secret life of pronouns in context
</div>
</div>
<div class="callout-body-container callout-body">
<p>I got interested in this book when I was deep into wikipedia research. I was in a place where I wanted to be able to attribute content to the authors. I had read some disturbing articles with prior work on the topic by some researchers from my Alma Mater. However, I eventually came across this book and it seemed to put the ideas in a better form. Less about ML and more about the type of writers and their personalities.</p>
</div>
</div>
<p>In <span class="citation" data-cites="pennebaker2013secret">(Pennebaker 2013)</span> “The Secret Life of Pronouns: What Our Words Say About Us,” the author explores how the seemingly innocuous function words we use—pronouns, articles, prepositions, and auxiliary verbs—can reveal a great deal about our personalities, emotions, social connections, and psychological states. James W. Pennebaker, the author and a social psychologist, uses computer-based text analysis methods to uncover these hidden patterns.</p>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li><p><strong>Key Questions and Themes:</strong></p>
<ul>
<li><strong>Can language reveal psychological states?</strong> The author is primarily interested in what people’s words say about their psychological states. The book investigates whether language features can reveal how people are thinking and whether this information can be used to change their thinking.</li>
<li><strong>How do function words differ from content words?</strong> The book distinguishes between content and style of speech, arguing that function words, which reflect language style, reveal more about a person than content words. Function words are tied to the relationship between speaker and listener.</li>
<li><strong>Do men and women use words differently?</strong> The book explores gender differences in language use, noting that women are typically more attentive to the thoughts and feelings of others, while men are less so. It also investigates whether different writing styles reflect different personalities.</li>
<li><strong>Can language predict behavior?</strong> The book examines whether language use can predict behaviors such as success in college, risk of suicide, leadership styles, and likelihood of declaring war.</li>
<li><strong>How can language be used as a tool for change?</strong> The author examines whether analyzing words can lead to improvements in relationships, teaching, and leadership. Additionally, the book looks at how writing about emotional upheavals can improve mental and physical health, and whether using certain types of words can influence the effectiveness of that writing.</li>
<li><strong>Can language reveal deception?</strong> The book explores linguistic markers of deception, such as the use of first-person singular pronouns and the mention of other individuals.</li>
<li><strong>Can language analysis help identify authors?</strong> The book presents methods for identifying authors using function words, punctuation, and obscure words.</li>
</ul></li>
<li><p><strong>Main Examples and Studies:</strong></p>
<ul>
<li><strong>Expressive Writing:</strong> Early research focused on the benefits of writing about traumatic experiences, leading to the development of the LIWC (Linguistic Inquiry and Word Count) program. This program helps analyze word categories associated with healthy writing such as positive emotion words, cognitive words, and the ability to construct a story.</li>
<li><strong>The Bottle and the Two People Pictures:</strong> Participants described pictures, and their descriptions were analyzed for function word use and content. For example, people who focused on shadows in the bottle picture tended to be more creative. The second picture revealed common themes such as social connections, clothing, and accessories.</li>
<li><strong>Thinking Styles:</strong> The book describes how people’s function words can be statistically grouped into categories reflecting thinking styles, such as formal, analytic, and narrative. These thinking styles can predict how people relate to others and organize their worlds.</li>
<li><strong>9/11 Blog Analysis:</strong> The language used in blog posts was analyzed before and after the 9/11 attacks. Researchers noted an increase in negative emotion words, a decrease in positive emotion words, and a temporary increase in cognitive words.</li>
<li><strong>College Admissions Essays:</strong> The study examined whether the writing style in college admissions essays could predict college grades.</li>
<li><strong>The Federalist Papers:</strong> The book discusses the use of function words to determine the authorship of the anonymous Federalist Papers.</li>
<li><strong>Language Style Matching (LSM):</strong> LSM is a technique to determine how similarly two people use function words. It is used in studies of couples, speed dating, and even the writings of historical figures to analyze the relationship between the authors.</li>
<li><strong>Obama’s Pronoun Use</strong>: The book analyzes Barack Obama’s use of first-person singular pronouns. His relatively low use of I-words is interpreted as a sign of self-confidence.</li>
</ul></li>
<li><p><strong>Additional Insights:</strong></p>
<ul>
<li><strong>Stealth Words:</strong> The book emphasizes the importance of function words, often called “stealth words,” which are often overlooked.</li>
<li><strong>The Role of Computers:</strong> Computer technology has revolutionized the study of language, allowing researchers to analyze large datasets and uncover hidden patterns that would be impossible to see with manual analysis.</li>
<li><strong>Language as a Tool:</strong> Language can be used not only as a window into the inner workings of people, but also as a tool for guiding thinking and promoting change.</li>
<li><strong>Interdisciplinary Approach</strong>: The author’s work draws on multiple disciplines including psychology, linguistics, computer science, history, and political science.</li>
</ul></li>
</ul>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-pennebaker2013secret" class="csl-entry">
Pennebaker, J. W. 2013. <em>The Secret Life of Pronouns: What Our Words Say about Us</em>. Bloomsbury USA. <a href="https://books.google.co.il/books?id=p9KmCAAAQBAJ">https://books.google.co.il/books?id=p9KmCAAAQBAJ</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {The {Secret} {Life} of {Pronouns} {What} {Our} {Words} {Say}
    {About} {Us}},
  date = {2025-01-30},
  url = {https://orenbochman.github.io/notes-nlp/reviews/book/seret-life-of-pronouns/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“The Secret Life of Pronouns What Our Words
Say About Us.”</span> January 30, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/book/seret-life-of-pronouns/">https://orenbochman.github.io/notes-nlp/reviews/book/seret-life-of-pronouns/</a>.
</div></div></section></div> ]]></description>
  <category>Review</category>
  <category>Book</category>
  <category>NLP</category>
  <category>Sentiment Analysis</category>
  <category>Sentiment Analysis</category>
  <category>Podcast</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/book/seret-life-of-pronouns/</guid>
  <pubDate>Wed, 29 Jan 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>When and Why are Pre-trained Word Embeddings Useful for Neural Machine Translation?</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/20180-PTWM-NMT/</link>
  <description><![CDATA[ 





<div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/o41avjKIg5o" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Talk covering this paper by Roee Aharoni
</figcaption>
</figure>
</div>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/literature-review-open-book.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Litrature review"><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="img-fluid figure-img" alt="Litrature review"></a></p>
<figcaption>Litrature review</figcaption>
</figure>
</div></div><div id="sup-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://roeeaharoni.com/NMT_slides_orginal.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Roee Aharoni’s slides"><embed src="https://roeeaharoni.com/NMT_slides_orginal.pdf" class="img-fluid"></a></p>
<figcaption>Roee Aharoni’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div>
<section id="podcast" class="level2">
<h2 class="anchored" data-anchor-id="podcast">Podcast</h2>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>The performance of Neural Machine Translation (NMT) systems often suffers in lowresource scenarios where sufficiently largescale parallel corpora cannot be obtained. Pretrained word embeddings have proven to be invaluable for improving performance in natural language analysis tasks, which often suffer from paucity of data. However, their utility for NMT has not been extensively explored. In this work, we perform five sets of experiments that analyze when we can expect pre-trained word embeddings to help in NMT tasks. We show that such embeddings can be surprisingly effective in some cases – providing gains of up to 20 BLEU points in the most favorable setting.. –<span class="citation" data-cites="qi-etal-2018-pre">(Qi et al. 2018)</span></p>
</blockquote>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li>**Introduction
<ul>
<li>Describes the problem of low-resource scenarios in Neural Machine Translation (NMT) and the potential utility of pre-trained word embeddings.</li>
<li>Highlights the success of pre-trained embeddings in natural language analysis tasks and the lack of extensive exploration in NMT.</li>
<li>Poses five researcb questions:
<ul>
<li>Q1 Is the behavior of pre-training affected by language families and other linguistic features of source and target languages? (§3)</li>
<li>Q2 Do pre-trained embeddings help more when the size of the training data is small? (§4)</li>
<li>Q3 How much does the similarity of the source and target languages affect the efficacy of using pre-trained embeddings? (§5)</li>
<li>Q4 Is it helpful to align the embedding spaces between the source and target languages? (§6)</li>
<li>Q5 Do pre-trained embeddings help more in multilingual systems as compared to bilingual systems? (§7)</li>
</ul></li>
</ul></li>
<li>Experimental Setup
<ul>
<li>Details the five sets of experiments conducted to evaluate the effectiveness of pre-trained word embeddings in NMT.</li>
<li>Describes the datasets used, including the WMT14 English-German and English-French translation tasks.</li>
<li>Outlines the models and training procedures employed in the experiments.</li>
</ul></li>
<li>Results and Analysis
<ul>
<li>Presents the results of the experiments, showing the impact of pre-trained word embeddings on NMT performance.</li>
<li>Discusses the observed gains in BLEU scores and the factors influencing the effectiveness of pre-trained embeddings.</li>
<li>Analyzes the relationship between the quality of pre-trained embeddings and the performance of NMT systems.</li>
</ul></li>
<li>Analysis
<ul>
<li>Considers the implications of the findings for NMT research and practice.</li>
<li>Discusses the potential benefits and limitations of using pre-trained word embeddings in NMT tasks.</li>
</ul></li>
<li>Conclusion
<ul>
<li>The sweet-spot is wgere there is very little training data yet enough to train the system.</li>
<li>PTWE are more effective if there are more similar translation pairs.</li>
<li>A priori alignment of embeddings may not be necessary in bilingual scenarios, but is helpful in multi-lingual training scenarios.</li>
</ul></li>
</ul>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-qi-etal-2018-pre" class="csl-entry">
Qi, Ye, Devendra Sachan, Matthieu Felix, Sarguna Padmanabhan, and Graham Neubig. 2018. <span>“When and Why Are Pre-Trained Word Embeddings Useful for Neural Machine Translation?”</span> In <em>Proceedings of the 2018 Conference of the North <span>A</span>merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</em>, edited by Marilyn Walker, Heng Ji, and Amanda Stent, 529–35. New Orleans, Louisiana: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/N18-2084">https://doi.org/10.18653/v1/N18-2084</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {When and {Why} Are {Pre-trained} {Word} {Embeddings} {Useful}
    for {Neural} {Machine} {Translation?}},
  date = {2024-02-11},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/20180-PTWM-NMT/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“When and Why Are Pre-Trained Word Embeddings
Useful for Neural Machine Translation?”</span> February 11, 2024. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/20180-PTWM-NMT/">https://orenbochman.github.io/notes-nlp/reviews/paper/20180-PTWM-NMT/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Bidirectional LSTM</category>
  <category>Deep learning</category>
  <category>Embeddings</category>
  <category>LLM</category>
  <category>NLP</category>
  <category>Paper</category>
  <category>Review</category>
  <category>Podcast</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/20180-PTWM-NMT/</guid>
  <pubDate>Sat, 10 Feb 2024 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Neural Machine Translation by Jointly Learning to Align and Translate</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/2014-NMT-by-jointly-learning-to-align-and-translate/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/literature-review-open-book.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Litrature review"><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="img-fluid figure-img" alt="Litrature review"></a></p>
<figcaption>Litrature review</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/o41avjKIg5o" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Roee Aharoni’s Talk covering this paper (Hebrew)
</figcaption>
</figure>
</div><div id="sup-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://roeeaharoni.com/NMT_slides_orginal.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Roee Aharoni’s slides"><embed src="https://roeeaharoni.com/NMT_slides_orginal.pdf" class="img-fluid"></a></p>
<figcaption>Roee Aharoni’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div>

<section id="podcast" class="level2">
<h2 class="anchored" data-anchor-id="podcast">Podcast</h2>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
</section>
<section id="abstract" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<div class="page-columns page-full"><blockquote class="blockquote">
<p><em>Neural machine translation</em> is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of <em>encoder-decoders</em> and consists of an encoder that encodes a source sentence into a <em>fixed-length vector</em><sup>1</sup> from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition. –<span class="citation" data-cites="bahdanau2016neuralmachinetranslationjointly">(Bahdanau, Cho, and Bengio 2016)</span></p>
</blockquote><div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;the encoded state</p></div></div></div>
</section>
<section id="outline" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li>Introduction
<ul>
<li>Positions NMT as a new approach, contrasting it with traditional phrase-based systems.</li>
<li>Describes encoder-decoder models that use a fixed-length vector to encode a source sentence.</li>
<li>Explains that these models are <mark>trained to maximize the probability of a correct translation.</mark></li>
<li>Discusses the <mark>limitation of compressing all source sentence information into <em>a fixed-length vector</em><sup>2</sup>, especially for long sentences.</mark></li>
<li><mark>Introduces the extension of the encoder-decoder model that jointly learns to align and translate.</mark></li>
<li>Emphasizes that the model searches for relevant source positions when generating a target word.</li>
<li>States that the new model encodes the input sentence into a sequence of vectors and adaptively chooses a subset during decoding.</li>
<li>Asserts that the new model performs better with long sentences and that the proposed approach performs better translation compared to the basic encoder-decoder approach.</li>
<li>Notes that the improvement is apparent with longer sentences and that the model achieves comparable performance to a conventional phrase-based system.</li>
<li>Mentions that the model finds linguistically plausible alignments.</li>
</ul></li>
<li>Background: <em>Neural Machine Translation</em> (NMT)
<ul>
<li><mark>Defines translation as <em>maximizing</em> the <strong>conditional probability of a target sentence given a source sentence</strong>.</mark> <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Barg%7D%5C,%5Cmax_%7By%7D%5C,%20%7Bp%7D(y%20%5Cmid%20x)."></li>
<li>Explains that <mark>NMT models learn this conditional distribution using <em>parallel training corpora</em>.</mark></li>
<li>Describes NMT models as consisting of <em>an encoder and a decoder</em>.</li>
<li>Notes that <em>Recurrent Neural Networks</em> (RNNs) have been used for encoding and decoding <em>variable-length sentences</em>.</li>
<li>Points out that NMT has shown promising results, and can achieve state-of-the-art performance.</li>
<li>Notes that adding <em>neural networks</em> to existing systems can boost performance levels.</li>
<li>RNN Encoder-Decoder
<ul>
<li>Describes the <em>RNN Encoder-Decoder framework</em>, where an encoder reads the input sentence into a vector c.</li>
<li>Explains that the <em>encoder</em> uses an RNN to generate <em>a sequence of hidden states</em>, from which the vector c is generated.</li>
<li>Notes that the <em>decoder</em> predicts the next word given the context vector and previously predicted words.</li>
<li>Presents a formula for the conditional probability, which is modeled with an RNN.</li>
</ul></li>
</ul></li>
<li>Learning to Align and Translate
<ul>
<li>Introduces a new architecture for NMT using a bidirectional RNN encoder and a decoder that searches through the source sentence.</li>
<li>Decoder: General Description
<ul>
<li>Presents a new conditional probability conditioned on a distinct context vector for each target word.</li>
<li>Explains that the context vector depends on a sequence of annotations from the encoder.</li>
<li>Defines the context vector as a weighted sum of annotations.</li>
<li>Describes how the weights are computed, using an alignment model.</li>
<li>Emphasizes that the alignment model computes a soft alignment.</li>
<li>Explains the weighted sum of annotations as computing an expected annotation.</li>
<li>States that the probability of the annotation reflects its importance in deciding the next state and generating the target word.</li>
<li>Notes that this implements an attention mechanism in the decoder.</li>
</ul></li>
<li>Encoder: Bidirectional RNN for Annotating Sequences
<ul>
<li>Introduces the use of a bidirectional RNN (BiRNN) to summarize preceding and following words.</li>
<li>Describes the forward and backward RNNs that comprise the BiRNN.</li>
<li>Explains how annotations are created by concatenating forward and backward hidden states.</li>
<li>Notes that the annotation will focus on words around the current word due to the nature of RNNs.</li>
</ul></li>
</ul></li>
<li>Experiment Settings
<ul>
<li>States that the proposed approach is evaluated on English-to-French translation using ACL WMT ’14 bilingual corpora.</li>
<li>Compares the approach with an RNN Encoder-Decoder.</li>
<li>Dataset
<ul>
<li>Details the corpora used, their sizes, and the data selection method used.</li>
<li>Notes that no monolingual data other than the mentioned parallel corpora is used.</li>
<li>Describes how the development and test sets were created.</li>
<li>Details the tokenization and word shortlist used for training the models.</li>
</ul></li>
<li>Models
<ul>
<li>Details the two types of models trained, RNN Encoder-Decoder (RNNencdec) and RNNsearch, and the sentence lengths used.</li>
<li>Describes the hidden units in the encoder and decoder for both models.</li>
<li>Mentions the use of a multilayer network with a maxout hidden layer.</li>
<li>Describes the use of minibatch stochastic gradient descent (SGD) and Adadelta for training, along with the minibatch size and training time.</li>
<li>Explains the use of beam search to find the translation that maximizes the conditional probability.</li>
<li>Refers to the appendices for more details on the architectures and training procedure.</li>
</ul></li>
</ul></li>
<li>Results
<ul>
<li>Quantitative Results
<ul>
<li>Presents translation performance measured in BLEU scores, showing that RNNsearch outperforms RNNencdec in all cases.</li>
<li>Notes that the performance of RNNsearch is comparable to that of the phrase-based system, even without using a separate monolingual corpus.</li>
<li>Shows that RNNencdec’s performance decreases with longer sentences.</li>
<li>Demonstrates that RNNsearch is more robust to sentence length, with no performance drop even with sentences of length 50 or more.</li>
<li>Highlights the superiority of RNNsearch by noting that RNNsearch-30 outperforms RNNencdec-50.</li>
<li>Presents a table of BLEU scores for each model.</li>
</ul></li>
<li>Qualitative Analysis
<ul>
<li>Alignment
<ul>
<li>Explains that the approach offers an intuitive way to inspect the soft alignment between words.</li>
<li>Describes visualizing annotation weights to see which source positions were considered important.</li>
<li>Notes the largely monotonic alignment of words, with strong weights along the diagonal.</li>
<li>Highlights examples of non-trivial alignments and how the model correctly translates phrases.</li>
<li>Explains the strength of soft-alignment using an example of the phrase “the man” being translated to “l’ homme”.</li>
<li>Notes that soft alignment deals naturally with phrases of different lengths.</li>
</ul></li>
<li>Long Sentences
<ul>
<li>Explains that the model does not require encoding a long sentence into a fixed-length vector perfectly.</li>
<li>Provides examples of translations of long sentences, showing that RNNencdec deviates from the source meaning.</li>
<li>Demonstrates that RNNsearch translates long sentences correctly, preserving the whole meaning.</li>
<li>Confirms that RNNsearch enables more reliable translation of long sentences than RNNencdec.</li>
</ul></li>
</ul></li>
</ul></li>
<li>Related Work
<ul>
<li>Learning to Align
<ul>
<li>Mentions a similar alignment approach used in handwriting synthesis.</li>
<li>Notes the key difference: in handwriting synthesis, the modes of the weights of the annotations only move in one direction.</li>
<li>Explains that, in machine translation, reordering is often needed, and the proposed approach computes annotation weight of every source word for each target word.</li>
</ul></li>
<li>Neural Networks for Machine Translation
<ul>
<li>Discusses the history of neural networks in machine translation, from providing single features to existing systems to reranking candidate translations.</li>
<li>Mentions examples of neural networks being used as a sub-component of existing translation systems.</li>
<li>Highlights that the paper focuses on designing a completely new translation system based on neural networks.</li>
</ul></li>
</ul></li>
<li>Conclusion
<ul>
<li>States that the paper proposes a new architecture that extends the basic model by allowing the model to (soft-)search for input words when generating a target word.</li>
<li>Highlights the benefit of freeing the model from encoding the whole sentence into a fixed-length vector.</li>
<li>Notes that <strong>all components are jointly trained towards a better log-probability of producing correct translations</strong>.</li>
<li>Confirms that the proposed RNNsearch outperforms the conventional encoder-decoder model and is more robust to sentence length.</li>
<li>Concludes that the model aligns each target word with the relevant words in the source sentence.</li>
<li>Points out that the proposed approach achieved translation performance comparable to the phrase-based statistical machine translation, despite its recent development.</li>
<li>Suggests that the approach is a promising step toward better machine translation and a better understanding of natural languages.</li>
<li>Identifies better handling of unknown or rare words as a challenge for the future.</li>
</ul></li>
<li>Appendix A: Model Architecture
<ul>
<li>Architectural Choices
<ul>
<li>Describes that the scheme is a general framework where the activation functions of RNNs and the alignment model can be defined.</li>
<li>Recurrent Neural Network
<ul>
<li>Explains the use of the <em>gated hidden unit</em> for the activation function, which is similar to LSTM units.</li>
<li>Provides details and equations for the computation of the RNN state using gated hidden units.</li>
<li>Explains how update and reset gates are computed.</li>
<li>Mentions the use of a multi-layered function with a single hidden layer of maxout units for computing the output probability.</li>
</ul></li>
</ul></li>
<li>Alignment Model
<ul>
<li>Explains the use of a single-layer multilayer perceptron for the alignment model.</li>
<li>Provides an equation describing the model and notes that some values can be pre-computed.</li>
</ul></li>
<li>Detailed Description of the Model
<ul>
<li>Encoder
<ul>
<li>Provides the equations and architecture details of the bidirectional RNN encoder.</li>
</ul></li>
<li>Decoder
<ul>
<li>Provides the equations and architecture details of the decoder with the attention mechanism.</li>
</ul></li>
<li>Model Size
<ul>
<li>Specifies the sizes of hidden layers, word embeddings, and maxout hidden layer.</li>
</ul></li>
</ul></li>
</ul></li>
<li>Appendix B: Training Procedure
<ul>
<li>Parameter Initialization
<ul>
<li>Describes the initialization of various weight matrices, including the use of random orthogonal matrices and Gaussian distributions.</li>
</ul></li>
<li>Training
<ul>
<li>Explains that the training is done with stochastic gradient descent (SGD) with Adadelta to adapt the learning rate.</li>
<li>Describes how the L2-norm of the gradient was normalized and that minibatches of 80 sentences are used.</li>
<li>Mentions sorting the sentence pairs and splitting them into minibatches.</li>
<li>Presents a table of learning statistics and related information.</li>
</ul></li>
</ul></li>
<li>Appendix C: Translations of Long Sentences
<ul>
<li>Presents sample translations generated by RNNenc-50, RNNsearch-50, and Google Translate.</li>
<li>Compares these translations with a reference (gold-standard) translation for each long source sentence.</li>
</ul></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;requiring the length to be fixed seems a mistake as sentences can sometimes get very long think hundreds of words</p></div></div></section>
<section id="reflections" class="level2">
<h2 class="anchored" data-anchor-id="reflections">Reflections</h2>
<p>Naively, translation is an easy task in the sense that we just need to look up each phrase in a statistically generated bi-lingual lexicon and append the translation of the phrase to the target sentence. In reality even when working with parallel text is tricky in that, the translation isn’t one to one but rather is n-m mappings between the source and target words with the number of words being variable. Picking the best entry in the lexicon is not obvious as the source words may be ambiguous and need some attention to other words in the context to disambiguate, and these too may be ambiguous ad infinitum… Finaly the translation may require some reordering of the words in the source sentence to conform to the target language’s grammar.</p>
<p>To sum up:</p>
<ul>
<li>We need to learn a bi-lingual phrase lexicon.</li>
<li>Each phrase may have multiple translations.</li>
<li>To pick the best entry the source context should be consulted.</li>
<li>The target sentence may require reordering to conform to the target language’s grammar.</li>
<li>Another challenge is that the target language may require words or even grammatical constructs e.g.&nbsp;gender and gender agreement that are lacking in the source language. These are <a href="../1997-floating-contraints/index.qmd">floating constraints</a> c.f. (<span class="citation" data-cites="mckeown1997floating">McKeown, Elhadad, and Robin (1997)</span>) that the model needs to propagate through the translation process.</li>
</ul>
<p>Each step of this process is probabilistic and thus prone to mistakes. Though there are two main constructs. The first is the contextual lexicon which needs to be learned using parallel text. The second is the destination grammar that can be learned as part of a language model using monolingual text. However reordering texts isn’t as much of a challenge, if we have sufficient parallel texts then the NMT hidden states should be able to learn the reordering as part of its hidden state.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-bahdanau2016neuralmachinetranslationjointly" class="csl-entry">
Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2016. <span>“Neural Machine Translation by Jointly Learning to Align and Translate.”</span> <a href="https://arxiv.org/abs/1409.0473">https://arxiv.org/abs/1409.0473</a>.
</div>
<div id="ref-mckeown1997floating" class="csl-entry">
McKeown, Kathleen, Michael Elhadad, and Jacques Robin. 1997. <span>“Floating Constraints in Lexical Choice.”</span>
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Neural {Machine} {Translation} by {Jointly} {Learning} to
    {Align} and {Translate}},
  date = {2024-02-11},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/2014-NMT-by-jointly-learning-to-align-and-translate/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Neural Machine Translation by Jointly
Learning to Align and Translate.”</span> February 11, 2024. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/2014-NMT-by-jointly-learning-to-align-and-translate/">https://orenbochman.github.io/notes-nlp/reviews/paper/2014-NMT-by-jointly-learning-to-align-and-translate/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Bidirectional LSTM</category>
  <category>Deep learning</category>
  <category>Embeddings</category>
  <category>LLM</category>
  <category>NLP</category>
  <category>NMT</category>
  <category>Paper</category>
  <category>Review</category>
  <category>Podcast</category>
  <category>Translation task</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/2014-NMT-by-jointly-learning-to-align-and-translate/</guid>
  <pubDate>Sat, 10 Feb 2024 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Typology: The Space of Languages</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w03-typology/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/tiling.png" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>course banner</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/iYmE6UCiOSQ" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Lesson Video
</figcaption>
</figure>
</div><div id="sup-slide-deck" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div>

<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>How to quantify similarity between languages</li>
<li>Language families and genealogical similarity</li>
<li>Linguistic typology and typological similarity</li>
<li>WALS and other typological databases</li>
<li>Typology Prediction / Typology-based language transfer (Lin et al.)</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- Transcript here is machine generated no punctuation and I eddited it a but but it needs more work and I dont see the benefits unless I can run it through some tool. -->
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<blockquote class="blockquote">
<p>I’m gonna run through this um we have about half an hour um i’m gonna talk about all languages on the planet um basically not not um not too big um and of course there are a lot of languages and the question really is how can we try to structure that space we could look at them directly um but we’d like to be able to gather them into groups or similarities and that’s where we come up with a more um organized structural way and we call this linguistically and typology and so there is official ways to try to find out similarities between language and that’s what we’re going to talk about in this um in this form so let’s have a look so first we’ve got all the languages in the planet we have a dot here and for each of the languages now um this is very hard to do because languages are not point they’re spread over multiple places and of course they’re spread over multiple places that um will overlap as well but you can see something about here that there are some places that seem to have more languages per you know square inch um than others and it seems to be the the richer countries the ones where there’s a common education system travel is very easy and they have a long history of fighting each other with borders you’re more likely to find languages that cover whole what would we we call countries while um when we’re looking at places that are um been around for a very long time but might not have the same definition of border and there’s more languages and sometimes these languages are related to each other and sometimes they’re not sometimes they’re from rival groups and sometimes they’re just not related like korean is not related to any of the other languages in the area and there’s some relations to japanese but it’s really the japanese and korean are not like anything else and therefore they’re more like each other but only because they’re not like anything else in the area at all um even though both of them have got some substantial chinese influence with words and english influence as well um what’s the definition definition of a language uh well that’s sort of quite hard and there’s different definitions and we basically are coming down to um whoever defines it but of course it become can become quite political we’ve talked about these things before uh urdu and hindi different languages well it depends what you’re talking about from a political point of view unquestionably from a linguistic point of view yeah they’re sort of different from a phonological point of view they’re not very different at all they really pretty much overlap um and we have lots of examples like that</p>
</blockquote>
</section>
<section id="defining-languages" class="level3">
<h3 class="anchored" data-anchor-id="defining-languages">Defining Languages</h3>
<blockquote class="blockquote">
<p>There is the standard joke but it’s a good joke and it’s very relevant and that is “a language is a dialect with an army” and there certainly is political decisions when it comes to defining what the border actually is between two languages and often it’s quite weak between two languages and people can float between one and the next um so depending on the area depending on the</p>
</blockquote>
<p>history depending on the politics depending to on the historical aspect of um ethnic groups and travel etc um defining what the languages actually are um is still pretty uh um hard but say in the in india there is something like about 460 languages which is a lot officially from the government i think it’s 21 if i remember correctly and which is about the same size as what europe is when it comes to official languages but of course as you look closer um you discover that you start getting distinctions between languages which might be relatively small but they might be very big to the extent that these people absolutely can’t understand each other but there may be many people who speak multiple languages remember it’s really only uh um america and britain um where people only speak one language and only one language so it’s pretty rare actually in the world that people are only speaking one language</p>
</section>
<section id="language-families" class="level3">
<h3 class="anchored" data-anchor-id="language-families">Language Families</h3>
<blockquote class="blockquote">
<p>There are different language families and so linguists have decided looking at languages especially looking at things like um where choice and a word overlap which might vary between languages but there might be relationships between languages that we can see language families that are actually sharing um information like um lexicons so the choice of the word um the grammatical aspects the morphology act actually even though there could be differences they might be more similar and so across africa we can see something like regions of about um six different major language families and you know we can see that geographical aspects madagascar is different but that’s because you know there’s a big sea between that in the mainland and that actually makes less um chance for interaction to happen and therefore it’s sort of easier to keep that um distinction so you find sometimes that languages have got borders that are quite um geographical mountains rivers um and the sea of course ### Online Languages</p>
</blockquote>
<blockquote class="blockquote">
<p>How many of these languages are online um we’re all very aware that both you know um english european languages chinese japanese korean are very well represented online and how many people within the country are actually getting online nowadays often many of the richer people are online and they can be quite substantial amounts of uh data online but also when you look at um a major areas of wealth that have computers that have internet often there may be mixed languages and there may be a preferred language you often discover that in india even though there are lots of different languages in india that are native that people who are online are often using english or they’re writing in the romanized form rather than their native script because well history etc it’s easier they’re fluent in english um there’s a lot of english influence that’s actually there china almost everything is written in putonghua stands mandarin even though there are many other dialects there so there’s not the same representation across everywhere because it depends on whether these people have access and that they’re willing to talk in these languages often they consider these to be spoken only or rarely written and so their language of literacy is some colonial language be that english and spanish french etc or swahili or arabic um but it might be that it’s just easier for them to do that and therefore there’s a tradition to be able to communicate in um one of the standard larger languages on the planet rather than their own language and so when we’re trying to do multilingual nlp we’re actually caring about these smaller languages that may not be as much online okay so how do we try to find similarity</p>
</blockquote>
</section>
<section id="similarity" class="level3">
<h3 class="anchored" data-anchor-id="similarity">Similarity</h3>
<blockquote class="blockquote">
<p>between languages well obviously the the clearest thing is what we can do is we can start looking at the words and so we can see whether there’s a shared information and we usually look for words not like iphone and computer that are relatively modern we look at words that have been in the language for a long time so this is often body parts um family relationships um core food aspects water air etc and there’s actually a specific list called the um swabish list which you can be often used to compare um similarities between um languages now you want to be a little careful about caring about the writing systems so often that there’s a writing system that writes things in a very different way but you may discover that there’s actually a shared information um between the language in the phonetic form that’s not there in the written form and therefore you can’t just use string match to find out whether it actually works or not um there’s a number of online um groups that try to identify all of the languages on the planet um ethnologue is one of the best to try to do that it actually comes out the summer institute of linguistics which is a religious organization which is caring about translation of the bible but they’re quite independent in in doing their list mostly and um it’s definitely the most comprehensive list in in the world glottalog tries to do a similar thing but it also is identifying um this the position of the language on the planet and that’s not the only influence to similarity because people get on boats and cross oceans and uh we’re currently in the united states and um we all know in this particular area um english is not um native at all and french is native no no french was there before english was there but there was earlier indigenous languages definitely iroquois and possibly a language called mingo um is somewhat native and though the name allegheny is probably a mingle word for probably it’s a word for river um maybe northern river because it’s the northern one in the obvious three rivers um but um glottalog tries to do this but also gives downloadable um spreadsheets that allow you to be able to do um a in other aspects now remember people move i mean huge populations have moved around the planet which has influenced how languages have moved and also trade has done a lot to have shared aspect of languages as well as grammatical history or linguistic history of languages and we want to be careful about that when we look at that distribution um you know one might think that if you go to take the example of um korean there are lots of english words in korean they have a different pronunciation from the english but they’re clearly derived from english but that doesn’t mean that there’s any linguistic relationship between korean and english it’s much more to do with um english being the international language of the last hundred years um and korea has picked up many of these more modern words um into its language because that’s a convenient way to do it um</p>
</blockquote>
</section>
<section id="genealogical-similarities" class="level3">
<h3 class="anchored" data-anchor-id="genealogical-similarities">Genealogical similarities</h3>
<blockquote class="blockquote">
<p>genealogical similarities so this is the language family and this is usually divine defined by linguists who make decisions about looking at the linguistic properties historically we used to do this in the animal kingdom before we could do dna tests and there are interesting errors in the dna tests for animals that two animals have come from different family and moved together because that’s a convenient co-evolution to end up with a different way but actually they’re quite got different histories but that might not be obvious and that’s going to happen with languages as well where things end up being borrowed maybe things get simplified over time we can see some examples of this but things also get more complex over time and there’s lots of interesting and boring from some of the major families that are out there niger congo in africa has got a lot of different languages and covers 21 um of the languages spoken on the planet that’s a lot okay well if we look at something like indo-european that covers most of europe not all of europe and through um the middle east at least the northern middle east m iran and into northern india and that’s a lot of people okay but it’s only about 6.3 of the languages because many of these languages are spoken by a very large number of people so you’re going to get very large numbers of people and also some of the links between languages that although you know german and english have lots of common lexical items that people can sort of work out and if they know english or they know german and be able to work out what the other one is but sometimes it’s not immediately obvious that there’s a relationship between the languages and very few of the words are actually overlapping lithuanian which is sometimes identified as the one that’s most archaic in the sense of it’s got more of the history of the original part of Indo-european and so does english but the relationship is not obvious at all to an english speaker</p>
</blockquote>
</section>
<section id="typological-similarities" class="level3">
<h3 class="anchored" data-anchor-id="typological-similarities">Typological similarities</h3>
<p>how do you work out these typological similarities well this is one of the major things that linguists have been doing for a long time and they’ve been looking at ways of linguistic properties to try to see what the similarities are between them and there’s a number of books and studies that try to collect that information together from multiple research studies okay now when we’re looking at um phonology so the actual pronunciations the ipa is an excellent way for being able to split down the possible ways that most languages on the planet actually do their pronunciations and we have a vowel space that’s continuous vowel space and we split each language splits it into different ways and there’s often drifts between different languages that are maybe even um predictable for consonants and things which are not vowels that’s probably the best definition of them um there’s lots of things about um a place of articulation in manner of articulation places where we put constrictions from the front of the mouth down to the back of the throat and we can sort of have things that deal with the lips with things like p and b things that deal with the teeth things like tea and things that deal with just behind the teeth which are things like um okay and all of these may have different variations depending on the um a on the languages that we’re actually speaking and have different distinctions in english we may produce some of these but we don’t make distinctions between them but for example um a korean has got um three different p’s that most english speakers would not distinguish between so when we’re looking at similarity between languages we could look at the similarity in the phonology and how many phonemes actually are um common between the different languages now um there’s actually a group called</p>
</section>
<section id="walls" class="level3">
<h3 class="anchored" data-anchor-id="walls">Walls</h3>
<p>walls now walls is also on a collection of all of these different typographic variations um that are uh um a over all of the languages and basically back in the early 2000s um a group of people tried to start collecting papers um and showing what the similarities so for the most part you can go to the walls and a website you can select some particular feature and you can see the distribution here we’ve got the distribution of the planet of different number bases so we all count um in all languages some more than others and um sometimes we use decimal and it’s probably related to the fact the number of fingers that we have but some count in twenties um and that’s not really unusual even in english we have some residual twenties that’s there and even in chinese there’s some religious um uh 20-ness where we have a specific word for 20 in english it’s score um and here’s a mapping of all of the languages now we’re not saying that these languages are related to each other we’re saying that when we look at numbers all of the languages with the blue dot um are um counting in decimal basically well those for example that are in the purple pinkish dot are counting in twenties across and some don’t have good ways of doing that um at all now walls is this excellent detailed form where you can go through and find different things you can find out which languages refer to t as t and which refers to it is chai which is quite interesting in itself some of them are maybe a little bit light-hearted and some of them are quite detailed like for example um a word order or a default word order of um in english we have a subject verb um object japanese is um a subject object verb now historically um walls originally came in a book okay and this book is what’s called really really big and i’m sorry i had to take it from underneath my monitor um because it normally keeps my monitor at the right level but the book of course isn’t updated but the website is and over the years the website gets more and more and people now actually think about registering the piece of work that they’re doing to be able to cover what’s actually there now not everything is in walls um because not all of the features have been studied in all of the languages most linguists are going to study something that’s interesting so if they’re interested in something like voicing inconsonants after long vowels that’s only going to be interesting in some languages and other languages there’s just nobody’s going to study that and so the question is can we actually predict the missing feature from other factors because often there’s information that’s in there so for example in linguistics um default word order seems to be less fixed when you have more morphology and that’s something to do with um if you’ve got morphology it allows you to be able to identify who did what to whom better and therefore you don’t need to care about word order in the same importance level so there’s some predictable things so if somebody tells me something about a language i don’t know and says it’s a really rich morphology i i’ll think maybe it’s got free word order that’s not true for everything but there’s a more likelihood that it does if something doesn’t have lots of um morphology it probably has more fixed order and can we learn this from the data by looking at all of the features all of the languages find the missing ones make predictions hold out the ones we do know and it may allow us to be able to make these predictions and lots of people have tried to do that at various um levels and being quite successful and in fact the paper that we asked you to look at already has looked at some how well some of these predictive things actually work and of course some of them work better for some aspects than others okay sometimes you can do it purely unsupervised some sometimes um you want to have supervised learning to be able to do this sometimes you want to make these predictions and go and explicitly ask somebody to to ask you ask you whether it’s correct or not.</p>
</section>
<section id="typological-databases" class="level3">
<h3 class="anchored" data-anchor-id="typological-databases">Typological databases</h3>
<p>There’s a number of these typological databases out there walls is only one it’s quite good it’s quite famous and there’s a number [Music] been derived from those or derived from multiple ones especially to fill in particular aspects of the features and these can be really useful in trying to do things when you’re doing multilingual um modeling because you want to know maybe these features make a difference on my downstream or my predicted prediction task and i like to be able to get these features I’d like to have reliable features i’d like to know when the features are confident when the features are missing when the features are um going to be more important than others so that maybe I look harder to be able to find these features here at cmu a number of years ago we had</p>
</section>
<section id="lorelai" class="level3">
<h3 class="anchored" data-anchor-id="lorelai">lorelai</h3>
<p>A project called lorelai which we’ll mention a few times in this um course where we actually tried to build a specific vector that tries to represent a language so lang to vec so given the name of the language given the code of a language we’ll give a vector representation that will try to um identify all of the aspects that would be relevant for feeding in as a prior when you’re building um various language models okay um there was a bunch of work done on this to be able to do this um basically both building the vector and also trying to predict and and across that there are still people doing phd’s graham is still very much working in that way and aditi is our phd is very much in that space um</p>
</section>
<section id="universals" class="level3">
<h3 class="anchored" data-anchor-id="universals">Universals</h3>
<p>are there any unit universals that are features that are there for everything for all possible languages and the answer is yes mostly and sometimes there’s a little bit caveats around the edge and so for example all languages do seem to have vowels and consonants but the definition that the boundary for vowels and consonants isn’t very good um so that’s might be an easy thing to fulfill but it seems to be true given that we’re all using the same vocal tract that we are actually trying to do that almost all languages have got nouns and verbs now there’s some languages where the distinction isn’t very strong and it’s morphological variants that allow you to be able to distinguish between it but pretty much everything has nouns and verbs now once you get the adjectives the next major class that’s not so clear and we end up with a number of languages that will use nouns as adjectives maybe with some morphological variation in english you can get away with quite complex nouns being used as adjectival forms more so in american english than in british english and but that’s sort of moved to that we get quite complex noun compounds which are really some form of adjectival form and that’s true across a number of languages now there’s other things that are very common across multiple language families or related languages that are relatively interesting and identifiable and there are multiple non-related languages that will do that you know species between um words in the written form um a morphology that’s um segmental so we’re joining things together as opposed to um templatic morphology where you have maybe a bunch of different um consonants and the vowels change inside it arabic and hebrew and a number of other northern african and languages have that that are not necessarily all in the semitic language family and so there’s a number of things that are relatively common but they’re not going to be everywhere but you know there’s also things about you know um uh if language is distinguished between voiced and unvoiced they don’t always um they’re going to care more about the voice than the unvoiced ones um how do we deal with the low repo low</p>
</section>
<section id="low-resource-languages" class="level3">
<h3 class="anchored" data-anchor-id="low-resource-languages">low resource languages</h3>
<p>resource aspect of this how can we actually find out um how if we were given a language and we maybe only got a few features for it how can we predict other things for it well we can look at all other languages that are similar and we can find out well most languages of these features have got those features and we might also say for that language what’s the most related language and we’ll just say well let’s assume all these features it might not be true but it’s probably better than just assuming everything is english because everything is certainly not english okay and the number of different groups and throughout the um world have been studying this is quite a major area of looking at how to be able to predict or get other features for low resource languages when you don’t have data that’s there now there’s a number of different ways of doing such multilingual nlp what you can do is you can say well i’m going to try to find a close by language and i’m just going to assume everything about that and maybe remove things that are not appropriate and often that happens so imagine that um you come to europe and um there’s this island nation that separated itself from the rest of europe uh pretending that it’s not part of europe at all and nobody knows anything about english and eventually somebody braved the channel and gets to england how might they understand english and the answer is well it seems sort of germanic-like so let’s just pretend it’s german or dutch um which is even closer and then just use everything that we have um from the dutch language to apply it to english and maybe train a little from that and you would get much further with that than if you took chinese or hindi or maybe even french although a lot of the words are borrowed from french and but the grammar um is very much germanic in english so we take another language and try to do things so what we’re doing is taking an existing model and then trying to fine-tune it for the target for okay another way to do this is to try to take</p>
</section>
<section id="multilingual-birth" class="level3">
<h3 class="anchored" data-anchor-id="multilingual-birth">multilingual birth</h3>
<p>all languages or all languages in some language family and what we would then do is we would then train everything together in some multiple joint way there’s lots of different ways of doing that and then we would have a model that was multilingual in a true sense that’s actually how multilingual birth is done rather than having different births from different languages and then doing adaptations of the target one we actually build a multi-lingual bar and then we’ve got this multilingual thing so it’s sharing some information about all of these languages and that might make it easier when you’re doing adaptation to the target one form and that is an open question okay um it probably depends on the amount of training data and the amount of languages and how close it is and whether it’s going to be similar or not to these other languages when you’re doing that whether you have a writing system that’s the same whether it’s different whether phrenology is different and all these things are going to be important so there won’t be one answer for everything but you should be aware of the different ways of actually trying to do that okay um why do we care about um typology at</p>
</section>
<section id="why-typology" class="level3">
<h3 class="anchored" data-anchor-id="why-typology">why typology</h3>
<p>all can’t we just well we’re going to train from everything um and the answer basically is for most low resource languages you just don’t have enough data and even if you pretend that you have enough data it’s been shown in english the more data you have the better your models are going to be now in general in machine learning the more structured data you have the easier it is to be able to learn things so if you have external data to help you when you’re dealing with small amounts of data it will usually not always but it will usually learn better so knowing about the default word order knowing whether morphology is an issue or not knowing the types of grammatical structures the types of verb structures the types of noun structures or their noun classes or their politeness these will potentially help you when you’re building your model to be able to get better results quicker okay</p>
</section>
<section id="how-to-choose-a-transfer-language" class="level3">
<h3 class="anchored" data-anchor-id="how-to-choose-a-transfer-language">how to choose a transfer language</h3>
<blockquote class="blockquote">
<p>How do you choose a transfer language? Well often what people will do is they’ll go and ask some um a knowledgeable person about which language is close and you’ll get an answer but that might not always be the right answer and there was a bunch of work done here um a few years ago and trying to look at that for particular tasks like translation and there’s non-trivial aspects of actual similarity of the language reliability of the language the amount of data that you have and whether the data on the domain of the data that you have is appropriate is it conversational um is it newspaper text is it bible text and um the linux all people actually did quite a lot of that to try to do it i remember the sort of default answer for that i am right about this i think it’s this one and said turkish by default is the best one over everything and that’s sort of probably because turkish is fairly well resourced it’s influenced by a lot of different other language families so it has arabic it has english it has um iranian and hindi farsi um sanskrit and things in it and it spreads over there’s a vast amount of um the world going from europe all the way into turkic languages all the way into china open research problems that we have.</p>
</blockquote>
</section>
<section id="open-research-problems" class="level3">
<h3 class="anchored" data-anchor-id="open-research-problems">open research problems</h3>
<blockquote class="blockquote">
<p>How to extract typological features automatically so if you give me a language can i find out the default word order that’s sort of hard i mean we’re gonna get some there but but when the it’s not obvious and maybe it’s different in written form compared to in the spoken form and therefore you have to be able to care for that but there is this thing called the universal dependency tree bank that um originally came out of google and so there are these um existing toolkits and data sets which try to give this information for many of the major languages and some of the minor language as well and these resources are hard to do yourself and therefore it’s always good to know about them and to be able to build on top of them okay um there’s lots of other things that are</p>
</blockquote>
</section>
<section id="multilingual-aspects" class="level2">
<h2 class="anchored" data-anchor-id="multilingual-aspects">Multilingual aspects</h2>
<p>out there if you want to learn about morphology or phonology you can look at multilingual aspects in the um in the computational linguistics and conferences there’s lots of geographical groups that are looking at say specifically looking at um indian languages african languages um there are lots of things that are looking at low resource languages there are lots of ones looking at interesting morphology languages and so often it’s worth looking at and though everybody who doesn’t know a language thinks i’ll just train from an infinite amount of data the answer is well you won’t have an infinite amount of data and sometimes it’s quite hard to find data and if you discover that morphology is rich in the particular language it might be worth doing morphological segmentation and there may already be an existing morphological analyzer that’s there or at least help to be able to find that okay so that’s a very quick view of um typology on how we actually structure languages um and it’s becoming more and more important in the computational form than what it was been before um 10 15 years ago you’d see less papers about it but now people are really caring about it because we are doing much more multilingual work you were asked to read this particular paper which was a survey paper on looking at aspects of um typology across um different people trying to do predictions and how well they were actually doing and the issues that are involved in this and what we’re going to do now is we’re going to split you off into groups and um in those groups you’ll have a ta or an instructor will be there and um what we want you to do is we want you each of you um to identify things which are unique or very rare compared to other languages that are important over the languages you know um in distinct from things which are not very interesting maybe whole classes of languages that are unrelated are all using a romanized form and to write them um but they’re not related but things that are going to be unique from that point of view now um a has someone set up the groups graham have you set up the groups have um maybe we could take some questions if people had questions yes sure yes thank you 22 uh indian languages yeah yeah so what language you write things in is quite interesting and especially once you’re in a code switching space um in india almost everybody when they’re code switching will write in a romanized form they’ll often call it english but it’s not english it’s the romanized form so they’re writing both hindi in a romanized form and english in romanesque form while when you look at um singapore for example where people can be um as fluent in chinese and english they actually use hansi for writing chinese and english for writing english words most of that’s got to do with input method actually um it’s like how easy is it to type these things on a computer and for historical reasons actually partly because there was less chinese speakers who spoke english the chinese input systems became better while in india for the past 200 years the educated elite were all english-speaking and therefore they were used to reading and writing in using romanized m form probably that’s got something to do with it but definitely information is lost when you may be using a non-native um script and for example spelling goes on completely out of the way in english when you’re doing it but remember most written most scripts are not appropriate for the language we are using a latin script for a germanic english in english we use kanji in japanese for writing lots of things and yeah there’s other scripts in japanese for dealing with more native things um and um hangul is native in korea but there’s still lots of um chinese borrowed words especially scientific words um that come from chinese and so often the writing system even for the native speakers is not very appropriate but often it’s just convention it’s like this is the way we write it and we’ve always written it and it was only since last year that people live but they think it’s facts that we’ve done it forever you know basically we’ve always sent dances to tiktok as a way to communicate and i can’t remember when when we ever did anything else</p>
</section>
</div>
</div>
</div>



<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Typology: {The} {Space} of {Languages}},
  date = {2022-02-25},
  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w03-typology/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Typology: The Space of Languages.”</span>
February 25, 2022. <a href="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w03-typology/">https://orenbochman.github.io/notes-nlp/notes/cs11-737-w03-typology/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Multilingual NLP</category>
  <category>NLP</category>
  <category>Notes</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w03-typology/</guid>
  <pubDate>Thu, 24 Feb 2022 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Translation Models</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w06/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/tiling.png" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>course banner</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/XeDCP0newd8" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Lesson Video
</figcaption>
</figure>
</div><div id="sup-slide-deck" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div>

<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Sequence-to-sequence models w/ attention</li>
<li>Decoding strategies</li>
<li>Transformers</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- Transcript here is machine generated no punctuation and I eddited it a but but it needs more work and I dont see the benefits unless I can run it through some tool. -->
<blockquote class="blockquote">
<p>looking forward to talking about machine translation and sequence sequence models today uh as always i’ll welcome discussion on this uh some of this might be this is kind of the basic modeling stuff regarding machine translation so i think some of this might be uh elementary for people who have taken an nlp class before but there are you know some people who might not have covered this so i’d like to go through it after this we’re going to be talking about some concrete code details so looking at code from the annotated transformer and also from the assignment and as always please feel free to ask questions on chat or like live anytime you want so language models are basically generative models of text and uh what they do is they calculate the probability of a text x given um you know calculate a probability of a text x and they allow you to sample from this probability distribution to generate outputs with a high probability and the um so given this uh you know if we train on a particular variety of texts like let’s say we train on harry potter books the sampling distribution will uh generate things that look like harry potter there’s also um conditional language models and conditional language models don’t just generate text but they generate text according to some specifications so if we have our input x uh and our output y our input x could be structured data output y could be a natural language description uh what we’re going to talk about this time is translation so input x could be english output y could be japanese um our input x could be a document output y could be a short description and our input could be an utterance output of response inputs image output text input speech output transcript so you can see that a lot of things a lot of different tasks you know data to text generation or natural language generation translation summarization response generation image captioning and speech recognition all fit within this general paradigm so um because of this you know these are a very uh powerful and widely used variety of models and so i’d like to go a little bit into exactly how we formulate this problem and learn these probabilistic models so when we calculate the probability of a sentence the way we can do that is by taking a sentence which consists of i words and calculating uh the probability by the product of the probabilities of the word in the sentence so if the next word is x i we uh calculate the probability of x i given all of the previous words in the sentence and this is a particular variety of um language model oh sorry there’s a language intense sorry um yeah i’ll find a good place to cut off this description and switch over to the language intent i am totally slipped my mind so [Music] then uh conditioned language models basically what they do is they condition on some added context and they calculate the probability of y given x and um then they do the same thing where you condition on x and all of the previous generated words i talked about uh calculating the probability of words in a sentence and in most cases we’re taking the previous context uh calculating uh the probability of the next word given the previous context these are also called um uh by a variety of names uh one name is an autoregressive uh link model uh where basically that means we’re predicting the next word uh given the previous words in some order specifically uh when the words that you’re predicting the uh the word here um from all come to the left they’re also called left to right language models um and there’s also a term causal language models which i i don’t prefer as much but you’ll also see in the literature um and basically if you have a left to right language model uh we can use something like a recurrent neural network to feed in the previous word predict the next word feed in the previous word predict the next word feed in the previous word predict the next word etc etc and if we want to develop a conditional language model that conditions on some sort of context to predict the probability of the words uh the upcoming words we can do something like feed in the conditioning context into a recurrent neural network and predict the uh the outputs like this so basically we read the conditioning context one by one maybe it’s a japanese sentence and then we predict the probability of the output of the english sentence one by one so we have the encoder and so this is often called an encoder and this is called the decoder so these are often called uh encoder decoder models and uh they’re used pretty widely and you know all of uh nlp nowadays so um i’ll actually skip that part so um now we have this model uh we can train it you know i think a lot of people are everybody in the class is now training uh you know models for part of speech tagging uh now so because of that i i assume you know like about the recurrent neural networks and other things like this the the place where these uh kind of sequence sequence models or translation models become different is that we need to do uh generation from the models and so instead of like taking in individual words and trying to put a tag on them we actually need to generate outputs and the generation problem is basically we form a model of p of y given x and how do we use it to generate a sentence and there’s basically two methods the first method is sampling where we try to generate a random sentence according to the probability distribution and the next method is argmax where we try to generate a sentence with the highest probability and in general in translation which is the main topic of this uh lecture here we we use the arg max and actually maybe i’ll explain why in a few slides after i explain each method so um for sampling uh the most common way to do this is called ancestral sampling where we randomly generate words one by one and the algorithm for doing so looks a little bit like this so we have uh while uh yj minus one is not equal to the end of sentence symbol so basically while we have not yet finished our sentence we calculate the probability of the next word given all the previous words in the conditioning context and we randomly sample a word from the next time step and this is a method for an exact method for sampling from p of y given x so this will give you you know basically samples that exactly follow the probability distribution that the model specifies if you had a really really good model where basically it only assigned probability to why that were you know plausible translations for x then probably the sampling would be fine you would get different translations every time but you know each translation that you sampled would be good however unfortunately our models are not all that great and very often if you sample kind of lower probability things according to this probability distribution they might not be very good translations so uh what we try to do instead is uh do some sort of search for the highest probability translation and one way to do this is doing something like greedy search so basically one by one we pick the single highest probability word in the output so that looks a little bit like this we similarly we go until we reach the end of sentence symbol but instead of sampling a random word according to the probability distribution each time we we take the highest probability word at the next time step each time so uh what this is doing is this is instead of uh picking you know any any old word next time it picks always fix the highest one which moves us closer to a very you know high scoring output unfortunately this is also not exact and it causes real problems so for example um it will often kind of regardless of what the syntax should look like it might try to generate uh the kind of easier words in the sentence first because the easier words in the sentence will have higher probabilities so uh this arg max this greedy arg max we’ll try to choose those words first and another issue that this can bring up is it will prefer multiple common words to one rare word so like for example if we had a choice between new york and pittsburgh even if the product of new in york was lower than the probability of pittsburgh if the probability of new only was higher than pittsburgh this greedy search would kind of make that decision prematurely and not be able to back off and uh and reconsider the decision later so what we do instead is we have a method called beam search there are other methods as well but beam search is the most common method where instead of picking one high probability word at every time step we maintain several paths so this is an example of beam search with size two where basically we um uh instead of like taking the highest probability word at the first time step which would be b we consider two high probability words of the first time step a and b we expand all next words for each of those possibilities and um and then select the highest scoring sequence of length two based on all of these and basically beam search allows you to spend more time computing to look for a high scoring high scoring output and basically allows you to do more precise search and help resolve some of the issues that i talked about on the previous slide cool um there’s also lots of other details if you want to go deep into the details of this like for example there are other methods for sampling that try to kind of alleviate the issues that you get from low probability [Music] low probability outputs by only sampling from the highest probability outputs things like top k sampling and nucleus sampling there’s also um other methods for search that incorporate uh you know like dynamic beam sizes or other things like this but basically these um standard b in machine translation standard beam search tends to be uh the thing that most people use and you usually just pick a beam size of five and forget about it and hope uh you know that that’s good enough and it usually is cool any questions about that before i move on okay i guess not um so actually sorry i need to check a chat so next i’d like to talk about attention so attention is uh very very important in um machine translation and uh one issue with the example uh like machine translation model that i talked about before is basically what we’re asking this model to do here is read in a whole sentence express all of the meaning in that sentence in a single vector and then uh output the appropriate translation based on this and i because i have previously worked as a translator i like to make an analog to what that would be like if you were asking a translator to do something so this is kind of like asking a translator to read a whole sentence memorize that whole sentence put the book that they were looking at away and then tried to translate output the whole sentence uh based entirely on their memory and you know for a lot of sentences a lot of things have people uh you know think of they might be able to do that but it’s a lot harder than if you can go back and reference the uh like original sentence when you’re doing translation and so uh this was kind of eloquently uh stated by ray mooney uh a professor at ut austin where he said you can’t cram the meaning of a whole bleeping sentence into a single bleeping vector and um so the idea of attention is basically to relax this constraint of memorizing all of the content of a sentence in a single vector and put it into multiple vectors one vector for each token in the input so then we have the model that goes back and references each of these vectors one at a time so the basic idea behind attention is that we encode each word in the sentence into a single vector and when decoding we kind of perform a linear combination of these vectors weighted by attention weights and we use this combination from the vectors in picking the next word and i have a graphical example of this here so let’s say we have our encoder here and our encoder calculates a representation for each word in the sentence and our decoder here basically is stepping through the sentence word by word and it’s gotten to the place where it’s at like i hate and that it wants to generate the next word so what a translator would do at this point is they would go back and reference the original sentence maybe after they’ve read through it once and try to decide which word they want to be translating next think about uh what word they want to translate that into and that’s basically what attention does so it takes this query vector down here and it compares that query vector to all of the key vectors in the original included input and it calculates a score or a weight based on the congruence between the query vectors and the key vectors so you put it through this function here that takes in the query vector and the key vector and it spits out a scalar weight and we do this for each pair of the query vector in one of the key vectors in the key matrix basically and uh then we normalize these so they add to one using a softmax function um and basically uh what this looks like is we take in these values and we output something that looks like a probability vector here and i’m sure you know everybody’s familiar with the softmax from the homework that you’re already working on then after we do this we take uh the value vectors which are the things that we want to attend to the types of information that we want to combine and we multiply this by the attention vector here and that gives us a single vector that we can then feed into any part of the model and specifically in the case of machine translation or whatever else we will be feeding this into the decoder in order to calculate the um the probability over the next word so if we have a attention mechanism when we’re doing translation this is a picture from the paper that introduced the attention mechanism basically you’ll see that every time it translates a next word um i believe in french here from english to french it will be attending to um it will be attending to multiple words in the original source documents so and you can see that these align with each other and even uh when like the order reverses then um the uh appropriate words are being attended to so i left the um the actual function that we used to take in the query and the key vector underspecified uh but if we assume that the query and the key q is a query and k is the key we can do various things like feeding the query and the key into a multi-layer perceptron where we concatenate them together multiply them by a weight matrix take a non-linearity like the tan h and multiply by another vector to get this value this is flexible and often very good with larger data sizes another thing that we can do and in fact this is maybe the most common method that we use nowadays we can multiply by the key uh we can have the key and the query and we have a weight matrix uh here that we multiply and this gives us uh the [Music] attention weight here and then we also have um other things like the dot product where we just take the dot product between the two vectors um this is very very simple and and can be effective but the problem is that this uh one issue with this is this definitely requires the key and the query to be the same size because otherwise you can’t take the dot product um so the bi-linear function gives you a bit more flexibility in that you don’t have to assume that the query and the key are in the same kind of like underlying space and it also allows you to compare things of different size and then finally um we can have the scaled dot product and so one problem is that the scale of the dot product increases as the number of dimensions gets larger and so the way of fixing this is basically scaling it by the size of the vector so you take the square root of the size of either the key vector or the query vector it doesn’t really matter which one because they’re both the same size here and this allows you to make sure that even if you have larger or smaller values the scale with which the attention weights varies stays approximately the same okay so this is a a brief overview uh we’ll be going through some concrete examples uh a bit later in the class but are there any questions here i imagine many people will be familiar with this already okay um if not i will go ahead so uh then we have improvements to attention um so attention uh one issue with attention is basically that um neural models often tend to drop or repeat content when they’re doing uh generation in particular for tasks like machine translation where in machine translation we can make the implicit assumption that all content in the source should be translated into the targets we can additionally add biases to that basically model how many times words in the input have been covered and if they have um not been covered you know approximately one time we can give some sort of penalty so basically this is making the assumption that each word should be translated about once or another option is to let the model to tell the model how many times each word has been covered but let it learn to use that information appropriately in making its next uh translation decisions so either way you know this is explicitly or implicitly encouraging the model to you know cover things once and only once or like as many times as necessary another um very common improvement used in most models nowadays is multi-head attention uh where the idea is that we have multiple attention heads that focus on different parts of the sentence so this is an example where essentially what we do this is an example with two attention heads and the two attention heads each have a different function the first attention head is like a normal attention head that is just used to calculate representations of the words in the sentence so kind of like how i explained attention before as a way to combine together information uh at each time step and then the second attention head is basically focusing on words that should be copied from the source to the target and this is used in particular for a summarization task where lots of copying needs to be done so basically it’s summarizing source code into um into function uh names so what you can see is the first attention head because it’s just pulling in information about the input sentence in general it’s kind of spread out across the entire input sentence and the second one because it’s kind of choosing which words to copy to generate outputs is very focused on individual words uh as opposed to being spread out across the whole sentence and uh so i like this example because it gives an example about how like theoretically you might want to have different attention heads with different functions but in standard models nowadays what we use instead is we have multiple attention heads but we just let the attention heads kind of learn on their own what they would like to focus on and here is an example of uh basically this word making attending to other words in the same sentence in self-attention which i’m going to talk about in a second and you can see that making one of the attention heads is attending to making itself there are other attention heads like the blue attention head orange one green red which are attending to more difficult which is kind of the the object of making and then you have another attention head that’s pulling in information from like the previous word to making so you can see that each of these heads has kind of specialized in a different like form or like function that it should be achieving so that’s good because you know there are things like local context like you know the previous word 2009 which could be very useful but there’s also other varieties of context farther away like the object of a verb that could be important for making particular decisions as well so another uh common improvement to attention is supervised training and normally we just train the attention together with the entire model and however sometimes we can get like quote unquote gold standard alignments a priori and um gold standard alignments are basically mit could be manual alignments where we’ve asked a human annotator to say specifically which words um in the source correspond to which words in the target and the reason why we might want to do this is because as you can see here or no um as you can see here even though this is a relatively good example it’s not always the case that attention will focus directly on the words that are being translated like for example here uh the period only has a very vague attention on the period in the input so because of this you can’t necessarily like look at the attention value and immediately tell which words are being translated and that’s even worse in the case of something like multi-headed attention however like as i talked about last class you know if you’re a human translator using machine translation it might be very important or useful to you to know which words were translated which other words so you could check that they’re correct or um something else like this and in that case um doing supervised training with manual alignments or something could make sure that at least you know some of the attention heads uh correspond with human intuition about which words were translated into which other words another way you can get gold standard alignments is by using a strong alignment model so you might have a an alignment model that was trained on manual alignments or something like this and you could also use that to kind of bootstrap the training of your model and this can also improve accuracy overall by making sure that the model is attending to uh like words that are actually aligned uh as well so um exactly how you do the supervised training basically the answer you can do a number of things but the simplest one is you just have another loss function that you incorporate into your model um where the loss is higher if the alignments are like incongruent with the um uh with the annotated alignments and so you can do things like take the the norm uh the l2 norm of the difference between the alignment vectors or something like that okay and uh now i’m going to go into self-attention um which is very widely used in nlp nowadays and self-attention basically the idea is uh up until now we talked about things like recurrent neural networks is a way to gather encodings for each word in the sentence um but another way you can do this is you can have some sort of attention that uses that where you attend each token in the sentence that you’re looking at now attends to the same sentence itself so for example the word this would attend to this is an example the word is would also attend to this is an example uh etc etc and the basic idea here is that this can function is kind of a drop-in replacement for another type of word-level encoder like rnns um and uh and calculate the representation of each word so for example for this um the word this might attend to this a lot is a little bit and the other one’s not very much is might attend to is and its subject and object and other things like this um the motivation for this becomes very obvious if you think about the translation of words like um like the word is um or maybe the word run is a better example so the word run you know depending on whether you’re running a marathon or running a company or running along the road or running along the creek or something like this uh run might be translated differently so if uh the encoding for the word run pulled in the subject information about the subject and the object that would make it a lot easier to perform like lexical selection in translation or something like that um so why would you want to use self-attention instead of an rnn or something like this unlike rnn’s it’s very parallelizable which allows for fast training on gpus you can calculate all of the words in the sentence at the same time so this is probably the most important reason why people like using transformers uh sorry like using self-attention and the transformers that i’m going to be talking about in a second another reason why people like it is unlike both rnn’s and other options like convolutional neural networks it can easily capture global context from the entire sentence so you you’ll note that every time each word is attending to all other words in the sentence so that gives a very direct connection to you know pulling context there um in general uh transformers seem to yield a pretty high accuracy and a large number of uh nlp casks although it’s not 100 clear that they’re actually all that much better with respect to accuracy um uh like there’s this study by chen uh at all that uh compared them with rnns and the results were uh were similar essentially um but the uh but the advantages uh here of like fast training um and being able to easily pull in global contacts are uh are widely appreciated one downside to this is quadratic computation time so essentially um because uh like you can even see this computation time directly from this attention graph here so you can see that if the length is 4 you have 4 times 4 16 attention values here and this can become onerous if you’re using very long sequences to compute so there’s also a lot of different methods to improve over this recently so um now that i’ve introduced a bunch of you know improvements or uh kind of uh elements that go into attentional models uh i’d like to introduce the transformer which is kind of the core element that is used in most state-of-the-art mt uh or nlp models nowadays and basically this is a self um a model that uses both self attention and regular attention between the source and the target um so uh we have a transformer encoder in a transformer decoder where the transformer encoder takes in some word embeddings it adds something called the positional encoding where the positional encoding basically is a another word embedding that tells you not the identity of the word but rather the position of the word in the sentence so instead of saying like this is the word summary it would say this is the first word in the sentence so you add them together and you get um like summary and first word uh in embedding indicating that um then you run this through multi-head self-attention you add in the original input and you do something called layer normalization which basically kind of normalizes the outputs uh to be average of around zero and norm of around one and uh then you run through a feed-forward network um and then you do the addition and normalization and then you do this over and over again you know six times or eight times or ten times or whatever um and then on the the right side over here you have the transformer decoder which is very similar output embeddings positional encodings self-attention but you also have additional cross attention that attends to the input here so that’s kind of the place where you get the attention that i showed before between the source and the target here so that this is in the cross-attention part and then in the self-attention part um you have the encoding of the sentences like you have here so you’re you’re doing both in the transformer so um often we talk about like the transformer is kind of like a monolithic block uh that is used in um in translation models uh but actually you know it contains a lot of the tricks that i talked about before so it contains self-attention multi-headed attention it’s using normalized dot product or actually maybe bilinear attention would be more accurate because it’s multiplying by a a weight matrix for the key in the queries and it has positional encodings it also uses a whole bunch of training tricks such as layer normalization helping ensure that layers remain in a reasonable range it also has a specialized training schedule where you have the atom optimizer but you have a specialized learning rate that kind of gradually ramps up and then slows down in the transformer paper they also used a technique called label smoothing i’m not going to go into a lot of detail about this but basically it kind of smooth smooths out the distribution you’re predicting so you’re not predicting 100 probability on the true next word but you’re giving a little bit of probability to all the other words uh this is pretty effective in a wide variety of neural network based tasks and um you also have a masking method for efficient training and so the way this masking works is basically um you uh decide when you’re calculating the representation for a particular word um you decide which previous uh which context to attend to so when you’re calculating the representation for the word i you only attend to the context in the input when you’re calculating the representation for the word hate you calcul you attend to any of these uh things over here um and similarly throughout the entire uh like calculation um of the representations for the decoders so on the encoder side you always attend to all the other words but on the decoder side you only attend to words in the input and previous words in the output and um so this was about transformers uh taking a step back we can kind of take a unified view of the models that we’ve talked about so far in this class um so we have sequence labeling where uh sequence labeling um has a feature extractor and then given the feature extractor we have a feature for each word and calculate the output so this is what you know everybody is doing for assignment one and then we have sequence sequence modeling where we have a feature x director so these can actually be exactly the same thing and calculate one representation for each word and then we have essentially a masked feature extractor here um where we calculate representations that can ref uh reference any of the things from the input feature extractor but only things that occurred previously in the current output sentence and then we predict the the words here as well cool um so this is all i have in terms of the lecture content and then i’m going to try to go uh through the code for the transformer just a little bit and then patrick’s going to introduce what you need to do for the assignment are there any questions about the lecture content that i talked about here okay i i guess not at the moment so um uh if you if you know about this already this might be a bit of review if you’re not familiar with this it’s very important so uh please do you know ask questions go to office hours other other things like this um and then the final thing i’d like to do is go through a brief code walk through this thing called the annotated transformer and this is a very good thing it’s linked on the class site and basically what it does is it um it demonstrates how you would actually implement the transformer model together with all of the description um from the original paper and so basically [Music] if you look at the model architecture first um the the way it looks is uh we’re talking about in encoder decoder architecture um we have our encoder here we have our decoder here and sorry my computer is very slow uh nowadays because my battery is broken as you heard last time but um i’ll try to do this nonetheless we have our source embedders we have our target embedders and then we have some sort of generation policy so this would be you know like something that helps us do uh sampling or um or helps us predict the probability of the next token and uh we take in and process uh source and target sentences the encoder encodes the source sentence and then the decoder encodes the target sentence and the memory so i believe this is the encoded source sentence and then we have the source mask and the target mask and the generator is basically uh just doing a soft max like you’re all familiar with um and so this is the kind of encoder architecture this is the decoder architecture and then we have our generator here and um one other feature of the uh of the transformer is basically we have uh n times this uh this encoder block in decoder block so uh this method here produces n identical layers uh so it it copies each module so you get n of them so that allows you to do the nx here the core encoder is a stack of n layers where we also have a layer norm after all of them or no sorry this is a final final layer moments and then we pass the uh the input and mask through each layer and turn so we just have a for loop over each of the the layers here and the layer norm what this does is this basically attempts to normalize so i mentioned we’re trying to set the mean uh close to zero and the um and the variance close to uh close to one so you can see that that’s kind of doing that here and we um we add dropout uh oh yes so dropout is uh kind of like regular um regularizing the model to prevent it from overfitting we have layer norm and uh then for each of these blocks you’ll notice that we have the multi-head attention and then we have this add-in norm after this so um this sub-layer what it’s doing is it is doing layer normalization and it’s doing layer normalization it’s calling the uh the sub-layer function here it’s doing dropout over the output and then it’s adding a residual connection so that’s kind of the the yellow block in the transformer stack here and then each layer um has in the encoder has two sub-layers one is the self-attention layer one is the feed-forward layer and then this is cloned you know uh two times uh essentially there then the decoder also has a stack of uh n uh layers basically and uh the decoder is the same except in addition to having self-attention it has a source of tension in the feed forward layer so that’s basically what you can see here in the diagram uh and then it talks a little bit about masking here so uh that was the masking that i talked about in the um in the lecture where you’re basically creating this mask that gets rid of all of the things that are in the future for the decoder and this is applied only to the decoder only to the decoder then finally i’m going to talk about the attention how the attention is done um within the uh the transformer so basically the transformer attention is doing this sort of dot product attention with scaling you take the soft max and you multiply this by the value vector so that’s you know what i talked about in class and that’s all implemented here it’s basically doing a matrix multiplication between the query and the key vector here query and key matrix here it’s taking the square root uh based on the size of the query vector um actually this is dk but this is query so that’s a little bit strange but uh you know it makes no difference as i mentioned before and then you take the soft max and you multiply the attention by the value vector so that’s basically the multiplication here so you can see um you can make a map between the equations of the code and uh then there’s also implementation of multi-head attention um this is a little bit more involved about exactly how this is implemented so i’m not going to go through all the equations here because i want to leave time for patrick but um if you want to look at exactly how the multi-head is uh implemented you can see this here um and yeah that that’s the basic idea of uh what this looks like um i think it’s important to know what’s going on in these models but if you uh want to like actually use this in your code itself um it’s also uh pytorch is also nice enough to just have this uh transformer class for you that you don’t need to um like you don’t need to implement yourself and uh basically if you look at this it’s doing the same implementation that i just showed you but it’s you know maybe more optimized or you know like you know it’s right so you won’t be making mistakes so if you’re actually going to be using this i suggest just using the transformer module but i think it’s important to know what’s going on inside so you can understand the behavior and other things like that um great so that’s all i have here are there any questions about uh the implementation or uh other things yes um isn’t the annotated transformer i seem to remember like it’s slower because it uses for loops for the batches instead of um converting them into like multi-dimensional tensors is that ringing a bell and is that accurate or um let me i didn’t cover batches um so like batching is uh important uh it’s a very uh it’s a very important thing because like basically when you batch together multiple sentences you can get them to uh like all process in a single gpu call instead of in many different gpus calls i’ve i talked about this a little bit more in uh advanced nlp um i don’t see actually i was thinking of the of the multi the multiple heads the attention heads um because i think i think the i think it actually matches by input sequence it definitely watches because he has access to a mask so if he has a mask it’s definitely watching yeah yeah i don’t like i i seem to remember this was actually implemented pretty well pretty efficiently um i don’t um i didn’t actually talk about how the multi-headed attention is implemented but i it’s implemented in a clever way which also makes it hard to read which is why i didn’t go into the details quite as much here but um i i do think this is actually a pretty good example of how you would implement it efficiently as well um although i’d have to go through uh in more detail to make sure for sure but that’s a good question what i can tell you is the pytorch one is definitely implemented efficiently so if you want to um look and see the the best way to do that you can look there um i’ll turn this over to patrick then who uh will i guess go through the uh assignment for assignment two and this is now available on the uh on the website also if you want to follow along and let me share the screen okay do you guys see the right screen yeah cool yeah so i’m going to introduce assignment 2 which was released today uh they’re still the code still needs to be uploaded but all the instructions are there and yeah so let’s get started because we’re short on time so ah and this was developed by me and vijay um which will uh explain the later part of this sign cool so the task for this assignment is machine translation so as gram described given a source language sentence you want to relate to a target language to target language uh so conditional language modeling and the goals of this assignment are uh to understand how the standard data processing pipeline is used in empty to be able to train and empty models both bilingual and multilingual using an empty framework and to learn how these models are evaluated and finally to investigate methods to tackle the data scarcity problem in low resource language pairs which is what you’re going to tackle in this assignment so for requirements you definitely need a machine with gpu uh unlike the previous assignments it will be very very hard to run this on a cpu as in you will run but no invisible amounts of time um i i didn’t test it but i’m pretty sure you can run it on a call out but still if you have access to the credits i would probably recommend doing aws for this and in terms of packages it’s mostly the same as the last assignment you can almost certainly reuse the the environment that you have we tested it and this design will also use firstec as a backbone for training models so first tech is like uh sequence modeling toolkits and and it automates data loading training decoding and a bunch of other small not boring but like you might not want to consider for this assignment and it also supports many other tasks other than translation for the basic requirements you will not need to dive deep into fair sec but if you want to modify for um the actual requirements you probably will need to dive into faresec and and yeah you also need to install soccer blue and combat to evaluate your models and but this is all automated in the instructions of the assignment you just have to follow the the assignment and everything should be installed automatically so the assignment is a collection of scripts which will pre-process you data train your data and score you’re there and we’ll go over them uh in a few slides so for that you will be using the ted talks corpus which contains parallel data between english and 58 languages and you will focus on two low resource language pairs english azerbaijani and ecospeller russian so to pre-process the data um there’s a script that does the following steps we will read the raw parallel data you will learn about byte pairing coding separately for the source and target languages and in particular this assignment will use sentence piece which is a particular implementation of uh bypass encoding we’ll apply bp to all splits there’s also a very minor data cleaning pipeline which just filters based on the size of sentences and finally you’ll binarize the data which makes it efficient for first to trade and crucially for this assignment we simplify the bits sometimes you also do some tokenization using moses prior to learning the bp but in this case we simplify then we just use center space which works fine by itself but sometimes people do moses organization before just as a remark for modeling um in training and generation uh we’ll use the reform architecture as describe diagram and embeddings will be shared between source and targets despite the fact that they use different bps this is still possible uh and this will be trained to minimize cross entropy with adam and decoding will be done using beam search in particular it’s fixed to beam search of five and is an example of what the scripts call and this is just like cli for fersae so you don’t have to worry about too much about training loops or anything for evaluation uh for mt you traditionally don’t look at perplexity because it’s not a good measure of how the models um perform in terms of translation and in this assignment we’ll use two metrics blur which is uh the de facto standard for empty evaluation which uses n-gram overlap between the reference in the target and because there’s some concerns with the reliability of blue and this correlation with human judgment would also consider comet which is part of a new generation of neural neural model based metrics which rely on pre-trained models and are trained to optimize relatively human correlations again you don’t have to think too much about it because there’s a script that already does it it’s just good to know that you’ll evaluate your models based on both okay and i’ll turn it over to vijay thanks patrick so the minimum requirements for this assignment involve first training these mt models on bilingual data and so here we would we you we expect you to train models from azerbaijani to english and vice versa and also bella russian english and vice versa next slide but then the more interesting bit which is also required in the minimal in the in the um our minimal expectations is to train multilingually using a transfer language which in the case of azerbaijani is turkish and we have scripts to do this already for both bilingual and multilingual but you’ll have to modify these scripts minimally in order to support belarusian and we’ve also introduced because nowadays while multilingual training is still very effective there’s a new paradigm that’s become super popular which is fine tuning a massive multilingual model that’s been trained on a large amount of data and to allow you to try this approach which is now pretty predominant uh you can also instead of doing multilingual training you can fine-tune a model that we have provided that has been trained on the floor s 101 data set which is like a massively multi-lingual mt data set and uh you can instead use this strategy and this will also get you to the um to the b b plus grading range next slide uh but uh i think where this assignment gets really exciting and what you need to do to get um an a minus a or a plus is to add extensions beyond what we provided in these scripts and uh i think graham will be talking about a lot of this in the next week some of it he already talked about this week but um uh data augmentation which he hasn’t talked about yet is one uh very promising strategy you could use uh uh these terms will make more sense once uh his lecture is completed next slide you can also try to use better transfer languages we’ve currently suggested two basic transfer languages for both azerbaijani and belarusian but as you might remember from the lecture last week there’s more of a structured way you can choose a good transfer language based on your knowledge of typology or based on learn models and we encourage you to try this and see what what results you can get and then lastly uh uh in terms of the data our baseline models use sentence piece for byte pair encoding which is basically a way of breaking up words into sub words but there’s like many other ways to do this and this is a particularly interesting and active area of research and so we encourage you to consider different variants of this and lastly this is probably the most broad category um feel free to try different models different uh training algorithms different ways of uh doing learning or different uh and uh there’s like a huge space here so we really encourage you to be creative and also when you when you choose something analyze it and try to deepen your understanding of why these things work in machine translation and just for reference like the assignment includes a bunch of references to different techniques that seem to work for this feel free to follow them or do your own literature review of course and in terms of what we ask from you when you submit the homework first you need to submit the code which should include both the minimum requirements for like the scripts that you’ve modified in order to support the um the bilingual and multilingual training for the language pairs in the assignment but also for the additional modifications that you make we also and this is probably the most important part uh we want to see a write-up that provides some solid analysis of uh of the methods describing the assignment and lastly uh we don’t want you to submit the actual model files because these are quite very large and it’ll take a long time to upload but we do want to see you submit your model outputs once you start playing with the software this will it’ll make sense what this is but basically this way we could compare the translations that your model predicts with our references and um reproduce your evaluations just to make sure everyone’s on the same page and these are all submitted on canvas as a tarball like the previous assignment and so i’ve been touching on this but uh the grading tranches are like first for the minimal requirements you just need to reproduce the um modify the scripts that we have for both azerbaijani and belarusian and that will get eub if you couple this with detailed analysis then you’ll get a b plus to get an a minus you need to implement at least one pre-existing method or a new method to try to improve the ability to do this multilingual machine translation next slide and then um to get an a we expect you to do uh two or more several methods to improve multilingual transfer and you might find that uh some of these methods are uh more lightweight than others so uh it might be quite doable to try uh several and uh we we also would hope that one of the methods you try is actually novel something that you’d come up with and then lastly building on that a um if you’ve done something that is particularly extensive or creative or your analysis is like super compelling then that’ll get you an a plus and just to recall that this is a group assignment unlike the first assignment so it’s expected that it will be done in groups of two three uh i’m not exactly sure how this will be arranged but i’m pretty sure gram and you don’t know what’s the best way to do this um but just to apply for expectations yeah regarding groups um feel free to make groups uh you know obviously you’ve already talked to people about um the language intent uh things so that would be one option uh but you’re obviously not obligated to do that if you don’t have a group yet um but would like to form one um please i think one one good idea would be to email the ta mailing list or put a post on piazza um by around the time the first assignment is due which is you know very soon obviously um and so then like when you finish the first assignment if there’s anybody who doesn’t have a group yet we can try to match you up with uh somebody else so um yeah um are there any questions about uh these yeah just uh if you need anything uh feel free to come to our office hours but mine and vijay’s or the other tiers can potentially help you as well and if you want to contact us piazza obviously or feel free to email us or the ta mailing list</p>
</blockquote>
</div>
</div>
</div>
<section id="see-also" class="level3">
<h3 class="anchored" data-anchor-id="see-also">See also</h3>
<p><span class="citation" data-cites="bahdanau2016neuralmachinetranslationjointly">Bahdanau, Cho, and Bengio (2016)</span></p>
<p><span class="citation" data-cites="luong2015effectiveapproachesattentionbasedneural">Luong, Pham, and Manning (2015)</span></p>
<p>Reference: Self Attention (Cheng et al.&nbsp;2016)</p>
<p><span class="citation" data-cites="vaswani2023attentionneed">Vaswani et al. (2023)</span></p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-bahdanau2016neuralmachinetranslationjointly" class="csl-entry">
Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2016. <span>“Neural Machine Translation by Jointly Learning to Align and Translate.”</span> <a href="https://arxiv.org/abs/1409.0473">https://arxiv.org/abs/1409.0473</a>.
</div>
<div id="ref-luong2015effectiveapproachesattentionbasedneural" class="csl-entry">
Luong, Minh-Thang, Hieu Pham, and Christopher D. Manning. 2015. <span>“Effective Approaches to Attention-Based Neural Machine Translation.”</span> <a href="https://arxiv.org/abs/1508.04025">https://arxiv.org/abs/1508.04025</a>.
</div>
<div id="ref-vaswani2023attentionneed" class="csl-entry">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. <span>“Attention Is All You Need.”</span> <a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Translation {Models}},
  date = {2022-02-24},
  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w06/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Translation Models.”</span> February 24,
2022. <a href="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w06/">https://orenbochman.github.io/notes-nlp/notes/cs11-737-w06/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Multilingual NLP</category>
  <category>NLP</category>
  <category>Notes</category>
  <category>seq2seq</category>
  <category>NMT</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w06/</guid>
  <pubDate>Wed, 23 Feb 2022 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Data-driven Strategies for NMT</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w07/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/tiling.png" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>course banner</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/XeDCP0newd8?list=PL8PYTP1V4I8BhCpzfdKKdd1OnTfLcyZr7&amp;index=17&amp;ab_channel=GrahamNeubig" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Lesson Video
</figcaption>
</figure>
</div><div id="sup-slide-deck" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div>

<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Transcript
</div>
</div>
<section id="introduction" class="level3 callout-body-container callout-body">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<blockquote class="blockquote">
<p>This time i’ll be talking about uh machine translation uh one more time uh i didn’t get a chance to talk about evaluation of translation two times ago so i’d like to start out with that because that’s pretty important and um then we can talk about uh the data augmentation strategies is this a little bit bright like and hard to see for people yeah okay did anything change okay cool um great so let me skip to the evaluation part since uh we’re going to be catching up on that so Machine Translation Evaluation so machine translation evaluation is a very important and difficult topic in doing machine translation research and in fact i think we’ve gotten to the point where almost evaluating how well we’re doing is maybe as difficult as like actually doing the translation itself and the reason for this is um if we output a translation there are many different correct translations right so if i say um like we could have paraphrases where the output is this is a dog i see a dog um there is a dog here other things like this and all of those would be appropriate for uh you know an equivalent sentence in another language Manual Evaluation so um the basic evaluation paradigm uh in machine translation there’s two different types um there’s human evaluation or manual evaluation in automatic evaluation and in the this is the basic evaluation paradigm for uh automatic evaluation where basically what we do is we have a parallel test set where we have an input and an output in the language fair that we’re interested in we use a system to generate translations and we compare the target translations with references um before i talk a little bit more detail about that i’d like to talk about kind of uh manual evaluation and this is the gold standard of doing evaluation of translation systems where basically we ask human evaluators to go in and check whether uh the answer is correct or not and to give an example um we’ve taken a source side sentence we generate some outputs and you can either evaluate by having a human evaluator look at the source in the output or look at a reference and um in the output and the reference would be like the so-called correct translation looking at the source and the output only works if you’re bilingual in both languages and it’s somewhat difficult to get bilingual speakers or at least more expensive however given the quality of machine translation systems nowadays very often you don’t know if the output of a human translator like your reference is better than the machine translation system uh so like very often if you hire a person to do evaluation you know they might not try super hard um and uh like i mentioned before so kind of the gold standard is to get somebody who knows the source and the target to do the evaluation um there’s a number of different axes along which you can do evaluation i just listed um a couple of them here one is adequacy and adequacy is basically whether the meaning of the translation is conveyed properly and the uh in this case this is the correct answer here you would know this if you knew japanese um which you know most people don’t but um if you knew japanese you would know the first one is correct so this is perfectly uh adequate it conveys the target message the middle one is um conveys the target message but is this fluent so it would score high on an adequacy scale but on a fluency scale it would score low the one over here is fluent um but not adequate so basically it switched the subject to that object for uh order so it would be wrong um and uh one notable thing about fluency is you don’t need to know the source language to evaluate fluency all you need to know is the target language because it only has to do with whether the output is fluent or not you can also do pairwise evaluation which just says which one of these is better um one of the good things about pairwise evaluation is it’s very simple uh because you just ask question which do you like better which do you think is a better translation the problem with it is it doesn’t give you an absolute idea of how well you’re doing so if you have two really bad systems and say which is better one might be better but they’re both really bad if you have two really good systems and say which is better one might be better but they’re both really good so um kind of absolute scales have that advantage um another thing is uh just like you might get bad translators you might get lazy evaluators uh you know if you hire people on mechanical turks uh and they’re not very motivated for example so um you need to be careful about quality control as well Human Evaluation Shared Tasks there’s human evaluation shared tasks so the most famous one is the conference on machine translation shared tasks so i can show you a little bit what this looks like they basically have um a whole bunch whoops they have a whole bunch of tasks uh that you can participate in most of these are tasks for actual uh translation but they also have evaluation tasks on metrics and quality estimation so basically what you try to do here is you try to create a metric that has the highest um correlation with uh with human evaluation and um for quality estimation what this is is this is essentially evaluation without a gold standard reference so you’re just given the input and the output and you want to guess how good the system output is and this is harder obviously because you don’t have an example of what a good translation looks like but it’s also very useful in practical situations where like let’s say you’re a machine translation company and you want to decide whether um you need to get a human translator to go in and check the output and correct it or something like that so if that’s the case if you can estimate very accurately whether the input and output are correct or not then that would save you money save you time uh give you confidence in the results Blue Scores there’s also other leaderboards and stuff for other sequence sequence models but that’s a little bit less important for uh this multilingual class um so there are other metrics uh like blue scores so blue score is very famous you know if you’ve done any research on machine translation or even heard of it you probably encountered blue the exact details of how blue is calculated are um what you do is you take the precision of engrams output by the system so for example if you look at the unigram precision that’s out of all the unigrams that the system output how many of them exist in a reference or one of the references that you’re provided um if in the case that you’re using multiple references and this gives you a position of like three out of five and then you take the geometric mean of uh engrams usually one to four um and then you also have a penalty for outputting two short sentences because one way to improve your precision is to not output very many things um so this is basically to prevent you from gaming system but the important thing is now the details of how blue is calculated probably the important thing is that this is a lexical metric which means that you’re just doing exact match with the references um and this has a few uh issues um one of the issues is essentially that um you so there’s there’s two major issues um that cause blue to either underestimate how good a translation is or overestimate how good a translation is blue tends to underestimate how good translations are when the translations are uh paraphrases of the true reference so um if you uh for example had um i have uh like i went uh i went to the store and bought a book yesterday and you compare that with uh yesterday i bought a book at the store those are almost identical in meaning but they would get a low blue square because i’ve just rearranged the phrases a little bit um so that’s when blue tends to underestimate scores blue tends to overestimate scores when you get very critical words in the sentence wrong but get everything else right so for example um could you please send this big package to uh philadelphia turns into could you please send this big package to japan um that would get a very good blue score because most of the words are the same but your package would go to japan and that’s probably not what you intended by that otherwise right so um Shortness Penalty uh that’s uh the the downside of blue it’s basically not smart enough with respect to these so recently um yeah so the brevity the brevity penalty basically gives you a penalty if your output is shorter than the intended output so if the reference is like 20 words then you’re um and your sentence is uh 15 words you would get a penalty of about 0.75 it’s not exactly like just the ratio it actually drops off faster and stuff like that but that’s a basic idea that’s a really good question so you pay a penalty in your precision your precision goes down by a lot because there’s no way to get good precision if you output too many things um one thing you should know about blue if you’re using it in your research which you might is that it’s very very sensitive to the length so if your length is a little bit too short or a little bit too long it hurts your blue really badly so that’s another problem with blue essentially is that it’s not sensitive to like paraphrases that are too long or too short as well um so recently in the past three years or Bert Score so there’s been a huge um improvement in embedding based metrics which are basically metrics that you know take advantage of um recent nlp techniques and one of the first ones was bert score and these can be separated into unsupervised metrics and supervised metrics unsupervised metrics require no annotated data of um whether a [Music] translation is a good translation or a bad translation supervised metrics are trained to basically regress to an estimation of how good a transplantation is or not so a birth score is an unsupervised metric that’s based essentially on the similarity between burton betting so it has this matching algorithm where you basically uh for each word in the output you try to find um you know how good uh it matches with one of the words in the input and this is good because it can do things like handle paraphrases as long as the paraphrases have similar bert embeddings um another famous one is bluert BlueRT and what they did essentially was they trained bert to predict uh human evaluation scores um so they essentially solve a regression problem from the uh sentences uh like the reference in this uh system output to an evaluation score so they’re just going in directly predicting uh evaluation and they have a bunch of other tricks like unsupervised training where they try to predict blue or rouge or other lexical metrics beforehand and that makes it more robust um the uh favorite one that uh we use in our Comet research on mt now is a comet and comet is also similarly it trains the model to predict human evaluation uh but in addition to using the uh just the system output in reference it also uses a source sentence which means that essentially uh i talked about human evaluation right uh where you can either ask a human evaluator to look only at the the reference or also look at the source and um for a similar reason to why we would like a speaker human speaker to do that um it’s also useful to have the model do that because the model can look at the source and see if the information is reflected in the target Bart Score and the final one um uh prism’s another one based on paraphrasing a final one is a bart score environment score is one that i’m a co-author on this is a unsupervised metric uh that is based on basically a generative model that tries to generate the uh the system output using the reference or the reference using the system output or the source using the system output et cetera et cetera and um bart score i think is good because it’s unsupervised like birth score but it’s essentially more accurate and more controllable so you can do things like calculate recall calculate a precision and other things like that so if that sounds interesting you can take a look at the paper as well but basically if you’re doing empty i would suggest using comment now because uh it’s well supported it has a nice uh package it’s pretty widely tested and follow-up reports have suggested that it has very good correlation with human evaluation um so that’s my suggestion uh you can Meta Evaluation trust me but you don’t also you don’t don’t necessarily need to trust me so like as i mentioned um meta evaluation basically what it does is it runs human evaluation and automatic evaluation on the same outputs and calculates the correlation this is what they do in the wmt shared tasks like the wmt metrics task for mt other things for summarization etc and one interesting thing is that evaluation as i mentioned at the beginning is pretty hard especially with good systems so most metrics actually had no correlation with human eval uh over a subset of the best systems at some of the wmt 2019 tasks which means that basically all of the evaluation metrics we had were kind of broken on uh like evaluating really good mt systems so fortunately now we have comment we have other things like this that actually seemed to be a lot more robust but um it was a major problem so you basically calculate the correlation you calculate pearson’s correlation there’s experiments correlation and um the way you do that is you human you do human eval of a whole bunch of sentences or humans develop a whole bunch of systems and you try to find the metric that has the highest correlation between the evaluation scores of the systems and the evaluations given by the humans yep all right they often don’t support that many languages well so that’s a good that’s a really good question um i many of them do uh use something like embert or xlmr which support a lot of languages xlmr actually envert’s a little bit more biased xlmr has pretty good coverage of the most common languages in the world but of course as you go down to less well resource languages that’s going to continue to be a problem um there you might be stuck with blue for now but honestly if you have really bad systems really bad empty systems i still think blue is probably good enough in many cases oh another option is carefu chrf and that’s a character-based evaluation metric for mt that’s particularly good for languages with like rich morphology or something like that so i think when you’re working with low resource languages your mnt systems are also going to be really bad so any metric you have is still going to be like reasonably good at measuring progress so um cool yeah so um the next thing i’d like to Database Strategies talk about is the database strategies uh to low resource mt um there’s not a whole lot of content here so i’ll try to go through it rather quickly to leave time for the discussion um but basically we have data challenges in the resource mt um so mt of high resource languages with large parallel corpora gives us you know very good uh translations but low resource languages with small parallel corporate you just train there you can end up with nonsense so this is an example of a system trained on 5 000 languages and the most frequent failure state is basically that a neural nt system will just spit out something that has nothing to do with the original inputs there’s a famous example of this um so uh that says why is google translate uh spitting out sinister religious prophecies um and basically if you put in a dog dog dog dog dog in maori it outputs doomsday clock is three minutes at 12 we are experiencing characters in a dramatic development uh in which jesus returns um [Music] can you guess why this happened exactly they use bible data in training their system and when you use bible data and training your system and your system doesn’t know what to do because it has so few resources or it sees something it doesn’t know it just reverts to using the language model and basically outputs whatever the language model thinks and uh thinks it looks likely and so you know if your system is trained on bible data that looks like the bible if your system’s trained on something else it looks like something else High and Low Resource Languages um so you know that’s basically what happened here as well um so some ways to fix this um we can transfer from high resource languages to low resource languages so basically what you do is you train on a high resource language or multiple high resource languages and then you adapt to the low resource language one the simplest way to do that is just to continue fine tuning on the low resource language um you can also do joint training with the low resource language in the high resource language so just concatenate all the data together um and uh in training so um this is okay but there are some problems with this as well one problem is a sub-optimal lexical or syntactic sharing and another problem is it’s not possible to leverage monolingual data because you still require a parallel data here and um i’m going to be talking more about like lexical overlap and loanwords and stuff in uh in the next class so i’ll cover that more there but basically suffice to say the high resource language and the lower resource language are different so training on different data is sub-optimal for uh information sure Data Augmentation so if we think about data augmentation data augmentation is basically generating other data that looks like the data that you want to have the very convenient thing about this is um generating more training data and feeding it into your existing system is uh easy but effective in uh in improving mt performance so it’s actually a pretty widely used technique now so if we look at the available resources um we might have a low resource language parallel data a high resource language parallel data and also for example target data which is monolingual hence the m here and uh what we do is we would like to create augmented data where we have target data and like pseudo low resource language data and uh train our model on this with the idea being that if we can create this uh this will be closer to our final evaluation scenario where we uh where we want to generate the target given a low resource language Back Translation so the first example is a back translation and the way back translation works is basically we train a target two low resource language system and we take our monolingual target data and we generate uh fake low resource language data by translating the target data into the low resource language data so um this is how it works uh we take our target to low resource language system um we back translate using the system and then we train a low resource language to target language system using the concatenation of this augmented data in the original data and the key point here is that when we are um when we’re training like a sequence sequence model or a machine translation model um we’re we’re training it to do two things we’re training it to do language modeling on the target side only um and we’re trying to do mapping between the source side and the target side and in order to do language modeling we only really need good target site data so even if there’s some degree of error in this uh like low resource language here we’ll still be able to learn uh target site data and we’ll be able to learn a the language model from target side data and we’ll be able to learn a mapping you know even if it’s imperfect from the low resource language to the target language Training Schedule so there’s a couple ways to generate translations uh when doing back translation um the first one is using beam search oh sorry yeah uh yeah that’s a really that’s a really good question so the question was is there any sort of training schedule that you use when you do this um so the the kind of quote-unquote obvious training schedule that you might do is you might train jointly on both of them at first and then fine-tune on this data over here um and uh that would make sense because you know this is good data this is like actually translated data however there’s another issue um which actually is not super obvious at first but it’s maybe obvious in hindsight which is that if this data is all from the bible and then you want to translate news then actually fine tuning on bible data will be really out of domain and cause issues for you um so in fact in the original black translation paper they threw away this data and only trained on this because it was more in domain and that ended up giving better results but that was predicated on the fact that they have a good you know batch translation system in the first place so um it’s not necessarily clear what the ideal schedule is but you would almost certainly benefit from some sort of schedule or balancing or something um but that’s a complicated hyper parameter so because it’s a complicated hyper parameter it’s also very common to just concatenate the two and these are good details to know for assignment too by the way because they might make a difference in your final scores Generating Translations um how to generate translations so beam search is one way uh and basically what this is doing is selecting the highest scoring output uh this was done in the original paper um this has the advantage of having higher quality but also lower diversity in the outputs and the potential for bias um so you might uh like for example uh one result is beam search tends to mostly output um pronouns from the majority gender because they’re over represented so you might get only get mail inflections uh if you do beam search so that’s the type of data bias that could result from here and so the other option is sampling and what you do is you randomly sample from the back translation model which gives a lower overall quality but higher diversity and most reports say this works better at the moment simply and uh also we had a recent paper uh which i’m going to introduce in a second but this has kind of a theoretical explanation for why we think sampling should be better which is that it’s a better model of the underlying data distribution that we’re trying to model so um i i think i’m pretty firmly a believer that sampling is the way to go there’s also a method of iterative back In iterative back translation translation this is particularly useful when you have a large monolingual data in both languages and again the idea is simple you train a low resource language to target system first so this is going in the direction you originally want to translate you generate pseudo data um with the target language you use that to train your target to low resource language system you back translate and then you use this to train your final system so this is uh now you have three systems your forward translation data augmentation system your back translation data augmentation system and your forward translation final system um you can do this as many times as you want obviously um you could also do it on the fly in the process of training the system so uh this can become arbitrarily complicated if you want Metaback translation um just one example of this um this is a paper that i just talked about but we have a paper called meta back translation which i think is kind of a an interesting idea um so normally uh when we’re training this system uh to train the uh the low resource language to uh the target language system we’re back propping the gradient uh from the slow resource language data um but uh we can also uh do a back propagation step where we uh um basically train oh sorry that arrow is thrown i apologize so the arrow actually should be going from here around this to here so the basic idea i’ll fix this later in the slides but the basic idea is we use the signal that we get from um from training the final system that we want to train to update the parameters of the back translation system so we’re essentially training the ideal back translation system to train a good forward translation system so um this is a uh i like the idea behind here uh which is basically the final goal of the back translation system is to improve the forward translation system so we can directly optimize it to do this Metaback translation issues um so there are a couple issues here um the first issue is that back translation often fails in low resource languages or domains and um as a solution uh one thing that we can do is we can use other high resource languages or we can combine them with monolingual data maybe with denoising objectives which we’re covering in a following class and we can perform other uh varieties of rule-based uh augmentation so i’m gonna go through these uh in a little bit uh in maybe uh in a few minutes so um also actually we’ll have discussion about uh about these two so maybe i’ll just briefly explain the idea and we can discuss more in the discussion for people who read those papers so um using uh high resource languages High resource languages augmentation and augmentation um the problem is uh target to low resource language back translation might be very low quality so the idea is we can also use a high resource language that’s similar to the low resource language and basically for example if we have um something like azerbaijani in turkish azerbaijani and turkish are very highly related so maybe we could uh use information from azerbaijani to english translation back translate into az into turkish which is certainly going to give us higher quality data and use that to augment our data for azerbaijani english system and then we can just throw away this azerbaijani data that we know is not going to be very useful um so that would High resource languages pivoting give us additional high resource language to target language data and another thing we can do is we can augment via pivoting and so basically what that does is that gives us data where we take the high resource language data and we translate that into the low resource language and presumably translation from the high resource language to low resource language is easier because these languages are more related so basically what this does is that gives us a better uh like low resource language pseudo data here and um we can also do a similar thing where we generate more high resource language data and uh this basically gives us um three different ways to create this pseudo-parallel data between the low resource language and target language another simple trick uh this is kind of Monolingual data copying like frustratingly effective at improving your models is monolingual data copying and um the issue is that um back translation uh may help with structure but one of the issues with the resource language systems is that they tend to fail really badly on um unusual vocabulary so like for example proper names or something like this so you might get a back translation system that’s very good at getting the structure right but get it gets you know all of your proper names and entities incorrect so uh basically one thing that you can do here is you just copy the target data into the source data and then you’re done um and uh this uh kind of guarantees to maintain the entities so um or the the rare words so uh that will help uh mitigate these issues of like vocabulary being dropped yeah something to point out with copying is that even in languages with different scripts it seems to work really well maybe because of auto and clutter objective stuff yeah even in languages with different scripts and there’s actually a nice paper by the same authors who wrote this paper um Transfer learning where they examine this and basically the um the idea here is they are trying to figure out like what transfer learning why does transfer learning have this um uh positive effect and basically one of the things that they show is that even just you know making sure that the length is the same is approximately the same or making sure that the words are output in approximately the same order as the input is uh is effective for improving translation accuracy so if you have a low resource language um the translation system might drop half the content or it might uh like totally mess up the order or something like this so this uh this paper is demonstrating that kind of just like a monotonic bias and a bias towards outputting approximately the same number of words gets you a long way in improving the results um which of course monolingual data copying would also do um and so for the um Dictionarybased augmentation for the final things uh which we’re also reading so we can talk more about them in the discussion um we had dictionary based augmentation and dictionary based augmentation um basically finds rare words in the source sentences um it could also be in the target sentence and uh tries to replace uh the words with other words that are kind of in the same semantic class so it replaces car with motorbike um and then using a lexicon it uh replaces the words in the targeted sentence as well so it’s basically creating more uh sentences to augment uh augmented data uh with uh like words that are less frequent uh in the original purpose and in order to do this they need to use a tool called word alignment and what Word alignment word alignment does is it essentially takes in two parallel corpora and um the parallel corpora you want to find which words align to each other in the uh source and target sentences and this is useful for a number of reasons it’s useful for analysis it’s useful for cross-lingual transfer learning i talked about supervised alignment as a training method uh last time i believe um and there’s a couple uh methods to do so um there’s again traditional symbolic methods uh which like blue are based on exact lexical match or um you know some variety of clustering uh giza plus plus has some clustering involved in it but recently uh neural methods have been largely um uh outperforming these and i can recommend a highly our aligner called awesome lion i i didn’t name it um i’m far too humble to name my alignment or awesome line but um uh but it’s pretty awesome i have to say um and basically it uses multilingual perks and it tries to find things that are similar but it’s also fine-tuned multilingually on supervised data so basically there’s some supervision that goes into it to try to uh inform the aligner about the outputs and it works on any language that’s included in mvert again uh like the question before uh it won’t work on very low resource languages of course so then you might be stuck with keystone plus plus and faster um Word by word data augmentation so you can also do things like word by word data augmentation where you simply translate sentences word by word into the target sentence using a dictionary uh this is another frustratingly you know effective method like monolingual data copying um however there are problems like word order and syntactic divergence so if you get like i the new car bought number one the order is strange number two uh these words don’t actually align with each other so that’s a problem uh so um Reordering other things you can do or you can try to decrease this divergence with uh reordering or rules so this was also another paper in the potential reading and basically what the idea is that you a priori do some reordering from one language uh from english into like reordered english and then do data augmentation on top of that and the good thing about this is like english has a lot of analysis tools you could like do syntactic parsing of english get the syntactic structure build reordering rules on top of that and then uh just apply dictionary-based translation and then the hope would be that you would get something that looks a lot more like japanese than if you just translated english word by word and one interesting thing we showed here was we demonstrated that this was useful for japanese translation but then we applied the exact same reordering rules and also applied it to wigger which is another language that’s completely different different language family but it’s um it has a very similar syntax to japanese so because of that the exact same reordering rules for english were still effective in improving the results for weaker english translations so um because of that you know it’s not language dependent it’s rather syntax dependent and because there’s syntactic similarities between the language it helps so um yeah given that we now have um the Assignment assignment uh actually this is this slide is missing one of the uh one of the papers that was a potential paper to read um so first before we go to the discussion are there any questions so i kind of breezed through it the last part quickly but hopefully we can also talk about them in the discussion um okay if not this time we’re going to try a new experiment we’re going to try to make six groups um so the groups are going to be half the size and they’re going to be front middle front right front left back middle back left back middle in that right so we’re gonna ask everybody to talk a little bit more quietly uh but also you’ll be in a smaller circle so hopefully that’ll be easier um and yeah let’s go ahead and actually guys uh since we’re running a little bit late i think maybe we’ll skip the reporting part this time is that okay and we’ll just you know be within our groups and if there’s anything really interesting we can share on piazza or something okay</p>
</blockquote>
</section>
</div>



<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Data-Driven {Strategies} for {NMT}},
  date = {2022-02-03},
  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w07/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Data-Driven Strategies for NMT.”</span>
February 3, 2022. <a href="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w07/">https://orenbochman.github.io/notes-nlp/notes/cs11-737-w07/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Multilingual NLP</category>
  <category>NLP</category>
  <category>Notes</category>
  <category>NMT</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w07/</guid>
  <pubDate>Wed, 02 Feb 2022 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Translation and Translation Data</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w10/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/tiling.png" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>course banner</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/uDiiGWlU4K4" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Lesson Video
</figcaption>
</figure>
</div><div id="sup-slide-deck" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div>




<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Translation and {Translation} {Data}},
  date = {2022-01-24},
  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w10/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Translation and Translation Data.”</span>
January 24, 2022. <a href="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w10/">https://orenbochman.github.io/notes-nlp/notes/cs11-737-w10/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Multilingual NLP</category>
  <category>NLP</category>
  <category>Notes</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w10/</guid>
  <pubDate>Sun, 23 Jan 2022 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Translation and Translation Data</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w08/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/tiling.png" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>course banner</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/uDiiGWlU4K4" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Lesson Video
</figcaption>
</figure>
</div><div id="sup-slide-deck" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div>




<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Translation and {Translation} {Data}},
  date = {2022-01-24},
  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w08/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Translation and Translation Data.”</span>
January 24, 2022. <a href="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w08/">https://orenbochman.github.io/notes-nlp/notes/cs11-737-w08/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Multilingual NLP</category>
  <category>NLP</category>
  <category>Notes</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w08/</guid>
  <pubDate>Sun, 23 Jan 2022 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Translation and Translation Data</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w11/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/tiling.png" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>course banner</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/uDiiGWlU4K4" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Lesson Video
</figcaption>
</figure>
</div><div id="sup-slide-deck" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div>




<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Translation and {Translation} {Data}},
  date = {2022-01-24},
  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w11/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Translation and Translation Data.”</span>
January 24, 2022. <a href="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w11/">https://orenbochman.github.io/notes-nlp/notes/cs11-737-w11/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Multilingual NLP</category>
  <category>NLP</category>
  <category>Notes</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w11/</guid>
  <pubDate>Sun, 23 Jan 2022 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Words, Parts of Speech, Morphology</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w04/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/tiling.png" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>course banner</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/doZLYpTW_S0" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Lesson Video
</figcaption>
</figure>
</div><div id="sup-slide-deck" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div>

<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>What is a Word, What is Morphology</li>
<li>UD Treebank Morphology Annotations</li>
<li>Morphological Analysis</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- Transcript here is machine generated no punctuation and I eddited it a but but it needs more work and I dont see the benefits unless I can run it through some tool. -->
<section id="words" class="level2">
<h2 class="anchored" data-anchor-id="words">Words</h2>
<blockquote class="blockquote">
<p>So now um we’re going to move on to the main part where we’re um going to talk about um words which sort of seems really fairly appropriate for any um any nlp class because words are pretty important okay and this is going to give you a little bit more definition of what we mean by words and how that’s not always as well defined um as um it might be in languages like English so let’s try and count the words in this sentence bob’s handyman is a do-it-yourself kind of guy isn’t he okay so that’s a fairly standard sentence although i have selected it to be particularly interesting for this case but the question is where are the words okay so what i’ve done is i’ve tried to highlight what people might think the words actually are now if you grew up in um europe and maybe other places as well you probably think that white space separated tokens is a good approximation for words and it is a good approximation for words but from a linguistic point of view it’s actually a little bit more complex than that and that’s what we’re actually going to talk about and when we actually look at other languages that’s the thing we want to highlight that the notion of what a word boundary actually is might not be as trivial as what you hope it’s going to be okay</p>
</blockquote>
</section>
<section id="whitespace" class="level2">
<h2 class="anchored" data-anchor-id="whitespace">Whitespace</h2>
<blockquote class="blockquote">
<p>Let’s have a look at the first word of the white speak white space um uh identified token is bob’s the apostrophe s is what’s called a clitic it’s not isolated word on its own it can’t stand on its own it only appears when it’s actually bound to some other word but in some sense it’s independent of the word that it’s bound to so what you can have is um actually it can go into whole noun phrases rather than just single words so we talk about jack and jill’s bucket we’re not talking about jack and jill’s bucket we’re talking about jack and jill in the bucket belonging to jack and jill so that apostrophe s isn’t just going on the immediate token before but a bunch of tokens before that okay so be aware that that apostrophe s is a somewhat special thing in English and sometimes it’s a good idea to actually separate that apostrophe off and treat it as an independent token than actually just putting it together as bob’s let’s look at the next one handyman is really a now known compound okay we put a space in it because we’re not german um but uh um but some compounds in English don’t get a space between them and it’s sort of really quite complex to know which ones are which and it’s really sort of up to the speaker and to decide how to do that but sorry the writer from the speaking point of view it’s even harder to distinguish between these but handyman really here is being used as a single word and it would be useful to keep them together even though they have a ascii white space between them is is a word as a word now let’s have a look at do it yourself it’s hyphenated um and it really is a sort of set phrase it can be shortened to diy um and we’d like to treat that as a single word but sometimes hyphenated forms should be separated and we have to make um interesting decisions about that now we’ve got words like kinda and isn’t it that are fairly standard contractions um in English we could write them out as kind space off and we could write isn’t it as is not but often when people are speaking they don’t do that and they actually do reduce form and sometimes when writing they may actually do that simplified form as well and do use these contractions and we have to make some decision about how these actually might appear and whether we want to separate them out many of these are sort of closed class they can only be applied to a number of words and they’re not general to anything but some of them are not apostrophe s can clearly go into anything and apostrophe ll can sort of go into any noun as well um so that’s a hard it’s a decision that we actually have to make and it will affect all of our downstream tasks once we decide how to token these tokenize these into what we’re going to term words how we’re going to have word embeddings how we’re actually going to do parsing or whatever our next task is going to be.</p>
</blockquote>
</section>
<section id="other-languages" class="level2">
<h2 class="anchored" data-anchor-id="other-languages">Other Languages</h2>
<blockquote class="blockquote">
<p>In other languages it can be way more complex and sometimes way more simple. It could be that we actually have no spaces whatsoever and everything is joined together agglutinated um and there has to be some process to try to separate these individual parts out if we don’t separate these things out what you’re going to have in a language is an awful lot of words so if you look at turkish for example which is an agglutinate language there’s an awful lot of compounding in it an awful lot of interesting morphology in it what you end up with is the number of words number of white space separated tokens and in the language is much bigger than English for because what we maybe think about as being phrases in English are actually whole single words with no white space between them there also can be ambiguity in those particular words about how we actually decompose them and on the right here we actually have a hebrew example where i’m sorry i can’t read that this um where it can mean depending on how you separate that out into individual morphemes and her saturday and that in t and that her daughter all of those are potential meanings given the rest of the context but they’re all written as the same single word</p>
</blockquote>
</section>
<section id="linguistics" class="level2">
<h2 class="anchored" data-anchor-id="linguistics">Linguistics</h2>
<blockquote class="blockquote">
<p>Now let’s go back to our knowledge of linguistics and see how we might be able to use our knowledge of linguistics to be able to answer these questions about what words actually are.</p>
<p>Because we’re going to look at each of those and see how they we might be able to use that as a definition of a useful way of splitting things into little bits words so there could be phonetics so there could be something about um the way things are being said that could actually tell us something about that we could say it’s got to have a of uh every word has to have a vowel in it although I don’t know where apostle vs which sometimes is a violent and sometimes doesn’t sometimes and it’s sometimes just phonology and it could be something about there’s some structure that’s actually required in syllables and so everything has to be at least one syllable if it has to be a word it could be morphology if we’re looking at the actual atoms that are in the morphemes that are in the word we could talk about the individual morphemes in there and make some definition based on that we could talk about syntax we could talk about whether we could exchange that for another word in the same class and treat those as being words if they have classes over them we could talk about semantics of whether it changes meaning and we could talk about pragmatics about realistically new york is used as a single word even though we happen to put a space in it but it’s treated basically as a fundamental um a word because although it maybe historically has some relationship to the word new and the word york it really has nothing to do with that anymore um let’s have a look at the orthographic</p>
</blockquote>
</section>
<section id="orthography" class="level2">
<h2 class="anchored" data-anchor-id="orthography">Orthography</h2>
<blockquote class="blockquote">
<p>Definition so this is where white space comes into mind and be aware that even in English white space is quite complex So you know we’ve got spaces we’ve got tabs we’ve got new lines we’ve got carriage returns we’ve got vertical tabs are all within the standard ascii set but beyond that we actually have a lot more if we look at unicode and we want to make definitions of that and we also have things like non-breakable space and a non-breakable space can be used and may appear on on on the web in html and we may or may not want to decide that as being a word boundary or not depending on our definitions okay and remember we’re talking here about orthography we are pretending that the whole world writes everything down and that when they write everything down it’s the same as what speech is most languages are not written we write an awful lot and most people on the planet are actually literate but they may speak languages that they’re not literate in and when you speak you don’t put any spaces between words okay you don’t say a space between each word and we don’t do that at all okay and so when you’re wondering how chinese people can understand chinese um text when there’s no species in it think about how well you can understand speech which has got no species in it and yet you can still deal with it and the notion of a word has been around for a lot longer than the notion of writing so in other words we’ve had that notion and we need a definition of words that are actually independent of the written form</p>
</blockquote>
</section>
<section id="prosodic" class="level2">
<h2 class="anchored" data-anchor-id="prosodic">Prosodic</h2>
<blockquote class="blockquote">
<p>There can be a prosodic definition a prosodic definition is something to do with international phrases they can be hard to define but they’ll still have some reasonable definition and things which are actually grouped together in the single international phrase are often written as a single word in other languages so for example in the park in English the way i just said it is as a single international phrase in the park and many other languages would have the same concepts of location and determiners um and park all in the single word that would have no spaces between it so in the park could be treated as a single word um depending on what your definitions are going to be</p>
</blockquote>
</section>
<section id="semantic" class="level2">
<h2 class="anchored" data-anchor-id="semantic">Semantic</h2>
<blockquote class="blockquote">
<p>Semantic definitions these are word units that got some something like the same idea and so it might be useful to be able to treat these um in a reasonable way um we can think about colors that might have multiple names we can think about uh navy blue is um is a color and um but when written it would have probably have a space in it um but we may want to treat that as a color which is the same as blue or red or yellow or any other single word color because navy blue is still a color okay and although technically maybe has something to do with glue and something to do with navy um it might not be a reasonable thing to actually separate it out we can have a syntactic definition where we’re looking at blocks in these sentences so again new york is a good example here where um it really is being used in the same sense as the word pittsburgh is it’s referring to a particular city in north america and just because it’s got a space in it doesn’t mean that we want to treat it as two different cities a new um city and a york city which is elsewhere in there’s almost certainly places called york in north america but i can’t well there’s um a one just outside toronto now this comes to the notion of how can we identify classes of words and i’m currently really talking about syntactic classes of words rather than semantic classes so so colors are sort of a semantic class but i’m interested in what words have got the same class that i could exchange them without what it might change the meaning but you know they get used in the same way now we’ve all been talking talk about these standard open classes which appear in most languages not all languages but most languages nouns verbs and adjectives and adverbs there’s almost an infinite number of those um new ones can come along that didn’t exist before um but they all fall into these particular classes this is in contrast with closed classes where there’s a finite set in the language they’re sometimes called function words it might be a difficult class to list absolutely everything but you rarely very rarely find new things moving into that class so things like prepositions so in above uh behind there’s a sort of finite set of those determiners the and ah this and that and pronouns um i uh um are somewhat finite in English conjunctions and or but um not an exclusive or if you’re a computer scientist and other auxiliary verbs like is was have etc and these are sort of closed class they’re very common in the language they’re often short in a language and in most languages have something like them they might be something to do with morphemes but they don’t have random new ones appearing every day so if we think about what happens with the open class think about words which you know exist today that didn’t exist five years ago and think about some can you type them in the chat words which we have today that we didn’t have five years ago and you’ll find out that they’re all nouns verbs adjectives and adverbs and not closed classes because can anybody think of any words which we have now that we didn’t have um five years ago covid um excellent example um i’m sorry to disappoint you but if you have a look at wikipedia in 2015 there’s an article on coveted okay um but it was dull and boring and uninteresting and the covered um uh self-help group decided that they wanted to do something and become more popular but you’re right that covert was incredibly rare okay and probably was only used in occasional circumstances doomsco yes doom scrolling excellent quarantine yeah wfh all of these things are very very common now and sort of didn’t exist at all before and language is like that but all of these words for the most part are coming in to be nouns and verbs okay occasionally they’re going to be adjectives and maybe adverbs but for the most part these do change over time so you can’t list them all but for the close class ones you can sort of do it i mean there’s some really rare conjunctions that people don’t use nowadays like not withstanding okay um apart from used as examples of really rare conjunctions that’s about the only time i use that word</p>
</blockquote>
</section>
<section id="tag-sets" class="level2">
<h2 class="anchored" data-anchor-id="tag-sets">Tag Sets</h2>
<p>there’s a bunch of classes that define what part of speech tag sets the tags that are actually there and in English and many other languages they’re often based on the tag set that was used in um the pen tree bank which was one of the first large um data sets that was labeled consistently um with nouns verbs etc and there’s 43 in total i don’t know if there’s 43 there but i know from other things and there’s some that are relatively rare and maybe questionable and there’s some compound ones where sometimes things get joined together it does make distinctions between different types of nouns singular plural and and proper nouns um and um it it’s quite useful in many tag sets that are now used in in nlp are derived from this tag set because that was decided um mostly by the group at u pen at the time people were doing a part of speech tagging before that i mean even before computers they were doing it but coming up with a finite computational tag set was something that really started in the 80s and depending on the language that you’re dealing with you may want to have different tags because there are some tags that are really only relevant in some languages and not in others okay importantly um there actually is a definition of a small number of tags which is in some sense a reduction of the number of tags that were in the pen tree bank that has been used in the universal dependency m sets that originally came out of google and it’s now an independent project but it’s still quite google influenced um because of the original data sets and this covers somebody’s going to tell me the number of languages but i think it’s about 40 or 50 languages where they’re all labeled with the same tag set and produces a universal dependency grammar which is also very use useful from a syntactic point of view that over a bunch of fairly major important languages okay so um getting this tag set is useful and you might say why should i care about getting a tag set i can train from um words and the answer is yes if you have lots of data but as usual in trying to do machine learning if you can give more information in a structured way or in a reduced standardized way you can typically get by with less data and try to train better and training should happen faster so often being able to get um a part of speech for a language would be quite useful and of course this is going to be hard in the low resource language because you sort of need labeled data to start off with but there are unsupervised ways to try to find out what these tags actually are now i’m naively talking about words here and words having part of speech tag because i’m one of these English speakers where actually for the most part that’s pretty easy in English for the most part you know white space separated tokens or words and for the most part each token has got one um a tag one proper tag and the context mostly defines what it is but in most languages most languages and we really have to introduce the notion of morpheme which is sort of the single smallest atomic part of a word</p>
</section>
<section id="morphology" class="level2">
<h2 class="anchored" data-anchor-id="morphology">Morphology</h2>
<blockquote class="blockquote">
<p>Here for example in English we do have interesting morphology especially in what’s called <em>derivational morphology</em> which is not like the <em>ing</em> <em>eds</em> plurals where we’re changing some syntactic property the tense or the the number of a language but we’re actually changing the meaning and we’re often changing the class the part of the speech class so if we take a verb like <em>establish</em> we can have another verb that’s <em>disestablished</em> we can then make that into a noun by say <em>disestablishment</em>.</p>
<p>We can make it another noun putting <em>anti-</em> in front of it and saying <em>anti-disestablishment</em>. We can make it into an adjective by saying <em>anti-disestablishmentary</em>. We can make it a noun into <strong>anti-disestablishmentarian</strong>. We can then make that in a further noun by saying <strong>anti-disestablishmentarianism</strong> Now these things are a little bit extended, but actually we do this all the time. So it’s actually quite hard to really list all of the words that are in English because although some of these don’t appear very often. There will be new and novel words, and you’ll see a number per day of new words that you’ll understand. Where they’re actually morphologically variants of something that you can work out what the meaning actually is.</p>
</blockquote>
<blockquote class="blockquote">
<p>Now so it would be useful if we could get these words and decompose them into their roots and morphemes so that we can actually work out what the important classes are. So we’d like to be able to get some notion of these decomposed forms in from a word if we can do it. Now some of these forms are what we call stems or roots they’re often words on their own. And we’ll have prefixes and suffixes.<br>
In English we rarely have anything that inserts in the middle of a word we’re usually putting things at the beginning and end. In some languages you actually get sort of bracketed things that you have to put them at the beginning and the end some of the gaelics have got that. There are some things where you can actually put infixes and so there are some plural things that actually happen in interesting languages in southeast asia where they’re basically plural things where syllables or partial syllables will get duplicated and and therefore you have to deal with that.</p>
</blockquote>
<blockquote class="blockquote">
<p>In English the only example of being able to do that is um the infix form of putting swear words in the middle of a word so for example if you have the words pittsburgh and you want to put a swear word in the middle. I can only get away with this because I’m British i’m going to use the word bloody is a as a swear word although actually usually in linguistics we use the f-word but we can use the word <em>bloody</em> and if i want to put the word <em>bloody</em> in pittsburgh it’s going to be <em>pit’s— bloody—berg</em> and i could do that and it could be compounded and possibly but it’s a little bit of a stretch and maybe you would put species in in there to do that in other languages. Infects will happen in some languages you’ll actually even change things in templatic morphology in things like the Hebrew and Arabic and Tagalog is a an example. This is one of the examples from the philippines where we’ve got interesting morphology going on and we’ve got much more interesting morphology going on than what’s in English and we’d like to be able to decompose these things so that we’ve got finite sets of morphemes when we’re doing um processing so when we’re doing tokenization when before you give it to your word embedding system you’d like to have a standardized tokenizer that’s going to give you meaningful the most meaningful atomic parts when you actually do it.</p>
</blockquote>
</section>
<section id="arabic" class="level2">
<h2 class="anchored" data-anchor-id="arabic">Arabic</h2>
<blockquote class="blockquote">
<p>Arabic’s very interesting, because things actually are done within the consonants so you have a backbone of consonants and the vowels will change before and after after them. This changes meaning both semantically and inflectional so syntactic information about tense etc there’s a number of semitic languages that actually do templatic morphology and it always breaks a lot of our systems from doing it but we are not allowed to define what natural language actually is we still need to be able to deal with it chinese has a relatively um small amount of morphology but it still has derivational morphology so you can take words and join them together in interesting compounds which are not necessarily directly to do with the meaning of the individual character that you’re joining together so a number of things of um for example um if you take fire and wheel and put it together it doesn’t mean a wheel that’s on fire it actually means a steam train um and so or maybe that’s only in japanese i always get that one wrong um which varies between the different languages but um there’s often a relationship but the compound might be different and so sometimes you want to be able to decompose it and sometimes you don’t so there’s two types of morphology which are identified as what’s called derivational morphology and derivational morphology you’re mostly changing the part of speech class when you’re doing things English is a rich derivational morphology um and we can write it out and it’s mostly productive by productively means we’re allowed to construct new words without explaining to people what the meaning is um inflectional morphology is usually syntactic class changes or some classic feature changing so this is things like changing the tense changing the plurality and the number and other languages it may do things like used in agreement um used in tense and aspect and verbs um and these are usually treated as different classes and they’re usually quite different they don’t overlap they’re sometimes maybe a little bit confusing and for the most part inflectional morphology happens after derivational morphology in almost all languages we can talk about morphosyntax about how these phonemes join together and which ones are allowed to join to each other and we can talk about a lower level thing that we call it morphofunnymix or morphographymix where things are changing at the boundaries when we join them together so an example of morphophony thin morphography mix because it happens in the graphene form but the pronunciation is affected as well when you take apostrophe s okay or the plural s when we add it onto things sometimes we insert an e when we add e d to the end of things if it already ends in an e we don’t double the e and so this is i’ve got the e d and i’m going to join it on if the previous one is an e i’m just going to do it so move plus e d this is m-o-v-e-d okay while walked plus e-d is walk plus e-d joined together um there’s different classes of Types of Morphology morphology and typography and typology of these so that we can put these into different groups and to be able to identify what they’re all about um we have isolating or analytic m1s where there’s very little morphological changes vietnamese many of the chinese dialects English are good examples of that we have synthetic ones where things are being created all the time they’re sometimes called fusional or flectional so german greek russian and templatic where we’ve got some um form of often consonants and the vowels are changed and interspersed between those and we have a glutenatif where there’s lots of things joining together japanese finnish turkish are really good examples of that and we have polysynthetic which are really complexly joining things together where almost every phrase is actually ends up in a single word many of the north american um native american languages are that the reason the word snow is there is because you can often have lots of variations of the word snow in inuit that’s actually not true well it is sort of true um but um it’s notable that in scottish English we have lots of words for rain and for some reason i have no idea why that would be Morphology Analyzers and people have worked on morphology for a long time so often when you’re working a new language there already is a morphological analyzer is there and if you look at the project unimorph and they’ve actually collected together these in a fairly standardized way so you might just be able to use it from python and all of a sudden you get morphological decomposition for your language or maybe you could use a nearby language and it would almost work and that might make your life a lot easier often when we’re doing um novel languages especially when we’re caring about things under time we will look for one of those or we might even spend a couple of hours writing something because we’ll get something better than trying to do it fully automatically there are fully automatic ways but it might be better if there’s already something that allows us to do that okay there’s actually a competition Morphology Competition every year sigmar phone has been running for at least um 10 years and it gets harder and harder every year as they find harder and harder tasks from both supervised and unsupervised m techniques so it’s worthwhile looking at these and using that as a resource okay Shared Tasks um these shared tasks allow you to compete and people have done them from this class before actually and done interesting novel techniques to be able to work out to do it sharing information across language when you’ve got not enough data to train etc in order to be able to learn how to do that Finite State Morphology um finite state morphology is often used for morphemes actually um morphology is never very complex there’s probably something to do with a human brain it can’t really deal with something as complex as say syntax within um morphology and therefore it’s often quite localized and therefore finite state machines are quite good at being able to cover everything and there’s good toolkits out there that help you to be able to write these things there’s also completely unsupervised techniques there’s more fessor just a python thing you give it examples and it will try to find out the prefixes and suffixes that actually might um allow you to be able to do um analysis it does assume a certain segmental and view of phonology and therefore it can get confused sometimes and it might treat the different types of ed or just d um in different forms in English and separate them out and maybe you want to join them together or maybe you don’t um there’s a sort of related thing called stemming which is often quite useful especially when you’re doing things like information retrieval where you’d like to say look i i just i just want the root of the word and i don’t want all these other variations especially when you’re in a limited domain or when you’ve got limited amount of data and so maybe if you removed all of the morphological variants the plurals the eds it might be easier to do comparisons later especially if you don’t have good data in order to be able to do good word embedding there’s also purely completely automatic techniques um and bpe is a good example of that byte pair encoding you can’t really work it out from the name where what we do is we look at the string of the actual letters that are there and try to find and optimize the sequence of letters together and the overall predictability of the group of letters that we actually find and this originally came out of work in machine translation to try and find the best um segmentation for doing um translation um but we end up using it for lots of lots of things it’s often worth trying if you don’t have anything else because it does sort of work but you really want to know about for your particular language is it likely to work before you actually do all of your bpe you get a tokenization representation you build all your um word embedding that you learn from it and then learn oh no you could have downloaded the uh morphological analyzer that would have given a better result and a more consistent result and therefore you would have been able to learn back Tokenization okay um as i say tokenization is also something that we um get this often gets called where you’re actually just trying to split these things into words and you’ve got to care about what the tokenization actually is because if you have a different tokenization that won’t be the same lemmetization or stemming is somewhat similar but limit hydration is really talking about the linguistic root of the word which may or may not be well well defined and it’s usually after um morphological and decomposition you find the root of the word um a [Music] we can also do this across um languages you may want to care about characters rather than words and looking inside characters that can actually help and caring about things that are happening over long boundaries um somewhat related to this is um word segmentation in languages like japanese and um chinese and in fact they end up using something similar to bpe to be able to segment things there’s a couple of related things text normalization where you actually are trying to replace Text normalization everything as words we know that there’s an infinite number of numbers um and would be nice if we could change them into words maybe or maybe just change them into the word number or maybe classes of numbers and this is something that’s been studied in um a in text-to-speech and there’s various machine learning techniques to try to do well on this you might want to also care about spelling correction um and do be aware that tokenization this mismatch can really break everything so if you’re using bert you sort of have to use their tokenization because they’ve assumed that and it can be quite hard if you do something else okay that’s everything about works and morphology we will be talking about morphology again later on in more detail but um now we’re going to care about splitting out up out into groups and what we want you to do today is we want you to take one of these um languages language families um often morphology and or aspect of writing and orthography are similar within language families and i’d like you to identify something that you would need to care about if you were trying to do some form of tokenization</p>
</blockquote>
</section>
</div>
</div>
</div>
<section id="discussion-prompt" class="level2">
<h2 class="anchored" data-anchor-id="discussion-prompt">Discussion Prompt</h2>
<blockquote class="blockquote">
<p>Pick a language in one of the following branches of language families: Bantu, Dravidian, Finno-Ugric, Japonic, Papuan, Semitic, Slavic, Turkic. Tell us about some interesting aspects of morphology of that language, following examples from the assigned reading. Cite your sources.</p>
<p>If you would need to implement a tokenizer for that language, what language specific knowledge would need to be incorporated into the tokenizer?</p>
</blockquote>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Words, {Parts} of {Speech,} {Morphology}},
  date = {2022-01-24},
  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w04/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Words, Parts of Speech, Morphology.”</span>
January 24, 2022. <a href="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w04/">https://orenbochman.github.io/notes-nlp/notes/cs11-737-w04/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Multilingual NLP</category>
  <category>NLP</category>
  <category>Notes</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w04/</guid>
  <pubDate>Sun, 23 Jan 2022 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Translation and Translation Data</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w05/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/tiling.png" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>course banner</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/uDiiGWlU4K4" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Lesson Video
</figcaption>
</figure>
</div><div id="sup-slide-deck" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div>

<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>The Practice of Translation</li>
<li>Machine Translation</li>
<li>Translation Evaluation Metrics</li>
<li>Translation Data Sources</li>
<li>Bi-text Extraction/Filtering</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- Transcript here is machine generated no punctuation and I edited it a but but it needs more work and I don't see the benefits unless I can run it through some tool. -->
<section id="intro" class="level2">
<h2 class="anchored" data-anchor-id="intro">Intro</h2>
<blockquote class="blockquote">
<p>This time we’re going to be talking less about models and more about the actual phenomenon of translation itself and so translation i mean i barely need to introduce what translation is i’m sure everybody knows but you know basically it’s the conversion of content in one language into content in another language uh for the purpose of making that content understandable to somebody who doesn’t know the original language of course and that can be anything from you know translating novels like harry potter um to you know any other variety of translation that happens and this time i’m going to be talking about how this translation happens a little bit about machine translation and then also about machine translation data sources and empty evaluation</p>
</blockquote>
</section>
<section id="translation-vs-interpretation" class="level2">
<h2 class="anchored" data-anchor-id="translation-vs-interpretation">Translation vs Interpretation</h2>
<blockquote class="blockquote">
<p>There’s actually two types of conversion of language between uh different languages there are two main types one is translation another is interpretation and translation is basically the conversion of written word from one language to another and interpretation is conversion of spoken language from one language to another and they’re very different in the requirements for them uh also how people work and the type of type of people who work uh in these areas so uh translation in general is usually less time constrained you may still have a deadline uh for when your translation results are required but usually it’s on the order of you know like a day somebody gives you a translated document and they uh they say i want this back in a day whereas an interpreter may interpret while the content is being produced you know while i’m talking an interpreter may be interpreting my speech into another language that’s simultaneous interpretation uh you can also do consecutive interpretation which is basically like i speak for a while then the interpreter speaks for a while i speak for a while interpreter speaks for a while um translators a high degree of accuracy and fluent very fluent natural output is necessary and partially because of this uh translators in may translate all kinds of things but very often they specialize in a single area like i’m a medical translator or i’m a patent translator or something like that on the other hand interpreters may specialize in an area but more commonly they’re generalists who can interpret lots of different things um i actually worked as an interpreter and translator for about a year and a half and i did both of them because i was kind of in a position um that was a little bit less specialized i worked at a local government in japan they additionally had a single uh translator in a single interpreter um and it’s very interesting because uh personalities are also very different translators are are often somewhat introverted uh you know they like working on their own they really like being precise whereas interpreters have to be really good at talking because they spend their whole day talking they tend to be very extroverted um you know uh like talking to people in general not just interpreting between languages so um both of these jobs are very hard i found interpretation harder because um of the time pressure in the constraints and it’s not uh it’s you know you can’t get 100 accuracy in a situation where you’re expected to interpret things so you just need to do as well as you can</p>
</blockquote>
</section>
<section id="translation-industry" class="level2">
<h2 class="anchored" data-anchor-id="translation-industry">Translation Industry</h2>
<blockquote class="blockquote">
<p>How much translation is done the language services market is 56.1 billion dollars that’s a lot of dollars and there’s 640 000 translators worldwide about 75 of them are working freelance and the other ones are employed by a specific uh organization interestingly europe owns 49 of the market share so 49 of the people about half the people are working in europe um and one interesting thing is you might think that as machine translation technology is getting better that was you know threatening translators in their jobs and you know reducing the value of the industry actually the con the contrary is true um you know more translation is being done than ever both machine translation and human translation and the value of the industry is doing nothing but improving so in a way you know more language is being translated now than ever another thing is that the translation industry is becoming more technological so 88 of full-time translators use some variety of computer-aided translation which means that they’re using machine translation they’re using something like translation memory that allows you to look up uh other related translations uh the most common tool is something called Tranos uh and uh basically it has translation memory it has integrated empty uh it has terminology e management software other things like this and a lot of translation agencies require uh that you use Tranos in order to be able to work with them there’s also other ones that i’m less familiar with but basically if you have this idea of like human translators versus machine translators in reality it’s not the case anymore it’s now a hybrid of humans and machines working together um some people like this some people don’t like this some people would prefer not to be told what to translate by their translation memory but you know in order to improve efficiency people now have to do this as part of their life</p>
</blockquote>
<section id="why-people-dont-like-translation" class="level3">
<h3 class="anchored" data-anchor-id="why-people-dont-like-translation">Why people don’t like translation</h3>
<blockquote class="blockquote">
<p>The reason why some people don’t like translation computer aided translation is i think they feel it stifles their creativity or it um because they can do it more efficiently now the requirement is that they do do it more efficiently so they have less time to sit and think about the perfect translation and just have to crank out content basically so i think those are the main reasons why people are like resistant to technology but nonetheless 88 of people are using it so even if they don’t like it they’re using it because they you know uh have to in order to keep up with the amount they’re expected to produce a lot of people do like it so it’s not everybody yeah so from the modeling type of view the biggest difference is whether speech is your input or text is your input um another big difference is interpretation sometimes you’re expected to do it in real time so that’s called simultaneous translation so you need to create the output before you’ve like read the whole document essentially that’s a very interesting topic it might be a topic that some people in this class want to work on if you like the speech you like the translation it’s a very hot topic right now cool so now uh what about difficulty in translation why is this hard um so this is an example i i cannot read this example i inherited it with uh from someone else uh but it’s basically a um an old uh chinese poem and uh the reason why it’s difficult to translate in general is because there’s divergences in uh lexical uh information so words in structure so this is an alignment of the glosses in chinese with the words in english if you don’t know what a gloss is it’s basically a word by word translation in the same order as the original sentence so um this is uh what it looked like in chinese unfortunately i don’t have the chinese characters above for all the chinese speakers but you can kind of see what it looks like and basically if you look at the chinese it’s like daiyu alone on bed top think uh baochai which gets translated into as she lay there alone daiu’s thoughts turned about uh and um you can see that the ordering is different between the two also um you know some words exist in the chinese but don’t exist in the english even more so in the next sentence so um you need to get the words uh right you need to get the words in the right order and this is that non-trivial when you know the translations are different between the two languages</p>
</blockquote>
</section>
<section id="translation-ease" class="level3">
<h3 class="anchored" data-anchor-id="translation-ease">Translation Ease</h3>
<blockquote class="blockquote">
<p>Here’s another example from german which might be a little bit easier to parse if you’re not a chinese speaker so if you have um uh here you can see the gray gloss on top which is in the in city exploded uh car bomb and the uh the kind of canonical english translation that’s listed here is a car bomb exploded downtown so you can see that the word order changed also in uh in german some things like uh car bomb is a single word here it’s multiple words here um there’s also a phenomenon called translation ease and what translation eases is it’s not exactly natural in the language you’re translating into but it’s a translation that is direct and kind of maintains the original characteristics of the original language um there’s actually a fair amount of computational study of translations seeing how like which language you’re translating from affects the output and you can even take translation ease cluster it together and you get a very nice reproduction of the language family tree uh that the languages came from so basically the effects on translationese very strongly inherit the effects of of the original language and even you know are similar between similar languages etc so you can see there’s a very clear effect and here um in the inner city there exploded a car bomb would be a very like literal translation that you can understand in english but it’s also not natural english it’s not like what an english speaker would produce</p>
</blockquote>
</section>
</section>
<section id="lexical-ambiguity" class="level2">
<h2 class="anchored" data-anchor-id="lexical-ambiguity">Lexical Ambiguity</h2>
<blockquote class="blockquote">
<p>Another issue is not just structure but also lexical ambiguities so this is an example from jarefsky and martin speech and language processing where basically you have leg and foot and paw and how they are how they are translated in different ways based on whether it’s a an animal leg a leg of a journey a leg of a human a leg of a chair a bird foot or a human foot etc etc</p>
</blockquote>
</section>
<section id="literary-translation" class="level2">
<h2 class="anchored" data-anchor-id="literary-translation">Literary Translation</h2>
<blockquote class="blockquote">
<p>I also thought i had another example but in order to handle all these things you’ll have to handle um you know syntactic differences lexical differences um how idiomatic the output is so um you know there’s lots of issues here and then if you start talking about literary literary translation it becomes even harder right you know you want to translate a poem or something like this as we had in the assignments uh for the discussion today um and suddenly you need to think about rhyming you need to think about like beauty of the expressions that you’re using so um lots of depth in uh doing translation um any questions there before i talk about uh mt quickly yeah is there any criteria so um the criteria that i would use to define translations there might be a different formal definition but i think this is basically right is it is language that only occurs because you’re translating from another language and would not normally be you know how you would how a native speaker would express the same content in that language so it’s kind of like structural or lexical influences of the original language of the produced language it doesn’t need to be like the effect of translation of the output could be very subtle um and often for a good translator it is very subtle but it’s still like there i have no confidence um i’m a native speaker of english i’m very good at japanese but i have no confidence that i can produce english that sounds like my natural english when i’m transmitting to japanese for example um any other things okay cool um under the machine translation so machine translation um a checked is a three billion dollar market um they’re uh oh actually i should mention that some of the statistics i got here are from this very very nice blog of the translation industry in 2021 um if you uh didn’t look at this on the page uh i would definitely take a look it summarizes a whole bunch of statistics and was insightful to me as well um these statistics are also uh from there which is machine translation is a three billion dollar market now so it has about five percent of the market share of the language services industry overall um the top providers of it are google um amazon and l uh so google uh i think a lot of people know uh amazon and aws web services uh provide translation for a lot of businesses for example and uh deepel is a startup that many people might have heard of but it has actually very good translation accuracy um they haven’t revealed all of their secrets but one of the things is that they um use uh like cleaner training data they have good training data cleaning strategies and they also consider context uh in a better way than other things like google do another thing about machine translation is these are the markets that machine translation is used in you can see that the most common ones are uh healthcare automotive and military and defense markets but it’s kind of spread out pretty widely including e-commerce and other things like that um there’s a very interesting uh paper that examined the effect of translation on e-commerce that i don’t have in the references but i can share uh which demonstrated that when ebay i believe introduced automatic translation between spanish and english the number of sales from uh latin america to the us basically started increasing immediately after that so uh you can see also that you know mt has real world consequences impact uh etc so these are the lists this is maybe a slightly old list of languages uh supported by google translate uh it’s pretty impressive you know at least 100 languages maybe it’s uh nearing 200 now um one thing that i like to mention to people uh whenever you look at this list is just because there’s a hundred languages on this list doesn’t mean that mt is equally good for all of the languages on this list um you know it may be obvious if you think about it a little bit but sometimes you think well you know it’s on this list google released a product for it it must work um that’s definitely not the case uh and uh you know if you try to use it to understand articles you’ll see a very big uh very big difference</p>
</blockquote>
</section>
<section id="human-translators" class="level2">
<h2 class="anchored" data-anchor-id="human-translators">Human Translators</h2>
<blockquote class="blockquote">
<p>There are some reports that mt is now at the level of human professionals um in some areas like for example uh news translation between very high resource languages like chinese and english. I didn’t believe this at first when microsoft first put out an article that said this essentially um and i went in and like analyzed their data looked at their data and they actually their outputs are quite good and human translator outputs are not perfect either so for example um let’s say you hire a human translator on a freelancing site and tell them i want you to translate these new news articles uh because i would like to create training data for a machine translation system um if you do that the translator will say sure i want your money um i want your money i will i’ll be happy to do that but they’re not going to be super motivated and if you say instead to the translator say i’m going to be asking you to translate these news articles for cnn and uh or the new york times and a hundred thousand people are going to read your article you’re pretty sure they’re going to do a good job right they’re not going to make a mistake so which human translator you’re trans comparing people to also makes a big difference in these uh these outputs and not even just which human translator but how motivated that human translator is so i have a feeling that um mt systems and high resource languages are almost as good as moderately motivated good professional human translators but they’re not as good as somebody who’s translating for the new york times for like 100 000 people for example</p>
</blockquote>
</section>
<section id="translation" class="level2">
<h2 class="anchored" data-anchor-id="translation">Translation</h2>
<p>oh yeah so so sorry here are my other examples uh this is from google translate um i basically put in the first uh sorry i had animation on this until i had to switch computers but i put in the first um sentence from the wikipedia article on translation and i’ll put it through google translate which said translation refers to the act of replacing what is expressed in the form of a in a language uh with the form of b that corresponds to that meaning uh specifically in natural language it refers to the act of converting a sentence in a in the source language into a sentence in another target language um i did a few small edits here the the red stuff are my edits that i think would make it a little bit better but it’s pretty good um here’s another example of machine translation i entered a lexically ambiguous word um kodo in japanese which can be either code chord or chord depending on you know the context and it does a pretty good job at disambiguating like electrical cord uh code for the program chord for the on the score um but if i wrote as a musician i am good at reading chords um it made a mistake with that in java i wrote a chord that displays the chord of a guitar um that should have been code up here so you can see that it you know is not perfect for doing this as well um so you know translation is is hard even good things like google translate are not perfect but they’re pretty good in high resource languages anyway um so why do these work i’m gonna be talking more about translation models next class um but basically i’d like to go through a little bit of uh you know history into how um these were conceptualized and um from 1968 there’s this famous thing called the vaqua triangle and basically what it is saying is there’s multiple ways to do translation um you can go from words directly to words so you can basically replace words by words you can go up to the syntactic structure of languages and then generate from the syntactic structure you can go up to semantics in the language convert the semantics between the languages and go down and you can go up to uh something called an interlingua with lingua which is like universal semantics for all languages</p>
</section>
<section id="direct-transfer" class="level2">
<h2 class="anchored" data-anchor-id="direct-transfer">Direct Transfer</h2>
<p>um so what does this look like direct transfer looks like a word by word translation um so you would just be translating directly from word words to words syntactic transfer would be like analyzing the syntax of the sentence and then using that to translate you could also generate a syntax of the target sentence or translate from syntax of the source sentence to the target sentence and generate the output</p>
</section>
<section id="semantics" class="level2">
<h2 class="anchored" data-anchor-id="semantics">Semantics</h2>
<p>You could also have something like semantics which is a logical form which basically says well something was detonated what was detonated it was a bomb</p>
<p>like a car bomb uh it was that mediated downtown and that was in the past tense and then you generate the output based on that</p>
</section>
<section id="interlingua" class="level2">
<h2 class="anchored" data-anchor-id="interlingua">Interlingua</h2>
<p>You could do other things and then there’s also an interlingua which basically says nothing about the uh you know individual languages and instead is completely in kind of a form a logical form um so each of these methods has their own advantages and disadvantages um the advantage of going directly is like let’s say we have a language like spanish and italian which are very very similar in words and structure and other things like this there you could basically do a word by word translation and do a pretty good job um however if you have very different languages you know you’re going to have a lot of trouble you need to have very basically a very powerful model to allow you to do this and uh you know before neural networks we didn’t have any model that really did this very convincingly well</p>
</section>
<section id="neural-models" class="level2">
<h2 class="anchored" data-anchor-id="neural-models">Neural Models</h2>
<blockquote class="blockquote">
<p>Neural models now you can kind of view them as models that map word by word they take in a whole bunch of words they generate a whole bunch of words but you can also view them maybe as a interlingua based model where you know they’re taking in words and they’re generating hidden vectors they correspond to the meaning of all of those words and then they’re generating from that you know like interlingua between the the languages so where exactly we lie now on the spot triangle is kind of you know unclear but uh you know it’s kind of an interesting question uh as well and you know maybe considering syntax or other things like that would help us generalize better in the resource languages for example</p>
</blockquote>
</section>
<section id="data" class="level2">
<h2 class="anchored" data-anchor-id="data">Data</h2>
<blockquote class="blockquote">
<p>I realize i have a lot of slides left and i don’t want to take all of the time. Maybe i’ll just go through the data part and leave the evaluation part till next time um but are there any questions uh so far okay cool so um i’d like to talk a little bit about data because data is very important for training our um mt models and um so basically all models including you know like google translate amazon dpel any of the things that people are using are using machine learning based methods and basically the way they work is they’re trained from parallel data sources um where you have one language and then another language one language and then another language um this is an example that you can actually do yourself if you’re uh like interested in trying a puzzle which is basically like take this parallel corpus and then try to translate this sentence at the bottom yourself and uh you’ll see that you need to like form associations between words you need to understand about what the syntax of the language looks like but it’s definitely possible from this uh this small parallel corpus</p>
</blockquote>
</section>
<section id="parallelcorpora" class="level2">
<h2 class="anchored" data-anchor-id="parallelcorpora">Parallelcorpora</h2>
<blockquote class="blockquote">
<p>No i think these are made up yeah i’m pretty sure um so where can we get parallel corpora basically it’s anywhere that translators are doing lots of translation um this is an example from the united nations uh from a few days ago uh and you can see that this is translated into you know three languages here it’s actually six languages uh i believe it’s the official languages of the um um</p>
</blockquote>
</section>
<section id="languages" class="level2">
<h2 class="anchored" data-anchor-id="languages">Languages</h2>
<blockquote class="blockquote">
<p>another good source that we love using for um like very low resource languages is the bible because the bible is translated into more languages than any other text as far as i know</p>
</blockquote>
</section>
<section id="books" class="level2">
<h2 class="anchored" data-anchor-id="books">Books</h2>
<blockquote class="blockquote">
<p>You can also get uh books of course or other other things this is harry potter and english and chinese Restaurants when you go to a restaurant if you’re not chinese you can go to your favorite chinese restaurant if you’re uh chinese you can go to your favorite indian restaurant i don’t know something else um and uh get the menu and that’s a very good parallel purpose for you to try your your own learning skills on</p>
</blockquote>
</section>
<section id="web-data" class="level2">
<h2 class="anchored" data-anchor-id="web-data">Web Data</h2>
<blockquote class="blockquote">
<p>you can also harvest data from the web like you can harvest data from micro uh micro blogs twitter uh social media other things like this so um this can give you you know more informal language and that’s why you know google doesn’t completely fall over when it tries to translate twitter uh for example</p>
</blockquote>
</section>
<section id="opus" class="level2">
<h2 class="anchored" data-anchor-id="opus">Opus</h2>
<blockquote class="blockquote">
<p>and the canonical place to get data for any of these models now is this place called opus and what opus does is it collects a whole bunch of open parallel corpora in many many different languages language pairs and across many domains so if you want to train models this is your best place to get it um so to leave some time for the discussion i think i’ll uh move the evaluation part to tomorrow but are there any questions about stuff we talked about so far</p>
</blockquote>
</section>
</div>
</div>
</div>
<section id="discussion-question" class="level2">
<h2 class="anchored" data-anchor-id="discussion-question">Discussion Question:</h2>
<blockquote class="blockquote">
<p>Use Google translate to back-translate the text via a pivot language, e.g., “English → Spanish → English” or “English → L1 → L2 → English”, where L1 and L2 are typologically different from English and from each other. Compare the original text and its English back-translation, and share your observations. For example, (1) what information got lost in the process of translation? (2) are there translation errors associated with linguistic properties of pivot languages and with linguistic divergences across languages?</p>
<p>Try different pivot languages: can you provide insights about the quality of MT for those language pairs?</p>
</blockquote>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources:</h2>
<ul>
<li>https://redokun.com/blog/translation-statistics</li>
</ul>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Translation and {Translation} {Data}},
  date = {2022-01-24},
  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w05/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Translation and Translation Data.”</span>
January 24, 2022. <a href="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w05/">https://orenbochman.github.io/notes-nlp/notes/cs11-737-w05/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Multilingual NLP</category>
  <category>NLP</category>
  <category>Notes</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w05/</guid>
  <pubDate>Sun, 23 Jan 2022 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Translation and Translation Data</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w09/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/tiling.png" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>course banner</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/uDiiGWlU4K4" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Lesson Video
</figcaption>
</figure>
</div><div id="sup-slide-deck" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div>




<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Translation and {Translation} {Data}},
  date = {2022-01-24},
  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w09/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Translation and Translation Data.”</span>
January 24, 2022. <a href="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w09/">https://orenbochman.github.io/notes-nlp/notes/cs11-737-w09/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Multilingual NLP</category>
  <category>NLP</category>
  <category>Notes</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w09/</guid>
  <pubDate>Sun, 23 Jan 2022 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Sequence Labeling</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w02-sequence-labeling/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/tiling.png" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>course banner</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/5CXv0KakpYo" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Lesson Video
</figcaption>
</figure>
</div><div id="sup-slide-deck" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div>

<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Example Sequence Classification/Labeling Tasks</li>
<li>Overall Framework of Sequence Classification/Labeling</li>
<li>Sequence Featurization Models (BiRNN, Self Attention, CNNs)</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- Transcript here is machine generated no punctuation and I eddited it a but but it needs more work and I dont see the benefits unless I can run it through some tool. -->
<blockquote class="blockquote">
<p>This time we’ll be talking about text classification and sequence labeling and about half of this class will or half of the lecture portion of the class will be a review for some of the very basic things regarding building neural network models for NLP tasks in general.</p>
<p>If you participated in cs-11-711 or a similar class this will mostly be review or stuff that you know already. But I think it’s important because there are other people who are coming from other other backgrounds too. We haven’t done that before so it’d be good to have the basics before you dive into the code. In addition to that i’m going to be talking about uh kind of text classification and sequence labeling from a multilingual perspective. So giving some pointers to data sets and tasks and other things like that so hopefully that’ll be useful even After we’ll walk through the assignment, including a look at what is required for doing assignment one, and the code etc etc. I think potentially the assignment one description on the website is like a little bit old but we’ll be uh updating that very shortly.</p>
</blockquote>
<section id="text-classification-and-sequence-labeling" class="level2">
<h2 class="anchored" data-anchor-id="text-classification-and-sequence-labeling">Text classification and sequence labeling</h2>
<blockquote class="blockquote">
<p>So text classification uh and sequence labeling are both very broad. I like to call these task categories. They’re not tasks of themselves but they’re categories of tasks that’ll look very similar and thus can be solved in similar ways. Text classification: given, input text X, predicted output: some categorical label Y. This can be all kinds of different things like topic classification uh where we take i like peaches and pears and that gives us the topic food and i like peaches and herb and that gives you the topic music because peaches and herb is a old band basically we have language identification uh which is a particularly important task in multilingual learning and basically this is Taking in language and outputting labels. Taking in text and now putting the language that it’s written in some you know obviously the first is english and the second one is japanese here this becomes very interesting and difficult as i’ll be elaborating later in the lecture portion. Another example that’s very widely known is sentiment analysis and this can be done at the sentence level the document level or you can even do some sentiment analysis uh with respect to you know individual uh entities or things like that but from a text classification point of view uh it would be like sentences or documents so if we have i like peaches and pairs i’d be positive i hate peaches in paris i’d be negative obviously and you know there’s many many other many many other tasks that fall into this sequence labeling on the other hand is uh given an input text text predict an output label sequence y usually of equal length so this can be formulated as taking in a sequence of words and outputting one tag for each sequence of words so we have part of speech tagging is one example of this so we take in uh words and output parts of speech another one is lemmatization so what this is doing is it’s basically taking in words and outputting their base form uh this is kind of this is used sometimes in uh in english but it’s actually particularly uh useful or important in languages that have richer morphology than english because um essentially if they have lots and lots of conjugation or other things like this uh the words themselves can become very sparse so identifying the underlying base form uh helps you understand uh like what the word is referring to better another variety is morphological tagging in morphological tagging again we’re going to be talking about morphology in about two classes but basically it’s predicting various features of the word based on uh like for example here we have he saw becomes uh the past tense and a finite verb form uh two uh it’s a number and the type is a cardinal number uh this is plural uh for example so um this can become more complicated but more languages have more complicated morphologicals and there’s other ones as well there’s also span labeling tasks and sometimes these fan labeling tasks are treated in sequence labeling tasks but they’re actually a little bit different and basically the idea is given in input text x predict output spans and labels y and these include things like named entity recognition where you want to identify the spans and labels on the spans for named entities like person people or things so here we have graham newbig as a person in carnegie mellon university as an organization another example is something called syntactic chunking or shallow syntactic parsing where basically you want to split up the sentence into like noun phrases and verb phrases like this um and uh that’s another example and there’s also uh semantic role labeling where semantic role labeling basically identifies fans and tries to identify like uh this is an actor this is a predicate and this is a location so what role each of these arguments is playing with respect to the predicate but as you can see all of these basically have to do with like identifying spans and labeling them in some way so span labeling can also be treated as a sequence labeling task so you predict the beginning in and out tags bio tags for each word in the span so if we take our span labeling task like this where we want to identify a person or organization we convert this into a thing where we basically have one task or one tag for each word so we have beginning of person inside a person out out out which means no no span is identified beginning organization in organization in an organization like this so the good news is then um you know if you want to do this span identification task you can just solve it with a sequence labeling model so sequence labeling is nice in that way there might be better ways to handle span identification but this is this is one way to do it so another another task that is slightly different but also can be handled as uh sequence labeling is text segmentation so given in input text x split it into segmented text y so a very uh common uh example of this in many languages is tokenization where you want to take uh something like a well a well-conceived thought exercise that has like lots of punctuation and intervening hyphens and stuff like this and split it up into things that look a little bit more like just kind of natural word boundaries like like this adding spaces between each of uh the punctuation etc another variety of this which is necessary for some languages uh not not so many languages in the world but uh but some languages is word segmentation and um this is a example from japanese uh because japanese is written with no spaces between words so you can’t just split on white space using your python uh like split function you have to actually find the locations of the word boundaries and this is a non-trivial task um so this is one segmentation that you could have here this is the kind of like quote-unquote correct segmentation um but you could also make a mistake and segment like this and uh this phrase itself if you split it this way means uh foreign people voting rights or like suffrage for uh uh whereas this means uh the foreign government so um basically uh if you split the correct way you get one meaning if you split the wrong way you get another meeting and this can you know mess up information retrieval systems any translation systems anything that you would think of um and then there’s also morphological segmentation which again we’re going to talk about a bit more which is for example uh we might have something that looks a little bit like this um which can be uh split into different ways i i believe this is turkish i think someone can correct me if i’m wrong here i i actually forgot uh where i got the example but i believe it’s turkish um and uh so then we have uh dog and plural uh or dog paddle and uh attempts so basically whether you split it one way or the other also affects whether this is like a plural noun or a verb and thank you for confirming that this is a determination so um this is another uh another issue that you could have cool um are there any questions about this so far oh is the word segmentation solved with additional context in japanese yeah that’s a really good question um and morphological analysis also um uh is similar you know having context um so basically yes having context is very important because both of these are um both of these are reasonable morphological segmentations in different contexts you know they’re both uh things that could happen sure this might be a lot more frequent but there certainly are examples uh where this would occur um it’s also the case in japanese although that’s a little bit um uh more rare like one one example is um uh that i can put in the zoom uh chat um this means uh if you split off the first character that means american nuclear reactor if you split off the second character it means a train that’s departing from maihara which is a place in japan and uh so depending on whether you’re whether you’re in a news newspaper article um or a train schedule uh either of those would be correct so it’s um yeah it is based on context then i had another question explaining the distinction between b and um i here so b basically is the first tag in a sequence in a span so it would be applied to the first word in the span i would be applied to every subsequent word at the span so you you um if you had a single word span it would just have a b but if you have the multi word span it will be b i i i tell the span finishes and having these two different tags is necessary to distinguish cases where you have two spans in a row so if you have like person and then another person later how do we constrain the model so that b is predicted before i for a particular entity that’s a another really good question um so if you i maybe maybe i’ll talk about um how we actually do this prediction first and then get back to that question great okay so um i’d like to talk a little bit about modeling for sequence labeling and classification so um the first question is how do we make predictions so given an input x input text x extract features h and predict labels y and the way this works is basically we have text classification and sequence labeling um so we have like i like peaches we have some sort of feature extractor um that extracts a whole bunch of features from the whole sequence into like a single vector here and then given that single vector we make a prediction we have for sequence labeling we have a feature extractor that extracts one feature or one vector of features for each word in the input and then we make a prediction from this like that um and so either way we need a feature extractor it’s just a matter of whether we are extracting a single vector of features for the whole sequence or one vector for each word so a very simple feature extractor that we might use for text classification for example is bag of words where what we do is we look up a um a single feature for each of the words and then we add them together to get a vector um a vector representing the number of times a particular word occurred in the sentence and then we feed this into a predictor maybe we have a a matrix multiply that turns this into label label values and then we use this to calculate label probabilities um so just to clarify uh when i say bag of words um this uh we have a one hot vector which means a single uh value in this vector here is one and all the other values are zeros so this is essentially a um a vector that represents the identity of the word and nothing else so as our simple predictor we can have something like a linear transform and a softmax function so what that looks like is we take um we take the extracted features we multiply them by a weight matrix we add a bias here which tells you essentially how um how likely each of the labels is a priori and then the softmax converts arbitrary scores into probabilities so we exponentiate the score at each of the elements in the resulting vector divided by a normalizer to make sure that all of them add up to one add up to one and uh then that gives us a probability of our final output so it might look a little bit like this after the um after the linear transform and bias addition we might have a score vector that looks like this and we turn it into a probability so i think this should be pretty familiar to a lot of people um [Music] no questions about this any questions uh if not i have i have a little bit of a quiz uh so like we talked about text classification sorry every time i touch my mouse it moves my my slides it’s a little bit annoying but uh um so we talked about text classification where we extract features for the whole sequence and then we use that to predict the uh the label probabilities for the whole sequence by adding all of these together what do you think of a bag of words uh of a similar feature extractor used for sequence labeling does that make any sense uh whatsoever so we extract one uh one vector using this kind of bag of words lookup function and then make predictions based on this vector would that would that do anything would there be a reasonable way of solving this problem maybe not so reasonable because it’s neglecting the word order yeah maybe not so reasonable because it’s neglecting the word order yeah that’s a a pretty good uh idea but it’s actually maybe not that bad right what it what it would end up doing is it would basically look up i and then it would make a prediction of what part of speech tag i would be based on what the most frequent part of speech tag for i is and it would make the um prediction of like and it would make the prediction of the most frequent uh thing for likes so um yeah it’s a frequency-based model it would be looking up the majority class for each word and actually for part of speech tagging in english and even more so for other languages this actually isn’t that bad at part of speech tagging it gets like high 80s accuracy just because uh there’s relatively little like peaches is always a plural noun i i can’t i can’t think of any case where it wouldn’t be a plural so um even that would be you know a moderately okay feature extractor both for classification and uh um and sequence labeling um however uh you know it does neglect the word order so uh if you want to do like even better than just majority class you have to come up with something that way so another issue um language is not just a bag of words uh for classification or labeling so to have some examples we have i don’t love pairs um there’s nothing i don’t love about pears um if you just look at the words in the sentence like love is the positive word and then you have a negation but negation in itself is not you know very indicative of positive or negative sentiment um so both of these would be relatively hard to tackle with a bag of words model so what we want to do is we want to come up with a better futurizer to better pull out features for our sequences for each word in our sequences so one example of a better futurizer might be a bag of engrams model so basically instead of looking up each word we would look up each engram so here for example then don’t love would also become a feature in our model and if don’t love is a feature in our model that’s kind of like a negative feature right it would um maybe overpower the the love feature and make move that more negative uh you could also come up with syntax-based features like subject object pairs um or neural networks like recurrent neural networks convolutional neural networks self-attention and uh so until you know 20 or so it was very common to use these um kind of handcrafted features or at least handcrafted feature templates and throw them into a support vector machine model or something like this to do text classification um now it’s much more common to use uh neural networks and they also have some really nice properties like allowing transfer across languages which is a very big you know part of this class so we’re going to mainly be focusing on these models in this class this time i’ll talk about recurrent neural networks just because they’re conceptually easy but we’re also going to be talking about things like self-attention so a neural network um which a lot of people know is uh basically um a computation graph that is parameterized in a certain way in order to allow it to make predictions um the name neural networks comes from neurons in the brain uh where basically uh you know they take an information across synapses and then they fire and they give output over the uh the outputs and the absolutes um but the current conception of uh how we use them it’s basically a mathematical object that allows us to calculate something take in an input generate an output and in particular in this case it’s going to be our featurizer and our predictor so it’s going to take in an output input output some features for each word or each class or it’s going to take in an input and output a score for each for each class um so if we define an expression the way we represent it as a computation graph is through nodes and edges between the nodes and so here uh we have a single variable maybe a vector and uh that’s represented as a single node um the node can be like a scalar value of vector value matrix value tensor value and we can also have operations over all of these values so like let’s say we transpose the vector um that operation would be demonstrated as an edge to another node that implements the transpose so an edge represents a function argument and a node with an incoming edge is a function of that edge’s tail node and a node knows how to compute its value and the value of its derivative with respect to each argument and then functions can be uh nullary like input values here or unary unity or binary so here’s a something representing a matrix multiply so basically now we have x transpose uh times uh times a here and uh these are directed in the cyclic graphs that allow us to calculate more and more complicated expressions so they allow us to you know do things like calculate features um make predictions et cetera and we can also name individual parts of the graph and uh this allows us to um uh you know calculate uh values that we would like to be calculating like the score of the the probability of prediction and so some algorithms are graph construction and forward propagation that allow us to calculate values so forward propagation basically we we start out with the input values of the graph and we gradually uh move through the graph to calculate the final result here and um we also have uh back propagation and back propagation basically what it does is it processes examples in reverse topological order uh calculating the derivatives of the parameters with respect to the final value and uh this is usually uh the loss function the value we want to minimize so uh in many cases this is the negative log likelihood of the predictions over the true values and by minimizing this negative log likelihood that allows us to maximize the probability of getting the correct answer and then we take the derivatives calculated through back propagation to update the parameters of the model uh so back propagation basically works like this um we start from the end of the graph and gradually move back until we back propagate into parameters so let’s say a was a parameter a b was a parameter and c was a parameter we would back propagate into these values and um once we have the derivatives that we calculate through this process of back propagation we can use them to update the parameters to kind of improve the likelihood of getting the correct answer so this is a kind of five-minute intro to uh neural networks if you’re not very familiar with them there’s lots of good tutorials online in particular we’re going to be using a neural network framework pytorch for examples in this class that i think you know a lot of people are familiar with already um and our first assignment this time is aimed to kind of have the dual purpose of allowing you to get familiar with um you know building models in high torch and stuff like this and also learn more about kind of the interesting uh difficulties that you have to deal with when you apply these multilingually so if you’ve already taken a class or already used neural networks pretty widely in your in your work then you know all of this will be old to you and you’ll more or less know this already um if you haven’t then uh definitely take advantage of the ta office hours look at the examples ask lots of questions we can forward you some tutorials online to help you out as well so this will be your chance to catch up before we get into the more involved assignments that happen later so um yeah actually maybe i’ll skip that part um so to give an example of a type of neural network that can be used for featurizing a text classifier or a sequence labeler um we are going to talk briefly about recurrent neural networks and recurrent neural networks are models that allow us to do precisely what was mentioned before as being the [Music] issue with a bag of words model which is handling either short or long distance dependencies in language handling word order handling other things like this and um so in language there’s many dependencies that span across whole sentences so for example agreement is one example there’s not a whole lot of agreement in english um like for example there’s gender agreement and there’s a plural uh there’s like um uh number agreement between subjects and verbs so here we can see he and himself need to agree she and herself need to agree also um he does needs to agree here uh so if this were i would be i do he does so that’s an example um also word order in general um we talked about that last class so we need to have some sort of model that’s able to handle these things um these are syntactic characteristics that we need to be able to handle and there’s also semantic characteristics so um uh for example we need to have semantic congruency between uh rain and queen and rain and clouds here um you know they’re they’re just things that um make sense uh make sense semantically and don’t make sense mentally based on our knowledge of the world and we also need to handle these as well so recurrent neural networks basically are are one of the tools that we can use to encode sequences um either to get representations for each word or representations for other words so basically what recurrent neural networks do is they look up um they look up the context or the input at the current time step do a transformation of this into features and then they feed in the features from the previous time step for the next time steps so to give an example if we have i we would feed it through an rnn and get the next vector here um like we would feed um this through another rnn function we would calculate a vector corresponding to light this is a parameter of the model and then we feed in the result of running i through the rnn to get the um and this input here to get the representation for i like and then we have these and we calculate the representation of these uh i like these and then we have pairs and we calculate the representation of uh i like these pairs and uh basically um this is a recursive function so each time you’re using the result of the previous function to calculate the result of the next function so uh when we represent the sentence for text classification um basically it would look a bit like this so we would take the last vector in the sequence to make a prediction and that would be useful for things like text classification condition generation um retrieval uh we’ll be talking about the latter queue later in the class but text classification is the one we’re talking about this time and it can also be used to represent words so um if we wanted to predict a part of speech label for i and like and these and pairs we could use the immediate output after inputting that word to try to make that prediction as well and this would allow us to do things like pull in contacts from the left side to make this part of speech prediction for things like sequence labeling language modeling calculating representations for for parsing etc so the way we train the rnns is like let’s say we’re training one for sequence labeling um we have the predictions here um from these we could calculate a negative log likelihood or a loss function uh something like this we have the label we use the true label of the output to calculate the loss function and we add them together to get the sequence level uh total loss so this is one big computation graph for the whole sentence and then um we take the total loss and we do back propagation from this total loss into the whole uh the representations for the whole sentence so um the parameters of the model are all tied across time the derivatives are aggregated across all time steps and this gives us uh something called back propagation through time uh so you can back propagate through the whole sentence and uh basically optimize the probability of making the correct predictions for the whole science so what did i mean by parameter tying basically um the parameters are shared between this rnn function over the entire sentence and because of this this allows you to apply this to sentences of arbitrary length so if you have a sentence of length 50 or a sentence of length 20 or something like this then um this would essentially allow you to um uh represent all of them within the same recurrent neural network by just applying this rnn function 50 times or 20 times or three times for a three word sentence and when doing um when doing representation for things like sequence labeling it’s very common to use bi-directional rnns and what i mean by this is basically you take the left side and you run a recurrent neural network that steps from time to step zero to time step one to two to three to four um from left to right and then you have another recurrent neural network that steps from right to left in this way and aggregates information from both of the directions congratulates that together and makes a prediction and the reason why this is useful is you never know whether the context to disambiguate a particular word would be available on the left side or the right side so this allows you to pull in information from both sides okay so um that’s basically the overview of uh you know a simple method for um doing calculation of either uh representations for the whole sentence so for example uh if we’re representing the whole sentence we might take this vector we might concatenate uh the right side of the left rnn and the left side of the right the right side of the back forward rnn and the left side of the backward arm or if we want to represent individual words we might concatenate together the things in each time step uh to make predictions so this would allow us to do text classification or sequence labels okay um are there any questions about this before i jump into the multilingual part okay i guess not so uh we can jump into the multi-uh multilingual uh thing that is part of the name of the class of course so um i’m gonna be talking about some text uh some text classification and sequence labeling tasks most of these are tasks that are applicable to any language so they’re explored quite widely on english as well but some of them are inherently multilingual so for example language identification is one that’s inherently multiple so language identification as i mentioned before is the task of identifying the language that a particular text comes in and this is uh really important for a very broad number of reasons um the first reason might be if you want to create um like let’s say you want to show people content in only a language that they speak uh so you know people when they’re doing search online they’ll probably more appreciate results in the language they speak than in another language another example could be for creating data sets for something like machine translation or language modeling or something like this uh where you only want data in a particular language and uh and other things like this so actually um uh one of the largest language identification uh corporal was created by ralph brown here at uh at lti um it’s a benchmark on uh 1152 languages from a variety of free sources so this is kind of a widely known data set here if you want off the shelf tools for doing language identification one example of a relatively easy one to use is slang id.pai so you can just download this use it for 90 plus languages um another example is um there’s the chrome uh language identifier from the chrome browser i actually don’t have a link here but that’s also pretty widely used by people if you want uh an off-the-shelf uh method that you can use there’s also a nice survey it’s a little bit old by now but um automatic language identification and texts uh which i can recommend you can take a look at if you’re interested in this and oh um missing a slide that i thought i had added here weird okay so i i will just discuss um i’ll just discuss this paper um so this is a recent paper from 2020 by people at google working on kind of low resource languages it’s quite interesting it’s called language id in the wild unexpected challenges on the path to a thousand language web text corpus and this is not the only paper that has it has pointed out this problem that language identification doesn’t work well um but they have some uh very interesting insights and they also have a very nice kind of example of a of the issues that you encounter when trying to do language identification on uh web text and so um here here are some mind sentences from the web that were supposedly in one language according to google’s uh text uh like language identification model so like a whole bunch of people raising their hand and emojis got classified as amani beri um the in uh this was in twee uh which is why you lie in why you always lie in uh written in kind of like strange characters um a misrendered pdf uh was for hadi um the non-unicode font i’m not sure what this was um uh yeah it’s a non-non-unicode font i guess was uh written in this way um here uh this was as balinese it was just like boilerplate um this was also english but it was written in like the cherokee script for stabilization um he just wrote me ow that became cooler so you can see basically here um when people write in like slightly non-standard language um uh it gets identified as other things like even this is like clearly standard english but um there were hints of uh words that often occur in remote so um it got recognized as a remote so this kind of just demonstrates how difficult um this uh this task is when you start applying it to uh web text and i actually have had a similar experience when i was trying to do twitter um there was a certain uh like uh face uh like not emoji but like the um the faces written with regular characters it was written in canada characters and uh so many many things were recognized as commonly just because they use that like popular face um so there are lots of um large corpora like while i’m at it here i can also introduce the oscar purpose this is a very large corpus huge and multilingual obtained by language classification and filtering of the common crawl corpus it’s gotten a bit better since uh they first released it in terms of the noisiness but when it was first released it was like extremely noisy just because language id didn’t didn’t work so well so um this is actually like a really big problem that you need to be aware of if you’re if you’re starting out um are there any questions about language id before i move on to the next okay um so also kind of standard text classification like yes i said text classifications were like a class uh class of tasks that make tasks in itself um here are some representative ones that people have used uh these are mostly used for benchmarking multilingual models as opposed to like actually building anything useful um so uh but still you know sometimes you want to know how good your multilingual representations are so they could be good test beds uh one example is ml doc corpus um which is a corpus of a multilingual document classification there’s also the pos x corpus um so the this is a paraphrase detection between languages it’s sentence pair classification where you feed in two sentences um also cross-lingual natural language inference so this is textual entailment prediction or natural language inference which is also a sentence pair classification task there’s also cross-lingual sentiment classification um in chinese in english so this could be used for facing another thing is part of speech and morphological tagging i’m not going to go into a whole lot of about this because i know it’s going to be covered more when we talk about like words parts of speech and morphology um but basically there’s the universal dependencies treebank and the universal dependencies treebank um basically it contains uh syntactic parses like dependency forces but it also contains parts of speech and morphological features for 90 languages um and it has a standardized universal part of speech set in universal morphology headset to make things consistent across the languages so this is one of the highest quality like multilingual corpora that i’m aware of it’s you know well controlled well conceived um and there are some pre-trained models that can use uh that can do like syntactic analysis on many languages trained on these datasets like unify and stanza if you’re interested in doing uh multilingual like syntactic analysis named entity recognition this is going to be what we’re going to be doing for the assignment and um there’s different types of named entity recognition data sets um there’s a gold standard data set from connell 2002 2003 on language independent named entity recognition this is an english german spanish and dutch with human annotated data i actually uh forgot to add um one that i just remembered now uh that just came out i actually i helped out with this a little bit but it’s a uh an identity recognition data set for african languages i called uh masakonner and this is um this is nice because it’s also manually labeled but it’s in african languages that have like a lot fewer resources than um english german spanish and dutch so it gives kind of a better idea of um [Music] like you know how well we’ll be doing a lower resource languages there’s also this wiki and data set for entity recognition and linking in 282 languages um this was extracted from wikipedia using inter page links so in wikipedia of course if we go to um carnegie mellon university on wikipedia there are many uh there are many links um so there’s no link here but here’s pittsburgh pennsylvania um the melon institute of industrial research andrew carnegie so all of these links link to other pages and then if you look up the type of the page according to uh some annotations that come on wikipedia you can tell that pittsburgh is a city uh mellon institute of industrial researchers and organization and andrew carnegie is a is a person and then of course you know this is available in lots of languages so we can go to chinese and um find the chinese equivalent of andrew carnegie or the chinese equivalent of pittsburgh and uh and do the same thing so basically this data creates that in many different languages there’s also several composite benchmarks for multilingual learning so they aggregate many different sequence labeling or classification tasks for testing multilingual models um one popular one is extreme uh it’s a massively multilingual benchmark for 10 different tasks 40 different languages another one that came out at a similar time is exclu with 11 tasks over 19 languages um so there’s also a new version of extreme cold extreme r that just came out i had been a little bit involved in both of these and um extreme r is uh they swapped out some easy tasks added some harder tasks and added better analysis so you might also consider looking at that um for your class project i would warn you that these uh these benchmarks are very popular uh and there’s people with like lots of compute that are competing on these benchmarks so um you might not uh it might be a bit of a challenge to keep up with the state of the art there but i think you could work on individual tasks and still do a very good job like some of the tasks where kind of generic models are not working as well so you can definitely take a look to get inspiration for ideas okay um great so uh that’s all i have are there any questions before we move on to the um the like discussion period which in this case we’re not doing discussion we’re having a presentation of the assignment but uh any question about data sets or tasks or oh sorry the homework was on ner i said it beyond that in er but the top part of speech tagging i apologize um cool yeah but we’ll have the description of the uh the task um for assignment one um before i go into that i just like to point out that starting next time we will indeed be having discussion and reading assignments um so the reading assignment for next time is uh this uh modeling language variation in universals a survey on typological linguistics for natural language processing um the reading is actually uh it’s only required for suggested that you do uh sections one through three um but the whole survey is good so if you don’t mind reading 30 pages or so um it would be worth taking a look at that uh as well so um required is one through three um and then based on what you learned in that reading you can try to think of what are some unique typological features of a language that you know regarding phonology morphologies and dex pragmatics um doesn’t you don’t need to cover all of them but you can cover like one or two of these and uh we’ll have a discussion where everybody will share what they came up with cool and uh today is uh assignment one introduction uh tr vijay or who’s going to be presenting this thing um are you speaking if you’re speaking around me do i need to unmute me i don’t think he’s on mute okay there we go now we can uh take gray i think you are talking right can you hear me now yep yes great okay so uh okay hi hi i’m think gray and ti is going to give some introduction yeah all right so this first assignment uh is to give you a practical introduction to multilingual parts of speech tagging and i think briefly was mentioned so part of speech is just lexical categories or word classes or tags so in the example sentence he saw two words we assigned the part of speech pronoun to he verb to saw and so on so you’ll get a data set of a sentence sentences in different languages and you want to output the pos tags for each of the words in the sentence yeah so as mentioned previously aside from giving you guys a practical introduction to multilingual pause tagging yeah we want to give you a sort of like an experimental approach to multilingual problems such as investigating the challenges to languages which are low resource meaning that there’s a limited availability of label data and of course just be familiarized with deep learning frameworks aws and multilingual data sets so to do this assignment you would need a machine with a gpu so you can use aws or you can use your own computer if you have a gpu and you’ll have to install some python packages for this assignment so the tricky part here i guess would be like the aws setup so shortly i’ll be posting instructions on piazza on how to request aws credit and i think the assignment will have further instructions on how to set it up and all students should have an aws account using your andrew email and just follow the instructions on how to set it up we tried doing the assignment without a gpu but it’s strongly recommended because i think the next assignments are not really doable without a gpu and i trained it on a very old macbook air and it took me around three to four hours to complete the training and you need to retrain it you know when you’re changing the parameters and so on so yeah do it with the gpu and i think one more thing to take note is that make sure that you stop the aws instance when you’re not doing it because you will be continuously billed um so i think the aws setup should be fairly straightforward i’m not sure myself tingerie has done it before without instructions but the instruction instructors and the tas of the intro to deep learning course have provided a very comprehensive um aws fundamentals playlist so it will be linked in the assignment handout page as well and you can follow the steps in case you have any difficulties yeah so for this assignment we are going to give you a deep file and uh this this this file will contain all the code and the data that is to use for this assignment and we will post a link later on psl so uh in the date file you can find the training data it is the training data for six languages and the right hand side is an example uh what what the format looks like for this training data so this is an example of one sentence so you can see uh there are the words and the pls text of that word separated by some new lines and each line contains a word and it’s text separated by a tab but actually you don’t have to worry too much about this format because uh our code will handle this for you and in this homework we are going to use this simple baseline model by rcm model it is a model with an embedding layer and also a bi-directional stem there and so the input of this model will be a sequence of words and the outputs will be a sequence of pos tags so let’s work through the files in the the file so the first file is the config.json file it is the file that contains the hyper parameters that will be used to trend the model you may have to change this ma this file in order to write a report when you are doing some analysis and this udprs.pipe file is the place where we implement how to read a data set but actually i don’t think you will have to modify this so i won’t go into detail here and uh this model.pi file is the place where we implement this bi-directional stm model and if you are familiar with pytorch then you can see it’s a very simple model it just simply applies and it’s embedding layer and then lcn there and then finally predict the text using a fully connected layer yeah but if you want to make a stronger model then you may have to modify this file and most of the complex jobs are done in this main.pi file it handles the loading of data and it also do some pre-process and uh do the thing that have simple into batch and also the part that trends the model so let’s go through this content so the first thing it does is that a load the data set with the function we define and then it build a vocabulary for the input text and also the the output pos text the reason that we want to build this vocabulary is that uh in modern deep learning framework we are uh when we are using a embedding layer what the embedding layer does is that a map the index of sound balls into some batters so that’s to say before we can use this embedding layer we need to first build a mapping they can assign each word with an index so we have to iterate through the changing data to see what to see the words that occurs in the training data and for similar reason we have to iterate through the data set to see all the possible possible pos tags in this data so these are the purpose of this device and once we have the mapping that map the tokens to the indices and the mapping then map the pos tag to the index we can define these two functions that converge they convert the train data into indices and with these two functions we can define this collet batch function the purpose of this function is that it pack a bunch of text label pairs into a single batch and then this batch will be used to train the model so uh yeah so what this function does is that it first converts the words inside the the samples to a tensor by coding the function with the file above and you also can convert the tag into indexes by code by cleaning the function and then the most important thing it does is that it paid those sequence of tokens into the sentence so those tensors can be stacked into a single tensor and that single tensor can be later used to train the model and once we have this correct batch function we can define a data loader a data network is an iterator we can uh get some batches from it by iterate through this data loader so here we define three data loaders for the training set the validation set and testing set respectively and then we can use the data loader to change our model the way we train our model is that we repeat this process for a certain number of times defined in this hyper-parameter mesh epoch and the process is that we first train the model by using the training data and then evaluate the model using the validation set and if after this epoch the model get a better validation loss then this grip will set the model to some place so at the end of this training process you will have the model that has the minimum validation loss as for what this trend function does it is also very simple it just iterates through the data order the chain data loader and then makes prediction over the text and then compute the laws by comparing its prediction and the quantum tags and then use these those to do backward propagation and then code atomizer to update the parameters in the model yeah so it’s basically what our codes do so what you need to submit for this assignment is code and a write-up so part of the you know how to obtain points for this assignment is you need to run the code you need to make notifications to the model and so on um but it’s also equally important to give a detailed explanation on the results that you see or what you do and why do you think your changes have made an effect on the results and you can submit this on campus and i think this is um the part that everyone really wants to see is how do i get a good grade in this assignment so there’s a lot of ways um there’s a lot of tiers as well so if you just want to get you can get a b by just running the code on the existing english model and just running that uh running the model on the test set you’ll get a b but if you train the model on the different multilingual data sets and you evaluate them using their respective test sets you’ll get a b plus to get anywhere of an a you need to write a report with detailed analysis so there’s a lot of ways you can comment on the results you can see how the performance varies across different languages you can also see you know which tags are most often um misplaced for other tags so you know our pronouns and nouns more easily mistaken for each other and so on so that’s what we want to see and if you have a report you know detailing all these explanation you’ll probably get an a minus to get an a or above you will need to get to create a non-trivial extension to improve the existing scores and there’s really a lot of ways you can do this you can add like cnn input layer to capture character level features you can use pre-trained embeddings and so on so there’s really um it’s kind of like an experiment that you need to run on your own and we’re excited to see your results</p>
</blockquote>
</section>
</div>
</div>
</div>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Sequence {Labeling}},
  date = {2022-01-20},
  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w02-sequence-labeling/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Sequence Labeling.”</span> January 20, 2022.
<a href="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w02-sequence-labeling/">https://orenbochman.github.io/notes-nlp/notes/cs11-737-w02-sequence-labeling/</a>.
</div></div></section></div> ]]></description>
  <category>Multilingual NLP</category>
  <category>NLP</category>
  <category>Notes</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w02-sequence-labeling/</guid>
  <pubDate>Wed, 19 Jan 2022 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Text Classification</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w01-inro/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/tiling.png" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>course banner</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/JpOJiL9ZF7w" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Lesson Video
</figcaption>
</figure>
</div><div id="sup-slide-deck" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup" id="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1: Slides for this week
</figcaption>
</figure>
</div><div id="sup-slide-deck2" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides-speech.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides-speech.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup" id="sup-slide-deck2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;2: Speech Slides
</figcaption>
</figure>
</div></div>


<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Why multilingual NLP?</li>
<li>What makes multilingual NLP hard?</li>
<li>Survey of linguistic diversity of class</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- Transcript here is machine generated no punctuation and I eddited it a but but it needs more work and I dont see the benefits unless I can run it through some tool. -->
<blockquote class="blockquote">
<p>Welcome everyone to CS 11-737.</p>
</blockquote>
<blockquote class="blockquote">
<p>This is a multilingual natural language processing and it will be run by me (Graham Neubig) and Alan Black and Shinji Watanabe.</p>
</blockquote>
<blockquote class="blockquote">
<p>We’re going to be covering all aspects of NLP from a multilingual perspective and I am waiting for Alan to come in. I don’t know if he’s here</p>
</blockquote>
<blockquote class="blockquote">
<blockquote class="blockquote">
<p>I’m here</p>
</blockquote>
</blockquote>
<blockquote class="blockquote">
<p>The title here says “multilingual natural language processing” and the reason why we’re interested in multilingual natural language processing is that in the other classes that I teach like CS 11-711 most of the time when we talk about NLP a great amount of our work not all of it is on English processing.<br>
So we say natural language but what we actually mean is English. However there’s many many languages in the world.</p>
</blockquote>
<blockquote class="blockquote">
<p>Estimates say between six thousand five hundred to seven thousand depending on you know where you draw the boundaries or you know depending on how many of these languages still have a significant number of speakers if any at all.</p>
</blockquote>
<blockquote class="blockquote">
<p>But basically there’s a huge amount of languages.</p>
</blockquote>
<blockquote class="blockquote">
<p>A huge amount of linguistic diversity throughout the world and it’d be great if we could create systems that allow us to process all of them.</p>
</blockquote>
<blockquote class="blockquote">
<p>In addition, when I talk about languages what exactly are languages isn’t clear, right?</p>
</blockquote>
<blockquote class="blockquote">
<p>So we say English but the English I speak is very different from many of the Indian English speakers. You hear Australian english speakers British english speakers and speakers from South Africa. We all speak the same language we’re all kind of mutually intelligible.</p>
</blockquote>
<blockquote class="blockquote">
<p>But we have very different ways of pronunciation we have very different terms for some things. Like truck versus lorry or elevator versus lift. Even if you go all the way to singapore for example there’s even kind of different varieties of grammar and things.</p>
</blockquote>
<blockquote class="blockquote">
<p>Like a lot of variety even within what we consider a single language.</p>
</blockquote>
<blockquote class="blockquote">
<p>Another thing to think about is: consider how we build NLP systems and you know a very long time ago, you know 40 years ago or something like this, it was very standard to build rule-based nlp systems where basically what we would do is, we would come up with a rules that were written out in a PERL program a Python program or C++ program and these rules would be entirely devised by a linguist who knew the language under consideration and in some cases for some applications these actually work pretty okay.</p>
</blockquote>
<blockquote class="blockquote">
<p>For example language generation still mostly uses world-based systems with templates and other things like this however they require lots of human effort especially for more complex tasks and they require effort for each language for which they are developed and of course you know most people in this class are very familiar with machine learning based systems or at least know that most of nlp runs on machine learning based systems now and these work really well when a lot of lots of data is available but they also require that data is available and if no data is available whatsoever then they don’t work of course so basically um we’re looking at two different methodologies here the role-based methodology is relying on a scarce resource human expertise the machine learning based systems are requiring data which also is a scarce resource in many scenarios in many languages we don’t have a whole lot of it so because of this we are uh you know we run into similar problems that you know it’s hard to even uh create these um uh these models as well so machine learning models uh what we’re talking about uh is something formally that maps an input x into an output y and in the you know case of uh nlp uh usually the input x or the output y will be text so if you took 11 7 11 with me you’re familiar with this but basically we have input text x being text output y being text in another language which corresponds to translation uh input x being text output y being a response corresponds to dialogue input x being speech and the transcript is speech recognition [Music] in input x being text and output why being linguistic structure this would be language analysis so basically you know if we want to build a machine learning model uh we need an appropriate way to represent our input an appropriate way to generate our output in an appropriate data set that consists of you know at least some inputs and outputs uh or something you know related to the inputs and outputs in order to train the model well and to learn we can use paired data including both the inputs and outputs source data which includes only the input so that could be something like monolingual text or target data which also could be something like monolingual another really salient thing that we need to be able to do for multilingual learning or cross-lingual learning is we can use paired source and target data in similar languages so we’re going to talk a lot about language similarity in this class starting today and continuing in a few lectures and this is a really powerful tool in allowing us to generalize to new languages where we don’t have lots of data in that language itself so this is just one example of the long tail of data and um this is another interesting um or this is an interesting chart from wikipedia that i calculated myself it’s basically the number of articles in wikipedia compared to the language rank and basically what you can see is after the first 10 you know english has 60 let’s see six million articles on wikipedia um and then the number of articles for language quickly drops after you get outside of english uh this uh second language here does anyone have a guess what this is if if you don’t know already you’re never going to get it right but uh but you can try chinese chinese good good guesses everyone we have lots of guesses for high resource languages actually the answer is cebuano which is definitely not a high resource language and uh the reason why uh number two is chiboando is because some swedish software engineer wrote a bot to generate wikipedia articles in cebuano for every municipality in the world and every uh every like animal or plant in the world so there’s all these uh bot generated articles for cebuanos so uh you know you would never guess that that would be number two but it also highlights the problem that even if you have lots of data uh for some languages very often it’s kind of like low uh low quality as well. So uh there’s actually another paper that just came out uh recently let’s see if i can find it very very quickly. It’s from google where they also outline the the amount of training data that they are able to find in the languages that they process . It just came out of uh like week or two ago. If you can see this here. This is the amount of uh multilingual data that they have for certain languages so you can see that after you get down past the first hundred languages or so uh they’re able to get uh i’m guessing this is probably sentences they’re able to get somewhere on the order of maybe a million sentences of training data for the top 100 languages and then as you get down to you know 200 languages or so they still have um 10 million sentences in those languages so monolingual data there’s certainly more monolingual data than there is bilingual data um but you can see that this drops off and this is on a log scale so you can see that english and kind of the the top highest resource languages are uh you know have much much more data than all the others exponentially more data let me put this in the zoom zoom chat in case anyone wants to welcome so um how do we cope with this we can do that through better models or algorithms so sophisticated modeling and training algorithms uh in order to do this you need to know like uh natural language processing and machine learning and we’ll be talking about things like that um also more uh linguistically informed methods uh that means we need to know linguistics we can also do it through better data so every piece of relevant data for a task in a loan resource language can help and how we acquire this data requires us to be resourceful and we’ll talk about what good ways of gathering data um we can also make data if necessary so if you want to work on a particular language it helps to be connected uh to you know people who know the language and can help you out and um you know different situations require different solutions so every situation is going to be unique based on the task that you want to tackle and the language you’re interested in it’s uh you know its current situation so uh you need to be aware of uh how to basically map from whatever situation you’re interested in into the most appropriate solutions for that uh situation um i had a good question uh when we say bilingual data do we need parallel data generated or curated by human experts and ellen answered mostly curated by humans um it could be parallel data generated automatically but that usually isn’t counted at these sorts of counts because he would need to um you know it’s going to be less diverse it’s going to be stilted or things like that so yeah so this class will cover a lot of things um it will cover things like linguistics including things like typology orthography morphology syntax language contact and change in code switching and actually sorry i missed phonology there as well which is probably important for speech um also a data annotated and unannotated sources data annotation linguistic databases active learning and tasks like language identification sequence labeling translation speech recognition and synthesis and syntactic analysis um we’ll also cover some of the societal considerations like ethics and the connection between language and society and this is really important um especially in some multilingual contexts because when we’re talking about language it’s very easy to think about it in the abstract as you know something that we are uh you know we’re treating as a computational object in some cases but language is fundamentally used as a method of communication between humans and so humans it’s a very important part of people’s lives and especially if you’re talking about low resource languages or languages that have been historically oppressed by people or by you know colonizers or things like this this can be a important ethical issue about who owns the language and other things like that as well um and the idea is uh by the end of the class we’d like you to be able to build a strong functioning language system in a low resource language that you do not know um with the idea being that if you can do it in a language that you don’t know you’ll be able to do it even better in a language that you do know so um uh that’s kind of the goal of class so that’s a high level motivation are there any questions about that before i move on into a kind of an overview of what we’re going to cover okay um so we’re training multilingual nlp systems one very important part of this is data creation and curation and the first step is obtaining curated training data in the language that you’re interested in and what i mean by curated is basically it’s in the form in a form that you can use easily uh not random pdfs uh that were scanned in the 1950s uh somewhere on the internet because that’s hard to you know feed into your uh fercy machine translation so we need to think about like what types of data we’ll be using uh monolingual multilingual or annotated uh data so it could be text it could be multilingual text aligned or annotated data with whatever you know linguistic features or annotations you want to predict also where can you get it can you get it from annotated data sources like uh you know there’s a whole industry of nlp papers where people have gone in and done some variety of interesting annotation on text usually english text so that you could be using but unfortunately uh you know most of these wonderful like common sense reasoning data sets or numerical question answering data sets or whatever the popular thing that people are creating uh for acl 2022 2023 is don’t exist in most languages in the world um another thing is curated text collections so how can we get text in that language and uh or do we need to scrape it on our own from from the internet can we create data how can we efficiently create high quality data so efficient uh [Music] efficient is important but high quality is equally important it’s particularly difficult to create high quality data in you know languages that don’t have lots of trained linguists or where you don’t necessarily know the language so we’re going to be talking about some of those things as well and uh ethical issues like working with communities language ownership like i mentioned before um there was a question what’s the difference between multilingual data and annotated data um so you can annotated data is basically where a human annotator has gone in and written something on the data or like given some sort of uh you know information on the data in addition to the original text itself um so uh to give an example uh like part of speech tag corpora so somebody has gone in and read the text and assigned a part of speech tag to each um to each word another example would be named entity recognized corpora named entity tagged corpora or question answering corporal where you’ve annotated it with annotated an existing text with a question and the appropriate answer for that uh so these would all be annotated uh that’s an orthogonal access to multilingual so you can have a multilingual annotated data set as well so multi-languages means in multiple languages um can i give an example of a text of a task that is only applicable to some languages i think that’s an interesting question i don’t think there are many tasks that are only applicable to some languages um [Music] there are some tasks that are kind of trivially easy in some languages like um pronunciation estimation is trivially easy in languages where the pronunciation exactly matches the more the orthography morphological analysis for chinese is not very hard because chinese has basically no morphology so there are some cases where the typological features of the language basically uh mean that there’s no need to do that task but i think most nlp tasks are applicable to most languages very good question um so once we have a data set we apply multi lingual training techniques um and we’re going to be talking about lots and lots of these so it’s kind of a central focus of the class so one way we can do this is by training a large multilingual NLP system by feeding in lots of data this would be an example like of a translation system from many languages into english and there’s lots of challenges in doing this that we’ll need to tackle.</p>
</blockquote>
<blockquote class="blockquote">
<p>For example how to train effectively how to ensure representation of low resource languages.</p>
</blockquote>
<blockquote class="blockquote">
<p>Other things that we’re going to talk about is <em>transfer learning</em>.</p>
</blockquote>
<blockquote class="blockquote">
<p>Basically we train on one language and then transfer to another, or train on many languages and transfer to another.</p>
</blockquote>
<blockquote class="blockquote">
<p>So this is like a special variety of multilingual learning where we really care about one language in particular and we would really like to improve accuracy there.</p>
</blockquote>
<blockquote class="blockquote">
<p>If you’re working for the government of azerbaijan they might really care about your Azerbaijani English or English Azerbaijani translation so they don’t care if you’re doing really well on French to English translation they just want you to maximize accuracy and that’s where kind of a transfer you know focus transfer approach is useful and we’ll talk about methods to do that as well another really popular variety of transfer is training on unannotated data and transferring to supervised tasks and that’s useful for precisely the reason that i talked about before in the google paper where you can get more uh you know monolingual data or especially more textual data than annotated data so that’s an important topic as well so another thing that we’re going to talk about is multilingual learning kind of from a linguistic perspective and um an important concept here is typology and what typology does is it basically uh says that languages across the world have similarities and differences and typology is kind of the practice and the result of organizing languages along multiple axes so to give a couple examples of this um one way you can organize languages is according to the script that they use and this is really really important in nlp because for example it’s much easier to transfer across languages that use the same script just because you know words that are pronounced the same way uh tend to have similar meanings in related languages and uh if they’re written the same way then it’s easier to tell if they’re pronounced the same way there’s many many different scripts in the world of course the most common one is latin script which we’re all familiar with and using right now there’s also cyrillic arabic you know chinese in japan they use katakana and kanji many many different scripts in uh in india in the various languages uh around india and south asia in general and then there’s some other scripts that you’ve likely not even you know most people here have not heard about before like cree or cherokee um or things like this another one is phonology so how is the language pronounced um and uh this is probably very salient to you if you learned english as a second language speaker english has many many vowels many many vowel sounds in it whereas many other languages this is an example from farsi which really only has six vowel sounds and um so uh the ways the vowels are pronounced are diff different and like if you want to learn english from farsi uh then you’re going to have to learn a lot of new vowel sounds and that makes it kind of difficult to learn english also even if farsi has only six vowel sounds that might be slightly different or even very different vowel sounds from japanese which has five thousands um so uh you know phonology is basically how do we um how do we pronounce words and how does that interact with other parts of the language morphology and syntax so what is the system of word formation and syntax is how are words brought together to make sentences so if we look at uh this morphology here english has very simple morphology so she opened the door for him again um the only real kind of thing that happened in word formation here is the the lemma open got a suffix opened and that turned uh open into the past tense um if we look at japanese which is an agglutinative language that has more morphology here we have akite which means open the door for him basically and so all of this is expressed in a single uh in a single word and then we have a polysynthetic language which basically means very very rich morphology and in mohawk she opened the door for him again as one word basically so all of this is expressed in the suffix uh that you add into the uh that into the language and there’s uh interesting interactions between morphology and syntax also like uh morphology morphologically rich languages tend to have certain syntax and vice versa syntax is how words are brought together to make sentences so as we know in english uh the stereotypical word order for english or the major word order for english is a subject verb object in japanese it’s subject object verb in other languages like irish or arabic it’s a verb subject object like this and um in other languages like malagasy it’s verb object subject and of course this is not the only you know this is not the only order we use we use other orders in english as well but um you know lots of other features you can define here but this is kind of an important salient element of the languages there’s also language varieties contact and change so languages contact uh one another and gradually evolve um this is a beautiful illustration of a language family tree that some people might have seen before this is the indo-european language family and all of the languages said split off uh from it so like hindi uh english italian spanish punjabi kurdish all have split off at different times uh but they all have a common ancestor uh there’s a lot of language families in the world indo-european is the biggest one most widely spoken one but there’s also other ones like uralic um uh which includes things like finnish and hungarian and other things like that and there’s similarities in structure like syntax between similar languages and language families but more salient is the similarity in words so in proto-indo-european there was this word over here that split off into all these other words like the english ii all the way over to you know hindi languages hindi words and other words like this so because all the languages are related to each other uh this results in you know essentially similarities between the languages that we can exploit from learning uh multilingual and open systems as well um and another thing we’re going to be talking about in the class is multilingual applications so uh we’re going to be talking um in about sequence labeling and classification uh like language id part of speech tagging named entity recognition entity linking with sequence encoders and subword encodings and um we’re going to be talking about data from universal dependencies part of speech tags wikipedia based name density recognition and linking and other things like this and how can we get multilingual data for all of these uh important tests uh we’ll be talking about morphology and syntactic analysis so how can we take this quechua uh you know polysynthetic word and break it down into an analysis like this that tells us um you know what each of these segments means what part of speech it is uh things like this and syntactic analysis covers things like dependency parsing and uh you know phrase structure parsing and i’m particularly interested in these applications not you know because we all love staring at morphological analyses and parse trees uh all day you know it’s not like question answering or um or machine translation things that are immediately useful but rather i’m excited about them from the point of view of being able to you know create a grammar textbook for an under-resourced language or help linguists with their linguistic inquiry about uh you know how the languages and uh the whole world are similar or different or things like this so from that point of view i think it’s very interesting this is an application we’re also going to talk about machine translation and sequence sequence models so um sequence sequence problems can be all kinds of different problems like the ones i talked about before and we’re going to be talking about you know sequence sequence models with attention transformers and also if you took eleven seven eleven we’re going to be going into a bit more detail about kind of advanced methods for machine translation that we covered there uh because you know machine translation is one of the more important uh you know multilingual tasks um and also low resource considerations um in for modeling challenges uh we have multilingual sharing of structured vocabulary uh balancing training over many languages doing efficient transfer between languages and incorporating various varieties of the supervision we do have for low resource languages so these are going to be very important topics in classes so are there any questions on this here uh if not i’ll turn it over to shinji for speech and we’ll also have some time to ask questions at the end hopefully if you that you need to i’ll stop my share for you okay um i i don’t see any questions so shinji if you want to sure uh can you see my screen yes okay and also just i wanna test the audio is working welcome to the multilingual nlp pores either sounds okay yeah uh it’s obviously it’s not my voice it comes from the dds okay i briefly explained about the speech extra part in this marketing of energy welcome and the uh this is a kind of uh the schedule uh actually uh the that we uh have of five courses or mainly dealing with speech but of course some of the other important speech applications and so on are in the other part of the lectures but this the five lectures from the uh march 1st to march 15th we intensively uh introduce the speech technologies and the first part we first introduce some basic speech what is speech and i will also explain about the various speech applications and which uh we will also other gifts that are more uh the introduction to the uh towards the other other causes in the speech courses and they we also release assignments we hear in the march first which is also related to speech and the next part uh that i will explain about the automatic speed recognition and the the the after the automatic speed recognition uh i will also explain about sequence to sync as models however this is a little bit more customized to the speech processing there are some overlap in the the machine translation part but i will try to emphasize the difference how we customize it for the speech problem and we’ll cover the sequence to sequence based asn tedious and the professor aram black will also introduce a text to speech data in general and then the last part of the lecture of the speech part i will try to introduce the various uh marginal uh sr uh and the ttl systems and in technically there are a lot of overlaps with the multilingual the translation and other techniques for low resource or transfer learning and so on uh but again that i will try to emphasize the problem uh data used for the speech recognition and tts problems and i think this can be a quite obvious now but the data that i will explain about what is feature condition uh automatic speech recognition input is speech and the output is corresponding text and nowadays actually there are a lot of the speech recognition applications so i’m happy that i don’t have to introduce so much but it used to be even speech recognition is not a general term and i have to spend some of the time to explain what is speech recognition and so on and here uh that i listed some of the uh the the applications products but in cmu we actually uh developed a lot of speech technology speech recognition uh technologies uh and so on so in relation to the other contribution to the uh the other research cmu actually also leading the technology side open source side in the speech recognition and i would like to actually show you the demonstration uh of the speech recognition uh can you see the other google crop i think so okay cool yeah thank you and this is actually the tool that we are maintaining espnet and i checked just using the google prop of that and this will be also uh used for our assignment so it will be good if you guys know these other tools a little bit but of course we will explain it later uh in the assignments uh the face and here i want to give you some example of the speech recognition and i prepared a couple of models english japanese spanish mandarin and the last one is the multilingual asl it’s actually can accept 52 languages although the some of the languages the performance is not very good but at least uh that it’s potentially can recognize 52 languages and today uh since uh my other main language is japanese so i kind of make a demonstration based on the japanese i just you know uh the tuta japanese uh other model and the download and so on model download and so on takes time so i already kind of skipped i already uh did it in our uh in advance and let’s do some demonstration kyo yuukih foreign and hopefully to be recognized so this is a sentence and i believe many of them i cannot know that the japanese but please trust me actually this is perfectly recognized yes right they actually know that japanese so that the priests trust us that my uh in the japanese is correctly pronounced and also that this system correctly are they recognize my speech okay this is a speech recognition and the next part uh is the tts that is also data covering covered by our lecture and this is a kind of our inverse problem of asl given the text uh to produce the uh the uh natural human sound and similar to the asl uh in addition to many success uh in the uh the tts other products cmu is actually one of the uh the the the quite active institute that also uh developed various tools various open source tools and maintain them so that i think that you guys have a great experience of not only studying uh the tts technique itself but also the technology a system that we are developing okay so now i move to the remarks uh oh maybe i it’s better to give you some demonstration of the tps because i prepared that as well so tts uh i actually using the same other uh system demonstrates an espnet toolkit and the i we have our uh actually uh the three other language uh the uh in the option english japanese mandarin and so on and then i selected the english today and i finished the model setup already because it takes a time to download the model and let’s do the synthesis i have to shovel snow today okay i have to shovel snow today there’s some kind of a bit uh the uh the crazy sound happens but overall the sound uh okay right uh i will have another example [Music] it sounds very weird right actually the other we have to provide that the abbreviation correctly and hopefully do the work multilingual nlp okay it’s working so that’s the kind of tts system and i have a couple of remarks related to this the lecture so the asl and the tts and related technologies are explained these lectures but like the the other nlp part of the courses this lecture is more like introducing the high level explanation insta introduction and system description of asl and tts so if you want to know more about the such kind of technologies uh it’ll be great if you also consider to take the speech recognition and speech processing courses where you can find more fundamental and algorithm of these uh techniques and shamu is lucky you know cmu covers all entire human language technology courses so you guys in addition to this multilingual energy you can learn speech recognition tts and so on and the uh i also would like to mention that and it’s actually uh similar to the grams that are the remarks most of asl tts technologies are still studied with major languages so as you can see from my kind of list of the models most of the language is a major language japanese chinese mandarin german spanish and so on uh right and dude this is exactly what brown mentioned due to the resources the know-how the the marketing other priorities and so on uh makes the current situations however in this multilingual course try this problem and we want to actually provide uh what kind of our other languages any of the languages you can build at the front other based on the uh sr and the tts system in our lecture so what you can learn from this lecture is exactly like that i will try to explain how we built a new other asl and tts techniques uh systems for a new language and actually one of the assignments uh in assignment three is uh for you guys to pick up one language and collect some data or they’re using existing data whatever is fine but then building a asl system that will be your assignment and also in this lecture we did a bit focus on more on the end-to-end sr but we can also try to cover the other other pipeline techniques in asl and dts and the last slide one of the ultimate goals of the human language technologies and i think this is also one of the important uh the goal in this multilingual energy course is to realize the speech to speech translation this is a kind of we can say that combining sr machine translation tds and note that we don’t directly explain about these technologies but the that we can definitely providing the explanation instructions of the core technologies of these approaches so after you guys either finish these courses maybe you can also build a speech space translation and since we also have our uh the term project so if someone is interested in maybe you can also tackle a speech to speech translation problem yeah that is from my side great thanks for watching are there any um are there any questions about this i think it was relatively uh relatively straightforward but cool to have the demos cool um okay so i’m i’m gonna talk a little bit about logistics um before we go on to ellen’s part um and so logistics first to introduce the instructors and tas we have three instructors and five tas so um ellen uh ellen black is the other instructor uh he’ll be talking in a second um i had trouble guessing uh what ellen’s research multilingual research areas where because he does everything but these are the ones that i thought of recently code switching dialogue speech synthesis um if you want to add any areas of interest [Music] no i think that’s quite good i mean i have to project the things that i’m uh doing the multilingual aspect and these are definitely core parts of what i’m doing at the moment well um and so i i like to cover just about any part of multilingual nlp i’d like everything we can do in english to be done in any of the other languages i’m particularly interested in machine translation as well so that’s a big part of my research and i’m i guess more and more interested in computational linguistics and um language you know education for endangered languages and other things like this as well so those are kind of my areas shinji talked already but you know speech recognition synthesis speech translation anything that has to give a speech advice for tas we have five excellent tas so uh i’ll maybe i’ll go through everybody and if you want to add any anything else about your research interests you can um so schwankai chung is working on speech recognition um thing reaching has previously worked on machine translation and dialogue in multilingual things he’s worked on several different things um uh atheia daviani is working on number processing for speech synthesis uh patrick uh fernandez is working on machine translation also i guess model interpretability related things and uh vijay swanathan is working on information extraction so we have a pretty wide variety and for every class i try to share this with you so you know for your class project which people are the best people to be talking to basically um so you know we can a lot of us can cover other areas as well so if you don’t know who might be the best uh person to talk to just reach out to us we can tell you um oh uh what languages do we know um is another important thing so i am a native english speaker i am fluent in japanese i can i can speak not native but nearly native level japanese i know a little bit of chinese korean and spanish um enough to read but not enough to speak well um uh ella andersonji or any of the other tas would like to share so i speak a form of english called scots english i also speak scots and i also speak japanese not as well as graham but better than most foreigners i lived there for years um i can read chinese but can’t speak it and i can read most european languages and understand them and in fact for many languages i can work out um more than you would expect so uh i actually would not have so many uh language variations but probably uh the among other the members here my japanese is the best in terms of uh that i am the i was born and i was grew up in japan and another thing i want to mention is that yeah i’m kind of typical that i only speak japanese and english but can now uh work under the marginal processing so it will be great to have some kind of a language uh the experience adapt but the the marketing of techniques uh you don’t need our other uh marginal uh the knowledge and the expertise and i i liked how ellen also said he speaks the scots variety of english so i speak the midwestern uh american variety of english and i speak uh i speak the kansai variety of japanese uh most of the time [Music] tokyo japanese so so let’s um let’s also in the chat if people wouldn’t mind sharing what uh what languages are represented in the class that would be cool too to see especially you know um i i imagine we have a lot of people who speak you know chinese and hindi but if you have any others particularly less less common ones that would be really cool too so we have korean mandarin hindi marathi arabic canada tamil french russian haitian spanish canada indonesian punjabi bengali malaya portuguese uh latin [Music] telugu i’m skipping the ones that are duplicated but uh okay let’s see um sanskrit uh spanish cool yeah i’m sure we have more uh that was just people who are uh who we’re willing to share so that’s great um i’d also like to talk about the class format next um so the class format here is a little bit of a i wouldn’t say unusual but it’s not just a purely lecture format basically um the idea is that we have a shortish lecture um you know maybe 30 30 to 40 minutes and um we for each lecture there will be a discussion question that we will try to tell you before the lecture and also usually a reading that you can read before you come to the lecture to help prime you for the discussion question and um the reason why we do this is um this is uh you know a class where there’s lots of room for creativity especially you know in your final projects and other things like this so we’d like to give people a an opportunity to you know think deeply and actively about you know the questions we have here uh to get you ready for that after that we have a very special feature of this class which uh is always a very interesting called the language intent and basically what we’re going to do is uh we’re going to have uh one group every lecture introduce a language and introduce some of the unique features of it like the typological features or you know um just anything you think is interesting and ellen is going to give an example of this for hindi uh in about five minutes and um i wrote in groups of two to three i’m sorry this is a mistake uh this was from last time when the class was smaller this time the class is larger so we’re actually going to ask you to do it in groups of four so that everybody gets at least one chance to present um and then after we have the language intent we’re going to do the breakout room discussion uh or something like a code uh data assignment walk through so um most of the time we’re gonna have discussion but sometimes we’re we’re going to be uh doing other things um and then in the case that we did a discussion um because we’re going to break out into breakout rooms or groups to do the discussion um we’re going to have a short summary at the very end where every group talks about maybe one or two interesting things that they came up with that might be kind of unique i say breakout rooms for zoom because we’ll do that via zoom but if we reconvene to an inversing class um we will uh we’ll probably just do it by breaking into into groups in the class um for the grading policy uh i have here uh the class and discussion participation is 15 language intent everybody’s required to do this um there really aren’t very many wrong answers so like as long as you do it you basically will get five percent as long as you do a good job um and then uh we have uh three assignments on multilingual sequence labeling uh which is an individual assignment uh then a multilingual translation group assignment multi-lingual speech recognition group assignment and final project uh where you you can kind of do something free for um so for 120 like as i mentioned every single class we’re going to have a discussion period however on thursday we’re not going to have a discussion period because um on thursday the first assignment is going to be assigned so we’re going to have the tas like go through the assignment and uh give you an idea of like what you should be doing have a q a about that so uh yeah that’s all we have in terms of logistics for the class um are there any questions that people would like to ask uh and if not we’ll jump right into owen’s language uh yeah when it’s not the code work what is the discussion about usually so um one example of a discussion that we might be uh having soon is um [Music] like for example for a class when we talk about language similarity and typology um there is a reading about uh there might be a reading about transfer choosing which languages to transfer from in transfer learning or at least that’s what we did last year um that’s ellen’s class we’ll let him decide exactly what we do uh this year but um so there’s a paper where you’ll read about which languages to transfer from in transfer learning we’ll have a lecture about kind of similarities between languages and then in the discussion period we’d have a discussion about for example um given a language that you know which languages do you think would be the best languages to transfer from uh when you’re doing a particular nlp task and why um so then you’d have to think you know what have i learned about cross-lingual learning uh what have i learned about typology what do i know about the language that i’m unfamiliar with and so which languages would be the best language to transfer from based on this knowledge that i got from the class so that that’s just one example and then we go around in a small group and i’ll discuss this and hopefully that would give you some insights into you know what other languages look like and things like that thank you great and i’ll stop cheering okay i think we can see let’s go okay so this is an example of the language intent and i’m going to choose hindi and i’m going to use hindi because i sort of know a little hindi and being british hindi is one of our non-english languages that’s spoken in the uk because of historical reasons that you’re probably all aware of um so hindi um it’s spoken in northern india there’s about 320 million native speakers um but there’s also a substantial number of l2 speakers non-native speakers because lots of people in northern india get taught hindi as a second language and they’re quite fluent at it and they will use it often so there’s probably almost the same again uh l2 speakers and depending how you count um it’s somewhere like the third or fourth most commonly spoken language um on the planet after mandarin english spanish depending how you draw the boundary there so it is a very very common language on the planet even though its language technologies is significantly behind it is a lingua franca of northern india so most of the northern indians even though it might not be their um native language get taught it to school and are quite fluent in it it also gets taught in southern india but people are not as fluent in southern india and in some areas they might not want to speak it as much because they see it as the northern language not the southern language india in case you didn’t know it’s full of lots of languages there are about 20 standard languages for your country there are hundreds over the whole country and so each region basically has its own language in it they’re really quite distinct in many cases it’s also worth talking about the distinction between hindi and urdu urdu is mostly it’s the it’s the national language of pakistan and but it’s also spoken throughout um india as well um it is mutually intelligible so people who speak hindi and urdu can talk to each other they have to be a little careful in choosing their words but probably no more different in maybe americans and brits choosing it although if you have two speakers speaking together maybe a hindi person would not understand what they’re saying because they would drift away from what might be considered to be standard hindi the writing of these languages urdu and hindi is completely different urdu uses an arabic script while hindu uses dead nagari script and although maybe the uru speakers can read dev nagari the reverse is typically not true at all hindi has a lot of sanskrit in it while um urdu is more likely to have persian and arabic borrowings in it and therefore it may be that the urdu speaker is less understood than the um a hindi speaker sometimes these two languages together are called hindustani definitely historically that was the case and they do share phonology and they do share grammar so there’s some similarity in that language where you could maybe use for speech recognition or speech synthesis at a level that would be useful but the writing system is not that um hindi is indo-european um actually indo-aryan and that means that there are things in hindi that are actually similar with a historical um relationship with even english even though they really are very far away for example uh maharaja which you might have heard of um which was a term for a um major royal in um in india um actually is a direct cognate of magnus royal great king and and so there is relationships there and there’s a number of things that you find in numbers as well where um there’s some similarity that you can sort of guess the relationship there um the script normally uses dev nagari that’s ceramic script um um many of the writing systems used in the indian subcontinent are bramiscript and therefore they’re similar in the way that they write and break up the syllables um devnegarry itself is also used for marathi which is spoken in mumbai although there’s some characters are different in nepali though there are some characters that are different so it’s maybe a sort of more standard way of writing over the whole of the um subcontinent um urdu uses its own form of um arabic and therefore is really quite different okay however often in social media in hindi it’s written in the latin script so it’s romanized okay it’s often said it’s written in english um but it’s not written in english even though it may have lots of borrowed english actually in it hindi grammar is mostly um subject object verb so it’s verb final uh but it’s often pro dropped so the subject is often not said and it’s sort of understood um in the environment and that’s just fine and normal english doesn’t do that very often although sometimes it does um but in hindi it’s very common there’s gender in all the nouns um which is not true in english but in hindi it is and there’s agreement between the verbs and um the um the nouns and there’s quite a rich inflection morphology with agreement and therefore words have different forms depending on what they’re agreeing actually with um rather interestingly it has what’s called arrogative ma marking many of the european languages if they do have agreement they have agreement between the subject and the verb but in hindi and some other um indian subcontinent language the agreement is between the object and the verb and to europeans this is seemed as weird but it’s not weird at all because it’s what the hindi people actually do and there’s more of them than you so you should be aware of that the phrenology there’s quite a rich number of um a vowels but they’re different than uh english however because english is quite common in india it’s also a lingua franca especially over north and south and there’s actually a number of english um vowels that have been adopted into hindi and are just quite normal now and most people can pronounce them not everybody but most people um can pronounce them so in other words you have not just the native phonology but you’ve also got the borrowed english phonology coming into the um the language okay um the phonology is very rich um especially with um consonants and there’s much more distinction between consonants that for example in english we do not have so for example in um hindi they make distinctions between aspirated and unaspirated stops which we do in english but we don’t actually have a phonological distinction was that is that english native speakers generate them but we don’t really care about the distinction between them but um indian speakers do this and this often sounds that when they’re speaking english they sound like that there’s more example breath coming out when they’re actually speaking another thing that is very common in hindi which again doesn’t have any equivalent in um european languages is what’s called retroflex where the tongue is held a little bit further back in the mouth and that’s one of the things that distinguishes indian english and accented english and compared to um american or british english um but there are lots of phonological differences and rather interestingly some of the non-natives who speak hindi fluently don’t always do this so many of the southerners don’t do this and therefore when they’re speaking hindi even though they’re speaking it fluently all the northerners can tell that they’re speaking southern hindi because they’re not doing that in the same way and maybe you’re familiar with this in um other things like this in china where southern um chinese are not doing voicing the same as northern chinese that’s the same thing that’s happening in india and it’s worth actually talking about this other form of language that’s appearing in um the indian subcontinent and that’s what’s called english it’s a mixture of hindi in english and most educated hindi speakers are fluent in english i mean they’re not just can speak english they’re actually completely fluent and may even use it as their primary literate language because since attending maybe middle school onwards they’ve been reading and writing in english more than they’ve been doing that in hindi and so what happens when many hindi speakers are speaking hindi is they use a lot of borrowed english terms and they’ll mix them in this term of being code switching where they’ll basically use two languages at once and this isn’t just like borrowing english words that happens in say japanese and it’s very common this is actually mixing the languages so you’ll get mixed grammar mixed morphology um and choosing different words now code switching is very common in multilingual environments most of the planet um is multilingual and it’s really just um the americans and the brits that are not and therefore they’re not used to it but code switching is very common to the extent that english might even have become the standard way of speaking hindi and you can tease a hindi speaker by asking them to give a long sentence which contains hindi and only hindi words and has no um english words in it now there’s lots of borrowed words there’s been english in india for over 300 years so you know the word for doctor in hindi is doctor the word for boss is boss but um there are lots of other words that um on phrases which are actually mixed into english and they’re very common and lots of people do studies on english as a language in its own right because it’s really becoming one of the standard ways that hindi speakers are actually speaking today so that’s my 10 minute um hindi um presentation you can see what i’m doing i’m talking about the language i’m talking about technical aspects of the language historical action aspects of the language i didn’t say much about the language technologies of hindi it’s not very much actually despite it being the third or fourth spoken language on the planet and it doesn’t actually have that much technology and often when it does it’s all devnaggery based and not actually romanized while actually most people who are writing hindi are romanizing when they’re sending text messages and writing emails so i talk about history geography social position because there might be social aspects of how the language fits into the particular country it might be spoken in many different countries for example there is um hindi is an official language in jamaica it’s also an official language in um fiji um partly because of the british empire um and the hindi speakers have moved there although in those cases everything is romanized and they don’t use devnegary at all i talked about morphology grammar and phonology which is an interesting thing and you’ll be able to find information about that through wikipedia or elsewhere i talked about maybe something that’s linguistically interesting about the language arguative language is something actually distinguished it code switching is something that distinguishes i um maybe state something about the respect to the resources how big is the wikipedia have people got morphology how good is the speech recognition do people actually use it um for interacting with machines and maybe influences social uses um of the language or influences um on it or things which are sort of interesting and relevant that might be important if you are going to do some sort of language technology for the target language now hindi is one of the big languages it might be that you’re going to select something that’s much less common and therefore harder to be able to find information about and that’s fine too okay i know one group is probably going to try to do um standard mandarin um as an example but don’t think that you have to take big languages we’re quite interested in the smaller languages as well</p>
</blockquote>
</div>
</div>
</div>

<div class="no-row-height column-margin column-container"><div id="sup-slide-deck3" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="lang-in-10-hindi.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="lang-in-10-hindi.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup" id="sup-slide-deck3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;3: Hindi in 10 minutes!
</figcaption>
</figure>
</div></div><section id="discussion-question" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="discussion-question">Discussion Question:</h2>
<blockquote class="blockquote">
<p>What are some unique typological features of a language that you know, regarding phonology, morphology, syntax, semantics, pragmatics?</p>
</blockquote>
<p>Hebrew has unique typological features:</p>
<ul>
<li>Etymology:
<ul>
<li>Most of the Hebrew words are are derived from the bible.</li>
<li>Hebrew is a semitic language, and has a large number of cognates with other semitic languages like arabic and aramaic.</li>
<li>Hebrew has a large number of loan words from other languages, including English, Arabic, and Yiddish.</li>
<li>Hebrew is an old language, and has a large number of archaic words and constructions that are not used in modern Hebrew.</li>
<li>Words that were introduced into Hebrew during different areas may have some differences in stress patterns, etc.</li>
<li>Hebrew has died out as a spoken language and been revived a number of times being preserved in the intrim as a liturgical language. During the time of the new testament, Hebrew had been replaced by Aramaic as the spoken language, but was revived after the inserection of Bar Kochba. Historical israel was depopulated from hebrew speakers by the arabs in the 4th century A.D. The modern revival of Hebrew was started by Eliezer Ben Yehuda in the 19th century.</li>
</ul></li>
<li>Orthography:
<ul>
<li>Hebrew has a unique script that is written from right to left.</li>
<li>Hebrew orthography where the vowels are not written and final forms of the letters are used at the end of the word.</li>
</ul></li>
<li>Phonology:
<ul>
<li>Many sounds in hebrew have effectively merged when compared to neighboring semitic languages.</li>
<li>Hebrew also has a script for indicating vowels, but these are ommited in most written texts. Correctly annotating the full range vowels required a sophisticated understanding of the language and is not in the skill set of most writers.</li>
</ul></li>
<li>Morphology: Hebrew has
<ul>
<li>A templatic morphology, where roots are three or more consonants and template are vowels and affixes. The root are inserted into the template to form words.</li>
<li>Most foriegn words do not follow this pattern, but undergo a process of nativization that allow them to take some suffixes.</li>
<li>Hebrew has a smichut or construct state, where two nouns are combined to form a new noun and these follow a number of rules.</li>
<li>Hebrew can combines one or more small words as prefixes to the verb.</li>
<li>Hebrew morphology has a number of additional mechanism for word formation described in “The final word” by Uzzi Ornan.</li>
</ul></li>
<li>Semantics
<ul>
<li>Written hebrew is highly ambiguous, and the meaning of a word is frequently determined by the context. It takes about a year for readers to learn to read hebrew fluently and even professional<sup>1</sup> news casters make mistakes when the contexts the determine the meaning of a word appears later in the sentence.</li>
</ul></li>
<li>Syntax
<ul>
<li>Hebrew has a subject verb object word order, but this is not strict and can be changed for emphasis.</li>
<li>Hebrew has a number of particles that are used to indicate the relationship between words in a sentence.</li>
</ul></li>
</ul>


<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;today most news are not read by professionals.</p></div></div></section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Text {Classification}},
  date = {2022-01-18},
  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w01-inro/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Text Classification.”</span> January 18,
2022. <a href="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w01-inro/">https://orenbochman.github.io/notes-nlp/notes/cs11-737-w01-inro/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Multilingual NLP</category>
  <category>NLP</category>
  <category>Notes</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w01-inro/</guid>
  <pubDate>Mon, 17 Jan 2022 22:00:00 GMT</pubDate>
</item>
</channel>
</rss>
