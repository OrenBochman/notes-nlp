<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>NLP Course Notes &amp; Research</title>
<link>https://orenbochman.github.io/notes-nlp/</link>
<atom:link href="https://orenbochman.github.io/notes-nlp/index.xml" rel="self" type="application/rss+xml"/>
<description>Course and Research notes</description>
<image>
<url>https://orenbochman.github.io/notes-nlp/images/nlp-brain-wordcloud.jpg</url>
<title>NLP Course Notes &amp; Research</title>
<link>https://orenbochman.github.io/notes-nlp/</link>
</image>
<generator>quarto-1.6.39</generator>
<lastBuildDate>Wed, 12 Feb 2025 22:00:00 GMT</lastBuildDate>
<item>
  <title>A Convolutional Attention Network for Extreme Summarization of Source Code</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/2016-code-summerization/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/literature-review-open-book.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Litrature review"><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="img-fluid figure-img" alt="Litrature review"></a></p>
<figcaption>Litrature review</figcaption>
</figure>
</div></div><p><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2016-code-summerization/img/fig01.png" id="fig-01" class="img-fluid"> <img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2016-code-summerization/img/fig02.png" id="fig-02" class="img-fluid"> <img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2016-code-summerization/img/fig03.png" id="fig-03" class="img-fluid"></p>
<p>This is a paper mentioned in the course on multilingual NLP by Graham Neubig. With an interesting idea of an second attention head being used to copy stuff from the input directly to the output.</p>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>Attention mechanisms in neural networks have proved useful for problems in which the input and output do not have fixed dimension. Often there exist features that are locally translation invariant and would be valuable for directing the model’s attention, but previous attentional architectures are not constructed to learn such features specifically. We introduce an attentional neural network that employs convolution on the input tokens to detect local time-invariant and long-range topical attention features in a context-dependent way. We apply this architecture to the problem of extreme summarization of source code snippets into short, descriptive function name-like summaries. Using those features, the model sequentially generates a summary by marginalizing over two attention mechanisms: one that predicts the next summary token based on the attention weights of the input tokens and another that is able to copy a code token as-is directly into the summary. We demonstrate our convolutional attention neural network’s performance on 10 popular Java projects showing that it achieves better performance compared to previous attentional mechanisms. – <span class="citation" data-cites="allamanis2016convolutionalattentionnetworkextreme">(Allamanis, Peng, and Sutton 2016)</span></p>
</blockquote>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-allamanis2016convolutionalattentionnetworkextreme" class="csl-entry">
Allamanis, Miltiadis, Hao Peng, and Charles Sutton. 2016. <span>“A Convolutional Attention Network for Extreme Summarization of Source Code.”</span> <a href="https://arxiv.org/abs/1602.03001">https://arxiv.org/abs/1602.03001</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {A {Convolutional} {Attention} {Network} for {Extreme}
    {Summarization} of {Source} {Code}},
  date = {2025-02-13},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/2016-code-summerization/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“A Convolutional Attention Network for
Extreme Summarization of Source Code.”</span> February 13, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/2016-code-summerization/">https://orenbochman.github.io/notes-nlp/reviews/paper/2016-code-summerization/</a>.
</div></div></section></div> ]]></description>
  <category>NLP</category>
  <category>Paper</category>
  <category>Attention</category>
  <category>Deep learning</category>
  <category>Review</category>
  <category>Stub</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/2016-code-summerization/</guid>
  <pubDate>Wed, 12 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Floating Constraints in Lexical Choice</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/1997-floating-contraints-in-lexical-choice/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/literature-review-open-book.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Literature review"><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="img-fluid figure-img" alt="Literature review"></a></p>
<figcaption>Literature review</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/DffGdrfY9gI" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid quarto-uncaptioned" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1
</figcaption>
</figure>
</div></div>
<section id="context" class="level2">
<h2 class="anchored" data-anchor-id="context">Context</h2>
<p>This is one of the three paper mentioned by Kathleen McKeown in her heroes of NLP interview by Andrew NG. The paper is about the floating constraints in lexical choice.</p>
<p>Looking quickly over it I noticed a couple of things:</p>
<!--
@article{mckeown1997floating,
  title={Floating constraints in lexical choice},
  author={McKeown, Kathleen and Elhadad, Michael and Robin, Jacques},
  year={1997}
}
-->
<ol type="1">
<li>I was familiar with Elhadad’s work on generation. I had read his work on FUF Functional Unification Formalism in the 90s which introduced me to the paradigm of unification before I even knew about Prolog or logic programming.</li>
<li>I found it fascinating that McKeown and Elhadad had worked together on this paper and even a bit curious about what they were looking into here.</li>
<li>Since McKeown brought it up, it might intimate that there is something worth looking into here. And it is discusing aspects of generative language models.</li>
</ol>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>Lexical choice is a computationally complex task, requiring a generation system to consider a potentially large number of mappings between concepts and words. Constraints that aid in determining which word is best come from a wide variety of sources, including syntax, semantics, pragmatics, the lexicon, and the underlying domain. Furthermore, in some situations, different constraints come into play early on, while in others, they apply much later. This makes it difficult to determine a systematic ordering in which to apply constraints. In this paper, we present a general approach to lexical choice that can handle multiple, interacting constraints. We focus on the problem of floating constraints, semantic or pragmatic constraints that float, appearing at a variety of different syntactic ranks, often merged with other semantic constraints. This means that multiple content units can be realized by a single surface element, and conversely, that a single content unit can be realized by a variety of surface elements. Our approach uses the Functional Unification Formalism (FuF) to represent a generation lexicon, allowing for declarative and compositional representation of individual constraints. – <span class="citation" data-cites="mckeown1997floating">(McKeown, Elhadad, and Robin 1997)</span></p>
</blockquote>
</section>
<section id="outline" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li>Introduction
<ul>
<li>Mentions the complexity of lexical choice in language generation.</li>
<li>Notes the diverse sources of constraints on lexical choice, including syntax, semantics, lexicon, domain, and pragmatics.</li>
<li>Highlights the challenges posed by floating constraints, which are semantic or pragmatic constraints that can appear at various syntactic ranks and be merged with other semantic constraints.</li>
<li>Presents examples of floating constraints in sentences.</li>
</ul></li>
<li>An Architecture for Lexical Choice
<ul>
<li>Discusses the role of lexical choice within a typical language generation architecture.</li>
<li>Argues for placing the lexical choice module between the content planner and the surface sentence generator.</li>
<li>Describes the input and output of the lexical choice module, emphasizing the need for language-independent conceptual input.</li>
<li>Presents the two tasks of lexical choice: syntagmatic<sup>1</sup> decisions (determining thematic structure) and paradigmatic<sup>2</sup> decisions (choosing specific words).</li>
<li>Introduces the FUF/SURGE package as the implementation environment.</li>
</ul></li>
<li>Phrase Planning in Lexical Choice
<ul>
<li>Describes the need for phrase planning to articulate multiple semantic relations into a coherent linguistic structure.</li>
<li>Explains how the Lexical Chooser selects the head relation and builds its argument structure based on focus and perspective.</li>
<li>Details how remaining relations are attached as modifiers, considering options like embedding, subordination, and syntactic forms of modifiers.</li>
</ul></li>
<li>Cross-ranking and Merged Realizations
<ul>
<li>Discusses the non-isomorphism between semantic and syntactic structures, necessitating merging and cross-ranking realizations.</li>
<li>Provides examples of merging (multiple semantic elements realized by a single word) and cross-ranking (single semantic element realized at different syntactic ranks).</li>
<li>Emphasizes the need for linguistically neutral input to support this flexibility.</li>
<li>Explains the implementation of merging and cross-ranking using FUF, highlighting the challenges of search and the use of bk-class for efficient backtracking.</li>
</ul></li>
<li>Interlexical Constraints
<ul>
<li>Defines interlexical constraints as arising from alternative sets of collocations for realizing pairs of content units.</li>
<li>Presents an example of interlexical constraints with verbs and nouns in the basketball domain.</li>
<li>Discusses different strategies for encoding interlexical constraints in the Lexical Chooser.</li>
<li>Introduces the :wait mechanism in FUF to delay the choice of one collocate until the other is chosen, preserving modularity and avoiding backtracking.</li>
</ul></li>
<li>Other Approaches to Lexical Choice
<ul>
<li>Reviews other systems using FUF for lexical choice, comparing their architectures and features to ADVISOR-II.</li>
<li>Discusses non-FUF-based systems, categorizing them by their positioning of lexical choice in the generation process and analyzing their handling of various constraints.</li>
</ul></li>
<li>Conclusion
<ul>
<li>Summarizes the contributions of the paper, highlighting the ability to handle a wide range of constraints, the use of FUF for declarative representation and interaction, and the algorithm for lexical selection and syntactic structure building.</li>
<li>Emphasizes the importance of syntagmatic choice and perspective selection for handling floating constraints.</li>
<li>Notes the use of lazy evaluation and dependency-directed backtracking for computational efficiency.</li>
<li>Mentions future directions, such as developing domain-independent lexicon modules and methods for organizing large-scale lexica.</li>
</ul></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;of or denoting the relationship between two or more linguistic units used sequentially to make well-formed structures. The combination of ‘that handsome man + ate + some chicken’ forms a syntagmatic relationship. If the word position is changed, it also changes the meaning of the sentence, eg ’Some chicken ate the handsome man</p></div><div id="fn2"><p><sup>2</sup>&nbsp;Paradigmatic relation describes the relationship between words that can be substituted for words with the same word class (eg replacing a noun with another noun)</p></div></div></section>
<section id="reflection" class="level2">
<h2 class="anchored" data-anchor-id="reflection">Reflection</h2>
<ul>
<li>In unification-based approaches, any word is a candidate unless it is ruled out by a constraint. With an LLM a word is selected based on its probability.</li>
<li>The idea of constraints is deeply embedded in Linguistics. The two type that immediately come to mind are:
<ul>
<li>Subcategorization constraints which are rules that govern the selection of complements and adjuncts. These are syntactic constraints.</li>
<li>Selectional constraints which are rules that govern the selection of arguments. These are semantic constraints</li>
</ul></li>
<li>Other constraints that I did not have to deal with as often are pragmatics and these are resolved from the state of the world or common sense knowledge.</li>
<li>In FUF there are many more constraints and if care isn’t taken, the system can become over constrained and no words will be selected.</li>
<li>One Neat aspect of the approach though is that FUF language could be merged with a query to generate a text.</li>
<li>Unfortunately as fat as I recall the FUF had to be specified by hand. The only people who could do this were the people who designed the system. They needed to be expert at linguistics and logic programming. Even I was pretty sharp at the time the linguistics being referred was far too challenging. I learned that in reality every computational linguist worth their salt had their own theory of language and their own version of grammar and that FUF was just one of many.</li>
<li>It seems that today attention mechanisms within LSTMs or Transformers are capable of learning a pragmatic formalism without a linguist stepping in to specify it.</li>
</ul>
<p>Why this is still interesting:</p>
<p>Imagine we want to build a low resource capable language model.</p>
<p>We would really like it to be good with grammar Also we would like it to have a lexicon that is as rich as the person it is interacting with. And we would also like to make available to it the common sense knowledge that it would need to use in the context of current and possibly a profile based on past conversations…. On the other hand we don’t want to require a 70 billion parameter model to do this. So what do we do?</p>
<p>Let imagine we start by training on Wikipedia, after all wikipedia’s mission is to become the sum of all human knowledge. By do we realy need all of the information in Wikipedia in out edge model?</p>
<p>The answer is no but the real question is how can we partition the training information and the wights etc so that we have a powerful model with a basic grammar and lexicon but which can load a knowledge graph, extra lexicons and common sense knowledge as needed.</p>
<p>So I had this idea from lookin at a <span class="citation" data-cites="ouhalla1999introducing">Ouhalla (1999)</span>. It pointed out there were phrase boundries and that if the sentence was transformed in a number of ways t would still make sense if we did this with respect to a phrase but if we used cucnks that were too small or too big the transofrmation would create malformed sentences. The operations of intrerest were movement and deletion.</p>
<p>and that can still have a model that is both small but able to access this extra material.</p>
<p>One direction seems to me is that we want to apply queries to all the training material as we train the model. Based on the queries that we retrieve each clause or phrase we can decide where we want to learn it and if we want to learn it at all.</p>
<p>(If it is common sense knowledge we may already know it. If it is semi structured we may want to put it into wikidata. If it is neither we may want to avoid learning it at all. We may still want to use it for improving the grammar and lexicon.) However we may want to store it in a speclised lexicon for domains like medicinee or law.</p>
<p>We could also decide if and how to eliminate it from we want to The queries should match each bit of information in the sentence. These are our queries. While the facts may change form</p>
<ol type="1">
<li>say we want to decompose a wikipedia article</li>
</ol>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-mckeown1997floating" class="csl-entry">
McKeown, Kathleen, Michael Elhadad, and Jacques Robin. 1997. <span>“Floating Constraints in Lexical Choice.”</span>
</div>
<div id="ref-ouhalla1999introducing" class="csl-entry">
Ouhalla, J. 1999. <em>Introducing Transformational Grammar: From Principles and Parameters to Minimalism</em>. A Hodder Arnold Publication. Arnold. <a href="https://www.google.com/books?id=ZP-ZQ0lKI9QC">https://www.google.com/books?id=ZP-ZQ0lKI9QC</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Floating {Constraints} in {Lexical} {Choice}},
  date = {2025-02-12},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/1997-floating-contraints-in-lexical-choice/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Floating Constraints in Lexical
Choice.”</span> February 12, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/1997-floating-contraints-in-lexical-choice/">https://orenbochman.github.io/notes-nlp/reviews/paper/1997-floating-contraints-in-lexical-choice/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/1997-floating-contraints-in-lexical-choice/</guid>
  <pubDate>Wed, 12 Feb 2025 16:19:59 GMT</pubDate>
</item>
<item>
  <title>Coverage Embedding Models for Neural Machine Translation</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/2016-coverage-embedding-models-for-NMT/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/literature-review-open-book.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Literature review"><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="img-fluid figure-img" alt="Literature review"></a></p>
<figcaption>Literature review</figcaption>
</figure>
</div></div><section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<ol class="example" type="1">
<li>is discussed in week 6 the <a href="../../../notes/cs11-737-w06/index.html">Multilingual NLP course</a>. Neural generative models tend to drop or repeat content. But for NMT we can assume that all the inputs should be represented in the output. For each uncovered word it imposes a penalty on the attention model.</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR - Coverage Embedding
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/in_a_nutshell.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Coverage Embedding in a Nutshell"><img src="https://orenbochman.github.io/notes-nlp/images/in_a_nutshell.jpg" class="img-fluid figure-img" alt="Coverage Embedding in a Nutshell"></a></p>
<figcaption>Coverage Embedding in a Nutshell</figcaption>
</figure>
</div>
<ul>
<li>The paper introduces coverage embedding models to address the issues of repeating and dropping translations in NMT.</li>
<li>The coverage embedding vectors are updated at each time step to track the coverage status of source words.</li>
<li>The coverage embedding models significantly improve translation quality over a large vocabulary NMT system.</li>
<li>The best model uses a combination of updating with a GRU and updating as a subtraction.</li>
<li>The coverage embedding models also reduce the number of repeated phrases in the output.</li>
</ul>
</div>
</div>
</section>
<section id="podcast" class="level2">
<h2 class="anchored" data-anchor-id="podcast">Podcast</h2>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>In this paper, we enhance the attention-based neural machine translation (NMT) by adding explicit coverage embedding models to alleviate issues of repeating and dropping translations in NMT. For each source word, our model starts with a full coverage embedding vector to track the coverage status, and then keeps updating it with neural networks as the translation goes. Experiments on the large-scale Chinese-to-English task show that our enhanced model improves the translation quality significantly on various test sets over the strong large vocabulary NMT system. – <span class="citation" data-cites="mi-etal-2016-coverage">(Mi et al. 2016)</span></p>
</blockquote>
</section>
<section id="glossary" class="level2">
<h2 class="anchored" data-anchor-id="glossary">Glossary</h2>
<p>This paper has a number of big words and concepts that are important to understand. Lets break them down together:</p>
<dl>
<dt>Neural Machine Translation (NMT)</dt>
<dd>
A machine translation approach that uses neural networks to learn the mapping between source and target languages.
</dd>
<dt>Attention Mechanism</dt>
<dd>
In NMT, a mechanism that allows the model to focus on different parts of the source sentence when generating each word in the target sentence.
</dd>
<dt>Coverage Vector</dt>
<dd>
A vector used in statistical machine translation to explicitly track which source words have been translated.
</dd>
<dt>Coverage Embedding Vector</dt>
<dd>
A vector specific to each source word in this model, used to track the translation status of the word. It is initialized with a full embedding and is updated based on attention scores.
</dd>
<dt>Gated Recurrent Unit (GRU)</dt>
<dd>
A type of RNN cell used to model sequential data, including language. Here it is used to update coverage embeddings.
</dd>
<dt>Attention Probability (α)</dt>
<dd>
A set of weights that indicate how much attention the model pays to each source word when predicting a target word.
</dd>
<dt>Encoder-Decoder Network</dt>
<dd>
A neural network architecture commonly used in sequence-to-sequence tasks like NMT. The encoder processes the input sequence, and the decoder generates the output sequence.
</dd>
<dt>Bi-directional RNN</dt>
<dd>
A RNN that processes a sequence in both forward and backward directions, capturing contextual information from both sides of a word.
</dd>
<dt>Soft Probability</dt>
<dd>
Probabilities in the attention mechanism aren’t hard (0 or 1), but instead are on a continuum, indicating a degree of attention or importance.
</dd>
<dt>Fertility</dt>
<dd>
In the context of translation, fertility refers to the number of words in the target language that can be translated from a single word in the source language.
</dd>
<dt>One-to-many Translation</dt>
<dd>
A translation where one source word corresponds to multiple words in the target language.
</dd>
<dt>TER (Translation Error Rate)</dt>
<dd>
A metric used to evaluate the quality of machine translation by calculating the number of edits required to match the system’s translation to a reference translation, with lower scores being better.
</dd>
<dt>BLEU (Bilingual Evaluation Understudy)</dt>
<dd>
A metric to evaluate the quality of machine translation by comparing a candidate translation to one or more reference translations, with higher scores being better.
</dd>
<dt>UNK</dt>
<dd>
Abbreviation for “unknown.” In machine translation, it is used to denote words that are not in the model’s vocabulary.
</dd>
<dt>AdaDelta</dt>
<dd>
An adaptive learning rate optimization algorithm, that adjusts the learning rate during training for faster convergence.
</dd>
<dt>Alignment</dt>
<dd>
In the context of machine translation, the process of determining which words in the source sentence correspond to words in the target sentence.
</dd>
<dt>F1 Score</dt>
<dd>
A measure of a test’s accuracy and it considers both the precision and recall of the test to compute the score.
</dd>
</dl>
<p>With a solid understanding of this terminology we can now dive into the paper.</p>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ol type="1">
<li>Introduction
<ul>
<li>Notes that in NMT attention mechanisms focus on source words to predict target words.</li>
<li>Point out that <mark>these models lack history or coverage information, leading to repetition or dropping of words.</mark></li>
<li>Recalls how Statistical Machine Translation (SMT) used a binary “coverage vector” to track translated words.</li>
<li>Explains that SMT coverage vectors use 0s and 1s, whereas attention probabilities are soft. SMT systems handle one-to-many fertilities using phrases or hiero rules, while NMT systems predict one word at a time.</li>
<li>Introduces <strong>coverage embedding vectors</strong>, updated at each step, to address these issues.</li>
<li>Explains that <mark>each source word has its own coverage embedding vector that starts as a full embedding vector</mark>(as opposed to 0 in SMT).</li>
<li>States that coverage embedding vectors are updated based on attention weights, moving toward an empty vector for words that have been translated.</li>
</ul></li>
<li>Neural Machine Translation
<ul>
<li>Recalls that attention-based NMT uses an encoder-decoder architecture.
<ul>
<li>The encoder uses a bi-directional RNN to encode the source sentence into hidden states.</li>
<li><mark>The decoder predicts the target translation by maximizing the conditional log-probability of the correct translation.</mark></li>
<li>The probability of each target word is determined by the previous word and the hidden state.</li>
<li>The hidden state is computed using a weighted sum of the encoder states, with weights derived from a two-layer neural network.</li>
</ul></li>
<li>Introduces coverage embedding models into the NMT by adding an input to the attention model.</li>
</ul></li>
<li>Coverage Embedding Models
<ul>
<li>The model uses a coverage embedding for each source word that is updated at each time step.</li>
<li><mark>Each source word has its own coverage embedding vector,</mark> and the number of coverage embedding vectors is the same as the source vocabulary size.</li>
<li>The coverage embedding matrix is initialized with coverage embedding vectors for all source words.</li>
<li>Coverage embeddings are updated using neural networks (GRU or subtraction).</li>
<li>As the translation progresses, coverage embeddings of translated words should approach zero.</li>
<li>Two methods are proposed to update the coverage embedding vectors: GRU and subtraction.</li>
</ul>
<ol type="1">
<li>Updating Methods
<ol type="1">
<li>Updating with a GRU
<ul>
<li>The coverage model is updated using a GRU, incorporating the current target word and attention weights.</li>
<li>The GRU uses update and reset gates to control the update of the coverage embedding vector.</li>
</ul></li>
<li>Updating as Subtraction
<ul>
<li>The coverage embedding is updated by subtracting the embedding of the target word, weighted by the attention probability.</li>
</ul></li>
</ol></li>
<li>Objectives
<ul>
<li>Coverage embedding models are integrated into attention NMT by adding coverage embedding vectors to the first layer of the attention model.</li>
<li>The goal is to remove partial information from the coverage embedding vectors based on the attention probability.</li>
<li>The model minimizes the absolute values of the embedding matrix.</li>
<li>The model can also use supervised alignments to know when the coverage embedding should be close to zero.</li>
</ul></li>
</ol></li>
<li>Related Work
<ul>
<li>Tu et al.&nbsp;(2016) also uses a GRU to model the coverage vector. However, this approach differs from the current paper’s method by initializing the word coverage vector with a scalar and adding an accumulation operation and a fertility function.</li>
<li>Cohn et al.&nbsp;(2016) augments the attention model with features from traditional SMT.</li>
</ul></li>
<li>Experiments
<ol type="1">
<li>Data Preperation
<ul>
<li>Experiments were conducted on a Chinese-to-English translation task.</li>
<li>Two training sets were used: one with 5 million sentence pairs and another with 11 million.</li>
<li>The development set consisted of 4491 sentences.</li>
<li>Test sets included NIST MT06, MT08 news, and MT08 web.</li>
<li>Full vocabulary sizes were 300k and 500k, with coverage embedding vector sizes of 100.</li>
<li>AdaDelta was used to update model parameters with a mini-batch size of 80.</li>
<li>The output vocabulary was a subset of the full vocabulary, including the top 2k most frequent words and the top 10 candidates from word-to-word/phrase translation tables.</li>
<li>The maximum length of a source phrase was 4.</li>
<li>A traditional SMT system, a hybrid syntax-based tree-to-string model, was used as a baseline.</li>
<li>Four different settings for coverage embedding models were tested: updating with a GRU (UGRU), updating as a subtraction (USub), the combination of both methods (UGRU+USub), and UGRU+USub with an additional objective (+Obj).</li>
</ul></li>
<li>Translation Results
<ul>
<li>The coverage embedding models improved translation quality significantly over a large vocabulary NMT (LVNMT) baseline system.</li>
<li>UGRU+USub performed best, achieving a 2.6 point improvement over LVNMT.</li>
<li>Improvements of coverage models over LVNMT were statistically significant.</li>
<li>The UGRU model also improved performance when using a larger training set of 11 million sentences.</li>
</ul></li>
<li>Alignment Results
<ul>
<li>The best coverage model (UGRU + USub) improved the F1 score by 2.2 points over the baseline NMT system.</li>
<li>Coverage embedding models reduce the number of repeated phrases in the output.</li>
</ul></li>
</ol></li>
<li>Conclusion
<ul>
<li>The paper proposed coverage embedding models for attention-based NMT.</li>
<li>The models use a coverage embedding vector for each source word and update these vectors as the translation progresses.</li>
<li>Experiments showed significant improvements over a strong large vocabulary NMT system.</li>
</ul></li>
</ol>
</section>
<section id="reflection" class="level2">
<h2 class="anchored" data-anchor-id="reflection">Reflection</h2>
<p>The idea of tracking coverage is very simple. Lets face it many issues in NLP require simple solutions.</p>
<p>For instance in the summarization task we have a big headache with the autoregressive tendency to repeat. But it also requires a kind of coverage too, but one that is more spread out. Also in more extreme cases we want to direct the coverage using very specific information like the narative flow of a story. This seems to be an idea that can be further explored in other tasks.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-mi-etal-2016-coverage" class="csl-entry">
Mi, Haitao, Baskaran Sankaran, Zhiguo Wang, and Abe Ittycheriah. 2016. <span>“Coverage Embedding Models for Neural Machine Translation.”</span> In <em>Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</em>, edited by Jian Su, Kevin Duh, and Xavier Carreras, 955–60. Austin, Texas: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/D16-1096">https://doi.org/10.18653/v1/D16-1096</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Coverage {Embedding} {Models} for {Neural} {Machine}
    {Translation}},
  date = {2025-02-12},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/2016-coverage-embedding-models-for-NMT/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Coverage Embedding Models for Neural Machine
Translation.”</span> February 12, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/2016-coverage-embedding-models-for-NMT/">https://orenbochman.github.io/notes-nlp/reviews/paper/2016-coverage-embedding-models-for-NMT/</a>.
</div></div></section></div> ]]></description>
  <category>Paper</category>
  <category>Review</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/2016-coverage-embedding-models-for-NMT/</guid>
  <pubDate>Tue, 11 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Summarization Task</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/posts/2025-02-10-summerizer/</link>
  <description><![CDATA[ 





<section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p>In the Deeplearning.ai NLP specialization, we implemented both <strong>Q&amp;A</strong> and <strong>Summarization tasks</strong>. However, we only looked at these tasks in terms of the attention models. It is now time to dive deeper into the summarization task. In this article I will review a talk covering a body of research on summarization, which has many ideas about features. Consider some of these and see what aspects are relevant to a modern task implementation.</p>
<p>Some of the tasks here can be done with a simple LSTM or Transformer. However it seems that RL may be useful if we want to plan the summary in a non-linear fashion or to make use of a dynamic programming approach. For instance to identify the narrative structure might require many more steps then we typically use in a summerization task. For a speeches we may want to capture a gist of the rhetoric.</p>
</section>
<section id="ideas" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="ideas">Ideas</h2>
<p>Several ideas surfaced while working on the assignment for building a hybrid generative/abstractive summarizer based on GPT2<sup>1</sup>. Many ideas came from prior work developing search engines. I had noticed that some issues were anathema to summarization from its inception.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;a Generative Pre-training Transformer or a decoder transformer</p></div></div><section id="coverage-ranking-content-by-importance" class="level3">
<h3 class="anchored" data-anchor-id="coverage-ranking-content-by-importance">Coverage ranking content by importance</h3>
<p>The first issue is <strong>coverage</strong>, which is how much of the original document to include. Coverage is a much more difficult problem than it appears.</p>
<p>Consider summerizing a long novel a single paragraph summary may provide a good idea of the main plot points of the story. On the other hand summerising a similar sized encyclopedia might require listing all the top level headings and subheadings i.e.&nbsp;a one paragraph summary would not be able to provide a good idea of the content of the document. On the other hand it should be much easier to generate a summary for the encyclopedia than for the novel as the encyclopedia has a highly structured document with clear headings and subheadings and writing conventions that provide cues to the reader about the importance of the content. Summerizing a novel is more challanging as we would actualy want to ommit alsmost all the details and so deciding which parts to keep is a major challange with a very sparse signal.</p>
<p>For example, technical documents, like patents, headings, academic writing cues, and even IR statistics, may serve as features that we may feed into a regression that can guide us in discarding the chuff from the grain. On the other hand, for movie scripts or novels, we can’t use all that, and we need to decide what is essential based on the narrative structure, which requires sophisticated processing and extensive domain knowledge to extract. So, When we want to create a synopsis of a book like War and Peace, specific details are part of the narrative structure that stands out as essential to us, and I can say that even in 2025, LLM is not good at picking these out of a large document. For attentive readers with a suitable background, if we double the length of the text, they might pick additional plot points and then less significant subplots. If again we double we would, they would include more details concerning characters, their motivations, etc.</p>
<p>To conclude, different documents can have radically different structures and cues. These suggest different strategies for selecting and ranking the material to be included in a summary of a given length. More so, when we consider summaries of different lengths, one can see that we are talking about a hierarchal structure.</p>
</section>
</section>
<section id="avoiding-repetition" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="avoiding-repetition">Avoiding repetition</h2>
<p>A second challenge that is pervasive is avoiding <strong>repetition</strong>. In the generative settings we have a tougher challenge since sentences that are generated may well be equivalent to sentences we have already generated, and we want the model to redirect it attention from material already covered. Luckily we have learned to detect similar sentences in the Q&amp;A task<sup>2</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;using Siamese networks for oneshot similarity detection.</p></div></div><p>check if it is similar to any sentence in the summary</p>
<p>Alg1: similarity detection</p>
<pre><code>summary = []
tokenized_doc = tokenize(doc)
embeddings_doc= embed(tokenized_doc)
while len(summary) &lt; doc_tokens * summary_ratio: 
    a = gen_a_sentence(embeddings_doc,summary)
    for s in summary:
        if sim(a,s) &gt; threshold:
            continue
    else:
        summary.append(a)
s -&gt; gen a sentence,</code></pre>
</section>
<section id="context-window-constraints" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="context-window-constraints">Context window constraints</h2>
<p>The third challenge is technical and is related to the <strong>length of the context window</strong>. as transformer models have a limit on the number of tokens that they can process in a context window. A second issue here is that the summary itself needs to also fit in the context window. If the structure is linear and incremental we can chunk it into many smaller pieced say using sections. However in reality we are often dealing with trees of graphs structures and chunking can erode the model’s ability to inspect said hierarchy of the structure it is tasked with summarizing. In Q&amp;A or IR tasks since we can process each chunk separately and then combine the results.</p>
<p>If we are not using a generative model one imagines a tree data structure that can efficiently track what what part of the document has been summarized. If we can use that to work with the attention mechanism we may be able to reduce the effective window size as we progress with the summary. A more nuanced idea would come from <strong>bayesian search</strong> where we may want to keep moving the probability mass for the attention mechanism to regions that are significant but have not been covered.</p>
<p>Another challenge is that we may want to grow our summary in a non-linear fashion. We may consider the main narrative structure then add in the subplots and then the character motivations. This suggests two approaches - a top down <strong>planning</strong> based approach<sup>3</sup> and a bottom up approach where we start with the most important details, then add in the less important ones. In the second case we may want to revise the initial sentences to efficiently incorporate more details as we progress.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;using rl or dynamic programming</p></div><div id="fn4"><p><sup>4</sup>&nbsp;reformer layers</p></div></div><p>I also interviewed with a company that told me they often worked with patents and that these documents were frequently in excess of a hundred pages and they were having lots of problems with the summarization task. This suggest that an efficient transformer<sup>4</sup> would be needed if we are to support context windows that can span hundreds of pages. But that besides the context window we need some good way to ensure that the summary is balanced and that it is not too repetitive.</p>
</section>
<section id="compassion" class="level2">
<h2 class="anchored" data-anchor-id="compassion">Compassion</h2>
<p>My experience with summarization by hand is that most texts can be compressed by 5-10% without losing any information simply by rephrasing with more concise language. So another idea that should be obvious is that the generated summary should be <strong>compressive</strong>, i.e., once we generate a good sentence we should consider if we can make it shorter. This should be fairly easy as we are reducing the length of a single sentence. We may however be able to do even better by considering a section and trying to see if we can merge two sentences or if two sentences have partial overlap that can be merged or referenced via an anaphora. This is another area where we diverge from Q&amp;A, where we do not have a length constraint and may often want to include all relevant information. Implementation of this idea cam be easily embodied in a loss function that penalizes summaries for each token or character. This same idea can also be used in a RL summarizer.</p>
</section>
<section id="grounding" class="level2">
<h2 class="anchored" data-anchor-id="grounding">Grounding</h2>
<p>Generative summarization is a case where we can require that all generated text be grounded in the original document. In reality generative models have many glitches, due to tokenization issues, or failures of the attention mechanism, or out of distribution generation, due to bias learned from the corpus, due to negative recency bias and and due to emergent contradictions (where a contradiction is introduced into the context window and cannot be removed.) And even if we can avoid all these there is still nothing in the model that makes it prefer truthy generations. So it seems essential that the generated text is grounded in the original document. This seems to be verifiable by visually inspecting the using the attention mechanism. In reality there may be multiple heads and multiple layers. This means we need to access the attention mechanism programmatically and store annotations from the source document for each generated token. We may also want to make abstracting summarization explicit. c.f. [Extrinsic Hallucinations in LLMs in ](https://lilianweng.github.io/posts/2024-07-07-hallucination/) by Lilian Weng</p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Summarization {Task}},
  date = {2025-02-10},
  url = {https://orenbochman.github.io/notes-nlp/posts/2025-02-10-summerizer/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Summarization Task.”</span> February 10,
2025. <a href="https://orenbochman.github.io/notes-nlp/posts/2025-02-10-summerizer/">https://orenbochman.github.io/notes-nlp/posts/2025-02-10-summerizer/</a>.
</div></div></section></div> ]]></description>
  <category>NLP</category>
  <category>Notes</category>
  <category>Literature review</category>
  <category>Summarization task</category>
  <category>Relevance</category>
  <guid>https://orenbochman.github.io/notes-nlp/posts/2025-02-10-summerizer/</guid>
  <pubDate>Sun, 09 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Morphological Word Embeddings</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/2019-morphological-embeddings/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/literature-review-open-book.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Litrature review"><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="img-fluid figure-img" alt="Litrature review"></a></p>
<figcaption>Litrature review</figcaption>
</figure>
</div></div><p>This is a mentioned in the tokenization lab in course four week 3</p>
<p>Time permitting I will try and dive deeper into this paper.</p>
<section id="podcast" class="level2">
<h2 class="anchored" data-anchor-id="podcast">Podcast</h2>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>Linguistic similarity is multi-faceted. For instance, two words may be similar with respect to semantics, syntax, or morphology inter alia. Continuous word-embeddings have been shown to capture most of these shades of similarity to some degree. This work considers guiding word-embeddings with morphologically annotated data, a form of semisupervised learning, encouraging the vectors to encode a word’s morphology, i.e., words close in the embedded space share morphological features. We extend the log-bilinear model to this end and show that indeed our learned embeddings achieve this, using German as a case study. – <span class="citation" data-cites="cotterell2019morphological">(Cotterell and Schütze 2019)</span></p>
</blockquote>
<!-- TODO: add review and podcast for this paper-->
<!-- TODO: link here from LSH lessons-->
</section>
<section id="terminology" class="level2">
<h2 class="anchored" data-anchor-id="terminology">Terminology</h2>
<p>Here are some terms and concepts discussed in the sources, with explanations:</p>
<dl>
<dt>Word embeddings</dt>
<dd>
These are vector representations of words in a continuous space, where similar words are located close to each other in the vector space. The goal is to capture linguistic similarity, but the definition of “similarity” can vary (semantic, syntactic, morphological). Word embeddings are typically trained to produce representations that capture linguistic similarity.
</dd>
<dt>Log-bilinear model (LBL)</dt>
<dd>
This is a language model that learns features along with weights, as opposed to using hand-crafted features. It uses context to predict the next word, and word embeddings fall out as low dimensional representations of the context. The LBL is a generalization of the log-linear model.
</dd>
<dt>Morphology</dt>
<dd>
The study of word structure, including how words are formed from morphemes (the smallest units of meaning).
</dd>
<dt>Morphological tags</dt>
<dd>
These are annotations that describe the morphological properties of a word, such as case, gender, number, tense, etc.. They can be very detailed for morphologically rich languages. For example, in German, a word might be tagged to indicate its part of speech, case, gender, and number.
</dd>
<dt>Morphologically rich languages</dt>
<dd>
Languages with a high morpheme-per-word ratio, where word-internal structure is important for processing. German and Czech are cited as examples.
</dd>
<dt>Morphologically impoverished languages</dt>
<dd>
Languages with a low morpheme-per-word ratio, such as English.
</dd>
<dt>Multi-task objective</dt>
<dd>
Training a model to perform multiple tasks simultaneously. In this case, the model is trained to predict both the next word and its morphological tag.
</dd>
<dt>Semi-supervised learning</dt>
<dd>
Training a model on a partially annotated corpus, using both labeled and unlabeled data. This approach is useful when large amounts of unannotated text are available.
</dd>
<dt>Contextual signature</dt>
<dd>
The words surrounding a given word. The context in which a word appears can be used to determine the word’s meaning and morphological properties.
</dd>
<dt>Hamming distance</dt>
<dd>
A measure of the difference between two binary strings, calculated as the number of positions at which the corresponding bits are different. In this context, it is used to compare morphological tags, which are represented as bit vectors.
</dd>
<dt>MORPHOSIM</dt>
<dd>
A metric for evaluating morphologically-driven embeddings that measures how morphologically similar a word is to its nearest neighbors in the embedding space. It is calculated as the average Hamming distance between morphological tags of a word and its neighbors. Lower values are better, as they indicate that the nearest neighbors of each word are closer morphologically.
</dd>
</dl>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li>Introduction
<ul>
<li>Presents the importance of capturing word morphology, especially for morphologically-rich languages.</li>
<li>Highlights the multifaceted nature of linguistic similarity (semantics, syntax, morphology).</li>
<li>Discusses the goal of the paper: to develop word embeddings that specifically encode morphological relationships.</li>
</ul></li>
<li>Related Work
<ul>
<li>Discusses previous integration of morphology into language models, including factored language models and neural network-based approaches.</li>
<li>Notes the role of morphology in computational morphology, particularly in morphological tagging for inflectionally-rich languages.</li>
<li>Highlights the significance of distributional similarity in morphological analysis.</li>
</ul></li>
<li>Log-Bilinear Model
<ul>
<li>Describes the log-bilinear model (LBL), a generalization of the log-linear model with learned features.</li>
<li>Presents the LBL’s energy function and probability distribution in the context of language modeling.</li>
</ul></li>
<li>Morph-LBL
<ul>
<li>Proposes a multi-task objective that jointly predicts the next word and its morphological tag.</li>
<li>Describes the model’s joint probability distribution, incorporating morphological tag features.</li>
<li>Discusses the use of semi-supervised learning, allowing training on partially annotated corpora</li>
</ul></li>
<li>Evaluation
<ul>
<li>Mentions the qualitative evaluation using t-SNE, showing clusters reflecting morphological and POS relationships.</li>
<li>Introduces MorphoSim, a novel quantitative metric to assess the extent to which similar embeddings are morphologically related.</li>
</ul></li>
<li>Experiments and Results
<ul>
<li>Presents experiments on the German TIGER corpus, comparing Morph-LBL with the original LBL and Word2Vec.</li>
<li>Describes two experiments:
<ul>
<li>Experiment 1: Evaluates the morphological information encoded in embeddings using a k-NN classifier.</li>
<li>Experiment 2: Compares models using the MorphoSim metric to measure morphological similarity among nearest neighbors.</li>
</ul></li>
<li>Discusses the results, highlighting Morph-LBL’s superior performance in capturing morphological relationships, even without observing all tags during training.</li>
</ul></li>
<li>Conclusion and Future Work
<ul>
<li>Summarizes the contributions of the paper: introducing Morph-LBL for inducing morphologically guided embeddings.</li>
<li>Notes the model’s success in leveraging distributional signatures to capture morphology.</li>
<li>Discusses future work on integrating orthographic features for further improvement.</li>
<li>Mentions potential applications in morphological tagging and other NLP tasks.</li>
</ul></li>
</ul>
</section>
<section id="log-bilinear-model" class="level2">
<h2 class="anchored" data-anchor-id="log-bilinear-model">Log-Bilinear Model</h2>
<p><img src="https://latex.codecogs.com/png.latex?p(w%5Cmid%20h)%20=%0A%5Cfrac%7B%5Cexp%5Cleft(s_%5Ctheta(w,h)%5Cright)%7D%7B%5Csum_%7Bw'%7D%0A%5Cexp%5Cleft(s_%5Ctheta(w',h)%5Cright)%7D%20%5Cqquad%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?w"> is a word, <img src="https://latex.codecogs.com/png.latex?h"> is a history and <img src="https://latex.codecogs.com/png.latex?s_%5Ctheta"> is an energy function. Following the notation of , in the LBL we define <img src="https://latex.codecogs.com/png.latex?%0As_%5Ctheta(w,h)%20=%20%5Cleft(%5Csum_%7Bi=1%7D%5E%7Bn-1%7D%20C_i%0Ar_%7Bh_i%7D%5Cright)%5ET%20q_w%20+%20b_w%20%5Cqquad%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?n-1"> is the history length</p>
</section>
<section id="morph-lbl" class="level2">
<h2 class="anchored" data-anchor-id="morph-lbl">Morph-LBL</h2>
<p><img src="https://latex.codecogs.com/png.latex?%0A%20%20p(w,%20t%20%5Cmid%20h)%20%5Cpropto%20%5Cexp((%20f_t%5ET%20S%20%20+%20%5Csum_%7Bi=1%7D%5E%7Bn-1%7DC_i%20r_%7Bh_i%7D)%5ET%20q_w%20+%20b_%7Bw%7D%20)%20%5Cqquad%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?f_t"> is a hand-crafted feature vector for a morphological tag <img src="https://latex.codecogs.com/png.latex?t"> and <img src="https://latex.codecogs.com/png.latex?S"> is an additional weight matrix.</p>
<p>Upon inspection, we see that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap(t%20%5Cmid%20w,h)%20%5Cpropto%20%5Cexp(S%5ET%20f_t%20q_w)%20%5Cqquad%0A"></p>
<p>Hence given a fixed embedding <img src="https://latex.codecogs.com/png.latex?q_w"> for word <img src="https://latex.codecogs.com/png.latex?w">, we can interpret <img src="https://latex.codecogs.com/png.latex?S"> as the weights of a conditional log-linear model used to predict the tag <img src="https://latex.codecogs.com/png.latex?t">.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<ul>
<li><a href="http://www.eva.mpg.de/lingua/resources/glossing-rules.php">Leipzig Glossing Rules</a> which provides a standard way to explain morphological features by examples</li>
<li><a href="https://www.youtube.com/watch?v=y9sVFrmGu0w&amp;ab_channel=GrahamNeubig">CMU Multilingual NLP 2020 (17): Morphological Analysis and Inflection</a> <!--
- [nb-lm](https://notebooklm.google.com/notebook/f594ff01-19f2-49b0-a0ac-84176fb22667?_gl=1*1rba7bx*_ga*MzAyOTc3ODMwLjE3Mzg1MDQ5Njc.*_ga_W0LDH41ZCB*MTczODkyMzY5Ni41LjAuMTczODkyMzY5Ni42MC4wLjA.)
--></li>
<li><span class="citation" data-cites="kann-etal-2016-neural">Kann, Cotterell, and Schütze (2016)</span> <a href="https://github.com/ryancotterell/neural-canonical-segmentation">code</a></li>
<li><a href="https://unimorph.github.io/">unimorph</a> univorsal morphological database</li>
</ul>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-cotterell2019morphological" class="csl-entry">
Cotterell, Ryan, and Hinrich Schütze. 2019. <span>“Morphological Word Embeddings.”</span> <em>arXiv Preprint arXiv:1907.02423</em>.
</div>
<div id="ref-kann-etal-2016-neural" class="csl-entry">
Kann, Katharina, Ryan Cotterell, and Hinrich Schütze. 2016. <span>“Neural Morphological Analysis: Encoding-Decoding Canonical Segments.”</span> In <em>Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</em>, edited by Jian Su, Kevin Duh, and Xavier Carreras, 961–67. Austin, Texas: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/D16-1097">https://doi.org/10.18653/v1/D16-1097</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Morphological {Word} {Embeddings}},
  date = {2025-02-07},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/2019-morphological-embeddings/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Morphological Word Embeddings.”</span>
February 7, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/2019-morphological-embeddings/">https://orenbochman.github.io/notes-nlp/reviews/paper/2019-morphological-embeddings/</a>.
</div></div></section></div> ]]></description>
  <category>Paper</category>
  <category>Review</category>
  <category>Podcast</category>
  <category>NLP</category>
  <category>Morphology</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/2019-morphological-embeddings/</guid>
  <pubDate>Thu, 06 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Practical and Optimal LSH for Angular Distance</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/2015-LSH/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/literature-review-open-book.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Literature review"><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="img-fluid figure-img" alt="Literature review"></a></p>
<figcaption>Literature review</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/t_8SpFV0l7A" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Locality-Sensitive Hashing and Beyond
</figcaption>
</figure>
</div><div id="vid-02" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/Dkomk2wPaoc" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;2: Beyond Locality Sensitive Hashing; Alexandr Andoni
</figcaption>
</figure>
</div></div>

<p>In the NLP specialization we have covered and used LSH a number of times in at least two courses. <!-- TODO: link to the notes so as to make them more useful as this is a stub!!!! --> In one sense I have an understanding of what is going on when we implement this. In another sense this seems to be quite confusing. So I don’t fully understand some aspects of LSH.</p>
<p>One way to do better is to try and explain it to someone else. This is what I am trying to do here by going back to the source and trying to understand the paper, problems, motivation and finally isolate what it is that is hard to grasp.</p>
<p>This paper is a good introduction to LSH for the angular distance.</p>
<p>In <span class="citation" data-cites="andoni2015practical">(Andoni et al. 2015)</span> the authors delve into Locality-Sensitive Hashing (LSH) for the angular distance. They show the existence of an LSH family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal running time exponent. Unlike earlier algorithms with this property (e.g., Spherical LSH), their algorithm is also practical, improving upon the well-studied hyperplane LSH in practice.</p>
<p>For now the two videos which explain in some depth this an the related paper - <a href="https://arxiv.org/abs/1501.01062">Optimal Data-Dependent Hashing for Approximate Near Neighbors</a> will have to do.</p>
<p>Time permitting I will try and dive deeper into this paper.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR - Location Sensitive Hashing
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/in_a_nutshell.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Location Sensitive Hashing in a Nutshell"><img src="https://orenbochman.github.io/notes-nlp/images/in_a_nutshell.jpg" class="img-fluid figure-img" alt="Location Sensitive Hashing in a Nutshell"></a></p>
<figcaption>Location Sensitive Hashing in a Nutshell</figcaption>
</figure>
</div>
<ul>
<li>The paper introduces a Locality-Sensitive Hashing (LSH) family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal running time exponent.</li>
<li>The algorithm is also practical, improving upon the well-studied hyperplane LSH in practice.</li>
<li>The authors also introduce a multiprobe version of this algorithm, and conduct experimental evaluation on real and synthetic data sets.</li>
</ul>
</div>
</div>
<section id="podcast" class="level2">
<h2 class="anchored" data-anchor-id="podcast">Podcast</h2>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>We show the existence of a Locality-Sensitive Hashing (LSH) family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal running time exponent. Unlike earlier algorithms with this property (e.g., Spherical LSH [1 , 2]), our algorithm is also practical, improving upon the well-studied hyperplane LSH [3] in practice.</p>
<p>We also introduce a <strong>multiprobe</strong> version of this algorithm, and conduct experimental evaluation on real and synthetic data sets.</p>
<p>We complement the above positive results with a fine-grained lower bound for the quality of any LSH family for angular distance. Our lower bound implies that the above LSH family exhibits a trade-off between evaluation time and quality that is close to optimal for a natural class of LSH functions. – <span class="citation" data-cites="andoni2015practical">(Andoni et al. 2015)</span></p>
</blockquote>
<!-- TODO: add outline, reflection, and terminology and for this paper-->
<!-- TODO: link here from LSH lessons-->
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-andoni2015practical" class="csl-entry">
Andoni, Alexandr, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn, and Ludwig Schmidt. 2015. <span>“Practical and Optimal LSH for Angular Distance.”</span> <em>Advances in Neural Information Processing Systems</em> 28.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Practical and {Optimal} {LSH} for {Angular} {Distance}},
  date = {2025-02-05},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/2015-LSH/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Practical and Optimal LSH for Angular
Distance.”</span> February 5, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/2015-LSH/">https://orenbochman.github.io/notes-nlp/reviews/paper/2015-LSH/</a>.
</div></div></section></div> ]]></description>
  <category>Paper</category>
  <category>Review</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/2015-LSH/</guid>
  <pubDate>Tue, 04 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Finetuning Pretrained Transformers into RNNs</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/transforer-to-rnn/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./cover.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Litrature review"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/transforer-to-rnn/cover.jpg" class="img-fluid figure-img" alt="Litrature review"></a></p>
<figcaption>Litrature review</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/JMeYGYANEqU" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Transformer to RNN (T2RNN) Part-1 by Dr.&nbsp;Niraj Kumar
</figcaption>
</figure>
</div><div id="vid-02" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/UHgy2faOD_M" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;2: Transformer to RNN (T2RNN) Part-2 by Dr.&nbsp;Niraj Kumar
</figcaption>
</figure>
</div></div>

<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>Transformers have outperformed recurrent neural networks (RNNs) in natural language generation. But this comes with a significant computational cost, as the attention mechanism’s complexity scales quadratically with sequence length. Efficient transformer variants have received increasing interest in recent works. Among them, a linear-complexity recurrent variant has proven well suited for autoregressive generation. It approximates the softmax attention with randomized or heuristic feature maps, but can be difficult to train and may yield suboptimal accuracy. This work aims to convert a pretrained transformer into its efficient recurrent counterpart, improving efficiency while maintaining accuracy. Specifically, we propose a swap-then-finetune procedure: in an off-the-shelf pretrained transformer, we replace the softmax attention with its linear-complexity recurrent alternative and then finetune. With a learned feature map, our approach provides an improved tradeoff between efficiency and accuracy over the standard transformer and other recurrent variants. We also show that the finetuning process has lower training cost relative to training these recurrent variants from scratch. As many models for natural language tasks are increasingly dependent on large-scale pretrained transformers, this work presents a viable approach to improving inference efficiency without repeating the expensive pretraining process. – <span class="citation" data-cites="kasai2021finetuningpretrainedtransformersrnns">(Kasai et al. 2021)</span></p>
</blockquote>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-kasai2021finetuningpretrainedtransformersrnns" class="csl-entry">
Kasai, Jungo, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, and Noah A. Smith. 2021. <span>“Finetuning Pretrained Transformers into RNNs.”</span> <a href="https://arxiv.org/abs/2103.13076">https://arxiv.org/abs/2103.13076</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Finetuning {Pretrained} {Transformers} into {RNNs}},
  date = {2025-02-03},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/transforer-to-rnn/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Finetuning Pretrained Transformers into
RNNs.”</span> February 3, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/transforer-to-rnn/">https://orenbochman.github.io/notes-nlp/reviews/paper/transforer-to-rnn/</a>.
</div></div></section></div> ]]></description>
  <category>Paper</category>
  <category>Review</category>
  <category>LSTM</category>
  <category>RNN</category>
  <category>Transformer</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/transforer-to-rnn/</guid>
  <pubDate>Sun, 02 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>xLSTM: Extended Long Short-Term Memory</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="cover.jpg" class="lightbox" data-gallery="slides" title="Literature Review"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/cover.jpg" class="img-fluid figure-img" alt="Literature Review"></a></p>
<figcaption>Literature Review</figcaption>
</figure>
</div><div id="vid-02" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/0aWGTNS03PU?t=3" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Review by AI Bites
</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/0OaEv1a5jUM" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;2: Review by Yannic Kilcher
</figcaption>
</figure>
</div><div id="vid-03" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/8u2pW2zZLCs" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;3: LSTM: The Comeback Story? Talk with Sepp Hochreiter after the xLSTM paper
</figcaption>
</figure>
</div></div>


<blockquote class="blockquote">
<p>Whate’er the theme, the Maiden sang<br>
<mark>As if her song could have no ending;</mark><br>
I saw her singing at her work,<br>
And o’er the sickle bending;—<br>
I listened, motionless and still;<br>
And, as I mounted up the hill,<br>
<mark>The music in my heart I bore,<br>
Long after it was heard no more.</mark> &nbsp;</p>
<p>– The Solitary Reaper by William Wordsworth.</p>
</blockquote>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR – Scaling LSTMs to Billions of Parameters with <strong>xLSTM</strong>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/in_a_nutshell.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="LSTMs in a nutshell"><img src="https://orenbochman.github.io/notes-nlp/images/in_a_nutshell.jpg" class="img-fluid figure-img" alt="LSTMs in a nutshell"></a></p>
<figcaption>LSTMs in a nutshell</figcaption>
</figure>
</div>
<ul>
<li>Just when we think that the transformer rules supreme the RNN makes a comeback with a refreshing new look at the LSTMs.</li>
<li>This paper brings new architectures to the LSTM that are fully parallelizable and can scale to billions of parameters.</li>
<li>They demonstrate on synthetic tasks and language modeling benchmarks that these new xLSTMs can outperform state-of-the-art Transformers and State Space Models.</li>
</ul>
</div>
</div>
</div>
<section id="introduction" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Motivation
</div>
</div>
<div class="callout-body-container callout-body">
<p>So this is my first review of the RNN papers. I found that since the courses covering LSTM seemed to be over simplified and confusing, I should go back to the source. Since time is limited, it helps to review the papers in reverse chronological order. Each new papers explains the limitations that are generally not apparent when the paper is first published. The authors of the later papers also have the benefit of hindsight which they share in the obligatory literature review wherein they provide lots of context we cannot get by reading the original papers. Given all that there is also a benefit to reading the original paper - the real innovators while harder to understand are remarkable thinkers and often reveal motivations and deep insights that elude later interlocutors.</p>
</div>
</div>
<section id="sec-podcast" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-podcast">Podcast &amp; Other Reviews</h3>
<p>This paper is a bit heavy on the technical side. But if we are still here, we may enjoy this podcast reviewing the paper. Though AI generated I found it quite useful to get oriented with the paper and understand that the experiments the authors run to validate this new architectures.</p>
<p>We also cover something I found absent from the Course on sequence models . The limitations of LSTMs as viewed by the authors when considered in the context of transformers and Mamba.</p>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
<p>Although this paper is recent there are a number of other people who cover it.</p>
<ul>
<li>In Video&nbsp;2 Yannic Kilcher covers the paper in his youtube channel. He does a great job of explaining the paper in a way that is easy to understand. Kilcher is all too human, he makes many mistakes, stresses opinions over facts. If we don’t mind all that he is a great resource for understanding the paper. I find he rubs me the wrong way but at least I find this motivating to do better in my own reviews!</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
What can we expect from xLSTM?
</div>
</div>
<div class="callout-body-container callout-body">

<blockquote class="blockquote">
<p>xLSTM has the potential to considerably impact other fields like Reinforcement Learning, Time Series Prediction, or the modeling of physical systems. – <span class="citation" data-cites="beck2024xlstm">(Beck et al. 2024)</span></p>
</blockquote>
<p>In his book Understanding Media <span class="citation" data-cites="mcluhan1988understanding">(McLuhan 1988)</span> Marshal McLuhan introduced his <strong>Tetrad</strong>. <mark>The <strong>Tetrad</strong> is a mental model for understanding how a technological innovation like the xLSTM might disrupt society</mark>. To work the model we ask the following four questions that can be applied to any new technology to understand its impact. The four questions in this instance are:</p>
<ol type="1">
<li>What does the xLSTM enhance or amplify?</li>
</ol>
<blockquote class="blockquote">
<p>The xLSTM enhances the LSTM by introducing exponential gating and novel memory structures, boosting its capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models.</p>
</blockquote>
<ol start="2" type="1">
<li>What does the xLSTM make obsolete or replace?</li>
</ol>
<blockquote class="blockquote">
<p>The xLSTM makes the traditional LSTM obsolete by addressing its limitations, such as the inability to revise storage decisions, limited storage capacities, and lack of parallelizability.</p>
</blockquote>
<ol start="3" type="1">
<li>What does the xLSTM retrieve that was previously obsolesced?</li>
</ol>
<blockquote class="blockquote">
<p>The xLSTM retrieves the LSTM’s relevance in large-scale language modeling by scaling it to billions of parameters and integrating modern LLM techniques while mitigating known limitations. 4. What does the xLSTM reverse or flip into when pushed to its limits?</p>
</blockquote>
<blockquote class="blockquote">
<p>The xLSTM, when pushed to its limits, can compete with current Large Language Models based on Transformer technology, offering a competitive alternative for sequence modeling tasks.</p>
</blockquote>
<p>The xLSTM paper by <span class="citation" data-cites="beck2024xlstm">(Beck et al. 2024)</span> is a great example of the tetrad in action. The authors take the LSTM, a technology that has stood the test of time, and enhance it with exponential gating and novel memory structures. They address the limitations of traditional LSTMs and push the technology to new limits, creating a new architecture that can compete with state-of-the-art Transformers and State Space Models. Recent work by <span class="citation" data-cites="chen2024computationallimitsstatespacemodels">(Chen et al. 2024)</span> suggest that Transformers and The State Space Models are actually limited in their own ways.</p>
</div>
</div>
<div class="no-row-height column-margin column-container"><div id="sup-01" class="quarto-float quarto-figure quarto-figure-center anchored callout-margin-content">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="MediaTetrad.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Tetrad"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/MediaTetrad.svg" class="img-fluid figure-img"></a></p>
<figcaption>Tetrad</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div></section>
</section>
<section id="abstract" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining:</p>
<ol type="1">
<li>sLSTM with a scalar memory, a scalar update, and new memory mixing,<br>
</li>
<li>mLSTM that is fully parallelizable with a matrix memory and a covariance update rule.<br>
</li>
</ol>
<p>Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling. — <span class="citation" data-cites="beck2024xlstm">(Beck et al. 2024)</span></p>
</blockquote>

<div class="no-row-height column-margin column-container"><div id="fig-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="fig01.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="architecture"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/fig01.png" class="img-fluid figure-img"></a></p>
<figcaption>architecture</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: The extended LSTM (xLSTM) family. From left to right:<br>
1. The original LSTM memory cell with constant error carousel and gating.<br>
2. New <strong>sLSTM</strong> and <strong>mLSTM</strong> memory cells that introduce exponential gating. sLSTM offers a new memory mixing technique. <strong>mLSTM</strong> is fully parallelizable with a novel matrix memory cell state and new covariance update rule.<br>
3. mLSTM and sLSTM in residual blocks yield xLSTM blocks.<br>
4. Stacked xLSTM blocks give an xLSTM
</figcaption>
</figure>
</div><div id="fig-02" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="fig02.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="blocks"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/fig02.png" class="img-fluid figure-img"></a></p>
<figcaption>blocks</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: LSTM limitations.<br>
- Left: Nearest Neighbor Search problem in terms of mean squared error (MSE). Given a reference vector, a sequence is scanned sequentially for the most similar vector with the objective to return its attached value at sequence end. LSTM struggles to revise a stored value when a more similar vector is found. Our new xLSTM overcomes this limitation by exponential gating. - Right: Rare Token Prediction. The perplexity (PPL) of token prediction on Wikitext-103, in partitions of token frequency. LSTM performs worse on predicting rare tokens because of its limited storage capacities, whereas our new xLSTM solves this problem via a matrix memory.
</figcaption>
</figure>
</div><div id="fig-03" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="fig03.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="blocks"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/fig03.png" class="img-fluid figure-img"></a></p>
<figcaption>blocks</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: xLSTM blocks.<br>
Left: A residual sLSTM block with post up-projection (like Transformers): The input is fed into an sLSTM — with an optional convolution — followed by a gated MLP.<br>
Right: A residual mLSTM block with pre up-projection (like State Space models): mLSTM is wrapped inside two MLPs, via a convolution, a learnable skip connection, and an output gate that acts componentwise.
</figcaption>
</figure>
</div><div id="fig-04" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-04-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="fig04.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="blocks"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/fig04.png" class="img-fluid figure-img"></a></p>
<figcaption>blocks</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-04-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: xLSTM blocks.<br>
Left: A residual sLSTM block with post up-projection (like Transformers): The input is fed into an sLSTM — with an optional convolution — followed by a gated MLP.<br>
Right: A residual mLSTM block with pre up-projection (like State Space models): mLSTM is wrapped inside two MLPs, via a convolution, a learnable skip connection, and an output gate that acts componentwise.
</figcaption>
</figure>
</div><div id="fig-05" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-05-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="fig05.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="blocks"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/fig05.png" class="img-fluid figure-img"></a></p>
<figcaption>blocks</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-05-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: xLSTM blocks.<br>
Left: A residual sLSTM block with post up-projection (like Transformers): The input is fed into an sLSTM — with an optional convolution — followed by a gated MLP.<br>
Right: A residual mLSTM block with pre up-projection (like State Space models): mLSTM is wrapped inside two MLPs, via a convolution, a learnable skip connection, and an output gate that acts componentwise.
</figcaption>
</figure>
</div></div>



</section>
<section id="sec-outline" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-outline">Paper Outline</h2>
<ul>
<li>Introduction
<ul>
<li>Describes the constant error carousel and gating mechanisms of Long Short-Term Memory (LSTM). &gt; “In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories…”</li>
<li>Discusses three main limitations of LSTMs:
<ol type="1">
<li>The inability to revise storage decisions.</li>
<li>Limited storage capacities.</li>
<li>Lack of parallelizability due to memory mixing.</li>
</ol></li>
<li>Highlights <mark>the emergence of Transformers in language modeling due to these limitations</mark>. &gt; Transformer Era: The rise of Transformers with parallelizable self-attention has largely overshadowed LSTMs due to their superior scalability.</li>
</ul></li>
<li>Extended Long Short-Term Memory
<ul>
<li>Introduces Extended Long Short-Term Memory (xLSTM), which incorporates exponential gating and novel memory structures.</li>
<li>Presents two new LSTM variants:
<ol type="1">
<li>sLSTM with a scalar memory, a scalar update, and new memory mixing.</li>
<li>mLSTM with a matrix memory and a covariance update rule, which is fully parallelizable.</li>
</ol></li>
<li>Describes the integration of these LSTM variants into residual block modules, resulting in xLSTM blocks.</li>
<li>Presents xLSTM architectures that residually stack xLSTM blocks.</li>
</ul></li>
<li>Related Work:
<ul>
<li>Mentions various linear attention methods to overcome the quadratic complexity of Transformer attention.</li>
<li>Notes the popularity of State Space Models (SSMs) for language modeling.</li>
<li>Highlights the use of Recurrent Neural Networks (RNNs) as a replacement for Transformers.</li>
<li>Mentions the use of <strong>gating</strong> in recent RNN and SSM approaches.</li>
<li>Notes the use of <strong>covariance update rules</strong><sup>1</sup> to enhance storage capacities in various models.</li>
</ul></li>
<li>Experiments
<ul>
<li>Presents the experimental evaluation of xLSTM compared to existing methods with a focus on language modeling.</li>
<li>Discusses the effectiveness of xLSTM on synthetic tasks, including:
<ul>
<li>Formal languages.</li>
<li>Multi-Query Associative Recall.</li>
<li>Long Range Arena tasks.</li>
</ul></li>
<li>Presents a method comparison and ablation study, where different models are trained on 15 billion tokens from the SlimPajama dataset.</li>
<li>Assesses the scaling behavior of different methods based on validation perplexity.</li>
<li>Conducts a large-scale language modeling experiment:
<ul>
<li>Training different model sizes on 300B tokens from SlimPajama.</li>
<li>Evaluating models on length extrapolation.</li>
<li>Assessing models on validation perplexity and performance on downstream tasks.</li>
<li>Evaluating models on 571 text domains of the PALOMA language benchmark dataset.</li>
<li>Assessing the scaling behavior with increased training data.</li>
</ul></li>
</ul></li>
<li>Limitations
<ul>
<li>Highlights limitations of xLSTM, including:
<ul>
<li>Lack of parallelizability for sLSTM due to memory mixing.</li>
<li>Unoptimized CUDA kernels for mLSTM.</li>
<li>High computational complexity of mLSTM’s matrix memory.</li>
<li>Memory overload for longer contexts due to the independence of mLSTM’s memory from sequence length.</li>
</ul></li>
</ul></li>
<li>Conclusion
<ul>
<li>Concludes that xLSTM performs favorably in language modeling compared to Transformers and State Space Models.</li>
<li>Suggests that larger xLSTM models will be competitive with current Large Language Models based on Transformer technology.</li>
<li>Notes the potential impact of xLSTM on other deep learning fields such as reinforcement learning, time series prediction, and physical system modeling.</li>
</ul></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;explain covariance</p></div></div></section>
<section id="briefing-xlstm---extended-long-short-term-memory" class="level2">
<h2 class="anchored" data-anchor-id="briefing-xlstm---extended-long-short-term-memory">Briefing : xLSTM - Extended Long Short-Term Memory</h2>
<section id="introduction-and-motivation" class="level3">
<h3 class="anchored" data-anchor-id="introduction-and-motivation">Introduction and Motivation:</h3>
<p><strong>LSTM’s Legacy</strong>: The Long Short-Term Memory (LSTM), introduced in the 1990s, has been a cornerstone of deep learning, especially in sequence modeling and early Large Language Models (LLMs). LSTMs are known for their “constant error carousel” and gating mechanisms, designed to combat vanishing gradients in recurrent neural networks.</p>
<blockquote class="blockquote">
<p>“In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories…”</p>
</blockquote>
<p>Transformer Era: The rise of Transformers with parallelizable self-attention has largely overshadowed LSTMs due to their superior scalability.</p>
<blockquote class="blockquote">
<p>“However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale.” <strong>xLSTM Question</strong>: This research aims to explore the capabilities of LSTMs when scaled to billions of parameters, by integrating modern LLM techniques while addressing known limitations.</p>
</blockquote>
<blockquote class="blockquote">
<p>“We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs?”</p>
</blockquote>
</section>
<section id="key-limitations-of-traditional-lstms" class="level3">
<h3 class="anchored" data-anchor-id="key-limitations-of-traditional-lstms">Key Limitations of Traditional LSTMs:</h3>
<p>The paper identifies three major limitations of traditional LSTMs:</p>
<ol type="1">
<li>Inability to Revise Storage Decisions: LSTMs struggle to update previously stored information when more relevant information arrives later in the sequence. The paper uses a “Nearest Neighbor Search” task to demonstrate this.</li>
</ol>
<blockquote class="blockquote">
<p>“LSTM struggles to revise a stored value when a more similar vector is found…”</p>
</blockquote>
<ol start="2" type="1">
<li>Limited Storage Capacity: LSTMs store information in scalar cell states, which can lead to information compression and loss. This impacts performance on tasks requiring detailed memory, like predicting rare tokens.</li>
</ol>
<blockquote class="blockquote">
<p>“LSTM performs worse on predicting rare tokens because of its limited storage capacities…”</p>
</blockquote>
<ol start="3" type="1">
<li>Lack of Parallelizability: LSTMs process data sequentially due to the recurrent connections between hidden states, hindering parallel computation.</li>
</ol>
<blockquote class="blockquote">
<p>“Lack of parallelizability due to memory mixing, i.e., the hidden-hidden connections between hidden states from one time step to the next, which enforce sequential processing.”</p>
</blockquote>
<ol start="3" type="1">
<li>xLSTM Innovations:</li>
</ol>
<p>The paper introduces Extended Long Short-Term Memory (xLSTM), designed to address the above limitations, by modifying the original LSTM using two main approaches:</p>
<p><strong>Exponential Gating</strong>:Implemented to allow for better revision of storage decisions. Normalization and stabilization techniques are used to avoid overflows in calculations due to the exponential function.</p>
<blockquote class="blockquote">
<p>“To empower LSTMs with the ability to revise storage decisions, we introduce exponential gates (red) together with normalization and stabilization.” Novel Memory Structures: Two new LSTM variants are created with these structures:</p>
</blockquote>
<p><strong>sLSTM</strong>: A scalar memory LSTM with a scalar update and new memory mixing technique (within heads, not across). Features multiple memory cells and multiple heads, with memory mixing within each head.</p>
<blockquote class="blockquote">
<p>“The new sLSTM (see Section 2.2) with a scalar memory, a scalar update, and memory mixing…”</p>
</blockquote>
<p><strong>mLSTM</strong>: A matrix memory LSTM, fully parallelizable, with a covariance update rule for storing key-value pairs. Does not have memory mixing between cells or heads.</p>
<blockquote class="blockquote">
<p>“the new mLSTM (see Section 2.3) with a matrix memory and a covariance (outer product) update rule, which is fully parallelizable.”</p>
</blockquote>
<ol start="4" type="1">
<li>sLSTM Details:</li>
</ol>
<ul>
<li><strong>Exponential Gates</strong>: Introduces exponential activation functions for input and forget gates with normalization and a stabilizer state to prevent overflow.</li>
<li><strong>Normalizer State</strong>: A state used to normalize the hidden state output by accumulating a weighted sum of input and forget gates.</li>
<li><strong>Memory Mixing</strong>: sLSTM allows memory mixing between cells, and also introduces a new form of memory mixing through multiple heads, mixing between cells within a head, but not across heads.</li>
</ul>
<blockquote class="blockquote">
<p>“The new sLSTM can have multiple heads with memory mixing within each head but not across heads. The introduction of heads for sLSTM together with exponential gating establishes a new way of memory mixing.”</p>
</blockquote>
<ol start="5" type="1">
<li>mLSTM Details:</li>
</ol>
<p><strong>Matrix Memory</strong>: Replaces the scalar cell state with a matrix, increasing storage capacity.</p>
<p><strong>Key-Value Storage</strong>: The mLSTM memory is based around a key-value storage system inspired by Bidirectional Associative Memories (BAMs) using covariance updates, where keys and values are updated via an outer product.</p>
<blockquote class="blockquote">
<p>“At time <img src="https://latex.codecogs.com/png.latex?t">, we want to store a pair of vectors, the key <img src="https://latex.codecogs.com/png.latex?k_t%20%E2%88%88%20R%5Ed"> and the value <img src="https://latex.codecogs.com/png.latex?v_t%20%E2%88%88%20R%5Ed">… The covariance update rule… for storing a key-value pair…”</p>
</blockquote>
<p>Parallelization: mLSTM is fully parallelizable due to abandoning hidden-hidden recurrent connections and memory mixing. Computations can be performed efficiently on GPUs using standard matrix operations.</p>
<blockquote class="blockquote">
<p>“…the mLSTM… which is fully parallelizable.”</p>
</blockquote>
<ol start="6" type="1">
<li>xLSTM Architecture:</li>
</ol>
<p><strong>xLSTM Blocks</strong>: The sLSTM and mLSTM variants are integrated into residual blocks.</p>
<p><strong>sLSTM blocks</strong> use post up-projection (like Transformers).</p>
<p><strong>mLSTM blocks</strong> use pre up-projection (like State Space Models).</p>
<blockquote class="blockquote">
<p>“Integrating these new LSTM variants into residual block modules results in xLSTM blocks…”</p>
</blockquote>
<p><strong>Residual Stacking</strong>: xLSTM architectures are created by residually stacking these blocks, using pre-LayerNorm backbones similar to current LLMs.</p>
<blockquote class="blockquote">
<p>“An xLSTM architecture is constructed by residually stacking build-ing blocks…” <strong>Cover’s Theorem</strong>: Used to justify mapping information into a higher-dimensional space for better separation of histories or contexts.</p>
</blockquote>
<blockquote class="blockquote">
<p>“We resort to Cover’s Theorem (Cover, 1965), which states that in a higher dimensional space non-linearly embedded patterns can more likely be linearly separated than in the original space.”</p>
</blockquote>
<ol start="7" type="1">
<li>Performance and Scaling:</li>
</ol>
<p><strong>Linear Computation &amp; Constant Memory</strong>: Unlike Transformers, xLSTM networks have linear computation and constant memory complexity with respect to sequence length.</p>
<blockquote class="blockquote">
<p>“Contrary to Transformers, xLSTM networks have a linear computation and a constant memory complexity with respect to the sequence length.”</p>
</blockquote>
<p><strong>Synthetic Tasks</strong>: xLSTM showed improved performance over regular LSTMs on “Nearest Neighbor Search” and “Rare Token Prediction” synthetic tasks.</p>
<p><strong>SlimPajama Experiments</strong>: The xLSTM models were trained on 15B and 300B tokens from the SlimPajama dataset for language modelling evaluation.</p>
<p><strong>Competitive Performance</strong>: xLSTMs demonstrate competitive performance against state-of-the-art Transformers and State Space Models on language modelling benchmarks, both in terms of next token prediction perplexity and downstream tasks.</p>
<blockquote class="blockquote">
<p>“Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.”</p>
</blockquote>
<p>Ablation studies show importance of gating techniques.</p>
<ol start="8" type="1">
<li>Memory &amp; Speed:</li>
</ol>
<p><strong>sLSTM</strong>: Not parallelizable due to memory mixing, although the authors have created a fast CUDA implementation (less than twice as slow as mLSTM).</p>
<blockquote class="blockquote">
<p>“While mLSTM is parallelizable analog to FlashAttention… sLSTM is not parallelizable due to the memory mixing…”</p>
</blockquote>
<p><strong>mLSTM</strong>: Parallelizable, but currently slower than FlashAttention or Mamba due to non-optimized CUDA kernels. Matrix memory is computationally complex but does not require parameters and can be parallelized.</p>
<blockquote class="blockquote">
<p>“The memory of mLSTM does not require parameters but is computationally expensive through its d×d matrix memory and d× d update… the computations can be done in parallel on GPUs, therefore these computations have only a minor effect on the wall clock time.”</p>
</blockquote>
<ol start="9" type="1">
<li>Limitations:</li>
</ol>
<p><strong>sLSTM Parallelization</strong>: sLSTM’s memory mixing is non-parallelizable.</p>
<blockquote class="blockquote">
<p>“(i) In contrast to mLSTM, memory mixing of the sLSTM prohibits parallelizable operations, and therefore does not allow a fast parallel implementation.”</p>
</blockquote>
<p>mLSTM CUDA Optimization: The mLSTM CUDA kernels are not fully optimized.</p>
<blockquote class="blockquote">
<p>“(ii) The CUDA kernels for mLSTM are not optimized, and therefore the current implementation is about 4 times slower than FlashAttention or the scan used in Mamba.”</p>
</blockquote>
<p><strong>mLSTM Matrix Memory</strong>: High computational complexity for mLSTM due to matrix memory operations.</p>
<p><strong>Forget Gate Initialization</strong>: Careful initialization of the forget gates is needed.</p>
<p><strong>Long Context Memory</strong>: The matrix memory is independent of sequence length, and might overload memory for long context sizes.</p>
<blockquote class="blockquote">
<p>“(v) Since the matrix memory is independent of the sequence length, increasing the sequence length might overload the memory for longer context sizes.”</p>
</blockquote>
<p><strong>Hyperparameter Optimization</strong>: xLSTM is not yet fully optimized due to computational load, indicating further performance gains are possible.</p>
<blockquote class="blockquote">
<p>“(vi) Due to the expensive computational load for large language experiments, we did neither fully optimize the architecture nor the hyperparameters, especially for larger xLSTM architectures.”</p>
</blockquote>
<ol start="10" type="1">
<li>Related Work:</li>
</ol>
<p>The paper highlights connections of its ideas with the following areas:</p>
<p><strong>Gating</strong>: Many recent models such as HGRN, GLA, Gated State Space models, Mamba, and others also utilize gating mechanisms. Covariance Update Rule: The covariance update rule used in mLSTM is related to other methods such as Fast Weight Programmers, Retention, and Linear Transformers.</p>
<ol start="11" type="1">
<li>Conclusion:</li>
</ol>
<p>The xLSTM presents a compelling alternative to Transformers in sequence modeling, especially in contexts where memory efficiency and longer sequences are required. By combining insights from the original LSTM with exponential gating and novel memory structures, the paper shows that LSTMs can be competitive at scale. While limitations exist in terms of parallelization and hyperparameter tuning, the research shows promising potential for further development. The authors posit further research and improvements could allow xLSTM to reach its full potential.</p>
</section>
</section>
<section id="my-thoughts" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="my-thoughts">My Thoughts</h2>

<div class="no-row-height column-margin column-container"><div id="vid-sigmoid" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-sigmoid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/TPqr8t919YM" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-sigmoid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;4: Review of the sigmoid function
</figcaption>
</figure>
</div></div><div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Research questions
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li>How does the constant error carousel mitigate the vanishing gradient problem in the LSTM?
<ul>
<li>The constant error carousel is a way to keep the error constant over time. This helps to prevent the vanishing gradient problem in the LSTM.</li>
</ul></li>
<li>How are the gates in the original LSTM binary?
<ul>
<li>Sigmoid, saturation and a threshold at 0.5</li>
</ul></li>
<li>What is the long term memory in the LSTM?
<ul>
<li>the cell state <img src="https://latex.codecogs.com/png.latex?c_%7Bt-1%7D"></li>
</ul></li>
<li>What is the short term memory in the LSTM?
<ul>
<li>the hidden state <img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D"></li>
</ul></li>
<li>How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs</li>
</ol>
</div>
</div>

<div class="no-row-height column-margin column-container"><div id="sup-02" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="sigmoid-limits.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Supplementary Figure&nbsp;2: limits of the sigmoid function"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/sigmoid-limits.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup" id="sup-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;2: limits of the sigmoid function
</figcaption>
</figure>
</div><div id="sup-03" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="sigmoid-values.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="bias"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/sigmoid-values.png" class="img-fluid figure-img"></a></p>
<figcaption>bias</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup" id="sup-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;3: The inductive bias of the sigmoid decision function.
</figcaption>
</figure>
</div></div>
<section id="binary-nature-of-non-exponential-gating-mechanisms" class="level3">
<h3 class="anchored" data-anchor-id="binary-nature-of-non-exponential-gating-mechanisms">Binary nature of non exponential Gating Mechanisms</h3>
<p>If we try to understand why are the gating mechanisms in the original LSTM is described here as binary?</p>
<p>It helps to the properties of the sigmoid functions that are explained in Video&nbsp;4.</p>
<ol type="1">
<li><p>Supplementary Figure&nbsp;1 shows that The sigmoid function has a domain of <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D"> and a range of <img src="https://latex.codecogs.com/png.latex?(0,1)">. This means that the sigmoid function can only output values between 0 and 1.</p></li>
<li><p>It has a very sharp gradient around 0 if the bias is set to 0. This means that the sigmoid function acts as a binary function around 0.</p></li>
<li><p>The Sigmoid function tends to saturate. Now sigmoid function around 0 isn’t quite quite binary but outside of (-6,+6) it becomes saturated and acts very much as a binary function.</p></li>
<li><p>If we use it as a loss function or a decision function, the sigmoid function is used with a threshold of 0.5. This means that if the output of the sigmoid function is greater than 0.5, the decision is 1, and if it is less than 0.5, the decision is 0. That is how the sigmoid function can become binary.</p></li>
<li><p>Note that we can use thresholding during inference to make the decision binary and we can keep them as continuous values during training so we can have a smooth gradient for back-propagation.</p></li>
<li><p>Even without thresholding the sigmoid function can become binary this is due to the saturation of the sigmoid function. I recall that Hinton in his Coursera course on Neural Networks and Deep Learning mentioned that activation function that become saturated are problematic. I can say that at last I think I understand what he meant:<br>
I see that that sigmoid functions can fall off the manifold of the data and this can be a problem. This is why the ReLU function is used in the Gated Recurrent Unit (GRU) and the Transformer. <strong>Falling off the manifold of the data</strong> means that the sigmoid function becomes saturated (above 6 and below -6). At this point the back-propagation steps are no longer able to change the weight or bias since the gradient is effectively so learning stops.</p></li>
</ol>
<p>This becomes a is a problem in two senses. The sigmoid unit is likely to be stuck in a setting that only representative of part of the training data. The rest of the network which is not stuck now has to learn to compensate for this.</p>
<p>This issues may well be the root cause LSTM limitation number 1 - the inability to revise storage decisions. It seems that the authors decided to use the exponential gating function to address this issues.</p>
<p>This is why the gating mechanisms in the original LSTM are binary. The sigmoid function is used to control the flow of information in the LSTM by deciding which information to keep and which to discard. The sigmoid function is used to decide how much of the new input information should be added to the cell state and how much of the old cell state should be retained. This binary nature of the sigmoid function is what allows the LSTM to control the flow of information and decide what to remember and what to forget.</p>
</section>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li><p><a href="https://openreview.net/forum?id=ARAxPPIAhq&amp;noteId=gra7vHnb0q">The paper on open review</a> has some additional insights from the authors</p></li>
<li><p><a href="https://www.ai-bites.net/xlstm-extended-long-short-term-memory-networks/">XLSTM — Extended Long Short-Term Memory Networks</a> By Shrinivasan Sankar — May 20, 2024</p></li>
</ul>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-beck2024xlstm" class="csl-entry">
Beck, Maximilian, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, Günter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. 2024. <span>“xLSTM: Extended Long Short-Term Memory.”</span> <em>arXiv Preprint arXiv:2405.04517</em>.
</div>
<div id="ref-chen2024computationallimitsstatespacemodels" class="csl-entry">
Chen, Yifang, Xiaoyu Li, Yingyu Liang, Zhenmei Shi, and Zhao Song. 2024. <span>“The Computational Limits of State-Space Models and Mamba via the Lens of Circuit Complexity.”</span> <a href="https://arxiv.org/abs/2412.06148">https://arxiv.org/abs/2412.06148</a>.
</div>
<div id="ref-mcluhan1988understanding" class="csl-entry">
McLuhan, Marshall. 1988. <em>Understanding Media : The Extensions of Man</em>. New York: New American Library. <a href="http://www.worldcat.org/search?qt=worldcat_org_all&amp;q=0451624963">http://www.worldcat.org/search?qt=worldcat_org_all&amp;q=0451624963</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {xLSTM: {Extended} {Long} {Short-Term} {Memory}},
  date = {2025-02-03},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“xLSTM: Extended Long Short-Term
Memory.”</span> February 3, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/">https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/</a>.
</div></div></section></div> ]]></description>
  <category>Paper</category>
  <category>Review</category>
  <category>NLP</category>
  <category>LSTM</category>
  <category>Seq2Seqs</category>
  <category>RNN</category>
  <category>Podcast</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/</guid>
  <pubDate>Sun, 02 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>The Secret Life of Pronouns What Our Words Say About Us</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/book/seret-life-of-pronouns/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="cover.jpg" class="lightbox" data-gallery="slides" title="cover"><img src="https://orenbochman.github.io/notes-nlp/reviews/book/seret-life-of-pronouns/cover.jpg" class="img-fluid figure-img" alt="cover"></a></p>
<figcaption>cover</figcaption>
</figure>
</div></div><div class="page-columns page-full"><blockquote class="blockquote">
<p>To grunt and sweat under a weary life, But that the dread of something after death, <mark>The undiscovered country from whose bourn No traveler returns, puzzles the will, And makes us rather bear those ills we have, Than fly to others that we know not of</mark>?<sup>1</sup> — Hamlet Act 3, Scene 1 by William Shakespeare.</p>
</blockquote><div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Pronouns oft fall prey to the stop word filter, yet they hold the keys to unlocking the depth of intimate meaning</p></div></div></div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR – ML, NLP and the secret life of pronouns
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/in_a_nutshell.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Pronouns in a nutshell"><img src="https://orenbochman.github.io/notes-nlp/images/in_a_nutshell.jpg" class="img-fluid figure-img" alt="Pronouns in a nutshell"></a></p>
<figcaption>Pronouns in a nutshell</figcaption>
</figure>
</div>
<ul>
<li>When I got started with NLP I was coding search engines and conventional wisdom was <mark>pronouns don’t pass the <strong>stop word filter</strong></mark></li>
<li>One of the gems I learned on that job was that <mark>everything in the corpus can be provide invaluable information if we only know how to index it</mark>.</li>
<li>After reading this book and I started to unlocked the power of the pronouns.</li>
<li>At Hungarian School I discovered how pronouns combine with case ending to create a vast vistas of untapped NLP resource.</li>
<li>Cognitive AI I noted that pronouns and particles are key in extending the primitive verb system via thematic roles to get a very rich semantic systems in the lexicon with little costs in learning.</li>
</ul>
</div>
</div>
</div>
<section id="podcast" class="level2">
<h2 class="anchored" data-anchor-id="podcast">Podcast</h2>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The secret life of pronouns in context
</div>
</div>
<div class="callout-body-container callout-body">
<p>I got interested in this book when I was deep into wikipedia research. I was in a place where I wanted to be able to attribute content to the authors. I had read some disturbing articles with prior work on the topic by some researchers from my Alma Mater. However, I eventually came across this book and it seemed to put the ideas in a better form. Less about ML and more about the type of writers and their personalities.</p>
</div>
</div>
<p>In <span class="citation" data-cites="pennebaker2013secret">(Pennebaker 2013)</span> “The Secret Life of Pronouns: What Our Words Say About Us,” the author explores how the seemingly innocuous function words we use—pronouns, articles, prepositions, and auxiliary verbs—can reveal a great deal about our personalities, emotions, social connections, and psychological states. James W. Pennebaker, the author and a social psychologist, uses computer-based text analysis methods to uncover these hidden patterns.</p>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li><p><strong>Key Questions and Themes:</strong></p>
<ul>
<li><strong>Can language reveal psychological states?</strong> The author is primarily interested in what people’s words say about their psychological states. The book investigates whether language features can reveal how people are thinking and whether this information can be used to change their thinking.</li>
<li><strong>How do function words differ from content words?</strong> The book distinguishes between content and style of speech, arguing that function words, which reflect language style, reveal more about a person than content words. Function words are tied to the relationship between speaker and listener.</li>
<li><strong>Do men and women use words differently?</strong> The book explores gender differences in language use, noting that women are typically more attentive to the thoughts and feelings of others, while men are less so. It also investigates whether different writing styles reflect different personalities.</li>
<li><strong>Can language predict behavior?</strong> The book examines whether language use can predict behaviors such as success in college, risk of suicide, leadership styles, and likelihood of declaring war.</li>
<li><strong>How can language be used as a tool for change?</strong> The author examines whether analyzing words can lead to improvements in relationships, teaching, and leadership. Additionally, the book looks at how writing about emotional upheavals can improve mental and physical health, and whether using certain types of words can influence the effectiveness of that writing.</li>
<li><strong>Can language reveal deception?</strong> The book explores linguistic markers of deception, such as the use of first-person singular pronouns and the mention of other individuals.</li>
<li><strong>Can language analysis help identify authors?</strong> The book presents methods for identifying authors using function words, punctuation, and obscure words.</li>
</ul></li>
<li><p><strong>Main Examples and Studies:</strong></p>
<ul>
<li><strong>Expressive Writing:</strong> Early research focused on the benefits of writing about traumatic experiences, leading to the development of the LIWC (Linguistic Inquiry and Word Count) program. This program helps analyze word categories associated with healthy writing such as positive emotion words, cognitive words, and the ability to construct a story.</li>
<li><strong>The Bottle and the Two People Pictures:</strong> Participants described pictures, and their descriptions were analyzed for function word use and content. For example, people who focused on shadows in the bottle picture tended to be more creative. The second picture revealed common themes such as social connections, clothing, and accessories.</li>
<li><strong>Thinking Styles:</strong> The book describes how people’s function words can be statistically grouped into categories reflecting thinking styles, such as formal, analytic, and narrative. These thinking styles can predict how people relate to others and organize their worlds.</li>
<li><strong>9/11 Blog Analysis:</strong> The language used in blog posts was analyzed before and after the 9/11 attacks. Researchers noted an increase in negative emotion words, a decrease in positive emotion words, and a temporary increase in cognitive words.</li>
<li><strong>College Admissions Essays:</strong> The study examined whether the writing style in college admissions essays could predict college grades.</li>
<li><strong>The Federalist Papers:</strong> The book discusses the use of function words to determine the authorship of the anonymous Federalist Papers.</li>
<li><strong>Language Style Matching (LSM):</strong> LSM is a technique to determine how similarly two people use function words. It is used in studies of couples, speed dating, and even the writings of historical figures to analyze the relationship between the authors.</li>
<li><strong>Obama’s Pronoun Use</strong>: The book analyzes Barack Obama’s use of first-person singular pronouns. His relatively low use of I-words is interpreted as a sign of self-confidence.</li>
</ul></li>
<li><p><strong>Additional Insights:</strong></p>
<ul>
<li><strong>Stealth Words:</strong> The book emphasizes the importance of function words, often called “stealth words,” which are often overlooked.</li>
<li><strong>The Role of Computers:</strong> Computer technology has revolutionized the study of language, allowing researchers to analyze large datasets and uncover hidden patterns that would be impossible to see with manual analysis.</li>
<li><strong>Language as a Tool:</strong> Language can be used not only as a window into the inner workings of people, but also as a tool for guiding thinking and promoting change.</li>
<li><strong>Interdisciplinary Approach</strong>: The author’s work draws on multiple disciplines including psychology, linguistics, computer science, history, and political science.</li>
</ul></li>
</ul>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-pennebaker2013secret" class="csl-entry">
Pennebaker, J. W. 2013. <em>The Secret Life of Pronouns: What Our Words Say about Us</em>. Bloomsbury USA. <a href="https://books.google.co.il/books?id=p9KmCAAAQBAJ">https://books.google.co.il/books?id=p9KmCAAAQBAJ</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {The {Secret} {Life} of {Pronouns} {What} {Our} {Words} {Say}
    {About} {Us}},
  date = {2025-01-30},
  url = {https://orenbochman.github.io/notes-nlp/reviews/book/seret-life-of-pronouns/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“The Secret Life of Pronouns What Our Words
Say About Us.”</span> January 30, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/book/seret-life-of-pronouns/">https://orenbochman.github.io/notes-nlp/reviews/book/seret-life-of-pronouns/</a>.
</div></div></section></div> ]]></description>
  <category>Review</category>
  <category>Book</category>
  <category>NLP</category>
  <category>Sentiment Analysis</category>
  <category>Sentiment Analysis</category>
  <category>Podcast</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/book/seret-life-of-pronouns/</guid>
  <pubDate>Wed, 29 Jan 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>When and Why are Pre-trained Word Embeddings Useful for Neural Machine Translation?</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/20180-PTWM-NMT/</link>
  <description><![CDATA[ 





<div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/o41avjKIg5o" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Talk covering this paper by Roee Aharoni
</figcaption>
</figure>
</div>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/literature-review-open-book.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Litrature review"><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="img-fluid figure-img" alt="Litrature review"></a></p>
<figcaption>Litrature review</figcaption>
</figure>
</div></div><div id="sup-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://roeeaharoni.com/NMT_slides_orginal.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Roee Aharoni’s slides"><embed src="https://roeeaharoni.com/NMT_slides_orginal.pdf" class="img-fluid"></a></p>
<figcaption>Roee Aharoni’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div>
<section id="podcast" class="level2">
<h2 class="anchored" data-anchor-id="podcast">Podcast</h2>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>The performance of Neural Machine Translation (NMT) systems often suffers in lowresource scenarios where sufficiently largescale parallel corpora cannot be obtained. Pretrained word embeddings have proven to be invaluable for improving performance in natural language analysis tasks, which often suffer from paucity of data. However, their utility for NMT has not been extensively explored. In this work, we perform five sets of experiments that analyze when we can expect pre-trained word embeddings to help in NMT tasks. We show that such embeddings can be surprisingly effective in some cases – providing gains of up to 20 BLEU points in the most favorable setting.. –<span class="citation" data-cites="qi-etal-2018-pre">(Qi et al. 2018)</span></p>
</blockquote>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li>**Introduction
<ul>
<li>Describes the problem of low-resource scenarios in Neural Machine Translation (NMT) and the potential utility of pre-trained word embeddings.</li>
<li>Highlights the success of pre-trained embeddings in natural language analysis tasks and the lack of extensive exploration in NMT.</li>
<li>Poses five researcb questions:
<ul>
<li>Q1 Is the behavior of pre-training affected by language families and other linguistic features of source and target languages? (§3)</li>
<li>Q2 Do pre-trained embeddings help more when the size of the training data is small? (§4)</li>
<li>Q3 How much does the similarity of the source and target languages affect the efficacy of using pre-trained embeddings? (§5)</li>
<li>Q4 Is it helpful to align the embedding spaces between the source and target languages? (§6)</li>
<li>Q5 Do pre-trained embeddings help more in multilingual systems as compared to bilingual systems? (§7)</li>
</ul></li>
</ul></li>
<li>Experimental Setup
<ul>
<li>Details the five sets of experiments conducted to evaluate the effectiveness of pre-trained word embeddings in NMT.</li>
<li>Describes the datasets used, including the WMT14 English-German and English-French translation tasks.</li>
<li>Outlines the models and training procedures employed in the experiments.</li>
</ul></li>
<li>Results and Analysis
<ul>
<li>Presents the results of the experiments, showing the impact of pre-trained word embeddings on NMT performance.</li>
<li>Discusses the observed gains in BLEU scores and the factors influencing the effectiveness of pre-trained embeddings.</li>
<li>Analyzes the relationship between the quality of pre-trained embeddings and the performance of NMT systems.</li>
</ul></li>
<li>Analysis
<ul>
<li>Considers the implications of the findings for NMT research and practice.</li>
<li>Discusses the potential benefits and limitations of using pre-trained word embeddings in NMT tasks.</li>
</ul></li>
<li>Conclusion
<ul>
<li>The sweet-spot is wgere there is very little training data yet enough to train the system.</li>
<li>PTWE are more effective if there are more similar translation pairs.</li>
<li>A priori alignment of embeddings may not be necessary in bilingual scenarios, but is helpful in multi-lingual training scenarios.</li>
</ul></li>
</ul>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-qi-etal-2018-pre" class="csl-entry">
Qi, Ye, Devendra Sachan, Matthieu Felix, Sarguna Padmanabhan, and Graham Neubig. 2018. <span>“When and Why Are Pre-Trained Word Embeddings Useful for Neural Machine Translation?”</span> In <em>Proceedings of the 2018 Conference of the North <span>A</span>merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</em>, edited by Marilyn Walker, Heng Ji, and Amanda Stent, 529–35. New Orleans, Louisiana: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/N18-2084">https://doi.org/10.18653/v1/N18-2084</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {When and {Why} Are {Pre-trained} {Word} {Embeddings} {Useful}
    for {Neural} {Machine} {Translation?}},
  date = {2024-02-11},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/20180-PTWM-NMT/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“When and Why Are Pre-Trained Word Embeddings
Useful for Neural Machine Translation?”</span> February 11, 2024. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/20180-PTWM-NMT/">https://orenbochman.github.io/notes-nlp/reviews/paper/20180-PTWM-NMT/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Bidirectional LSTM</category>
  <category>Deep learning</category>
  <category>Embeddings</category>
  <category>LLM</category>
  <category>NLP</category>
  <category>Paper</category>
  <category>Review</category>
  <category>Podcast</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/20180-PTWM-NMT/</guid>
  <pubDate>Sat, 10 Feb 2024 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Neural Machine Translation by Jointly Learning to Align and Translate</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/2014-NMT-by-jointly-learning-to-align-and-translate/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/literature-review-open-book.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Litrature review"><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="img-fluid figure-img" alt="Litrature review"></a></p>
<figcaption>Litrature review</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/o41avjKIg5o" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Roee Aharoni’s Talk covering this paper (Hebrew)
</figcaption>
</figure>
</div><div id="sup-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://roeeaharoni.com/NMT_slides_orginal.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Roee Aharoni’s slides"><embed src="https://roeeaharoni.com/NMT_slides_orginal.pdf" class="img-fluid"></a></p>
<figcaption>Roee Aharoni’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div>

<section id="podcast" class="level2">
<h2 class="anchored" data-anchor-id="podcast">Podcast</h2>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
</section>
<section id="abstract" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<div class="page-columns page-full"><blockquote class="blockquote">
<p><em>Neural machine translation</em> is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of <em>encoder-decoders</em> and consists of an encoder that encodes a source sentence into a <em>fixed-length vector</em><sup>1</sup> from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition. –<span class="citation" data-cites="bahdanau2016neuralmachinetranslationjointly">(Bahdanau, Cho, and Bengio 2016)</span></p>
</blockquote><div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;the encoded state</p></div></div></div>
</section>
<section id="outline" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li>Introduction
<ul>
<li>Positions NMT as a new approach, contrasting it with traditional phrase-based systems.</li>
<li>Describes encoder-decoder models that use a fixed-length vector to encode a source sentence.</li>
<li>Explains that these models are <mark>trained to maximize the probability of a correct translation.</mark></li>
<li>Discusses the <mark>limitation of compressing all source sentence information into <em>a fixed-length vector</em><sup>2</sup>, especially for long sentences.</mark></li>
<li><mark>Introduces the extension of the encoder-decoder model that jointly learns to align and translate.</mark></li>
<li>Emphasizes that the model searches for relevant source positions when generating a target word.</li>
<li>States that the new model encodes the input sentence into a sequence of vectors and adaptively chooses a subset during decoding.</li>
<li>Asserts that the new model performs better with long sentences and that the proposed approach performs better translation compared to the basic encoder-decoder approach.</li>
<li>Notes that the improvement is apparent with longer sentences and that the model achieves comparable performance to a conventional phrase-based system.</li>
<li>Mentions that the model finds linguistically plausible alignments.</li>
</ul></li>
<li>Background: <em>Neural Machine Translation</em> (NMT)
<ul>
<li><mark>Defines translation as <em>maximizing</em> the <strong>conditional probability of a target sentence given a source sentence</strong>.</mark> <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Barg%7D%5C,%5Cmax_%7By%7D%5C,%20%7Bp%7D(y%20%5Cmid%20x)."></li>
<li>Explains that <mark>NMT models learn this conditional distribution using <em>parallel training corpora</em>.</mark></li>
<li>Describes NMT models as consisting of <em>an encoder and a decoder</em>.</li>
<li>Notes that <em>Recurrent Neural Networks</em> (RNNs) have been used for encoding and decoding <em>variable-length sentences</em>.</li>
<li>Points out that NMT has shown promising results, and can achieve state-of-the-art performance.</li>
<li>Notes that adding <em>neural networks</em> to existing systems can boost performance levels.</li>
<li>RNN Encoder-Decoder
<ul>
<li>Describes the <em>RNN Encoder-Decoder framework</em>, where an encoder reads the input sentence into a vector c.</li>
<li>Explains that the <em>encoder</em> uses an RNN to generate <em>a sequence of hidden states</em>, from which the vector c is generated.</li>
<li>Notes that the <em>decoder</em> predicts the next word given the context vector and previously predicted words.</li>
<li>Presents a formula for the conditional probability, which is modeled with an RNN.</li>
</ul></li>
</ul></li>
<li>Learning to Align and Translate
<ul>
<li>Introduces a new architecture for NMT using a bidirectional RNN encoder and a decoder that searches through the source sentence.</li>
<li>Decoder: General Description
<ul>
<li>Presents a new conditional probability conditioned on a distinct context vector for each target word.</li>
<li>Explains that the context vector depends on a sequence of annotations from the encoder.</li>
<li>Defines the context vector as a weighted sum of annotations.</li>
<li>Describes how the weights are computed, using an alignment model.</li>
<li>Emphasizes that the alignment model computes a soft alignment.</li>
<li>Explains the weighted sum of annotations as computing an expected annotation.</li>
<li>States that the probability of the annotation reflects its importance in deciding the next state and generating the target word.</li>
<li>Notes that this implements an attention mechanism in the decoder.</li>
</ul></li>
<li>Encoder: Bidirectional RNN for Annotating Sequences
<ul>
<li>Introduces the use of a bidirectional RNN (BiRNN) to summarize preceding and following words.</li>
<li>Describes the forward and backward RNNs that comprise the BiRNN.</li>
<li>Explains how annotations are created by concatenating forward and backward hidden states.</li>
<li>Notes that the annotation will focus on words around the current word due to the nature of RNNs.</li>
</ul></li>
</ul></li>
<li>Experiment Settings
<ul>
<li>States that the proposed approach is evaluated on English-to-French translation using ACL WMT ’14 bilingual corpora.</li>
<li>Compares the approach with an RNN Encoder-Decoder.</li>
<li>Dataset
<ul>
<li>Details the corpora used, their sizes, and the data selection method used.</li>
<li>Notes that no monolingual data other than the mentioned parallel corpora is used.</li>
<li>Describes how the development and test sets were created.</li>
<li>Details the tokenization and word shortlist used for training the models.</li>
</ul></li>
<li>Models
<ul>
<li>Details the two types of models trained, RNN Encoder-Decoder (RNNencdec) and RNNsearch, and the sentence lengths used.</li>
<li>Describes the hidden units in the encoder and decoder for both models.</li>
<li>Mentions the use of a multilayer network with a maxout hidden layer.</li>
<li>Describes the use of minibatch stochastic gradient descent (SGD) and Adadelta for training, along with the minibatch size and training time.</li>
<li>Explains the use of beam search to find the translation that maximizes the conditional probability.</li>
<li>Refers to the appendices for more details on the architectures and training procedure.</li>
</ul></li>
</ul></li>
<li>Results
<ul>
<li>Quantitative Results
<ul>
<li>Presents translation performance measured in BLEU scores, showing that RNNsearch outperforms RNNencdec in all cases.</li>
<li>Notes that the performance of RNNsearch is comparable to that of the phrase-based system, even without using a separate monolingual corpus.</li>
<li>Shows that RNNencdec’s performance decreases with longer sentences.</li>
<li>Demonstrates that RNNsearch is more robust to sentence length, with no performance drop even with sentences of length 50 or more.</li>
<li>Highlights the superiority of RNNsearch by noting that RNNsearch-30 outperforms RNNencdec-50.</li>
<li>Presents a table of BLEU scores for each model.</li>
</ul></li>
<li>Qualitative Analysis
<ul>
<li>Alignment
<ul>
<li>Explains that the approach offers an intuitive way to inspect the soft alignment between words.</li>
<li>Describes visualizing annotation weights to see which source positions were considered important.</li>
<li>Notes the largely monotonic alignment of words, with strong weights along the diagonal.</li>
<li>Highlights examples of non-trivial alignments and how the model correctly translates phrases.</li>
<li>Explains the strength of soft-alignment using an example of the phrase “the man” being translated to “l’ homme”.</li>
<li>Notes that soft alignment deals naturally with phrases of different lengths.</li>
</ul></li>
<li>Long Sentences
<ul>
<li>Explains that the model does not require encoding a long sentence into a fixed-length vector perfectly.</li>
<li>Provides examples of translations of long sentences, showing that RNNencdec deviates from the source meaning.</li>
<li>Demonstrates that RNNsearch translates long sentences correctly, preserving the whole meaning.</li>
<li>Confirms that RNNsearch enables more reliable translation of long sentences than RNNencdec.</li>
</ul></li>
</ul></li>
</ul></li>
<li>Related Work
<ul>
<li>Learning to Align
<ul>
<li>Mentions a similar alignment approach used in handwriting synthesis.</li>
<li>Notes the key difference: in handwriting synthesis, the modes of the weights of the annotations only move in one direction.</li>
<li>Explains that, in machine translation, reordering is often needed, and the proposed approach computes annotation weight of every source word for each target word.</li>
</ul></li>
<li>Neural Networks for Machine Translation
<ul>
<li>Discusses the history of neural networks in machine translation, from providing single features to existing systems to reranking candidate translations.</li>
<li>Mentions examples of neural networks being used as a sub-component of existing translation systems.</li>
<li>Highlights that the paper focuses on designing a completely new translation system based on neural networks.</li>
</ul></li>
</ul></li>
<li>Conclusion
<ul>
<li>States that the paper proposes a new architecture that extends the basic model by allowing the model to (soft-)search for input words when generating a target word.</li>
<li>Highlights the benefit of freeing the model from encoding the whole sentence into a fixed-length vector.</li>
<li>Notes that <strong>all components are jointly trained towards a better log-probability of producing correct translations</strong>.</li>
<li>Confirms that the proposed RNNsearch outperforms the conventional encoder-decoder model and is more robust to sentence length.</li>
<li>Concludes that the model aligns each target word with the relevant words in the source sentence.</li>
<li>Points out that the proposed approach achieved translation performance comparable to the phrase-based statistical machine translation, despite its recent development.</li>
<li>Suggests that the approach is a promising step toward better machine translation and a better understanding of natural languages.</li>
<li>Identifies better handling of unknown or rare words as a challenge for the future.</li>
</ul></li>
<li>Appendix A: Model Architecture
<ul>
<li>Architectural Choices
<ul>
<li>Describes that the scheme is a general framework where the activation functions of RNNs and the alignment model can be defined.</li>
<li>Recurrent Neural Network
<ul>
<li>Explains the use of the <em>gated hidden unit</em> for the activation function, which is similar to LSTM units.</li>
<li>Provides details and equations for the computation of the RNN state using gated hidden units.</li>
<li>Explains how update and reset gates are computed.</li>
<li>Mentions the use of a multi-layered function with a single hidden layer of maxout units for computing the output probability.</li>
</ul></li>
</ul></li>
<li>Alignment Model
<ul>
<li>Explains the use of a single-layer multilayer perceptron for the alignment model.</li>
<li>Provides an equation describing the model and notes that some values can be pre-computed.</li>
</ul></li>
<li>Detailed Description of the Model
<ul>
<li>Encoder
<ul>
<li>Provides the equations and architecture details of the bidirectional RNN encoder.</li>
</ul></li>
<li>Decoder
<ul>
<li>Provides the equations and architecture details of the decoder with the attention mechanism.</li>
</ul></li>
<li>Model Size
<ul>
<li>Specifies the sizes of hidden layers, word embeddings, and maxout hidden layer.</li>
</ul></li>
</ul></li>
</ul></li>
<li>Appendix B: Training Procedure
<ul>
<li>Parameter Initialization
<ul>
<li>Describes the initialization of various weight matrices, including the use of random orthogonal matrices and Gaussian distributions.</li>
</ul></li>
<li>Training
<ul>
<li>Explains that the training is done with stochastic gradient descent (SGD) with Adadelta to adapt the learning rate.</li>
<li>Describes how the L2-norm of the gradient was normalized and that minibatches of 80 sentences are used.</li>
<li>Mentions sorting the sentence pairs and splitting them into minibatches.</li>
<li>Presents a table of learning statistics and related information.</li>
</ul></li>
</ul></li>
<li>Appendix C: Translations of Long Sentences
<ul>
<li>Presents sample translations generated by RNNenc-50, RNNsearch-50, and Google Translate.</li>
<li>Compares these translations with a reference (gold-standard) translation for each long source sentence.</li>
</ul></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;requiring the length to be fixed seems a mistake as sentences can sometimes get very long think hundreds of words</p></div></div></section>
<section id="reflections" class="level2">
<h2 class="anchored" data-anchor-id="reflections">Reflections</h2>
<p>Naively, translation is an easy task in the sense that we just need to look up each phrase in a statistically generated bi-lingual lexicon and append the translation of the phrase to the target sentence. In reality even when working with parallel text is tricky in that, the translation isn’t one to one but rather is n-m mappings between the source and target words with the number of words being variable. Picking the best entry in the lexicon is not obvious as the source words may be ambiguous and need some attention to other words in the context to disambiguate, and these too may be ambiguous ad infinitum… Finaly the translation may require some reordering of the words in the source sentence to conform to the target language’s grammar.</p>
<p>To sum up:</p>
<ul>
<li>We need to learn a bi-lingual phrase lexicon.</li>
<li>Each phrase may have multiple translations.</li>
<li>To pick the best entry the source context should be consulted.</li>
<li>The target sentence may require reordering to conform to the target language’s grammar.</li>
<li>Another challenge is that the target language may require words or even grammatical constructs e.g.&nbsp;gender and gender agreement that are lacking in the source language. These are <a href="../1997-floating-contraints/index.qmd">floating constraints</a> c.f. (<span class="citation" data-cites="mckeown1997floating">McKeown, Elhadad, and Robin (1997)</span>) that the model needs to propagate through the translation process.</li>
</ul>
<p>Each step of this process is probabilistic and thus prone to mistakes. Though there are two main constructs. The first is the contextual lexicon which needs to be learned using parallel text. The second is the destination grammar that can be learned as part of a language model using monolingual text. However reordering texts isn’t as much of a challenge, if we have sufficient parallel texts then the NMT hidden states should be able to learn the reordering as part of its hidden state.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-bahdanau2016neuralmachinetranslationjointly" class="csl-entry">
Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2016. <span>“Neural Machine Translation by Jointly Learning to Align and Translate.”</span> <a href="https://arxiv.org/abs/1409.0473">https://arxiv.org/abs/1409.0473</a>.
</div>
<div id="ref-mckeown1997floating" class="csl-entry">
McKeown, Kathleen, Michael Elhadad, and Jacques Robin. 1997. <span>“Floating Constraints in Lexical Choice.”</span>
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Neural {Machine} {Translation} by {Jointly} {Learning} to
    {Align} and {Translate}},
  date = {2024-02-11},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/2014-NMT-by-jointly-learning-to-align-and-translate/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Neural Machine Translation by Jointly
Learning to Align and Translate.”</span> February 11, 2024. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/2014-NMT-by-jointly-learning-to-align-and-translate/">https://orenbochman.github.io/notes-nlp/reviews/paper/2014-NMT-by-jointly-learning-to-align-and-translate/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Bidirectional LSTM</category>
  <category>Deep learning</category>
  <category>Embeddings</category>
  <category>LLM</category>
  <category>NLP</category>
  <category>NMT</category>
  <category>Paper</category>
  <category>Review</category>
  <category>Podcast</category>
  <category>Translation task</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/2014-NMT-by-jointly-learning-to-align-and-translate/</guid>
  <pubDate>Sat, 10 Feb 2024 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Speech</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w19-syntax-and-parsing/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/tiling.png" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>course banner</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/f-3N0stPtbw" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Lesson Video
</figcaption>
</figure>
</div><div id="sup-slide-deck1" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div><div id="sup-slide-deck2" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides2.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides2.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;2
</figcaption>
</figure>
</div></div>


<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>What is speech?</li>
<li>Speech applications</li>
<li>Speech databases</li>
<li>Speech hierarchy</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- Transcript here is machine generated no punctuation and I eddited it a but but it needs more work and I dont see the benefits unless I can run it through some tool. -->
</div>
</div>
</div>
<section id="navajo-in-10" class="level2">
<h2 class="anchored" data-anchor-id="navajo-in-10">Navajo in 10</h2>
<ul>
<li><p><strong>Language Name and Classification</strong>:</p>
<ul>
<li><strong>Navajo</strong> (also known as <strong>Navaho</strong>).</li>
<li>Navajo: <em>Diné bizaad</em> [tìnépìz̥ɑ̀ːt] or <em>Naabeehó bizaad</em> [nɑ̀ːpèːhópìz̥ɑ̀ːt].</li>
<li>It is a <strong>Southern Athabaskan language</strong> of the <strong>Na-Dené family</strong>.</li>
</ul></li>
<li><p><strong>Speakers and Location</strong>:</p>
<ul>
<li>Spoken primarily in the <strong>Southwestern United States</strong>, especially in the <strong>Navajo Nation</strong>.</li>
<li>One of the most widely spoken <strong>Native American languages</strong>.</li>
<li>The most widely spoken Native American language north of the <strong>Mexico–United States border</strong>.</li>
<li>Almost <strong>170,000 Americans</strong> speaking Navajo at home as of 2011.</li>
</ul></li>
<li><p><strong>Nomenclature</strong>:</p>
<ul>
<li>The word <em>Navajo</em> is an <strong>exonym</strong> from the Tewa word <em>Navahu</em>, meaning ‘large field’.</li>
<li>The Navajo refer to themselves as the <em>Diné</em> (‘People’), with their language known as <em>Diné bizaad</em> (‘People’s language’) or <em>Naabeehó bizaad</em>.</li>
</ul></li>
<li><p><strong>Official Status</strong>:</p>
<ul>
<li>Official language in <strong>Navajo Nation</strong>.</li>
</ul></li>
<li><p><strong>History and Development</strong>:</p>
<ul>
<li>The Apachean languages, of which Navajo is one, are thought to have arrived in the American Southwest from the north by 1500.</li>
<li>Speakers of the Navajo language were employed as <strong>Navajo code talkers</strong> during World Wars I and II.</li>
<li>Orthography developed in the late 1930s and is based on the <strong>Latin script</strong>.</li>
</ul></li>
<li><p><strong>Writing System</strong>:</p>
<ul>
<li>Based on the <strong>Latin script</strong>.</li>
<li>Developed between 1935 and 1940.</li>
<li>Uses an apostrophe to mark <strong>ejective consonants</strong> and mid-word or final <strong>glottal stops</strong>.</li>
<li>Represents nasalized vowels with an <strong>ogonek</strong> and the voiceless alveolar lateral fricative with a <strong>barred L</strong>.</li>
</ul></li>
<li><p><strong>Phonology</strong>:</p>
<ul>
<li>Has a fairly large <strong>consonant inventory</strong>.</li>
<li><strong>Stop consonants</strong> exist in three laryngeal forms: aspirated, unaspirated, and ejective.</li>
<li>Has a simple <strong>glottal stop</strong> used after vowels.</li>
<li>Four <strong>vowel qualities</strong>: /a/, /e/, /i/, and /o/.</li>
<li>Each vowel exists in both <strong>oral and nasalized</strong> forms and can be either <strong>short or long</strong>.</li>
<li>Distinguishes for <strong>tone</strong> between high and low.</li>
</ul>
<p><img src="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w19-syntax-and-parsing/navajo-consonants.png" class="img-fluid" width="400" alt="Navajo Consonants"> <img src="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w19-syntax-and-parsing/navajo-vowels.png" class="img-fluid" width="400" alt="Navajo Vowels"></p></li>
<li><p><strong>Grammar</strong>:</p>
<ul>
<li>Relies heavily on <strong>affixes</strong>, mainly prefixes.</li>
<li>Affixes are joined in unpredictable, overlapping ways that make them difficult to segment.</li>
<li>Basic word order is <strong>subject–object–verb</strong>.</li>
<li>Verbs are conjugated for <strong>aspect and mood</strong>.</li>
</ul></li>
<li><p><strong>Vocabulary</strong>:</p>
<ul>
<li>Most Navajo vocabulary is of <strong>Athabaskan origin</strong>.</li>
<li>Has been conservative with <strong>loanwords</strong> due to its highly complex noun morphology.</li>
<li>Expanded its vocabulary to include Western technological and cultural terms through <strong>calques and Navajo descriptive terms</strong>.</li>
</ul></li>
<li><p><strong>Revitalization and Current Status</strong>:</p>
<ul>
<li><strong>Bilingual Education Act</strong> in 1968 provided funds for educating young students who are not native English speakers.</li>
<li>Navajo Nation Council decreed in 1984 that the Navajo language would be available and comprehensive for students of all grade levels in schools of the Navajo Nation.</li>
<li><strong>Navajo-immersion programs</strong> have cropped up across the Navajo Nation.</li>
<li>Diné College offers an associate degree in the subject of Navajo.</li>
<li>In December 2024, Navajo Nation President made Navajo language the official language of Navajo Nation.</li>
</ul></li>
</ul>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Speech},
  date = {2022-03-29},
  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w19-syntax-and-parsing/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Speech.”</span> March 29, 2022. <a href="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w19-syntax-and-parsing/">https://orenbochman.github.io/notes-nlp/notes/cs11-737-w19-syntax-and-parsing/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Multilingual NLP</category>
  <category>NLP</category>
  <category>Notes</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w19-syntax-and-parsing/</guid>
  <pubDate>Mon, 28 Mar 2022 21:00:00 GMT</pubDate>
</item>
<item>
  <title>Automatic Speech Recognition</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w14-ASR/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/tiling.png" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>course banner</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/OObrN8yMYZU" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Lesson Video
</figcaption>
</figure>
</div><div id="sup-slide-deck" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div>

<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>What is speech?</li>
<li>Speech applications</li>
<li>Speech databases</li>
<li>Speech hierarchy</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- Transcript here is machine generated no punctuation and I eddited it a but but it needs more work and I dont see the benefits unless I can run it through some tool. -->
<blockquote class="blockquote">
<p>okay so today i’d like to talk about uh automatic speech recognition uh this is uh one of the uh most uh the active uh research area in speech processing and also very important application for multilingual energy and today i will first explain about the speech recognition demonstration uh and also talking about evaluation metrics and the i will a little bit use the mass to explain about the formulation of the speech recognition and they will move to the explanation about the standard a speech recognition pipeline so uh let me first try to work on the demonstration um which i just tried to kind of use speech recognition here i want to go to the cmu campus affect right the technology and the i also want to show how the current sr technology is robust i want to go to the cmu campus oh it doesn’t work if i can work speak slowly i want to go to the cma campus that’s the current technology next i mentioned that the speech recognition is not a good for the noisy environment so let’s try to make the noisy environment i want to go to the cmu campus they are cool right yeah they are very very cool um so i try to kind of uh make some kind of a mistake uh but the google guides are doing a great job so it’s actually not easy to find some kind of a significant mistake with them one of the easy mistake that i can uh that try to make could be the the other query okay right yeah they should you know study more about me i will talk to my colleagues in google and the the other difficulty would be let me try so i mentioned that the speech recognition is not very good under the uh the simple noise it’s actually quite strong but if there are some uh the other speakers are speaking it’s still very challenging so i try to make this kind of situation okay so do you have you have a long like it’s not not if there’s scabbards raised in new york but i guess up there you all don’t have too long where we’re going i want to go to the cmu campus i want to go to the cmu campus but it’s actually not working quite well in general so um this is a current technology uh with some more other essays with the controlled environment the speech recognition is quite working well and today’s talk uh i will talk about the introduce about the speech recognition technology insider okay so uh the the first example i already mentioned this is one of the mistake that i found when i use acidity uh and yeah sometimes very good but sometimes it’s not working and the uh what kind of errors the speech equation has would be today’s uh discussion so please discuss it in the rest of the class after my lecture and the the now the many of the speech recognition engines actually also support the the other languages and i also uh try the uh japanese and it’s working quite well uh it i tried several times and then i found this kind of uh the the mistaken mistake but in general uh other uh language are also working quite well now and now i will uh move to the uh evaluation as you can see sometimes it’s perfectly revoking but sometimes it’s not working right and then the uh how to evaluate the major uh in speech recognition is quite important and the first uh the uh most intuitive measure would be the sentence level error anyway you know the the sentence uh uh correct or not i think it is the most other important measure i would say uh however uh by measuring the performance of the system in this case it’s almost correct right it’s just there are a few words are uh the wrong so i we actually want to give some partial score uh if the half of the rods are correct so to do that uh instead of using the sentence error rate uh we usually use the water rate and whatever it is computed by considering the three types of the error one is the insertion errors in this case uh this uh word is actually uh the the extra word compared from the difference so this uh the word error correspond to the insertion error and then the rest of the uh the error is the correspond to the substitution errors by the way it’s it is not unique of course you know uh we can consider this substitution uh this one is a substitution and the decision that other insertion uh and so on and usually we uh consider all the possible other error and then taking the minimum by also considering the each other cost of each error and then compute the edit distance i think many people here would be familiar with the distance so i may not need to explain about it indeed so much detail but anyway this uh is our kind of unusual uh uh measure to uh check the performance of the speech recognition and the um so this is a quite well defined major when we have a wide boundary but as you can see we have a lot of work uh the languages which actually don’t have a word boundary right and how to compute it there are uh two ways one is just using some uh the uh the tools to segment uh to get the gun in the world are you need and then other computing the model rate but we should be very careful about that because this chunking may depend on the tool or dictionary and so on i had a very bad experience that i had compared my japanese system and other other institute other systems and i are actually quite bad and i was very disappointed but i couldn’t reach to their numbers and finally we asked them how to chunk their word and their tools are different from ours and we actually cannot compare right so it is the other issue when we using the tanking but still uh if we use the same chunking we can compare so this is widely used but the mostly used uh the metallic for such kind of languages is actually character uh error rate just consider the the each character and the other computer uh edit distance so this is uh the more widely used when we uh the compute measure for the languages that doesn’t have hardware what boundary so the uh the assignment three that first you guys speak the uh the language right and then do the asl and do the evaluation be careful about the uh the which measure are are you guys using if the that language has a good word boundary you guys can use what uh error rate but otherwise a character error is safe yes when you’re speaking in japanese do you expect the system to transcribe the world cmu enrollment yes actually yes yeah the since uh it’s a kind of our other already the training data and so are mixed so other it’s uh the appeared other other the roman character uh in the japanese cases yeah japanese cases we have a ton of data so actually this kind of mix mixture will be handled by data delivery manner however are they that i think in some sense you’re right uh for example this cmu which can be also are the written other uh the katakanas sim the script which is the other or the japanese script to the described the word from the foreign countries and this may also happen instead of this kind of roman character happens and then in this case uh the other letter or character both cases it was correct but it’s not regarded as the wrong a mistake so actually uh in this sense the uh is depend on the language but the in some languages the same the world can be uh represented as a different script and then the uh the water right or even characteristic is not the perfect measure but this is a very difficult problem and usually we the normalize to either of the symbols or we actually don’t care so much about that and there are a lot of other metrics um since the the uh the script has an issue like as i mentioned some people use their phoneme literally but this actually lose the semantic other information a lot so uh this is uh used uh to more to uh that check the whether this uh the uh speech is acoustically phonetically uh the correct but it’s not sure it is the semantically uh the correct so it’s not so much used but anyway this is also another major prime error rate is also sometimes used but i skipped the other explanation and the other metrics decently these two uh the metrics are quite important the airtime factor in the latency since uh many of the systems are now in the uh the on the device and so on and the speech uh is used at the interface whatever speech has another application you know the offline archive uh and so on we don’t care about the uh at least latency a real-time factor other smaller may be better but the latest is quite important if you’re using a speech interface and the it’s also depend on the downstream uh evaluation metrics for example uh the if we combine this one uh with the uh the understanding system or if we combine it with the translation system uh of course we should evaluate it with such kind of downstream uh the evaluation method so in the speech transformation cases of of course it can be better if specification performance is better but we just check the group and the uh for speech recognition are the this uh the especially the characteristic of what error rate i recommend you to use the nist speech recognition scoring toolkit how many people knows nist nist is uh the government institute to standardize uh our uh the daily life activities uh including the uh the for example the um atomic clock and that was also uh the uh the standardized by the nist and the nist is actually uh helping us our community to standardize uh such kind of evaluation metric uh in speech and energy processing so uh and then of course if you know we use a different evaluation tools measures and then if this kind of result is different we cannot compare your result with others right so due to this reason i actually recommend you to use the sctk or other standard added the standardized or are the widely used uh the evaluation error evaluation uh metric uh then you know some tools that they did someone just made it to avoid to hover some uh the confusions again if the uh variation metric tools are different it is disaster right okay so uh with this kind of a great help from the uh the nist and so on uh the speech recognition actually uh was well measured in terms of the uh the the water rate or crop uh the kerati so everyone can for example either compare the performance the others are difficult right and they in the other previous lectures graham also discussed about the uh the blue or other evaluation metrics emerging translation and there are some discussions whether patreon is better or not while they’re at the characteristic there’s not so much discussion mostly it’s correct i say there are some kind of exceptions that i mentioned before but it’s mostly correct and also nice part is that this has a very high correlation with the downstream tasks better weather are definitely better in the speech translation right so anyway due to that study and also they start their needs are providing such kind of toolkit so that everyone can compare the error rate uh strictly so uh this is the one reason that the the speech recognition uh that has been studied for a long time with a common benchmark so now that everyone every field we have our other common benchmark and then you know performance major and readable and so on right speech condition actually has a long history of uh other comparing the techniques based on the shared uh the data are the same other evaluation metric and this is a kind of uh the the figure that i often show in this lecture uh this is a switchboard task and as i as i mentioned every month since 1990s up to now 20 to 30 years we still using this data and we still using the same vibration metric so that we can track the performance uh the improvement so this is possibly the one reason that actually deep running uh has been applied to many areas but the speech is actually one of the first area that deep running is applied since uh we can easily uh also fairly show the performance improvement uh based on this other evaluation measure okay uh so yes this area right whether this area is the the what when other i started speed recognition this area is one of the core data so are the any techniques that cannot improve the performance we also cannot uh the the the other further uh the the scale the training data and so on and also the uh uh the the budget big project uh the the uh kind of the the uh in this era for speech recognition uh anyway uh this is the kind of one of the other cold uh age in speech recognition it’s some slight improvement happens like a discriminative training and so on but uh before deep learning and this i i’ll say my most kind of a speech equation history by the way but i when i went to the conference we always talking about you know the method of a get a 0.3 percent well there are the improvement wow that’s a very good psychic only 2.3 or something like that yeah but the uh due to the deep running that’s kind of our are the uh the how they say we saw that this is the strings there and they cannot release to the human performance but thanks to the deep running and the computational uh the the uh the breakthrough gpu uh gpgpu and the strobe uh and also their open source or other other people’s uh did uh the knowledge uh sharing uh the now uh makes the performance to be included better and better okay so one more thing some people say speech recognition this search is easy this is because we have evaluation metric so the other areas it’s not easy because we have to start to make a variation method by ourselves or you know there’s multiple evaluation metric and we have to pick or something like that so instead speech condition is actually regarded as an easy research topic in terms of that we have a fixed evaluation metric so if you guys have a fancy neural network and then get the improvement by one person you can write the paper okay so uh i will move to the uh the uh speech recognition uh uh the oh yeah i need to kind of swap that a little bit using a mathematical formulation of uh speech recognition so first uh speech recognition as i mentioned in the yeah the the several times uh it’s quite uh interesting combustion problem input is completely physical signal wave pressure sound pressure right that’s undergoes the linguistic symbol so physical one becomes the linguistic uh symbol it’s very different right and the input other characteristics is also very different the waveform is gesture other than one sentence it can be like a two to three seconds and if with a final at the 16 kilohertz sampling grade the length will be the the order of ten thousand if we you use a short term fourier transform and other speech features it goes to the hundred dollar but still the bit long hundreds of thousands that only goes to the uh the three lengths of the symbols so uh a word uh in the vocabulary so this other conversion uh the from input and output is the i’ll say quite different and this actually makes the problem quite difficult okay so now i try to kind of explain that how other the speech function is realized one by one so the google uh the demonstration you guys just see that this is a one box right but it’s actually inside there are several books first one is the feature extraction this is i think i don’t have to mention about that so much about it any of the pattern recognition machine learning problem we first have our study the the feature extraction right and in speech anyway a waveform is not easy to deal with so instead we’re using the feature extraction called the uh mhcc i have a two more slides explaining this each of the modules so i will a bit more detail about it but anyway from now on i will start from the feature which is you know continuous vector time series of the continuous vector and then add a mapping to the word sequence how to formulate it one way is we just add hopefully uh making it a regression problem but instead the uh the people uh actually uh the the the formulate this problem uh more mathematically rigorous other ways they started to uh formulate this problem as a map decision theory here the posterior is from pw given node so the posterior probability of the word sequence are given the observation and then among all the kind of word sequence we just try to pick up the most likely uh sequence that’s because it becomes a speech condition uh quite simple right and the the the problem issue is how to obtain this pw given row so this is the kind of others the the speed recognition uh problem that we usually use for the probabilistic formulation and the just couple of the rules that we usually use i yeah if i have time i will explain bit more carefully about this one but anyway i just want to mention that why people using a probabilistic formulation there are a lot of reason but one of the reason is that we don’t actually have to remember various kind of equations we just have a three equation product rule sum rule conditional independence assumption conditional independence assumption is not rule but maybe just including this vector as a rule but by only using these three uh basic uh uh probabilistic rules we can actually other make this uh p double given no problem bit more tractable so the first thing that the people may often see for the speech recognition is to use a base rule to change this pw given o to p o w put p or given w p w divided by p o and then since the p o is not uh the uh depending on the w we actually uh the uh use the p o dot given w and the w so uh the two uh do to derive this one uh which rule other did we use from here to here mainly so yeah product rule yes so by using the product rule uh we kind of changing the problem uh from the original posterior distribution to the right grip and the prior distribution uh this is uh the methodology is called noisy channel model and the people actually using this method but is that enough to solve the problem for me it doesn’t actually change the difficulty or even that looks like it’s more difficult right and then the uh the how to uh the the make this uh problem more productive we actually uh the using the additional information so speech to text anyway this is a very different uh the conversion we want to have something between what they we can introduce like our linguistic knowledge we can use phoneme right and then the uh this is you know a little bit easier right from all to directory uh predicting the words are kind of difficult but by using the phoneme intermediate representation uh each kind of conversion is a bit sub problem and this is easier so this kind of a methodology is quite important to solve the very difficult combining problem okay so now we will we have our phoneme seekers let’s use the following sentence how to introduce this phoneme sequence in this uh the probability distribution some people may answer if either they took my course in the speech question and understanding which one we use some rule product rule coordination independence assumption it’s actually summer right some rule is great we actually can introduce the additional variable right still this is doesn’t change anything it doesn’t change the difficulties so how do the further other changes at this problem we just using the product rule like i mentioned before it’s part of the base rule but by using the product rule we can further factorize this problem to the three distribution and one the the other distribution in the denominator but this is not related to the our other optimization problem so we can safely actually ignore it right is that everything it’s actually not this is just equivalent uh the conversion right it actually doesn’t change the difficulty how to make the program more simple we’re using the conditional independence assumption for this case it’s where we use the condition of some individual functions here these are reasonable assumptions right the the relationship of the obligation speech features only depend on the election through the lexicon volume than the one it’s uh it’s a difficult approximation but it’s reasonable i would say right so we actually using this conditional independence assumption everywhere to make the problem attractable for example the acoustic model we first apply the conditional independence function hidden markov model is one of the conditional independence assumptions to make it tractable and uh we use a innogram language model now we use the neural language model which actually doesn’t have that but they used to use the endogram language model this is also conditional independence assumption so by using that we actually making this problem uh attractable and this methodology is quite rather powerful actually it’s not only used for speech recognition by the way this is also used for the machine translation as well you know before neural machine translation comes so uh actually the ibm uh is uh the the same other data the group same division are the proposed both speech recognition uh and the machine translation in this kind of a statistical form okay so now uh i uh decompose this uh the problem to the three distribution right this is actually structure uh that we uh that are solving the speech recognition so the first part feature extraction again this is not included in the probabilistic representation the first one is acoustic modeling lexicon language model it sounds like we just combined some kind of uh other sub problems right but it’s actually mathematically uh well uh decomposed and then we make each of the sub program tractable and then finally combine it based on the uh this uh the the equation this is the mass of the speech recognition or other other the problem of solving the secant sequence model before neural model comes the important concept is factorization to make the kind of problem to be decomposed and the other is its factorization itself doesn’t change the difficulty we also have to have a conditional independence assumption to make the problem practicable so uh i don’t know how you guys feel when i first learned this one i saw that this is very elegant the first thing that i just learned the speech recognition is these four components oh my god it’s just a complicated you know there are some modules that are combined to make a speech equation sounds like you know very cool but but a bit necessary but it turns out that it has a quite beautiful theory to other the original target is the base decision theory and i introduced each of the sub modules uh based on the uh the probabilistic formulation and actually at the in the uh the following uh the slide i will talk about each of the modules a little bit more but usually i skip the details and if you guys want to know more about each of the module and so on uh or please also consider to take a speech operation and understanding courses in the full semester okay um maybe i can accept one of a few questions here if not i can move to the each of the pipeline uh quickly um the first part uh feature extraction this is before you know the goes to the probabilistic model the feature extraction we use our other signal processing techniques uh to convert the waveform to the male frequency capture question mscc or other other features and i just want to think uh the multilingual energy this process is mostly uh the the language independent process as you can imagine right this is just a signal processing to convert the waveform to more tractable other part other feature so uh this is actually the the result of the conversion from the waveform to the uh ms60 i think most of people could agree the the bottom figures have more patterns right any more patterns so this has been very important uh to make the uh the feature uh to be uh used for the background of the processing uh by the way this signal processing based approach is gradually replaced by the deep running this is also happening now so one is the people using actually other cnn instead of mscg or some other people also using the sales supervisor learning now this is very powerful but the drawback is that this is a learnable learning based approaches so then the probably language independence property will be kind of mitigated to do that next acoustic modeling which converting the speech features to the following the sequence and for this other acoustic modeling uh we using the combination of the hidden markov model with gaussian mixture model or deep neural network and this approach is also the mostly uh language independent because speech features are more likely independent and the phoneme is also if we design well it can be a luggage independent or at least you know not so much other we can make it not so much depend on the languages although it is actually very difficult okay so the hidden markov model is uh the other actually quite important part in this uh the modeling what hidden markov model is doing is actually uh the um quite important role uh in the entire speech definition so as i mentioned the speech recognition the one over the difficulty is the output the symbol and the input uh the languages are very different so we need to make some arrangement uh all of the uh the speech features and the corresponding the following uh the information so in this case you know we don’t know which boundary we should take for each of the funding and to do this kind of alignment problem uh we are the classically uh using the hidden markov model and the hidden markov model is more like a charging of this other making this kind of alignment and given this alignment to provide a data accurate likelihood based on the as gaussian used to be gaussian but now deep neural network that is a kind of acoustic model and by the way this acoustic model part is the most important to get the performance than the other components in general the third module is the english and so on and this part is heavily language dependent so to do that uh we actually need to first access to get the other addiction information and i actually uh the uh usually show you the uh the cmu dictionary uh this is one of the most well-known uh english dictionary uh maintained by here but unfortunately it’s the stava is down now so i cannot make a demonstration uh and so on uh but uh this uh cmu dictionary the english is very lucky because we have a same reaction and then we can build a speech principle system the other language is it’s actually not easy to get that we don’t have so much kind of structure and the accessible dictionary in the other languages however other probably that i use this we dictionary and the if using navigationally we can also somehow get to the information about the other phoneme it’s actually covered many languages like this right by the way it’s not recovered yet not yet okay this is unfortunate so okay so uh by using that we can also uh the uh the use the other maker kind of of this uh dexcom component for each languages and the last part uh is the language model and the language model that we use the engram or a recurrent neural network and usually the first language model will be the the built is that the uh we uh try to kind of uh get the uh the uh how possibly uh that word uh uh given the pronunciation we can hover are the authority that given the kind of other the word sequence uh how possibly we select each language depending on the context that is what language model is doing and this uh the example i actually have these three uh the uh word sequence this is by the way the same go to the in terms of the phoneme and it can be actually go to or go to or go to right and then if for example check if we check the other uh this works journal sentences and check whether this but how many times this button appeared and then we can get the the uh how likely this other phrase appeared right go to appeared 51 times but the others actually are do not appear so this means that this go to will be most likely selected based on the language model right however this uh actually also other has some kind of other issues uh actually uh this pattern still exists in our languages so i actually include the text size from the ten thousand order to the median order and then can’t go to still other the biggest to more than two thousand can you guess how many times go to appears go to appears in the worst journal sentences two times only two times this is a one example it’s not completely good sentence but yeah it’s appearance right and the goal too there how many you you guys can guess it’s actually quite a large number and yeah most cases go too far uh this is the other sentence that the uh the uh the overnight show i saw when i uh search this other two other phrases so if we using the small corpus it doesn’t cover but if using the large coppers we can cover it so this is a kind of power up if using a large purpose we can cover many of the various are the language patterns as much as possible and this part is also language dependent so uh this is the most uh the the the building block of the speech recognition and actually i’d say that it is not easy to build that for you guys each of the components is governed by the different model feature extraction signal processing model acoustic model other pattern recognition machine learning deep learning and so on next come moderate coming from the other computational linguistic or other linguistics language modeling is also come from the nlp or are they now deep learning but anyway the each of the other modules has a different models so it is very difficult to actually develop all of them and also connecting all of this kind of module is also not easy so instead now people are also working on the uh end-to-end speech definition which try to make entire pipeline as a single neural network google’s a demonstration it’s actually they already switched to the end-to-end neural network other companies as far as i know they still didn’t add a seat to the neural network and to the neural network but either by using this pipeline and the uh the so maybe yeah maybe that’s it i want to uh finish my talk so uh the these are summary uh of the speech recognition first feature condition is very well defined problem and also fortunately we have a large corpus so it is quite you often use other other measure for the for our new machine learning algorithm and the factorization and making the problem productive this is a kind of great methodology for us to tackle this problem and the but since uh the this methodology is a bit complicated recently people are also that are the uh they’re the interesting and actually having a lot of development in the end to end a speech equation feature recovered in my next lecture and then this is the main topic or the other assignment three so please uh enjoy the speech recognition and let’s move to the discussion um the uh the priests i think you guys already tried a speech recognition engine by yourself right and then talk about what kind of errors you happened and also apply to the other language and discuss about it and since we don’t have enough time we don’t have our final discussion time just split and then either discuss it and then finish it okay so let’s just</p>
</blockquote>
</div>
</div>
</div>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<p>Here is a lesson outline in a markdown unnumbered list format, based on the sources:</p>
<ul>
<li><strong>Speech Recognition Demo and Evaluation Metrics</strong>
<ul>
<li>Demonstration of speech recognition.</li>
<li>Discussion of how well it works, and examples of when it fails.</li>
<li>Evaluation metrics.</li>
</ul></li>
<li><strong>Evaluation Metrics</strong>
<ul>
<li>Sentence error rate.
<ul>
<li>Discuss if the entire sentence is correct.</li>
<li>Explain why this is too strict of a measure, and the need to consider local correctness.</li>
</ul></li>
<li>Word error rate (WER).
<ul>
<li>Using edit distance word-by-word.</li>
<li>Calculating error rate percentage.</li>
<li>How to compute WER for languages without word boundaries.</li>
</ul></li>
<li>Other metrics.
<ul>
<li>Phoneme error rate (requires a pronunciation dictionary).</li>
<li>Frame error rate (requires an alignment).</li>
</ul></li>
<li>NIST Speech Recognition Scoring Toolkit (SCTK).</li>
</ul></li>
<li><strong>(A bit) Mathematical Formulation of Speech Recognition</strong>
<ul>
<li>Speech recognition as a conversion from a physical signal to a linguistic symbol.</li>
<li>Explanation of probabilistic formulation.
<ul>
<li>MAP decision theory to estimate the most probable word sequence.</li>
<li>Noisy channel model.</li>
<li>Factoring and conditional independence.</li>
</ul></li>
</ul></li>
<li><strong>Standard Speech Recognition Pipeline</strong>
<ul>
<li>Feature extraction.
<ul>
<li>Converting waveform to MFCC.</li>
<li>Language-independent process.</li>
<li>Desirable representations.</li>
</ul></li>
<li>Acoustic Modeling.
<ul>
<li>Converting speech features to phoneme sequences.</li>
<li>Using Hidden Markov Model (HMM) to align speech features and phoneme sequences.</li>
<li>Language-independent.</li>
</ul></li>
<li>Lexicon.
<ul>
<li>Pronunciation dictionary.</li>
<li>CMU dictionary.</li>
<li>Multilingual phone dictionary.</li>
</ul></li>
<li>Language Model.
<ul>
<li>Using N-grams or recurrent neural networks.</li>
<li>Word selection based on context.</li>
<li>Language-dependent.</li>
</ul></li>
</ul></li>
<li><strong>End-to-end Speech Recognition</strong>
<ul>
<li>Using a single neural network.</li>
<li>A simpler solution for multilingual ASR.</li>
</ul></li>
</ul>
</section>
<section id="papers" class="level2">
<h2 class="anchored" data-anchor-id="papers">Papers</h2>
<ul>
<li><span class="citation" data-cites="Pallett2003ALA">Pallett (2003)</span> <a href="http://www.itl.nist.gov/iad/mig/tests/rt/ASRhistory/pdf/NIST_benchmark_ASRtests_2003.pdf">A look at NIST’s benchmark ASR tests: past, present, and future</a></li>
<li><span class="citation" data-cites="saon2015ibm">Saon et al. (2015)</span> <a href="https://arxiv.org/pdf/1505.05899">The IBM 2015 English Conversational Telephone Speech Recognition System</a></li>
<li><span class="citation" data-cites="xiong2017achievinghumanparityconversational">Xiong et al. (2017)</span> <a href="https://arxiv.org/pdf/1610.05256">ACHIEVING HUMAN PARITY IN CONVERSATIONAL SPEECH RECOGNITION</a></li>
</ul>
</section>
<section id="warlpiri-in-10-minutes" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="warlpiri-in-10-minutes">Warlpiri in 10 minutes</h2>
<div class="page-columns page-full"><p>  </p><div class="no-row-height column-margin column-container"><img src="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w14-ASR/geom.png" id="fig-lit-greo" class="img-fluid" width="400" alt="Geography"><img src="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w14-ASR/consonants.png" id="fig-lit-cons" class="img-fluid" width="400" alt="Consonants"><img src="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w14-ASR/vowels.png" id="fig-lit-vowel" class="img-fluid" width="200" alt="Vowels"></div></div>
<ul>
<li>Spoken in the Northern Territory of Australia by the Warlpiri people.</li>
<li>Approximately 2,500-3,000 native speakers.</li>
<li>One of the largest Aboriginal languages in Australia based on the number of speakers.</li>
<li>One of only 13 indigenous languages in Australia still being passed down to children.</li>
<li>Alternative names include Walbiri and Waljbiri.</li>
</ul>
<p><strong>Language Family &amp; History</strong></p>
<ul>
<li>Pama-Nyungan.</li>
<li>Ngarrkic languages.</li>
<li>The term <em>Jukurrpa,</em> referring to Aboriginal spiritual beliefs, comes from Warlpiri.</li>
<li>A writing system was not developed until the 1970s when the language began to be taught in schools.</li>
</ul>
<p><strong>Grammar</strong></p>
<ul>
<li><strong>Free word order</strong>, but the auxiliary word is almost always the second word in a clause.</li>
<li><strong>Ergative</strong> marking. The actor takes a special ending called the ergative ending. The ergative ending marks the subject of a transitive sentence.</li>
<li><strong>Split ergativity</strong>. Nouns follow one set of rules, while pronouns and auxiliary verbs follow another.</li>
<li>Suffixes indicate person and number of the subject and object.</li>
<li><strong>Vowel harmony</strong>.</li>
</ul>
<p><strong>Phonology</strong></p>
<ul>
<li>Most Warlpiri languages have only <strong>three vowels</strong>.</li>
<li><strong>No voicing contrast</strong>. Aboriginal languages have no contrast between voiced and voiceless consonants. A sound can sound like a ‘p’ or a ‘b’ depending on its position in the word.</li>
<li><strong>No fricative sounds</strong>.</li>
<li>Love the ‘r’ sound. Warlpiri has three ‘r’ sounds.</li>
</ul>
<p><strong>Interesting Linguistic Features</strong></p>
<ul>
<li><strong>Avoidance register</strong>, a special style of language is used between certain family relations that have a drastically reduced lexicon.</li>
<li>Warlpiri Sign Language also exists.</li>
<li>Speakers are often multilingual, learning each other’s languages.</li>
<li>A strong tradition exists of not saying the names or showing images of people who have passed away.</li>
</ul>
<p><strong>Present Status</strong></p>
<ul>
<li>Warlpiri is considered a threatened language because children sometimes respond in English even when spoken to in Warlpiri.</li>
<li>There are efforts to teach the language in schools and create modern terminology.</li>
</ul>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-Pallett2003ALA" class="csl-entry">
Pallett, David. 2003. <span>“A Look at NIST’s Benchmark ASR Tests: Past, Present, and Future.”</span> <em>2003 IEEE Workshop on Automatic Speech Recognition and Understanding (IEEE Cat. No.03EX721)</em>, 483–88. <a href="https://api.semanticscholar.org/CorpusID:17986888">https://api.semanticscholar.org/CorpusID:17986888</a>.
</div>
<div id="ref-saon2015ibm" class="csl-entry">
Saon, George, Hong-Kwang J Kuo, Steven Rennie, and Michael Picheny. 2015. <span>“The IBM 2015 English Conversational Telephone Speech Recognition System.”</span> <em>arXiv Preprint arXiv:1505.05899</em>.
</div>
<div id="ref-xiong2017achievinghumanparityconversational" class="csl-entry">
Xiong, W., J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, and G. Zweig. 2017. <span>“Achieving Human Parity in Conversational Speech Recognition.”</span> <a href="https://arxiv.org/abs/1610.05256">https://arxiv.org/abs/1610.05256</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Automatic {Speech} {Recognition}},
  date = {2022-03-03},
  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w14-ASR/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Automatic Speech Recognition.”</span> March
3, 2022. <a href="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w14-ASR/">https://orenbochman.github.io/notes-nlp/notes/cs11-737-w14-ASR/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Multilingual NLP</category>
  <category>NLP</category>
  <category>Notes</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w14-ASR/</guid>
  <pubDate>Wed, 02 Mar 2022 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Speech</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w13-speech/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/tiling.png" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>course banner</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/Hsdf8Vjai5g" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Lesson Video
</figcaption>
</figure>
</div><div id="sup-slide-deck" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div>

<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>What is speech?</li>
<li>Speech applications</li>
<li>Speech databases</li>
<li>Speech hierarchy</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- Transcript here is machine generated no punctuation and I eddited it a but but it needs more work and I dont see the benefits unless I can run it through some tool. -->
<section id="intro" class="level2">
<h2 class="anchored" data-anchor-id="intro">Intro</h2>
<p>Today’s lecture today is about speech. Today, we also have our assignment release. After this lecture, we will have a walkthrough of the assignment speech. I hope I can finish it earlier so that you guys have enough time to explain the assignment three. We actually put a lot of energy into it, so I hope you guys can enjoy it. anyway, for today’s content, I put the four other items: speech, speech applications, speech databases, and speech hierarchy. I will probably go through at least three items, and the fourth maybe the this time or next Thursday so the next week that I will talk about the</p>
</section>
<section id="what-is-speech" class="level2">
<h2 class="anchored" data-anchor-id="what-is-speech">What is speech???</h2>
<p>The most important definition is speech. Many people say that “it’s sound produced by humans.” It may probably be true, but in many cases, we actually have a bit more narrow definition, which is “the sound produced by humans for the communication” that is mostly actually used in our kind of other speech processing but generally the wider meaning of the speech can be just by humans that is a kind of our other usual definition and then I will try to ask you guys about the four kinds of five audio and please answer this one is speech or not with water is it speech or not it is? Speech yes it’s very noisy but still speech next probably this is not speech or this thing is speech yeah probably business speech by the way all the sound oh no except for the first one. I picked up from the other free sound so this the link is also useful you guys can get a lot of kind of interesting sound and actually using it for your research. it’s also copyright free, so quite kind of easy to use the Saturn is this speech or not it’s actually the difficult so my definition still speech because laughing and so on actually accelerate our communication right so my definition are usually yeah they’re using it as a speech yeah even people are typing to do some communication but this is not human voice so this is not speech right and the last one what do you think is this speech or not so if this is the wide the sense of the the sound to produce a human this is speech but this is not free for the communication especially singing voice well some people can use singing words for the communication like opera and so on but in general it is not used for the communication so again this definition is not very clear but usually people actually regarding the speech and the singing voice are separate okay so this is the definition of the speech and the next a speech is made by the sun right and what is sound sound is just a change of the air pressure and that is captured my microphone so it’s quite physical phenomena so compared with the various topics we have been studying in multilingual nlp many of the kind of the problems are more like a syntax semantic other textual information but speech is actually quite that comes from this the physical phenomena and the actually this other sound</p>
</section>
<section id="speech-waveform" class="level2">
<h2 class="anchored" data-anchor-id="speech-waveform">Speech waveform</h2>
<p>pressure is captured by microphone and they converted either this other one-dimensional waveform and usually in this lecture, I use this one-dimensional waveform as an input to our problem in speech other processing but it’s actually not one-dimensional like for example human years a very good. example we actually at least have two two-dimensional other waveforms and the many of the the smart speakers including this one or the Alexa and so on. They also have the other more microphones but in general each of the kind of channel of the signal is the one other dimensional time domain waveform and since this is a waveform so this is governed by well-known physical properties. I am not sure how many people remember this kind of properties attenuation deflection the the the diffraction super position and so on so actually these properties are quite a making speech signal to be ditch information and due to this kind of property especially superposition and reflection is a kind of very important the property to include a lot of the information inside the waveform this becomes the least information in the speech so first no second question actually the second question to you guys is</p>
</section>
<section id="what-kind-of-information-does-speech-sound-contain" class="level2">
<h2 class="anchored" data-anchor-id="what-kind-of-information-does-speech-sound-contain">What kind of information does speech sound contain?</h2>
<p>what kind of information does speech sound contain of course I will talk about the speech condition so this means that the speech includes linguistic content transcription and so on right and the speech also has a speaker characteristics is there any other information that speech can deliver can someone tell me phonemic signals like emphasis and so on yes exactly exactly any other notion emotion yes good good are there any other examples actually quite a lot by the way for example one of them yeah you can tell if somebody’s sick yes if they have a cold yes they if they talk too much the previous night yes yes exactly exactly you know other other faculties here also that try to kind of detect the the person is copied or not from the voice signal so voice can also include the health information voice also that speech also includes a lot of other information for example we can speech information even include the location you guys can identify at least you know which direction speech comes right this is not purely speech but it’s a kind of multiple other signal of the speech we can identify other the the the location speech comes from speech also include the reverberation so from the evaporation we can even approximately say the room size and so on is there any other information maybe a lot that I can come up with other the many of them and actually this other information is that corresponds to various other speech applications so now move to the the speech applications so the speech applications are depend on the other problem but they actually the the many of the problem is to try to capture what information is delivered from speech speech condition is a good example right and there’s speech emotion recognition try to capture the emotion and of course we also have our audi the opposite problem given the text to generate the speech speech synthesis this is also the speech processing research topics and the we actually hover tons of the other problem in speech speaker recognition as I mentioned speech delivered speaker characteristics right so that from the speech signal we can actually identify who is speaking oh I forgot to mention about it this is actually very important for the modern thing or nlp language recognition from speech sound we can also identify the language and the there are a lot of the speech the program by the way these topics i extracted from the topics in the intel speech which is one of the biggest speech conference and the deadline is next month right one month right after right after this that day one month right after this today so if people here are thinking about doing something for speech inter speech is a good target but I think if we link it to the project here maybe it’s a bit too late okay so we have a tons of the speech application and by the way among them the next question among them what is the most widely used technique there’s you know under some how they say the definition what is other than the mostly used but I think everyone can agree if I mention the answer so can someone say what is the yes yes do not check the next line because people usually cannot answer so yeah speech recognition probably in terms of the number of the researchers the the biggest one is speech recognition and the next biggest the second biggest would be speech synthesis and other kind of following and the speech coding is actually a bit the the already sorted technique so there’s not so much research in this area but actually speech coding is the most used technique and the actually the speech coding is a unique compared with all others all the other tried to harder say some given input to extract some information or enrich some information by generating that and so on the speech coding is try to keep the original information but try to kind of compress the</p>
</section>
<section id="speech-coding" class="level2">
<h2 class="anchored" data-anchor-id="speech-coding">Speech coding</h2>
<p>information as much as possible and actually this was the one of the first or the most active study in the early the speech the processing research since at that time compression is quite important and also compression actually tells us what is speech what information is delivered is a quite important if we consider the speech coding and so on so that’s why people are starting to work on the speech coding and then the that we moved to various other applications and another the many of them are actually getting quite matured I would say and one of the techniques I would like to mention is a speech recognition and that can be covered</p>
</section>
<section id="automatic-speech-recognition-asr" class="level2">
<h2 class="anchored" data-anchor-id="automatic-speech-recognition-asr">Automatic Speech Recognition (ASR)</h2>
<p>in Thursday and after the break I will also other revisit the sr part again so using the two lectures to explain about a speech condition but the speech recognition is one of the very complicated system if you guys really want to understand a speech recognition it may take one semester so then please take the speech recognition and understanding in the next fall semester if you really want to know that the next the most the second the biggest obligation is speech synthesis</p>
</section>
<section id="speech-synthesis-tts-text-to-speech" class="level2">
<h2 class="anchored" data-anchor-id="speech-synthesis-tts-text-to-speech">Speech Synthesis (TTS: Text to Speech)</h2>
<p>and the speech synthesis is also are they given the text to generating the web form and how the wider and I think this is the possibly the the second biggest applications than speech recognition I believe because every and now you know are that hard about that the tts synthesize the voice right so in terms of the application this also has a big success and as everyone here may note that the young guys are lucky so professor aram black he is the pioneer of the text speech and he will have a lecture about dds I’m also very looking forward to that okay so there are a couple of others and I try to pick up not everything but some of them that are interesting or are important the next one that may be very related</p>
</section>
<section id="speech-translation" class="level2">
<h2 class="anchored" data-anchor-id="speech-translation">Speech Translation</h2>
<p>to this course is speech translation so directly converting the source speech to that target text so in this example a japanese speech it’s translated to english text how to do it one simple approach is to combine automatic speed recognition first and then machine translation and this is probably the other most kind of other the widely used way and the I think other that this way should be also considered but these are the two system combining two systems a bit complicated and also if we have our error in the asl side it cannot be well recovered in the second machine translation side so people also are starting to directly are using an end-to-end neural network to solve the japanese source speech like a Japanese speech to a english text directory without outputting Japanese text and so on and this approach is quite important if some languages don’t have their own scripts but they often kind of have a kind of a translation to their colonial languages and so on there’s such a lot of such kind of other databases actually and then this speech to speech to text direct information is also quite important in these cases and the this is to the speech to still text but if we try to use it for the interface to communicate to the foreign people in the foreign countries seamlessly we also want to develop the source a speech to speech translation in this case source speech is english and it is converted to the target speech and this is also started to be very popular not only as a research but also other other product and many of the systems are currently using the combination of the speech recognition first machine translation in the middle and then other other dts takes to speech but there are a lot of kind of a decent emergence techniques based on the end-to-end single neural network to even the model this kind of a very complicated pipeline based on a single architecture and I think this is one of the the ultimate goals of multilingual energy right if we have a perfect speech to speech translation systems probably we can solve many of the problems that discussed in this work but this problem is quite difficult and a lot of studies are all going on now so these are the related to marginal nlp and I will also introduce some of the other speech applications which will be very related to our problems so I just kind of combined the speaker recognition language construction speech emotion recognition all the event classification and detection so actually speech information has a lot of profiling information so as I mentioned the speech recognition is one of the example this means a linguistic information right and we speech also other the speaker gender aging information as well age is not very correct but we can still actually predict some range of the age based on speech and the language we can also add get the information from speech emotion that we have discussed that is also other extracted from speech and this is not completely speech information but the let’s say environment is important for us to understand about the communication scene and the song right and then audio event so when I am speaking the fun noises you know working on somewhere right this kind of information all that kind of informations are included in speech so superposition property is quite nice in terms of packing many of the information into the one sound wave form</p>
</section>
<section id="privacy-in-speech" class="level2">
<h2 class="anchored" data-anchor-id="privacy-in-speech">Privacy in speech</h2>
<p>so this the property is very good but it’s getting a bit more other difficult not difficult but challenging problem privacy in speech since speech has a lot of information so that the working on speech means we potentially touch a lot of information of that other person so the people are also seriously thinking about the privacy and on-device approach becomes quite important due to this i’m setting the how many slides I can have I have to finish maybe this is very cool so I can also either introduce one this one so as I mentioned the super position property is very cool in terms of the getting various information in the environment right but this information is not always useful actually many cases we just want to focus on target speaker right so then the speech enhancements peace separation techniques are also very important it’s not very related to this other lecture but I will just explain about it because many people actually often ask me the the my asl doesn’t work and most of the cases it comes from noise so I just want to emphasize that and there are way to deal with that and there are several types of the the noise</p>
</section>
<section id="speech-enhancement-several-types-of-problems" class="level2">
<h2 class="anchored" data-anchor-id="speech-enhancement-several-types-of-problems">Speech enhancement Several types of problems</h2>
<p>we have to deal with the first is a kind of how does this the the gesture or the environmental sound that adding to our the the usual sound you guys I guess can steal the recognize it right computer actually cannot at all probably you guys can do better than computer so to solve this problem we also cover other noising techniques this is actually after the denoising sounds better right this is actually very good for computer and the next debug variation is also another very the annoying issue again for you guys it is not so difficult to recognize but for computer this evaporation echo is quite harmful so there’s a technique to suppress the evaporation called the reverberation if you guys using the headset microphone you guys can even clearly see the the find that the the echo is removed since this room is a bit large so actually echo is further accomplished but I believe still people can understand it next one is the separation I think I prepared a very cool demonstration by oh yes I prepared this demonstration this is actually the he is my former colleague when I was working at the mitsubishi electric research laboratory jonathan and he actually it’s he was planning to come here this friday but it was cancelled so for it is unfortunate but instead other I just play his cool demonstration you can see that the the mixed speech is completely separated right so this is one of the pioneering work of using deep running for speech separation in the first time and this is already very clearly separated right but now the technology becomes mature and this other kind of a simple speech separation problem is almost solved however we have a lot of other difficult separation problems okay so I think these are more like the applications that I want to cover there are lots of other application if I have time I can also want to introduce the some of them like a spoken dialogue systems and so on but I just skipped them due to that time limitation okay so next the I will talk about speech databases to work on this kind of our core speech problems we have to have our database</p>
</section>
<section id="speech-variations-speaking-styles-and-environments" class="level2">
<h2 class="anchored" data-anchor-id="speech-variations-speaking-styles-and-environments">Speech variations Speaking styles and environments</h2>
<p>and the speech as I mentioned has a variations a lot of variations right and the there are many axes but mostly we separate classify the speech database with the speaking styles and the environment and so on, but I will just talk about the speaking styles for now so one style is wet speech and the other is spontaneous or non-bit speech, and I will explain how it is different maybe I can play some of them, so this one is the first one versus journal this is one of the most famous speech recognition corporates so if you guys have some cool idea for the speech equation techniques and the the showing the kind of other improvement in this abortion channel you guys can write the paper so it is a bit so the game is small, but it is a so-called red speech and the why it is called versus journal people are leading the world’s regional standards so that the the disco pacifi named work to each other switchboard this is an example of spontaneous speech. I think it is obvious that the second one it’s more difficult, right, but the second one is more the natural conversation and the third one this is the corpus called liberty speech and I mentioned that wall street just now is one of the most famous speech corporal, but now that this liberal speech is the most famous the most frequently used the corpus why it is used so popularly this is simply because the license is quite flexible wall street journal has a kind of a little bit limited license while river speech doesn’t have so much restriction for the license and also the amount of data is also quite large thousand hours so that’s the reason that people are using the legal speech so okay I will try to explain a bit more about what is red speech I kind of mentioned already about some of them so that speech is usually corrected as follows first other we have some sentence showing in the prompt, and then we can just read this prompt which are corresponding to we get the paradigm of the prompt and corresponding speech data that’s it right it’s very easy I believe i prepared this how many people knows this the the web page this kind of activity come on boys this is why widely the the used the speech the the data collection scheme led by the modular and this actually has tons of the multilingual speech resources now so if you guys want to get some speech data the one of the first phrase you will check is this common voice and this is a common voice the correction is based on the red speech prompt difficult for example if you click this one it’s working but she has also served as chair of the board of bomb health new zealand oh yes he has also served as jail the board of bomb health new zealand and the next prop this comes and I will speak that it is very cool system right and then we can actually create a lot of languages and again this supports many many languages now I was super happy when you know they started to support japanese and I spent you know half day just speaking probably many of the my voices are using this are the database however be careful people sometimes don’t speak correctly how to other than deal with that they actually are not only recording they actually hover oh sorry maybe I can reload it this is a bit difficult because I have to we also have another action listen this actually confirms whether the recorded speech is correct or not and I I haven’t tried this but I am only speaking in my Japanese so not sure if you know correctly you know they recognize my japanese and they registered it those are the useful corpus or not so this is the that speech</p>
</section>
<section id="read-speech-examples" class="level2">
<h2 class="anchored" data-anchor-id="read-speech-examples">Read speech examples</h2>
<p>so again this is easy to connect but I have a lot of experience that the people don’t speak correctly, so we actually need to check whether they are speaking correctly and also easy to anonymize compared to the spontaneous speech because it is prepared sentence it is not connected with the speaker’s identity, so if the speaker there is completely anonymized that we don’t care about it, but the problem is that let’s speech is not a real conversation, so this is a very good spot to starts to build a speech function system but if I put it in in this system in the conversational scenario it will not work so well</p>
</section>
<section id="spontaneous-speech" class="level2">
<h2 class="anchored" data-anchor-id="spontaneous-speech">Spontaneous speech</h2>
<p>so instead we also used a spontaneous speech, and this one is very tough. It’s actually hard to transcribe actual recording yeah again i prepared an example by the way, how many people use this to own that city did you know that audacity is developed here cmu yeah again you guys are very lucky cmu can provide any of the other tools for other speech and the nlp research and the I think I’ve prepared the wait a moment the way to transcribe the speech the situation speech I think, okay oh, I actually put the latest rider give it a moment my computer is not working now. Okay, now it’s working, so if I have more skills you know switch your screen and so on I can do some demonstration in front of you, but it’s very difficult to do it, you know, by checking the other back monitor and this monitor so instead i will just upgrade the play the audio a play the video of how to transcribe the spontaneous speech check the segment part manually, then type okay check the segment [Music] okay, just to transcribe this I don’t know five seconds of speech it takes a very long time compared with the left speech right so this is quite time consuming and it is quite rather expensive to collect such kind of data like for example this switchboard this is a switchboard corpus which is one of the other famous speech condition coppers, but this is spontaneous speech and the when the in the previous my lecture i actually had a homework to the student to transcribe two minutes of this dashboard conversation I think, yeah the 30 minutes is actually a shorter side most of the people takes random work this is the one of the most honestly biggest complaint of my homework actually since it is very very hard of course, transcribers are very professional, so they can finish it a little bit earlier but still the the spontaneous speech the the recognition and transcript transcription there is a very very difficult I think I need to finish in five minutes or so so this is the the just speech is very fluent but a dear situation is more difficult like for example the in the real situation the meeting scenario or compensation scenario this is a mostly we use for the speech right and then the situation is even more difficult As I mentioned last in the speech enhancement demonstration, we have background noise we have our interference speaker, and we have a reverberation and so on but this is kind of our again our usual the everyday conversation scenes right and we can easily recognize this kind of sound speech while the alexa and so on I guess you guys have like some other experience just you know, putting it to the the, and then we started to conversate it they actually cannot be recognized well they even don’t know that of each other person they recognize and so on it’s very difficult problem but again human can do it easily and if I am actually working on this area a lot so not only for the spontaneous speech but also in the kind of real everyday conversation scenario and the project is called chime project and this is we actually collected such kind of a very the adverse the the environment of this collected speech in the very adverse environment and then the also are organizing the challenge making a baseline and so on I can play some of them so compared with the previous conversation it has a lot of over by the way also noise and so on, but again, this is a natural conversation it makes the speech very difficult so I think yeah</p>
</section>
<section id="where-we-found-the-speech-data" class="level2">
<h2 class="anchored" data-anchor-id="where-we-found-the-speech-data">Where we found the speech data?</h2>
<p>I will finish it in a minute, so there are again a lot of types of the speech and many researchers many other the institutes actually collecting the data for the research and the development for the community and the we can actually the access to such data easily with some of the repository one of the most frequent one is ldc linguistic data consortium where you can actually get famous asr benchmarks like a teammate a world regional switchboard that I mentioned before however as I mentioned that this has some kind of the the restriction for the use we have to pay they do the I get this kind of data but again fortunately cmu is a member so everyone can actually get lgc data really so the for this part you guys would not cover any kind of issue, I believe, and another only for the fdc. There are a a lot of other institutes g,overnment institute or some the university and so on hosting a lot of other speech corporal so you guys can actually get such kind of the speech data through this kind of repository and the second are the bullet items are getting the more popular now the books first open the center of common boys are denoted and the common voice is that I mentioned before and this this data usually have a less restricted license a mostly creative command so that the that we may not go through the LDC and they can easily get the data and even redistribute the data and so on, so due to this reason, recently, many people actually starting to use this data in the box for your common voice and so forth, especially people working on the likewise machine learning and not in the nlp or speech some people may not have an access to the LDC and then people using this kind of repository to get the data, so these are also important resources by the way, these are important resources for assignment three as well, so that’s why I kind of spent some time for the explanation, and the last one is the audiobooks or public recordings with captions youtube actually the ten percent of the youtube videos hazard captions they are set by the the uploader so we can use speech and corresponding captions and the same for the podcast for data token and so on generally, they have some captions so that we can actually use such kind of data and so on, but for this data, maybe you guys also check the license. Sometimes it’s very strict, sometimes not but for most of the kind of research purpose in the academic institute, it shouldn’t be a problem, but the more difficult part is that this recording is required for just one hour and a half of the long recording and corresponding very long the caption so it’s it is just too long for us to use it so we need to correctly segment it and also this is caption is very noisy so it is not easy to use this kind of caption another very important difficulty is that this kind of data is updated frequently so they’re often deleted so we cannot actually others assure the reproducibility if we’re using this one and one of the the famous the databases actually cmu’s wireless the wiredness database game that professor alan black collected and it’s actually has a 700 languages wow that is very cool but it is also already has some issues written here like a bit noisy or API if API is changed it is not easy to get the data and so on, but in general, this is also very good source and the last very last this is important important for the assignment for speech recognition we often use our other unit so please remember I even saw that this is a common the sense for everyone and that they use it without caring about the your prior knowledge so I also want to actually define we often use the hour and, let’s say, more than a thousand dollars, we can make a commercial system but it requires a lot of computational cost 100 hours a speech equation started to be working so we can do the research and so on lesser 100 hour it is categorized as a low resource language in speech recognition so other please understand this range of the data if you try to practice speech recognition and then I would like to pass it to the assignment three but maybe I can accept one or two quick question there any questions yes, like annotating this spontaneous speech could we you first pass it into a voice activity detection: oh yeah segment it and then annotate it yes usually there are such kinds of assistance to a lot of assisted tools. Even we use a pre speech condition and then do it but the voice activity detection is super important so yes that we usually either have such kind of our other voice activity direction but voice activity detection is not working in the noisy environment so much so anyway most of the cases are the human segmentation is also quite important and in question yes definitely we have such kind of other issues since we tried to capture more like a similar closer information and then the the great program for the same gender and then speech separation performance is big world and it’s working very well in a male female and so on so it’s definitely that these kind of things exist but still the current technology almost perfectly separated we almost perfectly separate the speech even in the same gender but it’s a good question okay so maybe that’s it and I want to pass it to this chiang kai and patrick</p>
</section>
</div>
</div>
</div>
<section id="outline" class="level3">
<h3 class="anchored" data-anchor-id="outline">Outline</h3>
<p>Here is a lesson outline about speech, as a markdown unnumbered list:</p>
<ul>
<li>What is Speech?
<ul>
<li><strong>Speech is sound produced by humans for communication</strong>.</li>
<li>Speech is the most natural form of communication and more common than text, even if it is not the most recorded.</li>
<li>Speech can be captured by a microphone as air pressure.</li>
<li>A waveform is created by converting sound pressure into a time series.
<ul>
<li>Waveforms are usually one-dimensional (mono), but can be two-dimensional (stereo) or N-dimensional, using multiple microphones.</li>
</ul></li>
<li>Speech sounds contain information such as transcription and speaker identity.</li>
</ul></li>
<li>Producing and Hearing Speech.
<ul>
<li>Speech is generated from the vocal tract by pushing air from the lungs.</li>
<li>The vocal cords may vibrate to create voiced speech.</li>
<li>Positioning of the tongue, teeth, and lips controls the sounds produced.</li>
<li>The ear drum vibrates in response to air pressure vibrations.</li>
<li>The cochlea decomposes vibrations into frequency components.</li>
<li>The auditory nerve transmits signals from the cochlea to the brain.</li>
</ul></li>
<li>Phones and Phonemes.
<ul>
<li>Phones are segments of linguistic space; changing a phone can change the word.</li>
<li>The definition of phones can vary and can be hard to define exactly due to variations in how people speak them.</li>
<li>The International Phonetic Alphabet (IPA) is a structured way to define phones, accounting for ambiguities and variance.</li>
<li>The place of articulation of a consonant can range from the lips to the back of the throat.</li>
</ul></li>
<li>Computational Considerations.
<ul>
<li>The need to understand the different positions of the vocal tract.</li>
<li>The importance of decomposing signals into frequency components, as the ear does.</li>
<li>Digitizing speech requires sampling it at least 8,000 times a second.</li>
</ul></li>
<li>Speech Technologies.
<ul>
<li>Speech recognition (speech to text).</li>
<li>Speech synthesis (text to speech).</li>
<li>Speaker identification.</li>
<li>Diarization to determine who said what.</li>
</ul></li>
<li>Applications of Speech Technologies.
<ul>
<li>Voice conversion.</li>
<li>Speaker recognition.</li>
<li>Language recognition.</li>
<li>Speech emotion recognition.</li>
<li>Speech coding.</li>
<li>Speech perception.</li>
<li>Speech enhancement.</li>
<li>Microphone array processing.</li>
<li>Audio event classification and detection.</li>
<li>Speech separation.</li>
<li>Spoken language understanding.</li>
<li>Spoken dialogue systems.</li>
<li>Speech translation.</li>
<li>Multimodal processing.</li>
</ul></li>
<li>Data Considerations.
<ul>
<li>Style of speech.
<ul>
<li>Careful speech is easier to process than casual speech.</li>
<li>Speaking to a machine is different than speaking to a human.</li>
</ul></li>
<li>Channel and context.
<ul>
<li>Microphone quality and placement.</li>
<li>Background noise.</li>
</ul></li>
<li>Data repositories.
<ul>
<li>LDC.</li>
<li>Vox Forge.</li>
</ul></li>
<li>The amount of available data varies significantly by language.
<ul>
<li>Thousands of hours of speech are needed for significant results.</li>
<li>Low-resource languages may have limited data.</li>
</ul></li>
</ul></li>
<li>Differences Between Speech and Text.
<ul>
<li>Speech and text are different and may be considered dialects of each other.</li>
<li>Many languages have strong distinctions between spoken and written forms.</li>
<li>Social media is blurring the line between written and spoken language.</li>
</ul></li>
<li>Speech Hierarchy.
<ul>
<li>The speech hierarchy consists of speech, applications, databases, and levels of the hierarchy.</li>
</ul></li>
<li>Variations in Speech.
<ul>
<li>Speaking styles and environments affect speech.</li>
<li>Read speech involves reading prepared sentences.</li>
<li>Non-read speech is spontaneous and requires transcription.</li>
</ul></li>
<li>Data Collection.
<ul>
<li>Data can be found in LDC, ELRA, Voxforge, and CommonVoice.</li>
<li>Audiobooks and public recordings with captions can also be used.</li>
</ul></li>
</ul>
</section>
<section id="papers" class="level2">
<h2 class="anchored" data-anchor-id="papers">Papers</h2>
<p>Here is a list of papers mentioned in the lesson:</p>
<ul>
<li><span class="citation" data-cites="kim2017jointctcattentionbasedendtoend">Kim, Hori, and Watanabe (2017)</span> <a href="https://arxiv.org/abs/1609.06773">Joint CTC-Attention based End-to-End Speech Recognition using Multi-task Learning</a></li>
<li><span class="citation" data-cites="baevski2020wav2vec20frameworkselfsupervised">Baevski et al. (2020)</span> <a href="https://arxiv.org/abs/2006.11477">wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations</a></li>
<li><span class="citation" data-cites="hsu2021hubertselfsupervisedspeechrepresentation">Hsu et al. (2021)</span> <a href="https://arxiv.org/abs/2106.07447">HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units</a></li>
<li><span class="citation" data-cites="chang2021explorationselfsupervisedpretrainedrepresentations">Chang et al. (2021)</span> <a href="https://arxiv.org/abs/2110.04590">An Exploration of Self-Supervised Pretrained Representations for End-to-End Speech Recognition</a></li>
<li><span class="citation" data-cites="hershey2016deepclustering">Hershey et al. (2016)</span> <a href="https://ieeexplore.ieee.org/abstract/document/7471631">Deep clustering based speech separation</a></li>
</ul>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-baevski2020wav2vec20frameworkselfsupervised" class="csl-entry">
Baevski, Alexei, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. 2020. <span>“Wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations.”</span> <a href="https://arxiv.org/abs/2006.11477">https://arxiv.org/abs/2006.11477</a>.
</div>
<div id="ref-chang2021explorationselfsupervisedpretrainedrepresentations" class="csl-entry">
Chang, Xuankai, Takashi Maekaku, Pengcheng Guo, Jing Shi, Yen-Ju Lu, Aswin Shanmugam Subramanian, Tianzi Wang, et al. 2021. <span>“An Exploration of Self-Supervised Pretrained Representations for End-to-End Speech Recognition.”</span> <a href="https://arxiv.org/abs/2110.04590">https://arxiv.org/abs/2110.04590</a>.
</div>
<div id="ref-hershey2016deepclustering" class="csl-entry">
Hershey, John R., Zhuo Chen, Jonathan Le Roux, and Shinji Watanabe. 2016. <span>“Deep Clustering: Discriminative Embeddings for Segmentation and Separation.”</span> In <em>2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 31–35. <a href="https://doi.org/10.1109/ICASSP.2016.7471631">https://doi.org/10.1109/ICASSP.2016.7471631</a>.
</div>
<div id="ref-hsu2021hubertselfsupervisedspeechrepresentation" class="csl-entry">
Hsu, Wei-Ning, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. 2021. <span>“HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units.”</span> <a href="https://arxiv.org/abs/2106.07447">https://arxiv.org/abs/2106.07447</a>.
</div>
<div id="ref-kim2017jointctcattentionbasedendtoend" class="csl-entry">
Kim, Suyoun, Takaaki Hori, and Shinji Watanabe. 2017. <span>“Joint CTC-Attention Based End-to-End Speech Recognition Using Multi-Task Learning.”</span> <a href="https://arxiv.org/abs/1609.06773">https://arxiv.org/abs/1609.06773</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Speech},
  date = {2022-03-01},
  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w13-speech/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Speech.”</span> March 1, 2022. <a href="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w13-speech/">https://orenbochman.github.io/notes-nlp/notes/cs11-737-w13-speech/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Multilingual NLP</category>
  <category>NLP</category>
  <category>Notes</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w13-speech/</guid>
  <pubDate>Mon, 28 Feb 2022 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Speech</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w15/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/tiling.png" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>course banner</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/Hsdf8Vjai5g" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Lesson Video
</figcaption>
</figure>
</div><div id="sup-slide-deck" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div>

<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>What is speech?</li>
<li>Speech applications</li>
<li>Speech databases</li>
<li>Speech hierarchy</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- Transcript here is machine generated no punctuation and I eddited it a but but it needs more work and I dont see the benefits unless I can run it through some tool. -->
</div>
</div>
</div>



<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Speech},
  date = {2022-03-01},
  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w15/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Speech.”</span> March 1, 2022. <a href="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w15/">https://orenbochman.github.io/notes-nlp/notes/cs11-737-w15/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Multilingual NLP</category>
  <category>NLP</category>
  <category>Notes</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w15/</guid>
  <pubDate>Mon, 28 Feb 2022 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Typology: The Space of Languages</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w03-typology/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/tiling.png" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>course banner</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/iYmE6UCiOSQ" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Lesson Video
</figcaption>
</figure>
</div><div id="sup-slide-deck" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div>

<blockquote class="blockquote">
<p>We’ve always sent dances to tiktok as a way to communicate and I can’t remember when when we ever did anything else – Graham Neubig</p>
</blockquote>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>How to quantify similarity between languages</li>
<li>Language families and genealogical similarity</li>
<li>Linguistic typology and typological similarity</li>
<li>WALS and other typological databases</li>
<li>Typology Prediction / Typology-based language transfer (Lin et al.)</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- Transcript here is machine generated no punctuation and I eddited it a but but it needs more work and I dont see the benefits unless I can run it through some tool. -->
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<blockquote class="blockquote">
<p>I’m gonna run through this we have about half an hour i’m gonna talk about all languages on the planet basically not not not too big and of course there are a lot of languages and the question really is how can we try to structure that space we could look at them directly but we’d like to be able to gather them into groups or similarities and that’s where we come up with a more organized structural way and we call this linguistically and typology and so there is official ways to try to find out similarities between language and that’s what we’re going to talk about in this in this form so let’s have a look so first we’ve got all the languages in the planet we have a dot here and for each of the languages now this is very hard to do because languages are not point they’re spread over multiple places and of course they’re spread over multiple places that will overlap as well but you can see something about here that there are some places that seem to have more languages per you know square inch than others and it seems to be the the richer countries the ones where there’s a common education system travel is very easy and they have a long history of fighting each other with borders you’re more likely to find languages that cover whole what would we we call countries while when we’re looking at places that are been around for a very long time but might not have the same definition of border and there’s more languages and sometimes these languages are related to each other and sometimes they’re not sometimes they’re from rival groups and sometimes they’re just not related like korean is not related to any of the other languages in the area and there’s some relations to japanese but it’s really the japanese and korean are not like anything else and therefore they’re more like each other but only because they’re not like anything else in the area at all even though both of them have got some substantial chinese influence with words and english influence as well what’s the definition definition of a language well that’s sort of quite hard and there’s different definitions and we basically are coming down to whoever defines it but of course it become can become quite political we’ve talked about these things before urdu and hindi different languages well it depends what you’re talking about from a political point of view unquestionably from a linguistic point of view yeah they’re sort of different from a phonological point of view they’re not very different at all they really pretty much overlap and we have lots of examples like that</p>
</blockquote>
</section>
<section id="defining-languages" class="level3">
<h3 class="anchored" data-anchor-id="defining-languages">Defining Languages</h3>
<blockquote class="blockquote">
<p>There is the standard joke but it’s a good joke and it’s very relevant and that is “a language is a dialect with an army” and there certainly is political decisions when it comes to defining what the border actually is between two languages and often it’s quite weak between two languages and people can float between one and the next so depending on the area depending on the</p>
</blockquote>
<p>history depending on the politics depending to on the historical aspect of ethnic groups and travel etc defining what the languages actually are is still pretty hard but say in the in india there is something like about 460 languages which is a lot officially from the government i think it’s 21 if i remember correctly and which is about the same size as what europe is when it comes to official languages but of course as you look closer you discover that you start getting distinctions between languages which might be relatively small but they might be very big to the extent that these people absolutely can’t understand each other but there may be many people who speak multiple languages remember it’s really only america and britain where people only speak one language and only one language so it’s pretty rare actually in the world that people are only speaking one language</p>
</section>
<section id="language-families" class="level3">
<h3 class="anchored" data-anchor-id="language-families">Language Families</h3>
<blockquote class="blockquote">
<p>There are different language families and so linguists have decided looking at languages especially looking at things like where choice and a word overlap which might vary between languages but there might be relationships between languages that we can see language families that are actually sharing information like lexicons so the choice of the word the grammatical aspects the morphology act actually even though there could be differences they might be more similar and so across africa we can see something like regions of about six different major language families and you know we can see that geographical aspects madagascar is different but that’s because you know there’s a big sea between that in the mainland and that actually makes less chance for interaction to happen and therefore it’s sort of easier to keep that distinction so you find sometimes that languages have got borders that are quite geographical mountains rivers and the sea of course ### Online Languages</p>
</blockquote>
<blockquote class="blockquote">
<p>How many of these languages are online we’re all very aware that both you know english european languages chinese japanese korean are very well represented online and how many people within the country are actually getting online nowadays often many of the richer people are online and they can be quite substantial amounts of data online but also when you look at a major areas of wealth that have computers that have internet often there may be mixed languages and there may be a preferred language you often discover that in india even though there are lots of different languages in india that are native that people who are online are often using english or they’re writing in the romanized form rather than their native script because well history etc it’s easier they’re fluent in english there’s a lot of english influence that’s actually there china almost everything is written in putonghua stands mandarin even though there are many other dialects there so there’s not the same representation across everywhere because it depends on whether these people have access and that they’re willing to talk in these languages often they consider these to be spoken only or rarely written and so their language of literacy is some colonial language be that english and spanish french etc or swahili or arabic but it might be that it’s just easier for them to do that and therefore there’s a tradition to be able to communicate in one of the standard larger languages on the planet rather than their own language and so when we’re trying to do multilingual nlp we’re actually caring about these smaller languages that may not be as much online okay so how do we try to find similarity</p>
</blockquote>
</section>
<section id="similarity" class="level3">
<h3 class="anchored" data-anchor-id="similarity">Similarity</h3>
<blockquote class="blockquote">
<p>between languages well obviously the the clearest thing is what we can do is we can start looking at the words and so we can see whether there’s a shared information and we usually look for words not like iphone and computer that are relatively modern we look at words that have been in the language for a long time so this is often body parts family relationships core food aspects water air etc and there’s actually a specific list called the swabish list which you can be often used to compare similarities between languages now you want to be a little careful about caring about the writing systems so often that there’s a writing system that writes things in a very different way but you may discover that there’s actually a shared information between the language in the phonetic form that’s not there in the written form and therefore you can’t just use string match to find out whether it actually works or not there’s a number of online groups that try to identify all of the languages on the planet ethnologue is one of the best to try to do that it actually comes out the summer institute of linguistics which is a religious organization which is caring about translation of the bible but they’re quite independent in in doing their list mostly and it’s definitely the most comprehensive list in in the world glottalog tries to do a similar thing but it also is identifying this the position of the language on the planet and that’s not the only influence to similarity because people get on boats and cross oceans and we’re currently in the united states and we all know in this particular area english is not native at all and french is native no no french was there before english was there but there was earlier indigenous languages definitely iroquois and possibly a language called mingo is somewhat native and though the name allegheny is probably a mingle word for probably it’s a word for river maybe northern river because it’s the northern one in the obvious three rivers but glottalog tries to do this but also gives downloadable spreadsheets that allow you to be able to do a in other aspects now remember people move i mean huge populations have moved around the planet which has influenced how languages have moved and also trade has done a lot to have shared aspect of languages as well as grammatical history or linguistic history of languages and we want to be careful about that when we look at that distribution you know one might think that if you go to take the example of korean there are lots of english words in korean they have a different pronunciation from the english but they’re clearly derived from english but that doesn’t mean that there’s any linguistic relationship between korean and english it’s much more to do with english being the international language of the last hundred years and korea has picked up many of these more modern words into its language because that’s a convenient way to do it</p>
</blockquote>
</section>
<section id="genealogical-similarities" class="level3">
<h3 class="anchored" data-anchor-id="genealogical-similarities">Genealogical similarities</h3>
<blockquote class="blockquote">
<p>genealogical similarities so this is the language family and this is usually divine defined by linguists who make decisions about looking at the linguistic properties historically we used to do this in the animal kingdom before we could do dna tests and there are interesting errors in the dna tests for animals that two animals have come from different family and moved together because that’s a convenient co-evolution to end up with a different way but actually they’re quite got different histories but that might not be obvious and that’s going to happen with languages as well where things end up being borrowed maybe things get simplified over time we can see some examples of this but things also get more complex over time and there’s lots of interesting and boring from some of the major families that are out there niger congo in africa has got a lot of different languages and covers 21 of the languages spoken on the planet that’s a lot okay well if we look at something like indo-european that covers most of europe not all of europe and through the middle east at least the northern middle east m iran and into northern india and that’s a lot of people okay but it’s only about 6.3 of the languages because many of these languages are spoken by a very large number of people so you’re going to get very large numbers of people and also some of the links between languages that although you know german and english have lots of common lexical items that people can sort of work out and if they know english or they know german and be able to work out what the other one is but sometimes it’s not immediately obvious that there’s a relationship between the languages and very few of the words are actually overlapping lithuanian which is sometimes identified as the one that’s most archaic in the sense of it’s got more of the history of the original part of Indo-european and so does english but the relationship is not obvious at all to an english speaker</p>
</blockquote>
</section>
<section id="typological-similarities" class="level3">
<h3 class="anchored" data-anchor-id="typological-similarities">Typological similarities</h3>
<p>how do you work out these typological similarities well this is one of the major things that linguists have been doing for a long time and they’ve been looking at ways of linguistic properties to try to see what the similarities are between them and there’s a number of books and studies that try to collect that information together from multiple research studies okay now when we’re looking at phonology so the actual pronunciations the ipa is an excellent way for being able to split down the possible ways that most languages on the planet actually do their pronunciations and we have a vowel space that’s continuous vowel space and we split each language splits it into different ways and there’s often drifts between different languages that are maybe even predictable for consonants and things which are not vowels that’s probably the best definition of them there’s lots of things about a place of articulation in manner of articulation places where we put constrictions from the front of the mouth down to the back of the throat and we can sort of have things that deal with the lips with things like p and b things that deal with the teeth things like tea and things that deal with just behind the teeth which are things like okay and all of these may have different variations depending on the a on the languages that we’re actually speaking and have different distinctions in english we may produce some of these but we don’t make distinctions between them but for example a korean has got three different p’s that most english speakers would not distinguish between so when we’re looking at similarity between languages we could look at the similarity in the phonology and how many phonemes actually are common between the different languages now there’s actually a group called</p>
</section>
<section id="walls" class="level3">
<h3 class="anchored" data-anchor-id="walls">Walls</h3>
<p>walls now walls is also on a collection of all of these different typographic variations that are a over all of the languages and basically back in the early 2000s a group of people tried to start collecting papers and showing what the similarities so for the most part you can go to the walls and a website you can select some particular feature and you can see the distribution here we’ve got the distribution of the planet of different number bases so we all count in all languages some more than others and sometimes we use decimal and it’s probably related to the fact the number of fingers that we have but some count in twenties and that’s not really unusual even in english we have some residual twenties that’s there and even in chinese there’s some religious 20-ness where we have a specific word for 20 in english it’s score and here’s a mapping of all of the languages now we’re not saying that these languages are related to each other we’re saying that when we look at numbers all of the languages with the blue dot are counting in decimal basically well those for example that are in the purple pinkish dot are counting in twenties across and some don’t have good ways of doing that at all now walls is this excellent detailed form where you can go through and find different things you can find out which languages refer to t as t and which refers to it is chai which is quite interesting in itself some of them are maybe a little bit light-hearted and some of them are quite detailed like for example a word order or a default word order of in english we have a subject verb object japanese is a subject object verb now historically walls originally came in a book okay and this book is what’s called really really big and i’m sorry i had to take it from underneath my monitor because it normally keeps my monitor at the right level but the book of course isn’t updated but the website is and over the years the website gets more and more and people now actually think about registering the piece of work that they’re doing to be able to cover what’s actually there now not everything is in walls because not all of the features have been studied in all of the languages most linguists are going to study something that’s interesting so if they’re interested in something like voicing inconsonants after long vowels that’s only going to be interesting in some languages and other languages there’s just nobody’s going to study that and so the question is can we actually predict the missing feature from other factors because often there’s information that’s in there so for example in linguistics default word order seems to be less fixed when you have more morphology and that’s something to do with if you’ve got morphology it allows you to be able to identify who did what to whom better and therefore you don’t need to care about word order in the same importance level so there’s some predictable things so if somebody tells me something about a language i don’t know and says it’s a really rich morphology i i’ll think maybe it’s got free word order that’s not true for everything but there’s a more likelihood that it does if something doesn’t have lots of morphology it probably has more fixed order and can we learn this from the data by looking at all of the features all of the languages find the missing ones make predictions hold out the ones we do know and it may allow us to be able to make these predictions and lots of people have tried to do that at various levels and being quite successful and in fact the paper that we asked you to look at already has looked at some how well some of these predictive things actually work and of course some of them work better for some aspects than others okay sometimes you can do it purely unsupervised some sometimes you want to have supervised learning to be able to do this sometimes you want to make these predictions and go and explicitly ask somebody to to ask you ask you whether it’s correct or not.</p>
</section>
<section id="typological-databases" class="level3">
<h3 class="anchored" data-anchor-id="typological-databases">Typological databases</h3>
<p>There’s a number of these typological databases out there walls is only one it’s quite good it’s quite famous and there’s a number [Music] been derived from those or derived from multiple ones especially to fill in particular aspects of the features and these can be really useful in trying to do things when you’re doing multilingual modeling because you want to know maybe these features make a difference on my downstream or my predicted prediction task and i like to be able to get these features I’d like to have reliable features i’d like to know when the features are confident when the features are missing when the features are going to be more important than others so that maybe I look harder to be able to find these features here at cmu a number of years ago we had</p>
</section>
<section id="lorelai" class="level3">
<h3 class="anchored" data-anchor-id="lorelai">lorelai</h3>
<p>A project called lorelai which we’ll mention a few times in this course where we actually tried to build a specific vector that tries to represent a language so lang to vec so given the name of the language given the code of a language we’ll give a vector representation that will try to identify all of the aspects that would be relevant for feeding in as a prior when you’re building various language models okay there was a bunch of work done on this to be able to do this basically both building the vector and also trying to predict and and across that there are still people doing phd’s graham is still very much working in that way and aditi is our phd is very much in that space</p>
</section>
<section id="universals" class="level3">
<h3 class="anchored" data-anchor-id="universals">Universals</h3>
<p>are there any unit universals that are features that are there for everything for all possible languages and the answer is yes mostly and sometimes there’s a little bit caveats around the edge and so for example all languages do seem to have vowels and consonants but the definition that the boundary for vowels and consonants isn’t very good so that’s might be an easy thing to fulfill but it seems to be true given that we’re all using the same vocal tract that we are actually trying to do that almost all languages have got nouns and verbs now there’s some languages where the distinction isn’t very strong and it’s morphological variants that allow you to be able to distinguish between it but pretty much everything has nouns and verbs now once you get the adjectives the next major class that’s not so clear and we end up with a number of languages that will use nouns as adjectives maybe with some morphological variation in english you can get away with quite complex nouns being used as adjectival forms more so in american english than in british english and but that’s sort of moved to that we get quite complex noun compounds which are really some form of adjectival form and that’s true across a number of languages now there’s other things that are very common across multiple language families or related languages that are relatively interesting and identifiable and there are multiple non-related languages that will do that you know species between words in the written form a morphology that’s segmental so we’re joining things together as opposed to templatic morphology where you have maybe a bunch of different consonants and the vowels change inside it arabic and hebrew and a number of other northern african and languages have that that are not necessarily all in the semitic language family and so there’s a number of things that are relatively common but they’re not going to be everywhere but you know there’s also things about you know if language is distinguished between voiced and unvoiced they don’t always they’re going to care more about the voice than the unvoiced ones how do we deal with the low repo low</p>
</section>
<section id="low-resource-languages" class="level3">
<h3 class="anchored" data-anchor-id="low-resource-languages">low resource languages</h3>
<p>resource aspect of this how can we actually find out how if we were given a language and we maybe only got a few features for it how can we predict other things for it well we can look at all other languages that are similar and we can find out well most languages of these features have got those features and we might also say for that language what’s the most related language and we’ll just say well let’s assume all these features it might not be true but it’s probably better than just assuming everything is english because everything is certainly not english okay and the number of different groups and throughout the world have been studying this is quite a major area of looking at how to be able to predict or get other features for low resource languages when you don’t have data that’s there now there’s a number of different ways of doing such multilingual nlp what you can do is you can say well i’m going to try to find a close by language and i’m just going to assume everything about that and maybe remove things that are not appropriate and often that happens so imagine that you come to europe and there’s this island nation that separated itself from the rest of europe pretending that it’s not part of europe at all and nobody knows anything about english and eventually somebody braved the channel and gets to england how might they understand english and the answer is well it seems sort of germanic-like so let’s just pretend it’s german or dutch which is even closer and then just use everything that we have from the dutch language to apply it to english and maybe train a little from that and you would get much further with that than if you took chinese or hindi or maybe even french although a lot of the words are borrowed from french and but the grammar is very much germanic in english so we take another language and try to do things so what we’re doing is taking an existing model and then trying to fine-tune it for the target for okay another way to do this is to try to take</p>
</section>
<section id="multilingual-birth" class="level3">
<h3 class="anchored" data-anchor-id="multilingual-birth">multilingual birth</h3>
<p>all languages or all languages in some language family and what we would then do is we would then train everything together in some multiple joint way there’s lots of different ways of doing that and then we would have a model that was multilingual in a true sense that’s actually how multilingual birth is done rather than having different births from different languages and then doing adaptations of the target one we actually build a multi-lingual bar and then we’ve got this multilingual thing so it’s sharing some information about all of these languages and that might make it easier when you’re doing adaptation to the target one form and that is an open question okay it probably depends on the amount of training data and the amount of languages and how close it is and whether it’s going to be similar or not to these other languages when you’re doing that whether you have a writing system that’s the same whether it’s different whether phrenology is different and all these things are going to be important so there won’t be one answer for everything but you should be aware of the different ways of actually trying to do that okay why do we care about typology at</p>
</section>
<section id="why-typology" class="level3">
<h3 class="anchored" data-anchor-id="why-typology">why typology</h3>
<p>all can’t we just well we’re going to train from everything and the answer basically is for most low resource languages you just don’t have enough data and even if you pretend that you have enough data it’s been shown in english the more data you have the better your models are going to be now in general in machine learning the more structured data you have the easier it is to be able to learn things so if you have external data to help you when you’re dealing with small amounts of data it will usually not always but it will usually learn better so knowing about the default word order knowing whether morphology is an issue or not knowing the types of grammatical structures the types of verb structures the types of noun structures or their noun classes or their politeness these will potentially help you when you’re building your model to be able to get better results quicker okay</p>
</section>
<section id="how-to-choose-a-transfer-language" class="level3">
<h3 class="anchored" data-anchor-id="how-to-choose-a-transfer-language">how to choose a transfer language</h3>
<blockquote class="blockquote">
<p>How do you choose a transfer language? Well often what people will do is they’ll go and ask some a knowledgeable person about which language is close and you’ll get an answer but that might not always be the right answer and there was a bunch of work done here a few years ago and trying to look at that for particular tasks like translation and there’s non-trivial aspects of actual similarity of the language reliability of the language the amount of data that you have and whether the data on the domain of the data that you have is appropriate is it conversational is it newspaper text is it bible text and the linux all people actually did quite a lot of that to try to do it i remember the sort of default answer for that i am right about this i think it’s this one and said turkish by default is the best one over everything and that’s sort of probably because turkish is fairly well resourced it’s influenced by a lot of different other language families so it has arabic it has english it has iranian and hindi farsi sanskrit and things in it and it spreads over there’s a vast amount of the world going from europe all the way into turkic languages all the way into china open research problems that we have.</p>
</blockquote>
</section>
<section id="open-research-problems" class="level3">
<h3 class="anchored" data-anchor-id="open-research-problems">open research problems</h3>
<blockquote class="blockquote">
<p>How to extract typological features automatically so if you give me a language can i find out the default word order that’s sort of hard i mean we’re gonna get some there but but when the it’s not obvious and maybe it’s different in written form compared to in the spoken form and therefore you have to be able to care for that but there is this thing called the universal dependency tree bank that originally came out of google and so there are these existing toolkits and data sets which try to give this information for many of the major languages and some of the minor language as well and these resources are hard to do yourself and therefore it’s always good to know about them and to be able to build on top of them okay there’s lots of other things that are</p>
</blockquote>
</section>
<section id="multilingual-aspects" class="level2">
<h2 class="anchored" data-anchor-id="multilingual-aspects">Multilingual aspects</h2>
<p>out there if you want to learn about morphology or phonology you can look at multilingual aspects in the in the computational linguistics and conferences there’s lots of geographical groups that are looking at say specifically looking at indian languages african languages there are lots of things that are looking at low resource languages there are lots of ones looking at interesting morphology languages and so often it’s worth looking at and though everybody who doesn’t know a language thinks i’ll just train from an infinite amount of data the answer is well you won’t have an infinite amount of data and sometimes it’s quite hard to find data and if you discover that morphology is rich in the particular language it might be worth doing morphological segmentation and there may already be an existing morphological analyzer that’s there or at least help to be able to find that okay so that’s a very quick view of typology on how we actually structure languages and it’s becoming more and more important in the computational form than what it was been before 10 15 years ago you’d see less papers about it but now people are really caring about it because we are doing much more multilingual work you were asked to read this particular paper which was a survey paper on looking at aspects of typology across different people trying to do predictions and how well they were actually doing and the issues that are involved in this and what we’re going to do now is we’re going to split you off into groups and in those groups you’ll have a ta or an instructor will be there and what we want you to do is we want you each of you to identify things which are unique or very rare compared to other languages that are important over the languages you know in distinct from things which are not very interesting maybe whole classes of languages that are unrelated are all using a romanized form and to write them but they’re not related but things that are going to be unique from that point of view now a has someone set up the groups graham have you set up the groups have maybe we could take some questions if people had questions yes sure yes thank you 22 indian languages yeah yeah so what language you write things in is quite interesting and especially once you’re in a code switching space in india almost everybody when they’re code switching will write in a romanized form they’ll often call it english but it’s not english it’s the romanized form so they’re writing both hindi in a romanized form and english in romanesque form while when you look at singapore for example where people can be as fluent in chinese and english they actually use hansi for writing chinese and english for writing english words most of that’s got to do with input method actually it’s like how easy is it to type these things on a computer and for historical reasons actually partly because there was less Chinese speakers who spoke english the Chinese input systems became better while in india for the past 200 years the educated elite were all English-speaking and therefore they were used to reading and writing in using romanized m form probably that’s got something to do with it but definitely information is lost when you may be using a non-native script and for example spelling goes on completely out of the way in English when you’re doing it but remember most written most scripts are not appropriate for the language we are using a latin script for a germanic english in english. We use Kanji in Japanese for writing lots of things and yeah there’s other scripts in Japanese for dealing with more native things. Hangul is native in Korea but there’s still lots of Chinese borrowed words, especially scientific words, that come from Chinese. So often the writing system even for the native speakers is not very appropriate. Often it’s just convention. It’s like this is the way we write it and we’ve always written it. And it was only since last year that people live but they think it’s facts that we’ve done it forever. You know basically we’ve always sent dances to tiktok as a way to communicate and I can’t remember when when we ever did anything else</p>
</section>
</div>
</div>
</div>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li><strong>Introduction to Linguistic Typology</strong>
<ul>
<li><strong>Typology</strong> is a way to structure and organize languages based on similarities.</li>
<li>Languages are not fixed points, they exist across regions and often overlap.</li>
<li>Language definition can be political, e.g.&nbsp;Hindi &amp; Urdu.</li>
<li>Remember the gag: <mark>“a language is a dialect with an army”.</mark></li>
<li>Acknowledged the difficulty of defining the borders between languages.</li>
</ul></li>
<li><strong>Linguistic Diversity</strong>
<ul>
<li>The world has approximately 7,000 languages.</li>
<li>Some areas have higher concentrations of languages than others.</li>
<li><strong>India</strong> has around 460 languages.</li>
<li><strong>Africa</strong> has an estimated 1,500-2,000 languages from 6 language families.</li>
<li>Most people in the world are multilingual.</li>
<li>The U.S. and Britain are unusual in that many people speak only one language.</li>
<li>Geographical features can create borders between languages.</li>
</ul></li>
<li><strong>Language Families</strong>
<ul>
<li>Linguists identify language families by looking at shared features such as lexicon, grammar, and morphology.</li>
<li>Examples of major language families include:
<ul>
<li><strong>Niger-Congo</strong> (Africa).</li>
<li><strong>Indo-European</strong> (Europe, Middle East, and Northern India).</li>
</ul></li>
<li><mark>Languages within families share common linguistic information.</mark></li>
<li>Some languages, like Korean, may not be related to other languages in their area.</li>
</ul></li>
<li><strong>Identifying Similarities between Languages</strong>
<ul>
<li>Methods for identifying similarities include:
<ul>
<li><strong>Word overlap</strong>: looking at shared words, especially core vocabulary (body parts, family terms, food, water, air). A specific list called the <a href="https://en.wikipedia.org/wiki/Swadesh_list">Swadesh’s list</a> is often used for this.</li>
<li><strong>Phonetic form</strong>: considering phonetic similarities rather than just written forms.</li>
<li><strong>Areal similarity:</strong> geographic proximity and influence between languages.</li>
<li><strong>Genealogical similarity:</strong> language families based on linguistic history.</li>
<li><strong>Typological similarity:</strong> classifying languages based on functional and structural properties.</li>
</ul></li>
<li>Resources such as <a href="https://en.wikipedia.org/wiki/Ethnologue">Ethnologue</a> and <a href="https://en.wikipedia.org/wiki/Glottolog">Glottolog</a> try to identify and classify languages.</li>
<li><a href="https://wals.info/">The World Atlas of Language Structures</a> (WALS) is a key resource for typological data.</li>
</ul></li>
<li><strong>Typological Features</strong>
<ul>
<li>Typology involves classifying languages based on shared formal characteristics.</li>
<li>Examples of typological features include:
<ul>
<li><strong>Phonology</strong>: How languages pronounce sounds; the International Phonetic Alphabet (IPA) helps in comparing phonological systems.</li>
<li><strong>Numeral bases</strong>: Different languages use different counting systems, such as decimal or base-20.</li>
<li><strong>Word order</strong>: Languages may have a default word order such as subject-verb-object (English) or subject-object-verb (Japanese).</li>
</ul></li>
<li><a href="https://en.wikipedia.org/wiki/World_Atlas_of_Language_Structures">WALS</a> provides a database of 192 attributes across 2,676 languages.</li>
<li>Some features may be predictable based on others; for example, languages with rich morphology may have less fixed word order.</li>
</ul></li>
<li><strong>Typological Databases</strong>
<ul>
<li><strong>WALS</strong> is a major typological database.</li>
<li><strong>URIEL</strong> is another typological database which includes phonology, morphosyntax, and lexical semantics.
<ul>
<li><a href="">URIEL</a> has data for 8,070 languages and 284 attributes.</li>
</ul></li>
<li>These databases are useful in multilingual NLP, providing features for language models.</li>
</ul></li>
<li><strong>Linguistic Universals</strong>
<ul>
<li>Most languages have vowels and consonants.</li>
<li>Almost all languages distinguish between nouns and verbs.
<ul>
<li>The distinction between adjectives is less clear across languages.</li>
</ul></li>
</ul></li>
<li><strong>Multilingual Natural Language Processing (NLP)</strong>
<ul>
<li>Typological features can help in multilingual NLP by providing structured data.</li>
<li><strong>Low-resource languages</strong> benefit from typological information due to the lack of available data.</li>
<li><strong>Methods in multilingual NLP:</strong>
<ul>
<li><strong>Cross-lingual transfer:</strong> using a model from a resource-rich language on a resource-poor language.</li>
<li><strong>Zero-shot learning:</strong> applying a model from one domain to another with no extra training.</li>
<li><strong>Few-shot learning:</strong> adapting a model using a few examples from a low-resource domain.</li>
<li><strong>Joint multilingual learning</strong>: training a single model on multiple languages.</li>
</ul></li>
<li>Typological information can be used to select an appropriate transfer language.</li>
<li>The amount of training data and the similarity between languages is important.</li>
</ul></li>
<li><strong>Open Research Problems</strong>
<ul>
<li>How to automatically extract typological features from existing resources.</li>
<li>How to accurately predict typological knowledge while controlling for biases.</li>
<li>How to incorporate linguistic typology into models.</li>
<li>How to alleviate negative transfer in multilingual models using typological knowledge.</li>
</ul></li>
<li><strong>Further Resources</strong>
<ul>
<li>Papers in computational linguistics conferences.</li>
<li>Workshops such as SIGMORPHON, SIGTYP, and AfricaNLP.</li>
</ul></li>
<li><strong>Discussion</strong>
<ul>
<li>Identifying unique typological features in languages.</li>
<li>Considering aspects of phonology, morphology, syntax, semantics, and pragmatics.</li>
</ul></li>
</ul>
</section>
<section id="papers" class="level2">
<h2 class="anchored" data-anchor-id="papers">Papers</h2>
<p>Here is a list of the papers covered in the lesson</p>
<ul>
<li><span class="citation" data-cites="ponti-etal-2019-modeling">Ponti et al. (2019)</span> <a href="https://aclanthology.org/J19-3005/">Modeling language variation and universals: A survey on typological linguistics for natural language processing</a>
<ul>
<li>This paper is a survey on typological linguistics for natural language processing. It is cited as a key resource for understanding how typological information can be used in NLP. The paper also explores modeling language variation and universals.</li>
</ul></li>
<li><span class="citation" data-cites="lin-etal-2019-choosing">Lin et al. (2019)</span> <a href="https://aclanthology.org/P19-1301/">Choosing Transfer Languages for Cross-Lingual Learning</a>
<ul>
<li>This paper discusses how to choose appropriate transfer languages for cross-lingual learning in NLP. It is relevant to the topic of using typological features for low-resource languages.</li>
</ul></li>
<li><span class="citation" data-cites="littell-etal-2017-uriel">Littell et al. (2017)</span> <a href="https://aclanthology.org/E17-2002/">URIEL Typological database</a>
<ul>
<li>This paper introduces the URIEL typological database. It provides information on phonology, morphosyntax, and lexical semantics across many languages.</li>
</ul></li>
<li><span class="citation" data-cites="malaviya-etal-2017-learning">Malaviya, Neubig, and Littell (2017)</span> <a href="https://aclanthology.org/D17-1268/">Learning language representations for typology</a>
<ul>
<li>This paper is related to the <strong>lang2vec</strong> representations derived from the URIEL database and explores how to learn language representations for typology.</li>
</ul></li>
<li><span class="citation" data-cites="georgi-etal-2010-comparing">Georgi, Xia, and Lewis (2010)</span> <a href="https://aclanthology.org/C10-1044/">Comparing Language Similarity across Genetic and Typologically-Based Groupings</a>
<ul>
<li>An example of research in the automatic prediction of typological features.</li>
</ul></li>
</ul>
</section>
<section id="choosing-transfer-languages-for-cross-lingual-learning" class="level2">




</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">Choosing Transfer Languages for Cross-Lingual Learning</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-georgi-etal-2010-comparing" class="csl-entry">
Georgi, Ryan, Fei Xia, and William Lewis. 2010. <span>“Comparing Language Similarity Across Genetic and Typologically-Based Groupings.”</span> In <em>Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010)</em>, edited by Chu-Ren Huang and Dan Jurafsky, 385–93. Beijing, China: Coling 2010 Organizing Committee. <a href="https://aclanthology.org/C10-1044/">https://aclanthology.org/C10-1044/</a>.
</div>
<div id="ref-lin-etal-2019-choosing" class="csl-entry">
Lin, Yu-Hsiang, Chian-Yu Chen, Jean Lee, Zirui Li, Yuyan Zhang, Mengzhou Xia, Shruti Rijhwani, et al. 2019. <span>“Choosing Transfer Languages for Cross-Lingual Learning.”</span> In <em>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, edited by Anna Korhonen, David Traum, and Lluís Màrquez, 3125–35. Florence, Italy: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/P19-1301">https://doi.org/10.18653/v1/P19-1301</a>.
</div>
<div id="ref-littell-etal-2017-uriel" class="csl-entry">
Littell, Patrick, David R. Mortensen, Ke Lin, Katherine Kairis, Carlisle Turner, and Lori Levin. 2017. <span>“<span>URIEL</span> and Lang2vec: Representing Languages as Typological, Geographical, and Phylogenetic Vectors.”</span> In <em>Proceedings of the 15th Conference of the <span>E</span>uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</em>, edited by Mirella Lapata, Phil Blunsom, and Alexander Koller, 8–14. Valencia, Spain: Association for Computational Linguistics. <a href="https://aclanthology.org/E17-2002/">https://aclanthology.org/E17-2002/</a>.
</div>
<div id="ref-malaviya-etal-2017-learning" class="csl-entry">
Malaviya, Chaitanya, Graham Neubig, and Patrick Littell. 2017. <span>“Learning Language Representations for Typology Prediction.”</span> In <em>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</em>, edited by Martha Palmer, Rebecca Hwa, and Sebastian Riedel, 2529–35. Copenhagen, Denmark: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/D17-1268">https://doi.org/10.18653/v1/D17-1268</a>.
</div>
<div id="ref-ponti-etal-2019-modeling" class="csl-entry">
Ponti, Edoardo Maria, Helen O’Horan, Yevgeni Berzak, Ivan Vulić, Roi Reichart, Thierry Poibeau, Ekaterina Shutova, and Anna Korhonen. 2019. <span>“Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing.”</span> <em>Computational Linguistics</em> 45 (3): 559–601. <a href="https://doi.org/10.1162/coli_a_00357">https://doi.org/10.1162/coli_a_00357</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Typology: {The} {Space} of {Languages}},
  date = {2022-02-25},
  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w03-typology/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Typology: The Space of Languages.”</span>
February 25, 2022. <a href="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w03-typology/">https://orenbochman.github.io/notes-nlp/notes/cs11-737-w03-typology/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Multilingual NLP</category>
  <category>NLP</category>
  <category>Notes</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w03-typology/</guid>
  <pubDate>Thu, 24 Feb 2022 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Code Switching, Pidgins, Creoles</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w11-code-switching/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/tiling.png" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>course banner</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/CjcB5nkfLBM" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Lesson Video
</figcaption>
</figure>
</div><div id="sup-slide-deck" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div>

<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Unsupervised MT</li>
<li>Unsupervised Pre-training (LM, seq2seq)</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- Transcript here is machine generated no punctuation and I eddited it a but but it needs more work and I dont see the benefits unless I can run it through some tool. -->
</div>
</div>
</div>



<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Code {Switching,} {Pidgins,} {Creoles}},
  date = {2022-02-22},
  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w11-code-switching/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Code Switching, Pidgins, Creoles.”</span>
February 22, 2022. <a href="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w11-code-switching/">https://orenbochman.github.io/notes-nlp/notes/cs11-737-w11-code-switching/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Multilingual NLP</category>
  <category>NLP</category>
  <category>Notes</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w11-code-switching/</guid>
  <pubDate>Mon, 21 Feb 2022 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Code Switching, Pidgins, Creoles</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w12-qa/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/tiling.png" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>course banner</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/CjcB5nkfLBM" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Lesson Video
</figcaption>
</figure>
</div><div id="sup-slide-deck" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div>

<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Unsupervised MT</li>
<li>Unsupervised Pre-training (LM, seq2seq)</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- Transcript here is machine generated no punctuation and I eddited it a but but it needs more work and I dont see the benefits unless I can run it through some tool. -->
</div>
</div>
</div>



<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Code {Switching,} {Pidgins,} {Creoles}},
  date = {2022-02-22},
  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w12-qa/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Code Switching, Pidgins, Creoles.”</span>
February 22, 2022. <a href="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w12-qa/">https://orenbochman.github.io/notes-nlp/notes/cs11-737-w12-qa/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Multilingual NLP</category>
  <category>NLP</category>
  <category>Notes</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w12-qa/</guid>
  <pubDate>Mon, 21 Feb 2022 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Unsupervised Machine Translation</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w10-unsupervised-NMT/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/tiling.png" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>course banner</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/eT0gxcGKbD0" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Lesson Video
</figcaption>
</figure>
</div><div id="sup-slide-deck" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div>

<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Unsupervised MT</li>
<li>Unsupervised Pre-training (LM, seq2seq)</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- Transcript here is machine generated no punctuation and I eddited it a but but it needs more work and I dont see the benefits unless I can run it through some tool. -->
<section id="intro" class="level2">
<h2 class="anchored" data-anchor-id="intro">Intro</h2>
<p>I’m going to be talking about unsupervised machine translation and this is a very interesting topic overall. If you’ve done the reading, you can see that it’s to a greater or lesser extent practical for some varieties of text-to-text translation, but I think there are a lot of other applications as well, maybe including with speech or other things that we’re going to be talking about in the future. I think the underlying technology is interesting and worth discussing and knowing about both with respect to the techniques and the limitations and other things like this.</p>
</section>
<section id="conditional-text-generation" class="level2">
<h2 class="anchored" data-anchor-id="conditional-text-generation">Conditional Text Generation</h2>
<p>Conditional text generation, which we’ve talked about. Basically, we’re generating text according to a specification like I talked about before in the seq2seq models class. We have our input x and our output y, where it could be for machine translation image captioning summarization, speech recognition, etc. The way we model this is using some variety of conditional language models like seq2seq model, like the ones you’re training for assignment two with our encoder and our decoder, and the way we traditionally estimate the model parameters here is using maximum likelihood estimation to maximize the likelihood of the output given the input, and generally, this needs supervision in the form of parallel data, usually millions of parallel sentences. ## What if we don’t have parallel data?</p>
<blockquote class="blockquote">
<p>What we’re going to ask about in this class is what if we don’t have parallel data, so just to give a few examples of this. We have parallel data? Well, what if we don’t have parallel data? For example, let’s say we have a photo of a person’s face or something like that. We automatically want to turn it into a painting, you know to put on your wall and display or something like that or turn it into a cartoon because you want a picture of yourself for your social media profile in a cartoon or something so, unfortunately, we don’t have tons and tons of data for this but we do have tons of photos and tons of paintings so we have lots of input x and lots of output y but very few pairs of input x and output y we could also do other things like transferring images between genders or between ages or something like this I think you’ve seen apps that might do this text from impolite to polite so you know correcting the formality transferring a positive review to a negative review or vice versa or doing something like machine translation and I actually modified this to give a few other examples like some really but the slides disappeared some really interesting examples are what if we had an ancient language or a cipher where we didn’t actually know what it was we didn’t have any text but we wanted to decipher this old text and replicate and like understand what it meant in the modern language so that’s another thing that we could do with unsupervised translation.</p>
</blockquote>
</section>
<section id="cant-we-just-collectgenerate-the-data" class="level2">
<h2 class="anchored" data-anchor-id="cant-we-just-collectgenerate-the-data">Can’t we just collect/generate the data?</h2>
<blockquote class="blockquote">
<p>Another question is “Couldn’t we just collect or generate data for these tasks”. To some extent, the answer is yes we could for some but it could be too time-consuming or expensive and it can also be difficult to specify what to generate or even evaluate the quality of generations so if we said generate generate this text like Joe Biden said it many people here you know don’t know what Joe Biden sounds like enough to be able to even do this in the first place. You know it’s difficult and under-specified, and finding people who’d be able to do that would be difficult, and because of this, it often doesn’t result in good-quality data sets.</p>
</blockquote>
</section>
<section id="unsupervised-translation" class="level2">
<h2 class="anchored" data-anchor-id="unsupervised-translation">Unsupervised Translation</h2>
<blockquote class="blockquote">
<p>An unsupervised translation the basic idea is we have some seq2seq task you know translation being the stereotypical example but it could be any of the other ones that I talked about where we instead of using monolingual data to improve an existing NMT system trained on parallel data or reducing the amount of supervision we’d like to talk about can we learn without any supervision whatsoever.</p>
</blockquote>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<blockquote class="blockquote">
<p>There are some core concepts in unsupervised MT and these core concepts include initialization iterative back translation bidirectional model sharing and denoising auto encoding and actually from the point of view of unsupervised MT. In some cases people have also used older statistical machine translation techniques instead of neural machine translation techniques because they were more robust.</p>
<p>I’ll talk a little bit about what statistical MT you know means because we haven’t really talked about it here yet and I’ll also explain a little bit about why it is more robust and you know what what we could do to also improve robustness of neural MT as well.</p>
</blockquote>
</section>
<section id="step-1-initialization" class="level2">
<h2 class="anchored" data-anchor-id="step-1-initialization">Step 1: Initialization</h2>
<blockquote class="blockquote">
<p>For step one in initialization, basically a prerequisite for unsupervised mt is that we start out with an initial model that can do something that can do some sort of mapping between sequences in an appropriate way so that we can use it to seed a downstream learning process to do translation and it basically adds a good prior to the state of solutions we want to reach and the way this is done is by using approximate translations of subwords words or phrases usually and the way we take advantage of this is we take advantage of the fact that the context of a word is often similar across languages since each language refers to the same underlying physical world and what we do is we rely on unsupervised word translation.</p>
</blockquote>
</section>
<section id="initialization-unsupervised-word-translatic" class="level2">
<h2 class="anchored" data-anchor-id="initialization-unsupervised-word-translatic">Initialization: Unsupervised Word Translatic</h2>
<blockquote class="blockquote">
<p>I talked about this a little bit two classes ago. I also called it a bilingual lexicon induction, and the basic idea is that word embedding spaces in two languages are isomorphic and what I mean by this is if you take an embedding space from one language like English in embedding space from another language, let’s say, Spanish we can learn some function that isn’t overly complicated that allows us to map between these two embedding spaces so, for example, we might run a model like word to back or any other you know kind of word embedding induction technique to embed individual words and embedding spaces and then we learn a mapping between them like an orthogonal like a matrix transformation w x equals y maybe with some constraints like the w is orthogonal which basically makes the embedding mutually makes the embedding like bijective so you can mutually map between one embedding space and another embedding space and we hope that by applying this transformation we will end up with something where the words in one embedding space are or the words in both embedding spaces if they’re close together the words are similar semantically or syntactically and this is hard to believe that this would actually work. I actually remember going to a presentation in ACL, I think 2016 where this method was proposed and I was like there’s no way this could possibly work because you’re assuming that you embed words and then just transform them in some way and the distributional properties cause them to line up, in fact, it does work better than you would think it would and there’s a couple of reasons for this one reason for this is that in addition to distributional properties a lot of the word embedding techniques that are used in these mappings here also take into account like sub word information and then if you’re mapping between English and Spanish or English and German.</p>
<p>A lot of the words have similar spellings and that can give additional indication about whether the words are similar another reason is like words like gato and cat are both common and you know common words tend to map the common words uncommon words so you’re also basically implicitly using word frequency information in the mapping and frequent words in word embedding spaces often tend to have larger norms because they’re updated more frequently and so that kind of implicitly gets added into this calculation as well so there’s a bunch of things working for this nonetheless it doesn’t work perfectly it works kind of well enough to do something and in the case of initialization that’s mainly what we’re looking for we’re mainly looking for something that you know starts us out in some sort of reasonable space</p>
</blockquote>
</section>
<section id="unsupervised-word-translation-adversarial-t" class="level2">
<h2 class="anchored" data-anchor-id="unsupervised-word-translation-adversarial-t">Unsupervised Word Translation: Adversarial T</h2>
<blockquote class="blockquote">
<p>What if the words in the two domains are not one to one or what if the words in the languages are not one to one. To give an example let’s say we’re mapping English to Turkish where in English a single verb has you know one or five conjugations and then in Turkish it has like 80. The answer is it basically doesn’t work it doesn’t work it doesn’t work very well so that’s another thing that you need to be concerned about but if the morphology is approximately the same it can it can still do something and the later steps of the unsupervised translation pipeline can also help disambiguate like ambiguous words and other things like that so yeah I saw another hand yeah so it’s a it’s a combination of words being frequent and words being words appearing in the same context. So for example just to give an example proper names proper names don’t work particularly well for this method because especially if you have like a Japanese newspaper and English newspaper the proper names mentioned in the English newspaper tend to be English names and the ones in the Japanese newspaper tend to be Japanese names for example however, there are these clusters of proper names and proper names have a certain frequency profile and they tend to cluster together so at the very least you should be able to get the fact that things in this cluster are proper names for example and you know that like I think intuitively you can see how that would be universal across languages more or less so you can get at least that close and then there’s other things like if two languages have determiners are almost always like the most common things so you could map determiners between languages and get them right uh most of the time stuff but uncertain completely unsupervised methods work kind of okay but not really well so yeah also an auxiliary point is that there’s very few languages in the world where you don’t have any dictionary whatsoever if there are languages in the world that don’t have a dictionary they also probably don’t have written text that you could learn word embeddings from so completely unsupervised you know you might not need it but you might want to do something with like just a dictionary and so another another thing that’s commonly done is like so the way another way to do this distribution matching or to learn this distribution matching is basically to have an adversarial objective and the adversarial objective basically what it does is it tries to prevent a model from being able to distinguish between the this x times w and y. So the idea is you want to move the spaces so close together that like a sophisticated neural network or some sort of you know discriminator is not able to distinguish between them so that’s the actual mechanism for doing the distribution matching another thing that is commonly done which I talked about two classes ago but isn’t included in the slides here is you get an initial first pass where you you find like several points that you’re very confident in like several points that don’t that are mutual nearest neighors of each other but don’t have close other close mutual nearest neighbors and you use those as basically pseudo supervision and then super create a supervised model that tries to make sure that those get mapped as close together as possible while making others farther apart and do an iterative process where you gradually like increase the number of words that are mapped together and that further improves accuracy now if you have like a small amount of supervision if you have like i don’t know 50 words in a dictionary you could use that you could use that to do supervision directly without having to do the unsupervised distribution matching at first and in fact two papers were presented at basically the same time one was a paper and completely unsupervised mapping another was a paper where they only use numbers like numbers were the only thing that they used to cross the language because numbers tend to be written in Roman characters in many languages in the world so sorry let Latin characters in many languages in the world so because of that you could use just that as supervision and then that would get you a long way too so if you have a dictionary that gives you better results. Usually, even a smaller does that actually really work because just because numbers I feel like they’re the information about that number and the word embedding is often not it doesn’t really encode what the number actually is just kind of that it is a number a lot of the time right and but I think basically the idea is if you can get any if you can get any supervision it’s better than no supervision and you’re still gonna have like ideally some sort of distribution matching component in your objective anyway so yeah yes with this method we still ensure that your viewers that’s a really good question so you if you have supervision you wouldn’t necessarily have to do that we’ve done some work on unsupervised embedding induction and it almost always helps to ensure that w is orthogonal and it or it almost always helped us anyway to ensure that w was orthogonal and it almost always helped us to not use anything more complicated than a linear transform like you would think you’d be able to throw a big neural network at it and it would do better but like even in supervised settings I guess the problem is too underspecified and that didn’t help very much not to say it wouldn’t have verb cool okay so the next so the next thing is we pull out our favorite data augmentation method date back translation and so we take for example French and back translated monolingual data into English and we take English and we back translate monolingual data into French and so here we have parallel data here but what we can do instead is we can do like pseudo parallel data which I’ll talk about in a second.</p>
</blockquote>
</section>
<section id="one-slide-primer-on-phrase-based-statistical" class="level2">
<h2 class="anchored" data-anchor-id="one-slide-primer-on-phrase-based-statistical">One slide primer on phrase-based statistical</h2>
<blockquote class="blockquote">
<p>Next I’d like to explain how we apply these methods to a non-neural MT system and just to give a very brief overview. A one-slide primer on phrase-based machine translation so this is what a lot of people used to do machine translation before neural mt came out and it consists of three steps first the input the source input is segmented into phrases these phrases can be any sequence of words not necessarily linguistically motivated so they don’t need to be like a noun phrase or verb phrase or anything like this then you take out a dictionary and you replace like the dictionary has translations of the phrases into the target language maybe it has you know 10 or so candidates for each phrase and then the phrases are reordered to get into the correct order and this is a nice method for some reasons and the one of the reasons why it’s a nice method is it’s guaranteed to basically cover every word in the inputs and not do any you know if the model is trained okay not do any really really crazy things so if you guys are struggling with assignment two right now already and you’re getting your like low resource machine translation neural machine translation system it’s probably doing things like repeating the same word 300 times in a row or something like that a phrase-based machine translation system would not do this it might give you a bad output but it wouldn’t you know repeat the same thing over and over again or translate a sentence into like an empty sentence or something like that precisely because it has to cover all the words and it can only use a fixed set of translations so because of this it has a strong bias to generate like non-nonsense but it’s also you know not as powerful as neural mt basically so the segmenting into phrases is easy the reordering is not easy but possible but in order to translate each phrase you need parallel data for this that’s a problem in unsupervised MT of course</p>
</blockquote>
</section>
<section id="unsupervised-statistical-mt" class="level2">
<h2 class="anchored" data-anchor-id="unsupervised-statistical-mt">Unsupervised Statistical MT</h2>
<blockquote class="blockquote">
<p>The way unsupervised statistical mt could work which is also detailed in these two papers by arteza in limple is you learn monolingual embeddings for unigrams diagrams and trigrams you initialize phrase tables from cross-lingual mappings of these embeddings so basically you initialize the phrases that could be used in a phrase-based machine translation system and then you do supervised training based on back translation and iterate so what this what this means is basically you take the phrase tables from cross-lingual mappings you estimate a language model on the target language and then you just translate all of the all of the monolingual data and after you’ve translated all the monolingual data then you can then feed it into your like normal machine translation training pipeline and the key here is that you know you’re inducing like each of these phrase translations just by mapping embeddings of unigram’s diagrams and trigrams cool and what we can see here is that if we start out with the unsupervised phrase table and translate from French to English we get a blue score of 17.5 and then as we add more iterations and translate and translate and translate and learn from the translated data the score gets a bit better every time. Basically, it’s this iterative process of recreating the data back translation in one direction the other direction one direction the other direction in training.</p>
<p>Unsupervised neural mt the way this works is we create a neural MT system and actually the exact same procedure could also be done for neural mt with the caveat that you can’t create a phrase table so you would need another you would need another method for learning the initial model because you can’t like induce a phrase table from embeddings so in addition to using that same procedure there’s one thing that you can do to improve the neural MT model and basically the way it works is you take the encoder-decoder model and you use the same encoder-decoder for both languages and another thing that you can do is you can initialize the inputs with cross-lingual embeddings and the idea of what you do here is you train the model to output French but you have the inputs be either French so it’s like an auto encoding objective or English here so if you have this French token here this is basically saying, I want you to generate french next so the model is you know basically guaranteed to generate French and but because these input embeddings are initialized with bilingual like coordinated embeddings the inputs look very similar so it’s like the inputs look similar I know I want to generate French, so if we just train on this encoding objective in the bottom which we can do for monolingual data it nonetheless learns how to translate is the hope and dream so we have a cool we have a couple objectives objective is the denoising</p>
</blockquote>
</section>
<section id="unsupervised-mt-training-objective-1" class="level2">
<h2 class="anchored" data-anchor-id="unsupervised-mt-training-objective-1">Unsupervised MT: Training Objective 1</h2>
<blockquote class="blockquote">
<p>Autoencoder objective and so what we do is we take the target sentence the source sentence we map it into a denoising autoencoder and what a denoising autoencoder does is basically we have a sentence in the source side we add some according to this corruption function c oops this corruption function c so we move x into some other place c we map it into a latent space and then we try to regenerate the original x and I’ll give an example of this in a moment and the other objective that we can use an unsupervised NMT is back translation so what we do is we translate the target to the source and we use this as a quote-unquote supervised example to translate the source to the targets.</p>
<p>So one example of but the noising objective would be just to like cross off individual words here and try to reproduce the original sentence from the crosstalk chords so we set their word embeddings to zero or something like this and so that would be going from this x to the c x and then we could just run a seq2seq model to try to take in this noise input and generate the output so that would be one example and in this particular case what we could do is we could try to translate this into the into another language and this translation could either be done with like a pre-trained model that we have already from our a previous iteration about unsupervised translation or it could be done with some sort of heuristic like mapping the words in the input and I think I have a slide about that coming up soon and so the reason why it works is as I said before cross-lingual embeddings and the shared encoder decoder gives the model a good starting point where translating from this is similar to translating from from this here and so then another objective that people use to further enforce this is an adversarial objective and what they try to do essentially is you have an auto encoder example or a back translation example and you apply an adversarial objective to try to force these two phrases together so this is also similar to what they use an unsupervised like word translation where you have a model and you try to get these encoder vectors to be so similar that you fool the model and it’s not able to distinguish between them yes yeah so actually mast lms are a variety of denoising autoencoding it’s basically a subset of denoising yeah well so here we’re not using any parallel data whatsoever so the back translation is we’re not using any real parallel data all of the parallel data we have is obtained through like that translation for example. ## How does it work? Basically there’s two ways that you can you can do this the the first way is with just using an auto encoding or denoising auto encoding objectives so to take the example here we have like I am a student and and so if you have bilingual embeddings or something like that where bilingual embeddings gotten out close together basically you know these word events look the same these sport embeddings maybe look the same and these were embedding the same so overall and so like despite the fact that what you want to do is on the bottom the vector the light blue vectors on the top if each of the word wrappers look similar also looks similar and then you can also do things like randomly masking out words to make it look a little bit like basically to make the problem of auto encoding more difficult and so you need to fill in more information so when you move from this queen English over here with some without words to French you know the problem also gets harder so you kind of need to infer missing things or differences from this but because you’re doing denoising it allows you to do that for back translation back translation is basically like what we talked about before it’s like more or less the same so actually perhaps the more important thing is yeah so wouldn’t that lead to more hallucination from the model yeah basically yes I think it’s going to be very hard to train a model that’s that works perfectly with through purely unsupervised translation but the idea one thing is that any mistakes in your one reason why back translation works in the first place is any mistakes in your training data tend to be more random than the actual correct training data so as you refine the model more and more and get a better and better model the hallucinations because they’re random tend to get washed out whereas the like correct translations tend to be reinforced because they’re more consistent so it still will hallucinate and make mistakes but hopefully the idea is that hopefully they get washed out it works yeah because yes we do not directly use this move by statement during the training the veterans practice later is was trained based on this data so somehow that we use the difference between not that of dsd yeah so this this slide is a little bit deceptive because this would be the ## Step 2: Back-translation example of like supervised back translation here in unsupervised back translation in the back translation in the unsupervised like MT paradigm basically you don’t use any parallel data you just use a denoising auto-encoding objective to seed your back translator and then use that to generate data so you’re never using any like actually parallel data yeah so basically you start out with just a model training using monolingual data so for example, English is ## Performance This is a graph from the original paper. I think there’s a big caveat in this graph so you need to be a little bit careful in interpreting it but basically the horizontal lines are an unsupervised translation model that uses lots of monolingual data but no parallel data. The non-horizontal lines are a model are the supervised translation model and basically what they’re showing here is with no monolingual no parallel data they’re able to achieve scores of about the same level as something trained with 10 to the five so 10 thousand parallel sentences a big caveat here is that they didn’t use any monolingual data in the in the supervised translation system so if they did the supervised translation system would probably be a lot better but still I mean it’s kind of interesting that you’re able to do anything at all with unsupervised translations so I think as long as you’re aware of their caveats it’s kind of an interesting graph·yeah so another so basically I’ll go to the open problems in unsupervised mts so basically this is exactly the problem that Ellen was pointing out which is unsupervised machine translation works in ideal situations so basically languages are fairly similar written with similar writing systems large monolingual data sets in the same domain and match the test domain so in this particular case they were using data from the European parliament and it basically the data in the English and the French or the English and the Germans was from exactly the same distribution and that really helps in like inducing the lexicon or doing translation and so when you have less related languages truly low resource languages diverse domains less monolingual data unsupervised machine translation performs less well and reasons for performance basically small monolingual data in low resource languages can result in bad embedding so if we don’t have lots of data in the low-resource language this won’t work because the embeddings will be too poor to get a good initialization different word frequencies or morphology like the English and Turkish example I talked about before also different content makes things like back translation are less effective in bootstrapping a translation model. So for example if you’re trying to translate Twitter in one language and you have news text in another language that’s not like back translation is not gonna be good for covering what is said on Twitter so just in an interest of time I’ll skip that one but so there are some things that can be used to improve so better initialization recently people have been using things like cross-lingual language models or multilingual language models to improve the initialization of multi of unsupervised translation models so things like masked language modeling across language. So basically what you what you can do is you can train a single monolingual or bilingual single multilingual language model as your encoder or both your encoder and your decoder and you use that to initialize the translation model and this is good because you’re initializing the whole model as opposed to just the input and for various reasons it’s nice to have it’s nice to have a single model that is trained on all the languages and just to give one example even in like Chinese or something that’s written an entirely different script than English there’s still lots of English words so if you’re training the English model in the Chinese model on tons and tons of monolingual data the English words can also help anchor you know things into the same semantic space because they appear in various languages as well another thing is masked sequence sequence modeling so there’s things like mass which basically they have an encoder decoder form formulation of mass language modeling where basically you mask out a piece of the input and you generate that masked-out piece of the output and recently a model that lots of people have been using is this mBART model multilingual bart model and the way it works is basically you mask out words in the input but then you you generate all of the words in the outputs so then you you basically train this on tons and tons of data it’s a pre-trained model that you can then use to initialize your downstream unsupervised NTM And I had some stuff about the unsupervised multilingual 70. I’m going to skip over that in the interest of time, but how practical is this strict unsupervised scenario? So one thing that I definitely can say is that a lot of the techniques that are used in unsupervised mt are practical and semi-supervised learning scenarios where we have a little bit of training data so we can either train the model first with an unsupervised method and then fine-tune using the parallel corpus or train the model using a parallel corpus and update with iterative back translation and part of the reason why it’s why this is particularly good is there’s very few languages in the world that don’t have any parallel data but have lots of monolingual data for example almost every language in the world has parallel data from the bible if it has any amount of monolingual data but the bible is very out of domain very you know small so because of that using that to speed a model but then doing basic almost unsupervised translation seems like a practical way to do like news translation or something like that and then another area where these techniques are practical is if you have a task where you like legitimately cannot get very much data whatsoever so one example being style transfer from informal to informal text where you know there’s not very much data especially in different languages. Another example that has been used recently is the translation between Java and Python for example where there’s like lots of Java lots of Python but very little you know very little parallel data there’s some parallel data but not very much so yeah the discussion question is pick a low resource language or dialect research all of the monolingual or parallel data that you can find online for it would unsupervised or semi-supervised mt methods be helpful and how could you best use the existing resources to set up an unsupervised or something supervised empty for success on this language or dialect and the this is a reference to a paper that you could take a look at to discuss that so cool any questions before we start the discussion.</p>
</blockquote>
</section>
</div>
</div>
</div>



<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Unsupervised {Machine} {Translation}},
  date = {2022-02-17},
  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w10-unsupervised-NMT/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Unsupervised Machine Translation.”</span>
February 17, 2022. <a href="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w10-unsupervised-NMT/">https://orenbochman.github.io/notes-nlp/notes/cs11-737-w10-unsupervised-NMT/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Multilingual NLP</category>
  <category>NLP</category>
  <category>Notes</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w10-unsupervised-NMT/</guid>
  <pubDate>Wed, 16 Feb 2022 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Assignment 1: Multilingual POS Tagging</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w02-assignment-1/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/tiling.png" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>course banner</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/Gx5eLtOY10g" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Recitation of AWS Fundamentals 01
</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/TAn-WlnwZhY" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;2: Recitation of AWS Fundamentals 02
</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/25VuoasPues" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;3: Recitation of AWS Fundamentals 03
</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/32vgKiOEpPA" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;4: Recitation of AWS Fundamentals 04
</figcaption>
</figure>
</div><div id="sup-slide-deck" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="slides" title="assignment slides"><embed src="slides.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>assignment slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div>




<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Example Sequence Classification/Labeling Tasks</li>
<li>Overall Framework of Sequence Classification/Labeling</li>
<li>Sequence Featurization Models (BiRNN, Self Attention, CNNs)</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- Transcript here is machine generated no punctuation and I eddited it a but but it needs more work and I dont see the benefits unless I can run it through some tool. -->
</div>
</div>
</div>
<section id="some-ideas" class="level2">
<h2 class="anchored" data-anchor-id="some-ideas">Some Ideas</h2>
<ol type="1">
<li>crearing a multilingual POS tagger</li>
<li>using a hierarchical model that does partial pooling to learn from multiple languages when working on a low-resource language</li>
<li>creating a surrogate simulated language which
<ol type="1">
<li>has parameters that correspond to the low resource language - resources are drawn from language database like WALS, Ethnologue, and Glottolog Note that the challange then becomes in how to generate the surrogate language based on these parameters. One could try to create real phonetic and morphological rules etc or one might side step this complexity and use a simple mathematical construct to create data to create suitable embeddings.</li>
<li>Use a phrase book as a template for generating texts in the surrogate languages. The outcome should be a dataset of translations of the phrase book in multiple languages. Note that it could also be feasible to generate multiple variants for the both the source and target language to avoid overfitting on the phrase book.</li>
<li>a priors distribution that follows high resource languages. (i.e.&nbsp;idea that high frequency source words are more likely to be translated to high frequency target words)</li>
<li>a language model that is trained on the high resource language and then used to generate the surrogate language</li>
<li>a model that is trained on the surrogate language and then used to tag the low resource language</li>
</ol></li>
</ol>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Assignment 1: {Multilingual} {POS} {Tagging}},
  date = {2022-02-15},
  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w02-assignment-1/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Assignment 1: Multilingual POS
Tagging.”</span> February 15, 2022. <a href="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w02-assignment-1/">https://orenbochman.github.io/notes-nlp/notes/cs11-737-w02-assignment-1/</a>.
</div></div></section></div> ]]></description>
  <category>Multilingual NLP</category>
  <category>NLP</category>
  <category>Notes</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w02-assignment-1/</guid>
  <pubDate>Mon, 14 Feb 2022 22:00:00 GMT</pubDate>
</item>
</channel>
</rss>
