<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>NLP Course Notes &amp; Research</title>
<link>https://orenbochman.github.io/notes-nlp/</link>
<atom:link href="https://orenbochman.github.io/notes-nlp/index.xml" rel="self" type="application/rss+xml"/>
<description>Course and Research notes</description>
<image>
<url>https://orenbochman.github.io/notes-nlp/images/nlp-brain-wordcloud.jpg</url>
<title>NLP Course Notes &amp; Research</title>
<link>https://orenbochman.github.io/notes-nlp/</link>
</image>
<generator>quarto-1.6.39</generator>
<lastBuildDate>Wed, 12 Feb 2025 22:00:00 GMT</lastBuildDate>
<item>
  <title>A Convolutional Attention Network for Extreme Summarization of Source Code</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/2016-code-summerization/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/literature-review-open-book.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Litrature review"><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="img-fluid figure-img" alt="Litrature review"></a></p>
<figcaption>Litrature review</figcaption>
</figure>
</div></div><p><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2016-code-summerization/img/fig01.png" id="fig-01" class="img-fluid"> <img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2016-code-summerization/img/fig02.png" id="fig-02" class="img-fluid"> <img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2016-code-summerization/img/fig03.png" id="fig-03" class="img-fluid"></p>
<p>This is a paper mentioned in the course on multilingual NLP by Graham Neubig. With an interesting idea of an second attention head being used to copy stuff from the input directly to the output.</p>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>Attention mechanisms in neural networks have proved useful for problems in which the input and output do not have fixed dimension. Often there exist features that are locally translation invariant and would be valuable for directing the model’s attention, but previous attentional architectures are not constructed to learn such features specifically. We introduce an attentional neural network that employs convolution on the input tokens to detect local time-invariant and long-range topical attention features in a context-dependent way. We apply this architecture to the problem of extreme summarization of source code snippets into short, descriptive function name-like summaries. Using those features, the model sequentially generates a summary by marginalizing over two attention mechanisms: one that predicts the next summary token based on the attention weights of the input tokens and another that is able to copy a code token as-is directly into the summary. We demonstrate our convolutional attention neural network’s performance on 10 popular Java projects showing that it achieves better performance compared to previous attentional mechanisms. – <span class="citation" data-cites="allamanis2016convolutionalattentionnetworkextreme">(Allamanis, Peng, and Sutton 2016)</span></p>
</blockquote>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-allamanis2016convolutionalattentionnetworkextreme" class="csl-entry">
Allamanis, Miltiadis, Hao Peng, and Charles Sutton. 2016. <span>“A Convolutional Attention Network for Extreme Summarization of Source Code.”</span> <a href="https://arxiv.org/abs/1602.03001">https://arxiv.org/abs/1602.03001</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {A {Convolutional} {Attention} {Network} for {Extreme}
    {Summarization} of {Source} {Code}},
  date = {2025-02-13},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/2016-code-summerization/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“A Convolutional Attention Network for
Extreme Summarization of Source Code.”</span> February 13, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/2016-code-summerization/">https://orenbochman.github.io/notes-nlp/reviews/paper/2016-code-summerization/</a>.
</div></div></section></div> ]]></description>
  <category>NLP</category>
  <category>Paper</category>
  <category>Attention</category>
  <category>Deep learning</category>
  <category>Review</category>
  <category>Stub</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/2016-code-summerization/</guid>
  <pubDate>Wed, 12 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Floating Constraints in Lexical Choice</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/1997-floating-contraints-in-lexical-choice/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/literature-review-open-book.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Literature review"><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="img-fluid figure-img" alt="Literature review"></a></p>
<figcaption>Literature review</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/DffGdrfY9gI" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid quarto-uncaptioned" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1
</figcaption>
</figure>
</div></div>
<section id="context" class="level2">
<h2 class="anchored" data-anchor-id="context">Context</h2>
<p>This is one of the three paper mentioned by Kathleen McKeown in her heroes of NLP interview by Andrew NG. The paper is about the floating constraints in lexical choice.</p>
<p>Looking quickly over it I noticed a couple of things:</p>
<!--
@article{mckeown1997floating,
  title={Floating constraints in lexical choice},
  author={McKeown, Kathleen and Elhadad, Michael and Robin, Jacques},
  year={1997}
}
-->
<ol type="1">
<li>I was familiar with Elhadad’s work on generation. I had read his work on FUF Functional Unification Formalism in the 90s which introduced me to the paradigm of unification before I even knew about Prolog or logic programming.</li>
<li>I found it fascinating that McKeown and Elhadad had worked together on this paper and even a bit curious about what they were looking into here.</li>
<li>Since McKeown brought it up, it might intimate that there is something worth looking into here. And it is discusing aspects of generative language models.</li>
</ol>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>Lexical choice is a computationally complex task, requiring a generation system to consider a potentially large number of mappings between concepts and words. Constraints that aid in determining which word is best come from a wide variety of sources, including syntax, semantics, pragmatics, the lexicon, and the underlying domain. Furthermore, in some situations, different constraints come into play early on, while in others, they apply much later. This makes it difficult to determine a systematic ordering in which to apply constraints. In this paper, we present a general approach to lexical choice that can handle multiple, interacting constraints. We focus on the problem of floating constraints, semantic or pragmatic constraints that float, appearing at a variety of different syntactic ranks, often merged with other semantic constraints. This means that multiple content units can be realized by a single surface element, and conversely, that a single content unit can be realized by a variety of surface elements. Our approach uses the Functional Unification Formalism (FuF) to represent a generation lexicon, allowing for declarative and compositional representation of individual constraints. – <span class="citation" data-cites="mckeown1997floating">(McKeown, Elhadad, and Robin 1997)</span></p>
</blockquote>
</section>
<section id="outline" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li>Introduction
<ul>
<li>Mentions the complexity of lexical choice in language generation.</li>
<li>Notes the diverse sources of constraints on lexical choice, including syntax, semantics, lexicon, domain, and pragmatics.</li>
<li>Highlights the challenges posed by floating constraints, which are semantic or pragmatic constraints that can appear at various syntactic ranks and be merged with other semantic constraints.</li>
<li>Presents examples of floating constraints in sentences.</li>
</ul></li>
<li>An Architecture for Lexical Choice
<ul>
<li>Discusses the role of lexical choice within a typical language generation architecture.</li>
<li>Argues for placing the lexical choice module between the content planner and the surface sentence generator.</li>
<li>Describes the input and output of the lexical choice module, emphasizing the need for language-independent conceptual input.</li>
<li>Presents the two tasks of lexical choice: syntagmatic<sup>1</sup> decisions (determining thematic structure) and paradigmatic<sup>2</sup> decisions (choosing specific words).</li>
<li>Introduces the FUF/SURGE package as the implementation environment.</li>
</ul></li>
<li>Phrase Planning in Lexical Choice
<ul>
<li>Describes the need for phrase planning to articulate multiple semantic relations into a coherent linguistic structure.</li>
<li>Explains how the Lexical Chooser selects the head relation and builds its argument structure based on focus and perspective.</li>
<li>Details how remaining relations are attached as modifiers, considering options like embedding, subordination, and syntactic forms of modifiers.</li>
</ul></li>
<li>Cross-ranking and Merged Realizations
<ul>
<li>Discusses the non-isomorphism between semantic and syntactic structures, necessitating merging and cross-ranking realizations.</li>
<li>Provides examples of merging (multiple semantic elements realized by a single word) and cross-ranking (single semantic element realized at different syntactic ranks).</li>
<li>Emphasizes the need for linguistically neutral input to support this flexibility.</li>
<li>Explains the implementation of merging and cross-ranking using FUF, highlighting the challenges of search and the use of bk-class for efficient backtracking.</li>
</ul></li>
<li>Interlexical Constraints
<ul>
<li>Defines interlexical constraints as arising from alternative sets of collocations for realizing pairs of content units.</li>
<li>Presents an example of interlexical constraints with verbs and nouns in the basketball domain.</li>
<li>Discusses different strategies for encoding interlexical constraints in the Lexical Chooser.</li>
<li>Introduces the :wait mechanism in FUF to delay the choice of one collocate until the other is chosen, preserving modularity and avoiding backtracking.</li>
</ul></li>
<li>Other Approaches to Lexical Choice
<ul>
<li>Reviews other systems using FUF for lexical choice, comparing their architectures and features to ADVISOR-II.</li>
<li>Discusses non-FUF-based systems, categorizing them by their positioning of lexical choice in the generation process and analyzing their handling of various constraints.</li>
</ul></li>
<li>Conclusion
<ul>
<li>Summarizes the contributions of the paper, highlighting the ability to handle a wide range of constraints, the use of FUF for declarative representation and interaction, and the algorithm for lexical selection and syntactic structure building.</li>
<li>Emphasizes the importance of syntagmatic choice and perspective selection for handling floating constraints.</li>
<li>Notes the use of lazy evaluation and dependency-directed backtracking for computational efficiency.</li>
<li>Mentions future directions, such as developing domain-independent lexicon modules and methods for organizing large-scale lexica.</li>
</ul></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;of or denoting the relationship between two or more linguistic units used sequentially to make well-formed structures. The combination of ‘that handsome man + ate + some chicken’ forms a syntagmatic relationship. If the word position is changed, it also changes the meaning of the sentence, eg ’Some chicken ate the handsome man</p></div><div id="fn2"><p><sup>2</sup>&nbsp;Paradigmatic relation describes the relationship between words that can be substituted for words with the same word class (eg replacing a noun with another noun)</p></div></div></section>
<section id="reflection" class="level2">
<h2 class="anchored" data-anchor-id="reflection">Reflection</h2>
<ul>
<li>In unification-based approaches, any word is a candidate unless it is ruled out by a constraint. With an LLM a word is selected based on its probability.</li>
<li>The idea of constraints is deeply embedded in Linguistics. The two type that immediately come to mind are:
<ul>
<li>Subcategorization constraints which are rules that govern the selection of complements and adjuncts. These are syntactic constraints.</li>
<li>Selectional constraints which are rules that govern the selection of arguments. These are semantic constraints</li>
</ul></li>
<li>Other constraints that I did not have to deal with as often are pragmatics and these are resolved from the state of the world or common sense knowledge.</li>
<li>In FUF there are many more constraints and if care isn’t taken, the system can become over constrained and no words will be selected.</li>
<li>One Neat aspect of the approach though is that FUF language could be merged with a query to generate a text.</li>
<li>Unfortunately as fat as I recall the FUF had to be specified by hand. The only people who could do this were the people who designed the system. They needed to be expert at linguistics and logic programming. Even I was pretty sharp at the time the linguistics being referred was far too challenging. I learned that in reality every computational linguist worth their salt had their own theory of language and their own version of grammar and that FUF was just one of many.</li>
<li>It seems that today attention mechanisms within LSTMs or Transformers are capable of learning a pragmatic formalism without a linguist stepping in to specify it.</li>
</ul>
<p>Why this is still interesting:</p>
<p>Imagine we want to build a low resource capable language model.</p>
<p>We would really like it to be good with grammar Also we would like it to have a lexicon that is as rich as the person it is interacting with. And we would also like to make available to it the common sense knowledge that it would need to use in the context of current and possibly a profile based on past conversations…. On the other hand we don’t want to require a 70 billion parameter model to do this. So what do we do?</p>
<p>Let imagine we start by training on Wikipedia, after all wikipedia’s mission is to become the sum of all human knowledge. By do we realy need all of the information in Wikipedia in out edge model?</p>
<p>The answer is no but the real question is how can we partition the training information and the wights etc so that we have a powerful model with a basic grammar and lexicon but which can load a knowledge graph, extra lexicons and common sense knowledge as needed.</p>
<p>So I had this idea from lookin at a <span class="citation" data-cites="ouhalla1999introducing">Ouhalla (1999)</span>. It pointed out there were phrase boundries and that if the sentence was transformed in a number of ways t would still make sense if we did this with respect to a phrase but if we used cucnks that were too small or too big the transofrmation would create malformed sentences. The operations of intrerest were movement and deletion.</p>
<p>and that can still have a model that is both small but able to access this extra material.</p>
<p>One direction seems to me is that we want to apply queries to all the training material as we train the model. Based on the queries that we retrieve each clause or phrase we can decide where we want to learn it and if we want to learn it at all.</p>
<p>(If it is common sense knowledge we may already know it. If it is semi structured we may want to put it into wikidata. If it is neither we may want to avoid learning it at all. We may still want to use it for improving the grammar and lexicon.) However we may want to store it in a speclised lexicon for domains like medicinee or law.</p>
<p>We could also decide if and how to eliminate it from we want to The queries should match each bit of information in the sentence. These are our queries. While the facts may change form</p>
<ol type="1">
<li>say we want to decompose a wikipedia article</li>
</ol>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-mckeown1997floating" class="csl-entry">
McKeown, Kathleen, Michael Elhadad, and Jacques Robin. 1997. <span>“Floating Constraints in Lexical Choice.”</span>
</div>
<div id="ref-ouhalla1999introducing" class="csl-entry">
Ouhalla, J. 1999. <em>Introducing Transformational Grammar: From Principles and Parameters to Minimalism</em>. A Hodder Arnold Publication. Arnold. <a href="https://www.google.com/books?id=ZP-ZQ0lKI9QC">https://www.google.com/books?id=ZP-ZQ0lKI9QC</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Floating {Constraints} in {Lexical} {Choice}},
  date = {2025-02-12},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/1997-floating-contraints-in-lexical-choice/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Floating Constraints in Lexical
Choice.”</span> February 12, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/1997-floating-contraints-in-lexical-choice/">https://orenbochman.github.io/notes-nlp/reviews/paper/1997-floating-contraints-in-lexical-choice/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/1997-floating-contraints-in-lexical-choice/</guid>
  <pubDate>Wed, 12 Feb 2025 16:19:59 GMT</pubDate>
</item>
<item>
  <title>Coverage Embedding Models for Neural Machine Translation</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/2016-coverage-embedding-models-for-NMT/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/literature-review-open-book.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Literature review"><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="img-fluid figure-img" alt="Literature review"></a></p>
<figcaption>Literature review</figcaption>
</figure>
</div></div><section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<ol class="example" type="1">
<li>is discussed in week 6 the <a href="../../../notes/cs11-737-w06/index.html">Multilingual NLP course</a>. Neural generative models tend to drop or repeat content. But for NMT we can assume that all the inputs should be represented in the output. For each uncovered word it imposes a penalty on the attention model.</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR - Coverage Embedding
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/in_a_nutshell.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Coverage Embedding in a Nutshell"><img src="https://orenbochman.github.io/notes-nlp/images/in_a_nutshell.jpg" class="img-fluid figure-img" alt="Coverage Embedding in a Nutshell"></a></p>
<figcaption>Coverage Embedding in a Nutshell</figcaption>
</figure>
</div>
<ul>
<li>The paper introduces coverage embedding models to address the issues of repeating and dropping translations in NMT.</li>
<li>The coverage embedding vectors are updated at each time step to track the coverage status of source words.</li>
<li>The coverage embedding models significantly improve translation quality over a large vocabulary NMT system.</li>
<li>The best model uses a combination of updating with a GRU and updating as a subtraction.</li>
<li>The coverage embedding models also reduce the number of repeated phrases in the output.</li>
</ul>
</div>
</div>
</section>
<section id="podcast" class="level2">
<h2 class="anchored" data-anchor-id="podcast">Podcast</h2>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>In this paper, we enhance the attention-based neural machine translation (NMT) by adding explicit coverage embedding models to alleviate issues of repeating and dropping translations in NMT. For each source word, our model starts with a full coverage embedding vector to track the coverage status, and then keeps updating it with neural networks as the translation goes. Experiments on the large-scale Chinese-to-English task show that our enhanced model improves the translation quality significantly on various test sets over the strong large vocabulary NMT system. – <span class="citation" data-cites="mi-etal-2016-coverage">(Mi et al. 2016)</span></p>
</blockquote>
</section>
<section id="glossary" class="level2">
<h2 class="anchored" data-anchor-id="glossary">Glossary</h2>
<p>This paper has a number of big words and concepts that are important to understand. Lets break them down together:</p>
<dl>
<dt>Neural Machine Translation (NMT)</dt>
<dd>
A machine translation approach that uses neural networks to learn the mapping between source and target languages.
</dd>
<dt>Attention Mechanism</dt>
<dd>
In NMT, a mechanism that allows the model to focus on different parts of the source sentence when generating each word in the target sentence.
</dd>
<dt>Coverage Vector</dt>
<dd>
A vector used in statistical machine translation to explicitly track which source words have been translated.
</dd>
<dt>Coverage Embedding Vector</dt>
<dd>
A vector specific to each source word in this model, used to track the translation status of the word. It is initialized with a full embedding and is updated based on attention scores.
</dd>
<dt>Gated Recurrent Unit (GRU)</dt>
<dd>
A type of RNN cell used to model sequential data, including language. Here it is used to update coverage embeddings.
</dd>
<dt>Attention Probability (α)</dt>
<dd>
A set of weights that indicate how much attention the model pays to each source word when predicting a target word.
</dd>
<dt>Encoder-Decoder Network</dt>
<dd>
A neural network architecture commonly used in sequence-to-sequence tasks like NMT. The encoder processes the input sequence, and the decoder generates the output sequence.
</dd>
<dt>Bi-directional RNN</dt>
<dd>
A RNN that processes a sequence in both forward and backward directions, capturing contextual information from both sides of a word.
</dd>
<dt>Soft Probability</dt>
<dd>
Probabilities in the attention mechanism aren’t hard (0 or 1), but instead are on a continuum, indicating a degree of attention or importance.
</dd>
<dt>Fertility</dt>
<dd>
In the context of translation, fertility refers to the number of words in the target language that can be translated from a single word in the source language.
</dd>
<dt>One-to-many Translation</dt>
<dd>
A translation where one source word corresponds to multiple words in the target language.
</dd>
<dt>TER (Translation Error Rate)</dt>
<dd>
A metric used to evaluate the quality of machine translation by calculating the number of edits required to match the system’s translation to a reference translation, with lower scores being better.
</dd>
<dt>BLEU (Bilingual Evaluation Understudy)</dt>
<dd>
A metric to evaluate the quality of machine translation by comparing a candidate translation to one or more reference translations, with higher scores being better.
</dd>
<dt>UNK</dt>
<dd>
Abbreviation for “unknown.” In machine translation, it is used to denote words that are not in the model’s vocabulary.
</dd>
<dt>AdaDelta</dt>
<dd>
An adaptive learning rate optimization algorithm, that adjusts the learning rate during training for faster convergence.
</dd>
<dt>Alignment</dt>
<dd>
In the context of machine translation, the process of determining which words in the source sentence correspond to words in the target sentence.
</dd>
<dt>F1 Score</dt>
<dd>
A measure of a test’s accuracy and it considers both the precision and recall of the test to compute the score.
</dd>
</dl>
<p>With a solid understanding of this terminology we can now dive into the paper.</p>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ol type="1">
<li>Introduction
<ul>
<li>Notes that in NMT attention mechanisms focus on source words to predict target words.</li>
<li>Point out that <mark>these models lack history or coverage information, leading to repetition or dropping of words.</mark></li>
<li>Recalls how Statistical Machine Translation (SMT) used a binary “coverage vector” to track translated words.</li>
<li>Explains that SMT coverage vectors use 0s and 1s, whereas attention probabilities are soft. SMT systems handle one-to-many fertilities using phrases or hiero rules, while NMT systems predict one word at a time.</li>
<li>Introduces <strong>coverage embedding vectors</strong>, updated at each step, to address these issues.</li>
<li>Explains that <mark>each source word has its own coverage embedding vector that starts as a full embedding vector</mark>(as opposed to 0 in SMT).</li>
<li>States that coverage embedding vectors are updated based on attention weights, moving toward an empty vector for words that have been translated.</li>
</ul></li>
<li>Neural Machine Translation
<ul>
<li>Recalls that attention-based NMT uses an encoder-decoder architecture.
<ul>
<li>The encoder uses a bi-directional RNN to encode the source sentence into hidden states.</li>
<li><mark>The decoder predicts the target translation by maximizing the conditional log-probability of the correct translation.</mark></li>
<li>The probability of each target word is determined by the previous word and the hidden state.</li>
<li>The hidden state is computed using a weighted sum of the encoder states, with weights derived from a two-layer neural network.</li>
</ul></li>
<li>Introduces coverage embedding models into the NMT by adding an input to the attention model.</li>
</ul></li>
<li>Coverage Embedding Models
<ul>
<li>The model uses a coverage embedding for each source word that is updated at each time step.</li>
<li><mark>Each source word has its own coverage embedding vector,</mark> and the number of coverage embedding vectors is the same as the source vocabulary size.</li>
<li>The coverage embedding matrix is initialized with coverage embedding vectors for all source words.</li>
<li>Coverage embeddings are updated using neural networks (GRU or subtraction).</li>
<li>As the translation progresses, coverage embeddings of translated words should approach zero.</li>
<li>Two methods are proposed to update the coverage embedding vectors: GRU and subtraction.</li>
</ul>
<ol type="1">
<li>Updating Methods
<ol type="1">
<li>Updating with a GRU
<ul>
<li>The coverage model is updated using a GRU, incorporating the current target word and attention weights.</li>
<li>The GRU uses update and reset gates to control the update of the coverage embedding vector.</li>
</ul></li>
<li>Updating as Subtraction
<ul>
<li>The coverage embedding is updated by subtracting the embedding of the target word, weighted by the attention probability.</li>
</ul></li>
</ol></li>
<li>Objectives
<ul>
<li>Coverage embedding models are integrated into attention NMT by adding coverage embedding vectors to the first layer of the attention model.</li>
<li>The goal is to remove partial information from the coverage embedding vectors based on the attention probability.</li>
<li>The model minimizes the absolute values of the embedding matrix.</li>
<li>The model can also use supervised alignments to know when the coverage embedding should be close to zero.</li>
</ul></li>
</ol></li>
<li>Related Work
<ul>
<li>Tu et al.&nbsp;(2016) also uses a GRU to model the coverage vector. However, this approach differs from the current paper’s method by initializing the word coverage vector with a scalar and adding an accumulation operation and a fertility function.</li>
<li>Cohn et al.&nbsp;(2016) augments the attention model with features from traditional SMT.</li>
</ul></li>
<li>Experiments
<ol type="1">
<li>Data Preperation
<ul>
<li>Experiments were conducted on a Chinese-to-English translation task.</li>
<li>Two training sets were used: one with 5 million sentence pairs and another with 11 million.</li>
<li>The development set consisted of 4491 sentences.</li>
<li>Test sets included NIST MT06, MT08 news, and MT08 web.</li>
<li>Full vocabulary sizes were 300k and 500k, with coverage embedding vector sizes of 100.</li>
<li>AdaDelta was used to update model parameters with a mini-batch size of 80.</li>
<li>The output vocabulary was a subset of the full vocabulary, including the top 2k most frequent words and the top 10 candidates from word-to-word/phrase translation tables.</li>
<li>The maximum length of a source phrase was 4.</li>
<li>A traditional SMT system, a hybrid syntax-based tree-to-string model, was used as a baseline.</li>
<li>Four different settings for coverage embedding models were tested: updating with a GRU (UGRU), updating as a subtraction (USub), the combination of both methods (UGRU+USub), and UGRU+USub with an additional objective (+Obj).</li>
</ul></li>
<li>Translation Results
<ul>
<li>The coverage embedding models improved translation quality significantly over a large vocabulary NMT (LVNMT) baseline system.</li>
<li>UGRU+USub performed best, achieving a 2.6 point improvement over LVNMT.</li>
<li>Improvements of coverage models over LVNMT were statistically significant.</li>
<li>The UGRU model also improved performance when using a larger training set of 11 million sentences.</li>
</ul></li>
<li>Alignment Results
<ul>
<li>The best coverage model (UGRU + USub) improved the F1 score by 2.2 points over the baseline NMT system.</li>
<li>Coverage embedding models reduce the number of repeated phrases in the output.</li>
</ul></li>
</ol></li>
<li>Conclusion
<ul>
<li>The paper proposed coverage embedding models for attention-based NMT.</li>
<li>The models use a coverage embedding vector for each source word and update these vectors as the translation progresses.</li>
<li>Experiments showed significant improvements over a strong large vocabulary NMT system.</li>
</ul></li>
</ol>
</section>
<section id="reflection" class="level2">
<h2 class="anchored" data-anchor-id="reflection">Reflection</h2>
<p>The idea of tracking coverage is very simple. Lets face it many issues in NLP require simple solutions.</p>
<p>For instance in the summarization task we have a big headache with the autoregressive tendency to repeat. But it also requires a kind of coverage too, but one that is more spread out. Also in more extreme cases we want to direct the coverage using very specific information like the narative flow of a story. This seems to be an idea that can be further explored in other tasks.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-mi-etal-2016-coverage" class="csl-entry">
Mi, Haitao, Baskaran Sankaran, Zhiguo Wang, and Abe Ittycheriah. 2016. <span>“Coverage Embedding Models for Neural Machine Translation.”</span> In <em>Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</em>, edited by Jian Su, Kevin Duh, and Xavier Carreras, 955–60. Austin, Texas: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/D16-1096">https://doi.org/10.18653/v1/D16-1096</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Coverage {Embedding} {Models} for {Neural} {Machine}
    {Translation}},
  date = {2025-02-12},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/2016-coverage-embedding-models-for-NMT/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Coverage Embedding Models for Neural Machine
Translation.”</span> February 12, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/2016-coverage-embedding-models-for-NMT/">https://orenbochman.github.io/notes-nlp/reviews/paper/2016-coverage-embedding-models-for-NMT/</a>.
</div></div></section></div> ]]></description>
  <category>Paper</category>
  <category>Review</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/2016-coverage-embedding-models-for-NMT/</guid>
  <pubDate>Tue, 11 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Summarization Task</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/posts/2025-02-10-summerizer/</link>
  <description><![CDATA[ 





<section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p>In the Deeplearning.ai NLP specialization, we implemented both <strong>Q&amp;A</strong> and <strong>Summarization tasks</strong>. However, we only looked at these tasks in terms of the attention models. It is now time to dive deeper into the summarization task. In this article I will review a talk covering a body of research on summarization, which has many ideas about features. Consider some of these and see what aspects are relevant to a modern task implementation.</p>
<p>Some of the tasks here can be done with a simple LSTM or Transformer. However it seems that RL may be useful if we want to plan the summary in a non-linear fashion or to make use of a dynamic programming approach. For instance to identify the narrative structure might require many more steps then we typically use in a summerization task. For a speeches we may want to capture a gist of the rhetoric.</p>
</section>
<section id="ideas" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="ideas">Ideas</h2>
<p>Several ideas surfaced while working on the assignment for building a hybrid generative/abstractive summarizer based on GPT2<sup>1</sup>. Many ideas came from prior work developing search engines. I had noticed that some issues were anathema to summarization from its inception.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;a Generative Pre-training Transformer or a decoder transformer</p></div></div><section id="coverage-ranking-content-by-importance" class="level3">
<h3 class="anchored" data-anchor-id="coverage-ranking-content-by-importance">Coverage ranking content by importance</h3>
<p>The first issue is <strong>coverage</strong>, which is how much of the original document to include. Coverage is a much more difficult problem than it appears.</p>
<p>Consider summerizing a long novel a single paragraph summary may provide a good idea of the main plot points of the story. On the other hand summerising a similar sized encyclopedia might require listing all the top level headings and subheadings i.e.&nbsp;a one paragraph summary would not be able to provide a good idea of the content of the document. On the other hand it should be much easier to generate a summary for the encyclopedia than for the novel as the encyclopedia has a highly structured document with clear headings and subheadings and writing conventions that provide cues to the reader about the importance of the content. Summerizing a novel is more challanging as we would actualy want to ommit alsmost all the details and so deciding which parts to keep is a major challange with a very sparse signal.</p>
<p>For example, technical documents, like patents, headings, academic writing cues, and even IR statistics, may serve as features that we may feed into a regression that can guide us in discarding the chuff from the grain. On the other hand, for movie scripts or novels, we can’t use all that, and we need to decide what is essential based on the narrative structure, which requires sophisticated processing and extensive domain knowledge to extract. So, When we want to create a synopsis of a book like War and Peace, specific details are part of the narrative structure that stands out as essential to us, and I can say that even in 2025, LLM is not good at picking these out of a large document. For attentive readers with a suitable background, if we double the length of the text, they might pick additional plot points and then less significant subplots. If again we double we would, they would include more details concerning characters, their motivations, etc.</p>
<p>To conclude, different documents can have radically different structures and cues. These suggest different strategies for selecting and ranking the material to be included in a summary of a given length. More so, when we consider summaries of different lengths, one can see that we are talking about a hierarchal structure.</p>
</section>
</section>
<section id="avoiding-repetition" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="avoiding-repetition">Avoiding repetition</h2>
<p>A second challenge that is pervasive is avoiding <strong>repetition</strong>. In the generative settings we have a tougher challenge since sentences that are generated may well be equivalent to sentences we have already generated, and we want the model to redirect it attention from material already covered. Luckily we have learned to detect similar sentences in the Q&amp;A task<sup>2</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;using Siamese networks for oneshot similarity detection.</p></div></div><p>check if it is similar to any sentence in the summary</p>
<p>Alg1: similarity detection</p>
<pre><code>summary = []
tokenized_doc = tokenize(doc)
embeddings_doc= embed(tokenized_doc)
while len(summary) &lt; doc_tokens * summary_ratio: 
    a = gen_a_sentence(embeddings_doc,summary)
    for s in summary:
        if sim(a,s) &gt; threshold:
            continue
    else:
        summary.append(a)
s -&gt; gen a sentence,</code></pre>
</section>
<section id="context-window-constraints" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="context-window-constraints">Context window constraints</h2>
<p>The third challenge is technical and is related to the <strong>length of the context window</strong>. as transformer models have a limit on the number of tokens that they can process in a context window. A second issue here is that the summary itself needs to also fit in the context window. If the structure is linear and incremental we can chunk it into many smaller pieced say using sections. However in reality we are often dealing with trees of graphs structures and chunking can erode the model’s ability to inspect said hierarchy of the structure it is tasked with summarizing. In Q&amp;A or IR tasks since we can process each chunk separately and then combine the results.</p>
<p>If we are not using a generative model one imagines a tree data structure that can efficiently track what what part of the document has been summarized. If we can use that to work with the attention mechanism we may be able to reduce the effective window size as we progress with the summary. A more nuanced idea would come from <strong>bayesian search</strong> where we may want to keep moving the probability mass for the attention mechanism to regions that are significant but have not been covered.</p>
<p>Another challenge is that we may want to grow our summary in a non-linear fashion. We may consider the main narrative structure then add in the subplots and then the character motivations. This suggests two approaches - a top down <strong>planning</strong> based approach<sup>3</sup> and a bottom up approach where we start with the most important details, then add in the less important ones. In the second case we may want to revise the initial sentences to efficiently incorporate more details as we progress.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;using rl or dynamic programming</p></div><div id="fn4"><p><sup>4</sup>&nbsp;reformer layers</p></div></div><p>I also interviewed with a company that told me they often worked with patents and that these documents were frequently in excess of a hundred pages and they were having lots of problems with the summarization task. This suggest that an efficient transformer<sup>4</sup> would be needed if we are to support context windows that can span hundreds of pages. But that besides the context window we need some good way to ensure that the summary is balanced and that it is not too repetitive.</p>
</section>
<section id="compassion" class="level2">
<h2 class="anchored" data-anchor-id="compassion">Compassion</h2>
<p>My experience with summarization by hand is that most texts can be compressed by 5-10% without losing any information simply by rephrasing with more concise language. So another idea that should be obvious is that the generated summary should be <strong>compressive</strong>, i.e., once we generate a good sentence we should consider if we can make it shorter. This should be fairly easy as we are reducing the length of a single sentence. We may however be able to do even better by considering a section and trying to see if we can merge two sentences or if two sentences have partial overlap that can be merged or referenced via an anaphora. This is another area where we diverge from Q&amp;A, where we do not have a length constraint and may often want to include all relevant information. Implementation of this idea cam be easily embodied in a loss function that penalizes summaries for each token or character. This same idea can also be used in a RL summarizer.</p>
</section>
<section id="grounding" class="level2">
<h2 class="anchored" data-anchor-id="grounding">Grounding</h2>
<p>Generative summarization is a case where we can require that all generated text be grounded in the original document. In reality generative models have many glitches, due to tokenization issues, or failures of the attention mechanism, or out of distribution generation, due to bias learned from the corpus, due to negative recency bias and and due to emergent contradictions (where a contradiction is introduced into the context window and cannot be removed.) And even if we can avoid all these there is still nothing in the model that makes it prefer truthy generations. So it seems essential that the generated text is grounded in the original document. This seems to be verifiable by visually inspecting the using the attention mechanism. In reality there may be multiple heads and multiple layers. This means we need to access the attention mechanism programmatically and store annotations from the source document for each generated token. We may also want to make abstracting summarization explicit. c.f. [Extrinsic Hallucinations in LLMs in ](https://lilianweng.github.io/posts/2024-07-07-hallucination/) by Lilian Weng</p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Summarization {Task}},
  date = {2025-02-10},
  url = {https://orenbochman.github.io/notes-nlp/posts/2025-02-10-summerizer/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Summarization Task.”</span> February 10,
2025. <a href="https://orenbochman.github.io/notes-nlp/posts/2025-02-10-summerizer/">https://orenbochman.github.io/notes-nlp/posts/2025-02-10-summerizer/</a>.
</div></div></section></div> ]]></description>
  <category>NLP</category>
  <category>Notes</category>
  <category>Literature review</category>
  <category>Summarization task</category>
  <category>Relevance</category>
  <guid>https://orenbochman.github.io/notes-nlp/posts/2025-02-10-summerizer/</guid>
  <pubDate>Sun, 09 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Morphological Word Embeddings</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/2019-morphological-embeddings/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/literature-review-open-book.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Litrature review"><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="img-fluid figure-img" alt="Litrature review"></a></p>
<figcaption>Litrature review</figcaption>
</figure>
</div></div><p>This is a mentioned in the tokenization lab in course four week 3</p>
<p>Time permitting I will try and dive deeper into this paper.</p>
<section id="podcast" class="level2">
<h2 class="anchored" data-anchor-id="podcast">Podcast</h2>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>Linguistic similarity is multi-faceted. For instance, two words may be similar with respect to semantics, syntax, or morphology inter alia. Continuous word-embeddings have been shown to capture most of these shades of similarity to some degree. This work considers guiding word-embeddings with morphologically annotated data, a form of semisupervised learning, encouraging the vectors to encode a word’s morphology, i.e., words close in the embedded space share morphological features. We extend the log-bilinear model to this end and show that indeed our learned embeddings achieve this, using German as a case study. – <span class="citation" data-cites="cotterell2019morphological">(Cotterell and Schütze 2019)</span></p>
</blockquote>
<!-- TODO: add review and podcast for this paper-->
<!-- TODO: link here from LSH lessons-->
</section>
<section id="terminology" class="level2">
<h2 class="anchored" data-anchor-id="terminology">Terminology</h2>
<p>Here are some terms and concepts discussed in the sources, with explanations:</p>
<dl>
<dt>Word embeddings</dt>
<dd>
These are vector representations of words in a continuous space, where similar words are located close to each other in the vector space. The goal is to capture linguistic similarity, but the definition of “similarity” can vary (semantic, syntactic, morphological). Word embeddings are typically trained to produce representations that capture linguistic similarity.
</dd>
<dt>Log-bilinear model (LBL)</dt>
<dd>
This is a language model that learns features along with weights, as opposed to using hand-crafted features. It uses context to predict the next word, and word embeddings fall out as low dimensional representations of the context. The LBL is a generalization of the log-linear model.
</dd>
<dt>Morphology</dt>
<dd>
The study of word structure, including how words are formed from morphemes (the smallest units of meaning).
</dd>
<dt>Morphological tags</dt>
<dd>
These are annotations that describe the morphological properties of a word, such as case, gender, number, tense, etc.. They can be very detailed for morphologically rich languages. For example, in German, a word might be tagged to indicate its part of speech, case, gender, and number.
</dd>
<dt>Morphologically rich languages</dt>
<dd>
Languages with a high morpheme-per-word ratio, where word-internal structure is important for processing. German and Czech are cited as examples.
</dd>
<dt>Morphologically impoverished languages</dt>
<dd>
Languages with a low morpheme-per-word ratio, such as English.
</dd>
<dt>Multi-task objective</dt>
<dd>
Training a model to perform multiple tasks simultaneously. In this case, the model is trained to predict both the next word and its morphological tag.
</dd>
<dt>Semi-supervised learning</dt>
<dd>
Training a model on a partially annotated corpus, using both labeled and unlabeled data. This approach is useful when large amounts of unannotated text are available.
</dd>
<dt>Contextual signature</dt>
<dd>
The words surrounding a given word. The context in which a word appears can be used to determine the word’s meaning and morphological properties.
</dd>
<dt>Hamming distance</dt>
<dd>
A measure of the difference between two binary strings, calculated as the number of positions at which the corresponding bits are different. In this context, it is used to compare morphological tags, which are represented as bit vectors.
</dd>
<dt>MORPHOSIM</dt>
<dd>
A metric for evaluating morphologically-driven embeddings that measures how morphologically similar a word is to its nearest neighbors in the embedding space. It is calculated as the average Hamming distance between morphological tags of a word and its neighbors. Lower values are better, as they indicate that the nearest neighbors of each word are closer morphologically.
</dd>
</dl>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li>Introduction
<ul>
<li>Presents the importance of capturing word morphology, especially for morphologically-rich languages.</li>
<li>Highlights the multifaceted nature of linguistic similarity (semantics, syntax, morphology).</li>
<li>Discusses the goal of the paper: to develop word embeddings that specifically encode morphological relationships.</li>
</ul></li>
<li>Related Work
<ul>
<li>Discusses previous integration of morphology into language models, including factored language models and neural network-based approaches.</li>
<li>Notes the role of morphology in computational morphology, particularly in morphological tagging for inflectionally-rich languages.</li>
<li>Highlights the significance of distributional similarity in morphological analysis.</li>
</ul></li>
<li>Log-Bilinear Model
<ul>
<li>Describes the log-bilinear model (LBL), a generalization of the log-linear model with learned features.</li>
<li>Presents the LBL’s energy function and probability distribution in the context of language modeling.</li>
</ul></li>
<li>Morph-LBL
<ul>
<li>Proposes a multi-task objective that jointly predicts the next word and its morphological tag.</li>
<li>Describes the model’s joint probability distribution, incorporating morphological tag features.</li>
<li>Discusses the use of semi-supervised learning, allowing training on partially annotated corpora</li>
</ul></li>
<li>Evaluation
<ul>
<li>Mentions the qualitative evaluation using t-SNE, showing clusters reflecting morphological and POS relationships.</li>
<li>Introduces MorphoSim, a novel quantitative metric to assess the extent to which similar embeddings are morphologically related.</li>
</ul></li>
<li>Experiments and Results
<ul>
<li>Presents experiments on the German TIGER corpus, comparing Morph-LBL with the original LBL and Word2Vec.</li>
<li>Describes two experiments:
<ul>
<li>Experiment 1: Evaluates the morphological information encoded in embeddings using a k-NN classifier.</li>
<li>Experiment 2: Compares models using the MorphoSim metric to measure morphological similarity among nearest neighbors.</li>
</ul></li>
<li>Discusses the results, highlighting Morph-LBL’s superior performance in capturing morphological relationships, even without observing all tags during training.</li>
</ul></li>
<li>Conclusion and Future Work
<ul>
<li>Summarizes the contributions of the paper: introducing Morph-LBL for inducing morphologically guided embeddings.</li>
<li>Notes the model’s success in leveraging distributional signatures to capture morphology.</li>
<li>Discusses future work on integrating orthographic features for further improvement.</li>
<li>Mentions potential applications in morphological tagging and other NLP tasks.</li>
</ul></li>
</ul>
</section>
<section id="log-bilinear-model" class="level2">
<h2 class="anchored" data-anchor-id="log-bilinear-model">Log-Bilinear Model</h2>
<p><img src="https://latex.codecogs.com/png.latex?p(w%5Cmid%20h)%20=%0A%5Cfrac%7B%5Cexp%5Cleft(s_%5Ctheta(w,h)%5Cright)%7D%7B%5Csum_%7Bw'%7D%0A%5Cexp%5Cleft(s_%5Ctheta(w',h)%5Cright)%7D%20%5Cqquad%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?w"> is a word, <img src="https://latex.codecogs.com/png.latex?h"> is a history and <img src="https://latex.codecogs.com/png.latex?s_%5Ctheta"> is an energy function. Following the notation of , in the LBL we define <img src="https://latex.codecogs.com/png.latex?%0As_%5Ctheta(w,h)%20=%20%5Cleft(%5Csum_%7Bi=1%7D%5E%7Bn-1%7D%20C_i%0Ar_%7Bh_i%7D%5Cright)%5ET%20q_w%20+%20b_w%20%5Cqquad%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?n-1"> is the history length</p>
</section>
<section id="morph-lbl" class="level2">
<h2 class="anchored" data-anchor-id="morph-lbl">Morph-LBL</h2>
<p><img src="https://latex.codecogs.com/png.latex?%0A%20%20p(w,%20t%20%5Cmid%20h)%20%5Cpropto%20%5Cexp((%20f_t%5ET%20S%20%20+%20%5Csum_%7Bi=1%7D%5E%7Bn-1%7DC_i%20r_%7Bh_i%7D)%5ET%20q_w%20+%20b_%7Bw%7D%20)%20%5Cqquad%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?f_t"> is a hand-crafted feature vector for a morphological tag <img src="https://latex.codecogs.com/png.latex?t"> and <img src="https://latex.codecogs.com/png.latex?S"> is an additional weight matrix.</p>
<p>Upon inspection, we see that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap(t%20%5Cmid%20w,h)%20%5Cpropto%20%5Cexp(S%5ET%20f_t%20q_w)%20%5Cqquad%0A"></p>
<p>Hence given a fixed embedding <img src="https://latex.codecogs.com/png.latex?q_w"> for word <img src="https://latex.codecogs.com/png.latex?w">, we can interpret <img src="https://latex.codecogs.com/png.latex?S"> as the weights of a conditional log-linear model used to predict the tag <img src="https://latex.codecogs.com/png.latex?t">.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<ul>
<li><a href="http://www.eva.mpg.de/lingua/resources/glossing-rules.php">Leipzig Glossing Rules</a> which provides a standard way to explain morphological features by examples</li>
<li><a href="https://www.youtube.com/watch?v=y9sVFrmGu0w&amp;ab_channel=GrahamNeubig">CMU Multilingual NLP 2020 (17): Morphological Analysis and Inflection</a> <!--
- [nb-lm](https://notebooklm.google.com/notebook/f594ff01-19f2-49b0-a0ac-84176fb22667?_gl=1*1rba7bx*_ga*MzAyOTc3ODMwLjE3Mzg1MDQ5Njc.*_ga_W0LDH41ZCB*MTczODkyMzY5Ni41LjAuMTczODkyMzY5Ni42MC4wLjA.)
--></li>
<li><span class="citation" data-cites="kann-etal-2016-neural">Kann, Cotterell, and Schütze (2016)</span> <a href="https://github.com/ryancotterell/neural-canonical-segmentation">code</a></li>
<li><a href="https://unimorph.github.io/">unimorph</a> univorsal morphological database</li>
</ul>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-cotterell2019morphological" class="csl-entry">
Cotterell, Ryan, and Hinrich Schütze. 2019. <span>“Morphological Word Embeddings.”</span> <em>arXiv Preprint arXiv:1907.02423</em>.
</div>
<div id="ref-kann-etal-2016-neural" class="csl-entry">
Kann, Katharina, Ryan Cotterell, and Hinrich Schütze. 2016. <span>“Neural Morphological Analysis: Encoding-Decoding Canonical Segments.”</span> In <em>Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</em>, edited by Jian Su, Kevin Duh, and Xavier Carreras, 961–67. Austin, Texas: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/D16-1097">https://doi.org/10.18653/v1/D16-1097</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Morphological {Word} {Embeddings}},
  date = {2025-02-07},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/2019-morphological-embeddings/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Morphological Word Embeddings.”</span>
February 7, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/2019-morphological-embeddings/">https://orenbochman.github.io/notes-nlp/reviews/paper/2019-morphological-embeddings/</a>.
</div></div></section></div> ]]></description>
  <category>Paper</category>
  <category>Review</category>
  <category>Podcast</category>
  <category>NLP</category>
  <category>Morphology</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/2019-morphological-embeddings/</guid>
  <pubDate>Thu, 06 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Practical and Optimal LSH for Angular Distance</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/2015-LSH/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/literature-review-open-book.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Literature review"><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="img-fluid figure-img" alt="Literature review"></a></p>
<figcaption>Literature review</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/t_8SpFV0l7A" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Locality-Sensitive Hashing and Beyond
</figcaption>
</figure>
</div><div id="vid-02" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/Dkomk2wPaoc" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;2: Beyond Locality Sensitive Hashing; Alexandr Andoni
</figcaption>
</figure>
</div></div>

<p>In the NLP specialization we have covered and used LSH a number of times in at least two courses. <!-- TODO: link to the notes so as to make them more useful as this is a stub!!!! --> In one sense I have an understanding of what is going on when we implement this. In another sense this seems to be quite confusing. So I don’t fully understand some aspects of LSH.</p>
<p>One way to do better is to try and explain it to someone else. This is what I am trying to do here by going back to the source and trying to understand the paper, problems, motivation and finally isolate what it is that is hard to grasp.</p>
<p>This paper is a good introduction to LSH for the angular distance.</p>
<p>In <span class="citation" data-cites="andoni2015practical">(Andoni et al. 2015)</span> the authors delve into Locality-Sensitive Hashing (LSH) for the angular distance. They show the existence of an LSH family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal running time exponent. Unlike earlier algorithms with this property (e.g., Spherical LSH), their algorithm is also practical, improving upon the well-studied hyperplane LSH in practice.</p>
<p>For now the two videos which explain in some depth this an the related paper - <a href="https://arxiv.org/abs/1501.01062">Optimal Data-Dependent Hashing for Approximate Near Neighbors</a> will have to do.</p>
<p>Time permitting I will try and dive deeper into this paper.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR - Location Sensitive Hashing
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/in_a_nutshell.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Location Sensitive Hashing in a Nutshell"><img src="https://orenbochman.github.io/notes-nlp/images/in_a_nutshell.jpg" class="img-fluid figure-img" alt="Location Sensitive Hashing in a Nutshell"></a></p>
<figcaption>Location Sensitive Hashing in a Nutshell</figcaption>
</figure>
</div>
<ul>
<li>The paper introduces a Locality-Sensitive Hashing (LSH) family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal running time exponent.</li>
<li>The algorithm is also practical, improving upon the well-studied hyperplane LSH in practice.</li>
<li>The authors also introduce a multiprobe version of this algorithm, and conduct experimental evaluation on real and synthetic data sets.</li>
</ul>
</div>
</div>
<section id="podcast" class="level2">
<h2 class="anchored" data-anchor-id="podcast">Podcast</h2>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>We show the existence of a Locality-Sensitive Hashing (LSH) family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal running time exponent. Unlike earlier algorithms with this property (e.g., Spherical LSH [1 , 2]), our algorithm is also practical, improving upon the well-studied hyperplane LSH [3] in practice.</p>
<p>We also introduce a <strong>multiprobe</strong> version of this algorithm, and conduct experimental evaluation on real and synthetic data sets.</p>
<p>We complement the above positive results with a fine-grained lower bound for the quality of any LSH family for angular distance. Our lower bound implies that the above LSH family exhibits a trade-off between evaluation time and quality that is close to optimal for a natural class of LSH functions. – <span class="citation" data-cites="andoni2015practical">(Andoni et al. 2015)</span></p>
</blockquote>
<!-- TODO: add outline, reflection, and terminology and for this paper-->
<!-- TODO: link here from LSH lessons-->
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-andoni2015practical" class="csl-entry">
Andoni, Alexandr, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn, and Ludwig Schmidt. 2015. <span>“Practical and Optimal LSH for Angular Distance.”</span> <em>Advances in Neural Information Processing Systems</em> 28.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Practical and {Optimal} {LSH} for {Angular} {Distance}},
  date = {2025-02-05},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/2015-LSH/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Practical and Optimal LSH for Angular
Distance.”</span> February 5, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/2015-LSH/">https://orenbochman.github.io/notes-nlp/reviews/paper/2015-LSH/</a>.
</div></div></section></div> ]]></description>
  <category>Paper</category>
  <category>Review</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/2015-LSH/</guid>
  <pubDate>Tue, 04 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Finetuning Pretrained Transformers into RNNs</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/transforer-to-rnn/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./cover.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Litrature review"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/transforer-to-rnn/cover.jpg" class="img-fluid figure-img" alt="Litrature review"></a></p>
<figcaption>Litrature review</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/JMeYGYANEqU" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Transformer to RNN (T2RNN) Part-1 by Dr.&nbsp;Niraj Kumar
</figcaption>
</figure>
</div><div id="vid-02" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/UHgy2faOD_M" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;2: Transformer to RNN (T2RNN) Part-2 by Dr.&nbsp;Niraj Kumar
</figcaption>
</figure>
</div></div>

<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>Transformers have outperformed recurrent neural networks (RNNs) in natural language generation. But this comes with a significant computational cost, as the attention mechanism’s complexity scales quadratically with sequence length. Efficient transformer variants have received increasing interest in recent works. Among them, a linear-complexity recurrent variant has proven well suited for autoregressive generation. It approximates the softmax attention with randomized or heuristic feature maps, but can be difficult to train and may yield suboptimal accuracy. This work aims to convert a pretrained transformer into its efficient recurrent counterpart, improving efficiency while maintaining accuracy. Specifically, we propose a swap-then-finetune procedure: in an off-the-shelf pretrained transformer, we replace the softmax attention with its linear-complexity recurrent alternative and then finetune. With a learned feature map, our approach provides an improved tradeoff between efficiency and accuracy over the standard transformer and other recurrent variants. We also show that the finetuning process has lower training cost relative to training these recurrent variants from scratch. As many models for natural language tasks are increasingly dependent on large-scale pretrained transformers, this work presents a viable approach to improving inference efficiency without repeating the expensive pretraining process. – <span class="citation" data-cites="kasai2021finetuningpretrainedtransformersrnns">(Kasai et al. 2021)</span></p>
</blockquote>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-kasai2021finetuningpretrainedtransformersrnns" class="csl-entry">
Kasai, Jungo, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, and Noah A. Smith. 2021. <span>“Finetuning Pretrained Transformers into RNNs.”</span> <a href="https://arxiv.org/abs/2103.13076">https://arxiv.org/abs/2103.13076</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Finetuning {Pretrained} {Transformers} into {RNNs}},
  date = {2025-02-03},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/transforer-to-rnn/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Finetuning Pretrained Transformers into
RNNs.”</span> February 3, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/transforer-to-rnn/">https://orenbochman.github.io/notes-nlp/reviews/paper/transforer-to-rnn/</a>.
</div></div></section></div> ]]></description>
  <category>Paper</category>
  <category>Review</category>
  <category>LSTM</category>
  <category>RNN</category>
  <category>Transformer</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/transforer-to-rnn/</guid>
  <pubDate>Sun, 02 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>xLSTM: Extended Long Short-Term Memory</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="cover.jpg" class="lightbox" data-gallery="slides" title="Literature Review"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/cover.jpg" class="img-fluid figure-img" alt="Literature Review"></a></p>
<figcaption>Literature Review</figcaption>
</figure>
</div><div id="vid-02" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/0aWGTNS03PU?t=3" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Review by AI Bites
</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/0OaEv1a5jUM" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;2: Review by Yannic Kilcher
</figcaption>
</figure>
</div><div id="vid-03" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/8u2pW2zZLCs" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;3: LSTM: The Comeback Story? Talk with Sepp Hochreiter after the xLSTM paper
</figcaption>
</figure>
</div></div>


<blockquote class="blockquote">
<p>Whate’er the theme, the Maiden sang<br>
<mark>As if her song could have no ending;</mark><br>
I saw her singing at her work,<br>
And o’er the sickle bending;—<br>
I listened, motionless and still;<br>
And, as I mounted up the hill,<br>
<mark>The music in my heart I bore,<br>
Long after it was heard no more.</mark> &nbsp;</p>
<p>– The Solitary Reaper by William Wordsworth.</p>
</blockquote>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR – Scaling LSTMs to Billions of Parameters with <strong>xLSTM</strong>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/in_a_nutshell.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="LSTMs in a nutshell"><img src="https://orenbochman.github.io/notes-nlp/images/in_a_nutshell.jpg" class="img-fluid figure-img" alt="LSTMs in a nutshell"></a></p>
<figcaption>LSTMs in a nutshell</figcaption>
</figure>
</div>
<ul>
<li>Just when we think that the transformer rules supreme the RNN makes a comeback with a refreshing new look at the LSTMs.</li>
<li>This paper brings new architectures to the LSTM that are fully parallelizable and can scale to billions of parameters.</li>
<li>They demonstrate on synthetic tasks and language modeling benchmarks that these new xLSTMs can outperform state-of-the-art Transformers and State Space Models.</li>
</ul>
</div>
</div>
</div>
<section id="introduction" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Motivation
</div>
</div>
<div class="callout-body-container callout-body">
<p>So this is my first review of the RNN papers. I found that since the courses covering LSTM seemed to be over simplified and confusing, I should go back to the source. Since time is limited, it helps to review the papers in reverse chronological order. Each new papers explains the limitations that are generally not apparent when the paper is first published. The authors of the later papers also have the benefit of hindsight which they share in the obligatory literature review wherein they provide lots of context we cannot get by reading the original papers. Given all that there is also a benefit to reading the original paper - the real innovators while harder to understand are remarkable thinkers and often reveal motivations and deep insights that elude later interlocutors.</p>
</div>
</div>
<section id="sec-podcast" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-podcast">Podcast &amp; Other Reviews</h3>
<p>This paper is a bit heavy on the technical side. But if we are still here, we may enjoy this podcast reviewing the paper. Though AI generated I found it quite useful to get oriented with the paper and understand that the experiments the authors run to validate this new architectures.</p>
<p>We also cover something I found absent from the Course on sequence models . The limitations of LSTMs as viewed by the authors when considered in the context of transformers and Mamba.</p>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
<p>Although this paper is recent there are a number of other people who cover it.</p>
<ul>
<li>In Video&nbsp;2 Yannic Kilcher covers the paper in his youtube channel. He does a great job of explaining the paper in a way that is easy to understand. Kilcher is all too human, he makes many mistakes, stresses opinions over facts. If we don’t mind all that he is a great resource for understanding the paper. I find he rubs me the wrong way but at least I find this motivating to do better in my own reviews!</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
What can we expect from xLSTM?
</div>
</div>
<div class="callout-body-container callout-body">

<blockquote class="blockquote">
<p>xLSTM has the potential to considerably impact other fields like Reinforcement Learning, Time Series Prediction, or the modeling of physical systems. – <span class="citation" data-cites="beck2024xlstm">(Beck et al. 2024)</span></p>
</blockquote>
<p>In his book Understanding Media <span class="citation" data-cites="mcluhan1988understanding">(McLuhan 1988)</span> Marshal McLuhan introduced his <strong>Tetrad</strong>. <mark>The <strong>Tetrad</strong> is a mental model for understanding how a technological innovation like the xLSTM might disrupt society</mark>. To work the model we ask the following four questions that can be applied to any new technology to understand its impact. The four questions in this instance are:</p>
<ol type="1">
<li>What does the xLSTM enhance or amplify?</li>
</ol>
<blockquote class="blockquote">
<p>The xLSTM enhances the LSTM by introducing exponential gating and novel memory structures, boosting its capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models.</p>
</blockquote>
<ol start="2" type="1">
<li>What does the xLSTM make obsolete or replace?</li>
</ol>
<blockquote class="blockquote">
<p>The xLSTM makes the traditional LSTM obsolete by addressing its limitations, such as the inability to revise storage decisions, limited storage capacities, and lack of parallelizability.</p>
</blockquote>
<ol start="3" type="1">
<li>What does the xLSTM retrieve that was previously obsolesced?</li>
</ol>
<blockquote class="blockquote">
<p>The xLSTM retrieves the LSTM’s relevance in large-scale language modeling by scaling it to billions of parameters and integrating modern LLM techniques while mitigating known limitations. 4. What does the xLSTM reverse or flip into when pushed to its limits?</p>
</blockquote>
<blockquote class="blockquote">
<p>The xLSTM, when pushed to its limits, can compete with current Large Language Models based on Transformer technology, offering a competitive alternative for sequence modeling tasks.</p>
</blockquote>
<p>The xLSTM paper by <span class="citation" data-cites="beck2024xlstm">(Beck et al. 2024)</span> is a great example of the tetrad in action. The authors take the LSTM, a technology that has stood the test of time, and enhance it with exponential gating and novel memory structures. They address the limitations of traditional LSTMs and push the technology to new limits, creating a new architecture that can compete with state-of-the-art Transformers and State Space Models. Recent work by <span class="citation" data-cites="chen2024computationallimitsstatespacemodels">(Chen et al. 2024)</span> suggest that Transformers and The State Space Models are actually limited in their own ways.</p>
</div>
</div>
<div class="no-row-height column-margin column-container"><div id="sup-01" class="quarto-float quarto-figure quarto-figure-center anchored callout-margin-content">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="MediaTetrad.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Tetrad"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/MediaTetrad.svg" class="img-fluid figure-img"></a></p>
<figcaption>Tetrad</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div></section>
</section>
<section id="abstract" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining:</p>
<ol type="1">
<li>sLSTM with a scalar memory, a scalar update, and new memory mixing,<br>
</li>
<li>mLSTM that is fully parallelizable with a matrix memory and a covariance update rule.<br>
</li>
</ol>
<p>Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling. — <span class="citation" data-cites="beck2024xlstm">(Beck et al. 2024)</span></p>
</blockquote>

<div class="no-row-height column-margin column-container"><div id="fig-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="fig01.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="architecture"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/fig01.png" class="img-fluid figure-img"></a></p>
<figcaption>architecture</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: The extended LSTM (xLSTM) family. From left to right:<br>
1. The original LSTM memory cell with constant error carousel and gating.<br>
2. New <strong>sLSTM</strong> and <strong>mLSTM</strong> memory cells that introduce exponential gating. sLSTM offers a new memory mixing technique. <strong>mLSTM</strong> is fully parallelizable with a novel matrix memory cell state and new covariance update rule.<br>
3. mLSTM and sLSTM in residual blocks yield xLSTM blocks.<br>
4. Stacked xLSTM blocks give an xLSTM
</figcaption>
</figure>
</div><div id="fig-02" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="fig02.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="blocks"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/fig02.png" class="img-fluid figure-img"></a></p>
<figcaption>blocks</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: LSTM limitations.<br>
- Left: Nearest Neighbor Search problem in terms of mean squared error (MSE). Given a reference vector, a sequence is scanned sequentially for the most similar vector with the objective to return its attached value at sequence end. LSTM struggles to revise a stored value when a more similar vector is found. Our new xLSTM overcomes this limitation by exponential gating. - Right: Rare Token Prediction. The perplexity (PPL) of token prediction on Wikitext-103, in partitions of token frequency. LSTM performs worse on predicting rare tokens because of its limited storage capacities, whereas our new xLSTM solves this problem via a matrix memory.
</figcaption>
</figure>
</div><div id="fig-03" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="fig03.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="blocks"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/fig03.png" class="img-fluid figure-img"></a></p>
<figcaption>blocks</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: xLSTM blocks.<br>
Left: A residual sLSTM block with post up-projection (like Transformers): The input is fed into an sLSTM — with an optional convolution — followed by a gated MLP.<br>
Right: A residual mLSTM block with pre up-projection (like State Space models): mLSTM is wrapped inside two MLPs, via a convolution, a learnable skip connection, and an output gate that acts componentwise.
</figcaption>
</figure>
</div><div id="fig-04" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-04-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="fig04.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="blocks"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/fig04.png" class="img-fluid figure-img"></a></p>
<figcaption>blocks</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-04-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: xLSTM blocks.<br>
Left: A residual sLSTM block with post up-projection (like Transformers): The input is fed into an sLSTM — with an optional convolution — followed by a gated MLP.<br>
Right: A residual mLSTM block with pre up-projection (like State Space models): mLSTM is wrapped inside two MLPs, via a convolution, a learnable skip connection, and an output gate that acts componentwise.
</figcaption>
</figure>
</div><div id="fig-05" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-05-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="fig05.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="blocks"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/fig05.png" class="img-fluid figure-img"></a></p>
<figcaption>blocks</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-05-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: xLSTM blocks.<br>
Left: A residual sLSTM block with post up-projection (like Transformers): The input is fed into an sLSTM — with an optional convolution — followed by a gated MLP.<br>
Right: A residual mLSTM block with pre up-projection (like State Space models): mLSTM is wrapped inside two MLPs, via a convolution, a learnable skip connection, and an output gate that acts componentwise.
</figcaption>
</figure>
</div></div>



</section>
<section id="sec-outline" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-outline">Paper Outline</h2>
<ul>
<li>Introduction
<ul>
<li>Describes the constant error carousel and gating mechanisms of Long Short-Term Memory (LSTM). &gt; “In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories…”</li>
<li>Discusses three main limitations of LSTMs:
<ol type="1">
<li>The inability to revise storage decisions.</li>
<li>Limited storage capacities.</li>
<li>Lack of parallelizability due to memory mixing.</li>
</ol></li>
<li>Highlights <mark>the emergence of Transformers in language modeling due to these limitations</mark>. &gt; Transformer Era: The rise of Transformers with parallelizable self-attention has largely overshadowed LSTMs due to their superior scalability.</li>
</ul></li>
<li>Extended Long Short-Term Memory
<ul>
<li>Introduces Extended Long Short-Term Memory (xLSTM), which incorporates exponential gating and novel memory structures.</li>
<li>Presents two new LSTM variants:
<ol type="1">
<li>sLSTM with a scalar memory, a scalar update, and new memory mixing.</li>
<li>mLSTM with a matrix memory and a covariance update rule, which is fully parallelizable.</li>
</ol></li>
<li>Describes the integration of these LSTM variants into residual block modules, resulting in xLSTM blocks.</li>
<li>Presents xLSTM architectures that residually stack xLSTM blocks.</li>
</ul></li>
<li>Related Work:
<ul>
<li>Mentions various linear attention methods to overcome the quadratic complexity of Transformer attention.</li>
<li>Notes the popularity of State Space Models (SSMs) for language modeling.</li>
<li>Highlights the use of Recurrent Neural Networks (RNNs) as a replacement for Transformers.</li>
<li>Mentions the use of <strong>gating</strong> in recent RNN and SSM approaches.</li>
<li>Notes the use of <strong>covariance update rules</strong><sup>1</sup> to enhance storage capacities in various models.</li>
</ul></li>
<li>Experiments
<ul>
<li>Presents the experimental evaluation of xLSTM compared to existing methods with a focus on language modeling.</li>
<li>Discusses the effectiveness of xLSTM on synthetic tasks, including:
<ul>
<li>Formal languages.</li>
<li>Multi-Query Associative Recall.</li>
<li>Long Range Arena tasks.</li>
</ul></li>
<li>Presents a method comparison and ablation study, where different models are trained on 15 billion tokens from the SlimPajama dataset.</li>
<li>Assesses the scaling behavior of different methods based on validation perplexity.</li>
<li>Conducts a large-scale language modeling experiment:
<ul>
<li>Training different model sizes on 300B tokens from SlimPajama.</li>
<li>Evaluating models on length extrapolation.</li>
<li>Assessing models on validation perplexity and performance on downstream tasks.</li>
<li>Evaluating models on 571 text domains of the PALOMA language benchmark dataset.</li>
<li>Assessing the scaling behavior with increased training data.</li>
</ul></li>
</ul></li>
<li>Limitations
<ul>
<li>Highlights limitations of xLSTM, including:
<ul>
<li>Lack of parallelizability for sLSTM due to memory mixing.</li>
<li>Unoptimized CUDA kernels for mLSTM.</li>
<li>High computational complexity of mLSTM’s matrix memory.</li>
<li>Memory overload for longer contexts due to the independence of mLSTM’s memory from sequence length.</li>
</ul></li>
</ul></li>
<li>Conclusion
<ul>
<li>Concludes that xLSTM performs favorably in language modeling compared to Transformers and State Space Models.</li>
<li>Suggests that larger xLSTM models will be competitive with current Large Language Models based on Transformer technology.</li>
<li>Notes the potential impact of xLSTM on other deep learning fields such as reinforcement learning, time series prediction, and physical system modeling.</li>
</ul></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;explain covariance</p></div></div></section>
<section id="briefing-xlstm---extended-long-short-term-memory" class="level2">
<h2 class="anchored" data-anchor-id="briefing-xlstm---extended-long-short-term-memory">Briefing : xLSTM - Extended Long Short-Term Memory</h2>
<section id="introduction-and-motivation" class="level3">
<h3 class="anchored" data-anchor-id="introduction-and-motivation">Introduction and Motivation:</h3>
<p><strong>LSTM’s Legacy</strong>: The Long Short-Term Memory (LSTM), introduced in the 1990s, has been a cornerstone of deep learning, especially in sequence modeling and early Large Language Models (LLMs). LSTMs are known for their “constant error carousel” and gating mechanisms, designed to combat vanishing gradients in recurrent neural networks.</p>
<blockquote class="blockquote">
<p>“In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories…”</p>
</blockquote>
<p>Transformer Era: The rise of Transformers with parallelizable self-attention has largely overshadowed LSTMs due to their superior scalability.</p>
<blockquote class="blockquote">
<p>“However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale.” <strong>xLSTM Question</strong>: This research aims to explore the capabilities of LSTMs when scaled to billions of parameters, by integrating modern LLM techniques while addressing known limitations.</p>
</blockquote>
<blockquote class="blockquote">
<p>“We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs?”</p>
</blockquote>
</section>
<section id="key-limitations-of-traditional-lstms" class="level3">
<h3 class="anchored" data-anchor-id="key-limitations-of-traditional-lstms">Key Limitations of Traditional LSTMs:</h3>
<p>The paper identifies three major limitations of traditional LSTMs:</p>
<ol type="1">
<li>Inability to Revise Storage Decisions: LSTMs struggle to update previously stored information when more relevant information arrives later in the sequence. The paper uses a “Nearest Neighbor Search” task to demonstrate this.</li>
</ol>
<blockquote class="blockquote">
<p>“LSTM struggles to revise a stored value when a more similar vector is found…”</p>
</blockquote>
<ol start="2" type="1">
<li>Limited Storage Capacity: LSTMs store information in scalar cell states, which can lead to information compression and loss. This impacts performance on tasks requiring detailed memory, like predicting rare tokens.</li>
</ol>
<blockquote class="blockquote">
<p>“LSTM performs worse on predicting rare tokens because of its limited storage capacities…”</p>
</blockquote>
<ol start="3" type="1">
<li>Lack of Parallelizability: LSTMs process data sequentially due to the recurrent connections between hidden states, hindering parallel computation.</li>
</ol>
<blockquote class="blockquote">
<p>“Lack of parallelizability due to memory mixing, i.e., the hidden-hidden connections between hidden states from one time step to the next, which enforce sequential processing.”</p>
</blockquote>
<ol start="3" type="1">
<li>xLSTM Innovations:</li>
</ol>
<p>The paper introduces Extended Long Short-Term Memory (xLSTM), designed to address the above limitations, by modifying the original LSTM using two main approaches:</p>
<p><strong>Exponential Gating</strong>:Implemented to allow for better revision of storage decisions. Normalization and stabilization techniques are used to avoid overflows in calculations due to the exponential function.</p>
<blockquote class="blockquote">
<p>“To empower LSTMs with the ability to revise storage decisions, we introduce exponential gates (red) together with normalization and stabilization.” Novel Memory Structures: Two new LSTM variants are created with these structures:</p>
</blockquote>
<p><strong>sLSTM</strong>: A scalar memory LSTM with a scalar update and new memory mixing technique (within heads, not across). Features multiple memory cells and multiple heads, with memory mixing within each head.</p>
<blockquote class="blockquote">
<p>“The new sLSTM (see Section 2.2) with a scalar memory, a scalar update, and memory mixing…”</p>
</blockquote>
<p><strong>mLSTM</strong>: A matrix memory LSTM, fully parallelizable, with a covariance update rule for storing key-value pairs. Does not have memory mixing between cells or heads.</p>
<blockquote class="blockquote">
<p>“the new mLSTM (see Section 2.3) with a matrix memory and a covariance (outer product) update rule, which is fully parallelizable.”</p>
</blockquote>
<ol start="4" type="1">
<li>sLSTM Details:</li>
</ol>
<ul>
<li><strong>Exponential Gates</strong>: Introduces exponential activation functions for input and forget gates with normalization and a stabilizer state to prevent overflow.</li>
<li><strong>Normalizer State</strong>: A state used to normalize the hidden state output by accumulating a weighted sum of input and forget gates.</li>
<li><strong>Memory Mixing</strong>: sLSTM allows memory mixing between cells, and also introduces a new form of memory mixing through multiple heads, mixing between cells within a head, but not across heads.</li>
</ul>
<blockquote class="blockquote">
<p>“The new sLSTM can have multiple heads with memory mixing within each head but not across heads. The introduction of heads for sLSTM together with exponential gating establishes a new way of memory mixing.”</p>
</blockquote>
<ol start="5" type="1">
<li>mLSTM Details:</li>
</ol>
<p><strong>Matrix Memory</strong>: Replaces the scalar cell state with a matrix, increasing storage capacity.</p>
<p><strong>Key-Value Storage</strong>: The mLSTM memory is based around a key-value storage system inspired by Bidirectional Associative Memories (BAMs) using covariance updates, where keys and values are updated via an outer product.</p>
<blockquote class="blockquote">
<p>“At time <img src="https://latex.codecogs.com/png.latex?t">, we want to store a pair of vectors, the key <img src="https://latex.codecogs.com/png.latex?k_t%20%E2%88%88%20R%5Ed"> and the value <img src="https://latex.codecogs.com/png.latex?v_t%20%E2%88%88%20R%5Ed">… The covariance update rule… for storing a key-value pair…”</p>
</blockquote>
<p>Parallelization: mLSTM is fully parallelizable due to abandoning hidden-hidden recurrent connections and memory mixing. Computations can be performed efficiently on GPUs using standard matrix operations.</p>
<blockquote class="blockquote">
<p>“…the mLSTM… which is fully parallelizable.”</p>
</blockquote>
<ol start="6" type="1">
<li>xLSTM Architecture:</li>
</ol>
<p><strong>xLSTM Blocks</strong>: The sLSTM and mLSTM variants are integrated into residual blocks.</p>
<p><strong>sLSTM blocks</strong> use post up-projection (like Transformers).</p>
<p><strong>mLSTM blocks</strong> use pre up-projection (like State Space Models).</p>
<blockquote class="blockquote">
<p>“Integrating these new LSTM variants into residual block modules results in xLSTM blocks…”</p>
</blockquote>
<p><strong>Residual Stacking</strong>: xLSTM architectures are created by residually stacking these blocks, using pre-LayerNorm backbones similar to current LLMs.</p>
<blockquote class="blockquote">
<p>“An xLSTM architecture is constructed by residually stacking build-ing blocks…” <strong>Cover’s Theorem</strong>: Used to justify mapping information into a higher-dimensional space for better separation of histories or contexts.</p>
</blockquote>
<blockquote class="blockquote">
<p>“We resort to Cover’s Theorem (Cover, 1965), which states that in a higher dimensional space non-linearly embedded patterns can more likely be linearly separated than in the original space.”</p>
</blockquote>
<ol start="7" type="1">
<li>Performance and Scaling:</li>
</ol>
<p><strong>Linear Computation &amp; Constant Memory</strong>: Unlike Transformers, xLSTM networks have linear computation and constant memory complexity with respect to sequence length.</p>
<blockquote class="blockquote">
<p>“Contrary to Transformers, xLSTM networks have a linear computation and a constant memory complexity with respect to the sequence length.”</p>
</blockquote>
<p><strong>Synthetic Tasks</strong>: xLSTM showed improved performance over regular LSTMs on “Nearest Neighbor Search” and “Rare Token Prediction” synthetic tasks.</p>
<p><strong>SlimPajama Experiments</strong>: The xLSTM models were trained on 15B and 300B tokens from the SlimPajama dataset for language modelling evaluation.</p>
<p><strong>Competitive Performance</strong>: xLSTMs demonstrate competitive performance against state-of-the-art Transformers and State Space Models on language modelling benchmarks, both in terms of next token prediction perplexity and downstream tasks.</p>
<blockquote class="blockquote">
<p>“Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.”</p>
</blockquote>
<p>Ablation studies show importance of gating techniques.</p>
<ol start="8" type="1">
<li>Memory &amp; Speed:</li>
</ol>
<p><strong>sLSTM</strong>: Not parallelizable due to memory mixing, although the authors have created a fast CUDA implementation (less than twice as slow as mLSTM).</p>
<blockquote class="blockquote">
<p>“While mLSTM is parallelizable analog to FlashAttention… sLSTM is not parallelizable due to the memory mixing…”</p>
</blockquote>
<p><strong>mLSTM</strong>: Parallelizable, but currently slower than FlashAttention or Mamba due to non-optimized CUDA kernels. Matrix memory is computationally complex but does not require parameters and can be parallelized.</p>
<blockquote class="blockquote">
<p>“The memory of mLSTM does not require parameters but is computationally expensive through its d×d matrix memory and d× d update… the computations can be done in parallel on GPUs, therefore these computations have only a minor effect on the wall clock time.”</p>
</blockquote>
<ol start="9" type="1">
<li>Limitations:</li>
</ol>
<p><strong>sLSTM Parallelization</strong>: sLSTM’s memory mixing is non-parallelizable.</p>
<blockquote class="blockquote">
<p>“(i) In contrast to mLSTM, memory mixing of the sLSTM prohibits parallelizable operations, and therefore does not allow a fast parallel implementation.”</p>
</blockquote>
<p>mLSTM CUDA Optimization: The mLSTM CUDA kernels are not fully optimized.</p>
<blockquote class="blockquote">
<p>“(ii) The CUDA kernels for mLSTM are not optimized, and therefore the current implementation is about 4 times slower than FlashAttention or the scan used in Mamba.”</p>
</blockquote>
<p><strong>mLSTM Matrix Memory</strong>: High computational complexity for mLSTM due to matrix memory operations.</p>
<p><strong>Forget Gate Initialization</strong>: Careful initialization of the forget gates is needed.</p>
<p><strong>Long Context Memory</strong>: The matrix memory is independent of sequence length, and might overload memory for long context sizes.</p>
<blockquote class="blockquote">
<p>“(v) Since the matrix memory is independent of the sequence length, increasing the sequence length might overload the memory for longer context sizes.”</p>
</blockquote>
<p><strong>Hyperparameter Optimization</strong>: xLSTM is not yet fully optimized due to computational load, indicating further performance gains are possible.</p>
<blockquote class="blockquote">
<p>“(vi) Due to the expensive computational load for large language experiments, we did neither fully optimize the architecture nor the hyperparameters, especially for larger xLSTM architectures.”</p>
</blockquote>
<ol start="10" type="1">
<li>Related Work:</li>
</ol>
<p>The paper highlights connections of its ideas with the following areas:</p>
<p><strong>Gating</strong>: Many recent models such as HGRN, GLA, Gated State Space models, Mamba, and others also utilize gating mechanisms. Covariance Update Rule: The covariance update rule used in mLSTM is related to other methods such as Fast Weight Programmers, Retention, and Linear Transformers.</p>
<ol start="11" type="1">
<li>Conclusion:</li>
</ol>
<p>The xLSTM presents a compelling alternative to Transformers in sequence modeling, especially in contexts where memory efficiency and longer sequences are required. By combining insights from the original LSTM with exponential gating and novel memory structures, the paper shows that LSTMs can be competitive at scale. While limitations exist in terms of parallelization and hyperparameter tuning, the research shows promising potential for further development. The authors posit further research and improvements could allow xLSTM to reach its full potential.</p>
</section>
</section>
<section id="my-thoughts" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="my-thoughts">My Thoughts</h2>

<div class="no-row-height column-margin column-container"><div id="vid-sigmoid" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-sigmoid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/TPqr8t919YM" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-sigmoid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;4: Review of the sigmoid function
</figcaption>
</figure>
</div></div><div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Research questions
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li>How does the constant error carousel mitigate the vanishing gradient problem in the LSTM?
<ul>
<li>The constant error carousel is a way to keep the error constant over time. This helps to prevent the vanishing gradient problem in the LSTM.</li>
</ul></li>
<li>How are the gates in the original LSTM binary?
<ul>
<li>Sigmoid, saturation and a threshold at 0.5</li>
</ul></li>
<li>What is the long term memory in the LSTM?
<ul>
<li>the cell state <img src="https://latex.codecogs.com/png.latex?c_%7Bt-1%7D"></li>
</ul></li>
<li>What is the short term memory in the LSTM?
<ul>
<li>the hidden state <img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D"></li>
</ul></li>
<li>How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs</li>
</ol>
</div>
</div>

<div class="no-row-height column-margin column-container"><div id="sup-02" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="sigmoid-limits.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Supplementary Figure&nbsp;2: limits of the sigmoid function"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/sigmoid-limits.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup" id="sup-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;2: limits of the sigmoid function
</figcaption>
</figure>
</div><div id="sup-03" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="sigmoid-values.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="bias"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/sigmoid-values.png" class="img-fluid figure-img"></a></p>
<figcaption>bias</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup" id="sup-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;3: The inductive bias of the sigmoid decision function.
</figcaption>
</figure>
</div></div>
<section id="binary-nature-of-non-exponential-gating-mechanisms" class="level3">
<h3 class="anchored" data-anchor-id="binary-nature-of-non-exponential-gating-mechanisms">Binary nature of non exponential Gating Mechanisms</h3>
<p>If we try to understand why are the gating mechanisms in the original LSTM is described here as binary?</p>
<p>It helps to the properties of the sigmoid functions that are explained in Video&nbsp;4.</p>
<ol type="1">
<li><p>Supplementary Figure&nbsp;1 shows that The sigmoid function has a domain of <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D"> and a range of <img src="https://latex.codecogs.com/png.latex?(0,1)">. This means that the sigmoid function can only output values between 0 and 1.</p></li>
<li><p>It has a very sharp gradient around 0 if the bias is set to 0. This means that the sigmoid function acts as a binary function around 0.</p></li>
<li><p>The Sigmoid function tends to saturate. Now sigmoid function around 0 isn’t quite quite binary but outside of (-6,+6) it becomes saturated and acts very much as a binary function.</p></li>
<li><p>If we use it as a loss function or a decision function, the sigmoid function is used with a threshold of 0.5. This means that if the output of the sigmoid function is greater than 0.5, the decision is 1, and if it is less than 0.5, the decision is 0. That is how the sigmoid function can become binary.</p></li>
<li><p>Note that we can use thresholding during inference to make the decision binary and we can keep them as continuous values during training so we can have a smooth gradient for back-propagation.</p></li>
<li><p>Even without thresholding the sigmoid function can become binary this is due to the saturation of the sigmoid function. I recall that Hinton in his Coursera course on Neural Networks and Deep Learning mentioned that activation function that become saturated are problematic. I can say that at last I think I understand what he meant:<br>
I see that that sigmoid functions can fall off the manifold of the data and this can be a problem. This is why the ReLU function is used in the Gated Recurrent Unit (GRU) and the Transformer. <strong>Falling off the manifold of the data</strong> means that the sigmoid function becomes saturated (above 6 and below -6). At this point the back-propagation steps are no longer able to change the weight or bias since the gradient is effectively so learning stops.</p></li>
</ol>
<p>This becomes a is a problem in two senses. The sigmoid unit is likely to be stuck in a setting that only representative of part of the training data. The rest of the network which is not stuck now has to learn to compensate for this.</p>
<p>This issues may well be the root cause LSTM limitation number 1 - the inability to revise storage decisions. It seems that the authors decided to use the exponential gating function to address this issues.</p>
<p>This is why the gating mechanisms in the original LSTM are binary. The sigmoid function is used to control the flow of information in the LSTM by deciding which information to keep and which to discard. The sigmoid function is used to decide how much of the new input information should be added to the cell state and how much of the old cell state should be retained. This binary nature of the sigmoid function is what allows the LSTM to control the flow of information and decide what to remember and what to forget.</p>
</section>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li><p><a href="https://openreview.net/forum?id=ARAxPPIAhq&amp;noteId=gra7vHnb0q">The paper on open review</a> has some additional insights from the authors</p></li>
<li><p><a href="https://www.ai-bites.net/xlstm-extended-long-short-term-memory-networks/">XLSTM — Extended Long Short-Term Memory Networks</a> By Shrinivasan Sankar — May 20, 2024</p></li>
</ul>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-beck2024xlstm" class="csl-entry">
Beck, Maximilian, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, Günter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. 2024. <span>“xLSTM: Extended Long Short-Term Memory.”</span> <em>arXiv Preprint arXiv:2405.04517</em>.
</div>
<div id="ref-chen2024computationallimitsstatespacemodels" class="csl-entry">
Chen, Yifang, Xiaoyu Li, Yingyu Liang, Zhenmei Shi, and Zhao Song. 2024. <span>“The Computational Limits of State-Space Models and Mamba via the Lens of Circuit Complexity.”</span> <a href="https://arxiv.org/abs/2412.06148">https://arxiv.org/abs/2412.06148</a>.
</div>
<div id="ref-mcluhan1988understanding" class="csl-entry">
McLuhan, Marshall. 1988. <em>Understanding Media : The Extensions of Man</em>. New York: New American Library. <a href="http://www.worldcat.org/search?qt=worldcat_org_all&amp;q=0451624963">http://www.worldcat.org/search?qt=worldcat_org_all&amp;q=0451624963</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {xLSTM: {Extended} {Long} {Short-Term} {Memory}},
  date = {2025-02-03},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“xLSTM: Extended Long Short-Term
Memory.”</span> February 3, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/">https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/</a>.
</div></div></section></div> ]]></description>
  <category>Paper</category>
  <category>Review</category>
  <category>NLP</category>
  <category>LSTM</category>
  <category>Seq2Seqs</category>
  <category>RNN</category>
  <category>Podcast</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/2025-02-03-xLSTM/</guid>
  <pubDate>Sun, 02 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>The Secret Life of Pronouns What Our Words Say About Us</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/book/seret-life-of-pronouns/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="cover.jpg" class="lightbox" data-gallery="slides" title="cover"><img src="https://orenbochman.github.io/notes-nlp/reviews/book/seret-life-of-pronouns/cover.jpg" class="img-fluid figure-img" alt="cover"></a></p>
<figcaption>cover</figcaption>
</figure>
</div></div><div class="page-columns page-full"><blockquote class="blockquote">
<p>To grunt and sweat under a weary life, But that the dread of something after death, <mark>The undiscovered country from whose bourn No traveler returns, puzzles the will, And makes us rather bear those ills we have, Than fly to others that we know not of</mark>?<sup>1</sup> — Hamlet Act 3, Scene 1 by William Shakespeare.</p>
</blockquote><div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Pronouns oft fall prey to the stop word filter, yet they hold the keys to unlocking the depth of intimate meaning</p></div></div></div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR – ML, NLP and the secret life of pronouns
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/in_a_nutshell.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Pronouns in a nutshell"><img src="https://orenbochman.github.io/notes-nlp/images/in_a_nutshell.jpg" class="img-fluid figure-img" alt="Pronouns in a nutshell"></a></p>
<figcaption>Pronouns in a nutshell</figcaption>
</figure>
</div>
<ul>
<li>When I got started with NLP I was coding search engines and conventional wisdom was <mark>pronouns don’t pass the <strong>stop word filter</strong></mark></li>
<li>One of the gems I learned on that job was that <mark>everything in the corpus can be provide invaluable information if we only know how to index it</mark>.</li>
<li>After reading this book and I started to unlocked the power of the pronouns.</li>
<li>At Hungarian School I discovered how pronouns combine with case ending to create a vast vistas of untapped NLP resource.</li>
<li>Cognitive AI I noted that pronouns and particles are key in extending the primitive verb system via thematic roles to get a very rich semantic systems in the lexicon with little costs in learning.</li>
</ul>
</div>
</div>
</div>
<section id="podcast" class="level2">
<h2 class="anchored" data-anchor-id="podcast">Podcast</h2>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The secret life of pronouns in context
</div>
</div>
<div class="callout-body-container callout-body">
<p>I got interested in this book when I was deep into wikipedia research. I was in a place where I wanted to be able to attribute content to the authors. I had read some disturbing articles with prior work on the topic by some researchers from my Alma Mater. However, I eventually came across this book and it seemed to put the ideas in a better form. Less about ML and more about the type of writers and their personalities.</p>
</div>
</div>
<p>In <span class="citation" data-cites="pennebaker2013secret">(Pennebaker 2013)</span> “The Secret Life of Pronouns: What Our Words Say About Us,” the author explores how the seemingly innocuous function words we use—pronouns, articles, prepositions, and auxiliary verbs—can reveal a great deal about our personalities, emotions, social connections, and psychological states. James W. Pennebaker, the author and a social psychologist, uses computer-based text analysis methods to uncover these hidden patterns.</p>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li><p><strong>Key Questions and Themes:</strong></p>
<ul>
<li><strong>Can language reveal psychological states?</strong> The author is primarily interested in what people’s words say about their psychological states. The book investigates whether language features can reveal how people are thinking and whether this information can be used to change their thinking.</li>
<li><strong>How do function words differ from content words?</strong> The book distinguishes between content and style of speech, arguing that function words, which reflect language style, reveal more about a person than content words. Function words are tied to the relationship between speaker and listener.</li>
<li><strong>Do men and women use words differently?</strong> The book explores gender differences in language use, noting that women are typically more attentive to the thoughts and feelings of others, while men are less so. It also investigates whether different writing styles reflect different personalities.</li>
<li><strong>Can language predict behavior?</strong> The book examines whether language use can predict behaviors such as success in college, risk of suicide, leadership styles, and likelihood of declaring war.</li>
<li><strong>How can language be used as a tool for change?</strong> The author examines whether analyzing words can lead to improvements in relationships, teaching, and leadership. Additionally, the book looks at how writing about emotional upheavals can improve mental and physical health, and whether using certain types of words can influence the effectiveness of that writing.</li>
<li><strong>Can language reveal deception?</strong> The book explores linguistic markers of deception, such as the use of first-person singular pronouns and the mention of other individuals.</li>
<li><strong>Can language analysis help identify authors?</strong> The book presents methods for identifying authors using function words, punctuation, and obscure words.</li>
</ul></li>
<li><p><strong>Main Examples and Studies:</strong></p>
<ul>
<li><strong>Expressive Writing:</strong> Early research focused on the benefits of writing about traumatic experiences, leading to the development of the LIWC (Linguistic Inquiry and Word Count) program. This program helps analyze word categories associated with healthy writing such as positive emotion words, cognitive words, and the ability to construct a story.</li>
<li><strong>The Bottle and the Two People Pictures:</strong> Participants described pictures, and their descriptions were analyzed for function word use and content. For example, people who focused on shadows in the bottle picture tended to be more creative. The second picture revealed common themes such as social connections, clothing, and accessories.</li>
<li><strong>Thinking Styles:</strong> The book describes how people’s function words can be statistically grouped into categories reflecting thinking styles, such as formal, analytic, and narrative. These thinking styles can predict how people relate to others and organize their worlds.</li>
<li><strong>9/11 Blog Analysis:</strong> The language used in blog posts was analyzed before and after the 9/11 attacks. Researchers noted an increase in negative emotion words, a decrease in positive emotion words, and a temporary increase in cognitive words.</li>
<li><strong>College Admissions Essays:</strong> The study examined whether the writing style in college admissions essays could predict college grades.</li>
<li><strong>The Federalist Papers:</strong> The book discusses the use of function words to determine the authorship of the anonymous Federalist Papers.</li>
<li><strong>Language Style Matching (LSM):</strong> LSM is a technique to determine how similarly two people use function words. It is used in studies of couples, speed dating, and even the writings of historical figures to analyze the relationship between the authors.</li>
<li><strong>Obama’s Pronoun Use</strong>: The book analyzes Barack Obama’s use of first-person singular pronouns. His relatively low use of I-words is interpreted as a sign of self-confidence.</li>
</ul></li>
<li><p><strong>Additional Insights:</strong></p>
<ul>
<li><strong>Stealth Words:</strong> The book emphasizes the importance of function words, often called “stealth words,” which are often overlooked.</li>
<li><strong>The Role of Computers:</strong> Computer technology has revolutionized the study of language, allowing researchers to analyze large datasets and uncover hidden patterns that would be impossible to see with manual analysis.</li>
<li><strong>Language as a Tool:</strong> Language can be used not only as a window into the inner workings of people, but also as a tool for guiding thinking and promoting change.</li>
<li><strong>Interdisciplinary Approach</strong>: The author’s work draws on multiple disciplines including psychology, linguistics, computer science, history, and political science.</li>
</ul></li>
</ul>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-pennebaker2013secret" class="csl-entry">
Pennebaker, J. W. 2013. <em>The Secret Life of Pronouns: What Our Words Say about Us</em>. Bloomsbury USA. <a href="https://books.google.co.il/books?id=p9KmCAAAQBAJ">https://books.google.co.il/books?id=p9KmCAAAQBAJ</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {The {Secret} {Life} of {Pronouns} {What} {Our} {Words} {Say}
    {About} {Us}},
  date = {2025-01-30},
  url = {https://orenbochman.github.io/notes-nlp/reviews/book/seret-life-of-pronouns/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“The Secret Life of Pronouns What Our Words
Say About Us.”</span> January 30, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/book/seret-life-of-pronouns/">https://orenbochman.github.io/notes-nlp/reviews/book/seret-life-of-pronouns/</a>.
</div></div></section></div> ]]></description>
  <category>Review</category>
  <category>Book</category>
  <category>NLP</category>
  <category>Sentiment Analysis</category>
  <category>Sentiment Analysis</category>
  <category>Podcast</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/book/seret-life-of-pronouns/</guid>
  <pubDate>Wed, 29 Jan 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>When and Why are Pre-trained Word Embeddings Useful for Neural Machine Translation?</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/20180-PTWM-NMT/</link>
  <description><![CDATA[ 





<div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/o41avjKIg5o" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Talk covering this paper by Roee Aharoni
</figcaption>
</figure>
</div>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/literature-review-open-book.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Litrature review"><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="img-fluid figure-img" alt="Litrature review"></a></p>
<figcaption>Litrature review</figcaption>
</figure>
</div></div><div id="sup-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://roeeaharoni.com/NMT_slides_orginal.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Roee Aharoni’s slides"><embed src="https://roeeaharoni.com/NMT_slides_orginal.pdf" class="img-fluid"></a></p>
<figcaption>Roee Aharoni’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div>
<section id="podcast" class="level2">
<h2 class="anchored" data-anchor-id="podcast">Podcast</h2>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>The performance of Neural Machine Translation (NMT) systems often suffers in lowresource scenarios where sufficiently largescale parallel corpora cannot be obtained. Pretrained word embeddings have proven to be invaluable for improving performance in natural language analysis tasks, which often suffer from paucity of data. However, their utility for NMT has not been extensively explored. In this work, we perform five sets of experiments that analyze when we can expect pre-trained word embeddings to help in NMT tasks. We show that such embeddings can be surprisingly effective in some cases – providing gains of up to 20 BLEU points in the most favorable setting.. –<span class="citation" data-cites="qi-etal-2018-pre">(Qi et al. 2018)</span></p>
</blockquote>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li>**Introduction
<ul>
<li>Describes the problem of low-resource scenarios in Neural Machine Translation (NMT) and the potential utility of pre-trained word embeddings.</li>
<li>Highlights the success of pre-trained embeddings in natural language analysis tasks and the lack of extensive exploration in NMT.</li>
<li>Poses five researcb questions:
<ul>
<li>Q1 Is the behavior of pre-training affected by language families and other linguistic features of source and target languages? (§3)</li>
<li>Q2 Do pre-trained embeddings help more when the size of the training data is small? (§4)</li>
<li>Q3 How much does the similarity of the source and target languages affect the efficacy of using pre-trained embeddings? (§5)</li>
<li>Q4 Is it helpful to align the embedding spaces between the source and target languages? (§6)</li>
<li>Q5 Do pre-trained embeddings help more in multilingual systems as compared to bilingual systems? (§7)</li>
</ul></li>
</ul></li>
<li>Experimental Setup
<ul>
<li>Details the five sets of experiments conducted to evaluate the effectiveness of pre-trained word embeddings in NMT.</li>
<li>Describes the datasets used, including the WMT14 English-German and English-French translation tasks.</li>
<li>Outlines the models and training procedures employed in the experiments.</li>
</ul></li>
<li>Results and Analysis
<ul>
<li>Presents the results of the experiments, showing the impact of pre-trained word embeddings on NMT performance.</li>
<li>Discusses the observed gains in BLEU scores and the factors influencing the effectiveness of pre-trained embeddings.</li>
<li>Analyzes the relationship between the quality of pre-trained embeddings and the performance of NMT systems.</li>
</ul></li>
<li>Analysis
<ul>
<li>Considers the implications of the findings for NMT research and practice.</li>
<li>Discusses the potential benefits and limitations of using pre-trained word embeddings in NMT tasks.</li>
</ul></li>
<li>Conclusion
<ul>
<li>The sweet-spot is wgere there is very little training data yet enough to train the system.</li>
<li>PTWE are more effective if there are more similar translation pairs.</li>
<li>A priori alignment of embeddings may not be necessary in bilingual scenarios, but is helpful in multi-lingual training scenarios.</li>
</ul></li>
</ul>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-qi-etal-2018-pre" class="csl-entry">
Qi, Ye, Devendra Sachan, Matthieu Felix, Sarguna Padmanabhan, and Graham Neubig. 2018. <span>“When and Why Are Pre-Trained Word Embeddings Useful for Neural Machine Translation?”</span> In <em>Proceedings of the 2018 Conference of the North <span>A</span>merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</em>, edited by Marilyn Walker, Heng Ji, and Amanda Stent, 529–35. New Orleans, Louisiana: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/N18-2084">https://doi.org/10.18653/v1/N18-2084</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {When and {Why} Are {Pre-trained} {Word} {Embeddings} {Useful}
    for {Neural} {Machine} {Translation?}},
  date = {2024-02-11},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/20180-PTWM-NMT/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“When and Why Are Pre-Trained Word Embeddings
Useful for Neural Machine Translation?”</span> February 11, 2024. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/20180-PTWM-NMT/">https://orenbochman.github.io/notes-nlp/reviews/paper/20180-PTWM-NMT/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Bidirectional LSTM</category>
  <category>Deep learning</category>
  <category>Embeddings</category>
  <category>LLM</category>
  <category>NLP</category>
  <category>Paper</category>
  <category>Review</category>
  <category>Podcast</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/20180-PTWM-NMT/</guid>
  <pubDate>Sat, 10 Feb 2024 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Neural Machine Translation by Jointly Learning to Align and Translate</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/2014-NMT-by-jointly-learning-to-align-and-translate/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/literature-review-open-book.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Litrature review"><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="img-fluid figure-img" alt="Litrature review"></a></p>
<figcaption>Litrature review</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/o41avjKIg5o" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Roee Aharoni’s Talk covering this paper (Hebrew)
</figcaption>
</figure>
</div><div id="sup-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://roeeaharoni.com/NMT_slides_orginal.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Roee Aharoni’s slides"><embed src="https://roeeaharoni.com/NMT_slides_orginal.pdf" class="img-fluid"></a></p>
<figcaption>Roee Aharoni’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div>

<section id="podcast" class="level2">
<h2 class="anchored" data-anchor-id="podcast">Podcast</h2>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
</section>
<section id="abstract" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<div class="page-columns page-full"><blockquote class="blockquote">
<p><em>Neural machine translation</em> is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of <em>encoder-decoders</em> and consists of an encoder that encodes a source sentence into a <em>fixed-length vector</em><sup>1</sup> from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition. –<span class="citation" data-cites="bahdanau2016neuralmachinetranslationjointly">(Bahdanau, Cho, and Bengio 2016)</span></p>
</blockquote><div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;the encoded state</p></div></div></div>
</section>
<section id="outline" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li>Introduction
<ul>
<li>Positions NMT as a new approach, contrasting it with traditional phrase-based systems.</li>
<li>Describes encoder-decoder models that use a fixed-length vector to encode a source sentence.</li>
<li>Explains that these models are <mark>trained to maximize the probability of a correct translation.</mark></li>
<li>Discusses the <mark>limitation of compressing all source sentence information into <em>a fixed-length vector</em><sup>2</sup>, especially for long sentences.</mark></li>
<li><mark>Introduces the extension of the encoder-decoder model that jointly learns to align and translate.</mark></li>
<li>Emphasizes that the model searches for relevant source positions when generating a target word.</li>
<li>States that the new model encodes the input sentence into a sequence of vectors and adaptively chooses a subset during decoding.</li>
<li>Asserts that the new model performs better with long sentences and that the proposed approach performs better translation compared to the basic encoder-decoder approach.</li>
<li>Notes that the improvement is apparent with longer sentences and that the model achieves comparable performance to a conventional phrase-based system.</li>
<li>Mentions that the model finds linguistically plausible alignments.</li>
</ul></li>
<li>Background: <em>Neural Machine Translation</em> (NMT)
<ul>
<li><mark>Defines translation as <em>maximizing</em> the <strong>conditional probability of a target sentence given a source sentence</strong>.</mark> <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Barg%7D%5C,%5Cmax_%7By%7D%5C,%20%7Bp%7D(y%20%5Cmid%20x)."></li>
<li>Explains that <mark>NMT models learn this conditional distribution using <em>parallel training corpora</em>.</mark></li>
<li>Describes NMT models as consisting of <em>an encoder and a decoder</em>.</li>
<li>Notes that <em>Recurrent Neural Networks</em> (RNNs) have been used for encoding and decoding <em>variable-length sentences</em>.</li>
<li>Points out that NMT has shown promising results, and can achieve state-of-the-art performance.</li>
<li>Notes that adding <em>neural networks</em> to existing systems can boost performance levels.</li>
<li>RNN Encoder-Decoder
<ul>
<li>Describes the <em>RNN Encoder-Decoder framework</em>, where an encoder reads the input sentence into a vector c.</li>
<li>Explains that the <em>encoder</em> uses an RNN to generate <em>a sequence of hidden states</em>, from which the vector c is generated.</li>
<li>Notes that the <em>decoder</em> predicts the next word given the context vector and previously predicted words.</li>
<li>Presents a formula for the conditional probability, which is modeled with an RNN.</li>
</ul></li>
</ul></li>
<li>Learning to Align and Translate
<ul>
<li>Introduces a new architecture for NMT using a bidirectional RNN encoder and a decoder that searches through the source sentence.</li>
<li>Decoder: General Description
<ul>
<li>Presents a new conditional probability conditioned on a distinct context vector for each target word.</li>
<li>Explains that the context vector depends on a sequence of annotations from the encoder.</li>
<li>Defines the context vector as a weighted sum of annotations.</li>
<li>Describes how the weights are computed, using an alignment model.</li>
<li>Emphasizes that the alignment model computes a soft alignment.</li>
<li>Explains the weighted sum of annotations as computing an expected annotation.</li>
<li>States that the probability of the annotation reflects its importance in deciding the next state and generating the target word.</li>
<li>Notes that this implements an attention mechanism in the decoder.</li>
</ul></li>
<li>Encoder: Bidirectional RNN for Annotating Sequences
<ul>
<li>Introduces the use of a bidirectional RNN (BiRNN) to summarize preceding and following words.</li>
<li>Describes the forward and backward RNNs that comprise the BiRNN.</li>
<li>Explains how annotations are created by concatenating forward and backward hidden states.</li>
<li>Notes that the annotation will focus on words around the current word due to the nature of RNNs.</li>
</ul></li>
</ul></li>
<li>Experiment Settings
<ul>
<li>States that the proposed approach is evaluated on English-to-French translation using ACL WMT ’14 bilingual corpora.</li>
<li>Compares the approach with an RNN Encoder-Decoder.</li>
<li>Dataset
<ul>
<li>Details the corpora used, their sizes, and the data selection method used.</li>
<li>Notes that no monolingual data other than the mentioned parallel corpora is used.</li>
<li>Describes how the development and test sets were created.</li>
<li>Details the tokenization and word shortlist used for training the models.</li>
</ul></li>
<li>Models
<ul>
<li>Details the two types of models trained, RNN Encoder-Decoder (RNNencdec) and RNNsearch, and the sentence lengths used.</li>
<li>Describes the hidden units in the encoder and decoder for both models.</li>
<li>Mentions the use of a multilayer network with a maxout hidden layer.</li>
<li>Describes the use of minibatch stochastic gradient descent (SGD) and Adadelta for training, along with the minibatch size and training time.</li>
<li>Explains the use of beam search to find the translation that maximizes the conditional probability.</li>
<li>Refers to the appendices for more details on the architectures and training procedure.</li>
</ul></li>
</ul></li>
<li>Results
<ul>
<li>Quantitative Results
<ul>
<li>Presents translation performance measured in BLEU scores, showing that RNNsearch outperforms RNNencdec in all cases.</li>
<li>Notes that the performance of RNNsearch is comparable to that of the phrase-based system, even without using a separate monolingual corpus.</li>
<li>Shows that RNNencdec’s performance decreases with longer sentences.</li>
<li>Demonstrates that RNNsearch is more robust to sentence length, with no performance drop even with sentences of length 50 or more.</li>
<li>Highlights the superiority of RNNsearch by noting that RNNsearch-30 outperforms RNNencdec-50.</li>
<li>Presents a table of BLEU scores for each model.</li>
</ul></li>
<li>Qualitative Analysis
<ul>
<li>Alignment
<ul>
<li>Explains that the approach offers an intuitive way to inspect the soft alignment between words.</li>
<li>Describes visualizing annotation weights to see which source positions were considered important.</li>
<li>Notes the largely monotonic alignment of words, with strong weights along the diagonal.</li>
<li>Highlights examples of non-trivial alignments and how the model correctly translates phrases.</li>
<li>Explains the strength of soft-alignment using an example of the phrase “the man” being translated to “l’ homme”.</li>
<li>Notes that soft alignment deals naturally with phrases of different lengths.</li>
</ul></li>
<li>Long Sentences
<ul>
<li>Explains that the model does not require encoding a long sentence into a fixed-length vector perfectly.</li>
<li>Provides examples of translations of long sentences, showing that RNNencdec deviates from the source meaning.</li>
<li>Demonstrates that RNNsearch translates long sentences correctly, preserving the whole meaning.</li>
<li>Confirms that RNNsearch enables more reliable translation of long sentences than RNNencdec.</li>
</ul></li>
</ul></li>
</ul></li>
<li>Related Work
<ul>
<li>Learning to Align
<ul>
<li>Mentions a similar alignment approach used in handwriting synthesis.</li>
<li>Notes the key difference: in handwriting synthesis, the modes of the weights of the annotations only move in one direction.</li>
<li>Explains that, in machine translation, reordering is often needed, and the proposed approach computes annotation weight of every source word for each target word.</li>
</ul></li>
<li>Neural Networks for Machine Translation
<ul>
<li>Discusses the history of neural networks in machine translation, from providing single features to existing systems to reranking candidate translations.</li>
<li>Mentions examples of neural networks being used as a sub-component of existing translation systems.</li>
<li>Highlights that the paper focuses on designing a completely new translation system based on neural networks.</li>
</ul></li>
</ul></li>
<li>Conclusion
<ul>
<li>States that the paper proposes a new architecture that extends the basic model by allowing the model to (soft-)search for input words when generating a target word.</li>
<li>Highlights the benefit of freeing the model from encoding the whole sentence into a fixed-length vector.</li>
<li>Notes that <strong>all components are jointly trained towards a better log-probability of producing correct translations</strong>.</li>
<li>Confirms that the proposed RNNsearch outperforms the conventional encoder-decoder model and is more robust to sentence length.</li>
<li>Concludes that the model aligns each target word with the relevant words in the source sentence.</li>
<li>Points out that the proposed approach achieved translation performance comparable to the phrase-based statistical machine translation, despite its recent development.</li>
<li>Suggests that the approach is a promising step toward better machine translation and a better understanding of natural languages.</li>
<li>Identifies better handling of unknown or rare words as a challenge for the future.</li>
</ul></li>
<li>Appendix A: Model Architecture
<ul>
<li>Architectural Choices
<ul>
<li>Describes that the scheme is a general framework where the activation functions of RNNs and the alignment model can be defined.</li>
<li>Recurrent Neural Network
<ul>
<li>Explains the use of the <em>gated hidden unit</em> for the activation function, which is similar to LSTM units.</li>
<li>Provides details and equations for the computation of the RNN state using gated hidden units.</li>
<li>Explains how update and reset gates are computed.</li>
<li>Mentions the use of a multi-layered function with a single hidden layer of maxout units for computing the output probability.</li>
</ul></li>
</ul></li>
<li>Alignment Model
<ul>
<li>Explains the use of a single-layer multilayer perceptron for the alignment model.</li>
<li>Provides an equation describing the model and notes that some values can be pre-computed.</li>
</ul></li>
<li>Detailed Description of the Model
<ul>
<li>Encoder
<ul>
<li>Provides the equations and architecture details of the bidirectional RNN encoder.</li>
</ul></li>
<li>Decoder
<ul>
<li>Provides the equations and architecture details of the decoder with the attention mechanism.</li>
</ul></li>
<li>Model Size
<ul>
<li>Specifies the sizes of hidden layers, word embeddings, and maxout hidden layer.</li>
</ul></li>
</ul></li>
</ul></li>
<li>Appendix B: Training Procedure
<ul>
<li>Parameter Initialization
<ul>
<li>Describes the initialization of various weight matrices, including the use of random orthogonal matrices and Gaussian distributions.</li>
</ul></li>
<li>Training
<ul>
<li>Explains that the training is done with stochastic gradient descent (SGD) with Adadelta to adapt the learning rate.</li>
<li>Describes how the L2-norm of the gradient was normalized and that minibatches of 80 sentences are used.</li>
<li>Mentions sorting the sentence pairs and splitting them into minibatches.</li>
<li>Presents a table of learning statistics and related information.</li>
</ul></li>
</ul></li>
<li>Appendix C: Translations of Long Sentences
<ul>
<li>Presents sample translations generated by RNNenc-50, RNNsearch-50, and Google Translate.</li>
<li>Compares these translations with a reference (gold-standard) translation for each long source sentence.</li>
</ul></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;requiring the length to be fixed seems a mistake as sentences can sometimes get very long think hundreds of words</p></div></div></section>
<section id="reflections" class="level2">
<h2 class="anchored" data-anchor-id="reflections">Reflections</h2>
<p>Naively, translation is an easy task in the sense that we just need to look up each phrase in a statistically generated bi-lingual lexicon and append the translation of the phrase to the target sentence. In reality even when working with parallel text is tricky in that, the translation isn’t one to one but rather is n-m mappings between the source and target words with the number of words being variable. Picking the best entry in the lexicon is not obvious as the source words may be ambiguous and need some attention to other words in the context to disambiguate, and these too may be ambiguous ad infinitum… Finaly the translation may require some reordering of the words in the source sentence to conform to the target language’s grammar.</p>
<p>To sum up:</p>
<ul>
<li>We need to learn a bi-lingual phrase lexicon.</li>
<li>Each phrase may have multiple translations.</li>
<li>To pick the best entry the source context should be consulted.</li>
<li>The target sentence may require reordering to conform to the target language’s grammar.</li>
<li>Another challenge is that the target language may require words or even grammatical constructs e.g.&nbsp;gender and gender agreement that are lacking in the source language. These are <a href="../1997-floating-contraints/index.qmd">floating constraints</a> c.f. (<span class="citation" data-cites="mckeown1997floating">McKeown, Elhadad, and Robin (1997)</span>) that the model needs to propagate through the translation process.</li>
</ul>
<p>Each step of this process is probabilistic and thus prone to mistakes. Though there are two main constructs. The first is the contextual lexicon which needs to be learned using parallel text. The second is the destination grammar that can be learned as part of a language model using monolingual text. However reordering texts isn’t as much of a challenge, if we have sufficient parallel texts then the NMT hidden states should be able to learn the reordering as part of its hidden state.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-bahdanau2016neuralmachinetranslationjointly" class="csl-entry">
Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2016. <span>“Neural Machine Translation by Jointly Learning to Align and Translate.”</span> <a href="https://arxiv.org/abs/1409.0473">https://arxiv.org/abs/1409.0473</a>.
</div>
<div id="ref-mckeown1997floating" class="csl-entry">
McKeown, Kathleen, Michael Elhadad, and Jacques Robin. 1997. <span>“Floating Constraints in Lexical Choice.”</span>
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Neural {Machine} {Translation} by {Jointly} {Learning} to
    {Align} and {Translate}},
  date = {2024-02-11},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/2014-NMT-by-jointly-learning-to-align-and-translate/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Neural Machine Translation by Jointly
Learning to Align and Translate.”</span> February 11, 2024. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/2014-NMT-by-jointly-learning-to-align-and-translate/">https://orenbochman.github.io/notes-nlp/reviews/paper/2014-NMT-by-jointly-learning-to-align-and-translate/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Bidirectional LSTM</category>
  <category>Deep learning</category>
  <category>Embeddings</category>
  <category>LLM</category>
  <category>NLP</category>
  <category>NMT</category>
  <category>Paper</category>
  <category>Review</category>
  <category>Podcast</category>
  <category>Translation task</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/2014-NMT-by-jointly-learning-to-align-and-translate/</guid>
  <pubDate>Sat, 10 Feb 2024 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Typology: The Space of Languages</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w03-typology/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/tiling.png" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>course banner</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/iYmE6UCiOSQ" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Lesson Video
</figcaption>
</figure>
</div><div id="sup-slide-deck" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div>

<blockquote class="blockquote">
<p>We’ve always sent dances to tiktok as a way to communicate and I can’t remember when when we ever did anything else – Graham Neubig</p>
</blockquote>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>How to quantify similarity between languages</li>
<li>Language families and genealogical similarity</li>
<li>Linguistic typology and typological similarity</li>
<li>WALS and other typological databases</li>
<li>Typology Prediction / Typology-based language transfer (Lin et al.)</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- Transcript here is machine generated no punctuation and I eddited it a but but it needs more work and I dont see the benefits unless I can run it through some tool. -->
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<blockquote class="blockquote">
<p>I’m gonna run through this we have about half an hour i’m gonna talk about all languages on the planet basically not not not too big and of course there are a lot of languages and the question really is how can we try to structure that space we could look at them directly but we’d like to be able to gather them into groups or similarities and that’s where we come up with a more organized structural way and we call this linguistically and typology and so there is official ways to try to find out similarities between language and that’s what we’re going to talk about in this in this form so let’s have a look so first we’ve got all the languages in the planet we have a dot here and for each of the languages now this is very hard to do because languages are not point they’re spread over multiple places and of course they’re spread over multiple places that will overlap as well but you can see something about here that there are some places that seem to have more languages per you know square inch than others and it seems to be the the richer countries the ones where there’s a common education system travel is very easy and they have a long history of fighting each other with borders you’re more likely to find languages that cover whole what would we we call countries while when we’re looking at places that are been around for a very long time but might not have the same definition of border and there’s more languages and sometimes these languages are related to each other and sometimes they’re not sometimes they’re from rival groups and sometimes they’re just not related like korean is not related to any of the other languages in the area and there’s some relations to japanese but it’s really the japanese and korean are not like anything else and therefore they’re more like each other but only because they’re not like anything else in the area at all even though both of them have got some substantial chinese influence with words and english influence as well what’s the definition definition of a language well that’s sort of quite hard and there’s different definitions and we basically are coming down to whoever defines it but of course it become can become quite political we’ve talked about these things before urdu and hindi different languages well it depends what you’re talking about from a political point of view unquestionably from a linguistic point of view yeah they’re sort of different from a phonological point of view they’re not very different at all they really pretty much overlap and we have lots of examples like that</p>
</blockquote>
</section>
<section id="defining-languages" class="level3">
<h3 class="anchored" data-anchor-id="defining-languages">Defining Languages</h3>
<blockquote class="blockquote">
<p>There is the standard joke but it’s a good joke and it’s very relevant and that is “a language is a dialect with an army” and there certainly is political decisions when it comes to defining what the border actually is between two languages and often it’s quite weak between two languages and people can float between one and the next so depending on the area depending on the</p>
</blockquote>
<p>history depending on the politics depending to on the historical aspect of ethnic groups and travel etc defining what the languages actually are is still pretty hard but say in the in india there is something like about 460 languages which is a lot officially from the government i think it’s 21 if i remember correctly and which is about the same size as what europe is when it comes to official languages but of course as you look closer you discover that you start getting distinctions between languages which might be relatively small but they might be very big to the extent that these people absolutely can’t understand each other but there may be many people who speak multiple languages remember it’s really only america and britain where people only speak one language and only one language so it’s pretty rare actually in the world that people are only speaking one language</p>
</section>
<section id="language-families" class="level3">
<h3 class="anchored" data-anchor-id="language-families">Language Families</h3>
<blockquote class="blockquote">
<p>There are different language families and so linguists have decided looking at languages especially looking at things like where choice and a word overlap which might vary between languages but there might be relationships between languages that we can see language families that are actually sharing information like lexicons so the choice of the word the grammatical aspects the morphology act actually even though there could be differences they might be more similar and so across africa we can see something like regions of about six different major language families and you know we can see that geographical aspects madagascar is different but that’s because you know there’s a big sea between that in the mainland and that actually makes less chance for interaction to happen and therefore it’s sort of easier to keep that distinction so you find sometimes that languages have got borders that are quite geographical mountains rivers and the sea of course ### Online Languages</p>
</blockquote>
<blockquote class="blockquote">
<p>How many of these languages are online we’re all very aware that both you know english european languages chinese japanese korean are very well represented online and how many people within the country are actually getting online nowadays often many of the richer people are online and they can be quite substantial amounts of data online but also when you look at a major areas of wealth that have computers that have internet often there may be mixed languages and there may be a preferred language you often discover that in india even though there are lots of different languages in india that are native that people who are online are often using english or they’re writing in the romanized form rather than their native script because well history etc it’s easier they’re fluent in english there’s a lot of english influence that’s actually there china almost everything is written in putonghua stands mandarin even though there are many other dialects there so there’s not the same representation across everywhere because it depends on whether these people have access and that they’re willing to talk in these languages often they consider these to be spoken only or rarely written and so their language of literacy is some colonial language be that english and spanish french etc or swahili or arabic but it might be that it’s just easier for them to do that and therefore there’s a tradition to be able to communicate in one of the standard larger languages on the planet rather than their own language and so when we’re trying to do multilingual nlp we’re actually caring about these smaller languages that may not be as much online okay so how do we try to find similarity</p>
</blockquote>
</section>
<section id="similarity" class="level3">
<h3 class="anchored" data-anchor-id="similarity">Similarity</h3>
<blockquote class="blockquote">
<p>between languages well obviously the the clearest thing is what we can do is we can start looking at the words and so we can see whether there’s a shared information and we usually look for words not like iphone and computer that are relatively modern we look at words that have been in the language for a long time so this is often body parts family relationships core food aspects water air etc and there’s actually a specific list called the swabish list which you can be often used to compare similarities between languages now you want to be a little careful about caring about the writing systems so often that there’s a writing system that writes things in a very different way but you may discover that there’s actually a shared information between the language in the phonetic form that’s not there in the written form and therefore you can’t just use string match to find out whether it actually works or not there’s a number of online groups that try to identify all of the languages on the planet ethnologue is one of the best to try to do that it actually comes out the summer institute of linguistics which is a religious organization which is caring about translation of the bible but they’re quite independent in in doing their list mostly and it’s definitely the most comprehensive list in in the world glottalog tries to do a similar thing but it also is identifying this the position of the language on the planet and that’s not the only influence to similarity because people get on boats and cross oceans and we’re currently in the united states and we all know in this particular area english is not native at all and french is native no no french was there before english was there but there was earlier indigenous languages definitely iroquois and possibly a language called mingo is somewhat native and though the name allegheny is probably a mingle word for probably it’s a word for river maybe northern river because it’s the northern one in the obvious three rivers but glottalog tries to do this but also gives downloadable spreadsheets that allow you to be able to do a in other aspects now remember people move i mean huge populations have moved around the planet which has influenced how languages have moved and also trade has done a lot to have shared aspect of languages as well as grammatical history or linguistic history of languages and we want to be careful about that when we look at that distribution you know one might think that if you go to take the example of korean there are lots of english words in korean they have a different pronunciation from the english but they’re clearly derived from english but that doesn’t mean that there’s any linguistic relationship between korean and english it’s much more to do with english being the international language of the last hundred years and korea has picked up many of these more modern words into its language because that’s a convenient way to do it</p>
</blockquote>
</section>
<section id="genealogical-similarities" class="level3">
<h3 class="anchored" data-anchor-id="genealogical-similarities">Genealogical similarities</h3>
<blockquote class="blockquote">
<p>genealogical similarities so this is the language family and this is usually divine defined by linguists who make decisions about looking at the linguistic properties historically we used to do this in the animal kingdom before we could do dna tests and there are interesting errors in the dna tests for animals that two animals have come from different family and moved together because that’s a convenient co-evolution to end up with a different way but actually they’re quite got different histories but that might not be obvious and that’s going to happen with languages as well where things end up being borrowed maybe things get simplified over time we can see some examples of this but things also get more complex over time and there’s lots of interesting and boring from some of the major families that are out there niger congo in africa has got a lot of different languages and covers 21 of the languages spoken on the planet that’s a lot okay well if we look at something like indo-european that covers most of europe not all of europe and through the middle east at least the northern middle east m iran and into northern india and that’s a lot of people okay but it’s only about 6.3 of the languages because many of these languages are spoken by a very large number of people so you’re going to get very large numbers of people and also some of the links between languages that although you know german and english have lots of common lexical items that people can sort of work out and if they know english or they know german and be able to work out what the other one is but sometimes it’s not immediately obvious that there’s a relationship between the languages and very few of the words are actually overlapping lithuanian which is sometimes identified as the one that’s most archaic in the sense of it’s got more of the history of the original part of Indo-european and so does english but the relationship is not obvious at all to an english speaker</p>
</blockquote>
</section>
<section id="typological-similarities" class="level3">
<h3 class="anchored" data-anchor-id="typological-similarities">Typological similarities</h3>
<p>how do you work out these typological similarities well this is one of the major things that linguists have been doing for a long time and they’ve been looking at ways of linguistic properties to try to see what the similarities are between them and there’s a number of books and studies that try to collect that information together from multiple research studies okay now when we’re looking at phonology so the actual pronunciations the ipa is an excellent way for being able to split down the possible ways that most languages on the planet actually do their pronunciations and we have a vowel space that’s continuous vowel space and we split each language splits it into different ways and there’s often drifts between different languages that are maybe even predictable for consonants and things which are not vowels that’s probably the best definition of them there’s lots of things about a place of articulation in manner of articulation places where we put constrictions from the front of the mouth down to the back of the throat and we can sort of have things that deal with the lips with things like p and b things that deal with the teeth things like tea and things that deal with just behind the teeth which are things like okay and all of these may have different variations depending on the a on the languages that we’re actually speaking and have different distinctions in english we may produce some of these but we don’t make distinctions between them but for example a korean has got three different p’s that most english speakers would not distinguish between so when we’re looking at similarity between languages we could look at the similarity in the phonology and how many phonemes actually are common between the different languages now there’s actually a group called</p>
</section>
<section id="walls" class="level3">
<h3 class="anchored" data-anchor-id="walls">Walls</h3>
<p>walls now walls is also on a collection of all of these different typographic variations that are a over all of the languages and basically back in the early 2000s a group of people tried to start collecting papers and showing what the similarities so for the most part you can go to the walls and a website you can select some particular feature and you can see the distribution here we’ve got the distribution of the planet of different number bases so we all count in all languages some more than others and sometimes we use decimal and it’s probably related to the fact the number of fingers that we have but some count in twenties and that’s not really unusual even in english we have some residual twenties that’s there and even in chinese there’s some religious 20-ness where we have a specific word for 20 in english it’s score and here’s a mapping of all of the languages now we’re not saying that these languages are related to each other we’re saying that when we look at numbers all of the languages with the blue dot are counting in decimal basically well those for example that are in the purple pinkish dot are counting in twenties across and some don’t have good ways of doing that at all now walls is this excellent detailed form where you can go through and find different things you can find out which languages refer to t as t and which refers to it is chai which is quite interesting in itself some of them are maybe a little bit light-hearted and some of them are quite detailed like for example a word order or a default word order of in english we have a subject verb object japanese is a subject object verb now historically walls originally came in a book okay and this book is what’s called really really big and i’m sorry i had to take it from underneath my monitor because it normally keeps my monitor at the right level but the book of course isn’t updated but the website is and over the years the website gets more and more and people now actually think about registering the piece of work that they’re doing to be able to cover what’s actually there now not everything is in walls because not all of the features have been studied in all of the languages most linguists are going to study something that’s interesting so if they’re interested in something like voicing inconsonants after long vowels that’s only going to be interesting in some languages and other languages there’s just nobody’s going to study that and so the question is can we actually predict the missing feature from other factors because often there’s information that’s in there so for example in linguistics default word order seems to be less fixed when you have more morphology and that’s something to do with if you’ve got morphology it allows you to be able to identify who did what to whom better and therefore you don’t need to care about word order in the same importance level so there’s some predictable things so if somebody tells me something about a language i don’t know and says it’s a really rich morphology i i’ll think maybe it’s got free word order that’s not true for everything but there’s a more likelihood that it does if something doesn’t have lots of morphology it probably has more fixed order and can we learn this from the data by looking at all of the features all of the languages find the missing ones make predictions hold out the ones we do know and it may allow us to be able to make these predictions and lots of people have tried to do that at various levels and being quite successful and in fact the paper that we asked you to look at already has looked at some how well some of these predictive things actually work and of course some of them work better for some aspects than others okay sometimes you can do it purely unsupervised some sometimes you want to have supervised learning to be able to do this sometimes you want to make these predictions and go and explicitly ask somebody to to ask you ask you whether it’s correct or not.</p>
</section>
<section id="typological-databases" class="level3">
<h3 class="anchored" data-anchor-id="typological-databases">Typological databases</h3>
<p>There’s a number of these typological databases out there walls is only one it’s quite good it’s quite famous and there’s a number [Music] been derived from those or derived from multiple ones especially to fill in particular aspects of the features and these can be really useful in trying to do things when you’re doing multilingual modeling because you want to know maybe these features make a difference on my downstream or my predicted prediction task and i like to be able to get these features I’d like to have reliable features i’d like to know when the features are confident when the features are missing when the features are going to be more important than others so that maybe I look harder to be able to find these features here at cmu a number of years ago we had</p>
</section>
<section id="lorelai" class="level3">
<h3 class="anchored" data-anchor-id="lorelai">lorelai</h3>
<p>A project called lorelai which we’ll mention a few times in this course where we actually tried to build a specific vector that tries to represent a language so lang to vec so given the name of the language given the code of a language we’ll give a vector representation that will try to identify all of the aspects that would be relevant for feeding in as a prior when you’re building various language models okay there was a bunch of work done on this to be able to do this basically both building the vector and also trying to predict and and across that there are still people doing phd’s graham is still very much working in that way and aditi is our phd is very much in that space</p>
</section>
<section id="universals" class="level3">
<h3 class="anchored" data-anchor-id="universals">Universals</h3>
<p>are there any unit universals that are features that are there for everything for all possible languages and the answer is yes mostly and sometimes there’s a little bit caveats around the edge and so for example all languages do seem to have vowels and consonants but the definition that the boundary for vowels and consonants isn’t very good so that’s might be an easy thing to fulfill but it seems to be true given that we’re all using the same vocal tract that we are actually trying to do that almost all languages have got nouns and verbs now there’s some languages where the distinction isn’t very strong and it’s morphological variants that allow you to be able to distinguish between it but pretty much everything has nouns and verbs now once you get the adjectives the next major class that’s not so clear and we end up with a number of languages that will use nouns as adjectives maybe with some morphological variation in english you can get away with quite complex nouns being used as adjectival forms more so in american english than in british english and but that’s sort of moved to that we get quite complex noun compounds which are really some form of adjectival form and that’s true across a number of languages now there’s other things that are very common across multiple language families or related languages that are relatively interesting and identifiable and there are multiple non-related languages that will do that you know species between words in the written form a morphology that’s segmental so we’re joining things together as opposed to templatic morphology where you have maybe a bunch of different consonants and the vowels change inside it arabic and hebrew and a number of other northern african and languages have that that are not necessarily all in the semitic language family and so there’s a number of things that are relatively common but they’re not going to be everywhere but you know there’s also things about you know if language is distinguished between voiced and unvoiced they don’t always they’re going to care more about the voice than the unvoiced ones how do we deal with the low repo low</p>
</section>
<section id="low-resource-languages" class="level3">
<h3 class="anchored" data-anchor-id="low-resource-languages">low resource languages</h3>
<p>resource aspect of this how can we actually find out how if we were given a language and we maybe only got a few features for it how can we predict other things for it well we can look at all other languages that are similar and we can find out well most languages of these features have got those features and we might also say for that language what’s the most related language and we’ll just say well let’s assume all these features it might not be true but it’s probably better than just assuming everything is english because everything is certainly not english okay and the number of different groups and throughout the world have been studying this is quite a major area of looking at how to be able to predict or get other features for low resource languages when you don’t have data that’s there now there’s a number of different ways of doing such multilingual nlp what you can do is you can say well i’m going to try to find a close by language and i’m just going to assume everything about that and maybe remove things that are not appropriate and often that happens so imagine that you come to europe and there’s this island nation that separated itself from the rest of europe pretending that it’s not part of europe at all and nobody knows anything about english and eventually somebody braved the channel and gets to england how might they understand english and the answer is well it seems sort of germanic-like so let’s just pretend it’s german or dutch which is even closer and then just use everything that we have from the dutch language to apply it to english and maybe train a little from that and you would get much further with that than if you took chinese or hindi or maybe even french although a lot of the words are borrowed from french and but the grammar is very much germanic in english so we take another language and try to do things so what we’re doing is taking an existing model and then trying to fine-tune it for the target for okay another way to do this is to try to take</p>
</section>
<section id="multilingual-birth" class="level3">
<h3 class="anchored" data-anchor-id="multilingual-birth">multilingual birth</h3>
<p>all languages or all languages in some language family and what we would then do is we would then train everything together in some multiple joint way there’s lots of different ways of doing that and then we would have a model that was multilingual in a true sense that’s actually how multilingual birth is done rather than having different births from different languages and then doing adaptations of the target one we actually build a multi-lingual bar and then we’ve got this multilingual thing so it’s sharing some information about all of these languages and that might make it easier when you’re doing adaptation to the target one form and that is an open question okay it probably depends on the amount of training data and the amount of languages and how close it is and whether it’s going to be similar or not to these other languages when you’re doing that whether you have a writing system that’s the same whether it’s different whether phrenology is different and all these things are going to be important so there won’t be one answer for everything but you should be aware of the different ways of actually trying to do that okay why do we care about typology at</p>
</section>
<section id="why-typology" class="level3">
<h3 class="anchored" data-anchor-id="why-typology">why typology</h3>
<p>all can’t we just well we’re going to train from everything and the answer basically is for most low resource languages you just don’t have enough data and even if you pretend that you have enough data it’s been shown in english the more data you have the better your models are going to be now in general in machine learning the more structured data you have the easier it is to be able to learn things so if you have external data to help you when you’re dealing with small amounts of data it will usually not always but it will usually learn better so knowing about the default word order knowing whether morphology is an issue or not knowing the types of grammatical structures the types of verb structures the types of noun structures or their noun classes or their politeness these will potentially help you when you’re building your model to be able to get better results quicker okay</p>
</section>
<section id="how-to-choose-a-transfer-language" class="level3">
<h3 class="anchored" data-anchor-id="how-to-choose-a-transfer-language">how to choose a transfer language</h3>
<blockquote class="blockquote">
<p>How do you choose a transfer language? Well often what people will do is they’ll go and ask some a knowledgeable person about which language is close and you’ll get an answer but that might not always be the right answer and there was a bunch of work done here a few years ago and trying to look at that for particular tasks like translation and there’s non-trivial aspects of actual similarity of the language reliability of the language the amount of data that you have and whether the data on the domain of the data that you have is appropriate is it conversational is it newspaper text is it bible text and the linux all people actually did quite a lot of that to try to do it i remember the sort of default answer for that i am right about this i think it’s this one and said turkish by default is the best one over everything and that’s sort of probably because turkish is fairly well resourced it’s influenced by a lot of different other language families so it has arabic it has english it has iranian and hindi farsi sanskrit and things in it and it spreads over there’s a vast amount of the world going from europe all the way into turkic languages all the way into china open research problems that we have.</p>
</blockquote>
</section>
<section id="open-research-problems" class="level3">
<h3 class="anchored" data-anchor-id="open-research-problems">open research problems</h3>
<blockquote class="blockquote">
<p>How to extract typological features automatically so if you give me a language can i find out the default word order that’s sort of hard i mean we’re gonna get some there but but when the it’s not obvious and maybe it’s different in written form compared to in the spoken form and therefore you have to be able to care for that but there is this thing called the universal dependency tree bank that originally came out of google and so there are these existing toolkits and data sets which try to give this information for many of the major languages and some of the minor language as well and these resources are hard to do yourself and therefore it’s always good to know about them and to be able to build on top of them okay there’s lots of other things that are</p>
</blockquote>
</section>
<section id="multilingual-aspects" class="level2">
<h2 class="anchored" data-anchor-id="multilingual-aspects">Multilingual aspects</h2>
<p>out there if you want to learn about morphology or phonology you can look at multilingual aspects in the in the computational linguistics and conferences there’s lots of geographical groups that are looking at say specifically looking at indian languages african languages there are lots of things that are looking at low resource languages there are lots of ones looking at interesting morphology languages and so often it’s worth looking at and though everybody who doesn’t know a language thinks i’ll just train from an infinite amount of data the answer is well you won’t have an infinite amount of data and sometimes it’s quite hard to find data and if you discover that morphology is rich in the particular language it might be worth doing morphological segmentation and there may already be an existing morphological analyzer that’s there or at least help to be able to find that okay so that’s a very quick view of typology on how we actually structure languages and it’s becoming more and more important in the computational form than what it was been before 10 15 years ago you’d see less papers about it but now people are really caring about it because we are doing much more multilingual work you were asked to read this particular paper which was a survey paper on looking at aspects of typology across different people trying to do predictions and how well they were actually doing and the issues that are involved in this and what we’re going to do now is we’re going to split you off into groups and in those groups you’ll have a ta or an instructor will be there and what we want you to do is we want you each of you to identify things which are unique or very rare compared to other languages that are important over the languages you know in distinct from things which are not very interesting maybe whole classes of languages that are unrelated are all using a romanized form and to write them but they’re not related but things that are going to be unique from that point of view now a has someone set up the groups graham have you set up the groups have maybe we could take some questions if people had questions yes sure yes thank you 22 indian languages yeah yeah so what language you write things in is quite interesting and especially once you’re in a code switching space in india almost everybody when they’re code switching will write in a romanized form they’ll often call it english but it’s not english it’s the romanized form so they’re writing both hindi in a romanized form and english in romanesque form while when you look at singapore for example where people can be as fluent in chinese and english they actually use hansi for writing chinese and english for writing english words most of that’s got to do with input method actually it’s like how easy is it to type these things on a computer and for historical reasons actually partly because there was less Chinese speakers who spoke english the Chinese input systems became better while in india for the past 200 years the educated elite were all English-speaking and therefore they were used to reading and writing in using romanized m form probably that’s got something to do with it but definitely information is lost when you may be using a non-native script and for example spelling goes on completely out of the way in English when you’re doing it but remember most written most scripts are not appropriate for the language we are using a latin script for a germanic english in english. We use Kanji in Japanese for writing lots of things and yeah there’s other scripts in Japanese for dealing with more native things. Hangul is native in Korea but there’s still lots of Chinese borrowed words, especially scientific words, that come from Chinese. So often the writing system even for the native speakers is not very appropriate. Often it’s just convention. It’s like this is the way we write it and we’ve always written it. And it was only since last year that people live but they think it’s facts that we’ve done it forever. You know basically we’ve always sent dances to tiktok as a way to communicate and I can’t remember when when we ever did anything else</p>
</section>
</div>
</div>
</div>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li><strong>Introduction to Linguistic Typology</strong>
<ul>
<li><strong>Typology</strong> is a way to structure and organize languages based on similarities.</li>
<li>Languages are not fixed points, they exist across regions and often overlap.</li>
<li>Language definition can be political, e.g.&nbsp;Hindi &amp; Urdu.</li>
<li>Remember the gag: <mark>“a language is a dialect with an army”.</mark></li>
<li>Acknowledged the difficulty of defining the borders between languages.</li>
</ul></li>
<li><strong>Linguistic Diversity</strong>
<ul>
<li>The world has approximately 7,000 languages.</li>
<li>Some areas have higher concentrations of languages than others.</li>
<li><strong>India</strong> has around 460 languages.</li>
<li><strong>Africa</strong> has an estimated 1,500-2,000 languages from 6 language families.</li>
<li>Most people in the world are multilingual.</li>
<li>The U.S. and Britain are unusual in that many people speak only one language.</li>
<li>Geographical features can create borders between languages.</li>
</ul></li>
<li><strong>Language Families</strong>
<ul>
<li>Linguists identify language families by looking at shared features such as lexicon, grammar, and morphology.</li>
<li>Examples of major language families include:
<ul>
<li><strong>Niger-Congo</strong> (Africa).</li>
<li><strong>Indo-European</strong> (Europe, Middle East, and Northern India).</li>
</ul></li>
<li><mark>Languages within families share common linguistic information.</mark></li>
<li>Some languages, like Korean, may not be related to other languages in their area.</li>
</ul></li>
<li><strong>Identifying Similarities between Languages</strong>
<ul>
<li>Methods for identifying similarities include:
<ul>
<li><strong>Word overlap</strong>: looking at shared words, especially core vocabulary (body parts, family terms, food, water, air). A specific list called the <a href="https://en.wikipedia.org/wiki/Swadesh_list">Swadesh’s list</a> is often used for this.</li>
<li><strong>Phonetic form</strong>: considering phonetic similarities rather than just written forms.</li>
<li><strong>Areal similarity:</strong> geographic proximity and influence between languages.</li>
<li><strong>Genealogical similarity:</strong> language families based on linguistic history.</li>
<li><strong>Typological similarity:</strong> classifying languages based on functional and structural properties.</li>
</ul></li>
<li>Resources such as <a href="https://en.wikipedia.org/wiki/Ethnologue">Ethnologue</a> and <a href="https://en.wikipedia.org/wiki/Glottolog">Glottolog</a> try to identify and classify languages.</li>
<li><a href="https://wals.info/">The World Atlas of Language Structures</a> (WALS) is a key resource for typological data.</li>
</ul></li>
<li><strong>Typological Features</strong>
<ul>
<li>Typology involves classifying languages based on shared formal characteristics.</li>
<li>Examples of typological features include:
<ul>
<li><strong>Phonology</strong>: How languages pronounce sounds; the International Phonetic Alphabet (IPA) helps in comparing phonological systems.</li>
<li><strong>Numeral bases</strong>: Different languages use different counting systems, such as decimal or base-20.</li>
<li><strong>Word order</strong>: Languages may have a default word order such as subject-verb-object (English) or subject-object-verb (Japanese).</li>
</ul></li>
<li><a href="https://en.wikipedia.org/wiki/World_Atlas_of_Language_Structures">WALS</a> provides a database of 192 attributes across 2,676 languages.</li>
<li>Some features may be predictable based on others; for example, languages with rich morphology may have less fixed word order.</li>
</ul></li>
<li><strong>Typological Databases</strong>
<ul>
<li><strong>WALS</strong> is a major typological database.</li>
<li><strong>URIEL</strong> is another typological database which includes phonology, morphosyntax, and lexical semantics.
<ul>
<li><a href="">URIEL</a> has data for 8,070 languages and 284 attributes.</li>
</ul></li>
<li>These databases are useful in multilingual NLP, providing features for language models.</li>
</ul></li>
<li><strong>Linguistic Universals</strong>
<ul>
<li>Most languages have vowels and consonants.</li>
<li>Almost all languages distinguish between nouns and verbs.
<ul>
<li>The distinction between adjectives is less clear across languages.</li>
</ul></li>
</ul></li>
<li><strong>Multilingual Natural Language Processing (NLP)</strong>
<ul>
<li>Typological features can help in multilingual NLP by providing structured data.</li>
<li><strong>Low-resource languages</strong> benefit from typological information due to the lack of available data.</li>
<li><strong>Methods in multilingual NLP:</strong>
<ul>
<li><strong>Cross-lingual transfer:</strong> using a model from a resource-rich language on a resource-poor language.</li>
<li><strong>Zero-shot learning:</strong> applying a model from one domain to another with no extra training.</li>
<li><strong>Few-shot learning:</strong> adapting a model using a few examples from a low-resource domain.</li>
<li><strong>Joint multilingual learning</strong>: training a single model on multiple languages.</li>
</ul></li>
<li>Typological information can be used to select an appropriate transfer language.</li>
<li>The amount of training data and the similarity between languages is important.</li>
</ul></li>
<li><strong>Open Research Problems</strong>
<ul>
<li>How to automatically extract typological features from existing resources.</li>
<li>How to accurately predict typological knowledge while controlling for biases.</li>
<li>How to incorporate linguistic typology into models.</li>
<li>How to alleviate negative transfer in multilingual models using typological knowledge.</li>
</ul></li>
<li><strong>Further Resources</strong>
<ul>
<li>Papers in computational linguistics conferences.</li>
<li>Workshops such as SIGMORPHON, SIGTYP, and AfricaNLP.</li>
</ul></li>
<li><strong>Discussion</strong>
<ul>
<li>Identifying unique typological features in languages.</li>
<li>Considering aspects of phonology, morphology, syntax, semantics, and pragmatics.</li>
</ul></li>
</ul>
</section>
<section id="papers" class="level2">
<h2 class="anchored" data-anchor-id="papers">Papers</h2>
<p>Here is a list of the papers covered in the lesson</p>
<ul>
<li><span class="citation" data-cites="ponti-etal-2019-modeling">Ponti et al. (2019)</span> <a href="https://aclanthology.org/J19-3005/">Modeling language variation and universals: A survey on typological linguistics for natural language processing</a>
<ul>
<li>This paper is a survey on typological linguistics for natural language processing. It is cited as a key resource for understanding how typological information can be used in NLP. The paper also explores modeling language variation and universals.</li>
</ul></li>
<li><span class="citation" data-cites="lin-etal-2019-choosing">Lin et al. (2019)</span> <a href="https://aclanthology.org/P19-1301/">Choosing Transfer Languages for Cross-Lingual Learning</a>
<ul>
<li>This paper discusses how to choose appropriate transfer languages for cross-lingual learning in NLP. It is relevant to the topic of using typological features for low-resource languages.</li>
</ul></li>
<li><span class="citation" data-cites="littell-etal-2017-uriel">Littell et al. (2017)</span> <a href="https://aclanthology.org/E17-2002/">URIEL Typological database</a>
<ul>
<li>This paper introduces the URIEL typological database. It provides information on phonology, morphosyntax, and lexical semantics across many languages.</li>
</ul></li>
<li><span class="citation" data-cites="malaviya-etal-2017-learning">Malaviya, Neubig, and Littell (2017)</span> <a href="https://aclanthology.org/D17-1268/">Learning language representations for typology</a>
<ul>
<li>This paper is related to the <strong>lang2vec</strong> representations derived from the URIEL database and explores how to learn language representations for typology.</li>
</ul></li>
<li><span class="citation" data-cites="georgi-etal-2010-comparing">Georgi, Xia, and Lewis (2010)</span> <a href="https://aclanthology.org/C10-1044/">Comparing Language Similarity across Genetic and Typologically-Based Groupings</a>
<ul>
<li>An example of research in the automatic prediction of typological features.</li>
</ul></li>
</ul>
<p>These papers are referenced in the sources as being important to the topics of linguistic typology and its applications in multilingual NLP, and they also are noted as relevant to this lesson.</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-georgi-etal-2010-comparing" class="csl-entry">
Georgi, Ryan, Fei Xia, and William Lewis. 2010. <span>“Comparing Language Similarity Across Genetic and Typologically-Based Groupings.”</span> In <em>Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010)</em>, edited by Chu-Ren Huang and Dan Jurafsky, 385–93. Beijing, China: Coling 2010 Organizing Committee. <a href="https://aclanthology.org/C10-1044/">https://aclanthology.org/C10-1044/</a>.
</div>
<div id="ref-lin-etal-2019-choosing" class="csl-entry">
Lin, Yu-Hsiang, Chian-Yu Chen, Jean Lee, Zirui Li, Yuyan Zhang, Mengzhou Xia, Shruti Rijhwani, et al. 2019. <span>“Choosing Transfer Languages for Cross-Lingual Learning.”</span> In <em>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, edited by Anna Korhonen, David Traum, and Lluís Màrquez, 3125–35. Florence, Italy: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/P19-1301">https://doi.org/10.18653/v1/P19-1301</a>.
</div>
<div id="ref-littell-etal-2017-uriel" class="csl-entry">
Littell, Patrick, David R. Mortensen, Ke Lin, Katherine Kairis, Carlisle Turner, and Lori Levin. 2017. <span>“<span>URIEL</span> and Lang2vec: Representing Languages as Typological, Geographical, and Phylogenetic Vectors.”</span> In <em>Proceedings of the 15th Conference of the <span>E</span>uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</em>, edited by Mirella Lapata, Phil Blunsom, and Alexander Koller, 8–14. Valencia, Spain: Association for Computational Linguistics. <a href="https://aclanthology.org/E17-2002/">https://aclanthology.org/E17-2002/</a>.
</div>
<div id="ref-malaviya-etal-2017-learning" class="csl-entry">
Malaviya, Chaitanya, Graham Neubig, and Patrick Littell. 2017. <span>“Learning Language Representations for Typology Prediction.”</span> In <em>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</em>, edited by Martha Palmer, Rebecca Hwa, and Sebastian Riedel, 2529–35. Copenhagen, Denmark: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/D17-1268">https://doi.org/10.18653/v1/D17-1268</a>.
</div>
<div id="ref-ponti-etal-2019-modeling" class="csl-entry">
Ponti, Edoardo Maria, Helen O’Horan, Yevgeni Berzak, Ivan Vulić, Roi Reichart, Thierry Poibeau, Ekaterina Shutova, and Anna Korhonen. 2019. <span>“Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing.”</span> <em>Computational Linguistics</em> 45 (3): 559–601. <a href="https://doi.org/10.1162/coli_a_00357">https://doi.org/10.1162/coli_a_00357</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Typology: {The} {Space} of {Languages}},
  date = {2022-02-25},
  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w03-typology/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Typology: The Space of Languages.”</span>
February 25, 2022. <a href="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w03-typology/">https://orenbochman.github.io/notes-nlp/notes/cs11-737-w03-typology/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Multilingual NLP</category>
  <category>NLP</category>
  <category>Notes</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w03-typology/</guid>
  <pubDate>Thu, 24 Feb 2022 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Translation Models</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w06-translation-models/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/tiling.png" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>course banner</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/XeDCP0newd8" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Lesson Video
</figcaption>
</figure>
</div><div id="sup-slide-deck" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div>

<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Sequence-to-sequence models w/ attention</li>
<li>Decoding strategies</li>
<li>Transformers</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- Transcript here is machine generated no punctuation and I eddited it a but but it needs more work and I dont see the benefits unless I can run it through some tool. -->
<blockquote class="blockquote">
<p>looking forward to talking about machine translation and sequence sequence models today as always I’ll welcome discussion on this some of this might be this is kind of the basic modeling stuff regarding machine translation so I think some of this might be elementary for people who have taken an nlp class before but there are you know some people who might not have covered this so I’d like to go through it after this we’re going to be talking about some concrete code details so looking at code from the annotated transformer and also from the assignment and as always please feel free to ask questions on chat or like live anytime you want so language models are basically generative models of text and what they do is they calculate the probability of a text x given you know calculate a probability of a text x and they allow you to sample from this probability distribution to generate outputs with a high probability and the so given this you know if we train on a particular variety of texts like let’s say we train on harry potter books the sampling distribution will generate things that look like harry potter there’s also conditional language models and conditional language models don’t just generate text but they generate text according to some specifications so if we have our input x and our output y our input x could be structured data output y could be a natural language description what we’re going to talk about this time is translation so input x could be english output y could be japanese our input x could be a document output y could be a short description and our input could be an utterance output of response inputs image output text input speech output transcript so you can see that a lot of things a lot of different tasks you know data to text generation or natural language generation translation summarization response generation image captioning and speech recognition all fit within this general paradigm so because of this you know these are a very powerful and widely used variety of models and so I’d like to go a little bit into exactly how we formulate this problem and learn these probabilistic models so when we calculate the probability of a sentence the way we can do that is by taking a sentence which consists of I words and calculating the probability by the product of the probabilities of the word in the sentence so if the next word is x I we calculate the probability of x i given all of the previous words in the sentence and this is a particular variety of language model oh sorry there’s a language intense sorry yeah I’ll find a good place to cut off this description and switch over to the language intent I am totally slipped my mind so [Music] then conditioned language models basically what they do is they condition on some added context and they calculate the probability of y given x and then they do the same thing where you condition on x and all of the previous generated words i talked about calculating the probability of words in a sentence and in most cases we’re taking the previous context calculating the probability of the next word given the previous context these are also called by a variety of names one name is an autoregressive link model where basically that means we’re predicting the next word given the previous words in some order specifically when the words that you’re predicting the the word here from all come to the left they’re also called left to right language models and there’s also a term causal language models which I I don’t prefer as much but you’ll also see in the literature and basically if you have a left to right language model we can use something like a recurrent neural network to feed in the previous word predict the next word feed in the previous word predict the next word feed in the previous word predict the next word etc etc and if we want to develop a conditional language model that conditions on some sort of context to predict the probability of the words the upcoming words we can do something like feed in the conditioning context into a recurrent neural network and predict the the outputs like this so basically we read the conditioning context one by one maybe it’s a japanese sentence and then we predict the probability of the output of the english sentence one by one so we have the encoder and so this is often called an encoder and this is called the decoder so these are often called encoder decoder models and they’re used pretty widely and you know all of nlp nowadays so I’ll actually skip that part so now we have this model we can train it you know I think a lot of people are everybody in the class is now training you know models for part of speech tagging now so because of that I I assume you know like about the recurrent neural networks and other things like this the the place where these kind of sequence sequence models or translation models become different is that we need to do generation from the models and so instead of like taking in individual words and trying to put a tag on them we actually need to generate outputs and the generation problem is basically we form a model of p of y given x and how do we use it to generate a sentence and there’s basically two methods the first method is sampling where we try to generate a random sentence according to the probability distribution and the next method is argmax where we try to generate a sentence with the highest probability and in general in translation which is the main topic of this lecture here we we use the arg max and actually maybe I’ll explain why in a few slides after I explain each method so for sampling the most common way to do this is called ancestral sampling where we randomly generate words one by one and the algorithm for doing so looks a little bit like this so we have while yj minus one is not equal to the end of sentence symbol so basically while we have not yet finished our sentence we calculate the probability of the next word given all the previous words in the conditioning context and we randomly sample a word from the next time step and this is a method for an exact method for sampling from p of y given x so this will give you you know basically samples that exactly follow the probability distribution that the model specifies if you had a really really good model where basically it only assigned probability to why that were you know plausible translations for x then probably the sampling would be fine you would get different translations every time but you know each translation that you sampled would be good however unfortunately our models are not all that great and very often if you sample kind of lower probability things according to this probability distribution they might not be very good translations so what we try to do instead is do some sort of search for the highest probability translation and one way to do this is doing something like greedy search so basically one by one we pick the single highest probability word in the output so that looks a little bit like this we similarly we go until we reach the end of sentence symbol but instead of sampling a random word according to the probability distribution each time we we take the highest probability word at the next time step each time so what this is doing is this is instead of picking you know any any old word next time it picks always fix the highest one which moves us closer to a very you know high scoring output unfortunately this is also not exact and it causes real problems so for example it will often kind of regardless of what the syntax should look like it might try to generate the kind of easier words in the sentence first because the easier words in the sentence will have higher probabilities so this arg max this greedy arg max we’ll try to choose those words first and another issue that this can bring up is it will prefer multiple common words to one rare word so like for example if we had a choice between new york and pittsburgh even if the product of new in york was lower than the probability of pittsburgh if the probability of new only was higher than pittsburgh this greedy search would kind of make that decision prematurely and not be able to back off and and reconsider the decision later so what we do instead is we have a method called beam search there are other methods as well but beam search is the most common method where instead of picking one high probability word at every time step we maintain several paths so this is an example of beam search with size two where basically we instead of like taking the highest probability word at the first time step which would be b we consider two high probability words of the first time step a and b we expand all next words for each of those possibilities and and then select the highest scoring sequence of length two based on all of these and basically beam search allows you to spend more time computing to look for a high scoring high scoring output and basically allows you to do more precise search and help resolve some of the issues that I talked about on the previous slide cool there’s also lots of other details if you want to go deep into the details of this like for example there are other methods for sampling that try to kind of alleviate the issues that you get from low probability [Music] low probability outputs by only sampling from the highest probability outputs things like top k sampling and nucleus sampling there’s also other methods for search that incorporate you know like dynamic beam sizes or other things like this but basically these standard b in machine translation standard beam search tends to be the thing that most people use and you usually just pick a beam size of five and forget about it and hope you know that that’s good enough and it usually is cool any questions about that before I move on okay I guess not so actually sorry I need to check a chat so next I’d like to talk about attention so attention is very very important in machine translation and one issue with the example like machine translation model that I talked about before is basically what we’re asking this model to do here is read in a whole sentence express all of the meaning in that sentence in a single vector and then output the appropriate translation based on this and i because I have previously worked as a translator I like to make an analog to what that would be like if you were asking a translator to do something so this is kind of like asking a translator to read a whole sentence memorize that whole sentence put the book that they were looking at away and then tried to translate output the whole sentence based entirely on their memory and you know for a lot of sentences a lot of things have people you know think of they might be able to do that but it’s a lot harder than if you can go back and reference the like original sentence when you’re doing translation and so this was kind of eloquently stated by ray mooney a professor at ut austin where he said you can’t cram the meaning of a whole bleeping sentence into a single bleeping vector and so the idea of attention is basically to relax this constraint of memorizing all of the content of a sentence in a single vector and put it into multiple vectors one vector for each token in the input so then we have the model that goes back and references each of these vectors one at a time so the basic idea behind attention is that we encode each word in the sentence into a single vector and when decoding we kind of perform a linear combination of these vectors weighted by attention weights and we use this combination from the vectors in picking the next word and I have a graphical example of this here so let’s say we have our encoder here and our encoder calculates a representation for each word in the sentence and our decoder here basically is stepping through the sentence word by word and it’s gotten to the place where it’s at like I hate and that it wants to generate the next word so what a translator would do at this point is they would go back and reference the original sentence maybe after they’ve read through it once and try to decide which word they want to be translating next think about what word they want to translate that into and that’s basically what attention does so it takes this query vector down here and it compares that query vector to all of the key vectors in the original included input and it calculates a score or a weight based on the congruence between the query vectors and the key vectors so you put it through this function here that takes in the query vector and the key vector and it spits out a scalar weight and we do this for each pair of the query vector in one of the key vectors in the key matrix basically and then we normalize these so they add to one using a softmax function and basically what this looks like is we take in these values and we output something that looks like a probability vector here and I’m sure you know everybody’s familiar with the softmax from the homework that you’re already working on then after we do this we take the value vectors which are the things that we want to attend to the types of information that we want to combine and we multiply this by the attention vector here and that gives us a single vector that we can then feed into any part of the model and specifically in the case of machine translation or whatever else we will be feeding this into the decoder in order to calculate the the probability over the next word so if we have a attention mechanism when we’re doing translation this is a picture from the paper that introduced the attention mechanism basically you’ll see that every time it translates a next word I believe in french here from english to french it will be attending to it will be attending to multiple words in the original source documents so and you can see that these align with each other and even when like the order reverses then the appropriate words are being attended to so I left the the actual function that we used to take in the query and the key vector underspecified but if we assume that the query and the key q is a query and k is the key we can do various things like feeding the query and the key into a multi-layer perceptron where we concatenate them together multiply them by a weight matrix take a non-linearity like the tan h and multiply by another vector to get this value this is flexible and often very good with larger data sizes another thing that we can do and in fact this is maybe the most common method that we use nowadays we can multiply by the key we can have the key and the query and we have a weight matrix here that we multiply and this gives us the [Music] attention weight here and then we also have other things like the dot product where we just take the dot product between the two vectors this is very very simple and and can be effective but the problem is that this one issue with this is this definitely requires the key and the query to be the same size because otherwise you can’t take the dot product so the bi-linear function gives you a bit more flexibility in that you don’t have to assume that the query and the key are in the same kind of like underlying space and it also allows you to compare things of different size and then finally we can have the scaled dot product and so one problem is that the scale of the dot product increases as the number of dimensions gets larger and so the way of fixing this is basically scaling it by the size of the vector so you take the square root of the size of either the key vector or the query vector it doesn’t really matter which one because they’re both the same size here and this allows you to make sure that even if you have larger or smaller values the scale with which the attention weights varies stays approximately the same okay so this is a a brief overview we’ll be going through some concrete examples a bit later in the class but are there any questions here I imagine many people will be familiar with this already okay if not I will go ahead so then we have improvements to attention so attention one issue with attention is basically that neural models often tend to drop or repeat content when they’re doing generation in particular for tasks like machine translation where in machine translation we can make the implicit assumption that all content in the source should be translated into the targets we can additionally add biases to that basically model how many times words in the input have been covered and if they have not been covered you know approximately one time we can give some sort of penalty so basically this is making the assumption that each word should be translated about once or another option is to let the model to tell the model how many times each word has been covered but let it learn to use that information appropriately in making its next translation decisions so either way you know this is explicitly or implicitly encouraging the model to you know cover things once and only once or like as many times as necessary another very common improvement used in most models nowadays is multi-head attention where the idea is that we have multiple attention heads that focus on different parts of the sentence so this is an example where essentially what we do this is an example with two attention heads and the two attention heads each have a different function the first attention head is like a normal attention head that is just used to calculate representations of the words in the sentence so kind of like how I explained attention before as a way to combine together information at each time step and then the second attention head is basically focusing on words that should be copied from the source to the target and this is used in particular for a summarization task where lots of copying needs to be done so basically it’s summarizing source code into into function names so what you can see is the first attention head because it’s just pulling in information about the input sentence in general it’s kind of spread out across the entire input sentence and the second one because it’s kind of choosing which words to copy to generate outputs is very focused on individual words as opposed to being spread out across the whole sentence and so I like this example because it gives an example about how like theoretically you might want to have different attention heads with different functions but in standard models nowadays what we use instead is we have multiple attention heads but we just let the attention heads kind of learn on their own what they would like to focus on and here is an example of basically this word making attending to other words in the same sentence in self-attention which I’m going to talk about in a second and you can see that making one of the attention heads is attending to making itself there are other attention heads like the blue attention head orange one green red which are attending to more difficult which is kind of the the object of making and then you have another attention head that’s pulling in information from like the previous word to making so you can see that each of these heads has kind of specialized in a different like form or like function that it should be achieving so that’s good because you know there are things like local context like you know the previous word 2009 which could be very useful but there’s also other varieties of context farther away like the object of a verb that could be important for making particular decisions as well so another common improvement to attention is supervised training and normally we just train the attention together with the entire model and however sometimes we can get like quote unquote gold standard alignments a priori and gold standard alignments are basically mit could be manual alignments where we’ve asked a human annotator to say specifically which words in the source correspond to which words in the target and the reason why we might want to do this is because as you can see here or no as you can see here even though this is a relatively good example it’s not always the case that attention will focus directly on the words that are being translated like for example here the period only has a very vague attention on the period in the input so because of this you can’t necessarily like look at the attention value and immediately tell which words are being translated and that’s even worse in the case of something like multi-headed attention however like as I talked about last class you know if you’re a human translator using machine translation it might be very important or useful to you to know which words were translated which other words so you could check that they’re correct or something else like this and in that case doing supervised training with manual alignments or something could make sure that at least you know some of the attention heads correspond with human intuition about which words were translated into which other words another way you can get gold standard alignments is by using a strong alignment model so you might have a an alignment model that was trained on manual alignments or something like this and you could also use that to kind of bootstrap the training of your model and this can also improve accuracy overall by making sure that the model is attending to like words that are actually aligned as well so exactly how you do the supervised training basically the answer you can do a number of things but the simplest one is you just have another loss function that you incorporate into your model where the loss is higher if the alignments are like incongruent with the with the annotated alignments and so you can do things like take the the norm the l2 norm of the difference between the alignment vectors or something like that okay and now I’m going to go into self-attention which is very widely used in nlp nowadays and self-attention basically the idea is up until now we talked about things like recurrent neural networks is a way to gather encodings for each word in the sentence but another way you can do this is you can have some sort of attention that uses that where you attend each token in the sentence that you’re looking at now attends to the same sentence itself so for example the word this would attend to this is an example the word is would also attend to this is an example etc etc and the basic idea here is that this can function is kind of a drop-in replacement for another type of word-level encoder like rnns and and calculate the representation of each word so for example for this the word this might attend to this a lot is a little bit and the other one’s not very much is might attend to is and its subject and object and other things like this the motivation for this becomes very obvious if you think about the translation of words like like the word is or maybe the word run is a better example so the word run you know depending on whether you’re running a marathon or running a company or running along the road or running along the creek or something like this run might be translated differently so if the encoding for the word run pulled in the subject information about the subject and the object that would make it a lot easier to perform like lexical selection in translation or something like that so why would you want to use self-attention instead of an rnn or something like this unlike rnn’s it’s very parallelizable which allows for fast training on gpus you can calculate all of the words in the sentence at the same time so this is probably the most important reason why people like using transformers sorry like using self-attention and the transformers that I’m going to be talking about in a second another reason why people like it is unlike both rnn’s and other options like convolutional neural networks it can easily capture global context from the entire sentence so you you’ll note that every time each word is attending to all other words in the sentence so that gives a very direct connection to you know pulling context there in general transformers seem to yield a pretty high accuracy and a large number of nlp casks although it’s not 100 clear that they’re actually all that much better with respect to accuracy like there’s this study by chen at all that compared them with rnns and the results were were similar essentially but the but the advantages here of like fast training and being able to easily pull in global contacts are are widely appreciated one downside to this is quadratic computation time so essentially because like you can even see this computation time directly from this attention graph here so you can see that if the length is 4 you have 4 times 4 16 attention values here and this can become onerous if you’re using very long sequences to compute so there’s also a lot of different methods to improve over this recently so now that I’ve introduced a bunch of you know improvements or kind of elements that go into attentional models I’d like to introduce the transformer which is kind of the core element that is used in most state-of-the-art mt or nlp models nowadays and basically this is a self a model that uses both self attention and regular attention between the source and the target so we have a transformer encoder in a transformer decoder where the transformer encoder takes in some word embeddings it adds something called the positional encoding where the positional encoding basically is a another word embedding that tells you not the identity of the word but rather the position of the word in the sentence so instead of saying like this is the word summary it would say this is the first word in the sentence so you add them together and you get like summary and first word in embedding indicating that then you run this through multi-head self-attention you add in the original input and you do something called layer normalization which basically kind of normalizes the outputs to be average of around zero and norm of around one and then you run through a feed-forward network and then you do the addition and normalization and then you do this over and over again you know six times or eight times or ten times or whatever and then on the the right side over here you have the transformer decoder which is very similar output embeddings positional encodings self-attention but you also have additional cross attention that attends to the input here so that’s kind of the place where you get the attention that I showed before between the source and the target here so that this is in the cross-attention part and then in the self-attention part you have the encoding of the sentences like you have here so you’re you’re doing both in the transformer so often we talk about like the transformer is kind of like a monolithic block that is used in in translation models but actually you know it contains a lot of the tricks that I talked about before so it contains self-attention multi-headed attention it’s using normalized dot product or actually maybe bilinear attention would be more accurate because it’s multiplying by a a weight matrix for the key in the queries and it has positional encodings it also uses a whole bunch of training tricks such as layer normalization helping ensure that layers remain in a reasonable range it also has a specialized training schedule where you have the atom optimizer but you have a specialized learning rate that kind of gradually ramps up and then slows down in the transformer paper they also used a technique called label smoothing i’m not going to go into a lot of detail about this but basically it kind of smooth smooths out the distribution you’re predicting so you’re not predicting 100 probability on the true next word but you’re giving a little bit of probability to all the other words this is pretty effective in a wide variety of neural network based tasks and you also have a masking method for efficient training and so the way this masking works is basically you decide when you’re calculating the representation for a particular word you decide which previous which context to attend to so when you’re calculating the representation for the word I you only attend to the context in the input when you’re calculating the representation for the word hate you calculate you attend to any of these things over here and similarly throughout the entire like calculation of the representations for the decoders so on the encoder side you always attend to all the other words but on the decoder side you only attend to words in the input and previous words in the output and so this was about transformers taking a step back we can kind of take a unified view of the models that we’ve talked about so far in this class so we have sequence labeling where sequence labeling has a feature extractor and then given the feature extractor we have a feature for each word and calculate the output so this is what you know everybody is doing for assignment one and then we have sequence sequence modeling where we have a feature x director so these can actually be exactly the same thing and calculate one representation for each word and then we have essentially a masked feature extractor here where we calculate representations that can ref reference any of the things from the input feature extractor but only things that occurred previously in the current output sentence and then we predict the the words here as well cool so this is all I have in terms of the lecture content and then I’m going to try to go through the code for the transformer just a little bit and then Patrick’s going to introduce what you need to do for the assignment are there any questions about the lecture content that I talked about here okay I guess not at the moment so if you if you know about this already this might be a bit of review if you’re not familiar with this it’s very important so please do you know ask questions go to office hours other other things like this and then the final thing I’d like to do is go through a brief code walk through this thing called the annotated transformer and this is a very good thing it’s linked on the class site and basically what it does is it it demonstrates how you would actually implement the transformer model together with all of the description from the original paper and so basically [Music] if you look at the model architecture first the the way it looks is we’re talking about in encoder decoder architecture we have our encoder here we have our decoder here and sorry my computer is very slow nowadays because my battery is broken as you heard last time but i’ll try to do this nonetheless we have our source embedders we have our target embedders and then we have some sort of generation policy so this would be you know like something that helps us do sampling or or helps us predict the probability of the next token and we take in and process source and target sentences the encoder encodes the source sentence and then the decoder encodes the target sentence and the memory so I believe this is the encoded source sentence and then we have the source mask and the target mask and the generator is basically just doing a soft max like you’re all familiar with and so this is the kind of encoder architecture this is the decoder architecture and then we have our generator here and one other feature of the of the transformer is basically we have n times this this encoder block in decoder block so this method here produces n identical layers so it it copies each module so you get n of them so that allows you to do the nx here the core encoder is a stack of n layers where we also have a layer norm after all of them or no sorry this is a final final layer moments and then we pass the the input and mask through each layer and turn so we just have a for loop over each of the the layers here and the layer norm what this does is this basically attempts to normalize so I mentioned we’re trying to set the mean close to zero and the and the variance close to close to one so you can see that that’s kind of doing that here and we we add dropout oh yes so dropout is kind of like regular regularizing the model to prevent it from overfitting we have layer norm and then for each of these blocks you’ll notice that we have the multi-head attention and then we have this add-in norm after this so this sub-layer what it’s doing is it is doing layer normalization and it’s doing layer normalization it’s calling the the sub-layer function here it’s doing dropout over the output and then it’s adding a residual connection so that’s kind of the the yellow block in the transformer stack here and then each layer has in the encoder has two sub-layers one is the self-attention layer one is the feed-forward layer and then this is cloned you know two times essentially there then the decoder also has a stack of n layers basically and the decoder is the same except in addition to having self-attention it has a source of tension in the feed forward layer so that’s basically what you can see here in the diagram and then it talks a little bit about masking here so that was the masking that I talked about in the in the lecture where you’re basically creating this mask that gets rid of all of the things that are in the future for the decoder and this is applied only to the decoder only to the decoder then finally I’m going to talk about the attention how the attention is done within the the transformer so basically the transformer attention is doing this sort of dot product attention with scaling you take the soft max and you multiply this by the value vector so that’s you know what I talked about in class and that’s all implemented here it’s basically doing a matrix multiplication between the query and the key vector here query and key matrix here it’s taking the square root based on the size of the query vector actually this is dk but this is query so that’s a little bit strange but you know it makes no difference as I mentioned before and then you take the soft max and you multiply the attention by the value vector so that’s basically the multiplication here so you can see you can make a map between the equations of the code and then there’s also implementation of multi-head attention this is a little bit more involved about exactly how this is implemented so I’m not going to go through all the equations here because I want to leave time for patrick but if you want to look at exactly how the multi-head is implemented you can see this here and yeah that that’s the basic idea of what this looks like i think it’s important to know what’s going on in these models but if you want to like actually use this in your code itself it’s also pytorch is also nice enough to just have this transformer class for you that you don’t need to like you don’t need to implement yourself and basically if you look at this it’s doing the same implementation that I just showed you but it’s you know maybe more optimized or you know like you know it’s right so you won’t be making mistakes so if you’re actually going to be using this I suggest just using the transformer module but I think it’s important to know what’s going on inside so you can understand the behavior and other things like that great so that’s all I have here are there any questions about the implementation or other things yes isn’t the annotated transformer I seem to remember like it’s slower because it uses for loops for the batches instead of converting them into like multi-dimensional tensors is that ringing a bell and is that accurate or let me I didn’t cover batches so like batching is important it’s a very it’s a very important thing because like basically when you batch together multiple sentences you can get them to like all process in a single gpu call instead of in many different gpus calls I’ve I talked about this a little bit more in advanced nlp i don’t see actually I was thinking of the of the multi the multiple heads the attention heads because I think I think the I think it actually matches by input sequence it definitely watches because he has access to a mask so if he has a mask it’s definitely watching yeah yeah i don’t like I I seem to remember this was actually implemented pretty well pretty efficiently i don’t I didn’t actually talk about how the multi-headed attention is implemented but I it’s implemented in a clever way which also makes it hard to read which is why I didn’t go into the details quite as much here but I I do think this is actually a pretty good example of how you would implement it efficiently as well although I’d have to go through in more detail to make sure for sure but that’s a good question what I can tell you is the pytorch one is definitely implemented efficiently so if you want to look and see the the best way to do that you can look there I’ll turn this over to patrick then who will I guess go through the assignment for assignment two and this is now available on the on the website also if you want to follow along and let me share the screen okay do you guys see the right screen yeah cool yeah so I’m going to introduce assignment 2 which was released today they’re still the code still needs to be uploaded but all the instructions are there and yeah so let’s get started because we’re short on time so ah and this was developed by me and vijay which will explain the later part of this sign cool so the task for this assignment is machine translation so as gram described given a source language sentence you want to relate to a target language to target language so conditional language modeling and the goals of this assignment are to understand how the standard data processing pipeline is used in empty to be able to train and empty models both bilingual and multilingual using an empty framework and to learn how these models are evaluated and finally to investigate methods to tackle the data scarcity problem in low resource language pairs which is what you’re going to tackle in this assignment so for requirements you definitely need a machine with gpu unlike the previous assignments it will be very very hard to run this on a cpu as in you will run but no invisible amounts of time I I didn’t test it but I’m pretty sure you can run it on a call out but still if you have access to the credits I would probably recommend doing aws for this and in terms of packages it’s mostly the same as the last assignment you can almost certainly reuse the the environment that you have we tested it and this design will also use firstec as a backbone for training models so first tech is like sequence modeling toolkits and and it automates data loading training decoding and a bunch of other small not boring but like you might not want to consider for this assignment and it also supports many other tasks other than translation for the basic requirements you will not need to dive deep into fair sec but if you want to modify for the actual requirements you probably will need to dive into faresec and and yeah you also need to install soccer blue and combat to evaluate your models and but this is all automated in the instructions of the assignment you just have to follow the the assignment and everything should be installed automatically so the assignment is a collection of scripts which will pre-process you data train your data and score you’re there and we’ll go over them in a few slides so for that you will be using the ted talks corpus which contains parallel data between english and 58 languages and you will focus on two low resource language pairs english azerbaijani and ecospeller russian so to pre-process the data there’s a script that does the following steps we will read the raw parallel data you will learn about byte pairing coding separately for the source and target languages and in particular this assignment will use sentence piece which is a particular implementation of bypass encoding we’ll apply bp to all splits there’s also a very minor data cleaning pipeline which just filters based on the size of sentences and finally you’ll binarize the data which makes it efficient for first to trade and crucially for this assignment we simplify the bits sometimes you also do some tokenization using moses prior to learning the bp but in this case we simplify then we just use center space which works fine by itself but sometimes people do moses organization before just as a remark for modeling in training and generation we’ll use the reform architecture as describe diagram and embeddings will be shared between source and targets despite the fact that they use different bps this is still possible and this will be trained to minimize cross entropy with adam and decoding will be done using beam search in particular it’s fixed to beam search of five and is an example of what the scripts call and this is just like cli for fersae so you don’t have to worry about too much about training loops or anything for evaluation for mt you traditionally don’t look at perplexity because it’s not a good measure of how the models perform in terms of translation and in this assignment we’ll use two metrics blur which is the de facto standard for empty evaluation which uses n-gram overlap between the reference in the target and because there’s some concerns with the reliability of blue and this correlation with human judgment would also consider comet which is part of a new generation of neural neural model based metrics which rely on pre-trained models and are trained to optimize relatively human correlations again you don’t have to think too much about it because there’s a script that already does it it’s just good to know that you’ll evaluate your models based on both okay and I’ll turn it over to vijay thanks patrick so the minimum requirements for this assignment involve first training these mt models on bilingual data and so here we would we you we expect you to train models from azerbaijani to english and vice versa and also bella russian english and vice versa next slide but then the more interesting bit which is also required in the minimal in the in the our minimal expectations is to train multilingually using a transfer language which in the case of azerbaijani is turkish and we have scripts to do this already for both bilingual and multilingual but you’ll have to modify these scripts minimally in order to support belarusian and we’ve also introduced because nowadays while multilingual training is still very effective there’s a new paradigm that’s become super popular which is fine tuning a massive multilingual model that’s been trained on a large amount of data and to allow you to try this approach which is now pretty predominant you can also instead of doing multilingual training you can fine-tune a model that we have provided that has been trained on the floor s 101 data set which is like a massively multi-lingual mt data set and you can instead use this strategy and this will also get you to the to the b b plus grading range next slide but I think where this assignment gets really exciting and what you need to do to get an a minus a or a plus is to add extensions beyond what we provided in these scripts and I think graham will be talking about a lot of this in the next week some of it he already talked about this week but data augmentation which he hasn’t talked about yet is one very promising strategy you could use these terms will make more sense once his lecture is completed next slide you can also try to use better transfer languages we’ve currently suggested two basic transfer languages for both azerbaijani and belarusian but as you might remember from the lecture last week there’s more of a structured way you can choose a good transfer language based on your knowledge of typology or based on learn models and we encourage you to try this and see what what results you can get and then lastly in terms of the data our baseline models use sentence piece for byte pair encoding which is basically a way of breaking up words into sub words but there’s like many other ways to do this and this is a particularly interesting and active area of research and so we encourage you to consider different variants of this and lastly this is probably the most broad category feel free to try different models different training algorithms different ways of doing learning or different and there’s like a huge space here so we really encourage you to be creative and also when you when you choose something analyze it and try to deepen your understanding of why these things work in machine translation and just for reference like the assignment includes a bunch of references to different techniques that seem to work for this feel free to follow them or do your own literature review of course and in terms of what we ask from you when you submit the homework first you need to submit the code which should include both the minimum requirements for like the scripts that you’ve modified in order to support the the bilingual and multilingual training for the language pairs in the assignment but also for the additional modifications that you make we also and this is probably the most important part we want to see a write-up that provides some solid analysis of of the methods describing the assignment and lastly we don’t want you to submit the actual model files because these are quite very large and it’ll take a long time to upload but we do want to see you submit your model outputs once you start playing with the software this will it’ll make sense what this is but basically this way we could compare the translations that your model predicts with our references and reproduce your evaluations just to make sure everyone’s on the same page and these are all submitted on canvas as a tarball like the previous assignment and so I’ve been touching on this but the grading tranches are like first for the minimal requirements you just need to reproduce the modify the scripts that we have for both azerbaijani and belarusian and that will get eub if you couple this with detailed analysis then you’ll get a b plus to get an a minus you need to implement at least one pre-existing method or a new method to try to improve the ability to do this multilingual machine translation next slide and then to get an a we expect you to do two or more several methods to improve multilingual transfer and you might find that some of these methods are more lightweight than others so it might be quite doable to try several and we we also would hope that one of the methods you try is actually novel something that you’d come up with and then lastly building on that a if you’ve done something that is particularly extensive or creative or your analysis is like super compelling then that’ll get you an a plus and just to recall that this is a group assignment unlike the first assignment so it’s expected that it will be done in groups of two three I’m not exactly sure how this will be arranged but I’m pretty sure gram and you don’t know what’s the best way to do this but just to apply for expectations yeah regarding groups feel free to make groups you know obviously you’ve already talked to people about the language intent things so that would be one option but you’re obviously not obligated to do that if you don’t have a group yet but would like to form one please I think one one good idea</p>
</blockquote>
</div>
</div>
</div>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<p>Here is a lesson outline for your notes, based on the provided sources:</p>
<ol type="1">
<li><strong>Introduction to Language Models</strong>
<ul>
<li>Language models are generative models of text.</li>
<li>They calculate the probability of a text, P(x), and allow sampling to generate outputs with high probability.</li>
<li>Training on specific texts (e.g., Harry Potter) results in the model generating similar text.</li>
</ul></li>
<li><strong>Conditional Language Models</strong>
<ul>
<li>These models generate text based on some specifications or conditions.</li>
<li>They calculate the probability of an output, y, given an input, x: P(Y|X).</li>
<li>Examples include:
<ul>
<li>Translation (English to Japanese)</li>
<li>Summarization (document to short description)</li>
<li>Response generation (utterance to response)</li>
<li>Image captioning (image to text)</li>
<li>Speech recognition (speech to transcript)</li>
</ul></li>
</ul></li>
<li><strong>Calculating the Probability of a Sentence</strong>
<ul>
<li>The probability of a sentence is calculated by multiplying the probabilities of each word in the sentence: P(X) = ∏ P(xi | x1, …, xi-1)</li>
<li>Conditional language models add context to this calculation: P(Y|X) = ∏ P(yj | X, y1, …, yj-1)</li>
<li>The probability of the next word is conditioned on the previous context.</li>
<li>This is also called an autoregressive model, a left-to-right language model, or a causal language model.</li>
</ul></li>
<li><strong>Sequence-to-Sequence Models and Encoder-Decoder Architecture</strong>
<ul>
<li>Recurrent neural networks (RNNs) can be used to feed in previous words and predict the next word in a sequence.</li>
<li>For conditional language models, the conditioning context (e.g., a Japanese sentence) is fed into an encoder, and the output (e.g., an English sentence) is predicted using a decoder.</li>
<li>This architecture is called an <strong>encoder-decoder model</strong> and is widely used.</li>
</ul></li>
<li><strong>Methods of Generation</strong>
<ul>
<li>The generation problem is how to use the model P(Y|X) to generate a sentence.</li>
<li>Two main methods:
<ul>
<li><strong>Sampling</strong>: Generating a random sentence based on the probability distribution.
<ul>
<li><strong>Ancestral sampling</strong> generates words one by one.</li>
</ul></li>
<li><strong>Argmax</strong>: Generating the sentence with the highest probability.
<ul>
<li><strong>Greedy search</strong> picks the highest probability word one by one.</li>
</ul></li>
</ul></li>
<li>Greedy search is not exact and can cause issues like generating easy words first or preferring multiple common words over one rare word.</li>
<li><strong>Beam search</strong> maintains several paths instead of just one, allowing for a more precise search.</li>
</ul></li>
<li><strong>Attention Mechanisms</strong>
<ul>
<li>Attention relaxes the constraint of memorizing the entire input sentence in a single vector, using multiple vectors for each token in the input.</li>
<li>The model references these vectors when decoding.</li>
<li>Each word in the input sentence is encoded into a vector.</li>
<li>During decoding, a linear combination of these vectors, weighted by attention weights, is used to pick the next word.</li>
<li>A query vector (from the decoder) is compared to key vectors (from the encoder) to calculate attention weights.</li>
<li>These weights are normalized using softmax.</li>
<li>Value vectors are combined using these weights.</li>
<li>The result is used in the decoder.</li>
</ul></li>
<li><strong>Attention Score Functions</strong>
<ul>
<li>Various functions can be used to calculate attention scores (how well a query vector matches a key vector):
<ul>
<li>Multi-layer Perceptron (MLP): flexible, good with large data</li>
<li>Bilinear: allows comparison of different sized vectors</li>
<li>Dot Product: simple, requires same-size vectors</li>
<li>Scaled Dot Product: scales the dot product by the size of the vectors</li>
</ul></li>
</ul></li>
<li><strong>Improvements to Attention</strong>
<ul>
<li><strong>Coverage:</strong> Addresses issues with dropping or repeating content.
<ul>
<li>Models how many times words in the input have been covered, adding a penalty if words are not covered approximately once.</li>
<li>Uses embeddings indicating coverage.</li>
</ul></li>
<li><strong>Multi-head attention:</strong> Multiple attention “heads” focus on different parts of the sentence.
<ul>
<li>Different heads can focus on different functions (e.g.&nbsp;copying vs regular attention).</li>
<li>In standard models, attention heads learn what to focus on independently.</li>
</ul></li>
<li><strong>Supervised training</strong>: Uses “gold standard” alignments (manual or from a strong alignment model) to train the model to match alignments.</li>
</ul></li>
<li><strong>Self-Attention and Transformers</strong>
<ul>
<li><strong>Self-attention</strong>: Each word in the sentence attends to other words in the same sentence, creating context-sensitive encodings.
<ul>
<li>It can be a drop-in replacement for RNNs or CNNs.</li>
</ul></li>
<li>Self-attention advantages:
<ul>
<li>Parallelizable, which allows for faster training on GPUs.</li>
<li>Easily captures global context.</li>
</ul></li>
<li>Self-attention disadvantage:
<ul>
<li>Quadratic computation time.</li>
</ul></li>
<li><strong>Transformers</strong> use both self-attention and regular attention between the source and target.
<ul>
<li>The transformer encoder processes input word embeddings with positional encodings and multi-head self-attention, adding the original input and layer normalization, and a feed-forward network repeatedly.</li>
<li>The transformer decoder is similar but includes cross-attention to the encoder output.</li>
<li>Transformers use tricks such as layer normalization, specialized training schedules, label smoothing, and masking.</li>
</ul></li>
</ul></li>
<li><strong>Unified View of Sequence-to-Sequence Models</strong>
<ul>
<li>Sequence labeling: feature extraction followed by output prediction for each word.</li>
<li>Sequence-to-sequence modeling: a feature extractor followed by a masked feature extractor, and then prediction of words.</li>
</ul></li>
<li><strong>The Annotated Transformer</strong>
<ul>
<li>Provides an implementation of the transformer model.</li>
<li>Includes an encoder, decoder, source embedders, target embedders and a generation policy.</li>
<li>Uses multiple identical encoder and decoder layers.</li>
<li>The core encoder consists of a stack of layers with layer normalization and dropout.</li>
<li>The core decoder is similar but has self-attention and source attention.</li>
<li>Uses masking for the decoder.</li>
<li>Implements scaled dot product attention.</li>
<li>Also implements multi-head attention.</li>
</ul></li>
<li><strong>Assignment 2</strong>
<ul>
<li>Task: Machine translation</li>
<li>Goals: Understand data processing, train bilingual/multilingual models, learn evaluation, and tackle data scarcity.</li>
<li>Uses the TED talks corpus.</li>
<li>Focuses on low-resource language pairs (English-Azerbaijani and English-Belarusian).</li>
<li>Data preprocessing includes byte pair encoding (BPE) with sentence piece.</li>
<li>Uses the transformer architecture.</li>
<li>Evaluation uses BLEU and COMET metrics.</li>
<li>Minimum requirements involve bilingual and multilingual training, as well as using a provided pre-trained model.</li>
<li>Additional extensions include: data augmentation, better transfer languages, different BPE variants, and other model improvements.</li>
<li>Submission requires code, analysis, and model outputs.</li>
<li>Grading is based on implementation, analysis, and novelty of methods.</li>
</ul></li>
</ol>
</section>
<section id="papers" class="level2">
<h2 class="anchored" data-anchor-id="papers">Papers</h2>
<p>Here are the papers mentioned in the lesson outline:</p>
<ul>
<li><strong>Mikolov et al.&nbsp;(2011)</strong>: This paper is referenced in the context of language models and is cited as “Extensions of recurrent neural network language model”.</li>
<li><strong>Sutskever et al.&nbsp;(2014)</strong>: This paper is mentioned in the context of sequence-to-sequence models and is cited as “Sequence to sequence learning with neural networks”. This paper is also mentioned in relation to how to pass hidden states in encoder-decoder models and initializing the decoder with the encoder.</li>
<li><strong>Kalchbrenner &amp; Blunsom (2013)</strong>: This paper is referenced in the context of input at every time step for conditional language models and is cited as “Recurrent continuous translation models”.</li>
<li><strong>Bahdanau et al.&nbsp;(2015)</strong>: This paper is cited in the context of attention mechanisms and is referenced as “Neural machine translation by jointly learning to align”. It is also referenced in relation to the multi-layer perceptron attention score function.</li>
<li><strong>Luong et al.&nbsp;(2015)</strong>: This paper is cited in the context of attention score functions and is referenced as “Effective approaches to attention-based neural”. It is also mentioned in relation to bilinear and dot product attention score functions.</li>
<li><strong>Vaswani et al.&nbsp;(2017)</strong>: This paper is cited in the context of scaled dot product attention, multi-headed attention, and the Transformer model.</li>
<li><strong>Cohn et al.&nbsp;(2015)</strong>: This paper is referenced in the context of coverage for attention models and is cited as “Incorporating structural alignment biases into an attentional neural translation model”.</li>
<li><strong>Mi et al.&nbsp;(2016)</strong>: This paper is also referenced in the context of coverage for attention models and adding embeddings indicating coverage.</li>
<li><strong>Allamanis et al.&nbsp;(2016)</strong>: This paper is mentioned in the context of multi-headed attention and is cited as “A convolutional attention network for extreme summarization of source code”.</li>
<li><strong>Liu et al.&nbsp;(2016)</strong>: This paper is referenced in the context of supervised training with gold standard alignments.</li>
<li><strong>Cheng et al.&nbsp;(2016)</strong>: This paper is mentioned in the context of self-attention and is cited as self-attention.</li>
<li><strong>Chen et al.&nbsp;(2018)</strong>: This paper is mentioned in the context of the accuracy of self-attention/transformers and is cited as “The best of both worlds: Combining recent advances in neural machine translation”.</li>
</ul>
<section id="see-also" class="level3">
<h3 class="anchored" data-anchor-id="see-also">See also</h3>
<p><span class="citation" data-cites="bahdanau2016neuralmachinetranslationjointly">Bahdanau, Cho, and Bengio (2016)</span></p>
<p><span class="citation" data-cites="luong2015effectiveapproachesattentionbasedneural">Luong, Pham, and Manning (2015)</span></p>
<p>Reference: Self Attention (Cheng et al.&nbsp;2016)</p>
<p><span class="citation" data-cites="vaswani2023attentionneed">Vaswani et al. (2023)</span></p>



</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-bahdanau2016neuralmachinetranslationjointly" class="csl-entry">
Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2016. <span>“Neural Machine Translation by Jointly Learning to Align and Translate.”</span> <a href="https://arxiv.org/abs/1409.0473">https://arxiv.org/abs/1409.0473</a>.
</div>
<div id="ref-luong2015effectiveapproachesattentionbasedneural" class="csl-entry">
Luong, Minh-Thang, Hieu Pham, and Christopher D. Manning. 2015. <span>“Effective Approaches to Attention-Based Neural Machine Translation.”</span> <a href="https://arxiv.org/abs/1508.04025">https://arxiv.org/abs/1508.04025</a>.
</div>
<div id="ref-vaswani2023attentionneed" class="csl-entry">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. <span>“Attention Is All You Need.”</span> <a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Translation {Models}},
  date = {2022-02-24},
  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w06-translation-models/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Translation Models.”</span> February 24,
2022. <a href="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w06-translation-models/">https://orenbochman.github.io/notes-nlp/notes/cs11-737-w06-translation-models/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Multilingual NLP</category>
  <category>NLP</category>
  <category>Notes</category>
  <category>seq2seq</category>
  <category>NMT</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w06-translation-models/</guid>
  <pubDate>Wed, 23 Feb 2022 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Data-driven Strategies for NMT</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w07-data-driven-NMT/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/tiling.png" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>course banner</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/XeDCP0newd8?list=PL8PYTP1V4I8BhCpzfdKKdd1OnTfLcyZr7&amp;index=17&amp;ab_channel=GrahamNeubig" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Lesson Video
</figcaption>
</figure>
</div><div id="vid-02" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video"><video id="video_shortcode_videojs_video1" class="video-js vjs-default-skin vjs-fluid" controls="" preload="auto" data-setup="{}" title=""><source src="https://aclanthology.org/P19-1579.mp4"></video></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;2: Generalized Data Augmentation for Low-Resource Translation (<span class="citation" data-cites="xia-etal-2019-generalized">Xia et al. (2019)</span>)
</figcaption>
</figure>
</div><div id="vid-03" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video"><video id="video_shortcode_videojs_video2" class="video-js vjs-default-skin vjs-fluid" controls="" preload="auto" data-setup="{}" title=""><source src="https://aclanthology.org/Q17-1024.mp4"></video></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;3: Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation (<span class="citation" data-cites="johnson-etal-2017-googles">Johnson et al. (2017)</span>)
</figcaption>
</figure>
</div><div id="vid-04" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-04-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video"><video id="video_shortcode_videojs_video3" class="video-js vjs-default-skin vjs-fluid" controls="" preload="auto" data-setup="{}" title=""><source src="https://aclanthology.org/D18-1103.mp4"></video></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-04-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;4: Rapid Adaptation of Neural Machine Translation to New Languages (<span class="citation" data-cites="neubig-hu-2018-rapid">Neubig and Hu (2018)</span>)
</figcaption>
</figure>
</div><div id="sup-slide-deck" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div>




<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<blockquote class="blockquote">
<p>This time I’ll be talking about Machine Translation one more time</p>
<p>I didn’t get a chance to talk about <strong>evaluation of translation</strong> two times ago. I’d like to start out with that because that’s pretty important and then we can talk about <strong>the data augmentation strategies</strong>.</p>
</blockquote>
</section>
<section id="machine-translation-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="machine-translation-evaluation">Machine Translation Evaluation</h2>
<blockquote class="blockquote">
<p>MT evaluation is a very important and difficult topic in doing machine translation research. <mark>I think we’ve gotten to the point where almost evaluating how well we’re doing is maybe as difficult as like actually doing the translation itself</mark> The reason for this is: if we output a translation there are many different correct translations. We could have paraphrases where the output is “this is a dog”, “I see a dog”, “there is a dog” here other things like this and all of those would be appropriate for an equivalent sentence in another language.</p>
</blockquote>
</section>
<section id="manual-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="manual-evaluation">Manual Evaluation</h2>
<blockquote class="blockquote">
<p>The basic evaluation paradigm in MT. There’s two different types. There’s human evaluation or manual evaluation and automatic evaluation. This is the basic evaluation paradigm for automatic evaluation. Where basically what we do is we have a parallel test set where we have an input and an output in the language pair that we’re interested in. We use a system to generate translations and we compare the target translations with references. Before I talk a little bit more detail about that I’d like to talk about kind of manual evaluation. This is the gold standard of doing evaluation of translation systems. Where basically we ask human evaluators to check whether the answer is correct or not. To give an example we’ve taken a source side sentence we generate some outputs and you can either evaluate by having a human evaluator look at the source in the output or look at a reference and in the output and the reference would be like the so-called correct translation looking at the source and the output only works if you’re bilingual in both languages and it’s somewhat difficult to get bilingual speakers or at least more expensive however given the quality of machine translation systems nowadays very often you don’t know if the output of a human translator like your reference is better than the machine translation system so like very often if you hire a person to do evaluation you know they might not try super hard and like I mentioned before so kind of the gold standard is to get somebody who knows the source and the target to do the evaluation there’s a number of different axes along which you can do evaluation I just listed a couple of them here one is adequacy and adequacy is basically whether the meaning of the translation is conveyed properly and the in this case this is the correct answer here you would know this if you knew japanese which you know most people don’t but if you knew japanese you would know the first one is correct so this is perfectly adequate it conveys the target message the middle one is conveys the target message but is this fluent so it would score high on an adequacy scale but on a fluency scale it would score low the one over here is fluent but not adequate so basically it switched the subject to that object for order so it would be wrong and one notable thing about fluency is you don’t need to know the source language to evaluate fluency all you need to know is the target language because it only has to do with whether the output is fluent or not you can also do pairwise evaluation which just says which one of these is better one of the good things about pairwise evaluation is it’s very simple because you just ask question which do you like better which do you think is a better translation the problem with it is it doesn’t give you an absolute idea of how well you’re doing so if you have two really bad systems and say which is better one might be better but they’re both really bad if you have two really good systems and say which is better one might be better but they’re both really good so kind of absolute scales have that advantage another thing is just like you might get bad translators you might get lazy evaluators you know if you hire people on mechanical turks and they’re not very motivated for example so you need to be careful about quality control as well</p>
</blockquote>
</section>
<section id="human-evaluation-shared-tasks" class="level2">
<h2 class="anchored" data-anchor-id="human-evaluation-shared-tasks">Human Evaluation Shared Tasks</h2>
<blockquote class="blockquote">
<p>There’s a <strong>human evaluation shared tasks</strong> so the most famous one is <em>the conference on machine translation shared tasks</em>. I can show you a little bit what this looks like. They basically have a whole bunch of tasks that you can participate in most of these are tasks for actual translation, but they also have evaluation tasks on metrics and quality estimation so basically what you try to do here is you try to create a metric that has the highest correlation with with human evaluation and for quality estimation what this is is this is essentially evaluation without a gold standard reference so you’re just given the input and the output and you want to guess how good the system output is and this is harder obviously because you don’t have an example of what a good translation looks like but it’s also very useful in practical situations where like let’s say you’re a machine translation company and you want to decide whether you need to get a human translator to go in and check the output and correct it or something like that so if that’s the case if you can estimate very accurately whether the input and output are correct or not then that would save you money save you time give you confidence in the results</p>
</blockquote>
</section>
<section id="bleu-scores-bilingual-evaluation-understudy" class="level2">
<h2 class="anchored" data-anchor-id="bleu-scores-bilingual-evaluation-understudy">BLEU Scores (BiLingual Evaluation Understudy)</h2>
<blockquote class="blockquote">
<p>There’s also other leaderboards and stuff for other seq2seq models but that’s a little bit less important for this multilingual class. There are other metrics like BLEU scores so BLEU score is very famous. You know if you’ve done any research on machine translation or even heard of it you probably encountered BLEU. The exact details of how BLEU is calculated are: What you do is you take the precision of N-grams output by the system. So for example if you look at the unigram precision that’s out of all the unigrams that the system output how many of them exist in a reference or one of the references that you’re provided. If in the case that you’re using multiple references and this gives you a position of like three out of five and then you take the geometric mean of N-grams, usually one to four and then you also have a penalty for outputting two short sentences. Because one way to improve your precision is to not output very many things &gt; This is basically to prevent you from gaming system. But the important thing is now the details of how BLEU is calculated probably the important thing is that this is a <strong>lexical metric</strong> which means that you’re just doing exact match with the references and this has a few issues. One of the issues is essentially that you So there’s there’s two major issues that cause BLEU to either underestimate how good a translation is or overestimate how good a translation is.</p>
</blockquote>
<blockquote class="blockquote">
<p>BLEU tends to underestimate how good translations are when the translations are paraphrases of the true reference. So for example had I have like “I went I went to the store and bought a book yesterday” and you compare that with “yesterday I bought a book at the store” Those are almost identical in meaning But they would get a low BLEU square because I’ve just rearranged the phrases a little bit. So that’s when BLEU tends to underestimate scores.</p>
</blockquote>
<blockquote class="blockquote">
<p>BLEU tends to overestimate scores when you get very critical words in the sentence wrong but get everything else right</p>
</blockquote>
<p>for example</p>
<p>“could you please send this big package to philadelphia”</p>
<p>turns into:</p>
<p>“could you please send this big package to japan”</p>
<blockquote class="blockquote">
<p>That would get a very good BLEU score because most of the words are the same but your package would go to Japan and that’s probably not what you intended by that, otherwise right.</p>
</blockquote>
<blockquote class="blockquote">
<p>That’s the the downside of BLEU. It’s basically not smart enough with respect to these.</p>
</blockquote>
</section>
<section id="shortness-penalty" class="level2">
<h2 class="anchored" data-anchor-id="shortness-penalty">Shortness Penalty</h2>
<blockquote class="blockquote">
<p>Yhe brevity the brevity penalty basically gives you a penalty if your output is shorter than the intended output. If the reference is like 20 words then you’re and your sentence is 15 words you would get a penalty of about 0.75 It’s not exactly like just the ratio it actually drops off faster and stuff like that but that’s a basic idea. That’s a really good question so you pay a penalty in your precision your precision goes down by a lot because there’s no way to get good precision if you output too many things. One thing you should know about BLEU if you’re using it in your research which you might is that it’s very very sensitive to the length. So if your length is a little bit too short or a little bit too long it hurts your BLEU really badly. That’s another problem with BLEU essentially is that it’s not sensitive to paraphrases that are too long or too short as well.</p>
</blockquote>
</section>
<section id="bert-score" class="level2">
<h2 class="anchored" data-anchor-id="bert-score">Bert Score</h2>
<blockquote class="blockquote">
<p>Recently in the past three years or so, there’s been a huge improvement in embedding based metrics which are basically metrics that you know take advantage of recent NLP techniques and one of the first ones was bert score and these can be separated into unsupervised metrics and supervised metrics unsupervised metrics require no annotated data of whether a translation is a good translation or a bad translation supervised metrics are trained to basically regress to an estimation of how good a transplantation is or not so a Bert score is an unsupervised metric that’s based essentially on the similarity between burton betting so it has this matching algorithm where you basically for each word in the output you try to find you know how good it matches with one of the words in the input and this is good because it can do things like handle paraphrases as long as the paraphrases have similar bert embeddings another famous one is BLEUrt</p>
</blockquote>
</section>
<section id="bluert" class="level2">
<h2 class="anchored" data-anchor-id="bluert">BlueRT</h2>
<p>What they did essentially was they trained bert to predict human evaluation scores so they essentially solve a regression problem from the sentences like the reference in this system output to an evaluation score so they’re just going in directly predicting evaluation and they have a bunch of other tricks like unsupervised training where they try to predict BLEU or rouge or other lexical metrics beforehand and that makes it more robust the favorite one that we use in our Comet research on mt now is a comet and comet is also similarly it trains the model to predict human evaluation but in addition to using the just the system output in reference it also uses a source sentence which means that essentially I talked about human evaluation right where you can either ask a human evaluator to look only at the the reference or also look at the source and for a similar reason to why we would like a speaker human speaker to do that it’s also useful to have the model do that because the model can look at the source and see if the information is reflected in the target</p>
</section>
<section id="bart-score" class="level2">
<h2 class="anchored" data-anchor-id="bart-score">Bart Score</h2>
<p>and the final one prism’s another one based on paraphrasing a final one is a bart score environment score is one that I’m a co-author on this is a unsupervised metric that is based on basically a generative model that tries to generate the the system output using the reference or the reference using the system output or the source using the system output et cetera et cetera and bart score I think is good because it’s unsupervised like birth score but it’s essentially more accurate and more controllable so you can do things like calculate recall calculate a precision and other things like that so if that sounds interesting you can take a look at the paper as well but basically if you’re doing empty I would suggest using comment now because it’s well supported it has a nice package it’s pretty widely tested and follow-up reports have suggested that it has very good correlation with human evaluation so that’s my suggestion you can</p>
</section>
<section id="meta-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="meta-evaluation">Meta Evaluation</h2>
<p>Meta evaluation runs human evaluation and automatic evaluation on the same outputs and calculates the correlation this is what they do in the wmt shared tasks like the wmt metrics task for mt other things for summarization etc and one interesting thing is that evaluation as I mentioned at the beginning is pretty hard especially with good systems so most metrics actually had no correlation with human eval over a subset of the best systems at some of the wmt 2019 tasks which means that basically all of the evaluation metrics we had were kind of broken on like evaluating really good mt systems so fortunately now we have comment we have other things like this that actually seemed to be a lot more robust but it was a major problem so you basically calculate the correlation you calculate pearson’s correlation there’s experiments correlation and the way you do that is you human you do human eval of a whole bunch of sentences or humans develop a whole bunch of systems and you try to find the metric that has the highest correlation between the evaluation scores of the systems and the evaluations given by the humans yep all right they often don’t support that many languages well so that’s a good that’s a really good question I many of them do use something like embert or xlmr which support a lot of languages xlmr actually envert’s a little bit more biased xlmr has pretty good coverage of the most common languages in the world but of course as you go down to less well resource languages that’s going to continue to be a problem there you might be stuck with BLEU for now but honestly if you have really bad systems really bad empty systems I still think BLEU is probably good enough in many cases oh another option is carefu chrf and that’s a character-based evaluation metric for mt that’s particularly good for languages with like rich morphology or something like that so I think when you’re working with low resource languages your mnt systems are also going to be really bad so any metric you have is still going to be like reasonably good at measuring progress so</p>
</section>
<section id="database-strategies" class="level2">
<h2 class="anchored" data-anchor-id="database-strategies">Database Strategies</h2>
<p>The next thing I’d like to talk about is the database strategies to low resource mt there’s not a whole lot of content here so I’ll try to go through it rather quickly to leave time for the discussion but basically we have data challenges in the resource mt so mt of high resource languages with large parallel corpora gives us you know very good translations but low resource languages with small parallel corporate you just train there you can end up with nonsense so this is an example of a system trained on 5 000 languages and the most frequent failure state is basically that a neural nt system will just spit out something that has nothing to do with the original inputs there’s a famous example of this so that says why is google translate spitting out sinister religious prophecies and basically if you put in a dog dog dog dog dog in maori it outputs doomsday clock is three minutes at 12 we are experiencing characters in a dramatic development in which jesus returns [Music] can you guess why this happened exactly they use bible data in training their system and when you use bible data and training your system and your system doesn’t know what to do because it has so few resources or it sees something it doesn’t know it just reverts to using the language model and basically outputs whatever the language model thinks and thinks it looks likely and so you know if your system is trained on bible data that looks like the bible if your system’s trained on something else it looks like something else High and Low Resource Languages so you know that’s basically what happened here as well so some ways to fix this we can transfer from high resource languages to low resource languages so basically what you do is you train on a high resource language or multiple high resource languages and then you adapt to the low resource language one the simplest way to do that is just to continue fine tuning on the low resource language you can also do joint training with the low resource language in the high resource language so just concatenate all the data together and in training so this is okay but there are some problems with this as well one problem is a sub-optimal lexical or syntactic sharing and another problem is it’s not possible to leverage monolingual data because you still require a parallel data here and I’m going to be talking more about like lexical overlap and loanwords and stuff in in the next class so I’ll cover that more there but basically suffice to say the high resource language and the lower resource language are different so training on different data is sub-optimal for information sure</p>
</section>
<section id="data-augmentation" class="level2">
<h2 class="anchored" data-anchor-id="data-augmentation">Data Augmentation</h2>
<p>so if we think about data augmentation data augmentation is basically generating other data that looks like the data that you want to have the very convenient thing about this is generating more training data and feeding it into your existing system is easy but effective in in improving mt performance so it’s actually a pretty widely used technique now so if we look at the available resources we might have a low resource language parallel data a high resource language parallel data and also for example target data which is monolingual hence the m here and what we do is we would like to create augmented data where we have target data and like pseudo low resource language data and train our model on this with the idea being that if we can create this this will be closer to our final evaluation scenario where we where we want to generate the target given a low resource language</p>
</section>
<section id="back-translation" class="level2">
<h2 class="anchored" data-anchor-id="back-translation">Back Translation</h2>
<p>so the first example is a back translation and the way back translation works is basically we train a target two low resource language system and we take our monolingual target data and we generate fake low resource language data by translating the target data into the low resource language data so this is how it works we take our target to low resource language system we back translate using the system and then we train a low resource language to target language system using the concatenation of this augmented data in the original data and the key point here is that when we are when we’re training like a sequence sequence model or a machine translation model we’re we’re training it to do two things we’re training it to do language modeling on the target side only and we’re trying to do mapping between the source side and the target side and in order to do language modeling we only really need good target site data so even if there’s some degree of error in this like low resource language here we’ll still be able to learn target site data and we’ll be able to learn a the language model from target side data and we’ll be able to learn a mapping you know even if it’s imperfect from the low resource language to the target language</p>
</section>
<section id="training-schedule" class="level2">
<h2 class="anchored" data-anchor-id="training-schedule">Training Schedule</h2>
<p>so there’s a couple ways to generate translations when doing back translation the first one is using beam search oh sorry yeah yeah that’s a really that’s a really good question so the question was is there any sort of training schedule that you use when you do this so the the kind of quote-unquote obvious training schedule that you might do is you might train jointly on both of them at first and then fine-tune on this data over here and that would make sense because you know this is good data this is like actually translated data however there’s another issue which actually is not super obvious at first but it’s maybe obvious in hindsight which is that if this data is all from the bible and then you want to translate news then actually fine tuning on bible data will be really out of domain and cause issues for you so in fact in the original black translation paper they threw away this data and only trained on this because it was more in domain and that ended up giving better results but that was predicated on the fact that they have a good you know batch translation system in the first place so it’s not necessarily clear what the ideal schedule is but you would almost certainly benefit from some sort of schedule or balancing or something but that’s a complicated hyper parameter so because it’s a complicated hyper parameter it’s also very common to just concatenate the two and these are good details to know for assignment too by the way because they might make a difference in your final scores</p>
</section>
<section id="generating-translations" class="level2">
<h2 class="anchored" data-anchor-id="generating-translations">Generating Translations</h2>
<blockquote class="blockquote">
<p>How to generate translations?</p>
<p>Beam search is one way and basically what this is doing is selecting the highest scoring output. This was done in the original paper. This has the advantage of having higher quality but also lower diversity in the outputs and the potential for bias. You might like for example one result is beam search tends to mostly output pronouns from the majority gender because they’re over-represented. You might get only get male inflections if you do beam search. That’s the type of data bias that could result from here. The other option is sampling. What you do is you randomly sample from the back translation model which gives a lower overall quality but higher diversity. Most reports say this works better at the moment. We had a recent paper which I’m going to introduce in a second but this has kind of a theoretical explanation for why we think sampling should be better which is that it’s a better model of the underlying data distribution that we’re trying to model. I think I’m pretty firmly a believer that sampling is the way to go there’s also a method of iterative back translation.</p>
</blockquote>
</section>
<section id="iterative-back-translation." class="level2">
<h2 class="anchored" data-anchor-id="iterative-back-translation.">Iterative back translation.</h2>
<blockquote class="blockquote">
<p>Iterative back translation is particularly useful when you have a large monolingual data in both languages. Again the idea is simple you train a low resource language to target system first. This is going in the direction you originally want to translate. You generate pseudo data with the target language. You use that to train your target to low resource language system. You back translate and then you use this to train your final system so this is now you have three systems your forward translation data augmentation system your back translation data augmentation system and your forward translation final system you can do this as many times as you want obviously you could also do it on the fly in the process of training the system so this can become arbitrarily complicated if you want</p>
</blockquote>
</section>
<section id="metaback-translation" class="level2">
<h2 class="anchored" data-anchor-id="metaback-translation">Metaback translation</h2>
<p>just one example of this this is a paper that I just talked about but we have a paper called meta back translation which I think is kind of a an interesting idea so normally when we’re training this system to train the the low resource language to the target language system we’re back propping the gradient from the slow resource language data but we can also do a back propagation step where we basically train oh sorry that arrow is thrown I apologize so the arrow actually should be going from here around this to here so the basic idea I’ll fix this later in the slides but the basic idea is we use the signal that we get from from training the final system that we want to train to update the parameters of the back translation system so we’re essentially training the ideal back translation system to train a good forward translation system so this is a I like the idea behind here which is basically the final goal of the back translation system is to improve the forward translation system so we can directly optimize it to do this</p>
</section>
<section id="metaback-translation-issues" class="level2">
<h2 class="anchored" data-anchor-id="metaback-translation-issues">Metaback translation issues</h2>
<p>so there are a couple issues here the first issue is that back translation often fails in low resource languages or domains and as a solution one thing that we can do is we can use other high resource languages or we can combine them with monolingual data maybe with denoising objectives which we’re covering in a following class and we can perform other varieties of rule-based augmentation so I’m gonna go through these in a little bit in maybe in a few minutes so also actually we’ll have discussion about about these two so maybe I’ll just briefly explain the idea and we can discuss more in the discussion for people who read those papers so using high resource languages High resource languages augmentation and augmentation the problem is target to low resource language back translation might be very low quality so the idea is we can also use a high resource language that’s similar to the low resource language and basically for example if we have something like azerbaijani in turkish azerbaijani and turkish are very highly related so maybe we could use information from azerbaijani to english translation back translate into az into turkish which is certainly going to give us higher quality data and use that to augment our data for azerbaijani english system and then we can just throw away this azerbaijani data that we know is not going to be very useful so that would High resource languages pivoting give us additional high resource language to target language data and another thing we can do is we can augment via pivoting and so basically what that does is that gives us data where we take the high resource language data and we translate that into the low resource language and presumably translation from the high resource language to low resource language is easier because these languages are more related so basically what this does is that gives us a better like low resource language pseudo data here and we can also do a similar thing where we generate more high resource language data and this basically gives us three different ways to create this pseudo-parallel data between the low resource language and target language another simple trick this is kind of</p>
</section>
<section id="monolingual-data-copying" class="level2">
<h2 class="anchored" data-anchor-id="monolingual-data-copying">Monolingual data copying</h2>
<p>like frustratingly effective at improving your models is monolingual data copying and the issue is that back translation may help with structure but one of the issues with the resource language systems is that they tend to fail really badly on unusual vocabulary so like for example proper names or something like this so you might get a back translation system that’s very good at getting the structure right but get it gets you know all of your proper names and entities incorrect so basically one thing that you can do here is you just copy the target data into the source data and then you’re done and this kind of guarantees to maintain the entities so or the the rare words so that will help mitigate these issues of like vocabulary being dropped yeah something to point out with copying is that even in languages with different scripts it seems to work really well. &gt; Maybe because of auto and clutter objective stuff yeah even in languages with different scripts</p>
</section>
<section id="transfer-learning" class="level2">
<h2 class="anchored" data-anchor-id="transfer-learning">Transfer learning</h2>
<blockquote class="blockquote">
<p>There’s actually a nice paper by the same authors who wrote this paper where they examine this. Basically they are trying to figure out <strong>transfer learning</strong>. Why does transfer learning have this positive effect? One of the things that they <mark>show is that even making sure the length is the same or approximately the same or making sure that the words are output in approximately the same order as the input is is effective for improving translation accuracy.</mark> If you have a low resource language the translation system might drop half the content or it might like totally mess up the order or something like this This paper is demonstrating that kind of just like a monotonic bias and a bias towards outputting approximately the same number of words gets you a long way in improving the results which of course monolingual data copying would also do</p>
</blockquote>
</section>
<section id="dictionary-based-augmentation" class="level2">
<h2 class="anchored" data-anchor-id="dictionary-based-augmentation">Dictionary based augmentation</h2>
<blockquote class="blockquote">
<p>So for the final things which we’re also reading so we can talk more about them in the discussion. We had dictionary based augmentation and dictionary based augmentation basically finds rare words in the source sentences it could also be in the target sentence and tries to replace the words with other words that are kind of in the same semantic class. It replaces car with motorbike and then using a lexicon. It replaces the words in the targeted sentence as well. It’s basically creating more sentences to augment augmented data with like words that are less frequent in the original</p>
</blockquote>
</section>
<section id="word-alignment" class="level2">
<h2 class="anchored" data-anchor-id="word-alignment">Word alignment</h2>
<blockquote class="blockquote">
<p>In order to do this they need to use a tool called word alignment What Word alignment does is, it essentially takes in two parallel corpora. The parallel corpora you want to find which words align to each other in the source and target sentences. This is useful for a number of reasons it’s useful for analysis it’s useful for cross-lingual transfer learning I talked about supervised alignment as a training method last time I believe and there’s a couple methods to do so there’s again traditional symbolic methods which like BLEU are based on exact lexical match or you know some variety of clustering</p>
</blockquote>
<p>giza ++ has some clustering involved in it but recently neural methods have been largely outperforming these and I can recommend a highly our aligner called awesome lion I i didn’t name it I’m far too humble to name my alignment or awesome line but but it’s pretty awesome I have to say and basically it uses multilingual perks and it tries to find things that are similar but it’s also fine-tuned multilingually on supervised data so basically there’s some supervision that goes into it to try to inform the aligner about the outputs and it works on any language that’s included in mvert again like the question before it won’t work on very low resource languages of course so then you might be stuck with keystone plus plus and faster Word by word data augmentation so you can also do things like word by word data augmentation where you simply translate sentences word by word into the target sentence using a dictionary this is another frustratingly you know effective method like monolingual data copying however there are problems like word order and syntactic divergence so if you get like I the new car bought number one the order is strange number two these words don’t actually align with each other so that’s a problem so Reordering other things you can do or you can try to decrease this divergence with reordering or rules so this was also another paper in the potential reading and basically what the idea is that you a priori do some reordering from one language from english into like reordered english and then do data augmentation on top of that and the good thing about this is like english has a lot of analysis tools you could like do syntactic parsing of english get the syntactic structure build reordering rules on top of that and then just apply dictionary-based translation and then the hope would be that you would get something that looks a lot more like japanese than if you just translated english word by word and one interesting thing we showed here was we demonstrated that this was useful for japanese translation but then we applied the exact same reordering rules and also applied it to wigger which is another language that’s completely different different language family but it’s it has a very similar syntax to japanese so because of that the exact same reordering rules for english were still effective in improving the results for weaker english translations so because of that you know it’s not language dependent it’s rather syntax dependent and because there’s syntactic similarities between the language it helps so yeah given that we now have the</p>
</section>
<section id="assignment" class="level2">
<h2 class="anchored" data-anchor-id="assignment">Assignment</h2>
<p>assignment actually this is this slide is missing one of the one of the papers that was a potential paper to read so first before we go to the discussion are there any questions so I kind of breezed through it the last part quickly but hopefully we can also talk about them in the discussion okay if not this time we’re going to try a new experiment we’re going to try to make six groups so the groups are going to be half the size and they’re going to be front middle front right front left back middle back left back middle in that right so we’re gonna ask everybody to talk a little bit more quietly but also you’ll be in a smaller circle so hopefully that’ll be easier and yeah let’s go ahead and actually guys since we’re running a little bit late I think maybe we’ll skip the reporting part this time is that okay and we’ll just you know be within our groups and if there’s anything really interesting we can share on piazza or something okay</p>
</section>
</div>
</div>
</div>
<section id="papers" class="level2">
<h2 class="anchored" data-anchor-id="papers">Papers:</h2>
<ul>
<li><span class="citation" data-cites="xia-etal-2019-generalized">Xia et al. (2019)</span> <a href="https://aclanthology.org/P19-1579/">Generalized data augmentation for low-resource translation</a>.</li>
<li>Transfer HRL to LRL
<ul>
<li><span class="citation" data-cites="zoph-etal-2016-transfer">Zoph et al. (2016)</span> <a href="https://aclanthology.org/D16-1163/">Transfer Learning for Low-Resource Neural Machine Translation</a><br>
</li>
<li><span class="citation" data-cites="nguyen-chiang-2017-transfer">Nguyen and Chiang (2017)</span> <a href="https://aclanthology.org/I17-2050/">Transfer Learning across Low-Resource, Related Languages for Neural Machine Translation</a></li>
</ul></li>
<li>Joint training with LRL and HRL parallel data
<ul>
<li><span class="citation" data-cites="johnson-etal-2017-googles">Johnson et al. (2017)</span> <a href="https://aclanthology.org/Q17-1024/">Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation</a></li>
<li><span class="citation" data-cites="neubig-hu-2018-rapid">Neubig and Hu (2018)</span> <a href="https://aclanthology.org/D18-1103/">Rapid Adaptation of Neural Machine Translation to New Languages</a></li>
</ul></li>
<li>Back Translation
<ul>
<li><span class="citation" data-cites="sennrich-etal-2016-improving">Sennrich, Haddow, and Birch (2016)</span> <a href="https://aclanthology.org/P16-1009.pdf">Improving neural machine translation models with monolingual data</a>.</li>
<li><span class="citation" data-cites="edunov-etal-2018-understanding">Edunov et al. (2018)</span> <a href="https://aclanthology.org/D18-1045.pdf">Understanding Back-Translation at Scale</a></li>
<li><span class="citation" data-cites="pham2021metabacktranslation">Pham et al. (2021)</span> <a href="https://arxiv.org/abs/2102.07847">Meta Back-translation</a></li>
</ul></li>
<li><span class="citation" data-cites="currey-etal-2017-copied">Currey, Miceli Barone, and Heafield (2017)</span> <a href="https://aclanthology.org/W17-4715/">Copied Monolingual Data Improves Low-Resource Neural Machine Translation</a></li>
<li><span class="citation" data-cites="fadaee-etal-2017-data">Fadaee, Bisazza, and Monz (2017)</span> <a href="https://aclanthology.org/P17-2090/">Data Augmentation for Low-Resource Neural Machine Translation</a></li>
<li>Word-by-word Data Augmentation
<ul>
<li><span class="citation" data-cites="lample2018unsupervisedmachinetranslationusing">Lample et al. (2018)</span> <a href="https://arxiv.org/abs/1711.00043">Unsupervised Machine Translation Using Monolingual Corpora Only</a></li>
</ul></li>
<li>Word-by-word Augmentation w/ Reordering
<ul>
<li><span class="citation" data-cites="zhou2019handlingsyntacticdivergencelowresource">Zhou et al. (2019)</span> <a href="https://arxiv.org/abs/1909.00040">Handling Syntactic Divergence in Low-resource Machine Translation</a></li>
</ul></li>
</ul>
</section>
<section id="in-class-assignment" class="level2">
<h2 class="anchored" data-anchor-id="in-class-assignment">In-class Assignment</h2>
<p>Read one of the cited papers on heuristic data augmentation - <span class="citation" data-cites="fadaee-etal-2017-data">Fadaee, Bisazza, and Monz (2017)</span> or - <span class="citation" data-cites="zhou2019handlingsyntacticdivergencelowresource">Zhou et al. (2019)</span></p>
<ul>
<li>Try to think of how it would work for one of the languages you’re familiar with</li>
<li>Are there any potential hurdles to applying such a method? Are there any improvements you can think of?</li>
</ul>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<ul>
<li><a href="https://github.com/moses-smt/giza-pp">GizA++</a></li>
<li><a href="https://github.com/moses-smt/mgiza">mgiza</a></li>
<li><a href="https://github.com/clab/fast_align">Fast Align</a></li>
<li><a href="https://github.com/neulab/awesome-align">awesome-align</a> (<span class="citation" data-cites="dou2021wordalignmentfinetuningembeddings">Dou and Neubig (2021)</span>)</li>
</ul>
<pre><code>doch jetzt ist der Held gefallen . ||| but now the hero has fallen .
neue Modelle werden erprobt . ||| new models are being tested .
doch fehlen uns neue Ressourcen . ||| but we lack new resources .</code></pre>
<pre><code>0-0 1-1 2-4 3-2 4-3 5-5 6-6
0-0 1-1 2-2 2-3 3-4 4-5
0-0 1-2 2-1 3-3 4-4 5-5</code></pre>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-currey-etal-2017-copied" class="csl-entry">
Currey, Anna, Antonio Valerio Miceli Barone, and Kenneth Heafield. 2017. <span>“Copied Monolingual Data Improves Low-Resource Neural Machine Translation.”</span> In <em>Proceedings of the Second Conference on Machine Translation</em>, edited by Ondřej Bojar, Christian Buck, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, and Julia Kreutzer, 148–56. Copenhagen, Denmark: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/W17-4715">https://doi.org/10.18653/v1/W17-4715</a>.
</div>
<div id="ref-dou2021wordalignmentfinetuningembeddings" class="csl-entry">
Dou, Zi-Yi, and Graham Neubig. 2021. <span>“Word Alignment by Fine-Tuning Embeddings on Parallel Corpora.”</span> <a href="https://arxiv.org/abs/2101.08231">https://arxiv.org/abs/2101.08231</a>.
</div>
<div id="ref-edunov-etal-2018-understanding" class="csl-entry">
Edunov, Sergey, Myle Ott, Michael Auli, and David Grangier. 2018. <span>“Understanding Back-Translation at Scale.”</span> In <em>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, edited by Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii, 489–500. Brussels, Belgium: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/D18-1045">https://doi.org/10.18653/v1/D18-1045</a>.
</div>
<div id="ref-fadaee-etal-2017-data" class="csl-entry">
Fadaee, Marzieh, Arianna Bisazza, and Christof Monz. 2017. <span>“Data Augmentation for Low-Resource Neural Machine Translation.”</span> In <em>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em>, edited by Regina Barzilay and Min-Yen Kan, 567–73. Vancouver, Canada: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/P17-2090">https://doi.org/10.18653/v1/P17-2090</a>.
</div>
<div id="ref-johnson-etal-2017-googles" class="csl-entry">
Johnson, Melvin, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, et al. 2017. <span>“<span>G</span>oogle‘s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation.”</span> Edited by Lillian Lee, Mark Johnson, and Kristina Toutanova. <em>Transactions of the Association for Computational Linguistics</em> 5: 339–51. <a href="https://doi.org/10.1162/tacl_a_00065">https://doi.org/10.1162/tacl_a_00065</a>.
</div>
<div id="ref-lample2018unsupervisedmachinetranslationusing" class="csl-entry">
Lample, Guillaume, Alexis Conneau, Ludovic Denoyer, and Marc’Aurelio Ranzato. 2018. <span>“Unsupervised Machine Translation Using Monolingual Corpora Only.”</span> <a href="https://arxiv.org/abs/1711.00043">https://arxiv.org/abs/1711.00043</a>.
</div>
<div id="ref-neubig-hu-2018-rapid" class="csl-entry">
Neubig, Graham, and Junjie Hu. 2018. <span>“Rapid Adaptation of Neural Machine Translation to New Languages.”</span> In <em>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, edited by Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii, 875–80. Brussels, Belgium: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/D18-1103">https://doi.org/10.18653/v1/D18-1103</a>.
</div>
<div id="ref-nguyen-chiang-2017-transfer" class="csl-entry">
Nguyen, Toan Q., and David Chiang. 2017. <span>“Transfer Learning Across Low-Resource, Related Languages for Neural Machine Translation.”</span> In <em>Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</em>, edited by Greg Kondrak and Taro Watanabe, 296–301. Taipei, Taiwan: Asian Federation of Natural Language Processing. <a href="https://aclanthology.org/I17-2050/">https://aclanthology.org/I17-2050/</a>.
</div>
<div id="ref-pham2021metabacktranslation" class="csl-entry">
Pham, Hieu, Xinyi Wang, Yiming Yang, and Graham Neubig. 2021. <span>“Meta Back-Translation.”</span> <a href="https://arxiv.org/abs/2102.07847">https://arxiv.org/abs/2102.07847</a>.
</div>
<div id="ref-sennrich-etal-2016-improving" class="csl-entry">
Sennrich, Rico, Barry Haddow, and Alexandra Birch. 2016. <span>“Improving Neural Machine Translation Models with Monolingual Data.”</span> In <em>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, edited by Katrin Erk and Noah A. Smith, 86–96. Berlin, Germany: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/P16-1009">https://doi.org/10.18653/v1/P16-1009</a>.
</div>
<div id="ref-xia-etal-2019-generalized" class="csl-entry">
Xia, Mengzhou, Xiang Kong, Antonios Anastasopoulos, and Graham Neubig. 2019. <span>“Generalized Data Augmentation for Low-Resource Translation.”</span> In <em>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, edited by Anna Korhonen, David Traum, and Lluís Màrquez, 5786–96. Florence, Italy: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/P19-1579">https://doi.org/10.18653/v1/P19-1579</a>.
</div>
<div id="ref-zhou2019handlingsyntacticdivergencelowresource" class="csl-entry">
Zhou, Chunting, Xuezhe Ma, Junjie Hu, and Graham Neubig. 2019. <span>“Handling Syntactic Divergence in Low-Resource Machine Translation.”</span> <a href="https://arxiv.org/abs/1909.00040">https://arxiv.org/abs/1909.00040</a>.
</div>
<div id="ref-zoph-etal-2016-transfer" class="csl-entry">
Zoph, Barret, Deniz Yuret, Jonathan May, and Kevin Knight. 2016. <span>“Transfer Learning for Low-Resource Neural Machine Translation.”</span> In <em>Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</em>, edited by Jian Su, Kevin Duh, and Xavier Carreras, 1568–75. Austin, Texas: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/D16-1163">https://doi.org/10.18653/v1/D16-1163</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Data-Driven {Strategies} for {NMT}},
  date = {2022-02-03},
  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w07-data-driven-NMT/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Data-Driven Strategies for NMT.”</span>
February 3, 2022. <a href="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w07-data-driven-NMT/">https://orenbochman.github.io/notes-nlp/notes/cs11-737-w07-data-driven-NMT/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Multilingual NLP</category>
  <category>NLP</category>
  <category>Notes</category>
  <category>NMT</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w07-data-driven-NMT/</guid>
  <pubDate>Wed, 02 Feb 2022 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Unsupervised Machine Translation</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w10-unsupervised-NMT/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/tiling.png" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>course banner</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/eT0gxcGKbD0" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Lesson Video
</figcaption>
</figure>
</div><div id="sup-slide-deck" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div>

<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Unsupervised MT</li>
<li>Unsupervised Pre-training (LM, seq2seq)</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- Transcript here is machine generated no punctuation and I eddited it a but but it needs more work and I dont see the benefits unless I can run it through some tool. -->
<section id="intro" class="level2">
<h2 class="anchored" data-anchor-id="intro">Intro</h2>
<p>I’m going to be talking about unsupervised machine translation and this is a very interesting topic overall. If you’ve done the reading, you can see that it’s to a greater or lesser extent practical for some varieties of text-to-text translation, but I think there are a lot of other applications as well, maybe including with speech or other things that we’re going to be talking about in the future. I think the underlying technology is interesting and worth discussing and knowing about both with respect to the techniques and the limitations and other things like this.</p>
</section>
<section id="conditional-text-generation" class="level2">
<h2 class="anchored" data-anchor-id="conditional-text-generation">Conditional Text Generation</h2>
<p>Conditional text generation, which we’ve talked about. Basically, we’re generating text according to a specification like I talked about before in the seq2seq models class. We have our input x and our output y, where it could be for machine translation image captioning summarization, speech recognition, etc. The way we model this is using some variety of conditional language models like seq2seq model, like the ones you’re training for assignment two with our encoder and our decoder, and the way we traditionally estimate the model parameters here is using maximum likelihood estimation to maximize the likelihood of the output given the input, and generally, this needs supervision in the form of parallel data, usually millions of parallel sentences. ## What if we don’t have parallel data?</p>
<blockquote class="blockquote">
<p>What we’re going to ask about in this class is what if we don’t have parallel data, so just to give a few examples of this. We have parallel data? Well, what if we don’t have parallel data? For example, let’s say we have a photo of a person’s face or something like that. We automatically want to turn it into a painting, you know to put on your wall and display or something like that or turn it into a cartoon because you want a picture of yourself for your social media profile in a cartoon or something so, unfortunately, we don’t have tons and tons of data for this but we do have tons of photos and tons of paintings so we have lots of input x and lots of output y but very few pairs of input x and output y we could also do other things like transferring images between genders or between ages or something like this I think you’ve seen apps that might do this text from impolite to polite so you know correcting the formality transferring a positive review to a negative review or vice versa or doing something like machine translation and I actually modified this to give a few other examples like some really but the slides disappeared some really interesting examples are what if we had an ancient language or a cipher where we didn’t actually know what it was we didn’t have any text but we wanted to decipher this old text and replicate and like understand what it meant in the modern language so that’s another thing that we could do with unsupervised translation.</p>
</blockquote>
</section>
<section id="cant-we-just-collectgenerate-the-data" class="level2">
<h2 class="anchored" data-anchor-id="cant-we-just-collectgenerate-the-data">Can’t we just collect/generate the data?</h2>
<blockquote class="blockquote">
<p>Another question is “Couldn’t we just collect or generate data for these tasks”. To some extent, the answer is yes we could for some but it could be too time-consuming or expensive and it can also be difficult to specify what to generate or even evaluate the quality of generations so if we said generate generate this text like Joe Biden said it many people here you know don’t know what Joe Biden sounds like enough to be able to even do this in the first place. You know it’s difficult and under-specified, and finding people who’d be able to do that would be difficult, and because of this, it often doesn’t result in good-quality data sets.</p>
</blockquote>
</section>
<section id="unsupervised-translation" class="level2">
<h2 class="anchored" data-anchor-id="unsupervised-translation">Unsupervised Translation</h2>
<blockquote class="blockquote">
<p>An unsupervised translation the basic idea is we have some seq2seq task you know translation being the stereotypical example but it could be any of the other ones that I talked about where we instead of using monolingual data to improve an existing NMT system trained on parallel data or reducing the amount of supervision we’d like to talk about can we learn without any supervision whatsoever.</p>
</blockquote>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<blockquote class="blockquote">
<p>There are some core concepts in unsupervised MT and these core concepts include initialization iterative back translation bidirectional model sharing and denoising auto encoding and actually from the point of view of unsupervised MT. In some cases people have also used older statistical machine translation techniques instead of neural machine translation techniques because they were more robust.</p>
<p>I’ll talk a little bit about what statistical MT you know means because we haven’t really talked about it here yet and I’ll also explain a little bit about why it is more robust and you know what what we could do to also improve robustness of neural MT as well.</p>
</blockquote>
</section>
<section id="step-1-initialization" class="level2">
<h2 class="anchored" data-anchor-id="step-1-initialization">Step 1: Initialization</h2>
<blockquote class="blockquote">
<p>For step one in initialization, basically a prerequisite for unsupervised mt is that we start out with an initial model that can do something that can do some sort of mapping between sequences in an appropriate way so that we can use it to seed a downstream learning process to do translation and it basically adds a good prior to the state of solutions we want to reach and the way this is done is by using approximate translations of subwords words or phrases usually and the way we take advantage of this is we take advantage of the fact that the context of a word is often similar across languages since each language refers to the same underlying physical world and what we do is we rely on unsupervised word translation.</p>
</blockquote>
</section>
<section id="initialization-unsupervised-word-translatic" class="level2">
<h2 class="anchored" data-anchor-id="initialization-unsupervised-word-translatic">Initialization: Unsupervised Word Translatic</h2>
<blockquote class="blockquote">
<p>I talked about this a little bit two classes ago. I also called it a bilingual lexicon induction, and the basic idea is that word embedding spaces in two languages are isomorphic and what I mean by this is if you take an embedding space from one language like English in embedding space from another language, let’s say, Spanish we can learn some function that isn’t overly complicated that allows us to map between these two embedding spaces so, for example, we might run a model like word to back or any other you know kind of word embedding induction technique to embed individual words and embedding spaces and then we learn a mapping between them like an orthogonal like a matrix transformation w x equals y maybe with some constraints like the w is orthogonal which basically makes the embedding mutually makes the embedding like bijective so you can mutually map between one embedding space and another embedding space and we hope that by applying this transformation we will end up with something where the words in one embedding space are or the words in both embedding spaces if they’re close together the words are similar semantically or syntactically and this is hard to believe that this would actually work. I actually remember going to a presentation in ACL, I think 2016 where this method was proposed and I was like there’s no way this could possibly work because you’re assuming that you embed words and then just transform them in some way and the distributional properties cause them to line up, in fact, it does work better than you would think it would and there’s a couple of reasons for this one reason for this is that in addition to distributional properties a lot of the word embedding techniques that are used in these mappings here also take into account like sub word information and then if you’re mapping between English and Spanish or English and German.</p>
<p>A lot of the words have similar spellings and that can give additional indication about whether the words are similar another reason is like words like gato and cat are both common and you know common words tend to map the common words uncommon words so you’re also basically implicitly using word frequency information in the mapping and frequent words in word embedding spaces often tend to have larger norms because they’re updated more frequently and so that kind of implicitly gets added into this calculation as well so there’s a bunch of things working for this nonetheless it doesn’t work perfectly it works kind of well enough to do something and in the case of initialization that’s mainly what we’re looking for we’re mainly looking for something that you know starts us out in some sort of reasonable space</p>
</blockquote>
</section>
<section id="unsupervised-word-translation-adversarial-t" class="level2">
<h2 class="anchored" data-anchor-id="unsupervised-word-translation-adversarial-t">Unsupervised Word Translation: Adversarial T</h2>
<blockquote class="blockquote">
<p>What if the words in the two domains are not one to one or what if the words in the languages are not one to one. To give an example let’s say we’re mapping English to Turkish where in English a single verb has you know one or five conjugations and then in Turkish it has like 80. The answer is it basically doesn’t work it doesn’t work it doesn’t work very well so that’s another thing that you need to be concerned about but if the morphology is approximately the same it can it can still do something and the later steps of the unsupervised translation pipeline can also help disambiguate like ambiguous words and other things like that so yeah I saw another hand yeah so it’s a it’s a combination of words being frequent and words being words appearing in the same context. So for example just to give an example proper names proper names don’t work particularly well for this method because especially if you have like a Japanese newspaper and English newspaper the proper names mentioned in the English newspaper tend to be English names and the ones in the Japanese newspaper tend to be Japanese names for example however, there are these clusters of proper names and proper names have a certain frequency profile and they tend to cluster together so at the very least you should be able to get the fact that things in this cluster are proper names for example and you know that like I think intuitively you can see how that would be universal across languages more or less so you can get at least that close and then there’s other things like if two languages have determiners are almost always like the most common things so you could map determiners between languages and get them right uh most of the time stuff but uncertain completely unsupervised methods work kind of okay but not really well so yeah also an auxiliary point is that there’s very few languages in the world where you don’t have any dictionary whatsoever if there are languages in the world that don’t have a dictionary they also probably don’t have written text that you could learn word embeddings from so completely unsupervised you know you might not need it but you might want to do something with like just a dictionary and so another another thing that’s commonly done is like so the way another way to do this distribution matching or to learn this distribution matching is basically to have an adversarial objective and the adversarial objective basically what it does is it tries to prevent a model from being able to distinguish between the this x times w and y. So the idea is you want to move the spaces so close together that like a sophisticated neural network or some sort of you know discriminator is not able to distinguish between them so that’s the actual mechanism for doing the distribution matching another thing that is commonly done which I talked about two classes ago but isn’t included in the slides here is you get an initial first pass where you you find like several points that you’re very confident in like several points that don’t that are mutual nearest neighors of each other but don’t have close other close mutual nearest neighbors and you use those as basically pseudo supervision and then super create a supervised model that tries to make sure that those get mapped as close together as possible while making others farther apart and do an iterative process where you gradually like increase the number of words that are mapped together and that further improves accuracy now if you have like a small amount of supervision if you have like i don’t know 50 words in a dictionary you could use that you could use that to do supervision directly without having to do the unsupervised distribution matching at first and in fact two papers were presented at basically the same time one was a paper and completely unsupervised mapping another was a paper where they only use numbers like numbers were the only thing that they used to cross the language because numbers tend to be written in Roman characters in many languages in the world so sorry let Latin characters in many languages in the world so because of that you could use just that as supervision and then that would get you a long way too so if you have a dictionary that gives you better results. Usually, even a smaller does that actually really work because just because numbers I feel like they’re the information about that number and the word embedding is often not it doesn’t really encode what the number actually is just kind of that it is a number a lot of the time right and but I think basically the idea is if you can get any if you can get any supervision it’s better than no supervision and you’re still gonna have like ideally some sort of distribution matching component in your objective anyway so yeah yes with this method we still ensure that your viewers that’s a really good question so you if you have supervision you wouldn’t necessarily have to do that we’ve done some work on unsupervised embedding induction and it almost always helps to ensure that w is orthogonal and it or it almost always helped us anyway to ensure that w was orthogonal and it almost always helped us to not use anything more complicated than a linear transform like you would think you’d be able to throw a big neural network at it and it would do better but like even in supervised settings I guess the problem is too underspecified and that didn’t help very much not to say it wouldn’t have verb cool okay so the next so the next thing is we pull out our favorite data augmentation method date back translation and so we take for example French and back translated monolingual data into English and we take English and we back translate monolingual data into French and so here we have parallel data here but what we can do instead is we can do like pseudo parallel data which I’ll talk about in a second.</p>
</blockquote>
</section>
<section id="one-slide-primer-on-phrase-based-statistical" class="level2">
<h2 class="anchored" data-anchor-id="one-slide-primer-on-phrase-based-statistical">One slide primer on phrase-based statistical</h2>
<blockquote class="blockquote">
<p>Next I’d like to explain how we apply these methods to a non-neural MT system and just to give a very brief overview. A one-slide primer on phrase-based machine translation so this is what a lot of people used to do machine translation before neural mt came out and it consists of three steps first the input the source input is segmented into phrases these phrases can be any sequence of words not necessarily linguistically motivated so they don’t need to be like a noun phrase or verb phrase or anything like this then you take out a dictionary and you replace like the dictionary has translations of the phrases into the target language maybe it has you know 10 or so candidates for each phrase and then the phrases are reordered to get into the correct order and this is a nice method for some reasons and the one of the reasons why it’s a nice method is it’s guaranteed to basically cover every word in the inputs and not do any you know if the model is trained okay not do any really really crazy things so if you guys are struggling with assignment two right now already and you’re getting your like low resource machine translation neural machine translation system it’s probably doing things like repeating the same word 300 times in a row or something like that a phrase-based machine translation system would not do this it might give you a bad output but it wouldn’t you know repeat the same thing over and over again or translate a sentence into like an empty sentence or something like that precisely because it has to cover all the words and it can only use a fixed set of translations so because of this it has a strong bias to generate like non-nonsense but it’s also you know not as powerful as neural mt basically so the segmenting into phrases is easy the reordering is not easy but possible but in order to translate each phrase you need parallel data for this that’s a problem in unsupervised MT of course</p>
</blockquote>
</section>
<section id="unsupervised-statistical-mt" class="level2">
<h2 class="anchored" data-anchor-id="unsupervised-statistical-mt">Unsupervised Statistical MT</h2>
<blockquote class="blockquote">
<p>The way unsupervised statistical mt could work which is also detailed in these two papers by arteza in limple is you learn monolingual embeddings for unigrams diagrams and trigrams you initialize phrase tables from cross-lingual mappings of these embeddings so basically you initialize the phrases that could be used in a phrase-based machine translation system and then you do supervised training based on back translation and iterate so what this what this means is basically you take the phrase tables from cross-lingual mappings you estimate a language model on the target language and then you just translate all of the all of the monolingual data and after you’ve translated all the monolingual data then you can then feed it into your like normal machine translation training pipeline and the key here is that you know you’re inducing like each of these phrase translations just by mapping embeddings of unigram’s diagrams and trigrams cool and what we can see here is that if we start out with the unsupervised phrase table and translate from French to English we get a blue score of 17.5 and then as we add more iterations and translate and translate and translate and learn from the translated data the score gets a bit better every time. Basically, it’s this iterative process of recreating the data back translation in one direction the other direction one direction the other direction in training.</p>
<p>Unsupervised neural mt the way this works is we create a neural MT system and actually the exact same procedure could also be done for neural mt with the caveat that you can’t create a phrase table so you would need another you would need another method for learning the initial model because you can’t like induce a phrase table from embeddings so in addition to using that same procedure there’s one thing that you can do to improve the neural MT model and basically the way it works is you take the encoder-decoder model and you use the same encoder-decoder for both languages and another thing that you can do is you can initialize the inputs with cross-lingual embeddings and the idea of what you do here is you train the model to output French but you have the inputs be either French so it’s like an auto encoding objective or English here so if you have this French token here this is basically saying, I want you to generate french next so the model is you know basically guaranteed to generate French and but because these input embeddings are initialized with bilingual like coordinated embeddings the inputs look very similar so it’s like the inputs look similar I know I want to generate French, so if we just train on this encoding objective in the bottom which we can do for monolingual data it nonetheless learns how to translate is the hope and dream so we have a cool we have a couple objectives objective is the denoising</p>
</blockquote>
</section>
<section id="unsupervised-mt-training-objective-1" class="level2">
<h2 class="anchored" data-anchor-id="unsupervised-mt-training-objective-1">Unsupervised MT: Training Objective 1</h2>
<blockquote class="blockquote">
<p>Autoencoder objective and so what we do is we take the target sentence the source sentence we map it into a denoising autoencoder and what a denoising autoencoder does is basically we have a sentence in the source side we add some according to this corruption function c oops this corruption function c so we move x into some other place c we map it into a latent space and then we try to regenerate the original x and I’ll give an example of this in a moment and the other objective that we can use an unsupervised NMT is back translation so what we do is we translate the target to the source and we use this as a quote-unquote supervised example to translate the source to the targets.</p>
<p>So one example of but the noising objective would be just to like cross off individual words here and try to reproduce the original sentence from the crosstalk chords so we set their word embeddings to zero or something like this and so that would be going from this x to the c x and then we could just run a seq2seq model to try to take in this noise input and generate the output so that would be one example and in this particular case what we could do is we could try to translate this into the into another language and this translation could either be done with like a pre-trained model that we have already from our a previous iteration about unsupervised translation or it could be done with some sort of heuristic like mapping the words in the input and I think I have a slide about that coming up soon and so the reason why it works is as I said before cross-lingual embeddings and the shared encoder decoder gives the model a good starting point where translating from this is similar to translating from from this here and so then another objective that people use to further enforce this is an adversarial objective and what they try to do essentially is you have an auto encoder example or a back translation example and you apply an adversarial objective to try to force these two phrases together so this is also similar to what they use an unsupervised like word translation where you have a model and you try to get these encoder vectors to be so similar that you fool the model and it’s not able to distinguish between them yes yeah so actually mast lms are a variety of denoising autoencoding it’s basically a subset of denoising yeah well so here we’re not using any parallel data whatsoever so the back translation is we’re not using any real parallel data all of the parallel data we have is obtained through like that translation for example. ## How does it work? Basically there’s two ways that you can you can do this the the first way is with just using an auto encoding or denoising auto encoding objectives so to take the example here we have like I am a student and and so if you have bilingual embeddings or something like that where bilingual embeddings gotten out close together basically you know these word events look the same these sport embeddings maybe look the same and these were embedding the same so overall and so like despite the fact that what you want to do is on the bottom the vector the light blue vectors on the top if each of the word wrappers look similar also looks similar and then you can also do things like randomly masking out words to make it look a little bit like basically to make the problem of auto encoding more difficult and so you need to fill in more information so when you move from this queen English over here with some without words to French you know the problem also gets harder so you kind of need to infer missing things or differences from this but because you’re doing denoising it allows you to do that for back translation back translation is basically like what we talked about before it’s like more or less the same so actually perhaps the more important thing is yeah so wouldn’t that lead to more hallucination from the model yeah basically yes I think it’s going to be very hard to train a model that’s that works perfectly with through purely unsupervised translation but the idea one thing is that any mistakes in your one reason why back translation works in the first place is any mistakes in your training data tend to be more random than the actual correct training data so as you refine the model more and more and get a better and better model the hallucinations because they’re random tend to get washed out whereas the like correct translations tend to be reinforced because they’re more consistent so it still will hallucinate and make mistakes but hopefully the idea is that hopefully they get washed out it works yeah because yes we do not directly use this move by statement during the training the veterans practice later is was trained based on this data so somehow that we use the difference between not that of dsd yeah so this this slide is a little bit deceptive because this would be the ## Step 2: Back-translation example of like supervised back translation here in unsupervised back translation in the back translation in the unsupervised like MT paradigm basically you don’t use any parallel data you just use a denoising auto-encoding objective to seed your back translator and then use that to generate data so you’re never using any like actually parallel data yeah so basically you start out with just a model training using monolingual data so for example, English is ## Performance This is a graph from the original paper. I think there’s a big caveat in this graph so you need to be a little bit careful in interpreting it but basically the horizontal lines are an unsupervised translation model that uses lots of monolingual data but no parallel data. The non-horizontal lines are a model are the supervised translation model and basically what they’re showing here is with no monolingual no parallel data they’re able to achieve scores of about the same level as something trained with 10 to the five so 10 thousand parallel sentences a big caveat here is that they didn’t use any monolingual data in the in the supervised translation system so if they did the supervised translation system would probably be a lot better but still I mean it’s kind of interesting that you’re able to do anything at all with unsupervised translations so I think as long as you’re aware of their caveats it’s kind of an interesting graph·yeah so another so basically I’ll go to the open problems in unsupervised mts so basically this is exactly the problem that Ellen was pointing out which is unsupervised machine translation works in ideal situations so basically languages are fairly similar written with similar writing systems large monolingual data sets in the same domain and match the test domain so in this particular case they were using data from the European parliament and it basically the data in the English and the French or the English and the Germans was from exactly the same distribution and that really helps in like inducing the lexicon or doing translation and so when you have less related languages truly low resource languages diverse domains less monolingual data unsupervised machine translation performs less well and reasons for performance basically small monolingual data in low resource languages can result in bad embedding so if we don’t have lots of data in the low-resource language this won’t work because the embeddings will be too poor to get a good initialization different word frequencies or morphology like the English and Turkish example I talked about before also different content makes things like back translation are less effective in bootstrapping a translation model. So for example if you’re trying to translate Twitter in one language and you have news text in another language that’s not like back translation is not gonna be good for covering what is said on Twitter so just in an interest of time I’ll skip that one but so there are some things that can be used to improve so better initialization recently people have been using things like cross-lingual language models or multilingual language models to improve the initialization of multi of unsupervised translation models so things like masked language modeling across language. So basically what you what you can do is you can train a single monolingual or bilingual single multilingual language model as your encoder or both your encoder and your decoder and you use that to initialize the translation model and this is good because you’re initializing the whole model as opposed to just the input and for various reasons it’s nice to have it’s nice to have a single model that is trained on all the languages and just to give one example even in like Chinese or something that’s written an entirely different script than English there’s still lots of English words so if you’re training the English model in the Chinese model on tons and tons of monolingual data the English words can also help anchor you know things into the same semantic space because they appear in various languages as well another thing is masked sequence sequence modeling so there’s things like mass which basically they have an encoder decoder form formulation of mass language modeling where basically you mask out a piece of the input and you generate that masked-out piece of the output and recently a model that lots of people have been using is this mBART model multilingual bart model and the way it works is basically you mask out words in the input but then you you generate all of the words in the outputs so then you you basically train this on tons and tons of data it’s a pre-trained model that you can then use to initialize your downstream unsupervised NTM And I had some stuff about the unsupervised multilingual 70. I’m going to skip over that in the interest of time, but how practical is this strict unsupervised scenario? So one thing that I definitely can say is that a lot of the techniques that are used in unsupervised mt are practical and semi-supervised learning scenarios where we have a little bit of training data so we can either train the model first with an unsupervised method and then fine-tune using the parallel corpus or train the model using a parallel corpus and update with iterative back translation and part of the reason why it’s why this is particularly good is there’s very few languages in the world that don’t have any parallel data but have lots of monolingual data for example almost every language in the world has parallel data from the bible if it has any amount of monolingual data but the bible is very out of domain very you know small so because of that using that to speed a model but then doing basic almost unsupervised translation seems like a practical way to do like news translation or something like that and then another area where these techniques are practical is if you have a task where you like legitimately cannot get very much data whatsoever so one example being style transfer from informal to informal text where you know there’s not very much data especially in different languages. Another example that has been used recently is the translation between Java and Python for example where there’s like lots of Java lots of Python but very little you know very little parallel data there’s some parallel data but not very much so yeah the discussion question is pick a low resource language or dialect research all of the monolingual or parallel data that you can find online for it would unsupervised or semi-supervised mt methods be helpful and how could you best use the existing resources to set up an unsupervised or something supervised empty for success on this language or dialect and the this is a reference to a paper that you could take a look at to discuss that so cool any questions before we start the discussion.</p>
</blockquote>
</section>
</div>
</div>
</div>



<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Unsupervised {Machine} {Translation}},
  date = {2022-01-24},
  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w10-unsupervised-NMT/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Unsupervised Machine Translation.”</span>
January 24, 2022. <a href="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w10-unsupervised-NMT/">https://orenbochman.github.io/notes-nlp/notes/cs11-737-w10-unsupervised-NMT/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Multilingual NLP</category>
  <category>NLP</category>
  <category>Notes</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w10-unsupervised-NMT/</guid>
  <pubDate>Sun, 23 Jan 2022 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Translation and Translation Data</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w09-training-transfer/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/tiling.png" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>course banner</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/OO8dewmvm9w" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Lesson Video
</figcaption>
</figure>
</div><div id="sup-slide-deck" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div>




<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Translation and {Translation} {Data}},
  date = {2022-01-24},
  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w09-training-transfer/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Translation and Translation Data.”</span>
January 24, 2022. <a href="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w09-training-transfer/">https://orenbochman.github.io/notes-nlp/notes/cs11-737-w09-training-transfer/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Multilingual NLP</category>
  <category>NLP</category>
  <category>Notes</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w09-training-transfer/</guid>
  <pubDate>Sun, 23 Jan 2022 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Words, Parts of Speech, Morphology</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w04-words/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/tiling.png" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>course banner</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/doZLYpTW_S0" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Lesson Video
</figcaption>
</figure>
</div><div id="sup-slide-deck" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div>

<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>What is a Word, What is Morphology</li>
<li>UD Treebank Morphology Annotations</li>
<li>Morphological Analysis</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- Transcript here is machine generated no punctuation and I eddited it a but but it needs more work and I dont see the benefits unless I can run it through some tool. -->
<blockquote class="blockquote">
<p>So now we’re going to move on to the main part where we’re going to talk about words which sort of seems really fairly appropriate for any any nlp class because words are pretty important okay and this is going to give you a little bit more definition of what we mean by words and how that’s not always as well defined as it might be in languages like English so let’s try and count the words in this sentence bob’s handyman is a do-it-yourself kind of guy isn’t he okay so that’s a fairly standard sentence although I have selected it to be particularly interesting for this case but the question is where are the words okay so what I’ve done is I’ve tried to highlight what people might think the words actually are now if you grew up in europe and maybe other places as well you probably think that white space separated tokens is a good approximation for words and it is a good approximation for words but from a linguistic point of view it’s actually a little bit more complex than that and that’s what we’re actually going to talk about and when we actually look at other languages that’s the thing we want to highlight that the notion of what a word boundary actually is might not be as trivial as what you hope it’s going to be okay</p>
</blockquote>
<section id="whitespace" class="level2">
<h2 class="anchored" data-anchor-id="whitespace">Whitespace</h2>
<blockquote class="blockquote">
<p>Let’s have a look at the first word of the white speak white space identified token is Bob’s the apostrophe s is what’s called a clitic it’s not isolated word on its own it can’t stand on its own it only appears when it’s actually bound to some other word but in some sense it’s independent of the word that it’s bound to so what you can have is actually it can go into whole noun phrases rather than just single words so we talk about jack and jill’s bucket we’re not talking about Jack and Jill’s bucket we’re talking about Jack and Jill in the bucket belonging to Jack and Jill so that apostrophe s isn’t just going on the immediate token before but a bunch of tokens before that okay so be aware that that apostrophe s is a somewhat special thing in English and sometimes it’s a good idea to actually separate that apostrophe off and treat it as an independent token than actually just putting it together as bob’s let’s look at the next one handyman is really a now known compound okay we put a space in it because we’re not German but but some compounds in English, don’t get a space between them and it’s sort of really quite complex to know which ones are which and it’s really sort of up to the speaker and to decide how to do that but sorry the writer from the speaking point of view it’s even harder to distinguish between these but handyman really here is being used as a single word and it would be useful to keep them together even though they have a ascii white space between them is is a word as a word now let’s have a look at do it yourself it’s hyphenated and it really is a sort of set phrase it can be shortened to diy and we’d like to treat that as a single word but sometimes hyphenated forms should be separated and we have to make interesting decisions about that now we’ve got words like kinda and isn’t it that are fairly standard contractions in English we could write them out as kind space off and we could write isn’t it as is not but often when people are speaking they don’t do that and they actually do reduce form and sometimes when writing they may actually do that simplified form as well and do use these contractions and we have to make some decision about how these actually might appear and whether we want to separate them out many of these are sort of closed class they can only be applied to a number of words and they’re not general to anything but some of them are not apostrophe s can clearly go into anything and apostrophe ll can sort of go into any noun as well so that’s a hard it’s a decision that we actually have to make and it will affect all of our downstream tasks once we decide how to token these tokenize these into what we’re going to term words how we’re going to have word embeddings how we’re actually going to do parsing or whatever our next task is going to be.</p>
</blockquote>
</section>
<section id="other-languages" class="level2">
<h2 class="anchored" data-anchor-id="other-languages">Other Languages</h2>
<blockquote class="blockquote">
<p>In other languages it can be way more complex and sometimes way more simple. It could be that we actually have no spaces whatsoever and everything is joined together agglutinated and there has to be some process to try to separate these individual parts out if we don’t separate these things out what you’re going to have in a language is an awful lot of words so if you look at turkish for example which is an agglutinate language there’s an awful lot of compounding in it an awful lot of interesting morphology in it what you end up with is the number of words number of white space separated tokens and in the language is much bigger than English for because what we maybe think about as being phrases in English are actually whole single words with no white space between them there also can be ambiguity in those particular words about how we actually decompose them and on the right here we actually have a hebrew example where I’m sorry I can’t read that this where it can mean depending on how you separate that out into individual morphemes and her saturday and that in t and that her daughter all of those are potential meanings given the rest of the context but they’re all written as the same single word</p>
</blockquote>
</section>
<section id="linguistics" class="level2">
<h2 class="anchored" data-anchor-id="linguistics">Linguistics</h2>
<blockquote class="blockquote">
<p>Now let’s go back to our knowledge of linguistics and see how we might be able to use our knowledge of linguistics to be able to answer these questions about what words actually are.</p>
<p>Because we’re going to look at each of those and see how they we might be able to use that as a definition of a useful way of splitting things into little bits words so there could be phonetics so there could be something about the way things are being said that could actually tell us something about that we could say it’s got to have a of every word has to have a vowel in it although I don’t know where apostle vs which sometimes is a violent and sometimes doesn’t sometimes and it’s sometimes just phonology and it could be something about there’s some structure that’s actually required in syllables and so everything has to be at least one syllable if it has to be a word it could be morphology if we’re looking at the actual atoms that are in the morphemes that are in the word we could talk about the individual morphemes in there and make some definition based on that we could talk about syntax we could talk about whether we could exchange that for another word in the same class and treat those as being words if they have classes over them we could talk about semantics of whether it changes meaning and we could talk about pragmatics about realistically new york is used as a single word even though we happen to put a space in it but it’s treated basically as a fundamental a word because although it maybe historically has some relationship to the word new and the word york it really has nothing to do with that anymore let’s have a look at the orthographic</p>
</blockquote>
</section>
<section id="orthography" class="level2">
<h2 class="anchored" data-anchor-id="orthography">Orthography</h2>
<blockquote class="blockquote">
<p>Definition so this is where white space comes into mind and be aware that even in English white space is quite complex So you know we’ve got spaces we’ve got tabs we’ve got new lines we’ve got carriage returns we’ve got vertical tabs are all within the standard ascii set but beyond that we actually have a lot more if we look at unicode and we want to make definitions of that and we also have things like non-breakable space and a non-breakable space can be used and may appear on on on the web in html and we may or may not want to decide that as being a word boundary or not depending on our definitions okay and remember we’re talking here about orthography we are pretending that the whole world writes everything down and that when they write everything down it’s the same as what speech is most languages are not written we write an awful lot and most people on the planet are actually literate but they may speak languages that they’re not literate in and when you speak you don’t put any spaces between words okay you don’t say a space between each word and we don’t do that at all okay and so when you’re wondering how chinese people can understand chinese text when there’s no species in it think about how well you can understand speech which has got no species in it and yet you can still deal with it and the notion of a word has been around for a lot longer than the notion of writing so in other words we’ve had that notion and we need a definition of words that are actually independent of the written form</p>
</blockquote>
</section>
<section id="prosodic" class="level2">
<h2 class="anchored" data-anchor-id="prosodic">Prosodic</h2>
<blockquote class="blockquote">
<p>There can be a prosodic definition a prosodic definition is something to do with international phrases they can be hard to define but they’ll still have some reasonable definition and things which are actually grouped together in the single international phrase are often written as a single word in other languages so for example in the park in English the way I just said it is as a single international phrase in the park and many other languages would have the same concepts of location and determiners and park all in the single word that would have no spaces between it so in the park could be treated as a single word depending on what your definitions are going to be</p>
</blockquote>
</section>
<section id="semantic" class="level2">
<h2 class="anchored" data-anchor-id="semantic">Semantic</h2>
<blockquote class="blockquote">
<p>Semantic definitions these are word units that got some something like the same idea and so it might be useful to be able to treat these in a reasonable way we can think about colors that might have multiple names we can think about navy blue is is a color and but when written it would have probably have a space in it but we may want to treat that as a color which is the same as blue or red or yellow or any other single word color because navy blue is still a color okay and although technically maybe has something to do with glue and something to do with navy it might not be a reasonable thing to actually separate it out we can have a syntactic definition where we’re looking at blocks in these sentences so again new york is a good example here where it really is being used in the same sense as the word pittsburgh is it’s referring to a particular city in north america and just because it’s got a space in it doesn’t mean that we want to treat it as two different cities a new city and a york city which is elsewhere in there’s almost certainly places called york in north america but I can’t. Well there’s a one just outside toronto now this comes to the notion of how can we identify classes of words and I’m currently really talking about syntactic classes of words rather than semantic classes so so colors are sort of a semantic class but I’m interested in what words have got the same class that I could exchange them without what it might change the meaning but you know they get used in the same way now we’ve all been talking talk about these standard open classes which appear in most languages not all languages but most languages nouns verbs and adjectives and adverbs there’s almost an infinite number of those new ones can come along that didn’t exist before but they all fall into these particular classes. This is in contrast with closed classes where there’s a finite set in the language they’re sometimes called function words. It might be a difficult class to list absolutely everything but you rarely very rarely find new things moving into that class so things like prepositions so in above behind there’s a sort of finite set of those determiners the and ah this and that and pronouns I are somewhat finite in English conjunctions and or but not an exclusive or if you’re a computer scientist and other auxiliary verbs like is was have etc and these are sort of closed class they’re very common in the language they’re often short in a language and in most languages have something like them they might be something to do with morphemes but they don’t have random new ones appearing every day so if we think about what happens with the open class think about words which you know exist today that didn’t exist five years ago and think about some can you type them in the chat words which we have today that we didn’t have five years ago and you’ll find out that they’re all nouns verbs adjectives and adverbs and not closed classes because can anybody think of any words which we have now that we didn’t have five years ago covid excellent example I’m sorry to disappoint you, but if you have a look at wikipedia in 2015 there’s an article on coveted okay but it was dull and boring and uninteresting and the covered self-help group decided that they wanted to do something and become more popular but you’re right that covert was incredibly rare okay and probably was only used in occasional circumstances doom scrolling excellent quarantine yeah wfh all of these things are very very common now and sort of didn’t exist at all before and language is like that but all of these words for the most part are coming in to be nouns and verbs okay occasionally they’re going to be adjectives and maybe adverbs but for the most part these do change over time so you can’t list them all but for the close class ones you can sort of do it I mean there’s some really rare conjunctions that people don’t use nowadays like not withstanding okay apart from used as examples of really rare conjunctions that’s about the only time I use that word</p>
</blockquote>
</section>
<section id="tag-sets" class="level2">
<h2 class="anchored" data-anchor-id="tag-sets">Tag Sets</h2>
<blockquote class="blockquote">
<p>There’s a bunch of classes that define what part of speech tag sets the tags that are actually there and in English and many other languages they’re often based on the tag set that was used in the pen tree bank which was one of the first large data sets that was labeled consistently with nouns verbs etc and there’s 43 in total I don’t know if there’s 43 there but I know from other things and there’s some that are relatively rare and maybe questionable and there’s some compound ones where sometimes things get joined together it does make distinctions between different types of nouns singular plural and and proper nouns and it it’s quite useful in many tag sets that are now used in in NLP are derived from this tag set because that was decided mostly by the group at U-pen at the time people were doing a part of speech tagging before that. I mean even before computers they were doing it but coming up with a finite computational tag set was something that really started in the 80s and depending on the language that you’re dealing with you may want to have different tags because there are some tags that are really only relevant in some languages and not in others okay importantly there actually is a definition of a small number of tags which is in some sense a reduction of the number of tags that were in the pen tree bank that has been used in the universal dependency m sets that originally came out of google and it’s now an independent project but it’s still quite google influenced because of the original data sets and this covers somebody’s going to tell me the number of languages but I think it’s about 40 or 50 languages where they’re all labeled with the same tag set and produces a universal dependency grammar which is also very use useful from a syntactic point of view that over a bunch of fairly major important languages okay so getting this tag set is useful and you might say why should I care about getting a tag set I can train from words and the answer is yes if you have lots of data but as usual in trying to do machine learning if you can give more information in a structured way or in a reduced standardized way you can typically get by with less data and try to train better and training should happen faster so often being able to get a part of speech for a language would be quite useful and of course this is going to be hard in the low resource language because you sort of need labeled data to start off with but there are unsupervised ways to try to find out what these tags actually are now I’m naively talking about words here and words having part of speech tag because I’m one of these English speakers where actually for the most part that’s pretty easy in English for the most part you know white space separated tokens or words and for the most part each token has got one a tag one proper tag and the context mostly defines what it is but in most languages most languages and we really have to introduce the notion of morpheme which is sort of the single smallest atomic part of a word</p>
</blockquote>
</section>
<section id="morphology" class="level2">
<h2 class="anchored" data-anchor-id="morphology">Morphology</h2>
<blockquote class="blockquote">
<p>Here for example in English we do have interesting morphology especially in what’s called <em>derivational morphology</em> which is not like the <em>ing</em> <em>eds</em> plurals where we’re changing some syntactic property the tense or the number of a language but we’re actually changing the meaning and we’re often changing the class the part of the speech class so if we take a verb like <em>establish</em> we can have another verb that’s <em>disestablished</em> we can then make that into a noun by say <em>disestablishment</em>.</p>
<p>We can make it another noun putting <em>anti-</em> in front of it and saying <em>anti-disestablishment</em>. We can make it into an adjective by saying <em>anti-disestablishmentary</em>. We can make it a noun into <strong>anti-disestablishmentarian</strong>. We can then make that in a further noun by saying <strong>anti-disestablishmentarianism</strong> Now these things are a little bit extended, but actually we do this all the time. So it’s actually quite hard to really list all of the words that are in English because although some of these don’t appear very often. There will be new and novel words, and you’ll see a number per day of new words that you’ll understand. Where they’re actually morphologically variants of something that you can work out what the meaning actually is.</p>
<p>Now so it would be useful if we could get these words and decompose them into their roots and morphemes so that we can actually work out what the important classes are. So we’d like to be able to get some notion of these decomposed forms in from a word if we can do it. Now some of these forms are what we call stems or roots they’re often words on their own. And we’ll have prefixes and suffixes.<br>
In English we rarely have anything that inserts in the middle of a word we’re usually putting things at the beginning and end. In some languages you actually get sort of bracketed things that you have to put them at the beginning and the end some of the gaelics have got that. There are some things where you can actually put infixes and so there are some plural things that actually happen in interesting languages in southeast asia where they’re basically plural things where syllables or partial syllables will get duplicated and and therefore you have to deal with that.</p>
<p>In English the only example of being able to do that is the infix form of putting swear words in the middle of a word so for example if you have the words pittsburgh and you want to put a swear word in the middle. I can only get away with this because I’m British. I’m going to use the word bloody as a swear word, although actually, usually in linguistics we use the f-word but we can use the word <em>bloody</em> and if I want to put the word <em>bloody</em> in Pittsburgh, it’s going to be <em>pit’s— bloody—berg</em> and I could do that and it could be compounded and possibly but it’s a little bit of a stretch and maybe you would put species in in there to do that in other languages. Infects will happen in some languages. You’ll actually even change things in Templatic morphology in things like Hebrew and Arabic, and Tagalog is an example.</p>
<p>This is one of the examples from the Philippines where we’ve got interesting morphology going on and we’ve got much more interesting morphology going on than what’s in English and we’d like to be able to decompose these things so that we’ve got finite sets of morphemes when we’re doing processing so when we’re doing tokenization when before you give it to your word embedding system you’d like to have a standardized tokenizer that’s going to give you meaningful the most meaningful atomic parts when you actually do it.</p>
</blockquote>
</section>
<section id="arabic" class="level2">
<h2 class="anchored" data-anchor-id="arabic">Arabic</h2>
<blockquote class="blockquote">
<p>Arabic’s very interesting, because things actually are done within the consonants so you have a backbone of consonants and the vowels will change before and after after them. This changes meaning both semantically and inflectional so syntactic information about tense etc there’s a number of Semitic languages that actually do templatic morphology and it always breaks a lot of our systems from doing it but we are not allowed to define what natural language actually is we still need to be able to deal with it Chinese has a relatively small amount of morphology but it still has derivational morphology so you can take words and join them together in interesting compounds which are not necessarily directly to do with the meaning of the individual character that you’re joining together so a number of things of for example if you take fire and wheel and put it together it doesn’t mean a wheel that’s on fire it actually means a steam train and so or maybe that’s only in Japanese I always get that one wrong which varies between the different languages but there’s often a relationship but the compound might be different and so sometimes you want to be able to decompose it and sometimes you don’t so there’s two types of morphology which are identified as what’s called derivational morphology and derivational morphology you’re mostly changing the part of speech class when you’re doing things English is a rich derivational morphology and we can write it out and it’s mostly productive by productively means we’re allowed to construct new words without explaining to people what the meaning is inflectional morphology is usually syntactic class changes or some classic feature changing so this is things like changing the tense changing the plurality and the number and other languages it may do things like used in agreement used in tense and aspect and verbs and these are usually treated as different classes and they’re usually quite different they don’t overlap they’re sometimes maybe a little bit confusing and for the most part inflectional morphology happens after derivational morphology in almost all languages we can talk about morphosyntax about how these phonemes join together and which ones are allowed to join to each other and we can talk about a lower level thing that we call it morphofunnymix or morphographymix where things are changing at the boundaries when we join them together so an example of morphophony thin morphography mix because it happens in the graphene form but the pronunciation is affected as well when you take apostrophe s okay or the plural s when we add it onto things sometimes we insert an e when we add e d to the end of things if it already ends in an we don’t double the e and so this is I’ve got the e d and I’m going to join it on if the previous one is an e I’m just going to do it so move plus e d this is m-o-v-e-d okay while walked plus e-d is walk plus e-d joined together there’s different classes of</p>
</blockquote>
</section>
<section id="types-of-morphology" class="level2">
<h2 class="anchored" data-anchor-id="types-of-morphology">Types of Morphology</h2>
<blockquote class="blockquote">
<p>morphology and typography and typology of these so that we can put these into different groups and to be able to identify what they’re all about we have isolating or analytic m1s where there’s very little morphological changes in Vietnamese many of the Chinese dialects English are good examples of that we have synthetic ones where things are being created all the time they’re sometimes called fusional or flectional so german greek russian and templatic where we’ve got some form of often consonants and the vowels are changed and interspersed between those and we have a glutenatif where there’s lots of things joining together japanese finnish turkish are really good examples of that and we have polysynthetic which are really complexly joining things together where almost every phrase is actually ends up in a single word many of the north american native american languages are that the reason the word snow is there is because you can often have lots of variations of the word snow in inuit that’s actually not true well it is sort of true but it’s notable that in Scottish English we have lots of words for rain and for some reason I have no idea why that would be</p>
</blockquote>
</section>
<section id="morphology-analyzers" class="level2">
<h2 class="anchored" data-anchor-id="morphology-analyzers">Morphology Analyzers</h2>
<blockquote class="blockquote">
<p>People have worked on morphology for a long time so often when you’re working a new language there already is a morphological analyzer is there and if you look at the project unimorph and they’ve actually collected together these in a fairly standardized way so you might just be able to use it from python and all of a sudden you get morphological decomposition for your language or maybe you could use a nearby language and it would almost work and that might make your life a lot easier often when we’re doing novel languages especially when we’re caring about things under time we will look for one of those or we might even spend a couple of hours writing something because we’ll get something better than trying to do it fully automatically there are fully automatic ways but it might be better if there’s already something that allows us to do that okay there’s actually a competition</p>
</blockquote>
</section>
<section id="morphology-competition" class="level2">
<h2 class="anchored" data-anchor-id="morphology-competition">Morphology Competition</h2>
<blockquote class="blockquote">
<p>every year sigmar phone has been running for at least 10 years and it gets harder and harder every year as they find harder and harder tasks from both supervised and unsupervised m techniques so it’s worthwhile looking at these and using that as a resource</p>
</blockquote>
</section>
<section id="shared-tasks" class="level2">
<h2 class="anchored" data-anchor-id="shared-tasks">Shared Tasks</h2>
<blockquote class="blockquote">
<p>These shared tasks allow you to compete and people have done them from this class before actually and done interesting novel techniques to be able to work out to do it sharing information across language when you’ve got not enough data to train etc in order to be able to learn how to do that</p>
</blockquote>
</section>
<section id="finite-state-morphology" class="level2">
<h2 class="anchored" data-anchor-id="finite-state-morphology">Finite State Morphology</h2>
<blockquote class="blockquote">
<p>Finite state morphology is often used for morphemes actually morphology is never very complex there’s probably something to do with a human brain it can’t really deal with something as complex as say syntax within morphology and therefore it’s often quite localized and therefore finite state machines are quite good at being able to cover everything and there’s good toolkits out there that help you to be able to write these things there’s also completely unsupervised techniques there’s more fessor just a python thing you give it examples and it will try to find out the prefixes and suffixes that actually might allow you to be able to do analysis it does assume a certain segmental and view of phonology and therefore it can get confused sometimes and it might treat the different types of ed or just d in different forms in English and separate them out and maybe you want to join them together or maybe you don’t there’s a sort of related thing called stemming which is often quite useful especially when you’re doing things like information retrieval where you’d like to say look I i just I just want the root of the word and I don’t want all these other variations especially when you’re in a limited domain or when you’ve got limited amount of data and so maybe if you removed all of the morphological variants the plurals the eds it might be easier to do comparisons later especially if you don’t have good data in order to be able to do good word embedding there’s also purely completely automatic techniques and bpe is a good example of that byte pair encoding you can’t really work it out from the name where what we do is we look at the string of the actual letters that are there and try to find and optimize the sequence of letters together and the overall predictability of the group of letters that we actually find and this originally came out of work in machine translation to try and find the best segmentation for doing translation but we end up using it for lots of lots of things it’s often worth trying if you don’t have anything else because it does sort of work but you really want to know about for your particular language is it likely to work before you actually do all of your bpe you get a tokenization representation you build all your word embedding that you learn from it and then learn oh no you could have downloaded the morphological analyzer that would have given a better result and a more consistent result and therefore you would have been able to learn back</p>
</blockquote>
</section>
<section id="tokenization" class="level2">
<h2 class="anchored" data-anchor-id="tokenization">Tokenization</h2>
<blockquote class="blockquote">
<p>Tokenization is also something that we get this often gets called where you’re actually just trying to split these things into words and you’ve got to care about what the tokenization actually is because if you have a different tokenization that won’t be the same lemmetization or stemming is somewhat similar but limit hydration is really talking about the linguistic root of the word which may or may not be well well defined and it’s usually after morphological and decomposition you find the root of the word a we can also do this across languages you may want to care about characters rather than words and looking inside characters that can actually help and caring about things that are happening over long boundaries somewhat related to this is word segmentation in languages like japanese and chinese and in fact they end up using something similar to bpe to be able to segment things there’s a couple of related things</p>
</blockquote>
</section>
<section id="text-normalization" class="level2">
<h2 class="anchored" data-anchor-id="text-normalization">Text normalization</h2>
<blockquote class="blockquote">
<p>Text normalization where you actually are trying to replace everything as words we know that there’s an infinite number of numbers and would be nice if we could change them into words maybe or maybe just change them into the word number or maybe classes of numbers and this is something that’s been studied in a in text-to-speech and there’s various machine learning techniques to try to do well on this you might want to also care about spelling correction and do be aware that tokenization this mismatch can really break everything so if you’re using bert you sort of have to use their tokenization because they’ve assumed that and it can be quite hard if you do something else okay that’s everything about works and morphology we will be talking about morphology again later on in more detail but now we’re going to care about splitting out up out into groups and what we want you to do today is we want you to take one of these languages language families often morphology and or aspect of writing and orthography are similar within language families and I’d like you to identify something that you would need to care about if you were trying to do some form of tokenization</p>
</blockquote>
</section>
</div>
</div>
</div>
<section id="discussion-prompt" class="level2">
<h2 class="anchored" data-anchor-id="discussion-prompt">Discussion Prompt</h2>
<blockquote class="blockquote">
<p>Pick a language in one of the following branches of language families: Bantu, Dravidian, Finno-Ugric, Japonic, Papuan, Semitic, Slavic, Turkic. Tell us about some interesting aspects of morphology of that language, following examples from the assigned reading. Cite your sources.</p>
<p>If you would need to implement a tokenizer for that language, what language specific knowledge would need to be incorporated into the tokenizer?</p>
</blockquote>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Words, {Parts} of {Speech,} {Morphology}},
  date = {2022-01-24},
  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w04-words/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Words, Parts of Speech, Morphology.”</span>
January 24, 2022. <a href="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w04-words/">https://orenbochman.github.io/notes-nlp/notes/cs11-737-w04-words/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Multilingual NLP</category>
  <category>NLP</category>
  <category>Notes</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w04-words/</guid>
  <pubDate>Sun, 23 Jan 2022 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Code Switching, Pidgins, Creoles</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w11-code-switching/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/tiling.png" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>course banner</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/CjcB5nkfLBM" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Lesson Video
</figcaption>
</figure>
</div><div id="sup-slide-deck" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div>

<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Unsupervised MT</li>
<li>Unsupervised Pre-training (LM, seq2seq)</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- Transcript here is machine generated no punctuation and I eddited it a but but it needs more work and I dont see the benefits unless I can run it through some tool. -->
</div>
</div>
</div>



<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Code {Switching,} {Pidgins,} {Creoles}},
  date = {2022-01-24},
  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w11-code-switching/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Code Switching, Pidgins, Creoles.”</span>
January 24, 2022. <a href="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w11-code-switching/">https://orenbochman.github.io/notes-nlp/notes/cs11-737-w11-code-switching/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Multilingual NLP</category>
  <category>NLP</category>
  <category>Notes</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w11-code-switching/</guid>
  <pubDate>Sun, 23 Jan 2022 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Code Switching, Pidgins, Creoles</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w12-qa/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/tiling.png" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>course banner</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/CjcB5nkfLBM" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Lesson Video
</figcaption>
</figure>
</div><div id="sup-slide-deck" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div>

<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Unsupervised MT</li>
<li>Unsupervised Pre-training (LM, seq2seq)</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- Transcript here is machine generated no punctuation and I eddited it a but but it needs more work and I dont see the benefits unless I can run it through some tool. -->
</div>
</div>
</div>



<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Code {Switching,} {Pidgins,} {Creoles}},
  date = {2022-01-24},
  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w12-qa/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Code Switching, Pidgins, Creoles.”</span>
January 24, 2022. <a href="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w12-qa/">https://orenbochman.github.io/notes-nlp/notes/cs11-737-w12-qa/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Multilingual NLP</category>
  <category>NLP</category>
  <category>Notes</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w12-qa/</guid>
  <pubDate>Sun, 23 Jan 2022 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Translation and Translation Data</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w08-contact/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/tiling.png" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>course banner</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/kfbMP3oKK-E" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Lesson Video
</figcaption>
</figure>
</div><div id="sup-slide-deck" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div>




<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Translation and {Translation} {Data}},
  date = {2022-01-24},
  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w08-contact/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Translation and Translation Data.”</span>
January 24, 2022. <a href="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w08-contact/">https://orenbochman.github.io/notes-nlp/notes/cs11-737-w08-contact/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Multilingual NLP</category>
  <category>NLP</category>
  <category>Notes</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w08-contact/</guid>
  <pubDate>Sun, 23 Jan 2022 22:00:00 GMT</pubDate>
</item>
</channel>
</rss>
