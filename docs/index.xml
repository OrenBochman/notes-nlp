<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>NLP Course Notes &amp; Research</title>
<link>https://orenbochman.github.io/notes-nlp/</link>
<atom:link href="https://orenbochman.github.io/notes-nlp/index.xml" rel="self" type="application/rss+xml"/>
<description>Course and Research notes</description>
<image>
<url>https://orenbochman.github.io/notes-nlp/images/nlp-brain-wordcloud.jpg</url>
<title>NLP Course Notes &amp; Research</title>
<link>https://orenbochman.github.io/notes-nlp/</link>
</image>
<generator>quarto-1.6.39</generator>
<lastBuildDate>Mon, 17 Feb 2025 22:00:00 GMT</lastBuildDate>
<item>
  <title>Domination Games</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/posts/2025-02-18-domination/</link>
  <description><![CDATA[ 





<section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p>In a previous post titled <a href="..\25-02-17-samurai-world\index.qmd">Samurai’s world</a>, I considered the complex states and a framing game of a Lewis signaling game for which the requisite linguistics aspects of politeness and formality should emerge out of language evolution</p>
<p>Today though I’d like to pause from signaling for a bit and consider the establishment of hierarchies in a multi-agent system. This is also an extension that seems of interest to Sugarscape and other agent based models where one might be interested in the emergence of social hierarchies.</p>
<p>For language generation we may assign the hierarchy arbitrarily. But to speed learning we may want both the language and the hierarchy to emerge together. This can let us consider how different social structures may result in different linguistic structures.</p>
</section>
<section id="establishing-a-hierarchy." class="level2">
<h2 class="anchored" data-anchor-id="establishing-a-hierarchy.">Establishing a hierarchy.</h2>
<p>So lets lay down some ground rules. In our society the agents are initially an Egalitarian society meaning they are without a hierarchy or language. They are heterogenous and they can also evolve and learn via RL.</p>
<p>The game takes a decentralized forms for agent interactions and we are generally interested in finding an form of incentive often embodied as rule and or a utility that can when maximized individually leads to better overall performance in the society. This can be in the carrying capacity, mean wealth, or social welfare or expected progeny.</p>
<p>As the simulation progresses conditions may change to favor different strategies. For example if the carrying capacity of the environment changes, the agents may need to change their strategy to adapt. Agents less fit may die out, while fitter agents may migrate to greener pastures.</p>
<p>Eventually though the agents will have to interact with each other strategically to maximize their utility. Agents that don’t learn to do so will face greater risks of being eliminated. Further on agents may need to cooperate, coordinate and compete as groups to survive. It is somewhere along this axis that learning language and establishing social hierarchies may be increasingly beneficial.</p>
<p>Finally although the focus here is growing hierarchies I think that we should agree that that we are interested in the interplay of social structures and language.</p>
</section>
<section id="dual-view-of-hierarchies" class="level2">
<h2 class="anchored" data-anchor-id="dual-view-of-hierarchies">Dual view of Hierarchies</h2>
<p>In terms of society the cost of a leader’s actions and decisions may have far reaching impact on the future of group. The benefits and costs may be in proportion to the group’s size. Given the costs of bad decisions larger groups have vested interests in seeing that the leaders are competent and that the group’s resources are used wisely.</p>
<p>The leader on the other hand may like to do as they please, avoid criticism and challenges to their authority. They may also wish to avoid the costs of bad decisions by foisting them onto others. To do this they would like to reduce the group they are accountable to.</p>
<section id="the-principal-agent-dilemma" class="level3">
<h3 class="anchored" data-anchor-id="the-principal-agent-dilemma">The principal-agent dilemma</h3>
<p>The ability of leaders to pick members of the hierarchy based on loyalty rather then merit subverts the society’s goals yet this is often the paths taken by leaders.</p>
<p>Looking at emergent hierarchies we would like to study how the group can best select a hierarchy and leaders that primarily serve the group’s interests. The leaders of the hierarchy may try to game the system to serve their own interests. This is called the principal-agent dilemma and is a common problem in economics and politics. And we should formalize this aspect of the the game as it appears there are no political systems that are immune to this problem.</p>
</section>
</section>
<section id="emergent-hierarchies" class="level2">
<h2 class="anchored" data-anchor-id="emergent-hierarchies">Emergent Hierarchies</h2>
<p>Dominance hierarchy is common in the nature. While most of the time it is established by physical characteristics, we have many examples where other mechanisms. The use of brute force is risky for the individual and in the long term for the group.</p>
<ul>
<li>Egalitarian</li>
<li>Age group - initially all have the same age</li>
<li>Dominance - some agents are stronger</li>
<li>Egalitarian meaning no hierarchy</li>
<li>Partiarchal - succession is by the oldest son progeny of the leader</li>
<li>Matriarchal - succession is by the oldest daughter progeny of the leader</li>
<li>Oligarchy - some agents are more fit in harvesting resources and may be able to increase this advantage via trade. This should lead to specialization and division of labor under suitable circumstances.</li>
<li>Oligrachy with welfare. In Japan the CEO’s pay may only be 20x that of the lowest paid employee. Another way to improve welfare is to require an agent to share wealth when they come into contact with agents which are worse off. These gifts though can be used to keep track of social status based on amount given and received!</li>
<li>Meritocracy - this is a form of hierarchy where agents are ranked by their ability to perform a task. This means that there are many possible hierarchies in a group. However specialization implies we will consider individuals in a group for their top roles for skills that are have unmet demand by more qualified agents.</li>
<li>Aristocracy - authority is based on birthright and passes by rules of succession. Land ownership and rents dervided from these are restricted to a few families. Power is derived from wealth and influence can be increased by wealth, marriage, and alliances.</li>
</ul>
<p>Another aspect of dominance hierarchies is that they can lead to chaos or disharmony whenever the leader’s ability falls into question. This can lead to a challenge to the leader’s authority.</p>
</section>
<section id="games-of-domination" class="level2">
<h2 class="anchored" data-anchor-id="games-of-domination">Games of domination</h2>
<p>Leaders will wish to deter challenges to their authority i.e.&nbsp;disagreements over their actions or decisions for the group. This can be done in many ways. Here are a few:</p>
<ul>
<li>they can do this by being requiring that challengers first qualify</li>
<li>i.e.&nbsp;win a tournament that places them as the current challenger</li>
<li>they may need to also bring some resource, i.e.&nbsp;a wager, to the table</li>
<li>the challenges require a quorum and may take place only at certain times like once a year.</li>
<li>the challenged may also decide the terms of the challenge like the weapons used and the stakes involved.</li>
</ul>
<p>They may go further yet by outlawing such challenges and punishing those who do so. This leads to <a href="https://en.wikipedia.org/wiki/Selectorate_theory">selectorate</a> theory where the leader may want to keep the selectorate small to reduce the risk of being challenged. I.e. the pool of people who can pose challenges are kept small and given high incentives to support the leader. This may be done by rewarding them with resources or status well beyond they could achieve on their own.</p>
<p>So in terms of this game:</p>
<p>if <img src="https://latex.codecogs.com/png.latex?X%20%3C%20Y%20%5Cwedge%20X%20%3E%20Z%5C%20%5Cforall(Z%20%3C%20Y)"> X then X may challenge Y. Y may however consider his and X’s interinsics and skill abd set the terms most favorable to him.</p>
</section>
<section id="winner-takes-all" class="level2">
<h2 class="anchored" data-anchor-id="winner-takes-all">winner takes all:</h2>
<ul>
<li>The most extreme format of domination, yet one which may also deter challenges to the leader</li>
<li>The winner takes the loser’s life, status, possession, mates, and can kill his progeny and may impose similar punishments to the loser’s group.+</li>
<li>Leaders will want to avoid having to play in this game and may</li>
</ul>
<p>In the next three sections we will consider social structure that are common both family and state levels and are setup to maintain a heirarchy.</p>
</section>
<section id="patriarchy" class="level2">
<h2 class="anchored" data-anchor-id="patriarchy">Patriarchy</h2>
<ul>
<li>the oldest son progeny of the leader takes the leader’s social role in the event of the leader’s death.</li>
<li>the leader may also decide the heir to the throne is someone else other than his oldest son.</li>
</ul>
</section>
<section id="matriarchy" class="level2">
<h2 class="anchored" data-anchor-id="matriarchy">Matriarchy</h2>
<ul>
<li>the oldest daughter progeny of the leader takes the leader’s social role in the event of the leader’s death.</li>
</ul>
</section>
<section id="succession" class="level2">
<h2 class="anchored" data-anchor-id="succession">Succession</h2>
<ul>
<li>the leaders oldest son or daughter may be the heir to the throne.</li>
<li>if the leader has no children, the succession is by the oldest direct descendant of the most recent leader.</li>
</ul>
<p>It seems that our agents might need a leaders. And the leaders need a leader too. This might be a prequisite for planning in which an agent assigns tasks to other agents. This wouldn’t work well if all agents shared the same role.</p>
<p>One way to go is by age of the agent. The older agents are the leaders. But what if there are no older agents in the group? Another aspect is skill and specialization. To be assigned certain roles agents need to demonstrate skills. This may involve rites of passage or other tests to initiate agents into a new role. E.g. masai hunting a lion solo with a spear</p>
<p>We need a game for establishing a hierarchy.</p>
<p>The game of domination can be used to create social hierarchy of agents. Domination can be risky and costly. But once established it can be used to assign tasks. This means that any tasked one is given may be reassigned to a lower status agent.</p>
<p>The rewards for such social tasks may be shared. - they may be shared by the group - they may be given to the leader to keep/share/distribute/assign - they may be given</p>
<ul>
<li>It requires time.</li>
<li>It requires witnesses.</li>
<li>Agents are at risk of losing face</li>
<li>Agents may also risk injury or death if the opponent is stronger.</li>
<li>An agent of lower or equal status may challange another agent to a duel.</li>
<li>The challange may be</li>
<li>The winner takes the loser’s status.</li>
<li>The loser is demoted to the status of the winner.</li>
<li>The winner may also take the loser’s resources.</li>
<li>The winner may also take the loser’s mate.</li>
<li>The winner may also take the loser’s life.</li>
<li>This has a cost in terms of resources and status.</li>
<li>Viewers may wager on the outcome of the duel.</li>
<li>The challenged decides the terms of the duel.
<ul>
<li>The duel may be to the death or to first blood.</li>
<li>The duel may be non-lethal e.g.&nbsp;a mating dance or call</li>
</ul></li>
<li>Note: that the ideal leader is not always the strongest or most effective killer. But perhaps the one who can best assign tasks and resources to make the group most effective.</li>
<li>duels to the death may be effective for deterring challenges to the leader, but they also risk the loss of one or two valuable member of the group each time they take place.</li>
<li>thus a challenge to the leader may need to qualify, say by dominating all the warriors younger then the leader.</li>
<li>so we can see there may be soft or hard mechanisms for establishing a hierarchy.</li>
<li>there could be multiple hierarchies in a group.
<ul>
<li>hunters may not want to participate in foraging tasks and so the foragers may establish their own hierarchy.</li>
<li>hunters may also decide rank by ability with a spear pow or throwing rocks.</li>
<li>elders may establish their own hierarchy and act as advisors to the leaders.</li>
</ul></li>
</ul>
<p>We probably want the hierarchy to be transitive.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Domination {Games}},
  date = {2025-02-18},
  url = {https://orenbochman.github.io/notes-nlp/posts/2025-02-18-domination/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Domination Games.”</span> February 18, 2025.
<a href="https://orenbochman.github.io/notes-nlp/posts/2025-02-18-domination/">https://orenbochman.github.io/notes-nlp/posts/2025-02-18-domination/</a>.
</div></div></section></div> ]]></description>
  <category>Game Theory</category>
  <category>Agent Based Models</category>
  <category>Social Structures</category>
  <category>Hierarchies</category>
  <guid>https://orenbochman.github.io/notes-nlp/posts/2025-02-18-domination/</guid>
  <pubDate>Mon, 17 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>A Samurai’s World</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/posts/2025-02-17-samurai-world/</link>
  <description><![CDATA[ 





<blockquote class="blockquote">
<p>“if proof theory is about the sacred, then model theory is about the profane” – <span class="citation" data-cites="van2012logic">(Dalen 2012, 3:1)</span></p>
</blockquote>
<section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p>In the previous post, “Further Desiderata for Emergent Language”, I discussed the need to adapt emerging languages to conform with established properties of natural languages. You can read more about motivations in that post.</p>
<p>Rather then considering all the ways this could be achieved in a single essay, I believe that this should be broken down and considered one aspect at a time. This can make both the linguistics and the development easier to understand.</p>
<p>Once enough examples are available it should become clearer on how to integrate the desiderata using suitable states or other mechanisms.</p>
</section>
<section id="the-task" class="level2">
<h2 class="anchored" data-anchor-id="the-task">The task</h2>
<p>Moving on I’d like to tackle the capacity of certain languages to encode politeness and honorifics. This is a feature that is present in many natural languages, and it it bering some awareness that may be absent from many ai systems where agents lack a sense of social structure.</p>
<p>In this post I’d like to consider the emergence of politeness in a multi-agent language game. We saw that politeness emerged as a way to avoid conflict and to maintain social harmony. In this post, we will explore the emergence of honor in a multi-agent language game. Honor is a concept that is closely related to politeness, but it has some distinct features that make it an interesting topic of study.</p>
</section>
<section id="over-thinking-politeness" class="level2 callout-info">
<h2 class="anchored" data-anchor-id="over-thinking-politeness">Over thinking politeness</h2>
<p>Is politeness is not some random flag in some language?</p>
<p>I think that language is a social construct and that politeness is a social construct. It is not just a flag that is set in a language, but rather a complex set of rules and norms that govern how people interact with each other.</p>
<p>The social and biological basis for politeness can be viewed as an expression of dominance hierarchies and submission. In this view, politeness is a way of signaling respect and deference to others, and it is a way of maintaining social harmony and avoiding conflict associated with reestablishing dominance hierarchies when there are disputes within a social group.</p>
<p>This suggests three phenomena that are relevant to the emergence of politeness in a multi-agent language game:</p>
<ol type="1">
<li>Face for a speaker has a dual. It is the ability and a requirement of an individual to express their dominance and status with respect to his subordinates in society as well as the acknowledgment of the , but more so it due to subordinates giving the speaker their due respect. Thus loss of face can happen in private but is more significant in public. Face can be lost when the an individual is shamed by actions as well as words.</li>
</ol>
<p>Any loss of face in public may be viewed as a challenge to the speaker’s dominance and status. Such a challange when viewed by some third party may be viewd by others at large as destablizing the social harmony of the established order embodied in the entire hierarchy.</p>
<p>could be seen as a challenge to the speaker’ 2. Informal speech may be used when there is no conflict or challenge to dominance hierarchies. 3. Politeness forms that are ingrained in a language are means to permanently establish dominance hierarchies. and to avoid conflict by signaling respect and deference to others.</p>
</section>
<section id="how-is-politeness-encoded-in-japanese" class="level2">
<h2 class="anchored" data-anchor-id="how-is-politeness-encoded-in-japanese">How is politeness encoded in Japanese?</h2>
<ul>
<li>Politeness persists into modern Japanese, and is a challange for learners of the language who wish to master it as they must become aware of social aspects in which they are participating.</li>
<li>Japanese has three forms of politeness and formality:
<ul>
<li>informal
<ul>
<li>the informal form is used with friends, family, and people of lower social status.</li>
</ul></li>
<li>Polite (desu/masu)
<ul>
<li>the polite form is used in formal situations, with strangers, and with people of higher social status.</li>
<li>Verbs end in -masu (affirmative) or -masen (negative); copula is desu.</li>
</ul></li>
<li>Honorific Language <strong>Keigo</strong>
<ul>
<li>the honorific form is used to show respect to the listener or the subject of the conversation.</li>
<li><strong>Sonkeigo</strong> (Respectful Language): Elevates the subject’s actions (e.g., o-hanashi ni naru for “to speak”).</li>
<li><strong>Kenjougo</strong> (Humble Language): Lowers the speaker’s actions to show deference (e.g., moushiageru for “to say”).</li>
<li><strong>Teineigo</strong> (Polite Language): The desu/masu form falls under this when speaking politely without changing perspective.</li>
</ul></li>
</ul></li>
</ul>
<section id="questions" class="level3">
<h3 class="anchored" data-anchor-id="questions">Questions</h3>
<ul>
<li>what is changing perspective?
<ul>
<li>In Japanese, the speaker must change their perspective when using honorific language. <mark>This means that the speaker must consider the listener’s perspective and use language that is appropriate for that perspective.</mark></li>
</ul></li>
<li>Are politeness and honorifics only encoded in the verb inflection or do they further manifest in the subject object as agreements or other forms ?</li>
</ul>
</section>
</section>
<section id="samurai-world---sub-states-that-imbue-an-emergent-language-with-politeness-and-formality" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="samurai-world---sub-states-that-imbue-an-emergent-language-with-politeness-and-formality">Samurai world - Sub-States that imbue an emergent language with politeness and formality</h2>
<p>This is a small state space which is used as a model<sup>1</sup> for politeness and formality in natural language based on Japanese which has a more sophisticated system of politeness and formality then most languages.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;in the sense of logic</p></div></div><p>In this game agents interact in a framing game. In the lewis sub-game they need to coordinate a system in which they observe politeness and formality to avoid being decapitated by their superiors, yet avoid losing face, i.e.&nbsp;without being too polite or formal to their inferiors.</p>
<ul>
<li>state:
<ul>
<li>speaker’s status
<ul>
<li>gender, age, social status, relationship to subject</li>
</ul></li>
<li>subject’s status</li>
</ul></li>
</ul>
<p>One could evolve the use of poltiness in a reconstruciton game or a discrimination game.</p>
<ul>
<li>In the reconstruction task, Sender gets an input item, it sends a message to Receiver, and Receiver must generate an output item identical to Sender’s input.</li>
<li>In the discrimination task, Sender gets an input item (the target); Receiver gets multiple input items (the same target and a number of distractors, in random order).</li>
</ul>
<p>Note that reconstruction is very much like the original lewis signaling game. While the discrimination easier task as the receiver could in theory fail to reconstruct, or have to choose at random from a very large, perhaps even infinite set of possible reconstructions and that having a just K-distractions, it is down to 1/k probability of success. More so if it is equipped with the ability of learning from errors he might score the distractions and make progress with much higher information levels then in the reconstruction game.</p>
</section>
<section id="code" class="level2">
<h2 class="anchored" data-anchor-id="code">Code</h2>
<p>Here is a data generation script that may be used with <span class="citation" data-cites="kharitonov:etal:2021">(Kharitonov et al. 2021)</span> EGG emergence game toolkit to model politeness and formality in a multi-agent language game.</p>
</section>
<section id="states-for-verbs-and-nouns" class="level2 callout-info">
<h2 class="anchored" data-anchor-id="states-for-verbs-and-nouns">states for verbs and nouns</h2>
<p>One issue is that there that I have not yet written up the states needed for differentiating between nouns and verbs. Not the states for creating inflected verbs.</p>
<p>Both of these are bigger tasks and I will need to write them up quickly.</p>
<p>Anyhow is this case we should assume that there are already</p>
<ul>
<li>nouns and verbs and possible other parts of speech.</li>
<li>verbs may be inflected.</li>
</ul>
<p>now we wish to split certain verb states by adding sub-states that correspond to a politeness and formality flag.</p>
</section>
<section id="code-1" class="level2">
<h2 class="anchored" data-anchor-id="code-1">Code</h2>
<p>Here is a data generation script that may be used with <span class="citation" data-cites="kharitonov:etal:2021">(Kharitonov et al. 2021)</span> EGG emergence game toolkit to model politeness and formality in a multi-agent language game.</p>
<div id="36149a32" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> random</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-3"></span>
<span id="cb1-4"></span>
<span id="cb1-5">state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>()</span>
<span id="cb1-6"></span>
<span id="cb1-7"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> add_politenes_state(state):</span>
<span id="cb1-8"></span>
<span id="cb1-9">    state[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"speaker_status"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> random.choice([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, ])</span>
<span id="cb1-10">    state[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"subject_status"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> random.choice([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, ])</span>
<span id="cb1-11">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> state</span>
<span id="cb1-12"></span>
<span id="cb1-13"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>):</span>
<span id="cb1-14">    state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>()</span>
<span id="cb1-15">    state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> add_politenes_state(state)</span>
<span id="cb1-16">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">,</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>state<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> should be polite </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>state[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'subject_status'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> state[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'speaker_status'</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> "</span>)</span>
<span id="cb1-17"></span>
<span id="cb1-18"> </span>
<span id="cb1-19"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">## </span><span class="al" style="color: #AD0000;
background-color: null;
font-style: inherit;">TODO</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">: estimate politeness and face in a states</span></span>
<span id="cb1-20"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">## </span><span class="al" style="color: #AD0000;
background-color: null;
font-style: inherit;">TODO</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">: estimate politeness and face in an utterance</span></span>
<span id="cb1-21"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">## </span><span class="al" style="color: #AD0000;
background-color: null;
font-style: inherit;">TODO</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">: the politeness loss is the formality of the utterance and the politeness of the state. I.e. if the state is more polite then the utterance there is a loss associated with that.</span></span>
<span id="cb1-22"></span>
<span id="cb1-23"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> politeness_loss_penalty(state):</span>
<span id="cb1-24">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> state[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"subject_status"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> state[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"speaker_status"</span>]:</span>
<span id="cb1-25">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb1-26">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb1-27">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0,{'speaker_status': 0, 'subject_status': 1} should be polite True 
1,{'speaker_status': 0, 'subject_status': 1} should be polite True 
2,{'speaker_status': 2, 'subject_status': 2} should be polite False 
3,{'speaker_status': 2, 'subject_status': 0} should be polite False 
4,{'speaker_status': 2, 'subject_status': 2} should be polite False 
5,{'speaker_status': 1, 'subject_status': 2} should be polite True 
6,{'speaker_status': 1, 'subject_status': 2} should be polite True 
7,{'speaker_status': 1, 'subject_status': 1} should be polite False 
8,{'speaker_status': 1, 'subject_status': 0} should be polite False 
9,{'speaker_status': 1, 'subject_status': 1} should be polite False 
10,{'speaker_status': 0, 'subject_status': 1} should be polite True 
11,{'speaker_status': 0, 'subject_status': 1} should be polite True 
12,{'speaker_status': 1, 'subject_status': 0} should be polite False 
13,{'speaker_status': 2, 'subject_status': 2} should be polite False 
14,{'speaker_status': 0, 'subject_status': 1} should be polite True 
15,{'speaker_status': 1, 'subject_status': 0} should be polite False 
16,{'speaker_status': 1, 'subject_status': 0} should be polite False 
17,{'speaker_status': 1, 'subject_status': 0} should be polite False 
18,{'speaker_status': 1, 'subject_status': 0} should be polite False 
19,{'speaker_status': 1, 'subject_status': 2} should be polite True </code></pre>
</div>
</div>
<p>ok now that we have the states we can consider</p>
<ol type="1">
<li>ensuring that the speaker’s status is encoded in the message
<ol start="2" type="1">
<li>we can do this indirectly as part of the reconstruction.</li>
<li>we can also do this as a penalty coming from the framing game.
<ul>
<li>Rude speakers are penalized!</li>
<li>Losing face is also penalized.</li>
</ul></li>
</ol></li>
<li>we can refine the model by adding flags for <code>informal_settings</code>, <code>friendship_settings</code> and <code>gender_speaker</code> <code>gender_subject</code> <code>age_speaker</code> <code>age_subject</code> to further refine the politeness and formality of the language.</li>
<li>we may also want to consider the setting where the subject social status is unknown to the speaker and thus the speaker must use a default politeness setting.</li>
</ol>
<ul>
<li>https://blog.duolingo.com/japanese-politeness-formal-language</li>
</ul>
</section>
<section id="establishing-a-hierarchy." class="level2">
<h2 class="anchored" data-anchor-id="establishing-a-hierarchy.">Establishing a hierarchy.</h2>
<p>The establishment of hierarchies, particularly in a multi-agent system, is a complex process that involves a variety of factors. This may well be a bigger topic than we need to consider here and may easily confuse the issues I wish to discuss here which is the minimal example for the emergence of politeness and honorifics in a multi-agent language game. I therefore moved further discussion to the next post titled <a href="../../posts/2025-02-18-domination/index.html">Establishing Hierarchies</a>.</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-van2012logic" class="csl-entry">
Dalen, Dirk van. 2012. <em>Logic and Structure</em>. Vol. 3. Universitext. Springer London. <a href="https://doi.org/DOI 10.1007/978-1-4471-4558-5">https://doi.org/DOI 10.1007/978-1-4471-4558-5</a>.
</div>
<div id="ref-kharitonov:etal:2021" class="csl-entry">
Kharitonov, Eugene, Roberto Dessì, Rahma Chaabouni, Diane Bouchacourt, and Marco Baroni. 2021. <span>“<span>EGG</span>: A Toolkit for Research on <span>E</span>mergence of Lan<span>G</span>uage in <span>G</span>ames.”</span> <a href="https://github.com/facebookresearch/EGG" class="uri">https://github.com/facebookresearch/EGG</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {A {Samurai’s} {World}},
  date = {2025-02-17},
  url = {https://orenbochman.github.io/notes-nlp/posts/2025-02-17-samurai-world/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“A Samurai’s World.”</span> February 17,
2025. <a href="https://orenbochman.github.io/notes-nlp/posts/2025-02-17-samurai-world/">https://orenbochman.github.io/notes-nlp/posts/2025-02-17-samurai-world/</a>.
</div></div></section></div> ]]></description>
  <category>Emergent Language</category>
  <category>Game Theory</category>
  <category>Social Structure</category>
  <guid>https://orenbochman.github.io/notes-nlp/posts/2025-02-17-samurai-world/</guid>
  <pubDate>Sun, 16 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Knights and Knaves world</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/posts/2025-02-18-logic-worlds/</link>
  <description><![CDATA[ 





<blockquote class="blockquote">
<p>“if proof theory is about the sacred, then model theory is about the profane” – <span class="citation" data-cites="van2012logic">(Dalen 2012, 3:1)</span></p>
</blockquote>
<section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p>In the previous post, “Further Desiderata for Emergent Language”, I discussed the need to adapt emerging languages to conform with established properties of natural languages. You can read more about motivations in that post.</p>
<p>Rather then considering all the ways this could be achieved in a single essay, I believe that this should be broken down and considered one aspect at a time. This can make both the linguistics and the development easier to understand.</p>
<p>Once enough examples are available it should become clearer on how to integrate the desiderata using suitable states or other mechanisms.</p>
</section>
<section id="the-task" class="level2">
<h2 class="anchored" data-anchor-id="the-task">The task</h2>
<p>Today I want to consider the ability of language to encode logical reasoning. Logic is such a big field it can encampass all of mathematics and philosophy. So for this post we need to narrow things down.</p>
<p>I’d like the agents not only to learn to speak in a langauge that captures logical reasoning, but also to be able to reason about states and statements made in that language.</p>
<p>I am targeting reconstruction and discrimination games as the inner game which is used to evolve the language. The frameing game might be drawn from</p>
<ol type="1">
<li>Knights and Knaves puzzles</li>
<li>Lewis Carroll’s <a href="https://en.wikipedia.org/wiki/The_Game_of_Logic">The Game of Logic</a> - for</li>
<li>Jon Barwise and John Etchemendy created <a href="https://en.wikipedia.org/wiki/Tarski%27s_World">Tarski’s World</a> - for first order logic</li>
</ol>
</section>
<section id="knights-and-knaves-worlds" class="level2">
<h2 class="anchored" data-anchor-id="knights-and-knaves-worlds">Knights and Knaves worlds</h2>
<p>In “What Is the Name of This Book” and his other works Raymond Smullyan use this as a framework to cover this puzzle to cover from propositional logic to the problem of undecidability…</p>
<p>The initial state is simple - each individual is either a knight or a knave. Knights always tell the truth, and knaves always lie.</p>
<p>In most puzzles we need to determine the type of each individual from a set of statements made by them.</p>
<p>Another type of variation is that we want to find out some fact about the world but must ask the right question to get the correct answer regardless of the individual’s type.</p>
<p>Another variant that seems salient in this context is when the people in the puzzle respond in their own language, which is unkown to us. In this case we need to deduce the meaning of the words from the context.</p>
<p>To spice his puzzles up Smullyan introduced individuals that, who can lie or tell the truth as he pleases these might be called “Normals”</p>
<p>In his Transilvanian puzzles he introduced the notion that half the population are insane and have false beliefs e.g.&nbsp;that <img src="https://latex.codecogs.com/png.latex?2+2%5Cne4"> and they are also devided into truthfull and lying types this time humans and vampires. In another book he introduced monkeys that look like humans. The only real difference was that monkeys have a tail and humans don’t. In terms of logic it just add another collumn to the truth table for each individual.</p>
<p>We have for n individuals we have 2^n possible states.</p>
<p>next comes the creative part of the task we want to automate. The statements the individuals make. While each individual can make a statement tht reveals their ground truth we the idea is to</p>
<ol type="1">
<li>ensure all the ground truths are revealed</li>
<li>use a minimal number of statements (i.e.&nbsp;by omitting a statement the problem should be rendered unsolvable)</li>
<li>ensure that the ground truths are unique</li>
</ol>
<p>In the website https://christopherphelps.trinket.io/sites/knight_knave_puzzler the generator can be used to generate a number of statements:</p>
<ol type="1">
<li>meta statements - is the puzzle solvable</li>
<li>name calling - calling some one a knight or knave a normal a monkey, insane etc.</li>
<li>Ascriptive statements - where an someone says what some type of individual would say about another speaker</li>
<li>Prime statements- statements on the prime number of knights or knaves in the group</li>
<li>independent statements - statements that don’t seem to be related to the puzzle</li>
</ol>
<p>One property of the puzzle is if no one makes a statement about an individual then his type is unconstrained and could be swapped without affecting the consistency of the puzzle.</p>
</section>
<section id="how-is-logic-encoded-in-the-knights-and-knaves-puzzles" class="level2">
<h2 class="anchored" data-anchor-id="how-is-logic-encoded-in-the-knights-and-knaves-puzzles">How is logic encoded in the knights and knaves Puzzles?</h2>
<p>So it is interesting to consider how one give a minimal general solution for such puzzles. In reality most puzzles do have short solutions but in general when we consider logic and language we can’t be certain that there is a neat solution or that the puzlle has a unique answer or that the puzzle is solvable.</p>
<p>So here is a general approach to solve these puzzles using boolean logic:</p>
<ol type="1">
<li>We encode all possible combinations of sub-states of the world using as inputs for a truth table. I.e. a column for each individual titled with their name and stating that they are a knight. We don’t need to encode the knave column as the two are mutually exclusive. If there are other sub-states like being a monkey, insane, a spy we would need to add a column for these too.</li>
<li>For each statement said we should rewrite it as a boolean expression in terms of these states.</li>
<li>We need to verify the outcome of the statement for each combination of</li>
</ol>
<p>Then we encode the statements made by the individuals as a column in the truth table.</p>
<p>let’s look at some examples with just knights and knaves</p>
<section id="name-calling" class="level3">
<h3 class="anchored" data-anchor-id="name-calling">Name calling</h3>
<ol type="1">
<li><label><input type="checkbox">A says “I am knave.”</label>
<ul>
<li>not possible for a knight (False)</li>
<li>not possible for a knave (True)</li>
<li>This will therefore not appear on it own.</li>
<li>This will not be part of a conjunction made by a knight. i.e.&nbsp;“and …”</li>
<li>It can be used conjunction with a truthy statement made by a knave. … and I am a knave</li>
</ul></li>
<li><label><input type="checkbox">A says “I am knight.”</label>
<ul>
<li>if A is a knight (True)</li>
<li>if A is a knave (False)</li>
</ul></li>
<li>A says “B is a knave.”
<ul>
<li>if A is a knight and B is knave (True)</li>
<li>if A is a knave and B is a knight (False) note: that both types will call the other type a knave. so this only tells us the speaker is the same type as another individual.</li>
</ul></li>
<li><label><input type="checkbox">A says “B is a knight.”</label>
<ul>
<li>if A is a knight and B is a knight (True)</li>
<li>if A is a knave and B is a knave (False)</li>
</ul></li>
<li><label><input type="checkbox">A says “I am the same type as B” same situation as #3</label></li>
<li><label><input type="checkbox">A says “B would say that I am a knave”</label></li>
<li><label><input type="checkbox">A says “B would say that I am a knight”</label></li>
<li>There are a prime number of knaves in the group.</li>
<li>There are a prime number of knights in the group.</li>
<li>“The puzzle is solvable” means there isn’t a contradiction in the statements made by the individuals.</li>
<li>“The puzzle is unsolvable” means there is a contradiction in the statements made by the individuals.</li>
</ol>
<table class="caption-top table">
<thead>
<tr class="header">
<th>A</th>
<th>B</th>
<th>A:#1</th>
<th>B:#1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="odd">
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="even">
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>



</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-van2012logic" class="csl-entry">
Dalen, Dirk van. 2012. <em>Logic and Structure</em>. Vol. 3. Universitext. Springer London. <a href="https://doi.org/DOI 10.1007/978-1-4471-4558-5">https://doi.org/DOI 10.1007/978-1-4471-4558-5</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Knights and {Knaves} World},
  date = {2025-02-17},
  url = {https://orenbochman.github.io/notes-nlp/posts/2025-02-18-logic-worlds/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Knights and Knaves World.”</span> February
17, 2025. <a href="https://orenbochman.github.io/notes-nlp/posts/2025-02-18-logic-worlds/">https://orenbochman.github.io/notes-nlp/posts/2025-02-18-logic-worlds/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/notes-nlp/posts/2025-02-18-logic-worlds/</guid>
  <pubDate>Sun, 16 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Further Desiderata for Emergent Language</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/posts/2025-02-16-more-desidirata/</link>
  <description><![CDATA[ 





<section id="three-applications-for-emergent-language" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="three-applications-for-emergent-language">Three applications for emergent language</h2>
<p>Some additional desiderata for emergent language come from:</p>
<ol type="1">
<li><p>Using the emergent language as <strong>surrogate for low resource languages</strong>. Though in a Bayesian setting we may consider them as priors. We may want to have multiple surrogates for a low resource language so that we can fit to the set of surrogates rather then to the lexemes of one specific surrogate! Fitting to a set of languages should teach the neural model about relations. A single surrogate would be less useful as it models would tend to overfit to features like lexemes which are made-up.</p></li>
<li><p>Using games where agents learn multiple languages. This allows to study the field of language contact. In this case speakers speak more than one language and the languages influence each other. To make things more concreate we may want to evolve languages that are similar to certain specification that come from the WALS database. <sup>1</sup></p></li>
<li><p>A third idea that may be of interest is to evolve languages that can be used as a <strong>universal interlingua</strong>. This are emergent language that are minimally ambiguous have capture most deep features. It would be created using a large state space and be given a long time for planning or to evolve and become more uniform, the verbs more regular and transformations more compositional. Ideally the verbs and nouns would only require one form to master all possible forms and their meanings. Perhaps the nouns could even be derived from the verbs using an automatic process, to reduce the learning load even further. Why multiple interlingua? Since we propose to use these for translations it may be possible to create smaller-interlinguas that are used for a restricted language hierarchy. A second reason is that again we may not want to overfit to a single interlingua but to some set of interlinguas captring diverse deep features and surface features. Idealy some interlinguas would be a better fit for certain languages then others and an attention mechanism could be used to relay on the interlinguas that are the best fit for the language being translated. Think of using different shortcuts to get from one place to another. (e.g.&nbsp;An interlingua with politeness and honorifics would be a better fit for Japanese then one without.)</p></li>
</ol>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;this is at last another good reason for multiple senders to be used and for them to send different languages.</p></div></div></section>
<section id="section" class="level2">
<h2 class="anchored" data-anchor-id="section">??</h2>
<p>So the main thrust is that we want to evolve language that are similar to a WALS specification. THis may require too much work though and we may prefer to consider certain aspects of language in the database that are both common, easy to implement and measure and ideally can become surrogates that are more like a certain language then any high resource language (e.g.&nbsp;Turkish).</p>
<p>While a number of <em>tricks</em> may be used to make the emergent language more like a low resource language, the more systematic route is to consider a suitable set of states for the lewis game. Spontaneous symmetry breaking may then serve us. In other cases we may be able to incorporate different aggregation rule</p>
<p>However to get started I think one needs to simply generate suitable states. The desedirate has a duality with the states. The number and structure of the state will shape the size and complexity of the emegent languages. Since these will likely be large languages we may also need to find algorithms that allow these to emerge quickly.</p>
<p>Another idea is to make to make three views of the state space. - an image based on a chart - a frame based view</p>
<ul>
<li>We may want to express propositions using a ‘Block world’</li>
<li>We may want to express possession using a ‘Possession world’</li>
<li>We may want to first breakdown the verb to indicate tense, aspect, mood, number, politeness, formality &amp; counterfactual.</li>
<li>For thematic roles we may start the figure from <span class="citation" data-cites="winston1992artificial">(Winston 1992, 211)</span> and the frame it shows.</li>
<li>Winston points out that the number of roles which can range from 6 to 24 is less important, so long as we can learn the constraints verbs place on the roles when forming a sentence. This seems to be even more important if we are going to learn these constraints from data using an attention mechanism.</li>
<li>Filamore explains that semantic roles are a tool to eliminate the polisemy inherent in verbs.</li>
</ul>
<ol type="1">
<li>Distributional Similarity - to a NL
<ul>
<li>lexeme should be distributed similarly to a corresponding word in a natural langauge.</li>
<li>this is desirable because when translating from high frequency words should map to high frequency words.</li>
<li>this seems a challenge but lets recall that we also wanted to have a power law distribution.</li>
<li>if lexemes are distributed following a power law. so fee words are high frequency and almost all are low frequency.</li>
<li>metrics for this could be cosine similarity, KL divergence, or some other measure of distributional similarity.</li>
<li>probability theorem has convergence in distribution and it may be of interest here, particularly as we may be interested in more then the lexemes distribution but of other probabilistically modeled aspects of the language.</li>
</ul></li>
<li>We may want to choose a fully emergent language with phonology, morphology, syntax, and semantics. This might be more complexity than we need though. We might want to work with a system that is simpler but can be used to make embeddings that are useful for approximating low resource languages. In such a case we may use concatentaed numeric codes for representing the lexems.</li>
<li>verb tense, aspect, mood, conterfactuals
<ul>
<li>tense is a time reference
<ul>
<li>past, present, future.</li>
</ul></li>
<li>aspect is the way the action is viewed e.g.&nbsp;
<ul>
<li>perfective means: the action is viewed as a whole (e.g.&nbsp;“I have eaten”)</li>
<li>imperfective means: the action is viewed as ongoing (e.g.&nbsp;“I am eating”)</li>
<li>progressive means: the action is viewed as ongoing (e.g.&nbsp;“I am eating right now, but I will call you when I am done”)</li>
<li>habitual means: the action is viewed as a habit (e.g.&nbsp;“I eat breakfast every day”)</li>
<li>perfect means: the action is viewed as completed (e.g.&nbsp;“I have eaten”)<br>
</li>
</ul></li>
<li>mood is the attitude of the speaker some examples are:
<ul>
<li>indicative means: the speaker is making a statement of fact (e.g.&nbsp;“I am happy”)</li>
<li>subjunctive means: the speaker is expressing a wish, a doubt, or a hypothetical (e.g.&nbsp;“I wish I were happy”)</li>
<li>imperative means: the speaker is giving a command (e.g.&nbsp;“Be happy!”)</li>
<li>conditional means: the speaker is expressing a condition (e.g.&nbsp;“If I were happy, I would be smiling”)</li>
<li>interrogative means: the speaker is asking a question (e.g.&nbsp;“Are you happy?”)</li>
<li>exclamatory means: the speaker is making an exclamation (e.g.&nbsp;“How happy I am!”)</li>
</ul></li>
<li>counterfactuals are statements that are contrary to fact. (e.g.&nbsp;“In the best of possible worlds, I would be smiling”)</li>
</ul></li>
<li>thematic roles
<ul>
<li>agent</li>
<li>patient</li>
<li>experiencer</li>
<li>theme</li>
<li>goal</li>
<li>source</li>
<li>instrument</li>
<li>location</li>
<li>benefactor</li>
</ul></li>
<li>possession world
<ul>
<li>possessor</li>
<li>possessed States:</li>
</ul></li>
<li>We may want to have verbs and nouns and other parts of speech.
<ul>
<li>states could be frames for verbs with slots for subjects, objects. this can</li>
</ul></li>
</ol>
<p>Propositions could be materialized using a block world.</p>
<p>Possession could be materialized using a possession world.</p>
<ul>
<li>block world <span class="citation" data-cites="winston1992artificial">(Winston 1992, 47–60)</span> , c.f. Mover and SHRDLU by Terry Winograd</li>
</ul>
</section>
<section id="block-world" class="level2">
<h2 class="anchored" data-anchor-id="block-world">Block world</h2>
</section>
<section id="zookeeper" class="level2">
<h2 class="anchored" data-anchor-id="zookeeper">Zookeeper</h2>
<ul>
<li>in Zookeeper <span class="citation" data-cites="winston1992artificial">(Winston 1992, 121–25)</span> we need to classify animals using deduction.</li>
<li>https://www.kaggle.com/uciml/zoo-animal-classification/data</li>
<li>we can give each of 101 animal 18 properties in a table via a zoo.csv. We can then sample from the table to create sets of animals with different properties.</li>
<li>we might then play games based on this data:
<ul>
<li>we may need to identify the animal based on it properties.</li>
<li>each property may come from a different sender.</li>
<li>an early response may be to attack, evade or ignore based on a partial identification.</li>
<li>this requires some kind of reasoning about the properties of the animals in the ‘zoo’</li>
</ul></li>
</ul>
<p>This means states for animals and their properties.</p>
<p>Minsky’s K-lines - physical world 214 - mental world 215 - ownership world 216 - <span class="citation" data-cites="kanade1980theory">Kanade (1980)</span> <a href="https://www.ri.cmu.edu/pub_files/pub3/kanade_takeo_1980_1/kanade_takeo_1980_1.pdf">A Theory of Origami World</a></p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-kanade1980theory" class="csl-entry">
Kanade, Takeo. 1980. <span>“A Theory of Origami World.”</span> <em>Artificial Intelligence</em> 13 (3): 279–311.
</div>
<div id="ref-winston1992artificial" class="csl-entry">
Winston, Patrick Henry. 1992. <em>Artificial Intelligence</em>. 3rd ed. Reading, MA: Addison-Wesley.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Further {Desiderata} for {Emergent} {Language}},
  date = {2025-02-17},
  url = {https://orenbochman.github.io/notes-nlp/posts/2025-02-16-more-desidirata/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Further Desiderata for Emergent
Language.”</span> February 17, 2025. <a href="https://orenbochman.github.io/notes-nlp/posts/2025-02-16-more-desidirata/">https://orenbochman.github.io/notes-nlp/posts/2025-02-16-more-desidirata/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/notes-nlp/posts/2025-02-16-more-desidirata/</guid>
  <pubDate>Sun, 16 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>A Convolutional Attention Network for Extreme Summarization of Source Code</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/2016-code-summerization/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/literature-review-open-book.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Litrature review"><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="img-fluid figure-img" alt="Litrature review"></a></p>
<figcaption>Litrature review</figcaption>
</figure>
</div></div><p><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2016-code-summerization/img/fig01.png" id="fig-01" class="img-fluid"> <img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2016-code-summerization/img/fig02.png" id="fig-02" class="img-fluid"> <img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2016-code-summerization/img/fig03.png" id="fig-03" class="img-fluid"></p>
<p>This is a paper mentioned in the course on multilingual NLP by Graham Neubig. With an interesting idea of an second attention head being used to copy stuff from the input directly to the output.</p>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>Attention mechanisms in neural networks have proved useful for problems in which the input and output do not have fixed dimension. Often there exist features that are locally translation invariant and would be valuable for directing the model’s attention, but previous attentional architectures are not constructed to learn such features specifically. We introduce an attentional neural network that employs convolution on the input tokens to detect local time-invariant and long-range topical attention features in a context-dependent way. We apply this architecture to the problem of extreme summarization of source code snippets into short, descriptive function name-like summaries. Using those features, the model sequentially generates a summary by marginalizing over two attention mechanisms: one that predicts the next summary token based on the attention weights of the input tokens and another that is able to copy a code token as-is directly into the summary. We demonstrate our convolutional attention neural network’s performance on 10 popular Java projects showing that it achieves better performance compared to previous attentional mechanisms. – <span class="citation" data-cites="allamanis2016convolutionalattentionnetworkextreme">(Allamanis, Peng, and Sutton 2016)</span></p>
</blockquote>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-allamanis2016convolutionalattentionnetworkextreme" class="csl-entry">
Allamanis, Miltiadis, Hao Peng, and Charles Sutton. 2016. <span>“A Convolutional Attention Network for Extreme Summarization of Source Code.”</span> <a href="https://arxiv.org/abs/1602.03001">https://arxiv.org/abs/1602.03001</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {A {Convolutional} {Attention} {Network} for {Extreme}
    {Summarization} of {Source} {Code}},
  date = {2025-02-13},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/2016-code-summerization/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“A Convolutional Attention Network for
Extreme Summarization of Source Code.”</span> February 13, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/2016-code-summerization/">https://orenbochman.github.io/notes-nlp/reviews/paper/2016-code-summerization/</a>.
</div></div></section></div> ]]></description>
  <category>NLP</category>
  <category>Paper</category>
  <category>Attention</category>
  <category>Deep learning</category>
  <category>Review</category>
  <category>Stub</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/2016-code-summerization/</guid>
  <pubDate>Wed, 12 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Floating Constraints in Lexical Choice</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/1997-floating-contraints-in-lexical-choice/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/literature-review-open-book.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Literature review"><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="img-fluid figure-img" alt="Literature review"></a></p>
<figcaption>Literature review</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/DffGdrfY9gI" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid quarto-uncaptioned" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1
</figcaption>
</figure>
</div></div>
<section id="context" class="level2">
<h2 class="anchored" data-anchor-id="context">Context</h2>
<p>This is one of the three paper mentioned by Kathleen McKeown in her heroes of NLP interview by Andrew NG. The paper is about the floating constraints in lexical choice.</p>
<p>Looking quickly over it I noticed a couple of things:</p>
<!--
@article{mckeown1997floating,
  title={Floating constraints in lexical choice},
  author={McKeown, Kathleen and Elhadad, Michael and Robin, Jacques},
  year={1997}
}
-->
<ol type="1">
<li>I was familiar with Elhadad’s work on generation. I had read his work on FUF Functional Unification Formalism in the 90s which introduced me to the paradigm of unification before I even knew about Prolog or logic programming.</li>
<li>I found it fascinating that McKeown and Elhadad had worked together on this paper and even a bit curious about what they were looking into here.</li>
<li>Since McKeown brought it up, it might intimate that there is something worth looking into here. And it is discusing aspects of generative language models.</li>
</ol>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>Lexical choice is a computationally complex task, requiring a generation system to consider a potentially large number of mappings between concepts and words. Constraints that aid in determining which word is best come from a wide variety of sources, including syntax, semantics, pragmatics, the lexicon, and the underlying domain. Furthermore, in some situations, different constraints come into play early on, while in others, they apply much later. This makes it difficult to determine a systematic ordering in which to apply constraints. In this paper, we present a general approach to lexical choice that can handle multiple, interacting constraints. We focus on the problem of floating constraints, semantic or pragmatic constraints that float, appearing at a variety of different syntactic ranks, often merged with other semantic constraints. This means that multiple content units can be realized by a single surface element, and conversely, that a single content unit can be realized by a variety of surface elements. Our approach uses the Functional Unification Formalism (FuF) to represent a generation lexicon, allowing for declarative and compositional representation of individual constraints. – <span class="citation" data-cites="mckeown1997floating">(McKeown, Elhadad, and Robin 1997)</span></p>
</blockquote>
</section>
<section id="outline" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li>Introduction
<ul>
<li>Mentions the complexity of lexical choice in language generation.</li>
<li>Notes the diverse sources of constraints on lexical choice, including syntax, semantics, lexicon, domain, and pragmatics.</li>
<li>Highlights the challenges posed by floating constraints, which are semantic or pragmatic constraints that can appear at various syntactic ranks and be merged with other semantic constraints.</li>
<li>Presents examples of floating constraints in sentences.</li>
</ul></li>
<li>An Architecture for Lexical Choice
<ul>
<li>Discusses the role of lexical choice within a typical language generation architecture.</li>
<li>Argues for placing the lexical choice module between the content planner and the surface sentence generator.</li>
<li>Describes the input and output of the lexical choice module, emphasizing the need for language-independent conceptual input.</li>
<li>Presents the two tasks of lexical choice: syntagmatic<sup>1</sup> decisions (determining thematic structure) and paradigmatic<sup>2</sup> decisions (choosing specific words).</li>
<li>Introduces the FUF/SURGE package as the implementation environment.</li>
</ul></li>
<li>Phrase Planning in Lexical Choice
<ul>
<li>Describes the need for phrase planning to articulate multiple semantic relations into a coherent linguistic structure.</li>
<li>Explains how the Lexical Chooser selects the head relation and builds its argument structure based on focus and perspective.</li>
<li>Details how remaining relations are attached as modifiers, considering options like embedding, subordination, and syntactic forms of modifiers.</li>
</ul></li>
<li>Cross-ranking and Merged Realizations
<ul>
<li>Discusses the non-isomorphism between semantic and syntactic structures, necessitating merging and cross-ranking realizations.</li>
<li>Provides examples of merging (multiple semantic elements realized by a single word) and cross-ranking (single semantic element realized at different syntactic ranks).</li>
<li>Emphasizes the need for linguistically neutral input to support this flexibility.</li>
<li>Explains the implementation of merging and cross-ranking using FUF, highlighting the challenges of search and the use of bk-class for efficient backtracking.</li>
</ul></li>
<li>Interlexical Constraints
<ul>
<li>Defines interlexical constraints as arising from alternative sets of collocations for realizing pairs of content units.</li>
<li>Presents an example of interlexical constraints with verbs and nouns in the basketball domain.</li>
<li>Discusses different strategies for encoding interlexical constraints in the Lexical Chooser.</li>
<li>Introduces the :wait mechanism in FUF to delay the choice of one collocate until the other is chosen, preserving modularity and avoiding backtracking.</li>
</ul></li>
<li>Other Approaches to Lexical Choice
<ul>
<li>Reviews other systems using FUF for lexical choice, comparing their architectures and features to ADVISOR-II.</li>
<li>Discusses non-FUF-based systems, categorizing them by their positioning of lexical choice in the generation process and analyzing their handling of various constraints.</li>
</ul></li>
<li>Conclusion
<ul>
<li>Summarizes the contributions of the paper, highlighting the ability to handle a wide range of constraints, the use of FUF for declarative representation and interaction, and the algorithm for lexical selection and syntactic structure building.</li>
<li>Emphasizes the importance of syntagmatic choice and perspective selection for handling floating constraints.</li>
<li>Notes the use of lazy evaluation and dependency-directed backtracking for computational efficiency.</li>
<li>Mentions future directions, such as developing domain-independent lexicon modules and methods for organizing large-scale lexica.</li>
</ul></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;of or denoting the relationship between two or more linguistic units used sequentially to make well-formed structures. The combination of ‘that handsome man + ate + some chicken’ forms a syntagmatic relationship. If the word position is changed, it also changes the meaning of the sentence, eg ’Some chicken ate the handsome man</p></div><div id="fn2"><p><sup>2</sup>&nbsp;Paradigmatic relation describes the relationship between words that can be substituted for words with the same word class (eg replacing a noun with another noun)</p></div></div></section>
<section id="reflection" class="level2">
<h2 class="anchored" data-anchor-id="reflection">Reflection</h2>
<ul>
<li>In unification-based approaches, any word is a candidate unless it is ruled out by a constraint. With an LLM a word is selected based on its probability.</li>
<li>The idea of constraints is deeply embedded in Linguistics. The two type that immediately come to mind are:
<ul>
<li>Subcategorization constraints which are rules that govern the selection of complements and adjuncts. These are syntactic constraints.</li>
<li>Selectional constraints which are rules that govern the selection of arguments. These are semantic constraints</li>
</ul></li>
<li>Other constraints that I did not have to deal with as often are pragmatics and these are resolved from the state of the world or common sense knowledge.</li>
<li>In FUF there are many more constraints and if care isn’t taken, the system can become over constrained and no words will be selected.</li>
<li>One Neat aspect of the approach though is that FUF language could be merged with a query to generate a text.</li>
<li>Unfortunately as fat as I recall the FUF had to be specified by hand. The only people who could do this were the people who designed the system. They needed to be expert at linguistics and logic programming. Even I was pretty sharp at the time the linguistics being referred was far too challenging. I learned that in reality every computational linguist worth their salt had their own theory of language and their own version of grammar and that FUF was just one of many.</li>
<li>It seems that today attention mechanisms within LSTMs or Transformers are capable of learning a pragmatic formalism without a linguist stepping in to specify it.</li>
</ul>
<p>Why this is still interesting:</p>
<p>Imagine we want to build a low resource capable language model.</p>
<p>We would really like it to be good with grammar Also we would like it to have a lexicon that is as rich as the person it is interacting with. And we would also like to make available to it the common sense knowledge that it would need to use in the context of current and possibly a profile based on past conversations…. On the other hand we don’t want to require a 70 billion parameter model to do this. So what do we do?</p>
<p>Let imagine we start by training on Wikipedia, after all wikipedia’s mission is to become the sum of all human knowledge. By do we realy need all of the information in Wikipedia in out edge model?</p>
<p>The answer is no but the real question is how can we partition the training information and the wights etc so that we have a powerful model with a basic grammar and lexicon but which can load a knowledge graph, extra lexicons and common sense knowledge as needed.</p>
<p>So I had this idea from lookin at a <span class="citation" data-cites="ouhalla1999introducing">Ouhalla (1999)</span>. It pointed out there were phrase boundries and that if the sentence was transformed in a number of ways t would still make sense if we did this with respect to a phrase but if we used cucnks that were too small or too big the transofrmation would create malformed sentences. The operations of intrerest were movement and deletion.</p>
<p>and that can still have a model that is both small but able to access this extra material.</p>
<p>One direction seems to me is that we want to apply queries to all the training material as we train the model. Based on the queries that we retrieve each clause or phrase we can decide where we want to learn it and if we want to learn it at all.</p>
<p>(If it is common sense knowledge we may already know it. If it is semi structured we may want to put it into wikidata. If it is neither we may want to avoid learning it at all. We may still want to use it for improving the grammar and lexicon.) However we may want to store it in a speclised lexicon for domains like medicinee or law.</p>
<p>We could also decide if and how to eliminate it from we want to The queries should match each bit of information in the sentence. These are our queries. While the facts may change form</p>
<ol type="1">
<li>say we want to decompose a wikipedia article</li>
</ol>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-mckeown1997floating" class="csl-entry">
McKeown, Kathleen, Michael Elhadad, and Jacques Robin. 1997. <span>“Floating Constraints in Lexical Choice.”</span>
</div>
<div id="ref-ouhalla1999introducing" class="csl-entry">
Ouhalla, J. 1999. <em>Introducing Transformational Grammar: From Principles and Parameters to Minimalism</em>. A Hodder Arnold Publication. Arnold. <a href="https://www.google.com/books?id=ZP-ZQ0lKI9QC">https://www.google.com/books?id=ZP-ZQ0lKI9QC</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Floating {Constraints} in {Lexical} {Choice}},
  date = {2025-02-12},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/1997-floating-contraints-in-lexical-choice/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Floating Constraints in Lexical
Choice.”</span> February 12, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/1997-floating-contraints-in-lexical-choice/">https://orenbochman.github.io/notes-nlp/reviews/paper/1997-floating-contraints-in-lexical-choice/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/1997-floating-contraints-in-lexical-choice/</guid>
  <pubDate>Wed, 12 Feb 2025 16:19:59 GMT</pubDate>
</item>
<item>
  <title>Neural Morphological Analysis: Encoding-Decoding Canonical Segments</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/2016-neural-morphological-segmentation/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/literature-review-open-book.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Literature review"><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="img-fluid figure-img" alt="Literature review"></a></p>
<figcaption>Literature review</figcaption>
</figure>
</div></div><div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR - Neural Morphological Analysis
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/in_a_nutshell.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Neural Morphological Analysis in a Nutshell"><img src="https://orenbochman.github.io/notes-nlp/images/in_a_nutshell.jpg" class="img-fluid figure-img" alt="Neural Morphological Analysis in a Nutshell"></a></p>
<figcaption>Neural Morphological Analysis in a Nutshell</figcaption>
</figure>
</div>
</div>
</div>
<section id="podcast" class="level2">
<h2 class="anchored" data-anchor-id="podcast">Podcast</h2>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>Canonical morphological segmentation aims to divide words into a sequence of standardized segments. In this work, we propose a character-based neural encoder-decoder model for this task. Additionally, we extend our model to include morpheme-level and lexical information through a neural reranker. We set the new state of the art for the task improving previous results by up to 21% accuracy. Our experiments cover three languages: English, German and Indonesian. – <span class="citation" data-cites="kann-etal-2016-neural">(Kann, Cotterell, and Schütze 2016)</span></p>
</blockquote>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li><strong>Introduction</strong>
<ul>
<li>Discusses morphological segmentation and its applications in NLP.</li>
<li>Explains the difference between surface segmentation and canonical segmentation, providing an example.</li>
<li>Highlights the advantages of canonical segmentation and the algorithmic challenges it introduces.</li>
<li>Presents a neural encoder-decoder model for canonical segmentation and a neural reranker to incorporate linguistic structure.</li>
</ul></li>
<li><strong>Neural Canonical Segmentation</strong>
<ul>
<li>Formally describes the canonical segmentation task, mapping a word to a canonical segmentation.</li>
<li>Explains the probabilistic approach to learn a distribution p(c | w).</li>
<li>Details the two parts of the model: an encoder-decoder RNN and a neural reranker.</li>
<li>Describes the neural encoder-decoder model based on Bahdanau et al.&nbsp;(2014), using a bidirectional gated RNN (GRU) as the encoder.</li>
<li>Explains how the decoder defines a conditional probability distribution over possible segmentations.</li>
<li>Explains the attention mechanism and how attention weights are computed.</li>
<li>Explains the neural reranker’s role in rescoring candidate segmentations from a sample set generated by the encoder-decoder.</li>
<li>Describes the reranking model’s ability to embed morphemes and incorporate character-level information.</li>
</ul></li>
<li><strong>Related Work</strong>
<ul>
<li>Discusses various approaches to morphological segmentation.</li>
<li>Mentions unsupervised methods like LINGUISTICA and MORFESSOR.</li>
<li>Describes supervised approaches using conditional random fields (CRFs).</li>
<li>Distinguishes the approach from surface morphological segmentation methods using a window LSTM.</li>
<li>Relates the approach to other applications of recurrent neural network transduction models.</li>
</ul></li>
<li><strong>Experiments</strong>
<ul>
<li>Describes the dataset used for comparison to earlier work.</li>
<li>Specifies the three languages used in the experiments: English, German, and Indonesian.</li>
<li>Notes the potential cause of the high error rate for German due to its orthographic changes.</li>
<li>Explains the data extraction process from CELEX, DerivBase, and MORPHIND analyzer for English, German, and Indonesian, respectively.</li>
<li>Details the training setup, including the use of an ensemble of five encoder-decoder models.</li>
<li>Describes the training of the reranking model, including sample set gathering and optimization.</li>
<li>Describes the baseline models used for comparison: JOINT model and a weighted finite-state transducer (WFST).</li>
<li>Outlines the evaluation metrics used: error rate, edit distance, and morpheme F1.</li>
</ul></li>
<li><strong>Results</strong>
<ul>
<li>Presents the results of the canonical segmentation experiment, showing improvements over baselines with both the encoder-decoder and reranker.</li>
<li>Discusses the additional improvements achieved by the reranker due to access to morpheme embeddings and existing words.</li>
<li>Analyzes cases where the right answer is not in the samples and errors due to annotation problems.</li>
<li>Discusses cases where the encoder-decoder finds the right solution but assigns a higher probability to an incorrect analysis.</li>
<li>Explains how the reranker corrects some errors based on lexical information and morpheme embeddings.</li>
<li>Investigates whether segments unseen in the training set are a source of errors.</li>
</ul></li>
<li><strong>Conclusion and Future Work</strong>
<ul>
<li>Summarizes the developed model consisting of an encoder-decoder and neural reranker for canonical morphological segmentation.</li>
<li>States the model’s improvement over baseline models.</li>
<li>Discusses the potential for further performance increase by improving the reranker.</li>
</ul></li>
</ul>
<!-- TODO: add outline, reflection, and terminology and for this paper-->
</section>
<section id="reflections" class="level2">
<h2 class="anchored" data-anchor-id="reflections">Reflections</h2>
<ol type="1">
<li>why has this not caught on.</li>
<li>why are people using byte pair encoding and morphological segmentation.</li>
<li>what resources are needed to train this kind of model on a new language?
<ul>
<li>A dataset of words with canonical segmentation</li>
<li>Access to a lexicon or a large corpus to determine if a canonical segment occurs as an independent word in the language. What is it is a bound morpheme that never appears alone or part of a root-template morphological system? How should we verify that we are deleing with a morpheme and not a surface phonemic fragment.</li>
</ul></li>
<li>Can we do this without a canonical segmentation dataset. More specifically can we induct morphology by processing surface forms of words and induct the canonical morphological forms using one of three loss function that
<ul>
<li>affix loss (prefix,stem, suffix)</li>
<li>template loss (root,template) loss</li>
<li>agglunative loss (stem suffix sequence)</li>
</ul></li>
</ol>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-kann-etal-2016-neural" class="csl-entry">
Kann, Katharina, Ryan Cotterell, and Hinrich Schütze. 2016. <span>“Neural Morphological Analysis: Encoding-Decoding Canonical Segments.”</span> In <em>Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</em>, edited by Jian Su, Kevin Duh, and Xavier Carreras, 961–67. Austin, Texas: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/D16-1097">https://doi.org/10.18653/v1/D16-1097</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Neural {Morphological} {Analysis:} {Encoding-Decoding}
    {Canonical} {Segments}},
  date = {2025-02-12},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/2016-neural-morphological-segmentation/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Neural Morphological Analysis:
Encoding-Decoding Canonical Segments.”</span> February 12, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/2016-neural-morphological-segmentation/">https://orenbochman.github.io/notes-nlp/reviews/paper/2016-neural-morphological-segmentation/</a>.
</div></div></section></div> ]]></description>
  <category>Paper</category>
  <category>Review</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/2016-neural-morphological-segmentation/</guid>
  <pubDate>Tue, 11 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Coverage Embedding Models for Neural Machine Translation</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/2016-coverage-embedding-models-for-NMT/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/literature-review-open-book.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Literature review"><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="img-fluid figure-img" alt="Literature review"></a></p>
<figcaption>Literature review</figcaption>
</figure>
</div></div><section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<ol class="example" type="1">
<li>is discussed in week 6 the <a href="../../../notes/cs11-737-w06/index.qmd">Multilingual NLP course</a>. Neural generative models tend to drop or repeat content. But for NMT we can assume that all the inputs should be represented in the output. For each uncovered word it imposes a penalty on the attention model.</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR - Coverage Embedding
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/in_a_nutshell.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Coverage Embedding in a Nutshell"><img src="https://orenbochman.github.io/notes-nlp/images/in_a_nutshell.jpg" class="img-fluid figure-img" alt="Coverage Embedding in a Nutshell"></a></p>
<figcaption>Coverage Embedding in a Nutshell</figcaption>
</figure>
</div>
<ul>
<li>The paper introduces coverage embedding models to address the issues of repeating and dropping translations in NMT.</li>
<li>The coverage embedding vectors are updated at each time step to track the coverage status of source words.</li>
<li>The coverage embedding models significantly improve translation quality over a large vocabulary NMT system.</li>
<li>The best model uses a combination of updating with a GRU and updating as a subtraction.</li>
<li>The coverage embedding models also reduce the number of repeated phrases in the output.</li>
</ul>
</div>
</div>
</section>
<section id="podcast" class="level2">
<h2 class="anchored" data-anchor-id="podcast">Podcast</h2>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>In this paper, we enhance the attention-based neural machine translation (NMT) by adding explicit coverage embedding models to alleviate issues of repeating and dropping translations in NMT. For each source word, our model starts with a full coverage embedding vector to track the coverage status, and then keeps updating it with neural networks as the translation goes. Experiments on the large-scale Chinese-to-English task show that our enhanced model improves the translation quality significantly on various test sets over the strong large vocabulary NMT system. – <span class="citation" data-cites="mi-etal-2016-coverage">(Mi et al. 2016)</span></p>
</blockquote>
</section>
<section id="glossary" class="level2">
<h2 class="anchored" data-anchor-id="glossary">Glossary</h2>
<p>This paper has a number of big words and concepts that are important to understand. Lets break them down together:</p>
<dl>
<dt>Neural Machine Translation (NMT)</dt>
<dd>
A machine translation approach that uses neural networks to learn the mapping between source and target languages.
</dd>
<dt>Attention Mechanism</dt>
<dd>
In NMT, a mechanism that allows the model to focus on different parts of the source sentence when generating each word in the target sentence.
</dd>
<dt>Coverage Vector</dt>
<dd>
A vector used in statistical machine translation to explicitly track which source words have been translated.
</dd>
<dt>Coverage Embedding Vector</dt>
<dd>
A vector specific to each source word in this model, used to track the translation status of the word. It is initialized with a full embedding and is updated based on attention scores.
</dd>
<dt>Gated Recurrent Unit (GRU)</dt>
<dd>
A type of RNN cell used to model sequential data, including language. Here it is used to update coverage embeddings.
</dd>
<dt>Attention Probability (α)</dt>
<dd>
A set of weights that indicate how much attention the model pays to each source word when predicting a target word.
</dd>
<dt>Encoder-Decoder Network</dt>
<dd>
A neural network architecture commonly used in sequence-to-sequence tasks like NMT. The encoder processes the input sequence, and the decoder generates the output sequence.
</dd>
<dt>Bi-directional RNN</dt>
<dd>
A RNN that processes a sequence in both forward and backward directions, capturing contextual information from both sides of a word.
</dd>
<dt>Soft Probability</dt>
<dd>
Probabilities in the attention mechanism aren’t hard (0 or 1), but instead are on a continuum, indicating a degree of attention or importance.
</dd>
<dt>Fertility</dt>
<dd>
In the context of translation, fertility refers to the number of words in the target language that can be translated from a single word in the source language.
</dd>
<dt>One-to-many Translation</dt>
<dd>
A translation where one source word corresponds to multiple words in the target language.
</dd>
<dt>TER (Translation Error Rate)</dt>
<dd>
A metric used to evaluate the quality of machine translation by calculating the number of edits required to match the system’s translation to a reference translation, with lower scores being better.
</dd>
<dt>BLEU (Bilingual Evaluation Understudy)</dt>
<dd>
A metric to evaluate the quality of machine translation by comparing a candidate translation to one or more reference translations, with higher scores being better.
</dd>
<dt>UNK</dt>
<dd>
Abbreviation for “unknown.” In machine translation, it is used to denote words that are not in the model’s vocabulary.
</dd>
<dt>AdaDelta</dt>
<dd>
An adaptive learning rate optimization algorithm, that adjusts the learning rate during training for faster convergence.
</dd>
<dt>Alignment</dt>
<dd>
In the context of machine translation, the process of determining which words in the source sentence correspond to words in the target sentence.
</dd>
<dt>F1 Score</dt>
<dd>
A measure of a test’s accuracy and it considers both the precision and recall of the test to compute the score.
</dd>
</dl>
<p>With a solid understanding of this terminology we can now dive into the paper.</p>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ol type="1">
<li>Introduction
<ul>
<li>Notes that in NMT attention mechanisms focus on source words to predict target words.</li>
<li>Point out that <mark>these models lack history or coverage information, leading to repetition or dropping of words.</mark></li>
<li>Recalls how Statistical Machine Translation (SMT) used a binary “coverage vector” to track translated words.</li>
<li>Explains that SMT coverage vectors use 0s and 1s, whereas attention probabilities are soft. SMT systems handle one-to-many fertilities using phrases or hiero rules, while NMT systems predict one word at a time.</li>
<li>Introduces <strong>coverage embedding vectors</strong>, updated at each step, to address these issues.</li>
<li>Explains that <mark>each source word has its own coverage embedding vector that starts as a full embedding vector</mark>(as opposed to 0 in SMT).</li>
<li>States that coverage embedding vectors are updated based on attention weights, moving toward an empty vector for words that have been translated.</li>
</ul></li>
<li>Neural Machine Translation
<ul>
<li>Recalls that attention-based NMT uses an encoder-decoder architecture.
<ul>
<li>The encoder uses a bi-directional RNN to encode the source sentence into hidden states.</li>
<li><mark>The decoder predicts the target translation by maximizing the conditional log-probability of the correct translation.</mark></li>
<li>The probability of each target word is determined by the previous word and the hidden state.</li>
<li>The hidden state is computed using a weighted sum of the encoder states, with weights derived from a two-layer neural network.</li>
</ul></li>
<li>Introduces coverage embedding models into the NMT by adding an input to the attention model.</li>
</ul></li>
<li>Coverage Embedding Models
<ul>
<li>The model uses a coverage embedding for each source word that is updated at each time step.</li>
<li><mark>Each source word has its own coverage embedding vector,</mark> and the number of coverage embedding vectors is the same as the source vocabulary size.</li>
<li>The coverage embedding matrix is initialized with coverage embedding vectors for all source words.</li>
<li>Coverage embeddings are updated using neural networks (GRU or subtraction).</li>
<li>As the translation progresses, coverage embeddings of translated words should approach zero.</li>
<li>Two methods are proposed to update the coverage embedding vectors: GRU and subtraction.</li>
</ul>
<ol type="1">
<li>Updating Methods
<ol type="1">
<li>Updating with a GRU
<ul>
<li>The coverage model is updated using a GRU, incorporating the current target word and attention weights.</li>
<li>The GRU uses update and reset gates to control the update of the coverage embedding vector.</li>
</ul></li>
<li>Updating as Subtraction
<ul>
<li>The coverage embedding is updated by subtracting the embedding of the target word, weighted by the attention probability.</li>
</ul></li>
</ol></li>
<li>Objectives
<ul>
<li>Coverage embedding models are integrated into attention NMT by adding coverage embedding vectors to the first layer of the attention model.</li>
<li>The goal is to remove partial information from the coverage embedding vectors based on the attention probability.</li>
<li>The model minimizes the absolute values of the embedding matrix.</li>
<li>The model can also use supervised alignments to know when the coverage embedding should be close to zero.</li>
</ul></li>
</ol></li>
<li>Related Work
<ul>
<li>Tu et al.&nbsp;(2016) also uses a GRU to model the coverage vector. However, this approach differs from the current paper’s method by initializing the word coverage vector with a scalar and adding an accumulation operation and a fertility function.</li>
<li>Cohn et al.&nbsp;(2016) augments the attention model with features from traditional SMT.</li>
</ul></li>
<li>Experiments
<ol type="1">
<li>Data Preperation
<ul>
<li>Experiments were conducted on a Chinese-to-English translation task.</li>
<li>Two training sets were used: one with 5 million sentence pairs and another with 11 million.</li>
<li>The development set consisted of 4491 sentences.</li>
<li>Test sets included NIST MT06, MT08 news, and MT08 web.</li>
<li>Full vocabulary sizes were 300k and 500k, with coverage embedding vector sizes of 100.</li>
<li>AdaDelta was used to update model parameters with a mini-batch size of 80.</li>
<li>The output vocabulary was a subset of the full vocabulary, including the top 2k most frequent words and the top 10 candidates from word-to-word/phrase translation tables.</li>
<li>The maximum length of a source phrase was 4.</li>
<li>A traditional SMT system, a hybrid syntax-based tree-to-string model, was used as a baseline.</li>
<li>Four different settings for coverage embedding models were tested: updating with a GRU (UGRU), updating as a subtraction (USub), the combination of both methods (UGRU+USub), and UGRU+USub with an additional objective (+Obj).</li>
</ul></li>
<li>Translation Results
<ul>
<li>The coverage embedding models improved translation quality significantly over a large vocabulary NMT (LVNMT) baseline system.</li>
<li>UGRU+USub performed best, achieving a 2.6 point improvement over LVNMT.</li>
<li>Improvements of coverage models over LVNMT were statistically significant.</li>
<li>The UGRU model also improved performance when using a larger training set of 11 million sentences.</li>
</ul></li>
<li>Alignment Results
<ul>
<li>The best coverage model (UGRU + USub) improved the F1 score by 2.2 points over the baseline NMT system.</li>
<li>Coverage embedding models reduce the number of repeated phrases in the output.</li>
</ul></li>
</ol></li>
<li>Conclusion
<ul>
<li>The paper proposed coverage embedding models for attention-based NMT.</li>
<li>The models use a coverage embedding vector for each source word and update these vectors as the translation progresses.</li>
<li>Experiments showed significant improvements over a strong large vocabulary NMT system.</li>
</ul></li>
</ol>
</section>
<section id="reflection" class="level2">
<h2 class="anchored" data-anchor-id="reflection">Reflection</h2>
<p>The idea of tracking coverage is very simple. Lets face it many issues in NLP require simple solutions.</p>
<p>For instance in the summarization task we have a big headache with the autoregressive tendency to repeat. But it also requires a kind of coverage too, but one that is more spread out. Also in more extreme cases we want to direct the coverage using very specific information like the narative flow of a story. This seems to be an idea that can be further explored in other tasks.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-mi-etal-2016-coverage" class="csl-entry">
Mi, Haitao, Baskaran Sankaran, Zhiguo Wang, and Abe Ittycheriah. 2016. <span>“Coverage Embedding Models for Neural Machine Translation.”</span> In <em>Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</em>, edited by Jian Su, Kevin Duh, and Xavier Carreras, 955–60. Austin, Texas: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/D16-1096">https://doi.org/10.18653/v1/D16-1096</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Coverage {Embedding} {Models} for {Neural} {Machine}
    {Translation}},
  date = {2025-02-12},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/2016-coverage-embedding-models-for-NMT/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Coverage Embedding Models for Neural Machine
Translation.”</span> February 12, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/2016-coverage-embedding-models-for-NMT/">https://orenbochman.github.io/notes-nlp/reviews/paper/2016-coverage-embedding-models-for-NMT/</a>.
</div></div></section></div> ]]></description>
  <category>Paper</category>
  <category>Review</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/2016-coverage-embedding-models-for-NMT/</guid>
  <pubDate>Tue, 11 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Summarization Task</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/posts/2025-02-10-summerizer/</link>
  <description><![CDATA[ 





<section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p>In the Deeplearning.ai NLP specialization, we implemented both <strong>Q&amp;A</strong> and <strong>Summarization tasks</strong>. However, we only looked at these tasks in terms of the attention models. It is now time to dive deeper into the summarization task. In this article I will review a talk covering a body of research on summarization, which has many ideas about features. Consider some of these and see what aspects are relevant to a modern task implementation.</p>
<p>Some of the tasks here can be done with a simple LSTM or Transformer. However it seems that RL may be useful if we want to plan the summary in a non-linear fashion or to make use of a dynamic programming approach. For instance to identify the narrative structure might require many more steps then we typically use in a summerization task. For a speeches we may want to capture a gist of the rhetoric.</p>
</section>
<section id="ideas" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="ideas">Ideas</h2>
<p>Several ideas surfaced while working on the assignment for building a hybrid generative/abstractive summarizer based on GPT2<sup>1</sup>. Many ideas came from prior work developing search engines. I had noticed that some issues were anathema to summarization from its inception.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;a Generative Pre-training Transformer or a decoder transformer</p></div></div><section id="coverage-ranking-content-by-importance" class="level3">
<h3 class="anchored" data-anchor-id="coverage-ranking-content-by-importance">Coverage ranking content by importance</h3>
<p>The first issue is <strong>coverage</strong>, which is how much of the original document to include. Coverage is a much more difficult problem than it appears.</p>
<p>Consider summerizing a long novel a single paragraph summary may provide a good idea of the main plot points of the story. On the other hand summerising a similar sized encyclopedia might require listing all the top level headings and subheadings i.e.&nbsp;a one paragraph summary would not be able to provide a good idea of the content of the document. On the other hand it should be much easier to generate a summary for the encyclopedia than for the novel as the encyclopedia has a highly structured document with clear headings and subheadings and writing conventions that provide cues to the reader about the importance of the content. Summerizing a novel is more challanging as we would actualy want to ommit alsmost all the details and so deciding which parts to keep is a major challange with a very sparse signal.</p>
<p>For example, technical documents, like patents, headings, academic writing cues, and even IR statistics, may serve as features that we may feed into a regression that can guide us in discarding the chuff from the grain. On the other hand, for movie scripts or novels, we can’t use all that, and we need to decide what is essential based on the narrative structure, which requires sophisticated processing and extensive domain knowledge to extract. So, When we want to create a synopsis of a book like War and Peace, specific details are part of the narrative structure that stands out as essential to us, and I can say that even in 2025, LLM is not good at picking these out of a large document. For attentive readers with a suitable background, if we double the length of the text, they might pick additional plot points and then less significant subplots. If again we double we would, they would include more details concerning characters, their motivations, etc.</p>
<p>To conclude, different documents can have radically different structures and cues. These suggest different strategies for selecting and ranking the material to be included in a summary of a given length. More so, when we consider summaries of different lengths, one can see that we are talking about a hierarchal structure.</p>
</section>
</section>
<section id="avoiding-repetition" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="avoiding-repetition">Avoiding repetition</h2>
<p>A second challenge that is pervasive is avoiding <strong>repetition</strong>. In the generative settings we have a tougher challenge since sentences that are generated may well be equivalent to sentences we have already generated, and we want the model to redirect it attention from material already covered. Luckily we have learned to detect similar sentences in the Q&amp;A task<sup>2</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;using Siamese networks for oneshot similarity detection.</p></div></div><p>check if it is similar to any sentence in the summary</p>
<p>Alg1: similarity detection</p>
<pre><code>summary = []
tokenized_doc = tokenize(doc)
embeddings_doc= embed(tokenized_doc)
while len(summary) &lt; doc_tokens * summary_ratio: 
    a = gen_a_sentence(embeddings_doc,summary)
    for s in summary:
        if sim(a,s) &gt; threshold:
            continue
    else:
        summary.append(a)
s -&gt; gen a sentence,</code></pre>
</section>
<section id="context-window-constraints" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="context-window-constraints">Context window constraints</h2>
<p>The third challenge is technical and is related to the <strong>length of the context window</strong>. as transformer models have a limit on the number of tokens that they can process in a context window. A second issue here is that the summary itself needs to also fit in the context window. If the structure is linear and incremental we can chunk it into many smaller pieced say using sections. However in reality we are often dealing with trees of graphs structures and chunking can erode the model’s ability to inspect said hierarchy of the structure it is tasked with summarizing. In Q&amp;A or IR tasks since we can process each chunk separately and then combine the results.</p>
<p>If we are not using a generative model one imagines a tree data structure that can efficiently track what what part of the document has been summarized. If we can use that to work with the attention mechanism we may be able to reduce the effective window size as we progress with the summary. A more nuanced idea would come from <strong>bayesian search</strong> where we may want to keep moving the probability mass for the attention mechanism to regions that are significant but have not been covered.</p>
<p>Another challenge is that we may want to grow our summary in a non-linear fashion. We may consider the main narrative structure then add in the subplots and then the character motivations. This suggests two approaches - a top down <strong>planning</strong> based approach<sup>3</sup> and a bottom up approach where we start with the most important details, then add in the less important ones. In the second case we may want to revise the initial sentences to efficiently incorporate more details as we progress.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;using rl or dynamic programming</p></div><div id="fn4"><p><sup>4</sup>&nbsp;reformer layers</p></div></div><p>I also interviewed with a company that told me they often worked with patents and that these documents were frequently in excess of a hundred pages and they were having lots of problems with the summarization task. This suggest that an efficient transformer<sup>4</sup> would be needed if we are to support context windows that can span hundreds of pages. But that besides the context window we need some good way to ensure that the summary is balanced and that it is not too repetitive.</p>
</section>
<section id="compassion" class="level2">
<h2 class="anchored" data-anchor-id="compassion">Compassion</h2>
<p>My experience with summarization by hand is that most texts can be compressed by 5-10% without losing any information simply by rephrasing with more concise language. So another idea that should be obvious is that the generated summary should be <strong>compressive</strong>, i.e., once we generate a good sentence we should consider if we can make it shorter. This should be fairly easy as we are reducing the length of a single sentence. We may however be able to do even better by considering a section and trying to see if we can merge two sentences or if two sentences have partial overlap that can be merged or referenced via an anaphora. This is another area where we diverge from Q&amp;A, where we do not have a length constraint and may often want to include all relevant information. Implementation of this idea cam be easily embodied in a loss function that penalizes summaries for each token or character. This same idea can also be used in a RL summarizer.</p>
</section>
<section id="grounding" class="level2">
<h2 class="anchored" data-anchor-id="grounding">Grounding</h2>
<p>Generative summarization is a case where we can require that all generated text be grounded in the original document. In reality generative models have many glitches, due to tokenization issues, or failures of the attention mechanism, or out of distribution generation, due to bias learned from the corpus, due to negative recency bias and and due to emergent contradictions (where a contradiction is introduced into the context window and cannot be removed.) And even if we can avoid all these there is still nothing in the model that makes it prefer truthy generations. So it seems essential that the generated text is grounded in the original document. This seems to be verifiable by visually inspecting the using the attention mechanism. In reality there may be multiple heads and multiple layers. This means we need to access the attention mechanism programmatically and store annotations from the source document for each generated token. We may also want to make abstracting summarization explicit. c.f. [Extrinsic Hallucinations in LLMs in ](https://lilianweng.github.io/posts/2024-07-07-hallucination/) by Lilian Weng</p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Summarization {Task}},
  date = {2025-02-10},
  url = {https://orenbochman.github.io/notes-nlp/posts/2025-02-10-summerizer/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Summarization Task.”</span> February 10,
2025. <a href="https://orenbochman.github.io/notes-nlp/posts/2025-02-10-summerizer/">https://orenbochman.github.io/notes-nlp/posts/2025-02-10-summerizer/</a>.
</div></div></section></div> ]]></description>
  <category>NLP</category>
  <category>Notes</category>
  <category>Literature review</category>
  <category>Summarization task</category>
  <category>Relevance</category>
  <guid>https://orenbochman.github.io/notes-nlp/posts/2025-02-10-summerizer/</guid>
  <pubDate>Sun, 09 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Morphological Word Embeddings</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/2019-morphological-embeddings/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/literature-review-open-book.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Litrature review"><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="img-fluid figure-img" alt="Litrature review"></a></p>
<figcaption>Litrature review</figcaption>
</figure>
</div></div><p>This is a mentioned in the tokenization lab in course four week 3</p>
<p>Time permitting I will try and dive deeper into this paper.</p>
<section id="podcast" class="level2">
<h2 class="anchored" data-anchor-id="podcast">Podcast</h2>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>Linguistic similarity is multi-faceted. For instance, two words may be similar with respect to semantics, syntax, or morphology inter alia. Continuous word-embeddings have been shown to capture most of these shades of similarity to some degree. This work considers guiding word-embeddings with morphologically annotated data, a form of semisupervised learning, encouraging the vectors to encode a word’s morphology, i.e., words close in the embedded space share morphological features. We extend the log-bilinear model to this end and show that indeed our learned embeddings achieve this, using German as a case study. – <span class="citation" data-cites="cotterell2019morphological">(Cotterell and Schütze 2019)</span></p>
</blockquote>
<!-- TODO: add review and podcast for this paper-->
<!-- TODO: link here from LSH lessons-->
</section>
<section id="terminology" class="level2">
<h2 class="anchored" data-anchor-id="terminology">Terminology</h2>
<p>Here are some terms and concepts discussed in the sources, with explanations:</p>
<dl>
<dt>Word embeddings</dt>
<dd>
These are vector representations of words in a continuous space, where similar words are located close to each other in the vector space. The goal is to capture linguistic similarity, but the definition of “similarity” can vary (semantic, syntactic, morphological). Word embeddings are typically trained to produce representations that capture linguistic similarity.
</dd>
<dt>Log-bilinear model (LBL)</dt>
<dd>
This is a language model that learns features along with weights, as opposed to using hand-crafted features. It uses context to predict the next word, and word embeddings fall out as low dimensional representations of the context. The LBL is a generalization of the log-linear model.
</dd>
<dt>Morphology</dt>
<dd>
The study of word structure, including how words are formed from morphemes (the smallest units of meaning).
</dd>
<dt>Morphological tags</dt>
<dd>
These are annotations that describe the morphological properties of a word, such as case, gender, number, tense, etc.. They can be very detailed for morphologically rich languages. For example, in German, a word might be tagged to indicate its part of speech, case, gender, and number.
</dd>
<dt>Morphologically rich languages</dt>
<dd>
Languages with a high morpheme-per-word ratio, where word-internal structure is important for processing. German and Czech are cited as examples.
</dd>
<dt>Morphologically impoverished languages</dt>
<dd>
Languages with a low morpheme-per-word ratio, such as English.
</dd>
<dt>Multi-task objective</dt>
<dd>
Training a model to perform multiple tasks simultaneously. In this case, the model is trained to predict both the next word and its morphological tag.
</dd>
<dt>Semi-supervised learning</dt>
<dd>
Training a model on a partially annotated corpus, using both labeled and unlabeled data. This approach is useful when large amounts of unannotated text are available.
</dd>
<dt>Contextual signature</dt>
<dd>
The words surrounding a given word. The context in which a word appears can be used to determine the word’s meaning and morphological properties.
</dd>
<dt>Hamming distance</dt>
<dd>
A measure of the difference between two binary strings, calculated as the number of positions at which the corresponding bits are different. In this context, it is used to compare morphological tags, which are represented as bit vectors.
</dd>
<dt>MORPHOSIM</dt>
<dd>
A metric for evaluating morphologically-driven embeddings that measures how morphologically similar a word is to its nearest neighbors in the embedding space. It is calculated as the average Hamming distance between morphological tags of a word and its neighbors. Lower values are better, as they indicate that the nearest neighbors of each word are closer morphologically.
</dd>
</dl>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li>Introduction
<ul>
<li>Presents the importance of capturing word morphology, especially for morphologically-rich languages.</li>
<li>Highlights the multifaceted nature of linguistic similarity (semantics, syntax, morphology).</li>
<li>Discusses the goal of the paper: to develop word embeddings that specifically encode morphological relationships.</li>
</ul></li>
<li>Related Work
<ul>
<li>Discusses previous integration of morphology into language models, including factored language models and neural network-based approaches.</li>
<li>Notes the role of morphology in computational morphology, particularly in morphological tagging for inflectionally-rich languages.</li>
<li>Highlights the significance of distributional similarity in morphological analysis.</li>
</ul></li>
<li>Log-Bilinear Model
<ul>
<li>Describes the log-bilinear model (LBL), a generalization of the log-linear model with learned features.</li>
<li>Presents the LBL’s energy function and probability distribution in the context of language modeling.</li>
</ul></li>
<li>Morph-LBL
<ul>
<li>Proposes a multi-task objective that jointly predicts the next word and its morphological tag.</li>
<li>Describes the model’s joint probability distribution, incorporating morphological tag features.</li>
<li>Discusses the use of semi-supervised learning, allowing training on partially annotated corpora</li>
</ul></li>
<li>Evaluation
<ul>
<li>Mentions the qualitative evaluation using t-SNE, showing clusters reflecting morphological and POS relationships.</li>
<li>Introduces MorphoSim, a novel quantitative metric to assess the extent to which similar embeddings are morphologically related.</li>
</ul></li>
<li>Experiments and Results
<ul>
<li>Presents experiments on the German TIGER corpus, comparing Morph-LBL with the original LBL and Word2Vec.</li>
<li>Describes two experiments:
<ul>
<li>Experiment 1: Evaluates the morphological information encoded in embeddings using a k-NN classifier.</li>
<li>Experiment 2: Compares models using the MorphoSim metric to measure morphological similarity among nearest neighbors.</li>
</ul></li>
<li>Discusses the results, highlighting Morph-LBL’s superior performance in capturing morphological relationships, even without observing all tags during training.</li>
</ul></li>
<li>Conclusion and Future Work
<ul>
<li>Summarizes the contributions of the paper: introducing Morph-LBL for inducing morphologically guided embeddings.</li>
<li>Notes the model’s success in leveraging distributional signatures to capture morphology.</li>
<li>Discusses future work on integrating orthographic features for further improvement.</li>
<li>Mentions potential applications in morphological tagging and other NLP tasks.</li>
</ul></li>
</ul>
</section>
<section id="log-bilinear-model" class="level2">
<h2 class="anchored" data-anchor-id="log-bilinear-model">Log-Bilinear Model</h2>
<p><img src="https://latex.codecogs.com/png.latex?p(w%5Cmid%20h)%20=%0A%5Cfrac%7B%5Cexp%5Cleft(s_%5Ctheta(w,h)%5Cright)%7D%7B%5Csum_%7Bw'%7D%0A%5Cexp%5Cleft(s_%5Ctheta(w',h)%5Cright)%7D%20%5Cqquad%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?w"> is a word, <img src="https://latex.codecogs.com/png.latex?h"> is a history and <img src="https://latex.codecogs.com/png.latex?s_%5Ctheta"> is an energy function. Following the notation of , in the LBL we define <img src="https://latex.codecogs.com/png.latex?%0As_%5Ctheta(w,h)%20=%20%5Cleft(%5Csum_%7Bi=1%7D%5E%7Bn-1%7D%20C_i%0Ar_%7Bh_i%7D%5Cright)%5ET%20q_w%20+%20b_w%20%5Cqquad%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?n-1"> is the history length</p>
</section>
<section id="morph-lbl" class="level2">
<h2 class="anchored" data-anchor-id="morph-lbl">Morph-LBL</h2>
<p><img src="https://latex.codecogs.com/png.latex?%0A%20%20p(w,%20t%20%5Cmid%20h)%20%5Cpropto%20%5Cexp((%20f_t%5ET%20S%20%20+%20%5Csum_%7Bi=1%7D%5E%7Bn-1%7DC_i%20r_%7Bh_i%7D)%5ET%20q_w%20+%20b_%7Bw%7D%20)%20%5Cqquad%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?f_t"> is a hand-crafted feature vector for a morphological tag <img src="https://latex.codecogs.com/png.latex?t"> and <img src="https://latex.codecogs.com/png.latex?S"> is an additional weight matrix.</p>
<p>Upon inspection, we see that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap(t%20%5Cmid%20w,h)%20%5Cpropto%20%5Cexp(S%5ET%20f_t%20q_w)%20%5Cqquad%0A"></p>
<p>Hence given a fixed embedding <img src="https://latex.codecogs.com/png.latex?q_w"> for word <img src="https://latex.codecogs.com/png.latex?w">, we can interpret <img src="https://latex.codecogs.com/png.latex?S"> as the weights of a conditional log-linear model used to predict the tag <img src="https://latex.codecogs.com/png.latex?t">.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<ul>
<li><a href="http://www.eva.mpg.de/lingua/resources/glossing-rules.php">Leipzig Glossing Rules</a> which provides a standard way to explain morphological features by examples</li>
<li><a href="https://www.youtube.com/watch?v=y9sVFrmGu0w&amp;ab_channel=GrahamNeubig">CMU Multilingual NLP 2020 (17): Morphological Analysis and Inflection</a> <!--
- [nb-lm](https://notebooklm.google.com/notebook/f594ff01-19f2-49b0-a0ac-84176fb22667?_gl=1*1rba7bx*_ga*MzAyOTc3ODMwLjE3Mzg1MDQ5Njc.*_ga_W0LDH41ZCB*MTczODkyMzY5Ni41LjAuMTczODkyMzY5Ni42MC4wLjA.)
--></li>
<li><span class="citation" data-cites="kann-etal-2016-neural">Kann, Cotterell, and Schütze (2016)</span> <a href="https://github.com/ryancotterell/neural-canonical-segmentation">code</a></li>
<li><a href="https://unimorph.github.io/">unimorph</a> univorsal morphological database</li>
</ul>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-cotterell2019morphological" class="csl-entry">
Cotterell, Ryan, and Hinrich Schütze. 2019. <span>“Morphological Word Embeddings.”</span> <em>arXiv Preprint arXiv:1907.02423</em>.
</div>
<div id="ref-kann-etal-2016-neural" class="csl-entry">
Kann, Katharina, Ryan Cotterell, and Hinrich Schütze. 2016. <span>“Neural Morphological Analysis: Encoding-Decoding Canonical Segments.”</span> In <em>Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</em>, edited by Jian Su, Kevin Duh, and Xavier Carreras, 961–67. Austin, Texas: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/D16-1097">https://doi.org/10.18653/v1/D16-1097</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Morphological {Word} {Embeddings}},
  date = {2025-02-07},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/2019-morphological-embeddings/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Morphological Word Embeddings.”</span>
February 7, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/2019-morphological-embeddings/">https://orenbochman.github.io/notes-nlp/reviews/paper/2019-morphological-embeddings/</a>.
</div></div></section></div> ]]></description>
  <category>Paper</category>
  <category>Review</category>
  <category>Podcast</category>
  <category>NLP</category>
  <category>Morphology</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/2019-morphological-embeddings/</guid>
  <pubDate>Thu, 06 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Practical and Optimal LSH for Angular Distance</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/2015-LSH/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/literature-review-open-book.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Literature review"><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="img-fluid figure-img" alt="Literature review"></a></p>
<figcaption>Literature review</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/t_8SpFV0l7A" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Locality-Sensitive Hashing and Beyond
</figcaption>
</figure>
</div><div id="vid-02" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/Dkomk2wPaoc" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;2: Beyond Locality Sensitive Hashing; Alexandr Andoni
</figcaption>
</figure>
</div></div>

<p>In the NLP specialization we have covered and used LSH a number of times in at least two courses. <!-- TODO: link to the notes so as to make them more useful as this is a stub!!!! --> In one sense I have an understanding of what is going on when we implement this. In another sense this seems to be quite confusing. So I don’t fully understand some aspects of LSH.</p>
<p>One way to do better is to try and explain it to someone else. This is what I am trying to do here by going back to the source and trying to understand the paper, problems, motivation and finally isolate what it is that is hard to grasp.</p>
<p>This paper is a good introduction to LSH for the angular distance.</p>
<p>In <span class="citation" data-cites="andoni2015practical">(Andoni et al. 2015)</span> the authors delve into Locality-Sensitive Hashing (LSH) for the angular distance. They show the existence of an LSH family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal running time exponent. Unlike earlier algorithms with this property (e.g., Spherical LSH), their algorithm is also practical, improving upon the well-studied hyperplane LSH in practice.</p>
<p>For now the two videos which explain in some depth this an the related paper - <a href="https://arxiv.org/abs/1501.01062">Optimal Data-Dependent Hashing for Approximate Near Neighbors</a> will have to do.</p>
<p>Time permitting I will try and dive deeper into this paper.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR - Location Sensitive Hashing
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/in_a_nutshell.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Location Sensitive Hashing in a Nutshell"><img src="https://orenbochman.github.io/notes-nlp/images/in_a_nutshell.jpg" class="img-fluid figure-img" alt="Location Sensitive Hashing in a Nutshell"></a></p>
<figcaption>Location Sensitive Hashing in a Nutshell</figcaption>
</figure>
</div>
<ul>
<li>The paper introduces a Locality-Sensitive Hashing (LSH) family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal running time exponent.</li>
<li>The algorithm is also practical, improving upon the well-studied hyperplane LSH in practice.</li>
<li>The authors also introduce a multiprobe version of this algorithm, and conduct experimental evaluation on real and synthetic data sets.</li>
</ul>
</div>
</div>
<section id="podcast" class="level2">
<h2 class="anchored" data-anchor-id="podcast">Podcast</h2>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>We show the existence of a Locality-Sensitive Hashing (LSH) family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal running time exponent. Unlike earlier algorithms with this property (e.g., Spherical LSH [1 , 2]), our algorithm is also practical, improving upon the well-studied hyperplane LSH [3] in practice.</p>
<p>We also introduce a <strong>multiprobe</strong> version of this algorithm, and conduct experimental evaluation on real and synthetic data sets.</p>
<p>We complement the above positive results with a fine-grained lower bound for the quality of any LSH family for angular distance. Our lower bound implies that the above LSH family exhibits a trade-off between evaluation time and quality that is close to optimal for a natural class of LSH functions. – <span class="citation" data-cites="andoni2015practical">(Andoni et al. 2015)</span></p>
</blockquote>
<!-- TODO: add outline, reflection, and terminology and for this paper-->
<!-- TODO: link here from LSH lessons-->
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-andoni2015practical" class="csl-entry">
Andoni, Alexandr, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn, and Ludwig Schmidt. 2015. <span>“Practical and Optimal LSH for Angular Distance.”</span> <em>Advances in Neural Information Processing Systems</em> 28.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Practical and {Optimal} {LSH} for {Angular} {Distance}},
  date = {2025-02-05},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/2015-LSH/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Practical and Optimal LSH for Angular
Distance.”</span> February 5, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/2015-LSH/">https://orenbochman.github.io/notes-nlp/reviews/paper/2015-LSH/</a>.
</div></div></section></div> ]]></description>
  <category>Paper</category>
  <category>Review</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/2015-LSH/</guid>
  <pubDate>Tue, 04 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>xLSTM: Extended Long Short-Term Memory</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/2025-xLSTM/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="cover.jpg" class="lightbox" data-gallery="slides" title="Literature Review"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-xLSTM/cover.jpg" class="img-fluid figure-img" alt="Literature Review"></a></p>
<figcaption>Literature Review</figcaption>
</figure>
</div><div id="vid-02" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/0aWGTNS03PU?t=3" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Review by AI Bites
</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/0OaEv1a5jUM" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;2: Review by Yannic Kilcher
</figcaption>
</figure>
</div><div id="vid-03" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/8u2pW2zZLCs" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;3: LSTM: The Comeback Story? Talk with Sepp Hochreiter after the xLSTM paper
</figcaption>
</figure>
</div></div>


<blockquote class="blockquote">
<p>Whate’er the theme, the Maiden sang<br>
<mark>As if her song could have no ending;</mark><br>
I saw her singing at her work,<br>
And o’er the sickle bending;—<br>
I listened, motionless and still;<br>
And, as I mounted up the hill,<br>
<mark>The music in my heart I bore,<br>
Long after it was heard no more.</mark> &nbsp;</p>
<p>– The Solitary Reaper by William Wordsworth.</p>
</blockquote>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR – Scaling LSTMs to Billions of Parameters with <strong>xLSTM</strong>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/in_a_nutshell.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="LSTMs in a nutshell"><img src="https://orenbochman.github.io/notes-nlp/images/in_a_nutshell.jpg" class="img-fluid figure-img" alt="LSTMs in a nutshell"></a></p>
<figcaption>LSTMs in a nutshell</figcaption>
</figure>
</div>
<ul>
<li>Just when we think that the transformer rules supreme the RNN makes a comeback with a refreshing new look at the LSTMs.</li>
<li>This paper brings new architectures to the LSTM that are fully parallelizable and can scale to billions of parameters.</li>
<li>They demonstrate on synthetic tasks and language modeling benchmarks that these new xLSTMs can outperform state-of-the-art Transformers and State Space Models.</li>
</ul>
</div>
</div>
</div>
<section id="introduction" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Motivation
</div>
</div>
<div class="callout-body-container callout-body">
<p>So this is my first review of the RNN papers. I found that since the courses covering LSTM seemed to be over simplified and confusing, I should go back to the source. Since time is limited, it helps to review the papers in reverse chronological order. Each new papers explains the limitations that are generally not apparent when the paper is first published. The authors of the later papers also have the benefit of hindsight which they share in the obligatory literature review wherein they provide lots of context we cannot get by reading the original papers. Given all that there is also a benefit to reading the original paper - the real innovators while harder to understand are remarkable thinkers and often reveal motivations and deep insights that elude later interlocutors.</p>
</div>
</div>
<section id="sec-podcast" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-podcast">Podcast &amp; Other Reviews</h3>
<p>This paper is a bit heavy on the technical side. But if we are still here, we may enjoy this podcast reviewing the paper. Though AI generated I found it quite useful to get oriented with the paper and understand that the experiments the authors run to validate this new architectures.</p>
<p>We also cover something I found absent from the Course on sequence models . The limitations of LSTMs as viewed by the authors when considered in the context of transformers and Mamba.</p>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
<p>Although this paper is recent there are a number of other people who cover it.</p>
<ul>
<li>In Video&nbsp;2 Yannic Kilcher covers the paper in his youtube channel. He does a great job of explaining the paper in a way that is easy to understand. Kilcher is all too human, he makes many mistakes, stresses opinions over facts. If we don’t mind all that he is a great resource for understanding the paper. I find he rubs me the wrong way but at least I find this motivating to do better in my own reviews!</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
What can we expect from xLSTM?
</div>
</div>
<div class="callout-body-container callout-body">

<blockquote class="blockquote">
<p>xLSTM has the potential to considerably impact other fields like Reinforcement Learning, Time Series Prediction, or the modeling of physical systems. – <span class="citation" data-cites="beck2024xlstm">(Beck et al. 2024)</span></p>
</blockquote>
<p>In his book Understanding Media <span class="citation" data-cites="mcluhan1988understanding">(McLuhan 1988)</span> Marshal McLuhan introduced his <strong>Tetrad</strong>. <mark>The <strong>Tetrad</strong> is a mental model for understanding how a technological innovation like the xLSTM might disrupt society</mark>. To work the model we ask the following four questions that can be applied to any new technology to understand its impact. The four questions in this instance are:</p>
<ol type="1">
<li>What does the xLSTM enhance or amplify?</li>
</ol>
<blockquote class="blockquote">
<p>The xLSTM enhances the LSTM by introducing exponential gating and novel memory structures, boosting its capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models.</p>
</blockquote>
<ol start="2" type="1">
<li>What does the xLSTM make obsolete or replace?</li>
</ol>
<blockquote class="blockquote">
<p>The xLSTM makes the traditional LSTM obsolete by addressing its limitations, such as the inability to revise storage decisions, limited storage capacities, and lack of parallelizability.</p>
</blockquote>
<ol start="3" type="1">
<li>What does the xLSTM retrieve that was previously obsolesced?</li>
</ol>
<blockquote class="blockquote">
<p>The xLSTM retrieves the LSTM’s relevance in large-scale language modeling by scaling it to billions of parameters and integrating modern LLM techniques while mitigating known limitations. 4. What does the xLSTM reverse or flip into when pushed to its limits?</p>
</blockquote>
<blockquote class="blockquote">
<p>The xLSTM, when pushed to its limits, can compete with current Large Language Models based on Transformer technology, offering a competitive alternative for sequence modeling tasks.</p>
</blockquote>
<p>The xLSTM paper by <span class="citation" data-cites="beck2024xlstm">(Beck et al. 2024)</span> is a great example of the tetrad in action. The authors take the LSTM, a technology that has stood the test of time, and enhance it with exponential gating and novel memory structures. They address the limitations of traditional LSTMs and push the technology to new limits, creating a new architecture that can compete with state-of-the-art Transformers and State Space Models. Recent work by <span class="citation" data-cites="chen2024computationallimitsstatespacemodels">(Chen et al. 2024)</span> suggest that Transformers and The State Space Models are actually limited in their own ways.</p>
</div>
</div>
<div class="no-row-height column-margin column-container"><div id="sup-01" class="quarto-float quarto-figure quarto-figure-center anchored callout-margin-content">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="MediaTetrad.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Tetrad"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-xLSTM/MediaTetrad.svg" class="img-fluid figure-img"></a></p>
<figcaption>Tetrad</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div></section>
</section>
<section id="abstract" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining:</p>
<ol type="1">
<li>sLSTM with a scalar memory, a scalar update, and new memory mixing,<br>
</li>
<li>mLSTM that is fully parallelizable with a matrix memory and a covariance update rule.<br>
</li>
</ol>
<p>Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling. — <span class="citation" data-cites="beck2024xlstm">(Beck et al. 2024)</span></p>
</blockquote>

<div class="no-row-height column-margin column-container"><div id="fig-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="fig01.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="architecture"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-xLSTM/fig01.png" class="img-fluid figure-img"></a></p>
<figcaption>architecture</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: The extended LSTM (xLSTM) family. From left to right:<br>
1. The original LSTM memory cell with constant error carousel and gating.<br>
2. New <strong>sLSTM</strong> and <strong>mLSTM</strong> memory cells that introduce exponential gating. sLSTM offers a new memory mixing technique. <strong>mLSTM</strong> is fully parallelizable with a novel matrix memory cell state and new covariance update rule.<br>
3. mLSTM and sLSTM in residual blocks yield xLSTM blocks.<br>
4. Stacked xLSTM blocks give an xLSTM
</figcaption>
</figure>
</div><div id="fig-02" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="fig02.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="blocks"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-xLSTM/fig02.png" class="img-fluid figure-img"></a></p>
<figcaption>blocks</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: LSTM limitations.<br>
- Left: Nearest Neighbor Search problem in terms of mean squared error (MSE). Given a reference vector, a sequence is scanned sequentially for the most similar vector with the objective to return its attached value at sequence end. LSTM struggles to revise a stored value when a more similar vector is found. Our new xLSTM overcomes this limitation by exponential gating. - Right: Rare Token Prediction. The perplexity (PPL) of token prediction on Wikitext-103, in partitions of token frequency. LSTM performs worse on predicting rare tokens because of its limited storage capacities, whereas our new xLSTM solves this problem via a matrix memory.
</figcaption>
</figure>
</div><div id="fig-03" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="fig03.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="blocks"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-xLSTM/fig03.png" class="img-fluid figure-img"></a></p>
<figcaption>blocks</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: xLSTM blocks.<br>
Left: A residual sLSTM block with post up-projection (like Transformers): The input is fed into an sLSTM — with an optional convolution — followed by a gated MLP.<br>
Right: A residual mLSTM block with pre up-projection (like State Space models): mLSTM is wrapped inside two MLPs, via a convolution, a learnable skip connection, and an output gate that acts componentwise.
</figcaption>
</figure>
</div><div id="fig-04" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-04-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="fig04.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="blocks"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-xLSTM/fig04.png" class="img-fluid figure-img"></a></p>
<figcaption>blocks</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-04-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: xLSTM blocks.<br>
Left: A residual sLSTM block with post up-projection (like Transformers): The input is fed into an sLSTM — with an optional convolution — followed by a gated MLP.<br>
Right: A residual mLSTM block with pre up-projection (like State Space models): mLSTM is wrapped inside two MLPs, via a convolution, a learnable skip connection, and an output gate that acts componentwise.
</figcaption>
</figure>
</div><div id="fig-05" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-05-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="fig05.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="blocks"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-xLSTM/fig05.png" class="img-fluid figure-img"></a></p>
<figcaption>blocks</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-05-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: xLSTM blocks.<br>
Left: A residual sLSTM block with post up-projection (like Transformers): The input is fed into an sLSTM — with an optional convolution — followed by a gated MLP.<br>
Right: A residual mLSTM block with pre up-projection (like State Space models): mLSTM is wrapped inside two MLPs, via a convolution, a learnable skip connection, and an output gate that acts componentwise.
</figcaption>
</figure>
</div></div>



</section>
<section id="sec-outline" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-outline">Paper Outline</h2>
<ul>
<li>Introduction
<ul>
<li>Describes the constant error carousel and gating mechanisms of Long Short-Term Memory (LSTM). &gt; “In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories…”</li>
<li>Discusses three main limitations of LSTMs:
<ol type="1">
<li>The inability to revise storage decisions.</li>
<li>Limited storage capacities.</li>
<li>Lack of parallelizability due to memory mixing.</li>
</ol></li>
<li>Highlights <mark>the emergence of Transformers in language modeling due to these limitations</mark>. &gt; Transformer Era: The rise of Transformers with parallelizable self-attention has largely overshadowed LSTMs due to their superior scalability.</li>
</ul></li>
<li>Extended Long Short-Term Memory
<ul>
<li>Introduces Extended Long Short-Term Memory (xLSTM), which incorporates exponential gating and novel memory structures.</li>
<li>Presents two new LSTM variants:
<ol type="1">
<li>sLSTM with a scalar memory, a scalar update, and new memory mixing.</li>
<li>mLSTM with a matrix memory and a covariance update rule, which is fully parallelizable.</li>
</ol></li>
<li>Describes the integration of these LSTM variants into residual block modules, resulting in xLSTM blocks.</li>
<li>Presents xLSTM architectures that residually stack xLSTM blocks.</li>
</ul></li>
<li>Related Work:
<ul>
<li>Mentions various linear attention methods to overcome the quadratic complexity of Transformer attention.</li>
<li>Notes the popularity of State Space Models (SSMs) for language modeling.</li>
<li>Highlights the use of Recurrent Neural Networks (RNNs) as a replacement for Transformers.</li>
<li>Mentions the use of <strong>gating</strong> in recent RNN and SSM approaches.</li>
<li>Notes the use of <strong>covariance update rules</strong><sup>1</sup> to enhance storage capacities in various models.</li>
</ul></li>
<li>Experiments
<ul>
<li>Presents the experimental evaluation of xLSTM compared to existing methods with a focus on language modeling.</li>
<li>Discusses the effectiveness of xLSTM on synthetic tasks, including:
<ul>
<li>Formal languages.</li>
<li>Multi-Query Associative Recall.</li>
<li>Long Range Arena tasks.</li>
</ul></li>
<li>Presents a method comparison and ablation study, where different models are trained on 15 billion tokens from the SlimPajama dataset.</li>
<li>Assesses the scaling behavior of different methods based on validation perplexity.</li>
<li>Conducts a large-scale language modeling experiment:
<ul>
<li>Training different model sizes on 300B tokens from SlimPajama.</li>
<li>Evaluating models on length extrapolation.</li>
<li>Assessing models on validation perplexity and performance on downstream tasks.</li>
<li>Evaluating models on 571 text domains of the PALOMA language benchmark dataset.</li>
<li>Assessing the scaling behavior with increased training data.</li>
</ul></li>
</ul></li>
<li>Limitations
<ul>
<li>Highlights limitations of xLSTM, including:
<ul>
<li>Lack of parallelizability for sLSTM due to memory mixing.</li>
<li>Unoptimized CUDA kernels for mLSTM.</li>
<li>High computational complexity of mLSTM’s matrix memory.</li>
<li>Memory overload for longer contexts due to the independence of mLSTM’s memory from sequence length.</li>
</ul></li>
</ul></li>
<li>Conclusion
<ul>
<li>Concludes that xLSTM performs favorably in language modeling compared to Transformers and State Space Models.</li>
<li>Suggests that larger xLSTM models will be competitive with current Large Language Models based on Transformer technology.</li>
<li>Notes the potential impact of xLSTM on other deep learning fields such as reinforcement learning, time series prediction, and physical system modeling.</li>
</ul></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;explain covariance</p></div></div></section>
<section id="briefing-xlstm---extended-long-short-term-memory" class="level2">
<h2 class="anchored" data-anchor-id="briefing-xlstm---extended-long-short-term-memory">Briefing : xLSTM - Extended Long Short-Term Memory</h2>
<section id="introduction-and-motivation" class="level3">
<h3 class="anchored" data-anchor-id="introduction-and-motivation">Introduction and Motivation:</h3>
<p><strong>LSTM’s Legacy</strong>: The Long Short-Term Memory (LSTM), introduced in the 1990s, has been a cornerstone of deep learning, especially in sequence modeling and early Large Language Models (LLMs). LSTMs are known for their “constant error carousel” and gating mechanisms, designed to combat vanishing gradients in recurrent neural networks.</p>
<blockquote class="blockquote">
<p>“In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories…”</p>
</blockquote>
<p>Transformer Era: The rise of Transformers with parallelizable self-attention has largely overshadowed LSTMs due to their superior scalability.</p>
<blockquote class="blockquote">
<p>“However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale.” <strong>xLSTM Question</strong>: This research aims to explore the capabilities of LSTMs when scaled to billions of parameters, by integrating modern LLM techniques while addressing known limitations.</p>
</blockquote>
<blockquote class="blockquote">
<p>“We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs?”</p>
</blockquote>
</section>
<section id="key-limitations-of-traditional-lstms" class="level3">
<h3 class="anchored" data-anchor-id="key-limitations-of-traditional-lstms">Key Limitations of Traditional LSTMs:</h3>
<p>The paper identifies three major limitations of traditional LSTMs:</p>
<ol type="1">
<li>Inability to Revise Storage Decisions: LSTMs struggle to update previously stored information when more relevant information arrives later in the sequence. The paper uses a “Nearest Neighbor Search” task to demonstrate this.</li>
</ol>
<blockquote class="blockquote">
<p>“LSTM struggles to revise a stored value when a more similar vector is found…”</p>
</blockquote>
<ol start="2" type="1">
<li>Limited Storage Capacity: LSTMs store information in scalar cell states, which can lead to information compression and loss. This impacts performance on tasks requiring detailed memory, like predicting rare tokens.</li>
</ol>
<blockquote class="blockquote">
<p>“LSTM performs worse on predicting rare tokens because of its limited storage capacities…”</p>
</blockquote>
<ol start="3" type="1">
<li>Lack of Parallelizability: LSTMs process data sequentially due to the recurrent connections between hidden states, hindering parallel computation.</li>
</ol>
<blockquote class="blockquote">
<p>“Lack of parallelizability due to memory mixing, i.e., the hidden-hidden connections between hidden states from one time step to the next, which enforce sequential processing.”</p>
</blockquote>
<ol start="3" type="1">
<li>xLSTM Innovations:</li>
</ol>
<p>The paper introduces Extended Long Short-Term Memory (xLSTM), designed to address the above limitations, by modifying the original LSTM using two main approaches:</p>
<p><strong>Exponential Gating</strong>:Implemented to allow for better revision of storage decisions. Normalization and stabilization techniques are used to avoid overflows in calculations due to the exponential function.</p>
<blockquote class="blockquote">
<p>“To empower LSTMs with the ability to revise storage decisions, we introduce exponential gates (red) together with normalization and stabilization.” Novel Memory Structures: Two new LSTM variants are created with these structures:</p>
</blockquote>
<p><strong>sLSTM</strong>: A scalar memory LSTM with a scalar update and new memory mixing technique (within heads, not across). Features multiple memory cells and multiple heads, with memory mixing within each head.</p>
<blockquote class="blockquote">
<p>“The new sLSTM (see Section 2.2) with a scalar memory, a scalar update, and memory mixing…”</p>
</blockquote>
<p><strong>mLSTM</strong>: A matrix memory LSTM, fully parallelizable, with a covariance update rule for storing key-value pairs. Does not have memory mixing between cells or heads.</p>
<blockquote class="blockquote">
<p>“the new mLSTM (see Section 2.3) with a matrix memory and a covariance (outer product) update rule, which is fully parallelizable.”</p>
</blockquote>
<ol start="4" type="1">
<li>sLSTM Details:</li>
</ol>
<ul>
<li><strong>Exponential Gates</strong>: Introduces exponential activation functions for input and forget gates with normalization and a stabilizer state to prevent overflow.</li>
<li><strong>Normalizer State</strong>: A state used to normalize the hidden state output by accumulating a weighted sum of input and forget gates.</li>
<li><strong>Memory Mixing</strong>: sLSTM allows memory mixing between cells, and also introduces a new form of memory mixing through multiple heads, mixing between cells within a head, but not across heads.</li>
</ul>
<blockquote class="blockquote">
<p>“The new sLSTM can have multiple heads with memory mixing within each head but not across heads. The introduction of heads for sLSTM together with exponential gating establishes a new way of memory mixing.”</p>
</blockquote>
<ol start="5" type="1">
<li>mLSTM Details:</li>
</ol>
<p><strong>Matrix Memory</strong>: Replaces the scalar cell state with a matrix, increasing storage capacity.</p>
<p><strong>Key-Value Storage</strong>: The mLSTM memory is based around a key-value storage system inspired by Bidirectional Associative Memories (BAMs) using covariance updates, where keys and values are updated via an outer product.</p>
<blockquote class="blockquote">
<p>“At time <img src="https://latex.codecogs.com/png.latex?t">, we want to store a pair of vectors, the key <img src="https://latex.codecogs.com/png.latex?k_t%20%E2%88%88%20R%5Ed"> and the value <img src="https://latex.codecogs.com/png.latex?v_t%20%E2%88%88%20R%5Ed">… The covariance update rule… for storing a key-value pair…”</p>
</blockquote>
<p>Parallelization: mLSTM is fully parallelizable due to abandoning hidden-hidden recurrent connections and memory mixing. Computations can be performed efficiently on GPUs using standard matrix operations.</p>
<blockquote class="blockquote">
<p>“…the mLSTM… which is fully parallelizable.”</p>
</blockquote>
<ol start="6" type="1">
<li>xLSTM Architecture:</li>
</ol>
<p><strong>xLSTM Blocks</strong>: The sLSTM and mLSTM variants are integrated into residual blocks.</p>
<p><strong>sLSTM blocks</strong> use post up-projection (like Transformers).</p>
<p><strong>mLSTM blocks</strong> use pre up-projection (like State Space Models).</p>
<blockquote class="blockquote">
<p>“Integrating these new LSTM variants into residual block modules results in xLSTM blocks…”</p>
</blockquote>
<p><strong>Residual Stacking</strong>: xLSTM architectures are created by residually stacking these blocks, using pre-LayerNorm backbones similar to current LLMs.</p>
<blockquote class="blockquote">
<p>“An xLSTM architecture is constructed by residually stacking build-ing blocks…” <strong>Cover’s Theorem</strong>: Used to justify mapping information into a higher-dimensional space for better separation of histories or contexts.</p>
</blockquote>
<blockquote class="blockquote">
<p>“We resort to Cover’s Theorem (Cover, 1965), which states that in a higher dimensional space non-linearly embedded patterns can more likely be linearly separated than in the original space.”</p>
</blockquote>
<ol start="7" type="1">
<li>Performance and Scaling:</li>
</ol>
<p><strong>Linear Computation &amp; Constant Memory</strong>: Unlike Transformers, xLSTM networks have linear computation and constant memory complexity with respect to sequence length.</p>
<blockquote class="blockquote">
<p>“Contrary to Transformers, xLSTM networks have a linear computation and a constant memory complexity with respect to the sequence length.”</p>
</blockquote>
<p><strong>Synthetic Tasks</strong>: xLSTM showed improved performance over regular LSTMs on “Nearest Neighbor Search” and “Rare Token Prediction” synthetic tasks.</p>
<p><strong>SlimPajama Experiments</strong>: The xLSTM models were trained on 15B and 300B tokens from the SlimPajama dataset for language modelling evaluation.</p>
<p><strong>Competitive Performance</strong>: xLSTMs demonstrate competitive performance against state-of-the-art Transformers and State Space Models on language modelling benchmarks, both in terms of next token prediction perplexity and downstream tasks.</p>
<blockquote class="blockquote">
<p>“Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.”</p>
</blockquote>
<p>Ablation studies show importance of gating techniques.</p>
<ol start="8" type="1">
<li>Memory &amp; Speed:</li>
</ol>
<p><strong>sLSTM</strong>: Not parallelizable due to memory mixing, although the authors have created a fast CUDA implementation (less than twice as slow as mLSTM).</p>
<blockquote class="blockquote">
<p>“While mLSTM is parallelizable analog to FlashAttention… sLSTM is not parallelizable due to the memory mixing…”</p>
</blockquote>
<p><strong>mLSTM</strong>: Parallelizable, but currently slower than FlashAttention or Mamba due to non-optimized CUDA kernels. Matrix memory is computationally complex but does not require parameters and can be parallelized.</p>
<blockquote class="blockquote">
<p>“The memory of mLSTM does not require parameters but is computationally expensive through its d×d matrix memory and d× d update… the computations can be done in parallel on GPUs, therefore these computations have only a minor effect on the wall clock time.”</p>
</blockquote>
<ol start="9" type="1">
<li>Limitations:</li>
</ol>
<p><strong>sLSTM Parallelization</strong>: sLSTM’s memory mixing is non-parallelizable.</p>
<blockquote class="blockquote">
<p>“(i) In contrast to mLSTM, memory mixing of the sLSTM prohibits parallelizable operations, and therefore does not allow a fast parallel implementation.”</p>
</blockquote>
<p>mLSTM CUDA Optimization: The mLSTM CUDA kernels are not fully optimized.</p>
<blockquote class="blockquote">
<p>“(ii) The CUDA kernels for mLSTM are not optimized, and therefore the current implementation is about 4 times slower than FlashAttention or the scan used in Mamba.”</p>
</blockquote>
<p><strong>mLSTM Matrix Memory</strong>: High computational complexity for mLSTM due to matrix memory operations.</p>
<p><strong>Forget Gate Initialization</strong>: Careful initialization of the forget gates is needed.</p>
<p><strong>Long Context Memory</strong>: The matrix memory is independent of sequence length, and might overload memory for long context sizes.</p>
<blockquote class="blockquote">
<p>“(v) Since the matrix memory is independent of the sequence length, increasing the sequence length might overload the memory for longer context sizes.”</p>
</blockquote>
<p><strong>Hyperparameter Optimization</strong>: xLSTM is not yet fully optimized due to computational load, indicating further performance gains are possible.</p>
<blockquote class="blockquote">
<p>“(vi) Due to the expensive computational load for large language experiments, we did neither fully optimize the architecture nor the hyperparameters, especially for larger xLSTM architectures.”</p>
</blockquote>
<ol start="10" type="1">
<li>Related Work:</li>
</ol>
<p>The paper highlights connections of its ideas with the following areas:</p>
<p><strong>Gating</strong>: Many recent models such as HGRN, GLA, Gated State Space models, Mamba, and others also utilize gating mechanisms. Covariance Update Rule: The covariance update rule used in mLSTM is related to other methods such as Fast Weight Programmers, Retention, and Linear Transformers.</p>
<ol start="11" type="1">
<li>Conclusion:</li>
</ol>
<p>The xLSTM presents a compelling alternative to Transformers in sequence modeling, especially in contexts where memory efficiency and longer sequences are required. By combining insights from the original LSTM with exponential gating and novel memory structures, the paper shows that LSTMs can be competitive at scale. While limitations exist in terms of parallelization and hyperparameter tuning, the research shows promising potential for further development. The authors posit further research and improvements could allow xLSTM to reach its full potential.</p>
</section>
</section>
<section id="my-thoughts" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="my-thoughts">My Thoughts</h2>

<div class="no-row-height column-margin column-container"><div id="vid-sigmoid" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-sigmoid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/TPqr8t919YM" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-sigmoid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;4: Review of the sigmoid function
</figcaption>
</figure>
</div></div><div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Research questions
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li>How does the constant error carousel mitigate the vanishing gradient problem in the LSTM?
<ul>
<li>The constant error carousel is a way to keep the error constant over time. This helps to prevent the vanishing gradient problem in the LSTM.</li>
</ul></li>
<li>How are the gates in the original LSTM binary?
<ul>
<li>Sigmoid, saturation and a threshold at 0.5</li>
</ul></li>
<li>What is the long term memory in the LSTM?
<ul>
<li>the cell state <img src="https://latex.codecogs.com/png.latex?c_%7Bt-1%7D"></li>
</ul></li>
<li>What is the short term memory in the LSTM?
<ul>
<li>the hidden state <img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D"></li>
</ul></li>
<li>How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs</li>
</ol>
</div>
</div>

<div class="no-row-height column-margin column-container"><div id="sup-02" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="sigmoid-limits.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Supplementary Figure&nbsp;2: limits of the sigmoid function"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-xLSTM/sigmoid-limits.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup" id="sup-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;2: limits of the sigmoid function
</figcaption>
</figure>
</div><div id="sup-03" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="sigmoid-values.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="bias"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-xLSTM/sigmoid-values.png" class="img-fluid figure-img"></a></p>
<figcaption>bias</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup" id="sup-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;3: The inductive bias of the sigmoid decision function.
</figcaption>
</figure>
</div></div>
<section id="binary-nature-of-non-exponential-gating-mechanisms" class="level3">
<h3 class="anchored" data-anchor-id="binary-nature-of-non-exponential-gating-mechanisms">Binary nature of non exponential Gating Mechanisms</h3>
<p>If we try to understand why are the gating mechanisms in the original LSTM is described here as binary?</p>
<p>It helps to the properties of the sigmoid functions that are explained in Video&nbsp;4.</p>
<ol type="1">
<li><p>Supplementary Figure&nbsp;1 shows that The sigmoid function has a domain of <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D"> and a range of <img src="https://latex.codecogs.com/png.latex?(0,1)">. This means that the sigmoid function can only output values between 0 and 1.</p></li>
<li><p>It has a very sharp gradient around 0 if the bias is set to 0. This means that the sigmoid function acts as a binary function around 0.</p></li>
<li><p>The Sigmoid function tends to saturate. Now sigmoid function around 0 isn’t quite quite binary but outside of (-6,+6) it becomes saturated and acts very much as a binary function.</p></li>
<li><p>If we use it as a loss function or a decision function, the sigmoid function is used with a threshold of 0.5. This means that if the output of the sigmoid function is greater than 0.5, the decision is 1, and if it is less than 0.5, the decision is 0. That is how the sigmoid function can become binary.</p></li>
<li><p>Note that we can use thresholding during inference to make the decision binary and we can keep them as continuous values during training so we can have a smooth gradient for back-propagation.</p></li>
<li><p>Even without thresholding the sigmoid function can become binary this is due to the saturation of the sigmoid function. I recall that Hinton in his Coursera course on Neural Networks and Deep Learning mentioned that activation function that become saturated are problematic. I can say that at last I think I understand what he meant:<br>
I see that that sigmoid functions can fall off the manifold of the data and this can be a problem. This is why the ReLU function is used in the Gated Recurrent Unit (GRU) and the Transformer. <strong>Falling off the manifold of the data</strong> means that the sigmoid function becomes saturated (above 6 and below -6). At this point the back-propagation steps are no longer able to change the weight or bias since the gradient is effectively so learning stops.</p></li>
</ol>
<p>This becomes a is a problem in two senses. The sigmoid unit is likely to be stuck in a setting that only representative of part of the training data. The rest of the network which is not stuck now has to learn to compensate for this.</p>
<p>This issues may well be the root cause LSTM limitation number 1 - the inability to revise storage decisions. It seems that the authors decided to use the exponential gating function to address this issues.</p>
<p>This is why the gating mechanisms in the original LSTM are binary. The sigmoid function is used to control the flow of information in the LSTM by deciding which information to keep and which to discard. The sigmoid function is used to decide how much of the new input information should be added to the cell state and how much of the old cell state should be retained. This binary nature of the sigmoid function is what allows the LSTM to control the flow of information and decide what to remember and what to forget.</p>
</section>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li><p><a href="https://openreview.net/forum?id=ARAxPPIAhq&amp;noteId=gra7vHnb0q">The paper on open review</a> has some additional insights from the authors</p></li>
<li><p><a href="https://www.ai-bites.net/xlstm-extended-long-short-term-memory-networks/">XLSTM — Extended Long Short-Term Memory Networks</a> By Shrinivasan Sankar — May 20, 2024</p></li>
</ul>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-beck2024xlstm" class="csl-entry">
Beck, Maximilian, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, Günter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. 2024. <span>“xLSTM: Extended Long Short-Term Memory.”</span> <em>arXiv Preprint arXiv:2405.04517</em>.
</div>
<div id="ref-chen2024computationallimitsstatespacemodels" class="csl-entry">
Chen, Yifang, Xiaoyu Li, Yingyu Liang, Zhenmei Shi, and Zhao Song. 2024. <span>“The Computational Limits of State-Space Models and Mamba via the Lens of Circuit Complexity.”</span> <a href="https://arxiv.org/abs/2412.06148">https://arxiv.org/abs/2412.06148</a>.
</div>
<div id="ref-mcluhan1988understanding" class="csl-entry">
McLuhan, Marshall. 1988. <em>Understanding Media : The Extensions of Man</em>. New York: New American Library. <a href="http://www.worldcat.org/search?qt=worldcat_org_all&amp;q=0451624963">http://www.worldcat.org/search?qt=worldcat_org_all&amp;q=0451624963</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {xLSTM: {Extended} {Long} {Short-Term} {Memory}},
  date = {2025-02-03},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/2025-xLSTM/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“xLSTM: Extended Long Short-Term
Memory.”</span> February 3, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-xLSTM/">https://orenbochman.github.io/notes-nlp/reviews/paper/2025-xLSTM/</a>.
</div></div></section></div> ]]></description>
  <category>Paper</category>
  <category>Review</category>
  <category>NLP</category>
  <category>LSTM</category>
  <category>Seq2Seqs</category>
  <category>RNN</category>
  <category>Podcast</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/2025-xLSTM/</guid>
  <pubDate>Sun, 02 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>The Secret Life of Pronouns What Our Words Say About Us</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/book/seret-life-of-pronouns/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="cover.jpg" class="lightbox" data-gallery="slides" title="cover"><img src="https://orenbochman.github.io/notes-nlp/reviews/book/seret-life-of-pronouns/cover.jpg" class="img-fluid figure-img" alt="cover"></a></p>
<figcaption>cover</figcaption>
</figure>
</div></div><div class="page-columns page-full"><blockquote class="blockquote">
<p>To grunt and sweat under a weary life, But that the dread of something after death, <mark>The undiscovered country from whose bourn No traveler returns, puzzles the will, And makes us rather bear those ills we have, Than fly to others that we know not of</mark>?<sup>1</sup> — Hamlet Act 3, Scene 1 by William Shakespeare.</p>
</blockquote><div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Pronouns oft fall prey to the stop word filter, yet they hold the keys to unlocking the depth of intimate meaning</p></div></div></div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR – ML, NLP and the secret life of pronouns
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/in_a_nutshell.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Pronouns in a nutshell"><img src="https://orenbochman.github.io/notes-nlp/images/in_a_nutshell.jpg" class="img-fluid figure-img" alt="Pronouns in a nutshell"></a></p>
<figcaption>Pronouns in a nutshell</figcaption>
</figure>
</div>
<ul>
<li>When I got started with NLP I was coding search engines and conventional wisdom was <mark>pronouns don’t pass the <strong>stop word filter</strong></mark></li>
<li>One of the gems I learned on that job was that <mark>everything in the corpus can be provide invaluable information if we only know how to index it</mark>.</li>
<li>After reading this book and I started to unlocked the power of the pronouns.</li>
<li>At Hungarian School I discovered how pronouns combine with case ending to create a vast vistas of untapped NLP resource.</li>
<li>Cognitive AI I noted that pronouns and particles are key in extending the primitive verb system via thematic roles to get a very rich semantic systems in the lexicon with little costs in learning.</li>
</ul>
</div>
</div>
</div>
<section id="podcast" class="level2">
<h2 class="anchored" data-anchor-id="podcast">Podcast</h2>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The secret life of pronouns in context
</div>
</div>
<div class="callout-body-container callout-body">
<p>I got interested in this book when I was deep into wikipedia research. I was in a place where I wanted to be able to attribute content to the authors. I had read some disturbing articles with prior work on the topic by some researchers from my Alma Mater. However, I eventually came across this book and it seemed to put the ideas in a better form. Less about ML and more about the type of writers and their personalities.</p>
</div>
</div>
<p>In <span class="citation" data-cites="pennebaker2013secret">(Pennebaker 2013)</span> “The Secret Life of Pronouns: What Our Words Say About Us,” the author explores how the seemingly innocuous function words we use—pronouns, articles, prepositions, and auxiliary verbs—can reveal a great deal about our personalities, emotions, social connections, and psychological states. James W. Pennebaker, the author and a social psychologist, uses computer-based text analysis methods to uncover these hidden patterns.</p>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li><p><strong>Key Questions and Themes:</strong></p>
<ul>
<li><strong>Can language reveal psychological states?</strong> The author is primarily interested in what people’s words say about their psychological states. The book investigates whether language features can reveal how people are thinking and whether this information can be used to change their thinking.</li>
<li><strong>How do function words differ from content words?</strong> The book distinguishes between content and style of speech, arguing that function words, which reflect language style, reveal more about a person than content words. Function words are tied to the relationship between speaker and listener.</li>
<li><strong>Do men and women use words differently?</strong> The book explores gender differences in language use, noting that women are typically more attentive to the thoughts and feelings of others, while men are less so. It also investigates whether different writing styles reflect different personalities.</li>
<li><strong>Can language predict behavior?</strong> The book examines whether language use can predict behaviors such as success in college, risk of suicide, leadership styles, and likelihood of declaring war.</li>
<li><strong>How can language be used as a tool for change?</strong> The author examines whether analyzing words can lead to improvements in relationships, teaching, and leadership. Additionally, the book looks at how writing about emotional upheavals can improve mental and physical health, and whether using certain types of words can influence the effectiveness of that writing.</li>
<li><strong>Can language reveal deception?</strong> The book explores linguistic markers of deception, such as the use of first-person singular pronouns and the mention of other individuals.</li>
<li><strong>Can language analysis help identify authors?</strong> The book presents methods for identifying authors using function words, punctuation, and obscure words.</li>
</ul></li>
<li><p><strong>Main Examples and Studies:</strong></p>
<ul>
<li><strong>Expressive Writing:</strong> Early research focused on the benefits of writing about traumatic experiences, leading to the development of the LIWC (Linguistic Inquiry and Word Count) program. This program helps analyze word categories associated with healthy writing such as positive emotion words, cognitive words, and the ability to construct a story.</li>
<li><strong>The Bottle and the Two People Pictures:</strong> Participants described pictures, and their descriptions were analyzed for function word use and content. For example, people who focused on shadows in the bottle picture tended to be more creative. The second picture revealed common themes such as social connections, clothing, and accessories.</li>
<li><strong>Thinking Styles:</strong> The book describes how people’s function words can be statistically grouped into categories reflecting thinking styles, such as formal, analytic, and narrative. These thinking styles can predict how people relate to others and organize their worlds.</li>
<li><strong>9/11 Blog Analysis:</strong> The language used in blog posts was analyzed before and after the 9/11 attacks. Researchers noted an increase in negative emotion words, a decrease in positive emotion words, and a temporary increase in cognitive words.</li>
<li><strong>College Admissions Essays:</strong> The study examined whether the writing style in college admissions essays could predict college grades.</li>
<li><strong>The Federalist Papers:</strong> The book discusses the use of function words to determine the authorship of the anonymous Federalist Papers.</li>
<li><strong>Language Style Matching (LSM):</strong> LSM is a technique to determine how similarly two people use function words. It is used in studies of couples, speed dating, and even the writings of historical figures to analyze the relationship between the authors.</li>
<li><strong>Obama’s Pronoun Use</strong>: The book analyzes Barack Obama’s use of first-person singular pronouns. His relatively low use of I-words is interpreted as a sign of self-confidence.</li>
</ul></li>
<li><p><strong>Additional Insights:</strong></p>
<ul>
<li><strong>Stealth Words:</strong> The book emphasizes the importance of function words, often called “stealth words,” which are often overlooked.</li>
<li><strong>The Role of Computers:</strong> Computer technology has revolutionized the study of language, allowing researchers to analyze large datasets and uncover hidden patterns that would be impossible to see with manual analysis.</li>
<li><strong>Language as a Tool:</strong> Language can be used not only as a window into the inner workings of people, but also as a tool for guiding thinking and promoting change.</li>
<li><strong>Interdisciplinary Approach</strong>: The author’s work draws on multiple disciplines including psychology, linguistics, computer science, history, and political science.</li>
</ul></li>
</ul>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-pennebaker2013secret" class="csl-entry">
Pennebaker, J. W. 2013. <em>The Secret Life of Pronouns: What Our Words Say about Us</em>. Bloomsbury USA. <a href="https://books.google.co.il/books?id=p9KmCAAAQBAJ">https://books.google.co.il/books?id=p9KmCAAAQBAJ</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {The {Secret} {Life} of {Pronouns} {What} {Our} {Words} {Say}
    {About} {Us}},
  date = {2025-01-30},
  url = {https://orenbochman.github.io/notes-nlp/reviews/book/seret-life-of-pronouns/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“The Secret Life of Pronouns What Our Words
Say About Us.”</span> January 30, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/book/seret-life-of-pronouns/">https://orenbochman.github.io/notes-nlp/reviews/book/seret-life-of-pronouns/</a>.
</div></div></section></div> ]]></description>
  <category>Review</category>
  <category>Book</category>
  <category>NLP</category>
  <category>Sentiment Analysis</category>
  <category>Sentiment Analysis</category>
  <category>Podcast</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/book/seret-life-of-pronouns/</guid>
  <pubDate>Wed, 29 Jan 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>When and Why are Pre-trained Word Embeddings Useful for Neural Machine Translation?</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/2018-PTWM-NMT/</link>
  <description><![CDATA[ 





<div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/o41avjKIg5o" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Talk covering this paper by Roee Aharoni
</figcaption>
</figure>
</div>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/literature-review-open-book.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Litrature review"><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="img-fluid figure-img" alt="Litrature review"></a></p>
<figcaption>Litrature review</figcaption>
</figure>
</div></div><div id="sup-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://roeeaharoni.com/NMT_slides_orginal.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Roee Aharoni’s slides"><embed src="https://roeeaharoni.com/NMT_slides_orginal.pdf" class="img-fluid"></a></p>
<figcaption>Roee Aharoni’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div>
<section id="podcast" class="level2">
<h2 class="anchored" data-anchor-id="podcast">Podcast</h2>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>The performance of Neural Machine Translation (NMT) systems often suffers in lowresource scenarios where sufficiently largescale parallel corpora cannot be obtained. Pretrained word embeddings have proven to be invaluable for improving performance in natural language analysis tasks, which often suffer from paucity of data. However, their utility for NMT has not been extensively explored. In this work, we perform five sets of experiments that analyze when we can expect pre-trained word embeddings to help in NMT tasks. We show that such embeddings can be surprisingly effective in some cases – providing gains of up to 20 BLEU points in the most favorable setting.. –<span class="citation" data-cites="qi-etal-2018-pre">(Qi et al. 2018)</span></p>
</blockquote>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li>**Introduction
<ul>
<li>Describes the problem of low-resource scenarios in Neural Machine Translation (NMT) and the potential utility of pre-trained word embeddings.</li>
<li>Highlights the success of pre-trained embeddings in natural language analysis tasks and the lack of extensive exploration in NMT.</li>
<li>Poses five researcb questions:
<ul>
<li>Q1 Is the behavior of pre-training affected by language families and other linguistic features of source and target languages? (§3)</li>
<li>Q2 Do pre-trained embeddings help more when the size of the training data is small? (§4)</li>
<li>Q3 How much does the similarity of the source and target languages affect the efficacy of using pre-trained embeddings? (§5)</li>
<li>Q4 Is it helpful to align the embedding spaces between the source and target languages? (§6)</li>
<li>Q5 Do pre-trained embeddings help more in multilingual systems as compared to bilingual systems? (§7)</li>
</ul></li>
</ul></li>
<li>Experimental Setup
<ul>
<li>Details the five sets of experiments conducted to evaluate the effectiveness of pre-trained word embeddings in NMT.</li>
<li>Describes the datasets used, including the WMT14 English-German and English-French translation tasks.</li>
<li>Outlines the models and training procedures employed in the experiments.</li>
</ul></li>
<li>Results and Analysis
<ul>
<li>Presents the results of the experiments, showing the impact of pre-trained word embeddings on NMT performance.</li>
<li>Discusses the observed gains in BLEU scores and the factors influencing the effectiveness of pre-trained embeddings.</li>
<li>Analyzes the relationship between the quality of pre-trained embeddings and the performance of NMT systems.</li>
</ul></li>
<li>Analysis
<ul>
<li>Considers the implications of the findings for NMT research and practice.</li>
<li>Discusses the potential benefits and limitations of using pre-trained word embeddings in NMT tasks.</li>
</ul></li>
<li>Conclusion
<ul>
<li>The sweet-spot is wgere there is very little training data yet enough to train the system.</li>
<li>PTWE are more effective if there are more similar translation pairs.</li>
<li>A priori alignment of embeddings may not be necessary in bilingual scenarios, but is helpful in multi-lingual training scenarios.</li>
</ul></li>
</ul>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-qi-etal-2018-pre" class="csl-entry">
Qi, Ye, Devendra Sachan, Matthieu Felix, Sarguna Padmanabhan, and Graham Neubig. 2018. <span>“When and Why Are Pre-Trained Word Embeddings Useful for Neural Machine Translation?”</span> In <em>Proceedings of the 2018 Conference of the North <span>A</span>merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</em>, edited by Marilyn Walker, Heng Ji, and Amanda Stent, 529–35. New Orleans, Louisiana: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/N18-2084">https://doi.org/10.18653/v1/N18-2084</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {When and {Why} Are {Pre-trained} {Word} {Embeddings} {Useful}
    for {Neural} {Machine} {Translation?}},
  date = {2024-02-11},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/2018-PTWM-NMT/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“When and Why Are Pre-Trained Word Embeddings
Useful for Neural Machine Translation?”</span> February 11, 2024. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/2018-PTWM-NMT/">https://orenbochman.github.io/notes-nlp/reviews/paper/2018-PTWM-NMT/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Bidirectional LSTM</category>
  <category>Deep learning</category>
  <category>Embeddings</category>
  <category>LLM</category>
  <category>NLP</category>
  <category>Paper</category>
  <category>Review</category>
  <category>Podcast</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/2018-PTWM-NMT/</guid>
  <pubDate>Sat, 10 Feb 2024 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Neural Machine Translation by Jointly Learning to Align and Translate</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/2014-NMT-by-jointly-learning-to-align-and-translate/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/literature-review-open-book.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Litrature review"><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="img-fluid figure-img" alt="Litrature review"></a></p>
<figcaption>Litrature review</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/o41avjKIg5o" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Roee Aharoni’s Talk covering this paper (Hebrew)
</figcaption>
</figure>
</div><div id="sup-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://roeeaharoni.com/NMT_slides_orginal.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Roee Aharoni’s slides"><embed src="https://roeeaharoni.com/NMT_slides_orginal.pdf" class="img-fluid"></a></p>
<figcaption>Roee Aharoni’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div>

<section id="podcast" class="level2">
<h2 class="anchored" data-anchor-id="podcast">Podcast</h2>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
</section>
<section id="abstract" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<div class="page-columns page-full"><blockquote class="blockquote">
<p><em>Neural machine translation</em> is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of <em>encoder-decoders</em> and consists of an encoder that encodes a source sentence into a <em>fixed-length vector</em><sup>1</sup> from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition. –<span class="citation" data-cites="bahdanau2016neuralmachinetranslationjointly">(Bahdanau, Cho, and Bengio 2016)</span></p>
</blockquote><div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;the encoded state</p></div></div></div>
</section>
<section id="outline" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li>Introduction
<ul>
<li>Positions NMT as a new approach, contrasting it with traditional phrase-based systems.</li>
<li>Describes encoder-decoder models that use a fixed-length vector to encode a source sentence.</li>
<li>Explains that these models are <mark>trained to maximize the probability of a correct translation.</mark></li>
<li>Discusses the <mark>limitation of compressing all source sentence information into <em>a fixed-length vector</em><sup>2</sup>, especially for long sentences.</mark></li>
<li><mark>Introduces the extension of the encoder-decoder model that jointly learns to align and translate.</mark></li>
<li>Emphasizes that the model searches for relevant source positions when generating a target word.</li>
<li>States that the new model encodes the input sentence into a sequence of vectors and adaptively chooses a subset during decoding.</li>
<li>Asserts that the new model performs better with long sentences and that the proposed approach performs better translation compared to the basic encoder-decoder approach.</li>
<li>Notes that the improvement is apparent with longer sentences and that the model achieves comparable performance to a conventional phrase-based system.</li>
<li>Mentions that the model finds linguistically plausible alignments.</li>
</ul></li>
<li>Background: <em>Neural Machine Translation</em> (NMT)
<ul>
<li><mark>Defines translation as <em>maximizing</em> the <strong>conditional probability of a target sentence given a source sentence</strong>.</mark> <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Barg%7D%5C,%5Cmax_%7By%7D%5C,%20%7Bp%7D(y%20%5Cmid%20x)."></li>
<li>Explains that <mark>NMT models learn this conditional distribution using <em>parallel training corpora</em>.</mark></li>
<li>Describes NMT models as consisting of <em>an encoder and a decoder</em>.</li>
<li>Notes that <em>Recurrent Neural Networks</em> (RNNs) have been used for encoding and decoding <em>variable-length sentences</em>.</li>
<li>Points out that NMT has shown promising results, and can achieve state-of-the-art performance.</li>
<li>Notes that adding <em>neural networks</em> to existing systems can boost performance levels.</li>
<li>RNN Encoder-Decoder
<ul>
<li>Describes the <em>RNN Encoder-Decoder framework</em>, where an encoder reads the input sentence into a vector c.</li>
<li>Explains that the <em>encoder</em> uses an RNN to generate <em>a sequence of hidden states</em>, from which the vector c is generated.</li>
<li>Notes that the <em>decoder</em> predicts the next word given the context vector and previously predicted words.</li>
<li>Presents a formula for the conditional probability, which is modeled with an RNN.</li>
</ul></li>
</ul></li>
<li>Learning to Align and Translate
<ul>
<li>Introduces a new architecture for NMT using a bidirectional RNN encoder and a decoder that searches through the source sentence.</li>
<li>Decoder: General Description
<ul>
<li>Presents a new conditional probability conditioned on a distinct context vector for each target word.</li>
<li>Explains that the context vector depends on a sequence of annotations from the encoder.</li>
<li>Defines the context vector as a weighted sum of annotations.</li>
<li>Describes how the weights are computed, using an alignment model.</li>
<li>Emphasizes that the alignment model computes a soft alignment.</li>
<li>Explains the weighted sum of annotations as computing an expected annotation.</li>
<li>States that the probability of the annotation reflects its importance in deciding the next state and generating the target word.</li>
<li>Notes that this implements an attention mechanism in the decoder.</li>
</ul></li>
<li>Encoder: Bidirectional RNN for Annotating Sequences
<ul>
<li>Introduces the use of a bidirectional RNN (BiRNN) to summarize preceding and following words.</li>
<li>Describes the forward and backward RNNs that comprise the BiRNN.</li>
<li>Explains how annotations are created by concatenating forward and backward hidden states.</li>
<li>Notes that the annotation will focus on words around the current word due to the nature of RNNs.</li>
</ul></li>
</ul></li>
<li>Experiment Settings
<ul>
<li>States that the proposed approach is evaluated on English-to-French translation using ACL WMT ’14 bilingual corpora.</li>
<li>Compares the approach with an RNN Encoder-Decoder.</li>
<li>Dataset
<ul>
<li>Details the corpora used, their sizes, and the data selection method used.</li>
<li>Notes that no monolingual data other than the mentioned parallel corpora is used.</li>
<li>Describes how the development and test sets were created.</li>
<li>Details the tokenization and word shortlist used for training the models.</li>
</ul></li>
<li>Models
<ul>
<li>Details the two types of models trained, RNN Encoder-Decoder (RNNencdec) and RNNsearch, and the sentence lengths used.</li>
<li>Describes the hidden units in the encoder and decoder for both models.</li>
<li>Mentions the use of a multilayer network with a maxout hidden layer.</li>
<li>Describes the use of minibatch stochastic gradient descent (SGD) and Adadelta for training, along with the minibatch size and training time.</li>
<li>Explains the use of beam search to find the translation that maximizes the conditional probability.</li>
<li>Refers to the appendices for more details on the architectures and training procedure.</li>
</ul></li>
</ul></li>
<li>Results
<ul>
<li>Quantitative Results
<ul>
<li>Presents translation performance measured in BLEU scores, showing that RNNsearch outperforms RNNencdec in all cases.</li>
<li>Notes that the performance of RNNsearch is comparable to that of the phrase-based system, even without using a separate monolingual corpus.</li>
<li>Shows that RNNencdec’s performance decreases with longer sentences.</li>
<li>Demonstrates that RNNsearch is more robust to sentence length, with no performance drop even with sentences of length 50 or more.</li>
<li>Highlights the superiority of RNNsearch by noting that RNNsearch-30 outperforms RNNencdec-50.</li>
<li>Presents a table of BLEU scores for each model.</li>
</ul></li>
<li>Qualitative Analysis
<ul>
<li>Alignment
<ul>
<li>Explains that the approach offers an intuitive way to inspect the soft alignment between words.</li>
<li>Describes visualizing annotation weights to see which source positions were considered important.</li>
<li>Notes the largely monotonic alignment of words, with strong weights along the diagonal.</li>
<li>Highlights examples of non-trivial alignments and how the model correctly translates phrases.</li>
<li>Explains the strength of soft-alignment using an example of the phrase “the man” being translated to “l’ homme”.</li>
<li>Notes that soft alignment deals naturally with phrases of different lengths.</li>
</ul></li>
<li>Long Sentences
<ul>
<li>Explains that the model does not require encoding a long sentence into a fixed-length vector perfectly.</li>
<li>Provides examples of translations of long sentences, showing that RNNencdec deviates from the source meaning.</li>
<li>Demonstrates that RNNsearch translates long sentences correctly, preserving the whole meaning.</li>
<li>Confirms that RNNsearch enables more reliable translation of long sentences than RNNencdec.</li>
</ul></li>
</ul></li>
</ul></li>
<li>Related Work
<ul>
<li>Learning to Align
<ul>
<li>Mentions a similar alignment approach used in handwriting synthesis.</li>
<li>Notes the key difference: in handwriting synthesis, the modes of the weights of the annotations only move in one direction.</li>
<li>Explains that, in machine translation, reordering is often needed, and the proposed approach computes annotation weight of every source word for each target word.</li>
</ul></li>
<li>Neural Networks for Machine Translation
<ul>
<li>Discusses the history of neural networks in machine translation, from providing single features to existing systems to reranking candidate translations.</li>
<li>Mentions examples of neural networks being used as a sub-component of existing translation systems.</li>
<li>Highlights that the paper focuses on designing a completely new translation system based on neural networks.</li>
</ul></li>
</ul></li>
<li>Conclusion
<ul>
<li>States that the paper proposes a new architecture that extends the basic model by allowing the model to (soft-)search for input words when generating a target word.</li>
<li>Highlights the benefit of freeing the model from encoding the whole sentence into a fixed-length vector.</li>
<li>Notes that <strong>all components are jointly trained towards a better log-probability of producing correct translations</strong>.</li>
<li>Confirms that the proposed RNNsearch outperforms the conventional encoder-decoder model and is more robust to sentence length.</li>
<li>Concludes that the model aligns each target word with the relevant words in the source sentence.</li>
<li>Points out that the proposed approach achieved translation performance comparable to the phrase-based statistical machine translation, despite its recent development.</li>
<li>Suggests that the approach is a promising step toward better machine translation and a better understanding of natural languages.</li>
<li>Identifies better handling of unknown or rare words as a challenge for the future.</li>
</ul></li>
<li>Appendix A: Model Architecture
<ul>
<li>Architectural Choices
<ul>
<li>Describes that the scheme is a general framework where the activation functions of RNNs and the alignment model can be defined.</li>
<li>Recurrent Neural Network
<ul>
<li>Explains the use of the <em>gated hidden unit</em> for the activation function, which is similar to LSTM units.</li>
<li>Provides details and equations for the computation of the RNN state using gated hidden units.</li>
<li>Explains how update and reset gates are computed.</li>
<li>Mentions the use of a multi-layered function with a single hidden layer of maxout units for computing the output probability.</li>
</ul></li>
</ul></li>
<li>Alignment Model
<ul>
<li>Explains the use of a single-layer multilayer perceptron for the alignment model.</li>
<li>Provides an equation describing the model and notes that some values can be pre-computed.</li>
</ul></li>
<li>Detailed Description of the Model
<ul>
<li>Encoder
<ul>
<li>Provides the equations and architecture details of the bidirectional RNN encoder.</li>
</ul></li>
<li>Decoder
<ul>
<li>Provides the equations and architecture details of the decoder with the attention mechanism.</li>
</ul></li>
<li>Model Size
<ul>
<li>Specifies the sizes of hidden layers, word embeddings, and maxout hidden layer.</li>
</ul></li>
</ul></li>
</ul></li>
<li>Appendix B: Training Procedure
<ul>
<li>Parameter Initialization
<ul>
<li>Describes the initialization of various weight matrices, including the use of random orthogonal matrices and Gaussian distributions.</li>
</ul></li>
<li>Training
<ul>
<li>Explains that the training is done with stochastic gradient descent (SGD) with Adadelta to adapt the learning rate.</li>
<li>Describes how the L2-norm of the gradient was normalized and that minibatches of 80 sentences are used.</li>
<li>Mentions sorting the sentence pairs and splitting them into minibatches.</li>
<li>Presents a table of learning statistics and related information.</li>
</ul></li>
</ul></li>
<li>Appendix C: Translations of Long Sentences
<ul>
<li>Presents sample translations generated by RNNenc-50, RNNsearch-50, and Google Translate.</li>
<li>Compares these translations with a reference (gold-standard) translation for each long source sentence.</li>
</ul></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;requiring the length to be fixed seems a mistake as sentences can sometimes get very long think hundreds of words</p></div></div></section>
<section id="reflections" class="level2">
<h2 class="anchored" data-anchor-id="reflections">Reflections</h2>
<p>Naively, translation is an easy task in the sense that we just need to look up each phrase in a statistically generated bi-lingual lexicon and append the translation of the phrase to the target sentence. In reality even when working with parallel text is tricky in that, the translation isn’t one to one but rather is n-m mappings between the source and target words with the number of words being variable. Picking the best entry in the lexicon is not obvious as the source words may be ambiguous and need some attention to other words in the context to disambiguate, and these too may be ambiguous ad infinitum… Finaly the translation may require some reordering of the words in the source sentence to conform to the target language’s grammar.</p>
<p>To sum up:</p>
<ul>
<li>We need to learn a bi-lingual phrase lexicon.</li>
<li>Each phrase may have multiple translations.</li>
<li>To pick the best entry the source context should be consulted.</li>
<li>The target sentence may require reordering to conform to the target language’s grammar.</li>
<li>Another challenge is that the target language may require words or even grammatical constructs e.g.&nbsp;gender and gender agreement that are lacking in the source language. These are <a href="../1997-floating-contraints/index.qmd">floating constraints</a> c.f. (<span class="citation" data-cites="mckeown1997floating">McKeown, Elhadad, and Robin (1997)</span>) that the model needs to propagate through the translation process.</li>
</ul>
<p>Each step of this process is probabilistic and thus prone to mistakes. Though there are two main constructs. The first is the contextual lexicon which needs to be learned using parallel text. The second is the destination grammar that can be learned as part of a language model using monolingual text. However reordering texts isn’t as much of a challenge, if we have sufficient parallel texts then the NMT hidden states should be able to learn the reordering as part of its hidden state.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-bahdanau2016neuralmachinetranslationjointly" class="csl-entry">
Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2016. <span>“Neural Machine Translation by Jointly Learning to Align and Translate.”</span> <a href="https://arxiv.org/abs/1409.0473">https://arxiv.org/abs/1409.0473</a>.
</div>
<div id="ref-mckeown1997floating" class="csl-entry">
McKeown, Kathleen, Michael Elhadad, and Jacques Robin. 1997. <span>“Floating Constraints in Lexical Choice.”</span>
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Neural {Machine} {Translation} by {Jointly} {Learning} to
    {Align} and {Translate}},
  date = {2024-02-11},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/2014-NMT-by-jointly-learning-to-align-and-translate/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Neural Machine Translation by Jointly
Learning to Align and Translate.”</span> February 11, 2024. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/2014-NMT-by-jointly-learning-to-align-and-translate/">https://orenbochman.github.io/notes-nlp/reviews/paper/2014-NMT-by-jointly-learning-to-align-and-translate/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Bidirectional LSTM</category>
  <category>Deep learning</category>
  <category>Embeddings</category>
  <category>LLM</category>
  <category>NLP</category>
  <category>NMT</category>
  <category>Paper</category>
  <category>Review</category>
  <category>Podcast</category>
  <category>Translation task</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/2014-NMT-by-jointly-learning-to-align-and-translate/</guid>
  <pubDate>Sat, 10 Feb 2024 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Syntax and Parsing</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w19-syntax-and-parsing/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/tiling.png" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>course banner</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/f-3N0stPtbw" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Lesson Video
</figcaption>
</figure>
</div><div id="sup-slide-deck1" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div><div id="sup-slide-deck2" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides2.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides2.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;2
</figcaption>
</figure>
</div></div>


<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Obj·ectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Syntax, Major Word Order</li>
<li>Dependency Parsing and Models</li>
<li>Explanation and demo of AutoLex, a system for linguistic discovery</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- Transcript here is machine generated no punctuation and I eddited it a but but it needs more work and I don't see the benefits unless I can run it through some tool. -->
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<blockquote class="blockquote">
<p>This time we’re going to be talking about syntax in general but dependency parsing in particular. and when we talk about linguistic structure and syntax there’s two types of linguistic structure that we talk about.</p>
</blockquote>
<blockquote class="blockquote">
<p>The first one is <strong>dependency structure</strong> which focuses on the relations between words and it looks a little bit like this: For example we have the word saw and the word <code>saw</code> as the root of the sentence and this is connected to other words like <code>i</code> and <code>with</code> and girl</p>
</blockquote>
<blockquote class="blockquote">
<p>We also have <strong>phrase structure</strong> which is focuses on the kind of structure of the sentence as opposed to the relationships between words Both of these are common expressions of syntax or common ways to represent syntax. <mark>In particular phrase structure was widely used in English and you know kind of developed by chomsky and other very influential linguists from 1950s 1960s until now</mark> However recently there’s been a big move towards dependency structure and the reason why is because they’re relatively straightforward to express. And in particularly relatively straightforward to express across a wide variety of languages. and so for example we can do things like say saw is the subject of the sentence so or saw is the root of the sentence and then it has a subject it has a direct object and it has a prepositional phrase and these kinds of things are relatively you know constant across languages maybe not prepositional phrases but you know a phrase like this and it’s particularly good for multilinguality because in some free word order languages it’s also possible to have basically words intervene into a phrase which makes it very difficult to say this is like a particular phrase and what i mean by this is if you have a dependency tree and words cross it’s very hard to come up with an example of this in English. I don’t know if anybody knows one of the top of your head like ellen okay it’s like i went i saw a movie yesterday that was good pt so this is a relatively natural English sentence but actually yesterday yesterday is a child of saw and movie that is a child of movie so you can see the dependency is crossing here basically the only place where we get this in in English is is with adverbs adverbs can be like are basically the only thing with really free word order in English but they break this kind of phrase structure representation because you can’t say that any there’s a consistent phrase here so basically that’s an issue and there are other languages where these are like extremely common where the word order is free for all different kinds of phrases so because of this a lot of syntactic analysis is kind of moving in the direction of using dependencies instead of phrase structure</p>
</blockquote>
</section>
<section id="universal-dependencies" class="level2">
<h2 class="anchored" data-anchor-id="universal-dependencies">Universal Dependencies</h2>
<p>There is an amazing effort called the universal dependencies treebank that gives you a standard format for parse trees in many languages this started out of stanford dependencies and also universal part of speech tags and universal dependencies created by google and the basic motivation for this was they wanted to be able to build like one parser and use it for all of the languages instead of have having 400 parsers for 400 languages so if you come up with something in a unified format that you can that you can parse in process then that makes deploying to many different languages much easier the disadvantage of doing this is that in order to make something universal you have to give up on some language specific details so there are details that don’t easily fit within this universal dependencies format but in most cases they cover you know a lot of the major things that you see in different languages</p>
</section>
<section id="syntax-vs-semantic" class="level2">
<h2 class="anchored" data-anchor-id="syntax-vs-semantic">Syntax vs Semantic</h2>
<p>An other thing that you should know about if you’re considering dependencies or kind of like dependency structure is that there’s actually different types of dependencies – Syntactic and Semantic. And they look very different. <strong>Syntactic dependencies</strong> basically are trying to as closely as possible reflect the phrase structure of the sentence and how sentences are put together So for example in a <em>prepositional phrase</em> the head of the <em>prepositional phrase</em> will be a <em>preposition</em>. Whereas for <strong>semantic dependencies</strong> the head of a prepositional phrase will be like the main <em>noun</em> or <em>verb</em> in that <em>prepositional phrase</em>.</p>
<p>The idea being that syntactic dependencies are good for understanding the structure of the language whereas semantic dependencies might be good, for example, for Question Answering because if you ask a question if you have a sentence like the one i had here “I saw a movie yesterday that was good” or “I saw a movie yesterday at the movie theater” and you want to say where did the person watch the movie and the <em>syntactic dependency</em> <em>movie</em> would be connected to <em>at</em> so you would have to like jump down the tree several nodes wherein <em>semantic dependencies</em> <em>movie</em> would be connected to <em>movie theater</em>. It would have an appropriate label so you could just like look up the edge and connect the two together to answer that question in a single hop instead of multiple hops. This is really important! Aditi might talk about things related to this later and you need to choose which one you want based on which application you’re interested in</p>
</section>
<section id="semantics" class="level2">
<h2 class="anchored" data-anchor-id="semantics">Semantics</h2>
<p>One of the very interesting things about yeah sorry god the last play the the semantics how do we club all the nouns together like we need some external knowledge to understand the semantic of the norm do you mean like a noun phrase or no no just to know just a name of something so like it’s a concept right like so that’s a good question so how how do we understand the semantics of a noun or something so even semantic dependencies this is only talking about the relationship between words in the sentence it’s now telling you about the underlying semantics of the nouns so if you wanted to know about like underlying semantics of a noun you would have to have something else like a semantic like knowledge base or something like this the typical version of this is something like wordnet which basically says well let’s say we have chevrolet chevrolet is a type of car which is a type of vehicle which is a type of you know man-made object which is a type of object or something like this and i think some people might have used wordnet it’s now like lesson style than it was before but it’s it’s basically telling you this information and there’s actually since we’re in a multilingual class i can tell you about something like babelnet which is a multilingual like a multilingual version of wordnet so if i put in [Music] oh i’m sorry i was searching in English no wonder so if i search for this in japanese it can tell you that this word is a concept for like automobile and it’s it has is a has part part of relations and all of these are linked across languages so for example it’s in i can’t find the language link but here yeah here yeah so you can see that it links to car automobile other things like this so it’s all linked together so if you want to specifically talk about semantics of words on their own then you can either use things like this or you can use word embeddings also which give a concept so can’t you get a semantic graph so what word net tells you is it tells you about the semantics of words it doesn’t tell you about the semantics of like words relating to each other so it doesn’t tell you for example that challenge is the object of love it doesn’t tell you that a challenge is being loved which is what this this semantic dependency tells you and then if you go even a little bit farther there’s something called predicate argument analysis or frame semantic parsing or something that gives you an even more abstract version but semantic dependencies are kind of like something related to that so regular regular universal dependencies are semantic sud is syntactic so you should know the difference between them so there’s a lot of cross-lingual differences in structure so we’ve talked about this a lot before like word ordering so we have svo we have hindi which is a verb final and arabic which is verb initial and i got this actually from the pud tree bank which is the parallel universal dependencies treebank it has a whole bunch of translated sentences in different languages along with their dependency trees and the interesting thing is these sentences all i guess mean the same thing hindi speakers can confirm that’s actually the case yes nobody’s saying no so i’ll assume i’ll assume yes but you can see the structure is very different so we have like is and then in English we have we have is is in the middle of the sentence with the noun first and the verb second and then what i can what i can see is we in handy we have an auxiliary verb but then we have a verb here and then we have the object and i guess an oblique indirect object and stuff like this but these all come on the left side of the verb so we can tell that hindi is verb final and then for arabic we can tell that arabic is verb initial and we have a noun here at position sorry a subject and then an oblique here so you can tell the difference in the structure even though i i can’t even read the script in arabic and hindi i can still tell that just by looking at the dependency</p>
</section>
<section id="dependencies" class="level2">
<h2 class="anchored" data-anchor-id="dependencies">Dependencies</h2>
<p>so what can we do with dependencies so i actually previously they were used for feature engineering and systems and they’re still useful in some cases but now our default is just to you know pull up m mbert or xlmr and fine tune it and get reasonable accuracy on a lot of tasks that we care about and in fact you know dependencies are probably captured somewhat to some extent implicitly in these models so why care about syntax i would argue that these are more useful now in human-facing applications and a while ago june 3rd i think last year i actually asked a question on twitter what are convicts are i guess two years ago i asked a question on twitter what are convincing use cases in 2020 for dependencies and i got 39 answers and just to give some examples they still can be useful for incorporating inductive bias into neural models so biasing self-attention to follow syntax or other things like this i think in it still is useful to encourage models to be right for kind of like the right reasons instead of the wrong reasons because this improves model robustness especially out of domain and other things like this and syntactic conductive biases can provide you a way to do this another thing is understanding language structure and this is an example from the aditi’s work which he’s going to present in much more detail in a few minutes so i’ll let her talk to that another very interesting example is searching over parsed corpora so like i talked about before like let’s say we want to find examples with that are talking about x was the founder of y so we want to find it lots of examples of founders of something or other you could try to do this with a regular expression but it’d be pretty tough to come up with a regular expression that gives you this you know with high precision high recall but if instead you find where founder is the verb the subject and it has a subject and an object here then you can just search all examples of this and it actually highlights the appropriate ones here so this is from the spike system created by ai2 and another thing is analysis of other linguistic phenomena or like you know if you want to identify for example when this is coincidentally i actually one one of these is from <a href="https://strubell.github.io/">Emma Strubell</a> you know assistant professor here another one is from <a href="https://maartensap.com/">Martin Sap</a> who will be an assistant professor here I actually made the slides before I knew he was going to be a professor here but but anyway this is examining whether film scripts demonstrate that people have power or agency and analyzing it along the gender of the participants in the film so whether male or female characters are you know like demonstrating more power agency and film scripts and this is used to answer like social and sociologically interesting questions for example and this is made a lot easier by analyzing the syntax because then you can do things like say who did what to whom more easily</p>
</section>
<section id="parsing" class="level2">
<h2 class="anchored" data-anchor-id="parsing">Parsing</h2>
<p>This is kind of a motivation for like what our dependency parses why would you want to be using dependency parts or syntax in general so to talk a little bit more about dependency parsing how would you get these especially in a multilingual fashion parsing is predicting linguistic structure from an input sentence and there’s a couple methods that we use to do this the first one is transition based models and basically the way they work is they step through some steps one by one until we we can turn those steps into a tree another one is graph based models and basically they calculate the probability for each edge in a dependency parse and perform some sort of dynamic programming over them and if you’re familiar with like part of speech tagging from the first assignment transition based models are kind of like a history based model for part of speech tagging and what this would look like is if you if you had like an encoder decoder model where the next tag was always conditioned on the previous tag for graph based models we didn’t really cover crfs here but if you’re familiar with <a href="https://en.wikipedia.org/wiki/Conditional_random_field">CRF</a>s the graph based models are a little bit like these they calculate some scores and then they have a dynamic program to get the best output so just to give a very brief flavor of what these look like</p>
</section>
<section id="shift-reduce" class="level2">
<h2 class="anchored" data-anchor-id="shift-reduce">Shift Reduce</h2>
<p>Shift reduce parsing basically it processes words one by one left to right and it maintains two data structures one is a queue of unprocessed words another is a stack of partially processed words and at each point we choose to either shift moving one word from the queue to the stack reduce left where the top word on the stack becomes the head of the second word or reduce right where the second word on the stack is becomes ahead of the top word and we learn how to choose each of these actions with a classifier so just to give an example we want to parse the sentence i saw a girl so what we do is we first shift and move something sorry this says buffer it’s the same thing as q there’s multiple ways to say this but just think buffer equals q we move one thing from the the queue to the stack we move another word from the queue to the stack and then sorry that’s another typo this should be reduced left and so we reduce left and we get a left arc here then we shift again and then we shift again then we reduce left we reduce right and then we reduce right and then we have a final basically parse tree here so basically what you can see is at each point we choose an action and based on the action we add we either move something from the queue to the stack or we add an arc between the top two things on the stack so you probably won’t need to implement this yourself there are plenty of good dependency parsers out there but just to get an idea of what the algorithm looks like in case you’re interested in doing that</p>
</section>
<section id="classification" class="level2">
<h2 class="anchored" data-anchor-id="classification">Classification</h2>
<p>so the way we do classification for any variety of shift-reduced parsing is basically we take in the stack and the buffer and we need to choose between one of the actions shift left and right so this is a regular classification problem three-way classification problem and we can encode the stack configuration with any variety of recurrent neural network or auto-regressive neural network so one example of this is where we encode basically the stack the previous history of actions and the buffer and feed them into the into the model an even simpler way of doing this is you just encode the words in the input sentence and then have a decoder that generates the actions for you so you could even just throw that into like a regular transformer model and train it as well it would just be you know input is the entire words in the the source sentence and then the output is the sequence of actions so that’s a basic like very quick overview of shift-reduce pricing are there any questions or yeah so you said that we use the dependency passing as an auxiliary fast increase of westminster’s products and inputs yeah so now we have some methods about like adding adapters into xlr to make them more robust for a given language so with adapters do we still need these to increase robustness or if you have a car first and find an adapter and yeah so that’s a really good question if we have something like adapters like multilingual adapters do we still need something like an auxiliary test like dependency parsing and i i would say it’s quite likely that those two things are kind of orthogonal in that adapters are allowing you to more effectively adapt to individual tasks whereas the supervision that you would get from a dependency parsing objective would essentially enforce the model to more strictly follow the syntax of the sentence as linguists kind of describe it so i think that both probably would stack together but i i don’t have empirical results or i i don’t know immediately off the top of my head about a paper that demonstrates maddox yeah so i know medics but i don’t remember if they used like dependency parsing it’s an auxiliary task but they show performance improvement on very low resource languages that xlr is failing and then they used access to improve the performance but i’m not sure whether it was one of the things and so basically the the comment to repeat it for other people who couldn’t hear the so there’s a paper called man x that basically does adapters we talked a little bit about adapters in class i think but basically they demonstrated that it improves on very low resource languages but i it’s not perfect still after using medx right and i think this and that could be combined together probably to improve a bit but as i said i think that’s not the main good use case of dependencies right now i think the better use cases of the dependencies are they give very like intuitive human facing interfaces if you want to do like analysis of corpora or extract detected phenomena on a more holistic level or other things like that so it was actually inspired by one of your tweets you had put a tweet i think a year back that when a person releases the model for 100 languages doesn’t mean it works languages so yeah that changed my perspective that okay it’s not perfect then i kind of searched for these people yeah so i to repeat in case people in the back couldn’t hear i i said something on twitter about a year ago where it’s like when somebody releases a model for 100 languages that doesn’t necessarily mean it works on 100 languages it means it does something on 100 languages probably and it’s probably better than nothing but it’s not perfect for sure so and i’m sure you guys all noticed this as you were implementing your various assignments as well cool.</p>
</section>
<section id="graph-based" class="level2">
<h2 class="anchored" data-anchor-id="graph-based">Graph based</h2>
<p>The other alternative is a graph based parsing and graph based parsing basically what it looks like is we express the sentence as a fully connected directed graph which means that we have all pairs of words as potential candidates for a dependency edge existing between them another way you can think of it is a matrix of scores for each for each edge where the rows are the head and the the columns are the children and the diagonal obviously something can’t be ahead of itself so it would it’s irrelevant but you predict all of the other things there and then after you do that you score each edge independently so you basically calculate the values of that that score matrix and then you have some algorithm that allows you to find the maximal spanning tree which is basically the highest scoring set of edges that form a tree in the form a tree with a root in it</p>
</section>
<section id="graphbased-vs-transitionbased" class="level2">
<h2 class="anchored" data-anchor-id="graphbased-vs-transitionbased">Graphbased vs Transitionbased</h2>
<p>and i have a comparison of graph based versus transition based one ex one advantage of transition based parsing is that it allows you to condition on infinite context basically so it allows you to condition on all of your previous actions so theoretically it has as much expressiveness as you want just like a regular encoder decoder model can do you know can condition on all your previous sections however for transition based parsing greedy search algorithms can cause short-term mistakes to propagate into damaging your long-term performance so you know if you accidentally connect an edge too early there’s no way to recover from it on the other hand graph based parsing you can find the exact best global solution via dynamic programming however you have to make some local independence assumptions so you can’t like necessarily easily condition you know the choice of one edge on whether you chose one edge or at another time so for the dynamic programming</p>
</section>
<section id="dynamic-programming" class="level2">
<h2 class="anchored" data-anchor-id="dynamic-programming">Dynamic Programming</h2>
<p>algorithm i’m not going to go into a lot of detail here because as i said like people are probably not going to be implementing this on your own you’re probably going to be using a parser but basically we have a graph and want to find it spanning trees so what we do is we greedily select the best incoming edge to each node and we subtract its score from all other incoming edges because we want a tree and not anything with any cycles in it what we do is we contract the cycles together into a single node and resolve the cycles within that node and basically we recursively call the algorithm on the graph with the contracted node and then we can expand the contracted node deleting at an edge from the root node there so if you’re more interested in exactly how this works you can look at jaravsky and martin’s textbook on dependency parsing or something like this but basically that’s the general idea for these models basically what we do is we extract features over the whole sequence so we feed in all of the you know all of the words in the input into a feature extractor in this particular in this particular example it says lstm but in reality now it’s xlmr or bert or you know whatever</p>
</section>
<section id="feature-extractor" class="level2">
<h2 class="anchored" data-anchor-id="feature-extractor">Feature Extractor</h2>
<p>Your favorite feature extractor is the classifier that you use to basically apply scores to each of the nodes of the input is a by affine classifier this looks a little bit scary if you look at the equations but it’s actually super simple what you do is you learn specific representations through an mllp for each head word in each dependent word so basically you feed the representations that you get from burke or whatever else into an mlp where you have a separate mlp when you’re considering the word as a head and when you’re considering the word as a child or a dependent and then you calculate the score of each arc where sorry i thought i had an animation here but basically the first part of this here is calculating how likely a child word is to connect to a head word so it’s calculating basically the congruence between a child word and a head word and of course you know this is optimized end to end but it will be considering things like how close together are the words how likely is a child to be a child of a head like so nouns tend to be children of verbs et cetera et cetera determines children of nouns and then in addition it also calculates the score saying how likely is this word to be a head word in the first place so for example determiners are almost never head words in in semantic dependencies and so they’ll get a very low score according to this thing down here whereas verbs are very often head words so we’ll get a high score</p>
</section>
<section id="difficulty" class="level2">
<h2 class="anchored" data-anchor-id="difficulty">Difficulty</h2>
<p>For multi-lingual dependency parsing,the difficulty in multilingual dependency parsing is, that actually <strong>syntactic analysis</strong> is a particularly hard multilingual task. I think it’s harder than named entity recognition; it’s probably harder than part of speech tagging and it might even be on the same level of difficulty as translation. Yeah maybe not maybe not on the same level of difficulty as translation but it’s hard it’s a hard task and the reason why is because it’s on a global level not just a word by word level so you need to consider long distance dependencies within the sentence and syntax varies widely across different languages so like English is very very different from Hindi and both are very very different from Arabic so you can’t like just say you know well like named entities look pretty similar in all languages they all have like low frequency words and is a key that always works for named entities but you can’t do something like that when deciding syntax is easily</p>
</section>
<section id="order-insensitive-encoder" class="level2">
<h2 class="anchored" data-anchor-id="order-insensitive-encoder">Order insensitive encoder</h2>
<p>so there’s a bunch of improvements that people have proposed i had some examples of these and the papers that i suggested for reading for the discussion one example is that people have shown that when you’re transferring to very very different languages you can remove the bias on ordering in encoders so for example if you’re using a transformer based model you can remove the positional encodings so you basically get an order insensitive encoder and that transfers much better to very very different languages so in this case they always use English as the source language and transferred to another language but can you think of any other examples where this might be useful can you think of it can you think of an example of a language where it would be very hard to get another language with similar syntax that’s included in your dependency treatment yeah i mean if you’re studying a low resource language that’s part of a language family that only has other low resource languages yeah exactly so if you’re studying a low resource language with a language family that only has other low research languages i could give an example what about navajo that’s probably the highest resource language in its language family but it’s extremely low resource and i think that’s i think it’s also useful because one of the or one of it’s also important because dependency parsing is particularly useful for things like linguistic inquiry or human facing interfaces when you can’t just train an end to end model easily so you know that’s a particularly salient use case another paper that i</p>
</section>
<section id="dependency-parsing" class="level2">
<h2 class="anchored" data-anchor-id="dependency-parsing">Dependency parsing</h2>
<p>this is one of my papers i like it a lot i i wish more people liked it a lot but but basically the i think it’s a really really cool paper so i i’m trying to sell it to everybody but basically we came up with a generative model for dependency parsing and the idea of the generative model for dependency parsing is that you have a model that jointly generates the dependency tree and the sentence and one of the nice things about generative models is that you can train them unsupervised as well so you can take the unsupervised model and learn the structure that’s used in the unsupervised model together with like generating the sentences on unlabeled data and the reason why i think this is cool is because you can take a model that was trained on English and then train it unsupervised on the language that you’re interested in parsing and it improves the performance by a lot especially if the languages are very unrelated so i think this is another cool tool if you have a language that is from like a different language family for example the input is the dependency tree and your model just generates a random sentence in the target so the input is it’s a generative it’s you can think of it like a language model but it calculates instead of just calculating the probability of the output sentence it calculates the joint probability of the dependency tree and the output sentence so if you’ve given a sentence like it reconstructs the sentence right and it can do other things like it can get the the highest probability dependency tree given a sentence that’s called like latent variable inference and other things like that basically in this case it’s initialization it can be initialization with regularization towards the original parameters as well does that imply supervised data for the chemistry it does not it only it only requires text and basically the way it works is it it you have a dynamic program that finds the dependency tree that iterates over the dependency trees and it optimizes the parameters of like the dependency parser and the output at the same time but the probability of the output will go up basically if you have a more coherent dependency tree for a completely different family yes yeah the different family could have a completely different type of synthetic structure yes exactly yes you read read the paper i’m not lying so that’s a good that’s a good question and that will link it to what aditi is going to talk about in a second too we didn’t we didn’t evaluate it that you know like that extensively so i think that would be a natural interesting next question for any of these improvements that i’m talking about here does do these improvements lead to a more coherent you know grammatical sketch for the language or something like that if we extract the grammatical sketch cool yeah so i’m i’m</p>
</section>
<section id="linguistic-informed-constraints" class="level2">
<h2 class="anchored" data-anchor-id="linguistic-informed-constraints">Linguistic informed constraints</h2>
<p>as i said i i’m excited about that so come talk to me if you if you want to hear more a final thing is linguistically informed constraints so there are big atlases of data about linguistic structures like the world atlas of language structures which i believe ellen or somebody talked about earlier and they they tell you things like is an adjective before a noun and so using this what this paper that i’m introducing here does is they use something called posterior regularization which basically says we’re going to parse our whole corpus we’re going to look at the proportion of the time that an adjective appears before a noun or after a noun and then if our big atlas of language structure says that adjectives should happen before nouns but our dependency parser is putting adjectives after nouns much more often then we are going to decrease the probability of it putting an adjective after the noun and increase the probability of it putting an adjective before the noun so this is a way to introduce basically prior knowledge into the predictor in order to make it work better so these are just three examples of cross-lingual dependency parsing but they demonstrate some you know ways to incorporate intuitions and stuff cool and any other things if not i’ll turn it over to aditi to talk in a bit of detail we might or might not have time for discussion formal discussion but we could have maybe all like class discussion and and it’s the keynote slides yes these are the keywords so can i open the demo on the google chrome this one yes thank you and then yeah that works cool and that’s the mind sure okay hi everyone i’m aditi i’m a phd student working with graham and today i’m going to show you a part of my research where i’ve used these dependency analysis okay so we just saw some because it’s kind of annoying okay so dependency analysis basically told us on a high level how words are related to each other so it’s information is useful but we also need to understand a more complex linguistic phenomena if you truly want to understand the language as a whole so some of these complex linguistic phenomena are like morphological agreement word order case marking suffix usage to name a few and these are important not just for like a language communication or understanding but also has some concrete applications so there are some human-centric or human-facing applications for instance like if you want to like learn the language then you need to know how to arrange these words when does the ordering of the word changes what kind of suffix to use when what happens gender is like for each gender you might have a different word ending and so on another important application which for navajo we also saw was language documentation because languages are getting extinct quite frequently and quite quickly also so language documentation is a way where linguists document the salient grammar points of a language not just for preservation but also as a way to like also create pedagogical resources maybe even create language technologies from that so another kind of application is from machine centric applications some examples that we saw where dependency analysis were used to give inductive bias into models another application is like we sort of used these rules that we extract automatically to evaluate a machine output so often across languages as we saw syntax is quite different so we want to have a way to automatically evaluate how grammatically correct let’s say a machine translation output is so basically to achieve both human centric application and machine centric applications we need to extract rules which explain this phenomena in a format which is both human readable and machine readable and i’m going to quickly explain like how we do this using a process of</p>
</section>
<section id="definition-of-required-agreement" class="level2">
<h2 class="anchored" data-anchor-id="definition-of-required-agreement">Definition of required agreement</h2>
<p>morphological agreement so agreement is a quite complex process wherein basically words in a sentence often change their forms or morphology based on some other words in the same sentence based on some category like gender number and person so i’ll give a quickly an example from number agreement in spanish so here you can see that girl is in singular and the word for verb has also been singular so now if i change the word for girl to be in a plural form then the words form also changes to the plural form so basically any change in the subjects number has to bring about a change in the verbs number so we call this as subject verb required agreement now if you look at the object and the word they are also both in the singular form in the first sentence and the second sentence when the form of the object dog has become plural the form of the word still remains in a singular form so essentially any change in the object’s number is not bringing a change in the verbs number so this is it’s not required agreement so any sort of agreement that we may observe between object and work is purely by chance so we call this as object work chance agreement so to basically understand what are the rules which govern subject agreement and orders or how to require agreement you basically formulate it into a classification task</p>
</section>
<section id="prediction-task" class="level2">
<h2 class="anchored" data-anchor-id="prediction-task">Prediction task</h2>
<p>so the task is here of predicting required agreement versus chance agreement and how do we extract these rules automatically just from the raw text so here i have an example of greek this is a greek sentence and we first automatically perform some syntactic analysis which basically gives us what is the part of speech of each word what is this morphological features what are the dependency links between them now from this syntactic analysis we basically create our training data for this prediction task so here’s an example so on this box here you basically have a dependency link between the determiner and the proper noun now the gender of the determinant and the proper noun are both matching they’re both in feminine gender so basically we can create a training data point saying that proper noun and determiner when they are in a relation then the gender is matching so the agreement value becomes yes now another dependency link here is between the noun and the proper noun now here the gender values are not matching so our data point here becomes that any relation between noun and proper noun in this example the agreement value is known so essentially we are basically creating a binary classification task from this example and we create this data set for all the sentences we then basically learn a model on top of this training data from which we extract rules where the rules are telling us which of these rules are actually leading to a required agreement and which are leading to a chance agreement so essentially this is an example of the model so again going back to the previous slide where i mentioned about human centric applications so we want to understand and extract these rules which are understandable to humans so we want to use a model which is more interpretable so here i have used the model of decision trees the decision trees can exactly tell you what are the features which led to one decision so once we have applied a decision tree style model on our training data it gives us a leaf node the leaf here is inducing a distribution of agreement over these examples the leaf 3 here is showing us that there are 58 000 examples where the gender values were matching and 778 but they were not matching but how do we know whether this distribution is leading to a required agreement or a chance agreement and to automatically extract this label we basically apply a statistical threshold i won’t go into the much details of it but essentially we apply a significance test which tells us whether the observed agreement distribution is significant or not and this can tell us whether the leaf is truly capturing a required agreement or not and this is like an example of the label decision tree where for spanish gender agreement this is the leaf or this is the tree after the leaf labeling stage and the leaf three here has been marked as required agreement so basically from this leaf three we can extract some discrete rules which says okay if determinant and noun are in the following relation then they need to agree on gender so basically from the raw text we started training data over which we train an interpretable model from which we then extracted some discrete and very simple rules and this is just a very basic pipeline and now we are trying to extract this</p>
</section>
<section id="linguistic-questions" class="level2">
<h2 class="anchored" data-anchor-id="linguistic-questions">Linguistic questions</h2>
<p>sort of or apply this pipeline for potentially any linguistic question so we saw this for agreement where our linguistic question was when do syntactic heads show agreement with their dependence on gender number and so on another interesting question was for case marking in case marking we’re interested to know when does a particular class of words like nouns take nominative case over the other so for example in this sentence anna has food anna is in the nominated case but if it becomes anna’s food then ani is a generative base so we want to basically if you are learning this language you need to know that when to add an apostrophe is when you are basically showing a possession another kind of interesting linguistic phenomena is of word order like you need to know how to arrange the words appropriately and even in English typically when we say about word order people just say it’s one word that English follows svo but it’s even within English there are multiple word orders so for instance if if i’m saying English is sorry anna is eating an apple then your word order is simply subject verb and object but if i’m asking a question what is anna eating then these order changes it now becomes object subject and work so if i’m learning English i need to know that when i’m asking a question what is the typical order but if i am just using or saying a decorative sentence what is the what order so essentially for any linguistic question we want to formulate it as a prediction task and learn and extract its rules and this is the final general framework which we have been working on from the raw text you basically extract some</p>
</section>
<section id="general-framework" class="level2">
<h2 class="anchored" data-anchor-id="general-framework">General framework</h2>
<p>features but in our case the features were part of speech tags dependency analysis from which we then extract rules for each of this phenomena and i’ll show you what the rules look like in a minute okay so now what are the kind of syntactic analysis which we can use so graham just showed you the universal dependencies project within that is also the sud tree banks so basically if for a language we have this kind of data available which more or less is annotated by language experts we can directly use these sud treatment as a starting point but for many more languages we even don’t have annotations so in that case your multilingual parsers come into the picture where you can take some raw text first parse it using these models and then apply the same approach okay so this is the toolkit</p>
</section>
<section id="toolkit" class="level2">
<h2 class="anchored" data-anchor-id="toolkit">Toolkit</h2>
<p>which we have developed and okay this basically this is an autolex framework where we have extracted such kind of rules for different linguistic behaviors for a bunch of languages sure so for each language we extract a bunch of features and i’ll just go through some of them here so let’s say we want to explore the spanish agreement features and let’s go into gender so okay so this is exactly the so if you look at the first thing here basically say that okay mostly in spanish gender usually agrees between the determiner and its head but there are some significant number of cases when this does not hold true and some of these cases we have highlighted here that if let’s say a determiner is governed by an adjective in some cases the gender did not agree so we basically extracts rules in a human readable format and further for each of these rules because rules alone can often get quite overwhelming we also show some illustrative examples so here basically if you look here so here the determiner gender is masculine and the adjective gender is feminine so here is one example where although the determiner is governed by an adjective the gender is not matching but then there are some other examples where the determiner’s gender is masculine and the adjectives gender is also masculine here the general values are matching so these are some sort of exceptions to the language general oriented and knowing these exceptions are also important because they do occur quite frequently in the language again we have some rules for word order also so let’s say we want to can go adjective noun so for instance so typically in spanish unlike English most adjectives come after the noun that’s the typical ordering of objectives but there are very specific cases or or very specific adjectives which come before the nouns and the model has correctly identified some of them for instance this rule is telling us that if the adjective has the lima primero then the adjectives come before the noun and again for each of the rules we also ex like show some this illustrative examples where indeed these adjective which has the lima primero is coming before the noun but again language is not that simple there are even exceptions to exceptions so there are again examples which show that even when the lima is primarily there are certain times when this rule is not followed so we are trying to show a more softer view of the rule also that this rule is not 100 applicable every time there are conditions where this rule is not followed so essentially here we have used dependencies as our feature base to explain the rules in a human readable format and i guess one another important aspect here is the quality of the rules also depend a lot on the quality of the underlying analysis so as we improve the multilingual dependency parsing the quality of the rules should also improve so we also applied this system for an endangered language variety called hmong so hmong is spoken in north america also china thailand vietnam and laos and one of its variety is close to endangered and we were trying to we simply had access to a bunch of monk sentences and we wanted to analyze some of his interests and linguistic properties and we had david in lti who also speaks more and knows a lot about it so we basically presented such kind of rules for mong to david and there were a couple of interesting observations first the quality of rules was not as good as what we were seeing for spanish but despite that the model had still been able to identify some cluster of examples which showed rules which david was not aware about so such kind of data-driven approaches is also able to capture or identify some rules which the linguist was not initially aware of so i think dependency analysis is a very useful tool which has a lot of applications especially for exploring a language because there’s so much data out there the dependency analysis helps us find the key components to it so i can go into more detail but these are the main features sure so right now we basically i showed you grammar rules and how it was</p>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>useful from let’s say a linguist or language documentation point of view we’re also now using these tools to actually teach languages to a more less linguistically aware audience because often learners or teachers they don’t go into the linguistic jargon that much but they’re still interested in knowing the grammar rules and especially we are focusing on two languages marathi and kanada these are both indian languages and there are schools in north america where they are teaching these languages to English speakers so a lot of the times there are immigrants who are settled here and they want their children to learn these languages so that so there are English native speakers here and the schools here are interested in teaching them these languages from English so here we basically extract a lot of interesting features so for example for example here we have basically extracted some of the most common words observed in kannada what is the transliteration in English and what are the different kind of forms they are like what are the different forms these lemma have been observed across different genders so this lemma do here i don’t know i’m pronouncing it correctly but you can see here how this lima is used for a masculine gender or how this lima is used in the feminine gender and such kind of tables are pretty common when you are teaching how a word should be used in a language so we are automatically again performing syntactic analysis on these languages extracting such kind of useful insights and then presenting them to actual teachers so that they can check if this material is useful for their teaching process so we are trying to ease the teachers job so that they can focus on the creative aspect of teaching and yeah i guess those are the main points great thank you thank you aditi yeah so i’m sorry we’re right up against the the time so i don’t think we’re gonna have time for discussion this time i really apologize i try not to do this but i packed too much stuff in but i also invited the dt and didn’t want to ever prepare for nothing because of this if you took time to like prepare for the discussion today and read the things if you want to send like a short summary of the things that you prepared we can give like extra credit it won’t be required but we i’ll give like one discussion worth of extra credit for that are there any questions for about the stuff that you was talking about here yeah obviously mentioned like different regions and different species like america are there any like grammatical differences region-wise or are they like that’s a good question so there are these different varieties so we worked so the question was about because hmong is spread across so many different regions are there any difference in the grammatical properties so there are because there are different hmong varieties and we worked on one of the variety which is predominantly spoken in north america so we didn’t have the chance to investigate how the grammar rules changes across these different varieties but the interesting thing so we did another sort of a separate set of experiments where the universal dependencies project they have a lot of free bang for the same language for different domains so there are data from grammar books they have data from news articles and even within the same language across these domains the grammar structure varies quite a lot so in this tool we are basically also offering linguists to like check okay these are the do’s extracted from one domain how do they change in another domain is their model able to account for instances of agreement where there’s not exact matching values for instance if the subject is marked as duo so that would be considered as that if the agreement is not happening because the value is not matching i’m not sure if the universal dependency tree bank actually has that level of detail it might i maybe you have experience with that but i think the universal dependency annotations are actually quite i don’t know coarse so that my level of detail might not show up in the first place but it might yeah that is like an important point that this analysis has been done on one schema the schema used here is the ud schema or the sud schema which is the the we purposefully chose sud or ud because first of all it’s available for a lot more languages and it has a consistent annotation format so the kind of rules we extract now they’re also consistent across languages but that being said it might not consider the language the system is that well that is like a drawback for that but potentially you can apply this pipeline or this model to any other annotation statement it will give you rules according to that schema okay great thanks a lot everyone we can answer other other questions</p>
</section>
</div>
</div>
</div>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li><strong>Introduction to Syntax and Linguistic Structure</strong>
<ul>
<li>Syntax focuses on the structure of sentences and the relationships between words.</li>
<li>Two main types of linguistic structure:
<ul>
<li><strong>Dependency structure</strong>: Focuses on the relations between words.</li>
<li><strong>Phrase structure</strong>: Focuses on the structure of the sentence.</li>
</ul></li>
<li><mark>Dependency structures are often more straightforward to express across various languages and are thus useful in multilingual applications.</mark></li>
</ul></li>
<li><strong>Dependency Parsing</strong>
<ul>
<li>Dependency parsing involves predicting linguistic structure from an input sentence.</li>
<li><strong>Different types of dependencies</strong>:
<ul>
<li><strong>Syntactic dependencies</strong>: Reflect the phrase structure of the sentence.</li>
<li><strong>Semantic dependencies</strong>: Useful for applications like question answering by connecting semantically related words.</li>
</ul></li>
</ul></li>
<li><strong>Universal Dependencies Treebank</strong>
<ul>
<li><mark>A standard format for parse trees across many languages, facilitating the development of parsers that can be used for multiple languages.</mark></li>
<li>Allows for a unified format that simplifies processing across different languages.</li>
<li><mark>To achieve universality, it sometimes requires sacrificing language-specific details.</mark></li>
</ul></li>
<li><strong>Cross-Lingual Differences in Structure</strong>
<ul>
<li>Languages can vary significantly in word order (e.g., SVO, verb-final, verb-initial).</li>
<li>Dependency trees can visually highlight these structural differences, even without understanding the specific language.</li>
</ul></li>
<li><strong>Dependency Parsing Methods</strong>
<ul>
<li><mark><strong>Transition-based models</strong>: Step through actions to build a tree. They use a queue of unprocessed words and a stack of partially processed words, choosing actions like shift and reduce.</mark></li>
<li><mark><strong>Graph-based models</strong>: Calculate the probability of each edge and use dynamic programming to find the best tree. They express a sentence as a fully connected directed graph and find the maximal spanning tree.</mark></li>
</ul></li>
<li><strong>Applications of Dependency Parsing</strong>
<ul>
<li><strong>Human-facing applications</strong>: Useful for analyzing corpora, understanding language structure, and other linguistic phenomena.</li>
<li><strong>Adding inductive bias to neural models</strong>: Improving model robustness and encouraging models to be correct for the right reasons.</li>
<li><strong>Searching parsed corpora</strong>: Finding examples based on syntactic relationships.</li>
<li><strong>Analysis of linguistic phenomena</strong>: Examining power dynamics or other sociological questions in film scripts.</li>
<li><strong>Language learning and documentation</strong>: Extracting morphological agreement rules and documenting salient grammar points.</li>
<li><strong>Evaluating machine output</strong>: Assessing the grammatical correctness of machine translation outputs.</li>
</ul></li>
<li><strong>Multilingual Dependency Parsing</strong>
<ul>
<li><mark>Syntactic analysis is a challenging multilingual task due to global-level considerations and wide syntax variations.</mark></li>
<li><strong>Techniques to improve cross-lingual transfer</strong>:
<ul>
<li>Removing bias on ordering in encoders.</li>
<li>Using generative models for unsupervised training.</li>
<li>Applying linguistically informed constraints.</li>
</ul></li>
</ul></li>
<li><strong>Extracting Linguistic Insights Automatically</strong>
<ul>
<li><strong>Goal</strong>: To extract rules that explain linguistic phenomena in a human-readable and machine-readable format.</li>
<li><strong>Method</strong>: Formulate linguistic questions as prediction tasks and extract rules from raw text using syntactic analysis.</li>
<li><strong>Example</strong>: Extracting morphological agreement rules by predicting required agreement versus chance agreement.</li>
<li><strong>General Framework</strong>: Extract features (POS tags, dependency parses) from raw text, then extract rules for agreement, case marking, word order, etc..</li>
</ul></li>
<li><strong>Practical uses of automatically extracted rules</strong>
<ul>
<li>Language documentation and preservation.</li>
<li>Creating pedagogical resources and language technologies.</li>
<li>Aiding language learners and teachers by presenting grammar rules and common word usages.</li>
<li>Evaluating and correcting machine-generated text.</li>
</ul></li>
</ul>
</section>
<section id="reflection" class="level2">
<h2 class="anchored" data-anchor-id="reflection">Reflection</h2>
<p>Morphological agreements discussion by Aditi is of particular interest.</p>
<p>It seems that understanding agreement is central to ‘second order’ model that need to beat the <strong>baseline</strong>. To get this to happen in an interpretable form we may want to train an attention head that is looking at thing that are in agreement and responding to the feature of agreement and disagreement as both are signals of some value.</p>
<p>I belief that we can learn a probabilistic model of agreement between different cases but that they may be affected by all sorts of constraints like clause and phrase boundaries and the degree of morphological markings. So this is why attention seems so appropriate as it can learn what to focus on. And what we want is to make it interpretable so we can also extract the rules that it has learned!</p>
<p>Aditi talks about <strong>require agreement</strong> v.s. <strong>chance agreement</strong> for verb-subject and verb-object. I think that we may want to have a theory that looks at agreement in winder context and create a model that can predict agreement in a wider context so that we can consider the entropy of a sentence once agreement has been accounted for. In fact my thinking is that we may want to go further and determine the functional load of the agreement and have a more redined metric that can be used to evaluate the entropy of a sentence accounting to agreement in a functional context. (Error correction ambiguity, Reducing cognitive load, Reducing ambiguity, Parsing)</p>
</section>
<section id="navajo-in-10" class="level2">
<h2 class="anchored" data-anchor-id="navajo-in-10">Navajo in 10</h2>
<ul>
<li><p><strong>Language Name and Classification</strong>:</p>
<ul>
<li><strong>Navajo</strong> (also known as <strong>Navaho</strong>).</li>
<li>Navajo: <em>Diné bizaad</em> [tìnépìz̥ɑ̀ːt] or <em>Naabeehó bizaad</em> [nɑ̀ːpèːhópìz̥ɑ̀ːt].</li>
<li>It is a <strong>Southern Athabaskan language</strong> of the <strong>Na-Dené family</strong>.</li>
</ul></li>
<li><p><strong>Speakers and Location</strong>:</p>
<ul>
<li>Spoken primarily in the <strong>Southwestern United States</strong>, especially in the <strong>Navajo Nation</strong>.</li>
<li>One of the most widely spoken <strong>Native American languages</strong>.</li>
<li>The most widely spoken Native American language north of the <strong>Mexico–United States border</strong>.</li>
<li>Almost <strong>170,000 Americans</strong> speaking Navajo at home as of 2011.</li>
</ul></li>
<li><p><strong>Nomenclature</strong>:</p>
<ul>
<li>The word <em>Navajo</em> is an <strong>exonym</strong> from the Tewa word <em>Navahu</em>, meaning ‘large field’.</li>
<li>The Navajo refer to themselves as the <em>Diné</em> (‘People’), with their language known as <em>Diné bizaad</em> (‘People’s language’) or <em>Naabeehó bizaad</em>.</li>
</ul></li>
<li><p><strong>Official Status</strong>:</p>
<ul>
<li>Official language in <strong>Navajo Nation</strong>.</li>
</ul></li>
<li><p><strong>History and Development</strong>:</p>
<ul>
<li>The Apachean languages, of which Navajo is one, are thought to have arrived in the American Southwest from the north by 1500.</li>
<li>Speakers of the Navajo language were employed as <strong>Navajo code talkers</strong> during World Wars I and II.</li>
<li>Orthography developed in the late 1930s and is based on the <strong>Latin script</strong>.</li>
</ul></li>
<li><p><strong>Writing System</strong>:</p>
<ul>
<li>Based on the <strong>Latin script</strong>.</li>
<li>Developed between 1935 and 1940.</li>
<li>Uses an apostrophe to mark <strong>ejective consonants</strong> and mid-word or final <strong>glottal stops</strong>.</li>
<li>Represents nasalized vowels with an <strong>ogonek</strong> and the voiceless alveolar lateral fricative with a <strong>barred L</strong>.</li>
</ul></li>
<li><p><strong>Phonology</strong>:</p>
<ul>
<li>Has a fairly large <strong>consonant inventory</strong>.</li>
<li><strong>Stop consonants</strong> exist in three laryngeal forms: aspirated, unaspirated, and ejective.</li>
<li>Has a simple <strong>glottal stop</strong> used after vowels.</li>
<li>Four <strong>vowel qualities</strong>: /a/, /e/, /i/, and /o/.</li>
<li>Each vowel exists in both <strong>oral and nasalized</strong> forms and can be either <strong>short or long</strong>.</li>
<li>Distinguishes for <strong>tone</strong> between high and low.</li>
</ul>
<p><img src="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w19-syntax-and-parsing/navajo-consonants.png" class="img-fluid" width="400" alt="Navajo Consonants"> <img src="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w19-syntax-and-parsing/navajo-vowels.png" class="img-fluid" width="400" alt="Navajo Vowels"></p></li>
<li><p><strong>Grammar</strong>:</p>
<ul>
<li>Relies heavily on <strong>affixes</strong>, mainly prefixes.</li>
<li>Affixes are joined in unpredictable, overlapping ways that make them difficult to segment.</li>
<li>Basic word order is <strong>subject–object–verb</strong>.</li>
<li>Verbs are conjugated for <strong>aspect and mood</strong>.</li>
</ul></li>
<li><p><strong>Vocabulary</strong>:</p>
<ul>
<li>Most Navajo vocabulary is of <strong>Athabaskan origin</strong>.</li>
<li>Has been conservative with <strong>loanwords</strong> due to its highly complex noun morphology.</li>
<li>Expanded its vocabulary to include Western technological and cultural terms through <strong>calques and Navajo descriptive terms</strong>.</li>
</ul></li>
<li><p><strong>Revitalization and Current Status</strong>:</p>
<ul>
<li><strong>Bilingual Education Act</strong> in 1968 provided funds for educating young students who are not native English speakers.</li>
<li>Navajo Nation Council decreed in 1984 that the Navajo language would be available and comprehensive for students of all grade levels in schools of the Navajo Nation.</li>
<li><strong>Navajo-immersion programs</strong> have cropped up across the Navajo Nation.</li>
<li>Diné College offers an associate degree in the subject of Navajo.</li>
<li>In December 2024, Navajo Nation President made Navajo language the official language of Navajo Nation.</li>
</ul></li>
</ul>
</section>
<section id="hard-to-parse-in-english" class="level2">
<h2 class="anchored" data-anchor-id="hard-to-parse-in-english">Hard to parse in English</h2>
<blockquote class="blockquote">
<p>Alice drove down the street in her car</p>
<p>– prepositional phrase attachment ambiguity</p>
</blockquote>
<blockquote class="blockquote">
<p>time flies like an arrow. Fruit flies like bananas</p>
<p>– polysemic ambiguity &amp; garden path sentences</p>
</blockquote>
<blockquote class="blockquote">
<p>I drove my car to the hospital in town on Saturday</p>
<p>– Linear projection in English</p>
</blockquote>
<blockquote class="blockquote">
<p>You cannot add flavour to a bean that isn’t there</p>
<p>– Non-linear projection in English</p>
</blockquote>
<blockquote class="blockquote">
<p>Alex went to Sam’s house, where he told her that they would miss his show.</p>
<p>– Coreference resolution ambiguity</p>
</blockquote>
<blockquote class="blockquote">
<p>I saw an elephant yesterday in my pajamas</p>
<p>– Non Projectives in English</p>
</blockquote>
<blockquote class="blockquote">
<p>Bill loves and mary hates soccer</p>
<p>– Non Projectives in English</p>
</blockquote>
</section>
<section id="papers" class="level2">
<h2 class="anchored" data-anchor-id="papers">Papers</h2>
<ul>
<li><ol class="example" type="1">
<li><a href="https://aclanthology.org/2020.emnlp-main.422/">Automatic Extraction of Rules Governing Morphological Agreement</a> EMNLP 2020. <a href="https://slideslive.com/38939038/automatic-extraction-of-rules-governing-morphological-agreement">video</a></li>
</ol>
<ul>
<li>This paper is related to extracting morphological agreement rules using dependency relations.</li>
</ul></li>
<li><ol start="2" class="example" type="1">
<li><a href="https://arxiv.org/abs/1811.00570">On difficulties of cross-lingual transfer with order differences: A case study on dependency parsing</a></li>
</ol>
<ul>
<li>This paper discusses challenges in cross-lingual transfer due to word order differences.</li>
</ul></li>
<li><ol start="3" class="example" type="1">
<li><a href="https://arxiv.org/abs/1906.02656">Cross-lingual syntactic transfer through unsupervised adaptation of invertible projections</a></li>
</ol>
<ul>
<li>This paper is about cross-lingual syntactic transfer using unsupervised adaptation of invertible projections.</li>
</ul></li>
<li><ol start="4" class="example" type="1">
<li><a href="https://arxiv.org/abs/1909.01482">Target language-aware constrained inference for cross-lingual dependency parsing</a>.”</li>
</ol>
<ul>
<li>This paper focuses on target language-aware constrained inference for cross-lingual dependency parsing.</li>
</ul></li>
<li><ol start="5" class="example" type="1">
<li><a href="https://github.com/jungyeul/chu-liu-1965/tree/main?tab=readme-ov-file">On the Shortest Arborescence of a Directed Graph</a></li>
</ol></li>
<li><ol start="6" class="example" type="1">
<li><a href="https://nvlpubs.nist.gov/nistpubs/jres/71B/jresv71Bn4p233_A1b.pdf">Optimum Branchings*</a></li>
</ol>
<ul>
<li>These papers are related to the <a href="https://en.wikipedia.org/wiki/Edmonds%27_algorithm">Chu-Liu-Edmonds algorithm</a>.</li>
</ul></li>
<li><span class="citation" data-cites="kiperwasser2016simpleaccuratedependencyparsing">Kiperwasser and Goldberg (2016)</span> <a href="https://arxiv.org/abs/1603.04351">Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations</a>
<ul>
<li>This paper concerns Sequence Model Feature Extractors.</li>
</ul></li>
<li><ol start="7" class="example" type="1">
<li><a href="https://arxiv.org/abs/1611.01734">Deep Biaffine Attention for Neural Dependency Parsing</a>**</li>
</ol>
<ul>
<li>This paper discusses the BiAffine Classifier.</li>
</ul></li>
<li><span class="citation" data-cites="yamada-matsumoto-2003-statistical">Yamada and Matsumoto (2003)</span> <a href="https://aclanthology.org/W03-3023/">Statistical Dependency Analysis with Support Vector Machines</a>
<ul>
<li>These paper describes Arc Standard Shift-Reduce Parsing.</li>
</ul></li>
<li><ol start="8" class="example" type="1">
<li><a href="https://aclanthology.org/W03-3017/">An Efficient Algorithm for Projective Dependency Parsing</a>
<ul>
<li>These paper describes Arc Standard Shift-Reduce Parsing.</li>
</ul></li>
</ol></li>
<li><ol start="9" class="example" type="1">
<li><a href="https://arxiv.org/abs/2203.13901">AutoLEX: An Automatic Framework for Linguistic Exploration</a> <a href="https://aditi138.github.io/auto-lex-learn/index.html">project</a></li>
</ol></li>
<li><ol start="10" class="example" type="1">
<li><a href="https://aclanthology.org/2021.emnlp-main.570/">Evaluating the Morphosyntactic Well-formedness of Generated Texts</a> <a href="https://aclanthology.org/2021.emnlp-main.570.mp4">video</a></li>
</ol></li>
<li><ol start="11" class="example" type="1">
<li><a href="https://aclanthology.org/P17-2090/">Data augmentation for low resource neural machine translation</a></li>
</ol></li>
<li><ol start="12" class="example" type="1">
<li><a href="https://aclanthology.org/D19-1143/">Handling syntactic divergence and low resource translation</a></li>
</ol></li>
</ul>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-kiperwasser2016simpleaccuratedependencyparsing" class="csl-entry">
Kiperwasser, Eliyahu, and Yoav Goldberg. 2016. <span>“Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations.”</span> <a href="https://arxiv.org/abs/1603.04351">https://arxiv.org/abs/1603.04351</a>.
</div>
<div id="ref-yamada-matsumoto-2003-statistical" class="csl-entry">
Yamada, Hiroyasu, and Yuji Matsumoto. 2003. <span>“Statistical Dependency Analysis with Support Vector Machines.”</span> In <em>Proceedings of the Eighth International Conference on Parsing Technologies</em>, 195–206. Nancy, France. <a href="https://aclanthology.org/W03-3023/">https://aclanthology.org/W03-3023/</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Syntax and {Parsing}},
  date = {2022-03-29},
  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w19-syntax-and-parsing/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Syntax and Parsing.”</span> March 29, 2022.
<a href="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w19-syntax-and-parsing/">https://orenbochman.github.io/notes-nlp/notes/cs11-737-w19-syntax-and-parsing/</a>.
</div></div></section></div> ]]></description>
  <category>Syntax</category>
  <category>Dependency parsing</category>
  <category>Multilingual NLP</category>
  <category>NLP</category>
  <category>Notes</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w19-syntax-and-parsing/</guid>
  <pubDate>Mon, 28 Mar 2022 21:00:00 GMT</pubDate>
</item>
<item>
  <title>Automatic Speech Recognition</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w14-ASR/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/tiling.png" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>course banner</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/OObrN8yMYZU" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Lesson Video
</figcaption>
</figure>
</div><div id="sup-slide-deck" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div>

<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>What is speech?</li>
<li>Speech applications</li>
<li>Speech databases</li>
<li>Speech hierarchy</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- Transcript here is machine generated no punctuation and I eddited it a but but it needs more work and I dont see the benefits unless I can run it through some tool. -->
<blockquote class="blockquote">
<p>okay so today i’d like to talk about uh automatic speech recognition uh this is uh one of the uh most uh the active uh research area in speech processing and also very important application for multilingual energy and today i will first explain about the speech recognition demonstration uh and also talking about evaluation metrics and the i will a little bit use the mass to explain about the formulation of the speech recognition and they will move to the explanation about the standard a speech recognition pipeline so uh let me first try to work on the demonstration um which i just tried to kind of use speech recognition here i want to go to the cmu campus affect right the technology and the i also want to show how the current sr technology is robust i want to go to the cmu campus oh it doesn’t work if i can work speak slowly i want to go to the cma campus that’s the current technology next i mentioned that the speech recognition is not a good for the noisy environment so let’s try to make the noisy environment i want to go to the cmu campus they are cool right yeah they are very very cool um so i try to kind of uh make some kind of a mistake uh but the google guides are doing a great job so it’s actually not easy to find some kind of a significant mistake with them one of the easy mistake that i can uh that try to make could be the the other query okay right yeah they should you know study more about me i will talk to my colleagues in google and the the other difficulty would be let me try so i mentioned that the speech recognition is not very good under the uh the simple noise it’s actually quite strong but if there are some uh the other speakers are speaking it’s still very challenging so i try to make this kind of situation okay so do you have you have a long like it’s not not if there’s scabbards raised in new york but i guess up there you all don’t have too long where we’re going i want to go to the cmu campus i want to go to the cmu campus but it’s actually not working quite well in general so um this is a current technology uh with some more other essays with the controlled environment the speech recognition is quite working well and today’s talk uh i will talk about the introduce about the speech recognition technology insider okay so uh the the first example i already mentioned this is one of the mistake that i found when i use acidity uh and yeah sometimes very good but sometimes it’s not working and the uh what kind of errors the speech equation has would be today’s uh discussion so please discuss it in the rest of the class after my lecture and the the now the many of the speech recognition engines actually also support the the other languages and i also uh try the uh japanese and it’s working quite well uh it i tried several times and then i found this kind of uh the the mistaken mistake but in general uh other uh language are also working quite well now and now i will uh move to the uh evaluation as you can see sometimes it’s perfectly revoking but sometimes it’s not working right and then the uh how to evaluate the major uh in speech recognition is quite important and the first uh the uh most intuitive measure would be the sentence level error anyway you know the the sentence uh uh correct or not i think it is the most other important measure i would say uh however uh by measuring the performance of the system in this case it’s almost correct right it’s just there are a few words are uh the wrong so i we actually want to give some partial score uh if the half of the rods are correct so to do that uh instead of using the sentence error rate uh we usually use the water rate and whatever it is computed by considering the three types of the error one is the insertion errors in this case uh this uh word is actually uh the the extra word compared from the difference so this uh the word error correspond to the insertion error and then the rest of the uh the error is the correspond to the substitution errors by the way it’s it is not unique of course you know uh we can consider this substitution uh this one is a substitution and the decision that other insertion uh and so on and usually we uh consider all the possible other error and then taking the minimum by also considering the each other cost of each error and then compute the edit distance i think many people here would be familiar with the distance so i may not need to explain about it indeed so much detail but anyway this uh is our kind of unusual uh uh measure to uh check the performance of the speech recognition and the um so this is a quite well defined major when we have a wide boundary but as you can see we have a lot of work uh the languages which actually don’t have a word boundary right and how to compute it there are uh two ways one is just using some uh the uh the tools to segment uh to get the gun in the world are you need and then other computing the model rate but we should be very careful about that because this chunking may depend on the tool or dictionary and so on i had a very bad experience that i had compared my japanese system and other other institute other systems and i are actually quite bad and i was very disappointed but i couldn’t reach to their numbers and finally we asked them how to chunk their word and their tools are different from ours and we actually cannot compare right so it is the other issue when we using the tanking but still uh if we use the same chunking we can compare so this is widely used but the mostly used uh the metallic for such kind of languages is actually character uh error rate just consider the the each character and the other computer uh edit distance so this is uh the more widely used when we uh the compute measure for the languages that doesn’t have hardware what boundary so the uh the assignment three that first you guys speak the uh the language right and then do the asl and do the evaluation be careful about the uh the which measure are are you guys using if the that language has a good word boundary you guys can use what uh error rate but otherwise a character error is safe yes when you’re speaking in japanese do you expect the system to transcribe the world cmu enrollment yes actually yes yeah the since uh it’s a kind of our other already the training data and so are mixed so other it’s uh the appeared other other the roman character uh in the japanese cases yeah japanese cases we have a ton of data so actually this kind of mix mixture will be handled by data delivery manner however are they that i think in some sense you’re right uh for example this cmu which can be also are the written other uh the katakanas sim the script which is the other or the japanese script to the described the word from the foreign countries and this may also happen instead of this kind of roman character happens and then in this case uh the other letter or character both cases it was correct but it’s not regarded as the wrong a mistake so actually uh in this sense the uh is depend on the language but the in some languages the same the world can be uh represented as a different script and then the uh the water right or even characteristic is not the perfect measure but this is a very difficult problem and usually we the normalize to either of the symbols or we actually don’t care so much about that and there are a lot of other metrics um since the the uh the script has an issue like as i mentioned some people use their phoneme literally but this actually lose the semantic other information a lot so uh this is uh used uh to more to uh that check the whether this uh the uh speech is acoustically phonetically uh the correct but it’s not sure it is the semantically uh the correct so it’s not so much used but anyway this is also another major prime error rate is also sometimes used but i skipped the other explanation and the other metrics decently these two uh the metrics are quite important the airtime factor in the latency since uh many of the systems are now in the uh the on the device and so on and the speech uh is used at the interface whatever speech has another application you know the offline archive uh and so on we don’t care about the uh at least latency a real-time factor other smaller may be better but the latest is quite important if you’re using a speech interface and the it’s also depend on the downstream uh evaluation metrics for example uh the if we combine this one uh with the uh the understanding system or if we combine it with the translation system uh of course we should evaluate it with such kind of downstream uh the evaluation method so in the speech transformation cases of of course it can be better if specification performance is better but we just check the group and the uh for speech recognition are the this uh the especially the characteristic of what error rate i recommend you to use the nist speech recognition scoring toolkit how many people knows nist nist is uh the government institute to standardize uh our uh the daily life activities uh including the uh the for example the um atomic clock and that was also uh the uh the standardized by the nist and the nist is actually uh helping us our community to standardize uh such kind of evaluation metric uh in speech and energy processing so uh and then of course if you know we use a different evaluation tools measures and then if this kind of result is different we cannot compare your result with others right so due to this reason i actually recommend you to use the sctk or other standard added the standardized or are the widely used uh the evaluation error evaluation uh metric uh then you know some tools that they did someone just made it to avoid to hover some uh the confusions again if the uh variation metric tools are different it is disaster right okay so uh with this kind of a great help from the uh the nist and so on uh the speech recognition actually uh was well measured in terms of the uh the the water rate or crop uh the kerati so everyone can for example either compare the performance the others are difficult right and they in the other previous lectures graham also discussed about the uh the blue or other evaluation metrics emerging translation and there are some discussions whether patreon is better or not while they’re at the characteristic there’s not so much discussion mostly it’s correct i say there are some kind of exceptions that i mentioned before but it’s mostly correct and also nice part is that this has a very high correlation with the downstream tasks better weather are definitely better in the speech translation right so anyway due to that study and also they start their needs are providing such kind of toolkit so that everyone can compare the error rate uh strictly so uh this is the one reason that the the speech recognition uh that has been studied for a long time with a common benchmark so now that everyone every field we have our other common benchmark and then you know performance major and readable and so on right speech condition actually has a long history of uh other comparing the techniques based on the shared uh the data are the same other evaluation metric and this is a kind of uh the the figure that i often show in this lecture uh this is a switchboard task and as i as i mentioned every month since 1990s up to now 20 to 30 years we still using this data and we still using the same vibration metric so that we can track the performance uh the improvement so this is possibly the one reason that actually deep running uh has been applied to many areas but the speech is actually one of the first area that deep running is applied since uh we can easily uh also fairly show the performance improvement uh based on this other evaluation measure okay uh so yes this area right whether this area is the the what when other i started speed recognition this area is one of the core data so are the any techniques that cannot improve the performance we also cannot uh the the the other further uh the the scale the training data and so on and also the uh uh the the budget big project uh the the uh kind of the the uh in this era for speech recognition uh anyway uh this is the kind of one of the other cold uh age in speech recognition it’s some slight improvement happens like a discriminative training and so on but uh before deep learning and this i i’ll say my most kind of a speech equation history by the way but i when i went to the conference we always talking about you know the method of a get a 0.3 percent well there are the improvement wow that’s a very good psychic only 2.3 or something like that yeah but the uh due to the deep running that’s kind of our are the uh the how they say we saw that this is the strings there and they cannot release to the human performance but thanks to the deep running and the computational uh the the uh the breakthrough gpu uh gpgpu and the strobe uh and also their open source or other other people’s uh did uh the knowledge uh sharing uh the now uh makes the performance to be included better and better okay so one more thing some people say speech recognition this search is easy this is because we have evaluation metric so the other areas it’s not easy because we have to start to make a variation method by ourselves or you know there’s multiple evaluation metric and we have to pick or something like that so instead speech condition is actually regarded as an easy research topic in terms of that we have a fixed evaluation metric so if you guys have a fancy neural network and then get the improvement by one person you can write the paper okay so uh i will move to the uh the uh speech recognition uh uh the oh yeah i need to kind of swap that a little bit using a mathematical formulation of uh speech recognition so first uh speech recognition as i mentioned in the yeah the the several times uh it’s quite uh interesting combustion problem input is completely physical signal wave pressure sound pressure right that’s undergoes the linguistic symbol so physical one becomes the linguistic uh symbol it’s very different right and the input other characteristics is also very different the waveform is gesture other than one sentence it can be like a two to three seconds and if with a final at the 16 kilohertz sampling grade the length will be the the order of ten thousand if we you use a short term fourier transform and other speech features it goes to the hundred dollar but still the bit long hundreds of thousands that only goes to the uh the three lengths of the symbols so uh a word uh in the vocabulary so this other conversion uh the from input and output is the i’ll say quite different and this actually makes the problem quite difficult okay so now i try to kind of explain that how other the speech function is realized one by one so the google uh the demonstration you guys just see that this is a one box right but it’s actually inside there are several books first one is the feature extraction this is i think i don’t have to mention about that so much about it any of the pattern recognition machine learning problem we first have our study the the feature extraction right and in speech anyway a waveform is not easy to deal with so instead we’re using the feature extraction called the uh mhcc i have a two more slides explaining this each of the modules so i will a bit more detail about it but anyway from now on i will start from the feature which is you know continuous vector time series of the continuous vector and then add a mapping to the word sequence how to formulate it one way is we just add hopefully uh making it a regression problem but instead the uh the people uh actually uh the the the formulate this problem uh more mathematically rigorous other ways they started to uh formulate this problem as a map decision theory here the posterior is from pw given node so the posterior probability of the word sequence are given the observation and then among all the kind of word sequence we just try to pick up the most likely uh sequence that’s because it becomes a speech condition uh quite simple right and the the the problem issue is how to obtain this pw given row so this is the kind of others the the speed recognition uh problem that we usually use for the probabilistic formulation and the just couple of the rules that we usually use i yeah if i have time i will explain bit more carefully about this one but anyway i just want to mention that why people using a probabilistic formulation there are a lot of reason but one of the reason is that we don’t actually have to remember various kind of equations we just have a three equation product rule sum rule conditional independence assumption conditional independence assumption is not rule but maybe just including this vector as a rule but by only using these three uh basic uh uh probabilistic rules we can actually other make this uh p double given no problem bit more tractable so the first thing that the people may often see for the speech recognition is to use a base rule to change this pw given o to p o w put p or given w p w divided by p o and then since the p o is not uh the uh depending on the w we actually uh the uh use the p o dot given w and the w so uh the two uh do to derive this one uh which rule other did we use from here to here mainly so yeah product rule yes so by using the product rule uh we kind of changing the problem uh from the original posterior distribution to the right grip and the prior distribution uh this is uh the methodology is called noisy channel model and the people actually using this method but is that enough to solve the problem for me it doesn’t actually change the difficulty or even that looks like it’s more difficult right and then the uh the how to uh the the make this uh problem more productive we actually uh the using the additional information so speech to text anyway this is a very different uh the conversion we want to have something between what they we can introduce like our linguistic knowledge we can use phoneme right and then the uh this is you know a little bit easier right from all to directory uh predicting the words are kind of difficult but by using the phoneme intermediate representation uh each kind of conversion is a bit sub problem and this is easier so this kind of a methodology is quite important to solve the very difficult combining problem okay so now we will we have our phoneme seekers let’s use the following sentence how to introduce this phoneme sequence in this uh the probability distribution some people may answer if either they took my course in the speech question and understanding which one we use some rule product rule coordination independence assumption it’s actually summer right some rule is great we actually can introduce the additional variable right still this is doesn’t change anything it doesn’t change the difficulties so how do the further other changes at this problem we just using the product rule like i mentioned before it’s part of the base rule but by using the product rule we can further factorize this problem to the three distribution and one the the other distribution in the denominator but this is not related to the our other optimization problem so we can safely actually ignore it right is that everything it’s actually not this is just equivalent uh the conversion right it actually doesn’t change the difficulty how to make the program more simple we’re using the conditional independence assumption for this case it’s where we use the condition of some individual functions here these are reasonable assumptions right the the relationship of the obligation speech features only depend on the election through the lexicon volume than the one it’s uh it’s a difficult approximation but it’s reasonable i would say right so we actually using this conditional independence assumption everywhere to make the problem attractable for example the acoustic model we first apply the conditional independence function hidden markov model is one of the conditional independence assumptions to make it tractable and uh we use a innogram language model now we use the neural language model which actually doesn’t have that but they used to use the endogram language model this is also conditional independence assumption so by using that we actually making this problem uh attractable and this methodology is quite rather powerful actually it’s not only used for speech recognition by the way this is also used for the machine translation as well you know before neural machine translation comes so uh actually the ibm uh is uh the the same other data the group same division are the proposed both speech recognition uh and the machine translation in this kind of a statistical form okay so now uh i uh decompose this uh the problem to the three distribution right this is actually structure uh that we uh that are solving the speech recognition so the first part feature extraction again this is not included in the probabilistic representation the first one is acoustic modeling lexicon language model it sounds like we just combined some kind of uh other sub problems right but it’s actually mathematically uh well uh decomposed and then we make each of the sub program tractable and then finally combine it based on the uh this uh the the equation this is the mass of the speech recognition or other other the problem of solving the secant sequence model before neural model comes the important concept is factorization to make the kind of problem to be decomposed and the other is its factorization itself doesn’t change the difficulty we also have to have a conditional independence assumption to make the problem practicable so uh i don’t know how you guys feel when i first learned this one i saw that this is very elegant the first thing that i just learned the speech recognition is these four components oh my god it’s just a complicated you know there are some modules that are combined to make a speech equation sounds like you know very cool but but a bit necessary but it turns out that it has a quite beautiful theory to other the original target is the base decision theory and i introduced each of the sub modules uh based on the uh the probabilistic formulation and actually at the in the uh the following uh the slide i will talk about each of the modules a little bit more but usually i skip the details and if you guys want to know more about each of the module and so on uh or please also consider to take a speech operation and understanding courses in the full semester okay um maybe i can accept one of a few questions here if not i can move to the each of the pipeline uh quickly um the first part uh feature extraction this is before you know the goes to the probabilistic model the feature extraction we use our other signal processing techniques uh to convert the waveform to the male frequency capture question mscc or other other features and i just want to think uh the multilingual energy this process is mostly uh the the language independent process as you can imagine right this is just a signal processing to convert the waveform to more tractable other part other feature so uh this is actually the the result of the conversion from the waveform to the uh ms60 i think most of people could agree the the bottom figures have more patterns right any more patterns so this has been very important uh to make the uh the feature uh to be uh used for the background of the processing uh by the way this signal processing based approach is gradually replaced by the deep running this is also happening now so one is the people using actually other cnn instead of mscg or some other people also using the sales supervisor learning now this is very powerful but the drawback is that this is a learnable learning based approaches so then the probably language independence property will be kind of mitigated to do that next acoustic modeling which converting the speech features to the following the sequence and for this other acoustic modeling uh we using the combination of the hidden markov model with gaussian mixture model or deep neural network and this approach is also the mostly uh language independent because speech features are more likely independent and the phoneme is also if we design well it can be a luggage independent or at least you know not so much other we can make it not so much depend on the languages although it is actually very difficult okay so the hidden markov model is uh the other actually quite important part in this uh the modeling what hidden markov model is doing is actually uh the um quite important role uh in the entire speech definition so as i mentioned the speech recognition the one over the difficulty is the output the symbol and the input uh the languages are very different so we need to make some arrangement uh all of the uh the speech features and the corresponding the following uh the information so in this case you know we don’t know which boundary we should take for each of the funding and to do this kind of alignment problem uh we are the classically uh using the hidden markov model and the hidden markov model is more like a charging of this other making this kind of alignment and given this alignment to provide a data accurate likelihood based on the as gaussian used to be gaussian but now deep neural network that is a kind of acoustic model and by the way this acoustic model part is the most important to get the performance than the other components in general the third module is the english and so on and this part is heavily language dependent so to do that uh we actually need to first access to get the other addiction information and i actually uh the uh usually show you the uh the cmu dictionary uh this is one of the most well-known uh english dictionary uh maintained by here but unfortunately it’s the stava is down now so i cannot make a demonstration uh and so on uh but uh this uh cmu dictionary the english is very lucky because we have a same reaction and then we can build a speech principle system the other language is it’s actually not easy to get that we don’t have so much kind of structure and the accessible dictionary in the other languages however other probably that i use this we dictionary and the if using navigationally we can also somehow get to the information about the other phoneme it’s actually covered many languages like this right by the way it’s not recovered yet not yet okay this is unfortunate so okay so uh by using that we can also uh the uh the use the other maker kind of of this uh dexcom component for each languages and the last part uh is the language model and the language model that we use the engram or a recurrent neural network and usually the first language model will be the the built is that the uh we uh try to kind of uh get the uh the uh how possibly uh that word uh uh given the pronunciation we can hover are the authority that given the kind of other the word sequence uh how possibly we select each language depending on the context that is what language model is doing and this uh the example i actually have these three uh the uh word sequence this is by the way the same go to the in terms of the phoneme and it can be actually go to or go to or go to right and then if for example check if we check the other uh this works journal sentences and check whether this but how many times this button appeared and then we can get the the uh how likely this other phrase appeared right go to appeared 51 times but the others actually are do not appear so this means that this go to will be most likely selected based on the language model right however this uh actually also other has some kind of other issues uh actually uh this pattern still exists in our languages so i actually include the text size from the ten thousand order to the median order and then can’t go to still other the biggest to more than two thousand can you guess how many times go to appears go to appears in the worst journal sentences two times only two times this is a one example it’s not completely good sentence but yeah it’s appearance right and the goal too there how many you you guys can guess it’s actually quite a large number and yeah most cases go too far uh this is the other sentence that the uh the uh the overnight show i saw when i uh search this other two other phrases so if we using the small corpus it doesn’t cover but if using the large coppers we can cover it so this is a kind of power up if using a large purpose we can cover many of the various are the language patterns as much as possible and this part is also language dependent so uh this is the most uh the the the building block of the speech recognition and actually i’d say that it is not easy to build that for you guys each of the components is governed by the different model feature extraction signal processing model acoustic model other pattern recognition machine learning deep learning and so on next come moderate coming from the other computational linguistic or other linguistics language modeling is also come from the nlp or are they now deep learning but anyway the each of the other modules has a different models so it is very difficult to actually develop all of them and also connecting all of this kind of module is also not easy so instead now people are also working on the uh end-to-end speech definition which try to make entire pipeline as a single neural network google’s a demonstration it’s actually they already switched to the end-to-end neural network other companies as far as i know they still didn’t add a seat to the neural network and to the neural network but either by using this pipeline and the uh the so maybe yeah maybe that’s it i want to uh finish my talk so uh the these are summary uh of the speech recognition first feature condition is very well defined problem and also fortunately we have a large corpus so it is quite you often use other other measure for the for our new machine learning algorithm and the factorization and making the problem productive this is a kind of great methodology for us to tackle this problem and the but since uh the this methodology is a bit complicated recently people are also that are the uh they’re the interesting and actually having a lot of development in the end to end a speech equation feature recovered in my next lecture and then this is the main topic or the other assignment three so please uh enjoy the speech recognition and let’s move to the discussion um the uh the priests i think you guys already tried a speech recognition engine by yourself right and then talk about what kind of errors you happened and also apply to the other language and discuss about it and since we don’t have enough time we don’t have our final discussion time just split and then either discuss it and then finish it okay so let’s just</p>
</blockquote>
</div>
</div>
</div>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li><strong>Speech Recognition Demo and Evaluation Metrics</strong>
<ul>
<li>Demonstration of speech recognition.</li>
<li>Discussion of how well it works, and examples of when it fails.</li>
<li>Evaluation metrics.</li>
</ul></li>
<li><strong>Evaluation Metrics</strong>
<ul>
<li>Sentence error rate.
<ul>
<li>Discuss if the entire sentence is correct.</li>
<li>Explain why this is too strict of a measure, and the need to consider local correctness.</li>
</ul></li>
<li>Word error rate (WER).
<ul>
<li>Using edit distance word-by-word.</li>
<li>Calculating error rate percentage.</li>
<li>How to compute WER for languages without word boundaries.</li>
</ul></li>
<li>Other metrics.
<ul>
<li>Phoneme error rate (requires a pronunciation dictionary).</li>
<li>Frame error rate (requires an alignment).</li>
</ul></li>
<li>NIST Speech Recognition Scoring Toolkit (SCTK).</li>
</ul></li>
<li><strong>(A bit) Mathematical Formulation of Speech Recognition</strong>
<ul>
<li>Speech recognition as a conversion from a physical signal to a linguistic symbol.</li>
<li>Explanation of probabilistic formulation.
<ul>
<li>MAP decision theory to estimate the most probable word sequence.</li>
<li>Noisy channel model.</li>
<li>Factoring and conditional independence.</li>
</ul></li>
</ul></li>
<li><strong>Standard Speech Recognition Pipeline</strong>
<ul>
<li>Feature extraction.
<ul>
<li>Converting waveform to MFCC.</li>
<li>Language-independent process.</li>
<li>Desirable representations.</li>
</ul></li>
<li>Acoustic Modeling.
<ul>
<li>Converting speech features to phoneme sequences.</li>
<li>Using Hidden Markov Model (HMM) to align speech features and phoneme sequences.</li>
<li>Language-independent.</li>
</ul></li>
<li>Lexicon.
<ul>
<li>Pronunciation dictionary.</li>
<li>CMU dictionary.</li>
<li>Multilingual phone dictionary.</li>
</ul></li>
<li>Language Model.
<ul>
<li>Using N-grams or recurrent neural networks.</li>
<li>Word selection based on context.</li>
<li>Language-dependent.</li>
</ul></li>
</ul></li>
<li><strong>End-to-end Speech Recognition</strong>
<ul>
<li>Using a single neural network.</li>
<li>A simpler solution for multilingual ASR.</li>
</ul></li>
</ul>
</section>
<section id="papers" class="level2">
<h2 class="anchored" data-anchor-id="papers">Papers</h2>
<ul>
<li><ol class="example" type="1">
<li><a href="http://www.itl.nist.gov/iad/mig/tests/rt/ASRhistory/pdf/NIST_benchmark_ASRtests_2003.pdf">A look at NIST’s benchmark ASR tests: past, present, and future</a></li>
</ol></li>
<li><ol start="2" class="example" type="1">
<li><a href="https://arxiv.org/pdf/1505.05899">The IBM 2015 English Conversational Telephone Speech Recognition System</a></li>
</ol></li>
<li><ol start="3" class="example" type="1">
<li><a href="https://arxiv.org/pdf/1610.05256">Achieving Human Parity in Conversational Speech Recognition</a></li>
</ol></li>
</ul>
</section>
<section id="warlpiri-in-10-minutes" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="warlpiri-in-10-minutes">Warlpiri in 10 minutes</h2>
<div class="page-columns page-full"><p>  </p><div class="no-row-height column-margin column-container"><img src="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w14-ASR/geom.png" id="fig-lit-greo" class="img-fluid" width="400" alt="Geography"><img src="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w14-ASR/consonants.png" id="fig-lit-cons" class="img-fluid" width="400" alt="Consonants"><img src="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w14-ASR/vowels.png" id="fig-lit-vowel" class="img-fluid" width="200" alt="Vowels"></div></div>
<ul>
<li>Spoken in the Northern Territory of Australia by the Warlpiri people.</li>
<li>Approximately 2,500-3,000 native speakers.</li>
<li>One of the largest Aboriginal languages in Australia based on the number of speakers.</li>
<li>One of only 13 indigenous languages in Australia still being passed down to children.</li>
<li>Alternative names include Walbiri and Waljbiri.</li>
</ul>
<p><strong>Language Family &amp; History</strong></p>
<ul>
<li>Pama-Nyungan.</li>
<li>Ngarrkic languages.</li>
<li>The term <em>Jukurrpa,</em> referring to Aboriginal spiritual beliefs, comes from Warlpiri.</li>
<li>A writing system was not developed until the 1970s when the language began to be taught in schools.</li>
</ul>
<p><strong>Grammar</strong></p>
<ul>
<li><strong>Free word order</strong>, but the auxiliary word is almost always the second word in a clause.</li>
<li><strong>Ergative</strong> marking. The actor takes a special ending called the ergative ending. The ergative ending marks the subject of a transitive sentence.</li>
<li><strong>Split ergativity</strong>. Nouns follow one set of rules, while pronouns and auxiliary verbs follow another.</li>
<li>Suffixes indicate person and number of the subject and object.</li>
<li><strong>Vowel harmony</strong>.</li>
</ul>
<p><strong>Phonology</strong></p>
<ul>
<li>Most Warlpiri languages have only <strong>three vowels</strong>.</li>
<li><strong>No voicing contrast</strong>. Aboriginal languages have no contrast between voiced and voiceless consonants. A sound can sound like a ‘p’ or a ‘b’ depending on its position in the word.</li>
<li><strong>No fricative sounds</strong>.</li>
<li>Love the ‘r’ sound. Warlpiri has three ‘r’ sounds.</li>
</ul>
<p><strong>Interesting Linguistic Features</strong></p>
<ul>
<li><strong>Avoidance register</strong>, a special style of language is used between certain family relations that have a drastically reduced lexicon.</li>
<li>Warlpiri Sign Language also exists.</li>
<li>Speakers are often multilingual, learning each other’s languages.</li>
<li>A strong tradition exists of not saying the names or showing images of people who have passed away.</li>
</ul>
<p><strong>Present Status</strong></p>
<ul>
<li>Warlpiri is considered a threatened language because children sometimes respond in English even when spoken to in Warlpiri.</li>
<li>There are efforts to teach the language in schools and create modern terminology.</li>
</ul>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Automatic {Speech} {Recognition}},
  date = {2022-03-03},
  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w14-ASR/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Automatic Speech Recognition.”</span> March
3, 2022. <a href="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w14-ASR/">https://orenbochman.github.io/notes-nlp/notes/cs11-737-w14-ASR/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Multilingual NLP</category>
  <category>NLP</category>
  <category>Notes</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w14-ASR/</guid>
  <pubDate>Wed, 02 Mar 2022 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Speech</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w13-speech/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/tiling.png" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>course banner</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/Hsdf8Vjai5g" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Lesson Video
</figcaption>
</figure>
</div><div id="sup-slide-deck" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div>

<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>What is speech?</li>
<li>Speech applications</li>
<li>Speech databases</li>
<li>Speech hierarchy</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- Transcript here is machine generated no punctuation and I eddited it a but but it needs more work and I dont see the benefits unless I can run it through some tool. -->
<section id="intro" class="level2">
<h2 class="anchored" data-anchor-id="intro">Intro</h2>
<p>Today’s lecture today is about speech. Today, we also have our assignment release. After this lecture, we will have a walkthrough of the assignment speech. I hope I can finish it earlier so that you guys have enough time to explain the assignment three. We actually put a lot of energy into it, so I hope you guys can enjoy it. anyway, for today’s content, I put the four other items: speech, speech applications, speech databases, and speech hierarchy. I will probably go through at least three items, and the fourth maybe the this time or next Thursday so the next week that I will talk about the</p>
</section>
<section id="what-is-speech" class="level2">
<h2 class="anchored" data-anchor-id="what-is-speech">What is speech???</h2>
<p>The most important definition is speech. Many people say that “it’s sound produced by humans.” It may probably be true, but in many cases, we actually have a bit more narrow definition, which is “the sound produced by humans for the communication” that is mostly actually used in our kind of other speech processing but generally the wider meaning of the speech can be just by humans that is a kind of our other usual definition and then I will try to ask you guys about the four kinds of five audio and please answer this one is speech or not with water is it speech or not it is? Speech yes it’s very noisy but still speech next probably this is not speech or this thing is speech yeah probably business speech by the way all the sound oh no except for the first one. I picked up from the other free sound so this the link is also useful you guys can get a lot of kind of interesting sound and actually using it for your research. it’s also copyright free, so quite kind of easy to use the Saturn is this speech or not it’s actually the difficult so my definition still speech because laughing and so on actually accelerate our communication right so my definition are usually yeah they’re using it as a speech yeah even people are typing to do some communication but this is not human voice so this is not speech right and the last one what do you think is this speech or not so if this is the wide the sense of the the sound to produce a human this is speech but this is not free for the communication especially singing voice well some people can use singing words for the communication like opera and so on but in general it is not used for the communication so again this definition is not very clear but usually people actually regarding the speech and the singing voice are separate okay so this is the definition of the speech and the next a speech is made by the sun right and what is sound sound is just a change of the air pressure and that is captured my microphone so it’s quite physical phenomena so compared with the various topics we have been studying in multilingual nlp many of the kind of the problems are more like a syntax semantic other textual information but speech is actually quite that comes from this the physical phenomena and the actually this other sound</p>
</section>
<section id="speech-waveform" class="level2">
<h2 class="anchored" data-anchor-id="speech-waveform">Speech waveform</h2>
<p>pressure is captured by microphone and they converted either this other one-dimensional waveform and usually in this lecture, I use this one-dimensional waveform as an input to our problem in speech other processing but it’s actually not one-dimensional like for example human years a very good. example we actually at least have two two-dimensional other waveforms and the many of the the smart speakers including this one or the Alexa and so on. They also have the other more microphones but in general each of the kind of channel of the signal is the one other dimensional time domain waveform and since this is a waveform so this is governed by well-known physical properties. I am not sure how many people remember this kind of properties attenuation deflection the the the diffraction super position and so on so actually these properties are quite a making speech signal to be ditch information and due to this kind of property especially superposition and reflection is a kind of very important the property to include a lot of the information inside the waveform this becomes the least information in the speech so first no second question actually the second question to you guys is</p>
</section>
<section id="what-kind-of-information-does-speech-sound-contain" class="level2">
<h2 class="anchored" data-anchor-id="what-kind-of-information-does-speech-sound-contain">What kind of information does speech sound contain?</h2>
<p>what kind of information does speech sound contain of course I will talk about the speech condition so this means that the speech includes linguistic content transcription and so on right and the speech also has a speaker characteristics is there any other information that speech can deliver can someone tell me phonemic signals like emphasis and so on yes exactly exactly any other notion emotion yes good good are there any other examples actually quite a lot by the way for example one of them yeah you can tell if somebody’s sick yes if they have a cold yes they if they talk too much the previous night yes yes exactly exactly you know other other faculties here also that try to kind of detect the the person is copied or not from the voice signal so voice can also include the health information voice also that speech also includes a lot of other information for example we can speech information even include the location you guys can identify at least you know which direction speech comes right this is not purely speech but it’s a kind of multiple other signal of the speech we can identify other the the the location speech comes from speech also include the reverberation so from the evaporation we can even approximately say the room size and so on is there any other information maybe a lot that I can come up with other the many of them and actually this other information is that corresponds to various other speech applications so now move to the the speech applications so the speech applications are depend on the other problem but they actually the the many of the problem is to try to capture what information is delivered from speech speech condition is a good example right and there’s speech emotion recognition try to capture the emotion and of course we also have our audi the opposite problem given the text to generate the speech speech synthesis this is also the speech processing research topics and the we actually hover tons of the other problem in speech speaker recognition as I mentioned speech delivered speaker characteristics right so that from the speech signal we can actually identify who is speaking oh I forgot to mention about it this is actually very important for the modern thing or nlp language recognition from speech sound we can also identify the language and the there are a lot of the speech the program by the way these topics i extracted from the topics in the intel speech which is one of the biggest speech conference and the deadline is next month right one month right after right after this that day one month right after this today so if people here are thinking about doing something for speech inter speech is a good target but I think if we link it to the project here maybe it’s a bit too late okay so we have a tons of the speech application and by the way among them the next question among them what is the most widely used technique there’s you know under some how they say the definition what is other than the mostly used but I think everyone can agree if I mention the answer so can someone say what is the yes yes do not check the next line because people usually cannot answer so yeah speech recognition probably in terms of the number of the researchers the the biggest one is speech recognition and the next biggest the second biggest would be speech synthesis and other kind of following and the speech coding is actually a bit the the already sorted technique so there’s not so much research in this area but actually speech coding is the most used technique and the actually the speech coding is a unique compared with all others all the other tried to harder say some given input to extract some information or enrich some information by generating that and so on the speech coding is try to keep the original information but try to kind of compress the</p>
</section>
<section id="speech-coding" class="level2">
<h2 class="anchored" data-anchor-id="speech-coding">Speech coding</h2>
<p>information as much as possible and actually this was the one of the first or the most active study in the early the speech the processing research since at that time compression is quite important and also compression actually tells us what is speech what information is delivered is a quite important if we consider the speech coding and so on so that’s why people are starting to work on the speech coding and then the that we moved to various other applications and another the many of them are actually getting quite matured I would say and one of the techniques I would like to mention is a speech recognition and that can be covered</p>
</section>
<section id="automatic-speech-recognition-asr" class="level2">
<h2 class="anchored" data-anchor-id="automatic-speech-recognition-asr">Automatic Speech Recognition (ASR)</h2>
<p>in Thursday and after the break I will also other revisit the sr part again so using the two lectures to explain about a speech condition but the speech recognition is one of the very complicated system if you guys really want to understand a speech recognition it may take one semester so then please take the speech recognition and understanding in the next fall semester if you really want to know that the next the most the second the biggest obligation is speech synthesis</p>
</section>
<section id="speech-synthesis-tts-text-to-speech" class="level2">
<h2 class="anchored" data-anchor-id="speech-synthesis-tts-text-to-speech">Speech Synthesis (TTS: Text to Speech)</h2>
<p>and the speech synthesis is also are they given the text to generating the web form and how the wider and I think this is the possibly the the second biggest applications than speech recognition I believe because every and now you know are that hard about that the tts synthesize the voice right so in terms of the application this also has a big success and as everyone here may note that the young guys are lucky so professor aram black he is the pioneer of the text speech and he will have a lecture about dds I’m also very looking forward to that okay so there are a couple of others and I try to pick up not everything but some of them that are interesting or are important the next one that may be very related</p>
</section>
<section id="speech-translation" class="level2">
<h2 class="anchored" data-anchor-id="speech-translation">Speech Translation</h2>
<p>to this course is speech translation so directly converting the source speech to that target text so in this example a japanese speech it’s translated to english text how to do it one simple approach is to combine automatic speed recognition first and then machine translation and this is probably the other most kind of other the widely used way and the I think other that this way should be also considered but these are the two system combining two systems a bit complicated and also if we have our error in the asl side it cannot be well recovered in the second machine translation side so people also are starting to directly are using an end-to-end neural network to solve the japanese source speech like a Japanese speech to a english text directory without outputting Japanese text and so on and this approach is quite important if some languages don’t have their own scripts but they often kind of have a kind of a translation to their colonial languages and so on there’s such a lot of such kind of other databases actually and then this speech to speech to text direct information is also quite important in these cases and the this is to the speech to still text but if we try to use it for the interface to communicate to the foreign people in the foreign countries seamlessly we also want to develop the source a speech to speech translation in this case source speech is english and it is converted to the target speech and this is also started to be very popular not only as a research but also other other product and many of the systems are currently using the combination of the speech recognition first machine translation in the middle and then other other dts takes to speech but there are a lot of kind of a decent emergence techniques based on the end-to-end single neural network to even the model this kind of a very complicated pipeline based on a single architecture and I think this is one of the the ultimate goals of multilingual energy right if we have a perfect speech to speech translation systems probably we can solve many of the problems that discussed in this work but this problem is quite difficult and a lot of studies are all going on now so these are the related to marginal nlp and I will also introduce some of the other speech applications which will be very related to our problems so I just kind of combined the speaker recognition language construction speech emotion recognition all the event classification and detection so actually speech information has a lot of profiling information so as I mentioned the speech recognition is one of the example this means a linguistic information right and we speech also other the speaker gender aging information as well age is not very correct but we can still actually predict some range of the age based on speech and the language we can also add get the information from speech emotion that we have discussed that is also other extracted from speech and this is not completely speech information but the let’s say environment is important for us to understand about the communication scene and the song right and then audio event so when I am speaking the fun noises you know working on somewhere right this kind of information all that kind of informations are included in speech so superposition property is quite nice in terms of packing many of the information into the one sound wave form</p>
</section>
<section id="privacy-in-speech" class="level2">
<h2 class="anchored" data-anchor-id="privacy-in-speech">Privacy in speech</h2>
<p>so this the property is very good but it’s getting a bit more other difficult not difficult but challenging problem privacy in speech since speech has a lot of information so that the working on speech means we potentially touch a lot of information of that other person so the people are also seriously thinking about the privacy and on-device approach becomes quite important due to this i’m setting the how many slides I can have I have to finish maybe this is very cool so I can also either introduce one this one so as I mentioned the super position property is very cool in terms of the getting various information in the environment right but this information is not always useful actually many cases we just want to focus on target speaker right so then the speech enhancements peace separation techniques are also very important it’s not very related to this other lecture but I will just explain about it because many people actually often ask me the the my asl doesn’t work and most of the cases it comes from noise so I just want to emphasize that and there are way to deal with that and there are several types of the the noise</p>
</section>
<section id="speech-enhancement-several-types-of-problems" class="level2">
<h2 class="anchored" data-anchor-id="speech-enhancement-several-types-of-problems">Speech enhancement Several types of problems</h2>
<p>we have to deal with the first is a kind of how does this the the gesture or the environmental sound that adding to our the the usual sound you guys I guess can steal the recognize it right computer actually cannot at all probably you guys can do better than computer so to solve this problem we also cover other noising techniques this is actually after the denoising sounds better right this is actually very good for computer and the next debug variation is also another very the annoying issue again for you guys it is not so difficult to recognize but for computer this evaporation echo is quite harmful so there’s a technique to suppress the evaporation called the reverberation if you guys using the headset microphone you guys can even clearly see the the find that the the echo is removed since this room is a bit large so actually echo is further accomplished but I believe still people can understand it next one is the separation I think I prepared a very cool demonstration by oh yes I prepared this demonstration this is actually the he is my former colleague when I was working at the mitsubishi electric research laboratory jonathan and he actually it’s he was planning to come here this friday but it was cancelled so for it is unfortunate but instead other I just play his cool demonstration you can see that the the mixed speech is completely separated right so this is one of the pioneering work of using deep running for speech separation in the first time and this is already very clearly separated right but now the technology becomes mature and this other kind of a simple speech separation problem is almost solved however we have a lot of other difficult separation problems okay so I think these are more like the applications that I want to cover there are lots of other application if I have time I can also want to introduce the some of them like a spoken dialogue systems and so on but I just skipped them due to that time limitation okay so next the I will talk about speech databases to work on this kind of our core speech problems we have to have our database</p>
</section>
<section id="speech-variations-speaking-styles-and-environments" class="level2">
<h2 class="anchored" data-anchor-id="speech-variations-speaking-styles-and-environments">Speech variations Speaking styles and environments</h2>
<p>and the speech as I mentioned has a variations a lot of variations right and the there are many axes but mostly we separate classify the speech database with the speaking styles and the environment and so on, but I will just talk about the speaking styles for now so one style is wet speech and the other is spontaneous or non-bit speech, and I will explain how it is different maybe I can play some of them, so this one is the first one versus journal this is one of the most famous speech recognition corporates so if you guys have some cool idea for the speech equation techniques and the the showing the kind of other improvement in this abortion channel you guys can write the paper so it is a bit so the game is small, but it is a so-called red speech and the why it is called versus journal people are leading the world’s regional standards so that the the disco pacifi named work to each other switchboard this is an example of spontaneous speech. I think it is obvious that the second one it’s more difficult, right, but the second one is more the natural conversation and the third one this is the corpus called liberty speech and I mentioned that wall street just now is one of the most famous speech corporal, but now that this liberal speech is the most famous the most frequently used the corpus why it is used so popularly this is simply because the license is quite flexible wall street journal has a kind of a little bit limited license while river speech doesn’t have so much restriction for the license and also the amount of data is also quite large thousand hours so that’s the reason that people are using the legal speech so okay I will try to explain a bit more about what is red speech I kind of mentioned already about some of them so that speech is usually corrected as follows first other we have some sentence showing in the prompt, and then we can just read this prompt which are corresponding to we get the paradigm of the prompt and corresponding speech data that’s it right it’s very easy I believe i prepared this how many people knows this the the web page this kind of activity come on boys this is why widely the the used the speech the the data collection scheme led by the modular and this actually has tons of the multilingual speech resources now so if you guys want to get some speech data the one of the first phrase you will check is this common voice and this is a common voice the correction is based on the red speech prompt difficult for example if you click this one it’s working but she has also served as chair of the board of bomb health new zealand oh yes he has also served as jail the board of bomb health new zealand and the next prop this comes and I will speak that it is very cool system right and then we can actually create a lot of languages and again this supports many many languages now I was super happy when you know they started to support japanese and I spent you know half day just speaking probably many of the my voices are using this are the database however be careful people sometimes don’t speak correctly how to other than deal with that they actually are not only recording they actually hover oh sorry maybe I can reload it this is a bit difficult because I have to we also have another action listen this actually confirms whether the recorded speech is correct or not and I I haven’t tried this but I am only speaking in my Japanese so not sure if you know correctly you know they recognize my japanese and they registered it those are the useful corpus or not so this is the that speech</p>
</section>
<section id="read-speech-examples" class="level2">
<h2 class="anchored" data-anchor-id="read-speech-examples">Read speech examples</h2>
<p>so again this is easy to connect but I have a lot of experience that the people don’t speak correctly, so we actually need to check whether they are speaking correctly and also easy to anonymize compared to the spontaneous speech because it is prepared sentence it is not connected with the speaker’s identity, so if the speaker there is completely anonymized that we don’t care about it, but the problem is that let’s speech is not a real conversation, so this is a very good spot to starts to build a speech function system but if I put it in in this system in the conversational scenario it will not work so well</p>
</section>
<section id="spontaneous-speech" class="level2">
<h2 class="anchored" data-anchor-id="spontaneous-speech">Spontaneous speech</h2>
<p>so instead we also used a spontaneous speech, and this one is very tough. It’s actually hard to transcribe actual recording yeah again i prepared an example by the way, how many people use this to own that city did you know that audacity is developed here cmu yeah again you guys are very lucky cmu can provide any of the other tools for other speech and the nlp research and the I think I’ve prepared the wait a moment the way to transcribe the speech the situation speech I think, okay oh, I actually put the latest rider give it a moment my computer is not working now. Okay, now it’s working, so if I have more skills you know switch your screen and so on I can do some demonstration in front of you, but it’s very difficult to do it, you know, by checking the other back monitor and this monitor so instead i will just upgrade the play the audio a play the video of how to transcribe the spontaneous speech check the segment part manually, then type okay check the segment [Music] okay, just to transcribe this I don’t know five seconds of speech it takes a very long time compared with the left speech right so this is quite time consuming and it is quite rather expensive to collect such kind of data like for example this switchboard this is a switchboard corpus which is one of the other famous speech condition coppers, but this is spontaneous speech and the when the in the previous my lecture i actually had a homework to the student to transcribe two minutes of this dashboard conversation I think, yeah the 30 minutes is actually a shorter side most of the people takes random work this is the one of the most honestly biggest complaint of my homework actually since it is very very hard of course, transcribers are very professional, so they can finish it a little bit earlier but still the the spontaneous speech the the recognition and transcript transcription there is a very very difficult I think I need to finish in five minutes or so so this is the the just speech is very fluent but a dear situation is more difficult like for example the in the real situation the meeting scenario or compensation scenario this is a mostly we use for the speech right and then the situation is even more difficult As I mentioned last in the speech enhancement demonstration, we have background noise we have our interference speaker, and we have a reverberation and so on but this is kind of our again our usual the everyday conversation scenes right and we can easily recognize this kind of sound speech while the alexa and so on I guess you guys have like some other experience just you know, putting it to the the, and then we started to conversate it they actually cannot be recognized well they even don’t know that of each other person they recognize and so on it’s very difficult problem but again human can do it easily and if I am actually working on this area a lot so not only for the spontaneous speech but also in the kind of real everyday conversation scenario and the project is called chime project and this is we actually collected such kind of a very the adverse the the environment of this collected speech in the very adverse environment and then the also are organizing the challenge making a baseline and so on I can play some of them so compared with the previous conversation it has a lot of over by the way also noise and so on, but again, this is a natural conversation it makes the speech very difficult so I think yeah</p>
</section>
<section id="where-we-found-the-speech-data" class="level2">
<h2 class="anchored" data-anchor-id="where-we-found-the-speech-data">Where we found the speech data?</h2>
<p>I will finish it in a minute, so there are again a lot of types of the speech and many researchers many other the institutes actually collecting the data for the research and the development for the community and the we can actually the access to such data easily with some of the repository one of the most frequent one is ldc linguistic data consortium where you can actually get famous asr benchmarks like a teammate a world regional switchboard that I mentioned before however as I mentioned that this has some kind of the the restriction for the use we have to pay they do the I get this kind of data but again fortunately cmu is a member so everyone can actually get lgc data really so the for this part you guys would not cover any kind of issue, I believe, and another only for the fdc. There are a a lot of other institutes g,overnment institute or some the university and so on hosting a lot of other speech corporal so you guys can actually get such kind of the speech data through this kind of repository and the second are the bullet items are getting the more popular now the books first open the center of common boys are denoted and the common voice is that I mentioned before and this this data usually have a less restricted license a mostly creative command so that the that we may not go through the LDC and they can easily get the data and even redistribute the data and so on, so due to this reason, recently, many people actually starting to use this data in the box for your common voice and so forth, especially people working on the likewise machine learning and not in the nlp or speech some people may not have an access to the LDC and then people using this kind of repository to get the data, so these are also important resources by the way, these are important resources for assignment three as well, so that’s why I kind of spent some time for the explanation, and the last one is the audiobooks or public recordings with captions youtube actually the ten percent of the youtube videos hazard captions they are set by the the uploader so we can use speech and corresponding captions and the same for the podcast for data token and so on generally, they have some captions so that we can actually use such kind of data and so on, but for this data, maybe you guys also check the license. Sometimes it’s very strict, sometimes not but for most of the kind of research purpose in the academic institute, it shouldn’t be a problem, but the more difficult part is that this recording is required for just one hour and a half of the long recording and corresponding very long the caption so it’s it is just too long for us to use it so we need to correctly segment it and also this is caption is very noisy so it is not easy to use this kind of caption another very important difficulty is that this kind of data is updated frequently so they’re often deleted so we cannot actually others assure the reproducibility if we’re using this one and one of the the famous the databases actually cmu’s wireless the wiredness database game that professor alan black collected and it’s actually has a 700 languages wow that is very cool but it is also already has some issues written here like a bit noisy or API if API is changed it is not easy to get the data and so on, but in general, this is also very good source and the last very last this is important important for the assignment for speech recognition we often use our other unit so please remember I even saw that this is a common the sense for everyone and that they use it without caring about the your prior knowledge so I also want to actually define we often use the hour and, let’s say, more than a thousand dollars, we can make a commercial system but it requires a lot of computational cost 100 hours a speech equation started to be working so we can do the research and so on lesser 100 hour it is categorized as a low resource language in speech recognition so other please understand this range of the data if you try to practice speech recognition and then I would like to pass it to the assignment three but maybe I can accept one or two quick question there any questions yes, like annotating this spontaneous speech could we you first pass it into a voice activity detection: oh yeah segment it and then annotate it yes usually there are such kinds of assistance to a lot of assisted tools. Even we use a pre speech condition and then do it but the voice activity detection is super important so yes that we usually either have such kind of our other voice activity direction but voice activity detection is not working in the noisy environment so much so anyway most of the cases are the human segmentation is also quite important and in question yes definitely we have such kind of other issues since we tried to capture more like a similar closer information and then the the great program for the same gender and then speech separation performance is big world and it’s working very well in a male female and so on so it’s definitely that these kind of things exist but still the current technology almost perfectly separated we almost perfectly separate the speech even in the same gender but it’s a good question okay so maybe that’s it and I want to pass it to this chiang kai and patrick</p>
</section>
</div>
</div>
</div>
<section id="outline" class="level3">
<h3 class="anchored" data-anchor-id="outline">Outline</h3>
<ul>
<li>What is Speech?
<ul>
<li><strong>Speech is sound produced by humans for communication</strong>.</li>
<li>Speech is the most natural form of communication and more common than text, even if it is not the most recorded.</li>
<li>Speech can be captured by a microphone as air pressure.</li>
<li>A waveform is created by converting sound pressure into a time series.
<ul>
<li>Waveforms are usually one-dimensional (mono), but can be two-dimensional (stereo) or N-dimensional, using multiple microphones.</li>
</ul></li>
<li>Speech sounds contain information such as transcription and speaker identity.</li>
</ul></li>
<li>Producing and Hearing Speech.
<ul>
<li>Speech is generated from the vocal tract by pushing air from the lungs.</li>
<li>The vocal cords may vibrate to create voiced speech.</li>
<li>Positioning of the tongue, teeth, and lips controls the sounds produced.</li>
<li>The ear drum vibrates in response to air pressure vibrations.</li>
<li>The cochlea decomposes vibrations into frequency components.</li>
<li>The auditory nerve transmits signals from the cochlea to the brain.</li>
</ul></li>
<li>Phones and Phonemes.
<ul>
<li>Phones are segments of linguistic space; changing a phone can change the word.</li>
<li>The definition of phones can vary and can be hard to define exactly due to variations in how people speak them.</li>
<li>The International Phonetic Alphabet (IPA) is a structured way to define phones, accounting for ambiguities and variance.</li>
<li>The place of articulation of a consonant can range from the lips to the back of the throat.</li>
</ul></li>
<li>Computational Considerations.
<ul>
<li>The need to understand the different positions of the vocal tract.</li>
<li>The importance of decomposing signals into frequency components, as the ear does.</li>
<li>Digitizing speech requires sampling it at least 8,000 times a second.</li>
</ul></li>
<li>Speech Technologies.
<ul>
<li>Speech recognition (speech to text).</li>
<li>Speech synthesis (text to speech).</li>
<li>Speaker identification.</li>
<li>Diarization to determine who said what.</li>
</ul></li>
<li>Applications of Speech Technologies.
<ul>
<li>Voice conversion.</li>
<li>Speaker recognition.</li>
<li>Language recognition.</li>
<li>Speech emotion recognition.</li>
<li>Speech coding.</li>
<li>Speech perception.</li>
<li>Speech enhancement.</li>
<li>Microphone array processing.</li>
<li>Audio event classification and detection.</li>
<li>Speech separation.</li>
<li>Spoken language understanding.</li>
<li>Spoken dialogue systems.</li>
<li>Speech translation.</li>
<li>Multimodal processing.</li>
</ul></li>
<li>Data Considerations.
<ul>
<li>Style of speech.
<ul>
<li>Careful speech is easier to process than casual speech.</li>
<li>Speaking to a machine is different than speaking to a human.</li>
</ul></li>
<li>Channel and context.
<ul>
<li>Microphone quality and placement.</li>
<li>Background noise.</li>
</ul></li>
<li>Data repositories.
<ul>
<li>LDC.</li>
<li>Vox Forge.</li>
</ul></li>
<li>The amount of available data varies significantly by language.
<ul>
<li>Thousands of hours of speech are needed for significant results.</li>
<li>Low-resource languages may have limited data.</li>
</ul></li>
</ul></li>
<li>Differences Between Speech and Text.
<ul>
<li>Speech and text are different and may be considered dialects of each other.</li>
<li>Many languages have strong distinctions between spoken and written forms.</li>
<li>Social media is blurring the line between written and spoken language.</li>
</ul></li>
<li>Speech Hierarchy.
<ul>
<li>The speech hierarchy consists of speech, applications, databases, and levels of the hierarchy.</li>
</ul></li>
<li>Variations in Speech.
<ul>
<li>Speaking styles and environments affect speech.</li>
<li>Read speech involves reading prepared sentences.</li>
<li>Non-read speech is spontaneous and requires transcription.</li>
</ul></li>
<li>Data Collection.
<ul>
<li>Data can be found in LDC, ELRA, Voxforge, and CommonVoice.</li>
<li>Audiobooks and public recordings with captions can also be used.</li>
</ul></li>
</ul>
</section>
<section id="papers" class="level2">
<h2 class="anchored" data-anchor-id="papers">Papers</h2>
<p>Here is a list of papers mentioned in the lesson:</p>
<ul>
<li><span class="citation" data-cites="kim2017jointctcattentionbasedendtoend">Kim, Hori, and Watanabe (2017)</span> <a href="https://arxiv.org/abs/1609.06773">Joint CTC-Attention based End-to-End Speech Recognition using Multi-task Learning</a></li>
<li><span class="citation" data-cites="baevski2020wav2vec20frameworkselfsupervised">Baevski et al. (2020)</span> <a href="https://arxiv.org/abs/2006.11477">wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations</a></li>
<li><span class="citation" data-cites="hsu2021hubertselfsupervisedspeechrepresentation">Hsu et al. (2021)</span> <a href="https://arxiv.org/abs/2106.07447">HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units</a></li>
<li><span class="citation" data-cites="chang2021explorationselfsupervisedpretrainedrepresentations">Chang et al. (2021)</span> <a href="https://arxiv.org/abs/2110.04590">An Exploration of Self-Supervised Pretrained Representations for End-to-End Speech Recognition</a></li>
<li><span class="citation" data-cites="hershey2016deepclustering">Hershey et al. (2016)</span> <a href="https://ieeexplore.ieee.org/abstract/document/7471631">Deep clustering based speech separation</a></li>
</ul>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-baevski2020wav2vec20frameworkselfsupervised" class="csl-entry">
Baevski, Alexei, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. 2020. <span>“Wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations.”</span> <a href="https://arxiv.org/abs/2006.11477">https://arxiv.org/abs/2006.11477</a>.
</div>
<div id="ref-chang2021explorationselfsupervisedpretrainedrepresentations" class="csl-entry">
Chang, Xuankai, Takashi Maekaku, Pengcheng Guo, Jing Shi, Yen-Ju Lu, Aswin Shanmugam Subramanian, Tianzi Wang, et al. 2021. <span>“An Exploration of Self-Supervised Pretrained Representations for End-to-End Speech Recognition.”</span> <a href="https://arxiv.org/abs/2110.04590">https://arxiv.org/abs/2110.04590</a>.
</div>
<div id="ref-hershey2016deepclustering" class="csl-entry">
Hershey, John R., Zhuo Chen, Jonathan Le Roux, and Shinji Watanabe. 2016. <span>“Deep Clustering: Discriminative Embeddings for Segmentation and Separation.”</span> In <em>2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 31–35. <a href="https://doi.org/10.1109/ICASSP.2016.7471631">https://doi.org/10.1109/ICASSP.2016.7471631</a>.
</div>
<div id="ref-hsu2021hubertselfsupervisedspeechrepresentation" class="csl-entry">
Hsu, Wei-Ning, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. 2021. <span>“HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units.”</span> <a href="https://arxiv.org/abs/2106.07447">https://arxiv.org/abs/2106.07447</a>.
</div>
<div id="ref-kim2017jointctcattentionbasedendtoend" class="csl-entry">
Kim, Suyoun, Takaaki Hori, and Shinji Watanabe. 2017. <span>“Joint CTC-Attention Based End-to-End Speech Recognition Using Multi-Task Learning.”</span> <a href="https://arxiv.org/abs/1609.06773">https://arxiv.org/abs/1609.06773</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Speech},
  date = {2022-03-01},
  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w13-speech/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Speech.”</span> March 1, 2022. <a href="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w13-speech/">https://orenbochman.github.io/notes-nlp/notes/cs11-737-w13-speech/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Multilingual NLP</category>
  <category>NLP</category>
  <category>Notes</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w13-speech/</guid>
  <pubDate>Mon, 28 Feb 2022 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Models for multilingual ASR and TTS</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w17-ASR-TTS/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/tiling.png" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>course banner</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/33zeljs59mg" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Lesson Video
</figcaption>
</figure>
</div><div id="sup-slide-deck" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div>

<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>What is speech?</li>
<li>Speech applications</li>
<li>Speech databases</li>
<li>Speech hierarchy</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- Transcript here is machine generated no punctuation and I eddited it a but but it needs more work and I dont see the benefits unless I can run it through some tool. -->
<blockquote class="blockquote">
<p>Today’s lecture uh it is about multilingual speech processing i will mainly uh explain about the speech recognition but it also covers most of the techniques that are quite similar in the tts of speech translation and so on. First uh i will talk about n2 and base system because previous lectures we have run the end-to-end speech recognition so it may be better to start from that and then i will actually also uh explain our form-based system which is i call it the hm-based system and uh they actually are the multilingual processing there are some benefits of using phone based system so i will explain about it and for this uh marketing uh speech processing actually uh the cmu has been uh the contributing a lot of research the activities to the community and i’d like to also introduce this kind of activity okay so uh let me first start the uh the end to end the direction and this is actually one of the slider that i already showed when i introduced the end to end speech recognition so uh the the one of the kind of uh the uh difficulty of hm based speech recognition pipeline uh when i introduced this other figure uh is that requiring the linguistic resources and this is actually okay for the major languages like uh the english japanese mandarin and storm but if you if we move to the uh the other languages uh getting such kind of a linguistic resource is more difficult but still it’s possible so uh the one of the benefit i emphasize uh is that the end-to-end system doesn’t require a such kind of linguistic resources especially for the other phonetic uh the information and we can build other speech recognition so first uh the emphasize this kind of our order the other characteristics and then explained about the multilingual processing based on end-to-end speech definition however i uh emphasized several times uh this can be a process and that i will explain it later so first uh that i will explain about uh my experience of working on the end-to-end speech condition i started to work on the end-to-end speech recognition around 2015. i said one of the early adapter of other moving to entrance other systems compared with other researchers and then we actually tried to uh welcome the first uh english and it’s working quite well like the other people reported and then i am thinking about trying to the other languages which is japanese and the japanese is actually not a sr friendly language i would say so this is a typical sentence i actually extracted from one of my other japanese articles and this sentence is actually quite rather difficult to handle for speech recognition first there is no world boundary compared with other languages there is no word boundary and the other are the properties are that the someone may recognize anyway there are totally four scripts the mixed in this one sentence one is hiragana the other is katakana and the third one is kanji which is the original from china and roman alphabet and so on this kind of a mix of the script happened uh so often and actually that this other variety also making the pronunciation are very difficult it depends on the context on the it depends on the uh the uh the meaning uh some of the other the characters uh change the pronunciations uh so this uh the uh the pronouns providing the pronunciation uh is also very very difficult and the last uh the part is that one of the character may include many actually uh the phonemes like for example this uh and which is like the other languages but this character it actually has several pronunciations depending on the context but the one of the uh the way of uh the pronunciation uh we call this the character as kokoro jasi which other are 10 a phonemes five syllables only one character but the the cerebral are the language and the phoneme lengths are very very different so in general uh that it was regarded as that dealing with the japanese is very difficult due to this kind of four uh three uh reasons maybe there are several others and then what we will do is actually using the tokenizer or actually it’s just we don’t actually theory just doing the tokenization it’s actually trying to solve all this problem jointly uh by other concerns the pronunciation and some kind of a mathematical analysis and so on so this kind of a tokenizer morphological analyzer is very important when we start to work on japanese and the uh to do that uh japanese are the researchers because uh it’s more like a highly sourced side and there are a lot of researchers so we actually have a lot of very great tools to perform the tokenization which is very very good with that we can actually make speech definition work in japanese however the tokenizer also has an issue first it has some mistakes but it’s if the tokenizer is getting better and better probably we can ignore this kind of issues one of the most difficult issue for me is that it’s changed the result depending on the tokenizer if you’re using a different software different targets different dictionary uh tokenizer output will be changed and which we do the kind of speech recognition experiments we actually cannot compare them so this is actually quite a difficult problem not only program itself is kind of difficult problem if we’re using this as an other speech recognition uh this actually becomes a quite large barrier and actually uh the one of the example is that for example one company wants to use a tokenizer that they’re made by that company and the other company they want to use the other organizer developed by the other company and then that we are not easily going to compare the water since the unit of the world is different it happens in the company the divers belong to by the way so there is a such kind of issue anyway that is what pognasa is doing so this is one of the examples are using the tokenizer most widely used one megaboo and we just throw this text and then tokenize actually how to say the spirit uh each of the characters and as i mentioned this is not the the only things that we also have to care about the uh the pronunciation and the part of speech and so on so it’s actually this tool is not only tokenizing it but also providing a pronunciation this one is actually pronunciation and the part of speech information and so on so that we have to use this kind of tool to perform uh the speech condition which is great but it actually has a lot of problems as i mentioned and then i one of my research goal is actually want to remove uh tokenizer uh from the speech recognition uh the other process and i feel sorry about gram because gram is one of the person that make another very famous tokenizer are called kitty kitty yeah so actually a lot of people actually working on the tokenizer which is great and i also really appreciate this effort but at the same time as a pure sr purpose i actually want to skip this direction so that’s why they i actually started to apply end-to-end speech recognition to a japanese probably this is the first trial i’m not sure but at least in terms of the how does a paper uh our group is one of the first team that performing the end-to-end speech recognition uh with japanese without that the tokenizer and so on and actually the performance was very good surprisingly very good uh this is the uh calculate ten percent uh when i first tried this one and the data with a lot of techniques now it goes to five percent or less than five percent in the famous japanese benchmark called other corpus of spontaneous uh japanese and this is a wizard uh tokenizer to reaching this kind of performance so this experience was very good to me to actually moving to a multilingual end-to-end asr so as i mentioned japanese is in terms of the written form it’s very complicated but still just through the paired data it started to be working so maybe we can use this architecture to the multilingual and 2 and asl so this is actually given this story i started to apply and the speech equation to that multilingual processing and also color switching end to end asl and let me start to introduce the what are the margining of n2 and sr is doing so this is a kind of a typical pipeline of using the speech recognition and then if we uh they extend this one to the multilingual speech recognition first uh we have to do is detect which speak the which language is spoken by using the language detector or asking the user to put this information abroad anyway after the language detector we actually came almost completely separately built a speech recognition although of course uh this part which extraction part can be other than the unified and the part of the acoustic modeling part can be unified but generally to build uh the speech recognition in a multilingual method we actually have to uh prepare a lot of kind of linguistic resources or lot of kind of engineering to build various languages and then my attempt is to make it with a single neural network okay let’s start to discuss what kind of techniques i am using but actually this is super simple what i use is just first collect 10 languages and then train train regarding each other the one corpus and then uh the train the single neural network basically that’s it and i didn’t do any kind of special things except for by following the machine translation convention i also put the language id in the beginning of the sentence so that first network predict the language id which correspond to the language detector and then the other uh the transcription of that language is following so basically that’s it so uh i uh this is more like just using the data augmentation but no sorry data preparation and then added i can either make this kind of a neural network and note that i don’t use any other pronunciation other dictionary and so on and this is one of the uh the most kind of a difficult part when we don’t have a knowledge to get the kind of other access to the various language resources of course if there are some kind of knowledge definitely we can do but in general it is not easy to have such kind of access of that so due to that this process is quite easy and let’s check the performance actually performance was also okay i tried this one with the 10 languages mostly yeah the the languages in europe and the i mixed the chinese and japanese and the bruven is a language dependent uh which means that we actually built a sr system for each language and the red one is just uh they combine everything and the one single neural network to recognize this mixed 10 language speech and surprisingly it’s working well note that some languages especially when the uh the data is enough it’s actually degraded performance however some cases the improvement is quite large and this improvement comes from the data sharing structure since we kind of mix all the data and at least encoder part would be possibly doing a language independent processing so by mixing this kind of the other language data which is very helpful to regularize the encoder part of the list and then it’s working quite well in this experiment and the language prediction is almost perfect uh almost hundred percent in the each of the luggage pair except for the uh the spanish italian it has some kind of uh the similarity in the language and the language recognition is a little bit difficult but other than that our language recognition part is also perfect and the uh this is uh the applied to the other major languages and then i also moved to the other kind of low resource languages uh which is collected from the available bubble project and by the way some of the these all represent the hero and some of them are actually uh when i cut and paste the characters it’s as this but the information is disappeared this kind of how they say issues uh happens often so uh when you apply the monitoring of processing please be careful about copy and paste sometimes it’s not working and then that even for this uh the language uh the uh low resource uh language cases i had a kind of similar trend that mixing the language and the building a single neural network seemed to be working quite well and actually finally i uh my colleagues actually even uh extend this direction to uh using the 100 uh language it’s it’s not exactly 190 something languages are using the cmu uh wilderness data that professor black collected for this approach and then i we got similar performance uh based on that okay so uh this is actually a question so how many people were importing this development so 10 of course you know given that we have an n2 and airsoft and then uh we have to build 10 other language speech recognition how many people involved in the development and someone answered okay the answer is only one that many people actually expect only me actually and how long did it take to build this system 10 language good question very very good question the answer is that date of preparation one day gpu one week what 10 days sorry at that time yeah now if we use you know other several gpas probably uh that two to three days to finish this kind of uh the training and the linguistic i would say that only what i use is the unique order and this is actually one one of my mistake i use a python two i should have used python three and then the the things are more easy but the uh uh anyway uh we didn’t don’t have to use this kind of linguistic knowledge um with this kind of effort of course after that we actually write a paper about this one and when writing the paper we also need to have our uh other researchers help to uh do additional experiments and so on uh so they totally the uh two that finish this paper uh we actually uh they have a three or four also but the most of the work was all done by only me and then actually we submitted this paper to sru which is one of the uh the most kind of famous uh speech recognition a workshop and then this paper i got the best paper candidate not the best paper the selected are the uh nominated that are the best paper but we kind of lost today the best paper but i think it’s okay you know the other people spending you know a lot of time you know three months or half half year and a lot of kind of resources to and then finally light of paper this one as i mentioned most of the work one day of the scripting work and then a light of paper and then we got the kind of uh the best candidate so i’m actually very satisfied with this result okay so then uh the the important thing uh is that this kind of effort can be done just using the data preparation this is a very kind of a cool part of the uh using the this kind of end-to-end system and then i actually from now not i we i would say that my colleagues they started to extend this methodology even for the uh the other code switching and what we did in the very beginning of this experiment was that we just concatenate two sentences which is different languages that’s it and then simulate the code sitting uh we know that this is simulation you know there’s uh the the code switching uh the code reaching may happen in the uh the same speaker and even in the within the one sentence the code of sitting may happen but as a proof of concept we started to work on uh this kind of code switching with just mixing the data and then it’s actually other working other quite well i mean by using the uh the wizard quarter switching uh the of course uh the the water rate is quite uh high however uh by using the other code switching uh the simulation uh we can actually uh the correctly detect the other language boundary and then under the perform the code switching a speech condition uh the in this other experiment however i i just want to emphasize that this is accumulation the most difficult part of the code switching is actually we don’t have a real data so to do that in addition to the simulation uh we also have to the the uh the care about how to uh make simulation data to be close to real data which is a very important research direction now so anyway i will play with some of the audio and corresponding uh the uh the system out of it difference is the uh the difference uh the the ground tools and the sr is the other sr output and we use this kind of our other audio us exports rose in the month but not nearly as much as imports superior like this yeah okay it’s a simulation but uh the just using the data preparation uh we can handle uh this kind of code switch so this is a kind of uh uh one uh nice part of the end to end speech recognition uh we just uh try to make our program to be data preparation and then we can get to some other straightforward result of course to improve that to the real data is so long we need more effort but as a first step to do something it’s actually quite good and the other example is actually similar to the uh today’s uh the language stand that we are also working on the language uh indian jada language documentation andy uh is is actually uh working with the uh jonathan amis uh gets broken correct he will be a guest lecturer here actually uh this is the collaboration happened suddenly jonasson sent me email he wants to use speech question and then i am answered we have an end-to-end asl so if we have a you have data that we can do it and usually uh after the conversation this kind of a conversation happens often but usually it would be failed because the data size is very small however jonathan is great he actually collected over a hundred hours uh of this other endangered language called ios mistake which is in one of the village uh in mexico and he and his colleagues actually uh often visited uh this village and the other uh this area and collecting uh the data and the well-defined format so that we can actually perform the end-to-end sr but still the problem is that the uh the there are two kind of issues one is the other the transcription uh the the bottleneck it is very difficult to transcribe uh this kind of the uh the endangered language because it doesn’t cover uh the standard of the script form that we discussed before and the second one is that due to that we actually using a quite a linguistic oriented uh that script to uh transcribe the audio of this kind of endangered luggage which means that the transcriber must have a very good knowledge about linguistics so that the transcriber or the shortages is also very important uh the difficulty in the endangered languages and our end-to-end asl it’s actually as partly solving this club problem our performance is actually quite good uh it’s comparable to the novice transcriber it’s their process is actually two steps one step is a novice transcriber to transcribe it and then later expert will either correct it or expert will directly add that do it and the novice is just kind of a training uh by using the uh the the others uh the expat transcript and so on but anyway uh in this kind of a transcription process we usually have such kind of layers and the other end-to-end sr at least are comparable to this novice transcriber so which can mitigate possibly mitigate the transcriber shortage so uh that we start at the end-to-end sr is actually uh they’re showing some kind of uh the advantages uh they based on removing uh this problem in the endangered languages but i just want to emphasize that this is a very unique case this is other uh based on the great effort by other generations and historians to collect large amount of data hundred hours of data and then we can realize it in the other cases uh usually we don’t have so many uh transcribed data so the the other usual endangered language cases it is still very difficult for us this end-to-end area okay so anyway the end-to-end sl is somehow working but the uh the problem as i mentioned is that in general uh many languages don’t cover enough data and then what kind of technology we can use uh this is a transparent or fine tuning this is also very similar to the other machine translation and other other multilingual nlp techniques so the first we try to build a seed model which is uh that uh trained by uh the high resource language or uh the mix of the the multilingual data like i showed before and then uh making a big seed asl model and then given this kind of seed asl model we just have a small amount of target speech data and then added by using a seriousl model to transfer uh to the target language sr model by using the small amount of training data okay and then actually the one question so to transfer the model to the target language since the neural network has too many parameters right 100 million parameters and so on for standard speech recognition cases and then we usually freeze some of the layers and only fine tune some of the layers and then the question which layer we should fine tune in the language the transfer learning strategy in general i think everyone must have a correct answer in your mind actually uh we usually that only train higher layers that is enough first of course we have to change the uh the script from the given language to the uh the target language if they are not included right and then the last linearizer must be uh that are initialized must be fine-tuned and then the uh the speech recognition enter in the system or not in the system acoustic model or neural network in general lower layer more like doing some feature extraction to try to capture uh the invariant uh the phoneme like representation so if the kind of layer goes to gradually higher uh the it’s actually uh the uh goes to more kind of linguistic information and then the uh this kind of initial other fast layers are more towards to represent the acoustic information so that if we try to convert other per home the transfer learning from the big seed model to the target language model we should focus on tuning or fine-tuning the higher layers of a deep neural network in general by the way the question if for example the data is very noisy and the target is not language but to target to some kind of noisy speech feature layer we should fine-tune it should be shadow layers right so this is in general general property by the way so uh i am very lazy so i actually added other fine-tune all the layers because it’s rather easy to do it but yes yes yes but so i guess um fine tuning like just the early layers or fine-tuning just the light layer it’s just a very extreme version of regularization right you’re either like you’re basically regularizing some layers to not move at all then you’re regular you’re not regularizing right you’re basically allowing them to move freely and i wonder if there’s anything like in between those two i mean obviously that adds more hyper parameters to the model or something like is there is there anywhere that kind of more generally says well the layers at the bottom probably shouldn’t be moving much or they should be moving in a particular way um i don’t know so much about this book but definitely it should be a very good idea uh but the most people are just kind of analyzed these kind of things with the layer-wise other processing there are some work it’s not completely what you mean to say but some work is actually uh focusing on the like for example the uh some particular part of the network like for example inside the transformer some people only kind of adapt the feed of other parts and some people don’t control the freezer query key value and so on there are such kind of work exist but i actually don’t know so much about this kind of direction okay so uh there by doing that we can actually either perform the target target uh the transfer learning to the target uh the language and this kind of uh techniques are quite popular and mostly using uh for many other speech other processing tasks when we used uh try to have our target language uh the the the speech recognition and the tts and so on okay so uh the uh these are more like when we have anyway uh the pair data in the seed model but actually uh in some cases we have a huge amount of uh the speech-only data we don’t have so much uh the paired data uh how to kind of solve this problem and then actually people are using the cell supervised running in this other framework in the current speech recognition technologies and i just list the three techniques i will not dig into the uh each of the technique so if you are interested in that you can actually check some of the references one of them are web to back 2.0 actually 2.0 means that there is a prior study a web to vector which was okay pictures starting to be working but the after uh the the more effort uh the basically the making the training strategies simple and then by using the large uh the huge amount of the uh the speech on the data web dubac 2.0 started to be working and i’ll say that this is the first success in the speech recognition in terms of breaking the record of various benchmarks and then the later hubert the based cell supervised learning also is proposed by the uh the same other group uh the other meta people and this is actually i would say that simplified version compared with web to 3.0 web developer 2.0 is other very difficult to train in the uh the contrastive loss part while the cube part is actually quite similar to the other famous uh training criteria like masked language model and how to kind of make masks language model work in the hub is that instead of using the continuous speed representation uh they’re just using a k means and then this uh the hubert uh is using this k-means as a target to pre-train the model the k means it’s really simple k means like uh that we usually use in the other first or second uh the the courses in the pattern recognition and they decently uh the google actually uh pre uh the proposed the father simplified model called london projection quantizer and this is actually quite uh the sensational to me so before uh the uh london projection quantizer hubert or web feedback 2.0 anyway they try to somehow imitate phoneme as a target by using k means a contrastive loss or whatever however other london projection quantizer still using the some quantized target but how to get this quantized target they just convert mscc to the high dimensional uh features by using a random projection and then by you by sampling the some of the uh the uh the uh the centroid and then by using this other target for neural network so this london projection space is not close to volume at all however anyway this problem is very difficult and then by using this one as a target uh the neural network is somehow learning something and then it can be used as a fine-tuning uh of the initial model and the uh this runner projection quantizer is also other comparable performance to cuba to our website 2.0 and these are now very popular especially we have dubek 2.0 is very very popular right how many people harder the web to back the most people right i actually the people should know that because one of the advanced part of the assignment three is to use the either of the web to make the other human um and the i think uh the many people are using the uh s3pll shanghai right yeah so uh the our group is actually uh the the contributing not to making a pre-trained model but to make a how does a benchmark for this uh the speech self-supervised learning called the spark benchmark which is uh we try to kind of make a uh general benchmark including the speech recognition spoken language understanding the uh speaker recognition and so on and see whether this kind of or other self-supervised learning features are generally working on various speech recognition downstairs tasks and then through this kind of project we developed uh some kind of uh uh the mainly national taiwan university uh but that we actually uh developed together with them the software called s3prl which is a moroccan interface to use the various uh the pre-trend the cell supervised language model for speech recognition and then this one is actually called in espnet so that you guys can just changing the configuration file you can play with the hubert or web 2 back to uh and so on okay so this is about the end-to-end system and now i move to that form-based system doesn’t have so much time um so form-based system i mentioned that the uh without expert knowledge may have approximate accounts and i will explain about that actually some uh that many cases uh english technology is actually quite important to build a speech pension system for example uh remember our kind of a pipeline acoustic model lexicon language model and it looks like we need a paired data of the input and output right but please uh the focus on this part this is form well mostly people are using phonemes but they it can be universal form and then phone is actually language independent so if we collect a lot of kind of a speech data like english or mandarin and so on and making a phone recognizer and then later if we hover some kind of a long election of the target language we can actually connect all of this part so how to do it first build acoustic model but based on the form based language acoustic model and which can be possibly language independent and try to find the phonetic dictionary of the target language by us using the weak scenario or whatever are the using the linguistic resource and then sometimes if we don’t have enough uh the word coverage we can use the graphing dupont g2p techniques nowadays modeling part we don’t need a pair data right so we’re just using the text data to build language model so it turns out that we do not need a parallel data we just need to have our other descent and the phone recognizer and the language model uh to build the uh the speech recognition system for the target language so this approach is actually one of the method if we don’t have so much training data or if we don’t have our uh we if we don’t have so much training data or if don’t have any training data and this methodology is very difficult when we use it in the end-to-end system because we don’t have this kind of clear modularity right so actually in many of the other the low resource speech equation still form based hrm based system is popular and actually better than other end-to-end system if we don’t have enough other training data and lastly i would like to introduce some of our ongoing other project first one is end-to-end air cell this is actually you guys are contributing this project our group and everyone now is try to actually build uh the speech recognition for many languages as many as possible public data reproducible recipes are preparing the model and our cases we use the esp net but any other toolkit is fine but the intention of our kind of uh uh this other project is anyway try to cover uh the uh the speech recognition uh for other many languages as many as possible and again you guys are a very important contributor for this project and the other approach is a phone-based asl and this is actually possibly we can build 2000 languages of course it is very difficult to evaluate it so we are still kind of a remaining 100 languages at an evaluation but actually by using the home based techniques even we can expand our 2000 languages are based on our effort and there are a lot of other activities in cmu uh two that are working on the multilingual speech processing not only speech recognition tts speech translation and any other kind of spoken language processing and so on and i want to show you one of the web page that so this is under construction but we are now trying to make a kind of this kind of cmu merging or speech database and the as we said that we have our many languages like 8 000 languages and in terms of this espnet project uh model our coverage is actually only this area there are some missing information by the way we also have some other the models here and so on so the coverage can be more but at least our the speech requisition language coverage is now only 0.59 percent and even uh the the compasses can be more but still the uh the if we don’t include the bible purpose the coverage is very small but if we include a viable purpose it becomes nearly uh the other one and so on and the recipes are also very small so uh this uh the showing that that uh we actually need to work more to improve the coverage of this part and your assignment is actually contributing to improve the coverage to be a larger size okay uh this is the end of my uh the talk and the today’s discussion uh is how to improve the coverage of asl and tts technology what kind of effort we can do for the uh to improve the coverage and there are a lot of kind of dimensions that we can consider and let’s try to discuss and let’s try to kind of come up with a very cool ideas to improve the coverage of the monitoring of speech process okay let’s start the discussion</p>
</blockquote>
</div>
</div>
</div>



<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Models for Multilingual {ASR} and {TTS}},
  date = {2022-03-01},
  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w17-ASR-TTS/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Models for Multilingual ASR and TTS.”</span>
March 1, 2022. <a href="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w17-ASR-TTS/">https://orenbochman.github.io/notes-nlp/notes/cs11-737-w17-ASR-TTS/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Multilingual NLP</category>
  <category>NLP</category>
  <category>Notes</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w17-ASR-TTS/</guid>
  <pubDate>Mon, 28 Feb 2022 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Morphology</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w18-morphology/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/tiling.png" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>course banner</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/y9sVFrmGu0w" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Lesson Video from 2020 as the video was missing in 2022
</figcaption>
</figure>
</div><div id="sup-slide-deck" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div>

<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>What is speech?</li>
<li>Speech applications</li>
<li>Speech databases</li>
<li>Speech hierarchy</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- Transcript here is machine generated no punctuation and I eddited it a but but it needs more work and I dont see the benefits unless I can run it through some tool. -->
<p>hi everyone welcome back to multilingual natural language processing and today we start the next part of the course in which we go deeper into individual topics in multilingual nlp and we will talk about morphosyntax and we talk about morphosyntax because the same meaning can be grammaticalized in different ways in different languages for example some languages will grammaticalize the meaning of future as word others as morpheme and others as grammatical structure and today we will talk about morphology morphological processing morphological analysis and inflection generation and in the next class we will talk about syntax we’ll start with a reminder from the beginning of the course when we discussed linguistic typology we have discussed earlier the definition of a word what is a word is a slippery topic and we have agreed on the following definition that words are the building blocks of phrases and sentences that are members in a syntactic category so they have a part of speech that words tend to be able to move around relatively to each other and be composed into sentences so until now what i mentioned is a syntactic definition and words are linguistic units that usually have main stress sometimes as secondary stress this is a phonological definition and they can correspond to a unit of mean this is a semantic definition so all this together is a definition of a word which works rather robustly although there are still some problems for example related to idioms and multi-word expressions such as an expression by and large now words themselves are not atoms they have internal structure and morphology is the study of the formation and internal structure of words as you can see in this drawing a fundamental concept in morphology is that of morphine morphine is a minimal pairing of form and meaning by form we mean the sequence of sounds like on the bottom of this drawing 3 and it can be a sequence of letters for example spelling of the word 3 and this is a mapping between the here the phonetic string 3 and the meaning or content of the word and this mapping is called is a morpheme and we say that morpheme is a minimal pairing because we require that morpheme is a pairing that cannot be reduced to smaller subunits so for example the plural form of the word three trees will contain two more themes three and s plural marker okay so words are made of morphemes and what i highlighted in a sentence on the top or in the example of rich morphology of english on the bottom is what you can see is the concatenations of [Music] morphemes and there are three morphemes morphemes that occur independently for example the word establish and there are bound morphemes those that are attached to other words so in the word this establishment established is a free morpheme but their prefix this and suffix meant are bound more things words can undergo morphological processes or formal operations what are these morphological processes these are for example concatenation it is the most common morphological process if we concatenate the stem which is the core of the word with the affixes which are purely grammatical morphemes that cannot occur in isolation this process is called a fixation so in the example this establishment the stem is established and the there are prefix and suffix these and meant and the concatenation of if you concatenate prefix and then stem and the suffix both from two sides this process is called circumfixation so these are concatenative processes uh there are also non-concatenative processes uh for example when we add an infix in there so this when we add a morpheme in the middle of a string and another common type of morphological process is compounding when rather than concatenating bound morphemes we concatenate three morphemes we concatenate several stems like in the word dishwasher or in the word skyscraper so you can see that there are two types of morphological processes and these morphological several types of morphological processes more than two and these morphological processes can change the meaning of the word like for example with compounding or with [Music] the the modification of establishing to disestablish or they can change the grammatical function for example cats turning is a plural form of the word cat but the meaning has not changed here are some examples of interesting languages and their morphologies tagalog language is spoken in philippines it illustrates a great many form of morphological processes including prefixations fixation in fixation and reduplication so here the singular form is formed with prefix ma and then the plural form is formed with both prefixing prefixation and prefixing reduplication the infix boo and you can read more examples of interesting examples in the book that i reference below by laurie levin and david mortensen this is still a draft but a very nice draft human languages for artificial intelligence here are some other interesting examples arabic is a semitic language that uses templatic morphology in these languages root consists of two or four consonants and then morphology is uh represented by vowels and consonants which are inserted in between the root consonants so the root and the pattern of specific vowels each function like separate morphemes uh so they are combined like a beads in the stream and the uh on the bottom we can see chinese so although chinese has poor inflectional morphology it has many compound words and so there are compounding processes you can see examples below which i cannot read okay morphological functions so talking about morphological processes uh such as concatenation so compounding only tells us part of a story our next question in what are the functions of these operations the two broadest functional categories of morphology are derivation and inflection derivational morphemes examples are on the top are used to create new words they change meaning they can result in a new part of speech but of course not always for example the same example establish and disestablish and then another modification is this establishment then when we move from the verb to noun the second type of morphological function is uh and the important class of morphemes are inflectional morphemes they use to mark grammatical distinctions like cat and the suffix s will turn it into a cat but it does not change the meaning of the word so how do we formally discuss morphological properties of a language and learn about languages morphology in linguistics so in linguistics we introduce a interlinear gloss text igt interlinear gloss text is a standard way to explain the structure and properties of the language via examples it is described in detail in a document that i link on the top of the slide and i recommend you read this document if the topic of morphology is interesting to you and on the bottom i paste the simplest example of igt interlinear gloss text from the same document so you can see that there are three lines the top line corresponds to the original source language or object language which is in our case indonesian the third line is a translation of the first line into a meta language which is typically in linguistic papers this is english this is a lingua franca the second line is the gloss and it is between the first and the third one this is a line this is why it’s called interlinear every token in the second line is written in a meta language in english but it is not an english sentence this is an alignment with talking with tokens in their object language so you can see that the word they in jakarta now are aligned with the first line all tokens are aligned and this is important because this gloss contains information more complicated examples of this gloss contain information about morphosyntax of the object language which is indonesian and in this example we can see from this even simple example of igt we can see the difference between english and indonesian and in particular how english uses copula verb r b while indonesian doesn’t and here is another more complicated example of the interlinear gloss text between the morphologically rich russian this is our object language the top line and the bottom line is that translation of the sentence into english which is a meta language and in between you can see two lines representing a little bit different information this is optional both variants are possible this these are interlinear gloss text so the word the the fourth token in the russian sentence [Music] is split into three more themes corresponding to stem past tense and plural and you can see this specification in the igt lines two and three so this would be a stem go and then past tense and the number plural and the word bas of tobusan its fifth token in russian is split into two more themes corresponding to stem and suffix marking the case instrumental case so you can see from this linguistic description and alignment between the two sentences and the morphological analysis of individual talking in the top sentence how the morphologies of russian and english are different and you can see that morphology of russian is significantly more complex than the morphology of english in this slide which i will not read to you but you can use it later as a reference i list the types of morphological categories across parts of speech all languages possess the same set of about 25 categories number gender tense in if each of which have several functions so there are roughly 25 categories and roughly 100 functions for example a category for nouns a category [Music] number can of the object can be singular plural dual across different languages this corresponds to number another example of a category is noun class which uh noun classes are linked to grammatical gender in indo-european languages but in other languages this can be arbitrary set of categories and all nouns must belong to a category so an interesting example of noun classes uh is in swahili which is an african bantu language uh there are several classes i don’t remember i think it’s seven there is a class corresponding to anime animate nouns denoting people animals insects birds fish there is a different class corresponding to trees plants body parts and another chorus class corresponding to foods and so on and there is a separate class for singular animated nouns versus plural animate now so these are semantic classes and the different morphological rules different prefixes and suffixes are applied to different classes so languages differ in how they express these categories some use like seems like chinese vietnamese these are morphologically what we call poor languages some use freestanding grammatical morphemes such as pronouns prepositions so which means that they express the same categories but through syntax and some languages use affixes prefixes suffixes and this means that they express these categories through morphology so this is what it means when i said in the beginning of the class that different languages grammaticalize the this relationships and the cutting function categories and functions differently through words or through syntax or through morphology and this is why we have different types of languages which we categorize into morphological typology so there is also a disagreement on the typology but i present uh an analysis from david morton sense and the lori levine’s book and traditionally languages have been divided into four times types based on how morphologically rich they are languages like chinese vietnamese and a little less english are called isolating or analytic languages these languages have relatively little morphology and very little inflectional morphology with the exception of compounding like i showed in the example of chinese so most words consist of a single frame or theme languages that similarly to isolating languages have relatively few three more themes per word but unlike isolating languages have many prefixes and suffixes are called fusional or flexional languages examples include greek russian german there is a special subclass of fusional languages which are called templatic languages as i showed in previous example arabic and hebrew is also like this the next category is agglutinative or agglutinating languages these languages often have many more themes within a world examples include finnish turkish and swahili and these languages have very long words and finally the most morphologically rich languages are polysynthetic which can include the whole sentences whole propositions uh they can concatenate them into a single topic so analytic languages have lower morpheme to word ratio and higher restrictions of word order in a sentence languages with richer morphology synthetic languages typically have higher morpheme to word ratio in more free word order there are tight token curves that also can visualize how [Music] properties of morphologically rich languages so we can see that the richer the morphology of a language the higher is type to token ratio in a language and consequently the higher is the sparsity of tokens in the corpus here for example [Music] this paper by 2012 they took a multi-parallel corpus consisting of six languages with varying degrees of morphological complexity so these are the same sentences this is a multi-parallel corpus but written in different languages in english chinese german arabic turkish and korean and you can see that as the morphological complexity of the language increases the number of types in the corpus increases resulting in a steeper curve the most morphologically rich in this drawing is innoctitude which is a polysynthetic language it concatenates many morphemes together into single words and on top of this it has it has complex morphophonological uh processes between morphemes so uh in for inuktitut at 1 million tokens it it it has approximately 225 000 types compared to english which is for around 1 million tokens has only 30 000 types okay so due to morphological complexity and the diversity of languages there are several challenges in processing morphologically rich languages first is related to high lexical sparsity due to the variability of morphological forms this leads to sparse statistics many forms of words appear only once or few times even in very large corpus and there are many plausible word forms that do not appear at all these are called out of vocabulary words or ovs and these out of vocabulary or rare words that appear only once so twice lead to errors in systems such as machine translation speech recognition dialogue question answering and so on so on so this is the first challenge of processing morphologically rich languages the corpora no matter how large they are they are very sparse but on top of this morphologically rich languages are often resource poor so the problem of sparsity is much more acute specifically for these languages an additional challenge of morphologically rich languages is their complex word agreement this leads to agreement errors in language generation especially when we need to model agreement between words that appear far from each other in a sentence just because as i mentioned in some of the in the few slides before that the morphologically rich sentences are languages also have more free word order okay so on top of this high variability there is also a mismatch between morphologies of variability between morphologists across languages and language families and this is so mapping between morphologies of different languages is difficult this leads to difficulties in transfer learning when we want to transfer training corpora from language one language to another uh this can lead to problems this can also lead problems in translation errors for example when we translate from languages that don’t have the concept of definiteness like chinese or russian and so chinese and russian don’t have definite indefinite articles and when we would like to translate from chinese or russian into english that have definitely indefinite articles so these translations tend to contain errors and another problem is that differences in mapping across morphologies of different languages can lead to exacerbation or amplification of social biases for example when we translate from languages like turkish or hungarian that do not mark gender of third person pronouns like he or she and we translate from turkish or hungarian into english so it was shown in previous work that for example translation of lower status occupation would be translated with the female pronouns whereas the translations of higher status occupations would be translated with male pronouns so a turkish sentence that is written basically they are a nurse would be translated as she is a nurse and they are a scientist would be translated as he is a scientist and this just amplifies biases in english training data so to summarize main challenges for processing morphologically rich languages are high sparsity different errors in generation for example caused by uh in for the need to enforce agreement between words and the differences in morphological properties across languages that are different to map across languages and because of these challenges we do morphological processing and incorporate morphological knowledge into models explicitly so what are the types of morphological processing first morphological analysis and we can talk about morphological parsing or morphological segmentation the second is related to generation and the task can be inflection generation or full paradigm completion i will show later an example and the third less common task would be in acquisition of morphology although we have resources for hundreds of languages where we know their morphology uh thousands of languages we still don’t have enough knowledge about their mythological properties so the task of acquisition of morphology would focus on learning languages morphologies automatically so the most common task is morphological analysis and also this it is called morphological parsing our input is a word and our output is a word stem and features expressed by other morphemes for example given in input cats morphological analyzer’s output will be cat plus n which corresponds to noun plus pl which corresponds to the plural form so morphological parsing is the process of determining what are those morphemes given an input word and as you can see in these examples uh this information morphological information comes from three major sources first from lexicon so we need to know the stem second from morpho tactics we need to know morphosyntactic rules and third is from spelling or pronunciation rules because we need to know uh for example um the world cities uh we need to know that as a stem city which ends with a y in a plural form will be changed into cities with eye having this information about lexicon about morphotag tactic rules and about spelling rules we can build a morphological analyzer classical approaches to morphological analysis used finite state transducers and they achieved the high accuracies high results good results so uh in these approaches we have two tapes uh lexical and surface as in this example in the drawing on the top these drawings are from the girovsky martin textbook so and we build a finite state transducer so either one so it can be inverted so either lexical form can be in in surface form can be an input and the output can be stem plus morphological analysis or stem plus morphological analysis can be an input and the output would be a surface form so we will build a transducer so that given an input word cats it would read the word the input letter c and output c lead and read a output a read t output t then read the plus n and would output an empty string epsilon and then it would read plus pl plural and would output s so the output would be cats and it’s coming from lexical interpretation to the surface interpretation so on the bottom we have a simplified fst for morphological parsing cats goes from q0 to q1 from state q0 to state q1 this is a regular noun then q1 outputs an epsilon the for for the noun plus n and then we have we can go from q4 to output s a plural form and we have the end of string in q7 so we can build such an fst finance state transducer as a recognizer and it would say is it a legal string if we if we accept it and or if it’s not a legal string if it’s not a word illegal word in a language in this case the fst would reject it and we can build this fst as a translator in which we input a string of characters from the surface form and output the string of morphemes okay so in 2016 there were a surge of papers that started to replace fin state transducers with current neural networks so this is one of the many examples of papers that i present here by katarina khan and her collaborators ryan cotterell and henrik schutzen and they showed how to implement the same task of morphological analysis using recurrent neural networks and i present specifically this paper because it’s interesting for two reasons first they show how to do canonical segmentation rather than surface level segmentation of words so canonical segmentation so you can see in this example for the word achievability uh how would for the input achievability how would an output surface segmentation look like and how would output canonical segmentation look like so for a surface segmentation we will just segment the input sequence into substrings in canonical segmentation we don’t only segment the sequences into substrings but also replace substrings with the canonical form this canonical segmentation has several representational advantages [Music] because it shows more themes that are not obfuscated by morphology so it reduces the sparsity more significantly but it is also more challenging for example it would be difficult to implement with finite state transducers uh because it doesn’t only segment uh words into substrings but also reverses orthographic changes so how did they do it they first used the standard character based bi-directional uh gru encoder with attention and then they have a unidirectional gre decoder so this is a very standard sequence to sequence architecture uh run end with attention and then a novel component they introduced was a neural re-ranking for segments to identify canonical segments so in particular when they kind of produce the segmentation of a word they first output [Music] a surface level segmentation but then they did re-ranking and specifically looked for segments that are more frequent when they occur as an independent word is in a lexicon and for this they incorporated simple lexical information into word embeddings that marked an additional that added an additional dimension to the word embedding a binary feature that said whether this word is appears as an independent word in the lexicon or not so this is a standard sequence to sequence character-based model plus a re-ranker on top of it to prioritize words that appear more frequently as independent words in the lexicon so in case in this case the in the segmentation of a word achievability the word so the second token able would appear more frequently than the token abil so how do we evaluate morphological analysis there are several evaluation measures that are described in the paper in the previous slide and in several other papers first is error rate defined as 1 minus the proportion of guesses that are completely correct so proportion of 1 minus proportion of outputs of the model that are completely correct second is the added distance so when the first first is a hard measure it’s uh the correct or incorrect output eleven stain distance would measure the similarity between the gas and the gold standard form and finally morpheme f1 score calculates the precision and recall of correctly generated morphemes where the training corpora usually come from in this paper and other papers on morphological analysis a most common resource to work with is unimorph it annotates hundreds of languages using a unified schema it includes currently annotations for 110 languages and i put the link in the slide okay so this was about morphological analysis uh the second type of prominent type of direction of research is focusing on morphological generation in particular inflection generation there are two tasks one is uh to generate a single inflection so our input would be a lemma and grammatical features for example [Music] this is an example in german cow which is a calf and the grammatical features such as case nominative number plural infraction generator will output calvary and the second task which is a generalization of inflection generation task introduced relatively recently in single phone competitions that given a lemma as an input we would need to fill a table and this table this means we need to general generate all possible inflections of a word for all morphological categories so an example where inflection generation either inflection generation or paradigm completion is useful is for reducing sparsity of the corpus for example in machine translation machine translation suffers from data sparsity as i mentioned before when translated thing morphologically rich languages since every surface form is considered every token is considered as an independent entity and to alleviate the problem of sparsity we can translate into lemmas in the target language and then apply inflection generation as a post-processing step so these tasks morphological analysis morphological inflection generation paradigm completion they are widely explored in particular in the sigmar phone competitions so sigma phone is a special interest group on computational morphology and for phonology and they organize workshop and workshops and share tasks every language and i put here some interesting summaries of workshop tasks that you can look into and there is a lot of research so i really gave a high level overview morphological inflection generation in general the base model is a usually a simple bile stem with attention so some recurrent sequence to sequence model uh and then while the base model is simple there are several interesting challenges specific to morphology an interesting challenge for example is how to enforce monotonic alignment between input and output because we unlike in machine translation morphology we don’t need to do a lot of reordering to output to the output sequence uh second interesting challenge is how to better learn longer distance relationships between characters to model variation in inflection due to spelling and phonological rules in a language so some of the papers focusing on shared tasks specifically focus on for example adding a crf layer on top of the decoder to better model relationships between characters in the outputs and if you want to know more about the field you can start with the sigma phone 2020 share task summary which provides an overview and the historical development of the task and the links to interesting papers and to read a particularly interesting paper about morphological generation let’s read this paper that i put in this slide so it particularly focuses on low resource settings so morphological inflection has been pretty thoroughly studied in monolingual high resource setting but in this paper adonis and graham focus on um low resource perspective and uh on cross-lingual transfer across languages and your task for the discussion is just to read this paper and provide your insights for example you can provide the critique about an individual component of the model so the model in the paper consists of several different components so you can talk about an individual component or individual experiment or experimental setup and in particular you can propose ideas for follow-up work or how a specific modeling decision could be adapted to a language of your choice for example a language that you speak and that’s it thank you and see you tomorrow</p>
</div>
</div>
</div>



<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Morphology},
  date = {2022-03-01},
  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737-w18-morphology/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Morphology.”</span> March 1, 2022. <a href="https://orenbochman.github.io/notes-nlp/notes/cs11-737-w18-morphology/">https://orenbochman.github.io/notes-nlp/notes/cs11-737-w18-morphology/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Multilingual NLP</category>
  <category>NLP</category>
  <category>Notes</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/cs11-737-w18-morphology/</guid>
  <pubDate>Mon, 28 Feb 2022 22:00:00 GMT</pubDate>
</item>
</channel>
</rss>
