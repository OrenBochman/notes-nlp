<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>NLP Course Notes &amp; Research</title>
<link>https://orenbochman.github.io/notes-nlp/</link>
<atom:link href="https://orenbochman.github.io/notes-nlp/index.xml" rel="self" type="application/rss+xml"/>
<description>Course and Research notes</description>
<image>
<url>https://orenbochman.github.io/notes-nlp/images/nlp-brain-wordcloud.jpg</url>
<title>NLP Course Notes &amp; Research</title>
<link>https://orenbochman.github.io/notes-nlp/</link>
</image>
<generator>quarto-1.6.39</generator>
<lastBuildDate>Sun, 23 Feb 2025 11:00:17 GMT</lastBuildDate>
<item>
  <title>Floating Constraints in Lexical Choice</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/1997-floating-contraints-in-lexical-choice/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>Literature review</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/DffGdrfY9gI" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid quarto-uncaptioned" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1
</figcaption>
</figure>
</div></div>
<section id="context" class="level2">
<h2 class="anchored" data-anchor-id="context">Context</h2>
<p>This is one of the three paper mentioned by <a href="https://en.wikipedia.org/wiki/Kathleen_McKeown">Kathleen McKeown</a> in her heroes of NLP interview by Andrew NG. The paper is about the floating constraints in lexical choice.</p>
<p>Looking quickly over it I noticed a couple of things:</p>
<!--
@article{mckeown1997floating,
  title={Floating constraints in lexical choice},
  author={McKeown, Kathleen and Elhadad, Michael and Robin, Jacques},
  year={1997}
}
-->
<ol type="1">
<li>I was familiar with Elhadad’s work on generation. I had read his work on FUF Functional Unification Formalism in the 90s which introduced me to the paradigm of unification before I even knew about Prolog or logic programming.</li>
<li>I found it fascinating that McKeown and Elhadad had worked together on this paper and even a bit curious about what they were looking into here.</li>
<li>Since McKeown brought it up, it might intimate that there is something worth looking into here. And it is discusing aspects of generative language models.</li>
</ol>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>Lexical choice is a computationally complex task, requiring a generation system to consider a potentially large number of mappings between concepts and words. Constraints that aid in determining which word is best come from a wide variety of sources, including syntax, semantics, pragmatics, the lexicon, and the underlying domain. Furthermore, in some situations, different constraints come into play early on, while in others, they apply much later. This makes it difficult to determine a systematic ordering in which to apply constraints. In this paper, we present a general approach to lexical choice that can handle multiple, interacting constraints. We focus on the problem of floating constraints, semantic or pragmatic constraints that float, appearing at a variety of different syntactic ranks, often merged with other semantic constraints. This means that multiple content units can be realized by a single surface element, and conversely, that a single content unit can be realized by a variety of surface elements. Our approach uses the Functional Unification Formalism (FuF) to represent a generation lexicon, allowing for declarative and compositional representation of individual constraints. – <span class="citation" data-cites="mckeown1997floating">(McKeown, Elhadad, and Robin 1997)</span></p>
</blockquote>
</section>
<section id="outline" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li>Introduction
<ul>
<li>Mentions the complexity of lexical choice in language generation.</li>
<li>Notes the diverse sources of constraints on lexical choice, including syntax, semantics, lexicon, domain, and pragmatics.</li>
<li>Highlights the challenges posed by floating constraints, which are semantic or pragmatic constraints that can appear at various syntactic ranks and be merged with other semantic constraints.</li>
<li>Presents examples of floating constraints in sentences.</li>
</ul></li>
<li>An Architecture for Lexical Choice
<ul>
<li>Discusses the role of lexical choice within a typical language generation architecture.</li>
<li>Argues for placing the lexical choice module between the content planner and the surface sentence generator.</li>
<li>Describes the input and output of the lexical choice module, emphasizing the need for language-independent conceptual input.</li>
<li>Presents the two tasks of lexical choice: syntagmatic<sup>1</sup> decisions (determining thematic structure) and paradigmatic<sup>2</sup> decisions (choosing specific words).</li>
<li>Introduces the FUF/SURGE package as the implementation environment.</li>
</ul></li>
<li>Phrase Planning in Lexical Choice
<ul>
<li>Describes the need for phrase planning to articulate multiple semantic relations into a coherent linguistic structure.</li>
<li>Explains how the Lexical Chooser selects the head relation and builds its argument structure based on focus and perspective.</li>
<li>Details how remaining relations are attached as modifiers, considering options like embedding, subordination, and syntactic forms of modifiers.</li>
</ul></li>
<li>Cross-ranking and Merged Realizations
<ul>
<li>Discusses the non-isomorphism between semantic and syntactic structures, necessitating merging and cross-ranking realizations.</li>
<li>Provides examples of merging (multiple semantic elements realized by a single word) and cross-ranking (single semantic element realized at different syntactic ranks).</li>
<li>Emphasizes the need for linguistically neutral input to support this flexibility.</li>
<li>Explains the implementation of merging and cross-ranking using FUF, highlighting the challenges of search and the use of bk-class for efficient backtracking.</li>
</ul></li>
<li>Interlexical Constraints
<ul>
<li>Defines interlexical constraints as arising from alternative sets of collocations for realizing pairs of content units.</li>
<li>Presents an example of interlexical constraints with verbs and nouns in the basketball domain.</li>
<li>Discusses different strategies for encoding interlexical constraints in the Lexical Chooser.</li>
<li>Introduces the :wait mechanism in FUF to delay the choice of one collocate until the other is chosen, preserving modularity and avoiding backtracking.</li>
</ul></li>
<li>Other Approaches to Lexical Choice
<ul>
<li>Reviews other systems using FUF for lexical choice, comparing their architectures and features to ADVISOR-II.</li>
<li>Discusses non-FUF-based systems, categorizing them by their positioning of lexical choice in the generation process and analyzing their handling of various constraints.</li>
</ul></li>
<li>Conclusion
<ul>
<li>Summarizes the contributions of the paper, highlighting the ability to handle a wide range of constraints, the use of FUF for declarative representation and interaction, and the algorithm for lexical selection and syntactic structure building.</li>
<li>Emphasizes the importance of syntagmatic choice and perspective selection for handling floating constraints.</li>
<li>Notes the use of lazy evaluation and dependency-directed backtracking for computational efficiency.</li>
<li>Mentions future directions, such as developing domain-independent lexicon modules and methods for organizing large-scale lexica.</li>
</ul></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;of or denoting the relationship between two or more linguistic units used sequentially to make well-formed structures. The combination of ‘that handsome man + ate + some chicken’ forms a syntagmatic relationship. If the word position is changed, it also changes the meaning of the sentence, eg ’Some chicken ate the handsome man</p></div><div id="fn2"><p><sup>2</sup>&nbsp;Paradigmatic relation describes the relationship between words that can be substituted for words with the same word class (eg replacing a noun with another noun)</p></div></div></section>
<section id="reflection" class="level2">
<h2 class="anchored" data-anchor-id="reflection">Reflection</h2>
<ul>
<li>In unification-based approaches, any word is a candidate unless it is ruled out by a constraint. With an LLM a word is selected based on its probability.</li>
<li>The idea of constraints is deeply embedded in Linguistics. The two type that immediately come to mind are:
<ul>
<li>Subcategorization constraints which are rules that govern the selection of complements and adjuncts. These are syntactic constraints.</li>
<li>Selectional constraints which are rules that govern the selection of arguments. These are semantic constraints</li>
</ul></li>
<li>Other constraints that I did not have to deal with as often are pragmatics and these are resolved from the state of the world or common sense knowledge.</li>
<li>In FUF there are many more constraints and if care isn’t taken, the system can become over constrained and no words will be selected.</li>
<li>One Neat aspect of the approach though is that FUF language could be merged with a query to generate a text.</li>
<li>Unfortunately as fat as I recall the FUF had to be specified by hand. The only people who could do this were the people who designed the system. They needed to be expert at linguistics and logic programming. Even I was pretty sharp at the time the linguistics being referred was far too challenging. I learned that in reality every computational linguist worth their salt had their own theory of language and their own version of grammar and that FUF was just one of many.</li>
<li>It seems that today attention mechanisms within LSTMs or Transformers are capable of learning a pragmatic formalism without a linguist stepping in to specify it.</li>
</ul>
<p>Why this is still interesting:</p>
<p>Imagine we want to build a low resource capable language model.</p>
<p>We would really like it to be good with grammar Also we would like it to have a lexicon that is as rich as the person it is interacting with. And we would also like to make available to it the common sense knowledge that it would need to use in the context of current and possibly a profile based on past conversations…. On the other hand we don’t want to require a 70 billion parameter model to do this. So what do we do?</p>
<p>Let imagine we start by training on Wikipedia, after all wikipedia’s mission is to become the sum of all human knowledge. By do we realy need all of the information in Wikipedia in out edge model?</p>
<p>The answer is no but the real question is how can we partition the training information and the wights etc so that we have a powerful model with a basic grammar and lexicon but which can load a knowledge graph, extra lexicons and common sense knowledge as needed.</p>
<p>So I had this idea from lookin at a <span class="citation" data-cites="ouhalla1999introducing">Ouhalla (1999)</span>. It pointed out there were phrase boundries and that if the sentence was transformed in a number of ways t would still make sense if we did this with respect to a phrase but if we used cucnks that were too small or too big the transofrmation would create malformed sentences. The operations of intrerest were movement and deletion.</p>
<p>and that can still have a model that is both small but able to access this extra material.</p>
<p>One direction seems to me is that we want to apply queries to all the training material as we train the model. Based on the queries that we retrieve each clause or phrase we can decide where we want to learn it and if we want to learn it at all.</p>
<p>(If it is common sense knowledge we may already know it. If it is semi structured we may want to put it into wikidata. If it is neither we may want to avoid learning it at all. We may still want to use it for improving the grammar and lexicon.) However we may want to store it in a speclised lexicon for domains like medicinee or law.</p>
<p>We could also decide if and how to eliminate it from we want to The queries should match each bit of information in the sentence. These are our queries. While the facts may change form</p>
<ol type="1">
<li>say we want to decompose a wikipedia article</li>
</ol>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-mckeown1997floating" class="csl-entry">
McKeown, Kathleen, Michael Elhadad, and Jacques Robin. 1997. <span>“Floating Constraints in Lexical Choice.”</span>
</div>
<div id="ref-ouhalla1999introducing" class="csl-entry">
Ouhalla, J. 1999. <em>Introducing Transformational Grammar: From Principles and Parameters to Minimalism</em>. A Hodder Arnold Publication. Arnold. <a href="https://www.google.com/books?id=ZP-ZQ0lKI9QC">https://www.google.com/books?id=ZP-ZQ0lKI9QC</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Floating {Constraints} in {Lexical} {Choice}},
  date = {2025-02-23},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/1997-floating-contraints-in-lexical-choice/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Floating Constraints in Lexical
Choice.”</span> February 23, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/1997-floating-contraints-in-lexical-choice/">https://orenbochman.github.io/notes-nlp/reviews/paper/1997-floating-contraints-in-lexical-choice/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/1997-floating-contraints-in-lexical-choice/</guid>
  <pubDate>Sun, 23 Feb 2025 11:00:17 GMT</pubDate>
</item>
<item>
  <title>Domination Games</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/posts/2025-02-18-domination/</link>
  <description><![CDATA[ 





<section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p>In a previous post titled <a href="..\25-02-17-samurai-world\index.qmd">Samurai’s world</a>, I considered the complex states and a framing game of a Lewis signaling game for which the requisite linguistics aspects of politeness and formality should emerge out of language evolution</p>
<p>Today though I’d like to pause from signaling for a bit and consider the establishment of hierarchies in a multi-agent system. This is also an extension that seems of interest to Sugarscape and other agent based models where one might be interested in the emergence of social hierarchies.</p>
<p>For language generation we may assign the hierarchy arbitrarily. But to speed learning we may want both the language and the hierarchy to emerge together. This can let us consider how different social structures may result in different linguistic structures.</p>
</section>
<section id="establishing-a-hierarchy." class="level2">
<h2 class="anchored" data-anchor-id="establishing-a-hierarchy.">Establishing a hierarchy.</h2>
<p>So lets lay down some ground rules. In our society the agents are initially an Egalitarian society meaning they are without a hierarchy or language. They are heterogenous and they can also evolve and learn via RL.</p>
<p>The game takes a decentralized forms for agent interactions and we are generally interested in finding an form of incentive often embodied as rule and or a utility that can when maximized individually leads to better overall performance in the society. This can be in the carrying capacity, mean wealth, or social welfare or expected progeny.</p>
<p>As the simulation progresses conditions may change to favor different strategies. For example if the carrying capacity of the environment changes, the agents may need to change their strategy to adapt. Agents less fit may die out, while fitter agents may migrate to greener pastures.</p>
<p>Eventually though the agents will have to interact with each other strategically to maximize their utility. Agents that don’t learn to do so will face greater risks of being eliminated. Further on agents may need to cooperate, coordinate and compete as groups to survive. It is somewhere along this axis that learning language and establishing social hierarchies may be increasingly beneficial.</p>
<p>Finally although the focus here is growing hierarchies I think that we should agree that that we are interested in the interplay of social structures and language.</p>
</section>
<section id="dual-view-of-hierarchies" class="level2">
<h2 class="anchored" data-anchor-id="dual-view-of-hierarchies">Dual view of Hierarchies</h2>
<p>In terms of society the cost of a leader’s actions and decisions may have far reaching impact on the future of group. The benefits and costs may be in proportion to the group’s size. Given the costs of bad decisions larger groups have vested interests in seeing that the leaders are competent and that the group’s resources are used wisely.</p>
<p>The leader on the other hand may like to do as they please, avoid criticism and challenges to their authority. They may also wish to avoid the costs of bad decisions by foisting them onto others. To do this they would like to reduce the group they are accountable to.</p>
<section id="the-principal-agent-dilemma" class="level3">
<h3 class="anchored" data-anchor-id="the-principal-agent-dilemma">The principal-agent dilemma</h3>
<p>The ability of leaders to pick members of the hierarchy based on loyalty rather then merit subverts the society’s goals yet this is often the paths taken by leaders.</p>
<p>Looking at emergent hierarchies we would like to study how the group can best select a hierarchy and leaders that primarily serve the group’s interests. The leaders of the hierarchy may try to game the system to serve their own interests. This is called the principal-agent dilemma and is a common problem in economics and politics. And we should formalize this aspect of the the game as it appears there are no political systems that are immune to this problem.</p>
</section>
</section>
<section id="emergent-hierarchies" class="level2">
<h2 class="anchored" data-anchor-id="emergent-hierarchies">Emergent Hierarchies</h2>
<p>Dominance hierarchy is common in the nature. While most of the time it is established by physical characteristics, we have many examples where other mechanisms. The use of brute force is risky for the individual and in the long term for the group.</p>
<ul>
<li>Egalitarian</li>
<li>Age group - initially all have the same age</li>
<li>Dominance - some agents are stronger</li>
<li>Egalitarian meaning no hierarchy</li>
<li>Partiarchal - succession is by the oldest son progeny of the leader</li>
<li>Matriarchal - succession is by the oldest daughter progeny of the leader</li>
<li>Oligarchy - some agents are more fit in harvesting resources and may be able to increase this advantage via trade. This should lead to specialization and division of labor under suitable circumstances.</li>
<li>Oligrachy with welfare. In Japan the CEO’s pay may only be 20x that of the lowest paid employee. Another way to improve welfare is to require an agent to share wealth when they come into contact with agents which are worse off. These gifts though can be used to keep track of social status based on amount given and received!</li>
<li>Meritocracy - this is a form of hierarchy where agents are ranked by their ability to perform a task. This means that there are many possible hierarchies in a group. However specialization implies we will consider individuals in a group for their top roles for skills that are have unmet demand by more qualified agents.</li>
<li>Aristocracy - authority is based on birthright and passes by rules of succession. Land ownership and rents dervided from these are restricted to a few families. Power is derived from wealth and influence can be increased by wealth, marriage, and alliances.</li>
</ul>
<p>Another aspect of dominance hierarchies is that they can lead to chaos or disharmony whenever the leader’s ability falls into question. This can lead to a challenge to the leader’s authority.</p>
</section>
<section id="games-of-domination" class="level2">
<h2 class="anchored" data-anchor-id="games-of-domination">Games of domination</h2>
<p>Leaders will wish to deter challenges to their authority i.e.&nbsp;disagreements over their actions or decisions for the group. This can be done in many ways. Here are a few:</p>
<ul>
<li>they can do this by being requiring that challengers first qualify</li>
<li>i.e.&nbsp;win a tournament that places them as the current challenger</li>
<li>they may need to also bring some resource, i.e.&nbsp;a wager, to the table</li>
<li>the challenges require a quorum and may take place only at certain times like once a year.</li>
<li>the challenged may also decide the terms of the challenge like the weapons used and the stakes involved.</li>
</ul>
<p>They may go further yet by outlawing such challenges and punishing those who do so. This leads to <a href="https://en.wikipedia.org/wiki/Selectorate_theory">selectorate</a> theory where the leader may want to keep the selectorate small to reduce the risk of being challenged. I.e. the pool of people who can pose challenges are kept small and given high incentives to support the leader. This may be done by rewarding them with resources or status well beyond they could achieve on their own.</p>
<p>So in terms of this game:</p>
<p>if <img src="https://latex.codecogs.com/png.latex?X%20%3C%20Y%20%5Cwedge%20X%20%3E%20Z%5C%20%5Cforall(Z%20%3C%20Y)"> X then X may challenge Y. Y may however consider his and X’s interinsics and skill abd set the terms most favorable to him.</p>
</section>
<section id="winner-takes-all" class="level2">
<h2 class="anchored" data-anchor-id="winner-takes-all">winner takes all:</h2>
<ul>
<li>The most extreme format of domination, yet one which may also deter challenges to the leader</li>
<li>The winner takes the loser’s life, status, possession, mates, and can kill his progeny and may impose similar punishments to the loser’s group.+</li>
<li>Leaders will want to avoid having to play in this game and may</li>
</ul>
<p>In the next three sections we will consider social structure that are common both family and state levels and are setup to maintain a heirarchy.</p>
</section>
<section id="patriarchy" class="level2">
<h2 class="anchored" data-anchor-id="patriarchy">Patriarchy</h2>
<ul>
<li>the oldest son progeny of the leader takes the leader’s social role in the event of the leader’s death.</li>
<li>the leader may also decide the heir to the throne is someone else other than his oldest son.</li>
</ul>
</section>
<section id="matriarchy" class="level2">
<h2 class="anchored" data-anchor-id="matriarchy">Matriarchy</h2>
<ul>
<li>the oldest daughter progeny of the leader takes the leader’s social role in the event of the leader’s death.</li>
</ul>
</section>
<section id="succession" class="level2">
<h2 class="anchored" data-anchor-id="succession">Succession</h2>
<ul>
<li>the leaders oldest son or daughter may be the heir to the throne.</li>
<li>if the leader has no children, the succession is by the oldest direct descendant of the most recent leader.</li>
</ul>
<p>It seems that our agents might need a leaders. And the leaders need a leader too. This might be a prequisite for planning in which an agent assigns tasks to other agents. This wouldn’t work well if all agents shared the same role.</p>
<p>One way to go is by age of the agent. The older agents are the leaders. But what if there are no older agents in the group? Another aspect is skill and specialization. To be assigned certain roles agents need to demonstrate skills. This may involve rites of passage or other tests to initiate agents into a new role. E.g. masai hunting a lion solo with a spear</p>
<p>We need a game for establishing a hierarchy.</p>
<p>The game of domination can be used to create social hierarchy of agents. Domination can be risky and costly. But once established it can be used to assign tasks. This means that any tasked one is given may be reassigned to a lower status agent.</p>
<p>The rewards for such social tasks may be shared. - they may be shared by the group - they may be given to the leader to keep/share/distribute/assign - they may be given</p>
<ul>
<li>It requires time.</li>
<li>It requires witnesses.</li>
<li>Agents are at risk of losing face</li>
<li>Agents may also risk injury or death if the opponent is stronger.</li>
<li>An agent of lower or equal status may challange another agent to a duel.</li>
<li>The challange may be</li>
<li>The winner takes the loser’s status.</li>
<li>The loser is demoted to the status of the winner.</li>
<li>The winner may also take the loser’s resources.</li>
<li>The winner may also take the loser’s mate.</li>
<li>The winner may also take the loser’s life.</li>
<li>This has a cost in terms of resources and status.</li>
<li>Viewers may wager on the outcome of the duel.</li>
<li>The challenged decides the terms of the duel.
<ul>
<li>The duel may be to the death or to first blood.</li>
<li>The duel may be non-lethal e.g.&nbsp;a mating dance or call</li>
</ul></li>
<li>Note: that the ideal leader is not always the strongest or most effective killer. But perhaps the one who can best assign tasks and resources to make the group most effective.</li>
<li>duels to the death may be effective for deterring challenges to the leader, but they also risk the loss of one or two valuable member of the group each time they take place.</li>
<li>thus a challenge to the leader may need to qualify, say by dominating all the warriors younger then the leader.</li>
<li>so we can see there may be soft or hard mechanisms for establishing a hierarchy.</li>
<li>there could be multiple hierarchies in a group.
<ul>
<li>hunters may not want to participate in foraging tasks and so the foragers may establish their own hierarchy.</li>
<li>hunters may also decide rank by ability with a spear pow or throwing rocks.</li>
<li>elders may establish their own hierarchy and act as advisors to the leaders.</li>
</ul></li>
</ul>
<p>We probably want the hierarchy to be transitive.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Domination {Games}},
  date = {2025-02-18},
  url = {https://orenbochman.github.io/notes-nlp/posts/2025-02-18-domination/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Domination Games.”</span> February 18, 2025.
<a href="https://orenbochman.github.io/notes-nlp/posts/2025-02-18-domination/">https://orenbochman.github.io/notes-nlp/posts/2025-02-18-domination/</a>.
</div></div></section></div> ]]></description>
  <category>Game Theory</category>
  <category>Agent Based Models</category>
  <category>Social Structures</category>
  <category>Hierarchies</category>
  <guid>https://orenbochman.github.io/notes-nlp/posts/2025-02-18-domination/</guid>
  <pubDate>Mon, 17 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>A Samurai’s World</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/posts/2025-02-17-samurai-world/</link>
  <description><![CDATA[ 





<blockquote class="blockquote">
<p>“if proof theory is about the sacred, then model theory is about the profane” – <span class="citation" data-cites="van2012logic">(Dalen 2012, 3:1)</span></p>
</blockquote>
<section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p>In the previous post, “Further Desiderata for Emergent Language”, I discussed the need to adapt emerging languages to conform with established properties of natural languages. You can read more about motivations in that post.</p>
<p>Rather then considering all the ways this could be achieved in a single essay, I believe that this should be broken down and considered one aspect at a time. This can make both the linguistics and the development easier to understand.</p>
<p>Once enough examples are available it should become clearer on how to integrate the desiderata using suitable states or other mechanisms.</p>
</section>
<section id="the-task" class="level2">
<h2 class="anchored" data-anchor-id="the-task">The task</h2>
<p>Moving on I’d like to tackle the capacity of certain languages to encode politeness and honorifics. This is a feature that is present in many natural languages, and it it bering some awareness that may be absent from many ai systems where agents lack a sense of social structure.</p>
<p>In this post I’d like to consider the emergence of politeness in a multi-agent language game. We saw that politeness emerged as a way to avoid conflict and to maintain social harmony. In this post, we will explore the emergence of honor in a multi-agent language game. Honor is a concept that is closely related to politeness, but it has some distinct features that make it an interesting topic of study.</p>
</section>
<section id="over-thinking-politeness" class="level2 callout-info">
<h2 class="anchored" data-anchor-id="over-thinking-politeness">Over thinking politeness</h2>
<p>Is politeness is not some random flag in some language?</p>
<p>I think that language is a social construct and that politeness is a social construct. It is not just a flag that is set in a language, but rather a complex set of rules and norms that govern how people interact with each other.</p>
<p>The social and biological basis for politeness can be viewed as an expression of dominance hierarchies and submission. In this view, politeness is a way of signaling respect and deference to others, and it is a way of maintaining social harmony and avoiding conflict associated with reestablishing dominance hierarchies when there are disputes within a social group.</p>
<p>This suggests three phenomena that are relevant to the emergence of politeness in a multi-agent language game:</p>
<ol type="1">
<li>Face for a speaker has a dual. It is the ability and a requirement of an individual to express their dominance and status with respect to his subordinates in society as well as the acknowledgment of the , but more so it due to subordinates giving the speaker their due respect. Thus loss of face can happen in private but is more significant in public. Face can be lost when the an individual is shamed by actions as well as words.</li>
</ol>
<p>Any loss of face in public may be viewed as a challenge to the speaker’s dominance and status. Such a challange when viewed by some third party may be viewd by others at large as destablizing the social harmony of the established order embodied in the entire hierarchy.</p>
<p>could be seen as a challenge to the speaker’ 2. Informal speech may be used when there is no conflict or challenge to dominance hierarchies. 3. Politeness forms that are ingrained in a language are means to permanently establish dominance hierarchies. and to avoid conflict by signaling respect and deference to others.</p>
</section>
<section id="how-is-politeness-encoded-in-japanese" class="level2">
<h2 class="anchored" data-anchor-id="how-is-politeness-encoded-in-japanese">How is politeness encoded in Japanese?</h2>
<ul>
<li>Politeness persists into modern Japanese, and is a challange for learners of the language who wish to master it as they must become aware of social aspects in which they are participating.</li>
<li>Japanese has three forms of politeness and formality:
<ul>
<li>informal
<ul>
<li>the informal form is used with friends, family, and people of lower social status.</li>
</ul></li>
<li>Polite (desu/masu)
<ul>
<li>the polite form is used in formal situations, with strangers, and with people of higher social status.</li>
<li>Verbs end in -masu (affirmative) or -masen (negative); copula is desu.</li>
</ul></li>
<li>Honorific Language <strong>Keigo</strong>
<ul>
<li>the honorific form is used to show respect to the listener or the subject of the conversation.</li>
<li><strong>Sonkeigo</strong> (Respectful Language): Elevates the subject’s actions (e.g., o-hanashi ni naru for “to speak”).</li>
<li><strong>Kenjougo</strong> (Humble Language): Lowers the speaker’s actions to show deference (e.g., moushiageru for “to say”).</li>
<li><strong>Teineigo</strong> (Polite Language): The desu/masu form falls under this when speaking politely without changing perspective.</li>
</ul></li>
</ul></li>
</ul>
<section id="questions" class="level3">
<h3 class="anchored" data-anchor-id="questions">Questions</h3>
<ul>
<li>what is changing perspective?
<ul>
<li>In Japanese, the speaker must change their perspective when using honorific language. <mark>This means that the speaker must consider the listener’s perspective and use language that is appropriate for that perspective.</mark></li>
</ul></li>
<li>Are politeness and honorifics only encoded in the verb inflection or do they further manifest in the subject object as agreements or other forms ?</li>
</ul>
</section>
</section>
<section id="samurai-world---sub-states-that-imbue-an-emergent-language-with-politeness-and-formality" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="samurai-world---sub-states-that-imbue-an-emergent-language-with-politeness-and-formality">Samurai world - Sub-States that imbue an emergent language with politeness and formality</h2>
<p>This is a small state space which is used as a model<sup>1</sup> for politeness and formality in natural language based on Japanese which has a more sophisticated system of politeness and formality then most languages.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;in the sense of logic</p></div></div><p>In this game agents interact in a framing game. In the lewis sub-game they need to coordinate a system in which they observe politeness and formality to avoid being decapitated by their superiors, yet avoid losing face, i.e.&nbsp;without being too polite or formal to their inferiors.</p>
<ul>
<li>state:
<ul>
<li>speaker’s status
<ul>
<li>gender, age, social status, relationship to subject</li>
</ul></li>
<li>subject’s status</li>
</ul></li>
</ul>
<p>One could evolve the use of poltiness in a reconstruciton game or a discrimination game.</p>
<ul>
<li>In the reconstruction task, Sender gets an input item, it sends a message to Receiver, and Receiver must generate an output item identical to Sender’s input.</li>
<li>In the discrimination task, Sender gets an input item (the target); Receiver gets multiple input items (the same target and a number of distractors, in random order).</li>
</ul>
<p>Note that reconstruction is very much like the original lewis signaling game. While the discrimination easier task as the receiver could in theory fail to reconstruct, or have to choose at random from a very large, perhaps even infinite set of possible reconstructions and that having a just K-distractions, it is down to 1/k probability of success. More so if it is equipped with the ability of learning from errors he might score the distractions and make progress with much higher information levels then in the reconstruction game.</p>
</section>
<section id="code" class="level2">
<h2 class="anchored" data-anchor-id="code">Code</h2>
<p>Here is a data generation script that may be used with <span class="citation" data-cites="kharitonov:etal:2021">(Kharitonov et al. 2021)</span> EGG emergence game toolkit to model politeness and formality in a multi-agent language game.</p>
</section>
<section id="states-for-verbs-and-nouns" class="level2 callout-info">
<h2 class="anchored" data-anchor-id="states-for-verbs-and-nouns">states for verbs and nouns</h2>
<p>One issue is that there that I have not yet written up the states needed for differentiating between nouns and verbs. Not the states for creating inflected verbs.</p>
<p>Both of these are bigger tasks and I will need to write them up quickly.</p>
<p>Anyhow is this case we should assume that there are already</p>
<ul>
<li>nouns and verbs and possible other parts of speech.</li>
<li>verbs may be inflected.</li>
</ul>
<p>now we wish to split certain verb states by adding sub-states that correspond to a politeness and formality flag.</p>
</section>
<section id="code-1" class="level2">
<h2 class="anchored" data-anchor-id="code-1">Code</h2>
<p>Here is a data generation script that may be used with <span class="citation" data-cites="kharitonov:etal:2021">(Kharitonov et al. 2021)</span> EGG emergence game toolkit to model politeness and formality in a multi-agent language game.</p>
<div id="36149a32" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> random</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-3"></span>
<span id="cb1-4"></span>
<span id="cb1-5">state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>()</span>
<span id="cb1-6"></span>
<span id="cb1-7"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> add_politenes_state(state):</span>
<span id="cb1-8"></span>
<span id="cb1-9">    state[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"speaker_status"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> random.choice([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, ])</span>
<span id="cb1-10">    state[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"subject_status"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> random.choice([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, ])</span>
<span id="cb1-11">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> state</span>
<span id="cb1-12"></span>
<span id="cb1-13"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>):</span>
<span id="cb1-14">    state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>()</span>
<span id="cb1-15">    state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> add_politenes_state(state)</span>
<span id="cb1-16">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">,</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>state<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> should be polite </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>state[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'subject_status'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> state[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'speaker_status'</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> "</span>)</span>
<span id="cb1-17"></span>
<span id="cb1-18"> </span>
<span id="cb1-19"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">## </span><span class="al" style="color: #AD0000;
background-color: null;
font-style: inherit;">TODO</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">: estimate politeness and face in a states</span></span>
<span id="cb1-20"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">## </span><span class="al" style="color: #AD0000;
background-color: null;
font-style: inherit;">TODO</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">: estimate politeness and face in an utterance</span></span>
<span id="cb1-21"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">## </span><span class="al" style="color: #AD0000;
background-color: null;
font-style: inherit;">TODO</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">: the politeness loss is the formality of the utterance and the politeness of the state. I.e. if the state is more polite then the utterance there is a loss associated with that.</span></span>
<span id="cb1-22"></span>
<span id="cb1-23"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> politeness_loss_penalty(state):</span>
<span id="cb1-24">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> state[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"subject_status"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> state[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"speaker_status"</span>]:</span>
<span id="cb1-25">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb1-26">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb1-27">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0,{'speaker_status': 0, 'subject_status': 1} should be polite True 
1,{'speaker_status': 0, 'subject_status': 1} should be polite True 
2,{'speaker_status': 2, 'subject_status': 2} should be polite False 
3,{'speaker_status': 2, 'subject_status': 0} should be polite False 
4,{'speaker_status': 2, 'subject_status': 2} should be polite False 
5,{'speaker_status': 1, 'subject_status': 2} should be polite True 
6,{'speaker_status': 1, 'subject_status': 2} should be polite True 
7,{'speaker_status': 1, 'subject_status': 1} should be polite False 
8,{'speaker_status': 1, 'subject_status': 0} should be polite False 
9,{'speaker_status': 1, 'subject_status': 1} should be polite False 
10,{'speaker_status': 0, 'subject_status': 1} should be polite True 
11,{'speaker_status': 0, 'subject_status': 1} should be polite True 
12,{'speaker_status': 1, 'subject_status': 0} should be polite False 
13,{'speaker_status': 2, 'subject_status': 2} should be polite False 
14,{'speaker_status': 0, 'subject_status': 1} should be polite True 
15,{'speaker_status': 1, 'subject_status': 0} should be polite False 
16,{'speaker_status': 1, 'subject_status': 0} should be polite False 
17,{'speaker_status': 1, 'subject_status': 0} should be polite False 
18,{'speaker_status': 1, 'subject_status': 0} should be polite False 
19,{'speaker_status': 1, 'subject_status': 2} should be polite True </code></pre>
</div>
</div>
<p>ok now that we have the states we can consider</p>
<ol type="1">
<li>ensuring that the speaker’s status is encoded in the message
<ol start="2" type="1">
<li>we can do this indirectly as part of the reconstruction.</li>
<li>we can also do this as a penalty coming from the framing game.
<ul>
<li>Rude speakers are penalized!</li>
<li>Losing face is also penalized.</li>
</ul></li>
</ol></li>
<li>we can refine the model by adding flags for <code>informal_settings</code>, <code>friendship_settings</code> and <code>gender_speaker</code> <code>gender_subject</code> <code>age_speaker</code> <code>age_subject</code> to further refine the politeness and formality of the language.</li>
<li>we may also want to consider the setting where the subject social status is unknown to the speaker and thus the speaker must use a default politeness setting.</li>
</ol>
<ul>
<li>https://blog.duolingo.com/japanese-politeness-formal-language</li>
</ul>
</section>
<section id="establishing-a-hierarchy." class="level2">
<h2 class="anchored" data-anchor-id="establishing-a-hierarchy.">Establishing a hierarchy.</h2>
<p>The establishment of hierarchies, particularly in a multi-agent system, is a complex process that involves a variety of factors. This may well be a bigger topic than we need to consider here and may easily confuse the issues I wish to discuss here which is the minimal example for the emergence of politeness and honorifics in a multi-agent language game. I therefore moved further discussion to the next post titled <a href="../../posts/2025-02-18-domination/index.html">Establishing Hierarchies</a>.</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-van2012logic" class="csl-entry">
Dalen, Dirk van. 2012. <em>Logic and Structure</em>. Vol. 3. Universitext. Springer London. <a href="https://doi.org/DOI 10.1007/978-1-4471-4558-5">https://doi.org/DOI 10.1007/978-1-4471-4558-5</a>.
</div>
<div id="ref-kharitonov:etal:2021" class="csl-entry">
Kharitonov, Eugene, Roberto Dessì, Rahma Chaabouni, Diane Bouchacourt, and Marco Baroni. 2021. <span>“<span>EGG</span>: A Toolkit for Research on <span>E</span>mergence of Lan<span>G</span>uage in <span>G</span>ames.”</span> <a href="https://github.com/facebookresearch/EGG" class="uri">https://github.com/facebookresearch/EGG</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {A {Samurai’s} {World}},
  date = {2025-02-17},
  url = {https://orenbochman.github.io/notes-nlp/posts/2025-02-17-samurai-world/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“A Samurai’s World.”</span> February 17,
2025. <a href="https://orenbochman.github.io/notes-nlp/posts/2025-02-17-samurai-world/">https://orenbochman.github.io/notes-nlp/posts/2025-02-17-samurai-world/</a>.
</div></div></section></div> ]]></description>
  <category>Emergent Language</category>
  <category>Game Theory</category>
  <category>Social Structure</category>
  <guid>https://orenbochman.github.io/notes-nlp/posts/2025-02-17-samurai-world/</guid>
  <pubDate>Sun, 16 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Knights and Knaves world</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/posts/2025-02-18-logic-worlds/</link>
  <description><![CDATA[ 





<blockquote class="blockquote">
<p>“if proof theory is about the sacred, then model theory is about the profane” – <span class="citation" data-cites="van2012logic">(Dalen 2012, 3:1)</span></p>
</blockquote>
<section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p>In the previous post, “Further Desiderata for Emergent Language”, I discussed the need to adapt emerging languages to conform with established properties of natural languages. You can read more about motivations in that post.</p>
<p>Rather then considering all the ways this could be achieved in a single essay, I believe that this should be broken down and considered one aspect at a time. This can make both the linguistics and the development easier to understand.</p>
<p>Once enough examples are available it should become clearer on how to integrate the desiderata using suitable states or other mechanisms.</p>
</section>
<section id="the-task" class="level2">
<h2 class="anchored" data-anchor-id="the-task">The task</h2>
<p>Today I want to consider the ability of language to encode logical reasoning. Logic is such a big field it can encampass all of mathematics and philosophy. So for this post we need to narrow things down.</p>
<p>I’d like the agents not only to learn to speak in a langauge that captures logical reasoning, but also to be able to reason about states and statements made in that language.</p>
<p>I am targeting reconstruction and discrimination games as the inner game which is used to evolve the language. The frameing game might be drawn from</p>
<ol type="1">
<li>Knights and Knaves puzzles</li>
<li>Lewis Carroll’s <a href="https://en.wikipedia.org/wiki/The_Game_of_Logic">The Game of Logic</a> - for</li>
<li>Jon Barwise and John Etchemendy created <a href="https://en.wikipedia.org/wiki/Tarski%27s_World">Tarski’s World</a> - for first order logic</li>
</ol>
</section>
<section id="knights-and-knaves-worlds" class="level2">
<h2 class="anchored" data-anchor-id="knights-and-knaves-worlds">Knights and Knaves worlds</h2>
<p>In “What Is the Name of This Book” and his other works Raymond Smullyan use this as a framework to cover this puzzle to cover from propositional logic to the problem of undecidability…</p>
<p>The initial state is simple - each individual is either a knight or a knave. Knights always tell the truth, and knaves always lie.</p>
<p>In most puzzles we need to determine the type of each individual from a set of statements made by them.</p>
<p>Another type of variation is that we want to find out some fact about the world but must ask the right question to get the correct answer regardless of the individual’s type.</p>
<p>Another variant that seems salient in this context is when the people in the puzzle respond in their own language, which is unkown to us. In this case we need to deduce the meaning of the words from the context.</p>
<p>To spice his puzzles up Smullyan introduced individuals that, who can lie or tell the truth as he pleases these might be called “Normals”</p>
<p>In his Transilvanian puzzles he introduced the notion that half the population are insane and have false beliefs e.g.&nbsp;that <img src="https://latex.codecogs.com/png.latex?2+2%5Cne4"> and they are also devided into truthfull and lying types this time humans and vampires. In another book he introduced monkeys that look like humans. The only real difference was that monkeys have a tail and humans don’t. In terms of logic it just add another collumn to the truth table for each individual.</p>
<p>We have for n individuals we have 2^n possible states.</p>
<p>next comes the creative part of the task we want to automate. The statements the individuals make. While each individual can make a statement tht reveals their ground truth we the idea is to</p>
<ol type="1">
<li>ensure all the ground truths are revealed</li>
<li>use a minimal number of statements (i.e.&nbsp;by omitting a statement the problem should be rendered unsolvable)</li>
<li>ensure that the ground truths are unique</li>
</ol>
<p>In the website https://christopherphelps.trinket.io/sites/knight_knave_puzzler the generator can be used to generate a number of statements:</p>
<ol type="1">
<li>meta statements - is the puzzle solvable</li>
<li>name calling - calling some one a knight or knave a normal a monkey, insane etc.</li>
<li>Ascriptive statements - where an someone says what some type of individual would say about another speaker</li>
<li>Prime statements- statements on the prime number of knights or knaves in the group</li>
<li>independent statements - statements that don’t seem to be related to the puzzle</li>
</ol>
<p>One property of the puzzle is if no one makes a statement about an individual then his type is unconstrained and could be swapped without affecting the consistency of the puzzle.</p>
</section>
<section id="how-is-logic-encoded-in-the-knights-and-knaves-puzzles" class="level2">
<h2 class="anchored" data-anchor-id="how-is-logic-encoded-in-the-knights-and-knaves-puzzles">How is logic encoded in the knights and knaves Puzzles?</h2>
<p>So it is interesting to consider how one give a minimal general solution for such puzzles. In reality most puzzles do have short solutions but in general when we consider logic and language we can’t be certain that there is a neat solution or that the puzlle has a unique answer or that the puzzle is solvable.</p>
<p>So here is a general approach to solve these puzzles using boolean logic:</p>
<ol type="1">
<li>We encode all possible combinations of sub-states of the world using as inputs for a truth table. I.e. a column for each individual titled with their name and stating that they are a knight. We don’t need to encode the knave column as the two are mutually exclusive. If there are other sub-states like being a monkey, insane, a spy we would need to add a column for these too.</li>
<li>For each statement said we should rewrite it as a boolean expression in terms of these states.</li>
<li>We need to verify the outcome of the statement for each combination of</li>
</ol>
<p>Then we encode the statements made by the individuals as a column in the truth table.</p>
<p>let’s look at some examples with just knights and knaves</p>
<section id="name-calling" class="level3">
<h3 class="anchored" data-anchor-id="name-calling">Name calling</h3>
<ol type="1">
<li><label><input type="checkbox">A says “I am knave.”</label>
<ul>
<li>not possible for a knight (False)</li>
<li>not possible for a knave (True)</li>
<li>This will therefore not appear on it own.</li>
<li>This will not be part of a conjunction made by a knight. i.e.&nbsp;“and …”</li>
<li>It can be used conjunction with a truthy statement made by a knave. … and I am a knave</li>
</ul></li>
<li><label><input type="checkbox">A says “I am knight.”</label>
<ul>
<li>if A is a knight (True)</li>
<li>if A is a knave (False)</li>
</ul></li>
<li>A says “B is a knave.”
<ul>
<li>if A is a knight and B is knave (True)</li>
<li>if A is a knave and B is a knight (False) note: that both types will call the other type a knave. so this only tells us the speaker is the same type as another individual.</li>
</ul></li>
<li><label><input type="checkbox">A says “B is a knight.”</label>
<ul>
<li>if A is a knight and B is a knight (True)</li>
<li>if A is a knave and B is a knave (False)</li>
</ul></li>
<li><label><input type="checkbox">A says “I am the same type as B” same situation as #3</label></li>
<li><label><input type="checkbox">A says “B would say that I am a knave”</label></li>
<li><label><input type="checkbox">A says “B would say that I am a knight”</label></li>
<li>There are a prime number of knaves in the group.</li>
<li>There are a prime number of knights in the group.</li>
<li>“The puzzle is solvable” means there isn’t a contradiction in the statements made by the individuals.</li>
<li>“The puzzle is unsolvable” means there is a contradiction in the statements made by the individuals.</li>
</ol>
<table class="caption-top table">
<thead>
<tr class="header">
<th>A</th>
<th>B</th>
<th>A:#1</th>
<th>B:#1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="odd">
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="even">
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>



</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-van2012logic" class="csl-entry">
Dalen, Dirk van. 2012. <em>Logic and Structure</em>. Vol. 3. Universitext. Springer London. <a href="https://doi.org/DOI 10.1007/978-1-4471-4558-5">https://doi.org/DOI 10.1007/978-1-4471-4558-5</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Knights and {Knaves} World},
  date = {2025-02-17},
  url = {https://orenbochman.github.io/notes-nlp/posts/2025-02-18-logic-worlds/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Knights and Knaves World.”</span> February
17, 2025. <a href="https://orenbochman.github.io/notes-nlp/posts/2025-02-18-logic-worlds/">https://orenbochman.github.io/notes-nlp/posts/2025-02-18-logic-worlds/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/notes-nlp/posts/2025-02-18-logic-worlds/</guid>
  <pubDate>Sun, 16 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Further Desiderata for Emergent Language</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/posts/2025-02-16-more-desidirata/</link>
  <description><![CDATA[ 





<section id="three-applications-for-emergent-language" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="three-applications-for-emergent-language">Three applications for emergent language</h2>
<p>Some additional desiderata for emergent language come from:</p>
<ol type="1">
<li><p>Using the emergent language as <strong>surrogate for low resource languages</strong>. Though in a Bayesian setting we may consider them as priors. We may want to have multiple surrogates for a low resource language so that we can fit to the set of surrogates rather then to the lexemes of one specific surrogate! Fitting to a set of languages should teach the neural model about relations. A single surrogate would be less useful as it models would tend to overfit to features like lexemes which are made-up.</p></li>
<li><p>Using games where agents learn multiple languages. This allows to study the field of language contact. In this case speakers speak more than one language and the languages influence each other. To make things more concreate we may want to evolve languages that are similar to certain specification that come from the WALS database. <sup>1</sup></p></li>
<li><p>A third idea that may be of interest is to evolve languages that can be used as a <strong>universal interlingua</strong>. This are emergent language that are minimally ambiguous have capture most deep features. It would be created using a large state space and be given a long time for planning or to evolve and become more uniform, the verbs more regular and transformations more compositional. Ideally the verbs and nouns would only require one form to master all possible forms and their meanings. Perhaps the nouns could even be derived from the verbs using an automatic process, to reduce the learning load even further. Why multiple interlingua? Since we propose to use these for translations it may be possible to create smaller-interlinguas that are used for a restricted language hierarchy. A second reason is that again we may not want to overfit to a single interlingua but to some set of interlinguas captring diverse deep features and surface features. Idealy some interlinguas would be a better fit for certain languages then others and an attention mechanism could be used to relay on the interlinguas that are the best fit for the language being translated. Think of using different shortcuts to get from one place to another. (e.g.&nbsp;An interlingua with politeness and honorifics would be a better fit for Japanese then one without.)</p></li>
</ol>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;this is at last another good reason for multiple senders to be used and for them to send different languages.</p></div></div></section>
<section id="section" class="level2">
<h2 class="anchored" data-anchor-id="section">??</h2>
<p>So the main thrust is that we want to evolve language that are similar to a WALS specification. THis may require too much work though and we may prefer to consider certain aspects of language in the database that are both common, easy to implement and measure and ideally can become surrogates that are more like a certain language then any high resource language (e.g.&nbsp;Turkish).</p>
<p>While a number of <em>tricks</em> may be used to make the emergent language more like a low resource language, the more systematic route is to consider a suitable set of states for the lewis game. Spontaneous symmetry breaking may then serve us. In other cases we may be able to incorporate different aggregation rule</p>
<p>However to get started I think one needs to simply generate suitable states. The desedirate has a duality with the states. The number and structure of the state will shape the size and complexity of the emegent languages. Since these will likely be large languages we may also need to find algorithms that allow these to emerge quickly.</p>
<p>Another idea is to make to make three views of the state space. - an image based on a chart - a frame based view</p>
<ul>
<li>We may want to express propositions using a ‘Block world’</li>
<li>We may want to express possession using a ‘Possession world’</li>
<li>We may want to first breakdown the verb to indicate tense, aspect, mood, number, politeness, formality &amp; counterfactual.</li>
<li>For thematic roles we may start the figure from <span class="citation" data-cites="winston1992artificial">(Winston 1992, 211)</span> and the frame it shows.</li>
<li>Winston points out that the number of roles which can range from 6 to 24 is less important, so long as we can learn the constraints verbs place on the roles when forming a sentence. This seems to be even more important if we are going to learn these constraints from data using an attention mechanism.</li>
<li>Filamore explains that semantic roles are a tool to eliminate the polisemy inherent in verbs.</li>
</ul>
<ol type="1">
<li>Distributional Similarity - to a NL
<ul>
<li>lexeme should be distributed similarly to a corresponding word in a natural langauge.</li>
<li>this is desirable because when translating from high frequency words should map to high frequency words.</li>
<li>this seems a challenge but lets recall that we also wanted to have a power law distribution.</li>
<li>if lexemes are distributed following a power law. so fee words are high frequency and almost all are low frequency.</li>
<li>metrics for this could be cosine similarity, KL divergence, or some other measure of distributional similarity.</li>
<li>probability theorem has convergence in distribution and it may be of interest here, particularly as we may be interested in more then the lexemes distribution but of other probabilistically modeled aspects of the language.</li>
</ul></li>
<li>We may want to choose a fully emergent language with phonology, morphology, syntax, and semantics. This might be more complexity than we need though. We might want to work with a system that is simpler but can be used to make embeddings that are useful for approximating low resource languages. In such a case we may use concatentaed numeric codes for representing the lexems.</li>
<li>verb tense, aspect, mood, conterfactuals
<ul>
<li>tense is a time reference
<ul>
<li>past, present, future.</li>
</ul></li>
<li>aspect is the way the action is viewed e.g.&nbsp;
<ul>
<li>perfective means: the action is viewed as a whole (e.g.&nbsp;“I have eaten”)</li>
<li>imperfective means: the action is viewed as ongoing (e.g.&nbsp;“I am eating”)</li>
<li>progressive means: the action is viewed as ongoing (e.g.&nbsp;“I am eating right now, but I will call you when I am done”)</li>
<li>habitual means: the action is viewed as a habit (e.g.&nbsp;“I eat breakfast every day”)</li>
<li>perfect means: the action is viewed as completed (e.g.&nbsp;“I have eaten”)<br>
</li>
</ul></li>
<li>mood is the attitude of the speaker some examples are:
<ul>
<li>indicative means: the speaker is making a statement of fact (e.g.&nbsp;“I am happy”)</li>
<li>subjunctive means: the speaker is expressing a wish, a doubt, or a hypothetical (e.g.&nbsp;“I wish I were happy”)</li>
<li>imperative means: the speaker is giving a command (e.g.&nbsp;“Be happy!”)</li>
<li>conditional means: the speaker is expressing a condition (e.g.&nbsp;“If I were happy, I would be smiling”)</li>
<li>interrogative means: the speaker is asking a question (e.g.&nbsp;“Are you happy?”)</li>
<li>exclamatory means: the speaker is making an exclamation (e.g.&nbsp;“How happy I am!”)</li>
</ul></li>
<li>counterfactuals are statements that are contrary to fact. (e.g.&nbsp;“In the best of possible worlds, I would be smiling”)</li>
</ul></li>
<li>thematic roles
<ul>
<li>agent</li>
<li>patient</li>
<li>experiencer</li>
<li>theme</li>
<li>goal</li>
<li>source</li>
<li>instrument</li>
<li>location</li>
<li>benefactor</li>
</ul></li>
<li>possession world
<ul>
<li>possessor</li>
<li>possessed States:</li>
</ul></li>
<li>We may want to have verbs and nouns and other parts of speech.
<ul>
<li>states could be frames for verbs with slots for subjects, objects. this can</li>
</ul></li>
</ol>
<p>Propositions could be materialized using a block world.</p>
<p>Possession could be materialized using a possession world.</p>
<ul>
<li>block world <span class="citation" data-cites="winston1992artificial">(Winston 1992, 47–60)</span> , c.f. Mover and SHRDLU by Terry Winograd</li>
</ul>
</section>
<section id="block-world" class="level2">
<h2 class="anchored" data-anchor-id="block-world">Block world</h2>
</section>
<section id="zookeeper" class="level2">
<h2 class="anchored" data-anchor-id="zookeeper">Zookeeper</h2>
<ul>
<li>in Zookeeper <span class="citation" data-cites="winston1992artificial">(Winston 1992, 121–25)</span> we need to classify animals using deduction.</li>
<li>https://www.kaggle.com/uciml/zoo-animal-classification/data</li>
<li>we can give each of 101 animal 18 properties in a table via a zoo.csv. We can then sample from the table to create sets of animals with different properties.</li>
<li>we might then play games based on this data:
<ul>
<li>we may need to identify the animal based on it properties.</li>
<li>each property may come from a different sender.</li>
<li>an early response may be to attack, evade or ignore based on a partial identification.</li>
<li>this requires some kind of reasoning about the properties of the animals in the ‘zoo’</li>
</ul></li>
</ul>
<p>This means states for animals and their properties.</p>
<p>Minsky’s K-lines - physical world 214 - mental world 215 - ownership world 216 - <span class="citation" data-cites="kanade1980theory">Kanade (1980)</span> <a href="https://www.ri.cmu.edu/pub_files/pub3/kanade_takeo_1980_1/kanade_takeo_1980_1.pdf">A Theory of Origami World</a></p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-kanade1980theory" class="csl-entry">
Kanade, Takeo. 1980. <span>“A Theory of Origami World.”</span> <em>Artificial Intelligence</em> 13 (3): 279–311.
</div>
<div id="ref-winston1992artificial" class="csl-entry">
Winston, Patrick Henry. 1992. <em>Artificial Intelligence</em>. 3rd ed. Reading, MA: Addison-Wesley.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Further {Desiderata} for {Emergent} {Language}},
  date = {2025-02-17},
  url = {https://orenbochman.github.io/notes-nlp/posts/2025-02-16-more-desidirata/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Further Desiderata for Emergent
Language.”</span> February 17, 2025. <a href="https://orenbochman.github.io/notes-nlp/posts/2025-02-16-more-desidirata/">https://orenbochman.github.io/notes-nlp/posts/2025-02-16-more-desidirata/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/notes-nlp/posts/2025-02-16-more-desidirata/</guid>
  <pubDate>Sun, 16 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>A Convolutional Attention Network for Extreme Summarization of Source Code</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/2016-code-summerization/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>Literature review</figcaption>
</figure>
</div></div><p><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2016-code-summerization/img/fig01.png" id="fig-01" class="img-fluid"> <img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2016-code-summerization/img/fig02.png" id="fig-02" class="img-fluid"> <img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2016-code-summerization/img/fig03.png" id="fig-03" class="img-fluid"></p>
<p>This is a paper mentioned in the course on multilingual NLP by Graham Neubig. With an interesting idea of an second attention head being used to copy stuff from the input directly to the output.</p>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>Attention mechanisms in neural networks have proved useful for problems in which the input and output do not have fixed dimension. Often there exist features that are locally translation invariant and would be valuable for directing the model’s attention, but previous attentional architectures are not constructed to learn such features specifically. We introduce an attentional neural network that employs convolution on the input tokens to detect local time-invariant and long-range topical attention features in a context-dependent way. We apply this architecture to the problem of extreme summarization of source code snippets into short, descriptive function name-like summaries. Using those features, the model sequentially generates a summary by marginalizing over two attention mechanisms: one that predicts the next summary token based on the attention weights of the input tokens and another that is able to copy a code token as-is directly into the summary. We demonstrate our convolutional attention neural network’s performance on 10 popular Java projects showing that it achieves better performance compared to previous attentional mechanisms. – <span class="citation" data-cites="allamanis2016convolutionalattentionnetworkextreme">(Allamanis, Peng, and Sutton 2016)</span></p>
</blockquote>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-allamanis2016convolutionalattentionnetworkextreme" class="csl-entry">
Allamanis, Miltiadis, Hao Peng, and Charles Sutton. 2016. <span>“A Convolutional Attention Network for Extreme Summarization of Source Code.”</span> <a href="https://arxiv.org/abs/1602.03001">https://arxiv.org/abs/1602.03001</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {A {Convolutional} {Attention} {Network} for {Extreme}
    {Summarization} of {Source} {Code}},
  date = {2025-02-13},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/2016-code-summerization/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“A Convolutional Attention Network for
Extreme Summarization of Source Code.”</span> February 13, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/2016-code-summerization/">https://orenbochman.github.io/notes-nlp/reviews/paper/2016-code-summerization/</a>.
</div></div></section></div> ]]></description>
  <category>NLP</category>
  <category>Paper</category>
  <category>Attention</category>
  <category>Deep learning</category>
  <category>Review</category>
  <category>Stub</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/2016-code-summerization/</guid>
  <pubDate>Wed, 12 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Neural Morphological Analysis: Encoding-Decoding Canonical Segments</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/2016-neural-morphological-segmentation/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>Literature review</figcaption>
</figure>
</div></div><div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR - Neural Morphological Analysis
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/in_a_nutshell.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Neural Morphological Analysis in a Nutshell"><img src="https://orenbochman.github.io/notes-nlp/images/in_a_nutshell.jpg" class="img-fluid figure-img" alt="Neural Morphological Analysis in a Nutshell"></a></p>
<figcaption>Neural Morphological Analysis in a Nutshell</figcaption>
</figure>
</div>
</div>
</div>
<section id="podcast" class="level2">
<h2 class="anchored" data-anchor-id="podcast">Podcast</h2>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>Canonical morphological segmentation aims to divide words into a sequence of standardized segments. In this work, we propose a character-based neural encoder-decoder model for this task. Additionally, we extend our model to include morpheme-level and lexical information through a neural reranker. We set the new state of the art for the task improving previous results by up to 21% accuracy. Our experiments cover three languages: English, German and Indonesian. – <span class="citation" data-cites="kann-etal-2016-neural">(Kann, Cotterell, and Schütze 2016)</span></p>
</blockquote>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li><strong>Introduction</strong>
<ul>
<li>Discusses morphological segmentation and its applications in NLP.</li>
<li>Explains the difference between surface segmentation and canonical segmentation, providing an example.</li>
<li>Highlights the advantages of canonical segmentation and the algorithmic challenges it introduces.</li>
<li>Presents a neural encoder-decoder model for canonical segmentation and a neural reranker to incorporate linguistic structure.</li>
</ul></li>
<li><strong>Neural Canonical Segmentation</strong>
<ul>
<li>Formally describes the canonical segmentation task, mapping a word to a canonical segmentation.</li>
<li>Explains the probabilistic approach to learn a distribution p(c | w).</li>
<li>Details the two parts of the model: an encoder-decoder RNN and a neural reranker.</li>
<li>Describes the neural encoder-decoder model based on Bahdanau et al.&nbsp;(2014), using a bidirectional gated RNN (GRU) as the encoder.</li>
<li>Explains how the decoder defines a conditional probability distribution over possible segmentations.</li>
<li>Explains the attention mechanism and how attention weights are computed.</li>
<li>Explains the neural reranker’s role in rescoring candidate segmentations from a sample set generated by the encoder-decoder.</li>
<li>Describes the reranking model’s ability to embed morphemes and incorporate character-level information.</li>
</ul></li>
<li><strong>Related Work</strong>
<ul>
<li>Discusses various approaches to morphological segmentation.</li>
<li>Mentions unsupervised methods like LINGUISTICA and MORFESSOR.</li>
<li>Describes supervised approaches using conditional random fields (CRFs).</li>
<li>Distinguishes the approach from surface morphological segmentation methods using a window LSTM.</li>
<li>Relates the approach to other applications of recurrent neural network transduction models.</li>
</ul></li>
<li><strong>Experiments</strong>
<ul>
<li>Describes the dataset used for comparison to earlier work.</li>
<li>Specifies the three languages used in the experiments: English, German, and Indonesian.</li>
<li>Notes the potential cause of the high error rate for German due to its orthographic changes.</li>
<li>Explains the data extraction process from CELEX, DerivBase, and MORPHIND analyzer for English, German, and Indonesian, respectively.</li>
<li>Details the training setup, including the use of an ensemble of five encoder-decoder models.</li>
<li>Describes the training of the reranking model, including sample set gathering and optimization.</li>
<li>Describes the baseline models used for comparison: JOINT model and a weighted finite-state transducer (WFST).</li>
<li>Outlines the evaluation metrics used: error rate, edit distance, and morpheme F1.</li>
</ul></li>
<li><strong>Results</strong>
<ul>
<li>Presents the results of the canonical segmentation experiment, showing improvements over baselines with both the encoder-decoder and reranker.</li>
<li>Discusses the additional improvements achieved by the reranker due to access to morpheme embeddings and existing words.</li>
<li>Analyzes cases where the right answer is not in the samples and errors due to annotation problems.</li>
<li>Discusses cases where the encoder-decoder finds the right solution but assigns a higher probability to an incorrect analysis.</li>
<li>Explains how the reranker corrects some errors based on lexical information and morpheme embeddings.</li>
<li>Investigates whether segments unseen in the training set are a source of errors.</li>
</ul></li>
<li><strong>Conclusion and Future Work</strong>
<ul>
<li>Summarizes the developed model consisting of an encoder-decoder and neural reranker for canonical morphological segmentation.</li>
<li>States the model’s improvement over baseline models.</li>
<li>Discusses the potential for further performance increase by improving the reranker.</li>
</ul></li>
</ul>
<!-- TODO: add outline, reflection, and terminology and for this paper-->
</section>
<section id="reflections" class="level2">
<h2 class="anchored" data-anchor-id="reflections">Reflections</h2>
<ol type="1">
<li>why has this not caught on.</li>
<li>why are people using byte pair encoding and morphological segmentation.</li>
<li>what resources are needed to train this kind of model on a new language?
<ul>
<li>A dataset of words with canonical segmentation</li>
<li>Access to a lexicon or a large corpus to determine if a canonical segment occurs as an independent word in the language. What is it is a bound morpheme that never appears alone or part of a root-template morphological system? How should we verify that we are deleing with a morpheme and not a surface phonemic fragment.</li>
</ul></li>
<li>Can we do this without a canonical segmentation dataset. More specifically can we induct morphology by processing surface forms of words and induct the canonical morphological forms using one of three loss function that
<ul>
<li>affix loss (prefix,stem, suffix)</li>
<li>template loss (root,template) loss</li>
<li>agglunative loss (stem suffix sequence)</li>
</ul></li>
</ol>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-kann-etal-2016-neural" class="csl-entry">
Kann, Katharina, Ryan Cotterell, and Hinrich Schütze. 2016. <span>“Neural Morphological Analysis: Encoding-Decoding Canonical Segments.”</span> In <em>Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</em>, edited by Jian Su, Kevin Duh, and Xavier Carreras, 961–67. Austin, Texas: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/D16-1097">https://doi.org/10.18653/v1/D16-1097</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Neural {Morphological} {Analysis:} {Encoding-Decoding}
    {Canonical} {Segments}},
  date = {2025-02-12},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/2016-neural-morphological-segmentation/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Neural Morphological Analysis:
Encoding-Decoding Canonical Segments.”</span> February 12, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/2016-neural-morphological-segmentation/">https://orenbochman.github.io/notes-nlp/reviews/paper/2016-neural-morphological-segmentation/</a>.
</div></div></section></div> ]]></description>
  <category>Paper</category>
  <category>Review</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/2016-neural-morphological-segmentation/</guid>
  <pubDate>Tue, 11 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Coverage Embedding Models for Neural Machine Translation</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/2016-coverage-embedding-models-for-NMT/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>Literature review</figcaption>
</figure>
</div></div><section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<ol class="example" type="1">
<li>is discussed in week 6 the <a href="../../../notes/cs11-737-w06-translation-models/index.qmd">Multilingual NLP course</a>. Neural generative models tend to drop or repeat content. But for NMT we can assume that all the inputs should be represented in the output. For each uncovered word it imposes a penalty on the attention model.</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR - Coverage Embedding
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/in_a_nutshell.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Coverage Embedding in a Nutshell"><img src="https://orenbochman.github.io/notes-nlp/images/in_a_nutshell.jpg" class="img-fluid figure-img" alt="Coverage Embedding in a Nutshell"></a></p>
<figcaption>Coverage Embedding in a Nutshell</figcaption>
</figure>
</div>
<ul>
<li>The paper introduces coverage embedding models to address the issues of repeating and dropping translations in NMT.</li>
<li>The coverage embedding vectors are updated at each time step to track the coverage status of source words.</li>
<li>The coverage embedding models significantly improve translation quality over a large vocabulary NMT system.</li>
<li>The best model uses a combination of updating with a GRU and updating as a subtraction.</li>
<li>The coverage embedding models also reduce the number of repeated phrases in the output.</li>
</ul>
</div>
</div>
</section>
<section id="podcast" class="level2">
<h2 class="anchored" data-anchor-id="podcast">Podcast</h2>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>In this paper, we enhance the attention-based neural machine translation (NMT) by adding explicit coverage embedding models to alleviate issues of repeating and dropping translations in NMT. For each source word, our model starts with a full coverage embedding vector to track the coverage status, and then keeps updating it with neural networks as the translation goes. Experiments on the large-scale Chinese-to-English task show that our enhanced model improves the translation quality significantly on various test sets over the strong large vocabulary NMT system. – <span class="citation" data-cites="mi-etal-2016-coverage">(Mi et al. 2016)</span></p>
</blockquote>
</section>
<section id="glossary" class="level2">
<h2 class="anchored" data-anchor-id="glossary">Glossary</h2>
<p>This paper has a number of big words and concepts that are important to understand. Lets break them down together:</p>
<dl>
<dt>Neural Machine Translation (NMT)</dt>
<dd>
A machine translation approach that uses neural networks to learn the mapping between source and target languages.
</dd>
<dt>Attention Mechanism</dt>
<dd>
In NMT, a mechanism that allows the model to focus on different parts of the source sentence when generating each word in the target sentence.
</dd>
<dt>Coverage Vector</dt>
<dd>
A vector used in statistical machine translation to explicitly track which source words have been translated.
</dd>
<dt>Coverage Embedding Vector</dt>
<dd>
A vector specific to each source word in this model, used to track the translation status of the word. It is initialized with a full embedding and is updated based on attention scores.
</dd>
<dt>Gated Recurrent Unit (GRU)</dt>
<dd>
A type of RNN cell used to model sequential data, including language. Here it is used to update coverage embeddings.
</dd>
<dt>Attention Probability (α)</dt>
<dd>
A set of weights that indicate how much attention the model pays to each source word when predicting a target word.
</dd>
<dt>Encoder-Decoder Network</dt>
<dd>
A neural network architecture commonly used in sequence-to-sequence tasks like NMT. The encoder processes the input sequence, and the decoder generates the output sequence.
</dd>
<dt>Bi-directional RNN</dt>
<dd>
A RNN that processes a sequence in both forward and backward directions, capturing contextual information from both sides of a word.
</dd>
<dt>Soft Probability</dt>
<dd>
Probabilities in the attention mechanism aren’t hard (0 or 1), but instead are on a continuum, indicating a degree of attention or importance.
</dd>
<dt>Fertility</dt>
<dd>
In the context of translation, fertility refers to the number of words in the target language that can be translated from a single word in the source language.
</dd>
<dt>One-to-many Translation</dt>
<dd>
A translation where one source word corresponds to multiple words in the target language.
</dd>
<dt>TER (Translation Error Rate)</dt>
<dd>
A metric used to evaluate the quality of machine translation by calculating the number of edits required to match the system’s translation to a reference translation, with lower scores being better.
</dd>
<dt>BLEU (Bilingual Evaluation Understudy)</dt>
<dd>
A metric to evaluate the quality of machine translation by comparing a candidate translation to one or more reference translations, with higher scores being better.
</dd>
<dt>UNK</dt>
<dd>
Abbreviation for “unknown.” In machine translation, it is used to denote words that are not in the model’s vocabulary.
</dd>
<dt>AdaDelta</dt>
<dd>
An adaptive learning rate optimization algorithm, that adjusts the learning rate during training for faster convergence.
</dd>
<dt>Alignment</dt>
<dd>
In the context of machine translation, the process of determining which words in the source sentence correspond to words in the target sentence.
</dd>
<dt>F1 Score</dt>
<dd>
A measure of a test’s accuracy and it considers both the precision and recall of the test to compute the score.
</dd>
</dl>
<p>With a solid understanding of this terminology we can now dive into the paper.</p>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ol type="1">
<li>Introduction
<ul>
<li>Notes that in NMT attention mechanisms focus on source words to predict target words.</li>
<li>Point out that <mark>these models lack history or coverage information, leading to repetition or dropping of words.</mark></li>
<li>Recalls how Statistical Machine Translation (SMT) used a binary “coverage vector” to track translated words.</li>
<li>Explains that SMT coverage vectors use 0s and 1s, whereas attention probabilities are soft. SMT systems handle one-to-many fertilities using phrases or hiero rules, while NMT systems predict one word at a time.</li>
<li>Introduces <strong>coverage embedding vectors</strong>, updated at each step, to address these issues.</li>
<li>Explains that <mark>each source word has its own coverage embedding vector that starts as a full embedding vector</mark>(as opposed to 0 in SMT).</li>
<li>States that coverage embedding vectors are updated based on attention weights, moving toward an empty vector for words that have been translated.</li>
</ul></li>
<li>Neural Machine Translation
<ul>
<li>Recalls that attention-based NMT uses an encoder-decoder architecture.
<ul>
<li>The encoder uses a bi-directional RNN to encode the source sentence into hidden states.</li>
<li><mark>The decoder predicts the target translation by maximizing the conditional log-probability of the correct translation.</mark></li>
<li>The probability of each target word is determined by the previous word and the hidden state.</li>
<li>The hidden state is computed using a weighted sum of the encoder states, with weights derived from a two-layer neural network.</li>
</ul></li>
<li>Introduces coverage embedding models into the NMT by adding an input to the attention model.</li>
</ul></li>
<li>Coverage Embedding Models
<ul>
<li>The model uses a coverage embedding for each source word that is updated at each time step.</li>
<li><mark>Each source word has its own coverage embedding vector,</mark> and the number of coverage embedding vectors is the same as the source vocabulary size.</li>
<li>The coverage embedding matrix is initialized with coverage embedding vectors for all source words.</li>
<li>Coverage embeddings are updated using neural networks (GRU or subtraction).</li>
<li>As the translation progresses, coverage embeddings of translated words should approach zero.</li>
<li>Two methods are proposed to update the coverage embedding vectors: GRU and subtraction.</li>
</ul>
<ol type="1">
<li>Updating Methods
<ol type="1">
<li>Updating with a GRU
<ul>
<li>The coverage model is updated using a GRU, incorporating the current target word and attention weights.</li>
<li>The GRU uses update and reset gates to control the update of the coverage embedding vector.</li>
</ul></li>
<li>Updating as Subtraction
<ul>
<li>The coverage embedding is updated by subtracting the embedding of the target word, weighted by the attention probability.</li>
</ul></li>
</ol></li>
<li>Objectives
<ul>
<li>Coverage embedding models are integrated into attention NMT by adding coverage embedding vectors to the first layer of the attention model.</li>
<li>The goal is to remove partial information from the coverage embedding vectors based on the attention probability.</li>
<li>The model minimizes the absolute values of the embedding matrix.</li>
<li>The model can also use supervised alignments to know when the coverage embedding should be close to zero.</li>
</ul></li>
</ol></li>
<li>Related Work
<ul>
<li>Tu et al.&nbsp;(2016) also uses a GRU to model the coverage vector. However, this approach differs from the current paper’s method by initializing the word coverage vector with a scalar and adding an accumulation operation and a fertility function.</li>
<li>Cohn et al.&nbsp;(2016) augments the attention model with features from traditional SMT.</li>
</ul></li>
<li>Experiments
<ol type="1">
<li>Data Preperation
<ul>
<li>Experiments were conducted on a Chinese-to-English translation task.</li>
<li>Two training sets were used: one with 5 million sentence pairs and another with 11 million.</li>
<li>The development set consisted of 4491 sentences.</li>
<li>Test sets included NIST MT06, MT08 news, and MT08 web.</li>
<li>Full vocabulary sizes were 300k and 500k, with coverage embedding vector sizes of 100.</li>
<li>AdaDelta was used to update model parameters with a mini-batch size of 80.</li>
<li>The output vocabulary was a subset of the full vocabulary, including the top 2k most frequent words and the top 10 candidates from word-to-word/phrase translation tables.</li>
<li>The maximum length of a source phrase was 4.</li>
<li>A traditional SMT system, a hybrid syntax-based tree-to-string model, was used as a baseline.</li>
<li>Four different settings for coverage embedding models were tested: updating with a GRU (UGRU), updating as a subtraction (USub), the combination of both methods (UGRU+USub), and UGRU+USub with an additional objective (+Obj).</li>
</ul></li>
<li>Translation Results
<ul>
<li>The coverage embedding models improved translation quality significantly over a large vocabulary NMT (LVNMT) baseline system.</li>
<li>UGRU+USub performed best, achieving a 2.6 point improvement over LVNMT.</li>
<li>Improvements of coverage models over LVNMT were statistically significant.</li>
<li>The UGRU model also improved performance when using a larger training set of 11 million sentences.</li>
</ul></li>
<li>Alignment Results
<ul>
<li>The best coverage model (UGRU + USub) improved the F1 score by 2.2 points over the baseline NMT system.</li>
<li>Coverage embedding models reduce the number of repeated phrases in the output.</li>
</ul></li>
</ol></li>
<li>Conclusion
<ul>
<li>The paper proposed coverage embedding models for attention-based NMT.</li>
<li>The models use a coverage embedding vector for each source word and update these vectors as the translation progresses.</li>
<li>Experiments showed significant improvements over a strong large vocabulary NMT system.</li>
</ul></li>
</ol>
</section>
<section id="reflection" class="level2">
<h2 class="anchored" data-anchor-id="reflection">Reflection</h2>
<p>The idea of tracking coverage is very simple. Lets face it many issues in NLP require simple solutions.</p>
<p>For instance in the summarization task we have a big headache with the autoregressive tendency to repeat. But it also requires a kind of coverage too, but one that is more spread out. Also in more extreme cases we want to direct the coverage using very specific information like the narative flow of a story. This seems to be an idea that can be further explored in other tasks.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-mi-etal-2016-coverage" class="csl-entry">
Mi, Haitao, Baskaran Sankaran, Zhiguo Wang, and Abe Ittycheriah. 2016. <span>“Coverage Embedding Models for Neural Machine Translation.”</span> In <em>Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</em>, edited by Jian Su, Kevin Duh, and Xavier Carreras, 955–60. Austin, Texas: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/D16-1096">https://doi.org/10.18653/v1/D16-1096</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Coverage {Embedding} {Models} for {Neural} {Machine}
    {Translation}},
  date = {2025-02-12},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/2016-coverage-embedding-models-for-NMT/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Coverage Embedding Models for Neural Machine
Translation.”</span> February 12, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/2016-coverage-embedding-models-for-NMT/">https://orenbochman.github.io/notes-nlp/reviews/paper/2016-coverage-embedding-models-for-NMT/</a>.
</div></div></section></div> ]]></description>
  <category>Paper</category>
  <category>Review</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/2016-coverage-embedding-models-for-NMT/</guid>
  <pubDate>Tue, 11 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Summarization Task</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/posts/2025-02-10-summerizer/</link>
  <description><![CDATA[ 





<section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p>In the Deeplearning.ai NLP specialization, we implemented both <strong>Q&amp;A</strong> and <strong>Summarization tasks</strong>. However, we only looked at these tasks in terms of the attention models. It is now time to dive deeper into the summarization task. In this article I will review a talk covering a body of research on summarization, which has many ideas about features. Consider some of these and see what aspects are relevant to a modern task implementation.</p>
<p>Some of the tasks here can be done with a simple LSTM or Transformer. However it seems that RL may be useful if we want to plan the summary in a non-linear fashion or to make use of a dynamic programming approach. For instance to identify the narrative structure might require many more steps then we typically use in a summerization task. For a speeches we may want to capture a gist of the rhetoric.</p>
</section>
<section id="ideas" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="ideas">Ideas</h2>
<p>Several ideas surfaced while working on the assignment for building a hybrid generative/abstractive summarizer based on GPT2<sup>1</sup>. Many ideas came from prior work developing search engines. I had noticed that some issues were anathema to summarization from its inception.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;a Generative Pre-training Transformer or a decoder transformer</p></div></div><section id="coverage-ranking-content-by-importance" class="level3">
<h3 class="anchored" data-anchor-id="coverage-ranking-content-by-importance">Coverage ranking content by importance</h3>
<p>The first issue is <strong>coverage</strong>, which is how much of the original document to include. Coverage is a much more difficult problem than it appears.</p>
<p>Consider summerizing a long novel a single paragraph summary may provide a good idea of the main plot points of the story. On the other hand summerising a similar sized encyclopedia might require listing all the top level headings and subheadings i.e.&nbsp;a one paragraph summary would not be able to provide a good idea of the content of the document. On the other hand it should be much easier to generate a summary for the encyclopedia than for the novel as the encyclopedia has a highly structured document with clear headings and subheadings and writing conventions that provide cues to the reader about the importance of the content. Summerizing a novel is more challanging as we would actualy want to ommit alsmost all the details and so deciding which parts to keep is a major challange with a very sparse signal.</p>
<p>For example, technical documents, like patents, headings, academic writing cues, and even IR statistics, may serve as features that we may feed into a regression that can guide us in discarding the chuff from the grain. On the other hand, for movie scripts or novels, we can’t use all that, and we need to decide what is essential based on the narrative structure, which requires sophisticated processing and extensive domain knowledge to extract. So, When we want to create a synopsis of a book like War and Peace, specific details are part of the narrative structure that stands out as essential to us, and I can say that even in 2025, LLM is not good at picking these out of a large document. For attentive readers with a suitable background, if we double the length of the text, they might pick additional plot points and then less significant subplots. If again we double we would, they would include more details concerning characters, their motivations, etc.</p>
<p>To conclude, different documents can have radically different structures and cues. These suggest different strategies for selecting and ranking the material to be included in a summary of a given length. More so, when we consider summaries of different lengths, one can see that we are talking about a hierarchal structure.</p>
</section>
</section>
<section id="avoiding-repetition" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="avoiding-repetition">Avoiding repetition</h2>
<p>A second challenge that is pervasive is avoiding <strong>repetition</strong>. In the generative settings we have a tougher challenge since sentences that are generated may well be equivalent to sentences we have already generated, and we want the model to redirect it attention from material already covered. Luckily we have learned to detect similar sentences in the Q&amp;A task<sup>2</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;using Siamese networks for oneshot similarity detection.</p></div></div><p>check if it is similar to any sentence in the summary</p>
<p>Alg1: similarity detection</p>
<pre><code>summary = []
tokenized_doc = tokenize(doc)
embeddings_doc= embed(tokenized_doc)
while len(summary) &lt; doc_tokens * summary_ratio: 
    a = gen_a_sentence(embeddings_doc,summary)
    for s in summary:
        if sim(a,s) &gt; threshold:
            continue
    else:
        summary.append(a)
s -&gt; gen a sentence,</code></pre>
</section>
<section id="context-window-constraints" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="context-window-constraints">Context window constraints</h2>
<p>The third challenge is technical and is related to the <strong>length of the context window</strong>. as transformer models have a limit on the number of tokens that they can process in a context window. A second issue here is that the summary itself needs to also fit in the context window. If the structure is linear and incremental we can chunk it into many smaller pieced say using sections. However in reality we are often dealing with trees of graphs structures and chunking can erode the model’s ability to inspect said hierarchy of the structure it is tasked with summarizing. In Q&amp;A or IR tasks since we can process each chunk separately and then combine the results.</p>
<p>If we are not using a generative model one imagines a tree data structure that can efficiently track what what part of the document has been summarized. If we can use that to work with the attention mechanism we may be able to reduce the effective window size as we progress with the summary. A more nuanced idea would come from <strong>bayesian search</strong> where we may want to keep moving the probability mass for the attention mechanism to regions that are significant but have not been covered.</p>
<p>Another challenge is that we may want to grow our summary in a non-linear fashion. We may consider the main narrative structure then add in the subplots and then the character motivations. This suggests two approaches - a top down <strong>planning</strong> based approach<sup>3</sup> and a bottom up approach where we start with the most important details, then add in the less important ones. In the second case we may want to revise the initial sentences to efficiently incorporate more details as we progress.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;using rl or dynamic programming</p></div><div id="fn4"><p><sup>4</sup>&nbsp;reformer layers</p></div></div><p>I also interviewed with a company that told me they often worked with patents and that these documents were frequently in excess of a hundred pages and they were having lots of problems with the summarization task. This suggest that an efficient transformer<sup>4</sup> would be needed if we are to support context windows that can span hundreds of pages. But that besides the context window we need some good way to ensure that the summary is balanced and that it is not too repetitive.</p>
</section>
<section id="compassion" class="level2">
<h2 class="anchored" data-anchor-id="compassion">Compassion</h2>
<p>My experience with summarization by hand is that most texts can be compressed by 5-10% without losing any information simply by rephrasing with more concise language. So another idea that should be obvious is that the generated summary should be <strong>compressive</strong>, i.e., once we generate a good sentence we should consider if we can make it shorter. This should be fairly easy as we are reducing the length of a single sentence. We may however be able to do even better by considering a section and trying to see if we can merge two sentences or if two sentences have partial overlap that can be merged or referenced via an anaphora. This is another area where we diverge from Q&amp;A, where we do not have a length constraint and may often want to include all relevant information. Implementation of this idea cam be easily embodied in a loss function that penalizes summaries for each token or character. This same idea can also be used in a RL summarizer.</p>
</section>
<section id="grounding" class="level2">
<h2 class="anchored" data-anchor-id="grounding">Grounding</h2>
<p>Generative summarization is a case where we can require that all generated text be grounded in the original document. In reality generative models have many glitches, due to tokenization issues, or failures of the attention mechanism, or out of distribution generation, due to bias learned from the corpus, due to negative recency bias and and due to emergent contradictions (where a contradiction is introduced into the context window and cannot be removed.) And even if we can avoid all these there is still nothing in the model that makes it prefer truthy generations. So it seems essential that the generated text is grounded in the original document. This seems to be verifiable by visually inspecting the using the attention mechanism. In reality there may be multiple heads and multiple layers. This means we need to access the attention mechanism programmatically and store annotations from the source document for each generated token. We may also want to make abstracting summarization explicit. c.f. [Extrinsic Hallucinations in LLMs in ](https://lilianweng.github.io/posts/2024-07-07-hallucination/) by Lilian Weng</p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Summarization {Task}},
  date = {2025-02-10},
  url = {https://orenbochman.github.io/notes-nlp/posts/2025-02-10-summerizer/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Summarization Task.”</span> February 10,
2025. <a href="https://orenbochman.github.io/notes-nlp/posts/2025-02-10-summerizer/">https://orenbochman.github.io/notes-nlp/posts/2025-02-10-summerizer/</a>.
</div></div></section></div> ]]></description>
  <category>NLP</category>
  <category>Notes</category>
  <category>Literature review</category>
  <category>Summarization task</category>
  <category>Relevance</category>
  <guid>https://orenbochman.github.io/notes-nlp/posts/2025-02-10-summerizer/</guid>
  <pubDate>Sun, 09 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Morphological Word Embeddings</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/2019-morphological-embeddings/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>Literature review</figcaption>
</figure>
</div></div><p>This is a mentioned in the tokenization lab in course four week 3</p>
<p>Time permitting I will try and dive deeper into this paper.</p>
<section id="podcast" class="level2">
<h2 class="anchored" data-anchor-id="podcast">Podcast</h2>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>Linguistic similarity is multi-faceted. For instance, two words may be similar with respect to semantics, syntax, or morphology inter alia. Continuous word-embeddings have been shown to capture most of these shades of similarity to some degree. This work considers guiding word-embeddings with morphologically annotated data, a form of semisupervised learning, encouraging the vectors to encode a word’s morphology, i.e., words close in the embedded space share morphological features. We extend the log-bilinear model to this end and show that indeed our learned embeddings achieve this, using German as a case study. – <span class="citation" data-cites="cotterell2019morphological">(Cotterell and Schütze 2019)</span></p>
</blockquote>
<!-- TODO: add review and podcast for this paper-->
<!-- TODO: link here from LSH lessons-->
</section>
<section id="terminology" class="level2">
<h2 class="anchored" data-anchor-id="terminology">Terminology</h2>
<p>Here are some terms and concepts discussed in the sources, with explanations:</p>
<dl>
<dt>Word embeddings</dt>
<dd>
These are vector representations of words in a continuous space, where similar words are located close to each other in the vector space. The goal is to capture linguistic similarity, but the definition of “similarity” can vary (semantic, syntactic, morphological). Word embeddings are typically trained to produce representations that capture linguistic similarity.
</dd>
<dt>Log-bilinear model (LBL)</dt>
<dd>
This is a language model that learns features along with weights, as opposed to using hand-crafted features. It uses context to predict the next word, and word embeddings fall out as low dimensional representations of the context. The LBL is a generalization of the log-linear model.
</dd>
<dt>Morphology</dt>
<dd>
The study of word structure, including how words are formed from morphemes (the smallest units of meaning).
</dd>
<dt>Morphological tags</dt>
<dd>
These are annotations that describe the morphological properties of a word, such as case, gender, number, tense, etc.. They can be very detailed for morphologically rich languages. For example, in German, a word might be tagged to indicate its part of speech, case, gender, and number.
</dd>
<dt>Morphologically rich languages</dt>
<dd>
Languages with a high morpheme-per-word ratio, where word-internal structure is important for processing. German and Czech are cited as examples.
</dd>
<dt>Morphologically impoverished languages</dt>
<dd>
Languages with a low morpheme-per-word ratio, such as English.
</dd>
<dt>Multi-task objective</dt>
<dd>
Training a model to perform multiple tasks simultaneously. In this case, the model is trained to predict both the next word and its morphological tag.
</dd>
<dt>Semi-supervised learning</dt>
<dd>
Training a model on a partially annotated corpus, using both labeled and unlabeled data. This approach is useful when large amounts of unannotated text are available.
</dd>
<dt>Contextual signature</dt>
<dd>
The words surrounding a given word. The context in which a word appears can be used to determine the word’s meaning and morphological properties.
</dd>
<dt>Hamming distance</dt>
<dd>
A measure of the difference between two binary strings, calculated as the number of positions at which the corresponding bits are different. In this context, it is used to compare morphological tags, which are represented as bit vectors.
</dd>
<dt>MORPHOSIM</dt>
<dd>
A metric for evaluating morphologically-driven embeddings that measures how morphologically similar a word is to its nearest neighbors in the embedding space. It is calculated as the average Hamming distance between morphological tags of a word and its neighbors. Lower values are better, as they indicate that the nearest neighbors of each word are closer morphologically.
</dd>
</dl>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li>Introduction
<ul>
<li>Presents the importance of capturing word morphology, especially for morphologically-rich languages.</li>
<li>Highlights the multifaceted nature of linguistic similarity (semantics, syntax, morphology).</li>
<li>Discusses the goal of the paper: to develop word embeddings that specifically encode morphological relationships.</li>
</ul></li>
<li>Related Work
<ul>
<li>Discusses previous integration of morphology into language models, including factored language models and neural network-based approaches.</li>
<li>Notes the role of morphology in computational morphology, particularly in morphological tagging for inflectionally-rich languages.</li>
<li>Highlights the significance of distributional similarity in morphological analysis.</li>
</ul></li>
<li>Log-Bilinear Model
<ul>
<li>Describes the log-bilinear model (LBL), a generalization of the log-linear model with learned features.</li>
<li>Presents the LBL’s energy function and probability distribution in the context of language modeling.</li>
</ul></li>
<li>Morph-LBL
<ul>
<li>Proposes a multi-task objective that jointly predicts the next word and its morphological tag.</li>
<li>Describes the model’s joint probability distribution, incorporating morphological tag features.</li>
<li>Discusses the use of semi-supervised learning, allowing training on partially annotated corpora</li>
</ul></li>
<li>Evaluation
<ul>
<li>Mentions the qualitative evaluation using t-SNE, showing clusters reflecting morphological and POS relationships.</li>
<li>Introduces MorphoSim, a novel quantitative metric to assess the extent to which similar embeddings are morphologically related.</li>
</ul></li>
<li>Experiments and Results
<ul>
<li>Presents experiments on the German TIGER corpus, comparing Morph-LBL with the original LBL and Word2Vec.</li>
<li>Describes two experiments:
<ul>
<li>Experiment 1: Evaluates the morphological information encoded in embeddings using a k-NN classifier.</li>
<li>Experiment 2: Compares models using the MorphoSim metric to measure morphological similarity among nearest neighbors.</li>
</ul></li>
<li>Discusses the results, highlighting Morph-LBL’s superior performance in capturing morphological relationships, even without observing all tags during training.</li>
</ul></li>
<li>Conclusion and Future Work
<ul>
<li>Summarizes the contributions of the paper: introducing Morph-LBL for inducing morphologically guided embeddings.</li>
<li>Notes the model’s success in leveraging distributional signatures to capture morphology.</li>
<li>Discusses future work on integrating orthographic features for further improvement.</li>
<li>Mentions potential applications in morphological tagging and other NLP tasks.</li>
</ul></li>
</ul>
</section>
<section id="log-bilinear-model" class="level2">
<h2 class="anchored" data-anchor-id="log-bilinear-model">Log-Bilinear Model</h2>
<p><img src="https://latex.codecogs.com/png.latex?p(w%5Cmid%20h)%20=%0A%5Cfrac%7B%5Cexp%5Cleft(s_%5Ctheta(w,h)%5Cright)%7D%7B%5Csum_%7Bw'%7D%0A%5Cexp%5Cleft(s_%5Ctheta(w',h)%5Cright)%7D%20%5Cqquad%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?w"> is a word, <img src="https://latex.codecogs.com/png.latex?h"> is a history and <img src="https://latex.codecogs.com/png.latex?s_%5Ctheta"> is an energy function. Following the notation of , in the LBL we define <img src="https://latex.codecogs.com/png.latex?%0As_%5Ctheta(w,h)%20=%20%5Cleft(%5Csum_%7Bi=1%7D%5E%7Bn-1%7D%20C_i%0Ar_%7Bh_i%7D%5Cright)%5ET%20q_w%20+%20b_w%20%5Cqquad%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?n-1"> is the history length</p>
</section>
<section id="morph-lbl" class="level2">
<h2 class="anchored" data-anchor-id="morph-lbl">Morph-LBL</h2>
<p><img src="https://latex.codecogs.com/png.latex?%0A%20%20p(w,%20t%20%5Cmid%20h)%20%5Cpropto%20%5Cexp((%20f_t%5ET%20S%20%20+%20%5Csum_%7Bi=1%7D%5E%7Bn-1%7DC_i%20r_%7Bh_i%7D)%5ET%20q_w%20+%20b_%7Bw%7D%20)%20%5Cqquad%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?f_t"> is a hand-crafted feature vector for a morphological tag <img src="https://latex.codecogs.com/png.latex?t"> and <img src="https://latex.codecogs.com/png.latex?S"> is an additional weight matrix.</p>
<p>Upon inspection, we see that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap(t%20%5Cmid%20w,h)%20%5Cpropto%20%5Cexp(S%5ET%20f_t%20q_w)%20%5Cqquad%0A"></p>
<p>Hence given a fixed embedding <img src="https://latex.codecogs.com/png.latex?q_w"> for word <img src="https://latex.codecogs.com/png.latex?w">, we can interpret <img src="https://latex.codecogs.com/png.latex?S"> as the weights of a conditional log-linear model used to predict the tag <img src="https://latex.codecogs.com/png.latex?t">.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<ul>
<li><a href="http://www.eva.mpg.de/lingua/resources/glossing-rules.php">Leipzig Glossing Rules</a> which provides a standard way to explain morphological features by examples</li>
<li><a href="https://www.youtube.com/watch?v=y9sVFrmGu0w&amp;ab_channel=GrahamNeubig">CMU Multilingual NLP 2020 (17): Morphological Analysis and Inflection</a> <!--
- [nb-lm](https://notebooklm.google.com/notebook/f594ff01-19f2-49b0-a0ac-84176fb22667?_gl=1*1rba7bx*_ga*MzAyOTc3ODMwLjE3Mzg1MDQ5Njc.*_ga_W0LDH41ZCB*MTczODkyMzY5Ni41LjAuMTczODkyMzY5Ni42MC4wLjA.)
--></li>
<li><span class="citation" data-cites="kann-etal-2016-neural">Kann, Cotterell, and Schütze (2016)</span> <a href="https://github.com/ryancotterell/neural-canonical-segmentation">code</a></li>
<li><a href="https://unimorph.github.io/">unimorph</a> univorsal morphological database</li>
</ul>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-cotterell2019morphological" class="csl-entry">
Cotterell, Ryan, and Hinrich Schütze. 2019. <span>“Morphological Word Embeddings.”</span> <em>arXiv Preprint arXiv:1907.02423</em>.
</div>
<div id="ref-kann-etal-2016-neural" class="csl-entry">
Kann, Katharina, Ryan Cotterell, and Hinrich Schütze. 2016. <span>“Neural Morphological Analysis: Encoding-Decoding Canonical Segments.”</span> In <em>Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</em>, edited by Jian Su, Kevin Duh, and Xavier Carreras, 961–67. Austin, Texas: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/D16-1097">https://doi.org/10.18653/v1/D16-1097</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Morphological {Word} {Embeddings}},
  date = {2025-02-07},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/2019-morphological-embeddings/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Morphological Word Embeddings.”</span>
February 7, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/2019-morphological-embeddings/">https://orenbochman.github.io/notes-nlp/reviews/paper/2019-morphological-embeddings/</a>.
</div></div></section></div> ]]></description>
  <category>Paper</category>
  <category>Review</category>
  <category>Podcast</category>
  <category>NLP</category>
  <category>Morphology</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/2019-morphological-embeddings/</guid>
  <pubDate>Thu, 06 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Practical and Optimal LSH for Angular Distance</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/2015-LSH/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>Literature review</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/t_8SpFV0l7A" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Locality-Sensitive Hashing and Beyond
</figcaption>
</figure>
</div><div id="vid-02" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/Dkomk2wPaoc" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;2: Beyond Locality Sensitive Hashing; Alexandr Andoni
</figcaption>
</figure>
</div></div>

<p>In the NLP specialization we have covered and used LSH a number of times in at least two courses. <!-- TODO: link to the notes so as to make them more useful as this is a stub!!!! --> In one sense I have an understanding of what is going on when we implement this. In another sense this seems to be quite confusing. So I don’t fully understand some aspects of LSH.</p>
<p>One way to do better is to try and explain it to someone else. This is what I am trying to do here by going back to the source and trying to understand the paper, problems, motivation and finally isolate what it is that is hard to grasp.</p>
<p>This paper is a good introduction to LSH for the angular distance.</p>
<p>In <span class="citation" data-cites="andoni2015practical">(Andoni et al. 2015)</span> the authors delve into Locality-Sensitive Hashing (LSH) for the angular distance. They show the existence of an LSH family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal running time exponent. Unlike earlier algorithms with this property (e.g., Spherical LSH), their algorithm is also practical, improving upon the well-studied hyperplane LSH in practice.</p>
<p>For now the two videos which explain in some depth this an the related paper - <a href="https://arxiv.org/abs/1501.01062">Optimal Data-Dependent Hashing for Approximate Near Neighbors</a> will have to do.</p>
<p>Time permitting I will try and dive deeper into this paper.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR - Location Sensitive Hashing
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/in_a_nutshell.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Location Sensitive Hashing in a Nutshell"><img src="https://orenbochman.github.io/notes-nlp/images/in_a_nutshell.jpg" class="img-fluid figure-img" alt="Location Sensitive Hashing in a Nutshell"></a></p>
<figcaption>Location Sensitive Hashing in a Nutshell</figcaption>
</figure>
</div>
<ul>
<li>The paper introduces a Locality-Sensitive Hashing (LSH) family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal running time exponent.</li>
<li>The algorithm is also practical, improving upon the well-studied hyperplane LSH in practice.</li>
<li>The authors also introduce a multiprobe version of this algorithm, and conduct experimental evaluation on real and synthetic data sets.</li>
</ul>
</div>
</div>
<section id="podcast" class="level2">
<h2 class="anchored" data-anchor-id="podcast">Podcast</h2>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>We show the existence of a Locality-Sensitive Hashing (LSH) family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal running time exponent. Unlike earlier algorithms with this property (e.g., Spherical LSH [1 , 2]), our algorithm is also practical, improving upon the well-studied hyperplane LSH [3] in practice.</p>
<p>We also introduce a <strong>multiprobe</strong> version of this algorithm, and conduct experimental evaluation on real and synthetic data sets.</p>
<p>We complement the above positive results with a fine-grained lower bound for the quality of any LSH family for angular distance. Our lower bound implies that the above LSH family exhibits a trade-off between evaluation time and quality that is close to optimal for a natural class of LSH functions. – <span class="citation" data-cites="andoni2015practical">(Andoni et al. 2015)</span></p>
</blockquote>
<!-- TODO: add outline, reflection, and terminology and for this paper-->
<!-- TODO: link here from LSH lessons-->
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-andoni2015practical" class="csl-entry">
Andoni, Alexandr, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn, and Ludwig Schmidt. 2015. <span>“Practical and Optimal LSH for Angular Distance.”</span> <em>Advances in Neural Information Processing Systems</em> 28.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Practical and {Optimal} {LSH} for {Angular} {Distance}},
  date = {2025-02-05},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/2015-LSH/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Practical and Optimal LSH for Angular
Distance.”</span> February 5, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/2015-LSH/">https://orenbochman.github.io/notes-nlp/reviews/paper/2015-LSH/</a>.
</div></div></section></div> ]]></description>
  <category>Paper</category>
  <category>Review</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/2015-LSH/</guid>
  <pubDate>Tue, 04 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>xLSTM: Extended Long Short-Term Memory</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/2025-xLSTM/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>Literature review</figcaption>
</figure>
</div><div id="vid-02" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/0aWGTNS03PU?t=3" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Review by AI Bites
</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/0OaEv1a5jUM" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;2: Review by Yannic Kilcher
</figcaption>
</figure>
</div><div id="vid-03" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/8u2pW2zZLCs" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;3: LSTM: The Comeback Story? Talk with Sepp Hochreiter after the xLSTM paper
</figcaption>
</figure>
</div></div>


<blockquote class="blockquote">
<p>Whate’er the theme, the Maiden sang<br>
<mark>As if her song could have no ending;</mark><br>
I saw her singing at her work,<br>
And o’er the sickle bending;—<br>
I listened, motionless and still;<br>
And, as I mounted up the hill,<br>
<mark>The music in my heart I bore,<br>
Long after it was heard no more.</mark> &nbsp;</p>
<p>– The Solitary Reaper by William Wordsworth.</p>
</blockquote>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR – Scaling LSTMs to Billions of Parameters with <strong>xLSTM</strong>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/in_a_nutshell.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="LSTMs in a nutshell"><img src="https://orenbochman.github.io/notes-nlp/images/in_a_nutshell.jpg" class="img-fluid figure-img" alt="LSTMs in a nutshell"></a></p>
<figcaption>LSTMs in a nutshell</figcaption>
</figure>
</div>
<ul>
<li>Just when we think that the transformer rules supreme the RNN makes a comeback with a refreshing new look at the LSTMs.</li>
<li>This paper brings new architectures to the LSTM that are fully parallelizable and can scale to billions of parameters.</li>
<li>They demonstrate on synthetic tasks and language modeling benchmarks that these new xLSTMs can outperform state-of-the-art Transformers and State Space Models.</li>
</ul>
</div>
</div>
</div>
<section id="introduction" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Motivation
</div>
</div>
<div class="callout-body-container callout-body">
<p>So this is my first review of the RNN papers. I found that since the courses covering LSTM seemed to be over simplified and confusing, I should go back to the source. Since time is limited, it helps to review the papers in reverse chronological order. Each new papers explains the limitations that are generally not apparent when the paper is first published. The authors of the later papers also have the benefit of hindsight which they share in the obligatory literature review wherein they provide lots of context we cannot get by reading the original papers. Given all that there is also a benefit to reading the original paper - the real innovators while harder to understand are remarkable thinkers and often reveal motivations and deep insights that elude later interlocutors.</p>
</div>
</div>
<section id="sec-podcast" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-podcast">Podcast &amp; Other Reviews</h3>
<p>This paper is a bit heavy on the technical side. But if we are still here, we may enjoy this podcast reviewing the paper. Though AI generated I found it quite useful to get oriented with the paper and understand that the experiments the authors run to validate this new architectures.</p>
<p>We also cover something I found absent from the Course on sequence models . The limitations of LSTMs as viewed by the authors when considered in the context of transformers and Mamba.</p>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
<p>Although this paper is recent there are a number of other people who cover it.</p>
<ul>
<li>In Video&nbsp;2 Yannic Kilcher covers the paper in his youtube channel. He does a great job of explaining the paper in a way that is easy to understand. Kilcher is all too human, he makes many mistakes, stresses opinions over facts. If we don’t mind all that he is a great resource for understanding the paper. I find he rubs me the wrong way but at least I find this motivating to do better in my own reviews!</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
What can we expect from xLSTM?
</div>
</div>
<div class="callout-body-container callout-body">

<blockquote class="blockquote">
<p>xLSTM has the potential to considerably impact other fields like Reinforcement Learning, Time Series Prediction, or the modeling of physical systems. – <span class="citation" data-cites="beck2024xlstm">(Beck et al. 2024)</span></p>
</blockquote>
<p>In his book Understanding Media <span class="citation" data-cites="mcluhan1988understanding">(McLuhan 1988)</span> Marshal McLuhan introduced his <strong>Tetrad</strong>. <mark>The <strong>Tetrad</strong> is a mental model for understanding how a technological innovation like the xLSTM might disrupt society</mark>. To work the model we ask the following four questions that can be applied to any new technology to understand its impact. The four questions in this instance are:</p>
<ol type="1">
<li>What does the xLSTM enhance or amplify?</li>
</ol>
<blockquote class="blockquote">
<p>The xLSTM enhances the LSTM by introducing exponential gating and novel memory structures, boosting its capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models.</p>
</blockquote>
<ol start="2" type="1">
<li>What does the xLSTM make obsolete or replace?</li>
</ol>
<blockquote class="blockquote">
<p>The xLSTM makes the traditional LSTM obsolete by addressing its limitations, such as the inability to revise storage decisions, limited storage capacities, and lack of parallelizability.</p>
</blockquote>
<ol start="3" type="1">
<li>What does the xLSTM retrieve that was previously obsolesced?</li>
</ol>
<blockquote class="blockquote">
<p>The xLSTM retrieves the LSTM’s relevance in large-scale language modeling by scaling it to billions of parameters and integrating modern LLM techniques while mitigating known limitations. 4. What does the xLSTM reverse or flip into when pushed to its limits?</p>
</blockquote>
<blockquote class="blockquote">
<p>The xLSTM, when pushed to its limits, can compete with current Large Language Models based on Transformer technology, offering a competitive alternative for sequence modeling tasks.</p>
</blockquote>
<p>The xLSTM paper by <span class="citation" data-cites="beck2024xlstm">(Beck et al. 2024)</span> is a great example of the tetrad in action. The authors take the LSTM, a technology that has stood the test of time, and enhance it with exponential gating and novel memory structures. They address the limitations of traditional LSTMs and push the technology to new limits, creating a new architecture that can compete with state-of-the-art Transformers and State Space Models. Recent work by <span class="citation" data-cites="chen2024computationallimitsstatespacemodels">(Chen et al. 2024)</span> suggest that Transformers and The State Space Models are actually limited in their own ways.</p>
</div>
</div>
<div class="no-row-height column-margin column-container"><div id="sup-01" class="quarto-float quarto-figure quarto-figure-center anchored callout-margin-content">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="MediaTetrad.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Tetrad"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-xLSTM/MediaTetrad.svg" class="img-fluid figure-img"></a></p>
<figcaption>Tetrad</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div></section>
</section>
<section id="abstract" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining:</p>
<ol type="1">
<li>sLSTM with a scalar memory, a scalar update, and new memory mixing,<br>
</li>
<li>mLSTM that is fully parallelizable with a matrix memory and a covariance update rule.<br>
</li>
</ol>
<p>Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling. — <span class="citation" data-cites="beck2024xlstm">(Beck et al. 2024)</span></p>
</blockquote>

<div class="no-row-height column-margin column-container"><div id="fig-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="fig01.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="architecture"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-xLSTM/fig01.png" class="img-fluid figure-img"></a></p>
<figcaption>architecture</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: The extended LSTM (xLSTM) family. From left to right:<br>
1. The original LSTM memory cell with constant error carousel and gating.<br>
2. New <strong>sLSTM</strong> and <strong>mLSTM</strong> memory cells that introduce exponential gating. sLSTM offers a new memory mixing technique. <strong>mLSTM</strong> is fully parallelizable with a novel matrix memory cell state and new covariance update rule.<br>
3. mLSTM and sLSTM in residual blocks yield xLSTM blocks.<br>
4. Stacked xLSTM blocks give an xLSTM
</figcaption>
</figure>
</div><div id="fig-02" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="fig02.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="blocks"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-xLSTM/fig02.png" class="img-fluid figure-img"></a></p>
<figcaption>blocks</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: LSTM limitations.<br>
- Left: Nearest Neighbor Search problem in terms of mean squared error (MSE). Given a reference vector, a sequence is scanned sequentially for the most similar vector with the objective to return its attached value at sequence end. LSTM struggles to revise a stored value when a more similar vector is found. Our new xLSTM overcomes this limitation by exponential gating. - Right: Rare Token Prediction. The perplexity (PPL) of token prediction on Wikitext-103, in partitions of token frequency. LSTM performs worse on predicting rare tokens because of its limited storage capacities, whereas our new xLSTM solves this problem via a matrix memory.
</figcaption>
</figure>
</div><div id="fig-03" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="fig03.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="blocks"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-xLSTM/fig03.png" class="img-fluid figure-img"></a></p>
<figcaption>blocks</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: xLSTM blocks.<br>
Left: A residual sLSTM block with post up-projection (like Transformers): The input is fed into an sLSTM — with an optional convolution — followed by a gated MLP.<br>
Right: A residual mLSTM block with pre up-projection (like State Space models): mLSTM is wrapped inside two MLPs, via a convolution, a learnable skip connection, and an output gate that acts componentwise.
</figcaption>
</figure>
</div><div id="fig-04" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-04-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="fig04.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="blocks"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-xLSTM/fig04.png" class="img-fluid figure-img"></a></p>
<figcaption>blocks</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-04-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: xLSTM blocks.<br>
Left: A residual sLSTM block with post up-projection (like Transformers): The input is fed into an sLSTM — with an optional convolution — followed by a gated MLP.<br>
Right: A residual mLSTM block with pre up-projection (like State Space models): mLSTM is wrapped inside two MLPs, via a convolution, a learnable skip connection, and an output gate that acts componentwise.
</figcaption>
</figure>
</div><div id="fig-05" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-05-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="fig05.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="blocks"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-xLSTM/fig05.png" class="img-fluid figure-img"></a></p>
<figcaption>blocks</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-05-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: xLSTM blocks.<br>
Left: A residual sLSTM block with post up-projection (like Transformers): The input is fed into an sLSTM — with an optional convolution — followed by a gated MLP.<br>
Right: A residual mLSTM block with pre up-projection (like State Space models): mLSTM is wrapped inside two MLPs, via a convolution, a learnable skip connection, and an output gate that acts componentwise.
</figcaption>
</figure>
</div></div>



</section>
<section id="sec-outline" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-outline">Paper Outline</h2>
<ul>
<li>Introduction
<ul>
<li>Describes the constant error carousel and gating mechanisms of Long Short-Term Memory (LSTM). &gt; “In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories…”</li>
<li>Discusses three main limitations of LSTMs:
<ol type="1">
<li>The inability to revise storage decisions.</li>
<li>Limited storage capacities.</li>
<li>Lack of parallelizability due to memory mixing.</li>
</ol></li>
<li>Highlights <mark>the emergence of Transformers in language modeling due to these limitations</mark>. &gt; Transformer Era: The rise of Transformers with parallelizable self-attention has largely overshadowed LSTMs due to their superior scalability.</li>
</ul></li>
<li>Extended Long Short-Term Memory
<ul>
<li>Introduces Extended Long Short-Term Memory (xLSTM), which incorporates exponential gating and novel memory structures.</li>
<li>Presents two new LSTM variants:
<ol type="1">
<li>sLSTM with a scalar memory, a scalar update, and new memory mixing.</li>
<li>mLSTM with a matrix memory and a covariance update rule, which is fully parallelizable.</li>
</ol></li>
<li>Describes the integration of these LSTM variants into residual block modules, resulting in xLSTM blocks.</li>
<li>Presents xLSTM architectures that residually stack xLSTM blocks.</li>
</ul></li>
<li>Related Work:
<ul>
<li>Mentions various linear attention methods to overcome the quadratic complexity of Transformer attention.</li>
<li>Notes the popularity of State Space Models (SSMs) for language modeling.</li>
<li>Highlights the use of Recurrent Neural Networks (RNNs) as a replacement for Transformers.</li>
<li>Mentions the use of <strong>gating</strong> in recent RNN and SSM approaches.</li>
<li>Notes the use of <strong>covariance update rules</strong><sup>1</sup> to enhance storage capacities in various models.</li>
</ul></li>
<li>Experiments
<ul>
<li>Presents the experimental evaluation of xLSTM compared to existing methods with a focus on language modeling.</li>
<li>Discusses the effectiveness of xLSTM on synthetic tasks, including:
<ul>
<li>Formal languages.</li>
<li>Multi-Query Associative Recall.</li>
<li>Long Range Arena tasks.</li>
</ul></li>
<li>Presents a method comparison and ablation study, where different models are trained on 15 billion tokens from the SlimPajama dataset.</li>
<li>Assesses the scaling behavior of different methods based on validation perplexity.</li>
<li>Conducts a large-scale language modeling experiment:
<ul>
<li>Training different model sizes on 300B tokens from SlimPajama.</li>
<li>Evaluating models on length extrapolation.</li>
<li>Assessing models on validation perplexity and performance on downstream tasks.</li>
<li>Evaluating models on 571 text domains of the PALOMA language benchmark dataset.</li>
<li>Assessing the scaling behavior with increased training data.</li>
</ul></li>
</ul></li>
<li>Limitations
<ul>
<li>Highlights limitations of xLSTM, including:
<ul>
<li>Lack of parallelizability for sLSTM due to memory mixing.</li>
<li>Unoptimized CUDA kernels for mLSTM.</li>
<li>High computational complexity of mLSTM’s matrix memory.</li>
<li>Memory overload for longer contexts due to the independence of mLSTM’s memory from sequence length.</li>
</ul></li>
</ul></li>
<li>Conclusion
<ul>
<li>Concludes that xLSTM performs favorably in language modeling compared to Transformers and State Space Models.</li>
<li>Suggests that larger xLSTM models will be competitive with current Large Language Models based on Transformer technology.</li>
<li>Notes the potential impact of xLSTM on other deep learning fields such as reinforcement learning, time series prediction, and physical system modeling.</li>
</ul></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;explain covariance</p></div></div></section>
<section id="briefing-xlstm---extended-long-short-term-memory" class="level2">
<h2 class="anchored" data-anchor-id="briefing-xlstm---extended-long-short-term-memory">Briefing : xLSTM - Extended Long Short-Term Memory</h2>
<section id="introduction-and-motivation" class="level3">
<h3 class="anchored" data-anchor-id="introduction-and-motivation">Introduction and Motivation:</h3>
<p><strong>LSTM’s Legacy</strong>: The Long Short-Term Memory (LSTM), introduced in the 1990s, has been a cornerstone of deep learning, especially in sequence modeling and early Large Language Models (LLMs). LSTMs are known for their “constant error carousel” and gating mechanisms, designed to combat vanishing gradients in recurrent neural networks.</p>
<blockquote class="blockquote">
<p>“In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories…”</p>
</blockquote>
<p>Transformer Era: The rise of Transformers with parallelizable self-attention has largely overshadowed LSTMs due to their superior scalability.</p>
<blockquote class="blockquote">
<p>“However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale.” <strong>xLSTM Question</strong>: This research aims to explore the capabilities of LSTMs when scaled to billions of parameters, by integrating modern LLM techniques while addressing known limitations.</p>
</blockquote>
<blockquote class="blockquote">
<p>“We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs?”</p>
</blockquote>
</section>
<section id="key-limitations-of-traditional-lstms" class="level3">
<h3 class="anchored" data-anchor-id="key-limitations-of-traditional-lstms">Key Limitations of Traditional LSTMs:</h3>
<p>The paper identifies three major limitations of traditional LSTMs:</p>
<ol type="1">
<li>Inability to Revise Storage Decisions: LSTMs struggle to update previously stored information when more relevant information arrives later in the sequence. The paper uses a “Nearest Neighbor Search” task to demonstrate this.</li>
</ol>
<blockquote class="blockquote">
<p>“LSTM struggles to revise a stored value when a more similar vector is found…”</p>
</blockquote>
<ol start="2" type="1">
<li>Limited Storage Capacity: LSTMs store information in scalar cell states, which can lead to information compression and loss. This impacts performance on tasks requiring detailed memory, like predicting rare tokens.</li>
</ol>
<blockquote class="blockquote">
<p>“LSTM performs worse on predicting rare tokens because of its limited storage capacities…”</p>
</blockquote>
<ol start="3" type="1">
<li>Lack of Parallelizability: LSTMs process data sequentially due to the recurrent connections between hidden states, hindering parallel computation.</li>
</ol>
<blockquote class="blockquote">
<p>“Lack of parallelizability due to memory mixing, i.e., the hidden-hidden connections between hidden states from one time step to the next, which enforce sequential processing.”</p>
</blockquote>
<ol start="3" type="1">
<li>xLSTM Innovations:</li>
</ol>
<p>The paper introduces Extended Long Short-Term Memory (xLSTM), designed to address the above limitations, by modifying the original LSTM using two main approaches:</p>
<p><strong>Exponential Gating</strong>:Implemented to allow for better revision of storage decisions. Normalization and stabilization techniques are used to avoid overflows in calculations due to the exponential function.</p>
<blockquote class="blockquote">
<p>“To empower LSTMs with the ability to revise storage decisions, we introduce exponential gates (red) together with normalization and stabilization.” Novel Memory Structures: Two new LSTM variants are created with these structures:</p>
</blockquote>
<p><strong>sLSTM</strong>: A scalar memory LSTM with a scalar update and new memory mixing technique (within heads, not across). Features multiple memory cells and multiple heads, with memory mixing within each head.</p>
<blockquote class="blockquote">
<p>“The new sLSTM (see Section 2.2) with a scalar memory, a scalar update, and memory mixing…”</p>
</blockquote>
<p><strong>mLSTM</strong>: A matrix memory LSTM, fully parallelizable, with a covariance update rule for storing key-value pairs. Does not have memory mixing between cells or heads.</p>
<blockquote class="blockquote">
<p>“the new mLSTM (see Section 2.3) with a matrix memory and a covariance (outer product) update rule, which is fully parallelizable.”</p>
</blockquote>
<ol start="4" type="1">
<li>sLSTM Details:</li>
</ol>
<ul>
<li><strong>Exponential Gates</strong>: Introduces exponential activation functions for input and forget gates with normalization and a stabilizer state to prevent overflow.</li>
<li><strong>Normalizer State</strong>: A state used to normalize the hidden state output by accumulating a weighted sum of input and forget gates.</li>
<li><strong>Memory Mixing</strong>: sLSTM allows memory mixing between cells, and also introduces a new form of memory mixing through multiple heads, mixing between cells within a head, but not across heads.</li>
</ul>
<blockquote class="blockquote">
<p>“The new sLSTM can have multiple heads with memory mixing within each head but not across heads. The introduction of heads for sLSTM together with exponential gating establishes a new way of memory mixing.”</p>
</blockquote>
<ol start="5" type="1">
<li>mLSTM Details:</li>
</ol>
<p><strong>Matrix Memory</strong>: Replaces the scalar cell state with a matrix, increasing storage capacity.</p>
<p><strong>Key-Value Storage</strong>: The mLSTM memory is based around a key-value storage system inspired by Bidirectional Associative Memories (BAMs) using covariance updates, where keys and values are updated via an outer product.</p>
<blockquote class="blockquote">
<p>“At time <img src="https://latex.codecogs.com/png.latex?t">, we want to store a pair of vectors, the key <img src="https://latex.codecogs.com/png.latex?k_t%20%E2%88%88%20R%5Ed"> and the value <img src="https://latex.codecogs.com/png.latex?v_t%20%E2%88%88%20R%5Ed">… The covariance update rule… for storing a key-value pair…”</p>
</blockquote>
<p>Parallelization: mLSTM is fully parallelizable due to abandoning hidden-hidden recurrent connections and memory mixing. Computations can be performed efficiently on GPUs using standard matrix operations.</p>
<blockquote class="blockquote">
<p>“…the mLSTM… which is fully parallelizable.”</p>
</blockquote>
<ol start="6" type="1">
<li>xLSTM Architecture:</li>
</ol>
<p><strong>xLSTM Blocks</strong>: The sLSTM and mLSTM variants are integrated into residual blocks.</p>
<p><strong>sLSTM blocks</strong> use post up-projection (like Transformers).</p>
<p><strong>mLSTM blocks</strong> use pre up-projection (like State Space Models).</p>
<blockquote class="blockquote">
<p>“Integrating these new LSTM variants into residual block modules results in xLSTM blocks…”</p>
</blockquote>
<p><strong>Residual Stacking</strong>: xLSTM architectures are created by residually stacking these blocks, using pre-LayerNorm backbones similar to current LLMs.</p>
<blockquote class="blockquote">
<p>“An xLSTM architecture is constructed by residually stacking build-ing blocks…” <strong>Cover’s Theorem</strong>: Used to justify mapping information into a higher-dimensional space for better separation of histories or contexts.</p>
</blockquote>
<blockquote class="blockquote">
<p>“We resort to Cover’s Theorem (Cover, 1965), which states that in a higher dimensional space non-linearly embedded patterns can more likely be linearly separated than in the original space.”</p>
</blockquote>
<ol start="7" type="1">
<li>Performance and Scaling:</li>
</ol>
<p><strong>Linear Computation &amp; Constant Memory</strong>: Unlike Transformers, xLSTM networks have linear computation and constant memory complexity with respect to sequence length.</p>
<blockquote class="blockquote">
<p>“Contrary to Transformers, xLSTM networks have a linear computation and a constant memory complexity with respect to the sequence length.”</p>
</blockquote>
<p><strong>Synthetic Tasks</strong>: xLSTM showed improved performance over regular LSTMs on “Nearest Neighbor Search” and “Rare Token Prediction” synthetic tasks.</p>
<p><strong>SlimPajama Experiments</strong>: The xLSTM models were trained on 15B and 300B tokens from the SlimPajama dataset for language modelling evaluation.</p>
<p><strong>Competitive Performance</strong>: xLSTMs demonstrate competitive performance against state-of-the-art Transformers and State Space Models on language modelling benchmarks, both in terms of next token prediction perplexity and downstream tasks.</p>
<blockquote class="blockquote">
<p>“Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.”</p>
</blockquote>
<p>Ablation studies show importance of gating techniques.</p>
<ol start="8" type="1">
<li>Memory &amp; Speed:</li>
</ol>
<p><strong>sLSTM</strong>: Not parallelizable due to memory mixing, although the authors have created a fast CUDA implementation (less than twice as slow as mLSTM).</p>
<blockquote class="blockquote">
<p>“While mLSTM is parallelizable analog to FlashAttention… sLSTM is not parallelizable due to the memory mixing…”</p>
</blockquote>
<p><strong>mLSTM</strong>: Parallelizable, but currently slower than FlashAttention or Mamba due to non-optimized CUDA kernels. Matrix memory is computationally complex but does not require parameters and can be parallelized.</p>
<blockquote class="blockquote">
<p>“The memory of mLSTM does not require parameters but is computationally expensive through its d×d matrix memory and d× d update… the computations can be done in parallel on GPUs, therefore these computations have only a minor effect on the wall clock time.”</p>
</blockquote>
<ol start="9" type="1">
<li>Limitations:</li>
</ol>
<p><strong>sLSTM Parallelization</strong>: sLSTM’s memory mixing is non-parallelizable.</p>
<blockquote class="blockquote">
<p>“(i) In contrast to mLSTM, memory mixing of the sLSTM prohibits parallelizable operations, and therefore does not allow a fast parallel implementation.”</p>
</blockquote>
<p>mLSTM CUDA Optimization: The mLSTM CUDA kernels are not fully optimized.</p>
<blockquote class="blockquote">
<p>“(ii) The CUDA kernels for mLSTM are not optimized, and therefore the current implementation is about 4 times slower than FlashAttention or the scan used in Mamba.”</p>
</blockquote>
<p><strong>mLSTM Matrix Memory</strong>: High computational complexity for mLSTM due to matrix memory operations.</p>
<p><strong>Forget Gate Initialization</strong>: Careful initialization of the forget gates is needed.</p>
<p><strong>Long Context Memory</strong>: The matrix memory is independent of sequence length, and might overload memory for long context sizes.</p>
<blockquote class="blockquote">
<p>“(v) Since the matrix memory is independent of the sequence length, increasing the sequence length might overload the memory for longer context sizes.”</p>
</blockquote>
<p><strong>Hyperparameter Optimization</strong>: xLSTM is not yet fully optimized due to computational load, indicating further performance gains are possible.</p>
<blockquote class="blockquote">
<p>“(vi) Due to the expensive computational load for large language experiments, we did neither fully optimize the architecture nor the hyperparameters, especially for larger xLSTM architectures.”</p>
</blockquote>
<ol start="10" type="1">
<li>Related Work:</li>
</ol>
<p>The paper highlights connections of its ideas with the following areas:</p>
<p><strong>Gating</strong>: Many recent models such as HGRN, GLA, Gated State Space models, Mamba, and others also utilize gating mechanisms. Covariance Update Rule: The covariance update rule used in mLSTM is related to other methods such as Fast Weight Programmers, Retention, and Linear Transformers.</p>
<ol start="11" type="1">
<li>Conclusion:</li>
</ol>
<p>The xLSTM presents a compelling alternative to Transformers in sequence modeling, especially in contexts where memory efficiency and longer sequences are required. By combining insights from the original LSTM with exponential gating and novel memory structures, the paper shows that LSTMs can be competitive at scale. While limitations exist in terms of parallelization and hyperparameter tuning, the research shows promising potential for further development. The authors posit further research and improvements could allow xLSTM to reach its full potential.</p>
</section>
</section>
<section id="my-thoughts" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="my-thoughts">My Thoughts</h2>

<div class="no-row-height column-margin column-container"><div id="vid-sigmoid" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-sigmoid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/TPqr8t919YM" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-sigmoid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;4: Review of the sigmoid function
</figcaption>
</figure>
</div></div><div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Research questions
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li>How does the constant error carousel mitigate the vanishing gradient problem in the LSTM?
<ul>
<li>The constant error carousel is a way to keep the error constant over time. This helps to prevent the vanishing gradient problem in the LSTM.</li>
</ul></li>
<li>How are the gates in the original LSTM binary?
<ul>
<li>Sigmoid, saturation and a threshold at 0.5</li>
</ul></li>
<li>What is the long term memory in the LSTM?
<ul>
<li>the cell state <img src="https://latex.codecogs.com/png.latex?c_%7Bt-1%7D"></li>
</ul></li>
<li>What is the short term memory in the LSTM?
<ul>
<li>the hidden state <img src="https://latex.codecogs.com/png.latex?h_%7Bt-1%7D"></li>
</ul></li>
<li>How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs</li>
</ol>
</div>
</div>

<div class="no-row-height column-margin column-container"><div id="sup-02" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="sigmoid-limits.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Supplementary Figure&nbsp;2: limits of the sigmoid function"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-xLSTM/sigmoid-limits.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup" id="sup-02-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;2: limits of the sigmoid function
</figcaption>
</figure>
</div><div id="sup-03" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="sigmoid-values.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="bias"><img src="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-xLSTM/sigmoid-values.png" class="img-fluid figure-img"></a></p>
<figcaption>bias</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup" id="sup-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;3: The inductive bias of the sigmoid decision function.
</figcaption>
</figure>
</div></div>
<section id="binary-nature-of-non-exponential-gating-mechanisms" class="level3">
<h3 class="anchored" data-anchor-id="binary-nature-of-non-exponential-gating-mechanisms">Binary nature of non exponential Gating Mechanisms</h3>
<p>If we try to understand why are the gating mechanisms in the original LSTM is described here as binary?</p>
<p>It helps to the properties of the sigmoid functions that are explained in Video&nbsp;4.</p>
<ol type="1">
<li><p>Supplementary Figure&nbsp;1 shows that The sigmoid function has a domain of <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D"> and a range of <img src="https://latex.codecogs.com/png.latex?(0,1)">. This means that the sigmoid function can only output values between 0 and 1.</p></li>
<li><p>It has a very sharp gradient around 0 if the bias is set to 0. This means that the sigmoid function acts as a binary function around 0.</p></li>
<li><p>The Sigmoid function tends to saturate. Now sigmoid function around 0 isn’t quite quite binary but outside of (-6,+6) it becomes saturated and acts very much as a binary function.</p></li>
<li><p>If we use it as a loss function or a decision function, the sigmoid function is used with a threshold of 0.5. This means that if the output of the sigmoid function is greater than 0.5, the decision is 1, and if it is less than 0.5, the decision is 0. That is how the sigmoid function can become binary.</p></li>
<li><p>Note that we can use thresholding during inference to make the decision binary and we can keep them as continuous values during training so we can have a smooth gradient for back-propagation.</p></li>
<li><p>Even without thresholding the sigmoid function can become binary this is due to the saturation of the sigmoid function. I recall that Hinton in his Coursera course on Neural Networks and Deep Learning mentioned that activation function that become saturated are problematic. I can say that at last I think I understand what he meant:<br>
I see that that sigmoid functions can fall off the manifold of the data and this can be a problem. This is why the ReLU function is used in the Gated Recurrent Unit (GRU) and the Transformer. <strong>Falling off the manifold of the data</strong> means that the sigmoid function becomes saturated (above 6 and below -6). At this point the back-propagation steps are no longer able to change the weight or bias since the gradient is effectively so learning stops.</p></li>
</ol>
<p>This becomes a is a problem in two senses. The sigmoid unit is likely to be stuck in a setting that only representative of part of the training data. The rest of the network which is not stuck now has to learn to compensate for this.</p>
<p>This issues may well be the root cause LSTM limitation number 1 - the inability to revise storage decisions. It seems that the authors decided to use the exponential gating function to address this issues.</p>
<p>This is why the gating mechanisms in the original LSTM are binary. The sigmoid function is used to control the flow of information in the LSTM by deciding which information to keep and which to discard. The sigmoid function is used to decide how much of the new input information should be added to the cell state and how much of the old cell state should be retained. This binary nature of the sigmoid function is what allows the LSTM to control the flow of information and decide what to remember and what to forget.</p>
</section>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li><p><a href="https://openreview.net/forum?id=ARAxPPIAhq&amp;noteId=gra7vHnb0q">The paper on open review</a> has some additional insights from the authors</p></li>
<li><p><a href="https://www.ai-bites.net/xlstm-extended-long-short-term-memory-networks/">XLSTM — Extended Long Short-Term Memory Networks</a> By Shrinivasan Sankar — May 20, 2024</p></li>
</ul>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-beck2024xlstm" class="csl-entry">
Beck, Maximilian, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, Günter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. 2024. <span>“xLSTM: Extended Long Short-Term Memory.”</span> <em>arXiv Preprint arXiv:2405.04517</em>.
</div>
<div id="ref-chen2024computationallimitsstatespacemodels" class="csl-entry">
Chen, Yifang, Xiaoyu Li, Yingyu Liang, Zhenmei Shi, and Zhao Song. 2024. <span>“The Computational Limits of State-Space Models and Mamba via the Lens of Circuit Complexity.”</span> <a href="https://arxiv.org/abs/2412.06148">https://arxiv.org/abs/2412.06148</a>.
</div>
<div id="ref-mcluhan1988understanding" class="csl-entry">
McLuhan, Marshall. 1988. <em>Understanding Media : The Extensions of Man</em>. New York: New American Library. <a href="http://www.worldcat.org/search?qt=worldcat_org_all&amp;q=0451624963">http://www.worldcat.org/search?qt=worldcat_org_all&amp;q=0451624963</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {xLSTM: {Extended} {Long} {Short-Term} {Memory}},
  date = {2025-02-03},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/2025-xLSTM/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“xLSTM: Extended Long Short-Term
Memory.”</span> February 3, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/2025-xLSTM/">https://orenbochman.github.io/notes-nlp/reviews/paper/2025-xLSTM/</a>.
</div></div></section></div> ]]></description>
  <category>Paper</category>
  <category>Review</category>
  <category>NLP</category>
  <category>LSTM</category>
  <category>Seq2Seqs</category>
  <category>RNN</category>
  <category>Podcast</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/2025-xLSTM/</guid>
  <pubDate>Sun, 02 Feb 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>The Secret Life of Pronouns What Our Words Say About Us</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/book/seret-life-of-pronouns/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="cover.jpg" class="lightbox" data-gallery="slides" title="cover"><img src="https://orenbochman.github.io/notes-nlp/reviews/book/seret-life-of-pronouns/cover.jpg" class="img-fluid figure-img" alt="cover"></a></p>
<figcaption>cover</figcaption>
</figure>
</div></div><div class="page-columns page-full"><blockquote class="blockquote">
<p>To grunt and sweat under a weary life, But that the dread of something after death, <mark>The undiscovered country from whose bourn No traveler returns, puzzles the will, And makes us rather bear those ills we have, Than fly to others that we know not of</mark>?<sup>1</sup> — Hamlet Act 3, Scene 1 by William Shakespeare.</p>
</blockquote><div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Pronouns oft fall prey to the stop word filter, yet they hold the keys to unlocking the depth of intimate meaning</p></div></div></div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR – ML, NLP and the secret life of pronouns
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/in_a_nutshell.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Pronouns in a nutshell"><img src="https://orenbochman.github.io/notes-nlp/images/in_a_nutshell.jpg" class="img-fluid figure-img" alt="Pronouns in a nutshell"></a></p>
<figcaption>Pronouns in a nutshell</figcaption>
</figure>
</div>
<ul>
<li>When I got started with NLP I was coding search engines and conventional wisdom was <mark>pronouns don’t pass the <strong>stop word filter</strong></mark></li>
<li>One of the gems I learned on that job was that <mark>everything in the corpus can be provide invaluable information if we only know how to index it</mark>.</li>
<li>After reading this book and I started to unlocked the power of the pronouns.</li>
<li>At Hungarian School I discovered how pronouns combine with case ending to create a vast vistas of untapped NLP resource.</li>
<li>Cognitive AI I noted that pronouns and particles are key in extending the primitive verb system via thematic roles to get a very rich semantic systems in the lexicon with little costs in learning.</li>
</ul>
</div>
</div>
</div>
<section id="podcast" class="level2">
<h2 class="anchored" data-anchor-id="podcast">Podcast</h2>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The secret life of pronouns in context
</div>
</div>
<div class="callout-body-container callout-body">
<p>I got interested in this book when I was deep into wikipedia research. I was in a place where I wanted to be able to attribute content to the authors. I had read some disturbing articles with prior work on the topic by some researchers from my Alma Mater. However, I eventually came across this book and it seemed to put the ideas in a better form. Less about ML and more about the type of writers and their personalities.</p>
</div>
</div>
<p>In <span class="citation" data-cites="pennebaker2013secret">(Pennebaker 2013)</span> “The Secret Life of Pronouns: What Our Words Say About Us,” the author explores how the seemingly innocuous function words we use—pronouns, articles, prepositions, and auxiliary verbs—can reveal a great deal about our personalities, emotions, social connections, and psychological states. James W. Pennebaker, the author and a social psychologist, uses computer-based text analysis methods to uncover these hidden patterns.</p>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li><p><strong>Key Questions and Themes:</strong></p>
<ul>
<li><strong>Can language reveal psychological states?</strong> The author is primarily interested in what people’s words say about their psychological states. The book investigates whether language features can reveal how people are thinking and whether this information can be used to change their thinking.</li>
<li><strong>How do function words differ from content words?</strong> The book distinguishes between content and style of speech, arguing that function words, which reflect language style, reveal more about a person than content words. Function words are tied to the relationship between speaker and listener.</li>
<li><strong>Do men and women use words differently?</strong> The book explores gender differences in language use, noting that women are typically more attentive to the thoughts and feelings of others, while men are less so. It also investigates whether different writing styles reflect different personalities.</li>
<li><strong>Can language predict behavior?</strong> The book examines whether language use can predict behaviors such as success in college, risk of suicide, leadership styles, and likelihood of declaring war.</li>
<li><strong>How can language be used as a tool for change?</strong> The author examines whether analyzing words can lead to improvements in relationships, teaching, and leadership. Additionally, the book looks at how writing about emotional upheavals can improve mental and physical health, and whether using certain types of words can influence the effectiveness of that writing.</li>
<li><strong>Can language reveal deception?</strong> The book explores linguistic markers of deception, such as the use of first-person singular pronouns and the mention of other individuals.</li>
<li><strong>Can language analysis help identify authors?</strong> The book presents methods for identifying authors using function words, punctuation, and obscure words.</li>
</ul></li>
<li><p><strong>Main Examples and Studies:</strong></p>
<ul>
<li><strong>Expressive Writing:</strong> Early research focused on the benefits of writing about traumatic experiences, leading to the development of the LIWC (Linguistic Inquiry and Word Count) program. This program helps analyze word categories associated with healthy writing such as positive emotion words, cognitive words, and the ability to construct a story.</li>
<li><strong>The Bottle and the Two People Pictures:</strong> Participants described pictures, and their descriptions were analyzed for function word use and content. For example, people who focused on shadows in the bottle picture tended to be more creative. The second picture revealed common themes such as social connections, clothing, and accessories.</li>
<li><strong>Thinking Styles:</strong> The book describes how people’s function words can be statistically grouped into categories reflecting thinking styles, such as formal, analytic, and narrative. These thinking styles can predict how people relate to others and organize their worlds.</li>
<li><strong>9/11 Blog Analysis:</strong> The language used in blog posts was analyzed before and after the 9/11 attacks. Researchers noted an increase in negative emotion words, a decrease in positive emotion words, and a temporary increase in cognitive words.</li>
<li><strong>College Admissions Essays:</strong> The study examined whether the writing style in college admissions essays could predict college grades.</li>
<li><strong>The Federalist Papers:</strong> The book discusses the use of function words to determine the authorship of the anonymous Federalist Papers.</li>
<li><strong>Language Style Matching (LSM):</strong> LSM is a technique to determine how similarly two people use function words. It is used in studies of couples, speed dating, and even the writings of historical figures to analyze the relationship between the authors.</li>
<li><strong>Obama’s Pronoun Use</strong>: The book analyzes Barack Obama’s use of first-person singular pronouns. His relatively low use of I-words is interpreted as a sign of self-confidence.</li>
</ul></li>
<li><p><strong>Additional Insights:</strong></p>
<ul>
<li><strong>Stealth Words:</strong> The book emphasizes the importance of function words, often called “stealth words,” which are often overlooked.</li>
<li><strong>The Role of Computers:</strong> Computer technology has revolutionized the study of language, allowing researchers to analyze large datasets and uncover hidden patterns that would be impossible to see with manual analysis.</li>
<li><strong>Language as a Tool:</strong> Language can be used not only as a window into the inner workings of people, but also as a tool for guiding thinking and promoting change.</li>
<li><strong>Interdisciplinary Approach</strong>: The author’s work draws on multiple disciplines including psychology, linguistics, computer science, history, and political science.</li>
</ul></li>
</ul>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-pennebaker2013secret" class="csl-entry">
Pennebaker, J. W. 2013. <em>The Secret Life of Pronouns: What Our Words Say about Us</em>. Bloomsbury USA. <a href="https://books.google.co.il/books?id=p9KmCAAAQBAJ">https://books.google.co.il/books?id=p9KmCAAAQBAJ</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {The {Secret} {Life} of {Pronouns} {What} {Our} {Words} {Say}
    {About} {Us}},
  date = {2025-01-30},
  url = {https://orenbochman.github.io/notes-nlp/reviews/book/seret-life-of-pronouns/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“The Secret Life of Pronouns What Our Words
Say About Us.”</span> January 30, 2025. <a href="https://orenbochman.github.io/notes-nlp/reviews/book/seret-life-of-pronouns/">https://orenbochman.github.io/notes-nlp/reviews/book/seret-life-of-pronouns/</a>.
</div></div></section></div> ]]></description>
  <category>Review</category>
  <category>Book</category>
  <category>NLP</category>
  <category>Sentiment Analysis</category>
  <category>Sentiment Analysis</category>
  <category>Podcast</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/book/seret-life-of-pronouns/</guid>
  <pubDate>Wed, 29 Jan 2025 22:00:00 GMT</pubDate>
</item>
<item>
  <title>When and Why are Pre-trained Word Embeddings Useful for Neural Machine Translation?</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/2018-PTWM-NMT/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>Literature review</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/o41avjKIg5o" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Talk covering this paper by Roee Aharoni
</figcaption>
</figure>
</div><div id="sup-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://roeeaharoni.com/NMT_slides_orginal.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Roee Aharoni’s slides"><embed src="https://roeeaharoni.com/NMT_slides_orginal.pdf" class="img-fluid"></a></p>
<figcaption>Roee Aharoni’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div>

<section id="podcast" class="level2">
<h2 class="anchored" data-anchor-id="podcast">Podcast</h2>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>The performance of Neural Machine Translation (NMT) systems often suffers in low resource scenarios where sufficiently large scale parallel corpora cannot be obtained. Pretrained word embeddings have proven to be invaluable for improving performance in natural language analysis tasks, which often suffer from paucity of data. However, their utility for NMT has not been extensively explored. In this work, we perform five sets of experiments that analyze when we can expect pre-trained word embeddings to help in NMT tasks. We show that such embeddings can be surprisingly effective in some cases – providing gains of up to 20 BLEU points in the most favorable setting.. –<span class="citation" data-cites="qi-etal-2018-pre">(Qi et al. 2018)</span></p>
</blockquote>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li>**Introduction
<ul>
<li>Describes the problem of low-resource scenarios in Neural Machine Translation (NMT) and the potential utility of pre-trained word embeddings.</li>
<li>Highlights the success of pre-trained embeddings in natural language analysis tasks and the lack of extensive exploration in NMT.</li>
<li>Poses five research questions:
<ul>
<li><strong>Q1</strong> Is the behavior of pre-training affected by language families and other linguistic features of source and target languages? (§3)</li>
<li><strong>Q2</strong> Do pre-trained embeddings help more when the size of the training data is small? (§4)</li>
<li><strong>Q3</strong> How much does the similarity of the source and target languages affect the efficacy of using pre-trained embeddings? (§5)</li>
<li><strong>Q4</strong> Is it helpful to align the embedding spaces between the source and target languages? (§6)</li>
<li><strong>Q5</strong> Do pre-trained embeddings help more in multilingual systems as compared to bilingual systems? (§7)</li>
</ul></li>
</ul></li>
<li>Experimental Setup
<ul>
<li>Details the five sets of experiments conducted to evaluate the effectiveness of pre-trained word embeddings in NMT.</li>
<li>Describes the datasets used, including the WMT14 English-German and English-French translation tasks.</li>
<li>Outlines the models and training procedures employed in the experiments.</li>
</ul></li>
<li>Results and Analysis
<ul>
<li>Presents the results of the experiments, showing the impact of pre-trained word embeddings on NMT performance.</li>
<li>Discusses the observed gains in BLEU scores and the factors influencing the effectiveness of pre-trained embeddings.</li>
<li>Analyzes the relationship between the quality of pre-trained embeddings and the performance of NMT systems.</li>
</ul></li>
<li>Analysis
<ul>
<li>Considers the implications of the findings for NMT research and practice.</li>
<li>Discusses the potential benefits and limitations of using pre-trained word embeddings in NMT tasks.</li>
</ul></li>
<li>Conclusion
<ul>
<li>The sweet-spot is where there is very little training data yet enough to train the system.</li>
<li>PTWE are more effective if there are more similar translation pairs.</li>
<li>A priori alignment of embeddings may not be necessary in bilingual scenarios, but is helpful in multi-lingual training scenarios.</li>
</ul></li>
</ul>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-qi-etal-2018-pre" class="csl-entry">
Qi, Ye, Devendra Sachan, Matthieu Felix, Sarguna Padmanabhan, and Graham Neubig. 2018. <span>“When and Why Are Pre-Trained Word Embeddings Useful for Neural Machine Translation?”</span> In <em>Proceedings of the 2018 Conference of the North <span>A</span>merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</em>, edited by Marilyn Walker, Heng Ji, and Amanda Stent, 529–35. New Orleans, Louisiana: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/N18-2084">https://doi.org/10.18653/v1/N18-2084</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {When and {Why} Are {Pre-trained} {Word} {Embeddings} {Useful}
    for {Neural} {Machine} {Translation?}},
  date = {2024-02-11},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/2018-PTWM-NMT/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“When and Why Are Pre-Trained Word Embeddings
Useful for Neural Machine Translation?”</span> February 11, 2024. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/2018-PTWM-NMT/">https://orenbochman.github.io/notes-nlp/reviews/paper/2018-PTWM-NMT/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Bidirectional LSTM</category>
  <category>Deep learning</category>
  <category>Embeddings</category>
  <category>LLM</category>
  <category>NLP</category>
  <category>Paper</category>
  <category>Review</category>
  <category>Podcast</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/2018-PTWM-NMT/</guid>
  <pubDate>Sat, 10 Feb 2024 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Neural Machine Translation by Jointly Learning to Align and Translate</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/reviews/paper/2014-NMT-by-jointly-learning-to-align-and-translate/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/literature-review-open-book.jpg" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>Literature review</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/o41avjKIg5o" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Roee Aharoni’s Talk covering this paper (Hebrew)
</figcaption>
</figure>
</div><div id="sup-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://roeeaharoni.com/NMT_slides_orginal.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Roee Aharoni’s slides"><embed src="https://roeeaharoni.com/NMT_slides_orginal.pdf" class="img-fluid"></a></p>
<figcaption>Roee Aharoni’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div>

<section id="podcast" class="level2">
<h2 class="anchored" data-anchor-id="podcast">Podcast</h2>
<audio controls="1">
<source src="podcast.mp3" type="audio/mpeg">
</audio>
</section>
<section id="abstract" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<div class="page-columns page-full"><blockquote class="blockquote">
<p><em>Neural machine translation</em> is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of <em>encoder-decoders</em> and consists of an encoder that encodes a source sentence into a <em>fixed-length vector</em><sup>1</sup> from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition. –<span class="citation" data-cites="bahdanau2016neuralmachinetranslationjointly">(Bahdanau, Cho, and Bengio 2016)</span></p>
</blockquote><div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;the encoded state</p></div></div></div>
</section>
<section id="outline" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li>Introduction
<ul>
<li>Positions NMT as a new approach, contrasting it with traditional phrase-based systems.</li>
<li>Describes encoder-decoder models that use a fixed-length vector to encode a source sentence.</li>
<li>Explains that these models are <mark>trained to maximize the probability of a correct translation.</mark></li>
<li>Discusses the <mark>limitation of compressing all source sentence information into <em>a fixed-length vector</em><sup>2</sup>, especially for long sentences.</mark></li>
<li><mark>Introduces the extension of the encoder-decoder model that jointly learns to align and translate.</mark></li>
<li>Emphasizes that the model searches for relevant source positions when generating a target word.</li>
<li>States that the new model encodes the input sentence into a sequence of vectors and adaptively chooses a subset during decoding.</li>
<li>Asserts that the new model performs better with long sentences and that the proposed approach performs better translation compared to the basic encoder-decoder approach.</li>
<li>Notes that the improvement is apparent with longer sentences and that the model achieves comparable performance to a conventional phrase-based system.</li>
<li>Mentions that the model finds linguistically plausible alignments.</li>
</ul></li>
<li>Background: <em>Neural Machine Translation</em> (NMT)
<ul>
<li><mark>Defines translation as <em>maximizing</em> the <strong>conditional probability of a target sentence given a source sentence</strong>.</mark> <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Barg%7D%5C,%5Cmax_%7By%7D%5C,%20%7Bp%7D(y%20%5Cmid%20x)."></li>
<li>Explains that <mark>NMT models learn this conditional distribution using <em>parallel training corpora</em>.</mark></li>
<li>Describes NMT models as consisting of <em>an encoder and a decoder</em>.</li>
<li>Notes that <em>Recurrent Neural Networks</em> (RNNs) have been used for encoding and decoding <em>variable-length sentences</em>.</li>
<li>Points out that NMT has shown promising results, and can achieve state-of-the-art performance.</li>
<li>Notes that adding <em>neural networks</em> to existing systems can boost performance levels.</li>
<li>RNN Encoder-Decoder
<ul>
<li>Describes the <em>RNN Encoder-Decoder framework</em>, where an encoder reads the input sentence into a vector c.</li>
<li>Explains that the <em>encoder</em> uses an RNN to generate <em>a sequence of hidden states</em>, from which the vector c is generated.</li>
<li>Notes that the <em>decoder</em> predicts the next word given the context vector and previously predicted words.</li>
<li>Presents a formula for the conditional probability, which is modeled with an RNN.</li>
</ul></li>
</ul></li>
<li>Learning to Align and Translate
<ul>
<li>Introduces a new architecture for NMT using a bidirectional RNN encoder and a decoder that searches through the source sentence.</li>
<li>Decoder: General Description
<ul>
<li>Presents a new conditional probability conditioned on a distinct context vector for each target word.</li>
<li>Explains that the context vector depends on a sequence of annotations from the encoder.</li>
<li>Defines the context vector as a weighted sum of annotations.</li>
<li>Describes how the weights are computed, using an alignment model.</li>
<li>Emphasizes that the alignment model computes a soft alignment.</li>
<li>Explains the weighted sum of annotations as computing an expected annotation.</li>
<li>States that the probability of the annotation reflects its importance in deciding the next state and generating the target word.</li>
<li>Notes that this implements an attention mechanism in the decoder.</li>
</ul></li>
<li>Encoder: Bidirectional RNN for Annotating Sequences
<ul>
<li>Introduces the use of a bidirectional RNN (BiRNN) to summarize preceding and following words.</li>
<li>Describes the forward and backward RNNs that comprise the BiRNN.</li>
<li>Explains how annotations are created by concatenating forward and backward hidden states.</li>
<li>Notes that the annotation will focus on words around the current word due to the nature of RNNs.</li>
</ul></li>
</ul></li>
<li>Experiment Settings
<ul>
<li>States that the proposed approach is evaluated on English-to-French translation using ACL WMT ’14 bilingual corpora.</li>
<li>Compares the approach with an RNN Encoder-Decoder.</li>
<li>Dataset
<ul>
<li>Details the corpora used, their sizes, and the data selection method used.</li>
<li>Notes that no monolingual data other than the mentioned parallel corpora is used.</li>
<li>Describes how the development and test sets were created.</li>
<li>Details the tokenization and word shortlist used for training the models.</li>
</ul></li>
<li>Models
<ul>
<li>Details the two types of models trained, RNN Encoder-Decoder (RNNencdec) and RNNsearch, and the sentence lengths used.</li>
<li>Describes the hidden units in the encoder and decoder for both models.</li>
<li>Mentions the use of a multilayer network with a maxout hidden layer.</li>
<li>Describes the use of minibatch stochastic gradient descent (SGD) and Adadelta for training, along with the minibatch size and training time.</li>
<li>Explains the use of beam search to find the translation that maximizes the conditional probability.</li>
<li>Refers to the appendices for more details on the architectures and training procedure.</li>
</ul></li>
</ul></li>
<li>Results
<ul>
<li>Quantitative Results
<ul>
<li>Presents translation performance measured in BLEU scores, showing that RNNsearch outperforms RNNencdec in all cases.</li>
<li>Notes that the performance of RNNsearch is comparable to that of the phrase-based system, even without using a separate monolingual corpus.</li>
<li>Shows that RNNencdec’s performance decreases with longer sentences.</li>
<li>Demonstrates that RNNsearch is more robust to sentence length, with no performance drop even with sentences of length 50 or more.</li>
<li>Highlights the superiority of RNNsearch by noting that RNNsearch-30 outperforms RNNencdec-50.</li>
<li>Presents a table of BLEU scores for each model.</li>
</ul></li>
<li>Qualitative Analysis
<ul>
<li>Alignment
<ul>
<li>Explains that the approach offers an intuitive way to inspect the soft alignment between words.</li>
<li>Describes visualizing annotation weights to see which source positions were considered important.</li>
<li>Notes the largely monotonic alignment of words, with strong weights along the diagonal.</li>
<li>Highlights examples of non-trivial alignments and how the model correctly translates phrases.</li>
<li>Explains the strength of soft-alignment using an example of the phrase “the man” being translated to “l’ homme”.</li>
<li>Notes that soft alignment deals naturally with phrases of different lengths.</li>
</ul></li>
<li>Long Sentences
<ul>
<li>Explains that the model does not require encoding a long sentence into a fixed-length vector perfectly.</li>
<li>Provides examples of translations of long sentences, showing that RNNencdec deviates from the source meaning.</li>
<li>Demonstrates that RNNsearch translates long sentences correctly, preserving the whole meaning.</li>
<li>Confirms that RNNsearch enables more reliable translation of long sentences than RNNencdec.</li>
</ul></li>
</ul></li>
</ul></li>
<li>Related Work
<ul>
<li>Learning to Align
<ul>
<li>Mentions a similar alignment approach used in handwriting synthesis.</li>
<li>Notes the key difference: in handwriting synthesis, the modes of the weights of the annotations only move in one direction.</li>
<li>Explains that, in machine translation, reordering is often needed, and the proposed approach computes annotation weight of every source word for each target word.</li>
</ul></li>
<li>Neural Networks for Machine Translation
<ul>
<li>Discusses the history of neural networks in machine translation, from providing single features to existing systems to reranking candidate translations.</li>
<li>Mentions examples of neural networks being used as a sub-component of existing translation systems.</li>
<li>Highlights that the paper focuses on designing a completely new translation system based on neural networks.</li>
</ul></li>
</ul></li>
<li>Conclusion
<ul>
<li>States that the paper proposes a new architecture that extends the basic model by allowing the model to (soft-)search for input words when generating a target word.</li>
<li>Highlights the benefit of freeing the model from encoding the whole sentence into a fixed-length vector.</li>
<li>Notes that <strong>all components are jointly trained towards a better log-probability of producing correct translations</strong>.</li>
<li>Confirms that the proposed RNNsearch outperforms the conventional encoder-decoder model and is more robust to sentence length.</li>
<li>Concludes that the model aligns each target word with the relevant words in the source sentence.</li>
<li>Points out that the proposed approach achieved translation performance comparable to the phrase-based statistical machine translation, despite its recent development.</li>
<li>Suggests that the approach is a promising step toward better machine translation and a better understanding of natural languages.</li>
<li>Identifies better handling of unknown or rare words as a challenge for the future.</li>
</ul></li>
<li>Appendix A: Model Architecture
<ul>
<li>Architectural Choices
<ul>
<li>Describes that the scheme is a general framework where the activation functions of RNNs and the alignment model can be defined.</li>
<li>Recurrent Neural Network
<ul>
<li>Explains the use of the <em>gated hidden unit</em> for the activation function, which is similar to LSTM units.</li>
<li>Provides details and equations for the computation of the RNN state using gated hidden units.</li>
<li>Explains how update and reset gates are computed.</li>
<li>Mentions the use of a multi-layered function with a single hidden layer of maxout units for computing the output probability.</li>
</ul></li>
</ul></li>
<li>Alignment Model
<ul>
<li>Explains the use of a single-layer multilayer perceptron for the alignment model.</li>
<li>Provides an equation describing the model and notes that some values can be pre-computed.</li>
</ul></li>
<li>Detailed Description of the Model
<ul>
<li>Encoder
<ul>
<li>Provides the equations and architecture details of the bidirectional RNN encoder.</li>
</ul></li>
<li>Decoder
<ul>
<li>Provides the equations and architecture details of the decoder with the attention mechanism.</li>
</ul></li>
<li>Model Size
<ul>
<li>Specifies the sizes of hidden layers, word embeddings, and maxout hidden layer.</li>
</ul></li>
</ul></li>
</ul></li>
<li>Appendix B: Training Procedure
<ul>
<li>Parameter Initialization
<ul>
<li>Describes the initialization of various weight matrices, including the use of random orthogonal matrices and Gaussian distributions.</li>
</ul></li>
<li>Training
<ul>
<li>Explains that the training is done with stochastic gradient descent (SGD) with Adadelta to adapt the learning rate.</li>
<li>Describes how the L2-norm of the gradient was normalized and that minibatches of 80 sentences are used.</li>
<li>Mentions sorting the sentence pairs and splitting them into minibatches.</li>
<li>Presents a table of learning statistics and related information.</li>
</ul></li>
</ul></li>
<li>Appendix C: Translations of Long Sentences
<ul>
<li>Presents sample translations generated by RNNenc-50, RNNsearch-50, and Google Translate.</li>
<li>Compares these translations with a reference (gold-standard) translation for each long source sentence.</li>
</ul></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;requiring the length to be fixed seems a mistake as sentences can sometimes get very long think hundreds of words</p></div></div></section>
<section id="reflections" class="level2">
<h2 class="anchored" data-anchor-id="reflections">Reflections</h2>
<p>Naively, translation is an easy task in the sense that we just need to look up each phrase in a statistically generated bi-lingual lexicon and append the translation of the phrase to the target sentence. In reality even when working with parallel text is tricky in that, the translation isn’t one to one but rather is n-m mappings between the source and target words with the number of words being variable. Picking the best entry in the lexicon is not obvious as the source words may be ambiguous and need some attention to other words in the context to disambiguate, and these too may be ambiguous ad infinitum… Finaly the translation may require some reordering of the words in the source sentence to conform to the target language’s grammar.</p>
<p>To sum up:</p>
<ul>
<li>We need to learn a bi-lingual phrase lexicon.</li>
<li>Each phrase may have multiple translations.</li>
<li>To pick the best entry the source context should be consulted.</li>
<li>The target sentence may require reordering to conform to the target language’s grammar.</li>
<li>Another challenge is that the target language may require words or even grammatical constructs e.g.&nbsp;gender and gender agreement that are lacking in the source language. These are <a href="../../../reviews/paper/1997-floating-contraints-in-lexical-choice/index.html">floating constraints</a> c.f. (<span class="citation" data-cites="mckeown1997floating">McKeown, Elhadad, and Robin (1997)</span>) that the model needs to propagate through the translation process.</li>
</ul>
<p>Each step of this process is probabilistic and thus prone to mistakes. Though there are two main constructs. The first is the contextual lexicon which needs to be learned using parallel text. The second is the destination grammar that can be learned as part of a language model using monolingual text. However reordering texts isn’t as much of a challenge, if we have sufficient parallel texts then the NMT hidden states should be able to learn the reordering as part of its hidden state.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="slides" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-bahdanau2016neuralmachinetranslationjointly" class="csl-entry">
Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2016. <span>“Neural Machine Translation by Jointly Learning to Align and Translate.”</span> <a href="https://arxiv.org/abs/1409.0473">https://arxiv.org/abs/1409.0473</a>.
</div>
<div id="ref-mckeown1997floating" class="csl-entry">
McKeown, Kathleen, Michael Elhadad, and Jacques Robin. 1997. <span>“Floating Constraints in Lexical Choice.”</span>
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Neural {Machine} {Translation} by {Jointly} {Learning} to
    {Align} and {Translate}},
  date = {2024-02-11},
  url = {https://orenbochman.github.io/notes-nlp/reviews/paper/2014-NMT-by-jointly-learning-to-align-and-translate/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Neural Machine Translation by Jointly
Learning to Align and Translate.”</span> February 11, 2024. <a href="https://orenbochman.github.io/notes-nlp/reviews/paper/2014-NMT-by-jointly-learning-to-align-and-translate/">https://orenbochman.github.io/notes-nlp/reviews/paper/2014-NMT-by-jointly-learning-to-align-and-translate/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Bidirectional LSTM</category>
  <category>Deep learning</category>
  <category>Embeddings</category>
  <category>LLM</category>
  <category>NLP</category>
  <category>NMT</category>
  <category>Paper</category>
  <category>Review</category>
  <category>Podcast</category>
  <category>Translation task</category>
  <guid>https://orenbochman.github.io/notes-nlp/reviews/paper/2014-NMT-by-jointly-learning-to-align-and-translate/</guid>
  <pubDate>Sat, 10 Feb 2024 22:00:00 GMT</pubDate>
</item>
<item>
  <title>The LORELEI Project</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/cs11-737/cs11-737-w21-LORELEI/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/tiling.png" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>course banner</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/l3F4zmYGU9g" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Lesson Video
</figcaption>
</figure>
</div></div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Obj·ectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Rapidly developing a low-resource information extraction in new languages</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- Transcript here is machine generated no punctuation and I eddited it a but but it needs more work and I don't see the benefits unless I can run it through some tool. -->
<p>The thing i want to talk about today is a large darpa project that happened 2015-ish for four or five years um called lorelai and the reason i’d like to talk about it is it basically was a big project by darpa so lots of sites caring about low resource languages and building resources for low low resource languages it was mostly about information extraction at least they wanted information about streams of information and languages that we might not know and um it introduced two very interesting things within the nlp community about moving from having a problem and solving it to thinking about what the problem is going to be in advance and pre-solving it by building appropriate pre-training and and data okay um the fundamental idea darpa usually has Overview grand schemes on what they want to do they don’t always implement them in the right way and we don’t really solve the problem during the project but they usually cause us to take significant steps forward that longer term actually helps a lot and this seems to be a pattern that happens in dartmouth and darpa’s aware that locally they might not solve the problem but actually five years or more down the line may actually do and i think this is true for uh lorelei so it’s basically how to deal with the low language um resource quickly okay um so i’m going to talk about that i’m going to talk about how funding normally works for law resource languages and how do you get funding how do you do research in law resource languages because we know that english put on what are the economically most reasonable languages to work on on the planet there are lots of speakers of them it’s worthwhile improving that but how do we deal with the other languages like basque where there’s not enough people to be able to fund and research in it or not as many and how can we actually do anything so it’s a case study looking at lorelei project lorelei stood for something um and it was clearly a backer name in the sense that they came up with the name and then thought what it might stand for but i don’t think i know what it ever stood for and went from 2015 up until 2020. Government Investment in Languages um so language technologies you know as we’ve seen is there for the big languages but how do we deal with the other languages and how do we get funding how do we build models that are going to deal with bas can we collect all enough data to be able to build a better bask or what can we do from a multilingual point of view um there’s a class of languages that have significant numbers of speakers these speakers might not all be monolingual but some of them might be and they’re big enough number of speakers that there’s tv radio news in those languages and it’s probably you know the rankings of around about the 300 to a thousand um so it’s not village local languages they’re languages that still have major cities and the languages where people will use the language outside the home uh how can we actually deal with those languages rather than just restricting ourselves to the high um colonial languages that are there okay and much more importantly okay uh if these languages okay only have a few million speakers and enough of the rich people there speak another language why would you ever build technology for them because if they are educated in english or in um standard chinese or russian or spanish then why do we ever need to care about them from language technology point of view because amazon’s going to work for them when they’re talking in their other language um so we don’t have to do anything about them okay that’s of course unfair okay um and it’s not just that we should be looking at commercial use of these languages and we should be looking at you know helping people on the planet and you know from the researcher point of view is why is it that we only work on the languages that we have an infinite amount of data from rather than actually know how to deal with a small amount of data in order to be able to get support and it’s not we’re not saying that we should each work on each language it’s how can we build support for these languages um so we can deal with them reasonably okay and there are two reasons on the planet that after um economics that will cause people to research into language when i say research into language i mean doing nlp nowadays but historically i mean teaching people to actually speak the language and that’s wars and religion okay so if you want your language to be important what you should do is you could pick a fight with a big country and and then everybody will start wanting to learn your language so they can send spies to your country this has happened a lot okay the reason that russian was a big language during the cold war that people were trying to learn in the united states was because they wanted to understand what was happening in russia but oh my god all these people were using russian and they weren’t using english so we had to learn russian in order to be able to talk to them okay um and then there’s religion okay so the other reason that people will spend money on translation etc is to be able to get their message out quite simply they want to get their message out and it’s independent of economics so they’re not there to try and make money out of amazon they want to get their story to these people and hence the most common form of um translation information that’s there parallel translation information that’s actually out there on the planet is of course linux manuals no sorry i mean the bible okay um but um other groups of people who also believe in their work and are willing to work for free or nearly for free um will give you translations of the linux manuals and the quran and the bible without being paid and therefore these are available and these are the sorts of groups of documents that you’re likely to find and translations for all over the world okay so wars if you cause a war or religion people trying to get their story out are the things that are most likely to cause funding um and cause interest in actually doing things in uh in the world US Government LT Investment um the us government is uh excellent example of um a a large organization that’s willing to invest in language in spite of some people believing that the united states is a monolingual culture and it’s actually somewhat amazingly diverse people have come here for the last 500 years depending how you count um usually from other um cultures speaking other languages the nativeness of people usually dies off after a few generations um but the us is very much aware that in order to be able to interact with the rest of the world they have to be able to interact in those other people language languages and so they have invested in technology to be able to help language learning language translation and language and resources okay um darpa started investing in machine translation pretty early on and there was lots of research that was done in machine translation from as far back as 1940s um in the 1970s especially here at um cmu um speech recognition was being invested by um darpa with the idea of being able to do speech trans transcription and invest in dialogue systems in the 1990s speech translation in the 1990s darpa was one of the leaders in being able to fund and build systems and rapidly build systems for new languages without having to do years of development okay and so it’s not really unusual or strange that um darpa then says okay let’s do something directly targeted at low resource languages okay The Scenario so here’s the scenario and always in darpa there’s a sort of background scenario that they use to pitch to their bosses about why this is an interesting thing to do why is it interesting research project okay and so for example a disaster happens an earthquake okay and the area affected doesn’t speak any of the major languages okay so they’re not uh english-speaking area of india’s mostly an english-speaking area which can usually find english-speaking people or indonesia malaysia you can find english-speaking people so the primary language that’s in there is not one of the english spanish russian and putin hua and there’s communications in the local language that are coming out they’re coming from official sources like news and also coming from social media okay um and they’re also just um speech so tv and radio um how can we do something to track to actually work out what’s going on so if you’ve got resources and you’re here to help you know there’s an earthquake you’ve got resources that will have um a heavy equipment that can that can fix buildings that have collapsed or medical care or shelter and where do you take it to and all this stream of information is coming to you in the language you do not know and or there’s very very few interpreters around that will allow you to be able to get that information so what you’d like to be able to know is um who should you uh provide support for um where should it go um who’s affected how many people need help okay and what’s the urgency so there are messages but you don’t know what they mean how can we actually understand them so um Lorelei Incident we ran with that um a disaster communication in a local language and we wanted to provide machine translation in a language we don’t know in advance um named entity recognition so we want to know people places um and we wanted to extract what became called situation frames these were 11 specific types plus location status urgency and there was another term gravity that nobody understood what it meant and this was things like needing medical help needing infrastructure um has broken down needing food shelter and unrest um so the riots um just giving information in our translated form in english that would allow people to do um actionable help when they’re actually doing the resources what’s more they wanted to do as fast as possible okay so um a they ended up although it didn’t start like this they ended up saying you’ll be given 24 hours notice earthquake will happen you’re going to be tested 24 hours later um we’ll also have another deadline one week later and another deadline 30 days later okay so make it a little bit more realistic because you know the earthquake happens you go i’ll work on that language and three years later you end up with some support for that language yeah it’s too late okay what can you do in 24 hours 24 hours is very very short okay um you’re told about the language at our zero and i mean you don’t have to try and work out what the language is if you’re told the name of the language here’s the wikipedia page for the language okay we would get a paragraph that described the disaster in english and then everything else was in the local language okay we weren’t allowed to use post hoc information like the wikipedia article about the disaster because there’ll be a wikipedia um page about the article in in english we’re not allowed to use that we can only use things from before that point okay Lorelei Evaluation Exercises um so we started um in 2015 and everybody had cool ideas about how to be able to do this and we all started working on it and then came 2016 they said okay we’re going to start with the first language we’re going to start with the language that there will be speakers in every single team so it won’t be an unusual language at all so we did mandarin okay because all of the teams had mandarin speakers in them because all teams do have mandarin speakers in them okay and so that’s what we did first um and then uh we sort of tested it there was evaluation metrics where we’d have to submit the results and we would try to um evaluate how well we’d extracted the information from the documents um and then in july uh the first language that they gave us was uyghur uyghur has spoken western china and you’re probably aware of it in the news it’s a muslim group in western china um that is not treated well by the government there and it’s a turkic language it’s written in a latin uh in a arabic script um which makes it a little bit harder to deal with but unlike other languages that use the arabic script it writes all the vowels so it makes it much easier than urdu or um persian um and so that was the first one and again it was an earthquake that was the actual event that we were actually trying to um understand now i’ll go into some more detail about these later uh the following year we did to grinya and oromo they’re both spoken and spoken in southern ethiopia depending on your definition of where the ethiopia ethiopian border is um a degrena is written in the giz script which is the same as amharic aromo is written in latin script and the spelling’s almost random and so there’s a lot of noise in the spelling and that’s something that we had to deal with because there’s no standardization for our spell um in july 2018 we had two different languages from different continents we have ken rwandan spoken to central africa um and sinhala spoken in sri lanka and sinhala is not a dravidian language it’s not a southern indian language it’s a northern indian language um and actually bengali might be one of the closer languages to it and so we built up support for that in 2018 we did um albanian and actually it sort of moved away from being this fixed evaluation that would happen with them albanian what actually happened is there was an exercise in europe which they did in albanian and we actually provided support from people who were sending in pretend messages of important things that were happening and we were trying to extract the information from the albanian um tweets that were coming out in order to be able to do that 2019 we did um oria audio spoken in uh um india and ilikano was one of the languages spoken in the philippines um and then again in september um 2019 we again moved away from this fixed form of evaluation to actually deal with something of a bunch of real news stories with an exercise that was happening in papua new guinea is one of the standard languages in papua new guinea it’s a pigeon english and it is related to english and it has a number of native forms in it but it’s now a standard language even though originally it was a pigeon um and so it has news and um tv reports in it um who was involved in this well you Lorelei Performers know there’s a bunch of researchers in north america that are very often involved in these big language and speech-related projects and it was the same there was big teams that were formed across different groups and we shared things so um university of southern california isi um with uiuc and notre dame and joined as one team cmu was also working with uh uw uh melbourne as in australia and lido’s private company and bbn was with johns hopkins and university pennsylvania and there was other um components as well so other groups would develop things and we could use them and try to bring them into our system so um colombia university colombia not the country um was doing analysis to give us urgency and sentiment automatically from data and rather interestingly university of texas at el paso what they were trying to do they were trying to recognize situation frames or at least urgency information from speech from prosody alone without knowing what the words are so can you tell that this recording is urgent or not and you might think oh yeah if it’s got screams in it it’s probably urgent but no these are mostly news reports or youtube type reports and it’s how people are speaking but you could still get significant information and it would actually help on combinations CMU System: Ariel the cmu system was called aerial but nobody can remember that we had a fancy name for it and again it was an acronym but i can’t remember what it was um rather interestingly compared to the other teams we decided to begin with to instead of build a translator that translates everything to english and dual extraction in english but we decided we’ll work in the target language itself okay because we knew that translation was going to be bad we knew that translation we wouldn’t have enough data um for it and much more importantly graham hadn’t moved to cnu yet so we knew that it wasn’t going to be good until we got graham to move to us and we mostly worked in the phonetic domain and this was something that we do is we take whatever the written form is we project it into the written form and we display it in the native script we display it in the phonetic script so now anybody could read it or at least it was easier to read and sometimes we project it into a romanized script because it’s much easier for you if you don’t speak these languages to remember things when you see them in a latin script if you’re more familiar with it um um we then would use um a cross into cross language information for example um uyghur um actually to protect the roman script and youtube uzbek and projected into roman script it writes in cyrillic or maybe it’s kazakh the rights and cirilla can never remember which one it is but once you put them into the roman script they’re actually very similar there’s lots of cognates that are immediately obvious in the language because they’re shared they’re both turkic languages and they’re both in this central asian and we could get information out of that and people who don’t speak the language could see these um similarities but also our models could see the similarities while if they’re written in different scripts it can be very hard to actually see that we built base keyword systems we were allowed to have native informants um to be able to tell us something about the language um but we had limited access to them so maybe four hours of access and that was it and so what do you do if you’ve got four hours access to a bilingual speaker who’s not an nlp expert can’t program in python what’s the best most useful thing that you can get out of them in order to be able to do this and one of the things was like to translate these 50 words um into your language so you know the word for helicopter doctor and medicine portable water etc okay um we also built cross-lingual word embedding systems to be able to build on top of these and we and a number of uh um research groups and that were working on that we had um active learning so how best can we use this um human who knew the language and be able to get the best information out of them in a very limited time such that the system really could work well um we had to do all of this in speech as well so we did lots of work in cross-lingual recognition where we didn’t really have time to train a language at best we could recognize it and then on top of that tried to extract information what happened was you end up with lots and lots and lots of pre-preparation so we’d build um language independent models or very large multilingual models in order to be able to to do the 24-hour thing and basically we couldn’t sleep in the 24 hour and so darpa doesn’t believe in sleep so we’d have to find out when the best things were and so the people who were building models that had a four hour training period were felt to be lucky because that means they could sleep during that training period of course unless it fails because then they got woken up um Techniques performing in the pronunciation space was definitely something that was worthwhile we gained a lot from that we were beating the other teams to begin with what usually happens these the there’s competitive things and people use different things but by the fourth or fifth year we then sort of settle on um more standardized forms and and machine translation got a lot better um when we got uh later on um uh doing this um in the pronunciation space rather than doing words or morphemes we sometimes make decisions about whether we do morphological decomposition or not depends on the language and whether it’s worthwhile we also do interesting cross-lingual transfer and like unknown translation we’d get some key words that we’d have in the other language and we’d use that to try and predict what embeddings might be other forms the example that was always given with this imagine you know that china japan and korea are always going to appear or are likely to appear together in standard news stories and then you have three um chinese characters that you don’t know pretend you don’t know what they are um and uh you’re told that this one means china and you’re told that this one’s means japan then you have to guess what the one actually is um and i assume the third one is the chinese for korea but it might not somebody we have a chinese speaker is it is this yeah okay right for south korea yeah north korea’s got a different one okay i wasn’t sure what he is but yeah um because he’s a japanese um but then you could work this out okay and i’ve been in situations like this where i’ve seen something written in chinese okay and i could recognize it said america it said france it said britain and it said something else so i thought well it must be another country it must be another big country okay and i had to try and work out um what it might be and so i had three characters in it and it wasn’t any characters i recognized from japanese but you know if i could uh um read any of them the last one was the big character so then it’s a named engine duh okay and so it’s canada okay um because you know from the context it was something about cost of calling the countries or something um these were all very low languages resource i mean not ridiculously low but low they would have media in that language but it was hard to be able to find data okay um darpa would provide us with language packs which would often have all of the parallel data was available for that particular language which really was bible quran and unix manuals because they are the most common form that you would find on opus or somewhere else often wikipedia had some information but it was really very varied and rather interestingly not based on the number of speakers in the language it’s based on the interest of people doing it or something and we also had this native informant it was often called a taxi driver and this is partly as a a joke but partly because we’re very serious about that because the taxi drivers in a country are often the people who speak these other interesting languages because if you can’t speak the native language of the country you move to very well you can often still be a taxi driver and so it would allow us to be able to meet them and so we every time we were in the taxi we would ask so what languages do you speak and would you like to come in and do some work for us um the techniques that we actually cared about here global linguistic knowledge you know um we knew the name of the language so we made sure um that when it started we could look up the wikipedia page and make fixed decisions about the writing system the local shared languages that we have the influence of any educational or colonial language that was nearby that would be worthwhile caring about um linguistic aspects of the morphology the word order etc and should we do anything okay and the answer might be no we should do nothing but it might be it’s worthwhile spending an hour or two david martinson would do this um to write a morphological analyzer because a standard bpe might not get the information that we’d like it to get and so we would do that and then pre-process everything after that but that depends on the language and you have to read the wikipedia entry and make the decision about the amount of time that you take to be able to do that um so linguistic closeness and helps a lot okay but also colonial close closeness the languages in the indian subcontinent linguistically bear literal resemblance to english even though they’re both got a shared root in proto-indo-european and but the influence of english on indian languages and also elsewhere it’s pretty large okay you know um and so you know the word for doctor in hindi you know the person you go to to get medicine when you don’t feel very well is the word doctor okay yes there’s some sanskrit rooted word that also means it but that’s not what you say when i feel sick i have to go to a doctor it’s doctor is the word and so there’s that english influence is there and so you want to know about that and so you want to know while in indonesia it might be spanish and in south america when you’re looking at native indigenous languages is going to be spanish or portuguese is what the influence language is going to be so you want to be aware of that um you might discover things like all the numbers are turkic because it’s a turkic language and then all of a sudden you get translation of all these numbers and it works well you might also find that there’s other interesting things like the mercy the french for thank you is the arabic for thank you at least the casual thank you in um arabic you also get um pastor spoken in afghanistan and which was one of our languages is uh is index but it’s got lots of influence from persian in it because the people who speak it there are speaking with daddy speakers who have got a lot of uh and you might also find that there’s um some other influence you know petrol but we all know is what petrol might be called gas or of course benzene is the other international term for that um nothing is spelled consistently okay and this is something you learn very quickly when you look at this is like there is no standard spelling so you’ve got a low amount of data but you’ve got random spelling you’ve got noise in the spelling so almost everything that you think should work isn’t going to work because if you do a word divect type thing there’s going to be a lot of space in how people are going to spell things okay um dialects aren’t very well defined okay and although a written form might be closer to the standard there might not be a standard written form and therefore it’s shared and therefore you have lots of noise when that’s actually happening um the registers so how people talk to each other whether it’s a politeness level or whether it’s just some form of familiarity level that’s going on can be very very important social media is probably not going to use the same register that announcements from the government actually are and you have to it’s almost a different language also um people code mix all the time so they share their languages when they’re speaking and realistically if there is only a few million people who speak that language these people are going to be multilingual at least lots of them are going to be multilingual so you’re going to get multilingual information and so if you’re looking for tweets in the language you’ll find it will also often be mixed with english or spanish or what again the colonial languages because people are trying to get information out and therefore they’re likely to use a more international language because it’s a widely um used language and they might not be fluent in that language but they’ll know some words and so you find that you get that too um Lorelei Questions so there’s some question about this we worked for this for five years um people uh darpa who were doing the evaluation uh which shows these numbers after evaluation about how badly we were doing and um we actually said well wait a minute we’re actually not doing badly okay we’re giving you answers in a day and a week in a language we’ve never seen before and you’re saying we’re nowhere near as good as what humans are who spent 25 years learning the language we think that’s a little unfair okay and what we should really do is find out how it is if you don’t have a system like this and see how well it works we also noted that the humans weren’t very good at it either or at least weren’t very consistent about it and so we started investigating how well they could label the data and there was lots of noise on whether people were labeling the data and properly or not then the next thing that we decided to look as a bigger thing is how can we make an estimate about how well we will do in a language so if you select a language and give it to us say disaster is going to happen here um how well are you going to do it recognizing the information as opposed to this other place on the planet can we look at the language the language resources its relationships to other um other languages uh how how much of a literate language is it and make predictions about how well we’re actually going to do it so we did we did these things because they’re always quite interesting to do it all of the System vs Annotator Performance languages were named by number um in official documents so and we ended up not saying the names of the languages although i’ve often added the name but i don’t always do that because i forget okay and i only think it’s il8 but i can’t remember which one so instance language eight but i can’t remember which one it actually was um so um this first graph here is basically showing you how well which is how well we did these situation breaks how we do information extraction out of um the system when compared to humans doing it so we give humans the documents and say label the important frames that are in there with extracting the information and so for two of the early ones durgenia and oromo um the systems were competitive for the human integrinia and um better than the humans in rome although that wasn’t quite true there were certain things in the romo that made the labeling not as well defined and so actually we have some weird things that actually come out of the definitions um i i should actually well there’s a couple of little examples i wanted to say so one of the things that we wanted to do was identify named entities um including locations and be able to say if there was a need and so in the first mandarin um system that we built our named entity expert um akash who’s indian and came up with the most common place name that he found labeled it with the best confidence and he says this one seems to have issues and i think there’s something like it needs water because they don’t have any water there and this is the chinese character for moon okay um and we’re absolutely right moon is a place and moon doesn’t have much water on it so it probably needs that but it’s not going to be affected by the earthquake on on on the planet our planet anyway okay and realistically i believe this is actually the word month it’s not the word moon at all okay so because you don’t know the language you get things like this coming out of it okay because it is a place okay but no it isn’t okay and the other one that came out a lot was um there’s this agency that says that is going to provide support and going to provide relief but they never do it okay and something like about 15 of the messages seem to end in a message that says that okay and so we had a closer look and we had our native informant tell us and uh after he stopped laughing he explained to us yeah that’s a quote from the quran and it says allah will save us okay and allah is not actually one of the agencies involved in the relief effort at least not directly okay indirectly maybe but he’s not going to come along with a truck of um uh your water okay probably he might cause a drop of water to come back that’s a different issue okay but of course we couldn’t recognize that this was just a standard phrase god save us um that appears because we didn’t know another even though i think i mentioned in the named entity thing before that it’s been defined that gods are not named entities okay and so allah is not a named entity so you get these weird things that go wrong but you also get these weird things that go wrong because humans are doing these things and they’re not very sure okay and therefore really this one where we did better than the humans was because they weren’t well informed about sometimes gonna move on Experiments on English Core Data um so we also tried to do this thing about trying to guess how well we’re going to do in a language okay um and really it came down to if you’ve got a good bilingual lexicon to start off with you’re going to do a lot better okay and so if you had a bilingual lexicon in all pairs of languages that might be the best thing to invest in in order to be able to improve um these systems because you’ve got a very good start and then you could start learning from it um so which languages do we better in which languages do we do worsen well it sort of varies i mean our worst language here is which is spoken in nigeria and we just didn’t have much data for that even though it’s a relatively well resourced language and which languages do we do particularly well in tamil because there’s quite a lot of information around for that okay and there’s another one near the end hungarian but the european ones were always a little bit easier Lessons Learned and what did we learn from all of this um [Music] so build it pre-building models makes a big difference really makes a big difference and that actually i’d say all of our research moves from being how can we build models on on time zero from how can we build our models from beforehand and all we’re doing is adapting within you know the first half hour of actually getting the systems um uh working in the instant language actually made is a big difference especially at the very beginning so we really do well in the first 24 hours but once we got a translation system up in one and we could actually bootstrap a better translation system in a week we were doing better in translation than doing things on the um english translated form well the translated form but to begin with it was very hard to be able to compete because you had to get the translation system up and running and then have the extraction running so working in the target language actually made a difference but of course you could do combinations and that’s something you could do as well um the the trying to use the native environment in the most efficient way became really important and finding the best way to ask them questions so that their result could immediately feed into your system and actually improve it um graeme mentioned something about this and the other day in active learning about you know having some examples you know at the beginning really gives you a big boost okay um it maybe doesn’t matter after you’ve got lots of data i mean even a small amount of data but you know the first few hundred things really does make a difference because there’s massive holes in the data that you’re actually dealing with Let’s Try It in a Real Disaster so at the end of the project we decided this was a really cool idea and we wanted to test it out in a real form so we took graham okay and we put him in the cmu timeless time machine you all know about the time machine that we have at cmu and we sent them back to 2011 to try to apply some of these techniques um to the japanese um earthquake in 2011. unfortunately the um the time machine didn’t work and grain was already there in 2011 so he actually did this work before he came to cmu okay um which was very informative post hoc um about what was going on so there was a a major earthquake in japan in uh march um 2011 and um graham and others um were trying to work out how could they build a system that would actually aid the effort so this meant looking at nlp techniques on tweets and news to try to identify what’s going on particularly where things are happening where the words disaster and also trying to track people okay um because you want to know who’s safe who isn’t safe who’s looking and be ordered to be able to do that so the issues were word segmentation because in japanese that’s actually way harder and because there was a lot of local names it made even harder than what it would normally be named entity recognition and tweet classification okay um you really need pre-existing tools so this wasn’t in the language that they hadn’t worked in before so it was a language that the people in japan have been working on for years and therefore there’s relatively good uh resources for this okay passive people tracking is very hard so you’re just trying to find out whether this person’s mentioned or not while things like google’s and google and person finder which gets used in disasters there’s a human looking at the result going yeah that that’s my brother or no that’s not my brother um but the name’s very similar but not having a human look at it who’s actively trying to help actually makes it significantly harder um organizing helpers people go hey i’ll help so what are you going to do and how are you going to get them to be able to help they speak the language they can do things they can do labeling for you they can do checking of data how do you make sure that you use those efficiently and how do you do that in the first few hours when there isn’t an organizer and there’s no um uh infrastructure to be able to do that um a japanese word segmentation is not a three conclusions and they did successfully get build a useful system which actually did classify things and be able to tell you about where things were happening speed is everything gathering data labeling and creating a classifier being able to do this quickly as possible is really really important it saves lives and really and a much better an annotation framework would be helpful for what they were actually doing they probably didn’t use brac because it might not have been around at the time um and being able to use these human resources efficiently is something that you think about in your system in addition to having the latest uh um dark um a multi-lingual back system you should say well i’ve got 20 people here how can i use them to be able to improve my system Lorelei’s Legacy um legacy well we’ve got a whole bunch of data sets out of it um speech social media and news text for i can’t remember the number of languages but i think it’s 12 to 20 um the ldc and they’re quite good and we still use these today and you need to pre-train you can’t wait till the last minute that you’ve got a plan before multilingual should be hundreds of languages not three in fact if you look at the papers before 2015 lots of multilingual papers we’re talking about three languages well now we can talk about hundreds because that’s actually what we try to do zero shot q shot and active learning are really important when we’re actually doing these things integrating native informants into the task in an efficient way is part of the problem it’s not a sort of side thing it’s something that was really important and massively multilingual data sets came out of there that hadn’t really existed before massive bible alignments um unix manuals wikipedia and speech and the wilderness data set that we did which has got 700 languages in order to do that so i’d also like to point out one other thing which is probably this class also IR: Discussion Point um yes yeah one of one of the reasons why ellen and i were focusing so much on massively multilingual things was because you know we were working on this project and you know needed to scale educating people about caring about multiple languages and so this course is part of the results so here’s the discussion point consider a lot of life manager major earthquake happens in haiti um news reports tv radio broadcasts and social media and posts are coming from port-au-prince um you have access to the relief aid so you’ve got the aid okay um in terms of medical food shelter infrastructure rescue and you’re asked to use the messages that are coming into you uh to be able to distribute the resources in the best way to save people’s lives what do you do consider translation related languages native informants active learning and what current resources are available for you and as sort of as a footnote this actually happened in 2010 okay so in 2010 there was a major earthquake in haiti and people couldn’t understand haitian creole the languages spoken in haiti um and there weren’t any resources and the local people didn’t speak anything else some of them speak french and some speak english but how can we do it and so the nlp community came together and distributed um existing because there was an existing project at cmu in the 1990s and distributed data that was parallel data haitian creole um english and um microsoft i think was the one who actually built the translation system and put it on twitter so that you could get and translations as quickly as possible so this is the discussion point for today</p>
</div>
</div>
</div>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li><strong>Introduction to LORELEI</strong>
<ul>
<li>LORELEI was a DARPA project from 2015-2020 focused on low-resource languages and information extraction.</li>
<li>The project aimed to develop technologies for quickly understanding streams of information in languages with limited resources, addressing the challenge of providing aid and support in disaster scenarios where affected populations communicate in unfamiliar languages.</li>
<li>The goal was to move from solving problems to proactively building pre-training and data resources.</li>
</ul></li>
<li><strong>Motivations and Funding for Low-Resource Languages</strong>
<ul>
<li>The economic incentives to develop language technology are typically focused on languages with large numbers of speakers, such as English.</li>
<li>Funding for low-resource languages is often driven by factors other than economics, such as wars and religion.</li>
<li>Governments, like that of the U.S., invest in language technology to interact with the world, regardless of the economic viability.</li>
</ul></li>
<li><strong>Scenario and Objectives</strong>
<ul>
<li>The project was based around a disaster scenario, such as an earthquake in an area where the local language is not widely spoken.</li>
<li>The primary goal was to extract actionable information from communications in the local language to provide effective support.</li>
<li>Key tasks included machine translation, named entity recognition (people, places), and extraction of situation frames (types of needs, location, status, urgency).</li>
<li>The project had very short deadlines: initially 24 hours, then one week, and 30 days to provide support for a new language.</li>
</ul></li>
<li><strong>Project Execution and Languages</strong>
<ul>
<li>The project began with Mandarin Chinese as a test language because every team had Mandarin speakers.</li>
<li>Followed by languages like Uyghur, Tigrinya, Oromo, Kinyarwanda, Sinhala, Albanian, Oriya, and Tok Pisin.</li>
<li>The project evolved from fixed evaluations to real-world exercises, such as supporting disaster relief efforts in Albania and Papua New Guinea.</li>
</ul></li>
<li><strong>Team Collaboration and System Design</strong>
<ul>
<li>The project involved collaboration among multiple research teams across North America and Australia.</li>
<li>Teams shared resources and tools, such as urgency and sentiment analysis from Columbia University and speech-based urgency detection from the University of Texas at El Paso.</li>
<li>The CMU system, AERIAL, initially focused on working in the target language rather than translating to English due to the limitations of machine translation at the time.</li>
<li>The CMU team emphasized projecting written forms into phonetic and romanized scripts to improve readability and cross-lingual understanding.</li>
</ul></li>
<li><strong>Techniques and Approaches</strong>
<ul>
<li>Building base keyword systems with the help of native informants was crucial, even with limited access to them.</li>
<li>Cross-lingual word embeddings were used to leverage similarities between languages, especially when written in different scripts.</li>
<li>Active learning techniques were employed to efficiently use the expertise of bilingual speakers.</li>
<li>Pronunciation space was used to do cross-lingual recognition.</li>
<li>Pre-preparation and building language-independent or multilingual models were essential for meeting the 24-hour deadline.</li>
</ul></li>
<li><strong>Challenges and Findings</strong>
<ul>
<li>Low resource languages often lack standard spelling, have poorly defined dialects, and exhibit code-mixing, increasing the difficulty of language processing.</li>
<li>Evaluation showed that initial system performance was competitive with or better than human performance, especially in languages like Tigrinya and Oromo.</li>
</ul></li>
<li><strong>Key Outcomes and Lessons Learned</strong>
<ul>
<li>Pre-building models significantly improves performance, shifting research focus from building models on time zero to adapting pre-existing models.</li>
<li>Working in the target language can be more effective than relying on initial machine translation.</li>
<li>Efficiently integrating native informants into the task is crucial.</li>
<li>Massively multilingual datasets and pre-training resources are essential for rapid adaptation to new languages.</li>
</ul></li>
<li><strong>Legacy and Impact</strong>
<ul>
<li>The project produced valuable datasets, including speech, social media, and news text in multiple languages, which are still used today.</li>
<li>The focus on massively multilingual approaches has influenced research and education in NLP, as seen in the development of multilingual courses.</li>
</ul></li>
<li><strong>Discussion Point: Applying LORELEI Principles to the Haiti Earthquake (2010)</strong>
<ul>
<li>Consider how the principles and techniques developed in LORELEI could be applied to a real-world disaster scenario like the 2010 Haiti earthquake, where a lack of resources for Haitian Creole hindered relief efforts.</li>
<li>Discuss the roles of translation, related languages, native informants, and active learning in such a situation.</li>
</ul></li>
</ul>
</section>
<section id="papers" class="level2">
<h2 class="anchored" data-anchor-id="papers">Papers</h2>
<p>The provided transcript focuses on the LORELEI project’s goals, execution, and outcomes rather than specific papers. However, it does mention some general areas and outcomes that could lead to research papers:</p>
<ul>
<li><strong>Techniques for Low-Resource Languages:</strong> The project explored various techniques, such as cross-lingual word embeddings and active learning, which could be detailed in research papers.</li>
<li><strong>System Design:</strong> The CMU system, AERIAL, and its focus on phonetic and romanized scripts could be the subject of a paper.</li>
<li><strong>Integration of Native Informants:</strong> Methods for efficiently integrating native informants, particularly in the initial stages of a project, could be described in a paper.</li>
<li><strong>Pre-training and Adaptation:</strong> The benefits of pre-building models and adapting them to new languages could be explored in a paper.</li>
<li><strong>Massively Multilingual Data Sets:</strong> The creation and use of massively multilingual data sets, including bible alignments, Unix manuals, Wikipedia data, and the Speech and the Wilderness data set, could be documented in a paper.</li>
<li><strong>Analysis of performance across languages</strong>: Analysis of the factors that contribute to better or worse performance in different low-resource languages, such as the availability of bilingual lexicons.</li>
<li><strong>Word Segmentation, NER, and Tweet Classification</strong>: Research focusing on these areas for specific languages with limited resources available.</li>
</ul>
<p>The teacher also suggests that many multilingual papers before 2015 involved only 3 languages, but research coming out of the LORELEI project aimed to scale to hundreds of languages. This shift could be a theme explored in papers related to the project.</p>
<p>It is also noted in the discussion that the data sets from the project, including speech, social media, and news text for multiple languages, are available via the LDC (Linguistic Data Consortium) and are still in use.</p>
</section>
<section id="reflection" class="level2">
<h2 class="anchored" data-anchor-id="reflection">Reflection</h2>
<p>I was not particularly impressed with this DARPA Project. The low resource languages listed seem to be mostly drawn from the biggest languages in the world after English. Also the material seems to be release into the LDC, perhaps the consortium with the highest paywall in the world. Also the material collected seems in many cases to be religious texts or just scraping news sites and social media so I was thinking that the project was not very useful.</p>
<p>However, professor Alan Black makes a good point. Research looked quite different before and after this project. So even if LORELEI like many Darpa initiatives is not very useful, it ended up shaking up the field a bit and people ended up doing better work after it!</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {The {LORELEI} {Project}},
  date = {2022-04-12},
  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737/cs11-737-w21-LORELEI/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“The LORELEI Project.”</span> April 12, 2022.
<a href="https://orenbochman.github.io/notes-nlp/notes/cs11-737/cs11-737-w21-LORELEI/">https://orenbochman.github.io/notes-nlp/notes/cs11-737/cs11-737-w21-LORELEI/</a>.
</div></div></section></div> ]]></description>
  <category>Active learning</category>
  <category>Multilingual NLP</category>
  <category>NLP</category>
  <category>Notes</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/cs11-737/cs11-737-w21-LORELEI/</guid>
  <pubDate>Mon, 11 Apr 2022 21:00:00 GMT</pubDate>
</item>
<item>
  <title>Active Learning</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/cs11-737/cs11-737-w20-active-learning/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/tiling.png" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>course banner</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/ed0aSU2mYZI" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Lesson Video
</figcaption>
</figure>
</div><div id="sup-slide-deck1" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div><div id="sup-slide-deck2" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides2.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides2.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;2
</figcaption>
</figure>
</div></div>


<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Obj·ectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Syntax, Major Word Order</li>
<li>Dependency Parsing and Models</li>
<li>Explanation and demo of AutoLex, a system for linguistic discovery</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- Transcript here is machine generated no punctuation and I eddited it a but but it needs more work and I don't see the benefits unless I can run it through some tool. -->
<blockquote class="blockquote">
<p>This week I’m going to be talking about active learning and this is a follow-up to the data creation stuff that ellen talked about last time this is a very good tool uh because basically the thing that we’re kind of perennially uh faced with when we’re working with low resource languages is the lack of data and like ellen said having a little data is always better than having no data and but at the same time we really like to get it as efficiently as possible and active learning is a tool that allows us to do so so if we look at the different types of learning we have supervised learning where we basically have uh learning from input output pairs uh x and y we also have unsupervised learning where we learn uh from inputs only x uh or i guess you could also consider learning from outputs only like a like a language model or something like this semi-supervised learning we learn from both so that’s that’s great as well where active learning is querying a human annotator to efficiently generate examples usually generate xy examples from just uh inputs x and uh just to give an example if we’re talking about machine translation we might want to be generating translated pairs between a low resource language in english given only examples in the low resource language or only examples in english but we don’t want to be just randomly generating sentences we want to be re generating sentences that are most useful to create the translation system that we want so the way an active learning pipeline works basically we have labeled data um usually we start out with a little bit of labeled data like this this might be a very small amount of label data not enough to make a good system but enough to make a system that does you know something and based on this we do training we get our model and then we have a huge amount of unlabeled data we run this uh prediction system over all of this unlabeled data we do data selection of some variety we identify a set of examples that we would like to be annotating x prime we do annotation according to annotators and we generate more training data but we generate training data particularly of the variety that is uh useful uh most useful for creating the model and um the question about why we do active learning i can kind of be illustrated from this slide this is a very very simple uh two-dimensional classification problem uh where we have positive examples and negative examples uh and we would like to train a classifier that does well at separating the two um if we look at the example on the left the example on the left is our ideal situation of a high resource task where we have lots and lots of labeled data and uh based on this lots and lots of labeled data we train a classifier and the classifier achieves very high accuracy uh because it’s able to draw an appropriate decision boundary between the two however on the right side we have two examples the first example we randomly selected six data points and uh six data points we selected we then train a classifier and you can see the classifier is not a very good one it makes mistakes on some of the examples that we didn’t label which are shown in lighter in lighter colors however the one on the bottom right we selected particularly useful examples we only selected five so actually the amount of data that we labeled is smaller but nonetheless we’re able to draw a very good decision boundary here and uh and get you know 100 accuracy on the examples that we have now uh why were we able to do this can anyone look at these examples and tell me what the difference is between the ones on the top and the ones on the bottom any ideas the ones on the bottom are chosen near the boundary lines yes so that that’s a good example um maybe i could uh maybe i could do another okay this is not a particularly good um this is not a particularly good example uh to show this but let me show another another example where just choosing based on the boundary lines might not be sufficient so let’s say we have something like this so if we have an example if we have an example like this is uh is choosing on the boundary line sufficient maybe not necessarily right because we have this this portion over here where we have a whole bunch of things that are really close to each other but if we just drew it based on these examples here we might have another portion of the space over here where it’s not it’s not sufficient so um in addition to building things uh in addition to finding things that are on the boundary lines we’d also like to have a good idea of kind of what the shape looks like uh overall uh so these are two principles important principles in active learning trying to find things that are kind of hard to handle and trying to find things that give you a good idea of the shape of the data distribution overall so i will um i will talk about this uh going forward so these are the two fundamental ideas the first one corresponds to being close to the boundary lines so um uncertainty we want uh data that are hard for our current models to handle and another idea is representativeness we want data that are similar to the data um actually sorry that the what’s written on the slide is a little bit vague but basically we want data that are good representatives of the entire data distribution that we want to be handling um so i’d like to start out with the simple example of classification problems and within classification problems there’s a couple common paradigms for how we characterize the uncertainty of a classifier or something like this um so the first one is uncertainty sampling um the name is uh the name is kind of repetitive i guess corresponding to my uncertainty criterion up here but uncertainty sampling is basically finding samples where a particular model is least certain about its predictions there’s also something called query by committee which is also commonly used where query by committee basically uses different classification models and measures the agreement between them it’s kind of like the ensembling version of of uncertainty sampling and um then i kind of defined uncertainty in a vague way initially but there’s actually a lot of uh more rigorous mathematical criteria that allow you to do this um and here we’re talking about let’s say we’re talking about text classification or image classification or whatever else and in this case the first example that we can use is entropy and um because we’re talking about a classification task we have a small number of labels potential labels y and we can basically enumerate over all of them so we can directly calculate the sum uh p e of y given x uh times log of p of y given x so this is the kind of standard definition of entropy and the higher the entropy the more uncertain the model is about its predictions essentially another uh uncertainty sampling criterion that you can use is top one confidence and this is basically the log probability or the probability of the highest scoring output and basically you know even if you’re you’re not super certain about the distribution as a whole if you’re pretty certain that there’s a top one candidate that has like 75 accuracy and your model is reasonably well um calibrated so if you say 75 accuracy and actually the model gives you and actually then you get it right 75 percent of the time uh then this is also a pretty good criterion as well and then finally uh there’s a margin based criterion where basically you measure the difference between the top scoring value and the second to top scoring value and try to decide um try to find the ones where the difference between the top scoring value and the second to top scoring value are larger all of these are reasonable um different people have compared them and found different uh levels of utility i have done a lot of work on active learning and personally i like the third criteria and the best uh it seems to work most consistently well however it’s not probabilistic uh it’s not a directly probabilistic criterion so if you want to do kind of a more probabilistic view of active learning than some of the other ones like entropy might be better are there any questions about this year so you’re taking the arc max to find the top one scoring value first to find uh why that’s a good question but where y hat is basically the top scoring value and then we’re uh characterizing the confidence is top top one so the lower this value is uh the more uncertain you are for entropy the higher the higher the value is the more uncertain you are for margin the smaller the value is more uncertain you are okay um so there is also query by committee and basically [Music] does that mean that um if we want to do for instance not active learning but semi-supervised learning and we take those criteria in the contrary we will get rid of the like uh more uncertain values but then we say uh that all the values that are very certain are not useful to train the model yeah so semi-supervised learning is a tricky thing um for precisely this uh this reason and i think in particular you’re talking about self training um where self training basically what you do is you make predictions and then you use the predictions to train the model um in that case uh taking the more certain predictions is will give you cleaner data but it will also give you less useful data because the model’s already good at predicting them right so i think there’s essentially a sweet spot where it’s like you want to increase your threshold enough to get harder examples but not so much that the noise overwhelms the hard examples um the reason why the reason why self training works at all is because the model the models can essentially use some things to make predictions but then when you um when you update the parameters it’s also updating other other parameters which were not exactly the parameters that were used to make the predictions so um you could also think of you could also think of a self-training method where you i don’t know if actually anybody’s done this but you can think of a self-training method where you make confidence confident predictions but then you remove the evidence that was used to make the confidence predictions and force the model to like use other evidence that was not the like easy to parse evidence or something like that so um i think that but basically the the problem with self-training is that you’re um like you’re updating if you update the parameters which are the same ones that you use to make the decision then you’re going to always have this this issue any any other questions okay um so i’ll move on to query by committee um query by committee is also maybe a similar uh idea which is uh that you run multiple models and you measure the disagreement between the models um and the so for example you might train three or five models maybe they’re models with different architectures they’re models with different views of the data or something like this and the places where they disagree are the places where you’re less confident and you would thus like to label them to improve your accuracy this is similar to the uh to what i was talking about with the self-training question because uh they’re you know if you’re using here if you’re using different varieties of information to make the predictions then um essentially if one model is better at capturing certain traits than the other it might be it might be right the other might be wrong um and that gives you a good idea of the like confidence the models essentially um yeah and uh this can also be combined with other standard ensembling methods i i didn’t talk about those uh in this class so i’ll just skip over that point um so now we have uncertainty methods uh whether you get the uncertainty from kind of a probabilistic method or a margin based method or query by committee so that handles uh identifying the examples that are hard for the model um the second uh core idea in active learning is representativeness and there’s a reason why this is really important um and the reason why is because if you don’t have any idea of representativeness um basically active learning will just go and find all of the outliers in your training set and ask you to go and label them uh ask a human annotator to go in and label them and this is a problem because like i’ve actually done active learning on text classification and other stuff like this and you’re trying to classify between uh hockey and baseball for example hockey based social media posts and baseball based social media posts and it gives you like dollar sign dollar sign dollar sign dollar sign uh is the thing that you’re supposed to be labeling and obviously that’s not going to be very useful because it’s an outlier it’s not really belong to either of the classes it’s just garbage so um in order to overcome this we want to find examples that are representative of our whole training set while being uncertain and then the question is how can we classify examples as being similar to many other examples and in simple feature vectors the easy way to do this is to find vectors that are similar to others in vector space and this is an example from a very famous uh early paper on active learning for neural network based image classification models where basically they um they took the vectors from a pre-trained uh image classifier and clustered them together uh and this this is just a visualization in two-dimensional space probably using like disney or another visualization method but what you can see is if you sorry this is red and green which are the two colors that you’re not supposed to use if people are uh colorblind but um like the uh basically what you can see is the uh here the red is the data points that are sampled by an active learning model and you can see that an uncertainty based active learning model basically samples a lot of points from some part of the space but very few points from other parts of the space whereas if you consider representativeness and also sample from places that are similar to other vectors in the space you make sure that you get um you get them spread out all over the place um so this is a very simple way to do this in um nlp or something like this we have lots of methods to turn sentences into vectors for example like we have sentence for we have uh some cse or whatever else so these methods here are applicable uh to some extent uh to like representing sentences or maybe maybe words for example so this would be a first uh way to think about representatives um there are some other ways though uh that are applicable specifically to text and i’m going to be talking about those next um any questions about this okay so um if we think about prediction paradigms for language tasks uh there’s lots of different ways we can do it um one is text classification uh you know we take in a sentence uh we output a label another one is something like tagging where we take in a sentence and output uh one label for each uh token in the input sentence and another one is sequence to sequence uh which is you know empty or speech recognition or whatever else and each of these requires a different uh strategy for active learning um there’s some overlap but they can use different strategies so if we look at sequence or token level prediction problems um basically for sequence labeling in sequence to sequence we can do annotation at different levels so sequence labeling is what i call tagging here a sequence sequence so one way we can do it is we can do labeling at the sequence level so we can annotate the whole sequence and the other way we can do it is at the token level so we can annotate only a single ambiguous thing within the sequence and how many people here have done any sort of annotation for structured prediction tasks for nlp like um but let’s say okay shinji raising his hand um okay uh and uh ner labeling would be an example an er labeling is actually an interesting example because um it’s very sparse annotation but let’s say you want to do labeling for part of speech tagging or a translation a translation i would also say counts as this it takes a long time to label a sentence uh accurately so translating a whole sentence um for me to translate a whole news sentence from japanese english probably takes two or three minutes however translating a single word from japanese to english probably takes me about 10 seconds so given that labeling a whole sequence you know does take a lot longer than individual tokens within a sequence yeah so given you use this in a practical scenario need to go through the same sentence if the same example is picked up for the token level option yeah so that’s a good that’s a good uh question so in a practical scenario wouldn’t a annotator have to go through the same sentence again and again um if the same sentence is picked up for multiple labels the answer is um yes and no uh in most active learning scenarios you often assume you have far more input sentences than you’re able to label in the first place um i’m also going to talk about another method that considers this a bit later so i’ll get back to that um any any other uh questions or there are also some efforts on this unaligned labeling where you just give uh sequence level tags for even token level information and use the pointer networks to map them so oh sequence level tags for token level information you mean like basically you use the sequence sequence model to solve this problem or yeah mainly for cases like uh tax lot value cases like slot filling in er cases when it’s very sparse so you’re just giving information at the sequence level and you expect the network to learn the yeah um i i think that is uh that’s reasonable i think i still would say that’s something like token level annotation here uh in terms of the human effort required to do the annotation um cool so a token level makes it possible to just annotate difficult parts of the sentence which potentially could give you some time savings but it requires strategies to learn from individual uh like parts of the sentence um so first i’ll talk about sequence level uncertainty measures um so when we unlike text classification we can’t just take the entropy of a single label over uh the entire text um and somehow we need to combine a sequence of structured predictions into a single uncertainty measure to get which sequences you want to be labeling um so in terms of top one confidence this is a this is trivial to apply to sequence labeling tasks and the reason why the reason why is because even if we have a structured prediction task um we can still take the arc max of the output sequence using beam search or some other thing like this and calculate the log likelihood of this output so um in terms of top line confidence still the this log probability of the highest scoring output would be our top one um so this is not hard this is not hard to apply here um margin we can also do something similar so you can take the top one output the second output you can do a beam search with uh you know a beam size of at least two and get the top two scoring outputs and find the difference between them also you can do sequence level entropy this is non-trivial to enumerate all of the sequences obviously because you can’t enumerate all of the possible translations for an output but you can enumerate the 10 that like the top and best and calculate your entropy over that as well and that will give you a better idea if you have uh like a non you know uh a lot of peakiness in the distribution um that you’re outputting or uh or not very much um and also um i’d like to note that this reference down here has a very clear exposition of this uh it’s a paper a paper from a long time ago in 2008 but it’s very clearly described and you can read more details there um are there any questions for the time being though okay okay so now yeah sure so um basically what it would look like is um uh it would be it would look like this just like the entropy over labels but where y is a sequence and now you can’t enumerate you can’t enumerate over every possible sequence but you can get an invest from beam search or something and then enumerate over there um uh in terms of sequence level uncertainty measures i actually have worked more on token level active learning so i’m not as familiar with like which one works uh works the best overall but my impression is that they’re pretty they’re pretty similar but um like i’ve seen more people use top one confidence maybe just because it’s a lot easier to it’s like simpler conceptually cool um so now moving to training on the token level um the way this works is um uh we would like to train on partial data so basically we want to train on data that’s been annotated for some part but not the whole thing um for uh if we’re using an unstructured predictor for something like sequence labeling this is easy it’s uh it’s basically trivial to do so like let’s say we are taking in bert and based on the representations we are doing independent predictions of part of speech tags for every word in the sequence what we do is we basically calculate the loss function over this verb annotation here and we don’t calculate the loss function over the others so it’s it’s very very simple uh almost trivial to do so that’s good news um for other more complicated things um we didn’t really talk about uh conditional random fields or other things like this very much in this class here um but basically for any uh any method that we use to do analysis that requires some sort of a dynamic program over the output uh there are ways where you can do the dynamic program over only the uncertain parts of the output and uh and calculate the log likelihood uh based on that so that’s a possibility too uh if this sounds interesting to you you can read uh this paper uh training conditional random fields from incomplete uh annotations so um actually i think i didn’t mention this in the slides um but i think it’s an important thing so i think a lot of people are also going to be interested in things like machine translation and machine translation it used to be that you would be able to do things like annotate a part of the sentence and use only that part of the sentence to extract a phrase table uh in phrase-based machine translation so in france-based machine translation people uh basically if you remember from the unsupervised translation class i talked about it there you would extract a phrase table that showed you how to translate individual phrases and then you’d reorder them using a language model and other things like that there it was relatively simple to uh learn from like individual annotations because you would just throw them into your phrase table and use that as additional phrases however for neural mt now it’s much more difficult because we’re learning over full sequences and we actually have a paper that we just published last year called phrase level active learning for neural machine translation i’m very creative title obviously um that shows you how you can do things like this and basically we tried a whole bunch of different things like one thing is you can just take the labeled partial phrases for machine translation and put them directly into your training data another thing you can do is you can like swap them into sentences and do data augmentation and other things like that so if you want to find the details we have a number of uh methods that we talk about uh that we talk about in this paper as well so um this is just an empirical paper where we try a lot of a lot of things so if you’re interested in mt you can take a look there too cool um so any any questions about that okay great so then the next question is token level representativeness metrics um and actually um one of the issues with active learning especially using pre-trained representations is if you have a um a poor representation learning model because you don’t have enough data to train it on like for example a low resource language the representations that you use to calculate your representativeness metrics might not be that great in the first place so one thing that we’ve actually found is just considering like token similarity uh is actually a pretty sorry token identity is a pretty reasonable proxy with respect to um like representativeness in the early stages of active learning so for example you could accumulate the uncertainty over all instances of a particular token and use that as how necessary it is to annotate that token so just to give an example um when you’re accumulating when you’re accumulating your uncertainty like let’s say your entropy over x you might accumulate the entropy over all um all examples where x is uh corresponds to a token run and or uh in this example x corresponds to a token like so what that would say is in part of speech tagging i’m not very certain about the part of speech for the word like so i want you to annotate more examples of uh of the word like basically and um that’s particularly good for things like part of speech tagging or machine translation or something like that where you might have some words that are really easy to guess the part of speech of um or the translation of and some words that are are quite difficult because they’re polish on this or something like this so this would be um entropy um but it could be to make it more general i could make this uh uncertainty you know you have some sort of uncertainty metric and you accumulate that uncertainty metric over all words where the word identity is it could be the entropy it could be the top one um it could be the top one probability it could be the margin or something like that yeah and uh then after you do that you need to select a representative instance of that token um there’s a number of different ways you could do this you could pick the example that’s most uncertain out of all the examples of that token other things that we tried to do for part of speech taking for example is try to choose examples where one part of speech tag was predicted or the other and the idea being that that would give you examples of like like uh that would give you examples of um run as a noun and run is a verb because run can be both a noun and a verb uh especially if you live in pennsylvania where every small river is called a run um cool uh so the next thing is sequence sequence uncertainty metrics um uh there’s a bunch of different things both on the sequence level and on the token level um an example of a bet a sequence level is back translation likelihood and uh one uh the way you would calculate this is you might get your one best uh translation on the output side and then you would calculate the probability of the input given that one best translation so it’s basically uh you use your mt system to generate uh the output and then you calculate the score of the input given that and let’s say you do a really really bad translation if you do a really bad translation you’ll have a lot of trouble recovering the original sentence and because of that your back translation likelihood will be low so you can go in um another example which isn’t even based on the model but is actually very very effective um and also very easy to apply i’ve used it a lot is finding the most frequent uncovered phrases in your training corpus so this is based solely on um representativeness and on a very like weak uh notion of uncertainty but the basic idea is um you you count up the frequency of all of the phrases in a big monolingual corpus that you don’t have translated and then you also calculate the frequency of all of the um of all of the phrases in your translated corpus and you find the highest scoring um the highest scoring phrase x-bar in your monolingual corpus um where the frequency of x-bar in your bilingual purpose is equal to zero or you know below a threshold or something so the idea being that you’ve never seen it in your translated data but you see it a lot in your monolingual data and that will pick up things like names of famous politicians you know um brand names uh phrases from social media other things like this um and then you can take these you can add them to your data and your model will be able to uh translate them and this is an example of uh how i present these or how this is not my paper but this is how um they presented them and how i presented them when i i was doing active learning for translation uh basically you uh take the phrase and you present it in context so you highlight the uh highlight the phrases yellow and then you ask them to translate only that phrase but they get to see the surrounding context so they have an idea of like what the phrase means and if it’s a very strange phrase like you know you get half of uh half of a grammatical constituent or something like that it will um uh they’ll also be able to say i can’t translate this on its own because i don’t have the appropriate context highlighted or something cool um any questions about this part yeah for the back translation yeah you basically need two models for this um you need a for model and a backward model or you can train a single model that can go in both directions but um cool so cross-lingual learning plus active learning um so cross-lingual learning plus active learning i think is a really powerful combination because one of the big issues with active learning is when you don’t have a good model in the first place but cross-lingual learning gets you a long way uh there like our zero shot methods you know are are really quite good uh out of the box but they’re not anywhere near as good as supervised methods so we take a zero shot method and we adapt it using active learning and um this is an example from a paper by aditi who just uh presented two times ago um where basically we we showed that the combination of token level active learning and cross-lingual transfer is just like much better than any of the two in isolation um and you can start out with like the green line here is just active learning um the blue line here is active learning blue orange and some of the other lines are active learning plus cross-lingual transfer and you can see that like right at the very beginning uh cross-lingual transfer helps you start out at a at a good space and this is for named entity recognition cool um so one other thing that i’d like to talk about is active learning is very attractive you know it’s like um uh for a very small number small amount of the data annotation you get a much better system um but one thing that you should remember is that uh you know when people are actually doing annotation uh things are not quite as simple as it may look in like a simulation so for example in simulation it’s common to assess active learning based on the number of words or sentences annotated any ideas why this is uh maybe a little bit unrealistic let’s start with sentences so um you in uh and actually one thing i should mention is when we do active learning it’s very common to draw these graphs which is like amount of annotated data on the x-axis and the accuracy on the y-axis so let’s say this x-axis was based on the number of annotated sentences from your purpose for machine translation or something like that yeah exactly so different sentences might be different lengths so let’s say your active learning system decides to pick all of the 400 word sentences in your corpus to annotate of course it will do very well because you’re getting you know the average sentence is 20 so if it picked the 400 length sentences then you’d be getting 20 times the training data for the same like quote-unquote price um what about words does that seem realistic better maybe but still you know some words are harder to translate than others like i can blow through a lot of translation of just kind of uh you know standard let’s say conversational text um but if i start doing news i need to look up what uh the vice kernel of the third battalion of the ukrainian army is or how to transliterate all the russian and ukrainian names that i’m seeing in the news now um into japanese uh which is not easy right so uh those words will take me a lot more time than um like uh than something else and especially if it’s the case where i’m uh not uh you know an expert in the field uh one interesting thing though is a lot of translators do charge you by the word so uh until they figure out uh that you’re giving them really hard sentences it probably actually won’t the cost will be the same across words so you know you might burn a lot of good will with your translator if you give them like adversarial essentially adversarial examples uh but um like at least uh some translation services it will be you know a fixed price so in that case it’s actually reasonable to have words on your accent um another thing is there’s a higher chance of human error for these hard things so um you know like even if the price is the same they might make more mistakes yeah so this is an interesting paper from your group last year about the models pay the right attention and you had human annotators as well annotate what parts of the corpus they are looking at attention and modern attention so how much time did that take for them to as well okay yeah so there was a question we um basically we had a paper last year where we annotated not only or where we basically asked people to choose between two translations and annotate their um annotate like their rationale for doing that um they didn’t actually translate in that case that case they were just choosing between two words um so that’s a much easier task like um it’s a it’s basically a word level disambiguation task instead of a translation task um so it’s a little bit hard to say how much longer it would take to annotate rationales but i think um another really important thing that i don’t know if i mentioned it here is uh ui is extremely important uh for annotation tasks um ellen pointed that out a little bit when we were talking about brat and other things and i think if you have the appropriate ui and you give like people are working with the appropriate kind of uh i don’t know directions or interface it might actually not take that much longer to annotate a rationale um another thing is uh one one interesting thing is um actually for a lot of people when they’re browsing the web where they move their mouse corresponds to where they’re looking at so you might actually even be able to get these kind of things for free by watching where their mouse moves um another thing that people have done is they’ve strapped translators into eye trackers and asked them to do eye tracking while they were translating to see where they were paying attention to in things like this um the problem is uh that won’t be super easy to do um you know not not for every translator maybe for some uh portion of particularly like nice translators or maybe maybe if you could get like one of the recent vr headsets like the oculus rift or something like that uh you could do uh something so i think um so the answer is it’s complicated uh but with some creativity you might be able to get like extra annotations like rationales or other things like that with like less effort than you would if you had to do it like um uh in a different way so yeah i think that’s like a very great idea for like a very long document translation so so they are translating also saying which part of the document they’re focusing on have like a generalized ui framework for them to translate and also like highlight portions of what they’re looking at i think this could be a generalized framework for uh sequence yeah that’s a that’s a really good point um so i don’t immediately have the references here and it would take me a little time to look them up but there was somebody in holland or somewhere else that did like pretty extensive eye tracking experiments with uh translators so i think uh this is definitely an interesting topic and if you could use that to supervise attention or other things like that that’d be pretty cool like this was a single word and if you were to translate the whole sentence you would actually increase your human translation time so you have to be careful because if every word is like you have to look at the document and annotate particular rationale there’s like a single word you would have to pay a lot of money for other players so this case it worked because it was a single word but it’s unclear if this would be feasible if you were to do the whole translation just this is the point yeah but it so another another thing to note is you can actually do a pretty good job of uncovering rationales through alignment but it would be kind of cool to see if there was a way you could get like natural annotations to um to kind of like improve alignment or something else like that through eye tracking or whatever else and one thing i didn’t cover at all here in active learning is there’s the concept of like natural annotation or incidental annotation or things like this which is basically annotations that you get for free uh due to some other features so another example of that might be um websites where the websites are translated and then you have links where the link definitely goes around the words that mean the same thing and the two websites and you can use that to like extract alignments and other things like that so um that’s a very interesting thing that i’m not covering in this class but yeah good ideas okay so um because uh the cost might not be the same in active learning uh there are methods to consider cost in active learning um so uh there’s uh an older concept called proactive learning uh that basically considers different oracles that cost different amounts for each um and uh so for example um that might be something like hiring a professional translator versus hiring a crowd translator where we know the professional translator would do a better job but would cost more um another thing is cost sensitive annotation and um this is a uh this is an example of this uh this was a paper that i i did together with um matthias berber who’s a phd student i was working with at the time and basically this is for speech recognition transcription and the idea is that you want to create better transcriptions and you have different modalities with which you can um with which you can annotate you can either re-speak so re-speaking is basically saying the thing again um in a clearer voice where the like asr system is more likely to get it right you can also skip so like not annotate it or you can type and typing takes the most time but it’s the most the most certain and um so then uh you create a model of how much each one is going to cost and this model can be updated on the fly as you annotate more data and then you also create a model of how much this will improve your results which can also be updated on the fly where you have the data and then you take the two models you predict how much annotating each span would help and then you run a uh like optimization algorithm to find the set of things that would uh do the best and to get back to the question that we had before what if you had multiple tokens in the same sentence that uh required you to go like from one sentence go to a different sentence come back this model explicitly takes into account the cost of context switching so a span that’s contiguous will cost less to annotate than annotating each of the individual words in uh together so this is another way you can model things like this another thing to think about is reusability of active learning annotations so when is the best study to start doing active learning annotations is it after every epoch or um so i think that’s a good question and i think there’s a number of things that go into this i think um theoretically there’s some theory on active learning and theoretically the best thing to do is to annotate basically like recalculate every time so retrain the model every time you get a training example an update uh the problem with this is uh two-fold number one computational so it’s hard to update the model that quickly between when an annotator is working number two actually annotators will that kind of gives the annotators whiplash where you show them like one example then immediately they have to like look at a different example that they weren’t able to look at whereas normally when you’re annotating you can like read you know several sentences or something like that mentally prepare yourself for doing the annotation um so in reality almost always you do some sort of batching strategy uh where it’s like you you annotate ten sentences and update the model or a hundred sentences and update the model um but i think theoretically updating after every example is like supposedly the best from an ml perspective cool and i think this is the last thing um reusability of the active learning annotations so the annotations that you get from an active learning model uh depend on the model itself so especially if you’re talking about uncertainty um the uncertainty for different models will be different so if active learning annotations are obtained with one model they may not transfer well to other models and this is just an example um from a model uh from a nice paper uh called practical obstacles to deploy active learning and basically the idea is if you sample um blue is random sampling and here this is a svm based active learning method and the red line is an svm based model and you can see the svm based active learning method is doing relatively well um so is the cnn based active learning method um if you look at a cnn based model on a movies on the same data set you can see that basically all of the active learning methods are are similar the cnn based active learning method is better but the active learning method based on other models is basically the same as random and then if you look at the lstm on the same data set the lstm based active learning method is like maybe marginally better than random sampling maybe about the same but the cnn and svm based active learning methods actually produce data that’s worse for training the lsdm based model so because of the kind of mismatch between the models that were used to do active learning and the models that were used to train the active learning models uh this causes uh issues so one of the methods that i one of the methods that i introduced today might be able to remedy this particular problem uh does anyone want to remember back to the beginning of the class what’s a good active learning method for leveraging multiple models query by committee yes so query by committee might be able to solve this because you would use all three of these models and find where they disagree and those would be the places that you would want to be sampling but nonetheless the fact that you’re not sampling directly from the data distribution but sampling from some kind of bias distribution that’s vaguely related to the data distribution means that it’s not necessarily the case that you would train a model and it would do well uh in the future so it’s something to be aware of and be careful of but nonetheless um there are also methods that are completely model agnostic like for example sampling the most frequent phrases for machine translation um and so i i don’t think this is a reason to throw out active learning it’s just something to be you know aware of yeah this is like across different types of models but now that it’s just the competition of different language models uh you’re probably going to have a less costly language model helping you with the active learning part so how much is the difference of impact across different language models yeah i think that’s a good question it really boils down to whether the examples they find hard are similar or different um i don’t have a good idea of whether that is more or less different with the language models that we have now my my intuition is that probably it won’t change a huge amount but it may even be more different because now all the pre-trained language models are trained on different data whereas in this particular experiment all the models were trained on the same data so um also one thing this is just completely an aside but it’s an important thing to know anyway which is um in pre-trained language model in pre-trained language model literature people often focus very much on the methods they use as opposed to the data they train on but the data they train on is actually probably more important so um i mentioned much earlier in the class that ember tends to be better on like knowledge based tasks whereas roberto tends to be better on kind of like analysis or identification tasks oh sorry xlmr tends to be better on analysis or identification tasks and this is my conjecture is that this is because ember is trained on wikipedia whereas roberta xlmr is trained on like a whole bunch of other data um and so i i think that will also carry over into you know reusability of active learning uh annotations as well i implemented active learning in my base company so we used like word and sentence word to help so sentence burt was helping with the uh samples and then the birth model with the classification had to do the actual downstream task and the other challenge was how do we do it across languages like when we have like cross-legal modular texts on robota for doing like cross-lingual internal classification when you’re doing active learning with five annotators in five languages then it’s i guess little more challenging so then we use like multilingual sentence embeddings across my languages so yeah so that’s why i was like curious whether that method is right yeah and i i think like as i mentioned query by committee you know using different models uh finding places where all models are uncertain is probably you know a safer bet than finding places where one model is highly uncertain another model is uh not very certain other things like this so i don’t know if there’s like correct answers to any of these and there are some empirical questions in general how much newly annotated data do you need to make a difference how much newly annotated data do you need for it to make a difference to the model very small amount a very small amount um yeah so in this example it’s 400 tokens to get a 10 percent gain in parts in ner accuracy um that’s not very much that that would take you an hour or something um if you’re doing random sampling those 400 tokens would get you a gain of about one one or two points it depends on how good the model is in the first place original data set size um other things like this but i think the the nice thing about active learning is the beginning of the curve is really steep um especially if you’re using a good method uh that considers representativeness because um like for example zero shot zero shot transfer across languages will do like really really silly things like missing missing the name of the country that the language is from like it it will it will um in yoruba it will have nigeria be like an organization or something like that um or something and those are the things that you get in the first like 200 tokens you annotate basically cool um so i think you had mentioned that uh you had you had a paper on language model calibration for question tasks where you you mentioned that the model it’s high quality for a long answer as well so will that be a problem when we use rp learning for the uh uh maybe question answering tasks with a large language yeah so calibration um so did we i forget did we talk about calibration in this class advanced yeah we talked about it in advanced nlp maybe not here but basically to give an overview calibration is um how well the probability distributions predicted by your model associate with the probability of the answer actually being correct so in a well calibrated model if the probability if the confidence that the model says is 80 it will be correct about 80 of the time if the confidence it gives is 40 it will be correct about forty percent of the time um uh a poorly calibrated model will say it’s uh eighty ninety percent confident and get it right twenty percent of the time or say it’s twenty percent confident and get it right ninety percent of the time so that’s a bad calibration calibration is actually pretty important in some cases for active learning um it’ll be less important for things like margin based methods or other things that are not as clearly probabilistic but we do have a paper on um active learning for part of speech teching where we show that by improving calibration you can get like pretty significant gains and how well active learning works so yeah it’s it’s good yeah cool um so the final thing is the discussion question and um the discussion question is given the task in language or languages that you’re tackling in your project how could you use active learning to improve um how would you calculate uncertainty how would you calculate representativeness and how would you ensure annotator productivity so uh we can split into do we have six yeah we have we have enough for six groups so cool</p>
</blockquote>
</div>
</div>
</div>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<p>Here is a lesson outline on active learning, based on the sources:</p>
<ul>
<li><strong>Introduction to Active Learning</strong>
<ul>
<li>Active learning is a technique that strategically queries a human annotator to generate labeled examples from unlabeled data.</li>
<li>It is especially useful in low-resource scenarios where data is scarce.</li>
<li>Active learning aims to generate the most useful examples to efficiently train a model.</li>
</ul></li>
<li><strong>Types of Learning</strong>
<ul>
<li>Supervised learning: Learning from input-output pairs (x, y).</li>
<li>Unsupervised learning: Learning from inputs only (x).</li>
<li>Semi-supervised learning: Learning from both labeled and unlabeled data.</li>
<li>Active learning: Efficiently generating labeled examples (x, y) from inputs (x) by querying a human annotator.</li>
</ul></li>
<li><strong>Active Learning Pipeline</strong>
<ul>
<li>Start with a small amount of labeled data.</li>
<li>Train a model using the labeled data.</li>
<li>Apply the model to a large amount of unlabeled data.</li>
<li>Select the most informative examples from the unlabeled data.</li>
<li>Annotate the selected examples.</li>
<li>Incorporate the newly labeled data into the training set and retrain the model.</li>
</ul></li>
<li><strong>Why Active Learning?</strong>
<ul>
<li>Active learning can achieve high accuracy with fewer labeled examples compared to random sampling.</li>
<li>By selecting the most useful examples, active learning can create a better decision boundary with less data.</li>
</ul></li>
<li><strong>Fundamental Ideas of Active Learning</strong>
<ul>
<li><strong>Uncertainty</strong>: Select data that the current model finds difficult to handle.</li>
<li><strong>Representativeness:</strong> Choose data that are representative of the overall data distribution.</li>
</ul></li>
<li><strong>Uncertainty Sampling</strong>
<ul>
<li>Uncertainty sampling involves selecting samples where the model is least certain about its predictions.</li>
<li>Common criteria for uncertainty sampling include:
<ul>
<li><strong>Entropy</strong>: Higher entropy indicates more uncertainty.</li>
<li><strong>Top-1 Confidence</strong>: Lower top-1 confidence indicates more uncertainty.</li>
<li><strong>Margin</strong>: Smaller difference between the top two candidates indicates more uncertainty.</li>
</ul></li>
</ul></li>
<li><strong>Query by Committee</strong>
<ul>
<li>Query by committee involves using multiple models and measuring the disagreement between their predictions.</li>
<li>Train multiple models with different architectures or views of the data.</li>
<li>Areas where models disagree are considered less confident and are good candidates for labeling.</li>
<li>Query by committee can be combined with standard ensembling methods.</li>
</ul></li>
<li><strong>Representativeness</strong>
<ul>
<li>Representativeness ensures that the selected examples are similar to the overall data distribution.</li>
<li>Without representativeness, active learning may focus on outliers, which are not helpful for training a generalizable model.</li>
<li>Methods to determine representativeness include finding vectors that are similar to others in vector space.</li>
<li>Techniques like sentence embeddings can be used to represent sentences as vectors.</li>
</ul></li>
<li><strong>Active Learning Strategies for Text Prediction Tasks</strong>
<ul>
<li>Different prediction paradigms for language tasks include text classification, tagging, and sequence-to-sequence tasks.</li>
</ul></li>
<li><strong>Sequence/Token Level Annotation</strong>
<ul>
<li>For sequence labeling and sequence-to-sequence tasks, annotation can be done at different levels: sequence-level or token-level.</li>
<li>Token-level annotation allows focusing on the most difficult parts of sentences, potentially saving time.</li>
<li>However, token-level annotation requires strategies to learn from individual parts of the sentence.</li>
</ul></li>
<li><strong>Sequence-level Uncertainty Measures</strong>
<ul>
<li>Top-1 confidence: Straightforward to apply to sequence labeling tasks.</li>
<li>Margin: Calculate the difference between the top-1 and top-2 scoring outputs.</li>
<li>Sequence-level entropy: Enumerate over n-best candidates to approximate entropy.</li>
</ul></li>
<li><strong>Training on Token Level</strong>
<ul>
<li>For unstructured predictors, each prediction can be treated as independent, and training can be done only on annotated labels.</li>
<li>For structured prediction methods, marginalization can be used to sum over all unlabeled tokens.</li>
</ul></li>
<li><strong>Token-level Representativeness Metrics</strong>
<ul>
<li>Accumulate uncertainty across token instances to identify tokens that need more annotation.</li>
<li>Select a representative instance of that token for annotation.</li>
</ul></li>
<li><strong>Sequence-to-sequence Uncertainty Metrics</strong>
<ul>
<li>Sequence-level: Back-translation likelihood can be used as an uncertainty measure.</li>
<li>Phrase level: The most frequent uncovered phrases in the training corpus can be selected.</li>
</ul></li>
<li><strong>Cross-lingual Learning + Active Learning</strong>
<ul>
<li>Combining cross-lingual learning with active learning can improve performance compared to using either technique in isolation.</li>
<li>Cross-lingual transfer helps to start at a better initial state.</li>
</ul></li>
<li><strong>Active Learning and Human Effort</strong>
<ul>
<li>In simulations, active learning is often assessed based on the number of words or sentences annotated.</li>
<li>However, in reality, active learning may select harder examples that take more time and have a higher chance of human error.</li>
<li>Simulations often overestimate the gain from active learning.</li>
</ul></li>
<li><strong>Considering Cost in Active Learning</strong>
<ul>
<li>Proactive learning considers different oracles that cost different amounts for each annotation.</li>
<li>Cost-sensitive annotation creates a model of annotation cost and accuracy gain for each span in different modes.</li>
<li>The best spans and modes are chosen based on this model.</li>
</ul></li>
<li><strong>Reusability of Active Learning Annotations</strong>
<ul>
<li>Active learning annotations obtained with one model may not transfer well to other models.</li>
<li>The bias in the data distribution due to active learning can cause issues when training different models.</li>
<li>Query by committee, which leverages multiple models, may help to mitigate this problem.</li>
</ul></li>
</ul>
</section>
<section id="papers" class="level2">
<h2 class="anchored" data-anchor-id="papers">Papers</h2>
<p>The papers covered in the active learning lesson are:</p>
<ul>
<li><strong>“Support vector machine active learning with applications to text classification”</strong> by Tong, Simon, and Daphne Koller. This paper discusses support vector machine active learning and its applications for text classification.</li>
<li><strong>“An analysis of active learning strategies for sequence labeling tasks”</strong> by Settles, Burr, and Mark Craven. This paper analyzes active learning strategies for sequence labeling tasks.</li>
<li><strong>“Training conditional random fields using incomplete annotations”</strong> by Tsuboi, Yuta, et al.&nbsp;This paper presents a method for training conditional random fields (CRFs) using incomplete annotations.</li>
<li><strong>“Model transfer for tagging low-resource languages using a bilingual dictionary”</strong> by Fang, Meng, and Trevor Cohn. The paper focuses on model transfer for tagging in low-resource languages using a bilingual dictionary.</li>
<li><strong>“Empirical Evaluation of Active Learning Techniques for Neural MT”</strong> by Zeng, Xiangkai, et al.&nbsp;This paper provides an empirical evaluation of active learning techniques for neural machine translation.</li>
<li><strong>“A little annotation does a lot of good: A study in bootstrapping low-”</strong> by Chaudhary, Aditi, et al.</li>
<li><strong>“Proactive learning: cost-sensitive active learning with multiple imperfect oracles”</strong> by Donmez, Pinar, and Jaime G. Carbonell. This paper discusses proactive learning, which is cost-sensitive active learning with multiple imperfect oracles.</li>
<li><strong>“Segmentation for efficient supervised language annotation with an explicit cost-utility</strong> by Sperber, Matthias, et al.</li>
<li><strong>“Practical obstacles to deploying active learning”</strong> by Lowell, David, Zachary C. Lipton, and Byron C. Wallace. This paper describes the practical obstacles to deploying active learning.</li>
<li><strong>“Phrase level active learning for neural machine translation”</strong></li>
<li><strong>“Pointwise prediction for robust, adaptable Japanese morphological”</strong> by Neubig, Graham, Yosuke Nakata, and Shinsuke Mori.</li>
</ul>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Active {Learning}},
  date = {2022-04-05},
  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737/cs11-737-w20-active-learning/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Active Learning.”</span> April 5, 2022. <a href="https://orenbochman.github.io/notes-nlp/notes/cs11-737/cs11-737-w20-active-learning/">https://orenbochman.github.io/notes-nlp/notes/cs11-737/cs11-737-w20-active-learning/</a>.
</div></div></section></div> ]]></description>
  <category>Active learning</category>
  <category>Multilingual NLP</category>
  <category>NLP</category>
  <category>Notes</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/cs11-737/cs11-737-w20-active-learning/</guid>
  <pubDate>Mon, 04 Apr 2022 21:00:00 GMT</pubDate>
</item>
<item>
  <title>Syntax and Parsing</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/cs11-737/cs11-737-w19-syntax-and-parsing/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/tiling.png" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>course banner</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/f-3N0stPtbw" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Lesson Video
</figcaption>
</figure>
</div><div id="sup-slide-deck1" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div><div id="sup-slide-deck2" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides2.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides2.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;2
</figcaption>
</figure>
</div></div>


<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Obj·ectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Syntax, Major Word Order</li>
<li>Dependency Parsing and Models</li>
<li>Explanation and demo of AutoLex, a system for linguistic discovery</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- Transcript here is machine generated no punctuation and I eddited it a but but it needs more work and I don't see the benefits unless I can run it through some tool. -->
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<blockquote class="blockquote">
<p>This time we’re going to be talking about syntax in general but dependency parsing in particular. and when we talk about linguistic structure and syntax there’s two types of linguistic structure that we talk about.</p>
</blockquote>
<blockquote class="blockquote">
<p>The first one is <strong>dependency structure</strong> which focuses on the relations between words and it looks a little bit like this: For example we have the word saw and the word <code>saw</code> as the root of the sentence and this is connected to other words like <code>i</code> and <code>with</code> and girl</p>
</blockquote>
<blockquote class="blockquote">
<p>We also have <strong>phrase structure</strong> which is focuses on the kind of structure of the sentence as opposed to the relationships between words Both of these are common expressions of syntax or common ways to represent syntax. <mark>In particular phrase structure was widely used in English and you know kind of developed by chomsky and other very influential linguists from 1950s 1960s until now</mark> However recently there’s been a big move towards dependency structure and the reason why is because they’re relatively straightforward to express. And in particularly relatively straightforward to express across a wide variety of languages. and so for example we can do things like say saw is the subject of the sentence so or saw is the root of the sentence and then it has a subject it has a direct object and it has a prepositional phrase and these kinds of things are relatively you know constant across languages maybe not prepositional phrases but you know a phrase like this and it’s particularly good for multilinguality because in some free word order languages it’s also possible to have basically words intervene into a phrase which makes it very difficult to say this is like a particular phrase and what i mean by this is if you have a dependency tree and words cross it’s very hard to come up with an example of this in English. I don’t know if anybody knows one of the top of your head like ellen okay it’s like i went i saw a movie yesterday that was good pt so this is a relatively natural English sentence but actually yesterday yesterday is a child of saw and movie that is a child of movie so you can see the dependency is crossing here basically the only place where we get this in in English is is with adverbs adverbs can be like are basically the only thing with really free word order in English but they break this kind of phrase structure representation because you can’t say that any there’s a consistent phrase here so basically that’s an issue and there are other languages where these are like extremely common where the word order is free for all different kinds of phrases so because of this a lot of syntactic analysis is kind of moving in the direction of using dependencies instead of phrase structure</p>
</blockquote>
</section>
<section id="universal-dependencies" class="level2">
<h2 class="anchored" data-anchor-id="universal-dependencies">Universal Dependencies</h2>
<p>There is an amazing effort called the universal dependencies treebank that gives you a standard format for parse trees in many languages this started out of stanford dependencies and also universal part of speech tags and universal dependencies created by google and the basic motivation for this was they wanted to be able to build like one parser and use it for all of the languages instead of have having 400 parsers for 400 languages so if you come up with something in a unified format that you can that you can parse in process then that makes deploying to many different languages much easier the disadvantage of doing this is that in order to make something universal you have to give up on some language specific details so there are details that don’t easily fit within this universal dependencies format but in most cases they cover you know a lot of the major things that you see in different languages</p>
</section>
<section id="syntax-vs-semantic" class="level2">
<h2 class="anchored" data-anchor-id="syntax-vs-semantic">Syntax vs Semantic</h2>
<p>An other thing that you should know about if you’re considering dependencies or kind of like dependency structure is that there’s actually different types of dependencies – Syntactic and Semantic. And they look very different. <strong>Syntactic dependencies</strong> basically are trying to as closely as possible reflect the phrase structure of the sentence and how sentences are put together So for example in a <em>prepositional phrase</em> the head of the <em>prepositional phrase</em> will be a <em>preposition</em>. Whereas for <strong>semantic dependencies</strong> the head of a prepositional phrase will be like the main <em>noun</em> or <em>verb</em> in that <em>prepositional phrase</em>.</p>
<p>The idea being that syntactic dependencies are good for understanding the structure of the language whereas semantic dependencies might be good, for example, for Question Answering because if you ask a question if you have a sentence like the one i had here “I saw a movie yesterday that was good” or “I saw a movie yesterday at the movie theater” and you want to say where did the person watch the movie and the <em>syntactic dependency</em> <em>movie</em> would be connected to <em>at</em> so you would have to like jump down the tree several nodes wherein <em>semantic dependencies</em> <em>movie</em> would be connected to <em>movie theater</em>. It would have an appropriate label so you could just like look up the edge and connect the two together to answer that question in a single hop instead of multiple hops. This is really important! Aditi might talk about things related to this later and you need to choose which one you want based on which application you’re interested in</p>
</section>
<section id="semantics" class="level2">
<h2 class="anchored" data-anchor-id="semantics">Semantics</h2>
<p>One of the very interesting things about yeah sorry god the last play the the semantics how do we club all the nouns together like we need some external knowledge to understand the semantic of the norm do you mean like a noun phrase or no no just to know just a name of something so like it’s a concept right like so that’s a good question so how how do we understand the semantics of a noun or something so even semantic dependencies this is only talking about the relationship between words in the sentence it’s now telling you about the underlying semantics of the nouns so if you wanted to know about like underlying semantics of a noun you would have to have something else like a semantic like knowledge base or something like this the typical version of this is something like wordnet which basically says well let’s say we have chevrolet chevrolet is a type of car which is a type of vehicle which is a type of you know man-made object which is a type of object or something like this and i think some people might have used wordnet it’s now like lesson style than it was before but it’s it’s basically telling you this information and there’s actually since we’re in a multilingual class i can tell you about something like babelnet which is a multilingual like a multilingual version of wordnet so if i put in [Music] oh i’m sorry i was searching in English no wonder so if i search for this in japanese it can tell you that this word is a concept for like automobile and it’s it has is a has part part of relations and all of these are linked across languages so for example it’s in i can’t find the language link but here yeah here yeah so you can see that it links to car automobile other things like this so it’s all linked together so if you want to specifically talk about semantics of words on their own then you can either use things like this or you can use word embeddings also which give a concept so can’t you get a semantic graph so what word net tells you is it tells you about the semantics of words it doesn’t tell you about the semantics of like words relating to each other so it doesn’t tell you for example that challenge is the object of love it doesn’t tell you that a challenge is being loved which is what this this semantic dependency tells you and then if you go even a little bit farther there’s something called predicate argument analysis or frame semantic parsing or something that gives you an even more abstract version but semantic dependencies are kind of like something related to that so regular regular universal dependencies are semantic sud is syntactic so you should know the difference between them so there’s a lot of cross-lingual differences in structure so we’ve talked about this a lot before like word ordering so we have svo we have hindi which is a verb final and arabic which is verb initial and i got this actually from the pud tree bank which is the parallel universal dependencies treebank it has a whole bunch of translated sentences in different languages along with their dependency trees and the interesting thing is these sentences all i guess mean the same thing hindi speakers can confirm that’s actually the case yes nobody’s saying no so i’ll assume i’ll assume yes but you can see the structure is very different so we have like is and then in English we have we have is is in the middle of the sentence with the noun first and the verb second and then what i can what i can see is we in handy we have an auxiliary verb but then we have a verb here and then we have the object and i guess an oblique indirect object and stuff like this but these all come on the left side of the verb so we can tell that hindi is verb final and then for arabic we can tell that arabic is verb initial and we have a noun here at position sorry a subject and then an oblique here so you can tell the difference in the structure even though i i can’t even read the script in arabic and hindi i can still tell that just by looking at the dependency</p>
</section>
<section id="dependencies" class="level2">
<h2 class="anchored" data-anchor-id="dependencies">Dependencies</h2>
<p>so what can we do with dependencies so i actually previously they were used for feature engineering and systems and they’re still useful in some cases but now our default is just to you know pull up m mbert or xlmr and fine tune it and get reasonable accuracy on a lot of tasks that we care about and in fact you know dependencies are probably captured somewhat to some extent implicitly in these models so why care about syntax i would argue that these are more useful now in human-facing applications and a while ago june 3rd i think last year i actually asked a question on twitter what are convicts are i guess two years ago i asked a question on twitter what are convincing use cases in 2020 for dependencies and i got 39 answers and just to give some examples they still can be useful for incorporating inductive bias into neural models so biasing self-attention to follow syntax or other things like this i think in it still is useful to encourage models to be right for kind of like the right reasons instead of the wrong reasons because this improves model robustness especially out of domain and other things like this and syntactic conductive biases can provide you a way to do this another thing is understanding language structure and this is an example from the aditi’s work which he’s going to present in much more detail in a few minutes so i’ll let her talk to that another very interesting example is searching over parsed corpora so like i talked about before like let’s say we want to find examples with that are talking about x was the founder of y so we want to find it lots of examples of founders of something or other you could try to do this with a regular expression but it’d be pretty tough to come up with a regular expression that gives you this you know with high precision high recall but if instead you find where founder is the verb the subject and it has a subject and an object here then you can just search all examples of this and it actually highlights the appropriate ones here so this is from the spike system created by ai2 and another thing is analysis of other linguistic phenomena or like you know if you want to identify for example when this is coincidentally i actually one one of these is from <a href="https://strubell.github.io/">Emma Strubell</a> you know assistant professor here another one is from <a href="https://maartensap.com/">Martin Sap</a> who will be an assistant professor here I actually made the slides before I knew he was going to be a professor here but but anyway this is examining whether film scripts demonstrate that people have power or agency and analyzing it along the gender of the participants in the film so whether male or female characters are you know like demonstrating more power agency and film scripts and this is used to answer like social and sociologically interesting questions for example and this is made a lot easier by analyzing the syntax because then you can do things like say who did what to whom more easily</p>
</section>
<section id="parsing" class="level2">
<h2 class="anchored" data-anchor-id="parsing">Parsing</h2>
<p>This is kind of a motivation for like what our dependency parses why would you want to be using dependency parts or syntax in general so to talk a little bit more about dependency parsing how would you get these especially in a multilingual fashion parsing is predicting linguistic structure from an input sentence and there’s a couple methods that we use to do this the first one is transition based models and basically the way they work is they step through some steps one by one until we we can turn those steps into a tree another one is graph based models and basically they calculate the probability for each edge in a dependency parse and perform some sort of dynamic programming over them and if you’re familiar with like part of speech tagging from the first assignment transition based models are kind of like a history based model for part of speech tagging and what this would look like is if you if you had like an encoder decoder model where the next tag was always conditioned on the previous tag for graph based models we didn’t really cover crfs here but if you’re familiar with <a href="https://en.wikipedia.org/wiki/Conditional_random_field">CRF</a>s the graph based models are a little bit like these they calculate some scores and then they have a dynamic program to get the best output so just to give a very brief flavor of what these look like</p>
</section>
<section id="shift-reduce" class="level2">
<h2 class="anchored" data-anchor-id="shift-reduce">Shift Reduce</h2>
<p>Shift reduce parsing basically it processes words one by one left to right and it maintains two data structures one is a queue of unprocessed words another is a stack of partially processed words and at each point we choose to either shift moving one word from the queue to the stack reduce left where the top word on the stack becomes the head of the second word or reduce right where the second word on the stack is becomes ahead of the top word and we learn how to choose each of these actions with a classifier so just to give an example we want to parse the sentence i saw a girl so what we do is we first shift and move something sorry this says buffer it’s the same thing as q there’s multiple ways to say this but just think buffer equals q we move one thing from the the queue to the stack we move another word from the queue to the stack and then sorry that’s another typo this should be reduced left and so we reduce left and we get a left arc here then we shift again and then we shift again then we reduce left we reduce right and then we reduce right and then we have a final basically parse tree here so basically what you can see is at each point we choose an action and based on the action we add we either move something from the queue to the stack or we add an arc between the top two things on the stack so you probably won’t need to implement this yourself there are plenty of good dependency parsers out there but just to get an idea of what the algorithm looks like in case you’re interested in doing that</p>
</section>
<section id="classification" class="level2">
<h2 class="anchored" data-anchor-id="classification">Classification</h2>
<p>so the way we do classification for any variety of shift-reduced parsing is basically we take in the stack and the buffer and we need to choose between one of the actions shift left and right so this is a regular classification problem three-way classification problem and we can encode the stack configuration with any variety of recurrent neural network or auto-regressive neural network so one example of this is where we encode basically the stack the previous history of actions and the buffer and feed them into the into the model an even simpler way of doing this is you just encode the words in the input sentence and then have a decoder that generates the actions for you so you could even just throw that into like a regular transformer model and train it as well it would just be you know input is the entire words in the the source sentence and then the output is the sequence of actions so that’s a basic like very quick overview of shift-reduce pricing are there any questions or yeah so you said that we use the dependency passing as an auxiliary fast increase of westminster’s products and inputs yeah so now we have some methods about like adding adapters into xlr to make them more robust for a given language so with adapters do we still need these to increase robustness or if you have a car first and find an adapter and yeah so that’s a really good question if we have something like adapters like multilingual adapters do we still need something like an auxiliary test like dependency parsing and i i would say it’s quite likely that those two things are kind of orthogonal in that adapters are allowing you to more effectively adapt to individual tasks whereas the supervision that you would get from a dependency parsing objective would essentially enforce the model to more strictly follow the syntax of the sentence as linguists kind of describe it so i think that both probably would stack together but i i don’t have empirical results or i i don’t know immediately off the top of my head about a paper that demonstrates maddox yeah so i know medics but i don’t remember if they used like dependency parsing it’s an auxiliary task but they show performance improvement on very low resource languages that xlr is failing and then they used access to improve the performance but i’m not sure whether it was one of the things and so basically the the comment to repeat it for other people who couldn’t hear the so there’s a paper called man x that basically does adapters we talked a little bit about adapters in class i think but basically they demonstrated that it improves on very low resource languages but i it’s not perfect still after using medx right and i think this and that could be combined together probably to improve a bit but as i said i think that’s not the main good use case of dependencies right now i think the better use cases of the dependencies are they give very like intuitive human facing interfaces if you want to do like analysis of corpora or extract detected phenomena on a more holistic level or other things like that so it was actually inspired by one of your tweets you had put a tweet i think a year back that when a person releases the model for 100 languages doesn’t mean it works languages so yeah that changed my perspective that okay it’s not perfect then i kind of searched for these people yeah so i to repeat in case people in the back couldn’t hear i i said something on twitter about a year ago where it’s like when somebody releases a model for 100 languages that doesn’t necessarily mean it works on 100 languages it means it does something on 100 languages probably and it’s probably better than nothing but it’s not perfect for sure so and i’m sure you guys all noticed this as you were implementing your various assignments as well cool.</p>
</section>
<section id="graph-based" class="level2">
<h2 class="anchored" data-anchor-id="graph-based">Graph based</h2>
<p>The other alternative is a graph based parsing and graph based parsing basically what it looks like is we express the sentence as a fully connected directed graph which means that we have all pairs of words as potential candidates for a dependency edge existing between them another way you can think of it is a matrix of scores for each for each edge where the rows are the head and the the columns are the children and the diagonal obviously something can’t be ahead of itself so it would it’s irrelevant but you predict all of the other things there and then after you do that you score each edge independently so you basically calculate the values of that that score matrix and then you have some algorithm that allows you to find the maximal spanning tree which is basically the highest scoring set of edges that form a tree in the form a tree with a root in it</p>
</section>
<section id="graphbased-vs-transitionbased" class="level2">
<h2 class="anchored" data-anchor-id="graphbased-vs-transitionbased">Graphbased vs Transitionbased</h2>
<p>and i have a comparison of graph based versus transition based one ex one advantage of transition based parsing is that it allows you to condition on infinite context basically so it allows you to condition on all of your previous actions so theoretically it has as much expressiveness as you want just like a regular encoder decoder model can do you know can condition on all your previous sections however for transition based parsing greedy search algorithms can cause short-term mistakes to propagate into damaging your long-term performance so you know if you accidentally connect an edge too early there’s no way to recover from it on the other hand graph based parsing you can find the exact best global solution via dynamic programming however you have to make some local independence assumptions so you can’t like necessarily easily condition you know the choice of one edge on whether you chose one edge or at another time so for the dynamic programming</p>
</section>
<section id="dynamic-programming" class="level2">
<h2 class="anchored" data-anchor-id="dynamic-programming">Dynamic Programming</h2>
<p>algorithm i’m not going to go into a lot of detail here because as i said like people are probably not going to be implementing this on your own you’re probably going to be using a parser but basically we have a graph and want to find it spanning trees so what we do is we greedily select the best incoming edge to each node and we subtract its score from all other incoming edges because we want a tree and not anything with any cycles in it what we do is we contract the cycles together into a single node and resolve the cycles within that node and basically we recursively call the algorithm on the graph with the contracted node and then we can expand the contracted node deleting at an edge from the root node there so if you’re more interested in exactly how this works you can look at jaravsky and martin’s textbook on dependency parsing or something like this but basically that’s the general idea for these models basically what we do is we extract features over the whole sequence so we feed in all of the you know all of the words in the input into a feature extractor in this particular in this particular example it says lstm but in reality now it’s xlmr or bert or you know whatever</p>
</section>
<section id="feature-extractor" class="level2">
<h2 class="anchored" data-anchor-id="feature-extractor">Feature Extractor</h2>
<p>Your favorite feature extractor is the classifier that you use to basically apply scores to each of the nodes of the input is a by affine classifier this looks a little bit scary if you look at the equations but it’s actually super simple what you do is you learn specific representations through an mllp for each head word in each dependent word so basically you feed the representations that you get from burke or whatever else into an mlp where you have a separate mlp when you’re considering the word as a head and when you’re considering the word as a child or a dependent and then you calculate the score of each arc where sorry i thought i had an animation here but basically the first part of this here is calculating how likely a child word is to connect to a head word so it’s calculating basically the congruence between a child word and a head word and of course you know this is optimized end to end but it will be considering things like how close together are the words how likely is a child to be a child of a head like so nouns tend to be children of verbs et cetera et cetera determines children of nouns and then in addition it also calculates the score saying how likely is this word to be a head word in the first place so for example determiners are almost never head words in in semantic dependencies and so they’ll get a very low score according to this thing down here whereas verbs are very often head words so we’ll get a high score</p>
</section>
<section id="difficulty" class="level2">
<h2 class="anchored" data-anchor-id="difficulty">Difficulty</h2>
<p>For multi-lingual dependency parsing,the difficulty in multilingual dependency parsing is, that actually <strong>syntactic analysis</strong> is a particularly hard multilingual task. I think it’s harder than named entity recognition; it’s probably harder than part of speech tagging and it might even be on the same level of difficulty as translation. Yeah maybe not maybe not on the same level of difficulty as translation but it’s hard it’s a hard task and the reason why is because it’s on a global level not just a word by word level so you need to consider long distance dependencies within the sentence and syntax varies widely across different languages so like English is very very different from Hindi and both are very very different from Arabic so you can’t like just say you know well like named entities look pretty similar in all languages they all have like low frequency words and is a key that always works for named entities but you can’t do something like that when deciding syntax is easily</p>
</section>
<section id="order-insensitive-encoder" class="level2">
<h2 class="anchored" data-anchor-id="order-insensitive-encoder">Order insensitive encoder</h2>
<p>so there’s a bunch of improvements that people have proposed i had some examples of these and the papers that i suggested for reading for the discussion one example is that people have shown that when you’re transferring to very very different languages you can remove the bias on ordering in encoders so for example if you’re using a transformer based model you can remove the positional encodings so you basically get an order insensitive encoder and that transfers much better to very very different languages so in this case they always use English as the source language and transferred to another language but can you think of any other examples where this might be useful can you think of it can you think of an example of a language where it would be very hard to get another language with similar syntax that’s included in your dependency treatment yeah i mean if you’re studying a low resource language that’s part of a language family that only has other low resource languages yeah exactly so if you’re studying a low resource language with a language family that only has other low research languages i could give an example what about navajo that’s probably the highest resource language in its language family but it’s extremely low resource and i think that’s i think it’s also useful because one of the or one of it’s also important because dependency parsing is particularly useful for things like linguistic inquiry or human facing interfaces when you can’t just train an end to end model easily so you know that’s a particularly salient use case another paper that i</p>
</section>
<section id="dependency-parsing" class="level2">
<h2 class="anchored" data-anchor-id="dependency-parsing">Dependency parsing</h2>
<p>this is one of my papers i like it a lot i i wish more people liked it a lot but but basically the i think it’s a really really cool paper so i i’m trying to sell it to everybody but basically we came up with a generative model for dependency parsing and the idea of the generative model for dependency parsing is that you have a model that jointly generates the dependency tree and the sentence and one of the nice things about generative models is that you can train them unsupervised as well so you can take the unsupervised model and learn the structure that’s used in the unsupervised model together with like generating the sentences on unlabeled data and the reason why i think this is cool is because you can take a model that was trained on English and then train it unsupervised on the language that you’re interested in parsing and it improves the performance by a lot especially if the languages are very unrelated so i think this is another cool tool if you have a language that is from like a different language family for example the input is the dependency tree and your model just generates a random sentence in the target so the input is it’s a generative it’s you can think of it like a language model but it calculates instead of just calculating the probability of the output sentence it calculates the joint probability of the dependency tree and the output sentence so if you’ve given a sentence like it reconstructs the sentence right and it can do other things like it can get the the highest probability dependency tree given a sentence that’s called like latent variable inference and other things like that basically in this case it’s initialization it can be initialization with regularization towards the original parameters as well does that imply supervised data for the chemistry it does not it only it only requires text and basically the way it works is it it you have a dynamic program that finds the dependency tree that iterates over the dependency trees and it optimizes the parameters of like the dependency parser and the output at the same time but the probability of the output will go up basically if you have a more coherent dependency tree for a completely different family yes yeah the different family could have a completely different type of synthetic structure yes exactly yes you read read the paper i’m not lying so that’s a good that’s a good question and that will link it to what aditi is going to talk about in a second too we didn’t we didn’t evaluate it that you know like that extensively so i think that would be a natural interesting next question for any of these improvements that i’m talking about here does do these improvements lead to a more coherent you know grammatical sketch for the language or something like that if we extract the grammatical sketch cool yeah so i’m i’m</p>
</section>
<section id="linguistic-informed-constraints" class="level2">
<h2 class="anchored" data-anchor-id="linguistic-informed-constraints">Linguistic informed constraints</h2>
<p>as i said i i’m excited about that so come talk to me if you if you want to hear more a final thing is linguistically informed constraints so there are big atlases of data about linguistic structures like the world atlas of language structures which i believe ellen or somebody talked about earlier and they they tell you things like is an adjective before a noun and so using this what this paper that i’m introducing here does is they use something called posterior regularization which basically says we’re going to parse our whole corpus we’re going to look at the proportion of the time that an adjective appears before a noun or after a noun and then if our big atlas of language structure says that adjectives should happen before nouns but our dependency parser is putting adjectives after nouns much more often then we are going to decrease the probability of it putting an adjective after the noun and increase the probability of it putting an adjective before the noun so this is a way to introduce basically prior knowledge into the predictor in order to make it work better so these are just three examples of cross-lingual dependency parsing but they demonstrate some you know ways to incorporate intuitions and stuff cool and any other things if not i’ll turn it over to aditi to talk in a bit of detail we might or might not have time for discussion formal discussion but we could have maybe all like class discussion and and it’s the keynote slides yes these are the keywords so can i open the demo on the google chrome this one yes thank you and then yeah that works cool and that’s the mind sure okay hi everyone i’m aditi i’m a phd student working with graham and today i’m going to show you a part of my research where i’ve used these dependency analysis okay so we just saw some because it’s kind of annoying okay so dependency analysis basically told us on a high level how words are related to each other so it’s information is useful but we also need to understand a more complex linguistic phenomena if you truly want to understand the language as a whole so some of these complex linguistic phenomena are like morphological agreement word order case marking suffix usage to name a few and these are important not just for like a language communication or understanding but also has some concrete applications so there are some human-centric or human-facing applications for instance like if you want to like learn the language then you need to know how to arrange these words when does the ordering of the word changes what kind of suffix to use when what happens gender is like for each gender you might have a different word ending and so on another important application which for navajo we also saw was language documentation because languages are getting extinct quite frequently and quite quickly also so language documentation is a way where linguists document the salient grammar points of a language not just for preservation but also as a way to like also create pedagogical resources maybe even create language technologies from that so another kind of application is from machine centric applications some examples that we saw where dependency analysis were used to give inductive bias into models another application is like we sort of used these rules that we extract automatically to evaluate a machine output so often across languages as we saw syntax is quite different so we want to have a way to automatically evaluate how grammatically correct let’s say a machine translation output is so basically to achieve both human centric application and machine centric applications we need to extract rules which explain this phenomena in a format which is both human readable and machine readable and i’m going to quickly explain like how we do this using a process of</p>
</section>
<section id="definition-of-required-agreement" class="level2">
<h2 class="anchored" data-anchor-id="definition-of-required-agreement">Definition of required agreement</h2>
<p>morphological agreement so agreement is a quite complex process wherein basically words in a sentence often change their forms or morphology based on some other words in the same sentence based on some category like gender number and person so i’ll give a quickly an example from number agreement in spanish so here you can see that girl is in singular and the word for verb has also been singular so now if i change the word for girl to be in a plural form then the words form also changes to the plural form so basically any change in the subjects number has to bring about a change in the verbs number so we call this as subject verb required agreement now if you look at the object and the word they are also both in the singular form in the first sentence and the second sentence when the form of the object dog has become plural the form of the word still remains in a singular form so essentially any change in the object’s number is not bringing a change in the verbs number so this is it’s not required agreement so any sort of agreement that we may observe between object and work is purely by chance so we call this as object work chance agreement so to basically understand what are the rules which govern subject agreement and orders or how to require agreement you basically formulate it into a classification task</p>
</section>
<section id="prediction-task" class="level2">
<h2 class="anchored" data-anchor-id="prediction-task">Prediction task</h2>
<p>so the task is here of predicting required agreement versus chance agreement and how do we extract these rules automatically just from the raw text so here i have an example of greek this is a greek sentence and we first automatically perform some syntactic analysis which basically gives us what is the part of speech of each word what is this morphological features what are the dependency links between them now from this syntactic analysis we basically create our training data for this prediction task so here’s an example so on this box here you basically have a dependency link between the determiner and the proper noun now the gender of the determinant and the proper noun are both matching they’re both in feminine gender so basically we can create a training data point saying that proper noun and determiner when they are in a relation then the gender is matching so the agreement value becomes yes now another dependency link here is between the noun and the proper noun now here the gender values are not matching so our data point here becomes that any relation between noun and proper noun in this example the agreement value is known so essentially we are basically creating a binary classification task from this example and we create this data set for all the sentences we then basically learn a model on top of this training data from which we extract rules where the rules are telling us which of these rules are actually leading to a required agreement and which are leading to a chance agreement so essentially this is an example of the model so again going back to the previous slide where i mentioned about human centric applications so we want to understand and extract these rules which are understandable to humans so we want to use a model which is more interpretable so here i have used the model of decision trees the decision trees can exactly tell you what are the features which led to one decision so once we have applied a decision tree style model on our training data it gives us a leaf node the leaf here is inducing a distribution of agreement over these examples the leaf 3 here is showing us that there are 58 000 examples where the gender values were matching and 778 but they were not matching but how do we know whether this distribution is leading to a required agreement or a chance agreement and to automatically extract this label we basically apply a statistical threshold i won’t go into the much details of it but essentially we apply a significance test which tells us whether the observed agreement distribution is significant or not and this can tell us whether the leaf is truly capturing a required agreement or not and this is like an example of the label decision tree where for spanish gender agreement this is the leaf or this is the tree after the leaf labeling stage and the leaf three here has been marked as required agreement so basically from this leaf three we can extract some discrete rules which says okay if determinant and noun are in the following relation then they need to agree on gender so basically from the raw text we started training data over which we train an interpretable model from which we then extracted some discrete and very simple rules and this is just a very basic pipeline and now we are trying to extract this</p>
</section>
<section id="linguistic-questions" class="level2">
<h2 class="anchored" data-anchor-id="linguistic-questions">Linguistic questions</h2>
<p>sort of or apply this pipeline for potentially any linguistic question so we saw this for agreement where our linguistic question was when do syntactic heads show agreement with their dependence on gender number and so on another interesting question was for case marking in case marking we’re interested to know when does a particular class of words like nouns take nominative case over the other so for example in this sentence anna has food anna is in the nominated case but if it becomes anna’s food then ani is a generative base so we want to basically if you are learning this language you need to know that when to add an apostrophe is when you are basically showing a possession another kind of interesting linguistic phenomena is of word order like you need to know how to arrange the words appropriately and even in English typically when we say about word order people just say it’s one word that English follows svo but it’s even within English there are multiple word orders so for instance if if i’m saying English is sorry anna is eating an apple then your word order is simply subject verb and object but if i’m asking a question what is anna eating then these order changes it now becomes object subject and work so if i’m learning English i need to know that when i’m asking a question what is the typical order but if i am just using or saying a decorative sentence what is the what order so essentially for any linguistic question we want to formulate it as a prediction task and learn and extract its rules and this is the final general framework which we have been working on from the raw text you basically extract some</p>
</section>
<section id="general-framework" class="level2">
<h2 class="anchored" data-anchor-id="general-framework">General framework</h2>
<p>features but in our case the features were part of speech tags dependency analysis from which we then extract rules for each of this phenomena and i’ll show you what the rules look like in a minute okay so now what are the kind of syntactic analysis which we can use so graham just showed you the universal dependencies project within that is also the sud tree banks so basically if for a language we have this kind of data available which more or less is annotated by language experts we can directly use these sud treatment as a starting point but for many more languages we even don’t have annotations so in that case your multilingual parsers come into the picture where you can take some raw text first parse it using these models and then apply the same approach okay so this is the toolkit</p>
</section>
<section id="toolkit" class="level2">
<h2 class="anchored" data-anchor-id="toolkit">Toolkit</h2>
<p>which we have developed and okay this basically this is an autolex framework where we have extracted such kind of rules for different linguistic behaviors for a bunch of languages sure so for each language we extract a bunch of features and i’ll just go through some of them here so let’s say we want to explore the spanish agreement features and let’s go into gender so okay so this is exactly the so if you look at the first thing here basically say that okay mostly in spanish gender usually agrees between the determiner and its head but there are some significant number of cases when this does not hold true and some of these cases we have highlighted here that if let’s say a determiner is governed by an adjective in some cases the gender did not agree so we basically extracts rules in a human readable format and further for each of these rules because rules alone can often get quite overwhelming we also show some illustrative examples so here basically if you look here so here the determiner gender is masculine and the adjective gender is feminine so here is one example where although the determiner is governed by an adjective the gender is not matching but then there are some other examples where the determiner’s gender is masculine and the adjectives gender is also masculine here the general values are matching so these are some sort of exceptions to the language general oriented and knowing these exceptions are also important because they do occur quite frequently in the language again we have some rules for word order also so let’s say we want to can go adjective noun so for instance so typically in spanish unlike English most adjectives come after the noun that’s the typical ordering of objectives but there are very specific cases or or very specific adjectives which come before the nouns and the model has correctly identified some of them for instance this rule is telling us that if the adjective has the lima primero then the adjectives come before the noun and again for each of the rules we also ex like show some this illustrative examples where indeed these adjective which has the lima primero is coming before the noun but again language is not that simple there are even exceptions to exceptions so there are again examples which show that even when the lima is primarily there are certain times when this rule is not followed so we are trying to show a more softer view of the rule also that this rule is not 100 applicable every time there are conditions where this rule is not followed so essentially here we have used dependencies as our feature base to explain the rules in a human readable format and i guess one another important aspect here is the quality of the rules also depend a lot on the quality of the underlying analysis so as we improve the multilingual dependency parsing the quality of the rules should also improve so we also applied this system for an endangered language variety called hmong so hmong is spoken in north america also china thailand vietnam and laos and one of its variety is close to endangered and we were trying to we simply had access to a bunch of monk sentences and we wanted to analyze some of his interests and linguistic properties and we had david in lti who also speaks more and knows a lot about it so we basically presented such kind of rules for mong to david and there were a couple of interesting observations first the quality of rules was not as good as what we were seeing for spanish but despite that the model had still been able to identify some cluster of examples which showed rules which david was not aware about so such kind of data-driven approaches is also able to capture or identify some rules which the linguist was not initially aware of so i think dependency analysis is a very useful tool which has a lot of applications especially for exploring a language because there’s so much data out there the dependency analysis helps us find the key components to it so i can go into more detail but these are the main features sure so right now we basically i showed you grammar rules and how it was</p>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>useful from let’s say a linguist or language documentation point of view we’re also now using these tools to actually teach languages to a more less linguistically aware audience because often learners or teachers they don’t go into the linguistic jargon that much but they’re still interested in knowing the grammar rules and especially we are focusing on two languages marathi and kanada these are both indian languages and there are schools in north america where they are teaching these languages to English speakers so a lot of the times there are immigrants who are settled here and they want their children to learn these languages so that so there are English native speakers here and the schools here are interested in teaching them these languages from English so here we basically extract a lot of interesting features so for example for example here we have basically extracted some of the most common words observed in kannada what is the transliteration in English and what are the different kind of forms they are like what are the different forms these lemma have been observed across different genders so this lemma do here i don’t know i’m pronouncing it correctly but you can see here how this lima is used for a masculine gender or how this lima is used in the feminine gender and such kind of tables are pretty common when you are teaching how a word should be used in a language so we are automatically again performing syntactic analysis on these languages extracting such kind of useful insights and then presenting them to actual teachers so that they can check if this material is useful for their teaching process so we are trying to ease the teachers job so that they can focus on the creative aspect of teaching and yeah i guess those are the main points great thank you thank you aditi yeah so i’m sorry we’re right up against the the time so i don’t think we’re gonna have time for discussion this time i really apologize i try not to do this but i packed too much stuff in but i also invited the dt and didn’t want to ever prepare for nothing because of this if you took time to like prepare for the discussion today and read the things if you want to send like a short summary of the things that you prepared we can give like extra credit it won’t be required but we i’ll give like one discussion worth of extra credit for that are there any questions for about the stuff that you was talking about here yeah obviously mentioned like different regions and different species like america are there any like grammatical differences region-wise or are they like that’s a good question so there are these different varieties so we worked so the question was about because hmong is spread across so many different regions are there any difference in the grammatical properties so there are because there are different hmong varieties and we worked on one of the variety which is predominantly spoken in north america so we didn’t have the chance to investigate how the grammar rules changes across these different varieties but the interesting thing so we did another sort of a separate set of experiments where the universal dependencies project they have a lot of free bang for the same language for different domains so there are data from grammar books they have data from news articles and even within the same language across these domains the grammar structure varies quite a lot so in this tool we are basically also offering linguists to like check okay these are the do’s extracted from one domain how do they change in another domain is their model able to account for instances of agreement where there’s not exact matching values for instance if the subject is marked as duo so that would be considered as that if the agreement is not happening because the value is not matching i’m not sure if the universal dependency tree bank actually has that level of detail it might i maybe you have experience with that but i think the universal dependency annotations are actually quite i don’t know coarse so that my level of detail might not show up in the first place but it might yeah that is like an important point that this analysis has been done on one schema the schema used here is the ud schema or the sud schema which is the the we purposefully chose sud or ud because first of all it’s available for a lot more languages and it has a consistent annotation format so the kind of rules we extract now they’re also consistent across languages but that being said it might not consider the language the system is that well that is like a drawback for that but potentially you can apply this pipeline or this model to any other annotation statement it will give you rules according to that schema okay great thanks a lot everyone we can answer other other questions</p>
</section>
</div>
</div>
</div>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li><strong>Introduction to Syntax and Linguistic Structure</strong>
<ul>
<li>Syntax focuses on the structure of sentences and the relationships between words.</li>
<li>Two main types of linguistic structure:
<ul>
<li><strong>Dependency structure</strong>: Focuses on the relations between words.</li>
<li><strong>Phrase structure</strong>: Focuses on the structure of the sentence.</li>
</ul></li>
<li><mark>Dependency structures are often more straightforward to express across various languages and are thus useful in multilingual applications.</mark></li>
</ul></li>
<li><strong>Dependency Parsing</strong>
<ul>
<li>Dependency parsing involves predicting linguistic structure from an input sentence.</li>
<li><strong>Different types of dependencies</strong>:
<ul>
<li><strong>Syntactic dependencies</strong>: Reflect the phrase structure of the sentence.</li>
<li><strong>Semantic dependencies</strong>: Useful for applications like question answering by connecting semantically related words.</li>
</ul></li>
</ul></li>
<li><strong>Universal Dependencies Treebank</strong>
<ul>
<li><mark>A standard format for parse trees across many languages, facilitating the development of parsers that can be used for multiple languages.</mark></li>
<li>Allows for a unified format that simplifies processing across different languages.</li>
<li><mark>To achieve universality, it sometimes requires sacrificing language-specific details.</mark></li>
</ul></li>
<li><strong>Cross-Lingual Differences in Structure</strong>
<ul>
<li>Languages can vary significantly in word order (e.g., SVO, verb-final, verb-initial).</li>
<li>Dependency trees can visually highlight these structural differences, even without understanding the specific language.</li>
</ul></li>
<li><strong>Dependency Parsing Methods</strong>
<ul>
<li><mark><strong>Transition-based models</strong>: Step through actions to build a tree. They use a queue of unprocessed words and a stack of partially processed words, choosing actions like shift and reduce.</mark></li>
<li><mark><strong>Graph-based models</strong>: Calculate the probability of each edge and use dynamic programming to find the best tree. They express a sentence as a fully connected directed graph and find the maximal spanning tree.</mark></li>
</ul></li>
<li><strong>Applications of Dependency Parsing</strong>
<ul>
<li><strong>Human-facing applications</strong>: Useful for analyzing corpora, understanding language structure, and other linguistic phenomena.</li>
<li><strong>Adding inductive bias to neural models</strong>: Improving model robustness and encouraging models to be correct for the right reasons.</li>
<li><strong>Searching parsed corpora</strong>: Finding examples based on syntactic relationships.</li>
<li><strong>Analysis of linguistic phenomena</strong>: Examining power dynamics or other sociological questions in film scripts.</li>
<li><strong>Language learning and documentation</strong>: Extracting morphological agreement rules and documenting salient grammar points.</li>
<li><strong>Evaluating machine output</strong>: Assessing the grammatical correctness of machine translation outputs.</li>
</ul></li>
<li><strong>Multilingual Dependency Parsing</strong>
<ul>
<li><mark>Syntactic analysis is a challenging multilingual task due to global-level considerations and wide syntax variations.</mark></li>
<li><strong>Techniques to improve cross-lingual transfer</strong>:
<ul>
<li>Removing bias on ordering in encoders.</li>
<li>Using generative models for unsupervised training.</li>
<li>Applying linguistically informed constraints.</li>
</ul></li>
</ul></li>
<li><strong>Extracting Linguistic Insights Automatically</strong>
<ul>
<li><strong>Goal</strong>: To extract rules that explain linguistic phenomena in a human-readable and machine-readable format.</li>
<li><strong>Method</strong>: Formulate linguistic questions as prediction tasks and extract rules from raw text using syntactic analysis.</li>
<li><strong>Example</strong>: Extracting morphological agreement rules by predicting required agreement versus chance agreement.</li>
<li><strong>General Framework</strong>: Extract features (POS tags, dependency parses) from raw text, then extract rules for agreement, case marking, word order, etc..</li>
</ul></li>
<li><strong>Practical uses of automatically extracted rules</strong>
<ul>
<li>Language documentation and preservation.</li>
<li>Creating pedagogical resources and language technologies.</li>
<li>Aiding language learners and teachers by presenting grammar rules and common word usages.</li>
<li>Evaluating and correcting machine-generated text.</li>
</ul></li>
</ul>
</section>
<section id="reflection" class="level2">
<h2 class="anchored" data-anchor-id="reflection">Reflection</h2>
<p>Morphological agreements discussion by Aditi is of particular interest.</p>
<p>It seems that understanding agreement is central to ‘second order’ model that need to beat the <strong>baseline</strong>. To get this to happen in an interpretable form we may want to train an attention head that is looking at thing that are in agreement and responding to the feature of agreement and disagreement as both are signals of some value.</p>
<p>I belief that we can learn a probabilistic model of agreement between different cases but that they may be affected by all sorts of constraints like clause and phrase boundaries and the degree of morphological markings. So this is why attention seems so appropriate as it can learn what to focus on. And what we want is to make it interpretable so we can also extract the rules that it has learned!</p>
<p>Aditi talks about <strong>require agreement</strong> v.s. <strong>chance agreement</strong> for verb-subject and verb-object. I think that we may want to have a theory that looks at agreement in winder context and create a model that can predict agreement in a wider context so that we can consider the entropy of a sentence once agreement has been accounted for. In fact my thinking is that we may want to go further and determine the functional load of the agreement and have a more redined metric that can be used to evaluate the entropy of a sentence accounting to agreement in a functional context. (Error correction ambiguity, Reducing cognitive load, Reducing ambiguity, Parsing)</p>
</section>
<section id="navajo-in-10" class="level2">
<h2 class="anchored" data-anchor-id="navajo-in-10">Navajo in 10</h2>
<ul>
<li><p><strong>Language Name and Classification</strong>:</p>
<ul>
<li><strong>Navajo</strong> (also known as <strong>Navaho</strong>).</li>
<li>Navajo: <em>Diné bizaad</em> [tìnépìz̥ɑ̀ːt] or <em>Naabeehó bizaad</em> [nɑ̀ːpèːhópìz̥ɑ̀ːt].</li>
<li>It is a <strong>Southern Athabaskan language</strong> of the <strong>Na-Dené family</strong>.</li>
</ul></li>
<li><p><strong>Speakers and Location</strong>:</p>
<ul>
<li>Spoken primarily in the <strong>Southwestern United States</strong>, especially in the <strong>Navajo Nation</strong>.</li>
<li>One of the most widely spoken <strong>Native American languages</strong>.</li>
<li>The most widely spoken Native American language north of the <strong>Mexico–United States border</strong>.</li>
<li>Almost <strong>170,000 Americans</strong> speaking Navajo at home as of 2011.</li>
</ul></li>
<li><p><strong>Nomenclature</strong>:</p>
<ul>
<li>The word <em>Navajo</em> is an <strong>exonym</strong> from the Tewa word <em>Navahu</em>, meaning ‘large field’.</li>
<li>The Navajo refer to themselves as the <em>Diné</em> (‘People’), with their language known as <em>Diné bizaad</em> (‘People’s language’) or <em>Naabeehó bizaad</em>.</li>
</ul></li>
<li><p><strong>Official Status</strong>:</p>
<ul>
<li>Official language in <strong>Navajo Nation</strong>.</li>
</ul></li>
<li><p><strong>History and Development</strong>:</p>
<ul>
<li>The Apachean languages, of which Navajo is one, are thought to have arrived in the American Southwest from the north by 1500.</li>
<li>Speakers of the Navajo language were employed as <strong>Navajo code talkers</strong> during World Wars I and II.</li>
<li>Orthography developed in the late 1930s and is based on the <strong>Latin script</strong>.</li>
</ul></li>
<li><p><strong>Writing System</strong>:</p>
<ul>
<li>Based on the <strong>Latin script</strong>.</li>
<li>Developed between 1935 and 1940.</li>
<li>Uses an apostrophe to mark <strong>ejective consonants</strong> and mid-word or final <strong>glottal stops</strong>.</li>
<li>Represents nasalized vowels with an <strong>ogonek</strong> and the voiceless alveolar lateral fricative with a <strong>barred L</strong>.</li>
</ul></li>
<li><p><strong>Phonology</strong>:</p>
<ul>
<li>Has a fairly large <strong>consonant inventory</strong>.</li>
<li><strong>Stop consonants</strong> exist in three laryngeal forms: aspirated, unaspirated, and ejective.</li>
<li>Has a simple <strong>glottal stop</strong> used after vowels.</li>
<li>Four <strong>vowel qualities</strong>: /a/, /e/, /i/, and /o/.</li>
<li>Each vowel exists in both <strong>oral and nasalized</strong> forms and can be either <strong>short or long</strong>.</li>
<li>Distinguishes for <strong>tone</strong> between high and low.</li>
</ul>
<p><img src="https://orenbochman.github.io/notes-nlp/notes/cs11-737/cs11-737-w19-syntax-and-parsing/navajo-consonants.png" class="img-fluid" width="400" alt="Navajo Consonants"> <img src="https://orenbochman.github.io/notes-nlp/notes/cs11-737/cs11-737-w19-syntax-and-parsing/navajo-vowels.png" class="img-fluid" width="400" alt="Navajo Vowels"></p></li>
<li><p><strong>Grammar</strong>:</p>
<ul>
<li>Relies heavily on <strong>affixes</strong>, mainly prefixes.</li>
<li>Affixes are joined in unpredictable, overlapping ways that make them difficult to segment.</li>
<li>Basic word order is <strong>subject–object–verb</strong>.</li>
<li>Verbs are conjugated for <strong>aspect and mood</strong>.</li>
</ul></li>
<li><p><strong>Vocabulary</strong>:</p>
<ul>
<li>Most Navajo vocabulary is of <strong>Athabaskan origin</strong>.</li>
<li>Has been conservative with <strong>loanwords</strong> due to its highly complex noun morphology.</li>
<li>Expanded its vocabulary to include Western technological and cultural terms through <strong>calques and Navajo descriptive terms</strong>.</li>
</ul></li>
<li><p><strong>Revitalization and Current Status</strong>:</p>
<ul>
<li><strong>Bilingual Education Act</strong> in 1968 provided funds for educating young students who are not native English speakers.</li>
<li>Navajo Nation Council decreed in 1984 that the Navajo language would be available and comprehensive for students of all grade levels in schools of the Navajo Nation.</li>
<li><strong>Navajo-immersion programs</strong> have cropped up across the Navajo Nation.</li>
<li>Diné College offers an associate degree in the subject of Navajo.</li>
<li>In December 2024, Navajo Nation President made Navajo language the official language of Navajo Nation.</li>
</ul></li>
</ul>
</section>
<section id="hard-to-parse-in-english" class="level2">
<h2 class="anchored" data-anchor-id="hard-to-parse-in-english">Hard to parse in English</h2>
<blockquote class="blockquote">
<p>Alice drove down the street in her car</p>
<p>– prepositional phrase attachment ambiguity</p>
</blockquote>
<blockquote class="blockquote">
<p>time flies like an arrow. Fruit flies like bananas</p>
<p>– polysemic ambiguity &amp; garden path sentences</p>
</blockquote>
<blockquote class="blockquote">
<p>I drove my car to the hospital in town on Saturday</p>
<p>– Linear projection in English</p>
</blockquote>
<blockquote class="blockquote">
<p>You cannot add flavour to a bean that isn’t there</p>
<p>– Non-linear projection in English</p>
</blockquote>
<blockquote class="blockquote">
<p>Alex went to Sam’s house, where he told her that they would miss his show.</p>
<p>– Coreference resolution ambiguity</p>
</blockquote>
<blockquote class="blockquote">
<p>I saw an elephant yesterday in my pajamas</p>
<p>– Non Projectives in English</p>
</blockquote>
<blockquote class="blockquote">
<p>Bill loves and mary hates soccer</p>
<p>– Non Projectives in English</p>
</blockquote>
</section>
<section id="papers" class="level2">
<h2 class="anchored" data-anchor-id="papers">Papers</h2>
<ul>
<li><ol class="example" type="1">
<li><a href="https://aclanthology.org/2020.emnlp-main.422/">Automatic Extraction of Rules Governing Morphological Agreement</a> EMNLP 2020. <a href="https://slideslive.com/38939038/automatic-extraction-of-rules-governing-morphological-agreement">video</a></li>
</ol>
<ul>
<li>This paper is related to extracting morphological agreement rules using dependency relations.</li>
</ul></li>
<li><ol start="2" class="example" type="1">
<li><a href="https://arxiv.org/abs/1811.00570">On difficulties of cross-lingual transfer with order differences: A case study on dependency parsing</a></li>
</ol>
<ul>
<li>This paper discusses challenges in cross-lingual transfer due to word order differences.</li>
</ul></li>
<li><ol start="3" class="example" type="1">
<li><a href="https://arxiv.org/abs/1906.02656">Cross-lingual syntactic transfer through unsupervised adaptation of invertible projections</a></li>
</ol>
<ul>
<li>This paper is about cross-lingual syntactic transfer using unsupervised adaptation of invertible projections.</li>
</ul></li>
<li><ol start="4" class="example" type="1">
<li><a href="https://arxiv.org/abs/1909.01482">Target language-aware constrained inference for cross-lingual dependency parsing</a>.”</li>
</ol>
<ul>
<li>This paper focuses on target language-aware constrained inference for cross-lingual dependency parsing.</li>
</ul></li>
<li><ol start="5" class="example" type="1">
<li><a href="https://github.com/jungyeul/chu-liu-1965/tree/main?tab=readme-ov-file">On the Shortest Arborescence of a Directed Graph</a></li>
</ol></li>
<li><ol start="6" class="example" type="1">
<li><a href="https://nvlpubs.nist.gov/nistpubs/jres/71B/jresv71Bn4p233_A1b.pdf">Optimum Branchings*</a></li>
</ol>
<ul>
<li>These papers are related to the <a href="https://en.wikipedia.org/wiki/Edmonds%27_algorithm">Chu-Liu-Edmonds algorithm</a>.</li>
</ul></li>
<li><span class="citation" data-cites="kiperwasser2016simpleaccuratedependencyparsing">Kiperwasser and Goldberg (2016)</span> <a href="https://arxiv.org/abs/1603.04351">Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations</a>
<ul>
<li>This paper concerns Sequence Model Feature Extractors.</li>
</ul></li>
<li><ol start="7" class="example" type="1">
<li><a href="https://arxiv.org/abs/1611.01734">Deep Biaffine Attention for Neural Dependency Parsing</a>**</li>
</ol>
<ul>
<li>This paper discusses the BiAffine Classifier.</li>
</ul></li>
<li><span class="citation" data-cites="yamada-matsumoto-2003-statistical">Yamada and Matsumoto (2003)</span> <a href="https://aclanthology.org/W03-3023/">Statistical Dependency Analysis with Support Vector Machines</a>
<ul>
<li>These paper describes Arc Standard Shift-Reduce Parsing.</li>
</ul></li>
<li><ol start="8" class="example" type="1">
<li><a href="https://aclanthology.org/W03-3017/">An Efficient Algorithm for Projective Dependency Parsing</a>
<ul>
<li>These paper describes Arc Standard Shift-Reduce Parsing.</li>
</ul></li>
</ol></li>
<li><ol start="9" class="example" type="1">
<li><a href="https://arxiv.org/abs/2203.13901">AutoLEX: An Automatic Framework for Linguistic Exploration</a> <a href="https://aditi138.github.io/auto-lex-learn/index.html">project</a></li>
</ol></li>
<li><ol start="10" class="example" type="1">
<li><a href="https://aclanthology.org/2021.emnlp-main.570/">Evaluating the Morphosyntactic Well-formedness of Generated Texts</a> <a href="https://aclanthology.org/2021.emnlp-main.570.mp4">video</a></li>
</ol></li>
<li><ol start="11" class="example" type="1">
<li><a href="https://aclanthology.org/P17-2090/">Data augmentation for low resource neural machine translation</a></li>
</ol></li>
<li><ol start="12" class="example" type="1">
<li><a href="https://aclanthology.org/D19-1143/">Handling syntactic divergence and low resource translation</a></li>
</ol></li>
</ul>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-kiperwasser2016simpleaccuratedependencyparsing" class="csl-entry">
Kiperwasser, Eliyahu, and Yoav Goldberg. 2016. <span>“Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations.”</span> <a href="https://arxiv.org/abs/1603.04351">https://arxiv.org/abs/1603.04351</a>.
</div>
<div id="ref-yamada-matsumoto-2003-statistical" class="csl-entry">
Yamada, Hiroyasu, and Yuji Matsumoto. 2003. <span>“Statistical Dependency Analysis with Support Vector Machines.”</span> In <em>Proceedings of the Eighth International Conference on Parsing Technologies</em>, 195–206. Nancy, France. <a href="https://aclanthology.org/W03-3023/">https://aclanthology.org/W03-3023/</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Syntax and {Parsing}},
  date = {2022-03-29},
  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737/cs11-737-w19-syntax-and-parsing/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Syntax and Parsing.”</span> March 29, 2022.
<a href="https://orenbochman.github.io/notes-nlp/notes/cs11-737/cs11-737-w19-syntax-and-parsing/">https://orenbochman.github.io/notes-nlp/notes/cs11-737/cs11-737-w19-syntax-and-parsing/</a>.
</div></div></section></div> ]]></description>
  <category>Syntax</category>
  <category>Dependency parsing</category>
  <category>Multilingual NLP</category>
  <category>NLP</category>
  <category>Notes</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/cs11-737/cs11-737-w19-syntax-and-parsing/</guid>
  <pubDate>Mon, 28 Mar 2022 21:00:00 GMT</pubDate>
</item>
<item>
  <title>Text-to-speech</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/cs11-737/cs11-737-w16/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/tiling.png" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>course banner</figcaption>
</figure>
</div></div><section id="there-is-no-recording-or-slides-for-this-lecture-and-it-looks-like-it-is-part-of-the-next-weeks-lecture." class="level2">
<h2 class="anchored" data-anchor-id="there-is-no-recording-or-slides-for-this-lecture-and-it-looks-like-it-is-part-of-the-next-weeks-lecture.">There is no recording or slides for this lecture and it looks like it is part of the next week’s lecture.</h2>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Text-to-Speech},
  date = {2022-03-17},
  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737/cs11-737-w16/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Text-to-Speech.”</span> March 17, 2022. <a href="https://orenbochman.github.io/notes-nlp/notes/cs11-737/cs11-737-w16/">https://orenbochman.github.io/notes-nlp/notes/cs11-737/cs11-737-w16/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Multilingual NLP</category>
  <category>NLP</category>
  <category>Notes</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/cs11-737/cs11-737-w16/</guid>
  <pubDate>Wed, 16 Mar 2022 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Automatic Speech Recognition</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/notes-nlp/notes/cs11-737/cs11-737-w14-ASR/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/notes-nlp/images/tiling.png" class="nolightbox img-fluid figure-img" width="200"></p>
<figcaption>course banner</figcaption>
</figure>
</div><div id="vid-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/OObrN8yMYZU" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;1: Lesson Video
</figcaption>
</figure>
</div><div id="sup-slide-deck" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" style="@page {size: 16in 9in;  margin: 0;}">
<figure class="quarto-float quarto-float-sup figure">
<div aria-describedby="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="slides.pdf" class="lightbox" data-gallery="slides" title="This week’s slides"><embed src="slides.pdf" style="@page {size: 16in 9in;  margin: 0;}" width="420" height="340"></a></p>
<figcaption>This week’s slides</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-sup quarto-uncaptioned" id="sup-slide-deck-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Supplementary Figure&nbsp;1
</figcaption>
</figure>
</div></div>

<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Automatic Speech Recognition</li>
<li>ASR models</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- Transcript here is machine generated no punctuation and I eddited it a but but it needs more work and I dont see the benefits unless I can run it through some tool. -->
<blockquote class="blockquote">
<p>Today i’d like to talk about uh automatic speech recognition uh this is uh one of the uh most uh the active uh research area in speech processing and also very important application for multilingual energy and today i will first explain about the speech recognition demonstration uh and also talking about evaluation metrics and the i will a little bit use the mass to explain about the formulation of the speech recognition and they will move to the explanation about the standard a speech recognition pipeline so uh let me first try to work on the demonstration um which i just tried to kind of use speech recognition here i want to go to the cmu campus affect right the technology and the i also want to show how the current sr technology is robust i want to go to the cmu campus oh it doesn’t work if i can work speak slowly i want to go to the cma campus that’s the current technology next i mentioned that the speech recognition is not a good for the noisy environment so let’s try to make the noisy environment i want to go to the cmu campus they are cool right yeah they are very very cool um so i try to kind of uh make some kind of a mistake uh but the google guides are doing a great job so it’s actually not easy to find some kind of a significant mistake with them one of the easy mistake that i can uh that try to make could be the the other query okay right yeah they should you know study more about me i will talk to my colleagues in google and the the other difficulty would be let me try so i mentioned that the speech recognition is not very good under the uh the simple noise it’s actually quite strong but if there are some uh the other speakers are speaking it’s still very challenging so i try to make this kind of situation okay so do you have you have a long like it’s not not if there’s scabbards raised in new york but i guess up there you all don’t have too long where we’re going i want to go to the cmu campus i want to go to the cmu campus but it’s actually not working quite well in general so um this is a current technology uh with some more other essays with the controlled environment the speech recognition is quite working well and today’s talk uh i will talk about the introduce about the speech recognition technology insider okay so uh the the first example i already mentioned this is one of the mistake that i found when i use acidity uh and yeah sometimes very good but sometimes it’s not working and the uh what kind of errors the speech equation has would be today’s uh discussion so please discuss it in the rest of the class after my lecture and the the now the many of the speech recognition engines actually also support the the other languages and i also uh try the uh japanese and it’s working quite well uh it i tried several times and then i found this kind of uh the the mistaken mistake but in general uh other uh language are also working quite well now and now i will uh move to the uh evaluation as you can see sometimes it’s perfectly revoking but sometimes it’s not working right and then the uh how to evaluate the major uh in speech recognition is quite important and the first uh the uh most intuitive measure would be the sentence level error anyway you know the the sentence uh uh correct or not i think it is the most other important measure i would say uh however uh by measuring the performance of the system in this case it’s almost correct right it’s just there are a few words are uh the wrong so i we actually want to give some partial score uh if the half of the rods are correct so to do that uh instead of using the sentence error rate uh we usually use the water rate and whatever it is computed by considering the three types of the error one is the insertion errors in this case uh this uh word is actually uh the the extra word compared from the difference so this uh the word error correspond to the insertion error and then the rest of the uh the error is the correspond to the substitution errors by the way it’s it is not unique of course you know uh we can consider this substitution uh this one is a substitution and the decision that other insertion uh and so on and usually we uh consider all the possible other error and then taking the minimum by also considering the each other cost of each error and then compute the edit distance i think many people here would be familiar with the distance so i may not need to explain about it indeed so much detail but anyway this uh is our kind of unusual uh uh measure to uh check the performance of the speech recognition and the um so this is a quite well defined major when we have a wide boundary but as you can see we have a lot of work uh the languages which actually don’t have a word boundary right and how to compute it there are uh two ways one is just using some uh the uh the tools to segment uh to get the gun in the world are you need and then other computing the model rate but we should be very careful about that because this chunking may depend on the tool or dictionary and so on i had a very bad experience that i had compared my japanese system and other other institute other systems and i are actually quite bad and i was very disappointed but i couldn’t reach to their numbers and finally we asked them how to chunk their word and their tools are different from ours and we actually cannot compare right so it is the other issue when we using the tanking but still uh if we use the same chunking we can compare so this is widely used but the mostly used uh the metallic for such kind of languages is actually character uh error rate just consider the the each character and the other computer uh edit distance so this is uh the more widely used when we uh the compute measure for the languages that doesn’t have hardware what boundary so the uh the assignment three that first you guys speak the uh the language right and then do the asl and do the evaluation be careful about the uh the which measure are are you guys using if the that language has a good word boundary you guys can use what uh error rate but otherwise a character error is safe yes when you’re speaking in japanese do you expect the system to transcribe the world cmu enrollment yes actually yes yeah the since uh it’s a kind of our other already the training data and so are mixed so other it’s uh the appeared other other the roman character uh in the japanese cases yeah japanese cases we have a ton of data so actually this kind of mix mixture will be handled by data delivery manner however are they that i think in some sense you’re right uh for example this cmu which can be also are the written other uh the katakanas sim the script which is the other or the japanese script to the described the word from the foreign countries and this may also happen instead of this kind of roman character happens and then in this case uh the other letter or character both cases it was correct but it’s not regarded as the wrong a mistake so actually uh in this sense the uh is depend on the language but the in some languages the same the world can be uh represented as a different script and then the uh the water right or even characteristic is not the perfect measure but this is a very difficult problem and usually we the normalize to either of the symbols or we actually don’t care so much about that and there are a lot of other metrics um since the the uh the script has an issue like as i mentioned some people use their phoneme literally but this actually lose the semantic other information a lot so uh this is uh used uh to more to uh that check the whether this uh the uh speech is acoustically phonetically uh the correct but it’s not sure it is the semantically uh the correct so it’s not so much used but anyway this is also another major prime error rate is also sometimes used but i skipped the other explanation and the other metrics decently these two uh the metrics are quite important the airtime factor in the latency since uh many of the systems are now in the uh the on the device and so on and the speech uh is used at the interface whatever speech has another application you know the offline archive uh and so on we don’t care about the uh at least latency a real-time factor other smaller may be better but the latest is quite important if you’re using a speech interface and the it’s also depend on the downstream uh evaluation metrics for example uh the if we combine this one uh with the uh the understanding system or if we combine it with the translation system uh of course we should evaluate it with such kind of downstream uh the evaluation method so in the speech transformation cases of of course it can be better if specification performance is better but we just check the group and the uh for speech recognition are the this uh the especially the characteristic of what error rate i recommend you to use the nist speech recognition scoring toolkit how many people knows nist nist is uh the government institute to standardize uh our uh the daily life activities uh including the uh the for example the um atomic clock and that was also uh the uh the standardized by the nist and the nist is actually uh helping us our community to standardize uh such kind of evaluation metric uh in speech and energy processing so uh and then of course if you know we use a different evaluation tools measures and then if this kind of result is different we cannot compare your result with others right so due to this reason i actually recommend you to use the sctk or other standard added the standardized or are the widely used uh the evaluation error evaluation uh metric uh then you know some tools that they did someone just made it to avoid to hover some uh the confusions again if the uh variation metric tools are different it is disaster right okay so uh with this kind of a great help from the uh the nist and so on uh the speech recognition actually uh was well measured in terms of the uh the the water rate or crop uh the kerati so everyone can for example either compare the performance the others are difficult right and they in the other previous lectures graham also discussed about the uh the blue or other evaluation metrics emerging translation and there are some discussions whether patreon is better or not while they’re at the characteristic there’s not so much discussion mostly it’s correct i say there are some kind of exceptions that i mentioned before but it’s mostly correct and also nice part is that this has a very high correlation with the downstream tasks better weather are definitely better in the speech translation right so anyway due to that study and also they start their needs are providing such kind of toolkit so that everyone can compare the error rate uh strictly so uh this is the one reason that the the speech recognition uh that has been studied for a long time with a common benchmark so now that everyone every field we have our other common benchmark and then you know performance major and readable and so on right speech condition actually has a long history of uh other comparing the techniques based on the shared uh the data are the same other evaluation metric and this is a kind of uh the the figure that i often show in this lecture uh this is a switchboard task and as i as i mentioned every month since 1990s up to now 20 to 30 years we still using this data and we still using the same vibration metric so that we can track the performance uh the improvement so this is possibly the one reason that actually deep running uh has been applied to many areas but the speech is actually one of the first area that deep running is applied since uh we can easily uh also fairly show the performance improvement uh based on this other evaluation measure okay uh so yes this area right whether this area is the the what when other i started speed recognition this area is one of the core data so are the any techniques that cannot improve the performance we also cannot uh the the the other further uh the the scale the training data and so on and also the uh uh the the budget big project uh the the uh kind of the the uh in this era for speech recognition uh anyway uh this is the kind of one of the other cold uh age in speech recognition it’s some slight improvement happens like a discriminative training and so on but uh before deep learning and this i i’ll say my most kind of a speech equation history by the way but i when i went to the conference we always talking about you know the method of a get a 0.3 percent well there are the improvement wow that’s a very good psychic only 2.3 or something like that yeah but the uh due to the deep running that’s kind of our are the uh the how they say we saw that this is the strings there and they cannot release to the human performance but thanks to the deep running and the computational uh the the uh the breakthrough gpu uh gpgpu and the strobe uh and also their open source or other other people’s uh did uh the knowledge uh sharing uh the now uh makes the performance to be included better and better okay so one more thing some people say speech recognition this search is easy this is because we have evaluation metric so the other areas it’s not easy because we have to start to make a variation method by ourselves or you know there’s multiple evaluation metric and we have to pick or something like that so instead speech condition is actually regarded as an easy research topic in terms of that we have a fixed evaluation metric so if you guys have a fancy neural network and then get the improvement by one person you can write the paper okay so uh i will move to the uh the uh speech recognition uh uh the oh yeah i need to kind of swap that a little bit using a mathematical formulation of uh speech recognition so first uh speech recognition as i mentioned in the yeah the the several times uh it’s quite uh interesting combustion problem input is completely physical signal wave pressure sound pressure right that’s undergoes the linguistic symbol so physical one becomes the linguistic uh symbol it’s very different right and the input other characteristics is also very different the waveform is gesture other than one sentence it can be like a two to three seconds and if with a final at the 16 kilohertz sampling grade the length will be the the order of ten thousand if we you use a short term fourier transform and other speech features it goes to the hundred dollar but still the bit long hundreds of thousands that only goes to the uh the three lengths of the symbols so uh a word uh in the vocabulary so this other conversion uh the from input and output is the i’ll say quite different and this actually makes the problem quite difficult okay so now i try to kind of explain that how other the speech function is realized one by one so the google uh the demonstration you guys just see that this is a one box right but it’s actually inside there are several books first one is the feature extraction this is i think i don’t have to mention about that so much about it any of the pattern recognition machine learning problem we first have our study the the feature extraction right and in speech anyway a waveform is not easy to deal with so instead we’re using the feature extraction called the uh mhcc i have a two more slides explaining this each of the modules so i will a bit more detail about it but anyway from now on i will start from the feature which is you know continuous vector time series of the continuous vector and then add a mapping to the word sequence how to formulate it one way is we just add hopefully uh making it a regression problem but instead the uh the people uh actually uh the the the formulate this problem uh more mathematically rigorous other ways they started to uh formulate this problem as a map decision theory here the posterior is from pw given node so the posterior probability of the word sequence are given the observation and then among all the kind of word sequence we just try to pick up the most likely uh sequence that’s because it becomes a speech condition uh quite simple right and the the the problem issue is how to obtain this pw given row so this is the kind of others the the speed recognition uh problem that we usually use for the probabilistic formulation and the just couple of the rules that we usually use i yeah if i have time i will explain bit more carefully about this one but anyway i just want to mention that why people using a probabilistic formulation there are a lot of reason but one of the reason is that we don’t actually have to remember various kind of equations we just have a three equation product rule sum rule conditional independence assumption conditional independence assumption is not rule but maybe just including this vector as a rule but by only using these three uh basic uh uh probabilistic rules we can actually other make this uh p double given no problem bit more tractable so the first thing that the people may often see for the speech recognition is to use a base rule to change this pw given o to p o w put p or given w p w divided by p o and then since the p o is not uh the uh depending on the w we actually uh the uh use the p o dot given w and the w so uh the two uh do to derive this one uh which rule other did we use from here to here mainly so yeah product rule yes so by using the product rule uh we kind of changing the problem uh from the original posterior distribution to the right grip and the prior distribution uh this is uh the methodology is called noisy channel model and the people actually using this method but is that enough to solve the problem for me it doesn’t actually change the difficulty or even that looks like it’s more difficult right and then the uh the how to uh the the make this uh problem more productive we actually uh the using the additional information so speech to text anyway this is a very different uh the conversion we want to have something between what they we can introduce like our linguistic knowledge we can use phoneme right and then the uh this is you know a little bit easier right from all to directory uh predicting the words are kind of difficult but by using the phoneme intermediate representation uh each kind of conversion is a bit sub problem and this is easier so this kind of a methodology is quite important to solve the very difficult combining problem okay so now we will we have our phoneme seekers let’s use the following sentence how to introduce this phoneme sequence in this uh the probability distribution some people may answer if either they took my course in the speech question and understanding which one we use some rule product rule coordination independence assumption it’s actually summer right some rule is great we actually can introduce the additional variable right still this is doesn’t change anything it doesn’t change the difficulties so how do the further other changes at this problem we just using the product rule like i mentioned before it’s part of the base rule but by using the product rule we can further factorize this problem to the three distribution and one the the other distribution in the denominator but this is not related to the our other optimization problem so we can safely actually ignore it right is that everything it’s actually not this is just equivalent uh the conversion right it actually doesn’t change the difficulty how to make the program more simple we’re using the conditional independence assumption for this case it’s where we use the condition of some individual functions here these are reasonable assumptions right the the relationship of the obligation speech features only depend on the election through the lexicon volume than the one it’s uh it’s a difficult approximation but it’s reasonable i would say right so we actually using this conditional independence assumption everywhere to make the problem attractable for example the acoustic model we first apply the conditional independence function hidden markov model is one of the conditional independence assumptions to make it tractable and uh we use a innogram language model now we use the neural language model which actually doesn’t have that but they used to use the endogram language model this is also conditional independence assumption so by using that we actually making this problem uh attractable and this methodology is quite rather powerful actually it’s not only used for speech recognition by the way this is also used for the machine translation as well you know before neural machine translation comes so uh actually the ibm uh is uh the the same other data the group same division are the proposed both speech recognition uh and the machine translation in this kind of a statistical form okay so now uh i uh decompose this uh the problem to the three distribution right this is actually structure uh that we uh that are solving the speech recognition so the first part feature extraction again this is not included in the probabilistic representation the first one is acoustic modeling lexicon language model it sounds like we just combined some kind of uh other sub problems right but it’s actually mathematically uh well uh decomposed and then we make each of the sub program tractable and then finally combine it based on the uh this uh the the equation this is the mass of the speech recognition or other other the problem of solving the secant sequence model before neural model comes the important concept is factorization to make the kind of problem to be decomposed and the other is its factorization itself doesn’t change the difficulty we also have to have a conditional independence assumption to make the problem practicable so uh i don’t know how you guys feel when i first learned this one i saw that this is very elegant the first thing that i just learned the speech recognition is these four components oh my god it’s just a complicated you know there are some modules that are combined to make a speech equation sounds like you know very cool but but a bit necessary but it turns out that it has a quite beautiful theory to other the original target is the base decision theory and i introduced each of the sub modules uh based on the uh the probabilistic formulation and actually at the in the uh the following uh the slide i will talk about each of the modules a little bit more but usually i skip the details and if you guys want to know more about each of the module and so on uh or please also consider to take a speech operation and understanding courses in the full semester okay um maybe i can accept one of a few questions here if not i can move to the each of the pipeline uh quickly um the first part uh feature extraction this is before you know the goes to the probabilistic model the feature extraction we use our other signal processing techniques uh to convert the waveform to the male frequency capture question mscc or other other features and i just want to think uh the multilingual energy this process is mostly uh the the language independent process as you can imagine right this is just a signal processing to convert the waveform to more tractable other part other feature so uh this is actually the the result of the conversion from the waveform to the uh ms60 i think most of people could agree the the bottom figures have more patterns right any more patterns so this has been very important uh to make the uh the feature uh to be uh used for the background of the processing uh by the way this signal processing based approach is gradually replaced by the deep running this is also happening now so one is the people using actually other cnn instead of mscg or some other people also using the sales supervisor learning now this is very powerful but the drawback is that this is a learnable learning based approaches so then the probably language independence property will be kind of mitigated to do that next acoustic modeling which converting the speech features to the following the sequence and for this other acoustic modeling uh we using the combination of the hidden markov model with gaussian mixture model or deep neural network and this approach is also the mostly uh language independent because speech features are more likely independent and the phoneme is also if we design well it can be a luggage independent or at least you know not so much other we can make it not so much depend on the languages although it is actually very difficult okay so the hidden markov model is uh the other actually quite important part in this uh the modeling what hidden markov model is doing is actually uh the um quite important role uh in the entire speech definition so as i mentioned the speech recognition the one over the difficulty is the output the symbol and the input uh the languages are very different so we need to make some arrangement uh all of the uh the speech features and the corresponding the following uh the information so in this case you know we don’t know which boundary we should take for each of the funding and to do this kind of alignment problem uh we are the classically uh using the hidden markov model and the hidden markov model is more like a charging of this other making this kind of alignment and given this alignment to provide a data accurate likelihood based on the as gaussian used to be gaussian but now deep neural network that is a kind of acoustic model and by the way this acoustic model part is the most important to get the performance than the other components in general the third module is the english and so on and this part is heavily language dependent so to do that uh we actually need to first access to get the other addiction information and i actually uh the uh usually show you the uh the cmu dictionary uh this is one of the most well-known uh english dictionary uh maintained by here but unfortunately it’s the stava is down now so i cannot make a demonstration uh and so on uh but uh this uh cmu dictionary the english is very lucky because we have a same reaction and then we can build a speech principle system the other language is it’s actually not easy to get that we don’t have so much kind of structure and the accessible dictionary in the other languages however other probably that i use this we dictionary and the if using navigationally we can also somehow get to the information about the other phoneme it’s actually covered many languages like this right by the way it’s not recovered yet not yet okay this is unfortunate so okay so uh by using that we can also uh the uh the use the other maker kind of of this uh dexcom component for each languages and the last part uh is the language model and the language model that we use the engram or a recurrent neural network and usually the first language model will be the the built is that the uh we uh try to kind of uh get the uh the uh how possibly uh that word uh uh given the pronunciation we can hover are the authority that given the kind of other the word sequence uh how possibly we select each language depending on the context that is what language model is doing and this uh the example i actually have these three uh the uh word sequence this is by the way the same go to the in terms of the phoneme and it can be actually go to or go to or go to right and then if for example check if we check the other uh this works journal sentences and check whether this but how many times this button appeared and then we can get the the uh how likely this other phrase appeared right go to appeared 51 times but the others actually are do not appear so this means that this go to will be most likely selected based on the language model right however this uh actually also other has some kind of other issues uh actually uh this pattern still exists in our languages so i actually include the text size from the ten thousand order to the median order and then can’t go to still other the biggest to more than two thousand can you guess how many times go to appears go to appears in the worst journal sentences two times only two times this is a one example it’s not completely good sentence but yeah it’s appearance right and the goal too there how many you you guys can guess it’s actually quite a large number and yeah most cases go too far uh this is the other sentence that the uh the uh the overnight show i saw when i uh search this other two other phrases so if we using the small corpus it doesn’t cover but if using the large coppers we can cover it so this is a kind of power up if using a large purpose we can cover many of the various are the language patterns as much as possible and this part is also language dependent so uh this is the most uh the the the building block of the speech recognition and actually i’d say that it is not easy to build that for you guys each of the components is governed by the different model feature extraction signal processing model acoustic model other pattern recognition machine learning deep learning and so on next come moderate coming from the other computational linguistic or other linguistics language modeling is also come from the nlp or are they now deep learning but anyway the each of the other modules has a different models so it is very difficult to actually develop all of them and also connecting all of this kind of module is also not easy so instead now people are also working on the uh end-to-end speech definition which try to make entire pipeline as a single neural network google’s a demonstration it’s actually they already switched to the end-to-end neural network other companies as far as i know they still didn’t add a seat to the neural network and to the neural network but either by using this pipeline and the uh the so maybe yeah maybe that’s it i want to uh finish my talk so uh the these are summary uh of the speech recognition first feature condition is very well defined problem and also fortunately we have a large corpus so it is quite you often use other other measure for the for our new machine learning algorithm and the factorization and making the problem productive this is a kind of great methodology for us to tackle this problem and the but since uh the this methodology is a bit complicated recently people are also that are the uh they’re the interesting and actually having a lot of development in the end to end a speech equation feature recovered in my next lecture and then this is the main topic or the other assignment three so please uh enjoy the speech recognition and let’s move to the discussion um the uh the priests i think you guys already tried a speech recognition engine by yourself right and then talk about what kind of errors you happened and also apply to the other language and discuss about it and since we don’t have enough time we don’t have our final discussion time just split and then either discuss it and then finish it okay so let’s just</p>
</blockquote>
</div>
</div>
</div>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li><strong>Speech Recognition Demo and Evaluation Metrics</strong>
<ul>
<li>Demonstration of speech recognition.</li>
<li>Discussion of how well it works, and examples of when it fails.</li>
<li>Evaluation metrics.</li>
</ul></li>
<li><strong>Evaluation Metrics</strong>
<ul>
<li>Sentence error rate.
<ul>
<li>Discuss if the entire sentence is correct.</li>
<li>Explain why this is too strict of a measure, and the need to consider local correctness.</li>
</ul></li>
<li>Word error rate (WER).
<ul>
<li>Using edit distance word-by-word.</li>
<li>Calculating error rate percentage.</li>
<li>How to compute WER for languages without word boundaries.</li>
</ul></li>
<li>Other metrics.
<ul>
<li>Phoneme error rate (requires a pronunciation dictionary).</li>
<li>Frame error rate (requires an alignment).</li>
</ul></li>
<li>NIST Speech Recognition Scoring Toolkit (SCTK).</li>
</ul></li>
<li><strong>(A bit) Mathematical Formulation of Speech Recognition</strong>
<ul>
<li>Speech recognition as a conversion from a physical signal to a linguistic symbol.</li>
<li>Explanation of probabilistic formulation.
<ul>
<li>MAP decision theory to estimate the most probable word sequence.</li>
<li>Noisy channel model.</li>
<li>Factoring and conditional independence.</li>
</ul></li>
</ul></li>
<li><strong>Standard Speech Recognition Pipeline</strong>
<ul>
<li>Feature extraction.
<ul>
<li>Converting waveform to MFCC.</li>
<li>Language-independent process.</li>
<li>Desirable representations.</li>
</ul></li>
<li>Acoustic Modeling.
<ul>
<li>Converting speech features to phoneme sequences.</li>
<li>Using Hidden Markov Model (HMM) to align speech features and phoneme sequences.</li>
<li>Language-independent.</li>
</ul></li>
<li>Lexicon.
<ul>
<li>Pronunciation dictionary.</li>
<li>CMU dictionary.</li>
<li>Multilingual phone dictionary.</li>
</ul></li>
<li>Language Model.
<ul>
<li>Using N-grams or recurrent neural networks.</li>
<li>Word selection based on context.</li>
<li>Language-dependent.</li>
</ul></li>
</ul></li>
<li><strong>End-to-end Speech Recognition</strong>
<ul>
<li>Using a single neural network.</li>
<li>A simpler solution for multilingual ASR.</li>
</ul></li>
</ul>
</section>
<section id="papers" class="level2">
<h2 class="anchored" data-anchor-id="papers">Papers</h2>
<ul>
<li><ol class="example" type="1">
<li><a href="http://www.itl.nist.gov/iad/mig/tests/rt/ASRhistory/pdf/NIST_benchmark_ASRtests_2003.pdf">A look at NIST’s benchmark ASR tests: past, present, and future</a></li>
</ol></li>
<li><ol start="2" class="example" type="1">
<li><a href="https://arxiv.org/pdf/1505.05899">The IBM 2015 English Conversational Telephone Speech Recognition System</a></li>
</ol></li>
<li><ol start="3" class="example" type="1">
<li><a href="https://arxiv.org/pdf/1610.05256">Achieving Human Parity in Conversational Speech Recognition</a></li>
</ol></li>
</ul>
</section>
<section id="warlpiri-in-10-minutes" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="warlpiri-in-10-minutes">Warlpiri in 10 minutes</h2>
<div class="page-columns page-full"><p>  </p><div class="no-row-height column-margin column-container"><img src="https://orenbochman.github.io/notes-nlp/notes/cs11-737/cs11-737-w14-ASR/geom.png" id="fig-lit-greo" class="img-fluid" width="400" alt="Geography"><img src="https://orenbochman.github.io/notes-nlp/notes/cs11-737/cs11-737-w14-ASR/consonants.png" id="fig-lit-cons" class="img-fluid" width="400" alt="Consonants"><img src="https://orenbochman.github.io/notes-nlp/notes/cs11-737/cs11-737-w14-ASR/vowels.png" id="fig-lit-vowel" class="img-fluid" width="200" alt="Vowels"></div></div>
<ul>
<li>Spoken in the Northern Territory of Australia by the Warlpiri people.</li>
<li>Approximately 2,500-3,000 native speakers.</li>
<li>One of the largest Aboriginal languages in Australia based on the number of speakers.</li>
<li>One of only 13 indigenous languages in Australia still being passed down to children.</li>
<li>Alternative names include Walbiri and Waljbiri.</li>
</ul>
<p><strong>Language Family &amp; History</strong></p>
<ul>
<li>Pama-Nyungan.</li>
<li>Ngarrkic languages.</li>
<li>The term <em>Jukurrpa,</em> referring to Aboriginal spiritual beliefs, comes from Warlpiri.</li>
<li>A writing system was not developed until the 1970s when the language began to be taught in schools.</li>
</ul>
<p><strong>Grammar</strong></p>
<ul>
<li><strong>Free word order</strong>, but the auxiliary word is almost always the second word in a clause.</li>
<li><strong>Ergative</strong> marking. The actor takes a special ending called the ergative ending. The ergative ending marks the subject of a transitive sentence.</li>
<li><strong>Split ergativity</strong>. Nouns follow one set of rules, while pronouns and auxiliary verbs follow another.</li>
<li>Suffixes indicate person and number of the subject and object.</li>
<li><strong>Vowel harmony</strong>.</li>
</ul>
<p><strong>Phonology</strong></p>
<ul>
<li>Most Warlpiri languages have only <strong>three vowels</strong>.</li>
<li><strong>No voicing contrast</strong>. Aboriginal languages have no contrast between voiced and voiceless consonants. A sound can sound like a ‘p’ or a ‘b’ depending on its position in the word.</li>
<li><strong>No fricative sounds</strong>.</li>
<li>Love the ‘r’ sound. Warlpiri has three ‘r’ sounds.</li>
</ul>
<p><strong>Interesting Linguistic Features</strong></p>
<ul>
<li><strong>Avoidance register</strong>, a special style of language is used between certain family relations that have a drastically reduced lexicon.</li>
<li>Warlpiri Sign Language also exists.</li>
<li>Speakers are often multilingual, learning each other’s languages.</li>
<li>A strong tradition exists of not saying the names or showing images of people who have passed away.</li>
</ul>
<p><strong>Present Status</strong></p>
<ul>
<li>Warlpiri is considered a threatened language because children sometimes respond in English even when spoken to in Warlpiri.</li>
<li>There are efforts to teach the language in schools and create modern terminology.</li>
</ul>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Automatic {Speech} {Recognition}},
  date = {2022-03-03},
  url = {https://orenbochman.github.io/notes-nlp/notes/cs11-737/cs11-737-w14-ASR/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Automatic Speech Recognition.”</span> March
3, 2022. <a href="https://orenbochman.github.io/notes-nlp/notes/cs11-737/cs11-737-w14-ASR/">https://orenbochman.github.io/notes-nlp/notes/cs11-737/cs11-737-w14-ASR/</a>.
</div></div></section></div> ]]></description>
  <category>Attention</category>
  <category>Multilingual NLP</category>
  <category>NLP</category>
  <category>Notes</category>
  <guid>https://orenbochman.github.io/notes-nlp/notes/cs11-737/cs11-737-w14-ASR/</guid>
  <pubDate>Wed, 02 Mar 2022 22:00:00 GMT</pubDate>
</item>
</channel>
</rss>
