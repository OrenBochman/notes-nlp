[
  {
    "objectID": "posts/c1w1/index.html",
    "href": "posts/c1w1/index.html",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nFigure 1\nMy irreverent notes for Week 1 of the Natural Language Processing with Probabilistic Models Course in the Natural Language Processing Specialization Offered by DeepLearning.AI on Coursera.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "posts/c1w1/index.html#sec-supervised-ml-sentiment-analysis",
    "href": "posts/c1w1/index.html#sec-supervised-ml-sentiment-analysis",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "Supervised ML & Sentiment Analysis",
    "text": "Supervised ML & Sentiment Analysis\n\n\n\n\n\n\n\nFigure 6: classification overview\n\n\n\nIn supervised ML we get a dataframe with input features X and their corresponding ground truth label Y.\nThe goal is to minimize prediction error rates, AKA cost.\nTo do this, one runs the prediction function which takes in parameters and map the features of an input to an output label \\hat{Y}.\nThe optimal mapping from features to labels is when the difference between the expected values Y and the predicted values \\hat{Y} is minimized.\nThe cost function F does this by comparing how closely the output \\hat{Y} is to the label Y.\nUpdate the parameters and repeat the whole process until the cost is minimized.\nWe use the Sigmoid cost function:\n\n\n\n\n\n\n\n\nFigure 7: the Sigmoid cost function",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "posts/c1w1/index.html#sec-sentiment-analysis",
    "href": "posts/c1w1/index.html#sec-sentiment-analysis",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\n\n\n\n\n\n\n\nFigure 8: Sentiment Analysis\n\n\n\n\n\n\n\n\nSentiment Analysis - Motivation\n\n\n\n\n\nIf we are passionate about NLP here is a gem to get we started. This is a popular science with many ideas for additional classifiers using pronouns.\n\nTable of logistic regression tutorials\n\n\n\n\n\n\nvideo\nsubject\n\n\n\n\n\nThe Secret Life of Pronouns: James Pennebaker at TEDxAustin\n\n\n\nLanguage of Truth and Lies: I-words\n\n\n\nLIWC-22 2022 Tutorial 1: Getting started with LIWC-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: classification overview\n\n\n\nOne example of a Supervised machine learning classification task for sentiment analysis\nThe objective is to predict whether a tweet has a positive or negative sentiment. (If it is positive/optimistic or negative/pessimistic).\n\nTo perform sentiment analysis on a tweet, we need to:\n\nrepresent the text for example “I am happy because I am learning NLP” as features,\ntrain a logistic regression classifier\n\n\n1 for a positive sentiment\n0 for negative sentiment.\n\n\nand then we use it to classify the text.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "posts/c1w1/index.html#sec-vocabulary-feature-extraction",
    "href": "posts/c1w1/index.html#sec-vocabulary-feature-extraction",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "Vocabulary & Feature Extraction",
    "text": "Vocabulary & Feature Extraction\n\nSparse Representation\n\n\n\n\nProblems with sparse representation\n\nGiven a tweet, or some text, one can represent it as a vector. The vector has a dimension |V|, where V corresponds to the size of the vocabulary size. If we had the tweet “I am happy because I am learning NLP,” then we would put a 1 in the corresponding index for any word in the tweet, and a 0 otherwise.\n\n\n\n\n\n\n\nFigure 10: A sparse representation\n\n\nAs V gets larger, the vector becomes more sparse. Furthermore, we end up having many more features and end up training  \\theta_0 \\ldots \\theta_n  parameters. This results in a larger training time. And the inference time increases as well.\n\n\nFeature Extraction based on class frequencies\n\n\n\nTable 1: Table of tweets\n\n\n\n\n\n\n\n\n\nPositive tweets\nNegative tweets\n\n\n\n\nI am happy because I am learning NLP\nI am sad, I am not learning NLP\n\n\nI am happy\nI am sad\n\n\n\n\n\n\nGiven a corpus with positive and negative tweets we can represent it as follows:\n\n\n\nTable 2: Word class table\n\n\n\n\n\nVocabulary\nPosFreq (1)\nNegFreq (O)\n\n\n\n\nI\n3\n3\n\n\nam\n3\n3\n\n\nhappy\n2\n0\n\n\nbecause\n1\n0\n\n\nlearning\n1\n1\n\n\nNLP\n1\n1\n\n\nsad\n0\n2\n\n\nnot\n0\n1\n\n\n\n\n\n\nfreqs: dictionary mapping from (word, class) to frequency :\n\n\\underbrace{X_m}_{\\textcolor{#7200ac}{\\text{Features of tweet M}}} =\\left[ \\underbrace{1}_{\\textcolor{#126ed5}{\\text{bias}}}\n,\\sum_w \\underbrace{\\textcolor{#da7801}{freqs}(w,\\textcolor{#2db15d}{1})}_{\\textcolor{#931e18}{\\text{Sum Pos.freqs}}} ,\\sum_w \\underbrace{\\textcolor{#da7801}{frequencies}(w,\\textcolor{#931e18}{0})}_{\\textcolor{#2db15d}{\\text{Sum Neg. frequencies}}}\n\\right]\n\\tag{1}\nwe have to encode each tweet as a vector. Previously, this vector was of dimension VV. Now, as we’ll see in the upcoming videos, we’ll represent it with a vector of dimension 33. We create a dictionary to map the word, it class, either positive or negative, to the number of times that word appeared in its corresponding class.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "posts/c1w1/index.html#sec-preprocessing",
    "href": "posts/c1w1/index.html#sec-preprocessing",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "Preprocessing",
    "text": "Preprocessing\n\n\n\n\n\n\n\nFigure 11: preprocessing - feature extraction\n\n\n\n\n\n\n\n\nFigure 12: preprocessing - stemming\n\n\n\nWhen preprocessing, we have to perform the following:\n\nEliminate handles and URLs\nTokenize the string into words.\nRemove stop words like “and, is, a, on, etc.”\nStemming - converting each word to its stem. For example dancer, dancing, danced, becomes ‘danc’. We can use Porter’s Stemmer to take care of this.\nConvert all our words to lower case.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "posts/c1w1/index.html#sec-training-logistic-regression",
    "href": "posts/c1w1/index.html#sec-training-logistic-regression",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "Training Logistic Regression",
    "text": "Training Logistic Regression\n{}\nTo train our logistic regression function, we’ll do the following: we initialize our parameter  \\theta , that we can use in you Sigmoid, we then compute the gradient that we’ll use to update \\theta, and then calculate the cost. we keep doing so until good enough\n\n\n\n\n\n\n\nFigure 13: training\n\n\nUsually we keep training until the cost converges. If we were to plot the number of iterations versus the cost, we should see something like this:",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "posts/c1w1/index.html#sec-testing-logistic-regression",
    "href": "posts/c1w1/index.html#sec-testing-logistic-regression",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "Testing Logistic Regression",
    "text": "Testing Logistic Regression\n\n\n\n\n\n\n\nFigure 14: Testing logistic regression\n\n\nTo test our model, we would run a subset of our data, known as the validation set, on our model to get predictions. The predictions are the outputs of the Sigmoid function.\nIf the output is ≥ 0.5, we would assign it to a positive class, otherwise to the negative class.\nTo compute accuracy, we solve the following equation:\n\n\\text{accuracy} = \\sum_i \\frac{\\hat{y}^{(i)}= y^{(i)}_{val}}{m}\n\\tag{2}\nwhere:\nCross validation note:\n\nIn reality, given your X data we would usually split it into three components. X_{train}, X_{val}, X_{test}.\nThe distribution usually varies depending on the size of our data set. However, a 80%, 10%, 10% split usually works.\n\nIn other words, we go over all our training examples, m of them, and then for every prediction, if it was wright we add a one. we then divide by m.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "posts/c1w1/index.html#sec-logistic-regression-cost-function",
    "href": "posts/c1w1/index.html#sec-logistic-regression-cost-function",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "Logistic Regression: Cost Function",
    "text": "Logistic Regression: Cost Function\n\n\n\n\n\n\n\nFigure 15: Cost function for logistic regression\n\n\n\n\n\n\n\n\nSigmoid\n\n\n\nWe should start by developing intuition about how the cost function is designed the way it is. This is important because we’ll meet the sigmoid in Neural Networks, job interviews and so best make a friend of it. In(Hinton 2012) there is the full derivation of Sigmoid cost function. Also the sigmoid and Logistic regression are intimately related - we can’t have one without the other.\n\n\nIn plain english: “The cost function is mean log loss across all training examples”\n\nJ( \\theta)=−\\frac{1}{m} \\sum^m_{i=1}[y^{(i)}\\log h(x^{(i)}, \\theta)+(1 −y^{(i)}) \\log (1−h(x^{(i)}, \\theta))]\n\\tag{3}\nwhere:\n\nm is the count of rows of our training set.\ni indexes a single row in the dataset.\nx^{(i)} is the data for row i.\ny^{(i)} is the ground truth AKA label for rows i.\nh(x^{(i)},\\theta) is the model’s prediction for row i.\n\nWe’ll derive the logistic regression cost function to get the gradients.\nwe can see in the figure:\n\nIf y = 1 and our prediction is close to 0, we get a cost close to  ∞.\nThe same applies when y=0 and we predict ion is close to 1.\nOn the other hand if we get a prediction equal to the label, we get a cost of 0.\n\nIn either, case we are trying to minimize  J(\\theta)\n\n\nMathematical Derivation\nTo see why the cost function is designed that way, let’s take a step back and write up a function that compresses the two cases into one case.\nIf\n\nP(y \\mid x(i), \\theta) =h(x^{(i)}, \\theta)^{y^{(i)}}1−h(x^{(i)}, \\theta)^{1−y^{(i)}}\n\\tag{4}\nThen the likelihood of the data set is given by:\nFrom the preceding, we can see that when y = 1, we get h(x^{(i)}, \\theta)^{y^{(i)}} and when y≈0 the term 1 − h(x^{(i)}, \\theta)^{(1−y^{(i)})}, which makes sense, since the two probabilities equal to 1.\nIn either case, we want to maximize the function h(x^{(i)}, \\theta)^{y(i)} by making it as close to 1 as possible.\nWhen y ≈ 0 , we want the term 1-h(x^{(i)}, \\theta)^{1−y^{(i)}} ≈ 0 which then \\implies  h(x^{(i)}, \\theta)^{y^{(i)}} ≈ 1\nWhen y=1, we want h(x^{(i)}, \\theta)^{y^{(i)}} = 1\nNow we want to find a way to model the entire data set and not just one example. To do so, we’ll define the likelihood as follows:\n\nL(\\theta) = \\prod^m_{i=1} h(\\theta, x^{(i)})^{y^{(i)}} (1−h(\\theta, x^{(i)}))^{(1−y^{(i)})}\n\\tag{5}\nNote that if we mess up the classification of one example, we end up messing up the overall likelihood score, which is exactly what we intended. We want to fit a model to the entire dataset where all data points are related. to\n\n\\lim_{m \\to \\infty} L(\\theta) = 0\n\\tag{6}\nIt goes close to zero, because both h(\\theta, x^{(i)})^{y^{(i)}} and (1−h(\\theta, x^{(i)}))^{(1−y^{(i)})}  are bounded by [0,1]. Since we are trying to maximize  h(\\theta, x^{(i)}) in L(\\theta), we can introduce the log and just maximize the log of the function.\nIntroducing the log, allows us to write the log of a product as the sum of each log. Here are two identities that will come in handy:\n\n  \\log(a*b*c) = \\log(a) + \\log(b) + \\log(c)  \n\nand\n\n  \\log(a^b) = b \\times \\log(a)\n\nGiven the two preceding identities, we can rewrite the equation as follows:   \n  \\begin{align*}  \n    \\max_{ h(x^{(i)},\\theta)}\\log L(\\theta) &= \\log \\prod^m_{i=1}h(x^{(i)}, \\theta)^{y^{(i)}}(1−h(x^{(i)} ,\\theta))^{1−y^{(i)}} \\\\\n    &= \\sum^m_{i=1} \\log h(x^{(i)}, \\theta)^{y^{(i)}} (1−h(x^{(i)}, \\theta)^{1−y^{(i)}})            \\\\\n    &= \\sum^m_{i=1} \\log h(x^{(i)}, \\theta)^{y^{(i)}} + \\log(1−h(x^{(i)}, \\theta)^{1−y^{(i)}} \\\\\n    &= \\sum^m_{i=1} y^{(i)}\\log h(x^{(i)}, \\theta) + (1−y^{(i)}) \\log(1−h(x^{(i)}, \\theta))\n  \\end{align*}\n Hence, we now divide by m, because we want to see the average cost.   \n  \\begin{align*}  \n    \\frac{1}{m} \\sum^m_{i=1} y^{(i)}\\log h(x^{(i)}, \\theta) + (1−y^{(i)}) \\log(1−h(x^{(i)}, \\theta))  \n  \\end{align*}\n\nRemember that we were maximizing h(\\theta, x(i))  in the preceding equation. It turns out that maximizing an equation is the same as minimizing its negative. Think of x^2, feel free to plot it to see that for we yourself. Hence we add a negative sign and we end up minimizing the cost function as follows.\n  \n  \\begin{align*}  \n    J(\\theta)= − \\frac{1}{m} \\sum^m_{i=1} [y^{(i)} \\log h(x^{(i)}, \\theta) + ( 1 − y^{(i)}) \\log ( 1 − h(x^{(i)}, \\theta))]  \n  \\end{align*}\n\nA vectorized implementation is:\n\n\\begin{align*} & h = g(X\\theta)\\newline & J(\\theta)  = \\frac{1}{m} \\cdot \\left(-y^{T}\\log(h)-(1-y)^{T}\\log(1-h)\\right) \\end{align*}",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "posts/c1w1/index.html#logistic-regression-gradient",
    "href": "posts/c1w1/index.html#logistic-regression-gradient",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "Logistic Regression: Gradient",
    "text": "Logistic Regression: Gradient\n\n\\mathbf{w}(t+1) = \\mathbf{w}(t) - \\eta\\nabla E_{in}\n\nFor the case of logistic regression, the gradient of the error measure with respect to the weights, is calculated as:\n\n\\nabla E_{in}\\left(\\mathbf{w}\\right) = -\\frac{1}{N}\\sum\\limits_{n=1}^N \\frac{y_n\\mathbf{x_N}}{1 + \\exp\\left(y_n \\mathbf{w^T}(t)\\mathbf{x_n}\\right)}\n\nLet’s look into the gradient descent in more detail, as the gradient update rule is given without an explicitly derivation.\nThe general form Of gradient descent is defined\n\n  \\begin{align*} &\n    Repeat \\; \\lbrace \\newline & \\; \\theta_j := \\theta_j - \\alpha \\dfrac{\\partial}{\\partial \\theta_j}J(\\theta) \\newline & \\rbrace\n  \\end{align*}\n\nWe can work out the derivative part using Calculus to get:\n\n  \\begin{align*} &\n    Repeat \\; \\lbrace  \\theta_j := \\theta_j - \\frac{\\alpha}{m} \\sum_{i=1}^m ( h(x^{(i)}, \\theta) - y^{(i)}) x_j^{(i)} \\rbrace\n  \\end{align*}\n\nA vectorized formulation \n\\theta_j := \\theta_j - \\frac{\\alpha}{m} X^T ( H(X, \\theta) -Y)\n\n\nPartial derivative of J(\\theta)\n\n\\begin{align*}\n  h(x)'&=\\left(\\frac{1}{1+e^{-x}}\\right)'=\\frac{-(1+e^{-x})'}{(1+e^{-x})^2}=\\frac{-1'-(e^{-x})'}{(1+e^{-x})^2}=\\frac{0-(-x)'(e^{-x})}{(1+e^{-x})^2}=\\frac{-(-1)(e^{-x})}{(1+e^{-x})^2}=\\frac{e^{-x}}{(1+e^{-x})^2} \\newline &=\\left(\\frac{1}{1+e^{-x}}\\right)\\left(\\frac{e^{-x}}{1+e^{-x}}\\right)=h(x)\\left(\\frac{+1-1 + e^{-x}}{1+e^{-x}}\\right)=h(x)\\left(\\frac{1 + e^{-x}}{1+e^{-x}} - \\frac{1}{1+e^{-x}}\\right)=h(x)(1 - h(x))\n\\end{align*}\n\n\n\\begin{align*}\n\\frac{\\partial}{\\partial \\theta_j} J(\\theta) &= \\frac{\\partial}{\\partial \\theta_j} \\frac{-1}{m}\\sum_{i=1}^m \\left [ y^{(i)} log ( h(x^{(i)}, \\theta) ) + (1-y^{(i)}) log (1 -  h(x^{(i)}, \\theta)) \\right ] \\newline\n&= - \\frac{1}{m}\\sum_{i=1}^m \\left [     y^{(i)} \\frac{\\partial}{\\partial \\theta_j} log ( h(x^{(i)}, \\theta))   + (1-y^{(i)}) \\frac{\\partial}{\\partial \\theta_j} log (1 -  h(x^{(i)}, \\theta))\\right ] \\newline\n&= - \\frac{1}{m}\\sum_{i=1}^m \\left [ \\frac{y^{(i)} \\frac{\\partial}{\\partial \\theta_j}  h(x^{(i)}, \\theta)}{ h(x^{(i)}, \\theta)} + \\frac{(1-y^{(i)})\\frac{\\partial}{\\partial \\theta_j} (1 -  h(x^{(i)}, \\theta))}{1 -  h(x^{(i)}, \\theta)}\\right ] \\newline\n&= - \\frac{1}{m}\\sum_{i=1}^m \\left [ \\frac{y^{(i)} \\frac{\\partial}{\\partial \\theta_j}  h(x^{(i)}, \\theta)}{ h(x^{(i)}, \\theta)} + \\frac{(1-y^{(i)})\\frac{\\partial}{\\partial \\theta_j} (1 -  h(x^{(i)}, \\theta))}{1 -  h(x^{(i)}, \\theta)}\\right ] \\newline\n&= - \\frac{1}{m}\\sum_{i=1}^m \\left [ \\frac{y^{(i)}  h(x^{(i)}, \\theta) (1 -  h(x^{(i)}, \\theta)) \\frac{\\partial}{\\partial \\theta_j} \\theta^T x^{(i)}}{ h(x^{(i)}, \\theta)}   + \\frac{- (1-y^{(i)})  h(x^{(i)}, \\theta)(1 -  h(x^{(i)}, \\theta)) \\frac{\\partial}{\\partial \\theta_j} \\theta^T x^{(i)}}{1 -  h(x^{(i)}, \\theta)}\\right ] \\newline\n&= - \\frac{1}{m}\\sum_{i=1}^m \\left [ \\frac{y^{(i)}  h(x^{(i)}, \\theta) (1 -  h(x^{(i)}, \\theta)) \\frac{\\partial}{\\partial \\theta_j} \\theta^T x^{(i)}}{ h(x^{(i)}, \\theta)} - \\frac{(1-y^{(i)}) h(x^{(i)}, \\theta) (1 -  h(x^{(i)}, \\theta)) \\frac{\\partial}{\\partial \\theta_j} \\theta^T x^{(i)}}{1 -  h(x^{(i)}, \\theta)}\\right ] \\newline\n&= - \\frac{1}{m}\\sum_{i=1}^m \\left [ y^{(i)} (1 -  h(x^{(i)}, \\theta)) x^{(i)}_j - (1-y^{(i)})  h(x^{(i)}, \\theta) x^{(i)}_j\\right ] \\newline\n&= - \\frac{1}{m}\\sum_{i=1}^m \\left [ y^{(i)} (1 -  h(x^{(i)}, \\theta)) - (1-y^{(i)})  h(x^{(i)}, \\theta) \\right ] x^{(i)}_j \\newline\n&= - \\frac{1}{m}\\sum_{i=1}^m \\left [ y^{(i)} - y^{(i)}  h(x^{(i)}, \\theta) -  h(x^{(i)}, \\theta) + y^{(i)}  h(x^{(i)}, \\theta) \\right ] x^{(i)}_j \\newline\n&= - \\frac{1}{m}\\sum_{i=1}^m \\left [ y^{(i)} -  h(x^{(i)}, \\theta) \\right ] x^{(i)}_j \\newline\n&= \\frac{1}{m}\\sum_{i=1}^m \\left [ h(x^{(i)}, \\theta) - y^{(i)} \\right ] x^{(i)}_j\n\\end{align*}\n\\tag{7}\nFirst calculate derivative Of Sigmoid function (it be useful while finding partial derivative Of Note that we computed the partial derivative Of the Sigmoid function If We Were to derive , 9) with respect to O_j, we would get —\nNote that used the chain rule there. We multiply by the derivative Of with respect to Now we are ready to find out resulting partial derivative\nThe Vectorized Version:\n\n\\nabla J(\\theta) = \\frac{1}{m} X^T \\cdot (H(X,\\theta)-Y)\n\\tag{8}\nCongratulations. we now know the full derivation Of logistic regression.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "posts/c1w1/index.html#resources",
    "href": "posts/c1w1/index.html#resources",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "Resources:",
    "text": "Resources:\n\nDerivative of cost function for Logistic Regression as explained on Math Stack Exchange\nAn Intuitive Explanation of Bayes’ Theorem on Better Explained\n(Chadha 2020) Aman Chadha’s Notes\nIbrahim Jelliti’s Notes",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "posts/c1w1/lab01.html",
    "href": "posts/c1w1/lab01.html",
    "title": "Preprocessing",
    "section": "",
    "text": "course banner\nIn this lab, we will be exploring how to preprocess tweets for sentiment analysis. We will provide a function for preprocessing tweets during this week’s assignment, but it is still good to know what is going on under the hood. By the end of this lecture, you will see how to use the NLTK package to perform a preprocessing pipeline for Twitter datasets."
  },
  {
    "objectID": "posts/c1w1/lab01.html#setup",
    "href": "posts/c1w1/lab01.html#setup",
    "title": "Preprocessing",
    "section": "Setup",
    "text": "Setup\nYou will be doing sentiment analysis on tweets in the first two weeks of this course. To help with that, we will be using the Natural Language Toolkit (NLTK) package, an open-source Python library for natural language processing. It has modules for collecting, handling, and processing Twitter data, and you will be acquainted with them as we move along the course.\nFor this exercise, we will use a Twitter dataset that comes with NLTK. This dataset has been manually annotated and serves to establish baselines for models quickly. Let us import them now as well as a few other libraries we will be using.\n\nimport nltk                                # Python library for NLP\nfrom nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\nimport matplotlib.pyplot as plt            # library for visualization\nimport random                              # pseudo-random number generator"
  },
  {
    "objectID": "posts/c1w1/lab01.html#about-the-twitter-dataset",
    "href": "posts/c1w1/lab01.html#about-the-twitter-dataset",
    "title": "Preprocessing",
    "section": "About the Twitter dataset",
    "text": "About the Twitter dataset\nThe sample dataset from NLTK is separated into positive and negative tweets. It contains 5000 positive tweets and 5000 negative tweets exactly. The exact match between these classes is not a coincidence. The intention is to have a balanced dataset. That does not reflect the real distributions of positive and negative classes in live Twitter streams. It is just because balanced datasets simplify the design of most computational methods that are required for sentiment analysis. However, it is better to be aware that this balance of classes is artificial.\nThe dataset is already downloaded in the Coursera workspace. In a local computer however, you can download the data by doing:\n\n# downloads sample twitter dataset. uncomment the line below if running on a local machine.\n# nltk.download('twitter_samples')\n\nWe can load the text fields of the positive and negative tweets by using the module’s strings() method like this:\n\n# select the set of positive and negative tweets\nall_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')\n\nNext, we’ll print a report with the number of positive and negative tweets. It is also essential to know the data structure of the datasets\n\nprint('Number of positive tweets: ', len(all_positive_tweets))\nprint('Number of negative tweets: ', len(all_negative_tweets))\n\nprint('\\nThe type of all_positive_tweets is: ', type(all_positive_tweets))\nprint('The type of a tweet entry is: ', type(all_negative_tweets[0]))\n\nNumber of positive tweets:  5000\nNumber of negative tweets:  5000\n\nThe type of all_positive_tweets is:  &lt;class 'list'&gt;\nThe type of a tweet entry is:  &lt;class 'str'&gt;\n\n\nWe can see that the data is stored in a list and as you might expect, individual tweets are stored as strings.\nYou can make a more visually appealing report by using Matplotlib’s pyplot library. Let us see how to create a pie chart to show the same information as above. This simple snippet will serve you in future visualizations of this kind of data.\n\n# Declare a figure with a custom size\nfig = plt.figure(figsize=(5, 5))\n\n# labels for the two classes\nlabels = 'Positives', 'Negative'\n\n# Sizes for each slide\nsizes = [len(all_positive_tweets), len(all_negative_tweets)] \n\n# Declare pie chart, where the slices will be ordered and plotted counter-clockwise:\nplt.pie(sizes, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\n\n# Equal aspect ratio ensures that pie is drawn as a circle.\nplt.axis('equal')  \n\n# Display the chart\nplt.show()\n\n([&lt;matplotlib.patches.Wedge at 0x758640264910&gt;,\n  &lt;matplotlib.patches.Wedge at 0x758640265630&gt;],\n [Text(-1.0999999999999959, -9.616505800409723e-08, 'Positives'),\n  Text(1.0999999999999832, 1.9233011600819372e-07, 'Negative')],\n [Text(-0.5999999999999978, -5.2453668002234845e-08, '50.0%'),\n  Text(0.5999999999999908, 1.0490733600446929e-07, '50.0%')])\n\n\n(np.float64(-1.100000000000005),\n np.float64(1.100000000000106),\n np.float64(-1.1),\n np.float64(1.1))"
  },
  {
    "objectID": "posts/c1w1/lab01.html#looking-at-raw-texts",
    "href": "posts/c1w1/lab01.html#looking-at-raw-texts",
    "title": "Preprocessing",
    "section": "Looking at raw texts",
    "text": "Looking at raw texts\nBefore anything else, we can print a couple of tweets from the dataset to see how they look. Understanding the data is responsible for 80% of the success or failure in data science projects. We can use this time to observe aspects we’d like to consider when preprocessing our data.\nBelow, you will print one random positive and one random negative tweet. We have added a color mark at the beginning of the string to further distinguish the two. (Warning: This is taken from a public dataset of real tweets and a very small portion has explicit content.)\n\n# print positive in greeen\nprint('\\033[92m' + all_positive_tweets[random.randint(0,5000)])\n\n# print negative in red\nprint('\\033[91m' + all_negative_tweets[random.randint(0,5000)])\n\nThank you Esai :-)\n♛♛♛\n》》》》 \nI LOVE YOU SO MUCH.\nI BELİEVE THAT HE WİLL FOLLOW.\nPLEASE FOLLOW ME PLEASE JUSTİN @justinbieber :( x15.340\n》》》》ＳＥＥ ＭＥ\n♛♛♛\n\n\nOne observation you may have is the presence of emoticons and URLs in many of the tweets. This info will come in handy in the next steps."
  },
  {
    "objectID": "posts/c1w1/lab01.html#preprocess-raw-text-for-sentiment-analysis",
    "href": "posts/c1w1/lab01.html#preprocess-raw-text-for-sentiment-analysis",
    "title": "Preprocessing",
    "section": "Preprocess raw text for Sentiment analysis",
    "text": "Preprocess raw text for Sentiment analysis\nData preprocessing is one of the critical steps in any machine learning project. It includes cleaning and formatting the data before feeding into a machine learning algorithm. For NLP, the preprocessing steps are comprised of the following tasks:\n\nTokenizing the string\nLowercasing\nRemoving stop words and punctuation\nStemming\n\nThe videos explained each of these steps and why they are important. Let’s see how we can do these to a given tweet. We will choose just one and see how this is transformed by each preprocessing step.\n\n# Our selected sample. Complex enough to exemplify each step\ntweet = all_positive_tweets[2277]\nprint(tweet)\n\nMy beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i\n\n\nLet’s import a few more libraries for this purpose.\n\n# download the stopwords from NLTK\nnltk.download('stopwords')\n\n[nltk_data] Downloading package stopwords to /home/oren/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue\n\n\n\nimport re                                  # library for regular expression operations\nimport string                              # for string operations\n\nfrom nltk.corpus import stopwords          # module for stop words that come with NLTK\nfrom nltk.stem import PorterStemmer        # module for stemming\nfrom nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n\n\nRemove hyperlinks, Twitter marks and styles\nSince we have a Twitter dataset, we’d like to remove some substrings commonly used on the platform like the hashtag, retweet marks, and hyperlinks. We’ll use the re library to perform regular expression operations on our tweet. We’ll define our search pattern and use the sub() method to remove matches by substituting with an empty character (i.e. '')\n\nprint('\\033[92m' + tweet)\nprint('\\033[94m')\n\n# remove old style retweet text \"RT\"\ntweet2 = re.sub(r'^RT[\\s]+', '', tweet)\n\n# remove hyperlinks\ntweet2 = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet2)\n\n# remove hashtags\n# only removing the hash # sign from the word\ntweet2 = re.sub(r'#', '', tweet2)\n\nprint(tweet2)\n\nMy beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i\n\nMy beautiful sunflowers on a sunny Friday morning off :) sunflowers favourites happy Friday off… \n\n\n\n\nTokenize the string\nTo tokenize means to split the strings into individual words without blanks or tabs. In this same step, we will also convert each word in the string to lower case. The tokenize module from NLTK allows us to do these easily:\n\nprint()\nprint('\\033[92m' + tweet2)\nprint('\\033[94m')\n\n# instantiate tokenizer class\ntokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                               reduce_len=True)\n\n# tokenize tweets\ntweet_tokens = tokenizer.tokenize(tweet2)\n\nprint()\nprint('Tokenized string:')\nprint(tweet_tokens)\n\n\nMy beautiful sunflowers on a sunny Friday morning off :) sunflowers favourites happy Friday off… \n\n\nTokenized string:\n['my', 'beautiful', 'sunflowers', 'on', 'a', 'sunny', 'friday', 'morning', 'off', ':)', 'sunflowers', 'favourites', 'happy', 'friday', 'off', '…']\n\n\n\n\nRemove stop words and punctuations\nThe next step is to remove stop words and punctuation. Stop words are words that don’t add significant meaning to the text. You’ll see the list provided by NLTK when you run the cells below.\n\n#Import the english stop words list from NLTK\nstopwords_english = stopwords.words('english') \n\nprint('Stop words\\n')\nprint(stopwords_english)\n\nprint('\\nPunctuation\\n')\nprint(string.punctuation)\n\nStop words\n\n['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n\nPunctuation\n\n!\"#$%&'()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\n\n\nWe can see that the stop words list above contains some words that could be important in some contexts. These could be words like i, not, between, because, won, against. You might need to customize the stop words list for some applications. For our exercise, we will use the entire list.\nFor the punctuation, we saw earlier that certain groupings like ‘:)’ and ‘…’ should be retained when dealing with tweets because they are used to express emotions. In other contexts, like medical analysis, these should also be removed.\nTime to clean up our tokenized tweet!\n\nprint()\nprint('\\033[92m')\nprint(tweet_tokens)\nprint('\\033[94m')\n\ntweets_clean = []\n\nfor word in tweet_tokens: # Go through every word in your tokens list\n    if (word not in stopwords_english and  # remove stopwords\n        word not in string.punctuation):  # remove punctuation\n        tweets_clean.append(word)\n\nprint('removed stop words and punctuation:')\nprint(tweets_clean)\n\n\n\n['my', 'beautiful', 'sunflowers', 'on', 'a', 'sunny', 'friday', 'morning', 'off', ':)', 'sunflowers', 'favourites', 'happy', 'friday', 'off', '…']\n\nremoved stop words and punctuation:\n['beautiful', 'sunflowers', 'sunny', 'friday', 'morning', ':)', 'sunflowers', 'favourites', 'happy', 'friday', '…']\n\n\nPlease note that the words happy and sunny in this list are correctly spelled.\n\n\nStemming\nStemming is the process of converting a word to its most general form, or stem. This helps in reducing the size of our vocabulary.\nConsider the words: * learn * learning * learned * learnt\nAll these words are stemmed from its common root learn. However, in some cases, the stemming process produces words that are not correct spellings of the root word. For example, happi and sunni. That’s because it chooses the most common stem for related words. For example, we can look at the set of words that comprises the different forms of happy:\n\nhappy\nhappiness\nhappier\n\nWe can see that the prefix happi is more commonly used. We cannot choose happ because it is the stem of unrelated words like happen.\nNLTK has different modules for stemming and we will be using the PorterStemmer module which uses the Porter Stemming Algorithm. Let’s see how we can use it in the cell below.\n\nprint()\nprint('\\033[92m')\nprint(tweets_clean)\nprint('\\033[94m')\n\n# Instantiate stemming class\nstemmer = PorterStemmer() \n\n# Create an empty list to store the stems\ntweets_stem = [] \n\nfor word in tweets_clean:\n    stem_word = stemmer.stem(word)  # stemming word\n    tweets_stem.append(stem_word)  # append to the list\n\nprint('stemmed words:')\nprint(tweets_stem)\n\n\n\n['beautiful', 'sunflowers', 'sunny', 'friday', 'morning', ':)', 'sunflowers', 'favourites', 'happy', 'friday', '…']\n\nstemmed words:\n['beauti', 'sunflow', 'sunni', 'friday', 'morn', ':)', 'sunflow', 'favourit', 'happi', 'friday', '…']\n\n\nThat’s it! Now we have a set of words we can feed into to the next stage of our machine learning project."
  },
  {
    "objectID": "posts/c1w1/lab01.html#process_tweet",
    "href": "posts/c1w1/lab01.html#process_tweet",
    "title": "Preprocessing",
    "section": "process_tweet()",
    "text": "process_tweet()\nAs shown above, preprocessing consists of multiple steps before you arrive at the final list of words. We will not ask you to replicate these however. In the week’s assignment, you will use the function process_tweet(tweet) available in utils.py. We encourage you to open the file and you’ll see that this function’s implementation is very similar to the steps above.\nTo obtain the same result as in the previous code cells, you will only need to call the function process_tweet(). Let’s do that in the next cell.\n\nfrom utils import process_tweet # Import the process_tweet function\n\n# choose the same tweet\ntweet = all_positive_tweets[2277]\n\nprint()\nprint('\\033[92m')\nprint(tweet)\nprint('\\033[94m')\n\n# call the imported function\ntweets_stem = process_tweet(tweet); # Preprocess a given tweet\n\nprint('preprocessed tweet:')\nprint(tweets_stem) # Print the result\n\n\n\nMy beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i\n\npreprocessed tweet:\n['beauti', 'sunflow', 'sunni', 'friday', 'morn', ':)', 'sunflow', 'favourit', 'happi', 'friday', '…']\n\n\nThat’s it for this lab! You now know what is going on when you call the preprocessing helper function in this week’s assignment. Hopefully, this exercise has also given you some insights on how to tweak this for other types of text datasets."
  },
  {
    "objectID": "posts/c1w1/assignment.html",
    "href": "posts/c1w1/assignment.html",
    "title": "Assignment 1: Logistic Regression",
    "section": "",
    "text": "course banner\nWelcome to week one of this specialization. You will learn about logistic regression. Concretely, you will be implementing logistic regression for sentiment analysis on tweets. Given a tweet, you will decide if it has a positive sentiment or a negative one. Specifically you will:\nWe will be using a data set of tweets. Hopefully you will get more than 99% accuracy.\nRun the cell below to load in the packages."
  },
  {
    "objectID": "posts/c1w1/assignment.html#import-functions-and-data",
    "href": "posts/c1w1/assignment.html#import-functions-and-data",
    "title": "Assignment 1: Logistic Regression",
    "section": "Import functions and data",
    "text": "Import functions and data\n\n# run this cell to import nltk\nimport nltk\nfrom os import getcwd\n\n\nImported functions\nDownload the data needed for this assignment. Check out the documentation for the twitter_samples dataset.\n\ntwitter_samples: if you’re running this notebook on your local computer, you will need to download it using:\n\nnltk.download('twitter_samples')\n\nstopwords: if you’re running this notebook on your local computer, you will need to download it using:\n\nnltk.download('stopwords')\n\nImport some helper functions that we provided in the utils.py file:\n\nprocess_tweet(): cleans the text, tokenizes it into separate words, removes stopwords, and converts words to stems.\nbuild_freqs(): this counts how often a word in the ‘corpus’ (the entire set of tweets) was associated with a positive label ‘1’ or a negative label ‘0’, then builds the freqs dictionary, where each key is a (word,label) tuple, and the value is the count of its frequency within the corpus of tweets.\n\n\n# add folder, tmp2, from our local workspace containing pre-downloaded corpora files to nltk's data path\n# this enables importing of these files without downloading it again when we refresh our workspace\n\nfilePath = f\"{getcwd()}/../tmp2/\"\nnltk.data.path.append(filePath)\n\n\nimport numpy as np\nimport pandas as pd\nfrom nltk.corpus import twitter_samples \n\nfrom utils import process_tweet, build_freqs\n\n\n\n\nPrepare the data\n\nThe twitter_samples contains subsets of 5,000 positive tweets, 5,000 negative tweets, and the full set of 10,000 tweets.\n\nIf you used all three datasets, we would introduce duplicates of the positive tweets and negative tweets.\n\nYou will select just the five thousand positive tweets and five thousand negative tweets.\n\n\n\n# select the set of positive and negative tweets\nall_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')\n\n\nTrain test split: 20% will be in the test set, and 80% in the training set.\n\n\n# split the data into two pieces, one for training and one for testing (validation set) \ntest_pos = all_positive_tweets[4000:]\ntrain_pos = all_positive_tweets[:4000]\ntest_neg = all_negative_tweets[4000:]\ntrain_neg = all_negative_tweets[:4000]\n\ntrain_x = train_pos + train_neg \ntest_x = test_pos + test_neg\n\n\nCreate the numpy array of positive labels and negative labels.\n\n\n# combine positive and negative labels\ntrain_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\ntest_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)\n\n\n# Print the shape train and test sets\nprint(\"train_y.shape = \" + str(train_y.shape))\nprint(\"test_y.shape = \" + str(test_y.shape))\n\ntrain_y.shape = (8000, 1)\ntest_y.shape = (2000, 1)\n\n\n\nCreate the frequency dictionary using the imported build_freqs() function.\n\nWe highly recommend that you open utils.py and read the build_freqs() function to understand what it is doing.\nTo view the file directory, go to the menu and click File-&gt;Open.\n\n\n    for y,tweet in zip(ys, tweets):\n        for word in process_tweet(tweet):\n            pair = (word, y)\n            if pair in freqs:\n                freqs[pair] += 1\n            else:\n                freqs[pair] = 1\n\nNotice how the outer for loop goes through each tweet, and the inner for loop steps through each word in a tweet.\nThe freqs dictionary is the frequency dictionary that’s being built.\nThe key is the tuple (word, label), such as (“happy”,1) or (“happy”,0). The value stored for each key is the count of how many times the word “happy” was associated with a positive label, or how many times “happy” was associated with a negative label.\n\n\n# create frequency dictionary\nfreqs = build_freqs(train_x, train_y)\n\n# check the output\nprint(\"type(freqs) = \" + str(type(freqs)))\nprint(\"len(freqs) = \" + str(len(freqs.keys())))\n\ntype(freqs) = &lt;class 'dict'&gt;\nlen(freqs) = 11337\n\n\n\nExpected output\ntype(freqs) = &lt;class 'dict'&gt;\nlen(freqs) = 11346\n\n\n\nProcess tweet\nThe given function process_tweet() tokenizes the tweet into individual words, removes stop words and applies stemming.\n\n# test the function below\nprint('This is an example of a positive tweet: \\n', train_x[0])\nprint('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[0]))\n\nThis is an example of a positive tweet: \n #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n\nThis is an example of the processed version of the tweet: \n ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n\n\n\nExpected output\nThis is an example of a positive tweet: \n #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n \nThis is an example of the processes version: \n ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']"
  },
  {
    "objectID": "posts/c1w1/assignment.html#instructions-implement-gradient-descent-function",
    "href": "posts/c1w1/assignment.html#instructions-implement-gradient-descent-function",
    "title": "Assignment 1: Logistic Regression",
    "section": "Instructions: Implement gradient descent function",
    "text": "Instructions: Implement gradient descent function\n\nThe number of iterations num_iters is the number of times that you’ll use the entire training set.\nFor each iteration, you’ll calculate the cost function using all training examples (there are m training examples), and for all features.\nInstead of updating a single weight \\theta_i at a time, we can update all the weights in the column vector:\n\\mathbf{\\theta} = \\begin{pmatrix}\n\\theta_0\n\\\\\n\\theta_1\n\\\\\n\\theta_2\n\\\\\n\\vdots\n\\\\\n\\theta_n\n\\end{pmatrix}\n\\mathbf{\\theta} has dimensions (n+1, 1), where ‘n’ is the number of features, and there is one more element for the bias term \\theta_0 (note that the corresponding feature value \\mathbf{x_0} is 1).\nThe ‘logits’, ‘z’, are calculated by multiplying the feature matrix ‘x’ with the weight vector ‘theta’. z = \\mathbf{x}\\mathbf{\\theta}\n\n\\mathbf{x} has dimensions (m, n+1)\n\\mathbf{\\theta}: has dimensions (n+1, 1)\n\\mathbf{z}: has dimensions (m, 1)\n\nThe prediction ‘h’, is calculated by applying the sigmoid to each element in ‘z’: h(z) = sigmoid(z), and has dimensions (m,1).\nThe cost function J is calculated by taking the dot product of the vectors ‘y’ and ‘log(h)’. Since both ‘y’ and ‘h’ are column vectors (m,1), transpose the vector to the left, so that matrix multiplication of a row vector with column vector performs the dot product. J = \\frac{-1}{m} \\times \\left(\\mathbf{y}^T \\cdot log(\\mathbf{h}) + \\mathbf{(1-y)}^T \\cdot log(\\mathbf{1-h}) \\right)\nThe update of theta is also vectorized. Because the dimensions of \\mathbf{x} are (m, n+1), and both \\mathbf{h} and \\mathbf{y} are (m, 1), we need to transpose the \\mathbf{x} and place it on the left in order to perform matrix multiplication, which then yields the (n+1, 1) answer we need: \\mathbf{\\theta} = \\mathbf{\\theta} - \\frac{\\alpha}{m} \\times \\left( \\mathbf{x}^T \\cdot \\left( \\mathbf{h-y} \\right) \\right)\n\n\n\nHints\n\n\n\n\nuse np.dot for matrix multiplication.\n\n\nTo ensure that the fraction -1/m is a decimal value, cast either the numerator or denominator (or both), like float(1), or write 1. for the float version of 1.\n\n\n\n\n# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef gradientDescent(x, y, theta, alpha, num_iters):\n    '''\n    Input:\n        x: matrix of features which is (m,n+1)\n        y: corresponding labels of the input matrix x, dimensions (m,1)\n        theta: weight vector of dimension (n+1,1)\n        alpha: learning rate\n        num_iters: number of iterations you want to train your model for\n    Output:\n        J: the final cost\n        theta: your final weight vector\n    Hint: you might want to print the cost to make sure that it is going down.\n    '''\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    # get 'm', the number of rows in matrix x\n    m = None\n    \n    for i in range(0, num_iters):\n        \n        # get z, the dot product of x and theta\n        z = None\n        \n        # get the sigmoid of z\n        h = None\n        \n        # calculate the cost function\n        J = None\n\n        # update the weights theta\n        theta = None\n        \n    ### END CODE HERE ###\n    J = float(J)\n    return J, theta\n\n\n# Check the function\n# Construct a synthetic test case using numpy PRNG functions\nnp.random.seed(1)\n# X input is 10 x 3 with ones for the bias terms\ntmp_X = np.append(np.ones((10, 1)), np.random.rand(10, 2) * 2000, axis=1)\n# Y Labels are 10 x 1\ntmp_Y = (np.random.rand(10, 1) &gt; 0.35).astype(float)\n\n# Apply gradient descent\ntmp_J, tmp_theta = gradientDescent(tmp_X, tmp_Y, np.zeros((3, 1)), 1e-8, 700)\nprint(f\"The cost after training is {tmp_J:.8f}.\")\nprint(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(tmp_theta)]}\")\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[15], line 10\n      7 tmp_Y = (np.random.rand(10, 1) &gt; 0.35).astype(float)\n      9 # Apply gradient descent\n---&gt; 10 tmp_J, tmp_theta = gradientDescent(tmp_X, tmp_Y, np.zeros((3, 1)), 1e-8, 700)\n     11 print(f\"The cost after training is {tmp_J:.8f}.\")\n     12 print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(tmp_theta)]}\")\n\nCell In[14], line 34, in gradientDescent(x, y, theta, alpha, num_iters)\n     31     theta = None\n     33 ### END CODE HERE ###\n---&gt; 34 J = float(J)\n     35 return J, theta\n\nTypeError: float() argument must be a string or a real number, not 'NoneType'\n\n\n\n\nExpected output\nThe cost after training is 0.67094970.\nThe resulting vector of weights is [4.1e-07, 0.00035658, 7.309e-05]"
  },
  {
    "objectID": "posts/c1w1/assignment.html#part-2-extracting-the-features",
    "href": "posts/c1w1/assignment.html#part-2-extracting-the-features",
    "title": "Assignment 1: Logistic Regression",
    "section": "Part 2: Extracting the features",
    "text": "Part 2: Extracting the features\n\nGiven a list of tweets, extract the features and store them in a matrix. You will extract two features.\n\nThe first feature is the number of positive words in a tweet.\nThe second feature is the number of negative words in a tweet.\n\nThen train your logistic regression classifier on these features.\nTest the classifier on a validation set.\n\n\nInstructions: Implement the extract_features function.\n\nThis function takes in a single tweet.\nProcess the tweet using the imported process_tweet() function and save the list of tweet words.\nLoop through each word in the list of processed words\n\nFor each word, check the freqs dictionary for the count when that word has a positive ‘1’ label. (Check for the key (word, 1.0)\nDo the same for the count for when the word is associated with the negative label ‘0’. (Check for the key (word, 0.0).)\n\n\n\n\nHints\n\n\n\n\nMake sure you handle cases when the (word, label) key is not found in the dictionary.\n\n\nSearch the web for hints about using the .get() method of a Python dictionary. Here is an  example \n\n\n\n\n# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef extract_features(tweet, freqs):\n    '''\n    Input: \n        tweet: a list of words for one tweet\n        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n    Output: \n        x: a feature vector of dimension (1,3)\n    '''\n    # process_tweet tokenizes, stems, and removes stopwords\n    word_l = process_tweet(tweet)\n    \n    # 3 elements in the form of a 1 x 3 vector\n    x = np.zeros((1, 3)) \n    \n    #bias term is set to 1\n    x[0,0] = 1 \n    \n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    \n    # loop through each word in the list of words\n    for word in word_l:\n        \n        # increment the word count for the positive label 1\n        x[0,1] += None\n        \n        # increment the word count for the negative label 0\n        x[0,2] += None\n        \n    ### END CODE HERE ###\n    assert(x.shape == (1, 3))\n    return x\n\n\n# Check your function\n\n# test 1\n# test on training data\ntmp1 = extract_features(train_x[0], freqs)\nprint(tmp1)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[17], line 5\n      1 # Check your function\n      2 \n      3 # test 1\n      4 # test on training data\n----&gt; 5 tmp1 = extract_features(train_x[0], freqs)\n      6 print(tmp1)\n\nCell In[16], line 25, in extract_features(tweet, freqs)\n     19 ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n     20 \n     21 # loop through each word in the list of words\n     22 for word in word_l:\n     23     \n     24     # increment the word count for the positive label 1\n---&gt; 25     x[0,1] += None\n     27     # increment the word count for the negative label 0\n     28     x[0,2] += None\n\nTypeError: unsupported operand type(s) for +: 'float' and 'NoneType'\n\n\n\n\nExpected output\n[[1.00e+00 3.02e+03 6.10e+01]]\n\n# test 2:\n# check for when the words are not in the freqs dictionary\ntmp2 = extract_features('blorb bleeeeb bloooob', freqs)\nprint(tmp2)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[18], line 3\n      1 # test 2:\n      2 # check for when the words are not in the freqs dictionary\n----&gt; 3 tmp2 = extract_features('blorb bleeeeb bloooob', freqs)\n      4 print(tmp2)\n\nCell In[16], line 25, in extract_features(tweet, freqs)\n     19 ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n     20 \n     21 # loop through each word in the list of words\n     22 for word in word_l:\n     23     \n     24     # increment the word count for the positive label 1\n---&gt; 25     x[0,1] += None\n     27     # increment the word count for the negative label 0\n     28     x[0,2] += None\n\nTypeError: unsupported operand type(s) for +: 'float' and 'NoneType'\n\n\n\n\n\nExpected output\n[[1. 0. 0.]]"
  },
  {
    "objectID": "posts/c1w1/assignment.html#part-3-training-your-model",
    "href": "posts/c1w1/assignment.html#part-3-training-your-model",
    "title": "Assignment 1: Logistic Regression",
    "section": "Part 3: Training Your Model",
    "text": "Part 3: Training Your Model\nTo train the model: * Stack the features for all training examples into a matrix X. * Call gradientDescent, which you’ve implemented above.\nThis section is given to you. Please read it for understanding and run the cell.\n\n# collect the features 'x' and stack them into a matrix 'X'\nX = np.zeros((len(train_x), 3))\nfor i in range(len(train_x)):\n    X[i, :]= extract_features(train_x[i], freqs)\n\n# training labels corresponding to X\nY = train_y\n\n# Apply gradient descent\nJ, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\nprint(f\"The cost after training is {J:.8f}.\")\nprint(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[19], line 4\n      2 X = np.zeros((len(train_x), 3))\n      3 for i in range(len(train_x)):\n----&gt; 4     X[i, :]= extract_features(train_x[i], freqs)\n      6 # training labels corresponding to X\n      7 Y = train_y\n\nCell In[16], line 25, in extract_features(tweet, freqs)\n     19 ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n     20 \n     21 # loop through each word in the list of words\n     22 for word in word_l:\n     23     \n     24     # increment the word count for the positive label 1\n---&gt; 25     x[0,1] += None\n     27     # increment the word count for the negative label 0\n     28     x[0,2] += None\n\nTypeError: unsupported operand type(s) for +: 'float' and 'NoneType'\n\n\n\nExpected Output:\nThe cost after training is 0.24216529.\nThe resulting vector of weights is [7e-08, 0.0005239, -0.00055517]"
  },
  {
    "objectID": "posts/c1w1/assignment.html#check-performance-using-the-test-set",
    "href": "posts/c1w1/assignment.html#check-performance-using-the-test-set",
    "title": "Assignment 1: Logistic Regression",
    "section": "Check performance using the test set",
    "text": "Check performance using the test set\nAfter training your model using the training set above, check how your model might perform on real, unseen data, by testing it against the test set.\n\nInstructions: Implement test_logistic_regression\n\nGiven the test data and the weights of your trained model, calculate the accuracy of your logistic regression model.\nUse your predict_tweet() function to make predictions on each tweet in the test set.\nIf the prediction is &gt; 0.5, set the model’s classification y_hat to 1, otherwise set the model’s classification y_hat to 0.\nA prediction is accurate when y_hat equals test_y. Sum up all the instances when they are equal and divide by m.\n\n\n\nHints\n\n\n\n\nUse np.asarray() to convert a list to a numpy array\n\n\nUse np.squeeze() to make an (m,1) dimensional array into an (m,) array\n\n\n\n\n# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef test_logistic_regression(test_x, test_y, freqs, theta):\n    \"\"\"\n    Input: \n        test_x: a list of tweets\n        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n        freqs: a dictionary with the frequency of each pair (or tuple)\n        theta: weight vector of dimension (3, 1)\n    Output: \n        accuracy: (# of tweets classified correctly) / (total # of tweets)\n    \"\"\"\n    \n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    \n    # the list for storing predictions\n    y_hat = []\n    \n    for tweet in test_x:\n        # get the label prediction for the tweet\n        y_pred = predict_tweet(tweet, freqs, theta)\n        \n        if y_pred &gt; 0.5:\n            # append 1.0 to the list\n            None\n        else:\n            # append 0 to the list\n            None\n\n    # With the above implementation, y_hat is a list, but test_y is (m,1) array\n    # convert both to one-dimensional arrays in order to compare them using the '==' operator\n    accuracy = None\n\n    ### END CODE HERE ###\n    \n    return accuracy\n\n\ntmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\nprint(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[24], line 1\n----&gt; 1 tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n      2 print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")\n\nNameError: name 'theta' is not defined\n\n\n\n\n\nExpected Output:\n0.9950\nPretty good!"
  },
  {
    "objectID": "posts/c2w2/lab02.html",
    "href": "posts/c2w2/lab02.html",
    "title": "Parts-of-Speech Tagging - Working with tags and Numpy",
    "section": "",
    "text": "course banner\n\nIn this lecture notebook you will create a matrix using some tag information and then modify it using different approaches. This will serve as hands-on experience working with Numpy and as an introduction to some elements used for POS tagging.\n\nimport numpy as np\nimport pandas as pd\n\n\nSome information on tags\nFor this notebook you will be using a toy example including only three tags (or states). In a real world application there are many more tags which can be found here.\n\n# Define tags for Adverb, Noun and To (the preposition) , respectively\ntags = ['RB', 'NN', 'TO']\n\nIn this week’s assignment you will construct some dictionaries that provide useful information of the tags and words you will be working with.\nOne of these dictionaries is the transition_counts which counts the number of times a particular tag happened next to another. The keys of this dictionary have the form (previous_tag, tag) and the values are the frequency of occurrences.\nAnother one is the emission_counts dictionary which will count the number of times a particular pair of (tag, word) appeared in the training dataset.\nIn general think of transition when working with tags only and of emission when working with tags and words.\nIn this notebook you will be looking at the first one:\n\n# Define 'transition_counts' dictionary\n# Note: values are the same as the ones in the assignment\ntransition_counts = {\n    ('NN', 'NN'): 16241,\n    ('RB', 'RB'): 2263,\n    ('TO', 'TO'): 2,\n    ('NN', 'TO'): 5256,\n    ('RB', 'TO'): 855,\n    ('TO', 'NN'): 734,\n    ('NN', 'RB'): 2431,\n    ('RB', 'NN'): 358,\n    ('TO', 'RB'): 200\n}\n\nNotice that there are 9 combinations of the 3 tags used. Each tag can appear after the same tag so you should include those as well.\n\n\nUsing Numpy for matrix creation\nNow you will create a matrix that includes these frequencies using Numpy arrays:\n\n# Store the number of tags in the 'num_tags' variable\nnum_tags = len(tags)\n\n# Initialize a 3X3 numpy array with zeros\ntransition_matrix = np.zeros((num_tags, num_tags))\n\n# Print matrix\ntransition_matrix\n\narray([[0., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.]])\n\n\nVisually you can see the matrix has the correct dimensions. Don’t forget you can check this too using the shape attribute:\n\n# Print shape of the matrix\ntransition_matrix.shape\n\n(3, 3)\n\n\nBefore filling this matrix with the values of the transition_counts dictionary you should sort the tags so that their placement in the matrix is consistent:\n\n# Create sorted version of the tag's list\nsorted_tags = sorted(tags)\n\n# Print sorted list\nsorted_tags\n\n['NN', 'RB', 'TO']\n\n\nTo fill this matrix with the correct values you can use a double for loop. You could also use itertools.product to one line this double loop:\n\n# Loop rows\nfor i in range(num_tags):\n    # Loop columns\n    for j in range(num_tags):\n        # Define tag pair\n        tag_tuple = (sorted_tags[i], sorted_tags[j])\n        # Get frequency from transition_counts dict and assign to (i, j) position in the matrix\n        transition_matrix[i, j] = transition_counts.get(tag_tuple)\n\n# Print matrix\ntransition_matrix\n\narray([[1.6241e+04, 2.4310e+03, 5.2560e+03],\n       [3.5800e+02, 2.2630e+03, 8.5500e+02],\n       [7.3400e+02, 2.0000e+02, 2.0000e+00]])\n\n\nLooks like this worked fine. However the matrix can be hard to read as Numpy is more about efficiency, rather than presenting values in a pretty format.\nFor this you can use a Pandas DataFrame. In particular, a function that takes the matrix as input and prints out a pretty version of it will be very useful:\n\n# Define 'print_matrix' function\ndef print_matrix(matrix):\n    print(pd.DataFrame(matrix, index=sorted_tags, columns=sorted_tags))\n\nNotice that the tags are not a parameter of the function. This is because the sorted_tags list will not change in the rest of the notebook so it is safe to use the variable previously declared. To test this function simply run:\n\n# Print the 'transition_matrix' by calling the 'print_matrix' function\nprint_matrix(transition_matrix)\n\n         NN      RB      TO\nNN  16241.0  2431.0  5256.0\nRB    358.0  2263.0   855.0\nTO    734.0   200.0     2.0\n\n\nThat is a lot better, isn’t it?\nAs you may have already deducted this matrix is not symmetrical.\n\n\nWorking with Numpy for matrix manipulation\nNow that you got the matrix set up it is time to see how a matrix can be manipulated after being created.\nNumpy allows vectorized operations which means that operations that would normally include looping over the matrix can be done in a simpler manner. This is consistent with treating numpy arrays as matrices since you get support for common matrix operations. You can do matrix multiplication, scalar multiplication, vector addition and many more!\nFor instance try scaling each value in the matrix by a factor of \\frac{1}{10}. Normally you would loop over each value in the matrix, updating them accordingly. But in Numpy this is as easy as dividing the whole matrix by 10:\n\n# Scale transition matrix\ntransition_matrix = transition_matrix/10\n\n# Print scaled matrix\nprint_matrix(transition_matrix)\n\n        NN     RB     TO\nNN  1624.1  243.1  525.6\nRB    35.8  226.3   85.5\nTO    73.4   20.0    0.2\n\n\nAnother trickier example is to normalize each row so that each value is equal to \\frac{value}{sum \\,of \\,row}.\nThis can be easily done with vectorization. First you will compute the sum of each row:\n\n# Compute sum of row for each row\nrows_sum = transition_matrix.sum(axis=1, keepdims=True)\n\n# Print sum of rows\nrows_sum\n\narray([[2392.8],\n       [ 347.6],\n       [  93.6]])\n\n\nNotice that the sum() method was used. This method does exactly what its name implies. Since the sum of the rows was desired the axis was set to 1. In Numpy axis=1 refers to the columns so the sum is done by summing each column of a particular row, for each row.\nAlso the keepdims parameter was set to True so the resulting array had shape (3, 1) rather than (3,). This was done so that the axes were consistent with the desired operation.\nWhen working with Numpy, always remember to check the shape of the arrays you are working with, many unexpected errors happen because of axes not being consistent. The shape attribute is your friend for these cases.\n\n# Normalize transition matrix\ntransition_matrix = transition_matrix / rows_sum\n\n# Print normalized matrix\nprint_matrix(transition_matrix)\n\n          NN        RB        TO\nNN  0.678745  0.101596  0.219659\nRB  0.102992  0.651036  0.245972\nTO  0.784188  0.213675  0.002137\n\n\nNotice that the normalization that was carried out forces the sum of each row to be equal to 1. You can easily check this by running the sum method on the resulting matrix:\n\ntransition_matrix.sum(axis=1, keepdims=True)\n\narray([[1.],\n       [1.],\n       [1.]])\n\n\nFor a final example you are asked to modify each value of the diagonal of the matrix so that they are equal to the log of the sum of the current row plus the current value. When doing mathematical operations like this one don’t forget to import the math module.\nThis can be done using a standard for loop or vectorization. You’ll see both in action:\n\nimport math\n\n# Copy transition matrix for for-loop example\nt_matrix_for = np.copy(transition_matrix)\n\n# Copy transition matrix for numpy functions example\nt_matrix_np = np.copy(transition_matrix)\n\n\nUsing a for-loop\n\n# Loop values in the diagonal\nfor i in range(num_tags):\n    t_matrix_for[i, i] =  t_matrix_for[i, i] + math.log(rows_sum[i])\n\n# Print matrix\nprint_matrix(t_matrix_for)\n\n          NN        RB        TO\nNN  8.458964  0.101596  0.219659\nRB  0.102992  6.502088  0.245972\nTO  0.784188  0.213675  4.541167\n\n\n/tmp/ipykernel_128834/84584535.py:3: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  t_matrix_for[i, i] =  t_matrix_for[i, i] + math.log(rows_sum[i])\n\n\n\n\nUsing vectorization\n\n# Save diagonal in a numpy array\nd = np.diag(t_matrix_np)\n\n# Print shape of diagonal\nd.shape\n\n(3,)\n\n\nYou can save the diagonal in a numpy array using Numpy’s diag() function. Notice that this array has shape (3,) so it is inconsistent with the dimensions of the rows_sum array which are (3, 1). You’ll have to reshape before moving forward. For this you can use Numpy’s reshape() function, specifying the desired shape in a tuple:\n\n# Reshape diagonal numpy array\nd = np.reshape(d, (3,1))\n\n# Print shape of diagonal\nd.shape\n\n(3, 1)\n\n\nNow that the diagonal has the correct shape you can do the vectorized operation by applying the math.log() function to the rows_sum array and adding the diagonal.\nTo apply a function to each element of a numpy array use Numpy’s vectorize() function providing the desired function as a parameter. This function returns a vectorized function that accepts a numpy array as a parameter.\nTo update the original matrix you can use Numpy’s fill_diagonal() function.\n\n# Perform the vectorized operation\nd = d + np.vectorize(math.log)(rows_sum)\n\n# Use numpy's 'fill_diagonal' function to update the diagonal\nnp.fill_diagonal(t_matrix_np, d)\n\n# Print the matrix\nprint_matrix(t_matrix_np)\n\n          NN        RB        TO\nNN  8.458964  0.101596  0.219659\nRB  0.102992  6.502088  0.245972\nTO  0.784188  0.213675  4.541167\n\n\nTo perform a sanity check that both methods yield the same result you can compare both matrices. Notice that this operation is also vectorized so you will get the equality check for each element in both matrices:\n\n# Check for equality\nt_matrix_for == t_matrix_np\n\narray([[ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True]])\n\n\nCongratulations on finishing this lecture notebook! Now you should be more familiar with some elements used by a POS tagger such as the transition_counts dictionary and with working with Numpy.\nKeep it up!\n\n\n\n\n\nCitationBibTeX citation:@online{2020,\n  author = {},\n  title = {Parts-of-Speech {Tagging} - {Working} with Tags and {Numpy}},\n  date = {2020-10-23},\n  url = {https://orenbochman.github.io/notes-nlp/posts/c2w2/lab02.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“Parts-of-Speech Tagging - Working with Tags and Numpy.”\n2020. October 23, 2020. https://orenbochman.github.io/notes-nlp/posts/c2w2/lab02.html."
  },
  {
    "objectID": "posts/c2w2/assignment.html",
    "href": "posts/c2w2/assignment.html",
    "title": "Assignment 2: Parts-of-Speech Tagging (POS)",
    "section": "",
    "text": "course banner\nWelcome to the second assignment of Course 2 in the Natural Language Processing specialization. This assignment will develop skills in part-of-speech (POS) tagging, the process of assigning a part-of-speech tag (Noun, Verb, Adjective…) to each word in an input text. Tagging is difficult because some words can represent more than one part of speech at different times. They are Ambiguous. Let’s look at the following example:\nDistinguishing the parts-of-speech of a word in a sentence will help you better understand the meaning of a sentence. This would be critically important in search queries. Identifying the proper noun, the organization, the stock symbol, or anything similar would greatly improve everything ranging from speech recognition to search. By completing this assignment, you will:"
  },
  {
    "objectID": "posts/c2w2/assignment.html#outline",
    "href": "posts/c2w2/assignment.html#outline",
    "title": "Assignment 2: Parts-of-Speech Tagging (POS)",
    "section": "Outline",
    "text": "Outline\n\n0 Data Sources\n1 POS Tagging\n\n1.1 Training\n\nExercise 01\n\n1.2 Testing\n\nExercise 02\n\n\n2 Hidden Markov Models\n\n2.1 Generating Matrices\n\nExercise 03\nExercise 04\n\n\n3 Viterbi Algorithm\n\n3.1 Initialization\n\nExercise 05\n\n3.2 Viterbi Forward\n\nExercise 06\n\n3.3 Viterbi Backward\n\nExercise 07\n\n\n4 Predicting on a data set\n\nExercise 08\n\n\n\n# Importing packages and loading in the data set \nfrom utils_pos import get_word_tag, preprocess  \nimport pandas as pd\nfrom collections import defaultdict\nimport math\nimport numpy as np"
  },
  {
    "objectID": "posts/c2w2/assignment.html#0",
    "href": "posts/c2w2/assignment.html#0",
    "title": "Assignment 2: Parts-of-Speech Tagging (POS)",
    "section": "Part 0: Data Sources",
    "text": "Part 0: Data Sources\nThis assignment will use two tagged data sets collected from the Wall Street Journal (WSJ).\nHere is an example ‘tag-set’ or Part of Speech designation describing the two or three letter tag and their meaning.\n\nOne data set (WSJ-2_21.pos) will be used for training.\nThe other (WSJ-24.pos) for testing.\nThe tagged training data has been preprocessed to form a vocabulary (hmm_vocab.txt).\nThe words in the vocabulary are words from the training set that were used two or more times.\nThe vocabulary is augmented with a set of ‘unknown word tokens’, described below.\n\nThe training set will be used to create the emission, transmission and tag counts.\nThe test set (WSJ-24.pos) is read in to create y.\n\nThis contains both the test text and the true tag.\nThe test set has also been preprocessed to remove the tags to form test_words.txt.\nThis is read in and further processed to identify the end of sentences and handle words not in the vocabulary using functions provided in utils_pos.py.\nThis forms the list prep, the preprocessed text used to test our POS taggers.\n\nA POS tagger will necessarily encounter words that are not in its datasets.\n\nTo improve accuracy, these words are further analyzed during preprocessing to extract available hints as to their appropriate tag.\nFor example, the suffix ‘ize’ is a hint that the word is a verb, as in ‘final-ize’ or ‘character-ize’.\nA set of unknown-tokens, such as ‘–unk-verb–’ or ‘–unk-noun–’ will replace the unknown words in both the training and test corpus and will appear in the emission, transmission and tag data structures.\n\n\nImplementation note:\n\nFor python 3.6 and beyond, dictionaries retain the insertion order.\nFurthermore, their hash-based lookup makes them suitable for rapid membership tests.\n\nIf di is a dictionary, key in di will return True if di has a key key, else False.\n\n\nThe dictionary vocab will utilize these features.\n\n# load in the training corpus\nwith open(\"WSJ_02-21.pos\", 'r') as f:\n    training_corpus = f.readlines()\n\nprint(f\"A few items of the training corpus list\")\nprint(training_corpus[0:5])\n\nA few items of the training corpus list\n['In\\tIN\\n', 'an\\tDT\\n', 'Oct.\\tNNP\\n', '19\\tCD\\n', 'review\\tNN\\n']\n\n\n\n# read the vocabulary data, split by each line of text, and save the list\nwith open(\"hmm_vocab.txt\", 'r') as f:\n    voc_l = f.read().split('\\n')\n\nprint(\"A few items of the vocabulary list\")\nprint(voc_l[0:50])\nprint()\nprint(\"A few items at the end of the vocabulary list\")\nprint(voc_l[-50:])\n\nA few items of the vocabulary list\n['!', '#', '$', '%', '&', \"'\", \"''\", \"'40s\", \"'60s\", \"'70s\", \"'80s\", \"'86\", \"'90s\", \"'N\", \"'S\", \"'d\", \"'em\", \"'ll\", \"'m\", \"'n'\", \"'re\", \"'s\", \"'til\", \"'ve\", '(', ')', ',', '-', '--', '--n--', '--unk--', '--unk_adj--', '--unk_adv--', '--unk_digit--', '--unk_noun--', '--unk_punct--', '--unk_upper--', '--unk_verb--', '.', '...', '0.01', '0.0108', '0.02', '0.03', '0.05', '0.1', '0.10', '0.12', '0.13', '0.15']\n\nA few items at the end of the vocabulary list\n['yards', 'yardstick', 'year', 'year-ago', 'year-before', 'year-earlier', 'year-end', 'year-on-year', 'year-round', 'year-to-date', 'year-to-year', 'yearlong', 'yearly', 'years', 'yeast', 'yelled', 'yelling', 'yellow', 'yen', 'yes', 'yesterday', 'yet', 'yield', 'yielded', 'yielding', 'yields', 'you', 'young', 'younger', 'youngest', 'youngsters', 'your', 'yourself', 'youth', 'youthful', 'yuppie', 'yuppies', 'zero', 'zero-coupon', 'zeroing', 'zeros', 'zinc', 'zip', 'zombie', 'zone', 'zones', 'zoning', '{', '}', '']\n\n\n\n# vocab: dictionary that has the index of the corresponding words\nvocab = {} \n\n# Get the index of the corresponding words. \nfor i, word in enumerate(sorted(voc_l)): \n    vocab[word] = i       \n    \nprint(\"Vocabulary dictionary, key is the word, value is a unique integer\")\ncnt = 0\nfor k,v in vocab.items():\n    print(f\"{k}:{v}\")\n    cnt += 1\n    if cnt &gt; 20:\n        break\n\nVocabulary dictionary, key is the word, value is a unique integer\n:0\n!:1\n#:2\n$:3\n%:4\n&:5\n':6\n'':7\n'40s:8\n'60s:9\n'70s:10\n'80s:11\n'86:12\n'90s:13\n'N:14\n'S:15\n'd:16\n'em:17\n'll:18\n'm:19\n'n':20\n\n\n\n# load in the test corpus\nwith open(\"WSJ_24.pos\", 'r') as f:\n    y = f.readlines()\n\nprint(\"A sample of the test corpus\")\nprint(y[0:10])\n\nA sample of the test corpus\n['The\\tDT\\n', 'economy\\tNN\\n', \"'s\\tPOS\\n\", 'temperature\\tNN\\n', 'will\\tMD\\n', 'be\\tVB\\n', 'taken\\tVBN\\n', 'from\\tIN\\n', 'several\\tJJ\\n', 'vantage\\tNN\\n']\n\n\n\n#corpus without tags, preprocessed\n_, prep = preprocess(vocab, \"test.words\")     \n\nprint('The length of the preprocessed test corpus: ', len(prep))\nprint('This is a sample of the test_corpus: ')\nprint(prep[0:10])\n\nThe length of the preprocessed test corpus:  34199\nThis is a sample of the test_corpus: \n['The', 'economy', \"'s\", 'temperature', 'will', 'be', 'taken', 'from', 'several', '--unk--']"
  },
  {
    "objectID": "posts/c2w2/assignment.html#1.1",
    "href": "posts/c2w2/assignment.html#1.1",
    "title": "Assignment 2: Parts-of-Speech Tagging (POS)",
    "section": "Part 1.1 - Training",
    "text": "Part 1.1 - Training\nYou will start with the simplest possible parts-of-speech tagger and we will build up to the state of the art.\nIn this section, you will find the words that are not ambiguous.\n\nFor example, the word is is a verb and it is not ambiguous.\nIn the WSJ corpus, 86% of the token are unambiguous (meaning they have only one tag)\nAbout 14\\% are ambiguous (meaning that they have more than one tag)\n\n\nBefore you start predicting the tags of each word, you will need to compute a few dictionaries that will help you to generate the tables.\n\nTransition counts\n\nThe first dictionary is the transition_counts dictionary which computes the number of times each tag happened next to another tag.\n\nThis dictionary will be used to compute: \nP(t_i |t_{i-1}) \\tag{1}\n\nThis is the probability of a tag at position i given the tag at position i-1.\nIn order for you to compute equation 1, you will create a transition_counts dictionary where - The keys are (prev_tag, tag) - The values are the number of times those two tags appeared in that order.\n\n\nEmission counts\nThe second dictionary you will compute is the emission_counts dictionary. This dictionary will be used to compute:\n\nP(w_i|t_i)\\tag{2}\n\nIn other words, you will use it to compute the probability of a word given its tag.\nIn order for you to compute equation 2, you will create an emission_counts dictionary where - The keys are (tag, word) - The values are the number of times that pair showed up in your training set.\n\n\nTag counts\nThe last dictionary you will compute is the tag_counts dictionary. - The key is the tag - The value is the number of times each tag appeared.\n\n\nExercise 01\nInstructions: Write a program that takes in the training_corpus and returns the three dictionaries mentioned above transition_counts, emission_counts, and tag_counts.\n\nemission_counts: maps (tag, word) to the number of times it happened.\ntransition_counts: maps (prev_tag, tag) to the number of times it has appeared.\ntag_counts: maps (tag) to the number of times it has occured.\n\nImplementation note: This routine utilises defaultdict, which is a subclass of dict.\n\nA standard Python dictionary throws a KeyError if you try to access an item with a key that is not currently in the dictionary.\nIn contrast, the defaultdict will create an item of the type of the argument, in this case an integer with the default value of 0.\nSee defaultdict.\n\n\n# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: create_dictionaries\ndef create_dictionaries(training_corpus, vocab):\n    \"\"\"\n    Input: \n        training_corpus: a corpus where each line has a word followed by its tag.\n        vocab: a dictionary where keys are words in vocabulary and value is an index\n    Output: \n        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts\n        transition_counts: a dictionary where the keys are (prev_tag, tag) and the values are the counts\n        tag_counts: a dictionary where the keys are the tags and the values are the counts\n    \"\"\"\n    \n    # initialize the dictionaries using defaultdict\n    emission_counts = defaultdict(int)\n    transition_counts = defaultdict(int)\n    tag_counts = defaultdict(int)\n    \n    # Initialize \"prev_tag\" (previous tag) with the start state, denoted by '--s--'\n    prev_tag = '--s--' \n    \n    # use 'i' to track the line number in the corpus\n    i = 0 \n    \n    # Each item in the training corpus contains a word and its POS tag\n    # Go through each word and its tag in the training corpus\n    for word_tag in training_corpus:\n        \n        # Increment the word_tag count\n        i += 1\n        \n        # Every 50,000 words, print the word count\n        if i % 50000 == 0:\n            print(f\"word count = {i}\")\n            \n        ### START CODE HERE (Replace instances of 'None' with your code) ###\n        # get the word and tag using the get_word_tag helper function (imported from utils_pos.py)\n        word, tag = None \n        \n        # Increment the transition count for the previous word and tag\n        transition_counts[(prev_tag, tag)] += None\n        \n        # Increment the emission count for the tag and word\n        emission_counts[(tag, word)] += None\n\n        # Increment the tag count\n        tag_counts[tag] += None\n\n        # Set the previous tag to this tag (for the next iteration of the loop)\n        prev_tag = None\n        \n        ### END CODE HERE ###\n        \n    return emission_counts, transition_counts, tag_counts\n\n\nemission_counts, transition_counts, tag_counts = create_dictionaries(training_corpus, vocab)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[8], line 1\n----&gt; 1 emission_counts, transition_counts, tag_counts = create_dictionaries(training_corpus, vocab)\n\nCell In[7], line 38, in create_dictionaries(training_corpus, vocab)\n     34     print(f\"word count = {i}\")\n     36 ### START CODE HERE (Replace instances of 'None' with your code) ###\n     37 # get the word and tag using the get_word_tag helper function (imported from utils_pos.py)\n---&gt; 38 word, tag = None \n     40 # Increment the transition count for the previous word and tag\n     41 transition_counts[(prev_tag, tag)] += None\n\nTypeError: cannot unpack non-iterable NoneType object\n\n\n\n\n# get all the POS states\nstates = sorted(tag_counts.keys())\nprint(f\"Number of POS tags (number of 'states'): {len(states)}\")\nprint(\"View these POS tags (states)\")\nprint(states)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[9], line 2\n      1 # get all the POS states\n----&gt; 2 states = sorted(tag_counts.keys())\n      3 print(f\"Number of POS tags (number of 'states'): {len(states)}\")\n      4 print(\"View these POS tags (states)\")\n\nNameError: name 'tag_counts' is not defined\n\n\n\n\nExpected Output\nNumber of POS tags (number of 'states'46\nView these states\n['#', '$', \"''\", '(', ')', ',', '--s--', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\nThe ‘states’ are the Parts-of-speech designations found in the training data. They will also be referred to as ‘tags’ or POS in this assignment.\n\n“NN” is noun, singular,\n‘NNS’ is noun, plural.\nIn addition, there are helpful tags like ‘–s–’ which indicate a start of a sentence.\nYou can get a more complete description at Penn Treebank II tag set.\n\n\nprint(\"transition examples: \")\nfor ex in list(transition_counts.items())[:3]:\n    print(ex)\nprint()\n\nprint(\"emission examples: \")\nfor ex in list(emission_counts.items())[200:203]:\n    print (ex)\nprint()\n\nprint(\"ambiguous word example: \")\nfor tup,cnt in emission_counts.items():\n    if tup[1] == 'back': print (tup, cnt) \n\ntransition examples: \n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[10], line 2\n      1 print(\"transition examples: \")\n----&gt; 2 for ex in list(transition_counts.items())[:3]:\n      3     print(ex)\n      4 print()\n\nNameError: name 'transition_counts' is not defined\n\n\n\n\n\nExpected Output\ntransition examples: \n(('--s--', 'IN'), 5050)\n(('IN', 'DT'), 32364)\n(('DT', 'NNP'), 9044)\n\nemission examples: \n(('DT', 'any'), 721)\n(('NN', 'decrease'), 7)\n(('NN', 'insider-trading'), 5)\n\nambiguous word example: \n('RB', 'back') 304\n('VB', 'back') 20\n('RP', 'back') 84\n('JJ', 'back') 25\n('NN', 'back') 29\n('VBP', 'back') 4\n\n\n\nPart 1.2 - Testing\nNow you will test the accuracy of your parts-of-speech tagger using your emission_counts dictionary. - Given your preprocessed test corpus prep, you will assign a parts-of-speech tag to every word in that corpus. - Using the original tagged test corpus y, you will then compute what percent of the tags you got correct.\n\n\nExercise 02\nInstructions: Implement predict_pos that computes the accuracy of your model.\n\nThis is a warm up exercise.\nTo assign a part of speech to a word, assign the most frequent POS for that word in the training set.\nThen evaluate how well this approach works. Each time you predict based on the most frequent POS for the given word, check whether the actual POS of that word is the same. If so, the prediction was correct!\nCalculate the accuracy as the number of correct predictions divided by the total number of words for which you predicted the POS tag.\n\n\n# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: predict_pos\n\ndef predict_pos(prep, y, emission_counts, vocab, states):\n    '''\n    Input: \n        prep: a preprocessed version of 'y'. A list with the 'word' component of the tuples.\n        y: a corpus composed of a list of tuples where each tuple consists of (word, POS)\n        emission_counts: a dictionary where the keys are (tag,word) tuples and the value is the count\n        vocab: a dictionary where keys are words in vocabulary and value is an index\n        states: a sorted list of all possible tags for this assignment\n    Output: \n        accuracy: Number of times you classified a word correctly\n    '''\n    \n    # Initialize the number of correct predictions to zero\n    num_correct = 0\n    \n    # Get the (tag, word) tuples, stored as a set\n    all_words = set(emission_counts.keys())\n    \n    # Get the number of (word, POS) tuples in the corpus 'y'\n    total = len(y)\n    for word, y_tup in zip(prep, y): \n\n        # Split the (word, POS) string into a list of two items\n        y_tup_l = y_tup.split()\n        \n        # Verify that y_tup contain both word and POS\n        if len(y_tup_l) == 2:\n            \n            # Set the true POS label for this word\n            true_label = y_tup_l[1]\n\n        else:\n            # If the y_tup didn't contain word and POS, go to next word\n            continue\n    \n        count_final = 0\n        pos_final = ''\n        \n        # If the word is in the vocabulary...\n        if word in vocab:\n            for pos in states:\n\n            ### START CODE HERE (Replace instances of 'None' with your code) ###\n                        \n                # define the key as the tuple containing the POS and word\n                key = None\n\n                # check if the (pos, word) key exists in the emission_counts dictionary\n                if key in None: # complete this line\n\n                # get the emission count of the (pos,word) tuple \n                    count = None\n\n                    # keep track of the POS with the largest count\n                    if None: # complete this line\n\n                        # update the final count (largest count)\n                        count_final = None\n\n                        # update the final POS\n                        pos_final = None\n\n            # If the final POS (with the largest count) matches the true POS:\n            if None: # complete this line\n                \n                # Update the number of correct predictions\n                num_correct += None\n            \n    ### END CODE HERE ###\n    accuracy = num_correct / total\n    \n    return accuracy\n\n\naccuracy_predict_pos = predict_pos(prep, y, emission_counts, vocab, states)\nprint(f\"Accuracy of prediction using predict_pos is {accuracy_predict_pos:.4f}\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[12], line 1\n----&gt; 1 accuracy_predict_pos = predict_pos(prep, y, emission_counts, vocab, states)\n      2 print(f\"Accuracy of prediction using predict_pos is {accuracy_predict_pos:.4f}\")\n\nNameError: name 'emission_counts' is not defined\n\n\n\n\nExpected Output\nAccuracy of prediction using predict_pos is 0.8889\n88.9% is really good for this warm up exercise. With hidden markov models, you should be able to get 95% accuracy."
  },
  {
    "objectID": "posts/c2w2/assignment.html#2.1",
    "href": "posts/c2w2/assignment.html#2.1",
    "title": "Assignment 2: Parts-of-Speech Tagging (POS)",
    "section": "Part 2.1 Generating Matrices",
    "text": "Part 2.1 Generating Matrices\n\nCreating the ‘A’ transition probabilities matrix\nNow that you have your emission_counts, transition_counts, and tag_counts, you will start implementing the Hidden Markov Model.\nThis will allow you to quickly construct the - A transition probabilities matrix. - and the B emission probabilities matrix.\nYou will also use some smoothing when computing these matrices.\nHere is an example of what the A transition matrix would look like (it is simplified to 5 tags for viewing. It is 46x46 in this assignment.):\n\n\n\n\n\n\n\n\n\n\n\n\n\nA\n…\nRBS\nRP\nSYM\nTO\nUH\n…\n\n\n\n\nRBS\n…\n2.217069e-06\n2.217069e-06\n2.217069e-06\n0.008870\n2.217069e-06\n…\n\n\nRP\n…\n3.756509e-07\n7.516775e-04\n3.756509e-07\n0.051089\n3.756509e-07\n…\n\n\nSYM\n…\n1.722772e-05\n1.722772e-05\n1.722772e-05\n0.000017\n1.722772e-05\n…\n\n\nTO\n…\n4.477336e-05\n4.472863e-08\n4.472863e-08\n0.000090\n4.477336e-05\n…\n\n\nUH\n…\n1.030439e-05\n1.030439e-05\n1.030439e-05\n0.061837\n3.092348e-02\n…\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\nNote that the matrix above was computed with smoothing.\nEach cell gives you the probability to go from one part of speech to another.\n\nIn other words, there is a 4.47e-8 chance of going from parts-of-speech TO to RP.\nThe sum of each row has to equal 1, because we assume that the next POS tag must be one of the available columns in the table.\n\nThe smoothing was done as follows:\n\nP(t_i | t_{i-1}) = \\frac{C(t_{i-1}, t_{i}) + \\alpha }{C(t_{i-1}) +\\alpha * N}\\tag{3}\n\n\nN is the total number of tags\nC(t_{i-1}, t_{i}) is the count of the tuple (previous POS, current POS) in transition_counts dictionary.\nC(t_{i-1}) is the count of the previous POS in the tag_counts dictionary.\n\\alpha is a smoothing parameter.\n\n\n\nExercise 03\nInstructions: Implement the create_transition_matrix below for all tags. Your task is to output a matrix that computes equation 3 for each cell in matrix A.\n\n# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: create_transition_matrix\ndef create_transition_matrix(alpha, tag_counts, transition_counts):\n    ''' \n    Input: \n        alpha: number used for smoothing\n        tag_counts: a dictionary mapping each tag to its respective count\n        transition_counts: transition count for the previous word and tag\n    Output:\n        A: matrix of dimension (num_tags,num_tags)\n    '''\n    # Get a sorted list of unique POS tags\n    all_tags = sorted(tag_counts.keys())\n    \n    # Count the number of unique POS tags\n    num_tags = len(all_tags)\n    \n    # Initialize the transition matrix 'A'\n    A = np.zeros((num_tags,num_tags))\n    \n    # Get the unique transition tuples (previous POS, current POS)\n    trans_keys = set(transition_counts.keys())\n    \n    ### START CODE HERE (Replace instances of 'None' with your code) ### \n    \n    # Go through each row of the transition matrix A\n    for i in range(num_tags):\n        \n        # Go through each column of the transition matrix A\n        for j in range(num_tags):\n\n            # Initialize the count of the (prev POS, current POS) to zero\n            count = 0\n        \n            # Define the tuple (prev POS, current POS)\n            # Get the tag at position i and tag at position j (from the all_tags list)\n            key = None\n\n            # Check if the (prev POS, current POS) tuple \n            # exists in the transition counts dictionary\n            if None: #complete this line\n                \n                # Get count from the transition_counts dictionary \n                # for the (prev POS, current POS) tuple\n                count = None\n                \n            # Get the count of the previous tag (index position i) from tag_counts\n            count_prev_tag = None\n            \n            # Apply smoothing using count of the tuple, alpha, \n            # count of previous tag, alpha, and total number of tags\n            A[i,j] = None\n\n    ### END CODE HERE ###\n    \n    return A\n\n\nalpha = 0.001\nA = create_transition_matrix(alpha, tag_counts, transition_counts)\n# Testing your function\nprint(f\"A at row 0, col 0: {A[0,0]:.9f}\")\nprint(f\"A at row 3, col 1: {A[3,1]:.4f}\")\n\nprint(\"View a subset of transition matrix A\")\nA_sub = pd.DataFrame(A[30:35,30:35], index=states[30:35], columns = states[30:35] )\nprint(A_sub)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[14], line 2\n      1 alpha = 0.001\n----&gt; 2 A = create_transition_matrix(alpha, tag_counts, transition_counts)\n      3 # Testing your function\n      4 print(f\"A at row 0, col 0: {A[0,0]:.9f}\")\n\nNameError: name 'tag_counts' is not defined\n\n\n\n\nExpected Output\nA at row 0, col 0: 0.000007040\nA at row 3, col 1: 0.1691\nView a subset of transition matrix A\n              RBS            RP           SYM        TO            UH\nRBS  2.217069e-06  2.217069e-06  2.217069e-06  0.008870  2.217069e-06\nRP   3.756509e-07  7.516775e-04  3.756509e-07  0.051089  3.756509e-07\nSYM  1.722772e-05  1.722772e-05  1.722772e-05  0.000017  1.722772e-05\nTO   4.477336e-05  4.472863e-08  4.472863e-08  0.000090  4.477336e-05\nUH   1.030439e-05  1.030439e-05  1.030439e-05  0.061837  3.092348e-02\n\n\n\nCreate the ‘B’ emission probabilities matrix\nNow you will create the B transition matrix which computes the emission probability.\nYou will use smoothing as defined below:\n\nP(w_i | t_i) = \\frac{C(t_i, word_i)+ \\alpha}{C(t_{i}) +\\alpha * N}\\tag{4}\n\n\nC(t_i, word_i) is the number of times word_i was associated with tag_i in the training data (stored in emission_counts dictionary).\nC(t_i) is the number of times tag_i was in the training data (stored in tag_counts dictionary).\nN is the number of words in the vocabulary\n\\alpha is a smoothing parameter.\n\nThe matrix B is of dimension (num_tags, N), where num_tags is the number of possible parts-of-speech tags.\nHere is an example of the matrix, only a subset of tags and words are shown:\n\nB Emissions Probability Matrix (subset)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nB\n…\n725\nadroitly\nengineers\npromoted\nsynergy\n…\n\n\n\n\nCD\n…\n8.201296e-05\n2.732854e-08\n2.732854e-08\n2.732854e-08\n2.732854e-08\n…\n\n\nNN\n…\n7.521128e-09\n7.521128e-09\n7.521128e-09\n7.521128e-09\n2.257091e-05\n…\n\n\nNNS\n…\n1.670013e-08\n1.670013e-08\n4.676203e-04\n1.670013e-08\n1.670013e-08\n…\n\n\nVB\n…\n3.779036e-08\n3.779036e-08\n3.779036e-08\n3.779036e-08\n3.779036e-08\n…\n\n\nRB\n…\n3.226454e-08\n6.456135e-05\n3.226454e-08\n3.226454e-08\n3.226454e-08\n…\n\n\nRP\n…\n3.723317e-07\n3.723317e-07\n3.723317e-07\n3.723317e-07\n3.723317e-07\n…\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\n\n\nExercise 04\nInstructions: Implement the create_emission_matrix below that computes the B emission probabilities matrix. Your function takes in \\alpha, the smoothing parameter, tag_counts, which is a dictionary mapping each tag to its respective count, the emission_counts dictionary where the keys are (tag, word) and the values are the counts. Your task is to output a matrix that computes equation 4 for each cell in matrix B.\n\n# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: create_emission_matrix\n\ndef create_emission_matrix(alpha, tag_counts, emission_counts, vocab):\n    '''\n    Input: \n        alpha: tuning parameter used in smoothing \n        tag_counts: a dictionary mapping each tag to its respective count\n        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts\n        vocab: a dictionary where keys are words in vocabulary and value is an index.\n               within the function it'll be treated as a list\n    Output:\n        B: a matrix of dimension (num_tags, len(vocab))\n    '''\n    \n    # get the number of POS tag\n    num_tags = len(tag_counts)\n    \n    # Get a list of all POS tags\n    all_tags = sorted(tag_counts.keys())\n    \n    # Get the total number of unique words in the vocabulary\n    num_words = len(vocab)\n    \n    # Initialize the emission matrix B with places for\n    # tags in the rows and words in the columns\n    B = np.zeros((num_tags, num_words))\n    \n    # Get a set of all (POS, word) tuples \n    # from the keys of the emission_counts dictionary\n    emis_keys = set(list(emission_counts.keys()))\n    \n    ### START CODE HERE (Replace instances of 'None' with your code) ###\n    \n    # Go through each row (POS tags)\n    for i in None: # complete this line\n        \n        # Go through each column (words)\n        for j in None: # complete this line\n\n            # Initialize the emission count for the (POS tag, word) to zero\n            count = 0\n                    \n            # Define the (POS tag, word) tuple for this row and column\n            key =  None\n\n            # check if the (POS tag, word) tuple exists as a key in emission counts\n            if None: # complete this line\n        \n                # Get the count of (POS tag, word) from the emission_counts d\n                count = None\n                \n            # Get the count of the POS tag\n            count_tag = None\n                \n            # Apply smoothing and store the smoothed value \n            # into the emission matrix B for this row and column\n            B[i,j] = None\n\n    ### END CODE HERE ###\n    return B\n\n\n# creating your emission probability matrix. this takes a few minutes to run. \nB = create_emission_matrix(alpha, tag_counts, emission_counts, list(vocab))\n\nprint(f\"View Matrix position at row 0, column 0: {B[0,0]:.9f}\")\nprint(f\"View Matrix position at row 3, column 1: {B[3,1]:.9f}\")\n\n# Try viewing emissions for a few words in a sample dataframe\ncidx  = ['725','adroitly','engineers', 'promoted', 'synergy']\n\n# Get the integer ID for each word\ncols = [vocab[a] for a in cidx]\n\n# Choose POS tags to show in a sample dataframe\nrvals =['CD','NN','NNS', 'VB','RB','RP']\n\n# For each POS tag, get the row number from the 'states' list\nrows = [states.index(a) for a in rvals]\n\n# Get the emissions for the sample of words, and the sample of POS tags\nB_sub = pd.DataFrame(B[np.ix_(rows,cols)], index=rvals, columns = cidx )\nprint(B_sub)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[16], line 2\n      1 # creating your emission probability matrix. this takes a few minutes to run. \n----&gt; 2 B = create_emission_matrix(alpha, tag_counts, emission_counts, list(vocab))\n      4 print(f\"View Matrix position at row 0, column 0: {B[0,0]:.9f}\")\n      5 print(f\"View Matrix position at row 3, column 1: {B[3,1]:.9f}\")\n\nNameError: name 'tag_counts' is not defined\n\n\n\n\nExpected Output\nView Matrix position at row 0, column 0: 0.000006032\nView Matrix position at row 3, column 1: 0.000000720\n              725      adroitly     engineers      promoted       synergy\nCD   8.201296e-05  2.732854e-08  2.732854e-08  2.732854e-08  2.732854e-08\nNN   7.521128e-09  7.521128e-09  7.521128e-09  7.521128e-09  2.257091e-05\nNNS  1.670013e-08  1.670013e-08  4.676203e-04  1.670013e-08  1.670013e-08\nVB   3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08\nRB   3.226454e-08  6.456135e-05  3.226454e-08  3.226454e-08  3.226454e-08\nRP   3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07"
  },
  {
    "objectID": "posts/c2w2/assignment.html#3.1",
    "href": "posts/c2w2/assignment.html#3.1",
    "title": "Assignment 2: Parts-of-Speech Tagging (POS)",
    "section": "Part 3.1: Initialization",
    "text": "Part 3.1: Initialization\nYou will start by initializing two matrices of the same dimension.\n\nbest_probs: Each cell contains the probability of going from one POS tag to a word in the corpus.\nbest_paths: A matrix that helps you trace through the best possible path in the corpus.\n\n\nExercise 05\nInstructions: Write a program below that initializes the best_probs and the best_paths matrix.\nBoth matrices will be initialized to zero except for column zero of best_probs.\n\nColumn zero of best_probs is initialized with the assumption that the first word of the corpus was preceded by a start token (“–s–”).\nThis allows you to reference the A matrix for the transition probability\n\nHere is how to initialize column 0 of best_probs:\n\nThe probability of the best path going from the start index to a given POS tag indexed by integer i is denoted by \\textrm{best_probs}[s_{idx}, i].\nThis is estimated as the probability that the start tag transitions to the POS denoted by index i: \\mathbf{A}[s_{idx}, i] AND that the POS tag denoted by i emits the first word of the given corpus, which is \\mathbf{B}[i, vocab[corpus[0]]].\nNote that vocab[corpus[0]] refers to the first word of the corpus (the word at position 0 of the corpus).\nvocab is a dictionary that returns the unique integer that refers to that particular word.\n\nConceptually, it looks like this:\n\\textrm{best_probs}[s_{idx}, i] = \\mathbf{A}[s_{idx}, i] \\times \\mathbf{B}[i, corpus[0] ]\nIn order to avoid multiplying and storing small values on the computer, we’ll take the log of the product, which becomes the sum of two logs:\nbest\\_probs[i,0] = log(A[s_{idx}, i]) + log(B[i, vocab[corpus[0]]])\nAlso, to avoid taking the log of 0 (which is defined as negative infinity), the code itself will just set best\\_probs[i,0] = float('-inf') when A[s_{idx}, i] == 0\nSo the implementation to initialize best\\_probs looks like this:\n$ if A[s_{idx}, i] &lt;&gt; 0 : best_probs[i,0] = log(A[s_{idx}, i]) + log(B[i, vocab[corpus[0]]])$\n$ if A[s_{idx}, i] == 0 : best_probs[i,0] = float(‘-inf’)$\nPlease use math.log to compute the natural logarithm.\nThe example below shows the initialization assuming the corpus starts with the phrase “Loss tracks upward”.\n\nRepresent infinity and negative infinity like this:\nfloat('inf')\nfloat('-inf')\n\n# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: initialize\ndef initialize(states, tag_counts, A, B, corpus, vocab):\n    '''\n    Input: \n        states: a list of all possible parts-of-speech\n        tag_counts: a dictionary mapping each tag to its respective count\n        A: Transition Matrix of dimension (num_tags, num_tags)\n        B: Emission Matrix of dimension (num_tags, len(vocab))\n        corpus: a sequence of words whose POS is to be identified in a list \n        vocab: a dictionary where keys are words in vocabulary and value is an index\n    Output:\n        best_probs: matrix of dimension (num_tags, len(corpus)) of floats\n        best_paths: matrix of dimension (num_tags, len(corpus)) of integers\n    '''\n    # Get the total number of unique POS tags\n    num_tags = len(tag_counts)\n    \n    # Initialize best_probs matrix \n    # POS tags in the rows, number of words in the corpus as the columns\n    best_probs = np.zeros((num_tags, len(corpus)))\n    \n    # Initialize best_paths matrix\n    # POS tags in the rows, number of words in the corpus as columns\n    best_paths = np.zeros((num_tags, len(corpus)), dtype=int)\n    \n    # Define the start token\n    s_idx = states.index(\"--s--\")\n    ### START CODE HERE (Replace instances of 'None' with your code) ###\n    \n    # Go through each of the POS tags\n    for i in None: # complete this line\n        \n        # Handle the special case when the transition from start token to POS tag i is zero\n        if None: # complete this line\n            \n            # Initialize best_probs at POS tag 'i', column 0, to negative infinity\n            best_probs[i,0] = None\n        \n        # For all other cases when transition from start token to POS tag i is non-zero:\n        else:\n            \n            # Initialize best_probs at POS tag 'i', column 0\n            # Check the formula in the instructions above\n            best_probs[i,0] = None\n                        \n    ### END CODE HERE ### \n    return best_probs, best_paths\n\n\nbest_probs, best_paths = initialize(states, tag_counts, A, B, prep, vocab)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[18], line 1\n----&gt; 1 best_probs, best_paths = initialize(states, tag_counts, A, B, prep, vocab)\n\nNameError: name 'states' is not defined\n\n\n\n\n# Test the function\nprint(f\"best_probs[0,0]: {best_probs[0,0]:.4f}\") \nprint(f\"best_paths[2,3]: {best_paths[2,3]:.4f}\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[19], line 2\n      1 # Test the function\n----&gt; 2 print(f\"best_probs[0,0]: {best_probs[0,0]:.4f}\") \n      3 print(f\"best_paths[2,3]: {best_paths[2,3]:.4f}\")\n\nNameError: name 'best_probs' is not defined\n\n\n\n\nExpected Output\nbest_probs[0,0]: -22.6098\nbest_paths[2,3]: 0.0000"
  },
  {
    "objectID": "posts/c2w2/assignment.html#3.2",
    "href": "posts/c2w2/assignment.html#3.2",
    "title": "Assignment 2: Parts-of-Speech Tagging (POS)",
    "section": "Part 3.2 Viterbi Forward (prediction)",
    "text": "Part 3.2 Viterbi Forward (prediction)\nIn this part of the assignment, you will implement the viterbi_forward segment. In other words, you will populate your best_probs and best_paths matrices.\n\nWalk forward through the corpus.\nFor each word, compute a probability for each possible tag.\nUnlike the previous algorithm predict_pos (the ‘warm-up’ exercise), this will include the path up to that (word,tag) combination.\n\nHere is an example with a three-word corpus “Loss tracks upward”:\n\nNote, in this example, only a subset of states (POS tags) are shown in the diagram below, for easier reading.\nIn the diagram below, the first word “Loss” is already initialized.\nThe algorithm will compute a probability for each of the potential tags in the second and future words.\n\nCompute the probability that the tag of the second work (‘tracks’) is a verb, 3rd person singular present (VBZ).\n\nIn the best_probs matrix, go to the column of the second word (‘tracks’), and row 40 (VBZ), this cell is highlighted in light orange in the diagram below.\nExamine each of the paths from the tags of the first word (‘Loss’) and choose the most likely path.\n\nAn example of the calculation for one of those paths is the path from (‘Loss’, NN) to (‘tracks’, VBZ).\nThe log of the probability of the path up to and including the first word ‘Loss’ having POS tag NN is -14.32. The best_probs matrix contains this value -14.32 in the column for ‘Loss’ and row for ‘NN’.\nFind the probability that NN transitions to VBZ. To find this probability, go to the A transition matrix, and go to the row for ‘NN’ and the column for ‘VBZ’. The value is 4.37e-02, which is circled in the diagram, so add -14.32 + log(4.37e-02).\nFind the log of the probability that the tag VBS would ‘emit’ the word ‘tracks’. To find this, look at the ‘B’ emission matrix in row ‘VBZ’ and the column for the word ‘tracks’. The value 4.61e-04 is circled in the diagram below. So add -14.32 + log(4.37e-02) + log(4.61e-04).\nThe sum of -14.32 + log(4.37e-02) + log(4.61e-04) is -25.13. Store -25.13 in the best_probs matrix at row ‘VBZ’ and column ‘tracks’ (as seen in the cell that is highlighted in light orange in the diagram).\nAll other paths in best_probs are calculated. Notice that -25.13 is greater than all of the other values in column ‘tracks’ of matrix best_probs, and so the most likely path to ‘VBZ’ is from ‘NN’. ‘NN’ is in row 20 of the best_probs matrix, so 20 is the most likely path.\nStore the most likely path 20 in the best_paths table. This is highlighted in light orange in the diagram below.\n\nThe formula to compute the probability and path for the i^{th} word in the corpus, the prior word i-1 in the corpus, current POS tag j, and previous POS tag k is:\n\\mathrm{prob} = \\mathbf{best\\_prob}_{k, i-1} + \\mathrm{log}(\\mathbf{A}_{k, j}) + \\mathrm{log}(\\mathbf{B}_{j, vocab(corpus_{i})})\nwhere corpus_{i} is the word in the corpus at index i, and vocab is the dictionary that gets the unique integer that represents a given word.\n\\mathrm{path} = k\nwhere k is the integer representing the previous POS tag.\n\nExercise 06\nInstructions: Implement the viterbi_forward algorithm and store the best_path and best_prob for every possible tag for each word in the matrices best_probs and best_tags using the pseudo code below.\n`for each word in the corpus\nfor each POS tag type that this word may be\n\n    for POS tag type that the previous word could be\n    \n        compute the probability that the previous word had a given POS tag, that the current word has a given POS tag, and that the POS tag would emit this current word.\n        \n        retain the highest probability computed for the current word\n        \n        set best_probs to this highest probability\n        \n        set best_paths to the index 'k', representing the POS tag of the previous word which produced the highest probability `\nPlease use math.log to compute the natural logarithm.\n\n\n\nHints\n\n\n\n\nRemember that when accessing emission matrix B, the column index is the unique integer ID associated with the word. It can be accessed by using the ‘vocab’ dictionary, where the key is the word, and the value is the unique integer ID for that word.\n\n\n\n\n\n# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: viterbi_forward\ndef viterbi_forward(A, B, test_corpus, best_probs, best_paths, vocab):\n    '''\n    Input: \n        A, B: The transition and emission matrices respectively\n        test_corpus: a list containing a preprocessed corpus\n        best_probs: an initilized matrix of dimension (num_tags, len(corpus))\n        best_paths: an initilized matrix of dimension (num_tags, len(corpus))\n        vocab: a dictionary where keys are words in vocabulary and value is an index \n    Output: \n        best_probs: a completed matrix of dimension (num_tags, len(corpus))\n        best_paths: a completed matrix of dimension (num_tags, len(corpus))\n    '''\n    # Get the number of unique POS tags (which is the num of rows in best_probs)\n    num_tags = best_probs.shape[0]\n    \n    # Go through every word in the corpus starting from word 1\n    # Recall that word 0 was initialized in `initialize()`\n    for i in range(1, len(test_corpus)): \n        \n        # Print number of words processed, every 5000 words\n        if i % 5000 == 0:\n            print(\"Words processed: {:&gt;8}\".format(i))\n            \n        ### START CODE HERE (Replace instances of 'None' with your code EXCEPT the first 'best_path_i = None') ###\n        # For each unique POS tag that the current word can be\n        for j in None: # complete this line\n            \n            # Initialize best_prob for word i to negative infinity\n            best_prob_i = None\n            \n            # Initialize best_path for current word i to None\n            best_path_i = None\n\n            # For each POS tag that the previous word can be:\n            for k in None: # complete this line\n            \n                # Calculate the probability = \n                # best probs of POS tag k, previous word i-1 + \n                # log(prob of transition from POS k to POS j) + \n                # log(prob that emission of POS j is word i)\n                prob = None\n\n                # check if this path's probability is greater than\n                # the best probability up to and before this point\n                if None: # complete this line\n                    \n                    # Keep track of the best probability\n                    best_prob_i = None\n                    \n                    # keep track of the POS tag of the previous word\n                    # that is part of the best path.  \n                    # Save the index (integer) associated with \n                    # that previous word's POS tag\n                    best_path_i = None\n\n            # Save the best probability for the \n            # given current word's POS tag\n            # and the position of the current word inside the corpus\n            best_probs[j,i] = None\n            \n            # Save the unique integer ID of the previous POS tag\n            # into best_paths matrix, for the POS tag of the current word\n            # and the position of the current word inside the corpus.\n            best_paths[j,i] = None\n\n        ### END CODE HERE ###\n    return best_probs, best_paths\n\nRun the viterbi_forward function to fill in the best_probs and best_paths matrices.\nNote that this will take a few minutes to run. There are about 30,000 words to process.\n\n# this will take a few minutes to run =&gt; processes ~ 30,000 words\nbest_probs, best_paths = viterbi_forward(A, B, prep, best_probs, best_paths, vocab)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[21], line 2\n      1 # this will take a few minutes to run =&gt; processes ~ 30,000 words\n----&gt; 2 best_probs, best_paths = viterbi_forward(A, B, prep, best_probs, best_paths, vocab)\n\nNameError: name 'A' is not defined\n\n\n\n\n# Test this function \nprint(f\"best_probs[0,1]: {best_probs[0,1]:.4f}\") \nprint(f\"best_probs[0,4]: {best_probs[0,4]:.4f}\") \n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[22], line 2\n      1 # Test this function \n----&gt; 2 print(f\"best_probs[0,1]: {best_probs[0,1]:.4f}\") \n      3 print(f\"best_probs[0,4]: {best_probs[0,4]:.4f}\") \n\nNameError: name 'best_probs' is not defined\n\n\n\n\nExpected Output\nbest_probs[0,1]: -24.7822\nbest_probs[0,4]: -49.5601"
  },
  {
    "objectID": "posts/c2w2/assignment.html#3.3",
    "href": "posts/c2w2/assignment.html#3.3",
    "title": "Assignment 2: Parts-of-Speech Tagging (POS)",
    "section": "Part 3.3 Viterbi backward",
    "text": "Part 3.3 Viterbi backward\nNow you will implement the Viterbi backward algorithm.\n\nThe Viterbi backward algorithm gets the predictions of the POS tags for each word in the corpus using the best_paths and the best_probs matrices.\n\nThe example below shows how to walk backwards through the best_paths matrix to get the POS tags of each word in the corpus. Recall that this example corpus has three words: “Loss tracks upward”.\nPOS tag for ‘upward’ is RB\n\nSelect the the most likely POS tag for the last word in the corpus, ‘upward’ in the best_prob table.\nLook for the row in the column for ‘upward’ that has the largest probability.\nNotice that in row 28 of best_probs, the estimated probability is -34.99, which is larger than the other values in the column. So the most likely POS tag for ‘upward’ is RB an adverb, at row 28 of best_prob.\nThe variable z is an array that stores the unique integer ID of the predicted POS tags for each word in the corpus. In array z, at position 2, store the value 28 to indicate that the word ‘upward’ (at index 2 in the corpus), most likely has the POS tag associated with unique ID 28 (which is RB).\nThe variable pred contains the POS tags in string form. So pred at index 2 stores the string RB.\n\nPOS tag for ‘tracks’ is VBZ\n\nThe next step is to go backward one word in the corpus (‘tracks’). Since the most likely POS tag for ‘upward’ is RB, which is uniquely identified by integer ID 28, go to the best_paths matrix in column 2, row 28. The value stored in best_paths, column 2, row 28 indicates the unique ID of the POS tag of the previous word. In this case, the value stored here is 40, which is the unique ID for POS tag VBZ (verb, 3rd person singular present).\nSo the previous word at index 1 of the corpus (‘tracks’), most likely has the POS tag with unique ID 40, which is VBZ.\nIn array z, store the value 40 at position 1, and for array pred, store the string VBZ to indicate that the word ‘tracks’ most likely has POS tag VBZ.\n\nPOS tag for ‘Loss’ is NN\n\nIn best_paths at column 1, the unique ID stored at row 40 is 20. 20 is the unique ID for POS tag NN.\nIn array z at position 0, store 20. In array pred at position 0, store NN.\n\n\n\nExercise 07\nImplement the viterbi_backward algorithm, which returns a list of predicted POS tags for each word in the corpus.\n\nNote that the numbering of the index positions starts at 0 and not 1.\nm is the number of words in the corpus.\n\nSo the indexing into the corpus goes from 0 to m - 1.\nAlso, the columns in best_probs and best_paths are indexed from 0 to m - 1\n\n\nIn Step 1:\nLoop through all the rows (POS tags) in the last entry of best_probs and find the row (POS tag) with the maximum value. Convert the unique integer ID to a tag (a string representation) using the list states.\nReferring to the three-word corpus described above:\n\nz[2] = 28: For the word ‘upward’ at position 2 in the corpus, the POS tag ID is 28. Store 28 in z at position 2.\nstates[28] is ‘RB’: The POS tag ID 28 refers to the POS tag ‘RB’.\npred[2] = 'RB': In array pred, store the POS tag for the word ‘upward’.\n\nIn Step 2:\n\nStarting at the last column of best_paths, use best_probs to find the most likely POS tag for the last word in the corpus.\nThen use best_paths to find the most likely POS tag for the previous word.\nUpdate the POS tag for each word in z and in preds.\n\nReferring to the three-word example from above, read best_paths at column 2 and fill in z at position 1.\nz[1] = best_paths[z[2],2]\nThe small test following the routine prints the last few words of the corpus and their states to aid in debug.\n\n# UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: viterbi_backward\ndef viterbi_backward(best_probs, best_paths, corpus, states):\n    '''\n    This function returns the best path.\n    \n    '''\n    # Get the number of words in the corpus\n    # which is also the number of columns in best_probs, best_paths\n    m = best_paths.shape[1] \n    \n    # Initialize array z, same length as the corpus\n    z = [None] * m\n    \n    # Get the number of unique POS tags\n    num_tags = best_probs.shape[0]\n    \n    # Initialize the best probability for the last word\n    best_prob_for_last_word = float('-inf')\n    \n    # Initialize pred array, same length as corpus\n    pred = [None] * m\n    \n    ### START CODE HERE (Replace instances of 'None' with your code) ###\n    ## Step 1 ##\n    \n    # Go through each POS tag for the last word (last column of best_probs)\n    # in order to find the row (POS tag integer ID) \n    # with highest probability for the last word\n    for k in None: # complete this line\n\n        # If the probability of POS tag at row k \n        # is better than the previously best probability for the last word:\n        if None: # complete this line\n            \n            # Store the new best probability for the lsat word\n            best_prob_for_last_word = None\n    \n            # Store the unique integer ID of the POS tag\n            # which is also the row number in best_probs\n            z[m - 1] = None\n            \n    # Convert the last word's predicted POS tag\n    # from its unique integer ID into the string representation\n    # using the 'states' dictionary\n    # store this in the 'pred' array for the last word\n    pred[m - 1] = None\n    \n    ## Step 2 ##\n    # Find the best POS tags by walking backward through the best_paths\n    # From the last word in the corpus to the 0th word in the corpus\n    for i in range(None, None, None): # complete this line\n        \n        # Retrieve the unique integer ID of\n        # the POS tag for the word at position 'i' in the corpus\n        pos_tag_for_word_i = None\n        \n        # In best_paths, go to the row representing the POS tag of word i\n        # and the column representing the word's position in the corpus\n        # to retrieve the predicted POS for the word at position i-1 in the corpus\n        z[i - 1] = None\n        \n        # Get the previous word's POS tag in string form\n        # Use the 'states' dictionary, \n        # where the key is the unique integer ID of the POS tag,\n        # and the value is the string representation of that POS tag\n        pred[i - 1] = None\n        \n     ### END CODE HERE ###\n    return pred\n\n\n# Run and test your function\npred = viterbi_backward(best_probs, best_paths, prep, states)\nm=len(pred)\nprint('The prediction for pred[-7:m-1] is: \\n', prep[-7:m-1], \"\\n\", pred[-7:m-1], \"\\n\")\nprint('The prediction for pred[0:8] is: \\n', pred[0:7], \"\\n\", prep[0:7])\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[24], line 2\n      1 # Run and test your function\n----&gt; 2 pred = viterbi_backward(best_probs, best_paths, prep, states)\n      3 m=len(pred)\n      4 print('The prediction for pred[-7:m-1] is: \\n', prep[-7:m-1], \"\\n\", pred[-7:m-1], \"\\n\")\n\nNameError: name 'best_probs' is not defined\n\n\n\nExpected Output:\nThe prediction for pred[-7:m-1] is:  \n ['see', 'them', 'here', 'with', 'us', '.']  \n ['VB', 'PRP', 'RB', 'IN', 'PRP', '.']   \nThe prediction for pred[0:8] is:    \n ['DT', 'NN', 'POS', 'NN', 'MD', 'VB', 'VBN']   \n ['The', 'economy', \"'s\", 'temperature', 'will', 'be', 'taken'] \nNow you just have to compare the predicted labels to the true labels to evaluate your model on the accuracy metric!"
  },
  {
    "objectID": "posts/c1w3/index.html#lab-linear-algebra-in-python-with-numpy",
    "href": "posts/c1w3/index.html#lab-linear-algebra-in-python-with-numpy",
    "title": "Vector Space Models",
    "section": "LAB: Linear algebra in Python with numpy",
    "text": "LAB: Linear algebra in Python with numpy\nThe Numpy lab\n\nCosine Similarity Intuition\nOne of the issues with euclidean distance is that it is not always accurate and sometimes we are not looking for that type of similarity metric. For example, when comparing large documents to smaller ones with euclidean distance one could get an inaccurate result.\nLook at the diagram:\n\n\n\n\n\n\n\nCosine Similarity: Intuition\n\n\n\n\nFigure 9: The cosine similarity is the cosine of the angle between two vectors.\n\n\nNormally the food corpus and the agriculture corpus are more similar because they have the same proportion of words. However the food corpus is much smaller than the agriculture corpus. To further clarify, although the history corpus and the agriculture corpus are different, they have a smaller euclidean distance. Hence d_2&lt;d_1.\nTo solve this problem, we look at the cosine between the vectors. This allows us to compare B and α.\n\n\nBackground\nBefore getting into the cosine similarity function remember that the norm of a vector is defined as:\n\n\nNorm of a Vector\n\n||\\vec{A}|| = \\sqrt{\\sum_{i=1}^{n} a_i^2}\n\\tag{2}\n\n\nDot-product of Two Vectors\nThe dot product is then defined as:\n\n\\vec{A} \\cdot \\vec{B} = \\sum_{i=1}^{n} a_i \\cdot b_i\n\\tag{3}\n\n\n\n\n\n\n\nCosine Similarity\n\n\n\n\nFigure 10: The cosine similarity is the cosine of the angle between two vectors.\n\n\nThe following cosine similarity equation makes sense:\n\n\\cos(\\theta) = \\frac{\\vec{v} \\cdot \\vec{w}}{||\\vec{v}|| \\cdot ||\\vec{w}||}\n\\tag{4}",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models and PCA"
    ]
  },
  {
    "objectID": "posts/c1w3/index.html#implementation",
    "href": "posts/c1w3/index.html#implementation",
    "title": "Vector Space Models",
    "section": "Implementation",
    "text": "Implementation\nWhen \\vec{v} and \\vec{u} are parallel the numerator is equal to the denominator so cos(\\beta)=1 thus \\angle \\beta=0.\nOn the other hand, the dot product of two orthogonal (perpendicular) vectors is 0. That takes place when \\angle \\beta=90.\n\n\n\n\n\n\n\nCosine Similarity Examples\n\n\n\n\nFigure 11: Examples of cosine similarity between similar and dissimilar vectors.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models and PCA"
    ]
  },
  {
    "objectID": "posts/c1w3/index.html#lab-on-manipulating-word-vectors",
    "href": "posts/c1w3/index.html#lab-on-manipulating-word-vectors",
    "title": "Vector Space Models",
    "section": "Lab on Manipulating Word Vectors",
    "text": "Lab on Manipulating Word Vectors\nThe lab",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models and PCA"
    ]
  },
  {
    "objectID": "posts/c1w3/index.html#visualization-of-word-vectors",
    "href": "posts/c1w3/index.html#visualization-of-word-vectors",
    "title": "Vector Space Models",
    "section": "Visualization of Word Vectors",
    "text": "Visualization of Word Vectors\nPrincipal component analysis is an unsupervised learning algorithm which can be used to reduce the dimension of your data. As a result, it allows you to visualize your data. It tries to combine variances across features. Here is a concrete example of PCA:\n\n\n\n\n\n\n\nWord analogy\n\n\n\n\nFigure 14: Some word analogies\n\n\n\n\n\n\n\n\nWord analogy\n\n\n\n\nFigure 15: Some word analogies\n\n\n\nThose are the results of plotting a couple of vectors in two dimensions. Note that words with similar part of speech (POS) tags are next to one another. This is because many of the training algorithms learn words by identifying the neighboring words. Thus, words with similar POS tags tend to be found in similar locations. An interesting insight is that synonyms and antonyms tend to be found next to each other in the plot. Why is that the case?",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models and PCA"
    ]
  },
  {
    "objectID": "posts/c1w3/index.html#implementation-1",
    "href": "posts/c1w3/index.html#implementation-1",
    "title": "Vector Space Models",
    "section": "Implementation",
    "text": "Implementation",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models and PCA"
    ]
  },
  {
    "objectID": "posts/c1w3/index.html#pca-algorithm",
    "href": "posts/c1w3/index.html#pca-algorithm",
    "title": "Vector Space Models",
    "section": "PCA algorithm",
    "text": "PCA algorithm\nPCA is commonly used to reduce the dimension of your data. Intuitively the model collapses the data across principal components. You can think of the first principal component (in a 2D dataset) as the line where there is the most amount of variance. You can then collapse the data points on that line. Hence you went from 2D to 1D. You can generalize this intuition to several dimensions.\n\n\n\n\n\n\n\nPCA\n\n\n\n\nFigure 16: Some word analogies\n\n\n\nEigenvector\n\nthe resulting vectors, also known as the uncorrelated features of your data\n\nEigenvalue\n\nthe amount of information retained by each new feature. You can think of it as the variance in the eigenvector.\n\n\nAlso each eigenvalue has a corresponding eigenvector. The eigenvalue tells you how much variance there is in the eigenvector. Here are the steps required to compute PCA:\n\n\n\n\n\n\n\nPCA\n\n\n\n\nFigure 17: Some word analogies\n\n\n\nSteps to Compute PCA:\n\nMean normalize your data\nCompute the covariance matrix\nCompute SVD on your covariance matrix. This returns [USV]=svd(Σ) . The three matrices U, S, V are drawn above. U is labelled with eigenvectors, and S is labelled with eigenvalues.\nYou can then use the first n columns of vector U, to get your new data by multiplying XU[:,0:n].",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models and PCA"
    ]
  },
  {
    "objectID": "posts/c1w3/index.html#putting-it-together-with-code",
    "href": "posts/c1w3/index.html#putting-it-together-with-code",
    "title": "Vector Space Models",
    "section": "Putting It Together with Code",
    "text": "Putting It Together with Code\n\nimport numpy as np \n\ndef PCA(X , num_components):\n  # center data around the mean\n  X_meaned = X - np.mean(X , axis = 0) \n  # calculate the covariance matrix   \n  cov_mat = np.cov(X_meaned , rowvar = False) \n  # compute an uncorrelated feature basis (eigen vectors) \n  eigen_values , eigen_vectors = np.linalg.eigh(cov_mat)\n  # sort the new basis by decreasing eigen values (variance) \n  sorted_index = np.argsort(eigen_values)[::-1] \n  sorted_eigenvalue = eigen_values[sorted_index] \n  sorted_eigenvectors = eigen_vectors[:,sorted_index] \n  # by subseting the most leading features  \n  eigenvector_subset = sorted_eigenvectors[:,0:num_components] \n  #Step-6 \n  X_reduced = np.dot(eigenvector_subset.transpose() ,     \n                   X_meaned.transpose() ).transpose() \n  return X_reduced",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models and PCA"
    ]
  },
  {
    "objectID": "posts/c1w3/index.html#lab-on-pca",
    "href": "posts/c1w3/index.html#lab-on-pca",
    "title": "Vector Space Models",
    "section": "Lab on PCA",
    "text": "Lab on PCA\nPCA lab",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models and PCA"
    ]
  },
  {
    "objectID": "posts/c1w3/index.html#see-also",
    "href": "posts/c1w3/index.html#see-also",
    "title": "Vector Space Models",
    "section": "See Also",
    "text": "See Also\n\nAlex Williams - Everything you did and didn’t know about PCA\nUdell et al. (2015). Generalized Low-Rank Models  arxiv preprint\nTipping & Bishop (1999). Probabilistic principal component analysis Journal of the Royal Statistical Society: Series B\nIlin & Raiko (2010) Practical Approaches to Principal Component Analysis in the Presence of Missing Values Journal of Machine Learning Research\nGordon (2002). Generalized2 Linear2 Models NIPS\nCunningham & Ghahramani (2015)  Linear dimensionality reduction: survey, insights, and generalizations Journal of Machine Learning Research\nBurges (2009). Dimension Reduction: A Guided Tour Foundations varia Trends in Machine Learning\nM. Gavish and D. L. Donoho, The Optimal Hard Threshold for Singular Values is \\frac{4}{\\sqrt{3}} in IEEE Transactions on Information Theory, vol. 60, no. 8, pp. 5040-5053, Aug. 2014, doi: 10.1109/TIT.2014.2323359.\nThomas P. Minka Automatic choice of dimensionality for PCA Dec. 2000",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models and PCA"
    ]
  },
  {
    "objectID": "posts/c1w3/index.html#slides",
    "href": "posts/c1w3/index.html#slides",
    "title": "Vector Space Models",
    "section": "Slides",
    "text": "Slides\n\n\n\nslides",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models and PCA"
    ]
  },
  {
    "objectID": "posts/c1w3/lab01.html",
    "href": "posts/c1w3/lab01.html",
    "title": "Linear algebra in Python with NumPy",
    "section": "",
    "text": "course banner\nIn this lab, you will have the opportunity to remember some basic concepts about linear algebra and how to use them in Python.\nNumpy is one of the most used libraries in Python for arrays manipulation. It adds to Python a set of functions that allows us to operate on large multidimensional arrays with just a few lines. So forget about writing nested loops for adding matrices! With NumPy, this is as simple as adding numbers.\nLet us import the numpy library and assign the alias np for it. We will follow this convention in almost every notebook in this course, and you’ll see this in many resources outside this course as well.\nimport numpy as np  # The swiss knife of the data scientist."
  },
  {
    "objectID": "posts/c1w3/lab01.html#defining-lists-and-numpy-arrays",
    "href": "posts/c1w3/lab01.html#defining-lists-and-numpy-arrays",
    "title": "Linear algebra in Python with NumPy",
    "section": "Defining lists and numpy arrays",
    "text": "Defining lists and numpy arrays\n\nalist = [1, 2, 3, 4, 5]   # Define a python list. It looks like an np array\nnarray = np.array([1, 2, 3, 4]) # Define a numpy array\n\nNote the difference between a Python list and a NumPy array.\n\nprint(alist)\nprint(narray)\n\nprint(type(alist))\nprint(type(narray))\n\n[1, 2, 3, 4, 5]\n[1 2 3 4]\n&lt;class 'list'&gt;\n&lt;class 'numpy.ndarray'&gt;"
  },
  {
    "objectID": "posts/c1w3/lab01.html#algebraic-operators-on-numpy-arrays-vs.-python-lists",
    "href": "posts/c1w3/lab01.html#algebraic-operators-on-numpy-arrays-vs.-python-lists",
    "title": "Linear algebra in Python with NumPy",
    "section": "Algebraic operators on NumPy arrays vs. Python lists",
    "text": "Algebraic operators on NumPy arrays vs. Python lists\nOne of the common beginner mistakes is to mix up the concepts of NumPy arrays and Python lists. Just observe the next example, where we add two objects of the two mentioned types. Note that the ‘+’ operator on NumPy arrays perform an element-wise addition, while the same operation on Python lists results in a list concatenation. Be careful while coding. Knowing this can save many headaches.\n\nprint(narray + narray)\nprint(alist + alist)\n\n[2 4 6 8]\n[1, 2, 3, 4, 5, 1, 2, 3, 4, 5]\n\n\nIt is the same as with the product operator, *. In the first case, we scale the vector, while in the second case, we concatenate three times the same list.\n\nprint(narray * 3)\nprint(alist * 3)\n\n[ 3  6  9 12]\n[1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5]\n\n\nBe aware of the difference because, within the same function, both types of arrays can appear. Numpy arrays are designed for numerical and matrix operations, while lists are for more general purposes."
  },
  {
    "objectID": "posts/c1w3/lab01.html#matrix-or-array-of-arrays",
    "href": "posts/c1w3/lab01.html#matrix-or-array-of-arrays",
    "title": "Linear algebra in Python with NumPy",
    "section": "Matrix or Array of Arrays",
    "text": "Matrix or Array of Arrays\nIn linear algebra, a matrix is a structure composed of n rows by m columns. That means each row must have the same number of columns. With NumPy, we have two ways to create a matrix: * Creating an array of arrays using np.array (recommended). * Creating a matrix using np.matrix (still available but might be removed soon).\nNumPy arrays or lists can be used to initialize a matrix, but the resulting matrix will be composed of NumPy arrays only.\n\nnpmatrix1 = np.array([narray, narray, narray]) # Matrix initialized with NumPy arrays\nnpmatrix2 = np.array([alist, alist, alist]) # Matrix initialized with lists\nnpmatrix3 = np.array([narray, [1, 1, 1, 1], narray]) # Matrix initialized with both types\n\nprint(npmatrix1)\nprint(npmatrix2)\nprint(npmatrix3)\n\n[[1 2 3 4]\n [1 2 3 4]\n [1 2 3 4]]\n[[1 2 3 4 5]\n [1 2 3 4 5]\n [1 2 3 4 5]]\n[[1 2 3 4]\n [1 1 1 1]\n [1 2 3 4]]\n\n\nHowever, when defining a matrix, be sure that all the rows contain the same number of elements. Otherwise, the linear algebra operations could lead to unexpected results.\nAnalyze the following two examples:\n\n# Example 1:\n\nokmatrix = np.array([[1, 2], [3, 4]]) # Define a 2x2 matrix\nprint(okmatrix) # Print okmatrix\nprint(okmatrix * 2) # Print a scaled version of okmatrix\n\n[[1 2]\n [3 4]]\n[[2 4]\n [6 8]]\n\n\n\n# Example 2:\n\nbadmatrix = np.array([[1, 2], [3, 4], [5, 6, 7]]) # Define a matrix. Note the third row contains 3 elements\nprint(badmatrix) # Print the malformed matrix\nprint(badmatrix * 2) # It is supposed to scale the whole matrix\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[47], line 3\n      1 # Example 2:\n----&gt; 3 badmatrix = np.array([[1, 2], [3, 4], [5, 6, 7]]) # Define a matrix. Note the third row contains 3 elements\n      4 print(badmatrix) # Print the malformed matrix\n      5 print(badmatrix * 2) # It is supposed to scale the whole matrix\n\nValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part."
  },
  {
    "objectID": "posts/c1w3/lab01.html#scaling-and-translating-matrices",
    "href": "posts/c1w3/lab01.html#scaling-and-translating-matrices",
    "title": "Linear algebra in Python with NumPy",
    "section": "Scaling and translating matrices",
    "text": "Scaling and translating matrices\nNow that you know how to build correct NumPy arrays and matrices, let us see how easy it is to operate with them in Python using the regular algebraic operators like + and -.\nOperations can be performed between arrays and arrays or between arrays and scalars.\n\n# Scale by 2 and translate 1 unit the matrix\nresult = okmatrix * 2 + 1 # For each element in the matrix, multiply by 2 and add 1\nprint(result)\n\n[[3 5]\n [7 9]]\n\n\n\n# Add two compatible matrices\nresult1 = okmatrix + okmatrix\nprint(result1)\n\n# Subtract two compatible matrices. This is called the difference vector\nresult2 = okmatrix - okmatrix\nprint(result2)\n\n[[2 4]\n [6 8]]\n[[0 0]\n [0 0]]\n\n\nThe product operator * when used on arrays or matrices indicates element-wise multiplications. Do not confuse it with the dot product.\n\nresult = okmatrix * okmatrix # Multiply each element by itself\nprint(result)\n\n[[ 1  4]\n [ 9 16]]"
  },
  {
    "objectID": "posts/c1w3/lab01.html#transpose-a-matrix",
    "href": "posts/c1w3/lab01.html#transpose-a-matrix",
    "title": "Linear algebra in Python with NumPy",
    "section": "Transpose a matrix",
    "text": "Transpose a matrix\nIn linear algebra, the transpose of a matrix is an operator that flips a matrix over its diagonal, i.e., the transpose operator switches the row and column indices of the matrix producing another matrix. If the original matrix dimension is n by m, the resulting transposed matrix will be m by n.\nT denotes the transpose operations with NumPy matrices.\n\nmatrix3x2 = np.array([[1, 2], [3, 4], [5, 6]]) # Define a 3x2 matrix\nprint('Original matrix 3 x 2')\nprint(matrix3x2)\nprint('Transposed matrix 2 x 3')\nprint(matrix3x2.T)\n\nOriginal matrix 3 x 2\n[[1 2]\n [3 4]\n [5 6]]\nTransposed matrix 2 x 3\n[[1 3 5]\n [2 4 6]]\n\n\nHowever, note that the transpose operation does not affect 1D arrays.\n\nnparray = np.array([1, 2, 3, 4]) # Define an array\nprint('Original array')\nprint(nparray)\nprint('Transposed array')\nprint(nparray.T)\n\nOriginal array\n[1 2 3 4]\nTransposed array\n[1 2 3 4]\n\n\nperhaps in this case you wanted to do:\n\nnparray = np.array([[1, 2, 3, 4]]) # Define a 1 x 4 matrix. Note the 2 level of square brackets\nprint('Original array')\nprint(nparray)\nprint('Transposed array')\nprint(nparray.T)\n\nOriginal array\n[[1 2 3 4]]\nTransposed array\n[[1]\n [2]\n [3]\n [4]]"
  },
  {
    "objectID": "posts/c1w3/lab01.html#get-the-norm-of-a-nparray-or-matrix",
    "href": "posts/c1w3/lab01.html#get-the-norm-of-a-nparray-or-matrix",
    "title": "Linear algebra in Python with NumPy",
    "section": "Get the norm of a nparray or matrix",
    "text": "Get the norm of a nparray or matrix\nIn linear algebra, the norm of an n-dimensional vector \\vec a is defined as:\n norm(\\vec a) = ||\\vec a|| = \\sqrt {\\sum_{i=1}^{n} a_i ^ 2}\nCalculating the norm of vector or even of a matrix is a general operation when dealing with data. Numpy has a set of functions for linear algebra in the subpackage linalg, including the norm function. Let us see how to get the norm a given array or matrix:\n\nnparray1 = np.array([1, 2, 3, 4]) # Define an array\nnorm1 = np.linalg.norm(nparray1)\n\nnparray2 = np.array([[1, 2], [3, 4]]) # Define a 2 x 2 matrix. Note the 2 level of square brackets\nnorm2 = np.linalg.norm(nparray2) \n\nprint(norm1)\nprint(norm2)\n\n5.477225575051661\n5.477225575051661\n\n\nNote that without any other parameter, the norm function treats the matrix as being just an array of numbers. However, it is possible to get the norm by rows or by columns. The axis parameter controls the form of the operation: * axis=0 means get the norm of each column * axis=1 means get the norm of each row.\n\nnparray2 = np.array([[1, 1], [2, 2], [3, 3]]) # Define a 3 x 2 matrix. \n\nnormByCols = np.linalg.norm(nparray2, axis=0) # Get the norm for each column. Returns 2 elements\nnormByRows = np.linalg.norm(nparray2, axis=1) # get the norm for each row. Returns 3 elements\n\nprint(normByCols)\nprint(normByRows)\n\n[3.74165739 3.74165739]\n[1.41421356 2.82842712 4.24264069]\n\n\nHowever, there are more ways to get the norm of a matrix in Python. For that, let us see all the different ways of defining the dot product between 2 arrays."
  },
  {
    "objectID": "posts/c1w3/lab01.html#the-dot-product-between-arrays-all-the-flavors",
    "href": "posts/c1w3/lab01.html#the-dot-product-between-arrays-all-the-flavors",
    "title": "Linear algebra in Python with NumPy",
    "section": "The dot product between arrays: All the flavors",
    "text": "The dot product between arrays: All the flavors\nThe dot product or scalar product or inner product between two vectors \\vec a and \\vec b of the same size is defined as: \\vec a \\cdot \\vec b = \\sum_{i=1}^{n} a_i b_i\nThe dot product takes two vectors and returns a single number.\n\nnparray1 = np.array([0, 1, 2, 3]) # Define an array\nnparray2 = np.array([4, 5, 6, 7]) # Define an array\n\nflavor1 = np.dot(nparray1, nparray2) # Recommended way\nprint(flavor1)\n\nflavor2 = np.sum(nparray1 * nparray2) # Ok way\nprint(flavor2)\n\nflavor3 = nparray1 @ nparray2         # Geeks way\nprint(flavor3)\n\n# As you never should do:             # Noobs way\nflavor4 = 0\nfor a, b in zip(nparray1, nparray2):\n    flavor4 += a * b\n    \nprint(flavor4)\n\n38\n38\n38\n38\n\n\nWe strongly recommend using np.dot, since it is the only method that accepts arrays and lists without problems\n\nnorm1 = np.dot(np.array([1, 2]), np.array([3, 4])) # Dot product on nparrays\nnorm2 = np.dot([1, 2], [3, 4]) # Dot product on python lists\n\nprint(norm1, '=', norm2 )\n\n11 = 11\n\n\nFinally, note that the norm is the square root of the dot product of the vector with itself. That gives many options to write that function:\n norm(\\vec a) = ||\\vec a|| = \\sqrt {\\sum_{i=1}^{n} a_i ^ 2} = \\sqrt {a \\cdot a}"
  },
  {
    "objectID": "posts/c1w3/lab01.html#sums-by-rows-or-columns",
    "href": "posts/c1w3/lab01.html#sums-by-rows-or-columns",
    "title": "Linear algebra in Python with NumPy",
    "section": "Sums by rows or columns",
    "text": "Sums by rows or columns\nAnother general operation performed on matrices is the sum by rows or columns. Just as we did for the function norm, the axis parameter controls the form of the operation: * axis=0 means to sum the elements of each column together. * axis=1 means to sum the elements of each row together.\n\nnparray2 = np.array([[1, -1], [2, -2], [3, -3]]) # Define a 3 x 2 matrix. \n\nsumByCols = np.sum(nparray2, axis=0) # Get the sum for each column. Returns 2 elements\nsumByRows = np.sum(nparray2, axis=1) # get the sum for each row. Returns 3 elements\n\nprint('Sum by columns: ')\nprint(sumByCols)\nprint('Sum by rows:')\nprint(sumByRows)\n\nSum by columns: \n[ 6 -6]\nSum by rows:\n[0 0 0]"
  },
  {
    "objectID": "posts/c1w3/lab01.html#get-the-mean-by-rows-or-columns",
    "href": "posts/c1w3/lab01.html#get-the-mean-by-rows-or-columns",
    "title": "Linear algebra in Python with NumPy",
    "section": "Get the mean by rows or columns",
    "text": "Get the mean by rows or columns\nAs with the sums, one can get the mean by rows or columns using the axis parameter. Just remember that the mean is the sum of the elements divided by the length of the vector  mean(\\vec a) = \\frac {{\\sum_{i=1}^{n} a_i }}{n}\n\nnparray2 = np.array([[1, -1], [2, -2], [3, -3]]) # Define a 3 x 2 matrix. Chosen to be a matrix with 0 mean\n\nmean = np.mean(nparray2) # Get the mean for the whole matrix\nmeanByCols = np.mean(nparray2, axis=0) # Get the mean for each column. Returns 2 elements\nmeanByRows = np.mean(nparray2, axis=1) # get the mean for each row. Returns 3 elements\n\nprint('Matrix mean: ')\nprint(mean)\nprint('Mean by columns: ')\nprint(meanByCols)\nprint('Mean by rows:')\nprint(meanByRows)\n\nMatrix mean: \n0.0\nMean by columns: \n[ 2. -2.]\nMean by rows:\n[0. 0. 0.]"
  },
  {
    "objectID": "posts/c1w3/lab01.html#center-the-columns-of-a-matrix",
    "href": "posts/c1w3/lab01.html#center-the-columns-of-a-matrix",
    "title": "Linear algebra in Python with NumPy",
    "section": "Center the columns of a matrix",
    "text": "Center the columns of a matrix\nCentering the attributes of a data matrix is another essential preprocessing step. Centering a matrix means to remove the column mean to each element inside the column. The mean by columns of a centered matrix is always 0.\nWith NumPy, this process is as simple as this:\n\nnparray2 = np.array([[1, 1], [2, 2], [3, 3]]) # Define a 3 x 2 matrix. \n\nnparrayCentered = nparray2 - np.mean(nparray2, axis=0) # Remove the mean for each column\n\nprint('Original matrix')\nprint(nparray2)\nprint('Centered by columns matrix')\nprint(nparrayCentered)\n\nprint('New mean by column')\nprint(nparrayCentered.mean(axis=0))\n\nOriginal matrix\n[[1 1]\n [2 2]\n [3 3]]\nCentered by columns matrix\n[[-1. -1.]\n [ 0.  0.]\n [ 1.  1.]]\nNew mean by column\n[0. 0.]\n\n\nWarning: This process does not apply for row centering. In such cases, consider transposing the matrix, centering by columns, and then transpose back the result.\nSee the example below:\n\nnparray2 = np.array([[1, 3], [2, 4], [3, 5]]) # Define a 3 x 2 matrix. \n\nnparrayCentered = nparray2.T - np.mean(nparray2, axis=1) # Remove the mean for each row\nnparrayCentered = nparrayCentered.T # Transpose back the result\n\nprint('Original matrix')\nprint(nparray2)\nprint('Centered by rows matrix')\nprint(nparrayCentered)\n\nprint('New mean by rows')\nprint(nparrayCentered.mean(axis=1))\n\nOriginal matrix\n[[1 3]\n [2 4]\n [3 5]]\nCentered by rows matrix\n[[-1.  1.]\n [-1.  1.]\n [-1.  1.]]\nNew mean by rows\n[0. 0. 0.]\n\n\nNote that some operations can be performed using static functions like np.sum() or np.mean(), or by using the inner functions of the array\n\nnparray2 = np.array([[1, 3], [2, 4], [3, 5]]) # Define a 3 x 2 matrix. \n\nmean1 = np.mean(nparray2) # Static way\nmean2 = nparray2.mean()   # Dinamic way\n\nprint(mean1, ' == ', mean2)\n\n3.0  ==  3.0\n\n\nEven if they are equivalent, we recommend the use of the static way always.\nCongratulations! You have successfully reviewed vector and matrix operations with Numpy!"
  },
  {
    "objectID": "posts/c1lab0/index.html",
    "href": "posts/c1lab0/index.html",
    "title": "Preprocessing",
    "section": "",
    "text": "course banner",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Lab on preprocessing"
    ]
  },
  {
    "objectID": "posts/c1lab0/index.html#setup",
    "href": "posts/c1lab0/index.html#setup",
    "title": "Preprocessing",
    "section": "Setup",
    "text": "Setup\nYou will be doing sentiment analysis on tweets in the first two weeks of this course. To help with that, we will be using the Natural Language Toolkit (NLTK) package, an open-source Python library for natural language processing. It has modules for collecting, handling, and processing Twitter data, and you will be acquainted with them as we move along the course. For this exercise, we will use a Twitter dataset that comes with NLTK. This dataset has been manually annotated and serves to establish baselines for models quickly. Let us import them now as well as a few other libraries we will be using.\n\nimport nltk                                # Python library for NLP\nfrom nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\nimport matplotlib.pyplot as plt            # library for visualization\nimport random                              # pseudo-random number generator",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Lab on preprocessing"
    ]
  },
  {
    "objectID": "posts/c1lab0/index.html#about-the-twitter-dataset",
    "href": "posts/c1lab0/index.html#about-the-twitter-dataset",
    "title": "Preprocessing",
    "section": "About the Twitter dataset",
    "text": "About the Twitter dataset\nThe sample dataset from NLTK is separated into positive and negative tweets. It contains 5000 positive tweets and 5000 negative tweets exactly. The exact match between these classes is not a coincidence. The intention is to have a balanced dataset. That does not reflect the real distributions of positive and negative classes in live Twitter streams. It is just because balanced datasets simplify the design of most computational methods that are required for sentiment analysis. However, it is better to be aware that this balance of classes is artificial. The dataset is already downloaded in the Coursera workspace. In a local computer however, you can download the data by doing:\n\n# downloads sample twitter dataset. uncomment the line below if running on a local machine.\nnltk.download('twitter_samples')\n\n[nltk_data] Downloading package twitter_samples to\n[nltk_data]     /home/oren/nltk_data...\n[nltk_data]   Package twitter_samples is already up-to-date!\n\n\nTrue\n\n\nWe can load the text fields of the positive and negative tweets by using the module’s strings() method like this:\n\n# select the set of positive and negative tweets\nall_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')\n\nNext, we’ll print a report with the number of positive and negative tweets. It is also essential to know the data structure of the datasets\n\nprint('Number of positive tweets: ', len(all_positive_tweets))\nprint('Number of negative tweets: ', len(all_negative_tweets))\nprint('\\nThe type of all_positive_tweets is: ', type(all_positive_tweets))\nprint('The type of a tweet entry is: ', type(all_negative_tweets[0]))\n\nNumber of positive tweets:  5000\nNumber of negative tweets:  5000\n\nThe type of all_positive_tweets is:  &lt;class 'list'&gt;\nThe type of a tweet entry is:  &lt;class 'str'&gt;\n\n\nWe can see that the data is stored in a list and as you might expect, individual tweets are stored as strings. You can make a more visually appealing report by using Matplotlib’s pyplot library. Let us see how to create a pie chart to show the same information as above. This simple snippet will serve you in future visualizations of this kind of data.\n\n# Declare a figure with a custom size\nfig = plt.figure(figsize=(5, 5))\n\n# labels for the two classes\nlabels = 'Positives', 'Negative'\n\n# Sizes for each slide\nsizes = [len(all_positive_tweets), len(all_negative_tweets)] \n\n# Declare pie chart, where the slices will be ordered and plotted counter-clockwise:\nplt.pie(sizes, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\n\n# Equal aspect ratio ensures that pie is drawn as a circle.\nplt.axis('equal')  \n\n# Display the chart\nplt.show()\n\n([&lt;matplotlib.patches.Wedge at 0x7c448b1cba90&gt;,\n  &lt;matplotlib.patches.Wedge at 0x7c448b1cb9d0&gt;],\n [Text(-1.0999999999999959, -9.616505800409723e-08, 'Positives'),\n  Text(1.0999999999999832, 1.9233011600819372e-07, 'Negative')],\n [Text(-0.5999999999999978, -5.2453668002234845e-08, '50.0%'),\n  Text(0.5999999999999908, 1.0490733600446929e-07, '50.0%')])\n\n\n(np.float64(-1.100000000000005),\n np.float64(1.100000000000106),\n np.float64(-1.1),\n np.float64(1.1))",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Lab on preprocessing"
    ]
  },
  {
    "objectID": "posts/c1lab0/index.html#looking-at-raw-texts",
    "href": "posts/c1lab0/index.html#looking-at-raw-texts",
    "title": "Preprocessing",
    "section": "Looking at raw texts",
    "text": "Looking at raw texts\nBefore anything else, we can print a couple of tweets from the dataset to see how they look. Understanding the data is responsible for 80% of the success or failure in data science projects. We can use this time to observe aspects we’d like to consider when preprocessing our data. Below, you will print one random positive and one random negative tweet. We have added a color mark at the beginning of the string to further distinguish the two.\n\n# print positive in greeen\nprint('\\033[92m' + all_positive_tweets[random.randint(0,5000)])\n\n# print negative in red\nprint('\\033[91m' + all_negative_tweets[random.randint(0,5000)])\n\n@LilyTins congrats :) good news heading into the weekend!\n#BuyNotAnApologyOniTunes isn't avalible in Denmark :(... Could someone please gift it to me? im frustrated\n\n\nOne observation you may have is the presence of emoticons and URLs in many of the tweets. This info will come in handy in the next steps.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Lab on preprocessing"
    ]
  },
  {
    "objectID": "posts/c1lab0/index.html#preprocess-raw-text-for-sentiment-analysis",
    "href": "posts/c1lab0/index.html#preprocess-raw-text-for-sentiment-analysis",
    "title": "Preprocessing",
    "section": "Preprocess raw text for Sentiment analysis",
    "text": "Preprocess raw text for Sentiment analysis\nData preprocessing is one of the critical steps in any machine learning project. It includes cleaning and formatting the data before feeding into a machine learning algorithm. For NLP, the preprocessing steps are comprised of the following tasks:\n\nTokenizing the string\nLowercasing\nRemoving stop words and punctuation\nStemming\n\nThe videos explained each of these steps and why they are important. Let’s see how we can do these to a given tweet. We will choose just one and see how this is transformed by each preprocessing step.\n\n# Our selected sample. Complex enough to exemplify each step\ntweet = all_positive_tweets[2277]\nprint(tweet)\n\nMy beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i\n\n\nLet’s import a few more libraries for this purpose.\n\n# download the stopwords from NLTK\nnltk.download('stopwords')\n\n[nltk_data] Downloading package stopwords to /home/oren/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue\n\n\n\nimport re                                  # library for regular expression operations\nimport string                              # for string operations\nfrom nltk.corpus import stopwords          # module for stop words that come with NLTK\nfrom nltk.stem import PorterStemmer        # module for stemming\nfrom nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n\n\nRemove hyperlinks, Twitter marks and styles\nSince we have a Twitter dataset, we’d like to remove some substrings commonly used on the platform like the hashtag, retweet marks, and hyperlinks. We’ll use the re library to perform regular expression operations on our tweet. We’ll define our search pattern and use the sub() method to remove matches by substituting with an empty character (i.e. '')\n\nprint('\\033[92m' + tweet)\nprint('\\033[94m')\n\n# remove old style retweet text \"RT\"\ntweet2 = re.sub(r'^RT[\\s]+', '', tweet)\n\n# remove hyperlinks\ntweet2 = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet2)\n\n# remove hashtags\n\n# only removing the hash # sign from the word\ntweet2 = re.sub(r'#', '', tweet2)\nprint(tweet2)\n\nMy beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i\n\nMy beautiful sunflowers on a sunny Friday morning off :) sunflowers favourites happy Friday off… \n\n\n\n\nTokenize the string\nTo tokenize means to split the strings into individual words without blanks or tabs. In this same step, we will also convert each word in the string to lower case. The tokenize module from NLTK allows us to do these easily:\n\nprint()\nprint('\\033[92m' + tweet2)\nprint('\\033[94m')\n\n# instantiate tokenizer class\ntokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                               reduce_len=True)\n\n# tokenize tweets\ntweet_tokens = tokenizer.tokenize(tweet2)\nprint()\nprint('Tokenized string:')\nprint(tweet_tokens)\n\n\nMy beautiful sunflowers on a sunny Friday morning off :) sunflowers favourites happy Friday off… \n\n\nTokenized string:\n['my', 'beautiful', 'sunflowers', 'on', 'a', 'sunny', 'friday', 'morning', 'off', ':)', 'sunflowers', 'favourites', 'happy', 'friday', 'off', '…']\n\n\n\n\nRemove stop words and punctuations\nThe next step is to remove stop words and punctuation. Stop words are words that don’t add significant meaning to the text. You’ll see the list provided by NLTK when you run the cells below.\n\n#Import the english stop words list from NLTK\nstopwords_english = stopwords.words('english') \nprint('Stop words\\n')\nprint(stopwords_english)\nprint('\\nPunctuation\\n')\nprint(string.punctuation)\n\nStop words\n\n['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n\nPunctuation\n\n!\"#$%&'()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\n\n\nWe can see that the stop words list above contains some words that could be important in some contexts.\nThese could be words like i, not, between, because, won, against. You might need to customize the stop words list for some applications. For our exercise, we will use the entire list. For the punctuation, we saw earlier that certain groupings like ‘:)’ and ‘…’ should be retained when dealing with tweets because they are used to express emotions. In other contexts, like medical analysis, these should also be removed.\nTime to clean up our tokenized tweet!\n\nprint()\nprint('\\033[92m')\nprint(tweet_tokens)\nprint('\\033[94m')\ntweets_clean = []\nfor word in tweet_tokens: # Go through every word in your tokens list\n    if (word not in stopwords_english and  # remove stopwords\n        word not in string.punctuation):  # remove punctuation\n        tweets_clean.append(word)\nprint('removed stop words and punctuation:')\nprint(tweets_clean)\n\n\n\n['my', 'beautiful', 'sunflowers', 'on', 'a', 'sunny', 'friday', 'morning', 'off', ':)', 'sunflowers', 'favourites', 'happy', 'friday', 'off', '…']\n\nremoved stop words and punctuation:\n['beautiful', 'sunflowers', 'sunny', 'friday', 'morning', ':)', 'sunflowers', 'favourites', 'happy', 'friday', '…']\n\n\nPlease note that the words happy and sunny in this list are correctly spelled.\n\n\nStemming\nStemming is the process of converting a word to its most general form, or stem. This helps in reducing the size of our vocabulary. Consider the words:\n\nlearn\nlearning\nlearned\nlearnt\n\nAll these words are stemmed from its common root learn. However, in some cases, the stemming process produces words that are not correct spellings of the root word. For example, happi and sunni. That’s because it chooses the most common stem for related words. For example, we can look at the set of words that comprises the different forms of happy: * happy * happiness * happier\nWe can see that the prefix happi is more commonly used. We cannot choose happ because it is the stem of unrelated words like happen.\nNLTK has different modules for stemming and we will be using the PorterStemmer module which uses the Porter Stemming Algorithm. Let’s see how we can use it in the cell below.\n\nprint()\nprint('\\033[92m')\nprint(tweets_clean)\nprint('\\033[94m')\n\n# Instantiate stemming class\nstemmer = PorterStemmer() \n\n# Create an empty list to store the stems\ntweets_stem = [] \nfor word in tweets_clean:\n    stem_word = stemmer.stem(word)  # stemming word\n    tweets_stem.append(stem_word)  # append to the list\nprint('stemmed words:')\nprint(tweets_stem)\n\n\n\n['beautiful', 'sunflowers', 'sunny', 'friday', 'morning', ':)', 'sunflowers', 'favourites', 'happy', 'friday', '…']\n\nstemmed words:\n['beauti', 'sunflow', 'sunni', 'friday', 'morn', ':)', 'sunflow', 'favourit', 'happi', 'friday', '…']\n\n\nThat’s it! Now we have a set of words we can feed into to the next stage of our machine learning project.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Lab on preprocessing"
    ]
  },
  {
    "objectID": "posts/c1lab0/index.html#process_tweet",
    "href": "posts/c1lab0/index.html#process_tweet",
    "title": "Preprocessing",
    "section": "process_tweet()",
    "text": "process_tweet()\nAs shown above, preprocessing consists of multiple steps before you arrive at the final list of words. We will not ask you to replicate these however. In the week’s assignment, you will use the function process_tweet(tweet) available in utils.py. We encourage you to open the file and you’ll see that this function’s implementation is very similar to the steps above. To obtain the same result as in the previous code cells, you will only need to call the function process_tweet(). Let’s do that in the next cell.\n\nfrom utils import process_tweet # Import the process_tweet function\n\n# choose the same tweet\ntweet = all_positive_tweets[2277]\nprint()\nprint('\\033[92m')\nprint(tweet)\nprint('\\033[94m')\n\n# call the imported function\ntweets_stem = process_tweet(tweet); # Preprocess a given tweet\nprint('preprocessed tweet:')\nprint(tweets_stem) # Print the result\n\n\n\nMy beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i\n\npreprocessed tweet:\n['beauti', 'sunflow', 'sunni', 'friday', 'morn', ':)', 'sunflow', 'favourit', 'happi', 'friday', '…']\n\n\nThat’s it for this lab! You now know what is going on when you call the preprocessing helper function in this week’s assignment. Hopefully, this exercise has also given you some insights on how to tweak this for other types of text datasets.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Lab on preprocessing"
    ]
  },
  {
    "objectID": "posts/c1w2/code.html",
    "href": "posts/c1w2/code.html",
    "title": "Probability and Bayes Rule",
    "section": "",
    "text": "import pandas as pd\nimport string \nraw_tweets=[\n  \"I am happy because I am learning NLP\",\n  \"I am sad, I am not learning NLP\",\n  \"I am happy, not sad\",\n  \"I am sad, not happy\",\n]\ndef clean(tweet:str):\n  return  tweet.translate(str.maketrans('', '', string.punctuation)).lower()\ntweets = [clean(tweet) for tweet in raw_tweets]\nlabels=['+','-','+','-']\ndf = pd.DataFrame({'tweets': tweets, 'labels': labels})\ndf\n\n\n\n\n\n\n\n\ntweets\nlabels\n\n\n\n\n0\ni am happy because i am learning nlp\n+\n\n\n1\ni am sad i am not learning nlp\n-\n\n\n2\ni am happy not sad\n+\n\n\n3\ni am sad not happy\n-\n\n\n\n\n\n\n\n\nimport numpy as np\nfrom collections import Counter\np_freq,n_freq = Counter(), Counter()\n#print( df[df.labels == '+']['tweets'].to_list())\n[p_freq.update(tweet.split()) for tweet in df[df.labels == '+']['tweets'].to_list()]\n[n_freq.update(tweet.split()) for tweet in df[df.labels == '-']['tweets'].to_list()]\nprint(p_freq)\nprint(n_freq)\nvocab = list(set(p_freq.keys()).union(set(n_freq.keys())))\npos_freq = [p_freq[word] for word in vocab ]\nneg_freq = [n_freq[word] for word in vocab ]\nvocab_df=pd.DataFrame({'vocab':vocab,'pos_freq':pos_freq,'neg_freq':neg_freq})\nvocab_df['p_pos']=vocab_df.pos_freq/vocab_df.pos_freq.sum()\nvocab_df['p_neg']=vocab_df.neg_freq/vocab_df.neg_freq.sum()\nvocab_df['p_pos_sm']=(vocab_df.pos_freq+1)/(vocab_df.pos_freq.sum()+vocab_df.shape[1])\nvocab_df['p_neg_sm']=(vocab_df.neg_freq+1)/(vocab_df.neg_freq.sum()+vocab_df.shape[1])\nvocab_df['ratio']= vocab_df.p_pos_sm/vocab_df.p_neg_sm\nvocab_df['lambda']= np.log(vocab_df.p_pos_sm/vocab_df.p_neg_sm)\npd.set_option('display.float_format', '{:.2f}'.format)\nvocab_df\nprint(vocab_df.shape)\n\n[None, None]\n\n\n[None, None]\n\n\nCounter({'i': 3, 'am': 3, 'happy': 2, 'because': 1, 'learning': 1, 'nlp': 1, 'not': 1, 'sad': 1})\nCounter({'i': 3, 'am': 3, 'sad': 2, 'not': 2, 'learning': 1, 'nlp': 1, 'happy': 1})\n\n\n\n\n\n\n\n\n\nvocab\npos_freq\nneg_freq\np_pos\np_neg\np_pos_sm\np_neg_sm\nratio\nlambda\n\n\n\n\n0\nnot\n1\n2\n0.08\n0.15\n0.11\n0.16\n0.70\n-0.35\n\n\n1\ni\n3\n3\n0.23\n0.23\n0.22\n0.21\n1.06\n0.05\n\n\n2\nam\n3\n3\n0.23\n0.23\n0.22\n0.21\n1.06\n0.05\n\n\n3\nhappy\n2\n1\n0.15\n0.08\n0.17\n0.11\n1.58\n0.46\n\n\n4\nlearning\n1\n1\n0.08\n0.08\n0.11\n0.11\n1.06\n0.05\n\n\n5\nbecause\n1\n0\n0.08\n0.00\n0.11\n0.05\n2.11\n0.75\n\n\n6\nsad\n1\n2\n0.08\n0.15\n0.11\n0.16\n0.70\n-0.35\n\n\n7\nnlp\n1\n1\n0.08\n0.08\n0.11\n0.11\n1.06\n0.05\n\n\n\n\n\n\n\n(8, 9)\n\n\n\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\ntable = [[\"Sun\",696000,1989100000],\n         [\"Earth\",6371,5973.6],\n         [\"Moon\",1737,73.5],\n         [\"Mars\",3390,641.85]]\nMarkdown(tabulate(\n  table, \n  headers=[\"Planet\",\"R (km)\", \"mass (x 10^29 kg)\"]\n))\n\n\n\nTable 1: Planets\n\n\n\n\n\n\nPlanet\nR (km)\nmass (x 10^29 kg)\n\n\n\n\nSun\n696000\n1.9891e+09\n\n\nEarth\n6371\n5973.6\n\n\nMoon\n1737\n73.5\n\n\nMars\n3390\n641.85\n\n\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{2020,\n  author = {},\n  title = {Probability and {Bayes} {Rule}},\n  date = {2020-10-23},\n  url = {https://orenbochman.github.io/notes-nlp/posts/c1w2/code.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“Probability and Bayes Rule.” 2020. October 23, 2020. https://orenbochman.github.io/notes-nlp/posts/c1w2/code.html."
  },
  {
    "objectID": "posts/c1w2/assignment.html",
    "href": "posts/c1w2/assignment.html",
    "title": "Assignment 2: Naive Bayes",
    "section": "",
    "text": "course banner\nWelcome to week two of this specialization. You will learn about Naive Bayes. Concretely, you will be using Naive Bayes for sentiment analysis on tweets. Given a tweet, you will decide if it has a positive sentiment or a negative one. Specifically you will:\nYou may already be familiar with Naive Bayes and its justification in terms of conditional probabilities and independence. * In this week’s lectures and assignments we used the ratio of probabilities between positive and negative sentiments. * This approach gives us simpler formulas for these 2-way classification tasks.\nLoad the cell below to import some packages. You may want to browse the documentation of unfamiliar libraries and functions.\nfrom utils import process_tweet, lookup\nimport pdb\nfrom nltk.corpus import stopwords, twitter_samples\nimport numpy as np\nimport pandas as pd\nimport nltk\nimport string\nfrom nltk.tokenize import TweetTokenizer\nfrom os import getcwd\nIf you are running this notebook in your local computer, don’t forget to download the twitter samples and stopwords from nltk.\n# add folder, tmp2, from our local workspace containing pre-downloaded corpora files to nltk's data path\nfilePath = f\"{getcwd()}/../tmp2/\"\nnltk.data.path.append(filePath)\n# get the sets of positive and negative tweets\nall_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')\n\n# split the data into two pieces, one for training and one for testing (validation set)\ntest_pos = all_positive_tweets[4000:]\ntrain_pos = all_positive_tweets[:4000]\ntest_neg = all_negative_tweets[4000:]\ntrain_neg = all_negative_tweets[:4000]\n\ntrain_x = train_pos + train_neg\ntest_x = test_pos + test_neg\n\n# avoid assumptions about the length of all_positive_tweets\ntrain_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\ntest_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"
  },
  {
    "objectID": "posts/c1w2/assignment.html#part-1.1-implementing-your-helper-functions",
    "href": "posts/c1w2/assignment.html#part-1.1-implementing-your-helper-functions",
    "title": "Assignment 2: Naive Bayes",
    "section": "Part 1.1 Implementing your helper functions",
    "text": "Part 1.1 Implementing your helper functions\nTo help train your naive bayes model, you will need to build a dictionary where the keys are a (word, label) tuple and the values are the corresponding frequency. Note that the labels we’ll use here are 1 for positive and 0 for negative.\nYou will also implement a lookup() helper function that takes in the freqs dictionary, a word, and a label (1 or 0) and returns the number of times that word and label tuple appears in the collection of tweets.\nFor example: given a list of tweets [\"i am rather excited\", \"you are rather happy\"] and the label 1, the function will return a dictionary that contains the following key-value pairs:\n{ (“rather”, 1): 2 (“happi”, 1) : 1 (“excit”, 1) : 1 }\n\nNotice how for each word in the given string, the same label 1 is assigned to each word.\nNotice how the words “i” and “am” are not saved, since it was removed by process_tweet because it is a stopword.\nNotice how the word “rather” appears twice in the list of tweets, and so its count value is 2.\n\n\nInstructions\nCreate a function count_tweets() that takes a list of tweets as input, cleans all of them, and returns a dictionary. - The key in the dictionary is a tuple containing the stemmed word and its class label, e.g. (“happi”,1). - The value the number of times this word appears in the given collection of tweets (an integer).\n\n\nHints\n\n\n\n\nPlease use the process_tweet function that was imported above, and then store the words in their respective dictionaries and sets.\n\n\nYou may find it useful to use the zip function to match each element in tweets with each element in ys.\n\n\nRemember to check if the key in the dictionary exists before adding that key to the dictionary, or incrementing its value.\n\n\nAssume that the result dictionary that is input will contain clean key-value pairs (you can assume that the values will be integers that can be incremented). It is good practice to check the datatype before incrementing the value, but it’s not required here.\n\n\n\n\n# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef count_tweets(result, tweets, ys):\n    '''\n    Input:\n        result: a dictionary that will be used to map each pair to its frequency\n        tweets: a list of tweets\n        ys: a list corresponding to the sentiment of each tweet (either 0 or 1)\n    Output:\n        result: a dictionary mapping each pair to its frequency\n    '''\n\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    for y, tweet in zip(ys, tweets):\n        for word in process_tweet(tweet):\n            # define the key, which is the word and label tuple\n            pair = None\n\n            # if the key exists in the dictionary, increment the count\n            if pair in result:\n                result[pair] += None\n\n            # else, if the key is new, add it to the dictionary and set the count to 1\n            else:\n                result[pair] = None\n    ### END CODE HERE ###\n\n    return result\n\n\n# Testing your function\n\n\nresult = {}\ntweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\nys = [1, 0, 0, 0, 0]\ncount_tweets(result, tweets, ys)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[6], line 7\n      5 tweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n      6 ys = [1, 0, 0, 0, 0]\n----&gt; 7 count_tweets(result, tweets, ys)\n\nCell In[5], line 20, in count_tweets(result, tweets, ys)\n     18 # if the key exists in the dictionary, increment the count\n     19 if pair in result:\n---&gt; 20     result[pair] += None\n     22 # else, if the key is new, add it to the dictionary and set the count to 1\n     23 else:\n     24     result[pair] = None\n\nTypeError: unsupported operand type(s) for +=: 'NoneType' and 'NoneType'\n\n\n\nExpected Output: {(‘happi’, 1): 1, (‘trick’, 0): 1, (‘sad’, 0): 1, (‘tire’, 0): 2}"
  },
  {
    "objectID": "posts/c2w4/lab01.html",
    "href": "posts/c2w4/lab01.html",
    "title": "Word Embeddings First Steps: Data Preparation",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure 1\nIn this series of ungraded notebooks, you’ll try out all the individual techniques that you learned about in the lectures. Practicing on small examples will prepare you for the graded assignment, where you will combine the techniques in more advanced ways to create word embeddings from a real-life corpus.\nThis notebook focuses on data preparation, which is the first step of any machine learning algorithm. It is a very important step because models are only as good as the data they are trained on and the models used require the data to have a particular structure to process it properly.\nTo get started, import and initialize all the libraries you will need.\nimport re\nimport nltk\nimport emoji\nimport numpy as np\nfrom nltk.tokenize import word_tokenize\nfrom utils2 import get_dict"
  },
  {
    "objectID": "posts/c2w4/lab01.html#cleaning-and-tokenization",
    "href": "posts/c2w4/lab01.html#cleaning-and-tokenization",
    "title": "Word Embeddings First Steps: Data Preparation",
    "section": "Cleaning and tokenization",
    "text": "Cleaning and tokenization\nTo demonstrate the cleaning and tokenization process, consider a corpus that contains emojis and various punctuation signs.\n\n# Define a corpus\ncorpus = 'Who ❤️ \"word embeddings\" in 2020? I do!!!'\n\nFirst, replace all interrupting punctuation signs — such as commas and exclamation marks — with periods.\n\n# Print original corpus\nprint(f'Corpus:  {corpus}')\n\n# Do the substitution\ndata = re.sub(r'[,!?;-]+', '.', corpus)\n\n# Print cleaned corpus\nprint(f'After cleaning punctuation:  {data}')\n\nCorpus:  Who ❤️ \"word embeddings\" in 2020? I do!!!\nAfter cleaning punctuation:  Who ❤️ \"word embeddings\" in 2020. I do.\n\n\nNext, use NLTK’s tokenization engine to split the corpus into individual tokens.\n\n# Print cleaned corpus\nprint(f'Initial string:  {data}')\n\n# Tokenize the cleaned corpus\ndata = nltk.word_tokenize(data)\n\n# Print the tokenized version of the corpus\nprint(f'After tokenization:  {data}')\n\nInitial string:  Who ❤️ \"word embeddings\" in 2020. I do.\nAfter tokenization:  ['Who', '❤️', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '.']\n\n\nFinally, as you saw in the lecture, get rid of numbers and punctuation other than periods, and convert all the remaining tokens to lowercase.\n\n# Print the tokenized version of the corpus\nprint(f'Initial list of tokens:  {data}')\n\n# Filter tokenized corpus using list comprehension\ndata = [ ch.lower() for ch in data\n         if ch.isalpha()\n         or ch == '.'\n         or emoji.get_emoji_regexp().search(ch)\n       ]\n\n# Print the tokenized and filtered version of the corpus\nprint(f'After cleaning:  {data}')\n\nInitial list of tokens:  ['Who', '❤️', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '.']\nAfter cleaning:  ['who', '❤️', 'word', 'embeddings', 'in', '.', 'i', 'do', '.']\n\n\nNote that the heart emoji is considered as a token just like any normal word.\nNow let’s streamline the cleaning and tokenization process by wrapping the previous steps in a function.\n\n# Define the 'tokenize' function that will include the steps previously seen\ndef tokenize(corpus):\n    data = re.sub(r'[,!?;-]+', '.', corpus)\n    data = nltk.word_tokenize(data)  # tokenize string to words\n    data = [ ch.lower() for ch in data\n             if ch.isalpha()\n             or ch == '.'\n             or emoji.get_emoji_regexp().search(ch)\n           ]\n    return data\n\nApply this function to the corpus that you’ll be working on in the rest of this notebook: “I am happy because I am learning”\n\n# Define new corpus\ncorpus = 'I am happy because I am learning'\n\n# Print new corpus\nprint(f'Corpus:  {corpus}')\n\n# Save tokenized version of corpus into 'words' variable\nwords = tokenize(corpus)\n\n# Print the tokenized version of the corpus\nprint(f'Words (tokens):  {words}')\n\nCorpus:  I am happy because I am learning\nWords (tokens):  ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n\n\nNow try it out yourself with your own sentence.\n\n# Run this with any sentence\ntokenize(\"Now it's your turn: try with your own sentence!\")\n\n['now', 'it', 'your', 'turn', 'try', 'with', 'your', 'own', 'sentence', '.']"
  },
  {
    "objectID": "posts/c2w4/lab01.html#sliding-window-of-words",
    "href": "posts/c2w4/lab01.html#sliding-window-of-words",
    "title": "Word Embeddings First Steps: Data Preparation",
    "section": "Sliding window of words",
    "text": "Sliding window of words\nNow that you have transformed the corpus into a list of clean tokens, you can slide a window of words across this list. For each window you can extract a center word and the context words.\nThe get_windows function in the next cell was introduced in the lecture.\n\n# Define the 'get_windows' function\ndef get_windows(words, C):\n    i = C\n    while i &lt; len(words) - C:\n        center_word = words[i]\n        context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n        yield context_words, center_word\n        i += 1\n\nThe first argument of this function is a list of words (or tokens). The second argument, C, is the context half-size. Recall that for a given center word, the context words are made of C words to the left and C words to the right of the center word.\nHere is how you can use this function to extract context words and center words from a list of tokens. These context and center words will make up the training set that you will use to train the CBOW model.\n\n# Print 'context_words' and 'center_word' for the new corpus with a 'context half-size' of 2\nfor x, y in get_windows(['i', 'am', 'happy', 'because', 'i', 'am', 'learning'], 2):\n    print(f'{x}\\t{y}')\n\n['i', 'am', 'because', 'i'] happy\n['am', 'happy', 'i', 'am']  because\n['happy', 'because', 'am', 'learning']  i\n\n\nThe first example of the training set is made of:\n\nthe context words “i”, “am”, “because”, “i”,\nand the center word to be predicted: “happy”.\n\nNow try it out yourself. In the next cell, you can change both the sentence and the context half-size.\n\n# Print 'context_words' and 'center_word' for any sentence with a 'context half-size' of 1\nfor x, y in get_windows(tokenize(\"Now it's your turn: try with your own sentence!\"), 1):\n    print(f'{x}\\t{y}')\n\n['now', 'your'] it\n['it', 'turn']  your\n['your', 'try'] turn\n['turn', 'with']    try\n['try', 'your'] with\n['with', 'own'] your\n['your', 'sentence']    own\n['own', '.']    sentence"
  },
  {
    "objectID": "posts/c2w4/lab01.html#transforming-words-into-vectors-for-the-training-set",
    "href": "posts/c2w4/lab01.html#transforming-words-into-vectors-for-the-training-set",
    "title": "Word Embeddings First Steps: Data Preparation",
    "section": "Transforming words into vectors for the training set",
    "text": "Transforming words into vectors for the training set\nTo finish preparing the training set, you need to transform the context words and center words into vectors.\n\nMapping words to indices and indices to words\nThe center words will be represented as one-hot vectors, and the vectors that represent context words are also based on one-hot vectors.\nTo create one-hot word vectors, you can start by mapping each unique word to a unique integer (or index). We have provided a helper function, get_dict, that creates a Python dictionary that maps words to integers and back.\n\n# Get 'word2Ind' and 'Ind2word' dictionaries for the tokenized corpus\nword2Ind, Ind2word = get_dict(words)\n\nHere’s the dictionary that maps words to numeric indices.\n\n# Print 'word2Ind' dictionary\nword2Ind\n\n{'am': 0, 'because': 1, 'happy': 2, 'i': 3, 'learning': 4}\n\n\nYou can use this dictionary to get the index of a word.\n\n# Print value for the key 'i' within word2Ind dictionary\nprint(\"Index of the word 'i':  \",word2Ind['i'])\n\nIndex of the word 'i':   3\n\n\nAnd conversely, here’s the dictionary that maps indices to words.\n\n# Print 'Ind2word' dictionary\nInd2word\n\n{0: 'am', 1: 'because', 2: 'happy', 3: 'i', 4: 'learning'}\n\n\n\n# Print value for the key '2' within Ind2word dictionary\nprint(\"Word which has index 2:  \",Ind2word[2] )\n\nWord which has index 2:   happy\n\n\nFinally, get the length of either of these dictionaries to get the size of the vocabulary of your corpus, in other words the number of different words making up the corpus.\n\n# Save length of word2Ind dictionary into the 'V' variable\nV = len(word2Ind)\n\n# Print length of word2Ind dictionary\nprint(\"Size of vocabulary: \", V)\n\nSize of vocabulary:  5\n\n\n\n\nGetting one-hot word vectors\nRecall from the lecture that you can easily convert an integer, n, into a one-hot vector.\nConsider the word “happy”. First, retrieve its numeric index.\n\n# Save index of word 'happy' into the 'n' variable\nn = word2Ind['happy']\n\n# Print index of word 'happy'\nn\n\n2\n\n\nNow create a vector with the size of the vocabulary, and fill it with zeros.\n\n# Create vector with the same length as the vocabulary, filled with zeros\ncenter_word_vector = np.zeros(V)\n\n# Print vector\ncenter_word_vector\n\narray([0., 0., 0., 0., 0.])\n\n\nYou can confirm that the vector has the right size.\n\n# Assert that the length of the vector is the same as the size of the vocabulary\nlen(center_word_vector) == V\n\nTrue\n\n\nNext, replace the 0 of the n-th element with a 1.\n\n# Replace element number 'n' with a 1\ncenter_word_vector[n] = 1\n\nAnd you have your one-hot word vector.\n\n# Print vector\ncenter_word_vector\n\narray([0., 0., 1., 0., 0.])\n\n\nYou can now group all of these steps in a convenient function, which takes as parameters: a word to be encoded, a dictionary that maps words to indices, and the size of the vocabulary.\n\n# Define the 'word_to_one_hot_vector' function that will include the steps previously seen\ndef word_to_one_hot_vector(word, word2Ind, V):\n    one_hot_vector = np.zeros(V)\n    one_hot_vector[word2Ind[word]] = 1\n    return one_hot_vector\n\nCheck that it works as intended.\n\n# Print output of 'word_to_one_hot_vector' function for word 'happy'\nword_to_one_hot_vector('happy', word2Ind, V)\n\narray([0., 0., 1., 0., 0.])\n\n\nWhat is the word vector for “learning”?\n\n# Print output of 'word_to_one_hot_vector' function for word 'learning'\nword_to_one_hot_vector('learning', word2Ind, V)\n\narray([0., 0., 0., 0., 1.])\n\n\nExpected output:\narray([0., 0., 0., 0., 1.])\n\n\nGetting context word vectors\nTo create the vectors that represent context words, you will calculate the average of the one-hot vectors representing the individual words.\nLet’s start with a list of context words.\n\n# Define list containing context words\ncontext_words = ['i', 'am', 'because', 'i']\n\nUsing Python’s list comprehension construct and the word_to_one_hot_vector function that you created in the previous section, you can create a list of one-hot vectors representing each of the context words.\n\n# Create one-hot vectors for each context word using list comprehension\ncontext_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n\n# Print one-hot vectors for each context word\ncontext_words_vectors\n\n[array([0., 0., 0., 1., 0.]),\n array([1., 0., 0., 0., 0.]),\n array([0., 1., 0., 0., 0.]),\n array([0., 0., 0., 1., 0.])]\n\n\nAnd you can now simply get the average of these vectors using numpy’s mean function, to get the vector representation of the context words.\n\n# Compute mean of the vectors using numpy\nnp.mean(context_words_vectors, axis=0)\n\narray([0.25, 0.25, 0.  , 0.5 , 0.  ])\n\n\nNote the axis=0 parameter that tells mean to calculate the average of the rows (if you had wanted the average of the columns, you would have used axis=1).\nNow create the context_words_to_vector function that takes in a list of context words, a word-to-index dictionary, and a vocabulary size, and outputs the vector representation of the context words.\n\n# Define the 'context_words_to_vector' function that will include the steps previously seen\ndef context_words_to_vector(context_words, word2Ind, V):\n    context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n    context_words_vectors = np.mean(context_words_vectors, axis=0)\n    return context_words_vectors\n\nAnd check that you obtain the same output as the manual approach above.\n\n# Print output of 'context_words_to_vector' function for context words: 'i', 'am', 'because', 'i'\ncontext_words_to_vector(['i', 'am', 'because', 'i'], word2Ind, V)\n\narray([0.25, 0.25, 0.  , 0.5 , 0.  ])\n\n\nWhat is the vector representation of the context words “am happy i am”?\n\n# Print output of 'context_words_to_vector' function for context words: 'am', 'happy', 'i', 'am'\ncontext_words_to_vector(['am', 'happy', 'i', 'am'], word2Ind, V)\n\narray([0.5 , 0.  , 0.25, 0.25, 0.  ])\n\n\nExpected output:\narray([0.5 , 0.  , 0.25, 0.25, 0.  ])"
  },
  {
    "objectID": "posts/c2w4/lab01.html#building-the-training-set",
    "href": "posts/c2w4/lab01.html#building-the-training-set",
    "title": "Word Embeddings First Steps: Data Preparation",
    "section": "Building the training set",
    "text": "Building the training set\nYou can now combine the functions that you created in the previous sections, to build a training set for the CBOW model, starting from the following tokenized corpus.\n\n# Print corpus\nwords\n\n['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n\n\nTo do this you need to use the sliding window function (get_windows) to extract the context words and center words, and you then convert these sets of words into a basic vector representation using word_to_one_hot_vector and context_words_to_vector.\n\n# Print vectors associated to center and context words for corpus\nfor context_words, center_word in get_windows(words, 2):  # reminder: 2 is the context half-size\n    print(f'Context words:  {context_words} -&gt; {context_words_to_vector(context_words, word2Ind, V)}')\n    print(f'Center word:  {center_word} -&gt; {word_to_one_hot_vector(center_word, word2Ind, V)}')\n    print()\n\nContext words:  ['i', 'am', 'because', 'i'] -&gt; [0.25 0.25 0.   0.5  0.  ]\nCenter word:  happy -&gt; [0. 0. 1. 0. 0.]\n\nContext words:  ['am', 'happy', 'i', 'am'] -&gt; [0.5  0.   0.25 0.25 0.  ]\nCenter word:  because -&gt; [0. 1. 0. 0. 0.]\n\nContext words:  ['happy', 'because', 'am', 'learning'] -&gt; [0.25 0.25 0.25 0.   0.25]\nCenter word:  i -&gt; [0. 0. 0. 1. 0.]\n\n\n\nIn this practice notebook you’ll be performing a single iteration of training using a single example, but in this week’s assignment you’ll train the CBOW model using several iterations and batches of example. Here is how you would use a Python generator function (remember the yield keyword from the lecture?) to make it easier to iterate over a set of examples.\n\n# Define the generator function 'get_training_example'\ndef get_training_example(words, C, word2Ind, V):\n    for context_words, center_word in get_windows(words, C):\n        yield context_words_to_vector(context_words, word2Ind, V), word_to_one_hot_vector(center_word, word2Ind, V)\n\nThe output of this function can be iterated on to get successive context word vectors and center word vectors, as demonstrated in the next cell.\n\n# Print vectors associated to center and context words for corpus using the generator function\nfor context_words_vector, center_word_vector in get_training_example(words, 2, word2Ind, V):\n    print(f'Context words vector:  {context_words_vector}')\n    print(f'Center word vector:  {center_word_vector}')\n    print()\n\nContext words vector:  [0.25 0.25 0.   0.5  0.  ]\nCenter word vector:  [0. 0. 1. 0. 0.]\n\nContext words vector:  [0.5  0.   0.25 0.25 0.  ]\nCenter word vector:  [0. 1. 0. 0. 0.]\n\nContext words vector:  [0.25 0.25 0.25 0.   0.25]\nCenter word vector:  [0. 0. 0. 1. 0.]\n\n\n\nYour training set is ready, you can now move on to the CBOW model itself which will be covered in the next lecture notebook.\nCongratulations on finishing this lecture notebook! Hopefully you now have a better understanding of how to prepare your data before feeding it to a continuous bag-of-words model.\nKeep it up!"
  },
  {
    "objectID": "posts/c2w4/lab04.html",
    "href": "posts/c2w4/lab04.html",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure 1\nIn this ungraded notebook, you’ll try out all the individual techniques that you learned about in the lecture. Practicing on small examples will prepare you for the graded assignment, where you will combine the techniques in more advanced ways to create word embeddings from a real-life corpus.\nThis notebook is made of two main parts: data preparation, and the continuous bag-of-words (CBOW) model.\nTo get started, import and initialize all the libraries you will need.\nimport sys\n!{sys.executable} -m pip install emoji\n\nRequirement already satisfied: emoji in /home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages (1.4.1)\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize\nimport emoji\nimport numpy as np\n\nfrom utils2 import get_dict\n\nnltk.download('punkt')  # download pre-trained Punkt tokenizer for English\n\n[nltk_data] Downloading package punkt to /home/oren/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\nTrue"
  },
  {
    "objectID": "posts/c2w4/lab04.html#cleaning-and-tokenization",
    "href": "posts/c2w4/lab04.html#cleaning-and-tokenization",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Cleaning and tokenization",
    "text": "Cleaning and tokenization\nTo demonstrate the cleaning and tokenization process, consider a corpus that contains emojis and various punctuation signs.\n\ncorpus = 'Who ❤️ \"word embeddings\" in 2020? I do!!!'\n\nFirst, replace all interrupting punctuation signs — such as commas and exclamation marks — with periods.\n\nprint(f'Corpus:  {corpus}')\ndata = re.sub(r'[,!?;-]+', '.', corpus)\nprint(f'After cleaning punctuation:  {data}')\n\nCorpus:  Who ❤️ \"word embeddings\" in 2020? I do!!!\nAfter cleaning punctuation:  Who ❤️ \"word embeddings\" in 2020. I do.\n\n\nNext, use NLTK’s tokenization engine to split the corpus into individual tokens.\n\nprint(f'Initial string:  {data}')\ndata = nltk.word_tokenize(data)\nprint(f'After tokenization:  {data}')\n\nInitial string:  Who ❤️ \"word embeddings\" in 2020. I do.\nAfter tokenization:  ['Who', '❤️', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '.']\n\n\nFinally, as you saw in the lecture, get rid of numbers and punctuation other than periods, and convert all the remaining tokens to lowercase.\n\nprint(f'Initial list of tokens:  {data}')\ndata = [ ch.lower() for ch in data\n         if ch.isalpha()\n         or ch == '.'\n         or emoji.get_emoji_regexp().search(ch)\n       ]\nprint(f'After cleaning:  {data}')\n\nInitial list of tokens:  ['Who', '❤️', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '.']\nAfter cleaning:  ['who', '❤️', 'word', 'embeddings', 'in', '.', 'i', 'do', '.']\n\n\nNote that the heart emoji is considered as a token just like any normal word.\nNow let’s streamline the cleaning and tokenization process by wrapping the previous steps in a function.\n\ndef tokenize(corpus):\n    data = re.sub(r'[,!?;-]+', '.', corpus)\n    data = nltk.word_tokenize(data)  # tokenize string to words\n    data = [ ch.lower() for ch in data\n             if ch.isalpha()\n             or ch == '.'\n             or emoji.get_emoji_regexp().search(ch)\n           ]\n    return data\n\nApply this function to the corpus that you’ll be working on in the rest of this notebook: “I am happy because I am learning”\n\ncorpus = 'I am happy because I am learning'\nprint(f'Corpus:  {corpus}')\nwords = tokenize(corpus)\nprint(f'Words (tokens):  {words}')\n\nCorpus:  I am happy because I am learning\nWords (tokens):  ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n\n\nNow try it out yourself with your own sentence.\n\ntokenize(\"Now it's your turn: try with your own sentence!\")\n\n['now', 'it', 'your', 'turn', 'try', 'with', 'your', 'own', 'sentence', '.']"
  },
  {
    "objectID": "posts/c2w4/lab04.html#sliding-window-of-words",
    "href": "posts/c2w4/lab04.html#sliding-window-of-words",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Sliding window of words",
    "text": "Sliding window of words\nNow that you have transformed the corpus into a list of clean tokens, you can slide a window of words across this list. For each window you can extract a center word and the context words.\nThe get_windows function in the next cell was introduced in the lecture.\n\ndef get_windows(words, C):\n    i = C\n    while i &lt; len(words) - C:\n        center_word = words[i]\n        context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n        yield context_words, center_word\n        i += 1\n\nThe first argument of this function is a list of words (or tokens). The second argument, C, is the context half-size. Recall that for a given center word, the context words are made of C words to the left and C words to the right of the center word.\nHere is how you can use this function to extract context words and center words from a list of tokens. These context and center words will make up the training set that you will use to train the CBOW model.\n\nfor x, y in get_windows(\n            ['i', 'am', 'happy', 'because', 'i', 'am', 'learning'],\n            2\n        ):\n    print(f'{x}\\t{y}')\n\n['i', 'am', 'because', 'i'] happy\n['am', 'happy', 'i', 'am']  because\n['happy', 'because', 'am', 'learning']  i\n\n\nThe first example of the training set is made of:\n\nthe context words “i”, “am”, “because”, “i”,\nand the center word to be predicted: “happy”.\n\nNow try it out yourself. In the next cell, you can change both the sentence and the context half-size.\n\nfor x, y in get_windows(tokenize(\"Now it's your turn: try with your own sentence!\"), 1):\n    print(f'{x}\\t{y}')\n\n['now', 'your'] it\n['it', 'turn']  your\n['your', 'try'] turn\n['turn', 'with']    try\n['try', 'your'] with\n['with', 'own'] your\n['your', 'sentence']    own\n['own', '.']    sentence"
  },
  {
    "objectID": "posts/c2w4/lab04.html#transforming-words-into-vectors-for-the-training-set",
    "href": "posts/c2w4/lab04.html#transforming-words-into-vectors-for-the-training-set",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Transforming words into vectors for the training set",
    "text": "Transforming words into vectors for the training set\nTo finish preparing the training set, you need to transform the context words and center words into vectors.\n\nMapping words to indices and indices to words\nThe center words will be represented as one-hot vectors, and the vectors that represent context words are also based on one-hot vectors.\nTo create one-hot word vectors, you can start by mapping each unique word to a unique integer (or index). We have provided a helper function, get_dict, that creates a Python dictionary that maps words to integers and back.\n\nword2Ind, Ind2word = get_dict(words)\n\nHere’s the dictionary that maps words to numeric indices.\n\nword2Ind\n\n{'am': 0, 'because': 1, 'happy': 2, 'i': 3, 'learning': 4}\n\n\nYou can use this dictionary to get the index of a word.\n\nprint(\"Index of the word 'i':  \",word2Ind['i'])\n\nIndex of the word 'i':   3\n\n\nAnd conversely, here’s the dictionary that maps indices to words.\n\nInd2word\n\n{0: 'am', 1: 'because', 2: 'happy', 3: 'i', 4: 'learning'}\n\n\n\nprint(\"Word which has index 2:  \",Ind2word[2] )\n\nWord which has index 2:   happy\n\n\nFinally, get the length of either of these dictionaries to get the size of the vocabulary of your corpus, in other words the number of different words making up the corpus.\n\nV = len(word2Ind)\nprint(\"Size of vocabulary: \", V)\n\nSize of vocabulary:  5\n\n\n\n\nGetting one-hot word vectors\nRecall from the lecture that you can easily convert an integer, n, into a one-hot vector.\nConsider the word “happy”. First, retrieve its numeric index.\n\nn = word2Ind['happy']\nn\n\n2\n\n\nNow create a vector with the size of the vocabulary, and fill it with zeros.\n\ncenter_word_vector = np.zeros(V)\ncenter_word_vector\n\narray([0., 0., 0., 0., 0.])\n\n\nYou can confirm that the vector has the right size.\n\nlen(center_word_vector) == V\n\nTrue\n\n\nNext, replace the 0 of the n-th element with a 1.\n\ncenter_word_vector[n] = 1\n\nAnd you have your one-hot word vector.\n\ncenter_word_vector\n\narray([0., 0., 1., 0., 0.])\n\n\nYou can now group all of these steps in a convenient function, which takes as parameters: a word to be encoded, a dictionary that maps words to indices, and the size of the vocabulary.\n\ndef word_to_one_hot_vector(word, word2Ind, V):\n    # BEGIN your code here\n    one_hot_vector = np.zeros(V)\n    one_hot_vector[word2Ind[word]] = 1\n    # END your code here\n    return one_hot_vector\n\nCheck that it works as intended.\n\nword_to_one_hot_vector('happy', word2Ind, V)\n\narray([0., 0., 1., 0., 0.])\n\n\nWhat is the word vector for “learning”?\n\n# BEGIN your code here\nword_to_one_hot_vector('learning', word2Ind, V)\n# END your code here\n\narray([0., 0., 0., 0., 1.])\n\n\nExpected output:\narray([0., 0., 0., 0., 1.])\n\n\nGetting context word vectors\nTo create the vectors that represent context words, you will calculate the average of the one-hot vectors representing the individual words.\nLet’s start with a list of context words.\n\ncontext_words = ['i', 'am', 'because', 'i']\n\nUsing Python’s list comprehension construct and the word_to_one_hot_vector function that you created in the previous section, you can create a list of one-hot vectors representing each of the context words.\n\ncontext_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\ncontext_words_vectors\n\n[array([0., 0., 0., 1., 0.]),\n array([1., 0., 0., 0., 0.]),\n array([0., 1., 0., 0., 0.]),\n array([0., 0., 0., 1., 0.])]\n\n\nAnd you can now simply get the average of these vectors using numpy’s mean function, to get the vector representation of the context words.\n\nnp.mean(context_words_vectors, axis=0)\n\narray([0.25, 0.25, 0.  , 0.5 , 0.  ])\n\n\nNote the axis=0 parameter that tells mean to calculate the average of the rows (if you had wanted the average of the columns, you would have used axis=1).\nNow create the context_words_to_vector function that takes in a list of context words, a word-to-index dictionary, and a vocabulary size, and outputs the vector representation of the context words.\n\ndef context_words_to_vector(context_words, word2Ind, V):\n    # BEGIN your code here\n    context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n    context_words_vectors = np.mean(context_words_vectors, axis=0)\n    # END your code here\n    return context_words_vectors\n\nAnd check that you obtain the same output as the manual approach above.\n\ncontext_words_to_vector(['i', 'am', 'because', 'i'], word2Ind, V)\n\narray([0.25, 0.25, 0.  , 0.5 , 0.  ])\n\n\nWhat is the vector representation of the context words “am happy i am”?\n\n# BEGIN your code here\ncontext_words_to_vector(['am', 'happy', 'i', 'am'], word2Ind, V)\n# END your code here\n\narray([0.5 , 0.  , 0.25, 0.25, 0.  ])\n\n\nExpected output:\narray([0.5 , 0.  , 0.25, 0.25, 0.  ])"
  },
  {
    "objectID": "posts/c2w4/lab04.html#building-the-training-set",
    "href": "posts/c2w4/lab04.html#building-the-training-set",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Building the training set",
    "text": "Building the training set\nYou can now combine the functions that you created in the previous sections, to build a training set for the CBOW model, starting from the following tokenized corpus.\n\nwords\n\n['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n\n\nTo do this you need to use the sliding window function (get_windows) to extract the context words and center words, and you then convert these sets of words into a basic vector representation using word_to_one_hot_vector and context_words_to_vector.\n\nfor context_words, center_word in get_windows(words, 2):  # reminder: 2 is the context half-size\n    print(f'Context words:  {context_words} -&gt; {context_words_to_vector(context_words, word2Ind, V)}')\n    print(f'Center word:  {center_word} -&gt; {word_to_one_hot_vector(center_word, word2Ind, V)}')\n    print()\n\nContext words:  ['i', 'am', 'because', 'i'] -&gt; [0.25 0.25 0.   0.5  0.  ]\nCenter word:  happy -&gt; [0. 0. 1. 0. 0.]\n\nContext words:  ['am', 'happy', 'i', 'am'] -&gt; [0.5  0.   0.25 0.25 0.  ]\nCenter word:  because -&gt; [0. 1. 0. 0. 0.]\n\nContext words:  ['happy', 'because', 'am', 'learning'] -&gt; [0.25 0.25 0.25 0.   0.25]\nCenter word:  i -&gt; [0. 0. 0. 1. 0.]\n\n\n\nIn this practice notebook you’ll be performing a single iteration of training using a single example, but in this week’s assignment you’ll train the CBOW model using several iterations and batches of example. Here is how you would use a Python generator function (remember the yield keyword from the lecture?) to make it easier to iterate over a set of examples.\n\ndef get_training_example(words, C, word2Ind, V):\n    for context_words, center_word in get_windows(words, C):\n        yield context_words_to_vector(context_words, word2Ind, V), word_to_one_hot_vector(center_word, word2Ind, V)\n\nThe output of this function can be iterated on to get successive context word vectors and center word vectors, as demonstrated in the next cell.\n\nfor context_words_vector, center_word_vector in get_training_example(words, 2, word2Ind, V):\n    print(f'Context words vector:  {context_words_vector}')\n    print(f'Center word vector:  {center_word_vector}')\n    print()\n\nContext words vector:  [0.25 0.25 0.   0.5  0.  ]\nCenter word vector:  [0. 0. 1. 0. 0.]\n\nContext words vector:  [0.5  0.   0.25 0.25 0.  ]\nCenter word vector:  [0. 1. 0. 0. 0.]\n\nContext words vector:  [0.25 0.25 0.25 0.   0.25]\nCenter word vector:  [0. 0. 0. 1. 0.]\n\n\n\nYour training set is ready, you can now move on to the CBOW model itself."
  },
  {
    "objectID": "posts/c2w4/lab04.html#activation-functions",
    "href": "posts/c2w4/lab04.html#activation-functions",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Activation functions",
    "text": "Activation functions\nLet’s start by implementing the activation functions, ReLU and softmax.\n\nReLU\nReLU is used to calculate the values of the hidden layer, in the following formulas:\n\\begin{align}\n\\mathbf{z_1} &= \\mathbf{W_1}\\mathbf{x} + \\mathbf{b_1}  \\tag{1} \\\\\n\\mathbf{h} &= \\mathrm{ReLU}(\\mathbf{z_1})  \\tag{2} \\\\\n\\end{align}\nLet’s fix a value for \\mathbf{z_1} as a working example.\n\nnp.random.seed(10)\nz_1 = 10*np.random.rand(5, 1)-5\nz_1\n\narray([[ 2.71320643],\n       [-4.79248051],\n       [ 1.33648235],\n       [ 2.48803883],\n       [-0.01492988]])\n\n\nTo get the ReLU of this vector, you want all the negative values to become zeros.\nFirst create a copy of this vector.\n\nh = z_1.copy()\n\nNow determine which of its values are negative.\n\nh &lt; 0\n\narray([[False],\n       [ True],\n       [False],\n       [False],\n       [ True]])\n\n\nYou can now simply set all of the values which are negative to 0.\n\nh[h &lt; 0] = 0\n\nAnd that’s it: you have the ReLU of \\mathbf{z_1}!\n\nh\n\narray([[2.71320643],\n       [0.        ],\n       [1.33648235],\n       [2.48803883],\n       [0.        ]])\n\n\nNow implement ReLU as a function.\n\ndef relu(z):\n    # BEGIN your code here\n    result = z.copy()\n    result[result &lt; 0] = 0\n    # END your code here\n    \n    return result\n\nAnd check that it’s working.\n\nz = np.array([[-1.25459881], [ 4.50714306], [ 2.31993942], [ 0.98658484], [-3.4398136 ]])\nrelu(z)\n\narray([[0.        ],\n       [4.50714306],\n       [2.31993942],\n       [0.98658484],\n       [0.        ]])\n\n\nExpected output:\narray([[0.        ],\n       [4.50714306],\n       [2.31993942],\n       [0.98658484],\n       [0.        ]])\n\n\nSoftmax\nThe second activation function that you need is softmax. This function is used to calculate the values of the output layer of the neural network, using the following formulas:\n\\begin{align}\n\\mathbf{z_2} &= \\mathbf{W_2}\\mathbf{h} + \\mathbf{b_2}   \\tag{3} \\\\\n\\mathbf{\\hat y} &= \\mathrm{softmax}(\\mathbf{z_2})   \\tag{4} \\\\\n\\end{align}\nTo calculate softmax of a vector \\mathbf{z}, the i-th component of the resulting vector is given by:\n \\textrm{softmax}(\\textbf{z})_i = \\frac{e^{z_i} }{\\sum\\limits_{j=1}^{V} e^{z_j} }  \\tag{5} \nLet’s work through an example.\n\nz = np.array([9, 8, 11, 10, 8.5])\nz\n\narray([ 9. ,  8. , 11. , 10. ,  8.5])\n\n\nYou’ll need to calculate the exponentials of each element, both for the numerator and for the denominator.\n\ne_z = np.exp(z)\ne_z\n\narray([ 8103.08392758,  2980.95798704, 59874.1417152 , 22026.46579481,\n        4914.7688403 ])\n\n\nThe denominator is equal to the sum of these exponentials.\n\nsum_e_z = np.sum(e_z)\nsum_e_z\n\nnp.float64(97899.41826492078)\n\n\nAnd the value of the first element of \\textrm{softmax}(\\textbf{z}) is given by:\n\ne_z[0]/sum_e_z\n\nnp.float64(0.08276947985173956)\n\n\nThis is for one element. You can use numpy’s vectorized operations to calculate the values of all the elements of the \\textrm{softmax}(\\textbf{z}) vector in one go.\nImplement the softmax function.\n\ndef softmax(z):\n    # BEGIN your code here\n    e_z = np.exp(z)\n    sum_e_z = np.sum(e_z)\n    return e_z / sum_e_z\n    # END your code here\n\nNow check that it works.\n\nsoftmax([9, 8, 11, 10, 8.5])\n\narray([0.08276948, 0.03044919, 0.61158833, 0.22499077, 0.05020223])\n\n\nExpected output:\narray([0.08276948, 0.03044919, 0.61158833, 0.22499077, 0.05020223])"
  },
  {
    "objectID": "posts/c2w4/lab04.html#dimensions-1-d-arrays-vs-2-d-column-vectors",
    "href": "posts/c2w4/lab04.html#dimensions-1-d-arrays-vs-2-d-column-vectors",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Dimensions: 1-D arrays vs 2-D column vectors",
    "text": "Dimensions: 1-D arrays vs 2-D column vectors\nBefore moving on to implement forward propagation, backpropagation, and gradient descent, let’s have a look at the dimensions of the vectors you’ve been handling until now.\nCreate a vector of length V filled with zeros.\n\nx_array = np.zeros(V)\nx_array\n\narray([0., 0., 0., 0., 0.])\n\n\nThis is a 1-dimensional array, as revealed by the .shape property of the array.\n\nx_array.shape\n\n(5,)\n\n\nTo perform matrix multiplication in the next steps, you actually need your column vectors to be represented as a matrix with one column. In numpy, this matrix is represented as a 2-dimensional array.\nThe easiest way to convert a 1D vector to a 2D column matrix is to set its .shape property to the number of rows and one column, as shown in the next cell.\n\nx_column_vector = x_array.copy()\nx_column_vector.shape = (V, 1)  # alternatively ... = (x_array.shape[0], 1)\nx_column_vector\n\narray([[0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.]])\n\n\nThe shape of the resulting “vector” is:\n\nx_column_vector.shape\n\n(5, 1)\n\n\nSo you now have a 5x1 matrix that you can use to perform standard matrix multiplication."
  },
  {
    "objectID": "posts/c2w4/lab04.html#forward-propagation",
    "href": "posts/c2w4/lab04.html#forward-propagation",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Forward propagation",
    "text": "Forward propagation\nLet’s dive into the neural network itself, which is shown below with all the dimensions and formulas you’ll need.\n\n Figure 2\n\nSet N equal to 3. Remember that N is a hyperparameter of the CBOW model that represents the size of the word embedding vectors, as well as the size of the hidden layer.\n\nN = 3\n\n\nInitialization of the weights and biases\nBefore you start training the neural network, you need to initialize the weight matrices and bias vectors with random values.\nIn the assignment you will implement a function to do this yourself using numpy.random.rand. In this notebook, we’ve pre-populated these matrices and vectors for you.\n\nW1 = np.array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n               [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n               [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n\nW2 = np.array([[-0.22182064, -0.43008631,  0.13310965],\n               [ 0.08476603,  0.08123194,  0.1772054 ],\n               [ 0.1871551 , -0.06107263, -0.1790735 ],\n               [ 0.07055222, -0.02015138,  0.36107434],\n               [ 0.33480474, -0.39423389, -0.43959196]])\n\nb1 = np.array([[ 0.09688219],\n               [ 0.29239497],\n               [-0.27364426]])\n\nb2 = np.array([[ 0.0352008 ],\n               [-0.36393384],\n               [-0.12775555],\n               [-0.34802326],\n               [-0.07017815]])\n\nCheck that the dimensions of these matrices match those shown in the figure above.\n\n# BEGIN your code here\nprint(f'V (vocabulary size): {V}')\nprint(f'N (embedding size / size of the hidden layer): {N}')\nprint(f'size of W1: {W1.shape} (NxV)')\nprint(f'size of b1: {b1.shape} (Nx1)')\nprint(f'size of W2: {W1.shape} (VxN)')\nprint(f'size of b2: {b2.shape} (Vx1)')\n# END your code here\n\nV (vocabulary size): 5\nN (embedding size / size of the hidden layer): 3\nsize of W1: (3, 5) (NxV)\nsize of b1: (3, 1) (Nx1)\nsize of W2: (3, 5) (VxN)\nsize of b2: (5, 1) (Vx1)\n\n\n\n\nTraining example\nRun the next cells to get the first training example, made of the vector representing the context words “i am because i”, and the target which is the one-hot vector representing the center word “happy”.\n\nYou don’t need to worry about the Python syntax, but there are some explanations below if you want to know what’s happening behind the scenes.\n\n\ntraining_examples = get_training_example(words, 2, word2Ind, V)\n\n\nget_training_examples, which uses the yield keyword, is known as a generator. When run, it builds an iterator, which is a special type of object that… you can iterate on (using a for loop for instance), to retrieve the successive values that the function generates.\nIn this case get_training_examples yields training examples, and iterating on training_examples will return the successive training examples.\n\n\nx_array, y_array = next(training_examples)\n\n\nnext is another special keyword, which gets the next available value from an iterator. Here, you’ll get the very first value, which is the first training example. If you run this cell again, you’ll get the next value, and so on until the iterator runs out of values to return.\nIn this notebook next is used because you will only be performing one iteration of training. In this week’s assignment with the full training over several iterations you’ll use regular for loops with the iterator that supplies the training examples.\n\nThe vector representing the context words, which will be fed into the neural network, is:\n\nx_array\n\narray([0.25, 0.25, 0.  , 0.5 , 0.  ])\n\n\nThe one-hot vector representing the center word to be predicted is:\n\ny_array\n\narray([0., 0., 1., 0., 0.])\n\n\nNow convert these vectors into matrices (or 2D arrays) to be able to perform matrix multiplication on the right types of objects, as explained above.\n\nx = x_array.copy()\nx.shape = (V, 1)\nprint('x')\nprint(x)\nprint()\n\ny = y_array.copy()\ny.shape = (V, 1)\nprint('y')\nprint(y)\n\nx\n[[0.25]\n [0.25]\n [0.  ]\n [0.5 ]\n [0.  ]]\n\ny\n[[0.]\n [0.]\n [1.]\n [0.]\n [0.]]\n\n\n\n\nValues of the hidden layer\nNow that you have initialized all the variables that you need for forward propagation, you can calculate the values of the hidden layer using the following formulas:\n\\begin{align}\n\\mathbf{z_1} = \\mathbf{W_1}\\mathbf{x} + \\mathbf{b_1}  \\tag{1} \\\\\n\\mathbf{h} = \\mathrm{ReLU}(\\mathbf{z_1})  \\tag{2} \\\\\n\\end{align}\nFirst, you can calculate the value of \\mathbf{z_1}.\n\nz1 = np.dot(W1, x) + b1\n\n\n np.dot is numpy’s function for matrix multiplication.\n\nAs expected you get an N by 1 matrix, or column vector with N elements, where N is equal to the embedding size, which is 3 in this example.\n\nz1\n\narray([[ 0.36483875],\n       [ 0.63710329],\n       [-0.3236647 ]])\n\n\nYou can now take the ReLU of \\mathbf{z_1} to get \\mathbf{h}, the vector with the values of the hidden layer.\n\nh = relu(z1)\nh\n\narray([[0.36483875],\n       [0.63710329],\n       [0.        ]])\n\n\nApplying ReLU means that the negative element of \\mathbf{z_1} has been replaced with a zero.\n\n\nValues of the output layer\nHere are the formulas you need to calculate the values of the output layer, represented by the vector \\mathbf{\\hat y}:\n\\begin{align}\n\\mathbf{z_2} &= \\mathbf{W_2}\\mathbf{h} + \\mathbf{b_2}   \\tag{3} \\\\\n\\mathbf{\\hat y} &= \\mathrm{softmax}(\\mathbf{z_2})   \\tag{4} \\\\\n\\end{align}\nFirst, calculate \\mathbf{z_2}.\n\n# BEGIN your code here\nz2 = np.dot(W2, h) + b2\n# END your code here\n\nz2\n\narray([[-0.31973737],\n       [-0.28125477],\n       [-0.09838369],\n       [-0.33512159],\n       [-0.19919612]])\n\n\nExpected output:\narray([[-0.31973737],\n       [-0.28125477],\n       [-0.09838369],\n       [-0.33512159],\n       [-0.19919612]])\nThis is a V by 1 matrix, where V is the size of the vocabulary, which is 5 in this example.\nNow calculate the value of \\mathbf{\\hat y}.\n\n# BEGIN your code here\ny_hat = softmax(z2)\n# END your code here\n\ny_hat\n\narray([[0.18519074],\n       [0.19245626],\n       [0.23107446],\n       [0.18236353],\n       [0.20891502]])\n\n\nExpected output:\narray([[0.18519074],\n       [0.19245626],\n       [0.23107446],\n       [0.18236353],\n       [0.20891502]])\nAs you’ve performed the calculations with random matrices and vectors (apart from the input vector), the output of the neural network is essentially random at this point. The learning process will adjust the weights and biases to match the actual targets better.\nThat being said, what word did the neural network predict?\n\n\nSolution\n\n\nThe neural network predicted the word “happy”: the largest element of \\mathbf{\\hat y} is the third one, and the third word of the vocabulary is “happy”.\n\n\nHere’s how you could implement this in Python:\n\n\nprint(Ind2word[np.argmax(y_hat)])\n\nWell done, you’ve completed the forward propagation phase!"
  },
  {
    "objectID": "posts/c2w4/lab04.html#cross-entropy-loss",
    "href": "posts/c2w4/lab04.html#cross-entropy-loss",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Cross-entropy loss",
    "text": "Cross-entropy loss\nNow that you have the network’s prediction, you can calculate the cross-entropy loss to determine how accurate the prediction was compared to the actual target.\n\nRemember that you are working on a single training example, not on a batch of examples, which is why you are using loss and not cost, which is the generalized form of loss.\n\nFirst let’s recall what the prediction was.\n\ny_hat\n\narray([[0.18519074],\n       [0.19245626],\n       [0.23107446],\n       [0.18236353],\n       [0.20891502]])\n\n\nAnd the actual target value is:\n\ny\n\narray([[0.],\n       [0.],\n       [1.],\n       [0.],\n       [0.]])\n\n\nThe formula for cross-entropy loss is:\n J=-\\sum\\limits_{k=1}^{V}y_k\\log{\\hat{y}_k} \\tag{6}\nImplement the cross-entropy loss function.\nHere are a some hints if you’re stuck.\n\n\nHint 1\n\n&lt;p&gt;To multiply two numpy matrices (such as &lt;code&gt;y&lt;/code&gt; and &lt;code&gt;y_hat&lt;/code&gt;) element-wise, you can simply use the &lt;code&gt;*&lt;/code&gt; operator.&lt;/p&gt;\n\n\nHint 2\n\n\nOnce you have a vector equal to the element-wise multiplication of y and y_hat, you can use np.sum to calculate the sum of the elements of this vector.\n\n\ndef cross_entropy_loss(y_predicted, y_actual):\n    # BEGIN your code here\n    loss = np.sum(-np.log(y_hat)*y)\n    # END your code here\n    return loss\n\nNow use this function to calculate the loss with the actual values of \\mathbf{y} and \\mathbf{\\hat y}.\n\ncross_entropy_loss(y_hat, y)\n\nnp.float64(1.4650152923611108)\n\n\nExpected output:\n1.4650152923611106\nThis value is neither good nor bad, which is expected as the neural network hasn’t learned anything yet.\nThe actual learning will start during the next phase: backpropagation."
  },
  {
    "objectID": "posts/c2w4/lab04.html#backpropagation",
    "href": "posts/c2w4/lab04.html#backpropagation",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Backpropagation",
    "text": "Backpropagation\nThe formulas that you will implement for backpropagation are the following.\n\\begin{align}\n\\frac{\\partial J}{\\partial \\mathbf{W_1}} &= \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right )\\mathbf{x}^\\top \\tag{7}\\\\\n\\frac{\\partial J}{\\partial \\mathbf{W_2}} &= (\\mathbf{\\hat{y}} - \\mathbf{y})\\mathbf{h^\\top} \\tag{8}\\\\\n\\frac{\\partial J}{\\partial \\mathbf{b_1}} &= \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right ) \\tag{9}\\\\\n\\frac{\\partial J}{\\partial \\mathbf{b_2}} &= \\mathbf{\\hat{y}} - \\mathbf{y} \\tag{10}\n\\end{align}\n\nNote: these formulas are slightly simplified compared to the ones in the lecture as you’re working on a single training example, whereas the lecture provided the formulas for a batch of examples. In the assignment you’ll be implementing the latter.\n\nLet’s start with an easy one.\nCalculate the partial derivative of the loss function with respect to \\mathbf{b_2}, and store the result in grad_b2.\n\\frac{\\partial J}{\\partial \\mathbf{b_2}} = \\mathbf{\\hat{y}} - \\mathbf{y} \\tag{10}\n\n# BEGIN your code here\ngrad_b2 = y_hat - y\n# END your code here\n\ngrad_b2\n\narray([[ 0.18519074],\n       [ 0.19245626],\n       [-0.76892554],\n       [ 0.18236353],\n       [ 0.20891502]])\n\n\nExpected output:\narray([[ 0.18519074],\n       [ 0.19245626],\n       [-0.76892554],\n       [ 0.18236353],\n       [ 0.20891502]])\nNext, calculate the partial derivative of the loss function with respect to \\mathbf{W_2}, and store the result in grad_W2.\n\\frac{\\partial J}{\\partial \\mathbf{W_2}} = (\\mathbf{\\hat{y}} - \\mathbf{y})\\mathbf{h^\\top} \\tag{8}\n\nHint: use .T to get a transposed matrix, e.g. h.T returns \\mathbf{h^\\top}.\n\n\n# BEGIN your code here\ngrad_W2 = np.dot(y_hat - y, h.T)\n# END your code here\n\ngrad_W2\n\narray([[ 0.06756476,  0.11798563,  0.        ],\n       [ 0.0702155 ,  0.12261452,  0.        ],\n       [-0.28053384, -0.48988499,  0.        ],\n       [ 0.06653328,  0.1161844 ,  0.        ],\n       [ 0.07622029,  0.13310045,  0.        ]])\n\n\nExpected output:\narray([[ 0.06756476,  0.11798563,  0.        ],\n       [ 0.0702155 ,  0.12261452,  0.        ],\n       [-0.28053384, -0.48988499,  0.        ],\n       [ 0.06653328,  0.1161844 ,  0.        ],\n       [ 0.07622029,  0.13310045,  0.        ]])\nNow calculate the partial derivative with respect to \\mathbf{b_1} and store the result in grad_b1.\n\\frac{\\partial J}{\\partial \\mathbf{b_1}} = \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right ) \\tag{9}\n\n# BEGIN your code here\ngrad_b1 = relu(np.dot(W2.T, y_hat - y))\n# END your code here\n\ngrad_b1\n\narray([[0.        ],\n       [0.        ],\n       [0.17045858]])\n\n\nExpected output:\narray([[0.        ],\n       [0.        ],\n       [0.17045858]])\nFinally, calculate the partial derivative of the loss with respect to \\mathbf{W_1}, and store it in grad_W1.\n\\frac{\\partial J}{\\partial \\mathbf{W_1}} = \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right )\\mathbf{x}^\\top \\tag{7}\n\n# BEGIN your code here\ngrad_W1 = np.dot(relu(np.dot(W2.T, y_hat - y)), x.T)\n# END your code here\n\ngrad_W1\n\narray([[0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.04261464, 0.04261464, 0.        , 0.08522929, 0.        ]])\n\n\nExpected output:\narray([[0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.04261464, 0.04261464, 0.        , 0.08522929, 0.        ]])\nBefore moving on to gradient descent, double-check that all the matrices have the expected dimensions.\n\n# BEGIN your code here\nprint(f'V (vocabulary size): {V}')\nprint(f'N (embedding size / size of the hidden layer): {N}')\nprint(f'size of grad_W1: {grad_W1.shape} (NxV)')\nprint(f'size of grad_b1: {grad_b1.shape} (Nx1)')\nprint(f'size of grad_W2: {grad_W1.shape} (VxN)')\nprint(f'size of grad_b2: {grad_b2.shape} (Vx1)')\n# END your code here\n\nV (vocabulary size): 5\nN (embedding size / size of the hidden layer): 3\nsize of grad_W1: (3, 5) (NxV)\nsize of grad_b1: (3, 1) (Nx1)\nsize of grad_W2: (3, 5) (VxN)\nsize of grad_b2: (5, 1) (Vx1)"
  },
  {
    "objectID": "posts/c2w4/lab04.html#gradient-descent",
    "href": "posts/c2w4/lab04.html#gradient-descent",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Gradient descent",
    "text": "Gradient descent\nDuring the gradient descent phase, you will update the weights and biases by subtracting \\alpha times the gradient from the original matrices and vectors, using the following formulas.\n\\begin{align}\n\\mathbf{W_1} &:= \\mathbf{W_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_1}} \\tag{11}\\\\\n\\mathbf{W_2} &:= \\mathbf{W_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_2}} \\tag{12}\\\\\n\\mathbf{b_1} &:= \\mathbf{b_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_1}} \\tag{13}\\\\\n\\mathbf{b_2} &:= \\mathbf{b_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_2}} \\tag{14}\\\\\n\\end{align}\nFirst, let set a value for \\alpha.\n\nalpha = 0.03\n\nThe updated weight matrix \\mathbf{W_1} will be:\n\nW1_new = W1 - alpha * grad_W1\n\nLet’s compare the previous and new values of \\mathbf{W_1}:\n\nprint('old value of W1:')\nprint(W1)\nprint()\nprint('new value of W1:')\nprint(W1_new)\n\nold value of W1:\n[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]\n [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]\n [ 0.26637602 -0.23846886 -0.37770863 -0.11399446  0.34008124]]\n\nnew value of W1:\n[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]\n [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]\n [ 0.26509758 -0.2397473  -0.37770863 -0.11655134  0.34008124]]\n\n\nThe difference is very subtle (hint: take a closer look at the last row), which is why it takes a fair amount of iterations to train the neural network until it reaches optimal weights and biases starting from random values.\nNow calculate the new values of \\mathbf{W_2} (to be stored in W2_new), \\mathbf{b_1} (in b1_new), and \\mathbf{b_2} (in b2_new).\n\\begin{align}\n\\mathbf{W_2} &:= \\mathbf{W_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_2}} \\tag{12}\\\\\n\\mathbf{b_1} &:= \\mathbf{b_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_1}} \\tag{13}\\\\\n\\mathbf{b_2} &:= \\mathbf{b_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_2}} \\tag{14}\\\\\n\\end{align}\n\n# BEGIN your code here\nW2_new = W2 - alpha * grad_W2\nb1_new = b1 - alpha * grad_b1\nb2_new = b2 - alpha * grad_b2\n# END your code here\n\nprint('W2_new')\nprint(W2_new)\nprint()\nprint('b1_new')\nprint(b1_new)\nprint()\nprint('b2_new')\nprint(b2_new)\n\nW2_new\n[[-0.22384758 -0.43362588  0.13310965]\n [ 0.08265956  0.0775535   0.1772054 ]\n [ 0.19557112 -0.04637608 -0.1790735 ]\n [ 0.06855622 -0.02363691  0.36107434]\n [ 0.33251813 -0.3982269  -0.43959196]]\n\nb1_new\n[[ 0.09688219]\n [ 0.29239497]\n [-0.27875802]]\n\nb2_new\n[[ 0.02964508]\n [-0.36970753]\n [-0.10468778]\n [-0.35349417]\n [-0.0764456 ]]\n\n\nExpected output:\nW2_new\n[[-0.22384758 -0.43362588  0.13310965]\n [ 0.08265956  0.0775535   0.1772054 ]\n [ 0.19557112 -0.04637608 -0.1790735 ]\n [ 0.06855622 -0.02363691  0.36107434]\n [ 0.33251813 -0.3982269  -0.43959196]]\n\nb1_new\n[[ 0.09688219]\n [ 0.29239497]\n [-0.27875802]]\n\nb2_new\n[[ 0.02964508]\n [-0.36970753]\n [-0.10468778]\n [-0.35349417]\n [-0.0764456 ]]\nCongratulations, you have completed one iteration of training using one training example!\nYou’ll need many more iterations to fully train the neural network, and you can optimize the learning process by training on batches of examples, as described in the lecture. You will get to do this during this week’s assignment."
  },
  {
    "objectID": "posts/c2w4/lab04.html#extracting-word-embedding-vectors",
    "href": "posts/c2w4/lab04.html#extracting-word-embedding-vectors",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Extracting word embedding vectors",
    "text": "Extracting word embedding vectors\nOnce you have finished training the neural network, you have three options to get word embedding vectors for the words of your vocabulary, based on the weight matrices \\mathbf{W_1} and/or \\mathbf{W_2}.\n\nOption 1: extract embedding vectors from \\mathbf{W_1}\nThe first option is to take the columns of \\mathbf{W_1} as the embedding vectors of the words of the vocabulary, using the same order of the words as for the input and output vectors.\n\nNote: in this practice notebook the values of the word embedding vectors are meaningless after a single iteration with just one training example, but here’s how you would proceed after the training process is complete.\n\nFor example \\mathbf{W_1} is this matrix:\n\nW1\n\narray([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n       [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n       [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n\n\nThe first column, which is a 3-element vector, is the embedding vector of the first word of your vocabulary. The second column is the word embedding vector for the second word, and so on.\nThe first, second, etc. words are ordered as follows.\n\nfor i in range(V):\n    print(Ind2word[i])\n\nam\nbecause\nhappy\ni\nlearning\n\n\nSo the word embedding vectors corresponding to each word are:\n\n# loop through each word of the vocabulary\nfor word in word2Ind:\n    # extract the column corresponding to the index of the word in the vocabulary\n    word_embedding_vector = W1[:, word2Ind[word]]\n    \n    print(f'{word}: {word_embedding_vector}')\n\nam: [0.41687358 0.32735501 0.26637602]\nbecause: [ 0.08854191  0.22795148 -0.23846886]\nhappy: [-0.23495225 -0.23951958 -0.37770863]\ni: [ 0.28320538  0.4117634  -0.11399446]\nlearning: [ 0.41800106 -0.23924344  0.34008124]\n\n\n\n\nOption 2: extract embedding vectors from \\mathbf{W_2}\nThe second option is to take \\mathbf{W_2} transposed, and take its columns as the word embedding vectors just like you did for \\mathbf{W_1}.\n\nW2.T\n\narray([[-0.22182064,  0.08476603,  0.1871551 ,  0.07055222,  0.33480474],\n       [-0.43008631,  0.08123194, -0.06107263, -0.02015138, -0.39423389],\n       [ 0.13310965,  0.1772054 , -0.1790735 ,  0.36107434, -0.43959196]])\n\n\n\n# loop through each word of the vocabulary\nfor word in word2Ind:\n    # extract the column corresponding to the index of the word in the vocabulary\n    word_embedding_vector = W2.T[:, word2Ind[word]]\n    \n    print(f'{word}: {word_embedding_vector}')\n\nam: [-0.22182064 -0.43008631  0.13310965]\nbecause: [0.08476603 0.08123194 0.1772054 ]\nhappy: [ 0.1871551  -0.06107263 -0.1790735 ]\ni: [ 0.07055222 -0.02015138  0.36107434]\nlearning: [ 0.33480474 -0.39423389 -0.43959196]\n\n\n\n\nOption 3: extract embedding vectors from \\mathbf{W_1} and \\mathbf{W_2}\nThe third option, which is the one you will use in this week’s assignment, uses the average of \\mathbf{W_1} and \\mathbf{W_2^\\top}.\nCalculate the average of \\mathbf{W_1} and \\mathbf{W_2^\\top}, and store the result in W3.\n\n# BEGIN your code here\nW3 = (W1+W2.T)/2\n# END your code here\n\nW3\n\narray([[ 0.09752647,  0.08665397, -0.02389858,  0.1768788 ,  0.3764029 ],\n       [-0.05136565,  0.15459171, -0.15029611,  0.19580601, -0.31673866],\n       [ 0.19974284, -0.03063173, -0.27839106,  0.12353994, -0.04975536]])\n\n\nExpected output:\narray([[ 0.09752647,  0.08665397, -0.02389858,  0.1768788 ,  0.3764029 ],\n       [-0.05136565,  0.15459171, -0.15029611,  0.19580601, -0.31673866],\n       [ 0.19974284, -0.03063173, -0.27839106,  0.12353994, -0.04975536]])\nExtracting the word embedding vectors works just like the two previous options, by taking the columns of the matrix you’ve just created.\n\n# loop through each word of the vocabulary\nfor word in word2Ind:\n    # extract the column corresponding to the index of the word in the vocabulary\n    word_embedding_vector = W3[:, word2Ind[word]]\n    \n    print(f'{word}: {word_embedding_vector}')\n\nam: [ 0.09752647 -0.05136565  0.19974284]\nbecause: [ 0.08665397  0.15459171 -0.03063173]\nhappy: [-0.02389858 -0.15029611 -0.27839106]\ni: [0.1768788  0.19580601 0.12353994]\nlearning: [ 0.3764029  -0.31673866 -0.04975536]\n\n\nYou’re now ready to take on this week’s assignment!\n\n\nHow this practice relates to and differs from the upcoming graded assignment\n\nIn the assignment, for each iteration of training you will use batches of examples instead of a single example. The formulas for forward propagation and backpropagation will be modified accordingly, and you will use cross-entropy cost instead of cross-entropy loss.\nYou will also complete several iterations of training, until you reach an acceptably low cross-entropy cost, at which point you can extract good word embeddings from the weight matrices.\nAfter extracting the word embedding vectors, you will use principal component analysis (PCA) to visualize the vectors, which will enable you to perform an intrinsic evaluation of the quality of the vectors, as explained in the lecture."
  },
  {
    "objectID": "posts/c1lab1/index.html",
    "href": "posts/c1lab1/index.html",
    "title": "Assignment 1 Logistic Regression",
    "section": "",
    "text": "course banner",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Assignment 1 Logistic Regression"
    ]
  },
  {
    "objectID": "posts/c1lab1/index.html#import-functions-and-data",
    "href": "posts/c1lab1/index.html#import-functions-and-data",
    "title": "Assignment 1 Logistic Regression",
    "section": "Import functions and data",
    "text": "Import functions and data\n\n# run this cell to import nltk\nimport nltk\nfrom os import getcwd\n\n\nImported functions\nDownload the data needed for this assignment. Check out the documentation for the twitter_samples dataset. * twitter_samples: if you’re running this notebook on your local computer, you will need to download it using:\nnltk.download('twitter_samples')\n\nstopwords: if you’re running this notebook on your local computer, you will need to download it using:\n\n\nnltk.download('stopwords')\n\n[nltk_data] Downloading package stopwords to /home/oren/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue\n\n\n\nImport some helper functions that we provided in the utils.py file:\n\nprocess_tweet(): cleans the text, tokenizes it into separate words, removes stopwords, and converts words to stems.\nbuild_freqs(): this counts how often a word in the ‘corpus’ (the entire set of tweets) was associated with a positive label ‘1’ or a negative label ‘0’, then builds the freqs dictionary, where each key is a (word,label) tuple, and the value is the count of its frequency within the corpus of tweets.\n\n\n# add folder, tmp2, from our local workspace containing pre-downloaded corpora files to nltk's data path\n\n# this enables importing of these files without downloading it again when we refresh our workspace\nfilePath = f\"{getcwd()}/../tmp2/\"\nnltk.data.path.append(filePath)\n\n\nimport numpy as np\nimport pandas as pd\nfrom nltk.corpus import twitter_samples \nfrom utils import process_tweet, build_freqs\n\n\n\n\nPrepare the data\n\nThe twitter_samples contains subsets of 5,000 positive tweets, 5,000 negative tweets, and the full set of 10,000 tweets.\n\nIf you used all three datasets, we would introduce duplicates of the positive tweets and negative tweets.\n\nYou will select just the five thousand positive tweets and five thousand negative tweets.\n\n\n\n# select the set of positive and negative tweets\nall_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')\n\n\nTrain test split: 20% will be in the test set, and 80% in the training set.\n\n\n# split the data into two pieces, one for training and one for testing (validation set) \ntest_pos = all_positive_tweets[4000:]\ntrain_pos = all_positive_tweets[:4000]\ntest_neg = all_negative_tweets[4000:]\ntrain_neg = all_negative_tweets[:4000]\ntrain_x = train_pos + train_neg \ntest_x = test_pos + test_neg\n\n\nCreate the numpy array of positive labels and negative labels.\n\n\n# combine positive and negative labels\ntrain_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\ntest_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)\n\n\n# Print the shape train and test sets\nprint(\"train_y.shape = \" + str(train_y.shape))\nprint(\"test_y.shape = \" + str(test_y.shape))\n\ntrain_y.shape = (8000, 1)\ntest_y.shape = (2000, 1)\n\n\n\nCreate the frequency dictionary using the imported build_freqs() function.\n\nWe highly recommend that you open utils.py and read the build_freqs() function to understand what it is doing.\nTo view the file directory, go to the menu and click File-&gt;Open.\n\n  for y,tweet in zip(ys, tweets):\n      for word in process_tweet(tweet):\n          pair = (word, y)\n          if pair in freqs:\n              freqs[pair] += 1\n          else:\n              freqs[pair] = 1\nNotice how the outer for loop goes through each tweet, and the inner for loop steps through each word in a tweet.\nThe freqs dictionary is the frequency dictionary that’s being built.\nThe key is the tuple (word, label), such as (“happy”,1) or (“happy”,0). The value stored for each key is the count of how many times the word “happy” was associated with a positive label, or how many times “happy” was associated with a negative label.\n\n\n# create frequency dictionary\nfreqs = build_freqs(train_x, train_y)\n\n# check the output\nprint(\"type(freqs) = \" + str(type(freqs)))\nprint(\"len(freqs) = \" + str(len(freqs.keys())))\n\ntype(freqs) = &lt;class 'dict'&gt;\nlen(freqs) = 11337\n\n\n\nExpected output\ntype(freqs) = &lt;class 'dict'&gt;\nlen(freqs) = 11346\n\n\n\nProcess tweet\nThe given function process_tweet() tokenizes the tweet into individual words, removes stop words and applies stemming.\n\n# test the function below\nprint('This is an example of a positive tweet: \\n', train_x[0])\nprint('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[0]))\n\nThis is an example of a positive tweet: \n #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n\nThis is an example of the processed version of the tweet: \n ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n\n\n\nExpected output\nThis is an example of a positive tweet: \n #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n \nThis is an example of the processes version: \n ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Assignment 1 Logistic Regression"
    ]
  },
  {
    "objectID": "posts/c1lab1/index.html#instructions-implement-gradient-descent-function",
    "href": "posts/c1lab1/index.html#instructions-implement-gradient-descent-function",
    "title": "Assignment 1 Logistic Regression",
    "section": "Instructions: Implement gradient descent function",
    "text": "Instructions: Implement gradient descent function\n\nThe number of iterations num_iters is the number of times that you’ll use the entire training set.\nFor each iteration, you’ll calculate the cost function using all training examples (there are m training examples), and for all features.\nInstead of updating a single weight \\theta_i at a time, we can update all the weights in the column vector:\n\n\n\\mathbf{\\theta} = \\begin{pmatrix}\n\\theta_0\n\\\\\n\\theta_1\n\\\\\n\\theta_2\n\\\\\n\\vdots\n\\\\\n\\theta_n\n\\end{pmatrix}\n\n\n\\mathbf{\\theta} has dimensions (n+1, 1), where ‘n’ is the number of features, and there is one more element for the bias term \\theta_0 (note that the corresponding feature value \\mathbf{x_0} is 1).\nThe ‘logits’, ‘z’, are calculated by multiplying the feature matrix ‘x’ with the weight vector ‘theta’. z = \\mathbf{x}\\mathbf{\\theta}\n\n\\mathbf{x} has dimensions (m, n+1)\n\\mathbf{\\theta}: has dimensions (n+1, 1)\n\\mathbf{z}: has dimensions (m, 1)\n\nThe prediction ‘h’, is calculated by applying the sigmoid to each element in ‘z’: h(z) = sigmoid(z), and has dimensions (m,1).\nThe cost function J is calculated by taking the dot product of the vectors ‘y’ and ‘log(h)’. Since both ‘y’ and ‘h’ are column vectors (m,1), transpose the vector to the left, so that matrix multiplication of a row vector with column vector performs the dot product.\n\n\nJ = \\frac{-1}{m} \\times \\left(\\mathbf{y}^T \\cdot log(\\mathbf{h}) + \\mathbf{(1-y)}^T \\cdot log(\\mathbf{1-h}) \\right)\n\n\nThe update of theta is also vectorized. Because the dimensions of \\mathbf{x} are (m, n+1), and both \\mathbf{h} and \\mathbf{y} are (m, 1), we need to transpose the \\mathbf{x} and place it on the left in order to perform matrix multiplication, which then yields the (n+1, 1) answer we need:\n\n\n\\mathbf{\\theta} = \\mathbf{\\theta} - \\frac{\\alpha}{m} \\times \\left( \\mathbf{x}^T \\cdot \\left( \\mathbf{h-y} \\right) \\right)\n\n\n\nHints\n\n\n\n\nuse np.dot for matrix multiplication.\n\n\nTo ensure that the fraction -1/m is a decimal value, cast either the numerator or denominator (or both), like float(1), or write 1. for the float version of 1.\n\n\n\n\n\n# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef gradientDescent(x, y, theta, alpha, num_iters):\n    '''\n    Input:\n        x: matrix of features which is (m,n+1)\n        y: corresponding labels of the input matrix x, dimensions (m,1)\n        theta: weight vector of dimension (n+1,1)\n        alpha: learning rate\n        num_iters: number of iterations you want to train your model for\n    Output:\n        J: the final cost\n        theta: your final weight vector\n    Hint: you might want to print the cost to make sure that it is going down.\n    '''\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    # get 'm', the number of rows in matrix x\n    m = x.shape[0]     \n    for i in range(0, num_iters):\n        \n        # get z, the dot product of x and theta\n        z = np.dot(x,theta)\n        \n        # get the sigmoid of h\n        h = sigmoid(z)\n        \n        # calculate the cost function\n        J = -1./m * (np.dot(y.transpose(), np.log(h)) + np.dot((1-y).transpose(),np.log(1-h)))                                                    \n        # update the weights theta\n        theta = theta - (alpha/m) * np.dot(x.transpose(),(h-y))\n        \n    ### END CODE HERE ###\n    J = float(J)\n    return J, theta\n\n\n# Check the function\n\n# Construct a synthetic test case using numpy PRNG functions\nnp.random.seed(1)\n\n# X input is 10 x 3 with ones for the bias terms\ntmp_X = np.append(np.ones((10, 1)), np.random.rand(10, 2) * 2000, axis=1)\n\n# Y Labels are 10 x 1\ntmp_Y = (np.random.rand(10, 1) &gt; 0.35).astype(float)\n\n# Apply gradient descent\ntmp_J, tmp_theta = gradientDescent(tmp_X, tmp_Y, np.zeros((3, 1)), 1e-8, 700)\nprint(f\"The cost after training is {tmp_J:.8f}.\")\nprint(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(tmp_theta)]}\")\n\nThe cost after training is 0.67094970.\nThe resulting vector of weights is [np.float64(4.1e-07), np.float64(0.00035658), np.float64(7.309e-05)]\n\n\n/tmp/ipykernel_108676/190191251.py:32: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  J = float(J)\n\n\n\nExpected output\nThe cost after training is 0.67094970.\nThe resulting vector of weights is [4.1e-07, 0.00035658, 7.309e-05]",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Assignment 1 Logistic Regression"
    ]
  },
  {
    "objectID": "posts/c1lab1/index.html#part-2-extracting-the-features",
    "href": "posts/c1lab1/index.html#part-2-extracting-the-features",
    "title": "Assignment 1 Logistic Regression",
    "section": "Part 2: Extracting the features",
    "text": "Part 2: Extracting the features\n\nGiven a list of tweets, extract the features and store them in a matrix. You will extract two features.\n\nThe first feature is the number of positive words in a tweet.\nThe second feature is the number of negative words in a tweet.\n\nThen train your logistic regression classifier on these features.\nTest the classifier on a validation set.\n\n\nInstructions: Implement the extract_features function.\n\nThis function takes in a single tweet.\nProcess the tweet using the imported process_tweet() function and save the list of tweet words.\nLoop through each word in the list of processed words\n\nFor each word, check the freqs dictionary for the count when that word has a positive ‘1’ label. (Check for the key (word, 1.0))\nDo the same for the count for when the word is associated with the negative label ‘0’. (Check for the key (word, 0.0).)\n\n\n\n\nHints\n\n\n\n\nMake sure you handle cases when the (word, label) key is not found in the dictionary.\n\n\nSearch the web for hints about using the .get() method of a Python dictionary. Here is an  example \n\n\n\n\n\n# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef extract_features(tweet, freqs):\n    '''\n    Input: \n        tweet: a list of words for one tweet\n        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n    Output: \n        x: a feature vector of dimension (1,3)\n    '''\n    # process_tweet tokenizes, stems, and removes stopwords\n    word_l = process_tweet(tweet)\n    # 3 elements in the form of a 1 x 3 vector\n    x = np.zeros((1, 3)) \n    \n    #bias term is set to 1\n    x[0,0] = 1 \n    \n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    \n    # loop through each word in the list of words\n    for word in word_l:\n        \n        # increment the word count for the positive label 1\n        x[0,1] += freqs.get((word, 1.0),0)\n        \n        # increment the word count for the negative label 0\n        x[0,2] += freqs.get((word, 0.0),0)\n        \n    ### END CODE HERE ###\n    assert(x.shape == (1, 3))\n    return x\n\n\n# Check your function\n\n# test 1\n\n# test on training data\ntmp1 = extract_features(train_x[0], freqs)\nprint(tmp1)\n\n[[1.00e+00 3.02e+03 6.10e+01]]\n\n\n\nExpected output\n[[1.00e+00 3.02e+03 6.10e+01]]\n\n# test 2:\n\n# check for when the words are not in the freqs dictionary\ntmp2 = extract_features('blorb bleeeeb bloooob', freqs)\nprint(tmp2)\n\n[[1. 0. 0.]]\n\n\n\n\nExpected output\n[[1. 0. 0.]]",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Assignment 1 Logistic Regression"
    ]
  },
  {
    "objectID": "posts/c1lab1/index.html#part-3-training-your-model",
    "href": "posts/c1lab1/index.html#part-3-training-your-model",
    "title": "Assignment 1 Logistic Regression",
    "section": "Part 3: Training Your Model",
    "text": "Part 3: Training Your Model\nTo train the model:\n\nStack the features for all training examples into a matrix X.\nCall gradientDescent, which you’ve implemented above.\n\nThis section is given to you. Please read it for understanding and run the cell.\n\n# collect the features 'x' and stack them into a matrix 'X'\nX = np.zeros((len(train_x), 3))\nfor i in range(len(train_x)):\n    X[i, :]= extract_features(train_x[i], freqs)\n\n# training labels corresponding to X\nY = train_y\n\n# Apply gradient descent\nJ, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\nprint(f\"The cost after training is {J:.8f}.\")\nprint(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")\n\nThe cost after training is 0.24215478.\nThe resulting vector of weights is [np.float64(7e-08), np.float64(0.00052391), np.float64(-0.00055517)]\n\n\n/tmp/ipykernel_108676/190191251.py:32: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  J = float(J)\n\n\nExpected Output:\nThe cost after training is 0.24216529.\nThe resulting vector of weights is [7e-08, 0.0005239, -0.00055517]",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Assignment 1 Logistic Regression"
    ]
  },
  {
    "objectID": "posts/c1lab1/index.html#check-performance-using-the-test-set",
    "href": "posts/c1lab1/index.html#check-performance-using-the-test-set",
    "title": "Assignment 1 Logistic Regression",
    "section": "Check performance using the test set",
    "text": "Check performance using the test set\nAfter training your model using the training set above, check how your model might perform on real, unseen data, by testing it against the test set.\n\nInstructions: Implement test_logistic_regression\n\nGiven the test data and the weights of your trained model, calculate the accuracy of your logistic regression model.\nUse your predict_tweet() function to make predictions on each tweet in the test set.\nIf the prediction is &gt; 0.5, set the model’s classification y_hat to 1, otherwise set the model’s classification y_hat to 0.\nA prediction is accurate when y_hat equals test_y. Sum up all the instances when they are equal and divide by m.\n\n\n\nHints\n\n\n\n\nUse np.asarray() to convert a list to a numpy array\n\n\nUse np.squeeze() to make an (m,1) dimensional array into an (m,) array\n\n\n\n\n\n# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef test_logistic_regression(test_x, test_y, freqs, theta):\n    \"\"\"\n    Input: \n        test_x: a list of tweets\n        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n        freqs: a dictionary with the frequency of each pair (or tuple)\n        theta: weight vector of dimension (3, 1)\n    Output: \n        accuracy: (# of tweets classified correctly) / (total # of tweets)\n    \"\"\"\n    \n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    \n    # the list for storing predictions\n    y_hat = []\n    \n    for tweet in test_x:\n        # get the label prediction for the tweet\n        y_pred = predict_tweet(tweet, freqs, theta)\n        if y_pred &gt; 0.5:\n            # append 1.0 to the list\n            y_hat.append(1)\n        else:\n            # append 0 to the list\n            y_hat.append(0)\n    # With the above implementation, y_hat is a list, but test_y is (m,1) array\n    # convert both to one-dimensional arrays in order to compare them using the '==' operator\n    \n    accuracy = (y_hat==np.squeeze(test_y)).sum()/len(test_x)\n    ### END CODE HERE ###\n    \n    return accuracy\n\n\ntmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\nprint(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")\n\nLogistic regression model's accuracy = 0.9950\n\n\n\n\nExpected Output:\n0.9950\nPretty good!",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Assignment 1 Logistic Regression"
    ]
  },
  {
    "objectID": "posts/c3w2/index.html",
    "href": "posts/c3w2/index.html",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "",
    "text": "course banner\n\n\nresources:\n\n(Chadha 2020) Aman Chadha’s Notes\nIbrahim Jelliti’s Notes\n\n\n\n\n\n\n\nReferences\n\nChadha, Aman. 2020. “Distilled Notes for the Natural Language Processing Specialization on Coursera (Offered by Deeplearning.ai).” https://www.aman.ai. www.aman.ai.\n\nCitationBibTeX citation:@online{2020,\n  author = {},\n  title = {Neural {Networks} for {Sentiment} {Analysis}},\n  date = {2020-10-23},\n  url = {https://orenbochman.github.io/notes-nlp/posts/c3w2/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“Neural Networks for Sentiment Analysis.” 2020. October 23,\n2020. https://orenbochman.github.io/notes-nlp/posts/c3w2/.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c4w2/index.html",
    "href": "posts/c4w2/index.html",
    "title": "Week 2 Text Summarization",
    "section": "",
    "text": "deeplearning.ai",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c4w2/index.html#additional-coding-notes",
    "href": "posts/c4w2/index.html#additional-coding-notes",
    "title": "Week 2 Text Summarization",
    "section": "Additional coding notes:",
    "text": "Additional coding notes:\nHere are some notable code snippets.\n\nHow to reshape a test tensor so it has a (size 0) batch dimension at the front?\nThis is needed when inspecting single test inputs instead of working with a batch. The model is expecting to process batches of inputs like it saw during training - we therefore need to add a dimension at the start.\npadded_with_batch = fastnp.expand_dims(fastnp.array(padded),axis=0)\n\n# get log probabilities from the last token output\nlog_probs = output[0,-1,:] \n\n\nHow to make TRAX take in string date as a stream ?\ninputs =  next(trax.data.tokenize(iter([input_str]),\n                vocab_dir='vocab_dir/',\n                vocab_file='summarize32k.subword.subwords'))\n\n\nHow to transpose batched tensors ?\n  # batch_size, seqlen, n_heads, d_head -&gt; batch_size, n_heads, seqlen, d_head\n  x = jnp.transpose(x, (0, 2, 1, 3))\n\n\nHow to de-structure tensors for use with multihead attention ?\n  # batch_size, seqlen, n_heads*d_head -&gt; batch_size, seqlen, n_heads, d_head\n  x = jnp.reshape(x,(batch_size, seqlen, n_heads, d_head))\n  # batch_size, seqlen, n_heads, d_head -&gt; batch_size, n_heads, seqlen, d_head\n  x = jnp.transpose(x, (0, 2, 1, 3))\n  # batch_size, n_heads, seqlen, d_head -&gt; batch_size*n_heads, seqlen, d_head\n  x = jnp.reshape(x,( batch_size*n_heads, seqlen, d_head))\ninput tensor shape: (3, 2, 6)\n[[[1 0 0 1 0 0]\n  [0 1 0 0 1 0]]\n [[1 0 0 1 0 0]\n  [0 1 0 0 1 0]]\n [[1 0 0 1 0 0]\n  [0 1 0 0 1 0]]]\noutput tensor shape: (6, 2, 3)\n[[[1 0 0]\n  [0 1 0]]\n [[1 0 0]\n  [0 1 0]]\n [[1 0 0]\n  [0 1 0]]\n [[1 0 0]\n  [0 1 0]]\n [[1 0 0]\n  [0 1 0]]\n [[1 0 0]\n  [0 1 0]]]",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c4w2/index.html#video-1-transformers-vs-rnns",
    "href": "posts/c4w2/index.html#video-1-transformers-vs-rnns",
    "title": "Week 2 Text Summarization",
    "section": "Video 1: Transformers vs RNNs",
    "text": "Video 1: Transformers vs RNNs\nRNNs were a big breakthrough and became the state of the art (SOTA) for machine translation (MT).\nThis illustrates a typical RNN that is used to translate the English sentence “How are you?” to its German equivalent, “Wie sind Sie?”.\n\n\n\n\nrnn-non-parallel\n\n\n\n\nlstm\n\n\nThe LSTM which goes a long way to solving the vanishing gradient problems requires three times the memory and cpu steps a the vanilla RNN.\nHowever, as time went by and models got longer and deeper the biggest challenge with improving RNNs, became their use of sequential computation.\n\n\n\n\nseq2seq-steps\n\nWhich entailed that to process the word “you”, the RNN it has to first go through “are” and then “you”. Two other issues with RNNs are the:\n\nInformation loss\nIt becomes harder to keep track of whether the subject is singular or plural as you move further away from the subject.\n\n\n\n\nTransformer\n\ntransformer architecture:\nin the encoder side - lookup layer - the source sequence is converted from one hot encoding to a distributed representation using an embedding. - this is converted to K V matrices in the decoder side\n\n\nVanishing Gradient\nWhen gradients you back-propagate, the gradients can become really small and as a result.\nWith small gradient the model will learn very little.\n\n\n\n\npositional-encoding\n\nTransformers which are based on attention and don’t require any sequential computation per layer, only a single step is needed.\n\n\n\n\nsummary\n\nAdditionally, the gradient steps that need to be taken from the last output to the first input in a transformer is just one. For RNNs, the number of steps increases with longer sequences. Finally, transformers don’t suffer from vanishing gradients problems that are related to the length of the sequences.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c4w2/index.html#video-2-transformer-applications",
    "href": "posts/c4w2/index.html#video-2-transformer-applications",
    "title": "Week 2 Text Summarization",
    "section": "Video 2: Transformer Applications",
    "text": "Video 2: Transformer Applications\n Transformers have essentially replaced RNN,LSTM and GRUs in sequence processing.\n\n\n\n\napplication-NLP\n\n\nApplications:\n\nText summarization\nAutocomplete\nNER\nQ&A\nTranslation\nChat Bots\nSentiment Analyses\nMarket Intelligence\nText Classification\nOCR\nSpell Checking\n\n\n\n\n\nsota\n\n\n\nSOTA Transformers\nTransformers Time Line:\n\nGPT-4:\nElmO\nBERT\nT5",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c4w2/index.html#t5---text-to-text-transfer-transformer",
    "href": "posts/c4w2/index.html#t5---text-to-text-transfer-transformer",
    "title": "Week 2 Text Summarization",
    "section": "T5 - Text-To-Text Transfer Transformer",
    "text": "T5 - Text-To-Text Transfer Transformer\n\n\n\n\nt5\n\n\n\n\nt5\n\n\n(Raffel et al. 2019) introduced T5 which can do a number of tasks with a single model. While the earlier transformer models were able to score high in many different tasks without specific training. T5 is setup to handle different inputs and respond with output that is relevant to the requested task.\n\nT5 Classification tasks\nThese tasks are selected using the initial string: - Translate English into German - Cola sentence - Question\n\n\n\n\ntext-to-text-transformer\n\n\n\nT5 regression tasks\n\nStbs Sentence1 … Stbs Sentence2 …\nSummarize:\n\nplay trivia against T5 here\n\n\n\n\ntransformers quiz\n\n\n\n\n\n\n\nWarning\n\n\n\nI found this one a little confusing\n\n\nWe are told that the transformers can do in one operation what RNN needed to do in many steps. Also when querying transformers it does one task at a time. It seem that this question is about the ability of multiple heads to do several tasks at once could not do this is not well understood.\n\n\n\n\nsummary of transformers",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c4w2/index.html#video-3-dot-product-attention",
    "href": "posts/c4w2/index.html#video-3-dot-product-attention",
    "title": "Week 2 Text Summarization",
    "section": "Video 3: Dot-Product Attention",
    "text": "Video 3: Dot-Product Attention\n\n\n\n\noutline-of-dot-product-attention\n\nDot product attention was introduced in 2015 by Minh-Thang Luong, Hieu Pham, Christopher D. Manning in Effective Approaches to Attention-based Neural Machine Translation which is available at papers with code.\nLook at Review of Effective Approaches to Attention-based NMT\nDot product attention is the main operation in transformers. It is the dot product between the embedding of source and target sequences. The embedding used is a cross language embedding in which distance between equivalent across languages are minimized. This facilitates finding the cosine similarity using the dot product between words.\n\n\n\n\nintro-to-attention\n\nLet’s try to understand dot product attention intuitively by walking over its operations at the word level. The actual algorithm uses linear algebra to perform many operations at once which is fast but more abstract and therefore difficult to understand.\n\nUsing a pre-trained cross-language embedding encode:\n\neach German word vector q_i is placed as a column vector to form the query matrix Q,\neach English word once as k_i and once as v_i, column vectors in the key K and value V matrices. This is more of a preprocessing step.\n\nFor each German word we want to derive a continuous filter function on the English sequence to pick the most relevant words for translation. We build this filter for word q_i by taking its dot product q_i \\cdot k_i with every word vector from the english sequence these products are called the the attention weights.\nnext we convert the rudimentary filter to a probabilistic one by applying a softmax() which is just a differentiable function that converts the attention weights to probabilities by keeping them at the same relative sizes while ensuring they add to one.\nnow that we have a q-filter we want to apply it. This is done by taking the weighed sum of the english words using the attention weights.\n\n\n\\hat q_i = \\sum_{i} softmax(q_i \\cdot k_i) \\times v_i =  \\sum w_a(q_i) * v_i\n\n\nQuery, Key & Value\n\n\n\n\nqueries-keys-values\n\nAttention uses three matrices which are formed as shown in the figure The Query Q, Key K and Value V are formed from the source and target (if there is no target then just from the source). Each word is converted into an embedding column vector and these are placed into the matracies as their columns. In the master class embedded bellow Dr. Łukasz Kaiser talks about attention and here he is talking about solving the problem of retrieving information from a long sequence. At around 16 minutes in he call Q a query vector and K and V a memory, of all the words we have seen, which we want to access.\n\nThe Query is the matrix formed from the column word vector for the German words.\nThe Key is the matrix formed from the column word vector for the English words.\nThe Value is the matrix formed from the column word vector for the English words.\n\n\n\n\n\n\n\nNote\n\n\n\nK and V are the same\n\n\nOnce these are called keys since we use them to are we doing a similarity lookup. And the second time they are called value because we use them in the activation when we apply the weights to them. The input and output sequences are mapped to an embedding layer to become the Q, K and V matrices.\n\nGiven an input, you transform it into a new representation or a column vector. Depending on the task you are working on, you will end up getting queries, keys, and values. Each column corresponds to a word in the figure above. Hence, when you compute the following:\n\n\n\n\nattention-formula\n\n\nmultiply Q by V.\napply the softmax() to transform to a probability.\nmultiply the softmax by V\n\n\n\n\n\nattention-math\n\nThis is restating the above in a very confusing way. I looked at it many times before I figured out that the square brackets are the dimensions and that we have the following two formulas indicated schematically above:\n\nZ = W_A V\n\nwhere:\n\nZ has size of is a ‘Q length’ \\times ‘Embedding size’ matrix\nor for coders [len(Q),D] dimensional array\n\n\nW_A = softmax(QK^T)\n\nThis concept implies that similar vectors are likely to have a higher score when you dot them with one another. You transform that score into a probability by using a softmax function. You can then multiply the output by\nYou can think of the keys and the values as being the same. Note that both K,V are of dimension L_k, D. Each query q_i picks the most similar key k_j.\n\n\n\n\nattention-formula\n\nQueries are the German words and the keys are the English words. Once you have the attention weights, you can just multiply it by V to get a weighted combination of the input.\n\n\n\n\nattention-quiz\n\n\n\n\nsummary-for-dot-product-attention\n\n\nanother interesting point made in the preceding talk is that dot product attention has O(n^2 *d) complexity but typically d &gt;&gt; n since d ~ 1000 while for n ~ 70. So transformers should perform better then an RNN whose complexity is O(n*d^2). And this is before the advantages of using an efficient transformer like reformer.\nIn (Kasai et al. 2021) there is a reversal of the trend from rnn to transformers. Here the latest results show a an idea of training big transformers and then converting them to RNN to improve performance. (One get an RNN by training a transformer.)",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c4w2/index.html#v4-causal-attention",
    "href": "posts/c4w2/index.html#v4-causal-attention",
    "title": "Week 2 Text Summarization",
    "section": "V4: Causal Attention",
    "text": "V4: Causal Attention\n\nWe are interested in three main types of attention.\nWe’ll see a brief overview of causal attention.\nWe’ll discover some mathematical foundations behind the causal attention.\n\n\n\n\n\nthree forms of attention\n\nIn terms of use cases there are three types of attention mechanisms:\n\nScaled dot product attention:\n\nAKA Encoder-Decoder attention.\none sentence in the decoder look at to another one in the encoder.\nuse cases:\n\nseq2seq\nmachine translation.\n\n\n\n\nCausal Attention:\n\nAKA self attention.\nattention is all you need.\nIn the same sentence words attend to previous words.\nFuture words have not been generated yet.\nuse cases:\n\ngeneration\ntext generation\nsummarization.\n\n\n\n\nBi-directional self attention:\n\nIn one sentence words look both at previous and future words.\nuse cases:\n\nmachine comprehension.\nquestion answering\n\n\n\n\n\n\ncausal attention\n\nIn causal attention, queries and keys come from the same sentence. That is why it is often referred to as self-attention. In general, causal attention allows words to attend to other words that are related in various ways.\n\n\n\n\ncausal attention mask\n\nAt a high-level We have K Q V matrices. corresponding However, token should not attend to words in the future since these were not generated yet. Therefore the future token’s data is masked by adding a big negative number.\n\n\n\n\ncausal-attention-math-\n\nMathematically, it looks like this:\n\n\n\n\ncausal-attention-quiz\n\n\n\n\nsummary-for-causal-attention",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c4w2/index.html#v5-multi-head-attention",
    "href": "posts/c4w2/index.html#v5-multi-head-attention",
    "title": "Week 2 Text Summarization",
    "section": "V5: Multi-head Attention",
    "text": "V5: Multi-head Attention\n Let’s summarize the intuition behind multi-head attention and scaled dot product attention.\n\n\n\n\nmuti-head-attention\n\nQ. What are multiple attention heads?\n\nMultiple attention heads are simply replicas of the attention mechanism. In this they are analogous to the multiple filters used in a convolutional neural networks (CNN).\nDuring training they specialize by learning different relationships between words.\nDuring inference the operate parallel and independently of each other.\n\n\n\n\n\noverview of muti-head attention\n\nThis is perhaps the most important slide - but it fails to show the critical part of the algorithm.\nLet’s suppose we have k attention heads. We see at the lowest level the K, Q and V being passed into passing through k linear layers. How is this accomplished and more important why. What is actually happening here is the opposite of concatenation. Instead of processing a query embedding from a space of d-dimensions we first split the embedding into k vectors of length D/k. We have now k vectors from a k D/k-dimensional subspace. We now perform a dot product attention on each of these subspaces.\n\nEach of these dot product attention is operating on a difference subspace. It sees different subsets of the data and therefore specializes. How do these heads specializes is anybody’s guess - unless we have a special embedding which has been processed using PCA or some other algorithm to ensure that each subspace corresponds to some interpretable subset of features.\n\n\n\n\nmuti-head attention scaled dot-product\n\nFor example if we used a 1024 dimension embedding which concatenates 4 representations.\n\n[0:256] is an embedding trained on a phonological task\n[256:512] is an embedding trained on a morphological task\n[513:768] is an embedding trained on a syntactical task\n[769:1024] is an embedding trained on a semantic task\n\nWe could devise a number of subspace sampling schemes to give the k different attention heads different areas of specializations.\n\nsample from a single sub-space\n4 heads sample from one subspace and 4 heads sample from 3 different sub-spaces\n5 heads sampling from 2 subspaces different sub-spaces and 3 from 1\n5 heads sampling from 2 subspaces different sub-spaces and 3 from three\n\nEach would specialize on a domain or on a interface between two domain or on all data but one domain. Language is rather redundant so they may be able to reconstruct most of the missing data - but at least they would specialize in a linguistically meaningful way.\nGiven a word, you take its embedding then you multiply it by the Q, K, V matrix to get the corresponding queries, keys and values. When you use multi-head attention, a head can learn different relationships between words from another head.\nHere’s one way to look at it:\n\nFirst, imagine that you have an embedding for a word. You multiply that embedding with Q to get q_1, K to get k_1, and V to get v_1\n\n\n\n\n\nmuti-head-attention-concatenation\n\n\n\n\nmuti-head-attention-math\n\n\n\n\nmuti-head-attention-fotmula\n\n\n\n\nmuti-head-attention-quiz\n\n\n\n\nmuti-head-attention-math\n\n\n\n\nmuti-head-attention-math\n\n\n\n\nmuti-head-attention-math\n\n\n\n\nmuti-head-attention-math\n\n\n\n\n\n\n\n\n\nNext, you feed it to the linear layer, once you go through the linear layer for each word, you need to calculate a score. After that, you end up having an embedding for each word. But you still need to get the score for how much of each word you are going to use. For example, this will tell you how similar two words are q_1 and k_1or even q_1 and k_2 by doing a simple q_1 \\dot k_1. You can take the softmax of those scores (the paper mentions that you have to divide by \\sqrt(d) to get a probability and then you multiply that by the value. That gives you the new representation of the word.) If you have many heads, you can concatenate them and then multiply again by a matrix that is of dimension (dim of each head by num heads - dim of each head) to get one final vector corresponding to each word.\n\nHere is step by step guide, first you get the Q, K, V matrices: Note that the computation above was done for one head. If you have several heads, concretely nn, then you will have Z_1, Z_2, \\ldots, Z_n. In which case, you can just concatenate them and multiply by a W_O matrix as follows:\nHence, the more heads you have, the more Zs you will end up concatenating and as a result, that will change the inner dimension of W_O, which will then project the combined embeddings into one final embedding.\n\n\n\n\nsummary-muti-head-attention",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c4w2/index.html#v6-transformer-decoder",
    "href": "posts/c4w2/index.html#v6-transformer-decoder",
    "title": "Week 2 Text Summarization",
    "section": "V6: Transformer Decoder",
    "text": "V6: Transformer Decoder\n\n\n\n\noutline\n\nThere is a learning objective here!\nthe transformer decoder has two parts\n\na decoder block (with multi-head attention) - think feature acquisition.\na feed forward block - think non-parametric regression on the features.\n\n\n\n\n\ntransformer-decoder-overview\n\n\n\n\ntransformer-decoder-explaination\n\n\n\n\ntransformer-decoder-ff\n\n\n\n\ntransformer-decoder-explaination\n\n\n\n\ntransformer-decoder-summary",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c4w2/index.html#cross-entropy-loss",
    "href": "posts/c4w2/index.html#cross-entropy-loss",
    "title": "Week 2 Text Summarization",
    "section": "Cross entropy loss",
    "text": "Cross entropy loss\nCross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label.\n\n\n\n\ninference\n\nAfter training GPT2 on summarization data we just treat it like a word model and mine it for summaries. We do this by supply it with an input and predicting the output token by token. A more sophisticated method might be to use a beam search.\nAn even more sophisticated method might be to use an information metric to reject sentences and back track or better yet to do negative sampling from the prediction distribution (i.e. erase some prediction’s probabilities and renormalize)\nOne could do even better by providing hints, especially if we also head some kind of extractive model with a high-level of certainty about the most important sentence and their most significant concepts.\n\n\n\n\nquiz\n\nWe want the model to be penalized if it makes the wrong prediction. In this case it it does not predict the next word in the summary.\nThis may not be ideal for a number of reasons:\n\nthe Big world view “we are interested in a summary not the next word” what if the model is generating a semantically equivalent summary, in such a case it should not be penalized at all.\n\nIn a previous assignment we used a siamese network to check if two queries were equivalent. I think that allowing the network would be beneficial. (A loss that examines a generated sequence and compares it to the output.) But I don’t really know how to back-propagate the outcome for all the words. Well not exactly\nAs we are using teacher forcing we can take a position that we ignore all the mistakes the model made and give it a good output sequence and ask it for the next word. This then allows us to back prop the last word’s loss all by itself.\nIf we do this for each word in the output in sequence we should be able to reuse most of the calculations.\nThere are cases we have multiple summaries:\n\nFor a wikipedia article we often have all version from inception to the current day. This can provide multiple summaries and text along with an a golden version (the current summary). Oh and we may have a better summary in other languages but that is a different story.\nFor IMDB movie plots we often have a long synopsis and multiple shorter summaries. Also we may also have the book or screen play.\n\nI mention these two cases since Curriculum Learning may be able to assist us in training\n\n\n\n\nsummary\n\nI think these is much missing from this lesson about summerization. However there are a number of good source in papers as well as some lectures on YouTube.\nI have quickly summarized one and should link to it from here once it is published.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c4w2/index.html#lab1-attention",
    "href": "posts/c4w2/index.html#lab1-attention",
    "title": "Week 2 Text Summarization",
    "section": "Lab1 : Attention",
    "text": "Lab1 : Attention\nThis was a numpy based realization of dot product and multi-head attention. Some of the main assignment required porting this to Jax.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c4w2/index.html#lab2-the-transformer-decoder",
    "href": "posts/c4w2/index.html#lab2-the-transformer-decoder",
    "title": "Week 2 Text Summarization",
    "section": "Lab2 : The Transformer Decoder",
    "text": "Lab2 : The Transformer Decoder\nthis covered the transformer block",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c4w2/index.html#assignment-transformer-summarizer",
    "href": "posts/c4w2/index.html#assignment-transformer-summarizer",
    "title": "Week 2 Text Summarization",
    "section": "Assignment: Transformer Summarizer",
    "text": "Assignment: Transformer Summarizer\nThis long assignment primarily focused on dot product attention, multi-head attention and on building the transformer blocks. These were manageable as their theory had been explained in the lectures and their code had already been covered in the labs. It glosses over the parts involving data processing, training, evaluation and the actual summarization task. The summarization is accomplished using maximum likelihood estimate. A beam search might have yielded better results.\nThe date as described by:\n\nWe use the CNN/Daily Mail dataset (Hermann et al., 2015; Nallapati et al., 2016), which contains online news articles (781 tokens on average) paired with multi-sentence summaries (3.75 sentences or 56 tokens on average).\n\n\nGet To The Point §4 (Abigail et all 2017) We used the non anatomized version. However the earlier paper used a preprocessed version which replaced the named entities with token like $entity5. This is probably ideal in other situations like event processing where each event looks quite different unless one anatomizes them rendering them much more similar and hopefully helping the model generalize better by learning from the partitions induced by the equivalency relation.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c4w2/index.html#expanding-the-lab-to-a-project",
    "href": "posts/c4w2/index.html#expanding-the-lab-to-a-project",
    "title": "Week 2 Text Summarization",
    "section": "Expanding the lab to a project:",
    "text": "Expanding the lab to a project:\nThis is one of the main areas I’d like to focus on for a project. I have in mind a tool for improving wikipedia article leads. Here is how I’d like to take this project to the next level:\n\nmore data\ntrain it on additional material:\n\npapers and abstracts.\nwikipedia articles (with good first paragraphs. )\nbooks and book summaries (could be problematic due to the book’s length)\nmovie scripts and outlines from IMDB\n\na Storyline\nsummary (paragraph)\na synopsis (longer)\n\n\n\n\nMore algorithms\n\nUsing a reformer to handle longer texts like books.\nBetter summarization using:\n\na beam search to build better summaries.\na bayesian search to avoid repetitions.\n\nuse curriculum learning to speed up training with\n\neasier examples first.\nmultiple summaries per text.\nlearning on anonymized NE before graduating to non-anonymized texts\n\nuse better method for evaluation of summary.\n\nPerhaps an f-score combining precision or recall on\nAttention Activation summed as a Coverage score for each token.\n\nuse of non zero loss-weights layer\n\ndrop to zero as training progresses.\ndepend on the actual length of source and output.\nuse tf-idf to make holes in the mask surrounding essential concepts.\n\n\n\n\nEvaluation\nuse sammy lib with\n\nrouge-n metric\nthe pyramid metric\n\n\n\nExtra features\n\npages for paper reviews\npages for research questions\npages to implement exra code/ experiments.\nvisualize the concepts/sentences/paragraphs/sections covered in the summary.\nestablish a hierarchy of what would go into a longer outline.\ndevelop a f-score metric combining precision and recall for the summary of the abstract.\nin academic writing one sentence per paragraphs should capture the main concept and it is generally the first second or the last. Is such a sentence is available identify it. This would be done by comparing each sentence with the rest of the paragraph.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c4w2/index.html#open-question",
    "href": "posts/c4w2/index.html#open-question",
    "title": "Week 2 Text Summarization",
    "section": "Open question",
    "text": "Open question\nFor me the assignment raised a number of questions about what really going on here during training.\nI’ll probably do this assignment again and look for some answers to my many questions. Once I have these I’ll add them in the body of these notes.\n\nQuestions\n\nLoading and prepocessing the data:\n\nWhat is going on after we load the dataset - is there data augmentation?\nWhat this sub word vocab?\nHow to make my own sub word vocab?\nHow are out of vocab words being handled?\nCan we query the model about these beside running decode ?\nHow are these created - I saw several sizes of vocab.\n\nTraining\n\nTraining data seems to be a little mangled - there seems to be missing white space after the first token of the summaries, is there some way to fix this?\nIn not sure but why do we use teacher forcing during training?\n\n\nIt should speed training up, but the setup is unclear.\n\nEvaluation\n\nWhy are we not looking at a summarization metic like pyramid, rouge5 or good old precision and recall.\n\nInference\n\nHow can we tell the model thinks its done?\n\n\nwhen it output and  token\n\n\nHow to generate one sentence for each paragraph/section\n\n\nChop up the input and summarise each section.\nCreate an new dataset that bases it summaries on the last and first sentences of each paragraph. If that’s too long summarize again for each section.\nIntroduce a timed mask that hides [0:t*len/T] where T is total number of tokens being generated.\nmake the mask a Bayesian search mechanism that hides concepts in the output.\n\n\nHow to use multiple summaries like in IMDB?\n\n\nscore using the pyramid scheme or rogue.\n\n\nHow to make the model avoid repeating /rephrasing themselves?\n\n\nuse a metric on new information. for example Maximal marginal relevance. MMR = \\argmax [\\lambda Sim_1(s_i,Q)- (1 - \\lambda) \\max Sim_2(s_i,s_j)] where Q is the query and s are output sentences and try to bake this into the regularization.\na coverage vector seems to be a recommend method.\n\nVisualization\n\n\nIs there a easy way to see the activation for each word in the output?\nIs there a easy way to see which concepts are significant (not too common and not too rare)\nIs there a easy way to see which concepts are salient - aligned to near by concepts.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c4w2/index.html#references",
    "href": "posts/c4w2/index.html#references",
    "title": "Week 2 Text Summarization",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c4w2/index.html#papers",
    "href": "posts/c4w2/index.html#papers",
    "title": "Week 2 Text Summarization",
    "section": "Papers",
    "text": "Papers\n\nTransformers\n\nExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Raffel et al, 2019)\n\nReformer: The Efficient Transformer (Kitaev et al, 2020)\nAttention Is All You Need (Vaswani et al, 2017)\nDeep contextualized word representations (Peters et al, 2018)\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al, 2018)\nFinetuning Pretrained Transformers into RNNs (Kasai et all 2021)\n\n\n\nSummarization\n\nA trainable document summarizer. (Kupiec et al., 1995) extractive\nConstructing literature abstracts by computer: techniques and prospects. (Paice, 1990) extractive\nAutomatic text summarization: Past, present and future (Saggion and Poibeau, 2013) extractive\nAbstractive sentence summarization with attentive recurrent neural networks. (Chopra et al., 2016) abstractive summarization\nPointing the unknown words. (Nallapati et al., 2016) abstractive summarization\nA neural attention model for abstractive sentence summarization. (Rush et al.,2015;) abstractive summarization\nEfficient summarization with read-again and copy mechanism(Zeng et al., 2016) abstractive summarization\nGet To The Point: Summarization with Pointer-Generator Networks (Abigail et all 2017) Hybrid summarization. Note: Christopher D. Manning\nExtractive Summarization as Text Matching (Zhong et all 2020)",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c4w2/index.html#articles",
    "href": "posts/c4w2/index.html#articles",
    "title": "Week 2 Text Summarization",
    "section": "Articles",
    "text": "Articles\n\nThe Illustrated Transformer (Alammar, 2018)\nThe Illustrated GPT-2 (Alammar, 2019)\nHow GPT3 Works - Visualizations and Animations (Alammar, 2020)\nAttention? Attention! (Lilian Weng, 2018)\nThe Transformer Family (Lilian Weng, 2020)\nTeacher forcing for RNNs",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c4w2/index.html#links",
    "href": "posts/c4w2/index.html#links",
    "title": "Week 2 Text Summarization",
    "section": "Links",
    "text": "Links\n\nJax\nTrax\nTrax community on Gitter\nCNN daily mail dataset",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c2w3/lab02.html",
    "href": "posts/c2w3/lab02.html",
    "title": "NLP Specialization",
    "section": "",
    "text": "# Building the language model\n ### Count matrix\nTo calculate the n-gram probability, you will need to count frequencies of n-grams and n-gram prefixes in the training dataset. In some of the code assignment exercises, you will store the n-gram frequencies in a dictionary.\nIn other parts of the assignment, you will build a count matrix that keeps counts of (n-1)-gram prefix followed by all possible last words in the vocabulary.\nThe following code shows how to check, retrieve and update counts of n-grams in the word count dictionary.\n\n# manipulate n_gram count dictionary\n\nn_gram_counts = {\n    ('i', 'am', 'happy'): 2,\n    ('am', 'happy', 'because'): 1}\n\n# get count for an n-gram tuple\nprint(f\"count of n-gram {('i', 'am', 'happy')}: {n_gram_counts[('i', 'am', 'happy')]}\")\n\n# check if n-gram is present in the dictionary\nif ('i', 'am', 'learning') in n_gram_counts:\n    print(f\"n-gram {('i', 'am', 'learning')} found\")\nelse:\n    print(f\"n-gram {('i', 'am', 'learning')} missing\")\n\n# update the count in the word count dictionary\nn_gram_counts[('i', 'am', 'learning')] = 1\nif ('i', 'am', 'learning') in n_gram_counts:\n    print(f\"n-gram {('i', 'am', 'learning')} found\")\nelse:\n    print(f\"n-gram {('i', 'am', 'learning')} missing\")\n\ncount of n-gram ('i', 'am', 'happy'): 2\nn-gram ('i', 'am', 'learning') missing\nn-gram ('i', 'am', 'learning') found\n\n\nThe next code snippet shows how to merge two tuples in Python. That will be handy when creating the n-gram from the prefix and the last word.\n\n# concatenate tuple for prefix and tuple with the last word to create the n_gram\nprefix = ('i', 'am', 'happy')\nword = 'because'\n\n# note here the syntax for creating a tuple for a single word\nn_gram = prefix + (word,)\nprint(n_gram)\n\n('i', 'am', 'happy', 'because')\n\n\nIn the lecture, you’ve seen that the count matrix could be made in a single pass through the corpus. Here is one approach to do that.\n\nimport numpy as np\nimport pandas as pd\nfrom collections import defaultdict\ndef single_pass_trigram_count_matrix(corpus):\n    \"\"\"\n    Creates the trigram count matrix from the input corpus in a single pass through the corpus.\n    \n    Args:\n        corpus: Pre-processed and tokenized corpus. \n    \n    Returns:\n        bigrams: list of all bigram prefixes, row index\n        vocabulary: list of all found words, the column index\n        count_matrix: pandas dataframe with bigram prefixes as rows, \n                      vocabulary words as columns \n                      and the counts of the bigram/word combinations (i.e. trigrams) as values\n    \"\"\"\n    bigrams = []\n    vocabulary = []\n    count_matrix_dict = defaultdict(dict)\n    \n    # go through the corpus once with a sliding window\n    for i in range(len(corpus) - 3 + 1):\n        # the sliding window starts at position i and contains 3 words\n        trigram = tuple(corpus[i : i + 3])\n        \n        bigram = trigram[0 : -1]\n        if not bigram in bigrams:\n            bigrams.append(bigram)        \n        \n        last_word = trigram[-1]\n        if not last_word in vocabulary:\n            vocabulary.append(last_word)\n        \n        if (bigram,last_word) not in count_matrix_dict:\n            count_matrix_dict[bigram,last_word] = 0\n            \n        count_matrix_dict[bigram,last_word] += 1\n    \n    # convert the count_matrix to np.array to fill in the blanks\n    count_matrix = np.zeros((len(bigrams), len(vocabulary)))\n    for trigram_key, trigam_count in count_matrix_dict.items():\n        count_matrix[bigrams.index(trigram_key[0]), \\\n                     vocabulary.index(trigram_key[1])]\\\n        = trigam_count\n    \n    # np.array to pandas dataframe conversion\n    count_matrix = pd.DataFrame(count_matrix, index=bigrams, columns=vocabulary)\n    return bigrams, vocabulary, count_matrix\n\ncorpus = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n\nbigrams, vocabulary, count_matrix = single_pass_trigram_count_matrix(corpus)\n\nprint(count_matrix)\n\n                  happy  because    i   am  learning    .\n(i, am)             1.0      0.0  0.0  0.0       1.0  0.0\n(am, happy)         0.0      1.0  0.0  0.0       0.0  0.0\n(happy, because)    0.0      0.0  1.0  0.0       0.0  0.0\n(because, i)        0.0      0.0  0.0  1.0       0.0  0.0\n(am, learning)      0.0      0.0  0.0  0.0       0.0  1.0\n\n\n ### Probability matrix The next step is to build a probability matrix from the count matrix.\nYou can use an object dataframe from library pandas and its methods sum and div to normalize the cell counts with the sum of the respective rows.\n\n# create the probability matrix from the count matrix\nrow_sums = count_matrix.sum(axis=1)\n# delete each row by its sum\nprob_matrix = count_matrix.div(row_sums, axis=0)\n\nprint(prob_matrix)\n\n                  happy  because    i   am  learning    .\n(i, am)             0.5      0.0  0.0  0.0       0.5  0.0\n(am, happy)         0.0      1.0  0.0  0.0       0.0  0.0\n(happy, because)    0.0      0.0  1.0  0.0       0.0  0.0\n(because, i)        0.0      0.0  0.0  1.0       0.0  0.0\n(am, learning)      0.0      0.0  0.0  0.0       0.0  1.0\n\n\nThe probability matrix now helps you to find a probability of an input trigram.\n\n# find the probability of a trigram in the probability matrix\ntrigram = ('i', 'am', 'happy')\n\n# find the prefix bigram \nbigram = trigram[:-1]\nprint(f'bigram: {bigram}')\n\n# find the last word of the trigram\nword = trigram[-1]\nprint(f'word: {word}')\n\n# we are using the pandas dataframes here, column with vocabulary word comes first, row with the prefix bigram second\ntrigram_probability = prob_matrix[word][bigram]\nprint(f'trigram_probability: {trigram_probability}')\n\nbigram: ('i', 'am')\nword: happy\ntrigram_probability: 0.5\n\n\nIn the code assignment, you will be searching for the most probable words starting with a prefix. You can use the method str.startswith to test if a word starts with a prefix.\nHere is a code snippet showing how to use this method.\n\n# lists all words in vocabulary starting with a given prefix\nvocabulary = ['i', 'am', 'happy', 'because', 'learning', '.', 'have', 'you', 'seen','it', '?']\nstarts_with = 'ha'\n\nprint(f'words in vocabulary starting with prefix: {starts_with}\\n')\nfor word in vocabulary:\n    if word.startswith(starts_with):\n        print(word)\n\nwords in vocabulary starting with prefix: ha\n\nhappy\nhave\n\n\n ## Language model evaluation  ### Train/validation/test split In the videos, you saw that to evaluate language models, you need to keep some of the corpus data for validation and testing.\nThe choice of the test and validation data should correspond as much as possible to the distribution of the data coming from the actual application. If nothing but the input corpus is known, then random sampling from the corpus is used to define the test and validation subset.\nHere is a code similar to what you’ll see in the code assignment. The following function allows you to randomly sample the input data and return train/validation/test subsets in a split given by the method parameters.\n\n# we only need train and validation %, test is the remainder\nimport random\ndef train_validation_test_split(data, train_percent, validation_percent):\n    \"\"\"\n    Splits the input data to  train/validation/test according to the percentage provided\n    \n    Args:\n        data: Pre-processed and tokenized corpus, i.e. list of sentences.\n        train_percent: integer 0-100, defines the portion of input corpus allocated for training\n        validation_percent: integer 0-100, defines the portion of input corpus allocated for validation\n        \n        Note: train_percent + validation_percent need to be &lt;=100\n              the reminder to 100 is allocated for the test set\n    \n    Returns:\n        train_data: list of sentences, the training part of the corpus\n        validation_data: list of sentences, the validation part of the corpus\n        test_data: list of sentences, the test part of the corpus\n    \"\"\"\n    # fixed seed here for reproducibility\n    random.seed(87)\n    \n    # reshuffle all input sentences\n    random.shuffle(data)\n\n    train_size = int(len(data) * train_percent / 100)\n    train_data = data[0:train_size]\n    \n    validation_size = int(len(data) * validation_percent / 100)\n    validation_data = data[train_size:train_size + validation_size]\n    \n    test_data = data[train_size + validation_size:]\n    \n    return train_data, validation_data, test_data\n\ndata = [x for x in range (0, 100)]\n\ntrain_data, validation_data, test_data = train_validation_test_split(data, 80, 10)\nprint(\"split 80/10/10:\\n\",f\"train data:{train_data}\\n\", f\"validation data:{validation_data}\\n\", \n      f\"test data:{test_data}\\n\")\n\ntrain_data, validation_data, test_data = train_validation_test_split(data, 98, 1)\nprint(\"split 98/1/1:\\n\",f\"train data:{train_data}\\n\", f\"validation data:{validation_data}\\n\", \n      f\"test data:{test_data}\\n\")\n\nsplit 80/10/10:\n train data:[28, 76, 5, 0, 62, 29, 54, 95, 88, 58, 4, 22, 92, 14, 50, 77, 47, 33, 75, 68, 56, 74, 43, 80, 83, 84, 73, 93, 66, 87, 9, 91, 64, 79, 20, 51, 17, 27, 12, 31, 67, 81, 7, 34, 45, 72, 38, 30, 16, 60, 40, 86, 48, 21, 70, 59, 6, 19, 2, 99, 37, 36, 52, 61, 97, 44, 26, 57, 89, 55, 53, 85, 3, 39, 10, 71, 23, 32, 25, 8]\n validation data:[78, 65, 63, 11, 49, 98, 1, 46, 15, 41]\n test data:[90, 96, 82, 42, 35, 13, 69, 24, 94, 18]\n\nsplit 98/1/1:\n train data:[66, 23, 29, 28, 52, 87, 70, 13, 15, 2, 62, 43, 82, 50, 40, 32, 30, 79, 71, 89, 6, 10, 34, 78, 11, 49, 39, 42, 26, 46, 58, 96, 97, 8, 56, 86, 33, 93, 92, 91, 57, 65, 95, 20, 72, 3, 12, 9, 47, 37, 67, 1, 16, 74, 53, 99, 54, 68, 5, 18, 27, 17, 48, 36, 24, 45, 73, 19, 41, 59, 21, 98, 0, 31, 4, 85, 80, 64, 84, 88, 25, 44, 61, 22, 60, 94, 76, 38, 77, 81, 90, 69, 63, 7, 51, 14, 55, 83]\n validation data:[35]\n test data:[75]\n\n\n\n ### Perplexity\nIn order to implement the perplexity formula, you’ll need to know how to implement m-th order root of a variable.\n\\begin{equation*}\nPP(W)=\\sqrt[M]{\\prod_{i=1}^{m}{\\frac{1}{P(w_i|w_{i-1})}}}\n\\end{equation*}\nRemember from calculus:\n\\begin{equation*}\n\\sqrt[M]{\\frac{1}{x}} = x^{-\\frac{1}{M}}\n\\end{equation*}\nHere is a code that will help you with the formula.\n\n# to calculate the exponent, use the following syntax\np = 10 ** (-250)\nM = 100\nperplexity = p ** (-1 / M)\nprint(perplexity)\n\n316.22776601683796\n\n\nThat’s all for the lab for “N-gram language model” lesson of week 3.\n\n\n\nCitationBibTeX citation:@online{2025,\n  author = {},\n  date = {2025-01-31},\n  url = {https://orenbochman.github.io/notes-nlp/posts/c2w3/lab02.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n2025. January 31, 2025. https://orenbochman.github.io/notes-nlp/posts/c2w3/lab02.html."
  },
  {
    "objectID": "posts/c2w3/lab03.html",
    "href": "posts/c2w3/lab03.html",
    "title": "NLP Specialization",
    "section": "",
    "text": "# Out of vocabulary words (OOV)  ### Vocabulary In the video about the out of vocabulary words, you saw that the first step in dealing with the unknown words is to decide which words belong to the vocabulary.\nIn the code assignment, you will try the method based on minimum frequency - all words appearing in the training set with frequency &gt;= minimum frequency are added to the vocabulary.\nHere is a code for the other method, where the target size of the vocabulary is known in advance and the vocabulary is filled with words based on their frequency in the training set.\n\n# build the vocabulary from M most frequent words\n# use Counter object from the collections library to find M most common words\nfrom collections import Counter\n\n# the target size of the vocabulary\nM = 3\n\n# pre-calculated word counts\n# Counter could be used to build this dictionary from the source corpus\nword_counts = {'happy': 5, 'because': 3, 'i': 2, 'am': 2, 'learning': 3, '.': 1}\n\nvocabulary = Counter(word_counts).most_common(M)\n\n# remove the frequencies and leave just the words\nvocabulary = [w[0] for w in vocabulary]\n\nprint(f\"the new vocabulary containing {M} most frequent words: {vocabulary}\\n\") \n\nthe new vocabulary containing 3 most frequent words: ['happy', 'because', 'learning']\n\n\n\nNow that the vocabulary is ready, you can use it to replace the OOV words with &lt;UNK&gt; as you saw in the lecture.\n\n# test if words in the input sentences are in the vocabulary, if OOV, print &lt;UNK&gt;\nsentence = ['am', 'i', 'learning']\noutput_sentence = []\nprint(f\"input sentence: {sentence}\")\n\nfor w in sentence:\n    # test if word w is in vocabulary\n    if w in vocabulary:\n        output_sentence.append(w)\n    else:\n        output_sentence.append('&lt;UNK&gt;')\n        \nprint(f\"output sentence: {output_sentence}\")\n\ninput sentence: ['am', 'i', 'learning']\noutput sentence: ['&lt;UNK&gt;', '&lt;UNK&gt;', 'learning']\n\n\nWhen building the vocabulary in the code assignment, you will need to know how to iterate through the word counts dictionary.\nHere is an example of a similar task showing how to go through all the word counts and print out only the words with the frequency equal to f. \n\n# iterate through all word counts and print words with given frequency f\nf = 3\n\nword_counts = {'happy': 5, 'because': 3, 'i': 2, 'am': 2, 'learning':3, '.': 1}\n\nfor word, freq in word_counts.items():\n    if freq == f:\n        print(word)\n\nbecause\nlearning\n\n\nAs mentioned in the videos, if there are many &lt;UNK&gt; replacements in your train and test set, you may get a very low perplexity even though the model itself wouldn’t be very helpful.\nHere is a sample code showing this unwanted effect.\n\n# many &lt;unk&gt; low perplexity \ntraining_set = ['i', 'am', 'happy', 'because','i', 'am', 'learning', '.']\ntraining_set_unk = ['i', 'am', '&lt;UNK&gt;', '&lt;UNK&gt;','i', 'am', '&lt;UNK&gt;', '&lt;UNK&gt;']\n\ntest_set = ['i', 'am', 'learning']\ntest_set_unk = ['i', 'am', '&lt;UNK&gt;']\n\nM = len(test_set)\nprobability = 1\nprobability_unk = 1\n\n# pre-calculated probabilities\nbigram_probabilities = {('i', 'am'): 1.0, ('am', 'happy'): 0.5, ('happy', 'because'): 1.0, ('because', 'i'): 1.0, ('am', 'learning'): 0.5, ('learning', '.'): 1.0}\nbigram_probabilities_unk = {('i', 'am'): 1.0, ('am', '&lt;UNK&gt;'): 1.0, ('&lt;UNK&gt;', '&lt;UNK&gt;'): 0.5, ('&lt;UNK&gt;', 'i'): 0.25}\n\n# got through the test set and calculate its bigram probability\nfor i in range(len(test_set) - 2 + 1):\n    bigram = tuple(test_set[i: i + 2])\n    probability = probability * bigram_probabilities[bigram]\n        \n    bigram_unk = tuple(test_set_unk[i: i + 2])\n    probability_unk = probability_unk * bigram_probabilities_unk[bigram_unk]\n\n# calculate perplexity for both original test set and test set with &lt;UNK&gt;\nperplexity = probability ** (-1 / M)\nperplexity_unk = probability_unk ** (-1 / M)\n\nprint(f\"perplexity for the training set: {perplexity}\")\nprint(f\"perplexity for the training set with &lt;UNK&gt;: {perplexity_unk}\")\n\nperplexity for the training set: 1.2599210498948732\nperplexity for the training set with &lt;UNK&gt;: 1.0\n\n\n ### Smoothing\nAdd-k smoothing was described as a method for smoothing of the probabilities for previously unseen n-grams.\nHere is an example code that shows how to implement add-k smoothing but also highlights a disadvantage of this method. The downside is that n-grams not previously seen in the training dataset get too high probability.\nIn the code output bellow you’ll see that a phrase that is in the training set gets the same probability as an unknown phrase.\n\ndef add_k_smooting_probability(k, vocabulary_size, n_gram_count, n_gram_prefix_count):\n    numerator = n_gram_count + k\n    denominator = n_gram_prefix_count + k * vocabulary_size\n    return numerator / denominator\n\ntrigram_probabilities = {('i', 'am', 'happy') : 2}\nbigram_probabilities = {( 'am', 'happy') : 10}\nvocabulary_size = 5\nk = 1\n\nprobability_known_trigram = add_k_smooting_probability(k, vocabulary_size, trigram_probabilities[('i', 'am', 'happy')], \n                           bigram_probabilities[( 'am', 'happy')])\n\nprobability_unknown_trigram = add_k_smooting_probability(k, vocabulary_size, 0, 0)\n\nprint(f\"probability_known_trigram: {probability_known_trigram}\")\nprint(f\"probability_unknown_trigram: {probability_unknown_trigram}\")\n\nprobability_known_trigram: 0.2\nprobability_unknown_trigram: 0.2\n\n\n ### Back-off Back-off is a model generalization method that leverages information from lower order n-grams in case information about the high order n-grams is missing. For example, if the probability of an trigram is missing, use bigram information and so on.\nHere you can see an example of a simple back-off technique.\n\n# pre-calculated probabilities of all types of n-grams\ntrigram_probabilities = {('i', 'am', 'happy'): 0}\nbigram_probabilities = {( 'am', 'happy'): 0.3}\nunigram_probabilities = {'happy': 0.4}\n\n# this is the input trigram we need to estimate\ntrigram = ('are', 'you', 'happy')\n\n# find the last bigram and unigram of the input\nbigram = trigram[1: 3]\nunigram = trigram[2]\nprint(f\"besides the trigram {trigram} we also use bigram {bigram} and unigram ({unigram})\\n\")\n\n# 0.4 is used as an example, experimentally found for web-scale corpuses when using the \"stupid\" back-off\nlambda_factor = 0.4\nprobability_hat_trigram = 0\n\n# search for first non-zero probability starting with trigram\n# to generalize this for any order of n-gram hierarchy, \n# you could loop through the probability dictionaries instead of if/else cascade\nif trigram not in trigram_probabilities or trigram_probabilities[trigram] == 0:\n    print(f\"probability for trigram {trigram} not found\")\n    \n    if bigram not in bigram_probabilities or bigram_probabilities[bigram] == 0:\n        print(f\"probability for bigram {bigram} not found\")\n        \n        if unigram in unigram_probabilities:\n            print(f\"probability for unigram {unigram} found\\n\")\n            probability_hat_trigram = lambda_factor * lambda_factor * unigram_probabilities[unigram]\n        else:\n            probability_hat_trigram = 0\n    else:\n        probability_hat_trigram = lambda_factor * bigram_probabilities[bigram]\nelse:\n    probability_hat_trigram = trigram_probabilities[trigram]\n\nprint(f\"probability for trigram {trigram} estimated as {probability_hat_trigram}\")\n\nbesides the trigram ('are', 'you', 'happy') we also use bigram ('you', 'happy') and unigram (happy)\n\nprobability for trigram ('are', 'you', 'happy') not found\nprobability for bigram ('you', 'happy') not found\nprobability for unigram happy found\n\nprobability for trigram ('are', 'you', 'happy') estimated as 0.06400000000000002\n\n\n ### Interpolation The other method for using probabilities of lower order n-grams is the interpolation. In this case, you use weighted probabilities of n-grams of all orders every time, not just when high order information is missing.\nFor example, you always combine trigram, bigram and unigram probability. You can see how this in the following code snippet.\n\n# pre-calculated probabilities of all types of n-grams\ntrigram_probabilities = {('i', 'am', 'happy'): 0.15}\nbigram_probabilities = {( 'am', 'happy'): 0.3}\nunigram_probabilities = {'happy': 0.4}\n\n# the weights come from optimization on a validation set\nlambda_1 = 0.8\nlambda_2 = 0.15\nlambda_3 = 0.05\n\n# this is the input trigram we need to estimate\ntrigram = ('i', 'am', 'happy')\n\n# find the last bigram and unigram of the input\nbigram = trigram[1: 3]\nunigram = trigram[2]\nprint(f\"besides the trigram {trigram} we also use bigram {bigram} and unigram ({unigram})\\n\")\n\n# in the production code, you would need to check if the probability n-gram dictionary contains the n-gram\nprobability_hat_trigram = lambda_1 * trigram_probabilities[trigram] \n+ lambda_2 * bigram_probabilities[bigram]\n+ lambda_3 * unigram_probabilities[unigram]\n\nprint(f\"estimated probability of the input trigram {trigram} is {probability_hat_trigram}\")\n\nbesides the trigram ('i', 'am', 'happy') we also use bigram ('am', 'happy') and unigram (happy)\n\n\n\n0.045\n\n\n0.020000000000000004\n\n\nestimated probability of the input trigram ('i', 'am', 'happy') is 0.12\n\n\nThat’s it for week 3, you should be ready now for the code assignment.\n\n\n\nCitationBibTeX citation:@online{2025,\n  author = {},\n  date = {2025-01-31},\n  url = {https://orenbochman.github.io/notes-nlp/posts/c2w3/lab03.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n2025. January 31, 2025. https://orenbochman.github.io/notes-nlp/posts/c2w3/lab03.html."
  },
  {
    "objectID": "posts/c4w4/index.html",
    "href": "posts/c4w4/index.html",
    "title": "Week 4 Chat Bots",
    "section": "",
    "text": "deeplearning.ai",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots"
    ]
  },
  {
    "objectID": "posts/c4w4/index.html#weeks-learning-objectives",
    "href": "posts/c4w4/index.html#weeks-learning-objectives",
    "title": "Week 4 Chat Bots",
    "section": "Week’s Learning Objectives:",
    "text": "Week’s Learning Objectives:\n\nExplain the motivation for reversible layers\nIntegrate locality sensitive hashing into attention layers\nDescribe the Reformer model\n\nDeep learning and A.I. researchers push the field forward by looking for new techniques as well as refinements of old ideas to get better performance on tasks. In this lesson we cover reversible layers which allow us to leverage a time memory tradeoff to process book length sequences and handle contexts over a conversation.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots"
    ]
  },
  {
    "objectID": "posts/c4w4/index.html#video-1-tasks-with-long-sequences",
    "href": "posts/c4w4/index.html#video-1-tasks-with-long-sequences",
    "title": "Week 4 Chat Bots",
    "section": "Video 1 : Tasks with Long Sequences",
    "text": "Video 1 : Tasks with Long Sequences\n In this week you are going to learn about tasks that require processing longer sequences: - Writing books - Storytelling and understanding - Building intelligent agents for conversations like chat-bots.\nMore specifically we will understand how re-former model (AKA the reversible transformer) and reversible layers work. This week you will learn about the bottlenecks in these larger transformer models, and solutions you can use to make them trainable for you. You will also learn about the. Here is what you will be building for your programming assignment: A chatbot!\nIn many ways a Chat bot is very similar to a Q&A system which we built last week and that is also similar to query based summarization another task we covered a week before that. The new challenge is to manage what parts of the new and old context we keep around as the dialogue progresses. Chatbot are smart A.I. agents and much of the techniques developed under the umbrella of knowledge-based AI is also relevant in developing these. For instance carrying out actions on behalf of the user. Chatbots can also get a very simple ui via the web or as an mobile app, which is another area I have some experience. However an even more powerful paradigm here is the ability to interact using voice which has many additional benefit for example supporting people with disabilities and operating in hands-free mode. Here is a link to an AI Storytelling system.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots"
    ]
  },
  {
    "objectID": "posts/c4w4/index.html#video-2-transformer-complexity",
    "href": "posts/c4w4/index.html#video-2-transformer-complexity",
    "title": "Week 4 Chat Bots",
    "section": "Video 2: Transformer Complexity",
    "text": "Video 2: Transformer Complexity\n\n\n\n\nweek-4\n\nOne of the biggest issues with the transformers is that it takes time and a lot of memory when training. Concretely here are the numbers. If you have a sequence of length L , then you need L^2*N memory to handle the sequence. So if you have N layers, that means your model will take N times more time to complete. As L gets larger, the memory and the time quickly increases.\nPerhaps this is the reason people are looking into converting transformers into RNN after training.\n\n\n\n\nweek-4\n\nWhen you are handling long sequences, you frequently don’t need to consider all L positions. You can just focus on an area of interest instead. For example, when translating a long text from one language to another, you don’t need to consider every word at once. You can instead focus on a single word being translated, and those immediately around it, by using attention.\nTo overcome the memory requirements you can recompute the activations. As long as you do it efficiently, you will be able to save a good amount of time and memory. You will learn this week how to do it. Instead of storing N layers, you will be able to recompute them when doing the back-propagation. That combined with local attention, will give you a much faster model that works at the same level as the transformer you learned about last week.\n\none area where we can make headway is working with a subsequence of interest.\nduring training we need to keep the activations in memory for the back propagation task. Clearly for inference we may be able to save on memory.\nthe alternative is to discard the activations as we go along and recalculate later. This can allows trading memory for compute time. However with larger models compute time is also a bottleneck.\n\n\n\n\n\nweek-4",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots"
    ]
  },
  {
    "objectID": "posts/c4w4/index.html#video-3-lsh-attention",
    "href": "posts/c4w4/index.html#video-3-lsh-attention",
    "title": "Week 4 Chat Bots",
    "section": "Video 3: LSH Attention",
    "text": "Video 3: LSH Attention\nIn Course 1, we covered how locality sensitive hashing (LSH) works. You learned about:\n\nKNN\nHash Tables and Hash Functions\nLocality Sensitive Hashing\nMultiple Planes\n\nHere are the steps to follow to compute LSH given some vectors, where the vectors may correspond to the transformed word embedding that your transformer outputs.\nAttention is used to try which query (q) and key (k) are the most similar. To do so, you hash q and the keys. This will put similar vectors in the same bucket that you can use. The drawing above shows the lines that separate the buckets. Those could be seen as the planes. Remember that the standard attention mechanism is defined as follows:\n\nA(Q,K,V) = softmax(QK^T)V\n\nOnce you hash Q and K you will then compute standard attention on the bins that you have created. You will repeat the same process several times to increase the probability of having the same key in the same bin as the query.\n\n\n\n\nweek-4\n\n\nGiven the sequence of queries and keys, you hash them into buckets. Check out Course 1 Week 4 for a review of the hashing.\nYou will then sort them by bucket.\nYou split the buckets into chunks (this is a technical detail for parallel computing purposes).\nYou then compute the attention within the same bucket of the chunk you are looking at and the previous chunk. &gt; Q. Why do you need to look at the previous chunk?\nYou can see in the figure some buckets (both blue and yellow) have been split across two chunks. Looking at the previous chunk will let you attend to the full bucket.\n\nIn Winograd schemas the resolution of the ambiguous pronoun switches between the two variants of the sentence. &gt; the animal didn’t cross the street because it was too tired &gt; the animal didn’t cross the street because it was too wide &gt;The city councilmen refused the demonstrators a permit because they feared violence. &gt;The city councilmen refused the demonstrators a permit because they advocated violence.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots"
    ]
  },
  {
    "objectID": "posts/c4w4/index.html#video-4-motivation-for-reversible-layers-memory",
    "href": "posts/c4w4/index.html#video-4-motivation-for-reversible-layers-memory",
    "title": "Week 4 Chat Bots",
    "section": "Video 4 Motivation for Reversible Layers: Memory!",
    "text": "Video 4 Motivation for Reversible Layers: Memory!\n\n\n\n\nweek-4\n\nFor example in this model:\n\n2 GB for the input\n2 GB are required to compute the Attention\n2 GB for the feed forward. There are 12 attention layers 12 feed forward layers. That is equal to 12 * 2 + 12*2 + 2 (for the input) = 50 GB. That is a lot of memory.\n\nIf N is the sequence length:\n\nTransformers need O(N^2) memory.\n\nEach layer of a transformers has an Attention block and feed-forward block. If we want to process, for example to train a document of length 1 million token with 12 layers we will need 50 GB of ram. As we use residual architecture during prediction we only need the current layers input and the output for the next layer. But during training we need to keep all the copies so we can back-propagate the errors.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots"
    ]
  },
  {
    "objectID": "posts/c4w4/index.html#video-5-reversible-residual-layers",
    "href": "posts/c4w4/index.html#video-5-reversible-residual-layers",
    "title": "Week 4 Chat Bots",
    "section": "Video 5 Reversible Residual Layers",
    "text": "Video 5 Reversible Residual Layers",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots"
    ]
  },
  {
    "objectID": "posts/c4w4/index.html#video-6-reformer",
    "href": "posts/c4w4/index.html#video-6-reformer",
    "title": "Week 4 Chat Bots",
    "section": "Video 6 Reformer",
    "text": "Video 6 Reformer\ncan run 1 million token in 16 gb",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots"
    ]
  },
  {
    "objectID": "posts/c4w4/index.html#lab-2-reversible-layers",
    "href": "posts/c4w4/index.html#lab-2-reversible-layers",
    "title": "Week 4 Chat Bots",
    "section": "Lab 2: Reversible layers",
    "text": "Lab 2: Reversible layers\nFrom the trax documents a Residual, involves first a split and then a merge:\nreturn Serial(\n    Branch(shortcut, layer), # split \n    Add(),                   # merge\n)\nwhere:\n\nBranch(shortcut, layers): makes two copies of the single incoming data stream, passes one copy via the shortcut (typically a no-op), and processes the other copy via the given layers (applied in series). [𝑛_{𝑖𝑛}=1, 𝑛_{𝑜𝑢𝑡}=2]\nAdd(): combines the two streams back into one by adding two tensors element-wise. [𝑛_{𝑖𝑛}=2, 𝑛_{𝑜𝑢𝑡}=1] In the Branch operation each layer in the input list copies as many inputs from the stack as it needs, and their outputs are successively combined on stack. Put another way, each element of the branch can have differing numbers of inputs and outputs. Let’s try a more complex example. To work these operations modify the stack by replicating the input needed as well as pushing the outputs (as specified using th out parameters).",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots"
    ]
  },
  {
    "objectID": "posts/c4w4/index.html#tokenization",
    "href": "posts/c4w4/index.html#tokenization",
    "title": "Week 4 Chat Bots",
    "section": "Tokenization",
    "text": "Tokenization\n\nSentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (Kudo & Richardson 2018) sub-word tokenization\nSubword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates (Kudo 2018) sub-word tokenization\nNeural Machine Translation of Rare Words with Subword Units (Sennrich et all 2016) sub-word tokenization\nSubword tokenizers TF tutorial sub-word tokenization\n[https://blog.floydhub.com/tokenization-nlp/]\nSwivel: Improving Embeddings by Noticing What’s Missing (Shazeer, 2016)",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots"
    ]
  },
  {
    "objectID": "posts/c4w4/index.html#transformers",
    "href": "posts/c4w4/index.html#transformers",
    "title": "Week 4 Chat Bots",
    "section": "Transformers",
    "text": "Transformers\n\nExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Raffel et al, 2019)\n\nReformer: The Efficient Transformer (Kitaev et al, 2020)\nAttention Is All You Need (Vaswani et al, 2017)\nDeep contextualized word representations (Peters et al, 2018)\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al, 2018)\nFinetuning Pretrained Transformers into RNNs (Kasai et all 2021)\nThe Illustrated Transformer (Alammar, 2018)\nThe Illustrated GPT-2 (Alammar, 2019)\nHow GPT3 Works - Visualizations and Animations (Alammar, 2020)\nAttention? Attention! (Lilian Weng, 2018)\nThe Transformer Family (Lilian Weng, 2020)\nTeacher forcing for RNNs\n\n\nQuestion Answering Task:\n\nTitle (Author et al., Year) note",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots"
    ]
  },
  {
    "objectID": "posts/c4w4/index.html#links",
    "href": "posts/c4w4/index.html#links",
    "title": "Week 4 Chat Bots",
    "section": "Links",
    "text": "Links\n\nJax\nTrax\nTrax community on Gitter\nCNN daily mail dataset\n\nLei Mao Machine Learning, Artificial Intelligence, Computer Science. [Byte Pair Encoding (Lei Mao 2021)] (https://leimao.github.io/blog/Byte-Pair-Encoding/) videos:\nQ&A\n\n\nSubword tokenizers  Swivel Embeddings https://youtu.be/hAvtJ516Mw4",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots"
    ]
  },
  {
    "objectID": "posts/c3w1/index.html",
    "href": "posts/c3w1/index.html",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "",
    "text": "course banner\n\n\nresources:\n\n(Chadha 2020) Aman Chadha’s Notes\nIbrahim Jelliti’s Notes\n\n\n\n\n\n\n\nReferences\n\nChadha, Aman. 2020. “Distilled Notes for the Natural Language Processing Specialization on Coursera (Offered by Deeplearning.ai).” https://www.aman.ai. www.aman.ai.\n\nCitationBibTeX citation:@online{2020,\n  author = {},\n  title = {Neural {Networks} for {Sentiment} {Analysis}},\n  date = {2020-10-23},\n  url = {https://orenbochman.github.io/notes-nlp/posts/c3w1/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“Neural Networks for Sentiment Analysis.” 2020. October 23,\n2020. https://orenbochman.github.io/notes-nlp/posts/c3w1/.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Machine Translation"
    ]
  },
  {
    "objectID": "posts/c1w4/lab02.html",
    "href": "posts/c1w4/lab02.html",
    "title": "Hash functions and multiplanes",
    "section": "",
    "text": "course banner\nIn this lab, we are going to practice the most important concepts related to the hash functions explained in the videos. You will be using these in this week’s assignment.\nA key point for the lookup using hash functions is the calculation of the hash key or bucket id that we assign for a given entry. In this notebook, we will cover:"
  },
  {
    "objectID": "posts/c1w4/lab02.html#basic-hash-tables",
    "href": "posts/c1w4/lab02.html#basic-hash-tables",
    "title": "Hash functions and multiplanes",
    "section": "Basic Hash tables",
    "text": "Basic Hash tables\nHash tables are data structures that allow indexing data to make lookup tasks more efficient. In this part, you will see the implementation of the simplest hash function.\n\nimport numpy as np                # library for array and matrix manipulation\nimport pprint                     # utilities for console printing \nfrom utils_nb import plot_vectors # helper function to plot vectors\nimport matplotlib.pyplot as plt   # visualization library\n\npp = pprint.PrettyPrinter(indent=4) # Instantiate a pretty printer\n\nIn the next cell, we will define a straightforward hash function for integer numbers. The function will receive a list of integer numbers and the desired amount of buckets. The function will produce a hash table stored as a dictionary, where keys contain the hash keys, and the values will provide the hashed elements of the input list.\nThe hash function is just the remainder of the integer division between each element and the desired number of buckets.\n\ndef basic_hash_table(value_l, n_buckets):\n    \n    def hash_function(value, n_buckets):\n        return int(value) % n_buckets\n    \n    hash_table = {i:[] for i in range(n_buckets)} # Initialize all the buckets in the hash table as empty lists\n\n    for value in value_l:\n        hash_value = hash_function(value,n_buckets) # Get the hash key for the given value\n        hash_table[hash_value].append(value) # Add the element to the corresponding bucket\n    \n    return hash_table\n\nNow let’s see the hash table function in action. The pretty print function (pprint()) will produce a visually appealing output.\n\nvalue_l = [100, 10, 14, 17, 97] # Set of values to hash\nhash_table_example = basic_hash_table(value_l, n_buckets=10)\npp.pprint(hash_table_example)\n\n{   0: [100, 10],\n    1: [],\n    2: [],\n    3: [],\n    4: [14],\n    5: [],\n    6: [],\n    7: [17, 97],\n    8: [],\n    9: []}\n\n\nIn this case, the bucket key must be the rightmost digit of each number."
  },
  {
    "objectID": "posts/c1w4/lab02.html#planes",
    "href": "posts/c1w4/lab02.html#planes",
    "title": "Hash functions and multiplanes",
    "section": "Planes",
    "text": "Planes\nMultiplanes hash functions are other types of hash functions. Multiplanes hash functions are based on the idea of numbering every single region that is formed by the intersection of n planes. In the following code, we show the most basic forms of the multiplanes principle. First, with a single plane:\n\nP = np.array([[1, 1]]) # Define a single plane. \nfig, ax1 = plt.subplots(figsize=(8, 8)) # Create a plot\n\nplot_vectors([P], axes=[2, 2], ax=ax1) # Plot the plane P as a vector\n\n# Plot  random points. \nfor i in range(0, 10):\n        v1 = np.array(np.random.uniform(-2, 2, 2)) # Get a pair of random numbers between -4 and 4 \n        side_of_plane = np.sign(np.dot(P, v1.T)) \n        \n        # Color the points depending on the sign of the result of np.dot(P, point.T)\n        if side_of_plane == 1:\n            ax1.plot([v1[0]], [v1[1]], 'bo') # Plot blue points\n        else:\n            ax1.plot([v1[0]], [v1[1]], 'ro') # Plot red points\n\nplt.show()\n\n\n\n\n\n\n\n\nThe first thing to note is that the vector that defines the plane does not mark the boundary between the two sides of the plane. It marks the direction in which you find the ‘positive’ side of the plane. Not intuitive at all!\nIf we want to plot the separation plane, we need to plot a line that is perpendicular to our vector P. We can get such a line using a 90^o rotation matrix.\nFeel free to change the direction of the plane P.\n\nP = np.array([[1, 2]])  # Define a single plane. You may change the direction\n\n# Get a new plane perpendicular to P. We use a rotation matrix\nPT = np.dot([[0, 1], [-1, 0]], P.T).T  \n\nfig, ax1 = plt.subplots(figsize=(8, 8)) # Create a plot with custom size\n\nplot_vectors([P], colors=['b'], axes=[2, 2], ax=ax1) # Plot the plane P as a vector\n\n# Plot the plane P as a 2 vectors. \n# We scale by 2 just to get the arrows outside the current box\nplot_vectors([PT * 4, PT * -4], colors=['k', 'k'], axes=[4, 4], ax=ax1)\n\n# Plot 20 random points. \nfor i in range(0, 20):\n        v1 = np.array(np.random.uniform(-4, 4, 2)) # Get a pair of random numbers between -4 and 4 \n        side_of_plane = np.sign(np.dot(P, v1.T)) # Get the sign of the dot product with P\n        # Color the points depending on the sign of the result of np.dot(P, point.T)\n        if side_of_plane == 1:\n            ax1.plot([v1[0]], [v1[1]], 'bo') # Plot a blue point\n        else:\n            ax1.plot([v1[0]], [v1[1]], 'ro') # Plot a red point\n\nplt.show()\n\n\n\n\n\n\n\n\nNow, let us see what is inside the code that color the points.\n\nP = np.array([[1, 1]])      # Single plane\nv1 = np.array([[1, 2]])     # Sample point 1\nv2 = np.array([[-1, 1]])    # Sample point 2\nv3 = np.array([[-2, -1]])   # Sample point 3\n\n\nnp.dot(P, v1.T)\n\narray([[3]])\n\n\n\nnp.dot(P, v2.T)\n\narray([[0]])\n\n\n\nnp.dot(P, v3.T)\n\narray([[-3]])\n\n\nThe function below checks in which side of the plane P is located the vector v\n\ndef side_of_plane(P, v):\n    dotproduct = np.dot(P, v.T) # Get the dot product P * v'\n    sign_of_dot_product = np.sign(dotproduct) # The sign of the elements of the dotproduct matrix \n    sign_of_dot_product_scalar = sign_of_dot_product.item() # The value of the first item\n    return sign_of_dot_product_scalar\n\n\nside_of_plane(P, v1) # In which side is [1, 2]\n\n1\n\n\n\nside_of_plane(P, v2) # In which side is [-1, 1]\n\n0\n\n\n\nside_of_plane(P, v3) # In which side is [-2, -1]\n\n-1"
  },
  {
    "objectID": "posts/c1w4/lab02.html#hash-function-with-multiple-planes",
    "href": "posts/c1w4/lab02.html#hash-function-with-multiple-planes",
    "title": "Hash functions and multiplanes",
    "section": "Hash Function with multiple planes",
    "text": "Hash Function with multiple planes\nIn the following section, we are going to define a hash function with a list of three custom planes in 2D.\n\nP1 = np.array([[1, 1]])   # First plane 2D\nP2 = np.array([[-1, 1]])  # Second plane 2D\nP3 = np.array([[-1, -1]]) # Third plane 2D\nP_l = [P1, P2, P3]  # List of arrays. It is the multi plane\n\n# Vector to search\nv = np.array([[2, 2]])\n\nThe next function creates a hash value based on a set of planes. The output value is a combination of the side of the plane where the vector is localized with respect to the collection of planes.\nWe can think of this list of planes as a set of basic hash functions, each of which can produce only 1 or 0 as output.\n\ndef hash_multi_plane(P_l, v):\n    hash_value = 0\n    for i, P in enumerate(P_l):\n        sign = side_of_plane(P,v)\n        hash_i = 1 if sign &gt;=0 else 0\n        hash_value += 2**i * hash_i\n    return hash_value\n\n\nhash_multi_plane(P_l, v) # Find the number of the plane that containes this value\n\n3"
  },
  {
    "objectID": "posts/c1w4/lab02.html#random-planes",
    "href": "posts/c1w4/lab02.html#random-planes",
    "title": "Hash functions and multiplanes",
    "section": "Random Planes",
    "text": "Random Planes\nIn the cell below, we create a set of three random planes\n\nnp.random.seed(0)\nnum_dimensions = 2 # is 300 in assignment\nnum_planes = 3 # is 10 in assignment\nrandom_planes_matrix = np.random.normal(\n                       size=(num_planes,\n                             num_dimensions))\nprint(random_planes_matrix)\n\n[[ 1.76405235  0.40015721]\n [ 0.97873798  2.2408932 ]\n [ 1.86755799 -0.97727788]]\n\n\n\nv = np.array([[2, 2]])\n\nThe next function is similar to the side_of_plane() function, but it evaluates more than a plane each time. The result is an array with the side of the plane of v, for the set of planes P\n\n# Side of the plane function. The result is a matrix\ndef side_of_plane_matrix(P, v):\n    dotproduct = np.dot(P, v.T)\n    sign_of_dot_product = np.sign(dotproduct) # Get a boolean value telling if the value in the cell is positive or negative\n    return sign_of_dot_product\n\nGet the side of the plane of the vector [2, 2] for the set of random planes.\n\nsides_l = side_of_plane_matrix(\n            random_planes_matrix, v)\nsides_l\n\narray([[1.],\n       [1.],\n       [1.]])\n\n\nNow, let us use the former function to define our multiplane hash function\n\ndef hash_multi_plane_matrix(P, v, num_planes):\n    sides_matrix = side_of_plane_matrix(P, v) # Get the side of planes for P and v\n    hash_value = 0\n    for i in range(num_planes):\n        sign = sides_matrix[i].item() # Get the value inside the matrix cell\n        hash_i = 1 if sign &gt;=0 else 0\n        hash_value += 2**i * hash_i # sum 2^i * hash_i\n        \n    return hash_value\n\nPrint the bucket hash for the vector v = [2, 2].\n\nhash_multi_plane_matrix(random_planes_matrix, v, num_planes)\n\n7\n\n\n\nNote\nThis showed you how to make one set of random planes. You will make multiple sets of random planes in order to make the approximate nearest neighbors more accurate."
  },
  {
    "objectID": "posts/c1w4/lab02.html#document-vectors",
    "href": "posts/c1w4/lab02.html#document-vectors",
    "title": "Hash functions and multiplanes",
    "section": "Document vectors",
    "text": "Document vectors\nBefore we finish this lab, remember that you can represent a document as a vector by adding up the word vectors for the words inside the document. In this example, our embedding contains only three words, each represented by a 3D array.\n\nword_embedding = {\"I\": np.array([1,0,1]),\n                   \"love\": np.array([-1,0,1]),\n                   \"learning\": np.array([1,0,1])\n                  }\nwords_in_document = ['I', 'love', 'learning', 'not_a_word']\ndocument_embedding = np.array([0,0,0])\nfor word in words_in_document:\n    document_embedding += word_embedding.get(word,0)\n    \nprint(document_embedding)\n\n[1 0 3]\n\n\nCongratulations! You’ve now completed this lab on hash functions and multiplanes!"
  },
  {
    "objectID": "posts/c1w4/assignment.html",
    "href": "posts/c1w4/assignment.html",
    "title": "Assignment 4 - Naive Machine Translation and LSH",
    "section": "",
    "text": "course banner\nYou will now implement your first machine translation system and then you will see how locality sensitive hashing works. Let’s get started by importing the required functions!\nIf you are running this notebook in your local computer, don’t forget to download the twitter samples and stopwords from nltk.\nNOTE: The Exercise xx numbers in this assignment are inconsistent with the UNQ_Cx numbers."
  },
  {
    "objectID": "posts/c1w4/assignment.html#the-data",
    "href": "posts/c1w4/assignment.html#the-data",
    "title": "Assignment 4 - Naive Machine Translation and LSH",
    "section": "The data",
    "text": "The data\nThe full dataset for English embeddings is about 3.64 gigabytes, and the French embeddings are about 629 megabytes. To prevent the Coursera workspace from crashing, we’ve extracted a subset of the embeddings for the words that you’ll use in this assignment.\nIf you want to run this on your local computer and use the full dataset, you can download the * English embeddings from Google code archive word2vec look for GoogleNews-vectors-negative300.bin.gz * You’ll need to unzip the file first. * and the French embeddings from cross_lingual_text_classification. * in the terminal, type (in one line) curl -o ./wiki.multi.fr.vec https://dl.fbaipublicfiles.com/arrival/vectors/wiki.multi.fr.vec\nThen copy-paste the code below and run it.\n# Use this code to download and process the full dataset on your local computer\n\nfrom gensim.models import KeyedVectors\n\nen_embeddings = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary = True)\nfr_embeddings = KeyedVectors.load_word2vec_format('./wiki.multi.fr.vec')\n\n\n# loading the english to french dictionaries\nen_fr_train = get_dict('en-fr.train.txt')\nprint('The length of the english to french training dictionary is', len(en_fr_train))\nen_fr_test = get_dict('en-fr.test.txt')\nprint('The length of the english to french test dictionary is', len(en_fr_train))\n\nenglish_set = set(en_embeddings.vocab)\nfrench_set = set(fr_embeddings.vocab)\nen_embeddings_subset = {}\nfr_embeddings_subset = {}\nfrench_words = set(en_fr_train.values())\n\nfor en_word in en_fr_train.keys():\n    fr_word = en_fr_train[en_word]\n    if fr_word in french_set and en_word in english_set:\n        en_embeddings_subset[en_word] = en_embeddings[en_word]\n        fr_embeddings_subset[fr_word] = fr_embeddings[fr_word]\n\n\nfor en_word in en_fr_test.keys():\n    fr_word = en_fr_test[en_word]\n    if fr_word in french_set and en_word in english_set:\n        en_embeddings_subset[en_word] = en_embeddings[en_word]\n        fr_embeddings_subset[fr_word] = fr_embeddings[fr_word]\n\n\npickle.dump( en_embeddings_subset, open( \"en_embeddings.p\", \"wb\" ) )\npickle.dump( fr_embeddings_subset, open( \"fr_embeddings.p\", \"wb\" ) )\n\nThe subset of data\nTo do the assignment on the Coursera workspace, we’ll use the subset of word embeddings.\n\nen_embeddings_subset = pickle.load(open(\"en_embeddings.p\", \"rb\"))\nfr_embeddings_subset = pickle.load(open(\"fr_embeddings.p\", \"rb\"))\n\n\n\nLook at the data\n\nen_embeddings_subset: the key is an English word, and the vaule is a 300 dimensional array, which is the embedding for that word.\n\n'the': array([ 0.08007812,  0.10498047,  0.04980469,  0.0534668 , -0.06738281, ....\n\nfr_embeddings_subset: the key is an French word, and the vaule is a 300 dimensional array, which is the embedding for that word.\n\n'la': array([-6.18250e-03, -9.43867e-04, -8.82648e-03,  3.24623e-02,...\n\n\nLoad two dictionaries mapping the English to French words\n\nA training dictionary\nand a testing dictionary.\n\n\n# loading the english to french dictionaries\nen_fr_train = get_dict('en-fr.train.txt')\nprint('The length of the English to French training dictionary is', len(en_fr_train))\nen_fr_test = get_dict('en-fr.test.txt')\nprint('The length of the English to French test dictionary is', len(en_fr_train))\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[4], line 2\n      1 # loading the english to french dictionaries\n----&gt; 2 en_fr_train = get_dict('en-fr.train.txt')\n      3 print('The length of the English to French training dictionary is', len(en_fr_train))\n      4 en_fr_test = get_dict('en-fr.test.txt')\n\nNameError: name 'get_dict' is not defined\n\n\n\n\n\nLooking at the English French dictionary\n\nen_fr_train is a dictionary where the key is the English word and the value is the French translation of that English word.\n\n{'the': 'la',\n 'and': 'et',\n 'was': 'était',\n 'for': 'pour',\n\nen_fr_test is similar to en_fr_train, but is a test set. We won’t look at it until we get to testing."
  },
  {
    "objectID": "posts/c1w4/assignment.html#generate-embedding-and-transform-matrices",
    "href": "posts/c1w4/assignment.html#generate-embedding-and-transform-matrices",
    "title": "Assignment 4 - Naive Machine Translation and LSH",
    "section": "1.1 Generate embedding and transform matrices",
    "text": "1.1 Generate embedding and transform matrices\n #### Exercise 01: Translating English dictionary to French by using embeddings\nYou will now implement a function get_matrices, which takes the loaded data and returns matrices X and Y.\nInputs: - en_fr : English to French dictionary - en_embeddings : English to embeddings dictionary - fr_embeddings : French to embeddings dictionary\nReturns: - Matrix X and matrix Y, where each row in X is the word embedding for an english word, and the same row in Y is the word embedding for the French version of that English word.\n\n Figure 2\n\nUse the en_fr dictionary to ensure that the ith row in the X matrix corresponds to the ith row in the Y matrix.\nInstructions: Complete the function get_matrices(): * Iterate over English words in en_fr dictionary. * Check if the word have both English and French embedding.\n\n\nHints\n\n&lt;p&gt;\n    &lt;ul&gt;\n        &lt;li&gt;&lt;a href=\"https://realpython.com/python-sets/#set-size-and-membership\" &gt;Sets&lt;/a&gt; are useful data structures that can be used to check if an item is a member of a group.&lt;/li&gt;\n        &lt;li&gt;You can get words which are embedded into the language by using &lt;a href=\"https://www.w3schools.com/python/ref_dictionary_keys.asp\"&gt; keys&lt;/a&gt; method.&lt;/li&gt;\n        &lt;li&gt;Keep vectors in `X` and `Y` sorted in list. You can use &lt;a href=\"https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.ma.vstack.html\"&gt; np.vstack()&lt;/a&gt; to merge them into the numpy matrix. &lt;/li&gt;\n        &lt;li&gt;&lt;a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.vstack.html\"&gt;numpy.vstack&lt;/a&gt; stacks the items in a list as rows in a matrix.&lt;/li&gt;\n    &lt;/ul&gt;\n&lt;/p&gt;\n\n# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef get_matrices(en_fr, french_vecs, english_vecs):\n    \"\"\"\n    Input:\n        en_fr: English to French dictionary\n        french_vecs: French words to their corresponding word embeddings.\n        english_vecs: English words to their corresponding word embeddings.\n    Output: \n        X: a matrix where the columns are the English embeddings.\n        Y: a matrix where the columns correspong to the French embeddings.\n        R: the projection matrix that minimizes the F norm ||X R -Y||^2.\n    \"\"\"\n\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n\n    # X_l and Y_l are lists of the english and french word embeddings\n    X_l = list()\n    Y_l = list()\n\n    # get the english words (the keys in the dictionary) and store in a set()\n    english_set = None\n\n    # get the french words (keys in the dictionary) and store in a set()\n    french_set = None\n\n    # store the french words that are part of the english-french dictionary (these are the values of the dictionary)\n    french_words = set(en_fr.values())\n\n    # loop through all english, french word pairs in the english french dictionary\n    for en_word, fr_word in en_fr.items():\n\n        # check that the french word has an embedding and that the english word has an embedding\n        if fr_word in french_set and en_word in english_set:\n\n            # get the english embedding\n            en_vec = english_vecs[en_word]\n\n            # get the french embedding\n            fr_vec = None\n\n            # add the english embedding to the list\n            X_l.append(en_vec)\n\n            # add the french embedding to the list\n            None\n\n    # stack the vectors of X_l into a matrix X\n    X = None\n\n    # stack the vectors of Y_l into a matrix Y\n    Y = None\n    ### END CODE HERE ###\n\n    return X, Y\n\nNow we will use function get_matrices() to obtain sets X_train and Y_train of English and French word embeddings into the corresponding vector space models.\n\n# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n\n# getting the training set:\nX_train, Y_train = get_matrices(\n    en_fr_train, fr_embeddings_subset, en_embeddings_subset)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[6], line 6\n      1 # UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n      2 # You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n      3 \n      4 # getting the training set:\n      5 X_train, Y_train = get_matrices(\n----&gt; 6     en_fr_train, fr_embeddings_subset, en_embeddings_subset)\n\nNameError: name 'en_fr_train' is not defined"
  },
  {
    "objectID": "posts/c1w4/assignment.html#calculate-transformation-matrix-r",
    "href": "posts/c1w4/assignment.html#calculate-transformation-matrix-r",
    "title": "Assignment 4 - Naive Machine Translation and LSH",
    "section": "Calculate transformation matrix R",
    "text": "Calculate transformation matrix R\nUsing those the training set, find the transformation matrix \\mathbf{R} by calling the function align_embeddings().\nNOTE: The code cell below will take a few minutes to fully execute (~3 mins)\n\n# UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\nR_train = align_embeddings(X_train, Y_train, train_steps=400, learning_rate=0.8)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[11], line 3\n      1 # UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n      2 # You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n----&gt; 3 R_train = align_embeddings(X_train, Y_train, train_steps=400, learning_rate=0.8)\n\nNameError: name 'X_train' is not defined\n\n\n\n\nExpected Output\nloss at iteration 0 is: 963.0146\nloss at iteration 25 is: 97.8292\nloss at iteration 50 is: 26.8329\nloss at iteration 75 is: 9.7893\nloss at iteration 100 is: 4.3776\nloss at iteration 125 is: 2.3281\nloss at iteration 150 is: 1.4480\nloss at iteration 175 is: 1.0338\nloss at iteration 200 is: 0.8251\nloss at iteration 225 is: 0.7145\nloss at iteration 250 is: 0.6534\nloss at iteration 275 is: 0.6185\nloss at iteration 300 is: 0.5981\nloss at iteration 325 is: 0.5858\nloss at iteration 350 is: 0.5782\nloss at iteration 375 is: 0.5735"
  },
  {
    "objectID": "posts/c1w4/assignment.html#testing-the-translation",
    "href": "posts/c1w4/assignment.html#testing-the-translation",
    "title": "Assignment 4 - Naive Machine Translation and LSH",
    "section": "2.2 Testing the translation",
    "text": "2.2 Testing the translation\n\nk-Nearest neighbors algorithm\nk-Nearest neighbors algorithm * k-NN is a method which takes a vector as input and finds the other vectors in the dataset that are closest to it. * The ‘k’ is the number of “nearest neighbors” to find (e.g. k=2 finds the closest two neighbors).\n\n\nSearching for the translation embedding\nSince we’re approximating the translation function from English to French embeddings by a linear transformation matrix \\mathbf{R}, most of the time we won’t get the exact embedding of a French word when we transform embedding \\mathbf{e} of some particular English word into the French embedding space. * This is where k-NN becomes really useful! By using 1-NN with \\mathbf{eR} as input, we can search for an embedding \\mathbf{f} (as a row) in the matrix \\mathbf{Y} which is the closest to the transformed vector \\mathbf{eR}\n\n\nCosine similarity\nCosine similarity between vectors u and v calculated as the cosine of the angle between them. The formula is\n\\cos(u,v)=\\frac{u\\cdot v}{\\left\\|u\\right\\|\\left\\|v\\right\\|}\n\n\\cos(u,v) = 1 when u and v lie on the same line and have the same direction.\n\\cos(u,v) is -1 when they have exactly opposite directions.\n\\cos(u,v) is 0 when the vectors are orthogonal (perpendicular) to each other.\n\n\nNote: Distance and similarity are pretty much opposite things.\n\nWe can obtain distance metric from cosine similarity, but the cosine similarity can’t be used directly as the distance metric.\nWhen the cosine similarity increases (towards 1), the “distance” between the two vectors decreases (towards 0).\nWe can define the cosine distance between u and v as d_{\\text{cos}}(u,v)=1-\\cos(u,v)\n\n\nExercise 05: Complete the function nearest_neighbor()\nInputs: * Vector v, * A set of possible nearest neighbors candidates * k nearest neighbors to find. * The distance metric should be based on cosine similarity. * cosine_similarity function is already implemented and imported for you. It’s arguments are two vectors and it returns the cosine of the angle between them. * Iterate over rows in candidates, and save the result of similarities between current row and vector v in a python list. Take care that similarities are in the same order as row vectors of candidates. * Now you can use numpy argsort to sort the indices for the rows of candidates.\n\n\nHints\n\n\n\n\nnumpy.argsort sorts values from most negative to most positive (smallest to largest)\n\n\nThe candidates that are nearest to ‘v’ should have the highest cosine similarity\n\n\nTo get the last element of a list ‘tmp’, the notation is tmp[-1:]\n\n\n\n\n# UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef nearest_neighbor(v, candidates, k=1):\n    \"\"\"\n    Input:\n      - v, the vector you are going find the nearest neighbor for\n      - candidates: a set of vectors where we will find the neighbors\n      - k: top k nearest neighbors to find\n    Output:\n      - k_idx: the indices of the top k closest vectors in sorted form\n    \"\"\"\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    similarity_l = []\n\n    # for each candidate vector...\n    for row in candidates:\n        # get the cosine similarity\n        cos_similarity = None\n\n        # append the similarity to the list\n        None\n        \n    # sort the similarity list and get the indices of the sorted list\n    sorted_ids = None\n\n    # get the indices of the k most similar candidate vectors\n    k_idx = None\n    ### END CODE HERE ###\n    return k_idx\n\n\n# UNQ_C9 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n\n# Test your implementation:\nv = np.array([1, 0, 1])\ncandidates = np.array([[1, 0, 5], [-2, 5, 3], [2, 0, 1], [6, -9, 5], [9, 9, 9]])\nprint(candidates[nearest_neighbor(v, candidates, 3)])\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[13], line 5\n      1 # UNQ_C9 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n      2 # You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n      3 \n      4 # Test your implementation:\n----&gt; 5 v = np.array([1, 0, 1])\n      6 candidates = np.array([[1, 0, 5], [-2, 5, 3], [2, 0, 1], [6, -9, 5], [9, 9, 9]])\n      7 print(candidates[nearest_neighbor(v, candidates, 3)])\n\nNameError: name 'np' is not defined\n\n\n\nExpected Output:\n[[9 9 9]  [1 0 5]  [2 0 1]]\n\n\n\nTest your translation and compute its accuracy\n Exercise 06: Complete the function test_vocabulary which takes in English embedding matrix X, French embedding matrix Y and the R matrix and returns the accuracy of translations from X to Y by R.\n\nIterate over transformed English word embeddings and check if the closest French word vector belongs to French word that is the actual translation.\nObtain an index of the closest French embedding by using nearest_neighbor (with argument k=1), and compare it to the index of the English embedding you have just transformed.\nKeep track of the number of times you get the correct translation.\nCalculate accuracy as \\text{accuracy}=\\frac{\\#(\\text{correct predictions})}{\\#(\\text{total predictions})}\n\n\n# UNQ_C10 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef test_vocabulary(X, Y, R):\n    '''\n    Input:\n        X: a matrix where the columns are the English embeddings.\n        Y: a matrix where the columns correspong to the French embeddings.\n        R: the transform matrix which translates word embeddings from\n        English to French word vector space.\n    Output:\n        accuracy: for the English to French capitals\n    '''\n\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    # The prediction is X times R\n    pred = None\n\n    # initialize the number correct to zero\n    num_correct = 0\n\n    # loop through each row in pred (each transformed embedding)\n    for i in range(len(pred)):\n        # get the index of the nearest neighbor of pred at row 'i'; also pass in the candidates in Y\n        pred_idx = None\n\n        # if the index of the nearest neighbor equals the row of i... \\\n        if pred_idx == i:\n            # increment the number correct by 1.\n            num_correct += None\n\n    # accuracy is the number correct divided by the number of rows in 'pred' (also number of rows in X)\n    accuracy = None\n\n    ### END CODE HERE ###\n\n    return accuracy\n\nLet’s see how is your translation mechanism working on the unseen data:\n\nX_val, Y_val = get_matrices(en_fr_test, fr_embeddings_subset, en_embeddings_subset)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[15], line 1\n----&gt; 1 X_val, Y_val = get_matrices(en_fr_test, fr_embeddings_subset, en_embeddings_subset)\n\nNameError: name 'en_fr_test' is not defined\n\n\n\n\n# UNQ_C11 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n\nacc = test_vocabulary(X_val, Y_val, R_train)  # this might take a minute or two\nprint(f\"accuracy on test set is {acc:.3f}\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[16], line 4\n      1 # UNQ_C11 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n      2 # You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n----&gt; 4 acc = test_vocabulary(X_val, Y_val, R_train)  # this might take a minute or two\n      5 print(f\"accuracy on test set is {acc:.3f}\")\n\nNameError: name 'X_val' is not defined\n\n\n\nExpected Output:\n0.557\nYou managed to translate words from one language to another language without ever seing them with almost 56% accuracy by using some basic linear algebra and learning a mapping of words from one language to another!"
  },
  {
    "objectID": "posts/c1w4/assignment.html#looking-up-the-tweets",
    "href": "posts/c1w4/assignment.html#looking-up-the-tweets",
    "title": "Assignment 4 - Naive Machine Translation and LSH",
    "section": "3.2 Looking up the tweets",
    "text": "3.2 Looking up the tweets\nNow you have a vector of dimension (m,d) where m is the number of tweets (10,000) and d is the dimension of the embeddings (300). Now you will input a tweet, and use cosine similarity to see which tweet in our corpus is similar to your tweet.\n\nmy_tweet = 'i am sad'\nprocess_tweet(my_tweet)\ntweet_embedding = get_document_embedding(my_tweet, en_embeddings_subset)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[23], line 2\n      1 my_tweet = 'i am sad'\n----&gt; 2 process_tweet(my_tweet)\n      3 tweet_embedding = get_document_embedding(my_tweet, en_embeddings_subset)\n\nNameError: name 'process_tweet' is not defined\n\n\n\n\n# UNQ_C16 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n\n# this gives you a similar tweet as your input.\n# this implementation is vectorized...\nidx = np.argmax(cosine_similarity(document_vecs, tweet_embedding))\nprint(all_tweets[idx])\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[24], line 6\n      1 # UNQ_C16 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n      2 # You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n      3 \n      4 # this gives you a similar tweet as your input.\n      5 # this implementation is vectorized...\n----&gt; 6 idx = np.argmax(cosine_similarity(document_vecs, tweet_embedding))\n      7 print(all_tweets[idx])\n\nNameError: name 'np' is not defined\n\n\n\n\nExpected Output\n@zoeeylim sad sad sad kid :( it's ok I help you watch the match HAHAHAHAHA"
  },
  {
    "objectID": "posts/c1w4/assignment.html#finding-the-most-similar-tweets-with-lsh",
    "href": "posts/c1w4/assignment.html#finding-the-most-similar-tweets-with-lsh",
    "title": "Assignment 4 - Naive Machine Translation and LSH",
    "section": "3.3 Finding the most similar tweets with LSH",
    "text": "3.3 Finding the most similar tweets with LSH\nYou will now implement locality sensitive hashing (LSH) to identify the most similar tweet. * Instead of looking at all 10,000 vectors, you can just search a subset to find its nearest neighbors.\nLet’s say your data points are plotted like this:\n\n Figure 3\n\nYou can divide the vector space into regions and search within one region for nearest neighbors of a given vector.\n\n Figure 4\n\n\nN_VECS = len(all_tweets)       # This many vectors.\nN_DIMS = len(ind2Tweet[1])     # Vector dimensionality.\nprint(f\"Number of vectors is {N_VECS} and each has {N_DIMS} dimensions.\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[25], line 1\n----&gt; 1 N_VECS = len(all_tweets)       # This many vectors.\n      2 N_DIMS = len(ind2Tweet[1])     # Vector dimensionality.\n      3 print(f\"Number of vectors is {N_VECS} and each has {N_DIMS} dimensions.\")\n\nNameError: name 'all_tweets' is not defined\n\n\n\n\nChoosing the number of planes\n\nEach plane divides the space to 2 parts.\nSo n planes divide the space into 2^{n} hash buckets.\nWe want to organize 10,000 document vectors into buckets so that every bucket has about ~16 vectors.\nFor that we need \\frac{10000}{16}=625 buckets.\nWe’re interested in n, number of planes, so that 2^{n}= 625. Now, we can calculate n=\\log_{2}625 = 9.29 \\approx 10.\n\n\n# The number of planes. We use log2(625) to have ~16 vectors/bucket.\nN_PLANES = 10\n# Number of times to repeat the hashing to improve the search.\nN_UNIVERSES = 25"
  },
  {
    "objectID": "posts/c1w4/assignment.html#getting-the-hash-number-for-a-vector",
    "href": "posts/c1w4/assignment.html#getting-the-hash-number-for-a-vector",
    "title": "Assignment 4 - Naive Machine Translation and LSH",
    "section": "3.4 Getting the hash number for a vector",
    "text": "3.4 Getting the hash number for a vector\nFor each vector, we need to get a unique number associated to that vector in order to assign it to a “hash bucket”.\n\nHyperlanes in vector spaces\n\nIn 3-dimensional vector space, the hyperplane is a regular plane. In 2 dimensional vector space, the hyperplane is a line.\nGenerally, the hyperplane is subspace which has dimension 1 lower than the original vector space has.\nA hyperplane is uniquely defined by its normal vector.\nNormal vector n of the plane \\pi is the vector to which all vectors in the plane \\pi are orthogonal (perpendicular in 3 dimensional case).\n\n\n\nUsing Hyperplanes to split the vector space\nWe can use a hyperplane to split the vector space into 2 parts. * All vectors whose dot product with a plane’s normal vector is positive are on one side of the plane. * All vectors whose dot product with the plane’s normal vector is negative are on the other side of the plane.\n\n\nEncoding hash buckets\n\nFor a vector, we can take its dot product with all the planes, then encode this information to assign the vector to a single hash bucket.\nWhen the vector is pointing to the opposite side of the hyperplane than normal, encode it by 0.\nOtherwise, if the vector is on the same side as the normal vector, encode it by 1.\nIf you calculate the dot product with each plane in the same order for every vector, you’ve encoded each vector’s unique hash ID as a binary number, like [0, 1, 1, … 0].\n\n\n\n\nExercise 09: Implementing hash buckets\nWe’ve initialized hash table hashes for you. It is list of N_UNIVERSES matrices, each describes its own hash table. Each matrix has N_DIMS rows and N_PLANES columns. Every column of that matrix is a N_DIMS-dimensional normal vector for each of N_PLANES hyperplanes which are used for creating buckets of the particular hash table.\nExercise: Your task is to complete the function hash_value_of_vector which places vector v in the correct hash bucket.\n\nFirst multiply your vector v, with a corresponding plane. This will give you a vector of dimension (1,\\text{N_planes}).\nYou will then convert every element in that vector to 0 or 1.\nYou create a hash vector by doing the following: if the element is negative, it becomes a 0, otherwise you change it to a 1.\nYou then compute the unique number for the vector by iterating over N_PLANES\nThen you multiply 2^i times the corresponding bit (0 or 1).\nYou will then store that sum in the variable hash_value.\n\nIntructions: Create a hash for the vector in the function below. Use this formula:\n hash = \\sum_{i=0}^{N-1} \\left( 2^{i} \\times h_{i} \\right) \n\nCreate the sets of planes\n\nCreate multiple (25) sets of planes (the planes that divide up the region).\nYou can think of these as 25 separate ways of dividing up the vector space with a different set of planes.\nEach element of this list contains a matrix with 300 rows (the word vector have 300 dimensions), and 10 columns (there are 10 planes in each “universe”).\n\n\nnp.random.seed(0)\nplanes_l = [np.random.normal(size=(N_DIMS, N_PLANES))\n            for _ in range(N_UNIVERSES)]\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[27], line 1\n----&gt; 1 np.random.seed(0)\n      2 planes_l = [np.random.normal(size=(N_DIMS, N_PLANES))\n      3             for _ in range(N_UNIVERSES)]\n\nNameError: name 'np' is not defined\n\n\n\n\n\nHints\n\n\n\n\nnumpy.squeeze() removes unused dimensions from an array; for instance, it converts a (10,1) 2D array into a (10,) 1D array\n\n\n\n\n# UNQ_C17 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef hash_value_of_vector(v, planes):\n    \"\"\"Create a hash for a vector; hash_id says which random hash to use.\n    Input:\n        - v:  vector of tweet. It's dimension is (1, N_DIMS)\n        - planes: matrix of dimension (N_DIMS, N_PLANES) - the set of planes that divide up the region\n    Output:\n        - res: a number which is used as a hash for your vector\n\n    \"\"\"\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    # for the set of planes,\n    # calculate the dot product between the vector and the matrix containing the planes\n    # remember that planes has shape (300, 10)\n    # The dot product will have the shape (1,10)\n    dot_product = None\n\n    # get the sign of the dot product (1,10) shaped vector\n    sign_of_dot_product = None\n\n    # set h to be false (eqivalent to 0 when used in operations) if the sign is negative,\n    # and true (equivalent to 1) if the sign is positive (1,10) shaped vector\n    h = None\n\n    # remove extra un-used dimensions (convert this from a 2D to a 1D array)\n    h = None\n\n    # initialize the hash value to 0\n    hash_value = 0\n\n    n_planes = planes.shape[1]\n    for i in range(n_planes):\n        # increment the hash value by 2^i * h_i\n        hash_value += None\n    ### END CODE HERE ###\n\n    # cast hash_value as an integer\n    hash_value = int(hash_value)\n\n    return hash_value\n\n\n# UNQ_C18 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n\nnp.random.seed(0)\nidx = 0\nplanes = planes_l[idx]  # get one 'universe' of planes to test the function\nvec = np.random.rand(1, 300)\nprint(f\" The hash value for this vector,\",\n      f\"and the set of planes at index {idx},\",\n      f\"is {hash_value_of_vector(vec, planes)}\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[29], line 4\n      1 # UNQ_C18 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n      2 # You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n----&gt; 4 np.random.seed(0)\n      5 idx = 0\n      6 planes = planes_l[idx]  # get one 'universe' of planes to test the function\n\nNameError: name 'np' is not defined\n\n\n\n\nExpected Output\nThe hash value for this vector, and the set of planes at index 0, is 768"
  },
  {
    "objectID": "posts/c1w4/assignment.html#creating-a-hash-table",
    "href": "posts/c1w4/assignment.html#creating-a-hash-table",
    "title": "Assignment 4 - Naive Machine Translation and LSH",
    "section": "3.5 Creating a hash table",
    "text": "3.5 Creating a hash table\n\n\nExercise 10\nGiven that you have a unique number for each vector (or tweet), You now want to create a hash table. You need a hash table, so that given a hash_id, you can quickly look up the corresponding vectors. This allows you to reduce your search by a significant amount of time.\n\n\n\nWe have given you the make_hash_table function, which maps the tweet vectors to a bucket and stores the vector there. It returns the hash_table and the id_table. The id_table allows you know which vector in a certain bucket corresponds to what tweet.\n\n\nHints\n\n\n\n\na dictionary comprehension, similar to a list comprehension, looks like this: {i:0 for i in range(10)}, where the key is ‘i’ and the value is zero for all key-value pairs.\n\n\n\n\n# UNQ_C19 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# This is the code used to create a hash table: feel free to read over it\ndef make_hash_table(vecs, planes):\n    \"\"\"\n    Input:\n        - vecs: list of vectors to be hashed.\n        - planes: the matrix of planes in a single \"universe\", with shape (embedding dimensions, number of planes).\n    Output:\n        - hash_table: dictionary - keys are hashes, values are lists of vectors (hash buckets)\n        - id_table: dictionary - keys are hashes, values are list of vectors id's\n                            (it's used to know which tweet corresponds to the hashed vector)\n    \"\"\"\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n\n    # number of planes is the number of columns in the planes matrix\n    num_of_planes = None\n\n    # number of buckets is 2^(number of planes)\n    num_buckets = None\n\n    # create the hash table as a dictionary.\n    # Keys are integers (0,1,2.. number of buckets)\n    # Values are empty lists\n    hash_table = None\n\n    # create the id table as a dictionary.\n    # Keys are integers (0,1,2... number of buckets)\n    # Values are empty lists\n    id_table = None\n\n    # for each vector in 'vecs'\n    for i, v in enumerate(vecs):\n        # calculate the hash value for the vector\n        h = None\n\n        # store the vector into hash_table at key h,\n        # by appending the vector v to the list at key h\n        None\n\n        # store the vector's index 'i' (each document is given a unique integer 0,1,2...)\n        # the key is the h, and the 'i' is appended to the list at key h\n        None\n\n    ### END CODE HERE ###\n\n    return hash_table, id_table\n\n\n# UNQ_C20 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n\nnp.random.seed(0)\nplanes = planes_l[0]  # get one 'universe' of planes to test the function\nvec = np.random.rand(1, 300)\ntmp_hash_table, tmp_id_table = make_hash_table(document_vecs, planes)\n\nprint(f\"The hash table at key 0 has {len(tmp_hash_table[0])} document vectors\")\nprint(f\"The id table at key 0 has {len(tmp_id_table[0])}\")\nprint(f\"The first 5 document indices stored at key 0 of are {tmp_id_table[0][0:5]}\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[31], line 4\n      1 # UNQ_C20 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n      2 # You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n----&gt; 4 np.random.seed(0)\n      5 planes = planes_l[0]  # get one 'universe' of planes to test the function\n      6 vec = np.random.rand(1, 300)\n\nNameError: name 'np' is not defined\n\n\n\n\nExpected output\nThe hash table at key 0 has 3 document vectors\nThe id table at key 0 has 3\nThe first 5 document indices stored at key 0 of are [3276, 3281, 3282]\n\n\n\n\n3.6 Creating all hash tables\nYou can now hash your vectors and store them in a hash table that would allow you to quickly look up and search for similar vectors. Run the cell below to create the hashes. By doing so, you end up having several tables which have all the vectors. Given a vector, you then identify the buckets in all the tables. You can then iterate over the buckets and consider much fewer vectors. The more buckets you use, the more accurate your lookup will be, but also the longer it will take.\n\n# Creating the hashtables\nhash_tables = []\nid_tables = []\nfor universe_id in range(N_UNIVERSES):  # there are 25 hashes\n    print('working on hash universe #:', universe_id)\n    planes = planes_l[universe_id]\n    hash_table, id_table = make_hash_table(document_vecs, planes)\n    hash_tables.append(hash_table)\n    id_tables.append(id_table)\n\nworking on hash universe #: 0\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[32], line 6\n      4 for universe_id in range(N_UNIVERSES):  # there are 25 hashes\n      5     print('working on hash universe #:', universe_id)\n----&gt; 6     planes = planes_l[universe_id]\n      7     hash_table, id_table = make_hash_table(document_vecs, planes)\n      8     hash_tables.append(hash_table)\n\nNameError: name 'planes_l' is not defined\n\n\n\n\n\nApproximate K-NN\n\n\n\nExercise 11\nImplement approximate K nearest neighbors using locality sensitive hashing, to search for documents that are similar to a given document at the index doc_id.\n\nInputs\n\ndoc_id is the index into the document list all_tweets.\nv is the document vector for the tweet in all_tweets at index doc_id.\nplanes_l is the list of planes (the global variable created earlier).\nk is the number of nearest neighbors to search for.\nnum_universes_to_use: to save time, we can use fewer than the total number of available universes. By default, it’s set to N_UNIVERSES, which is 25 for this assignment.\n\nThe approximate_knn function finds a subset of candidate vectors that are in the same “hash bucket” as the input vector ‘v’. Then it performs the usual k-nearest neighbors search on this subset (instead of searching through all 10,000 tweets).\n\n\nHints\n\n\n\n\nThere are many dictionaries used in this function. Try to print out planes_l, hash_tables, id_tables to understand how they are structured, what the keys represent, and what the values contain.\n\n\nTo remove an item from a list, use .remove()\n\n\nTo append to a list, use .append()\n\n\nTo add to a set, use .add()\n\n\n\n\n# UNQ_C21 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# This is the code used to do the fast nearest neighbor search. Feel free to go over it\ndef approximate_knn(doc_id, v, planes_l, k=1, num_universes_to_use=N_UNIVERSES):\n    \"\"\"Search for k-NN using hashes.\"\"\"\n    assert num_universes_to_use &lt;= N_UNIVERSES\n\n    # Vectors that will be checked as possible nearest neighbor\n    vecs_to_consider_l = list()\n\n    # list of document IDs\n    ids_to_consider_l = list()\n\n    # create a set for ids to consider, for faster checking if a document ID already exists in the set\n    ids_to_consider_set = set()\n\n    # loop through the universes of planes\n    for universe_id in range(num_universes_to_use):\n\n        # get the set of planes from the planes_l list, for this particular universe_id\n        planes = planes_l[universe_id]\n\n        # get the hash value of the vector for this set of planes\n        hash_value = hash_value_of_vector(v, planes)\n\n        # get the hash table for this particular universe_id\n        hash_table = hash_tables[universe_id]\n\n        # get the list of document vectors for this hash table, where the key is the hash_value\n        document_vectors_l = hash_table[hash_value]\n\n        # get the id_table for this particular universe_id\n        id_table = id_tables[universe_id]\n\n        # get the subset of documents to consider as nearest neighbors from this id_table dictionary\n        new_ids_to_consider = id_table[hash_value]\n\n        ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n\n        # remove the id of the document that we're searching\n        if doc_id in new_ids_to_consider:\n            None\n            print(f\"removed doc_id {doc_id} of input vector from new_ids_to_search\")\n\n        # loop through the subset of document vectors to consider\n        for i, new_id in enumerate(new_ids_to_consider):\n\n            # if the document ID is not yet in the set ids_to_consider...\n            if new_id not in ids_to_consider_set:\n                # access document_vectors_l list at index i to get the embedding\n                # then append it to the list of vectors to consider as possible nearest neighbors\n                document_vector_at_i = None\n                None\n\n                # append the new_id (the index for the document) to the list of ids to consider\n                None\n\n                # also add the new_id to the set of ids to consider\n                # (use this to check if new_id is not already in the IDs to consider)\n                None\n\n        ### END CODE HERE ###\n\n    # Now run k-NN on the smaller set of vecs-to-consider.\n    print(\"Fast considering %d vecs\" % len(vecs_to_consider_l))\n\n    # convert the vecs to consider set to a list, then to a numpy array\n    vecs_to_consider_arr = np.array(vecs_to_consider_l)\n\n    # call nearest neighbors on the reduced list of candidate vectors\n    nearest_neighbor_idx_l = nearest_neighbor(v, vecs_to_consider_arr, k=k)\n\n    # Use the nearest neighbor index list as indices into the ids to consider\n    # create a list of nearest neighbors by the document ids\n    nearest_neighbor_ids = [ids_to_consider_l[idx]\n                            for idx in nearest_neighbor_idx_l]\n\n    return nearest_neighbor_ids\n\n\n#document_vecs, ind2Tweet\ndoc_id = 0\ndoc_to_search = all_tweets[doc_id]\nvec_to_search = document_vecs[doc_id]\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[34], line 3\n      1 #document_vecs, ind2Tweet\n      2 doc_id = 0\n----&gt; 3 doc_to_search = all_tweets[doc_id]\n      4 vec_to_search = document_vecs[doc_id]\n\nNameError: name 'all_tweets' is not defined\n\n\n\n\n# UNQ_C22 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n\n# Sample\nnearest_neighbor_ids = approximate_knn(\n    doc_id, vec_to_search, planes_l, k=3, num_universes_to_use=5)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[35], line 6\n      1 # UNQ_C22 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n      2 # You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n      3 \n      4 # Sample\n      5 nearest_neighbor_ids = approximate_knn(\n----&gt; 6     doc_id, vec_to_search, planes_l, k=3, num_universes_to_use=5)\n\nNameError: name 'vec_to_search' is not defined\n\n\n\n\nprint(f\"Nearest neighbors for document {doc_id}\")\nprint(f\"Document contents: {doc_to_search}\")\nprint(\"\")\n\nfor neighbor_id in nearest_neighbor_ids:\n    print(f\"Nearest neighbor at document id {neighbor_id}\")\n    print(f\"document contents: {all_tweets[neighbor_id]}\")\n\nNearest neighbors for document 0\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[36], line 2\n      1 print(f\"Nearest neighbors for document {doc_id}\")\n----&gt; 2 print(f\"Document contents: {doc_to_search}\")\n      3 print(\"\")\n      5 for neighbor_id in nearest_neighbor_ids:\n\nNameError: name 'doc_to_search' is not defined"
  },
  {
    "objectID": "posts/c2w1/lab01.html",
    "href": "posts/c2w1/lab01.html",
    "title": "NLP Course 2 Week 1 Lesson : Building The Model - Lecture Exercise 01",
    "section": "",
    "text": "Estimated Time: 10 minutes\n\nVocabulary Creation\nCreate a tiny vocabulary from a tiny corpus\nIt’s time to start small !\n\nImports and Data\n\n# imports\nimport re # regular expression library; for tokenization of words\nfrom collections import Counter # collections library; counter: dict subclass for counting hashable objects\nimport matplotlib.pyplot as plt # for data visualization\n\n\n# the tiny corpus of text ! \ntext = 'red pink pink blue blue yellow ORANGE BLUE BLUE PINK' # 🌈\nprint(text)\nprint('string length : ',len(text))\n\n\n\nPreprocessing\n\n# convert all letters to lower case\ntext_lowercase = text.lower()\nprint(text_lowercase)\nprint('string length : ',len(text_lowercase))\n\n\n# some regex to tokenize the string to words and return them in a list\nwords = re.findall(r'\\w+', text_lowercase)\nprint(words)\nprint('count : ',len(words))\n\n\n\nCreate Vocabulary\nOption 1 : A set of distinct words from the text\n\n# create vocab\nvocab = set(words)\nprint(vocab)\nprint('count : ',len(vocab))\n\n\n\nAdd Information with Word Counts\nOption 2 : Two alternatives for including the word count as well\n\n# create vocab including word count\ncounts_a = dict()\nfor w in words:\n    counts_a[w] = counts_a.get(w,0)+1\nprint(counts_a)\nprint('count : ',len(counts_a))\n\n\n# create vocab including word count using collections.Counter\ncounts_b = dict()\ncounts_b = Counter(words)\nprint(counts_b)\nprint('count : ',len(counts_b))\n\n\n# barchart of sorted word counts\nd = {'blue': counts_b['blue'], 'pink': counts_b['pink'], 'red': counts_b['red'], 'yellow': counts_b['yellow'], 'orange': counts_b['orange']}\nplt.bar(range(len(d)), list(d.values()), align='center', color=d.keys())\n_ = plt.xticks(range(len(d)), list(d.keys()))\n\n\n\nUngraded Exercise\nNote that counts_b, above, returned by collections.Counter is sorted by word count\nCan you modify the tiny corpus of text so that a new color appears between pink and red in counts_b ?\nDo you need to run all the cells again, or just specific ones ?\n\nprint('counts_b : ', counts_b)\nprint('count : ', len(counts_b))\n\nExpected Outcome:\ncounts_b : Counter({‘blue’: 4, ‘pink’: 3, ‘your_new_color_here’: 2, red’: 1, ‘yellow’: 1, ‘orange’: 1})  count : 6\n\n\nSummary\nThis is a tiny example but the methodology scales very well.  In the assignment you will create a large vocabulary of thousands of words, from a corpus  of tens of thousands or words! But the mechanics are exactly the same.  The only extra things to pay attention to should be; run time, memory management and the vocab data structure.  So the choice of approach used in code blocks counts_a vs counts_b, above, will be important.\n\n\n\n\n\nCitationBibTeX citation:@online{2025,\n  author = {},\n  title = {NLP {Course} 2 {Week} 1 {Lesson} : {Building} {The} {Model} -\n    {Lecture} {Exercise} 01},\n  date = {2025-01-30},\n  url = {https://orenbochman.github.io/notes-nlp/posts/c2w1/lab01.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“NLP Course 2 Week 1 Lesson : Building The Model - Lecture\nExercise 01.” 2025. January 30, 2025. https://orenbochman.github.io/notes-nlp/posts/c2w1/lab01.html."
  },
  {
    "objectID": "posts/c2w1/index.html",
    "href": "posts/c2w1/index.html",
    "title": "Autocorrect and minimum edit distance",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nFigure 2",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect and Dynamic Programming"
    ]
  },
  {
    "objectID": "posts/c2w1/index.html#overview",
    "href": "posts/c2w1/index.html#overview",
    "title": "Autocorrect and minimum edit distance",
    "section": "Overview",
    "text": "Overview\nYou use auto-correct everyday. When you send your friend a text message, or when you make a mistake in a query, there is an autocorrect behind the scenes that corrects the sentence for you. This week you are also going to learn about minimum edit distance, which tells you the minimum amount of edits to change one word into another. In doing that, you will learn about dynamic programming which is an important programming concept which frequently comes up in interviews and could be used to solve a lot of optimization problems.\n\n\n\n\n\n\n\nautocorrect\n\n\n\n\nFigure 3: Learning Objectives",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect and Dynamic Programming"
    ]
  },
  {
    "objectID": "posts/c2w1/index.html#autocorrect",
    "href": "posts/c2w1/index.html#autocorrect",
    "title": "Autocorrect and minimum edit distance",
    "section": "Autocorrect",
    "text": "Autocorrect\nAutocorrects are used everywhere. You use them in your phones, tablets, and computers.\n\n\n\n\n\n\n\nfind candidates\n\n\n\n\nFigure 4: What is autocorrect\n\n\nTo implement autocorrect in this week’s assignment, you have to follow these steps:\n\nIdentify a misspelled word\nFind strings n edit distance away: (these could be random strings)\nFilter candidates: (keep only the real words from the previous steps)\nCalculate word probabilities: (choose the word that is most likely to occur in that context)\n\n\n\n\n\n\n\n\nfind candidates\n\n\n\n\nFigure 5: Find candidates\n\n\nBuilding the model:\n\nIdentify the misspelled word\n\n\nWhen identifying the misspelled word, you can check whether it is in the vocabulary. If you don’t find it, then it is probably a typo.\n\n\nFind strings n edit distance away\nFilter candidates\n\n\nIn this step, you want to take all the words generated above and then only keep the actual words that make sense and that you can find in your vocabulary.\n\n\n\n\n\n\n\n\nfilter\n\n\n\n\nFigure 6: Filter candidates",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect and Dynamic Programming"
    ]
  },
  {
    "objectID": "posts/c2w1/index.html#lab-building-the-vocabulary",
    "href": "posts/c2w1/index.html#lab-building-the-vocabulary",
    "title": "Autocorrect and minimum edit distance",
    "section": "Lab: Building the vocabulary",
    "text": "Lab: Building the vocabulary\nBuilding the vocabulary",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect and Dynamic Programming"
    ]
  },
  {
    "objectID": "posts/c2w1/index.html#building-the-model-ii",
    "href": "posts/c2w1/index.html#building-the-model-ii",
    "title": "Autocorrect and minimum edit distance",
    "section": "Building the model II",
    "text": "Building the model II\n\nCalculating word probabilities\n\n\n\n\n\n\n\n\nword probabilities\n\n\n\n\nFigure 7: calculating word probabilities\n\n\nNote that you are storing the count of words and then you can use that to generate the probabilities. For this week, you will be counting the probabilities of words occurring. If you want to build a slightly more sophisticated auto-correct you can keep track of two words occurring next to each other instead. You can then use the previous word to decide. For example which combo is more likely, there friend or their friend? For this week however you will be implementing the probabilities by just using the word frequencies.\nHere is a summary of everything you have seen before in the previous two videos.\n\n\n\n\n\n\n\nsummary\n\n\n\n\nFigure 8: summary of first four autocorrect steps",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect and Dynamic Programming"
    ]
  },
  {
    "objectID": "posts/c2w1/index.html#lab-building-the-vocabulary-1",
    "href": "posts/c2w1/index.html#lab-building-the-vocabulary-1",
    "title": "Autocorrect and minimum edit distance",
    "section": "Lab: Building the vocabulary",
    "text": "Lab: Building the vocabulary\nCandidates from Edits",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect and Dynamic Programming"
    ]
  },
  {
    "objectID": "posts/c2w1/index.html#minimum-edit-distance",
    "href": "posts/c2w1/index.html#minimum-edit-distance",
    "title": "Autocorrect and minimum edit distance",
    "section": "Minimum edit distance",
    "text": "Minimum edit distance\nMinimum edit distance allows you to:\n\nEvaluate similarity between two strings\nFind the minimum number of edits between two strings\nImplement spelling correction, document similarity, machine translation, DNA sequencing, and more\n\nRemember that the edits include:\n\nInsert (add a letter) ‘to’: ‘top’, ‘two’ …\nDelete (remove a letter) ‘hat’: ‘ha’, ‘at’, ‘ht’\nReplace (change 1 letter to another) ‘jaw’: ‘jar’, ‘paw’, …\n\nHere is a concrete example where we calculate the cost (i.e. edit distance) between two strings.\n\n\n\n\n\n\n\nMinimum edit distance\n\n\n\n\nFigure 9: Minimum edit distance\n\n\nNote that as your strings get larger it gets much harder to calculate the minimum edit distance. Hence you will now learn about the minimum edit distance algorithm!",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect and Dynamic Programming"
    ]
  },
  {
    "objectID": "posts/c2w1/index.html#minimum-edit-distance-algorithm",
    "href": "posts/c2w1/index.html#minimum-edit-distance-algorithm",
    "title": "Autocorrect and minimum edit distance",
    "section": "Minimum edit distance algorithm",
    "text": "Minimum edit distance algorithm\nWhen computing the minimum edit distance, you would start with a source word and transform it into the target word. Let’s look at the following example:\n\n\n\n\n\n\n\nMinimum edit distance\n\n\n\n\nFigure 10: Minimum edit distance algorithm\n\n\nTo go:\n\nfrom # → # has a cost of 0\nfrom p → # has a cost of 1, because that is the cost of a delete.\n\nfrom p → s has a cost of 2, delete p and insert s.\n\nYou can keep going this way by populating one element at a time, but it turns out there is a faster way to do this.\nWee will learn about it next.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect and Dynamic Programming"
    ]
  },
  {
    "objectID": "posts/c2w1/index.html#minimum-edit-distance-algorithm-ii",
    "href": "posts/c2w1/index.html#minimum-edit-distance-algorithm-ii",
    "title": "Autocorrect and minimum edit distance",
    "section": "Minimum edit distance algorithm II",
    "text": "Minimum edit distance algorithm II\nTo populate the following table:\n\n\n\n\n\n\n\nMinimum edit distance\n\n\n\n\nFigure 11: Minimum edit distance algorithm\n\n\nThere are three equations:\nD[i,j] = D[i-1, j] + del_cost: this indicates you want to populate the current cell (i,j) by using the cost in the cell found directly above.\nD[i,j] = D[i, j-1] + ins_cost: this indicates you want to populate the current cell (i,j) by using the cost in the cell found directly to its left.\nD[i,j] = D[i-1, j-1] + rep_cost: the rep cost can be 2 or 0 depending if you are going to actually replace it or not.\nAt every time step you check the three possible paths where you can come from and you select the least expensive one. Once you are done, you get the following:\n\n\n\n\n\n\n\nMinimum edit distance\n\n\n\n\nFigure 12: Minimum edit distance algorithm",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect and Dynamic Programming"
    ]
  },
  {
    "objectID": "posts/c2w1/index.html#minimum-edit-distance-iii",
    "href": "posts/c2w1/index.html#minimum-edit-distance-iii",
    "title": "Autocorrect and minimum edit distance",
    "section": "Minimum edit distance III",
    "text": "Minimum edit distance III\n\n\n\n\n\n\n\nMinimum edit distance\n\n\n\n\nFigure 13: Minimum edit distance algorithm\n\n\nTo summarize, you have seen the levenshtein distance which specifies the cost per operation. If you need to reconstruct the path of how you got from one string to the other, you can use a backtrace. You should keep a simple pointer in each cell letting you know where you came from to get there. So you know the path taken across the table from the top left corner, to the bottom right corner. You can then reconstruct it.\nThis method for computation instead of brute force is a technique known as dynamic programming. You first solve the smallest subproblem first and then reusing that result you solve the next biggest subproblem, saving that result, reusing it again, and so on. This is exactly what you did by populating each cell from the top right to the bottom left. It’s a well-known technique in computer science!",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect and Dynamic Programming"
    ]
  },
  {
    "objectID": "posts/c2w1/index.html#assignment-autocorrect",
    "href": "posts/c2w1/index.html#assignment-autocorrect",
    "title": "Autocorrect and minimum edit distance",
    "section": "Assignment : Autocorrect",
    "text": "Assignment : Autocorrect\n\nlab 3\n\nNote: due to the honor code, this is the original assignment notebook, and not the solution.\nI hope to later add some projects which should include similar material.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect and Dynamic Programming"
    ]
  },
  {
    "objectID": "posts/rnd/index.html",
    "href": "posts/rnd/index.html",
    "title": "NLP Specialization",
    "section": "",
    "text": "So I aced the course and am migrating more of the notes to this site. I have a tendency to add more more notes. But it is probably better to keep most of the extra material separate.\nIn the intervening time there has been massive breakthroughs in NLP. I have also learned a bit about RL. And on the advice of Richard S. Sutton I think to create a research notebook on NLP."
  },
  {
    "objectID": "posts/rnd/index.html#orthogonal-action-items",
    "href": "posts/rnd/index.html#orthogonal-action-items",
    "title": "NLP Specialization",
    "section": "Orthogonal Action Items",
    "text": "Orthogonal Action Items\n\nLearning\n\nFlash cards to keep everything fresh.\nPodcasts\nCreate a small app to drill though material from all the notes repos using spaced repetition.\n\nFeynman Technique\n\nCreate notes on what I know about NLP. These note already cover 90%+\nMy research questions\nCreate a list of all the areas I want to learn about.\nCreate a list of all the papers I want to read. -\nCreate a list of all the courses I want to take.\nCreate a list of all the notebooks I want to review.\nCreate a list of all the projects I want to do.\n\nNext I want to collect all the paper reviews.\nThere are some additional courses online that can be follow ups to this one\nthere are a number of notebooks from different sources that might be brought here for reference and further work.\nI’ve number of ideas for Wikipedia/Wikidata related projects to try.\nProjects around interlingual word embeddings and discourse atoms.\nCreate pure python libraries for Graph Based models (Viterbi, CYK, Earley, etc.) And better yet versions that can take advantage of GPUs SImd or Spark or Mojo"
  },
  {
    "objectID": "posts/rnd/index.html#sec-feynman",
    "href": "posts/rnd/index.html#sec-feynman",
    "title": "NLP Specialization",
    "section": "Feynman Technique",
    "text": "Feynman Technique\n\nNotes on stuff I know.\n\nThe stuff I know on search, tokenisers, baysian herircial model should be added\nI should have some notes on Juffansky\nI should make quick notes + podcasts on my favorite linguistics books including popular ones.\n\nJuffansky\nDavid Goldberg\nGuy Deutscher ??\n\nThe Unfolding of Language: An Evolutionary Tour of Mankind’s Greatest Invention\n\nReview\nPodcast\n\nThrough the Language Glass: Why the World Looks Different in Other Languages\n\nReview\nPodcast\n\n\nOrnan’s Morphology etc.\n\nReview\nPodcast\n\nSteven Pinker - Rubs me the wrond way but is a eloquent interloqutor\n\nThe language instinct.\netc\n\nJames W. Pennebaker\n\nSecret life of Pronouns - James W. Pennebaker"
  },
  {
    "objectID": "posts/rnd/index.html#sec-rq",
    "href": "posts/rnd/index.html#sec-rq",
    "title": "NLP Specialization",
    "section": "Research Questions",
    "text": "Research Questions\n\nThere are now nice tutorials by Andrej Karpathy on creating GPT clones.\n\nIt would be interesting to some of the material in his series: at Neural Networks: Zero to Hero\nThe tokenizers in a few notebooks c.f. video\n\nthis course is done now but I should like to get better and do follow up work and research. The best way now seems to create an online research notebook and the best way forward is to migrate the relevant material to thier own pages.\nReformer, the efficent transformer that was covered in the course is not used in modern LLM implementations, it would be interesting to\n\nunderstand its shortcomming based on progress made in later papers.\nunderstand if one can grab the weights of some LLM, load them into a reformer and enjoy the benefits of million token context windows."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NLP course notes and research notebooks",
    "section": "",
    "text": "Word Embeddings: Ungraded Practice Notebook\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nProbabilistic Models\n\n\n\n\n\n\n\n\n\nSaturday, February 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWord Embeddings: Training the CBOW model\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nProbabilistic Models\n\n\n\n\n\n\n\n\n\nSaturday, February 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWord Embeddings: Intro to CBOW model, activation functions and working with Numpy\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nProbabilistic Models\n\n\n\n\n\n\n\n\n\nSaturday, February 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWord Embeddings First Steps: Data Preparation\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nProbabilistic Models\n\n\n\n\n\n\n\n\n\nSaturday, February 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 4: Word Embeddings\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nProbabilistic Models\n\n\n\n\n\n\n\n\n\nSaturday, February 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding the language model\n\n\n\n\n\n\n\n\n\n\n\nFriday, January 31, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nOut of vocabulary words (OOV)\n\n\n\n\n\n\n\n\n\n\n\nFriday, January 31, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLinear algebra in Python with NumPy\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nLogistic regression\n\n\nSentiment analysis task\n\n\nClassification & Vector Spaces\n\n\n\n\n\n\n\n\n\nFriday, January 31, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nManipulating word embeddings\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nLogistic regression\n\n\nSentiment analysis task\n\n\nClassification & Vector Spaces\n\n\n\n\n\n\n\n\n\nFriday, January 31, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 3: Hello Vectors\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nLogistic regression\n\n\nSentiment analysis task\n\n\nClassification & Vector Spaces\n\n\n\n\n\n\n\n\n\nFriday, January 31, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing Naive Bayes\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nLogistic regression\n\n\nSentiment analysis task\n\n\nClassification & Vector Spaces\n\n\n\n\n\n\n\n\n\nFriday, January 31, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 2: Naive Bayes\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nLogistic regression\n\n\nSentiment analysis task\n\n\nClassification & Vector Spaces\n\n\n\n\n\n\n\n\n\nFriday, January 31, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 1: Logistic Regression\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nlab\n\n\nLogistic regression\n\n\nSentiment analysis task\n\n\nClassification & Vector Spaces\n\n\n\n\n\n\n\n\n\nFriday, January 31, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nNLP Course 2 Week 1 Lesson : Building The Model - Lecture Exercise 01\n\n\n\n\n\n\n\n\n\n\n\nThursday, January 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 1: Auto Correct\n\n\n\n\n\n\n\n\n\n\n\nThursday, January 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nNLP Course 2 Week 1 Lesson : Building The Model - Lecture Exercise 02\n\n\n\n\n\n\n\n\n\n\n\nThursday, January 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nOrthogonal Action Items\n\n\n\n\n\n\n\n\n\n\n\nThursday, January 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTL;DR – ML, NLP and the secret life of pronouns\n\n\n\n\n\n\n\n\n\n\n\nThursday, January 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 4 Chat Bots\n\n\nNLP with Attention Models\n\n\n\nNatural Language Processing Specialization\n\n\nNatural Language Processing with Attention Models\n\n\nNeural Machine Translation\n\n\ncoursera\n\n\nnotes\n\n\nNLP\n\n\nDeep Learning Algorithms\n\n\ntransformer\n\n\nreversible transformer\n\n\nteacher forcing\n\n\npositional encoding\n\n\nlocality sensitive hashing\n\n\nattention\n\n\ndot product attention\n\n\nLSH Attention\n\n\nGPT3\n\n\nreformer\n\n\nreversible layers\n\n\nchat bot development\n\n\nintelligent agents\n\n\nNLP\n\n\nquestion answering task\n\n\n\nThis week of the NLP Specialization, we explore the world of Chatbots. We will be building Reformer model, an efficent transformer to create intelligent conversational agents.We will learn how to train this model on dialogue datasets and generate responses that mimic human-like conversations. Through hands-on exercises using JAX, we will gain practical experience in building chatbots that can understand and respond to user queries. We will master the skills to develop sophisticated chatbot applications using state-of-the-art NLP techniques\n\n\n\n\n\nTuesday, April 27, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 3 Question Answering\n\n\nNLP with Attention Models\n\n\n\nNLP Specialization\n\n\nNLP with Attention Models\n\n\nNeural Machine Translation\n\n\ncoursera\n\n\nnotes\n\n\nNLP\n\n\nDeep Learning Algorithms\n\n\ntransformer\n\n\nteacher forcing\n\n\npositional encoding\n\n\nBERT\n\n\nT5\n\n\ntransformer decoder\n\n\nattention\n\n\ndot product attention\n\n\nself attention\n\n\ncausal attention\n\n\nmulti-head attention\n\n\nquestion answering task\n\n\n\nThis week of the NLP Specialization, we will uncover the field of Neural Question Answering. We will build advanced models like T5 and BERT to accurately answer questions based on given contexts. We will fine-tune these models to optimize their performance. We will gain practical experience in building question-answering systems. By the end of this week, we will have the skills to develop state-of-the-art NLP models to provide accurate answers to a wide range of questions.\n\n\n\n\n\nWednesday, April 14, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 2 Text Summarization\n\n\nNLP with Attention Models\n\n\n\nAttention Models\n\n\nNeural Machine Translation\n\n\nCoursera\n\n\nnotes\n\n\nDeep Learning Algorithms\n\n\ntransformer\n\n\nteacher forcing\n\n\npositional encoding\n\n\nGPT2\n\n\ntransformer decoder\n\n\nattention\n\n\ndot product attention\n\n\nself attention\n\n\ncausal attention\n\n\nmulti-head attention\n\n\nsummarization task\n\n\n\nThis week of the NLP Specialization, we unlock the secrets of Neural Text Summarization. We will building a powerful Transformer model to extract crucial information and create concise summaries. Through hands-on exercises using JAX, we will learn techniques like beam search and length normalization to enhance the quality of our summaries. Through hands-on exercises we will train our model on a dataset of articles. We will master the skills to create effective text summaries using state-of-the-art NLP techniques.\n\n\n\n\n\nWednesday, March 31, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 1 Neural Machine Translation\n\n\nNLP with Attention Models\n\n\n\nNatural Language Processing Specialization\n\n\nNatural Language Processing with Attention Models\n\n\nNeural Machine Translation\n\n\ncoursera\n\n\nnotes\n\n\nNLP\n\n\nDeepLearningAlgorithms\n\n\ntransformer\n\n\nteacher forcing\n\n\npositional encoding\n\n\nseq2seq\n\n\nword alignment\n\n\nattention\n\n\ndot product attention\n\n\ntranslation task\n\n\nbeam search\n\n\nMBR\n\n\nBLEU\n\n\nROUGE\n\n\n\nThis week of the Specialization, we’ll dive into the fundamentals of Neural Machine Translation (NMT). We’ll learn about the encoder-decoder architecture, which forms the basis of NMT models, and explore attention mechanisms that enable the model to focus on different parts of the input sequence during translation. Through hands-on exercises, we’ll implement an attention model for English to German translation using TensorFlow, train it on a dataset of sentence pairs, and evaluate its performance. By the end of this week, we’ll have gained a solid understanding of NMT and its applications in language translation.\n\n\n\n\n\nSaturday, March 20, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nAutocorrect and minimum edit distance\n\n\nProbabilistic Models\n\n\n\nNLP\n\n\nCoursera\n\n\nNotes\n\n\nProbabilistic Models\n\n\n\nAuto-correct text with minimum edit distances\n\n\n\n\n\nFriday, October 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Networks for Sentiment Analysis\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nnotes\n\n\nProbabilistic Models\n\n\n\nwe cover Neural networks for deep learning, then build a tweet classifier that places tweets into positive or negative sentiment categories, using a DNN.\n\n\n\n\n\nFriday, October 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 4 - Naive Machine Translation and LSH\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nWord Embeddings\n\n\nTranslation task\n\n\nSearch Task\n\n\n\n\n\n\n\n\n\nFriday, October 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nVector manipulation in Python\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nWord Embeddings\n\n\nTranslation task\n\n\nSearch Task\n\n\n\n\n\n\n\n\n\nFriday, October 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nHash functions and multiplanes\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nWord Embeddings\n\n\nTranslation task\n\n\nSearch Task\n\n\n\n\n\n\n\n\n\nFriday, October 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Translation and Document Search\n\n\nClassification & Vector Spaces\n\n\n\nNLP\n\n\nCoursera\n\n\nnotes\n\n\nWord Embeddings\n\n\nTranslation task\n\n\nSearch Task\n\n\n\nConcepts, code snippets, and slide commentaries for this week’s lesson of the Course notes from the deeplearning.ai natural language programming specialization.\n\n\n\n\n\nFriday, October 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Networks for Sentiment Analysis\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nnotes\n\n\nProbabilistic Models\n\n\n\nwe cover Neural networks for deep learning, then build a tweet classifier that places tweets into positive or negative sentiment categories, using a DNN.\n\n\n\n\n\nFriday, October 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding and Visualizing word frequencies\n\n\n\n\n\n\n\n\n\n\n\nFriday, October 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nLanguage Models: Auto-Complete\n\n\nProbabilistic Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nProbabilistic Models\n\n\n\n\n\n\n\n\n\nFriday, October 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nN-grams Corpus preprocessing\n\n\nProbabilistic Models\n\n\n\nNLP\n\n\nCoursera\n\n\nNotes\n\n\nProbabilistic Models\n\n\n\n\n\n\n\n\n\nFriday, October 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nAutocomplete and Language Models\n\n\nProbabilistic Models\n\n\n\nNLP\n\n\nCoursera\n\n\nNotes\n\n\nProbabilistic Models\n\n\n\nExtract features from text into numerical vectors, then build a binary classifier for tweets using logistic regression!\n\n\n\n\n\nFriday, October 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Networks for Sentiment Analysis\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nnotes\n\n\nProbabilistic Models\n\n\n\nwe cover Neural networks for deep learning, then build a tweet classifier that places tweets into positive or negative sentiment categories, using a DNN.\n\n\n\n\n\nFriday, October 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Networks for Sentiment Analysis\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nnotes\n\n\nProbabilistic Models\n\n\n\nwe cover Neural networks for deep learning, then build a tweet classifier that places tweets into positive or negative sentiment categories, using a DNN.\n\n\n\n\n\nFriday, October 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 1 Logistic Regression\n\n\nClassification & Vector Spaces\n\n\n\n\n\n\n\n\nFriday, October 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nWord embeddings with neural networks\n\n\nProbabilistic Models\n\n\n\nNLP\n\n\nCoursera\n\n\nnotes\n\n\nProbabilistic Models\n\n\n\nExtract features from text into numerical vectors, then build a binary classifier for tweets using logistic regression!\n\n\n\n\n\nFriday, October 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nProbability and Bayes Rule\n\n\nClassification & Vector Spaces\n\n\n\nNLP\n\n\nCoursera\n\n\ncode\n\n\nConditional Probability\n\n\nBayes rule\n\n\nNaïve Bayes\n\n\nLaplace smoothing\n\n\nLog-likelihood\n\n\nclassification\n\n\nsentiment analysis task\n\n\n\nConcepts, code snippets, and slide commentaries for this week’s lesson of the Course notes from the deeplearning.ai natural language programming specialization.\n\n\n\n\n\nFriday, October 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nProbability and Bayes Rule\n\n\nClassification & Vector Spaces\n\n\n\nNLP\n\n\nCoursera\n\n\nnotes\n\n\nConditional Probability\n\n\nBayes rule\n\n\nNaïve Bayes\n\n\nLaplace smoothing\n\n\nLog-likelihood\n\n\nSentiment analysis task\n\n\nClassification & Vector Spaces\n\n\n\nThe theory behind Bayes’ rule for conditional probabilities, and its application toward building a Naive Bayes tweet classifier\n\n\n\n\n\nFriday, October 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nPreprocessing\n\n\n\n\n\n\n\n\n\n\n\nFriday, October 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nVector Space Models\n\n\nClassification & Vector Spaces\n\n\n\nNLP\n\n\nCoursera\n\n\nnotes\n\n\nClassification & Vector Spaces\n\n\n\nVector space models capture semantic meaning and relationships between words. You’ll learn how to create word vectors that capture dependencies between words, then visualize their relationships in two dimensions using PCA.\n\n\n\n\n\nFriday, October 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing tweets and the Logistic Regression model\n\n\n\n\n\n\n\n\n\n\n\nFriday, October 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 2: Parts-of-Speech Tagging (POS)\n\n\nProbabilistic Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nProbabilistic Models\n\n\n\nDevelop skills in part-of-speech (POS) tagging, the process of assigning a part-of-speech tag (Noun, Verb, Adjective…) to each word in an input text.\n\n\n\n\n\nFriday, October 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nParts-of-Speech Tagging - First Steps: Working with text files, Creating a Vocabulary and Handling Unknown Words\n\n\nProbabilistic Models\n\n\n\nNLP\n\n\nCoursera\n\n\nlab\n\n\nProbabilistic Models\n\n\n\nExtract features from text into numerical vectors, then build a binary classifier for tweets using logistic regression!\n\n\n\n\n\nFriday, October 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nParts-of-Speech Tagging - Working with tags and Numpy\n\n\nProbabilistic Models\n\n\n\nNLP\n\n\nCoursera\n\n\nlab\n\n\nProbabilistic Models\n\n\n\nIn this lab you will create a matrix using some tag information and then modify it using different approaches. This will serve as hands-on experience working with Numpy\n\n\n\n\n\nFriday, October 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nPart of Speech Tagging and Hidden Markov Models\n\n\nProbabilistic Models\n\n\n\nNLP\n\n\nCoursera\n\n\nNotes\n\n\nProbabilistic Models\n\n\n\nExtract features from text into numerical vectors, then build a binary classifier for tweets using logistic regression!\n\n\n\n\n\nFriday, October 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing tweets and the Logistic Regression model\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nlab\n\n\nLogistic regression\n\n\nSentiment analysis task\n\n\nClassification & Vector Spaces\n\n\n\n\n\n\n\n\n\nFriday, October 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nPreprocessing\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nlab\n\n\nLogistic regression\n\n\nSentiment analysis task\n\n\nClassification & Vector Spaces\n\n\n\n\n\n\n\n\n\nFriday, October 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding and Visualizing word frequencies\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nWord frequencies\n\n\nNLTK\n\n\nClassification & Vector Spaces\n\n\n\n\n\n\n\n\n\nFriday, October 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nSentiment Analysis with Logistic Regression\n\n\nClassification & Vector Spaces\n\n\n\nNLP\n\n\nCoursera\n\n\nnotes\n\n\nLogistic regression\n\n\nSentiment analysis task\n\n\nClassification & Vector Spaces\n\n\n\nExtract features from text into numerical vectors, then build a binary classifier for tweets using logistic regression!\n\n\n\n\n\nFriday, October 23, 2020\n\n\n\n\n\n\nNo matching items\n\n\n  \n\nCitationBibTeX citation:@online{2025,\n  author = {},\n  title = {NLP Course Notes and Research Notebooks},\n  date = {2025-01-30},\n  url = {https://orenbochman.github.io/notes-nlp/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“NLP Course Notes and Research Notebooks .” 2025. January\n30, 2025. https://orenbochman.github.io/notes-nlp/."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog\n\n\nCitationBibTeX citation:@online{2025,\n  author = {},\n  title = {About},\n  date = {2025-01-29},\n  url = {https://orenbochman.github.io/notes-nlp/about.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“About.” 2025. January 29, 2025. https://orenbochman.github.io/notes-nlp/about.html."
  },
  {
    "objectID": "posts/review/seret-life-of-pronouns/index.html",
    "href": "posts/review/seret-life-of-pronouns/index.html",
    "title": "NLP Specialization",
    "section": "",
    "text": "cover\nIn (Pennebaker 2013) “The Secret Life of Pronouns: What Our Words Say About Us,” the author explores how the seemingly innocuous function words we use—pronouns, articles, prepositions, and auxiliary verbs—can reveal a great deal about our personalities, emotions, social connections, and psychological states. James W. Pennebaker, the author and a social psychologist, uses computer-based text analysis methods to uncover these hidden patterns."
  },
  {
    "objectID": "posts/review/seret-life-of-pronouns/index.html#outline",
    "href": "posts/review/seret-life-of-pronouns/index.html#outline",
    "title": "NLP Specialization",
    "section": "Outline",
    "text": "Outline\n\nKey Questions and Themes:\n\nCan language reveal psychological states? The author is primarily interested in what people’s words say about their psychological states. The book investigates whether language features can reveal how people are thinking and whether this information can be used to change their thinking.\nHow do function words differ from content words? The book distinguishes between content and style of speech, arguing that function words, which reflect language style, reveal more about a person than content words. Function words are tied to the relationship between speaker and listener.\nDo men and women use words differently? The book explores gender differences in language use, noting that women are typically more attentive to the thoughts and feelings of others, while men are less so. It also investigates whether different writing styles reflect different personalities.\nCan language predict behavior? The book examines whether language use can predict behaviors such as success in college, risk of suicide, leadership styles, and likelihood of declaring war.\nHow can language be used as a tool for change? The author examines whether analyzing words can lead to improvements in relationships, teaching, and leadership. Additionally, the book looks at how writing about emotional upheavals can improve mental and physical health, and whether using certain types of words can influence the effectiveness of that writing.\nCan language reveal deception? The book explores linguistic markers of deception, such as the use of first-person singular pronouns and the mention of other individuals.\nCan language analysis help identify authors? The book presents methods for identifying authors using function words, punctuation, and obscure words.\n\nMain Examples and Studies:\n\nExpressive Writing: Early research focused on the benefits of writing about traumatic experiences, leading to the development of the LIWC (Linguistic Inquiry and Word Count) program. This program helps analyze word categories associated with healthy writing such as positive emotion words, cognitive words, and the ability to construct a story.\nThe Bottle and the Two People Pictures: Participants described pictures, and their descriptions were analyzed for function word use and content. For example, people who focused on shadows in the bottle picture tended to be more creative. The second picture revealed common themes such as social connections, clothing, and accessories.\nThinking Styles: The book describes how people’s function words can be statistically grouped into categories reflecting thinking styles, such as formal, analytic, and narrative. These thinking styles can predict how people relate to others and organize their worlds.\n9/11 Blog Analysis: The language used in blog posts was analyzed before and after the 9/11 attacks. Researchers noted an increase in negative emotion words, a decrease in positive emotion words, and a temporary increase in cognitive words.\nCollege Admissions Essays: The study examined whether the writing style in college admissions essays could predict college grades.\nThe Federalist Papers: The book discusses the use of function words to determine the authorship of the anonymous Federalist Papers.\nLanguage Style Matching (LSM): LSM is a technique to determine how similarly two people use function words. It is used in studies of couples, speed dating, and even the writings of historical figures to analyze the relationship between the authors.\nObama’s Pronoun Use: The book analyzes Barack Obama’s use of first-person singular pronouns. His relatively low use of I-words is interpreted as a sign of self-confidence.\n\nAdditional Insights:\n\nStealth Words: The book emphasizes the importance of function words, often called “stealth words,” which are often overlooked.\nThe Role of Computers: Computer technology has revolutionized the study of language, allowing researchers to analyze large datasets and uncover hidden patterns that would be impossible to see with manual analysis.\nLanguage as a Tool: Language can be used not only as a window into the inner workings of people, but also as a tool for guiding thinking and promoting change.\nInterdisciplinary Approach: The author’s work draws on multiple disciplines including psychology, linguistics, computer science, history, and political science."
  },
  {
    "objectID": "posts/c4w3/index.html",
    "href": "posts/c4w3/index.html",
    "title": "Week 3 Question Answering",
    "section": "",
    "text": "deeplearning.ai",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#sec-question-answering",
    "href": "posts/c4w3/index.html#sec-question-answering",
    "title": "Week 3 Question Answering",
    "section": "Question Answering",
    "text": "Question Answering\nNotes for: NLP with Attention Models Week 3\nNatural Language Processing with Attention Models - Question Answering\n\n\n\n\n\n\nLearning Objectives:\n\n\n\n\nGain intuition for how transfer learning works in the context of NLP\nIdentify two approaches to transfer learning\nDiscuss the evolution of language models from CBOW to T5 and Bert\nFine-tune BERT on a dataset\nImplement context-based question answering with T5\nInterpret the GLUE benchmark",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#sec-video1-week3-overview",
    "href": "posts/c4w3/index.html#sec-video1-week3-overview",
    "title": "Week 3 Question Answering",
    "section": "Video 1 : Week 3 Overview",
    "text": "Video 1 : Week 3 Overview\n In this week you are going to learn about transfer learning. More specifically you will understand how T5 and BERT actually work.\n\n\n\n\nquestion-answering\n\n\n\n\n\n\n\nDefinitions:\n\n\n\nQ&A comes in two forms:\ncontext based : given a document and a question the model extracts an answer or generates an answer\nclosed book : the model picks an answer from servral options (classifier)\n\n\n\n\n\n\ntl\n\n\n\n\nclassical-training\n\n\n\n\ntransfer-learning\n\n\n\n You can see how a model initially trained on some type of sentiment classification, could now be used for question answering. One other model that has state of the art makes use of multi tasking. For example, the same model could be used for sentiment analysis, question answering, and many other things.\n\n\n\n\ngoals\n\nThese new types of models make use of a lot of data. For example the C4 (colossal cleaned crawled corpus) is about 800 GB when all of the english wikipedia is just 13 GB!\n\nC4 is a colossal, cleaned version of Common Crawl’s web crawl corpus. It was based on Common Crawl dataset: https://commoncrawl.org. It was used to train the T5 text-to-text Transformer models. Introduced by Raffel et al. in Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer The dataset can be downloaded in a pre-processed form from allennlp. C4 $ papers with code",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#sec-video2-transfer-learning-nlp",
    "href": "posts/c4w3/index.html#sec-video2-transfer-learning-nlp",
    "title": "Week 3 Question Answering",
    "section": "Video 2 : Transfer Learning in NLP",
    "text": "Video 2 : Transfer Learning in NLP\n\n\n\n\ntransfer-learning-options\n\n\n\n\ntl-general-purpose\n\n\n\n\ntl-features-vs-fine-tuning\n\n\n\n\ntl-fine-tuning\n\n\n\n\ntl-pretain-data-performance\n\n\n\n\ntl-pretain-data-supervision\n\n\n\n\ntl-pretain-unsupervised\n\n\n\n\ntl-pretrain-selfsupervised\n\n\n\n\ntl-pretrain-selfsupervised\n\n\n\n\ntl-per-task-fine-tuning\n\n\n\n\ntl-summary\n\n\n\n\n\n\n\n\n\n\n\nThere are three main advantages to transfer learning:\n\nReduce training time\nImprove predictions\nAllows you to use smaller datasets\n\nTwo methods that you can use for transfer learning are the following:\n\npre-training\nfine tuning\n\nIn feature based, you can train word embeddings by running a different model and then using those features (i.e. word vectors) on a different task. When fine tuning, you can use the exact same model and just run it on a different task. Sometimes when fine tuning, you can keep the model weights fixed and just add a new layer that you will train. Other times you can slowly unfreeze the layers one at a time. You can also use unlabelled data when pre-training, by masking words and trying to predict which word was masked.\nFor example, in the drawing above we try to predict the word “friend”. This allows your model to get a grasp of the overall structure of the data and to help the model learn some relationships within the words of a sentence",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#sec-video3-elmo-gpt-bert-t5",
    "href": "posts/c4w3/index.html#sec-video3-elmo-gpt-bert-t5",
    "title": "Week 3 Question Answering",
    "section": "Video 3 : ELMo, GPT, BERT, T5",
    "text": "Video 3 : ELMo, GPT, BERT, T5\n\n\n\n\noutline\n\n\n\n\n\n\nCBOW-fixed-window\n\nThe models mentioned in the previous video were discovered in the following order.\n\nCBOW in Word2Vec - Issue: Fixed window we want all the context\n\n2013 Word2Vec Google\nCBOW & Skip grams\n\n2014 Glove Stanfor GloVe: Global Vectors for Word ()\n\nElMo - Bidirectional LSTM\n\nSolves: fixed window size using a biderectional RNN\nIssue: weak long term dependency\n\nGPT2 - issue: unidirectional. only looks back\nBERT - just encoder - biderctional, multi mask learning\nT5 - Encoder Decoder - multi-task learning",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#cbow",
    "href": "posts/c4w3/index.html#cbow",
    "title": "Week 3 Question Answering",
    "section": "CBOW",
    "text": "CBOW\n\n\n\n\nCBOW-issues\n\n\n\n\nELMo-solution\n\n\n\n\nELMo-RNN\n\n\n\n\nGPT-unidirectional\n\n\n\n\nBERT\n\n\n\n\nmulti-mask\n\n\n\n\nBERT-pre-training\n\n\n\n\nt5-encoder-decoder\n\n\n\n\n\n\n\n\nIn CBOW, you want to encode a word as a vector. To do this we used the context before the word and the context after the word and we use that model to learn and creates features for the word. CBOW however uses a fixed window C (for the context).\nthe main isused with CBOW are:\n\nit has a fixed window size\nno concept of order\n\nso what do we do when we need more context to model the concept we are looking at?\nWhat ElMo does, it uses a bi-directional LSTM, which is a version of an RNN that looks at the inputs from the left and the right. This has the added benefit that the context size is no longer constrained. But since it is an RNN it has problems propagating information as sequences grow longer.\nThen Open AI introduced GPT. GPT unfortunately is uni-directional but it makes use of transformers. Although ElMo was bi-directional, it suffered from some issues such as capturing longer-term dependencies.\nBERT was then introduced which stands for the Bi-directional Encoder Representation from Transformers.\nT5 was introduced which makes use of transfer learning and uses the same model to predict on many tasks.\n\nGPT was a transformer decoder\nBERT was a transformer encoder\nT5 is a decoder encoder\n\n\n\n\n\nt5-text-to-text\n\nHere is an illustration of how T5 works:\n\n\n\n\nquestion\n\n\n\n\nsummary\n\n\nSo we can now flesh out the table",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#sec-video4-bert",
    "href": "posts/c4w3/index.html#sec-video4-bert",
    "title": "Week 3 Question Answering",
    "section": "Video 4 : BERT Bidirectional Encoder Representations from Transformers",
    "text": "Video 4 : BERT Bidirectional Encoder Representations from Transformers\n\n\n\n\nBERT-outline\n\n\n\n\nBERT-question\n\n\n\n\nBERT-summary\n\n\n\nlets dive deeper into BERT\nThere are two steps in the BERT framework: pre-training and fine-tuning. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For fine tuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. For example, in the figure above, you get the corresponding embeddings for the input words, you run it through a few transformer blocks, and then you make the prediction at each time point T_i.\nTraining procedures:\n\nChoose 15% of the tokens at random:\n\nmask them 80% of the time,\nreplace them with a random token 10% of the time,\nkeep as is 10% of the time. There could be multiple masked spans in a sentence. Next sentence prediction is also used when pre-training.\n\n\n\n\n\n\nBERT\n\n\n\n\nBERT-spec\n\n\nSpec and features:\n\n\n\n\nBERT-pre-training",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#sec-video5-bert-objective",
    "href": "posts/c4w3/index.html#sec-video5-bert-objective",
    "title": "Week 3 Question Answering",
    "section": "Video 5 : BERT Objective",
    "text": "Video 5 : BERT Objective\n\n\n\n\nBERT-outline\n\nMLM - masked language modeling.\nThis is the main unsupervised procedure to train the model with context left and right. It’s not clear how the model handles multiple masked items.\nDoes it try to predict them all at once or each one by considering input as context and unknowns.\n\n\n\n\nBERT-the-input\n\nThe input embeddings are the sum of the token embeddings, the segmentation embeddings and the position embeddings. The input embeddings: you have a CLS token to indicate the beginning of the sentence and a sep to indicate the end of the sentence The segment embeddings: allows you to indicate whether it is sentence a or b. Positional embeddings: allows you to indicate the word’s position in the sentence.\n\n\n\n\nBERT-the-output\n\nThe C token in the image above could be used for classification purposes. The unlabeled sentence A/B pair will depend on what you are trying to predict, it could range from question answering to sentiment. (in which case the second sentence could be just empty).\n\n\n\n\nBERT-objectives\n\nThe BERT objective is defined as follows:\n\n\n\n\nBERT-summary",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#sec-video6-fine-tuning-bert",
    "href": "posts/c4w3/index.html#sec-video6-fine-tuning-bert",
    "title": "Week 3 Question Answering",
    "section": "Video 6: Fine tuning BERT",
    "text": "Video 6: Fine tuning BERT\n\n\n\n\nBERT-fine-tuning-outline\n\nOnce you have a pre-trained model, you can fine tune it on different tasks.\n\n\n\n\ninputs\n\nFor example, given a hypothesis, you can identify the premise. Given a question, you can find the answer. You can also use it for named entity recognition. Here is a summary of the inputs.\n\nYou can replace sentences A/B\nParaphrase from sentence A\nQuestion/passage\nHypothesis premise pairs in entailment\nText and a Ø for classification/sequence tagging\nOutput tokens are fed into a layer for token level tasks otherwise use [CLS] embedding as input.\n\n\n\n\n\nsummary",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#video-7-transformer-t5",
    "href": "posts/c4w3/index.html#video-7-transformer-t5",
    "title": "Week 3 Question Answering",
    "section": "Video 7: Transformer: T5",
    "text": "Video 7: Transformer: T5\n\n\n\n\nt5-outline\n\n\n\n\nt5-text-to-text\n\n\n\n\nT5-transformer\n\n\n\nOne of the major techniques that allowed the T5 model to reach state of the art is the concept of masking:\nFor example, you represent the “for inviting” with &lt;X&gt; and last with &lt;Y&gt; then the model predicts what the X should be and what the Y should be. This is exactly what we saw in the BERT loss. You can also mask out a few positions, not just one. The loss is only on the mask for BERT, for T5 it is on the target.\n\n\n\n\nT5-architecture\n\nSo we start with the basic encoder-decoder representation. There you have a fully visible attention in the encoder and then causal attention in the decoder. So light gray lines correspond to causal masking. And dark gray lines correspond to the fully visible masking.\nIn the middle we have the language model which consists of a single transformer layer stack. And it’s being fed the concatenation of the inputs and the target. So it uses causal masking throughout as you can see because they’re all gray lines. And you have X_1 going inside, you get X_2, X_2 goes into the model and you get X3 and so forth.\nTo the right, we have prefix language model which corresponds to allowing fully visible masking over the inputs as you can see with the dark arrows. And then causal masking in the rest.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#sec-video8-multi-task-training-strategy",
    "href": "posts/c4w3/index.html#sec-video8-multi-task-training-strategy",
    "title": "Week 3 Question Answering",
    "section": "Video 8: Lecture Multi-Task Training Strategy",
    "text": "Video 8: Lecture Multi-Task Training Strategy\n\n\n\n\nT5-architecture\n\n\n\n\nT5-summary\n\n\n\n\nT5-multi-task-training\n\n\n\nThis is a reminder of how the T5 model works:\nYou can see that you only have to add a small prefix to the input and the model as a result will solve the task for you. There are many tasks that the t5 model can do for you. It is possible to formulate most NLP tasks in a “text-to-text” format – that is, a task where the model is fed some text for context or conditioning and is then asked to produce some output text. This framework provides a consistent training objective both for pre-training and fine-tuning. Specifically, the model is trained with a maximum likelihood objective (using “teacher forcing” ) regardless of the task.\n\nTraining data strategies\n\nExamples-proportional mixing\n\nsample in proportion to the size of each task’s dataset\n\nTemperature scaled mixing\n\nadjust the “temperature”” of the mixing rates. This temperature parameter allows you to weight certain examples more than others. To implement temperature scaling with temperature T, we raise each task’s mixing rate rm to the power of 1⁄T and renormalize the rates so that they sum to 1. When T = 1, this approach is equivalent to examples-proportional mixing and as T increases the proportions become closer to equal mixing\n\nEqual mixing\n\nIn this case, you sample examples from each task with equal probability. Specifically, each example in each batch is sampled uniformly at random from one of the datasets you train on.\n\n\n\n\n\n\nio-format\n\n\n\n\nmulti-task-training\n\n\n\n\ndata-training-strategy\n\n\n\n\nunfreezing-adapter-layers\n\n\n\n\nquestion\n\n\n\n\nfine-tuning\n\n\n\n\n\n\nYou can see how fine tuning on a specific task could work even though you were pre-training on different tasks.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#sec-video9-glue-benchmark",
    "href": "posts/c4w3/index.html#sec-video9-glue-benchmark",
    "title": "Week 3 Question Answering",
    "section": "Video 9: GLUE Benchmark",
    "text": "Video 9: GLUE Benchmark\n\n\n\n\nGLUE-evaluation\n\n\n\n\nGLUE-tasks\n\n\n\n\nGLUE\n\n\n\nGeneral Language Understanding Evaluation (GLUE) is contains:\n\nA collection used to train, evaluate, analyze natural language understanding systems\nDatasets with different genres, and of different sizes and difficulties\nLeaderboard\n\nCurrently T5 is state of the art according to this GLUE benchmark and you will be implementing it for homework this week! This GLUE bench mark is used for research purposes, it is model agnostic, and relies on models that make use of transfer learning.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#sec-video10-question-answering",
    "href": "posts/c4w3/index.html#sec-video10-question-answering",
    "title": "Week 3 Question Answering",
    "section": "Video 10: Question Answering",
    "text": "Video 10: Question Answering\n You will be implementing an encoder this week. Last week you implemented the decoder. So here it is:\n\n\n\n\nBERT-encoder-Block\n\n\n\n\nBERT-blocks\n\n\n\n\nq&a-data-example\n\n\n\n\nq&a-with-t5\n\n\n\n\nt5\n\n\n\n\nt5-question\n\n\n\n\n\n\nYou can see there is a feed forward and the encoder-block above. It makes use of two residual connections, layer normalization, and dropout.\nThe steps you will follow to implement it are:\n\nLoad a pre-trained model\nProcess data to get the required inputs and outputs: “question: Q context: C” as input and “A” as target\nFine tune your model on the new task and input\nPredict using your own model",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#sec-programming-assignment-question-answering",
    "href": "posts/c4w3/index.html#sec-programming-assignment-question-answering",
    "title": "Week 3 Question Answering",
    "section": "Programming Assignment: Question Answering",
    "text": "Programming Assignment: Question Answering",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#sec-lab-sentencepiece-and-bpe",
    "href": "posts/c4w3/index.html#sec-lab-sentencepiece-and-bpe",
    "title": "Week 3 Question Answering",
    "section": "Lab: SentencePiece and BPE",
    "text": "Lab: SentencePiece and BPE\n\nNFKC Normalization\nunicode normalization - for accents, diacritics and friends\nfrom unicodedata import normalize\nnorm_eaccent = normalize('NFKC', '\\u00E9')\nnorm_e_accent = normalize('NFKC', '\\u0065\\u0301')\nprint(f'{norm_eaccent} = {norm_e_accent} : {norm_eaccent == norm_e_accent}')",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#lossless-tokenization",
    "href": "posts/c4w3/index.html#lossless-tokenization",
    "title": "Week 3 Question Answering",
    "section": "lossless tokenization",
    "text": "lossless tokenization\nTo ensure this lossless tokenization it replaces white space with _ (U+2581).\ns_ = s.replace(' ', '\\u2581')\n\nSentencePiece\n\n\nBPE",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#sec-lab-bert-loss",
    "href": "posts/c4w3/index.html#sec-lab-bert-loss",
    "title": "Week 3 Question Answering",
    "section": "Lab: BERT Loss",
    "text": "Lab: BERT Loss",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#sec-lab-t5",
    "href": "posts/c4w3/index.html#sec-lab-t5",
    "title": "Week 3 Question Answering",
    "section": "Lab: T5",
    "text": "Lab: T5",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#representation.-pdf-bib",
    "href": "posts/c4w3/index.html#representation.-pdf-bib",
    "title": "Week 3 Question Answering",
    "section": "Representation. [pdf] [bib]",
    "text": "Representation. [pdf] [bib]\n\n2017 fasttext Facebook CBOW\n\nmorphological via sub words Algorithm of fasttext is based on these two papers:[8]\nEnriching Word Vectors with Subword Information , Piotr Bojanowski, Edouard Grave, Armand Joulin and Tomas Mikolov, 2016\nBag of Tricks for Efficient Text Classification, Armand Joulin, Edouard Grave, Piotr Bojanowski, Tomas Mikolov, 2016\n\n2018 ELMO Allen Institute for AI ELMo - Character based Bidirectional LSTM - Issue: long term dependency is weak due to vanishing gradient and information loss.\nGPT Encoder only with left context\nBert uses\n2020 T5 uses a label to specify task uses task specific bidirectional lstm to build the embeddings\nBERT Decoder only\n\nInput Token embedding - the distributed representation of the tokens in one space S with Dim(S)=D\nSegment embedding - because the model cannot tell the segment apart\n\nPosition embedding because the model cannot discriminate the word position. \nNote we are trying to mimic RNN behavior but we don’t have recursion:\nNote these are added - they all live in S. Question: would putting S and P in their own dimensions more interpretable. Questions: how do we know the model does not have embeddings that are similar to E_A and E_0 Output CLS - classification token SEP - separator token convert to embedding C is used for next sentence prediction T_i are used for masked word prediction T\nCross entropy loss + Binary loss\n\ncross entropy loss to compare between two distribution from Softmax\n\nbinary loss - could use cross entropy on two cat.\nPretraining\n        before feeding data we mask 15% of the tokens.\nmask 80% of the time:\ntraining data generator chooses 15%. of these at random for prediction\nreplace with:\nmask .8 of the time a random word .1 of the time\noriginal world otherwise.\n\na sentence may have multiple masks.\n\nnext sentence prediction also used in pre training.\nwhy/how\n(s1,s2) true/false\n\n\nBERT_Base\n12 layers\n12 attention heads\n110 million parameters\nFine tuning BERT\nFine tuning\nT5 like BERT does Transfer learning + fine tuning. classification, MT, NE, Sentiment\nSo you can see over here you have fully visible attention in the encoder and then causal attention in the decoder. \nAnd then you have the general encoder-decoder representation just as \nnotation. \nSo light gray lines correspond to causal masking. \nAnd dark gray lines correspond to the fully visible masking. \nSo on the left as I said again, it's the standard encoder-decoder architecture. \nIn the middle over here what we have, \nwe have the language model which consists of a single transformer layer stack. \nAnd it's being fed the concatenation of the inputs and the target. \nSo it uses causal masking throughout as you can see because they're \nall gray lines. \nAnd you have X1 going inside over here, get at X2, \nX2 goes into the model X3 and so forth. \nNow over here to the right, \nwe have prefix language model which corresponds to allowing fully \nvisible masking over the inputs as you can see here in the dark arrows. \nAnd then causal masking in the rest.\nPlay video starting at :3:2 and follow transcript3:02\nSo as you can see over here, it's doing causal masking. \nSo the model architecture, it uses encoder/decoder stack. \nIt has 12 transformer blocks each. \nSo you can think of it as a dozen eggs and then 220 million parameters. \nSo in summary, you've seen prefix language model attention. \nYou've seen the model architecture for T5. \nAnd you've seen how the pre-training is done similar to birds, but \nwe just use mask language modeling here.\n\n\nencoder/decoder\n1212 transformer blocks 220 million parameters pre training 2^18 steps = 262144",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#tokenization",
    "href": "posts/c4w3/index.html#tokenization",
    "title": "Week 3 Question Answering",
    "section": "Tokenization",
    "text": "Tokenization\n\nSentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (Kudo & Richardson 2018) sub-word tokenization\nSubword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates (Kudo 2018) sub-word tokenization\nNeural Machine Translation of Rare Words with Subword Units (Sennrich et all 2016) sub-word tokenization\nSubword tokenizers TF tutorial sub-word tokenization\n[https://blog.floydhub.com/tokenization-nlp/]\nSwivel: Improving Embeddings by Noticing What’s Missing (Shazeer, 2016)",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#transformers",
    "href": "posts/c4w3/index.html#transformers",
    "title": "Week 3 Question Answering",
    "section": "Transformers",
    "text": "Transformers\n\nExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Raffel et al, 2019)\n\nReformer: The Efficient Transformer (Kitaev et al, 2020)\nAttention Is All You Need (Vaswani et al, 2017)\nDeep contextualized word representations (Peters et al, 2018)\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al, 2018)\nFinetuning Pretrained Transformers into RNNs (Kasai et all 2021)\nThe Illustrated Transformer (Alammar, 2018)\nThe Illustrated GPT-2 (Alammar, 2019)\nHow GPT3 Works - Visualizations and Animations (Alammar, 2020)\nAttention? Attention! (Lilian Weng, 2018)\nThe Transformer Family (Lilian Weng, 2020)\nTeacher forcing for RNNs\n\n\nQuestion Answering Task:\n\nTitle (Author et al., Year) note",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#links",
    "href": "posts/c4w3/index.html#links",
    "title": "Week 3 Question Answering",
    "section": "Links",
    "text": "Links\n\nJax\nTrax\nTrax community on Gitter\nCNN daily mail dataset\n\nLei Mao Machine Learning, Artificial Intelligence, Computer Science. [Byte Pair Encoding (Lei Mao 2021)] (https://leimao.github.io/blog/Byte-Pair-Encoding/) videos: Q&A\n\n\nSubword tokenizers\n\n\nSwivel Embeddings\nhttps://youtu.be/hAvtJ516Mw4",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c2w1/lab03.html",
    "href": "posts/c2w1/lab03.html",
    "title": "Assignment 1: Auto Correct",
    "section": "",
    "text": "Welcome to the first assignment of Course 2. This assignment will give you a chance to brush up on your python and probability skills. In doing so, you will implement an auto-correct system that is very effective and useful."
  },
  {
    "objectID": "posts/c2w1/lab03.html#outline",
    "href": "posts/c2w1/lab03.html#outline",
    "title": "Assignment 1: Auto Correct",
    "section": "Outline",
    "text": "Outline\n\n0. Overview\n\n0.1 Edit Distance\n\n1. Data Preprocessing\n\n1.1 Exercise 1\n1.2 Exercise 2\n1.3 Exercise 3\n\n2. String Manipulation\n\n2.1 Exercise 4\n2.2 Exercise 5\n2.3 Exercise 6\n2.4 Exercise 7\n\n3. Combining the edits\n\n3.1 Exercise 8\n3.2 Exercise 9\n3.3 Exercise 10\n\n4. Minimum Edit Distance\n\n4.1 Exercise 11\n\n5. Backtrace (Optional)\n\n ## 0. Overview\nYou use autocorrect every day on your cell phone and computer. In this assignment, you will explore what really goes on behind the scenes. Of course, the model you are about to implement is not identical to the one used in your phone, but it is still quite good.\nBy completing this assignment you will learn how to:\n\nGet a word count given a corpus\nGet a word probability in the corpus\nManipulate strings\nFilter strings\nImplement Minimum edit distance to compare strings and to help find the optimal path for the edits.\nUnderstand how dynamic programming works\n\nSimilar systems are used everywhere. - For example, if you type in the word “I am lerningg”, chances are very high that you meant to write “learning”, as shown in Figure 1.\n\n Figure 1\n\n #### 0.1 Edit Distance\nIn this assignment, you will implement models that correct words that are 1 and 2 edit distances away. - We say two words are n edit distance away from each other when we need n edits to change one word into another.\nAn edit could consist of one of the following options:\n\nDelete (remove a letter): ‘hat’ =&gt; ‘at, ha, ht’\nSwitch (swap 2 adjacent letters): ‘eta’ =&gt; ‘eat, tea,…’\nReplace (change 1 letter to another): ‘jat’ =&gt; ‘hat, rat, cat, mat, …’\nInsert (add a letter): ‘te’ =&gt; ‘the, ten, ate, …’\n\nYou will be using the four methods above to implement an Auto-correct. - To do so, you will need to compute probabilities that a certain word is correct given an input.\nThis auto-correct you are about to implement was first created by Peter Norvig in 2007. - His original article may be a useful reference for this assignment.\nThe goal of our spell check model is to compute the following probability:\nP(c|w) = \\frac{P(w|c)\\times P(c)}{P(w)} \\tag{Eqn-1}\nThe equation above is Bayes Rule. - Equation 1 says that the probability of a word being correct $P(c|w) $is equal to the probability of having a certain word w, given that it is correct P(w|c), multiplied by the probability of being correct in general P(C) divided by the probability of that word w appearing P(w) in general. - To compute equation 1, you will first import a data set and then create all the probabilities that you need using that data set.\n # Part 1: Data Preprocessing\n\nimport re\nfrom collections import Counter\nimport numpy as np\nimport pandas as pd\n\nAs in any other machine learning task, the first thing you have to do is process your data set. - Many courses load in pre-processed data for you. - However, in the real world, when you build these NLP systems, you load the datasets and process them. - So let’s get some real world practice in pre-processing the data!\nYour first task is to read in a file called ‘shakespeare.txt’ which is found in your file directory. To look at this file you can go to File ==&gt; Open.\n ### Exercise 1 Implement the function process_data which\n\nReads in a corpus (text file)\nChanges everything to lowercase\nReturns a list of words.\n\n\nOptions and Hints\n\nIf you would like more of a real-life practice, don’t open the ‘Hints’ below (yet) and try searching the web to derive your answer.\nIf you want a little help, click on the green “General Hints” section by clicking on it with your mouse.\nIf you get stuck or are not getting the expected results, click on the green ‘Detailed Hints’ section to get hints for each step that you’ll take to complete this function.\n\n\n\nGeneral Hints\n\n\nGeneral Hints to get started\n\n\nPython input and output\n\n\nPython ‘re’ documentation \n\n\n\n\n\nDetailed Hints\n\n\nDetailed hints if you’re stuck\n\n\nUse ‘with’ syntax to read a file\n\n\nDecide whether to use ‘read()’ or ’readline(). What’s the difference?\n\n\nChoose whether to use either str.lower() or str.lowercase(). What is the difference?\n\n\nUse re.findall(pattern, string)\n\n\nLook for the “Raw String Notation” section in the Python ‘re’ documentation to understand the difference between r’‘, r’’ and ‘\\W’.\n\n\nFor the pattern, decide between using ‘’, ‘’, ‘+’ or ‘+’. What do you think are the differences?\n\n\n\n\n# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: process_data\ndef process_data(file_name):\n    \"\"\"\n    Input: \n        A file_name which is found in your current directory. You just have to read it in. \n    Output: \n        words: a list containing all the words in the corpus (text file you read) in lower case. \n    \"\"\"\n    words = [] # return this variable correctly\n\n    ### START CODE HERE ### \n    words = re.findall(r'\\w+',open(file_name).read().lower())\n    ### END CODE HERE ###\n    \n    return words\n\nNote, in the following cell, ‘words’ is converted to a python set. This eliminates any duplicate entries.\n\n#DO NOT MODIFY THIS CELL\nword_l = process_data('shakespeare.txt')\nvocab = set(word_l)  # this will be your new vocabulary\nprint(f\"The first ten words in the text are: \\n{word_l[0:10]}\")\nprint(f\"There are {len(vocab)} unique words in the vocabulary.\")\n\nThe first ten words in the text are: \n['o', 'for', 'a', 'muse', 'of', 'fire', 'that', 'would', 'ascend', 'the']\nThere are 6116 unique words in the vocabulary.\n\n\n\n\nExpected Output\nThe first ten words in the text are: \n['o', 'for', 'a', 'muse', 'of', 'fire', 'that', 'would', 'ascend', 'the']\nThere are 6116 unique words in the vocabulary.\n ### Exercise 2\nImplement a get_count function that returns a dictionary - The dictionary’s keys are words - The value for each word is the number of times that word appears in the corpus.\nFor example, given the following sentence: “I am happy because I am learning”, your dictionary should return the following:\n\n\n\nKey \n\n\nValue \n\n\n\n\nI\n\n\n2\n\n\n\n\nam\n\n\n2\n\n\n\n\nhappy\n\n\n1\n\n\n\n\nbecause\n\n\n1\n\n\n\n\nlearning\n\n\n1\n\n\n\nInstructions: Implement a get_count which returns a dictionary where the key is a word and the value is the number of times the word appears in the list.\n\n\nHints\n\n\n\n\nTry implementing this using a for loop and a regular dictionary. This may be good practice for similar coding interview questions\n\n\nYou can also use defaultdict instead of a regualr dictionary, along with the for loop\n\n\nOtherwise, to skip using a for loop, you can use Python’s  Counter class\n\n\n\n\n# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# UNIT TEST COMMENT: Candidate for Table Driven Tests\n# GRADED FUNCTION: get_count\ndef get_count(word_l):\n    '''\n    Input:\n        word_l: a set of words representing the corpus. \n    Output:\n        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.\n    '''\n    \n    word_count_dict = {}  # fill this with word counts\n    ### START CODE HERE \n    word_count_dict = Counter(word_l)\n    ### END CODE HERE ### \n    return word_count_dict\n\n\n#DO NOT MODIFY THIS CELL\nword_count_dict = get_count(word_l)\nprint(f\"There are {len(word_count_dict)} key values pairs\")\nprint(f\"The count for the word 'thee' is {word_count_dict.get('thee',0)}\")\n\nThere are 6116 key values pairs\nThe count for the word 'thee' is 240\n\n\n\n\nExpected Output\nThere are 6116 key values pairs\nThe count for the word 'thee' is 240\n ### Exercise 3 Given the dictionary of word counts, compute the probability that each word will appear if randomly selected from the corpus of words.\nP(w_i) = \\frac{C(w_i)}{M} \\tag{Eqn-2} where\nC(w_i) is the total number of times w_i appears in the corpus.\nM is the total number of words in the corpus.\nFor example, the probability of the word ‘am’ in the sentence ‘I am happy because I am learning’ is:\nP(am) = \\frac{C(w_i)}{M} = \\frac {2}{7} \\tag{Eqn-3}.\nInstructions: Implement get_probs function which gives you the probability that a word occurs in a sample. This returns a dictionary where the keys are words, and the value for each word is its probability in the corpus of words.\n\n\nHints\n\n\nGeneral advice\n\n\nUse dictionary.values()\n\n\nUse sum()\n\n\nThe cardinality (number of words in the corpus should be equal to len(word_l). You will calculate this same number, but using the word count dictionary.\n\n\nIf you’re using a for loop:\n\n\nUse dictionary.keys()\n\n\nIf you’re using a dictionary comprehension:\n\n\nUse dictionary.items()\n\n\n\n\n# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: get_probs\ndef get_probs(word_count_dict):\n    '''\n    Input:\n        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.\n    Output:\n        probs: A dictionary where keys are the words and the values are the probability that a word will occur. \n    '''\n    probs = {}  # return this variable correctly\n    \n    ### START CODE HERE ###\n    probs = {k:v/sum(word_count_dict.values()) for k, v in word_count_dict.items()}\n    ### END CODE HERE ###\n    return probs\n\n\n#DO NOT MODIFY THIS CELL\nprobs = get_probs(word_count_dict)\nprint(f\"Length of probs is {len(probs)}\")\nprint(f\"P('thee') is {probs['thee']:.4f}\")\n\nLength of probs is 6116\nP('thee') is 0.0045\n\n\n\n\nExpected Output\nLength of probs is 6116\nP('thee') is 0.0045\n # Part 2: String Manipulations\nNow, that you have computed P(w_i) for all the words in the corpus, you will write a few functions to manipulate strings so that you can edit the erroneous strings and return the right spellings of the words. In this section, you will implement four functions:\n\ndelete_letter: given a word, it returns all the possible strings that have one character removed.\nswitch_letter: given a word, it returns all the possible strings that have two adjacent letters switched.\nreplace_letter: given a word, it returns all the possible strings that have one character replaced by another different letter.\ninsert_letter: given a word, it returns all the possible strings that have an additional character inserted.\n\n\n\nList comprehensions\nString and list manipulation in python will often make use of a python feature called list comprehensions. The routines below will be described as using list comprehensions, but if you would rather implement them in another way, you are free to do so as long as the result is the same. Further, the following section will provide detailed instructions on how to use list comprehensions and how to implement the desired functions. If you are a python expert, feel free to skip the python hints and move to implementing the routines directly.\nPython List Comprehensions embed a looping structure inside of a list declaration, collapsing many lines of code into a single line. If you are not familiar with them, they seem slightly out of order relative to for loops.\n\n Figure 2\n\nThe diagram above shows that the components of a list comprehension are the same components you would find in a typical for loop that appends to a list, but in a different order. With that in mind, we’ll continue the specifics of this assignment. We will be very descriptive for the first function, deletes(), and less so in later functions as you become familiar with list comprehensions.\n ### Exercise 4\nInstructions for delete_letter(): Implement a delete_letter() function that, given a word, returns a list of strings with one character deleted.\nFor example, given the word nice, it would return the set: {‘ice’, ‘nce’, ‘nic’, ‘nie’}.\nStep 1: Create a list of ‘splits’. This is all the ways you can split a word into Left and Right: For example,\n’nice is split into : [('', 'nice'), ('n', 'ice'), ('ni', 'ce'), ('nic', 'e'), ('nice', '')] This is common to all four functions (delete, replace, switch, insert).\n\n Figure 3\n\nStep 2: This is specific to delete_letter. Here, we are generating all words that result from deleting one character.\nThis can be done in a single line with a list comprehension. You can make use of this type of syntax:\n[f(a,b) for a, b in splits if condition]\nFor our ‘nice’ example you get: [‘ice’, ‘nce’, ‘nie’, ‘nic’]\n\n Figure 4\n\n\n\nLevels of assistance\nTry this exercise with these levels of assistance.\n- We hope that this will make it both a meaningful experience but also not a frustrating experience. - Start with level 1, then move onto level 2, and 3 as needed.\n- Level 1. Try to think this through and implement this yourself.\n- Level 2. Click on the \"Level 2 Hints\" section for some hints to get started.\n- Level 3. If you would prefer more guidance, please click on the \"Level 3 Hints\" cell for step by step instructions.\n\nIf you are still stuck, look at the images in the “list comprehensions” section above.\n\n\n\nLevel 2 Hints\n\n\n\n\n Use array slicing like my_string[0:2] \n\n\n Use list comprehensions or for loops \n\n\n\n\n\nLevel 3 Hints\n\n\n\n\nsplits: Use array slicing, like my_str[0:2], to separate a string into two pieces.\n\n\nDo this in a loop or list comprehension, so that you have a list of tuples.\n\nFor example, “cake” can get split into “ca” and “ke”. They’re stored in a tuple (“ca”,“ke”), and the tuple is appended to a list. We’ll refer to these as L and R, so the tuple is (L,R)\n\n&lt;li&gt;When choosing the range for your loop, if you input the word \"cans\" and generate the tuple  ('cans',''), make sure to include an if statement to check the length of that right-side string (R) in the tuple (L,R) &lt;/li&gt;\n&lt;li&gt;deletes: Go through the list of tuples and combine the two strings together. You can use the + operator to combine two strings&lt;/li&gt;\n&lt;li&gt;When combining the tuples, make sure that you leave out a middle character.&lt;/li&gt;\n&lt;li&gt;Use array slicing to leave out the first character of the right substring.&lt;/li&gt;\n\n\n\n# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# UNIT TEST COMMENT: Candidate for Table Driven Tests\n# GRADED FUNCTION: deletes\ndef delete_letter(word, verbose=False):\n    '''\n    Input:\n        word: the string/word for which you will generate all possible words \n                in the vocabulary which have 1 missing character\n    Output:\n        delete_l: a list of all possible strings obtained by deleting 1 character from word\n    '''\n    \n    delete_l = []\n    split_l = []\n    \n    ### START CODE HERE ###\n    split_l = [(word[:i], word[i:]) for i in range(len(word))]\n    delete_l = [l+r[1:] for l, r in split_l]\n    ### END CODE HERE ###\n\n    if verbose: print(f\"input word {word}, \\nsplit_l = {split_l}, \\ndelete_l = {delete_l}\")\n\n    return delete_l\n\n\ndelete_word_l = delete_letter(word=\"cans\",\n                        verbose=True)\n\ninput word cans, \nsplit_l = [('', 'cans'), ('c', 'ans'), ('ca', 'ns'), ('can', 's')], \ndelete_l = ['ans', 'cns', 'cas', 'can']\n\n\n\n\nExpected Output\nNote: You might get a slightly different result with split_l\n\ninput word cans, \nsplit_l = [('', 'cans'), ('c', 'ans'), ('ca', 'ns'), ('can', 's')], \ndelete_l = ['ans', 'cns', 'cas', 'can']\n\n\nNote 1\n\nNotice how it has the extra tuple ('cans', '').\nThis will be fine as long as you have checked the size of the right-side substring in tuple (L,R).\nCan you explain why this will give you the same result for the list of deletion strings (delete_l)?\n\ninput word cans, \nsplit_l = [('', 'cans'), ('c', 'ans'), ('ca', 'ns'), ('can', 's'), ('cans', '')], \ndelete_l = ['ans', 'cns', 'cas', 'can']\n\n\nNote 2\nIf you end up getting the same word as your input word, like this:\ninput word cans, \nsplit_l = [('', 'cans'), ('c', 'ans'), ('ca', 'ns'), ('can', 's'), ('cans', '')], \ndelete_l = ['ans', 'cns', 'cas', 'can', 'cans']\n\nCheck how you set the range.\nSee if you check the length of the string on the right-side of the split.\n\n\n# test # 2\nprint(f\"Number of outputs of delete_letter('at') is {len(delete_letter('at'))}\")\n\nNumber of outputs of delete_letter('at') is 2\n\n\n\n\nExpected output\nNumber of outputs of delete_letter('at') is 2\n ### Exercise 5\nInstructions for switch_letter(): Now implement a function that switches two letters in a word. It takes in a word and returns a list of all the possible switches of two letters that are adjacent to each other. - For example, given the word ‘eta’, it returns {‘eat’, ‘tea’}, but does not return ‘ate’.\nStep 1: is the same as in delete_letter()\nStep 2: A list comprehension or for loop which forms strings by swapping adjacent letters. This is of the form:\n[f(L,R) for L, R in splits if condition] where ‘condition’ will test the length of R in a given iteration. See below.\n\n Figure 5\n\n\n\nLevels of difficulty\nTry this exercise with these levels of difficulty.\n- Level 1. Try to think this through and implement this yourself. - Level 2. Click on the “Level 2 Hints” section for some hints to get started. - Level 3. If you would prefer more guidance, please click on the “Level 3 Hints” cell for step by step instructions.\n\n\nLevel 2 Hints\n\n\n\n\n Use array slicing like my_string[0:2] \n\n\n Use list comprehensions or for loops \n\n\nTo do a switch, think of the whole word as divided into 4 distinct parts. Write out ‘cupcakes’ on a piece of paper and see how you can split it into (‘cupc’, ‘k’, ‘a’, ‘es’)\n\n\n\n\n\nLevel 3 Hints\n\n\n\n\nsplits: Use array slicing, like my_str[0:2], to separate a string into two pieces.\n\n\nSplitting is the same as for delete_letter\n\n\nTo perform the switch, go through the list of tuples and combine four strings together. You can use the + operator to combine strings\n\n\nThe four strings will be the left substring from the split tuple, followed by the first (index 1) character of the right substring, then the zero-th character (index 0) of the right substring, and then the remaining part of the right substring.\n\n\nUnlike delete_letter, you will want to check that your right substring is at least a minimum length. To see why, review the previous hint bullet point (directly before this one).\n\n\n\n\n# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# UNIT TEST COMMENT: Candidate for Table Driven Tests\n# GRADED FUNCTION: switches\ndef switch_letter(word, verbose=False):\n    '''\n    Input:\n        word: input string\n     Output:\n        switches: a list of all possible strings with one adjacent charater switched\n    ''' \n    \n    switch_l = []\n    split_l = []\n    \n    ### START CODE HERE ###\n    split_l = [(word[:i], word[i:]) for i in range(len(word))]\n    switch_l = [l+r[1]+r[0]+r[2:] for l, r in split_l if len(r) &gt; 1]\n    ### END CODE HERE ###\n    \n    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nswitch_l = {switch_l}\") \n\n    return switch_l\n\n\nswitch_word_l = switch_letter(word=\"eta\",\n                         verbose=True)\n\nInput word = eta \nsplit_l = [('', 'eta'), ('e', 'ta'), ('et', 'a')] \nswitch_l = ['tea', 'eat']\n\n\n\n\nExpected output\nInput word = eta \nsplit_l = [('', 'eta'), ('e', 'ta'), ('et', 'a')] \nswitch_l = ['tea', 'eat']\n\n\nNote 1\nYou may get this:\nInput word = eta \nsplit_l = [('', 'eta'), ('e', 'ta'), ('et', 'a'), ('eta', '')] \nswitch_l = ['tea', 'eat']\n\nNotice how it has the extra tuple ('eta', '').\nThis is also correct.\nCan you think of why this is the case?\n\n\n\nNote 2\nIf you get an error\nIndexError: string index out of range\n\nPlease see if you have checked the length of the strings when switching characters.\n\n\n# test # 2\nprint(f\"Number of outputs of switch_letter('at') is {len(switch_letter('at'))}\")\n\nNumber of outputs of switch_letter('at') is 1\n\n\n\n\nExpected output\nNumber of outputs of switch_letter('at') is 1\n ### Exercise 6 Instructions for replace_letter(): Now implement a function that takes in a word and returns a list of strings with one replaced letter from the original word.\nStep 1: is the same as in delete_letter()\nStep 2: A list comprehension or for loop which form strings by replacing letters. This can be of the form:\n[f(a,b,c) for a, b in splits if condition for c in string] Note the use of the second for loop.\nIt is expected in this routine that one or more of the replacements will include the original word. For example, replacing the first letter of ‘ear’ with ‘e’ will return ‘ear’.\nStep 3: Remove the original input letter from the output.\n\n\nHints\n\n\n\n\nTo remove a word from a list, first store its contents inside a set()\n\n\nUse set.discard(‘the_word’) to remove a word in a set (if the word does not exist in the set, then it will not throw a KeyError. Using set.remove(‘the_word’) throws a KeyError if the word does not exist in the set.\n\n\n\n\n# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# UNIT TEST COMMENT: Candidate for Table Driven Tests\n# GRADED FUNCTION: replaces\ndef replace_letter(word, verbose=False):\n    '''\n    Input:\n        word: the input string/word \n    Output:\n        replaces: a list of all possible strings where we replaced one letter from the original word. \n    ''' \n    \n    letters = 'abcdefghijklmnopqrstuvwxyz'\n    replace_l = []\n    split_l = []\n    \n    ### START CODE HERE ###\n    split_l = [(word[:i], word[i:]) for i in range(len(word))]\n    replace_set = set([l+replace+r[1:] for l, r in split_l if len(r) &gt; 0 for replace in letters])\n    replace_set.discard(word)\n    ### END CODE HERE ###\n    \n    # turn the set back into a list and sort it, for easier viewing\n    replace_l = sorted(list(replace_set))\n    \n    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nreplace_l {replace_l}\")   \n    \n    return replace_l\n\n\nreplace_l = replace_letter(word='can',\n                              verbose=True)\n\nInput word = can \nsplit_l = [('', 'can'), ('c', 'an'), ('ca', 'n')] \nreplace_l ['aan', 'ban', 'caa', 'cab', 'cac', 'cad', 'cae', 'caf', 'cag', 'cah', 'cai', 'caj', 'cak', 'cal', 'cam', 'cao', 'cap', 'caq', 'car', 'cas', 'cat', 'cau', 'cav', 'caw', 'cax', 'cay', 'caz', 'cbn', 'ccn', 'cdn', 'cen', 'cfn', 'cgn', 'chn', 'cin', 'cjn', 'ckn', 'cln', 'cmn', 'cnn', 'con', 'cpn', 'cqn', 'crn', 'csn', 'ctn', 'cun', 'cvn', 'cwn', 'cxn', 'cyn', 'czn', 'dan', 'ean', 'fan', 'gan', 'han', 'ian', 'jan', 'kan', 'lan', 'man', 'nan', 'oan', 'pan', 'qan', 'ran', 'san', 'tan', 'uan', 'van', 'wan', 'xan', 'yan', 'zan']\n\n\n\n\nExpected Output**:\nInput word = can \nsplit_l = [('', 'can'), ('c', 'an'), ('ca', 'n')] \nreplace_l ['aan', 'ban', 'caa', 'cab', 'cac', 'cad', 'cae', 'caf', 'cag', 'cah', 'cai', 'caj', 'cak', 'cal', 'cam', 'cao', 'cap', 'caq', 'car', 'cas', 'cat', 'cau', 'cav', 'caw', 'cax', 'cay', 'caz', 'cbn', 'ccn', 'cdn', 'cen', 'cfn', 'cgn', 'chn', 'cin', 'cjn', 'ckn', 'cln', 'cmn', 'cnn', 'con', 'cpn', 'cqn', 'crn', 'csn', 'ctn', 'cun', 'cvn', 'cwn', 'cxn', 'cyn', 'czn', 'dan', 'ean', 'fan', 'gan', 'han', 'ian', 'jan', 'kan', 'lan', 'man', 'nan', 'oan', 'pan', 'qan', 'ran', 'san', 'tan', 'uan', 'van', 'wan', 'xan', 'yan', 'zan']\n\nNote how the input word ‘can’ should not be one of the output words.\n\n\n\nNote 1\nIf you get something like this:\nInput word = can \nsplit_l = [('', 'can'), ('c', 'an'), ('ca', 'n'), ('can', '')] \nreplace_l ['aan', 'ban', 'caa', 'cab', 'cac', 'cad', 'cae', 'caf', 'cag', 'cah', 'cai', 'caj', 'cak', 'cal', 'cam', 'cao', 'cap', 'caq', 'car', 'cas', 'cat', 'cau', 'cav', 'caw', 'cax', 'cay', 'caz', 'cbn', 'ccn', 'cdn', 'cen', 'cfn', 'cgn', 'chn', 'cin', 'cjn', 'ckn', 'cln', 'cmn', 'cnn', 'con', 'cpn', 'cqn', 'crn', 'csn', 'ctn', 'cun', 'cvn', 'cwn', 'cxn', 'cyn', 'czn', 'dan', 'ean', 'fan', 'gan', 'han', 'ian', 'jan', 'kan', 'lan', 'man', 'nan', 'oan', 'pan', 'qan', 'ran', 'san', 'tan', 'uan', 'van', 'wan', 'xan', 'yan', 'zan']\n\nNotice how split_l has an extra tuple ('can', ''), but the output is still the same, so this is okay.\n\n\n\nNote 2\nIf you get something like this:\nInput word = can \nsplit_l = [('', 'can'), ('c', 'an'), ('ca', 'n'), ('can', '')] \nreplace_l ['aan', 'ban', 'caa', 'cab', 'cac', 'cad', 'cae', 'caf', 'cag', 'cah', 'cai', 'caj', 'cak', 'cal', 'cam', 'cana', 'canb', 'canc', 'cand', 'cane', 'canf', 'cang', 'canh', 'cani', 'canj', 'cank', 'canl', 'canm', 'cann', 'cano', 'canp', 'canq', 'canr', 'cans', 'cant', 'canu', 'canv', 'canw', 'canx', 'cany', 'canz', 'cao', 'cap', 'caq', 'car', 'cas', 'cat', 'cau', 'cav', 'caw', 'cax', 'cay', 'caz', 'cbn', 'ccn', 'cdn', 'cen', 'cfn', 'cgn', 'chn', 'cin', 'cjn', 'ckn', 'cln', 'cmn', 'cnn', 'con', 'cpn', 'cqn', 'crn', 'csn', 'ctn', 'cun', 'cvn', 'cwn', 'cxn', 'cyn', 'czn', 'dan', 'ean', 'fan', 'gan', 'han', 'ian', 'jan', 'kan', 'lan', 'man', 'nan', 'oan', 'pan', 'qan', 'ran', 'san', 'tan', 'uan', 'van', 'wan', 'xan', 'yan', 'zan']\n\nNotice how there are strings that are 1 letter longer than the original word, such as cana.\nPlease check for the case when there is an empty string '', and if so, do not use that empty string when setting replace_l.\n\n\n# test # 2\nprint(f\"Number of outputs of switch_letter('at') is {len(switch_letter('at'))}\")\n\nNumber of outputs of switch_letter('at') is 1\n\n\n\n\nExpected output\nNumber of outputs of switch_letter('at') is 1\n ### Exercise 7\nInstructions for insert_letter(): Now implement a function that takes in a word and returns a list with a letter inserted at every offset.\nStep 1: is the same as in delete_letter()\nStep 2: This can be a list comprehension of the form:\n[f(a,b,c) for a, b in splits if condition for c in string]\n\n# UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# UNIT TEST COMMENT: Candidate for Table Driven Tests\n# GRADED FUNCTION: inserts\ndef insert_letter(word, verbose=False):\n    '''\n    Input:\n        word: the input string/word \n    Output:\n        inserts: a set of all possible strings with one new letter inserted at every offset\n    ''' \n    letters = 'abcdefghijklmnopqrstuvwxyz'\n    insert_l = []\n    split_l = []\n    \n    ### START CODE HERE ###\n    split_l = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n    insert_l = [l+replace+r for l, r in split_l for replace in letters]\n    ### END CODE HERE ###\n\n    if verbose: print(f\"Input word {word} \\nsplit_l = {split_l} \\ninsert_l = {insert_l}\")\n    \n    return insert_l\n\n\ninsert_l = insert_letter('at', True)\nprint(f\"Number of strings output by insert_letter('at') is {len(insert_l)}\")\n\nInput word at \nsplit_l = [('', 'at'), ('a', 't'), ('at', '')] \ninsert_l = ['aat', 'bat', 'cat', 'dat', 'eat', 'fat', 'gat', 'hat', 'iat', 'jat', 'kat', 'lat', 'mat', 'nat', 'oat', 'pat', 'qat', 'rat', 'sat', 'tat', 'uat', 'vat', 'wat', 'xat', 'yat', 'zat', 'aat', 'abt', 'act', 'adt', 'aet', 'aft', 'agt', 'aht', 'ait', 'ajt', 'akt', 'alt', 'amt', 'ant', 'aot', 'apt', 'aqt', 'art', 'ast', 'att', 'aut', 'avt', 'awt', 'axt', 'ayt', 'azt', 'ata', 'atb', 'atc', 'atd', 'ate', 'atf', 'atg', 'ath', 'ati', 'atj', 'atk', 'atl', 'atm', 'atn', 'ato', 'atp', 'atq', 'atr', 'ats', 'att', 'atu', 'atv', 'atw', 'atx', 'aty', 'atz']\nNumber of strings output by insert_letter('at') is 78\n\n\n\n\nExpected output\nInput word at \nsplit_l = [('', 'at'), ('a', 't'), ('at', '')] \ninsert_l = ['aat', 'bat', 'cat', 'dat', 'eat', 'fat', 'gat', 'hat', 'iat', 'jat', 'kat', 'lat', 'mat', 'nat', 'oat', 'pat', 'qat', 'rat', 'sat', 'tat', 'uat', 'vat', 'wat', 'xat', 'yat', 'zat', 'aat', 'abt', 'act', 'adt', 'aet', 'aft', 'agt', 'aht', 'ait', 'ajt', 'akt', 'alt', 'amt', 'ant', 'aot', 'apt', 'aqt', 'art', 'ast', 'att', 'aut', 'avt', 'awt', 'axt', 'ayt', 'azt', 'ata', 'atb', 'atc', 'atd', 'ate', 'atf', 'atg', 'ath', 'ati', 'atj', 'atk', 'atl', 'atm', 'atn', 'ato', 'atp', 'atq', 'atr', 'ats', 'att', 'atu', 'atv', 'atw', 'atx', 'aty', 'atz']\nNumber of strings output by insert_letter('at') is 78\n\n\nNote 1\nIf you get a split_l like this:\nInput word at \nsplit_l = [('', 'at'), ('a', 't')] \ninsert_l = ['aat', 'bat', 'cat', 'dat', 'eat', 'fat', 'gat', 'hat', 'iat', 'jat', 'kat', 'lat', 'mat', 'nat', 'oat', 'pat', 'qat', 'rat', 'sat', 'tat', 'uat', 'vat', 'wat', 'xat', 'yat', 'zat', 'aat', 'abt', 'act', 'adt', 'aet', 'aft', 'agt', 'aht', 'ait', 'ajt', 'akt', 'alt', 'amt', 'ant', 'aot', 'apt', 'aqt', 'art', 'ast', 'att', 'aut', 'avt', 'awt', 'axt', 'ayt', 'azt']\nNumber of strings output by insert_letter('at') is 52\n\nNotice that split_l is missing the extra tuple (‘at’, ’’). For insertion, we actually WANT this tuple.\nThe function is not creating all the desired output strings.\nCheck the range that you use for the for loop.\n\n\n\nNote 2\nIf you see this:\nInput word at \nsplit_l = [('', 'at'), ('a', 't'), ('at', '')] \ninsert_l = ['aat', 'bat', 'cat', 'dat', 'eat', 'fat', 'gat', 'hat', 'iat', 'jat', 'kat', 'lat', 'mat', 'nat', 'oat', 'pat', 'qat', 'rat', 'sat', 'tat', 'uat', 'vat', 'wat', 'xat', 'yat', 'zat', 'aat', 'abt', 'act', 'adt', 'aet', 'aft', 'agt', 'aht', 'ait', 'ajt', 'akt', 'alt', 'amt', 'ant', 'aot', 'apt', 'aqt', 'art', 'ast', 'att', 'aut', 'avt', 'awt', 'axt', 'ayt', 'azt']\nNumber of strings output by insert_letter('at') is 52\n\nEven though you may have fixed the split_l so that it contains the tuple ('at', ''), notice that you’re still missing some output strings.\n\nNotice that it’s missing strings such as ‘ata’, ‘atb’, ‘atc’ all the way to ‘atz’.\n\nTo fix this, make sure that when you set insert_l, you allow the use of the empty string ''.\n\n\n# test # 2\nprint(f\"Number of outputs of insert_letter('at') is {len(insert_letter('at'))}\")\n\nNumber of outputs of insert_letter('at') is 78\n\n\n\n\nExpected output\nNumber of outputs of insert_letter('at') is 78"
  },
  {
    "objectID": "posts/c2w1/lab02.html",
    "href": "posts/c2w1/lab02.html",
    "title": "NLP Course 2 Week 1 Lesson : Building The Model - Lecture Exercise 02",
    "section": "",
    "text": "Estimated Time: 20 minutes  # Candidates from String Edits Create a list of candidate strings by applying an edit operation  ### Imports and Data\n\n# data\nword = 'dearz' # 🦌\n\n\nSplits\nFind all the ways you can split a word into 2 parts !\n\n# splits with a loop\nsplits_a = []\nfor i in range(len(word)+1):\n    splits_a.append([word[:i],word[i:]])\n\nfor i in splits_a:\n    print(i)\n\n['', 'dearz']\n['d', 'earz']\n['de', 'arz']\n['dea', 'rz']\n['dear', 'z']\n['dearz', '']\n\n\n\n# same splits, done using a list comprehension\nsplits_b = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n\nfor i in splits_b:\n    print(i)\n\n('', 'dearz')\n('d', 'earz')\n('de', 'arz')\n('dea', 'rz')\n('dear', 'z')\n('dearz', '')\n\n\n\n\nDelete Edit\nDelete a letter from each string in the splits list.  What this does is effectivly delete each possible letter from the original word being edited.\n\n# deletes with a loop\nsplits = splits_a\ndeletes = []\n\nprint('word : ', word)\nfor L,R in splits:\n    if R:\n        print(L + R[1:], ' &lt;-- delete ', R[0])\n\nword :  dearz\nearz  &lt;-- delete  d\ndarz  &lt;-- delete  e\nderz  &lt;-- delete  a\ndeaz  &lt;-- delete  r\ndear  &lt;-- delete  z\n\n\nIt’s worth taking a closer look at how this is excecuting a ‘delete’.  Taking the first item from the splits list :\n\n# breaking it down\nprint('word : ', word)\none_split = splits[0]\nprint('first item from the splits list : ', one_split)\nL = one_split[0]\nR = one_split[1]\nprint('L : ', L)\nprint('R : ', R)\nprint('*** now implicit delete by excluding the leading letter ***')\nprint('L + R[1:] : ',L + R[1:], ' &lt;-- delete ', R[0])\n\nword :  dearz\nfirst item from the splits list :  ['', 'dearz']\nL :  \nR :  dearz\n*** now implicit delete by excluding the leading letter ***\nL + R[1:] :  earz  &lt;-- delete  d\n\n\nSo the end result transforms ‘dearz’ to ‘earz’ by deleting the first character.  And you use a loop (code block above) or a list comprehension (code block below) to do  this for the entire splits list.\n\n# deletes with a list comprehension\nsplits = splits_a\ndeletes = [L + R[1:] for L, R in splits if R]\n\nprint(deletes)\nprint('*** which is the same as ***')\nfor i in deletes:\n    print(i)\n\n['earz', 'darz', 'derz', 'deaz', 'dear']\n*** which is the same as ***\nearz\ndarz\nderz\ndeaz\ndear\n\n\n\n\nUngraded Exercise\nYou now have a list of candidate strings created after performing a delete edit.  Next step will be to filter this list for candidate words found in a vocabulary.  Given the example vocab below, can you think of a way to create a list of candidate words ?  Remember, you already have a list of candidate strings, some of which are certainly not actual words you might find in your vocabulary !   So from the above list earz, darz, derz, deaz, dear.  You’re really only interested in dear.\n\nvocab = ['dean','deer','dear','fries','and','coke']\nedits = list(deletes)\n\nprint('vocab : ', vocab)\nprint('edits : ', edits)\n\ncandidates=[]\n\n### START CODE HERE ###\ncandidates = set(vocab).intersection(edits)  # hint: 'set.intersection'\n### END CODE HERE ###\n\nprint('candidate words : ', candidates)\n\nvocab :  ['dean', 'deer', 'dear', 'fries', 'and', 'coke']\nedits :  ['earz', 'darz', 'derz', 'deaz', 'dear']\ncandidate words :  {'dear'}\n\n\nExpected Outcome:\nvocab : [‘dean’, ‘deer’, ‘dear’, ‘fries’, ‘and’, ‘coke’]  edits : [‘earz’, ‘darz’, ‘derz’, ‘deaz’, ‘dear’]  candidate words : {‘dear’}\n\n\nSummary\nYou’ve unpacked an integral part of the assignment by breaking down splits and edits, specifically looking at deletes here.  Implementation of the other edit types (insert, replace, switch) follows a similar methodology and should now feel somewhat familiar when you see them.  This bit of the code isn’t as intuitive as other sections, so well done!  You should now feel confident facing some of the more technical parts of the assignment at the end of the week.\n\n\n\n\nCitationBibTeX citation:@online{2025,\n  author = {},\n  title = {NLP {Course} 2 {Week} 1 {Lesson} : {Building} {The} {Model} -\n    {Lecture} {Exercise} 02},\n  date = {2025-01-30},\n  url = {https://orenbochman.github.io/notes-nlp/posts/c2w1/lab02.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“NLP Course 2 Week 1 Lesson : Building The Model - Lecture\nExercise 02.” 2025. January 30, 2025. https://orenbochman.github.io/notes-nlp/posts/c2w1/lab02.html."
  },
  {
    "objectID": "posts/c3w3/index.html",
    "href": "posts/c3w3/index.html",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "",
    "text": "course banner\n\n\nresources:\n\n(Chadha 2020) Aman Chadha’s Notes\nIbrahim Jelliti’s Notes\n\n\n\n\n\n\n\nReferences\n\nChadha, Aman. 2020. “Distilled Notes for the Natural Language Processing Specialization on Coursera (Offered by Deeplearning.ai).” https://www.aman.ai. www.aman.ai.\n\nCitationBibTeX citation:@online{2020,\n  author = {},\n  title = {Neural {Networks} for {Sentiment} {Analysis}},\n  date = {2020-10-23},\n  url = {https://orenbochman.github.io/notes-nlp/posts/c3w3/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“Neural Networks for Sentiment Analysis.” 2020. October 23,\n2020. https://orenbochman.github.io/notes-nlp/posts/c3w3/.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c1w4/lab01.html",
    "href": "posts/c1w4/lab01.html",
    "title": "Vector manipulation in Python",
    "section": "",
    "text": "course banner\nIn this lab, you will have the opportunity to practice once again with the NumPy library. This time, we will explore some advanced operations with arrays and matrices.\nAt the end of the previous module, we used PCA to transform a set of many variables into a set of only two uncorrelated variables. This process was made through a transformation of the data called rotation.\nIn this week’s assignment, you will need to find a transformation matrix from English to French vector space embeddings. Such a transformation matrix is nothing else but a matrix that rotates and scales vector spaces.\nIn this notebook, we will explain in detail the rotation transformation."
  },
  {
    "objectID": "posts/c1w4/lab01.html#transforming-vectors",
    "href": "posts/c1w4/lab01.html#transforming-vectors",
    "title": "Vector manipulation in Python",
    "section": "Transforming vectors",
    "text": "Transforming vectors\nThere are three main vector transformations: * Scaling * Translation * Rotation\nIn previous notebooks, we have applied the first two kinds of transformations. Now, let us learn how to use a fundamental transformation on vectors called rotation.\nThe rotation operation changes the direction of a vector, letting unaffected its dimensionality and its norm. Let us explain with some examples.\nIn the following cells, we will define a NumPy matrix and a NumPy array. Soon we will explain how this is related to matrix rotation.\n\nimport numpy as np                     # Import numpy for array manipulation\nimport matplotlib.pyplot as plt        # Import matplotlib for charts\nfrom utils_nb import plot_vectors      # Function to plot vectors (arrows)\n\n\nExample 1\n\n# Create a 2 x 2 matrix\nR = np.array([[2, 0],\n              [0, -2]])\n\n\nx = np.array([[1, 1]]) # Create a 1 x 2 matrix\n\nThe dot product between a vector and a square matrix produces a rotation and a scaling of the original vector.\nRemember that our recommended way to get the dot product in Python is np.dot(a, b):\n\ny = np.dot(x, R) # Apply the dot product between x and R\ny\n\narray([[ 2, -2]])\n\n\nWe are going to use Pyplot to inspect the effect of the rotation on 2D vectors visually. For that, we have created a function plot_vectors() that takes care of all the intricate parts of the visual formatting. The code for this function is inside the utils_nb.py file.\nNow we can plot the vector \\vec x = [1, 1] in a cartesian plane. The cartesian plane will be centered at [0,0] and its x and y limits will be between [-4, +4]\n\nplot_vectors([x], axes=[4, 4], fname='transform_x.svg')\n\n\n\n\n\n\n\n\nNow, let’s plot in the same system our vector \\vec x = [1, 1] and its dot product with the matrix\nRo = \\begin{bmatrix} 2 & 0 \\\\ 0 & -2 \\end{bmatrix}\ny = x \\cdot Ro = [[-2, 2]]\n\nplot_vectors([x, y], axes=[4, 4], fname='transformx_and_y.svg')\n\n\n\n\n\n\n\n\nNote that the output vector y (blue) is transformed in another vector.\n\n\nExample 2\nWe are going to use Pyplot to inspect the effect of the rotation on 2D vectors visually. For that, we have created a function that takes care of all the intricate parts of the visual formatting. The following procedure plots an arrow within a Pyplot canvas.\nData that is composed of 2 real attributes is telling to belong to a $ RxR $ or $ R^2 $ space. Rotation matrices in R^2 rotate a given vector \\vec x by a counterclockwise angle \\theta in a fixed coordinate system. Rotation matrices are of the form:\nRo = \\begin{bmatrix} cos \\theta & -sin \\theta \\\\ sin \\theta & cos \\theta \\end{bmatrix}\nThe trigonometric functions in Numpy require the angle in radians, not in degrees. In the next cell, we define a rotation matrix that rotates vectors by 45^o.\n\nangle = 100 * (np.pi / 180) #convert degrees to radians\n\nRo = np.array([[np.cos(angle), -np.sin(angle)],\n              [np.sin(angle), np.cos(angle)]])\n\nx2 = np.array([2, 2]).reshape(1, -1) # make it a row vector\ny2 = np.dot(x2, Ro)\n\nprint('Rotation matrix')\nprint(Ro)\nprint('\\nRotated vector')\nprint(y2)\n\nprint('\\n x2 norm', np.linalg.norm(x2))\nprint('\\n y2 norm', np.linalg.norm(y2))\nprint('\\n Rotation matrix norm', np.linalg.norm(Ro))\n\nRotation matrix\n[[-0.17364818 -0.98480775]\n [ 0.98480775 -0.17364818]]\n\nRotated vector\n[[ 1.62231915 -2.31691186]]\n\n x2 norm 2.8284271247461903\n\n y2 norm 2.82842712474619\n\n Rotation matrix norm 1.414213562373095\n\n\n\nplot_vectors([x2, y2], fname='transform_02.svg')\n\n\n\n\n\n\n\n\nSome points to note:\n\nThe norm of the input vector is the same as the norm of the output vector. Rotations matrices do not modify the norm of the vector, only its direction.\nThe norm of any R^2 rotation matrix is always \\sqrt 2 = 1.414221"
  },
  {
    "objectID": "posts/c1w4/lab01.html#frobenius-norm",
    "href": "posts/c1w4/lab01.html#frobenius-norm",
    "title": "Vector manipulation in Python",
    "section": "Frobenius Norm",
    "text": "Frobenius Norm\nThe Frobenius norm is the generalization to R^2 of the already known norm function for vectors\n\\| \\vec a \\| = \\sqrt {{\\vec a} \\cdot {\\vec a}} \nFor a given R^2 matrix A, the frobenius norm is defined as:\n\\|\\mathrm{A}\\|_{F} \\equiv \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n}\\left|a_{i j}\\right|^{2}}\n\nA = np.array([[2, 2],\n              [2, 2]])\n\nnp.square() is a way to square each element of a matrix. It must be equivalent to use the * operator in Numpy arrays.\n\nA_squared = np.square(A)\nA_squared\n\narray([[4, 4],\n       [4, 4]])\n\n\nNow you can sum over the elements of the resulting array, and then get the square root of the sum.\n\nA_Frobenius = np.sqrt(np.sum(A_squared))\nA_Frobenius\n\nnp.float64(4.0)\n\n\nThat was the extended version of the np.linalg.norm() function. You can check that it yields the same result.\n\nprint('Frobenius norm of the Rotation matrix')\nprint(np.sqrt(np.sum(Ro * Ro)), '== ', np.linalg.norm(Ro))\n\nFrobenius norm of the Rotation matrix\n1.414213562373095 ==  1.414213562373095\n\n\nCongratulations!! We’ve covered a few more matrix operations in this lab. This will come in handy in this week’s programming assignment!"
  },
  {
    "objectID": "posts/c1w4/index.html",
    "href": "posts/c1w4/index.html",
    "title": "Machine Translation and Document Search",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nFigure 1",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Machine Translation and Document Search via KNN"
    ]
  },
  {
    "objectID": "posts/c1w4/index.html#assignment",
    "href": "posts/c1w4/index.html#assignment",
    "title": "Machine Translation and Document Search",
    "section": "Assignment",
    "text": "Assignment\nAssignment\n\nReading: Bibliography\n\n(Jurafsky and Martin 2025) :Speech and Language Processing",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Machine Translation and Document Search via KNN"
    ]
  },
  {
    "objectID": "posts/c1lab2/index.html",
    "href": "posts/c1lab2/index.html",
    "title": "Building and Visualizing word frequencies",
    "section": "",
    "text": "course banner",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Lab on Building and Visualizing word frequencies"
    ]
  },
  {
    "objectID": "posts/c1lab2/index.html#setup",
    "href": "posts/c1lab2/index.html#setup",
    "title": "Building and Visualizing word frequencies",
    "section": "Setup",
    "text": "Setup\nLet’s import the required libraries for this lab:\n\nimport nltk                                  # Python library for NLP\nfrom nltk.corpus import twitter_samples      # sample Twitter dataset from NLTK\nimport matplotlib.pyplot as plt              # visualization library\nimport numpy as np                           # library for scientific computing and matrix operations\n\n\nImport some helper functions that we provided in the utils.py file:\n\nprocess_tweet(): Cleans the text, tokenizes it into separate words, removes stopwords, and converts words to stems.\nbuild_freqs(): This counts how often a word in the ‘corpus’ (the entire set of tweets) was associated with a positive label 1 or a negative label 0. It then builds the freqs dictionary, where each key is a (word,label) tuple, and the value is the count of its frequency within the corpus of tweets.\n\n\n# download the stopwords for the process_tweet function\n#nltk.download('stopwords')\ntry:\n    nltk.data.find('corpora/stopwords.zip')\nexcept:\n    nltk.download('stopwords')\n\n\n# import our convenience functions\nfrom utils import process_tweet, build_freqs\n\nZipFilePathPointer('/home/oren/nltk_data/corpora/stopwords.zip', '')",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Lab on Building and Visualizing word frequencies"
    ]
  },
  {
    "objectID": "posts/c1lab2/index.html#load-the-nltk-sample-dataset",
    "href": "posts/c1lab2/index.html#load-the-nltk-sample-dataset",
    "title": "Building and Visualizing word frequencies",
    "section": "Load the NLTK sample dataset",
    "text": "Load the NLTK sample dataset\nAs in the previous lab, we will be using the Twitter dataset from NLTK.\n\n# select the lists of positive and negative tweets\nall_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')\n\n# concatenate the lists, 1st part is the positive tweets followed by the negative\ntweets = all_positive_tweets + all_negative_tweets\n\n# let's see how many tweets we have\nprint(\"Number of tweets: \", len(tweets))\n\nNumber of tweets:  10000\n\n\nNext, we will build a labels array that matches the sentiments of our tweets. This data type works pretty much like a regular list but is optimized for computations and manipulation. The labels array will be composed of 10000 elements. The first 5000 will be filled with 1 labels denoting positive sentiments, and the next 5000 will be 0 labels denoting the opposite. We can do this easily with a series of operations provided by the numpy library:\n\nnp.ones() - create an array of 1’s\nnp.zeros() - create an array of 0’s\nnp.append() - concatenate arrays\n\n\n# make a numpy array representing labels of the tweets\nlabels = np.append(np.ones((len(all_positive_tweets))), np.zeros((len(all_negative_tweets))))",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Lab on Building and Visualizing word frequencies"
    ]
  },
  {
    "objectID": "posts/c1lab2/index.html#dictionaries",
    "href": "posts/c1lab2/index.html#dictionaries",
    "title": "Building and Visualizing word frequencies",
    "section": "Dictionaries",
    "text": "Dictionaries\nIn Python, a dictionary is a mutable and indexed collection. It stores items as key-value pairs and uses hash tables underneath to allow practically constant time lookups. In NLP, dictionaries are essential because it enables fast retrieval of items or containment checks even with thousands of entries in the collection.\n\nDefinition\nA dictionary in Python is declared using curly brackets. Look at the next example:\n\ndictionary = {'key1': 1, 'key2': 2}\n\nThe former line defines a dictionary with two entries. Keys and values can be almost any type (with a few restriction on keys), and in this case, we used strings. We can also use floats, integers, tuples, etc.\n\n\nAdding or editing entries\nNew entries can be inserted into dictionaries using square brackets. If the dictionary already contains the specified key, its value is overwritten.\n\n# Add a new entry\ndictionary['key3'] = -5\n\n# Overwrite the value of key1\ndictionary['key1'] = 0\nprint(dictionary)\n\n{'key1': 0, 'key2': 2, 'key3': -5}\n\n\n\n\nAccessing values and lookup keys\nPerforming dictionary lookups and retrieval are common tasks in NLP. There are two ways to do this:\n\nUsing square bracket notation: This form is allowed if the lookup key is in the dictionary. It produces an error otherwise.\nUsing the get() method: This allows us to set a default value if the dictionary key does not exist. Let us see these in action:\n\n\n# Square bracket lookup when the key exist\nprint(dictionary['key2'])\n\n2\n\n\nHowever, if the key is missing, the operation produce an error\nWhen using a square bracket lookup, it is common to use an if-else block to check for containment first (with the keyword in) before getting the item. On the other hand, you can use the .get() method if you want to set a default value when the key is not found. Let’s compare these in the cells below:\n\n# This prints a value\nif 'key1' in dictionary:\n    print(\"item found: \", dictionary['key1'])\nelse:\n    print('key1 is not defined')\n\n# Same as what you get with get\nprint(\"item found: \", dictionary.get('key1', -1))\n\nitem found:  0\nitem found:  0\n\n\n\n# This prints a message because the key is not found\nif 'key7' in dictionary:\n    print(dictionary['key7'])\nelse:\n    print('key does not exist!')\n\n# This prints -1 because the key is not found and we set the default to -1\nprint(dictionary.get('key7', -1))\n\nkey does not exist!\n-1",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Lab on Building and Visualizing word frequencies"
    ]
  },
  {
    "objectID": "posts/c1lab2/index.html#word-frequency-dictionary",
    "href": "posts/c1lab2/index.html#word-frequency-dictionary",
    "title": "Building and Visualizing word frequencies",
    "section": "Word frequency dictionary",
    "text": "Word frequency dictionary\nNow that we know the building blocks, let’s finally take a look at the build_freqs() function in utils.py. This is the function that creates the dictionary containing the word counts from each corpus.\ndef build_freqs(tweets, ys):\n    \"\"\"Build frequencies.\n    Input:\n        tweets: a list of tweets\n        ys: an m x 1 array with the sentiment label of each tweet\n            (either 0 or 1)\n    Output:\n        freqs: a dictionary mapping each (word, sentiment) pair to its\n        frequency\n    \"\"\"\n    # Convert np array to list since zip needs an iterable.\n    # The squeeze is necessary or the list ends up with one element.\n    # Also note that this is just a NOP if ys is already a list.\n    yslist = np.squeeze(ys).tolist()\n    # Start with an empty dictionary and populate it by looping over all tweets\n    # and over all processed words in each tweet.\n    freqs = {}\n    for y, tweet in zip(yslist, tweets):\n        for word in process_tweet(tweet):\n            pair = (word, y)\n            if pair in freqs:\n                freqs[pair] += 1\n            else:\n                freqs[pair] = 1    \n    return freqs\nYou can also do the for loop like this to make it a bit more compact:\n    for y, tweet in zip(yslist, tweets):\n        for word in process_tweet(tweet):\n            pair = (word, y)\n            freqs[pair] = freqs.get(pair, 0) + 1\nAs shown above, each key is a 2-element tuple containing a (word, y) pair. The word is an element in a processed tweet while y is an integer representing the corpus: 1 for the positive tweets and 0 for the negative tweets. The value associated with this key is the number of times that word appears in the specified corpus.\nFor example:\n\n# \"folowfriday\" appears 25 times in the positive tweets\n('followfriday', 1.0): 25\n\n# \"shame\" appears 19 times in the negative tweets\n'shame', 0.0): 19 \nNow, it is time to use the dictionary returned by the build_freqs() function. First, let us feed our tweets and labels lists then print a basic report:\n\n# create frequency dictionary\nfreqs = build_freqs(tweets, labels)\n\n# check data type\nprint(f'type(freqs) = {type(freqs)}')\n\n# check length of the dictionary\nprint(f'len(freqs) = {len(freqs)}')\n\ntype(freqs) = &lt;class 'dict'&gt;\nlen(freqs) = 13065\n\n\nNow print the frequency of each word depending on its class.\n\nprint(freqs)\n\n{('followfriday', 1.0): 25, ('top', 1.0): 32, ('engag', 1.0): 7, ('member', 1.0): 16, ('commun', 1.0): 33, ('week', 1.0): 83, (':)', 1.0): 3568, ('hey', 1.0): 76, ('jame', 1.0): 7, ('odd', 1.0): 2, (':/', 1.0): 5, ('pleas', 1.0): 97, ('call', 1.0): 37, ('contact', 1.0): 7, ('centr', 1.0): 2, ('02392441234', 1.0): 1, ('abl', 1.0): 8, ('assist', 1.0): 1, ('mani', 1.0): 33, ('thank', 1.0): 620, ('listen', 1.0): 16, ('last', 1.0): 47, ('night', 1.0): 68, ('bleed', 1.0): 2, ('amaz', 1.0): 51, ('track', 1.0): 5, ('scotland', 1.0): 2, ('congrat', 1.0): 21, ('yeaaah', 1.0): 1, ('yipppi', 1.0): 1, ('accnt', 1.0): 2, ('verifi', 1.0): 2, ('rqst', 1.0): 1, ('succeed', 1.0): 1, ('got', 1.0): 69, ('blue', 1.0): 9, ('tick', 1.0): 1, ('mark', 1.0): 1, ('fb', 1.0): 6, ('profil', 1.0): 2, ('15', 1.0): 5, ('day', 1.0): 246, ('one', 1.0): 129, ('irresist', 1.0): 2, ('flipkartfashionfriday', 1.0): 17, ('like', 1.0): 233, ('keep', 1.0): 68, ('love', 1.0): 400, ('custom', 1.0): 4, ('wait', 1.0): 70, ('long', 1.0): 36, ('hope', 1.0): 141, ('enjoy', 1.0): 75, ('happi', 1.0): 211, ('friday', 1.0): 116, ('lwwf', 1.0): 1, ('second', 1.0): 10, ('thought', 1.0): 29, ('’', 1.0): 21, ('enough', 1.0): 18, ('time', 1.0): 127, ('dd', 1.0): 1, ('new', 1.0): 143, ('short', 1.0): 7, ('enter', 1.0): 9, ('system', 1.0): 2, ('sheep', 1.0): 1, ('must', 1.0): 18, ('buy', 1.0): 11, ('jgh', 1.0): 4, ('go', 1.0): 148, ('bayan', 1.0): 1, (':d', 1.0): 629, ('bye', 1.0): 7, ('act', 1.0): 8, ('mischiev', 1.0): 1, ('etl', 1.0): 1, ('layer', 1.0): 1, ('in-hous', 1.0): 1, ('wareh', 1.0): 1, ('app', 1.0): 16, ('katamari', 1.0): 1, ('well', 1.0): 81, ('…', 1.0): 38, ('name', 1.0): 18, ('impli', 1.0): 1, (':p', 1.0): 138, ('influenc', 1.0): 18, ('big', 1.0): 33, ('...', 1.0): 289, ('juici', 1.0): 3, ('selfi', 1.0): 12, ('follow', 1.0): 381, ('perfect', 1.0): 24, ('alreadi', 1.0): 28, ('know', 1.0): 145, (\"what'\", 1.0): 17, ('great', 1.0): 171, ('opportun', 1.0): 23, ('junior', 1.0): 2, ('triathlet', 1.0): 1, ('age', 1.0): 2, ('12', 1.0): 5, ('13', 1.0): 6, ('gatorad', 1.0): 1, ('seri', 1.0): 5, ('get', 1.0): 206, ('entri', 1.0): 4, ('lay', 1.0): 4, ('greet', 1.0): 5, ('card', 1.0): 8, ('rang', 1.0): 3, ('print', 1.0): 3, ('today', 1.0): 108, ('job', 1.0): 41, (':-)', 1.0): 692, (\"friend'\", 1.0): 3, ('lunch', 1.0): 5, ('yummm', 1.0): 1, ('nostalgia', 1.0): 1, ('tb', 1.0): 2, ('ku', 1.0): 1, ('id', 1.0): 8, ('conflict', 1.0): 1, ('help', 1.0): 41, (\"here'\", 1.0): 25, ('screenshot', 1.0): 3, ('work', 1.0): 110, ('hi', 1.0): 173, ('liv', 1.0): 2, ('hello', 1.0): 59, ('need', 1.0): 78, ('someth', 1.0): 28, ('u', 1.0): 175, ('fm', 1.0): 2, ('twitter', 1.0): 29, ('—', 1.0): 27, ('sure', 1.0): 58, ('thing', 1.0): 69, ('dm', 1.0): 39, ('x', 1.0): 72, (\"i'v\", 1.0): 35, ('heard', 1.0): 9, ('four', 1.0): 5, ('season', 1.0): 9, ('pretti', 1.0): 20, ('dope', 1.0): 2, ('penthous', 1.0): 1, ('obv', 1.0): 1, ('gobigorgohom', 1.0): 1, ('fun', 1.0): 58, (\"y'all\", 1.0): 3, ('yeah', 1.0): 47, ('suppos', 1.0): 7, ('lol', 1.0): 64, ('chat', 1.0): 13, ('bit', 1.0): 20, ('youth', 1.0): 19, ('💅🏽', 1.0): 1, ('💋', 1.0): 2, ('seen', 1.0): 10, ('year', 1.0): 43, ('rest', 1.0): 12, ('goe', 1.0): 7, ('quickli', 1.0): 3, ('bed', 1.0): 16, ('music', 1.0): 21, ('fix', 1.0): 10, ('dream', 1.0): 20, ('spiritu', 1.0): 1, ('ritual', 1.0): 1, ('festiv', 1.0): 8, ('népal', 1.0): 1, ('begin', 1.0): 4, ('line-up', 1.0): 4, ('left', 1.0): 13, ('see', 1.0): 184, ('sarah', 1.0): 4, ('send', 1.0): 22, ('us', 1.0): 109, ('email', 1.0): 26, ('bitsy@bitdefender.com', 1.0): 1, (\"we'll\", 1.0): 20, ('asap', 1.0): 5, ('kik', 1.0): 22, ('hatessuc', 1.0): 1, ('32429', 1.0): 1, ('kikm', 1.0): 1, ('lgbt', 1.0): 2, ('tinder', 1.0): 1, ('nsfw', 1.0): 1, ('akua', 1.0): 1, ('cumshot', 1.0): 1, ('come', 1.0): 70, ('hous', 1.0): 7, ('nsn_supplement', 1.0): 1, ('effect', 1.0): 4, ('press', 1.0): 1, ('releas', 1.0): 11, ('distribut', 1.0): 1, ('result', 1.0): 2, ('link', 1.0): 18, ('remov', 1.0): 3, ('pressreleas', 1.0): 1, ('newsdistribut', 1.0): 1, ('bam', 1.0): 44, ('bestfriend', 1.0): 50, ('lot', 1.0): 87, ('warsaw', 1.0): 44, ('&lt;3', 1.0): 134, ('x46', 1.0): 1, ('everyon', 1.0): 58, ('watch', 1.0): 46, ('documentari', 1.0): 1, ('earthl', 1.0): 2, ('youtub', 1.0): 13, ('support', 1.0): 27, ('buuut', 1.0): 1, ('oh', 1.0): 53, ('look', 1.0): 137, ('forward', 1.0): 29, ('visit', 1.0): 30, ('next', 1.0): 48, ('letsgetmessi', 1.0): 1, ('jo', 1.0): 1, ('make', 1.0): 99, ('feel', 1.0): 46, ('better', 1.0): 52, ('never', 1.0): 36, ('anyon', 1.0): 11, ('kpop', 1.0): 1, ('flesh', 1.0): 1, ('good', 1.0): 238, ('girl', 1.0): 44, ('best', 1.0): 65, ('wish', 1.0): 37, ('reason', 1.0): 13, ('epic', 1.0): 2, ('soundtrack', 1.0): 1, ('shout', 1.0): 12, ('ad', 1.0): 14, ('video', 1.0): 34, ('playlist', 1.0): 5, ('would', 1.0): 84, ('dear', 1.0): 17, ('jordan', 1.0): 1, ('okay', 1.0): 39, ('fake', 1.0): 2, ('gameplay', 1.0): 2, (';)', 1.0): 27, ('haha', 1.0): 53, ('im', 1.0): 51, ('kid', 1.0): 18, ('stuff', 1.0): 13, ('exactli', 1.0): 6, ('product', 1.0): 12, ('line', 1.0): 6, ('etsi', 1.0): 1, ('shop', 1.0): 16, ('check', 1.0): 52, ('vacat', 1.0): 6, ('recharg', 1.0): 1, ('normal', 1.0): 6, ('charger', 1.0): 2, ('asleep', 1.0): 9, ('talk', 1.0): 45, ('sooo', 1.0): 6, ('someon', 1.0): 34, ('text', 1.0): 18, ('ye', 1.0): 77, ('bet', 1.0): 6, (\"he'll\", 1.0): 4, ('fit', 1.0): 3, ('hear', 1.0): 33, ('speech', 1.0): 1, ('piti', 1.0): 3, ('green', 1.0): 3, ('garden', 1.0): 7, ('midnight', 1.0): 1, ('sun', 1.0): 6, ('beauti', 1.0): 50, ('canal', 1.0): 1, ('dasvidaniya', 1.0): 1, ('till', 1.0): 18, ('scout', 1.0): 1, ('sg', 1.0): 1, ('futur', 1.0): 13, ('wlan', 1.0): 1, ('pro', 1.0): 5, ('confer', 1.0): 1, ('asia', 1.0): 1, ('chang', 1.0): 24, ('lollipop', 1.0): 1, ('🍭', 1.0): 1, ('nez', 1.0): 1, ('agnezmo', 1.0): 1, ('oley', 1.0): 1, ('mama', 1.0): 1, ('stand', 1.0): 8, ('stronger', 1.0): 1, ('god', 1.0): 20, ('misti', 1.0): 1, ('babi', 1.0): 20, ('cute', 1.0): 26, ('woohoo', 1.0): 3, (\"can't\", 1.0): 43, ('sign', 1.0): 11, ('yet', 1.0): 13, ('still', 1.0): 48, ('think', 1.0): 63, ('mka', 1.0): 5, ('liam', 1.0): 8, ('access', 1.0): 3, ('welcom', 1.0): 73, ('stat', 1.0): 60, ('arriv', 1.0): 67, ('1', 1.0): 75, ('unfollow', 1.0): 63, ('via', 1.0): 69, ('surpris', 1.0): 10, ('figur', 1.0): 5, ('happybirthdayemilybett', 1.0): 1, ('sweet', 1.0): 19, ('talent', 1.0): 5, ('2', 1.0): 58, ('plan', 1.0): 27, ('drain', 1.0): 1, ('gotta', 1.0): 5, ('timezon', 1.0): 1, ('parent', 1.0): 5, ('proud', 1.0): 12, ('least', 1.0): 16, ('mayb', 1.0): 18, ('sometim', 1.0): 13, ('grade', 1.0): 4, ('al', 1.0): 4, ('grand', 1.0): 4, ('manila_bro', 1.0): 2, ('chosen', 1.0): 1, ('let', 1.0): 68, ('around', 1.0): 17, ('..', 1.0): 128, ('side', 1.0): 15, ('world', 1.0): 27, ('eh', 1.0): 2, ('take', 1.0): 43, ('care', 1.0): 18, ('final', 1.0): 30, ('fuck', 1.0): 26, ('weekend', 1.0): 75, ('real', 1.0): 21, ('x45', 1.0): 1, ('join', 1.0): 23, ('hushedcallwithfraydo', 1.0): 1, ('gift', 1.0): 8, ('yeahhh', 1.0): 1, ('hushedpinwithsammi', 1.0): 2, ('event', 1.0): 8, ('might', 1.0): 27, ('luv', 1.0): 6, ('realli', 1.0): 79, ('appreci', 1.0): 31, ('share', 1.0): 46, ('wow', 1.0): 22, ('tom', 1.0): 5, ('gym', 1.0): 4, ('monday', 1.0): 9, ('invit', 1.0): 17, ('scope', 1.0): 5, ('friend', 1.0): 61, ('nude', 1.0): 2, ('sleep', 1.0): 45, ('birthday', 1.0): 74, ('want', 1.0): 96, ('t-shirt', 1.0): 3, ('cool', 1.0): 38, ('haw', 1.0): 1, ('phela', 1.0): 1, ('mom', 1.0): 10, ('obvious', 1.0): 2, ('princ', 1.0): 1, ('charm', 1.0): 1, ('stage', 1.0): 2, ('luck', 1.0): 30, ('tyler', 1.0): 2, ('hipster', 1.0): 1, ('glass', 1.0): 5, ('marti', 1.0): 2, ('glad', 1.0): 43, ('done', 1.0): 54, ('afternoon', 1.0): 10, ('read', 1.0): 34, ('kahfi', 1.0): 1, ('finish', 1.0): 17, ('ohmyg', 1.0): 1, ('yaya', 1.0): 3, ('dub', 1.0): 2, ('stalk', 1.0): 2, ('ig', 1.0): 3, ('gondooo', 1.0): 1, ('moo', 1.0): 2, ('tologooo', 1.0): 1, ('becom', 1.0): 10, ('detail', 1.0): 10, ('zzz', 1.0): 1, ('xx', 1.0): 42, ('physiotherapi', 1.0): 1, ('hashtag', 1.0): 5, ('💪', 1.0): 1, ('monica', 1.0): 1, ('miss', 1.0): 27, ('sound', 1.0): 23, ('morn', 1.0): 101, (\"that'\", 1.0): 67, ('x43', 1.0): 1, ('definit', 1.0): 23, ('tri', 1.0): 44, ('tonight', 1.0): 20, ('took', 1.0): 8, ('advic', 1.0): 6, ('treviso', 1.0): 1, ('concert', 1.0): 24, ('citi', 1.0): 27, ('countri', 1.0): 23, (\"i'll\", 1.0): 90, ('start', 1.0): 61, ('fine', 1.0): 10, ('gorgeou', 1.0): 12, ('xo', 1.0): 2, ('oven', 1.0): 3, ('roast', 1.0): 2, ('garlic', 1.0): 1, ('oliv', 1.0): 1, ('oil', 1.0): 4, ('dri', 1.0): 5, ('tomato', 1.0): 1, ('basil', 1.0): 1, ('centuri', 1.0): 1, ('tuna', 1.0): 1, ('right', 1.0): 47, ('back', 1.0): 98, ('atchya', 1.0): 1, ('even', 1.0): 35, ('almost', 1.0): 10, ('chanc', 1.0): 6, ('cheer', 1.0): 20, ('po', 1.0): 4, ('ice', 1.0): 6, ('cream', 1.0): 6, ('agre', 1.0): 16, ('100', 1.0): 8, ('heheheh', 1.0): 2, ('that', 1.0): 13, ('point', 1.0): 13, ('stay', 1.0): 25, ('home', 1.0): 31, ('soon', 1.0): 47, ('promis', 1.0): 6, ('web', 1.0): 4, ('whatsapp', 1.0): 5, ('volta', 1.0): 1, ('funcionar', 1.0): 1, ('com', 1.0): 2, ('iphon', 1.0): 7, ('jailbroken', 1.0): 1, ('later', 1.0): 16, ('34', 1.0): 3, ('min', 1.0): 9, ('leia', 1.0): 1, ('appear', 1.0): 3, ('hologram', 1.0): 1, ('r2d2', 1.0): 1, ('w', 1.0): 18, ('messag', 1.0): 10, ('obi', 1.0): 1, ('wan', 1.0): 3, ('sit', 1.0): 8, ('luke', 1.0): 6, ('inter', 1.0): 1, ('3', 1.0): 31, ('ucl', 1.0): 1, ('arsen', 1.0): 2, ('small', 1.0): 4, ('team', 1.0): 29, ('pass', 1.0): 12, ('🚂', 1.0): 1, ('dewsburi', 1.0): 2, ('railway', 1.0): 1, ('station', 1.0): 4, ('dew', 1.0): 1, ('west', 1.0): 3, ('yorkshir', 1.0): 2, ('430', 1.0): 1, ('smh', 1.0): 2, ('9:25', 1.0): 1, ('live', 1.0): 26, ('strang', 1.0): 4, ('imagin', 1.0): 5, ('megan', 1.0): 1, ('masaantoday', 1.0): 6, ('a4', 1.0): 3, ('shweta', 1.0): 1, ('tripathi', 1.0): 1, ('5', 1.0): 17, ('20', 1.0): 6, ('kurta', 1.0): 3, ('half', 1.0): 7, ('number', 1.0): 13, ('wsalelov', 1.0): 16, ('ah', 1.0): 13, ('larri', 1.0): 3, ('anyway', 1.0): 16, ('kinda', 1.0): 13, ('goood', 1.0): 4, ('life', 1.0): 49, ('enn', 1.0): 1, ('could', 1.0): 32, ('warmup', 1.0): 1, ('15th', 1.0): 2, ('bath', 1.0): 7, ('dum', 1.0): 2, ('andar', 1.0): 1, ('ram', 1.0): 1, ('sampath', 1.0): 1, ('sona', 1.0): 1, ('mohapatra', 1.0): 1, ('samantha', 1.0): 1, ('edward', 1.0): 1, ('mein', 1.0): 1, ('tulan', 1.0): 1, ('razi', 1.0): 2, ('wah', 1.0): 2, ('josh', 1.0): 1, ('alway', 1.0): 67, ('smile', 1.0): 62, ('pictur', 1.0): 12, ('16.20', 1.0): 1, ('giveitup', 1.0): 1, ('given', 1.0): 3, ('ga', 1.0): 3, ('subsidi', 1.0): 1, ('initi', 1.0): 4, ('propos', 1.0): 3, ('delight', 1.0): 7, ('yesterday', 1.0): 7, ('x42', 1.0): 1, ('lmaoo', 1.0): 2, ('song', 1.0): 22, ('ever', 1.0): 23, ('shall', 1.0): 6, ('littl', 1.0): 31, ('throwback', 1.0): 3, ('outli', 1.0): 1, ('island', 1.0): 5, ('cheung', 1.0): 1, ('chau', 1.0): 1, ('mui', 1.0): 1, ('wo', 1.0): 1, ('total', 1.0): 9, ('differ', 1.0): 11, ('kfckitchentour', 1.0): 2, ('kitchen', 1.0): 4, ('clean', 1.0): 1, (\"i'm\", 1.0): 183, ('cusp', 1.0): 1, ('test', 1.0): 7, ('water', 1.0): 8, ('reward', 1.0): 1, ('arummzz', 1.0): 2, (\"let'\", 1.0): 23, ('drive', 1.0): 11, ('travel', 1.0): 20, ('yogyakarta', 1.0): 3, ('jeep', 1.0): 3, ('indonesia', 1.0): 4, ('instamood', 1.0): 3, ('wanna', 1.0): 30, ('skype', 1.0): 3, ('may', 1.0): 22, ('nice', 1.0): 98, ('friendli', 1.0): 2, ('pretend', 1.0): 2, ('film', 1.0): 9, ('congratul', 1.0): 15, ('winner', 1.0): 4, ('cheesydelight', 1.0): 1, ('contest', 1.0): 6, ('address', 1.0): 10, ('guy', 1.0): 60, ('market', 1.0): 5, ('24/7', 1.0): 1, ('14', 1.0): 1, ('hour', 1.0): 27, ('leav', 1.0): 12, ('without', 1.0): 12, ('delay', 1.0): 2, ('actual', 1.0): 19, ('easi', 1.0): 9, ('guess', 1.0): 14, ('train', 1.0): 10, ('wd', 1.0): 1, ('shift', 1.0): 5, ('engin', 1.0): 2, ('etc', 1.0): 2, ('sunburn', 1.0): 1, ('peel', 1.0): 2, ('blog', 1.0): 31, ('huge', 1.0): 11, ('warm', 1.0): 6, ('☆', 1.0): 3, ('complet', 1.0): 11, ('triangl', 1.0): 2, ('northern', 1.0): 1, ('ireland', 1.0): 2, ('sight', 1.0): 1, ('smthng', 1.0): 2, ('fr', 1.0): 3, ('hug', 1.0): 13, ('xoxo', 1.0): 3, ('uu', 1.0): 1, ('jaann', 1.0): 1, ('topnewfollow', 1.0): 2, ('connect', 1.0): 14, ('wonder', 1.0): 35, ('made', 1.0): 53, ('fluffi', 1.0): 1, ('insid', 1.0): 8, ('pirouett', 1.0): 1, ('moos', 1.0): 1, ('trip', 1.0): 14, ('philli', 1.0): 1, ('decemb', 1.0): 3, (\"i'd\", 1.0): 20, ('dude', 1.0): 6, ('x41', 1.0): 1, ('question', 1.0): 17, ('flaw', 1.0): 1, ('pain', 1.0): 9, ('negat', 1.0): 1, ('strength', 1.0): 3, ('went', 1.0): 12, ('solo', 1.0): 4, ('move', 1.0): 12, ('fav', 1.0): 13, ('nirvana', 1.0): 1, ('smell', 1.0): 2, ('teen', 1.0): 3, ('spirit', 1.0): 3, ('rip', 1.0): 3, ('ami', 1.0): 4, ('winehous', 1.0): 1, ('coupl', 1.0): 9, ('tomhiddleston', 1.0): 1, ('elizabetholsen', 1.0): 1, ('yaytheylookgreat', 1.0): 1, ('goodnight', 1.0): 24, ('vid', 1.0): 11, ('wake', 1.0): 12, ('gonna', 1.0): 21, ('shoot', 1.0): 6, ('itti', 1.0): 2, ('bitti', 1.0): 2, ('teeni', 1.0): 2, ('bikini', 1.0): 3, ('much', 1.0): 89, ('4th', 1.0): 4, ('togeth', 1.0): 7, ('end', 1.0): 20, ('xfile', 1.0): 1, ('content', 1.0): 4, ('rain', 1.0): 21, ('fabul', 1.0): 5, ('fantast', 1.0): 13, ('♡', 1.0): 20, ('jb', 1.0): 1, ('forev', 1.0): 5, ('belieb', 1.0): 3, ('nighti', 1.0): 1, ('bug', 1.0): 3, ('bite', 1.0): 1, ('bracelet', 1.0): 2, ('idea', 1.0): 26, ('foundri', 1.0): 1, ('game', 1.0): 27, ('sens', 1.0): 7, ('pic', 1.0): 27, ('ef', 1.0): 1, ('phone', 1.0): 19, ('woot', 1.0): 2, ('derek', 1.0): 1, ('use', 1.0): 44, ('parkshar', 1.0): 1, ('gloucestershir', 1.0): 1, ('aaaahhh', 1.0): 1, ('man', 1.0): 23, ('traffic', 1.0): 2, ('stress', 1.0): 8, ('reliev', 1.0): 1, (\"how'r\", 1.0): 1, ('arbeloa', 1.0): 1, ('turn', 1.0): 15, ('17', 1.0): 4, ('omg', 1.0): 15, ('say', 1.0): 61, ('europ', 1.0): 1, ('rise', 1.0): 2, ('find', 1.0): 23, ('hard', 1.0): 12, ('believ', 1.0): 9, ('uncount', 1.0): 1, ('coz', 1.0): 3, ('unlimit', 1.0): 1, ('cours', 1.0): 18, ('teamposit', 1.0): 1, ('aldub', 1.0): 2, ('☕', 1.0): 3, ('rita', 1.0): 2, ('info', 1.0): 13, (\"we'd\", 1.0): 4, ('way', 1.0): 46, ('boy', 1.0): 21, ('x40', 1.0): 1, ('true', 1.0): 22, ('sethi', 1.0): 2, ('high', 1.0): 7, ('exe', 1.0): 1, ('skeem', 1.0): 1, ('saam', 1.0): 1, ('peopl', 1.0): 48, ('polit', 1.0): 2, ('izzat', 1.0): 1, ('wese', 1.0): 1, ('trust', 1.0): 9, ('khawateen', 1.0): 1, ('k', 1.0): 9, ('sath', 1.0): 2, ('mana', 1.0): 1, ('kar', 1.0): 1, ('deya', 1.0): 1, ('sort', 1.0): 9, ('smart', 1.0): 5, ('hair', 1.0): 12, ('tbh', 1.0): 5, ('jacob', 1.0): 2, ('g', 1.0): 10, ('upgrad', 1.0): 6, ('tee', 1.0): 2, ('famili', 1.0): 19, ('person', 1.0): 19, ('two', 1.0): 22, ('convers', 1.0): 6, ('onlin', 1.0): 7, ('mclaren', 1.0): 1, ('fridayfeel', 1.0): 5, ('tgif', 1.0): 10, ('squar', 1.0): 1, ('enix', 1.0): 1, ('bissmillah', 1.0): 1, ('ya', 1.0): 23, ('allah', 1.0): 3, (\"we'r\", 1.0): 29, ('socent', 1.0): 1, ('startup', 1.0): 2, ('drop', 1.0): 9, ('your', 1.0): 3, ('arnd', 1.0): 1, ('town', 1.0): 5, ('basic', 1.0): 4, ('piss', 1.0): 3, ('cup', 1.0): 4, ('also', 1.0): 35, ('terribl', 1.0): 2, ('complic', 1.0): 1, ('discuss', 1.0): 3, ('snapchat', 1.0): 36, ('lynettelow', 1.0): 1, ('kikmenow', 1.0): 3, ('snapm', 1.0): 2, ('hot', 1.0): 24, ('amazon', 1.0): 1, ('kikmeguy', 1.0): 3, ('defin', 1.0): 2, ('grow', 1.0): 7, ('sport', 1.0): 4, ('rt', 1.0): 12, ('rakyat', 1.0): 1, ('write', 1.0): 13, ('sinc', 1.0): 15, ('mention', 1.0): 24, ('fli', 1.0): 5, ('fish', 1.0): 3, ('promot', 1.0): 5, ('post', 1.0): 21, ('cyber', 1.0): 1, ('ourdaughtersourprid', 1.0): 5, ('mypapamyprid', 1.0): 2, ('papa', 1.0): 2, ('coach', 1.0): 2, ('posit', 1.0): 8, ('kha', 1.0): 1, ('atleast', 1.0): 2, ('x39', 1.0): 1, ('mango', 1.0): 1, (\"lassi'\", 1.0): 1, (\"monty'\", 1.0): 1, ('marvel', 1.0): 2, ('though', 1.0): 19, ('suspect', 1.0): 3, ('meant', 1.0): 3, ('24', 1.0): 4, ('hr', 1.0): 2, ('touch', 1.0): 15, ('kepler', 1.0): 4, ('452b', 1.0): 5, ('chalna', 1.0): 1, ('hai', 1.0): 11, ('thankyou', 1.0): 14, ('hazel', 1.0): 1, ('food', 1.0): 6, ('brooklyn', 1.0): 1, ('pta', 1.0): 2, ('awak', 1.0): 10, ('okayi', 1.0): 2, ('awww', 1.0): 15, ('ha', 1.0): 23, ('doc', 1.0): 1, ('splendid', 1.0): 1, ('spam', 1.0): 1, ('folder', 1.0): 1, ('amount', 1.0): 1, ('nigeria', 1.0): 1, ('claim', 1.0): 1, ('rted', 1.0): 1, ('leg', 1.0): 5, ('hurt', 1.0): 8, ('bad', 1.0): 18, ('mine', 1.0): 14, ('saturday', 1.0): 8, ('thaaank', 1.0): 1, ('puhon', 1.0): 1, ('happinesss', 1.0): 1, ('tnc', 1.0): 1, ('prior', 1.0): 1, ('notif', 1.0): 2, ('fat', 1.0): 1, ('co', 1.0): 1, ('probabl', 1.0): 9, ('ate', 1.0): 4, ('yuna', 1.0): 2, ('tamesid', 1.0): 1, ('´', 1.0): 3, ('googl', 1.0): 6, ('account', 1.0): 19, ('scouser', 1.0): 1, ('everyth', 1.0): 13, ('zoe', 1.0): 2, ('mate', 1.0): 7, ('liter', 1.0): 6, (\"they'r\", 1.0): 12, ('samee', 1.0): 1, ('edgar', 1.0): 1, ('updat', 1.0): 13, ('log', 1.0): 4, ('bring', 1.0): 17, ('abe', 1.0): 1, ('meet', 1.0): 34, ('x38', 1.0): 1, ('sigh', 1.0): 3, ('dreamili', 1.0): 1, ('pout', 1.0): 1, ('eye', 1.0): 14, ('quacketyquack', 1.0): 7, ('funni', 1.0): 19, ('happen', 1.0): 16, ('phil', 1.0): 1, ('em', 1.0): 3, ('del', 1.0): 1, ('rodder', 1.0): 1, ('els', 1.0): 10, ('play', 1.0): 46, ('newest', 1.0): 1, ('gamejam', 1.0): 1, ('irish', 1.0): 2, ('literatur', 1.0): 2, ('inaccess', 1.0): 2, (\"kareena'\", 1.0): 2, ('fan', 1.0): 30, ('brain', 1.0): 13, ('dot', 1.0): 11, ('braindot', 1.0): 11, ('fair', 1.0): 5, ('rush', 1.0): 1, ('either', 1.0): 11, ('brandi', 1.0): 1, ('18', 1.0): 5, ('carniv', 1.0): 1, ('men', 1.0): 10, ('put', 1.0): 17, ('mask', 1.0): 3, ('xavier', 1.0): 1, ('forneret', 1.0): 1, ('jennif', 1.0): 1, ('site', 1.0): 9, ('free', 1.0): 37, ('50.000', 1.0): 3, ('8', 1.0): 10, ('ball', 1.0): 7, ('pool', 1.0): 5, ('coin', 1.0): 5, ('edit', 1.0): 7, ('trish', 1.0): 1, ('♥', 1.0): 19, ('grate', 1.0): 5, ('three', 1.0): 10, ('comment', 1.0): 8, ('wakeup', 1.0): 1, ('besid', 1.0): 2, ('dirti', 1.0): 2, ('sex', 1.0): 6, ('lmaooo', 1.0): 1, ('😤', 1.0): 2, ('loui', 1.0): 4, (\"he'\", 1.0): 11, ('throw', 1.0): 3, ('caus', 1.0): 15, ('inspir', 1.0): 7, ('ff', 1.0): 48, ('twoof', 1.0): 3, ('gr8', 1.0): 1, ('wkend', 1.0): 3, ('kind', 1.0): 24, ('exhaust', 1.0): 2, ('word', 1.0): 20, ('cheltenham', 1.0): 1, ('area', 1.0): 4, ('kale', 1.0): 1, ('crisp', 1.0): 1, ('ruin', 1.0): 5, ('x37', 1.0): 1, ('open', 1.0): 12, ('worldwid', 1.0): 2, ('outta', 1.0): 1, ('sfvbeta', 1.0): 1, ('vantast', 1.0): 1, ('xcylin', 1.0): 1, ('bundl', 1.0): 1, ('show', 1.0): 28, ('internet', 1.0): 2, ('price', 1.0): 4, ('realisticli', 1.0): 1, ('pay', 1.0): 8, ('net', 1.0): 1, ('educ', 1.0): 1, ('power', 1.0): 7, ('weapon', 1.0): 1, ('nelson', 1.0): 1, ('mandela', 1.0): 1, ('recent', 1.0): 9, ('j', 1.0): 3, ('chenab', 1.0): 1, ('flow', 1.0): 5, ('pakistan', 1.0): 2, ('incredibleindia', 1.0): 1, ('teenchoic', 1.0): 10, ('choiceinternationalartist', 1.0): 9, ('superjunior', 1.0): 9, ('caught', 1.0): 4, ('first', 1.0): 50, ('salmon', 1.0): 3, ('super-blend', 1.0): 1, ('project', 1.0): 6, ('youth@bipolaruk.org.uk', 1.0): 1, ('awesom', 1.0): 42, ('stream', 1.0): 14, ('alma', 1.0): 1, ('mater', 1.0): 1, ('highschoolday', 1.0): 1, ('clientvisit', 1.0): 1, ('faith', 1.0): 3, ('christian', 1.0): 1, ('school', 1.0): 9, ('lizaminnelli', 1.0): 1, ('upcom', 1.0): 2, ('uk', 1.0): 4, ('😄', 1.0): 5, ('singl', 1.0): 6, ('hill', 1.0): 4, ('everi', 1.0): 26, ('beat', 1.0): 10, ('wrong', 1.0): 10, ('readi', 1.0): 25, ('natur', 1.0): 1, ('pefumeri', 1.0): 1, ('workshop', 1.0): 3, ('neal', 1.0): 1, ('yard', 1.0): 1, ('covent', 1.0): 1, ('tomorrow', 1.0): 40, ('fback', 1.0): 27, ('indo', 1.0): 1, ('harmo', 1.0): 1, ('americano', 1.0): 1, ('rememb', 1.0): 16, ('aww', 1.0): 10, ('head', 1.0): 14, ('saw', 1.0): 19, ('dark', 1.0): 6, ('handshom', 1.0): 1, ('juga', 1.0): 1, ('hurray', 1.0): 1, ('hate', 1.0): 13, ('cant', 1.0): 15, ('decid', 1.0): 4, ('save', 1.0): 12, ('list', 1.0): 15, ('hiya', 1.0): 4, ('exec', 1.0): 1, ('loryn.good@lincs-chamber.co.uk', 1.0): 1, ('photo', 1.0): 19, ('thx', 1.0): 15, ('4', 1.0): 24, ('china', 1.0): 2, ('homosexu', 1.0): 1, ('hyungbot', 1.0): 1, ('give', 1.0): 48, ('fam', 1.0): 5, ('mind', 1.0): 23, ('timetunnel', 1.0): 1, ('1982', 1.0): 1, ('quit', 1.0): 13, ('radio', 1.0): 5, ('set', 1.0): 11, ('heart', 1.0): 11, ('hiii', 1.0): 2, ('jack', 1.0): 3, ('ili', 1.0): 5, ('✨', 1.0): 4, ('domino', 1.0): 1, ('pub', 1.0): 1, ('heat', 1.0): 1, ('prob', 1.0): 5, ('sorri', 1.0): 22, ('hastili', 1.0): 1, ('type', 1.0): 6, ('came', 1.0): 7, ('pakistani', 1.0): 1, ('x36', 1.0): 1, ('3point', 1.0): 1, ('dreamteam', 1.0): 1, ('gooo', 1.0): 2, ('bailey', 1.0): 2, ('pbb', 1.0): 4, ('737gold', 1.0): 3, ('drank', 1.0): 2, ('old', 1.0): 13, ('gotten', 1.0): 2, ('1/2', 1.0): 1, ('welsh', 1.0): 1, ('wale', 1.0): 3, ('yippe', 1.0): 1, ('💟', 1.0): 4, ('bro', 1.0): 24, ('lord', 1.0): 4, ('michael', 1.0): 4, (\"u'r\", 1.0): 1, ('ure', 1.0): 1, ('bigot', 1.0): 1, ('usual', 1.0): 6, ('front', 1.0): 4, ('squat', 1.0): 1, ('dobar', 1.0): 1, ('dan', 1.0): 5, ('brand', 1.0): 8, ('heavi', 1.0): 2, ('musicolog', 1.0): 1, ('2015', 1.0): 16, ('spend', 1.0): 2, ('marathon', 1.0): 1, ('iflix', 1.0): 2, ('offici', 1.0): 10, ('graduat', 1.0): 3, ('cri', 1.0): 9, ('__', 1.0): 1, ('yep', 1.0): 9, ('expert', 1.0): 4, ('bisexu', 1.0): 1, ('minal', 1.0): 1, ('aidzin', 1.0): 1, ('yo', 1.0): 7, ('pi', 1.0): 1, ('cook', 1.0): 2, ('book', 1.0): 21, ('dinner', 1.0): 7, ('tough', 1.0): 2, ('choic', 1.0): 8, ('other', 1.0): 12, ('chill', 1.0): 6, ('smu', 1.0): 1, ('oval', 1.0): 1, ('basketbal', 1.0): 1, ('player', 1.0): 4, ('whahahaha', 1.0): 1, ('soamaz', 1.0): 1, ('moment', 1.0): 12, ('onto', 1.0): 3, ('a5', 1.0): 1, ('wardrob', 1.0): 2, ('user', 1.0): 3, ('teamr', 1.0): 1, ('appar', 1.0): 6, ('depend', 1.0): 2, ('greatli', 1.0): 1, ('design', 1.0): 21, ('ahhh', 1.0): 1, ('7th', 1.0): 1, ('cinepambata', 1.0): 1, ('mechan', 1.0): 1, ('form', 1.0): 4, ('download', 1.0): 10, ('ur', 1.0): 38, ('swisher', 1.0): 1, ('cop', 1.0): 1, ('ducktail', 1.0): 1, ('surreal', 1.0): 3, ('exposur', 1.0): 1, ('sotw', 1.0): 1, ('halesowen', 1.0): 1, ('blackcountryfair', 1.0): 1, ('street', 1.0): 1, ('assess', 1.0): 1, ('mental', 1.0): 4, ('bodi', 1.0): 15, ('ooz', 1.0): 1, ('appeal', 1.0): 1, ('amassiveoverdoseofship', 1.0): 1, ('latest', 1.0): 5, ('isi', 1.0): 1, ('chan', 1.0): 1, ('c', 1.0): 9, ('note', 1.0): 6, ('pkwalasawa', 1.0): 1, ('gemma', 1.0): 1, ('orlean', 1.0): 1, ('fever', 1.0): 2, ('geskenya', 1.0): 1, ('obamainkenya', 1.0): 1, ('magicalkenya', 1.0): 1, ('greatkenya', 1.0): 1, ('allgoodthingsk', 1.0): 1, ('anim', 1.0): 6, ('umaru', 1.0): 1, ('singer', 1.0): 4, ('ship', 1.0): 8, ('order', 1.0): 17, ('room', 1.0): 5, ('car', 1.0): 6, ('gone', 1.0): 5, ('hahaha', 1.0): 14, ('stori', 1.0): 11, ('relat', 1.0): 4, ('label', 1.0): 1, ('worst', 1.0): 3, ('batch', 1.0): 1, ('princip', 1.0): 1, ('due', 1.0): 3, ('march', 1.0): 1, ('wooftast', 1.0): 2, ('receiv', 1.0): 8, ('necessari', 1.0): 1, ('regret', 1.0): 4, ('rn', 1.0): 4, ('whatev', 1.0): 5, ('hat', 1.0): 1, ('success', 1.0): 6, ('abstin', 1.0): 1, ('wtf', 1.0): 3, (\"there'\", 1.0): 11, ('thrown', 1.0): 1, ('middl', 1.0): 2, ('repeat', 1.0): 3, ('relentlessli', 1.0): 1, ('approxim', 1.0): 1, ('oldschool', 1.0): 1, ('runescap', 1.0): 1, ('daaay', 1.0): 1, ('jumma_mubarik', 1.0): 1, ('frnd', 1.0): 1, ('stay_bless', 1.0): 1, ('bless', 1.0): 12, ('pussycat', 1.0): 1, ('main', 1.0): 7, ('launch', 1.0): 4, ('pretoria', 1.0): 1, ('fahrinahmad', 1.0): 1, ('tengkuaaronshah', 1.0): 1, ('eksperimencinta', 1.0): 1, ('tykkäsin', 1.0): 1, ('videosta', 1.0): 1, ('month', 1.0): 13, ('hoodi', 1.0): 2, ('eeep', 1.0): 1, ('yay', 1.0): 16, ('sohappyrightnow', 1.0): 1, ('mmm', 1.0): 1, ('azz-set', 1.0): 1, ('babe', 1.0): 9, ('feedback', 1.0): 11, ('gain', 1.0): 6, ('valu', 1.0): 2, ('peac', 1.0): 8, ('refresh', 1.0): 5, ('manthan', 1.0): 1, ('tune', 1.0): 5, ('fresh', 1.0): 6, ('mother', 1.0): 5, ('determin', 1.0): 2, ('maxfreshmov', 1.0): 2, ('loneliest', 1.0): 1, ('tattoo', 1.0): 3, ('friday.and', 1.0): 1, ('magnific', 1.0): 2, ('e', 1.0): 5, ('achiev', 1.0): 2, ('rashmi', 1.0): 1, ('dedic', 1.0): 2, ('happyfriday', 1.0): 6, ('nearli', 1.0): 4, ('retweet', 1.0): 35, ('alert', 1.0): 1, ('da', 1.0): 5, ('dang', 1.0): 2, ('rad', 1.0): 2, ('fanart', 1.0): 1, ('massiv', 1.0): 1, ('niamh', 1.0): 1, ('fennel', 1.0): 1, ('journal', 1.0): 1, ('land', 1.0): 2, ('copi', 1.0): 5, ('past', 1.0): 7, ('tweet', 1.0): 61, ('yesss', 1.0): 5, ('ariana', 1.0): 2, ('selena', 1.0): 2, ('gomez', 1.0): 1, ('tomlinson', 1.0): 1, ('payn', 1.0): 1, ('caradelevingn', 1.0): 1, ('🌷', 1.0): 1, ('trade', 1.0): 3, ('tire', 1.0): 5, ('nope', 1.0): 7, ('appli', 1.0): 6, ('iamca', 1.0): 1, ('found', 1.0): 15, ('afti', 1.0): 1, ('goodmorn', 1.0): 8, ('prokabaddi', 1.0): 1, ('koel', 1.0): 1, ('mallick', 1.0): 1, ('recit', 1.0): 4, ('nation', 1.0): 3, ('anthem', 1.0): 1, ('6', 1.0): 23, ('yournaturallead', 1.0): 1, ('youngnaturallead', 1.0): 1, ('mon', 1.0): 3, ('27juli', 1.0): 1, ('cumbria', 1.0): 1, ('flockstar', 1.0): 1, ('thur', 1.0): 2, ('30juli', 1.0): 1, ('itv', 1.0): 1, ('sleeptight', 1.0): 1, ('haveagoodday', 1.0): 1, ('septemb', 1.0): 5, ('perhap', 1.0): 3, ('bb', 1.0): 4, ('full', 1.0): 19, ('album', 1.0): 6, ('fulli', 1.0): 2, ('intend', 1.0): 1, ('possibl', 1.0): 7, ('attack', 1.0): 3, ('&gt;:d', 1.0): 4, ('bird', 1.0): 4, ('teamadmicro', 1.0): 1, ('fridaydownpour', 1.0): 1, ('clear', 1.0): 4, ('rohit', 1.0): 1, ('queen', 1.0): 8, ('otwolgrandtrail', 1.0): 3, ('sheer', 1.0): 1, ('fact', 1.0): 8, ('obama', 1.0): 1, ('innumer', 1.0): 1, ('presid', 1.0): 2, ('ni', 1.0): 3, ('shauri', 1.0): 1, ('yako', 1.0): 1, ('memotohat', 1.0): 1, ('sunday', 1.0): 9, ('pamper', 1.0): 2, (\"t'wa\", 1.0): 1, ('cabincrew', 1.0): 1, ('interview', 1.0): 5, ('langkawi', 1.0): 1, ('1st', 1.0): 1, ('august', 1.0): 7, ('fulfil', 1.0): 5, ('fantasi', 1.0): 6, ('👉', 1.0): 6, ('ex-tweleb', 1.0): 1, ('apart', 1.0): 2, ('makeov', 1.0): 1, ('brilliantli', 1.0): 1, ('happyyi', 1.0): 1, ('birthdaaayyy', 1.0): 2, ('kill', 1.0): 3, ('interest', 1.0): 20, ('internship', 1.0): 3, ('program', 1.0): 5, ('sadli', 1.0): 1, ('career', 1.0): 3, ('page', 1.0): 9, ('issu', 1.0): 10, ('sad', 1.0): 5, ('overwhelmingli', 1.0): 1, ('aha', 1.0): 2, ('beaut', 1.0): 2, ('♬', 1.0): 2, ('win', 1.0): 16, ('deo', 1.0): 1, ('faaabul', 1.0): 1, ('freebiefriday', 1.0): 4, ('aluminiumfre', 1.0): 1, ('stayfresh', 1.0): 1, ('john', 1.0): 6, ('worri', 1.0): 18, ('navig', 1.0): 1, ('thnk', 1.0): 1, ('progrmr', 1.0): 1, ('9pm', 1.0): 1, ('9am', 1.0): 2, ('hardli', 1.0): 1, ('rose', 1.0): 4, ('emot', 1.0): 3, ('poetri', 1.0): 1, ('frequentfly', 1.0): 1, ('break', 1.0): 10, ('apolog', 1.0): 4, ('kb', 1.0): 1, ('londondairi', 1.0): 1, ('icecream', 1.0): 2, ('experi', 1.0): 7, ('cover', 1.0): 9, ('sin', 1.0): 1, ('excit', 1.0): 33, (\":')\", 1.0): 2, ('xxx', 1.0): 15, ('jim', 1.0): 1, ('chuckl', 1.0): 1, ('cake', 1.0): 10, ('doh', 1.0): 1, ('500', 1.0): 2, ('subscrib', 1.0): 2, ('reach', 1.0): 1, ('scorch', 1.0): 1, ('summer', 1.0): 17, ('younger', 1.0): 4, ('woman', 1.0): 4, ('stamina', 1.0): 1, ('expect', 1.0): 6, ('anyth', 1.0): 22, ('less', 1.0): 8, ('tweeti', 1.0): 1, ('fab', 1.0): 12, ('dont', 1.0): 13, ('--&gt;', 1.0): 2, ('10', 1.0): 16, ('loner', 1.0): 3, ('introduc', 1.0): 3, ('vs', 1.0): 4, ('alter', 1.0): 1, ('understand', 1.0): 6, ('spread', 1.0): 8, ('problem', 1.0): 19, ('supa', 1.0): 1, ('dupa', 1.0): 1, ('near', 1.0): 6, ('dartmoor', 1.0): 1, ('gold', 1.0): 7, ('colour', 1.0): 4, ('ok', 1.0): 38, ('someday', 1.0): 4, ('r', 1.0): 14, ('dii', 1.0): 1, ('n', 1.0): 17, ('forget', 1.0): 17, ('si', 1.0): 4, ('smf', 1.0): 1, ('ft', 1.0): 4, ('japanes', 1.0): 3, ('import', 1.0): 5, ('kitti', 1.0): 1, ('match', 1.0): 6, ('stationari', 1.0): 1, ('draw', 1.0): 6, ('close', 1.0): 14, ('broken', 1.0): 3, ('specialis', 1.0): 4, ('thermal', 1.0): 4, ('imag', 1.0): 6, ('survey', 1.0): 4, ('–', 1.0): 14, ('south', 1.0): 2, ('korea', 1.0): 3, ('scamper', 1.0): 1, ('slept', 1.0): 4, ('alarm', 1.0): 1, (\"ain't\", 1.0): 5, ('mad', 1.0): 4, ('chweina', 1.0): 1, ('xd', 1.0): 4, ('jotzh', 1.0): 1, ('wast', 1.0): 7, ('place', 1.0): 21, ('worth', 1.0): 11, ('coat', 1.0): 3, ('beforehand', 1.0): 1, ('tho', 1.0): 12, ('foh', 1.0): 2, ('outsid', 1.0): 5, ('holiday', 1.0): 11, ('menac', 1.0): 1, ('jojo', 1.0): 2, ('ta', 1.0): 2, ('accept', 1.0): 1, ('admin', 1.0): 2, ('lukri', 1.0): 1, ('😘', 1.0): 10, ('momma', 1.0): 2, ('bear', 1.0): 2, ('❤', 1.0): 29, ('️', 1.0): 20, ('redid', 1.0): 1, ('8th', 1.0): 1, ('v.ball', 1.0): 1, ('atm', 1.0): 4, ('build', 1.0): 8, ('pack', 1.0): 8, ('suitcas', 1.0): 2, ('hang-copi', 1.0): 1, ('translat', 1.0): 1, (\"dostoevsky'\", 1.0): 1, ('voucher', 1.0): 2, ('bugatti', 1.0): 1, ('bra', 1.0): 3, ('مطعم_هاشم', 1.0): 1, ('yummi', 1.0): 3, ('a7la', 1.0): 1, ('bdayt', 1.0): 1, ('mnwreeen', 1.0): 1, ('jazz', 1.0): 2, ('truck', 1.0): 1, ('x34', 1.0): 1, ('speak', 1.0): 8, ('pbevent', 1.0): 1, ('hq', 1.0): 1, ('add', 1.0): 22, ('yoona', 1.0): 1, ('hairpin', 1.0): 1, ('otp', 1.0): 1, ('collect', 1.0): 7, ('mastership', 1.0): 1, ('honey', 1.0): 4, ('paindo', 1.0): 1, ('await', 1.0): 1, ('report', 1.0): 3, ('manni', 1.0): 1, ('asshol', 1.0): 3, ('brijresid', 1.0): 1, ('structur', 1.0): 1, ('156', 1.0): 1, ('unit', 1.0): 3, ('encompass', 1.0): 1, ('bhk', 1.0): 1, ('flat', 1.0): 2, ('91', 1.0): 2, ('975-580-', 1.0): 1, ('444', 1.0): 1, ('honor', 1.0): 2, ('curri', 1.0): 2, ('clash', 1.0): 1, ('milano', 1.0): 1, ('👌', 1.0): 1, ('followback', 1.0): 6, (':-d', 1.0): 5, ('legit', 1.0): 1, ('loser', 1.0): 5, ('gass', 1.0): 1, ('dead', 1.0): 4, ('starsquad', 1.0): 4, ('⭐', 1.0): 3, ('news', 1.0): 25, ('utc', 1.0): 1, ('flume', 1.0): 1, ('kaytranada', 1.0): 1, ('alunageorg', 1.0): 1, ('ticket', 1.0): 12, ('km', 1.0): 1, ('certainti', 1.0): 1, ('solv', 1.0): 2, ('faster', 1.0): 3, ('👊', 1.0): 1, ('hurri', 1.0): 5, ('totem', 1.0): 1, ('somewher', 1.0): 5, ('alic', 1.0): 4, ('dog', 1.0): 6, ('cat', 1.0): 5, ('goodwynsgoodi', 1.0): 1, ('ugh', 1.0): 1, ('fade', 1.0): 2, ('moan', 1.0): 1, ('leed', 1.0): 1, ('jozi', 1.0): 1, ('wasnt', 1.0): 2, ('fifth', 1.0): 2, ('avail', 1.0): 10, ('tix', 1.0): 2, ('pa', 1.0): 2, ('ba', 1.0): 2, ('ng', 1.0): 2, ('atl', 1.0): 1, ('coldplay', 1.0): 1, ('favorit', 1.0): 14, ('scientist', 1.0): 1, ('yellow', 1.0): 2, ('atla', 1.0): 1, ('yein', 1.0): 1, ('selo', 1.0): 1, ('jabongatpumaurbanstamped', 1.0): 4, ('an', 1.0): 3, ('7', 1.0): 8, ('waiter', 1.0): 1, ('bill', 1.0): 5, ('sir', 1.0): 12, ('titl', 1.0): 2, ('pocket', 1.0): 1, ('wrip', 1.0): 1, ('jean', 1.0): 1, ('conni', 1.0): 2, ('crew', 1.0): 3, ('staff', 1.0): 2, ('sweetan', 1.0): 1, ('ask', 1.0): 37, ('mum', 1.0): 2, ('beg', 1.0): 2, ('soprano', 1.0): 1, ('ukrain', 1.0): 2, ('x33', 1.0): 1, ('olli', 1.0): 2, ('disney.art', 1.0): 1, ('elmoprinssi', 1.0): 1, ('salsa', 1.0): 1, ('danc', 1.0): 2, ('tell', 1.0): 25, ('truth', 1.0): 4, ('pl', 1.0): 8, ('4-6', 1.0): 1, ('2nd', 1.0): 5, ('blogiversari', 1.0): 1, ('review', 1.0): 9, ('cuti', 1.0): 6, ('bohol', 1.0): 1, ('briliant', 1.0): 1, ('v', 1.0): 9, ('key', 1.0): 3, ('annual', 1.0): 1, ('far', 1.0): 19, ('spin', 1.0): 2, ('voic', 1.0): 3, ('\\U000fe334', 1.0): 1, ('yeheyi', 1.0): 1, ('pinya', 1.0): 1, ('whoooah', 1.0): 1, ('tranc', 1.0): 1, ('lover', 1.0): 4, ('subject', 1.0): 7, ('physic', 1.0): 1, ('stop', 1.0): 15, ('ब', 1.0): 1, ('matter', 1.0): 6, ('jungl', 1.0): 1, ('accommod', 1.0): 1, ('secret', 1.0): 9, ('behind', 1.0): 3, ('sandroforceo', 1.0): 2, ('ceo', 1.0): 11, ('1month', 1.0): 11, ('swag', 1.0): 1, ('mia', 1.0): 1, ('workinprogress', 1.0): 1, ('choos', 1.0): 2, ('finnigan', 1.0): 1, ('loyal', 1.0): 2, ('royal', 1.0): 2, ('fotoset', 1.0): 1, ('reus', 1.0): 1, ('seem', 1.0): 10, ('somebodi', 1.0): 1, ('sell', 1.0): 1, ('young', 1.0): 3, ('muntu', 1.0): 1, ('anoth', 1.0): 23, ('gem', 1.0): 2, ('falco', 1.0): 1, ('supersmash', 1.0): 1, ('hotnsexi', 1.0): 1, ('friskyfriday', 1.0): 1, ('beach', 1.0): 4, ('movi', 1.0): 24, ('crop', 1.0): 2, ('nash', 1.0): 1, ('tissu', 1.0): 1, ('chocol', 1.0): 7, ('tea', 1.0): 6, ('hannib', 1.0): 3, ('episod', 1.0): 5, ('hotb', 1.0): 1, ('bush', 1.0): 2, ('classicassur', 1.0): 1, ('thrill', 1.0): 2, ('intern', 1.0): 2, ('assign', 1.0): 1, ('aerial', 1.0): 1, ('camera', 1.0): 6, ('oper', 1.0): 1, ('boom', 1.0): 3, ('hong', 1.0): 1, ('kong', 1.0): 1, ('ferri', 1.0): 1, ('central', 1.0): 2, ('girlfriend', 1.0): 4, ('after-work', 1.0): 1, ('drink', 1.0): 8, ('dj', 1.0): 3, ('resto', 1.0): 1, ('drinkt', 1.0): 1, ('koffi', 1.0): 1, ('a6', 1.0): 1, ('stargat', 1.0): 1, ('atlanti', 1.0): 1, ('muaahhh', 1.0): 1, ('ohh', 1.0): 3, ('hii', 1.0): 2, ('🙈', 1.0): 1, ('di', 1.0): 5, ('nagsend', 1.0): 1, ('yung', 1.0): 1, ('ko', 1.0): 4, ('&lt;/3', 1.0): 1, ('ulit', 1.0): 3, ('🎉', 1.0): 5, ('🎈', 1.0): 1, ('ugli', 1.0): 4, ('legget', 1.0): 1, ('qui', 1.0): 1, ('per', 1.0): 1, ('la', 1.0): 8, ('mar', 1.0): 1, ('encourag', 1.0): 3, ('employ', 1.0): 5, ('board', 1.0): 5, ('sticker', 1.0): 1, ('sponsor', 1.0): 4, ('prize', 1.0): 3, ('(:', 1.0): 1, ('milo', 1.0): 1, ('aurini', 1.0): 1, ('juicebro', 1.0): 1, ('pillar', 1.0): 2, ('respect', 1.0): 2, ('boii', 1.0): 1, ('smashingbook', 1.0): 1, ('bibl', 1.0): 2, ('ill', 1.0): 6, ('sick', 1.0): 4, ('lamo', 1.0): 1, ('fangirl', 1.0): 3, ('platon', 1.0): 1, ('scienc', 1.0): 5, ('resid', 1.0): 2, ('servicewithasmil', 1.0): 1, ('bloodlin', 1.0): 1, ('huski', 1.0): 1, ('obituari', 1.0): 1, ('advert', 1.0): 1, ('goofingaround', 1.0): 1, ('bollywood', 1.0): 1, ('giveaway', 1.0): 6, ('dah', 1.0): 2, ('noth', 1.0): 15, ('bitter', 1.0): 2, ('anger', 1.0): 1, ('hatr', 1.0): 2, ('toward', 1.0): 2, ('pure', 1.0): 2, ('indiffer', 1.0): 1, ('suit', 1.0): 5, ('zach', 1.0): 1, ('codi', 1.0): 2, ('deliv', 1.0): 3, ('ac', 1.0): 1, ('excel', 1.0): 6, ('produc', 1.0): 1, ('boggl', 1.0): 1, ('fatigu', 1.0): 1, ('baareeq', 1.0): 1, ('gamedev', 1.0): 2, ('hobbi', 1.0): 1, ('tweenie_fox', 1.0): 1, ('click', 1.0): 3, ('accessori', 1.0): 1, ('tamang', 1.0): 1, ('hinala', 1.0): 1, ('niam', 1.0): 1, ('selfiee', 1.0): 1, ('especi', 1.0): 4, ('lass', 1.0): 1, ('ale', 1.0): 1, ('swim', 1.0): 3, ('bout', 1.0): 3, ('goodby', 1.0): 5, ('feminist', 1.0): 1, ('fought', 1.0): 1, ('snobbi', 1.0): 1, ('bitch', 1.0): 3, ('carolin', 1.0): 2, ('mighti', 1.0): 1, ('🔥', 1.0): 1, ('threw', 1.0): 2, ('hbd', 1.0): 1, ('follback', 1.0): 19, ('jog', 1.0): 1, ('remot', 1.0): 2, ('newli', 1.0): 1, ('ebay', 1.0): 2, ('store', 1.0): 15, ('disneyinfin', 1.0): 1, ('starwar', 1.0): 1, ('charact', 1.0): 3, ('preorder', 1.0): 1, ('starter', 1.0): 1, ('hit', 1.0): 13, ('snap', 1.0): 4, ('homi', 1.0): 3, ('bought', 1.0): 4, ('skin', 1.0): 8, ('bday', 1.0): 11, ('chant', 1.0): 2, ('jai', 1.0): 1, ('itali', 1.0): 2, ('fast', 1.0): 4, ('heeeyyy', 1.0): 1, ('woah', 1.0): 3, ('★', 1.0): 5, ('😊', 1.0): 11, ('whenev', 1.0): 4, ('ang', 1.0): 2, ('kiss', 1.0): 4, ('philippin', 1.0): 2, ('packag', 1.0): 3, ('bruis', 1.0): 1, ('rib', 1.0): 2, ('😀', 1.0): 2, ('😁', 1.0): 6, ('😂', 1.0): 17, ('😃', 1.0): 1, ('😅', 1.0): 1, ('😉', 1.0): 2, ('tombraid', 1.0): 1, ('hype', 1.0): 1, ('thejuiceinthemix', 1.0): 1, ('rela', 1.0): 1, ('low', 1.0): 6, ('prioriti', 1.0): 1, ('harri', 1.0): 5, ('bc', 1.0): 9, ('collaps', 1.0): 2, ('chaotic', 1.0): 1, ('cosa', 1.0): 1, ('&lt;---', 1.0): 2, ('alliter', 1.0): 1, ('oppayaa', 1.0): 1, (\"how'\", 1.0): 4, ('natgeo', 1.0): 1, ('lick', 1.0): 1, ('elbow', 1.0): 2, ('. .', 1.0): 2, ('“', 1.0): 7, ('emu', 1.0): 1, ('stoke', 1.0): 1, ('woke', 1.0): 5, (\"people'\", 1.0): 3, ('approv', 1.0): 6, (\"god'\", 1.0): 2, ('jisung', 1.0): 1, ('sunshin', 1.0): 7, ('mm', 1.0): 6, ('nicola', 1.0): 1, ('brighten', 1.0): 2, ('helen', 1.0): 3, ('brian', 1.0): 3, ('2-3', 1.0): 1, ('australia', 1.0): 5, ('ol', 1.0): 2, ('bone', 1.0): 1, ('creak', 1.0): 1, ('abuti', 1.0): 1, ('tweetland', 1.0): 1, ('android', 1.0): 3, ('xma', 1.0): 2, ('skyblock', 1.0): 1, ('bcaus', 1.0): 1, ('2009', 1.0): 1, ('die', 1.0): 10, ('twitch', 1.0): 5, ('sympathi', 1.0): 1, ('laugh', 1.0): 5, ('unniee', 1.0): 1, ('nuka', 1.0): 1, ('penacova', 1.0): 1, ('djset', 1.0): 1, ('edm', 1.0): 1, ('kizomba', 1.0): 1, ('latinhous', 1.0): 1, ('housemus', 1.0): 3, ('portug', 1.0): 1, ('wild', 1.0): 2, ('ride', 1.0): 6, ('anytim', 1.0): 6, ('tast', 1.0): 5, ('yer', 1.0): 2, ('mtn', 1.0): 2, ('maganda', 1.0): 1, ('mistress', 1.0): 2, ('saphir', 1.0): 1, ('busi', 1.0): 19, ('4000', 1.0): 1, ('instagram', 1.0): 7, ('among', 1.0): 5, ('coconut', 1.0): 1, ('sambal', 1.0): 1, ('mussel', 1.0): 1, ('recip', 1.0): 5, ('kalin', 1.0): 1, ('mixcloud', 1.0): 1, ('sarcasm', 1.0): 2, ('chelsea', 1.0): 3, ('he', 1.0): 2, ('useless', 1.0): 2, ('thursday', 1.0): 2, ('hang', 1.0): 3, ('hehe', 1.0): 10, ('said', 1.0): 16, ('benson', 1.0): 1, ('facebook', 1.0): 5, ('solid', 1.0): 1, ('16/17', 1.0): 1, ('30', 1.0): 3, ('°', 1.0): 1, ('😜', 1.0): 2, ('maryhick', 1.0): 1, ('kikmeboy', 1.0): 7, ('photooftheday', 1.0): 4, ('musicbiz', 1.0): 2, ('sheskindahot', 1.0): 1, ('fleekil', 1.0): 1, ('mbalula', 1.0): 1, ('africa', 1.0): 1, ('mexican', 1.0): 1, ('scar', 1.0): 1, ('offic', 1.0): 8, ('donut', 1.0): 2, ('foiegra', 1.0): 2, ('despit', 1.0): 2, ('weather', 1.0): 9, ('wed', 1.0): 5, ('toni', 1.0): 2, ('stark', 1.0): 1, ('incred', 1.0): 7, ('poem', 1.0): 2, ('bubbl', 1.0): 3, ('dale', 1.0): 1, ('billion', 1.0): 1, ('magic', 1.0): 5, ('op', 1.0): 3, ('cast', 1.0): 1, ('vote', 1.0): 9, ('elect', 1.0): 1, ('jcreport', 1.0): 1, ('piggin', 1.0): 1, ('botan', 1.0): 2, ('soap', 1.0): 4, ('late', 1.0): 13, ('upload', 1.0): 5, ('freshli', 1.0): 1, ('3week', 1.0): 1, ('heal', 1.0): 1, ('tobi-bro', 1.0): 1, ('isp', 1.0): 1, ('steel', 1.0): 1, ('wednesday', 1.0): 1, ('swear', 1.0): 3, ('met', 1.0): 4, ('earlier', 1.0): 4, ('cam', 1.0): 3, ('😭', 1.0): 2, ('except', 1.0): 2, (\"masha'allah\", 1.0): 1, ('french', 1.0): 5, ('wwat', 1.0): 2, ('franc', 1.0): 5, ('yaaay', 1.0): 3, ('beirut', 1.0): 2, ('coffe', 1.0): 11, ('panda', 1.0): 6, ('eonni', 1.0): 2, ('favourit', 1.0): 13, ('soda', 1.0): 1, ('fuller', 1.0): 1, ('shit', 1.0): 13, ('healthi', 1.0): 2, ('💓', 1.0): 2, ('rettweet', 1.0): 3, ('mvg', 1.0): 1, ('valuabl', 1.0): 1, ('madrid', 1.0): 3, ('sore', 1.0): 6, ('bergerac', 1.0): 1, ('u21', 1.0): 1, ('individu', 1.0): 2, ('adam', 1.0): 1, (\"beach'\", 1.0): 1, ('suicid', 1.0): 1, ('squad', 1.0): 1, ('fond', 1.0): 1, ('christoph', 1.0): 2, ('cocki', 1.0): 1, ('prove', 1.0): 3, (\"attitude'\", 1.0): 1, ('improv', 1.0): 3, ('suggest', 1.0): 6, ('date', 1.0): 12, ('inde', 1.0): 10, ('intellig', 1.0): 3, ('strong', 1.0): 7, ('cs', 1.0): 2, ('certain', 1.0): 2, ('exam', 1.0): 5, ('forgot', 1.0): 3, ('home-bas', 1.0): 1, ('knee', 1.0): 4, ('sale', 1.0): 3, ('fleur', 1.0): 1, ('dress', 1.0): 10, ('readystock_hijabmart', 1.0): 1, ('idr', 1.0): 2, ('325.000', 1.0): 1, ('200.000', 1.0): 1, ('tompolo', 1.0): 1, ('aim', 1.0): 1, ('cannot', 1.0): 4, ('buyer', 1.0): 3, ('disappoint', 1.0): 1, ('paper', 1.0): 4, ('slack', 1.0): 1, ('crack', 1.0): 1, ('particularli', 1.0): 2, ('strike', 1.0): 1, ('31', 1.0): 1, ('mam', 1.0): 2, ('feytyaz', 1.0): 1, ('instant', 1.0): 1, ('stiffen', 1.0): 1, ('ricky_feb', 1.0): 1, ('grindea', 1.0): 1, ('courier', 1.0): 1, ('crypt', 1.0): 1, ('arma', 1.0): 1, ('record', 1.0): 5, ('gosh', 1.0): 2, ('limbo', 1.0): 1, ('orchard', 1.0): 1, ('art', 1.0): 10, ('super', 1.0): 15, ('karachi', 1.0): 2, ('ka', 1.0): 4, ('venic', 1.0): 1, ('sever', 1.0): 3, ('part', 1.0): 15, ('wit', 1.0): 2, ('accumul', 1.0): 1, ('maroon', 1.0): 1, ('cocktail', 1.0): 4, ('0-100', 1.0): 1, ('quick', 1.0): 7, ('1100d', 1.0): 1, ('auto-focu', 1.0): 1, ('manual', 1.0): 2, ('vein', 1.0): 1, ('crackl', 1.0): 1, ('glaze', 1.0): 1, ('layout', 1.0): 3, ('bomb', 1.0): 4, ('social', 1.0): 4, ('websit', 1.0): 8, ('pake', 1.0): 1, ('joim', 1.0): 1, ('feed', 1.0): 4, ('troop', 1.0): 1, ('mail', 1.0): 3, ('ladolcevitainluxembourg@hotmail.com', 1.0): 1, ('prrequest', 1.0): 1, ('journorequest', 1.0): 1, ('the_madstork', 1.0): 1, ('shaun', 1.0): 1, ('bot', 1.0): 4, ('chloe', 1.0): 2, ('actress', 1.0): 3, ('away', 1.0): 13, ('wick', 1.0): 9, ('hola', 1.0): 1, ('juan', 1.0): 1, ('houston', 1.0): 1, ('tx', 1.0): 2, ('jenni', 1.0): 1, (\"year'\", 1.0): 2, ('stumbl', 1.0): 1, ('upon', 1.0): 1, ('prob.nic', 1.0): 1, ('choker', 1.0): 1, ('btw', 1.0): 12, ('seouljin', 1.0): 1, ('photoset', 1.0): 3, ('sadomasochistsparadis', 1.0): 1, ('wynter', 1.0): 1, ('bottom', 1.0): 3, ('outtak', 1.0): 1, ('sadomasochist', 1.0): 1, ('paradis', 1.0): 1, ('ty', 1.0): 8, ('bbi', 1.0): 3, ('clip', 1.0): 1, ('lose', 1.0): 6, ('cypher', 1.0): 1, ('amen', 1.0): 2, ('x32', 1.0): 1, ('plant', 1.0): 4, ('allow', 1.0): 4, ('corner', 1.0): 3, ('addict', 1.0): 4, ('gurl', 1.0): 1, ('suck', 1.0): 9, ('special', 1.0): 8, ('owe', 1.0): 1, ('daniel', 1.0): 2, ('ape', 1.0): 1, ('saar', 1.0): 1, ('ahead', 1.0): 4, ('vers', 1.0): 1, ('butterfli', 1.0): 1, ('bonu', 1.0): 2, ('fill', 1.0): 5, ('tear', 1.0): 1, ('laughter', 1.0): 2, ('5so', 1.0): 6, ('yummmyyi', 1.0): 1, ('eat', 1.0): 6, ('dosa', 1.0): 1, ('easier', 1.0): 2, ('unless', 1.0): 3, ('achi', 1.0): 2, ('youuu', 1.0): 2, ('bawi', 1.0): 1, ('ako', 1.0): 1, ('queenesth', 1.0): 1, ('sharp', 1.0): 2, ('yess', 1.0): 1, ('poldi', 1.0): 1, ('cimbom', 1.0): 1, ('buddi', 1.0): 7, ('bruhhh', 1.0): 1, ('daddi', 1.0): 2, ('”', 1.0): 5, ('knowledg', 1.0): 2, ('attent', 1.0): 4, ('1tb', 1.0): 1, ('bank', 1.0): 1, ('credit', 1.0): 4, ('depart', 1.0): 2, ('anz', 1.0): 1, ('extrem', 1.0): 3, ('offshor', 1.0): 1, ('absolut', 1.0): 9, ('classic', 1.0): 3, ('gottolovebank', 1.0): 1, ('yup', 1.0): 6, ('in-shaa-allah', 1.0): 1, ('dua', 1.0): 1, ('thru', 1.0): 2, ('aameen', 1.0): 2, ('4/5', 1.0): 1, ('coca', 1.0): 1, ('cola', 1.0): 1, ('fanta', 1.0): 1, ('pepsi', 1.0): 1, ('sprite', 1.0): 1, ('all', 1.0): 1, ('sweeeti', 1.0): 1, (';-)', 1.0): 3, ('welcometweet', 1.0): 2, ('psygustokita', 1.0): 4, ('setup', 1.0): 1, ('wet', 1.0): 3, ('feet', 1.0): 3, ('carpet', 1.0): 1, ('judgment', 1.0): 1, ('hypocrit', 1.0): 1, ('narcissist', 1.0): 1, ('jumpsuit', 1.0): 1, ('bt', 1.0): 2, ('denim', 1.0): 1, ('verg', 1.0): 1, ('owl', 1.0): 1, ('constant', 1.0): 1, ('run', 1.0): 12, ('sia', 1.0): 1, ('count', 1.0): 7, ('brilliant', 1.0): 9, ('teacher', 1.0): 1, ('compar', 1.0): 2, ('religion', 1.0): 1, ('rant', 1.0): 1, ('student', 1.0): 6, ('bencher', 1.0): 1, ('1/5', 1.0): 1, ('porsch', 1.0): 1, ('paddock', 1.0): 1, ('budapestgp', 1.0): 1, ('johnyherbert', 1.0): 1, ('roll', 1.0): 5, ('porschesupercup', 1.0): 1, ('koyal', 1.0): 1, ('melodi', 1.0): 1, ('unexpect', 1.0): 4, ('creat', 1.0): 8, ('memori', 1.0): 3, ('35', 1.0): 1, ('ep', 1.0): 3, ('catch', 1.0): 10, ('wirh', 1.0): 1, ('arc', 1.0): 1, ('x31', 1.0): 1, ('wolv', 1.0): 2, ('desir', 1.0): 1, ('ameen', 1.0): 1, ('kca', 1.0): 1, ('votejkt', 1.0): 1, ('48id', 1.0): 1, ('helpinggroupdm', 1.0): 1, ('quot', 1.0): 6, ('weird', 1.0): 5, ('dp', 1.0): 1, ('wife', 1.0): 5, ('poor', 1.0): 4, ('chick', 1.0): 1, ('guid', 1.0): 3, ('zonzofox', 1.0): 3, ('bhaiya', 1.0): 1, ('brother', 1.0): 4, ('lucki', 1.0): 10, ('patti', 1.0): 1, ('elabor', 1.0): 1, ('kuch', 1.0): 1, ('rate', 1.0): 1, ('merdeka', 1.0): 1, ('palac', 1.0): 2, ('hotel', 1.0): 5, ('plusmil', 1.0): 1, ('servic', 1.0): 7, ('hahahaa', 1.0): 1, ('mean', 1.0): 25, ('nex', 1.0): 2, ('safe', 1.0): 5, ('gwd', 1.0): 1, ('she', 1.0): 2, ('okok', 1.0): 1, ('33', 1.0): 4, ('idiot', 1.0): 1, ('chaerin', 1.0): 1, ('unni', 1.0): 1, ('viabl', 1.0): 1, ('altern', 1.0): 3, ('nowaday', 1.0): 2, ('ip', 1.0): 1, ('tombow', 1.0): 1, ('abt', 1.0): 2, ('friyay', 1.0): 2, ('smug', 1.0): 1, ('marrickvil', 1.0): 1, ('public', 1.0): 3, ('ten', 1.0): 1, ('ago', 1.0): 8, ('eighteen', 1.0): 1, ('auvssscr', 1.0): 1, ('ncaaseason', 1.0): 1, ('slow', 1.0): 2, ('popsicl', 1.0): 1, ('soft', 1.0): 2, ('melt', 1.0): 1, ('mouth', 1.0): 2, ('thankyouuu', 1.0): 1, ('dianna', 1.0): 1, ('ngga', 1.0): 1, ('usah', 1.0): 1, ('dipikirin', 1.0): 1, ('elah', 1.0): 1, ('easili', 1.0): 1, (\"who'\", 1.0): 9, ('entp', 1.0): 1, ('killin', 1.0): 1, ('meme', 1.0): 1, ('worthi', 1.0): 1, ('shot', 1.0): 6, ('emon', 1.0): 1, ('decent', 1.0): 2, ('outdoor', 1.0): 1, ('rave', 1.0): 1, ('dv', 1.0): 1, ('aku', 1.0): 1, ('bakal', 1.0): 1, ('liat', 1.0): 1, ('kak', 1.0): 2, ('merri', 1.0): 1, ('tv', 1.0): 5, ('outfit', 1.0): 3, ('---&gt;', 1.0): 1, ('fashionfriday', 1.0): 1, ('angle.nelson', 1.0): 1, ('cheap', 1.0): 1, ('mymonsoonstori', 1.0): 2, ('tree', 1.0): 2, ('lotion', 1.0): 1, ('moistur', 1.0): 1, ('monsoon', 1.0): 1, ('whoop', 1.0): 6, ('romant', 1.0): 2, ('valencia', 1.0): 1, ('daaru', 1.0): 1, ('parti', 1.0): 12, ('chaddi', 1.0): 1, ('wonderful.great', 1.0): 1, ('trim', 1.0): 1, ('pube', 1.0): 1, ('es', 1.0): 2, ('mi', 1.0): 5, ('tio', 1.0): 1, ('sinaloa', 1.0): 1, ('arr', 1.0): 1, ('stylish', 1.0): 1, ('trendi', 1.0): 1, ('kim', 1.0): 5, ('fabfriday', 1.0): 2, ('facetim', 1.0): 4, ('calum', 1.0): 3, ('constantli', 1.0): 1, ('announc', 1.0): 1, ('filbarbarian', 1.0): 1, ('beer', 1.0): 3, ('arm', 1.0): 3, ('testicl', 1.0): 1, ('light', 1.0): 13, ('katerina', 1.0): 1, ('maniataki', 1.0): 1, ('ahh', 1.0): 5, ('alright', 1.0): 6, ('worthwhil', 1.0): 3, ('judg', 1.0): 2, ('tech', 1.0): 2, ('window', 1.0): 7, ('stupid', 1.0): 8, ('plugin', 1.0): 1, ('bass', 1.0): 1, ('slap', 1.0): 1, ('6pm', 1.0): 1, ('door', 1.0): 3, ('vip', 1.0): 1, ('gener', 1.0): 4, ('seat', 1.0): 2, ('earli', 1.0): 9, ('london', 1.0): 9, ('toptravelcentar', 1.0): 1, ('ttctop', 1.0): 1, ('lux', 1.0): 1, ('luxurytravel', 1.0): 1, ('beograd', 1.0): 1, ('srbija', 1.0): 1, ('putovanja', 1.0): 1, ('wendi', 1.0): 2, ('provid', 1.0): 4, ('drainag', 1.0): 1, ('homebound', 1.0): 1, ('hahahay', 1.0): 1, ('yeeeah', 1.0): 1, ('moar', 1.0): 2, ('kitteh', 1.0): 1, ('incom', 1.0): 1, ('tower', 1.0): 2, ('yippee', 1.0): 1, ('scrummi', 1.0): 1, ('bio', 1.0): 5, ('mcpe', 1.0): 1, ('-&gt;', 1.0): 1, ('vainglori', 1.0): 1, ('driver', 1.0): 1, ('6:01', 1.0): 1, ('lilydal', 1.0): 1, ('fss', 1.0): 1, ('rais', 1.0): 3, ('magicalmysterytour', 1.0): 1, ('chek', 1.0): 2, ('rule', 1.0): 2, ('weebli', 1.0): 1, ('donetsk', 1.0): 1, ('earth', 1.0): 7, ('personalis', 1.0): 1, ('wrap', 1.0): 2, ('stationeri', 1.0): 1, ('adrian', 1.0): 1, ('parcel', 1.0): 2, ('tuesday', 1.0): 7, ('pri', 1.0): 3, ('80', 1.0): 3, ('wz', 1.0): 1, ('pattern', 1.0): 1, ('cut', 1.0): 3, ('buttonhol', 1.0): 1, ('4mi', 1.0): 1, ('famou', 1.0): 1, ('client', 1.0): 1, ('p', 1.0): 3, ('aliv', 1.0): 2, ('trial', 1.0): 1, ('spm', 1.0): 1, ('dinooo', 1.0): 1, ('cardio', 1.0): 1, ('steak', 1.0): 1, ('cue', 1.0): 1, ('laptop', 1.0): 1, ('guinea', 1.0): 1, ('pig', 1.0): 1, ('salamat', 1.0): 1, ('sa', 1.0): 6, ('mga', 1.0): 1, ('nag.greet', 1.0): 1, ('guis', 1.0): 1, ('godbless', 1.0): 2, ('crush', 1.0): 3, ('appl', 1.0): 4, ('deserv', 1.0): 11, ('charl', 1.0): 1, ('workhard', 1.0): 1, ('model', 1.0): 7, ('forrit', 1.0): 1, ('bread', 1.0): 2, ('bacon', 1.0): 2, ('butter', 1.0): 2, ('afang', 1.0): 2, ('soup', 1.0): 2, ('semo', 1.0): 2, ('brb', 1.0): 1, ('forc', 1.0): 2, ('doesnt', 1.0): 5, ('tato', 1.0): 1, ('bulat', 1.0): 1, ('concern', 1.0): 1, ('snake', 1.0): 1, ('perform', 1.0): 3, ('con', 1.0): 1, ('todayyy', 1.0): 1, ('max', 1.0): 2, ('gaza', 1.0): 1, ('bbb', 1.0): 1, ('pc', 1.0): 3, ('22', 1.0): 2, ('legal', 1.0): 1, ('ditch', 1.0): 2, ('tori', 1.0): 1, ('bajrangibhaijaanhighestweek', 1.0): 6, (\"s'okay\", 1.0): 1, ('andi', 1.0): 2, ('you-and', 1.0): 1, ('return', 1.0): 3, ('tuitutil', 1.0): 1, ('bud', 1.0): 2, ('learn', 1.0): 8, ('takeaway', 1.0): 1, ('instead', 1.0): 7, ('1hr', 1.0): 1, ('genial', 1.0): 1, ('competit', 1.0): 1, ('yosh', 1.0): 1, ('procrastin', 1.0): 1, ('plu', 1.0): 4, ('kfc', 1.0): 2, ('itun', 1.0): 1, ('dedicatedfan', 1.0): 1, ('💜', 1.0): 7, ('daft', 1.0): 1, ('teeth', 1.0): 1, ('troubl', 1.0): 1, ('huxley', 1.0): 1, ('basket', 1.0): 2, ('ben', 1.0): 2, ('sent', 1.0): 8, ('gamer', 1.0): 3, ('activ', 1.0): 5, ('120', 1.0): 2, ('distanc', 1.0): 2, ('suitabl', 1.0): 1, ('stockholm', 1.0): 1, ('zack', 1.0): 1, ('destroy', 1.0): 1, ('heel', 1.0): 2, ('claw', 1.0): 1, ('q', 1.0): 2, ('blond', 1.0): 2, ('box', 1.0): 3, ('cheerio', 1.0): 1, ('seed', 1.0): 4, ('cutest', 1.0): 2, ('ffback', 1.0): 2, ('spotifi', 1.0): 3, (\"we'v\", 1.0): 7, ('vc', 1.0): 1, ('tgp', 1.0): 1, ('race', 1.0): 5, ('averag', 1.0): 2, (\"joe'\", 1.0): 1, ('bluejay', 1.0): 1, ('vinylbear', 1.0): 1, ('pal', 1.0): 1, ('furbabi', 1.0): 1, ('luff', 1.0): 1, ('mega', 1.0): 4, ('retail', 1.0): 4, ('boot', 1.0): 2, ('whsmith', 1.0): 1, ('ps3', 1.0): 1, ('shannon', 1.0): 1, ('na', 1.0): 9, ('redecor', 1.0): 1, ('bob', 1.0): 3, ('elli', 1.0): 4, ('mairi', 1.0): 1, ('workout', 1.0): 6, ('impair', 1.0): 1, ('uggghhh', 1.0): 1, ('dam', 1.0): 2, ('dun', 1.0): 2, ('eczema', 1.0): 1, ('suffer', 1.0): 4, ('ndee', 1.0): 1, ('pleasur', 1.0): 14, ('publiliu', 1.0): 1, ('syru', 1.0): 1, ('fear', 1.0): 1, ('death', 1.0): 3, ('dread', 1.0): 1, ('fell', 1.0): 3, ('fuk', 1.0): 1, ('unblock', 1.0): 1, ('tweak', 1.0): 2, ('php', 1.0): 1, ('fall', 1.0): 10, ('oomf', 1.0): 1, ('pippa', 1.0): 1, ('hschool', 1.0): 1, ('bu', 1.0): 3, ('cardi', 1.0): 1, ('everyday', 1.0): 3, ('everytim', 1.0): 3, ('hk', 1.0): 1, (\"why'd\", 1.0): 1, ('acorn', 1.0): 1, ('origin', 1.0): 7, ('c64', 1.0): 1, ('cpu', 1.0): 1, ('consider', 1.0): 1, ('advanc', 1.0): 1, ('onair', 1.0): 1, ('bay', 1.0): 1, ('hold', 1.0): 6, ('river', 1.0): 3, ('0878 0388', 1.0): 1, ('1033', 1.0): 1, ('0272 3306', 1.0): 1, ('70', 1.0): 5, ('rescu', 1.0): 1, ('mutt', 1.0): 1, ('confirm', 1.0): 3, ('deliveri', 1.0): 3, ('switch', 1.0): 2, ('lap', 1.0): 1, ('optim', 1.0): 1, ('lu', 1.0): 1, (':|', 1.0): 1, ('tweetofthedecad', 1.0): 1, ('class', 1.0): 5, ('happiest', 1.0): 2, ('bbmme', 1.0): 3, ('pin', 1.0): 4, ('7df9e60a', 1.0): 1, ('bbm', 1.0): 2, ('bbmpin', 1.0): 2, ('addmeonbbm', 1.0): 1, ('addm', 1.0): 1, (\"today'\", 1.0): 3, ('menu', 1.0): 1, ('marri', 1.0): 3, ('glenn', 1.0): 1, ('what', 1.0): 4, ('height', 1.0): 1, (\"sculptor'\", 1.0): 1, ('ti5', 1.0): 1, ('dota', 1.0): 3, ('nudg', 1.0): 1, ('spot', 1.0): 5, ('tasti', 1.0): 1, ('hilli', 1.0): 1, ('cycl', 1.0): 6, ('england', 1.0): 4, ('scotlandismass', 1.0): 1, ('gen', 1.0): 2, ('vikk', 1.0): 1, ('fna', 1.0): 1, ('mombasa', 1.0): 1, ('tukutanemombasa', 1.0): 1, ('100reasonstovisitmombasa', 1.0): 1, ('karibumombasa', 1.0): 1, ('hanbin', 1.0): 1, ('certainli', 1.0): 4, ('goosnight', 1.0): 1, ('kindli', 1.0): 4, ('familiar', 1.0): 2, ('jealou', 1.0): 4, ('tent', 1.0): 2, ('yea', 1.0): 2, ('cozi', 1.0): 1, ('phenomen', 1.0): 2, ('collab', 1.0): 2, ('gave', 1.0): 4, ('birth', 1.0): 1, ('behav', 1.0): 2, ('monster', 1.0): 1, ('spree', 1.0): 4, ('000', 1.0): 1, ('tank', 1.0): 6, ('outstand', 1.0): 1, ('donat', 1.0): 3, ('h', 1.0): 4, ('contestkiduniya', 1.0): 2, ('mfundo', 1.0): 1, ('och', 1.0): 1, ('hun', 1.0): 4, ('inner', 1.0): 2, ('nerd', 1.0): 2, ('tame', 1.0): 2, ('insidi', 1.0): 1, ('logic', 1.0): 1, ('math', 1.0): 1, ('channel', 1.0): 5, ('continu', 1.0): 4, ('doubt', 1.0): 3, ('300', 1.0): 2, ('sub', 1.0): 2, ('200', 1.0): 3, ('forgiven', 1.0): 1, ('manner', 1.0): 1, ('yhooo', 1.0): 1, ('ngi', 1.0): 1, ('mood', 1.0): 7, ('push', 1.0): 1, ('limit', 1.0): 6, ('obakeng', 1.0): 1, ('goat', 1.0): 1, ('alhamdullilah', 1.0): 1, ('pebbl', 1.0): 1, ('engross', 1.0): 1, ('bing', 1.0): 2, ('scream', 1.0): 2, ('whole', 1.0): 7, ('wide', 1.0): 2, ('🌎', 1.0): 2, ('😧', 1.0): 1, ('wat', 1.0): 2, ('muahhh', 1.0): 1, ('pausetim', 1.0): 1, ('drift', 1.0): 1, ('loos', 1.0): 3, ('campaign', 1.0): 4, ('kickstart', 1.0): 1, ('articl', 1.0): 9, ('jenna', 1.0): 1, ('bellybutton', 1.0): 5, ('inni', 1.0): 4, ('outi', 1.0): 4, ('havent', 1.0): 4, ('delish', 1.0): 1, ('joselito', 1.0): 1, ('freya', 1.0): 1, ('nth', 1.0): 1, ('latepost', 1.0): 1, ('lupet', 1.0): 1, ('mo', 1.0): 2, ('eric', 1.0): 3, ('askaman', 1.0): 1, ('150', 1.0): 1, ('0345', 1.0): 2, ('454', 1.0): 1, ('111', 1.0): 1, ('webz', 1.0): 1, ('oop', 1.0): 5, (\"they'll\", 1.0): 6, ('realis', 1.0): 2, ('anymor', 1.0): 3, ('carmel', 1.0): 1, ('decis', 1.0): 5, ('matt', 1.0): 6, ('@commoncultur', 1.0): 1, ('@connorfranta', 1.0): 1, ('honestli', 1.0): 3, ('explain', 1.0): 3, ('relationship', 1.0): 4, ('pick', 1.0): 15, ('tessnzach', 1.0): 1, ('paperboy', 1.0): 1, ('honest', 1.0): 3, ('reassur', 1.0): 1, ('guysss', 1.0): 3, ('mubank', 1.0): 2, (\"dongwoo'\", 1.0): 1, ('bright', 1.0): 2, ('tommorow', 1.0): 3, ('newyork', 1.0): 1, ('lolll', 1.0): 1, ('twinx', 1.0): 1, ('16', 1.0): 2, ('path', 1.0): 1, ('firmansyahbl', 1.0): 1, ('procedur', 1.0): 1, ('grim', 1.0): 1, ('fandango', 1.0): 1, ('ordinari', 1.0): 1, ('extraordinari', 1.0): 1, ('bo', 1.0): 2, ('birmingham', 1.0): 1, ('oracl', 1.0): 1, ('samosa', 1.0): 1, ('firebal', 1.0): 1, ('shoe', 1.0): 4, ('serv', 1.0): 1, ('sushi', 1.0): 2, ('shoeshi', 1.0): 1, ('�', 1.0): 2, ('lymond', 1.0): 1, ('philippa', 1.0): 2, ('novel', 1.0): 1, ('tara', 1.0): 3, ('. . .', 1.0): 2, ('aur', 1.0): 2, ('han', 1.0): 1, ('imran', 1.0): 3, ('khan', 1.0): 7, ('63', 1.0): 1, ('agaaain', 1.0): 1, ('doli', 1.0): 1, ('siregar', 1.0): 1, ('ninh', 1.0): 1, ('size', 1.0): 5, ('geekiest', 1.0): 1, ('geek', 1.0): 2, ('wallet', 1.0): 3, ('request', 1.0): 4, ('media', 1.0): 4, ('ralli', 1.0): 1, ('rotat', 1.0): 3, ('direct', 1.0): 3, ('eek', 1.0): 1, ('red', 1.0): 6, ('beij', 1.0): 1, ('meni', 1.0): 1, ('tebrik', 1.0): 1, ('etdi', 1.0): 1, ('700', 1.0): 1, ('💗', 1.0): 2, ('rod', 1.0): 1, ('embrac', 1.0): 1, ('actor', 1.0): 1, ('aplomb', 1.0): 1, ('foreveralon', 1.0): 2, ('mysumm', 1.0): 1, ('01482', 1.0): 1, ('333505', 1.0): 1, ('hahahaha', 1.0): 2, ('wear', 1.0): 6, ('uniform', 1.0): 1, ('evil', 1.0): 1, ('owww', 1.0): 1, ('choo', 1.0): 1, ('chweet', 1.0): 1, ('shorthair', 1.0): 1, ('oscar', 1.0): 1, ('realiz', 1.0): 7, ('harmoni', 1.0): 1, ('deneriveri', 1.0): 1, ('506', 1.0): 1, ('kiksext', 1.0): 5, ('kikkomansabor', 1.0): 2, ('killer', 1.0): 1, ('henessydiari', 1.0): 1, ('journey', 1.0): 4, ('band', 1.0): 4, ('plz', 1.0): 5, ('convo', 1.0): 3, ('11', 1.0): 5, ('vault', 1.0): 1, ('expand', 1.0): 2, ('vinni', 1.0): 1, ('money', 1.0): 9, ('hahahahaha', 1.0): 2, ('50cent', 1.0): 1, ('repay', 1.0): 1, ('debt', 1.0): 2, ('evet', 1.0): 1, ('wifi', 1.0): 3, ('lifestyl', 1.0): 1, ('qatarday', 1.0): 1, ('. ..', 1.0): 3, ('🌞', 1.0): 3, ('girli', 1.0): 1, ('india', 1.0): 4, ('innov', 1.0): 1, ('volunt', 1.0): 2, ('saran', 1.0): 1, ('drama', 1.0): 3, ('genr', 1.0): 1, ('romanc', 1.0): 1, ('comedi', 1.0): 1, ('leannerin', 1.0): 1, ('19', 1.0): 7, ('porno', 1.0): 1, ('l4l', 1.0): 3, ('weloveyounamjoon', 1.0): 1, ('homey', 1.0): 1, ('kenya', 1.0): 1, ('roller', 1.0): 2, ('coaster', 1.0): 1, ('aspect', 1.0): 1, ('najam', 1.0): 1, ('confess', 1.0): 2, ('pricelessantiqu', 1.0): 1, ('takesonetoknowon', 1.0): 1, ('extra', 1.0): 5, ('ucount', 1.0): 1, ('ji', 1.0): 3, ('turkish', 1.0): 1, ('knew', 1.0): 8, ('crap', 1.0): 1, ('burn', 1.0): 3, ('80x', 1.0): 1, ('airlin', 1.0): 1, ('sexi', 1.0): 10, ('yello', 1.0): 1, ('gail', 1.0): 1, ('yael', 1.0): 1, ('lesson', 1.0): 4, ('en', 1.0): 1, ('mano', 1.0): 1, ('hand', 1.0): 4, ('manag', 1.0): 6, ('prettiest', 1.0): 1, ('reader', 1.0): 4, ('dnt', 1.0): 1, ('ideal', 1.0): 2, ('weekli', 1.0): 2, ('idol', 1.0): 3, ('pose', 1.0): 2, ('shortlist', 1.0): 1, ('dominion', 1.0): 2, ('picnic', 1.0): 2, ('tmrw', 1.0): 3, ('nobodi', 1.0): 2, ('jummamubarak', 1.0): 1, ('shower', 1.0): 3, ('shalwarkameez', 1.0): 1, ('itter', 1.0): 1, ('offer', 1.0): 8, ('jummapray', 1.0): 1, ('af', 1.0): 8, ('display', 1.0): 1, ('enabl', 1.0): 1, ('compani', 1.0): 4, ('peep', 1.0): 4, ('tweep', 1.0): 2, ('folow', 1.0): 1, ('2k', 1.0): 1, ('ohhh', 1.0): 4, ('teaser', 1.0): 2, ('airec', 1.0): 1, ('009', 1.0): 1, ('acid', 1.0): 1, ('mous', 1.0): 2, ('31st', 1.0): 2, ('includ', 1.0): 5, ('robin', 1.0): 1, ('rough', 1.0): 4, ('control', 1.0): 1, ('remix', 1.0): 5, ('fave', 1.0): 3, ('toss', 1.0): 1, ('ladi', 1.0): 8, ('🐑', 1.0): 1, ('librari', 1.0): 3, ('mr2', 1.0): 1, ('climb', 1.0): 1, ('cuddl', 1.0): 1, ('jilla', 1.0): 1, ('headlin', 1.0): 1, ('2017', 1.0): 1, ('jumma', 1.0): 5, ('mubarik', 1.0): 2, ('spent', 1.0): 2, ('congratz', 1.0): 1, ('contribut', 1.0): 3, ('2.0', 1.0): 2, ('yuppiiee', 1.0): 1, ('alienthought', 1.0): 1, ('happyalien', 1.0): 1, ('crowd', 1.0): 2, ('loudest', 1.0): 2, ('gari', 1.0): 1, ('particular', 1.0): 1, ('attract', 1.0): 1, ('supprt', 1.0): 1, ('savag', 1.0): 1, ('cleans', 1.0): 1, ('scam', 1.0): 1, ('ridden', 1.0): 1, ('vyapam', 1.0): 2, ('renam', 1.0): 1, ('wave', 1.0): 2, ('couch', 1.0): 1, ('dodg', 1.0): 1, ('explan', 1.0): 2, ('bag', 1.0): 4, ('sanza', 1.0): 1, ('yaa', 1.0): 3, ('slr', 1.0): 1, ('som', 1.0): 1, ('honour', 1.0): 1, ('heheh', 1.0): 1, ('view', 1.0): 16, ('explor', 1.0): 2, ('wayanadan', 1.0): 1, ('forest', 1.0): 1, ('wayanad', 1.0): 1, ('srijith', 1.0): 1, ('whisper', 1.0): 1, ('lie', 1.0): 4, ('pokemon', 1.0): 1, ('dazzl', 1.0): 1, ('urself', 1.0): 2, ('doubl', 1.0): 2, ('flare', 1.0): 1, ('black', 1.0): 4, ('9', 1.0): 3, ('51', 1.0): 1, ('brows', 1.0): 1, ('bore', 1.0): 9, ('femal', 1.0): 2, ('tour', 1.0): 8, ('delv', 1.0): 2, ('muchhh', 1.0): 1, ('tmr', 1.0): 1, ('breakfast', 1.0): 4, ('gl', 1.0): 1, (\"tonight'\", 1.0): 2, ('):', 1.0): 7, ('litey', 1.0): 1, ('manuella', 1.0): 1, ('abhi', 1.0): 2, ('tak', 1.0): 2, ('nhi', 1.0): 2, ('dekhi', 1.0): 1, ('promo', 1.0): 3, ('se', 1.0): 4, ('xpax', 1.0): 1, ('lisa', 1.0): 2, ('aboard', 1.0): 3, ('institut', 1.0): 1, ('nc', 1.0): 2, ('chees', 1.0): 4, ('overload', 1.0): 1, ('pizza', 1.0): 1, ('•', 1.0): 3, ('mcfloat', 1.0): 1, ('fudg', 1.0): 3, ('sanda', 1.0): 1, ('munchkin', 1.0): 1, (\"d'd\", 1.0): 1, ('granni', 1.0): 1, ('baller', 1.0): 1, ('lil', 1.0): 4, ('chain', 1.0): 1, ('everybodi', 1.0): 1, ('ought', 1.0): 1, ('jay', 1.0): 3, ('events@breastcancernow.org', 1.0): 1, ('79x', 1.0): 1, ('champion', 1.0): 1, ('letter', 1.0): 2, ('uniqu', 1.0): 2, ('affaraid', 1.0): 1, ('dearslim', 1.0): 2, ('role', 1.0): 2, ('billi', 1.0): 2, ('lab', 1.0): 1, ('ovh', 1.0): 2, ('maxi', 1.0): 2, ('bunch', 1.0): 1, ('acc', 1.0): 2, ('sprit', 1.0): 1, ('you', 1.0): 1, ('til', 1.0): 2, ('hammi', 1.0): 1, ('freedom', 1.0): 2, ('pistol', 1.0): 1, ('unlock', 1.0): 1, ('bemeapp', 1.0): 1, ('thumb', 1.0): 1, ('beme', 1.0): 1, ('bemecod', 1.0): 1, ('proudtobem', 1.0): 1, ('round', 1.0): 2, ('calm', 1.0): 5, ('kepo', 1.0): 1, ('luckili', 1.0): 1, ('clearli', 1.0): 2, ('دعمم', 1.0): 1, ('للعودة', 1.0): 1, ('للحياة', 1.0): 1, ('heiyo', 1.0): 2, ('dudafti', 1.0): 1, ('breaktym', 1.0): 1, ('fatal', 1.0): 1, ('danger', 1.0): 1, ('term', 1.0): 2, ('health', 1.0): 2, ('outrag', 1.0): 1, ('645k', 1.0): 1, ('muna', 1.0): 1, ('magstart', 1.0): 1, ('salut', 1.0): 3, ('→', 1.0): 1, ('thq', 1.0): 1, ('contin', 1.0): 1, ('thalaivar', 1.0): 1, ('£', 1.0): 7, ('heiya', 1.0): 2, ('grab', 1.0): 3, ('30.000', 1.0): 2, ('av', 1.0): 1, ('gd', 1.0): 3, ('wknd', 1.0): 1, ('ear', 1.0): 12, (\"y'day\", 1.0): 1, ('hxh', 1.0): 1, ('badass', 1.0): 2, ('killua', 1.0): 1, ('scene', 1.0): 2, ('78x', 1.0): 1, ('unappreci', 1.0): 1, ('graciou', 1.0): 1, ('nailedit', 1.0): 1, ('ourdisneyinfin', 1.0): 1, ('mari', 1.0): 3, ('jillmil', 1.0): 1, ('webcam', 1.0): 2, ('elfindelmundo', 1.0): 1, ('mainli', 1.0): 1, ('favour', 1.0): 1, ('dancetast', 1.0): 1, ('satyajit', 1.0): 1, (\"ray'\", 1.0): 1, ('porosh', 1.0): 1, ('pathor', 1.0): 1, ('situat', 1.0): 3, ('goldbug', 1.0): 1, ('wine', 1.0): 3, ('bottl', 1.0): 2, ('spill', 1.0): 2, ('jazmin', 1.0): 3, ('bonilla', 1.0): 3, ('15000', 1.0): 1, ('star', 1.0): 9, ('hollywood', 1.0): 3, ('rofl', 1.0): 3, ('shade', 1.0): 1, ('grey', 1.0): 1, ('netsec', 1.0): 1, ('kev', 1.0): 1, ('sister', 1.0): 6, ('told', 1.0): 6, ('unlist', 1.0): 1, ('hickey', 1.0): 1, ('dad', 1.0): 5, ('hock', 1.0): 1, ('mamma', 1.0): 1, ('human', 1.0): 5, ('be', 1.0): 1, ('mere', 1.0): 1, ('holist', 1.0): 1, ('cosmovis', 1.0): 1, ('narrow-mind', 1.0): 1, ('charg', 1.0): 3, ('cess', 1.0): 1, ('alix', 1.0): 1, ('quan', 1.0): 1, ('tip', 1.0): 5, ('naaahhh', 1.0): 1, ('duh', 1.0): 2, ('emesh', 1.0): 1, ('hilari', 1.0): 4, ('kath', 1.0): 3, ('kia', 1.0): 1, ('@vauk', 1.0): 1, ('tango', 1.0): 1, ('tracerequest', 1.0): 2, ('dassi', 1.0): 1, ('fwm', 1.0): 1, ('selamat', 1.0): 1, ('nichola', 1.0): 2, ('malta', 1.0): 1, ('gto', 1.0): 1, ('tomorrowland', 1.0): 1, ('incal', 1.0): 1, ('shob', 1.0): 1, ('incomplet', 1.0): 1, ('barkada', 1.0): 1, ('silverston', 1.0): 1, ('pull', 1.0): 1, ('bookstor', 1.0): 1, ('ganna', 1.0): 1, ('hillari', 1.0): 1, ('clinton', 1.0): 1, ('court', 1.0): 2, ('notic', 1.0): 11, ('slice', 1.0): 2, ('life-so', 1.0): 1, ('hidden', 1.0): 1, ('untap', 1.0): 1, ('mca', 1.0): 2, ('gettin', 1.0): 1, ('hella', 1.0): 1, ('wana', 1.0): 1, ('bandz', 1.0): 1, ('hell', 1.0): 4, ('donington', 1.0): 1, ('park', 1.0): 8, ('24/25', 1.0): 1, ('x30', 1.0): 1, ('merci', 1.0): 1, ('bien', 1.0): 1, ('pitbul', 1.0): 1, ('777x', 1.0): 1, ('fri', 1.0): 3, ('annyeong', 1.0): 1, ('oppa', 1.0): 7, ('indonesian', 1.0): 1, ('elf', 1.0): 3, ('flight', 1.0): 2, ('bf', 1.0): 2, ('jennyjean', 1.0): 1, ('kikchat', 1.0): 1, ('sabadodeganarseguidor', 1.0): 1, ('sexysasunday', 1.0): 2, ('marseil', 1.0): 1, ('ganda', 1.0): 1, ('fnaf', 1.0): 5, ('steam', 1.0): 1, ('assur', 1.0): 2, ('current', 1.0): 7, ('goin', 1.0): 1, ('sweeti', 1.0): 4, ('strongest', 1.0): 1, (\"spot'\", 1.0): 1, ('barnstapl', 1.0): 1, ('bideford', 1.0): 1, ('abit', 1.0): 1, ('road', 1.0): 5, ('rocro', 1.0): 1, ('13glodyysbro', 1.0): 1, ('hire', 1.0): 1, ('2ne1', 1.0): 1, ('aspetti', 1.0): 1, ('chicken', 1.0): 4, ('chip', 1.0): 3, ('cupboard', 1.0): 1, ('empti', 1.0): 2, ('jami', 1.0): 2, ('ian', 1.0): 2, ('latin', 1.0): 5, ('asian', 1.0): 5, ('version', 1.0): 8, ('va', 1.0): 1, ('642', 1.0): 1, ('kikgirl', 1.0): 5, ('orgasm', 1.0): 1, ('phonesex', 1.0): 1, ('spacer', 1.0): 1, ('felic', 1.0): 1, ('smoak', 1.0): 1, ('👓', 1.0): 1, ('💘', 1.0): 3, ('children', 1.0): 3, ('psychopath', 1.0): 1, ('spoil', 1.0): 1, ('dimpl', 1.0): 1, ('contempl', 1.0): 1, ('indi', 1.0): 2, ('rout', 1.0): 4, ('jsl', 1.0): 1, ('76x', 1.0): 1, ('gotcha', 1.0): 1, ('kina', 1.0): 1, ('donna', 1.0): 3, ('reachabl', 1.0): 1, ('jk', 1.0): 1, ('s02e04', 1.0): 1, ('air', 1.0): 7, ('naggi', 1.0): 1, ('anal', 1.0): 1, ('child', 1.0): 3, ('vidcon', 1.0): 2, ('anxiou', 1.0): 1, ('shake', 1.0): 2, ('10:30', 1.0): 1, ('smoke', 1.0): 3, ('white', 1.0): 4, ('grandpa', 1.0): 4, ('prolli', 1.0): 1, ('stash', 1.0): 2, ('closer-chas', 1.0): 1, ('spec', 1.0): 1, ('leagu', 1.0): 3, ('chase', 1.0): 1, ('wall', 1.0): 3, ('angel', 1.0): 4, ('mochamichel', 1.0): 1, ('iph', 1.0): 4, ('0ne', 1.0): 4, ('simpli', 1.0): 3, ('bi0', 1.0): 8, ('x29', 1.0): 1, ('there', 1.0): 2, ('background', 1.0): 2, ('maggi', 1.0): 1, ('afraid', 1.0): 3, ('mull', 1.0): 1, ('nil', 1.0): 1, ('glasgow', 1.0): 2, ('netbal', 1.0): 1, ('thistl', 1.0): 1, ('thistlelov', 1.0): 1, ('minecraft', 1.0): 7, ('drew', 1.0): 3, ('delici', 1.0): 3, ('muddl', 1.0): 1, ('racket', 1.0): 2, ('isol', 1.0): 1, ('fa', 1.0): 1, ('particip', 1.0): 2, ('icecreammast', 1.0): 1, ('group', 1.0): 10, ('huhu', 1.0): 3, ('shet', 1.0): 1, ('desk', 1.0): 1, ('o_o', 1.0): 1, ('orz', 1.0): 1, ('problemmm', 1.0): 1, ('75x', 1.0): 1, ('english', 1.0): 4, ('yeeaayi', 1.0): 1, ('alhamdulillah', 1.0): 1, ('amin', 1.0): 1, ('weed', 1.0): 1, ('crowdfund', 1.0): 1, ('goal', 1.0): 2, ('walk', 1.0): 12, ('hellooo', 1.0): 2, ('select', 1.0): 1, ('lynn', 1.0): 1, ('buffer', 1.0): 2, ('button', 1.0): 2, ('compos', 1.0): 1, ('fridayfun', 1.0): 1, ('non-filipina', 1.0): 1, ('ejayst', 1.0): 1, ('state', 1.0): 2, ('le', 1.0): 2, ('stan', 1.0): 1, ('lee', 1.0): 2, ('discoveri', 1.0): 1, ('cousin', 1.0): 5, ('1400', 1.0): 1, ('yr', 1.0): 2, ('teleport', 1.0): 1, ('shahid', 1.0): 1, ('afridi', 1.0): 1, ('tou', 1.0): 1, ('mahnor', 1.0): 1, ('baloch', 1.0): 1, ('nikki', 1.0): 2, ('flower', 1.0): 4, ('blackfli', 1.0): 1, ('courgett', 1.0): 1, ('wont', 1.0): 5, ('affect', 1.0): 2, ('fruit', 1.0): 5, ('italian', 1.0): 1, ('netfilx', 1.0): 1, ('unmarri', 1.0): 1, ('finger', 1.0): 6, ('rock', 1.0): 10, ('wielli', 1.0): 1, ('paul', 1.0): 2, ('barcod', 1.0): 1, ('charlott', 1.0): 1, ('thta', 1.0): 1, ('trailblazerhonor', 1.0): 1, ('labour', 1.0): 3, ('leader', 1.0): 3, ('alot', 1.0): 2, ('agayhippiehippi', 1.0): 1, ('exercis', 1.0): 2, ('ginger', 1.0): 1, ('x28', 1.0): 1, ('teach', 1.0): 2, ('awar', 1.0): 1, ('::', 1.0): 4, ('portsmouth', 1.0): 1, ('sonal', 1.0): 1, ('hungri', 1.0): 2, ('hmmm', 1.0): 4, ('pedant', 1.0): 1, ('98', 1.0): 1, ('kit', 1.0): 2, ('ack', 1.0): 1, ('hih', 1.0): 1, ('choir', 1.0): 1, ('rosidbinr', 1.0): 1, ('duke', 1.0): 2, ('earl', 1.0): 1, ('tau', 1.0): 1, ('orayt', 1.0): 1, ('knw', 1.0): 1, ('block', 1.0): 3, ('dikha', 1.0): 1, ('reh', 1.0): 1, ('adolf', 1.0): 1, ('hitler', 1.0): 1, ('obstacl', 1.0): 1, ('exist', 1.0): 2, ('surrend', 1.0): 2, ('terrif', 1.0): 1, ('advaddict', 1.0): 1, ('_15', 1.0): 1, ('jimin', 1.0): 1, ('notanapolog', 1.0): 3, ('map', 1.0): 2, ('inform', 1.0): 5, ('0.7', 1.0): 1, ('motherfuck', 1.0): 1, (\"david'\", 1.0): 1, ('damn', 1.0): 3, ('colleg', 1.0): 2, ('24th', 1.0): 3, ('steroid', 1.0): 1, ('alansmithpart', 1.0): 1, ('servu', 1.0): 1, ('bonasio', 1.0): 1, (\"doido'\", 1.0): 1, ('task', 1.0): 2, ('deleg', 1.0): 1, ('aaahhh', 1.0): 1, ('jen', 1.0): 2, ('virgin', 1.0): 5, ('non-mapbox', 1.0): 1, ('restrict', 1.0): 1, ('mapbox', 1.0): 1, ('basemap', 1.0): 1, ('contractu', 1.0): 1, ('research', 1.0): 1, ('seafood', 1.0): 1, ('weltum', 1.0): 1, ('teh', 1.0): 1, ('deti', 1.0): 1, ('huh', 1.0): 2, ('=d', 1.0): 2, ('annoy', 1.0): 2, ('katmtan', 1.0): 1, ('swan', 1.0): 1, ('fandom', 1.0): 3, ('blurri', 1.0): 1, ('besok', 1.0): 1, ('b', 1.0): 8, ('urgent', 1.0): 3, ('within', 1.0): 4, ('dorset', 1.0): 1, ('goddess', 1.0): 1, ('blast', 1.0): 1, ('shitfac', 1.0): 1, ('soul', 1.0): 4, ('sing', 1.0): 5, ('disney', 1.0): 1, ('doug', 1.0): 3, ('28', 1.0): 2, ('bnte', 1.0): 1, ('hain', 1.0): 2, (';p', 1.0): 1, ('shiiitt', 1.0): 1, ('case', 1.0): 9, ('rm35', 1.0): 1, ('negooo', 1.0): 1, ('male', 1.0): 1, ('madelin', 1.0): 1, ('nun', 1.0): 1, ('mornin', 1.0): 2, ('yapster', 1.0): 1, ('pli', 1.0): 1, ('icon', 1.0): 2, ('alchemist', 1.0): 1, ('x27', 1.0): 1, ('dayz', 1.0): 1, ('preview', 1.0): 1, ('thug', 1.0): 1, ('lmao', 1.0): 3, ('sharethelov', 1.0): 2, ('highvalu', 1.0): 2, ('halsey', 1.0): 1, ('30th', 1.0): 1, ('anniversari', 1.0): 5, ('folk', 1.0): 10, ('bae', 1.0): 6, ('repli', 1.0): 5, ('complain', 1.0): 3, ('rude', 1.0): 3, ('bond', 1.0): 4, ('nigg', 1.0): 1, ('readingr', 1.0): 1, ('wordoftheweek', 1.0): 1, ('wotw', 1.0): 1, ('4:18', 1.0): 1, ('est', 1.0): 1, ('earn', 1.0): 1, ('jess', 1.0): 2, ('surri', 1.0): 1, ('botani', 1.0): 1, ('gel', 1.0): 1, ('alison', 1.0): 1, ('lsa', 1.0): 1, ('respons', 1.0): 7, ('fron', 1.0): 1, ('debbi', 1.0): 1, ('carol', 1.0): 2, ('patient', 1.0): 4, ('discharg', 1.0): 1, ('loung', 1.0): 1, ('walmart', 1.0): 1, ('balanc', 1.0): 2, ('studi', 1.0): 6, ('hayley', 1.0): 2, ('shoulder', 1.0): 1, ('pad', 1.0): 2, ('mount', 1.0): 1, ('inquisitor', 1.0): 1, ('cosplay', 1.0): 4, ('cosplayprogress', 1.0): 1, ('mike', 1.0): 3, ('dunno', 1.0): 2, ('insecur', 1.0): 2, ('nh', 1.0): 1, ('devolut', 1.0): 1, ('patriot', 1.0): 1, ('halla', 1.0): 1, ('ark', 1.0): 1, (\"jiyeon'\", 1.0): 1, ('buzz', 1.0): 2, ('burnt', 1.0): 1, ('mist', 1.0): 4, ('opi', 1.0): 1, ('avoplex', 1.0): 1, ('nail', 1.0): 3, ('cuticl', 1.0): 1, ('replenish', 1.0): 1, ('15ml', 1.0): 1, ('seriou', 1.0): 2, ('submiss', 1.0): 1, ('lb', 1.0): 2, ('cherish', 1.0): 2, ('flip', 1.0): 1, ('learnt', 1.0): 2, ('backflip', 1.0): 2, ('jumpgiant', 1.0): 1, ('foampit', 1.0): 1, ('usa', 1.0): 3, ('pamer', 1.0): 1, ('thk', 1.0): 1, ('actuallythough', 1.0): 1, ('craft', 1.0): 2, ('session', 1.0): 3, ('mehtab', 1.0): 1, ('aunti', 1.0): 1, ('gc', 1.0): 1, ('yeeew', 1.0): 1, ('pre', 1.0): 3, ('lan', 1.0): 1, ('yeey', 1.0): 1, ('arrang', 1.0): 1, ('doodl', 1.0): 2, ('comic', 1.0): 1, ('summon', 1.0): 1, ('none', 1.0): 1, ('🙅', 1.0): 1, ('lycra', 1.0): 1, ('vincent', 1.0): 1, ('couldnt', 1.0): 1, ('roy', 1.0): 1, ('bg', 1.0): 1, ('img', 1.0): 1, ('circl', 1.0): 1, ('font', 1.0): 1, ('deathofgrass', 1.0): 1, ('loan', 1.0): 2, ('lawnmow', 1.0): 1, ('popular', 1.0): 2, ('charismat', 1.0): 1, ('man.h', 1.0): 1, ('thrive', 1.0): 1, ('economi', 1.0): 1, ('burst', 1.0): 2, ('georgi', 1.0): 1, ('x26', 1.0): 1, ('million', 1.0): 4, ('fl', 1.0): 1, ('kindest', 1.0): 2, ('iceland', 1.0): 1, ('crazi', 1.0): 4, ('landscap', 1.0): 2, ('yok', 1.0): 1, ('lah', 1.0): 1, ('concordia', 1.0): 1, ('reunit', 1.0): 1, ('xxxibmchll', 1.0): 1, ('sea', 1.0): 4, ('prettier', 1.0): 2, ('imitatia', 1.0): 1, ('oe', 1.0): 1, ('michel', 1.0): 1, ('comeback', 1.0): 1, ('gross', 1.0): 1, ('treat', 1.0): 5, ('equal', 1.0): 2, ('injustic', 1.0): 1, ('femin', 1.0): 1, ('ineedfeminismbecaus', 1.0): 1, ('forgotten', 1.0): 3, ('stuck', 1.0): 4, ('recommend', 1.0): 4, ('redhead', 1.0): 1, ('wacki', 1.0): 1, ('rather', 1.0): 5, ('waytoliveahappylif', 1.0): 1, ('hoxton', 1.0): 1, ('holborn', 1.0): 1, ('karen', 1.0): 2, ('wag', 1.0): 2, ('bum', 1.0): 1, ('wwooo', 1.0): 1, ('nite', 1.0): 3, ('laiten', 1.0): 1, ('arond', 1.0): 1, ('1:30', 1.0): 1, ('consid', 1.0): 3, ('matur', 1.0): 3, ('journeyp', 1.0): 2, ('foam', 1.0): 1, (\"lady'\", 1.0): 1, ('mob', 1.0): 1, ('fals', 1.0): 1, ('bulletin', 1.0): 1, ('spring', 1.0): 1, ('fiesta', 1.0): 1, ('nois', 1.0): 2, ('awuuu', 1.0): 1, ('aich', 1.0): 1, ('sept', 1.0): 2, ('rudramadevi', 1.0): 1, ('anushka', 1.0): 1, ('gunashekar', 1.0): 1, ('harryxhood', 1.0): 1, ('upset', 1.0): 1, ('ooh', 1.0): 1, ('humanist', 1.0): 1, ('magazin', 1.0): 2, ('usernam', 1.0): 1, ('rape', 1.0): 1, ('csrrace', 1.0): 1, ('lack', 1.0): 6, ('hygien', 1.0): 1, ('tose', 1.0): 1, ('cloth', 1.0): 1, ('temperatur', 1.0): 1, ('planet', 1.0): 2, ('brave', 1.0): 2, ('ge', 1.0): 1, ('2015kenya', 1.0): 1, ('ryan', 1.0): 4, ('tidi', 1.0): 2, ('hagergang', 1.0): 1, ('chanhun', 1.0): 1, ('photoshoot', 1.0): 1, ('afteral', 1.0): 1, ('sadkaay', 1.0): 1, ('thark', 1.0): 1, ('peak', 1.0): 1, ('heatwav', 1.0): 1, ('lower', 1.0): 1, ('standard', 1.0): 2, ('x25', 1.0): 1, ('recruit', 1.0): 2, ('doom', 1.0): 1, ('nasti', 1.0): 1, ('affili', 1.0): 1, ('&gt;:)', 1.0): 2, ('64', 1.0): 2, ('74', 1.0): 1, ('40', 1.0): 4, ('00', 1.0): 1, ('hall', 1.0): 2, ('ted', 1.0): 3, ('pixgram', 1.0): 2, ('creativ', 1.0): 2, ('slideshow', 1.0): 1, ('nibbl', 1.0): 2, ('ivi', 1.0): 1, ('sho', 1.0): 1, ('superpow', 1.0): 2, ('obsess', 1.0): 2, ('oth', 1.0): 1, ('third', 1.0): 2, ('ngarepfollbackdarinabilahjkt', 1.0): 1, ('48', 1.0): 1, ('sunglass', 1.0): 1, ('jacki', 1.0): 2, ('sunni', 1.0): 6, ('style', 1.0): 5, ('jlo', 1.0): 1, ('jlover', 1.0): 1, ('turkey', 1.0): 1, ('goodafternoon', 1.0): 2, ('collag', 1.0): 2, ('furri', 1.0): 2, ('bruce', 1.0): 2, ('kunoriforceo', 1.0): 8, ('aayegi', 1.0): 1, ('tim', 1.0): 2, ('wiw', 1.0): 1, ('bip', 1.0): 1, ('zareen', 1.0): 1, ('daisi', 1.0): 1, (\"b'coz\", 1.0): 1, ('kart', 1.0): 1, ('mak', 1.0): 1, ('∗', 1.0): 2, ('lega', 1.0): 1, ('spag', 1.0): 1, ('boat', 1.0): 2, ('outboard', 1.0): 1, ('spell', 1.0): 4, ('reboard', 1.0): 1, ('fire', 1.0): 2, ('offboard', 1.0): 1, ('sn16', 1.0): 1, ('9dg', 1.0): 1, ('bnf', 1.0): 1, ('50', 1.0): 1, ('jason', 1.0): 1, ('rob', 1.0): 2, ('feb', 1.0): 1, ('victoriasecret', 1.0): 1, ('finland', 1.0): 1, ('helsinki', 1.0): 1, ('airport', 1.0): 3, ('plane', 1.0): 2, ('beyond', 1.0): 4, ('ont', 1.0): 1, ('tii', 1.0): 1, ('lng', 1.0): 2, ('yan', 1.0): 2, (\"u'll\", 1.0): 2, ('steve', 1.0): 2, ('bell', 1.0): 1, ('prescott', 1.0): 1, ('leadership', 1.0): 2, ('cartoon', 1.0): 1, ('upsid', 1.0): 2, ('statement', 1.0): 1, ('selamathariraya', 1.0): 1, ('lovesummertim', 1.0): 1, ('dumont', 1.0): 1, ('jax', 1.0): 1, ('jone', 1.0): 1, ('awesomee', 1.0): 1, ('x24', 1.0): 1, ('geoff', 1.0): 1, ('amazingli', 1.0): 1, ('talant', 1.0): 1, ('vsco', 1.0): 2, ('thanki', 1.0): 2, ('hash', 1.0): 1, ('tag', 1.0): 5, ('ifimeetanalien', 1.0): 1, ('bff', 1.0): 4, ('section', 1.0): 3, ('follbaaack', 1.0): 1, ('az', 1.0): 1, ('cauliflow', 1.0): 1, ('attempt', 1.0): 1, ('prinsesa', 1.0): 1, ('yaaah', 1.0): 2, ('law', 1.0): 3, ('toy', 1.0): 2, ('sonaaa', 1.0): 1, ('beautiful', 1.0): 2, (\"josephine'\", 1.0): 1, ('mirror', 1.0): 3, ('cretaperfect', 1.0): 2, ('4me', 1.0): 2, ('cretaperfectsuv', 1.0): 2, ('creta', 1.0): 1, ('load', 1.0): 1, ('telecom', 1.0): 2, ('judi', 1.0): 1, ('superb', 1.0): 1, ('slightli', 1.0): 1, ('rakna', 1.0): 1, ('ew', 1.0): 1, ('whose', 1.0): 1, ('fifa', 1.0): 1, ('lineup', 1.0): 1, ('surviv', 1.0): 2, ('p90x', 1.0): 1, ('p90', 1.0): 1, ('dishoom', 1.0): 2, ('rajnigandha', 1.0): 1, ('minju', 1.0): 1, ('rapper', 1.0): 1, ('lead', 1.0): 2, ('vocal', 1.0): 1, ('yujin', 1.0): 1, ('visual', 1.0): 2, ('makna', 1.0): 1, ('jane', 1.0): 2, ('hah', 1.0): 4, ('hawk', 1.0): 2, ('greatest', 1.0): 2, ('histori', 1.0): 2, ('along', 1.0): 6, ('talkback', 1.0): 1, ('process', 1.0): 4, ('featur', 1.0): 4, ('mostli', 1.0): 1, (\"cinema'\", 1.0): 1, ('defend', 1.0): 2, ('fashion', 1.0): 2, ('atroc', 1.0): 1, ('pandimension', 1.0): 1, ('manifest', 1.0): 1, ('argo', 1.0): 1, ('ring', 1.0): 4, ('640', 1.0): 1, ('nad', 1.0): 1, ('plezzz', 1.0): 1, ('asthma', 1.0): 1, ('inhal', 1.0): 1, ('breath', 1.0): 3, ('goodluck', 1.0): 1, ('hunger', 1.0): 1, ('mockingjay', 1.0): 1, ('thehungergam', 1.0): 1, ('ador', 1.0): 4, ('x23', 1.0): 1, ('reina', 1.0): 1, ('felt', 1.0): 3, ('excus', 1.0): 2, ('attend', 1.0): 2, ('whn', 1.0): 1, ('andr', 1.0): 1, ('mamayang', 1.0): 1, ('11pm', 1.0): 1, ('1d', 1.0): 2, ('89.9', 1.0): 1, ('powi', 1.0): 1, ('shropshir', 1.0): 1, ('border', 1.0): 1, (\"school'\", 1.0): 1, ('san', 1.0): 2, ('diego', 1.0): 1, ('jump', 1.0): 2, ('sourc', 1.0): 3, ('appeas', 1.0): 1, ('¦', 1.0): 1, ('aj', 1.0): 1, ('action', 1.0): 1, ('grunt', 1.0): 1, ('sc', 1.0): 1, ('anti-christ', 1.0): 1, ('m8', 1.0): 1, ('ju', 1.0): 1, ('halfway', 1.0): 1, ('ex', 1.0): 2, ('postiv', 1.0): 2, ('opinion', 1.0): 3, ('avi', 1.0): 1, ('dare', 1.0): 4, ('corridor', 1.0): 1, ('👯', 1.0): 2, ('neither', 1.0): 2, ('rundown', 1.0): 1, ('yah', 1.0): 4, ('leviboard', 1.0): 1, ('kleper', 1.0): 1, (':(', 1.0): 1, ('impecc', 1.0): 2, ('setokido', 1.0): 1, ('shoulda', 1.0): 3, ('hippo', 1.0): 1, ('materialist', 1.0): 1, ('showpo', 1.0): 1, ('cough', 1.0): 6, ('@artofsleepingin', 1.0): 1, ('x22', 1.0): 1, ('☺', 1.0): 5, ('makesm', 1.0): 1, ('santorini', 1.0): 1, ('escap', 1.0): 2, ('beatport', 1.0): 1, ('👊🏻', 1.0): 1, ('trmdhesit', 1.0): 2, ('manuel', 1.0): 1, ('vall', 1.0): 1, ('king', 1.0): 3, ('seven', 1.0): 2, ('kingdom', 1.0): 2, ('andal', 1.0): 1, ('taught', 1.0): 1, ('hide', 1.0): 3, ('privaci', 1.0): 1, ('wise', 1.0): 1, ('natsuki', 1.0): 1, ('often', 1.0): 2, ('catchi', 1.0): 1, ('neil', 1.0): 2, ('emir', 1.0): 2, ('brill', 1.0): 1, ('urquhart', 1.0): 1, ('castl', 1.0): 1, ('simpl', 1.0): 2, ('shatter', 1.0): 2, ('contrast', 1.0): 1, ('educampakl', 1.0): 1, ('rotorua', 1.0): 1, ('pehli', 1.0): 1, ('phir', 1.0): 1, ('somi', 1.0): 1, ('burfday', 1.0): 1, ('univers', 1.0): 3, ('santo', 1.0): 1, ('toma', 1.0): 1, ('norh', 1.0): 1, ('dialogu', 1.0): 2, ('chainsaw', 1.0): 2, ('amus', 1.0): 1, ('awe', 1.0): 1, ('protect', 1.0): 2, ('pop', 1.0): 5, ('2ish', 1.0): 1, ('fahad', 1.0): 1, ('bhai', 1.0): 3, ('iqrar', 1.0): 1, ('waseem', 1.0): 1, ('abroad', 1.0): 2, ('movie', 1.0): 1, ('chef', 1.0): 1, ('grogol', 1.0): 1, ('long-dist', 1.0): 1, ('rhi', 1.0): 1, ('pwrfl', 1.0): 1, ('benefit', 1.0): 2, ('b2b', 1.0): 1, ('b2c', 1.0): 1, (\"else'\", 1.0): 2, ('soo', 1.0): 2, ('enterprison', 1.0): 1, ('schoolsoutforsumm', 1.0): 1, ('fellow', 1.0): 4, ('juggl', 1.0): 1, ('purrtho', 1.0): 1, ('catho', 1.0): 1, ('catami', 1.0): 1, ('fourfivesecond', 1.0): 4, ('deaf', 1.0): 4, ('drug', 1.0): 1, ('alcohol', 1.0): 1, ('apexi', 1.0): 3, ('crystal', 1.0): 3, ('meth', 1.0): 1, ('champagn', 1.0): 1, ('fc', 1.0): 1, ('streamer', 1.0): 1, ('juic', 1.0): 1, ('correct', 1.0): 1, ('portrait', 1.0): 1, ('izumi', 1.0): 1, ('fugiwara', 1.0): 1, ('clonmel', 1.0): 1, ('vibrant', 1.0): 1, ('estim', 1.0): 1, ('server', 1.0): 2, ('quiet', 1.0): 1, ('yey', 1.0): 1, (\"insha'allah\", 1.0): 1, ('wil', 1.0): 1, ('x21', 1.0): 1, ('trend', 1.0): 3, ('akshaymostlovedsuperstarev', 1.0): 1, ('indirect', 1.0): 1, ('askurban', 1.0): 1, ('lyka', 1.0): 2, ('nap', 1.0): 4, ('aff', 1.0): 1, ('unam', 1.0): 1, ('jonginuh', 1.0): 1, ('forecast', 1.0): 2, ('10am', 1.0): 2, ('5am', 1.0): 1, ('sooth', 1.0): 1, ('vii', 1.0): 1, ('sweetheart', 1.0): 1, ('freak', 1.0): 3, ('zayn', 1.0): 3, ('fucker', 1.0): 1, ('pet', 1.0): 2, ('illustr', 1.0): 1, ('wohoo', 1.0): 1, ('gleam', 1.0): 1, ('paint', 1.0): 4, ('deal', 1.0): 2, ('prime', 1.0): 2, ('minist', 1.0): 2, ('sunjam', 1.0): 1, ('industri', 1.0): 1, ('present', 1.0): 7, ('practic', 1.0): 3, ('proactiv', 1.0): 1, ('environ', 1.0): 1, ('unreal', 1.0): 1, ('zain', 1.0): 1, ('zac', 1.0): 1, ('isaac', 1.0): 1, ('oss', 1.0): 1, ('frank', 1.0): 1, ('iero', 1.0): 1, ('phase', 1.0): 2, ('david', 1.0): 1, ('beginn', 1.0): 1, ('shine', 1.0): 3, ('sunflow', 1.0): 2, ('tommarow', 1.0): 1, ('yall', 1.0): 2, ('rank', 1.0): 2, ('birthdaymonth', 1.0): 1, ('vianey', 1.0): 1, ('juli', 1.0): 11, ('birthdaygirl', 1.0): 1, (\"town'\", 1.0): 1, ('andrew', 1.0): 2, ('checkout', 1.0): 2, ('otwol', 1.0): 1, ('awhil', 1.0): 1, ('x20', 1.0): 1, ('all-tim', 1.0): 1, ('julia', 1.0): 1, ('robert', 1.0): 1, ('awwhh', 1.0): 1, ('bulldog', 1.0): 1, ('unfortun', 1.0): 2, ('02079', 1.0): 1, ('490', 1.0): 1, ('132', 1.0): 1, ('born', 1.0): 2, ('fightstickfriday', 1.0): 1, ('extravag', 1.0): 2, ('tearout', 1.0): 1, ('selekt', 1.0): 1, ('yoot', 1.0): 1, ('cross', 1.0): 3, ('gudday', 1.0): 1, ('dave', 1.0): 5, ('haileyhelp', 1.0): 1, ('eid', 1.0): 2, ('mubarak', 1.0): 5, ('brotheeerrr', 1.0): 1, ('adventur', 1.0): 5, ('tokyo', 1.0): 2, ('kansai', 1.0): 1, ('l', 1.0): 4, ('upp', 1.0): 2, ('om', 1.0): 1, ('60', 1.0): 1, ('minut', 1.0): 7, ('data', 1.0): 1, ('jesu', 1.0): 5, ('amsterdam', 1.0): 2, ('3rd', 1.0): 3, ('nextweek', 1.0): 1, ('booti', 1.0): 2, ('bcuz', 1.0): 1, ('step', 1.0): 3, ('option', 1.0): 3, ('stabl', 1.0): 1, ('sturdi', 1.0): 1, ('lukkke', 1.0): 1, ('again.ensoi', 1.0): 1, ('tc', 1.0): 1, ('madam', 1.0): 1, ('siddi', 1.0): 1, ('unknown', 1.0): 2, ('roomi', 1.0): 1, ('gn', 1.0): 2, ('gf', 1.0): 2, ('consent', 1.0): 1, ('mister', 1.0): 2, ('vine', 1.0): 2, ('peyton', 1.0): 1, ('nagato', 1.0): 1, ('yuki-chan', 1.0): 1, ('shoushitsu', 1.0): 1, ('archdbanterburi', 1.0): 3, ('experttradesmen', 1.0): 1, ('banter', 1.0): 1, ('quiz', 1.0): 1, ('tradetalk', 1.0): 1, ('floof', 1.0): 1, ('face', 1.0): 13, ('muahah', 1.0): 1, ('x19', 1.0): 1, ('anticip', 1.0): 1, ('jd', 1.0): 1, ('laro', 1.0): 1, ('tayo', 1.0): 1, ('answer', 1.0): 8, ('ht', 1.0): 1, ('angelica', 1.0): 1, ('anghel', 1.0): 1, ('aa', 1.0): 3, ('kkk', 1.0): 1, ('macbook', 1.0): 1, ('rehears', 1.0): 1, ('youthcelebr', 1.0): 1, ('mute', 1.0): 1, ('29th', 1.0): 1, ('gohf', 1.0): 4, ('vegetarian', 1.0): 1, (\"she'll\", 1.0): 1, ('gooday', 1.0): 3, ('101', 1.0): 3, ('12000', 1.0): 1, ('oshieer', 1.0): 1, ('realreview', 1.0): 1, ('happycustom', 1.0): 1, ('realoshi', 1.0): 1, ('dealsuthaonotebachao', 1.0): 1, ('bigger', 1.0): 2, ('dime', 1.0): 1, ('uhuh', 1.0): 1, ('🎵', 1.0): 3, ('code', 1.0): 4, ('pleasant', 1.0): 2, ('on-board', 1.0): 1, ('raheel', 1.0): 1, ('flyhigh', 1.0): 1, ('bother', 1.0): 2, ('everett', 1.0): 1, ('taylor', 1.0): 1, ('ha-ha', 1.0): 1, ('peachyloan', 1.0): 1, ('fridayfreebi', 1.0): 1, ('noe', 1.0): 1, ('yisss', 1.0): 1, ('bindingofissac', 1.0): 1, ('xboxon', 1.0): 1, ('consol', 1.0): 1, ('justin', 1.0): 2, ('gladli', 1.0): 1, ('son', 1.0): 4, ('morocco', 1.0): 1, ('peru', 1.0): 1, ('nxt', 1.0): 1, ('bp', 1.0): 1, ('resort', 1.0): 1, ('x18', 1.0): 1, ('havuuulovey', 1.0): 1, ('uuu', 1.0): 1, ('possitv', 1.0): 1, ('hopey', 1.0): 1, ('throwbackfriday', 1.0): 1, ('christen', 1.0): 1, ('ki', 1.0): 1, ('yaad', 1.0): 1, ('gayi', 1.0): 1, ('opossum', 1.0): 1, ('belat', 1.0): 5, ('yeahh', 1.0): 2, ('kuffar', 1.0): 1, ('comput', 1.0): 5, ('cell', 1.0): 1, ('diarrhea', 1.0): 1, ('immigr', 1.0): 1, ('lice', 1.0): 1, ('goictiv', 1.0): 1, ('70685', 1.0): 1, ('tagsforlik', 1.0): 4, ('trapmus', 1.0): 1, ('hotmusicdeloco', 1.0): 1, ('kinick', 1.0): 1, ('01282', 1.0): 2, ('452096', 1.0): 1, ('shadi', 1.0): 1, ('reserv', 1.0): 3, ('tkt', 1.0): 1, ('likewis', 1.0): 4, ('overgener', 1.0): 1, ('ikr', 1.0): 1, ('😍', 1.0): 2, ('consumer', 1.0): 1, ('fic', 1.0): 2, ('ouch', 1.0): 2, ('slip', 1.0): 1, ('disc', 1.0): 1, ('thw', 1.0): 1, ('chute', 1.0): 1, ('chalut', 1.0): 1, ('replay', 1.0): 1, ('iplay', 1.0): 1, ('11am', 1.0): 3, ('unneed', 1.0): 1, ('megamoh', 1.0): 1, ('7/29', 1.0): 1, ('tool', 1.0): 2, ('zealand', 1.0): 1, ('pile', 1.0): 2, ('dump', 1.0): 1, ('couscou', 1.0): 3, (\"women'\", 1.0): 2, ('fiction', 1.0): 1, ('wahahaah', 1.0): 1, ('x17', 1.0): 1, ('orhan', 1.0): 1, ('pamuk', 1.0): 1, ('hero', 1.0): 3, ('canopi', 1.0): 1, ('mapl', 1.0): 2, ('syrup', 1.0): 1, ('farm', 1.0): 2, ('stephani', 1.0): 2, ('💖', 1.0): 2, ('congrtaualt', 1.0): 1, ('philea', 1.0): 1, ('club', 1.0): 4, ('inc', 1.0): 1, ('photograph', 1.0): 2, ('phonegraph', 1.0): 1, ('srsli', 1.0): 1, ('10:17', 1.0): 1, ('ripaaa', 1.0): 1, ('banat', 1.0): 1, ('ray', 1.0): 1, ('dept', 1.0): 1, ('hospit', 1.0): 3, ('grt', 1.0): 1, ('infograph', 1.0): 1, (\"o'clock\", 1.0): 2, ('habit', 1.0): 1, ('1dfor', 1.0): 1, ('roadtrip', 1.0): 1, ('19:30', 1.0): 1, ('ifc', 1.0): 1, ('whip', 1.0): 1, ('lilsisbro', 1.0): 1, ('pre-ord', 1.0): 2, (\"pixar'\", 1.0): 2, ('steelbook', 1.0): 1, ('hmm', 1.0): 2, ('pegel', 1.0): 1, ('lemess', 1.0): 1, ('kyle', 1.0): 2, ('paypal', 1.0): 1, ('oct', 1.0): 1, ('tud', 1.0): 1, ('jst', 1.0): 2, ('humphrey', 1.0): 1, ('yell', 1.0): 2, ('erm', 1.0): 1, ('breach', 1.0): 1, ('lemon', 1.0): 2, ('yogurt', 1.0): 2, ('pot', 1.0): 1, ('discov', 1.0): 2, ('liquoric', 1.0): 1, ('pud', 1.0): 1, ('cajun', 1.0): 1, ('spice', 1.0): 1, ('yum', 1.0): 2, ('cajunchicken', 1.0): 1, ('infinit', 1.0): 2, ('fight', 1.0): 4, ('gern', 1.0): 1, ('cikaaa', 1.0): 1, ('maaf', 1.0): 1, ('telat', 1.0): 1, ('ngucapinnya', 1.0): 1, ('maaay', 1.0): 1, ('x16', 1.0): 1, ('viparita', 1.0): 1, ('karani', 1.0): 1, ('legsupthewal', 1.0): 1, ('unwind', 1.0): 1, ('coco', 1.0): 3, ('comfi', 1.0): 1, ('jalulu', 1.0): 1, ('rosh', 1.0): 1, ('gla', 1.0): 1, ('pallavi', 1.0): 1, ('nairobi', 1.0): 1, ('hrdstellobama', 1.0): 1, ('region', 1.0): 2, ('civil', 1.0): 1, ('societi', 1.0): 2, ('globe', 1.0): 1, ('hajur', 1.0): 1, ('yayi', 1.0): 2, (\"must'v\", 1.0): 1, ('nerv', 1.0): 1, ('prelim', 1.0): 1, ('costacc', 1.0): 1, ('nwb', 1.0): 1, ('shud', 1.0): 1, ('cold', 1.0): 2, ('hmu', 1.0): 2, ('cala', 1.0): 1, ('brush', 1.0): 1, ('ego', 1.0): 1, ('wherev', 1.0): 1, ('interact', 1.0): 2, ('dongsaeng', 1.0): 1, ('chorong', 1.0): 1, ('friendship', 1.0): 1, ('impress', 1.0): 3, ('dragon', 1.0): 2, ('duck', 1.0): 5, ('mix', 1.0): 5, ('cheetah', 1.0): 1, ('wagga', 1.0): 2, ('coursework', 1.0): 1, ('lorna', 1.0): 1, ('scan', 1.0): 1, ('x12', 1.0): 2, ('canva', 1.0): 2, ('iqbal', 1.0): 1, ('ima', 1.0): 1, ('hon', 1.0): 1, ('aja', 1.0): 1, ('besi', 1.0): 1, ('chati', 1.0): 1, ('phulani', 1.0): 1, ('swasa', 1.0): 1, ('bahari', 1.0): 1, ('jiba', 1.0): 1, ('mumbai', 1.0): 1, ('gujarat', 1.0): 1, ('distrub', 1.0): 1, ('otherwis', 1.0): 5, ('190cr', 1.0): 1, ('inspit', 1.0): 1, ('highest', 1.0): 1, ('holder', 1.0): 1, ('threaten', 1.0): 1, ('daili', 1.0): 2, ('basi', 1.0): 1, ('vr', 1.0): 1, ('angelo', 1.0): 1, ('quezon', 1.0): 1, ('sweatpant', 1.0): 1, ('farbridg', 1.0): 1, ('segalakatakata', 1.0): 1, ('nixu', 1.0): 1, ('begun', 1.0): 1, ('flint', 1.0): 1, ('🍰', 1.0): 5, ('separ', 1.0): 1, ('criticis', 1.0): 1, ('gestur', 1.0): 1, ('pedal', 1.0): 1, ('stroke', 1.0): 1, ('caro', 1.0): 1, ('deposit', 1.0): 1, ('secur', 1.0): 2, ('shock', 1.0): 1, ('coff', 1.0): 2, ('tenerina', 1.0): 1, ('auguri', 1.0): 1, ('iso', 1.0): 1, ('certif', 1.0): 1, ('paralyz', 1.0): 1, ('anxieti', 1.0): 1, (\"it'd\", 1.0): 1, ('develop', 1.0): 3, ('spain', 1.0): 2, ('def', 1.0): 1, ('bantim', 1.0): 1, ('fail', 1.0): 5, ('2ban', 1.0): 1, ('x15', 1.0): 1, ('awkward', 1.0): 2, ('ab', 1.0): 1, ('gale', 1.0): 1, ('founder', 1.0): 1, ('loveyaaah', 1.0): 1, ('⅛', 1.0): 1, ('⅞', 1.0): 1, ('∞', 1.0): 1, ('specialist', 1.0): 1, ('aw', 1.0): 3, ('babyyi', 1.0): 1, ('djstruthmat', 1.0): 1, ('re-cap', 1.0): 1, ('flickr', 1.0): 1, ('tack', 1.0): 2, ('zephbot', 1.0): 1, ('hhahahahaha', 1.0): 1, ('blew', 1.0): 2, ('entir', 1.0): 2, ('vega', 1.0): 3, ('strip', 1.0): 1, ('hahahahahhaha', 1.0): 1, (\"callie'\", 1.0): 1, ('puppi', 1.0): 1, ('owner', 1.0): 2, ('callinganimalabusehotlineasap', 1.0): 1, ('gorefiend', 1.0): 1, ('mythic', 1.0): 1, ('remind', 1.0): 6, ('9:00', 1.0): 1, ('▪', 1.0): 2, ('️bea', 1.0): 1, ('miller', 1.0): 2, ('lockscreen', 1.0): 1, ('mbf', 1.0): 1, ('keesh', 1.0): 1, (\"yesterday'\", 1.0): 1, ('groupi', 1.0): 1, ('bebe', 1.0): 1, ('sizam', 1.0): 1, ('color', 1.0): 5, ('invoic', 1.0): 1, ('kanina', 1.0): 1, ('pong', 1.0): 1, ('umaga', 1.0): 1, ('browser', 1.0): 1, ('typic', 1.0): 2, ('pleass', 1.0): 5, ('leeteuk', 1.0): 1, ('pearl', 1.0): 1, ('thusi', 1.0): 1, ('pour', 1.0): 1, ('milk', 1.0): 2, ('tgv', 1.0): 1, ('pari', 1.0): 5, ('austerlitz', 1.0): 1, ('bloi', 1.0): 1, ('mile', 1.0): 3, ('chateau', 1.0): 1, ('de', 1.0): 1, ('marai', 1.0): 1, ('taxi', 1.0): 1, ('x14', 1.0): 1, ('nom', 1.0): 1, ('enji', 1.0): 1, ('hater', 1.0): 3, ('purchas', 1.0): 2, ('specially-mark', 1.0): 1, ('custard', 1.0): 1, ('sm', 1.0): 1, ('on-pack', 1.0): 1, ('instruct', 1.0): 1, ('tile', 1.0): 1, ('downstair', 1.0): 1, ('kelli', 1.0): 1, ('greek', 1.0): 2, ('petra', 1.0): 1, ('shadowplayloui', 1.0): 1, ('mutual', 1.0): 2, ('cuz', 1.0): 4, ('liveonstream', 1.0): 1, ('lani', 1.0): 1, ('graze', 1.0): 1, ('pride', 1.0): 1, ('bristolart', 1.0): 1, ('in-app', 1.0): 1, ('ensur', 1.0): 1, ('item', 1.0): 2, ('screw', 1.0): 1, ('amber', 1.0): 2, ('43', 1.0): 1, ('hpc', 1.0): 1, ('wip', 1.0): 2, ('sw', 1.0): 1, ('newsround', 1.0): 1, ('hound', 1.0): 1, ('7:40', 1.0): 1, ('ada', 1.0): 1, ('racist', 1.0): 1, ('hulk', 1.0): 1, ('tight', 1.0): 2, ('prayer', 1.0): 3, ('pardon', 1.0): 1, ('phl', 1.0): 1, ('abu', 1.0): 2, ('dhabi', 1.0): 1, ('hihihi', 1.0): 1, ('teamjanuaryclaim', 1.0): 1, ('godonna', 1.0): 1, ('msg', 1.0): 2, ('bowwowchicawowwow', 1.0): 1, ('settl', 1.0): 1, ('dkt', 1.0): 1, ('porch', 1.0): 1, ('uber', 1.0): 2, ('mobil', 1.0): 4, ('applic', 1.0): 3, ('giggl', 1.0): 2, ('bare', 1.0): 3, ('wind', 1.0): 2, ('kahlil', 1.0): 1, ('gibran', 1.0): 1, ('flash', 1.0): 1, ('stiff', 1.0): 1, ('upper', 1.0): 1, ('lip', 1.0): 1, ('britain', 1.0): 1, ('latmon', 1.0): 1, ('endeavour', 1.0): 1, ('ann', 1.0): 2, ('joy', 1.0): 4, ('os', 1.0): 1, ('exploit', 1.0): 1, ('ign', 1.0): 2, ('au', 1.0): 1, ('pubcast', 1.0): 1, ('tengaman', 1.0): 1, ('21', 1.0): 2, ('celebratio', 1.0): 1, ('women', 1.0): 1, ('instal', 1.0): 2, ('glorifi', 1.0): 1, ('infirm', 1.0): 1, ('silli', 1.0): 1, ('suav', 1.0): 1, ('gentlemen', 1.0): 1, ('monthli', 1.0): 1, ('mileag', 1.0): 1, ('target', 1.0): 2, ('samsung', 1.0): 1, ('qualiti', 1.0): 3, ('ey', 1.0): 1, ('beth', 1.0): 2, ('gangster', 1.0): 1, (\"athena'\", 1.0): 1, ('fanci', 1.0): 1, ('wellington', 1.0): 1, ('rich', 1.0): 2, ('christina', 1.0): 1, ('newslett', 1.0): 1, ('zy', 1.0): 1, ('olur', 1.0): 1, ('x13', 1.0): 1, ('flawless', 1.0): 1, ('reaction', 1.0): 2, ('hayli', 1.0): 1, ('edwin', 1.0): 1, ('elvena', 1.0): 1, ('emc', 1.0): 1, ('rubber', 1.0): 3, ('swearword', 1.0): 1, ('infect', 1.0): 1, ('10:16', 1.0): 1, ('wrote', 1.0): 3, ('gan', 1.0): 1, ('brotherhood', 1.0): 1, ('wolf', 1.0): 5, ('pill', 1.0): 1, ('nocturn', 1.0): 1, ('rrp', 1.0): 1, ('18.99', 1.0): 1, ('13.99', 1.0): 1, ('jah', 1.0): 1, ('wobbl', 1.0): 1, ('retard', 1.0): 1, ('50notif', 1.0): 1, ('check-up', 1.0): 1, ('pun', 1.0): 1, ('elit', 1.0): 1, ('camillu', 1.0): 1, ('pleasee', 1.0): 1, ('spare', 1.0): 1, ('tyre', 1.0): 2, ('joke', 1.0): 3, ('ahahah', 1.0): 1, ('shame', 1.0): 1, ('abandon', 1.0): 1, ('disagre', 1.0): 2, ('nowher', 1.0): 2, ('contradict', 1.0): 1, ('chao', 1.0): 1, ('contain', 1.0): 1, ('cranium', 1.0): 1, ('sneaker', 1.0): 1, ('nike', 1.0): 1, ('nikeorigin', 1.0): 1, ('nikeindonesia', 1.0): 1, ('pierojogg', 1.0): 1, ('skoy', 1.0): 1, ('winter', 1.0): 2, ('falkland', 1.0): 1, ('jamie-le', 1.0): 1, ('congraaat', 1.0): 1, ('hooh', 1.0): 1, ('chrome', 1.0): 1, ('storm', 1.0): 1, ('thunderstorm', 1.0): 1, ('circuscircu', 1.0): 1, ('omgg', 1.0): 1, ('tdi', 1.0): 1, ('(-:', 1.0): 2, ('peter', 1.0): 1, ('expel', 1.0): 2, ('boughi', 1.0): 1, ('kernel', 1.0): 1, ('paralysi', 1.0): 1, ('liza', 1.0): 1, ('lol.hook', 1.0): 1, ('vampir', 1.0): 2, ('diari', 1.0): 3, ('twice', 1.0): 1, ('thanq', 1.0): 2, ('goodwil', 1.0): 1, ('vandr', 1.0): 1, ('ash', 1.0): 1, ('debat', 1.0): 3, ('solar', 1.0): 1, ('6-5', 1.0): 1, ('shown', 1.0): 1, ('ek', 1.0): 1, ('taco', 1.0): 2, ('mexico', 1.0): 2, ('viva', 1.0): 1, ('méxico', 1.0): 1, ('burger', 1.0): 3, ('thebestangkapuso', 1.0): 1, ('lighter', 1.0): 1, ('tooth', 1.0): 2, ('korean', 1.0): 2, ('netizen', 1.0): 1, ('crueler', 1.0): 1, ('eleph', 1.0): 1, ('marula', 1.0): 1, ('tdif', 1.0): 1, ('shoutout', 1.0): 1, ('shortli', 1.0): 1, ('itsamarvelth', 1.0): 1, (\"japan'\", 1.0): 1, ('artist', 1.0): 1, ('homework', 1.0): 1, ('marco', 1.0): 1, ('herb', 1.0): 1, ('pm', 1.0): 3, ('self', 1.0): 1, ('esteem', 1.0): 1, ('patienc', 1.0): 1, ('sobtian', 1.0): 1, ('cowork', 1.0): 1, ('deathli', 1.0): 1, ('hallow', 1.0): 1, ('supernatur', 1.0): 1, ('consult', 1.0): 1, ('himach', 1.0): 1, ('2.25', 1.0): 1, ('asham', 1.0): 1, ('where.do.i.start', 1.0): 1, ('moviemarathon', 1.0): 1, ('skill', 1.0): 4, ('shadow', 1.0): 1, ('own', 1.0): 1, ('pair', 1.0): 3, (\"it'll\", 1.0): 6, ('cortez', 1.0): 1, ('superstar', 1.0): 1, ('tthank', 1.0): 1, ('colin', 1.0): 1, ('luxuou', 1.0): 1, ('tarryn', 1.0): 1, ('hbdme', 1.0): 1, ('yeeeyyy', 1.0): 1, ('barsostay', 1.0): 1, ('males', 1.0): 1, ('independ', 1.0): 1, ('sum', 1.0): 1, ('debacl', 1.0): 1, ('perfectli', 1.0): 1, ('longer', 1.0): 2, ('amyjackson', 1.0): 1, ('omegl', 1.0): 2, ('countrymus', 1.0): 1, ('five', 1.0): 2, (\"night'\", 1.0): 2, (\"freddy'\", 1.0): 2, ('demo', 1.0): 2, ('pump', 1.0): 2, ('fanboy', 1.0): 1, ('thegrandad', 1.0): 1, ('sidni', 1.0): 1, ('remarriag', 1.0): 1, ('occas', 1.0): 1, ('languag', 1.0): 1, ('java', 1.0): 1, (\"php'\", 1.0): 1, ('notion', 1.0): 1, ('refer', 1.0): 1, ('confus', 1.0): 3, ('ohioan', 1.0): 1, ('stick', 1.0): 2, ('doctor', 1.0): 3, ('offlin', 1.0): 1, ('thesim', 1.0): 1, ('mb', 1.0): 1, ('meaningless', 1.0): 1, ('common', 1.0): 1, ('celebr', 1.0): 9, ('muertosatfring', 1.0): 1, ('emul', 1.0): 1, ('brought', 1.0): 1, ('enemi', 1.0): 2, ('relax', 1.0): 3, ('ou', 1.0): 1, ('pink', 1.0): 2, ('cc', 1.0): 2, ('meooowww', 1.0): 1, ('barkkkiiidee', 1.0): 1, ('bark', 1.0): 1, ('x11', 1.0): 1, ('routin', 1.0): 4, ('alek', 1.0): 1, ('awh', 1.0): 2, ('kumpul', 1.0): 1, ('cantik', 1.0): 1, ('ganteng', 1.0): 1, ('kresna', 1.0): 1, ('jelli', 1.0): 1, ('simon', 1.0): 1, ('lesley', 1.0): 3, ('blood', 1.0): 2, ('panti', 1.0): 1, ('lion', 1.0): 1, ('artworkbyli', 1.0): 1, ('judo', 1.0): 1, ('daredevil', 1.0): 2, ('despond', 1.0): 1, ('re-watch', 1.0): 1, ('welcoma.hav', 1.0): 1, ('favor', 1.0): 5, ('tridon', 1.0): 1, ('21pic', 1.0): 1, ('master', 1.0): 3, ('nim', 1.0): 1, (\"there'r\", 1.0): 1, ('22pic', 1.0): 1, ('kebun', 1.0): 1, ('ubud', 1.0): 1, ('ladyposs', 1.0): 1, ('xoxoxo', 1.0): 1, ('sneak', 1.0): 3, ('peek', 1.0): 2, ('inbox', 1.0): 1, ('happyweekend', 1.0): 1, ('therealgolden', 1.0): 1, ('47', 1.0): 1, ('girlfriendsmya', 1.0): 1, ('ppl', 1.0): 2, ('closest', 1.0): 1, ('njoy', 1.0): 1, ('followingg', 1.0): 1, ('privat', 1.0): 1, ('pusher', 1.0): 1, ('stun', 1.0): 4, ('wooohooo', 1.0): 1, ('cuss', 1.0): 1, ('teenag', 1.0): 1, ('ace', 1.0): 1, ('sauc', 1.0): 3, ('livi', 1.0): 1, ('fowl', 1.0): 1, ('oliviafowl', 1.0): 1, ('891', 1.0): 1, ('burnout', 1.0): 1, ('johnforceo', 1.0): 1, ('matthew', 1.0): 1, ('provok', 1.0): 1, ('indiankultur', 1.0): 1, ('oppos', 1.0): 1, ('biker', 1.0): 1, ('lyk', 1.0): 1, ('gud', 1.0): 4, ('weight', 1.0): 6, ('bcu', 1.0): 1, ('rubbish', 1.0): 1, ('veggi', 1.0): 2, ('steph', 1.0): 1, ('nj', 1.0): 1, ('x10', 1.0): 1, ('cohes', 1.0): 1, ('gossip', 1.0): 2, ('alex', 1.0): 3, ('heswifi', 1.0): 1, ('7am', 1.0): 1, ('wub', 1.0): 1, ('cerbchan', 1.0): 1, ('jarraaa', 1.0): 1, ('morrrn', 1.0): 1, ('snooz', 1.0): 1, ('clicksco', 1.0): 1, ('gay', 1.0): 4, ('lesbian', 1.0): 2, ('rigid', 1.0): 1, ('theocrat', 1.0): 1, ('wing', 1.0): 1, ('fundamentalist', 1.0): 1, ('islamist', 1.0): 1, ('brianaaa', 1.0): 1, ('brianazabrocki', 1.0): 1, ('sky', 1.0): 2, ('batb', 1.0): 1, ('clap', 1.0): 3, ('whilst', 1.0): 1, ('aki', 1.0): 1, ('thencerest', 1.0): 2, ('547', 1.0): 2, ('indiemus', 1.0): 5, ('sexyjudi', 1.0): 3, ('pussi', 1.0): 4, ('sexo', 1.0): 3, ('humid', 1.0): 1, ('87', 1.0): 1, ('sloppi', 1.0): 1, (\"second'\", 1.0): 1, ('stock', 1.0): 3, ('marmit', 1.0): 2, ('x9', 1.0): 1, ('nic', 1.0): 3, ('taft', 1.0): 1, ('finalist', 1.0): 1, ('lotteri', 1.0): 1, ('award', 1.0): 3, ('usagi', 1.0): 1, ('looov', 1.0): 1, ('wowww', 1.0): 2, ('💙', 1.0): 8, ('💚', 1.0): 8, ('💕', 1.0): 12, ('lepa', 1.0): 1, ('sembuh', 1.0): 1, ('sibuk', 1.0): 1, ('balik', 1.0): 1, ('kin', 1.0): 1, ('gotham', 1.0): 1, ('sunnyday', 1.0): 1, ('dudett', 1.0): 1, ('cost', 1.0): 1, ('flippin', 1.0): 1, ('fortun', 1.0): 1, ('divinediscont', 1.0): 1, (';}', 1.0): 1, ('amnot', 1.0): 1, ('autofollow', 1.0): 3, ('teamfollowback', 1.0): 4, ('geer', 1.0): 1, ('bat', 1.0): 2, ('mz', 1.0): 1, ('yang', 1.0): 2, ('deennya', 1.0): 1, ('jehwan', 1.0): 1, ('11:00', 1.0): 1, ('ashton', 1.0): 1, ('✧', 1.0): 12, ('｡', 1.0): 4, ('chelni', 1.0): 2, ('datz', 1.0): 1, ('jeremi', 1.0): 1, ('fmt', 1.0): 1, ('dat', 1.0): 3, ('heartbeat', 1.0): 1, ('clutch', 1.0): 1, ('🐢', 1.0): 2, ('besteverdoctorwhoepisod', 1.0): 1, ('relev', 1.0): 1, ('puke', 1.0): 1, ('proper', 1.0): 1, ('x8', 1.0): 1, ('sublimin', 1.0): 1, ('eatmeat', 1.0): 1, ('brewproject', 1.0): 1, ('lovenafianna', 1.0): 1, ('mr', 1.0): 7, ('lewi', 1.0): 1, ('clock', 1.0): 1, ('3:02', 1.0): 2, ('muslim', 1.0): 1, ('prophet', 1.0): 1, ('غردلي', 1.0): 4, ('is.h', 1.0): 1, ('mistak', 1.0): 4, ('understood', 1.0): 1, ('politician', 1.0): 1, ('argu', 1.0): 1, ('intellect', 1.0): 1, ('shiva', 1.0): 1, ('mp3', 1.0): 1, ('standrew', 1.0): 1, ('sandcastl', 1.0): 1, ('ewok', 1.0): 1, ('nate', 1.0): 2, ('brawl', 1.0): 1, ('rear', 1.0): 1, ('nake', 1.0): 1, ('choke', 1.0): 1, ('heck', 1.0): 1, ('gun', 1.0): 2, ('associ', 1.0): 1, ('um', 1.0): 1, ('endow', 1.0): 1, ('ai', 1.0): 1, ('sikandar', 1.0): 1, ('pti', 1.0): 1, ('standwdik', 1.0): 1, ('westandwithik', 1.0): 1, ('starbuck', 1.0): 2, ('logo', 1.0): 2, ('renew', 1.0): 1, ('chariti', 1.0): 1, ('جمعة_مباركة', 1.0): 1, ('hoki', 1.0): 1, ('biz', 1.0): 1, ('non', 1.0): 1, ('america', 1.0): 1, ('california', 1.0): 1, ('01:16', 1.0): 1, ('45gameplay', 1.0): 2, ('ilovey', 1.0): 2, ('vex', 1.0): 1, ('iger', 1.0): 1, ('leicaq', 1.0): 1, ('leica', 1.0): 1, ('dudee', 1.0): 1, ('persona', 1.0): 1, ('yepp', 1.0): 1, ('5878e503', 1.0): 1, ('x7', 1.0): 1, ('greg', 1.0): 1, ('posey', 1.0): 1, ('miami', 1.0): 1, ('james_yammouni', 1.0): 1, ('breakdown', 1.0): 1, ('materi', 1.0): 2, ('thorin', 1.0): 1, ('hunt', 1.0): 1, ('choroo', 1.0): 1, ('nahi', 1.0): 2, ('aztec', 1.0): 1, ('princess', 1.0): 2, ('raini', 1.0): 1, ('kingfish', 1.0): 1, ('chinua', 1.0): 1, ('acheb', 1.0): 1, ('intellectu', 1.0): 2, ('liquid', 1.0): 1, ('melbournetrip', 1.0): 1, ('taxikitchen', 1.0): 1, ('nooow', 1.0): 2, ('mcdo', 1.0): 1, ('everywher', 1.0): 2, ('dreamer', 1.0): 1, ('tanisha', 1.0): 1, ('1nonli', 1.0): 1, ('attitud', 1.0): 1, ('kindl', 1.0): 2, ('flame', 1.0): 1, ('convict', 1.0): 1, ('bar', 1.0): 1, ('repath', 1.0): 2, ('adi', 1.0): 1, ('stefani', 1.0): 1, ('sg1', 1.0): 1, ('lightbox', 1.0): 1, ('ran', 1.0): 2, ('incorrect', 1.0): 1, ('apologist', 1.0): 1, ('x6', 1.0): 1, ('vuli', 1.0): 1, ('01:15', 1.0): 1, ('batman', 1.0): 1, ('pearson', 1.0): 1, ('reput', 1.0): 2, ('nikkei', 1.0): 1, ('woodford', 1.0): 1, ('vscocam', 1.0): 1, ('vscoph', 1.0): 1, ('vscogood', 1.0): 1, ('vscophil', 1.0): 1, ('vscocousin', 1.0): 1, ('yaap', 1.0): 1, ('urwelc', 1.0): 1, ('neon', 1.0): 1, ('pant', 1.0): 1, ('haaa', 1.0): 1, ('will', 1.0): 2, ('auspost', 1.0): 1, ('openfollow', 1.0): 1, ('rp', 1.0): 2, ('eng', 1.0): 1, ('yūjō-cosplay', 1.0): 1, ('luxembourg', 1.0): 1, ('bunni', 1.0): 1, ('broadcast', 1.0): 1, ('needa', 1.0): 1, ('gal', 1.0): 3, ('bend', 1.0): 3, ('heaven', 1.0): 2, ('score', 1.0): 2, ('januari', 1.0): 1, ('hanabutl', 1.0): 1, ('kikhorni', 1.0): 1, ('interraci', 1.0): 1, ('makeup', 1.0): 1, ('chu', 1.0): 1, (\"weekend'\", 1.0): 1, ('punt', 1.0): 1, ('horserac', 1.0): 1, ('hors', 1.0): 2, ('horseracingtip', 1.0): 1, ('guitar', 1.0): 1, ('cocoar', 1.0): 1, ('brief', 1.0): 1, ('introduct', 1.0): 1, ('earliest', 1.0): 1, ('indian', 1.0): 1, ('subcontin', 1.0): 1, ('bfr', 1.0): 1, ('maurya', 1.0): 1, ('jordanian', 1.0): 1, ('00962778381', 1.0): 1, ('838', 1.0): 1, ('tenyai', 1.0): 1, ('hee', 1.0): 2, ('ss', 1.0): 1, ('semi', 1.0): 1, ('atp', 1.0): 2, ('wimbledon', 1.0): 2, ('feder', 1.0): 1, ('nadal', 1.0): 1, ('monfil', 1.0): 1, ('handsom', 1.0): 2, ('cilic', 1.0): 3, ('firm', 1.0): 1, ('potenti', 1.0): 3, ('nyc', 1.0): 1, ('chillin', 1.0): 2, ('tail', 1.0): 2, ('kitten', 1.0): 1, ('garret', 1.0): 1, ('baz', 1.0): 1, ('leo', 1.0): 2, ('xst', 1.0): 1, ('centrifug', 1.0): 1, ('etern', 1.0): 3, ('forgiv', 1.0): 2, ('kangin', 1.0): 1, ('بندر', 1.0): 1, ('العنزي', 1.0): 1, ('kristin', 1.0): 1, ('cass', 1.0): 1, ('surajettan', 1.0): 1, ('kashi', 1.0): 1, ('ashwathi', 1.0): 1, ('mommi', 1.0): 2, ('tirth', 1.0): 1, ('brambhatt', 1.0): 1, ('snooker', 1.0): 1, ('compens', 1.0): 1, ('theoper', 1.0): 1, ('479', 1.0): 1, ('premiostumundo', 1.0): 2, ('philosoph', 1.0): 1, ('x5', 1.0): 1, ('graphic', 1.0): 2, ('level', 1.0): 1, ('aug', 1.0): 3, ('excl', 1.0): 1, ('raw', 1.0): 1, ('weeni', 1.0): 1, ('annoyingbabi', 1.0): 1, ('lazi', 1.0): 2, ('cosi', 1.0): 1, ('client_amends_edit', 1.0): 1, ('_5_final_final_fin', 1.0): 1, ('pdf', 1.0): 1, ('mauliat', 1.0): 1, ('ito', 1.0): 2, ('okkay', 1.0): 1, ('knock', 1.0): 3, (\"soloist'\", 1.0): 1, ('ryu', 1.0): 1, ('saera', 1.0): 1, ('pinkeu', 1.0): 1, ('angri', 1.0): 3, ('screencap', 1.0): 1, ('jonghyun', 1.0): 1, ('seungyeon', 1.0): 1, ('cnblue', 1.0): 1, ('mbc', 1.0): 1, ('wgm', 1.0): 1, ('masa', 1.0): 2, ('entrepreneurship', 1.0): 1, ('empow', 1.0): 1, ('limpopo', 1.0): 1, ('pict', 1.0): 1, ('norapowel', 1.0): 1, ('hornykik', 1.0): 2, ('livesex', 1.0): 1, ('pumpkin', 1.0): 1, ('thrice', 1.0): 1, ('patron', 1.0): 1, ('ventur', 1.0): 1, ('deathcur', 1.0): 1, ('boob', 1.0): 1, ('blame', 1.0): 1, ('dine', 1.0): 1, ('modern', 1.0): 1, ('grill', 1.0): 1, ('disk', 1.0): 1, ('nt4', 1.0): 1, ('iirc', 1.0): 1, ('ux', 1.0): 1, ('refin', 1.0): 1, ('zdp', 1.0): 1, ('didnt', 1.0): 2, ('justic', 1.0): 1, ('daw', 1.0): 1, ('tine', 1.0): 1, ('gensan', 1.0): 1, ('frightl', 1.0): 1, ('undead', 1.0): 1, ('plush', 1.0): 1, ('cushion', 1.0): 1, ('nba', 1.0): 3, ('2k15', 1.0): 3, ('mypark', 1.0): 3, ('chronicl', 1.0): 4, ('gryph', 1.0): 3, ('volum', 1.0): 3, ('ellen', 1.0): 1, ('degener', 1.0): 1, ('shirt', 1.0): 1, ('mint', 1.0): 1, ('superdri', 1.0): 1, ('berangkaat', 1.0): 1, ('lagiii', 1.0): 1, ('siguro', 1.0): 1, ('un', 1.0): 1, ('kesa', 1.0): 1, ('lotsa', 1.0): 2, ('organis', 1.0): 2, ('4am', 1.0): 1, ('fingers-cross', 1.0): 1, ('deep', 1.0): 1, ('htaccess', 1.0): 1, ('file', 1.0): 2, ('adf', 1.0): 1, ('womad', 1.0): 1, ('gran', 1.0): 1, ('canaria', 1.0): 1, ('gig', 1.0): 1, ('twist', 1.0): 1, ('youv', 1.0): 1, ('teamnatur', 1.0): 1, ('huni', 1.0): 1, ('yayayayay', 1.0): 1, ('yt', 1.0): 2, ('convent', 1.0): 1, ('brighton', 1.0): 1, ('slay', 1.0): 1, ('nicknam', 1.0): 1, ('babygirl', 1.0): 1, ('regard', 1.0): 2, ('himmat', 1.0): 1, ('karain', 1.0): 2, ('baat', 1.0): 1, ('meri', 1.0): 1, ('hotee-mi', 1.0): 1, ('uncl', 1.0): 1, ('tongu', 1.0): 1, ('pronounc', 1.0): 1, ('nativ', 1.0): 1, ('american', 1.0): 2, ('proverb', 1.0): 1, ('lovabl', 1.0): 1, ('yesha', 1.0): 1, ('montoya', 1.0): 1, ('eagerli', 1.0): 1, ('payment', 1.0): 1, ('suprem', 1.0): 1, ('leon', 1.0): 1, ('ks', 1.0): 2, ('randi', 1.0): 1, ('9bi', 1.0): 1, ('physiqu', 1.0): 1, ('shave', 1.0): 1, ('uncut', 1.0): 1, ('boi', 1.0): 1, ('cheapest', 1.0): 1, ('regular', 1.0): 3, ('printer', 1.0): 3, ('nz', 1.0): 1, ('larg', 1.0): 4, ('format', 1.0): 1, ('10/10', 1.0): 1, ('senior', 1.0): 1, ('raid', 1.0): 2, ('conserv', 1.0): 1, ('batteri', 1.0): 1, ('comfort', 1.0): 2, ('swt', 1.0): 1, ('reservations@sandsbeach.eu', 1.0): 1, ('localgaragederbi', 1.0): 1, ('campu', 1.0): 1, ('subgam', 1.0): 1, ('faceit', 1.0): 1, ('snpcaht', 1.0): 1, ('hakhakhak', 1.0): 1, ('t___t', 1.0): 1, (\"kyungsoo'\", 1.0): 1, ('3d', 1.0): 2, ('properti', 1.0): 2, ('agent', 1.0): 1, ('accur', 1.0): 1, ('descript', 1.0): 1, ('theori', 1.0): 1, ('x4', 1.0): 1, ('15.90', 1.0): 1, ('yvett', 1.0): 1, ('author', 1.0): 2, ('mwf', 1.0): 1, ('programm', 1.0): 1, ('taal', 1.0): 1, ('lake', 1.0): 1, ('2emt', 1.0): 1, ('«', 1.0): 2, ('scurri', 1.0): 1, ('agil', 1.0): 1, ('solut', 1.0): 1, ('sme', 1.0): 1, ('omar', 1.0): 1, ('biggest', 1.0): 5, ('kamaal', 1.0): 1, ('amm', 1.0): 1, ('3am', 1.0): 1, ('hopehousekid', 1.0): 1, ('pitmantrain', 1.0): 1, ('walkersmithway', 1.0): 1, ('keepitloc', 1.0): 2, ('sehun', 1.0): 1, ('se100lead', 1.0): 1, ('unev', 1.0): 1, ('sofa', 1.0): 1, ('surf', 1.0): 1, ('cunt', 1.0): 1, ('rescoop', 1.0): 1, ('multiraci', 1.0): 1, ('fk', 1.0): 1, ('narrow', 1.0): 1, ('warlock', 1.0): 1, ('balloon', 1.0): 3, ('mj', 1.0): 1, ('madison', 1.0): 1, ('beonknockknock', 1.0): 1, ('con-gradu', 1.0): 1, ('gent', 1.0): 1, ('bitchfac', 1.0): 1, ('😒', 1.0): 1, ('organ', 1.0): 1, ('12pm', 1.0): 2, ('york', 1.0): 2, ('nearest', 1.0): 1, ('lendal', 1.0): 1, ('pikami', 1.0): 1, ('captur', 1.0): 1, ('fulton', 1.0): 1, ('sheen', 1.0): 1, ('baloney', 1.0): 1, ('unvarnish', 1.0): 1, ('laid', 1.0): 2, ('thick', 1.0): 1, ('blarney', 1.0): 1, ('flatteri', 1.0): 1, ('thin', 1.0): 1, ('sachin', 1.0): 1, ('unimport', 1.0): 1, ('context', 1.0): 1, ('dampen', 1.0): 1, ('yu', 1.0): 1, ('rocket', 1.0): 1, ('narendra', 1.0): 1, ('modi', 1.0): 1, ('aaaand', 1.0): 1, (\"team'\", 1.0): 1, ('macauley', 1.0): 1, ('howev', 1.0): 3, ('x3', 1.0): 1, ('wheeen', 1.0): 1, ('heechul', 1.0): 1, ('toast', 1.0): 2, ('coffee-weekday', 1.0): 1, ('9-11', 1.0): 1, ('sail', 1.0): 1, (\"friday'\", 1.0): 1, ('commerci', 1.0): 1, ('insur', 1.0): 1, ('requir', 1.0): 2, ('lookfortheo', 1.0): 1, ('cl', 1.0): 1, ('thou', 1.0): 1, ('april', 1.0): 2, ('airforc', 1.0): 1, ('clark', 1.0): 1, ('field', 1.0): 1, ('pampanga', 1.0): 1, ('troll', 1.0): 1, ('⚡', 1.0): 1, ('brow', 1.0): 1, ('oili', 1.0): 1, ('maricarljanah', 1.0): 1, ('6:15', 1.0): 1, ('degre', 1.0): 3, ('fahrenheit', 1.0): 1, ('🍸', 1.0): 7, ('╲', 1.0): 4, ('─', 1.0): 8, ('╱', 1.0): 5, ('🍤', 1.0): 4, ('╭', 1.0): 4, ('╮', 1.0): 4, ('┓', 1.0): 2, ('┳', 1.0): 1, ('┣', 1.0): 1, ('╰', 1.0): 3, ('╯', 1.0): 3, ('┗', 1.0): 2, ('┻', 1.0): 1, ('stool', 1.0): 1, ('toppl', 1.0): 1, ('findyourfit', 1.0): 1, ('prefer', 1.0): 2, ('whomosexu', 1.0): 1, ('stack', 1.0): 1, ('pandora', 1.0): 3, ('digitalexet', 1.0): 1, ('digitalmarket', 1.0): 1, ('sociamedia', 1.0): 1, ('nb', 1.0): 1, ('bom', 1.0): 1, ('dia', 1.0): 1, ('todo', 1.0): 1, ('forklift', 1.0): 1, ('warehous', 1.0): 1, ('worker', 1.0): 1, ('lsceen', 1.0): 1, ('immatur', 1.0): 1, ('gandhi', 1.0): 1, ('grassi', 1.0): 1, ('feetblog', 1.0): 2, ('daughter', 1.0): 3, ('4yr', 1.0): 1, ('old-porridg', 1.0): 1, ('fiend', 1.0): 1, ('2nite', 1.0): 1, ('comp', 1.0): 1, ('vike', 1.0): 1, ('t20blast', 1.0): 1, ('np', 1.0): 1, ('tax', 1.0): 1, ('ooohh', 1.0): 1, ('petjam', 1.0): 1, ('virtual', 1.0): 2, ('pounc', 1.0): 1, ('bentek', 1.0): 1, ('agn', 1.0): 1, ('socialmedia@dpdgroup.co.uk', 1.0): 1, ('sam', 1.0): 3, ('fruiti', 1.0): 1, ('vodka', 1.0): 2, ('sellyourcarin', 1.0): 2, ('5word', 1.0): 2, ('chaloniklo', 1.0): 2, ('pic.twitter.com/jxz2lbv6o', 1.0): 1, (\"paperwhite'\", 1.0): 1, ('laser-lik', 1.0): 1, ('focu', 1.0): 1, ('ghost', 1.0): 3, ('tagsforlikesapp', 1.0): 2, ('instagood', 1.0): 2, ('tbt', 1.0): 1, ('socket', 1.0): 1, ('spanner', 1.0): 1, ('😴', 1.0): 1, ('pglcsgo', 1.0): 1, ('x2', 1.0): 1, ('tend', 1.0): 1, ('crave', 1.0): 1, ('slower', 1.0): 1, ('sjw', 1.0): 1, ('cakehamp', 1.0): 1, ('glow', 1.0): 2, ('yayyy', 1.0): 1, ('merced', 1.0): 1, ('hood', 1.0): 1, ('badg', 1.0): 1, ('host', 1.0): 1, ('drone', 1.0): 1, ('blow', 1.0): 1, ('ignor', 1.0): 1, ('retali', 1.0): 1, ('bolling', 1.0): 1, (\"where'\", 1.0): 1, ('denmark', 1.0): 1, ('whitey', 1.0): 1, ('cultur', 1.0): 2, ('course', 1.0): 1, ('intro', 1.0): 2, ('graphicdesign', 1.0): 1, ('videograph', 1.0): 1, ('space', 1.0): 2, (\"ted'\", 1.0): 1, ('bogu', 1.0): 1, ('1000', 1.0): 1, ('hahahaaah', 1.0): 1, ('owli', 1.0): 1, ('afternon', 1.0): 1, ('whangarei', 1.0): 1, ('kati', 1.0): 2, ('paulin', 1.0): 1, ('traffick', 1.0): 1, ('wors', 1.0): 3, ('henc', 1.0): 1, ('express', 1.0): 1, ('wot', 1.0): 1, ('hand-lett', 1.0): 1, ('roof', 1.0): 1, ('eas', 1.0): 1, ('2/2', 1.0): 1, ('sour', 1.0): 1, ('dough', 1.0): 1, ('egypt', 1.0): 1, ('hubbi', 1.0): 2, ('sakin', 1.0): 1, ('six', 1.0): 1, ('christma', 1.0): 2, ('avril', 1.0): 1, ('n04j', 1.0): 1, ('25', 1.0): 1, ('prosecco', 1.0): 1, ('pech', 1.0): 1, ('micro', 1.0): 1, ('catspj', 1.0): 1, ('4:15', 1.0): 1, ('lazyweekend', 1.0): 1, ('overdu', 1.0): 1, ('mice', 1.0): 1, ('💃', 1.0): 3, ('jurass', 1.0): 1, ('ding', 1.0): 1, ('nila', 1.0): 1, ('8)', 1.0): 1, ('cooki', 1.0): 1, ('shir', 1.0): 1, ('0', 1.0): 3, ('hale', 1.0): 1, ('cheshir', 1.0): 1, ('decor', 1.0): 1, ('lemm', 1.0): 2, ('rec', 1.0): 1, ('ingat', 1.0): 1, ('din', 1.0): 2, ('mono', 1.0): 1, ('kathryn', 1.0): 1, ('jr', 1.0): 1, ('hsr', 1.0): 1, ('base', 1.0): 3, ('major', 1.0): 1, ('sugarrush', 1.0): 1, ('knit', 1.0): 1, ('partli', 1.0): 1, ('homegirl', 1.0): 1, ('nanci', 1.0): 1, ('fenja', 1.0): 1, ('aapk', 1.0): 1, ('benchmark', 1.0): 1, ('ke', 1.0): 1, ('hisaab', 1.0): 1, ('ho', 1.0): 1, ('gaya', 1.0): 1, ('ofc', 1.0): 1, ('rtss', 1.0): 1, ('hwait', 1.0): 1, ('titanfal', 1.0): 1, ('xbox', 1.0): 2, ('ultim', 1.0): 2, ('gastronomi', 1.0): 1, ('newblogpost', 1.0): 1, ('foodiefriday', 1.0): 1, ('foodi', 1.0): 1, ('yoghurt', 1.0): 1, ('pancak', 1.0): 2, ('sabah', 1.0): 3, ('kapima', 1.0): 1, ('gelen', 1.0): 1, ('guzel', 1.0): 1, ('bir', 1.0): 1, ('hediy', 1.0): 1, ('thanx', 1.0): 1, ('💞', 1.0): 2, ('visa', 1.0): 1, ('parisa', 1.0): 1, ('epiphani', 1.0): 1, ('lit', 1.0): 1, ('em-con', 1.0): 1, ('swore', 1.0): 1, ('0330 333 7234', 1.0): 1, ('kianweareproud', 1.0): 1, ('distract', 1.0): 1, ('dayofarch', 1.0): 1, ('10-20', 1.0): 1, ('bapu', 1.0): 1, ('ivypowel', 1.0): 1, ('newmus', 1.0): 1, ('sexchat', 1.0): 1, ('🍅', 1.0): 1, ('pathway', 1.0): 1, ('balkan', 1.0): 1, ('gypsi', 1.0): 1, ('mayhem', 1.0): 1, ('burek', 1.0): 1, ('meat', 1.0): 1, ('gibanica', 1.0): 1, ('pie', 1.0): 1, ('surrey', 1.0): 1, ('afterward', 1.0): 1, ('10.30', 1.0): 1, ('tempor', 1.0): 1, ('void', 1.0): 1, ('stem', 1.0): 1, ('sf', 1.0): 1, ('ykr', 1.0): 1, ('sparki', 1.0): 1, ('40mm', 1.0): 1, ('3.5', 1.0): 1, ('gr', 1.0): 1, ('rockfish', 1.0): 1, ('topwat', 1.0): 1, ('twitlong', 1.0): 1, ('me.so', 1.0): 1, ('jummah', 1.0): 3, ('durood', 1.0): 1, ('pak', 1.0): 1, ('cjradacomateada', 1.0): 2, ('supris', 1.0): 1, ('debut', 1.0): 1, ('shipper', 1.0): 1, ('asid', 1.0): 1, ('housem', 1.0): 1, ('737bigatingconcert', 1.0): 1, ('jedzjabłka', 1.0): 1, ('pijjabłka', 1.0): 1, ('polish', 1.0): 1, ('cider', 1.0): 1, ('mustread', 1.0): 1, ('cricket', 1.0): 1, ('5pm', 1.0): 1, ('queri', 1.0): 2, ('abbi', 1.0): 1, ('sumedh', 1.0): 1, ('sunnah', 1.0): 2, ('عن', 1.0): 2, ('quad', 1.0): 1, ('bike', 1.0): 1, ('carri', 1.0): 2, ('proprieti', 1.0): 1, ('chronic', 1.0): 1, ('superday', 1.0): 1, ('chocolatey', 1.0): 1, ('yasu', 1.0): 1, ('ooooh', 1.0): 1, ('hallo', 1.0): 2, ('dylan', 1.0): 2, ('laura', 1.0): 1, ('patric', 1.0): 2, ('keepin', 1.0): 1, ('mohr', 1.0): 1, ('guest', 1.0): 1, (\"o'neal\", 1.0): 1, ('tk', 1.0): 1, ('lua', 1.0): 1, ('stone', 1.0): 2, ('quicker', 1.0): 1, ('diet', 1.0): 1, ('sosweet', 1.0): 1, ('nominier', 1.0): 1, ('und', 1.0): 1, ('hardcor', 1.0): 1, ('😌', 1.0): 1, ('ff__special', 1.0): 1, ('acha', 1.0): 2, ('banda', 1.0): 1, ('✌', 1.0): 1, ('bhi', 1.0): 2, ('krta', 1.0): 1, ('beautifully-craft', 1.0): 1, ('mockingbird', 1.0): 1, ('diploma', 1.0): 1, ('blend', 1.0): 3, ('numbero', 1.0): 1, ('lolz', 1.0): 1, ('ambros', 1.0): 1, ('gwinett', 1.0): 1, ('bierc', 1.0): 1, ('ravag', 1.0): 1, ('illadvis', 1.0): 1, ('marriag', 1.0): 1, ('stare', 1.0): 1, ('cynic', 1.0): 2, ('yahuda', 1.0): 1, ('nosmet', 1.0): 1, ('poni', 1.0): 1, ('cuuut', 1.0): 1, (\"f'ing\", 1.0): 1, ('vacant', 1.0): 1, ('hauc', 1.0): 1, ('lovesss', 1.0): 1, ('hiss', 1.0): 1, ('overnight', 1.0): 1, ('cornish', 1.0): 1, ('all-clear', 1.0): 1, ('raincoat', 1.0): 1, ('measur', 1.0): 1, ('wealth', 1.0): 1, ('invest', 1.0): 2, ('garbi', 1.0): 1, ('wash', 1.0): 2, ('refuel', 1.0): 1, ('dunedin', 1.0): 1, ('kall', 1.0): 1, ('rakhi', 1.0): 1, ('12th', 1.0): 2, ('repres', 1.0): 3, ('slovenia', 1.0): 1, ('fridg', 1.0): 2, ('ludlow', 1.0): 1, ('28th', 1.0): 1, ('selway', 1.0): 1, ('submit', 1.0): 1, ('spanish', 1.0): 2, ('90210', 1.0): 1, ('oitnb', 1.0): 1, ('prepar', 1.0): 3, ('condit', 1.0): 1, ('msged', 1.0): 1, ('chiquito', 1.0): 1, ('ohaha', 1.0): 1, ('delhi', 1.0): 1, ('95', 1.0): 1, ('webtogsaward', 1.0): 1, ('grace', 1.0): 2, ('sheffield', 1.0): 1, ('tramlin', 1.0): 1, ('tl', 1.0): 2, ('hack', 1.0): 1, ('lad', 1.0): 1, ('beeepin', 1.0): 1, ('duper', 1.0): 1, ('handl', 1.0): 1, ('critiqu', 1.0): 1, ('contectu', 1.0): 1, ('ultor', 1.0): 2, ('mamaya', 1.0): 1, ('loiyal', 1.0): 1, ('para', 1.0): 1, ('truthfulwordsof', 1.0): 1, ('beanatividad', 1.0): 1, ('nknkkpagpapakumbaba', 1.0): 1, ('birthdaypres', 1.0): 1, ('compliment', 1.0): 1, ('swerv', 1.0): 1, ('goodtim', 1.0): 1, ('sinist', 1.0): 1, ('scare', 1.0): 1, ('tryna', 1.0): 1, ('anonym', 1.0): 1, ('dipsatch', 1.0): 1, ('aunt', 1.0): 1, ('dagga', 1.0): 1, ('burket', 1.0): 1, ('2am', 1.0): 1, ('twine', 1.0): 1, (\"diane'\", 1.0): 1, ('happybirthday', 1.0): 1, ('thanksss', 1.0): 1, ('randomli', 1.0): 1, ('buckinghampalac', 1.0): 1, ('chibi', 1.0): 1, ('maker', 1.0): 1, ('timog', 1.0): 1, ('18th', 1.0): 1, ('otw', 1.0): 1, ('kami', 1.0): 1, ('feelinggood', 1.0): 1, ('demand', 1.0): 2, ('naman', 1.0): 1, ('barkin', 1.0): 1, ('yeap', 1.0): 2, ('onkey', 1.0): 1, ('umma', 1.0): 1, ('pervert', 1.0): 1, ('onyu', 1.0): 1, ('appa', 1.0): 1, ('luci', 1.0): 1, ('horribl', 1.0): 1, ('quantum', 1.0): 1, ('greater', 1.0): 1, ('blockchain', 1.0): 1, ('nowplay', 1.0): 1, ('loftey', 1.0): 1, ('routt', 1.0): 1, ('assia', 1.0): 1, ('.\\n.\\n.', 1.0): 1, ('joint', 1.0): 1, ('futurereleas', 1.0): 1, (\"look'\", 1.0): 1, ('scari', 1.0): 1, ('murder', 1.0): 1, ('mysteri', 1.0): 1, ('comma', 1.0): 1, (\"j'\", 1.0): 1, ('hunni', 1.0): 2, ('diva', 1.0): 1, ('emili', 1.0): 3, ('nathan', 1.0): 1, ('medit', 1.0): 1, ('alumni', 1.0): 1, ('mba', 1.0): 1, ('foto', 1.0): 1, ('what-is-your-fashion', 1.0): 1, ('lorenangel', 1.0): 1, ('kw', 1.0): 2, ('tellanoldjokeday', 1.0): 1, ('reqd', 1.0): 1, ('specul', 1.0): 1, ('consist', 1.0): 4, ('tropic', 1.0): 1, ('startupph', 1.0): 1, ('zodiac', 1.0): 1, ('rapunzel', 1.0): 1, ('therver', 1.0): 1, ('85552', 1.0): 1, ('bestoftheday', 1.0): 1, ('oralsex', 1.0): 1, ('carli', 1.0): 1, ('happili', 1.0): 1, ('contract', 1.0): 1, ('matsu_bouzu', 1.0): 1, ('sonic', 1.0): 2, ('videogam', 1.0): 1, ('harana', 1.0): 1, ('belfast', 1.0): 1, ('danni', 1.0): 1, ('rare', 1.0): 1, ('sponsorship', 1.0): 1, ('aswel', 1.0): 1, ('gigi', 1.0): 1, ('nick', 1.0): 1, ('austin', 1.0): 1, ('youll', 1.0): 1, ('weak', 1.0): 4, ('10,000', 1.0): 1, ('bravo', 1.0): 1, ('iamamonst', 1.0): 1, ('rxthedailysurveyvot', 1.0): 1, ('broke', 1.0): 1, ('ass', 1.0): 1, ('roux', 1.0): 1, ('walkin', 1.0): 1, ('audienc', 1.0): 2, ('pfb', 1.0): 1, ('jute', 1.0): 1, ('walangmakakapigilsakin', 1.0): 1, ('lori', 1.0): 1, ('ehm', 1.0): 1, ('trick', 1.0): 1, ('baekhyun', 1.0): 1, ('eyesmil', 1.0): 1, ('borrow', 1.0): 1, ('knive', 1.0): 1, ('thek', 1.0): 1, ('eventu', 1.0): 1, ('reaapear', 1.0): 1, ('kno', 1.0): 1, ('whet', 1.0): 1, ('gratti', 1.0): 1, ('shorter', 1.0): 1, ('tweetin', 1.0): 1, ('inshallah', 1.0): 1, ('banana', 1.0): 1, ('raspberri', 1.0): 2, ('healthylifestyl', 1.0): 1, ('aint', 1.0): 2, ('skate', 1.0): 1, ('analyz', 1.0): 1, ('varieti', 1.0): 1, ('4:13', 1.0): 1, ('insomnia', 1.0): 1, ('medic', 1.0): 1, ('opposit', 1.0): 1, ('everlast', 1.0): 1, ('yoga', 1.0): 1, ('massag', 1.0): 2, ('osteopath', 1.0): 1, ('trainer', 1.0): 1, ('sharm', 1.0): 1, ('al_master_band', 1.0): 1, ('tbc', 1.0): 1, ('unives', 1.0): 1, ('architectur', 1.0): 1, ('random', 1.0): 1, ('isnt', 1.0): 1, ('typo', 1.0): 1, ('snark', 1.0): 1, ('lession', 1.0): 1, ('drunk', 1.0): 1, ('bruuh', 1.0): 1, ('2week', 1.0): 1, ('50europ', 1.0): 1, ('🇫🇷', 1.0): 4, ('iov', 1.0): 1, ('accord', 1.0): 1, ('mne', 1.0): 1, ('pchelok', 1.0): 1, ('ja', 1.0): 1, ('=:', 1.0): 2, ('sweetest', 1.0): 1, ('comet', 1.0): 1, ('ahah', 1.0): 1, ('candi', 1.0): 2, ('axio', 1.0): 1, ('rabbit', 1.0): 2, ('nutshel', 1.0): 1, ('taken', 1.0): 1, ('letshavecocktailsafternuclai', 1.0): 1, ('malik', 1.0): 1, ('umair', 1.0): 1, ('canon', 1.0): 1, ('gang', 1.0): 1, ('grind', 1.0): 1, ('thoracicbridg', 1.0): 1, ('5minut', 1.0): 1, ('nonscript', 1.0): 1, ('password', 1.0): 1, ('shoshannavassil', 1.0): 1, ('addmeonsnapchat', 1.0): 1, ('dmme', 1.0): 1, ('mpoint', 1.0): 2, ('soph', 1.0): 1, ('anot', 1.0): 1, ('liao', 1.0): 2, ('ord', 1.0): 1, ('lor', 1.0): 1, ('sibei', 1.0): 1, ('xialan', 1.0): 1, ('thnx', 1.0): 1, ('malfunct', 1.0): 1, ('clown', 1.0): 1, ('joker', 1.0): 1, ('\\U000fec00', 1.0): 1, ('nigth', 1.0): 1, ('estoy', 1.0): 1, ('escuchando', 1.0): 1, ('elsewher', 1.0): 1, ('bipolar', 1.0): 1, ('hahahahahahahahahahahahahaha', 1.0): 1, ('yoohoo', 1.0): 1, ('bajrangibhaijaanstorm', 1.0): 1, ('superhappi', 1.0): 1, ('doll', 1.0): 1, ('energi', 1.0): 1, ('f', 1.0): 3, (\"m'dear\", 1.0): 1, ('emma', 1.0): 2, ('alrd', 1.0): 1, ('dhan', 1.0): 2, ('satguru', 1.0): 1, ('tera', 1.0): 1, ('aasra', 1.0): 1, ('pita', 1.0): 1, ('keeo', 1.0): 1, ('darl', 1.0): 2, ('akarshan', 1.0): 1, ('sweetpea', 1.0): 1, ('gluten', 1.0): 1, ('pastri', 1.0): 2, ('highfiv', 1.0): 1, ('artsi', 1.0): 1, ('verbal', 1.0): 1, ('kaaa', 1.0): 1, ('oxford', 1.0): 2, ('wahoo', 1.0): 1, ('anchor', 1.0): 1, ('partnership', 1.0): 1, ('robbenisland', 1.0): 1, ('whale', 1.0): 1, ('aquat', 1.0): 1, ('safari', 1.0): 1, ('garru', 1.0): 1, ('liara', 1.0): 1, ('appoint', 1.0): 1, ('burnley', 1.0): 1, ('453', 1.0): 1, ('110', 1.0): 2, ('49', 1.0): 1, ('footbal', 1.0): 1, ('fm15', 1.0): 1, ('fmfamili', 1.0): 1, ('aamir', 1.0): 1, ('difficult', 1.0): 1, ('medium', 1.0): 1, ('nva', 1.0): 1, ('minuet', 1.0): 1, ('gamec', 1.0): 1, ('headrest', 1.0): 1, ('pit', 1.0): 1, ('spoken', 1.0): 1, ('advis', 1.0): 1, ('paypoint', 1.0): 1, ('deepthroat', 1.0): 1, ('truli', 1.0): 3, ('bee', 1.0): 2, ('upward', 1.0): 1, ('bound', 1.0): 1, ('movingonup', 1.0): 1, ('aitor', 1.0): 1, ('sn', 1.0): 1, ('ps4', 1.0): 2, ('jawad', 1.0): 1, ('presal', 1.0): 1, ('betcha', 1.0): 1, ('dumb', 1.0): 2, ('butt', 1.0): 1, ('qualki', 1.0): 1, ('808', 1.0): 1, ('milf', 1.0): 1, ('4like', 1.0): 1, ('sexysaturday', 1.0): 1, ('vw', 1.0): 1, ('umpfff', 1.0): 1, ('ca', 1.0): 1, ('domg', 1.0): 1, ('nanti', 1.0): 1, ('difollow', 1.0): 1, ('stubborn', 1.0): 1, ('nothavingit', 1.0): 1, ('klee', 1.0): 1, ('hem', 1.0): 1, ('congrad', 1.0): 1, ('accomplish', 1.0): 1, ('kfcroleplay', 1.0): 3, ('tregaron', 1.0): 1, ('boar', 1.0): 1, ('sweati', 1.0): 1, ('glyon', 1.0): 1, ('🚮', 1.0): 1, (\"tee'\", 1.0): 1, ('johnni', 1.0): 1, ('utub', 1.0): 1, (\"video'\", 1.0): 1, ('loss', 1.0): 1, ('combin', 1.0): 2, ('pigeon', 1.0): 1, ('fingerscross', 1.0): 1, ('photobomb', 1.0): 1, ('90', 1.0): 1, ('23', 1.0): 1, ('gimm', 1.0): 1, ('definetli', 1.0): 1, ('exit', 1.0): 1, ('bom-dia', 1.0): 1, ('apod', 1.0): 1, ('ultraviolet', 1.0): 1, ('m31', 1.0): 1, ('jul', 1.0): 1, ('oooh', 1.0): 1, ('yawn', 1.0): 1, ('ftw', 1.0): 1, ('maman', 1.0): 1, ('afterznoon', 1.0): 1, ('tweeep', 1.0): 1, ('abp', 1.0): 2, ('kiya', 1.0): 1, ('van', 1.0): 1, ('olymp', 1.0): 1, ('😷', 1.0): 1, ('classi', 1.0): 1, ('attach', 1.0): 1, ('equip', 1.0): 1, ('bobbl', 1.0): 1, ('anu', 1.0): 1, ('mh3', 1.0): 1, ('patch', 1.0): 1, ('psp', 1.0): 1, ('huffpost', 1.0): 1, ('tribut', 1.0): 1, ('h_eartshapedbox', 1.0): 1, ('magictrikband', 1.0): 1, ('magictrik', 1.0): 2, ('roommat', 1.0): 1, ('tami', 1.0): 1, ('b3dk', 1.0): 1, ('7an', 1.0): 1, ('ank', 1.0): 1, ('purpos', 1.0): 1, ('struggl', 1.0): 1, ('eagl', 1.0): 1, ('oceana', 1.0): 1, ('idk', 1.0): 3, ('med', 1.0): 1, ('fridayfauxpa', 1.0): 1, ('subtl', 1.0): 1, ('hint', 1.0): 1, ('prim', 1.0): 1, ('algorithm', 1.0): 1, ('iii', 1.0): 1, ('rosa', 1.0): 1, ('yvw', 1.0): 1, ('here', 1.0): 1, ('boost', 1.0): 1, ('unforgett', 1.0): 1, ('humor', 1.0): 1, (\"mum'\", 1.0): 1, ('hahahhaah', 1.0): 1, ('sombrero', 1.0): 1, ('lost', 1.0): 2, ('spammer', 1.0): 1, ('proceed', 1.0): 1, ('entertain', 1.0): 1, ('100k', 1.0): 1, ('mileston', 1.0): 1, ('judith', 1.0): 1, ('district', 1.0): 1, ('council', 1.0): 1, ('midar', 1.0): 1, ('gender', 1.0): 1, ('ilysm', 1.0): 1, ('zen', 1.0): 1, ('neat', 1.0): 1, ('rider', 1.0): 1, ('fyi', 1.0): 1, ('dig', 1.0): 2, ('👱🏽', 1.0): 1, ('👽', 1.0): 1, ('🌳', 1.0): 1, ('suspici', 1.0): 1, ('calori', 1.0): 1, ('harder', 1.0): 1, ('jessica', 1.0): 1, ('carina', 1.0): 1, ('francisco', 1.0): 1, ('teret', 1.0): 1, ('potassium', 1.0): 1, ('rehydr', 1.0): 1, ('drinkitallup', 1.0): 1, ('thirstquench', 1.0): 1, ('tapir', 1.0): 1, ('calf', 1.0): 1, ('mealtim', 1.0): 1, ('uhc', 1.0): 1, ('scale', 1.0): 1, ('network', 1.0): 1, ('areal', 1.0): 1, ('extremesport', 1.0): 1, ('quadbik', 1.0): 1, ('bloggersrequir', 1.0): 1, ('bloggersw', 1.0): 1, ('brainer', 1.0): 1, ('mse', 1.0): 1, ('fund', 1.0): 1, ('nooowww', 1.0): 1, ('lile', 1.0): 1, ('tid', 1.0): 1, ('tmi', 1.0): 1, ('deploy', 1.0): 1, ('jule', 1.0): 1, ('betti', 1.0): 1, ('hddc', 1.0): 1, ('salman', 1.0): 1, ('pthht', 1.0): 1, ('lfc', 1.0): 3, ('tope', 1.0): 1, ('xxoo', 1.0): 2, ('russia', 1.0): 2, ('silver-wash', 1.0): 1, ('fritillari', 1.0): 1, ('moon', 1.0): 1, ('ap', 1.0): 2, ('trash', 1.0): 2, ('clever', 1.0): 1, (\"thank'\", 1.0): 1, ('keven', 1.0): 1, ('pastim', 1.0): 1, ('ashramcal', 1.0): 1, ('ontrack', 1.0): 1, ('german', 1.0): 1, ('subtitl', 1.0): 1, ('pinter', 1.0): 1, ('morninggg', 1.0): 1, ('🐶', 1.0): 1, ('pete', 1.0): 1, ('awesome-o', 1.0): 1, ('multipl', 1.0): 1, ('cya', 1.0): 1, ('harrog', 1.0): 1, ('jet', 1.0): 1, ('supplier', 1.0): 1, ('req', 1.0): 1, ('fridayloug', 1.0): 1, ('4thstreetmus', 1.0): 1, ('hawaii', 1.0): 1, ('kick', 1.0): 1, ('deepli', 1.0): 1, ('john@timney.eclipse.co.uk', 1.0): 1, ('thousand', 1.0): 2, ('newspap', 1.0): 1, ('lew', 1.0): 1, ('nah', 1.0): 1, ('fallout', 1.0): 2, ('technic', 1.0): 1, ('gunderson', 1.0): 1, ('europa', 1.0): 1, ('thoroughli', 1.0): 1, ('script', 1.0): 1, ('overtak', 1.0): 1, ('motorway', 1.0): 1, ('thu', 1.0): 1, ('niteflirt', 1.0): 1, ('hbu', 1.0): 2, ('bowl', 1.0): 1, ('chri', 1.0): 2, ('niall', 1.0): 2, ('94', 1.0): 1, ('ik', 1.0): 1, ('stydia', 1.0): 1, ('nawazuddin', 1.0): 1, ('siddiqu', 1.0): 1, ('nomnomnom', 1.0): 1, ('dukefreebiefriday', 1.0): 1, ('z', 1.0): 1, ('insyaallah', 1.0): 1, ('ham', 1.0): 1, ('villa', 1.0): 1, ('brum', 1.0): 1, ('deni', 1.0): 1, ('vagina', 1.0): 1, ('rli', 1.0): 1, ('izzi', 1.0): 1, ('mitch', 1.0): 1, ('minn', 1.0): 1, ('recently.websit', 1.0): 1, ('coolingtow', 1.0): 1, ('soon.thank', 1.0): 1, ('showinginterest', 1.0): 1, ('multicolor', 1.0): 1, ('wid', 1.0): 1, ('wedg', 1.0): 1, ('motiv', 1.0): 1, ('nnnnot', 1.0): 1, (\"gf'\", 1.0): 1, ('bluesidemenxix', 1.0): 1, ('ardent', 1.0): 1, ('mooorn', 1.0): 1, ('wuppert', 1.0): 1, ('fridayfunday', 1.0): 1, ('re-sign', 1.0): 1, ('chalkhil', 1.0): 1, ('midday', 1.0): 1, ('carter', 1.0): 1, ('remedi', 1.0): 1, ('atrack', 1.0): 1, ('christ', 1.0): 1, ('badminton', 1.0): 1, (\"littl'un\", 1.0): 1, ('ikprideofpak', 1.0): 1, ('janjua', 1.0): 1, ('pimpl', 1.0): 1, ('forehead', 1.0): 1, ('volcano', 1.0): 1, ('mag', 1.0): 1, ('miryenda', 1.0): 1, (\"technology'\", 1.0): 1, ('touchétoday', 1.0): 1, ('idownload', 1.0): 1, ('25ish', 1.0): 1, ('snowbal', 1.0): 1, ('nd', 1.0): 1, ('expir', 1.0): 1, ('6gb', 1.0): 1, ('loveu', 1.0): 1, ('morefuninthephilippin', 1.0): 1, ('laho', 1.0): 1, ('caramoan', 1.0): 1, ('kareem', 1.0): 1, ('surah', 1.0): 1, ('kahaf', 1.0): 1, ('melani', 1.0): 1, ('bosch', 1.0): 1, ('machin', 1.0): 1, (\"week'\", 1.0): 1, ('refollow', 1.0): 1, ('😎', 1.0): 1, ('💁🏻', 1.0): 1, ('relaps', 1.0): 1, ('prada', 1.0): 2, ('punjabiswillgetit', 1.0): 1, ('hitter', 1.0): 1, ('mass', 1.0): 2, ('shoud', 1.0): 1, ('1:12', 1.0): 1, ('ughtm', 1.0): 1, ('545', 1.0): 1, ('kissm', 1.0): 1, ('likeforfollow', 1.0): 1, ('overwhelm', 1.0): 1, ('groupmat', 1.0): 1, ('75', 1.0): 2, ('kyunk', 1.0): 1, ('aitchison', 1.0): 1, ('curvi', 1.0): 1, ('mont', 1.0): 1, ('doa', 1.0): 1, ('header', 1.0): 1, ('speaker', 1.0): 3, ('avoid', 1.0): 1, ('laboratori', 1.0): 1, ('idc', 1.0): 1, ('fuckin', 1.0): 2, ('wooo', 1.0): 2, ('neobyt', 1.0): 1, ('pirat', 1.0): 1, ('takedown', 1.0): 1, ('indirag', 1.0): 1, ('judiciari', 1.0): 1, ('commit', 1.0): 4, ('govt', 1.0): 1, ('polici', 1.0): 1, ('rbi', 1.0): 1, ('similar', 1.0): 1, (\"thought'\", 1.0): 1, ('progress', 1.0): 1, ('transfer', 1.0): 1, ('gg', 1.0): 1, ('defenit', 1.0): 1, ('nofx', 1.0): 1, ('friskyfiday', 1.0): 1, ('yipee', 1.0): 1, ('shed', 1.0): 1, ('incent', 1.0): 1, ('vege', 1.0): 1, ('marin', 1.0): 1, ('gz', 1.0): 1, ('rajeev', 1.0): 1, ('hvng', 1.0): 1, ('funfil', 1.0): 1, ('friday.it', 1.0): 1, ('ws', 1.0): 1, ('reali', 1.0): 1, ('diff', 1.0): 1, ('kabir.fel', 1.0): 1, ('dresden', 1.0): 1, ('germani', 1.0): 1, ('plot', 1.0): 1, ('tdf', 1.0): 1, ('🍷', 1.0): 2, ('☀', 1.0): 2, ('🚲', 1.0): 2, ('minion', 1.0): 2, ('slot', 1.0): 1, (\"b'day\", 1.0): 1, ('isabella', 1.0): 1, ('okeyyy', 1.0): 1, ('vddd', 1.0): 1, (');', 1.0): 1, ('selfee', 1.0): 1, ('insta', 1.0): 1, ('🙆', 1.0): 1, ('🙌', 1.0): 1, ('😛', 1.0): 1, ('🐒', 1.0): 1, ('😝', 1.0): 1, ('hhahhaaa', 1.0): 1, ('jeez', 1.0): 1, ('teamcannib', 1.0): 1, ('teamspacewhalingisthebest', 1.0): 1, ('fitfa', 1.0): 1, ('identifi', 1.0): 1, ('pharmaci', 1.0): 1, ('verylaterealis', 1.0): 1, ('iwishiknewbett', 1.0): 1, ('satisfi', 1.0): 1, ('ess-aych-eye-te', 1.0): 1, ('supposedli', 1.0): 1, ('👍', 1.0): 1, ('immedi', 1.0): 1, (\"foxy'\", 1.0): 1, ('instrument', 1.0): 1, ('alon', 1.0): 2, ('goldcoast', 1.0): 1, ('lelomustfal', 1.0): 1, ('meal', 1.0): 1, ('5g', 1.0): 1, ('liker', 1.0): 1, ('newdress', 1.0): 1, ('resist', 1.0): 1, ('fot', 1.0): 1, ('troy', 1.0): 1, ('twitterfollowerswhatsup', 1.0): 1, ('happyfriedday', 1.0): 1, ('keepsafealway', 1.0): 1, ('loveyeah', 1.0): 1, ('emojasp_her', 1.0): 1, ('vanilla', 1.0): 1, ('sidemen', 1.0): 1, ('yaaayyy', 1.0): 1, ('friendaaa', 1.0): 1, ('bulb', 1.0): 5, ('corn', 1.0): 6, ('1tbps4', 1.0): 1, ('divin', 1.0): 1, ('wheeli', 1.0): 1, ('bin', 1.0): 1, ('ubericecream', 1.0): 1, ('messengerforaday', 1.0): 1, ('kyli', 1.0): 1, ('toilet', 1.0): 1, ('ikaw', 1.0): 1, ('musta', 1.0): 1, ('cheatmat', 1.0): 1, ('kyuhyun', 1.0): 1, ('ghanton', 1.0): 1, ('easy.get', 1.0): 1, ('5:30', 1.0): 1, ('therein', 1.0): 1, ('majalah', 1.0): 1, ('dominiqu', 1.0): 1, ('lamp', 1.0): 1, ('a-foot', 1.0): 1, ('revamp', 1.0): 1, ('brainchild', 1.0): 1, ('confid', 1.0): 1, ('confin', 1.0): 1, ('colorado', 1.0): 1, ('goodyear', 1.0): 1, ('upto', 1.0): 1, ('cashback', 1.0): 1, ('yourewelcom', 1.0): 1, ('nightli', 1.0): 1, ('simpin', 1.0): 1, ('sketchbook', 1.0): 1, ('4wild', 1.0): 1, ('colorpencil', 1.0): 1, ('cray', 1.0): 1, ('6:30', 1.0): 1, ('imma', 1.0): 3, ('ob', 1.0): 1, ('11h', 1.0): 1, ('kino', 1.0): 1, ('adult', 1.0): 1, ('kardamena', 1.0): 1, ('samo', 1.0): 1, ('greec', 1.0): 1, ('caesar', 1.0): 1, ('salad', 1.0): 1, ('tad', 1.0): 1, ('bland', 1.0): 1, ('respond', 1.0): 1, ('okk', 1.0): 1, ('den', 1.0): 1, ('allov', 1.0): 1, ('hangout', 1.0): 1, ('whoever', 1.0): 1, ('tourist', 1.0): 1, ('♌', 1.0): 1, ('kutiyapanti', 1.0): 1, ('profession', 1.0): 1, ('boomshot', 1.0): 1, ('fuh', 1.0): 1, ('yeeey', 1.0): 1, ('donot', 1.0): 1, ('expos', 1.0): 1, ('lipstick', 1.0): 1, ('cran', 1.0): 1, ('prayr', 1.0): 1, ('හෙල', 1.0): 1, ('හවුල', 1.0): 1, ('onemochaonelov', 1.0): 1, ('southpaw', 1.0): 1, ('geniu', 1.0): 1, ('stroma', 1.0): 1, ('🔴', 1.0): 1, ('younow', 1.0): 1, ('jonah', 1.0): 1, ('jareddd', 1.0): 1, ('postcod', 1.0): 1, ('talkmobil', 1.0): 1, ('huha', 1.0): 1, ('transform', 1.0): 1, ('sword', 1.0): 3, ('misread', 1.0): 1, ('richard', 1.0): 1, ('ibiza', 1.0): 1, ('birthdaymoneyforjesusjuic', 1.0): 1, ('ytb', 1.0): 1, ('tutori', 1.0): 1, ('construct', 1.0): 2, ('critic', 1.0): 1, ('ganesha', 1.0): 1, ('textur', 1.0): 1, ('photographi', 1.0): 1, ('hinduism', 1.0): 1, ('hindugod', 1.0): 1, ('elephantgod', 1.0): 1, ('selfish', 1.0): 1, ('bboy', 1.0): 1, ('cardgam', 1.0): 1, ('pixelart', 1.0): 1, ('gamedesign', 1.0): 1, ('indiedev', 1.0): 1, ('pixel_daili', 1.0): 1, ('plateau', 1.0): 1, ('laguna', 1.0): 1, ('tha', 1.0): 4, ('bahot', 1.0): 1, ('baje', 1.0): 1, ('raat', 1.0): 1, ('liya', 1.0): 1, ('hath', 1.0): 1, ('ghant', 1.0): 1, ('itna', 1.0): 2, ('bana', 1.0): 1, ('paya', 1.0): 1, ('uta', 1.0): 1, ('manga', 1.0): 1, ('jamuna', 1.0): 1, ('\\\\:', 1.0): 1, ('swiftma', 1.0): 1, ('trion', 1.0): 1, ('forum', 1.0): 1, ('b-day', 1.0): 1, ('disgust', 1.0): 1, ('commodor', 1.0): 1, ('annabel', 1.0): 1, ('bridg', 1.0): 1, ('quest', 1.0): 1, ('borderland', 1.0): 1, ('wanderrook', 1.0): 1, ('gm', 1.0): 1, ('preciou', 1.0): 2, ('mizz', 1.0): 1, ('bleedgreen', 1.0): 1, ('✌🏻', 1.0): 1, ('sophia', 1.0): 1, ('chicago', 1.0): 1, ('honeymoon', 1.0): 1, (\"da'esh\", 1.0): 1, ('co-ord', 1.0): 1, ('fsa', 1.0): 1, ('estat', 1.0): 1, (\"when'\", 1.0): 1, ('dusti', 1.0): 1, ('tunisia', 1.0): 2, (\"class'\", 1.0): 1, ('irrit', 1.0): 1, ('fiverr', 1.0): 1, ('gina', 1.0): 1, ('soproud', 1.0): 1, ('enought', 1.0): 1, ('hole', 1.0): 1, ('melbourneburg', 1.0): 1, ('arianna', 1.0): 1, ('esai', 1.0): 1, ('rotterdam', 1.0): 1, ('jordi', 1.0): 1, ('clasi', 1.0): 1, ('horni', 1.0): 1, ('salon', 1.0): 1, ('bleach', 1.0): 1, ('olaplex', 1.0): 1, ('damag', 1.0): 1, ('teamwork', 1.0): 1, ('zitecofficestori', 1.0): 1, ('다쇼', 1.0): 1, ('colleagu', 1.0): 1, ('eb', 1.0): 1, (\"t'would\", 1.0): 1, ('tweetup', 1.0): 1, ('detect', 1.0): 1, ('jonathancreek', 1.0): 1, ('dvr', 1.0): 1, ('kat', 1.0): 1, ('rarer', 1.0): 1, ('okkk', 1.0): 1, ('frend', 1.0): 1, ('milt', 1.0): 1, ('mario', 1.0): 1, ('rewatch', 1.0): 1, ('1600', 1.0): 1, ('sige', 1.0): 1, ('punta', 1.0): 1, ('kayo', 1.0): 1, ('nooo', 1.0): 1, ('prompt', 1.0): 1, ('t-mobil', 1.0): 1, ('orang', 1.0): 1, ('ee', 1.0): 1, ('teapot', 1.0): 1, ('hotter', 1.0): 1, ('»', 1.0): 1, ('londoutrad', 1.0): 1, ('kal', 1.0): 1, ('wayward', 1.0): 1, ('pine', 1.0): 1, ('muscl', 1.0): 1, ('ilikeit', 1.0): 1, ('belong', 1.0): 1, ('watford', 1.0): 1, ('enterpris', 1.0): 1, ('cube', 1.0): 1, ('particp', 1.0): 1, ('saudi', 1.0): 1, ('arabia', 1.0): 1, ('recogn', 1.0): 1, ('fanbas', 1.0): 3, ('bailona', 1.0): 3, ('responsibilti', 1.0): 1, ('sunlight', 1.0): 1, ('tiger', 1.0): 1, ('elev', 1.0): 1, ('horror', 1.0): 1, ('bitchesss', 1.0): 1, ('shitti', 1.0): 1, ('squash', 1.0): 1, ('becca', 1.0): 1, ('delta', 1.0): 1, ('nut', 1.0): 1, ('yun', 1.0): 1, ('joe', 1.0): 1, ('dirt', 1.0): 1, ('sharon', 1.0): 1, ('medicin', 1.0): 1, ('ttyl', 1.0): 1, ('gav', 1.0): 1, ('linda', 1.0): 1, ('3hr', 1.0): 1, ('tym', 1.0): 2, ('dieback', 1.0): 1, ('endit', 1.0): 1, ('minecon', 1.0): 1, ('sere', 1.0): 1, ('joerin', 1.0): 1, ('joshan', 1.0): 1, ('tandem', 1.0): 1, ('ligao', 1.0): 1, ('albay', 1.0): 1, ('bcyc', 1.0): 1, ('lnh', 1.0): 1, ('sat', 1.0): 1, ('honorari', 1.0): 1, ('alac', 1.0): 1, ('skelo_ghost', 1.0): 1, ('madadagdagan', 1.0): 1, ('bmc', 1.0): 1, ('11:11', 1.0): 2, ('embarrass', 1.0): 1, ('entropi', 1.0): 1, ('evolut', 1.0): 2, ('loop', 1.0): 1, ('eva', 1.0): 1, ('camden', 1.0): 1, ('uhh', 1.0): 1, ('scoup', 1.0): 1, ('jren', 1.0): 1, ('nuest', 1.0): 1, ('lovelayyy', 1.0): 1, ('kidney', 1.0): 1, ('neuer', 1.0): 1, ('spray', 1.0): 1, ('donnae.strydom@westerncape.gov.za', 1.0): 1, ('uni', 1.0): 1, ('uff', 1.0): 1, ('karhi', 1.0): 1, ('thi', 1.0): 1, ('juaquin', 1.0): 1, ('v3nzor99', 1.0): 1, ('shell', 1.0): 1, ('heyi', 1.0): 1, ('flavor', 1.0): 1, ('thakyou', 1.0): 1, ('beatriz', 1.0): 1, ('cancel', 1.0): 1, ('puff', 1.0): 1, ('egg', 1.0): 2, ('tart', 1.0): 1, ('chai', 1.0): 1, ('mtr', 1.0): 1, ('alyssa', 1.0): 1, ('rub', 1.0): 1, ('tummi', 1.0): 1, ('zelda', 1.0): 1, ('ive', 1.0): 1, ('🎂', 1.0): 1, ('jiva', 1.0): 1, ('🍹', 1.0): 1, ('🍻', 1.0): 1, ('mubbarak', 1.0): 1, ('deborah', 1.0): 1, ('coupon', 1.0): 1, ('colourdeb', 1.0): 1, ('purpl', 1.0): 1, (\"chippy'\", 1.0): 1, ('vessel', 1.0): 1, ('ps', 1.0): 2, ('vintag', 1.0): 1, ('✫', 1.0): 4, ('˚', 1.0): 4, ('·', 1.0): 4, ('✵', 1.0): 4, ('⊹', 1.0): 4, ('1710', 1.0): 1, ('gooffeanotter', 1.0): 1, ('kiksex', 1.0): 1, ('mugshot', 1.0): 1, ('token', 1.0): 1, ('maritimen', 1.0): 1, ('rh', 1.0): 1, ('tatton', 1.0): 1, ('jump_julia', 1.0): 1, ('malema', 1.0): 1, ('fren', 1.0): 1, ('nuf', 1.0): 1, ('teas', 1.0): 1, ('alien', 1.0): 2, ('closer', 1.0): 1, ('monitor', 1.0): 1, ('kimmi', 1.0): 1, (\"channel'\", 1.0): 1, ('planetbollywoodnew', 1.0): 1, ('epi', 1.0): 1, ('tricki', 1.0): 1, ('be-shak', 1.0): 1, ('chenoweth', 1.0): 1, ('oodl', 1.0): 1, ('hailey', 1.0): 1, ('craźi', 1.0): 1, ('sęxxxÿ', 1.0): 1, ('cøôl', 1.0): 1, ('runway', 1.0): 1, ('gooodnight', 1.0): 1, ('iv', 1.0): 1, ('ri', 1.0): 1, ('jayci', 1.0): 1, ('karaok', 1.0): 1, ('ltsw', 1.0): 1, ('giant', 1.0): 1, ('1709', 1.0): 1, ('refus', 1.0): 1, ('collagen', 1.0): 1, ('2win', 1.0): 1, ('hopetowin', 1.0): 1, ('inventori', 1.0): 1, ('loveforfood', 1.0): 1, ('foodforthought', 1.0): 1, ('thoughtfortheday', 1.0): 1, ('carp', 1.0): 1, ('diem', 1.0): 1, ('nath', 1.0): 1, ('ning', 1.0): 1, ('although', 1.0): 1, ('harm', 1.0): 1, ('stormi', 1.0): 1, ('sync', 1.0): 1, ('devic', 1.0): 1, ('mess', 1.0): 1, ('nylon', 1.0): 1, ('gvb', 1.0): 1, ('cd', 1.0): 1, ('mountain.titl', 1.0): 1, ('unto', 1.0): 1, ('theworldwouldchang', 1.0): 1, ('categori', 1.0): 1, ('mah', 1.0): 1, ('panel', 1.0): 1, (\"i'am\", 1.0): 1, ('80-1', 1.0): 1, ('1708', 1.0): 1, ('neenkin', 1.0): 1, ('masterpiec', 1.0): 1, ('debit', 1.0): 1, ('beagl', 1.0): 1, ('♫', 1.0): 1, ('feat', 1.0): 1, ('charli', 1.0): 1, ('puth', 1.0): 1, ('wiz', 1.0): 1, ('khalifa', 1.0): 1, ('svu', 1.0): 1, ('darker', 1.0): 1, ('berni', 1.0): 1, ('henri', 1.0): 1, ('trap', 1.0): 1, ('tommi', 1.0): 1, (\"vivian'\", 1.0): 1, ('transpar', 1.0): 1, ('bitcoin', 1.0): 1, ('insight', 1.0): 1, ('ping', 1.0): 1, ('masquerad', 1.0): 1, ('zorroreturm', 1.0): 1, ('1707', 1.0): 1, ('pk', 1.0): 1, ('hay', 1.0): 1, ('jacquelin', 1.0): 1, ('passion', 1.0): 1, ('full-fledg', 1.0): 1, ('workplac', 1.0): 1, ('venu', 1.0): 1, ('lago', 1.0): 1, ('luxord', 1.0): 1, ('potato', 1.0): 1, ('hundr', 1.0): 1, ('cite', 1.0): 1, ('academ', 1.0): 1, ('pokiri', 1.0): 1, ('1nenokkadin', 1.0): 1, ('heritag', 1.0): 1, ('wood', 1.0): 1, ('beleaf', 1.0): 1, ('spnfamili', 1.0): 1, ('spn', 1.0): 1, ('alwayskeepfight', 1.0): 1, ('jaredpadalecki', 1.0): 1, ('jensenackl', 1.0): 1, ('peasant', 1.0): 2, ('ahahha', 1.0): 1, ('distant', 1.0): 1, ('shout-out', 1.0): 1, ('adulthood', 1.0): 1, ('hopeless', 0.0): 2, ('tmr', 0.0): 3, (':(', 0.0): 4571, ('everyth', 0.0): 17, ('kid', 0.0): 20, ('section', 0.0): 3, ('ikea', 0.0): 1, ('cute', 0.0): 43, ('shame', 0.0): 19, (\"i'm\", 0.0): 343, ('nearli', 0.0): 3, ('19', 0.0): 8, ('2', 0.0): 41, ('month', 0.0): 23, ('heart', 0.0): 27, ('slide', 0.0): 1, ('wast', 0.0): 5, ('basket', 0.0): 1, ('“', 0.0): 15, ('hate', 0.0): 57, ('japanes', 0.0): 4, ('call', 0.0): 29, ('bani', 0.0): 2, ('”', 0.0): 11, ('dang', 0.0): 2, ('start', 0.0): 44, ('next', 0.0): 40, ('week', 0.0): 56, ('work', 0.0): 133, ('oh', 0.0): 92, ('god', 0.0): 15, ('babi', 0.0): 47, ('face', 0.0): 20, ('make', 0.0): 102, ('smile', 0.0): 10, ('neighbour', 0.0): 1, ('motor', 0.0): 1, ('ask', 0.0): 29, ('said', 0.0): 33, ('updat', 0.0): 11, ('search', 0.0): 3, ('sialan', 0.0): 1, ('athabasca', 0.0): 2, ('glacier', 0.0): 2, ('1948', 0.0): 1, (':-(', 0.0): 493, ('jasper', 0.0): 1, ('jaspernationalpark', 0.0): 1, ('alberta', 0.0): 1, ('explorealberta', 0.0): 1, ('…', 0.0): 16, ('realli', 0.0): 131, ('good', 0.0): 101, ('g', 0.0): 8, ('idea', 0.0): 10, ('never', 0.0): 57, ('go', 0.0): 224, ('meet', 0.0): 31, ('mare', 0.0): 1, ('ivan', 0.0): 1, ('happi', 0.0): 25, ('trip', 0.0): 11, ('keep', 0.0): 34, ('safe', 0.0): 5, ('see', 0.0): 124, ('soon', 0.0): 45, ('tire', 0.0): 50, ('hahahah', 0.0): 3, ('knee', 0.0): 2, ('replac', 0.0): 4, ('get', 0.0): 232, ('day', 0.0): 149, ('ouch', 0.0): 3, ('relat', 0.0): 2, ('sweet', 0.0): 7, ('n', 0.0): 21, ('sour', 0.0): 2, ('kind', 0.0): 11, ('bi-polar', 0.0): 1, ('peopl', 0.0): 75, ('life', 0.0): 33, ('...', 0.0): 331, ('cuz', 0.0): 4, ('full', 0.0): 16, ('pleass', 0.0): 2, ('im', 0.0): 129, ('sure', 0.0): 31, ('tho', 0.0): 28, ('feel', 0.0): 158, ('stupid', 0.0): 8, (\"can't\", 0.0): 180, ('seem', 0.0): 15, ('grasp', 0.0): 1, ('basic', 0.0): 2, ('digit', 0.0): 8, ('paint', 0.0): 3, ('noth', 0.0): 26, (\"i'v\", 0.0): 77, ('research', 0.0): 1, ('help', 0.0): 54, ('lord', 0.0): 2, ('lone', 0.0): 9, ('someon', 0.0): 57, ('talk', 0.0): 45, ('guy', 0.0): 62, ('girl', 0.0): 28, ('assign', 0.0): 5, ('project', 0.0): 3, ('😩', 0.0): 14, ('want', 0.0): 246, ('play', 0.0): 48, ('video', 0.0): 23, ('game', 0.0): 28, ('watch', 0.0): 77, ('movi', 0.0): 24, ('choreograph', 0.0): 1, ('hard', 0.0): 35, ('email', 0.0): 10, ('link', 0.0): 12, ('still', 0.0): 124, ('say', 0.0): 63, ('longer', 0.0): 12, ('avail', 0.0): 13, ('cri', 0.0): 46, ('bc', 0.0): 50, ('miss', 0.0): 301, ('mingm', 0.0): 1, ('much', 0.0): 139, ('sorri', 0.0): 148, ('mom', 0.0): 13, ('far', 0.0): 18, ('away', 0.0): 28, (\"we'r\", 0.0): 30, ('truli', 0.0): 5, ('flight', 0.0): 6, ('friend', 0.0): 39, ('happen', 0.0): 51, ('sad', 0.0): 123, ('dog', 0.0): 17, ('pee', 0.0): 2, ('’', 0.0): 27, ('bag', 0.0): 8, ('take', 0.0): 49, ('newwin', 0.0): 1, ('15', 0.0): 10, ('doushit', 0.0): 1, ('late', 0.0): 27, ('suck', 0.0): 23, ('sick', 0.0): 43, ('plan', 0.0): 17, ('first', 0.0): 27, ('gundam', 0.0): 1, ('night', 0.0): 46, ('nope', 0.0): 6, ('dollar', 0.0): 1, ('😭', 0.0): 29, ('listen', 0.0): 18, ('back', 0.0): 122, ('old', 0.0): 16, ('show', 0.0): 26, ('know', 0.0): 131, ('weird', 0.0): 10, ('got', 0.0): 104, ('u', 0.0): 193, ('leav', 0.0): 42, ('might', 0.0): 11, ('give', 0.0): 36, ('pale', 0.0): 2, ('imit', 0.0): 1, ('went', 0.0): 32, ('sea', 0.0): 1, ('massiv', 0.0): 4, ('fuck', 0.0): 58, ('rash', 0.0): 1, ('bodi', 0.0): 12, ('pain', 0.0): 21, ('thing', 0.0): 52, ('ever', 0.0): 30, ('home', 0.0): 63, ('hi', 0.0): 34, ('absent', 0.0): 1, ('gran', 0.0): 2, ('knew', 0.0): 6, ('care', 0.0): 20, ('tell', 0.0): 26, ('love', 0.0): 152, ('wish', 0.0): 91, ('would', 0.0): 70, ('sequel', 0.0): 1, ('busi', 0.0): 28, ('sa', 0.0): 15, ('school', 0.0): 32, ('time', 0.0): 166, ('yah', 0.0): 3, ('xx', 0.0): 18, ('ouucchhh', 0.0): 1, ('one', 0.0): 148, ('wisdom', 0.0): 2, ('teeth', 0.0): 6, ('come', 0.0): 91, ('frighten', 0.0): 1, ('case', 0.0): 6, ('pret', 0.0): 1, ('wkwkw', 0.0): 1, ('verfi', 0.0): 1, ('activ', 0.0): 6, ('forget', 0.0): 8, ('follow', 0.0): 262, ('member', 0.0): 6, ('thank', 0.0): 107, ('join', 0.0): 8, ('goodby', 0.0): 14, ('´', 0.0): 4, ('chain', 0.0): 1, ('—', 0.0): 26, ('sentir-s', 0.0): 1, ('incompleta', 0.0): 1, ('okay', 0.0): 38, ('..', 0.0): 108, ('wednesday', 0.0): 5, ('marvel', 0.0): 1, ('thwart', 0.0): 1, ('awh', 0.0): 3, (\"what'\", 0.0): 15, ('chanc', 0.0): 16, ('zant', 0.0): 1, ('need', 0.0): 106, ('someth', 0.0): 28, ('x', 0.0): 39, (\"when'\", 0.0): 1, ('birthday', 0.0): 23, ('worst', 0.0): 14, ('part', 0.0): 11, ('bad', 0.0): 73, ('audraesar', 0.0): 1, ('sushi', 0.0): 3, ('pic', 0.0): 15, ('tl', 0.0): 8, ('drive', 0.0): 16, ('craaazzyy', 0.0): 2, ('pop', 0.0): 3, ('like', 0.0): 228, ('helium', 0.0): 1, ('balloon', 0.0): 1, ('climatechang', 0.0): 5, ('cc', 0.0): 6, (\"california'\", 0.0): 1, ('power', 0.0): 6, ('influenti', 0.0): 1, ('air', 0.0): 3, ('pollut', 0.0): 1, ('watchdog', 0.0): 1, ('califor', 0.0): 1, ('elhaida', 0.0): 1, ('rob', 0.0): 2, ('juri', 0.0): 1, ('came', 0.0): 16, ('10th', 0.0): 1, ('televot', 0.0): 1, ('idaho', 0.0): 2, ('restrict', 0.0): 2, ('fish', 0.0): 2, ('despit', 0.0): 2, ('region', 0.0): 2, ('drought-link', 0.0): 1, ('die-of', 0.0): 1, ('abrupt', 0.0): 1, ('climat', 0.0): 1, ('chang', 0.0): 27, ('may', 0.0): 16, ('doom', 0.0): 2, ('mammoth', 0.0): 1, ('megafauna', 0.0): 1, ('sc', 0.0): 3, (\"australia'\", 0.0): 1, ('dirtiest', 0.0): 2, ('station', 0.0): 3, ('consid', 0.0): 5, ('clean', 0.0): 6, ('energi', 0.0): 3, ('biomass', 0.0): 1, (\"ain't\", 0.0): 5, ('easi', 0.0): 6, ('green', 0.0): 7, ('golf', 0.0): 1, ('cours', 0.0): 7, ('california', 0.0): 1, ('ulti', 0.0): 1, ('well', 0.0): 56, ('mine', 0.0): 12, ('gonna', 0.0): 51, ('sexi', 0.0): 14, ('prexi', 0.0): 1, ('kindergarten', 0.0): 1, ('hungri', 0.0): 19, ('cant', 0.0): 47, ('find', 0.0): 53, ('book', 0.0): 20, ('sane', 0.0): 1, ('liter', 0.0): 15, ('three', 0.0): 7, ('loung', 0.0): 1, ('event', 0.0): 4, ('turn', 0.0): 17, ('boss', 0.0): 5, ('hozier', 0.0): 1, (\"that'\", 0.0): 61, ('true', 0.0): 22, ('soooner', 0.0): 1, ('ahh', 0.0): 7, ('fam', 0.0): 3, ('respectlost', 0.0): 1, ('hypercholesteloremia', 0.0): 1, ('ok', 0.0): 33, ('look', 0.0): 100, ('gift', 0.0): 11, ('calibraska', 0.0): 1, ('actual', 0.0): 24, ('genuin', 0.0): 2, ('contend', 0.0): 1, ('head', 0.0): 23, ('alway', 0.0): 56, ('hurt', 0.0): 41, ('stay', 0.0): 24, ('lmao', 0.0): 13, ('older', 0.0): 5, ('sound', 0.0): 19, ('upset', 0.0): 11, ('infinit', 0.0): 10, ('ao', 0.0): 1, ('stick', 0.0): 1, ('8th', 0.0): 1, ('either', 0.0): 13, ('seriou', 0.0): 8, ('yun', 0.0): 1, ('eh', 0.0): 4, ('room', 0.0): 11, ('way', 0.0): 42, ('hot', 0.0): 15, ('havent', 0.0): 11, ('found', 0.0): 11, ('handsom', 0.0): 2, ('jack', 0.0): 3, ('draw', 0.0): 2, ('shit', 0.0): 36, ('cut', 0.0): 14, ('encor', 0.0): 4, ('4thwin', 0.0): 4, ('baymax', 0.0): 1, ('french', 0.0): 4, ('mixer', 0.0): 1, ('💜', 0.0): 6, ('wft', 0.0): 1, ('awesom', 0.0): 5, ('replay', 0.0): 1, ('parti', 0.0): 15, ('promot', 0.0): 3, ('music', 0.0): 16, ('bank', 0.0): 9, ('short', 0.0): 11, ('boy', 0.0): 18, ('order', 0.0): 16, ('receiv', 0.0): 7, ('hub', 0.0): 1, ('nearest', 0.0): 1, ('deliv', 0.0): 3, ('today', 0.0): 108, ('1/2', 0.0): 3, ('mum', 0.0): 14, ('loud', 0.0): 2, ('final', 0.0): 35, ('parasyt', 0.0): 1, ('alll', 0.0): 1, ('zayniscomingbackonjuli', 0.0): 23, ('26', 0.0): 24, ('bye', 0.0): 8, ('era', 0.0): 1, ('。', 0.0): 3, ('ω', 0.0): 1, ('」', 0.0): 2, ('∠', 0.0): 2, ('):', 0.0): 6, ('nathann', 0.0): 1, ('💕', 0.0): 7, ('hug', 0.0): 29, ('😊', 0.0): 9, ('beauti', 0.0): 11, ('dieididieieiei', 0.0): 1, ('stage', 0.0): 15, ('mean', 0.0): 43, ('hello', 0.0): 13, ('lion', 0.0): 3, ('think', 0.0): 75, ('screw', 0.0): 4, ('netflix', 0.0): 5, ('chill', 0.0): 7, ('di', 0.0): 7, ('ervin', 0.0): 1, ('ohh', 0.0): 8, ('yeah', 0.0): 41, ('hope', 0.0): 102, ('accept', 0.0): 2, ('offer', 0.0): 10, ('desper', 0.0): 2, ('year', 0.0): 46, ('snapchat', 0.0): 79, ('amargolonnard', 0.0): 2, ('kikhorni', 0.0): 13, ('snapm', 0.0): 4, ('tagsforlik', 0.0): 5, ('batalladelosgallo', 0.0): 2, ('webcamsex', 0.0): 4, ('ugh', 0.0): 26, ('stream', 0.0): 24, ('duti', 0.0): 3, (\"u'v\", 0.0): 1, ('gone', 0.0): 24, ('alien', 0.0): 1, ('aww', 0.0): 21, ('wanna', 0.0): 94, ('sorka', 0.0): 1, ('funer', 0.0): 4, ('text', 0.0): 15, ('phone', 0.0): 34, ('sunni', 0.0): 1, ('nonexist', 0.0): 1, ('wowza', 0.0): 1, ('fah', 0.0): 1, ('taylor', 0.0): 3, ('crop', 0.0): 1, ('boo', 0.0): 5, ('count', 0.0): 7, ('new', 0.0): 51, ('guitar', 0.0): 1, ('jonghyun', 0.0): 1, ('hyung', 0.0): 1, ('pleas', 0.0): 275, ('predict', 0.0): 2, ('sj', 0.0): 3, ('nomin', 0.0): 1, ('vs', 0.0): 4, ('pl', 0.0): 45, ('dude', 0.0): 12, ('calm', 0.0): 3, ('brace', 0.0): 5, ('sir', 0.0): 5, ('plu', 0.0): 4, ('4', 0.0): 18, ('shock', 0.0): 3, ('omggg', 0.0): 2, ('yall', 0.0): 4, ('deserv', 0.0): 8, ('whenev', 0.0): 3, ('spend', 0.0): 8, ('smoke', 0.0): 3, ('end', 0.0): 40, ('fall', 0.0): 16, ('asleep', 0.0): 25, ('1', 0.0): 26, ('point', 0.0): 14, ('close', 0.0): 20, ('grand', 0.0): 1, ('whyyi', 0.0): 7, ('long', 0.0): 38, ('must', 0.0): 15, ('annoy', 0.0): 11, ('evan', 0.0): 1, ('option', 0.0): 3, ('opt', 0.0): 1, (\"who'\", 0.0): 7, ('giveaway', 0.0): 3, ('muster', 0.0): 1, ('merch', 0.0): 4, ('ah', 0.0): 18, ('funni', 0.0): 6, ('drink', 0.0): 7, ('savanna', 0.0): 1, ('straw', 0.0): 1, ('ignor', 0.0): 16, ('yester', 0.0): 1, ('afternoon', 0.0): 3, ('sleep', 0.0): 90, ('ye', 0.0): 48, ('sadli', 0.0): 11, ('when', 0.0): 2, ('album', 0.0): 16, ('last', 0.0): 72, ('chocol', 0.0): 8, ('consum', 0.0): 1, ('werk', 0.0): 1, ('morn', 0.0): 31, ('foreal', 0.0): 1, ('wesen', 0.0): 1, ('uwes', 0.0): 1, ('mj', 0.0): 1, ('😂', 0.0): 24, ('catch', 0.0): 9, ('onlin', 0.0): 20, ('enough', 0.0): 24, ('haha', 0.0): 30, (\"he'\", 0.0): 23, ('bosen', 0.0): 1, ('die', 0.0): 21, ('egg', 0.0): 4, ('benni', 0.0): 1, ('sometim', 0.0): 16, ('followback', 0.0): 6, ('huhu', 0.0): 17, ('understand', 0.0): 15, ('badli', 0.0): 12, ('scare', 0.0): 16, ('&gt;:(', 0.0): 47, ('al', 0.0): 4, ('kati', 0.0): 3, ('zaz', 0.0): 1, ('ami', 0.0): 2, ('lot', 0.0): 27, ('diari', 0.0): 1, ('read', 0.0): 20, ('rehash', 0.0): 1, ('websit', 0.0): 7, ('mushroom', 0.0): 1, ('piec', 0.0): 4, ('except', 0.0): 5, ('reach', 0.0): 3, ('anyway', 0.0): 12, ('vicki', 0.0): 1, ('omg', 0.0): 63, ('wtf', 0.0): 13, ('lip', 0.0): 3, ('virgin', 0.0): 2, ('your', 0.0): 8, ('45', 0.0): 1, ('hahah', 0.0): 6, ('ninasti', 0.0): 1, ('tsktsk', 0.0): 1, ('oppa', 0.0): 4, ('wont', 0.0): 9, ('dick', 0.0): 5, ('kawaii', 0.0): 1, ('manli', 0.0): 1, ('xbox', 0.0): 3, ('alreadi', 0.0): 52, ('comfi', 0.0): 1, ('bed', 0.0): 12, ('youu', 0.0): 2, ('sigh', 0.0): 13, ('lol', 0.0): 43, ('potato', 0.0): 1, ('fri', 0.0): 7, ('guess', 0.0): 14, (\"y'all\", 0.0): 2, ('ugli', 0.0): 9, ('asf', 0.0): 1, ('huh', 0.0): 7, ('eish', 0.0): 1, ('ive', 0.0): 11, ('quit', 0.0): 9, ('lost', 0.0): 25, ('twitter', 0.0): 30, ('mojo', 0.0): 1, ('dont', 0.0): 53, ('mara', 0.0): 1, ('neh', 0.0): 2, ('fever', 0.0): 7, ('&lt;3', 0.0): 25, ('poor', 0.0): 35, ('bb', 0.0): 7, ('abl', 0.0): 22, ('associ', 0.0): 1, ('councillor', 0.0): 1, ('confer', 0.0): 2, ('weekend', 0.0): 25, ('skype', 0.0): 6, ('account', 0.0): 20, ('hack', 0.0): 8, ('contact', 0.0): 7, ('creat', 0.0): 2, ('tweet', 0.0): 35, ('spree', 0.0): 4, ('na', 0.0): 29, ('sholong', 0.0): 1, ('reject', 0.0): 7, ('propos', 0.0): 2, ('gee', 0.0): 1, ('fli', 0.0): 10, ('gidi', 0.0): 1, ('pamper', 0.0): 1, ('lago', 0.0): 1, ('ehn', 0.0): 1, ('arrest', 0.0): 1, ('girlfriend', 0.0): 2, ('he', 0.0): 3, ('nice', 0.0): 19, ('person', 0.0): 15, ('idk', 0.0): 26, ('anybodi', 0.0): 7, ('song', 0.0): 27, ('disappear', 0.0): 1, ('itun', 0.0): 3, ('daze', 0.0): 1, ('confus', 0.0): 8, ('surviv', 0.0): 5, ('fragment', 0.0): 1, (\"would'v\", 0.0): 2, ('forc', 0.0): 2, ('horribl', 0.0): 9, ('weather', 0.0): 29, ('us', 0.0): 43, ('could', 0.0): 69, ('walao', 0.0): 1, ('kb', 0.0): 1, ('send', 0.0): 12, ('ill', 0.0): 16, ('djderek', 0.0): 1, ('mani', 0.0): 29, ('fun', 0.0): 32, ('gig', 0.0): 3, ('absolut', 0.0): 6, ('legend', 0.0): 3, ('wait', 0.0): 43, ('till', 0.0): 8, ('saturday', 0.0): 10, ('homework', 0.0): 2, ('pa', 0.0): 8, ('made', 0.0): 23, ('da', 0.0): 5, ('greek', 0.0): 2, ('tragedi', 0.0): 1, ('rain', 0.0): 43, ('gym', 0.0): 6, ('💪🏻', 0.0): 1, ('🐒', 0.0): 1, ('what', 0.0): 8, ('wrong', 0.0): 33, ('struck', 0.0): 1, ('anymor', 0.0): 20, ('belgium', 0.0): 4, ('fabian', 0.0): 2, ('delph', 0.0): 6, ('fallen', 0.0): 3, ('hide', 0.0): 4, ('drake', 0.0): 1, ('silent', 0.0): 1, ('hear', 0.0): 33, ('rest', 0.0): 21, ('peac', 0.0): 5, ('mo', 0.0): 4, ('tonight', 0.0): 24, ('t20blast', 0.0): 1, ('ahhh', 0.0): 5, ('wake', 0.0): 21, ('mumma', 0.0): 2, ('7', 0.0): 16, ('dead', 0.0): 10, ('tomorrow', 0.0): 34, (\"i'll\", 0.0): 41, ('high', 0.0): 8, ('low', 0.0): 8, ('pray', 0.0): 13, ('appropri', 0.0): 1, ('. . .', 0.0): 2, ('awak', 0.0): 10, ('woke', 0.0): 14, ('upp', 0.0): 1, ('dm', 0.0): 23, ('luke', 0.0): 6, ('hey', 0.0): 26, ('babe', 0.0): 19, ('across', 0.0): 4, ('hindi', 0.0): 1, ('reaction', 0.0): 1, ('5s', 0.0): 1, ('run', 0.0): 15, ('space', 0.0): 5, ('tbh', 0.0): 14, ('disabl', 0.0): 2, ('pension', 0.0): 1, ('ptsd', 0.0): 1, ('imposs', 0.0): 4, ('physic', 0.0): 7, ('financi', 0.0): 2, ('nooo', 0.0): 16, ('broke', 0.0): 9, ('soo', 0.0): 3, ('amaz', 0.0): 16, ('toghet', 0.0): 1, ('around', 0.0): 20, ('p', 0.0): 5, ('hold', 0.0): 9, ('anoth', 0.0): 27, ('septemb', 0.0): 2, ('21st', 0.0): 2, ('snsd', 0.0): 2, ('interact', 0.0): 2, ('anna', 0.0): 5, ('akana', 0.0): 1, ('askip', 0.0): 1, (\"t'exist\", 0.0): 1, ('channel', 0.0): 6, ('owner', 0.0): 1, ('decid', 0.0): 10, ('broadcast', 0.0): 6, ('kei', 0.0): 2, ('rate', 0.0): 4, ('se', 0.0): 2, ('notic', 0.0): 26, ('exist', 0.0): 2, ('traffic', 0.0): 5, ('terribl', 0.0): 12, ('eye', 0.0): 12, ('small', 0.0): 9, ('kate', 0.0): 2, ('spade', 0.0): 1, ('pero', 0.0): 3, ('walang', 0.0): 1, ('maganda', 0.0): 1, ('aw', 0.0): 42, ('seen', 0.0): 23, ('agesss', 0.0): 1, ('add', 0.0): 26, ('corinehurleigh', 0.0): 1, ('snapchatm', 0.0): 6, ('instagram', 0.0): 4, ('addmeonsnapchat', 0.0): 2, ('sf', 0.0): 3, ('quot', 0.0): 6, ('kiksext', 0.0): 6, ('bum', 0.0): 2, ('zara', 0.0): 1, ('trouser', 0.0): 1, ('effect', 0.0): 4, ('spanish', 0.0): 1, (\"it'okay\", 0.0): 1, ('health', 0.0): 2, ('luck', 0.0): 6, ('freed', 0.0): 1, ('rock', 0.0): 3, ('orcalov', 0.0): 1, ('tri', 0.0): 65, ('big', 0.0): 21, ('cuddl', 0.0): 8, ('lew', 0.0): 1, ('kiss', 0.0): 4, ('em', 0.0): 1, ('crave', 0.0): 8, ('banana', 0.0): 4, ('crumbl', 0.0): 1, ('mcflurri', 0.0): 1, ('cabl', 0.0): 1, ('car', 0.0): 17, ('brother', 0.0): 10, (\"venus'\", 0.0): 1, ('concept', 0.0): 4, ('rli', 0.0): 5, ('tea', 0.0): 7, ('tagal', 0.0): 2, (\"we'v\", 0.0): 3, ('appoint', 0.0): 1, (\"i'd\", 0.0): 11, ('sinc', 0.0): 35, (\"there'\", 0.0): 18, ('milk', 0.0): 3, ('left', 0.0): 26, ('cereal', 0.0): 2, ('film', 0.0): 6, ('date', 0.0): 7, ('previou', 0.0): 2, ('73', 0.0): 2, ('user', 0.0): 1, ('everywher', 0.0): 6, ('fansign', 0.0): 1, ('photo', 0.0): 15, ('expens', 0.0): 7, ('zzzz', 0.0): 1, ('let', 0.0): 37, ('sun', 0.0): 10, ('yet', 0.0): 33, (\"bff'\", 0.0): 1, ('extrem', 0.0): 3, ('stress', 0.0): 10, ('anyth', 0.0): 19, ('win', 0.0): 27, (\"deosn't\", 0.0): 1, ('liverpool', 0.0): 2, ('pool', 0.0): 3, ('though', 0.0): 57, ('bro', 0.0): 3, ('great', 0.0): 22, ('news', 0.0): 21, ('self', 0.0): 1, ('esteem', 0.0): 1, ('lowest', 0.0): 1, ('better', 0.0): 36, ('tacki', 0.0): 1, ('taken', 0.0): 9, ('man', 0.0): 32, ('lucki', 0.0): 16, ('charm', 0.0): 1, ('haaretz', 0.0): 1, ('israel', 0.0): 1, ('syria', 0.0): 2, ('continu', 0.0): 1, ('develop', 0.0): 5, ('chemic', 0.0): 1, ('weapon', 0.0): 2, ('offici', 0.0): 3, ('wsj', 0.0): 2, ('rep', 0.0): 1, ('bt', 0.0): 4, ('mr', 0.0): 9, ('wong', 0.0): 1, ('confisc', 0.0): 1, ('art', 0.0): 4, ('thought', 0.0): 31, ('icepack', 0.0): 1, ('dose', 0.0): 2, ('killer', 0.0): 2, ('board', 0.0): 1, ('whimper', 0.0): 1, ('fan', 0.0): 17, ('senpai', 0.0): 1, ('buttsex', 0.0): 1, ('joke', 0.0): 8, ('headlin', 0.0): 1, (\"dn't\", 0.0): 1, ('brk', 0.0): 1, (\":'(\", 0.0): 13, ('hit', 0.0): 7, ('voic', 0.0): 9, ('falsetto', 0.0): 1, ('zone', 0.0): 2, ('leannerin', 0.0): 1, ('hornykik', 0.0): 17, ('loveofmylif', 0.0): 2, ('dmme', 0.0): 2, ('pussi', 0.0): 2, ('newmus', 0.0): 3, ('sexo', 0.0): 2, ('s2', 0.0): 1, ('spain', 0.0): 4, ('delay', 0.0): 5, ('kill', 0.0): 22, ('singl', 0.0): 10, ('untruth', 0.0): 1, ('cross', 0.0): 4, ('countri', 0.0): 6, ('ij', 0.0): 1, ('💥', 0.0): 1, ('✨', 0.0): 1, ('💫', 0.0): 1, ('bear', 0.0): 2, ('littl', 0.0): 21, ('apart', 0.0): 7, ('live', 0.0): 37, ('soshi', 0.0): 1, ('didnt', 0.0): 24, ('buttt', 0.0): 2, ('congrat', 0.0): 2, ('sunday', 0.0): 8, ('friday', 0.0): 12, ('shoulda', 0.0): 1, ('move', 0.0): 12, ('w', 0.0): 22, ('caus', 0.0): 16, (\"they'r\", 0.0): 14, ('heyyy', 0.0): 1, ('yeol', 0.0): 2, ('solo', 0.0): 6, ('dancee', 0.0): 1, ('inter', 0.0): 1, ('nemanja', 0.0): 1, ('vidic', 0.0): 1, ('roma', 0.0): 1, (\"mom'\", 0.0): 2, ('linguist', 0.0): 1, (\"dad'\", 0.0): 1, ('comput', 0.0): 6, ('scientist', 0.0): 1, ('dumbest', 0.0): 1, ('famili', 0.0): 9, ('broken', 0.0): 11, ('ice', 0.0): 35, ('cream', 0.0): 32, ('pour', 0.0): 1, ('crash', 0.0): 6, ('scienc', 0.0): 1, ('resourc', 0.0): 1, ('vehicl', 0.0): 5, ('ate', 0.0): 10, ('ayex', 0.0): 1, ('eat', 0.0): 27, ('swear', 0.0): 6, ('lamon', 0.0): 1, ('scroll', 0.0): 1, ('curv', 0.0): 2, ('😉', 0.0): 1, ('cement', 0.0): 1, ('cast', 0.0): 5, ('10.3', 0.0): 1, ('k', 0.0): 9, ('sign', 0.0): 9, ('zayn', 0.0): 8, ('bot', 0.0): 1, ('plz', 0.0): 3, ('mention', 0.0): 9, ('jmu', 0.0): 1, ('camp', 0.0): 7, ('teas', 0.0): 3, ('sweetest', 0.0): 1, ('awuna', 0.0): 1, ('mbulelo', 0.0): 1, ('match', 0.0): 7, ('pig', 0.0): 2, ('although', 0.0): 5, ('crackl', 0.0): 1, ('nois', 0.0): 3, ('plug', 0.0): 2, ('fuse', 0.0): 1, ('dammit', 0.0): 3, ('tip', 0.0): 2, ('carlton', 0.0): 2, ('aflblueshawk', 0.0): 2, (\"alex'\", 0.0): 1, ('hous', 0.0): 16, ('motorsport', 0.0): 1, ('seri', 0.0): 3, ('disc', 0.0): 1, ('right', 0.0): 51, ('cheeki', 0.0): 1, ('j', 0.0): 1, ('instead', 0.0): 4, ('seo', 0.0): 1, ('nl', 0.0): 1, ('bud', 0.0): 1, ('christi', 0.0): 1, ('xo', 0.0): 1, ('niec', 0.0): 1, ('summer', 0.0): 19, ('bloodi', 0.0): 2, ('sandwhich', 0.0): 1, ('buset', 0.0): 1, ('discrimin', 0.0): 4, ('five', 0.0): 5, ('learn', 0.0): 5, ('pregnanc', 0.0): 2, ('foot', 0.0): 5, ('f', 0.0): 4, ('matern', 0.0): 1, ('kick', 0.0): 6, ('domesticviol', 0.0): 1, ('law', 0.0): 4, ('domest', 0.0): 1, ('violenc', 0.0): 2, ('victim', 0.0): 4, ('98fm', 0.0): 1, ('exactli', 0.0): 5, ('unfortun', 0.0): 21, ('yesterday', 0.0): 13, ('uk', 0.0): 9, ('govern', 0.0): 1, ('sapiosexu', 0.0): 1, ('damn', 0.0): 29, ('beta', 0.0): 4, ('12', 0.0): 8, ('hour', 0.0): 35, ('world', 0.0): 17, ('hulk', 0.0): 3, ('hogan', 0.0): 3, ('scrub', 0.0): 1, ('wwe', 0.0): 2, ('histori', 0.0): 2, ('iren', 0.0): 4, ('mistak', 0.0): 6, ('naa', 0.0): 1, ('sold', 0.0): 6, ('h_my_k', 0.0): 1, ('lose', 0.0): 7, ('valentin', 0.0): 2, ('et', 0.0): 3, (\"r'ship\", 0.0): 1, ('btwn', 0.0): 1, ('homo', 0.0): 2, ('biphob', 0.0): 2, ('comment', 0.0): 4, ('certain', 0.0): 6, ('disciplin', 0.0): 2, ('incl', 0.0): 2, ('european', 0.0): 3, ('lang', 0.0): 6, ('lit', 0.0): 2, ('educ', 0.0): 2, ('fresherstofin', 0.0): 1, ('💔', 0.0): 3, ('dream', 0.0): 24, ('gettin', 0.0): 2, ('realist', 0.0): 4, ('thx', 0.0): 1, ('real', 0.0): 21, ('isnt', 0.0): 7, ('prefer', 0.0): 4, ('benzema', 0.0): 2, ('hahahahahaah', 0.0): 1, ('donno', 0.0): 1, ('korean', 0.0): 2, ('languag', 0.0): 5, ('russian', 0.0): 2, ('waaa', 0.0): 1, ('eidwithgrof', 0.0): 1, ('boreddd', 0.0): 1, ('mug', 0.0): 3, ('piss', 0.0): 3, ('tiddler', 0.0): 1, ('silli', 0.0): 2, ('least', 0.0): 15, ('card', 0.0): 7, ('chorong', 0.0): 1, ('leader', 0.0): 1, ('에이핑크', 0.0): 3, ('더쇼', 0.0): 4, ('clan', 0.0): 1, ('slot', 0.0): 2, ('open', 0.0): 16, ('pfff', 0.0): 1, ('privat', 0.0): 2, ('bugbounti', 0.0): 1, ('self-xss', 0.0): 1, ('host', 0.0): 2, ('header', 0.0): 3, ('poison', 0.0): 3, ('code', 0.0): 8, ('execut', 0.0): 1, ('ktksbye', 0.0): 1, ('connect', 0.0): 3, ('compani', 0.0): 3, ('alert', 0.0): 2, ('cancel', 0.0): 10, ('uber', 0.0): 3, ('everyon', 0.0): 26, ('els', 0.0): 4, ('offic', 0.0): 7, ('ahahah', 0.0): 1, ('petit', 0.0): 1, ('relationship', 0.0): 4, ('height', 0.0): 2, ('cost', 0.0): 1, ('600', 0.0): 2, ('£', 0.0): 6, ('secur', 0.0): 4, ('odoo', 0.0): 2, ('8', 0.0): 11, ('partner', 0.0): 2, ('commun', 0.0): 2, ('spirit', 0.0): 3, ('jgh', 0.0): 2, ('effin', 0.0): 1, ('facebook', 0.0): 4, ('anyon', 0.0): 17, (\"else'\", 0.0): 1, ('box', 0.0): 8, ('ap', 0.0): 3, ('stori', 0.0): 13, ('london', 0.0): 12, ('imagin', 0.0): 2, ('elsewher', 0.0): 1, ('someday', 0.0): 1, ('ben', 0.0): 3, ('provid', 0.0): 3, ('name', 0.0): 15, ('branch', 0.0): 1, ('visit', 0.0): 12, ('address', 0.0): 3, ('concern', 0.0): 3, ('welsh', 0.0): 1, ('pod', 0.0): 1, ('juli', 0.0): 12, ('laura', 0.0): 4, ('insid', 0.0): 10, ('train', 0.0): 12, ('d;', 0.0): 1, ('talk-kama', 0.0): 1, ('hawako', 0.0): 1, ('waa', 0.0): 1, ('kimaaani', 0.0): 1, ('prisss', 0.0): 1, ('baggag', 0.0): 2, ('claim', 0.0): 3, ('plane', 0.0): 2, ('niamh', 0.0): 1, ('forev', 0.0): 10, ('hmmm', 0.0): 2, ('sugar', 0.0): 3, ('rare', 0.0): 1, ('paper', 0.0): 16, ('town', 0.0): 14, ('score', 0.0): 3, ('stuck', 0.0): 8, ('agh', 0.0): 2, ('middl', 0.0): 7, ('undercoverboss', 0.0): 1, ('تكفى', 0.0): 1, ('10', 0.0): 8, ('job', 0.0): 13, ('cat', 0.0): 17, ('forgotten', 0.0): 3, ('yep', 0.0): 5, ('stop', 0.0): 43, ('ach', 0.0): 2, ('wrist', 0.0): 1, ('nake', 0.0): 3, ('forgot', 0.0): 14, ('bracelet', 0.0): 3, ('ligo', 0.0): 1, ('dozen', 0.0): 1, ('parent', 0.0): 8, ('children', 0.0): 2, ('shark', 0.0): 2, ('selfi', 0.0): 6, ('heartach', 0.0): 1, ('zayniscomingback', 0.0): 3, ('mix', 0.0): 2, ('sweden', 0.0): 1, ('breath', 0.0): 4, ('moment', 0.0): 14, ('word', 0.0): 16, ('elmhurst', 0.0): 1, ('fc', 0.0): 1, ('etid', 0.0): 1, (\"chillin'with\", 0.0): 1, ('father', 0.0): 2, ('istanya', 0.0): 1, ('2suppli', 0.0): 1, ('extra', 0.0): 3, ('infrastructur', 0.0): 2, ('teacher', 0.0): 2, ('doctor', 0.0): 4, ('nurs', 0.0): 2, ('paramed', 0.0): 1, ('countless', 0.0): 1, ('2cope', 0.0): 1, ('bore', 0.0): 23, ('plea', 0.0): 2, ('arian', 0.0): 1, ('hahahaha', 0.0): 6, ('slr', 0.0): 1, ('kendal', 0.0): 1, ('kyli', 0.0): 3, (\"kylie'\", 0.0): 1, ('manila', 0.0): 3, ('jeebu', 0.0): 1, ('reabsorbt', 0.0): 1, ('tooth', 0.0): 2, ('abscess', 0.0): 1, ('threaten', 0.0): 2, ('affect', 0.0): 1, ('front', 0.0): 6, ('crown', 0.0): 1, ('ooouch', 0.0): 1, ('barney', 0.0): 1, (\"be'\", 0.0): 1, ('yo', 0.0): 4, ('later', 0.0): 14, ('realis', 0.0): 6, ('problemat', 0.0): 1, ('expect', 0.0): 5, ('proud', 0.0): 8, ('mess', 0.0): 7, ('maa', 0.0): 2, ('without', 0.0): 25, ('bangalor', 0.0): 1, ('awww', 0.0): 23, ('lui', 0.0): 1, ('manzano', 0.0): 1, ('shaaa', 0.0): 1, ('super', 0.0): 11, ('7th', 0.0): 1, ('conven', 0.0): 1, ('2:30', 0.0): 2, ('pm', 0.0): 8, ('forward', 0.0): 6, ('delet', 0.0): 5, ('turkey', 0.0): 1, ('bomb', 0.0): 3, ('isi', 0.0): 1, ('allow', 0.0): 9, ('usa', 0.0): 2, ('use', 0.0): 43, ('airfield', 0.0): 1, ('jet', 0.0): 1, (\"jack'\", 0.0): 1, ('spam', 0.0): 6, ('sooo', 0.0): 16, ('☺', 0.0): 3, (\"mommy'\", 0.0): 1, ('reason', 0.0): 8, ('overweight', 0.0): 1, ('sigeg', 0.0): 1, ('habhab', 0.0): 1, ('masud', 0.0): 1, ('kaha', 0.0): 1, ('ko', 0.0): 10, ('akong', 0.0): 1, ('un', 0.0): 1, ('hella', 0.0): 4, ('matter', 0.0): 4, ('pala', 0.0): 1, ('hahaha', 0.0): 11, ('lesson', 0.0): 1, ('dolphin', 0.0): 1, ('xxx', 0.0): 12, ('holi', 0.0): 2, ('anythin', 0.0): 1, ('trend', 0.0): 6, ('radio', 0.0): 4, ('sing', 0.0): 5, ('bewar', 0.0): 1, ('agonis', 0.0): 1, ('experi', 0.0): 2, ('ahead', 0.0): 3, ('modimo', 0.0): 1, ('ho', 0.0): 3, ('tseba', 0.0): 1, ('wena', 0.0): 1, ('fela', 0.0): 1, ('emot', 0.0): 8, ('hubbi', 0.0): 1, ('delight', 0.0): 1, ('return', 0.0): 6, ('bill', 0.0): 6, ('nowt', 0.0): 1, ('wors', 0.0): 8, ('willi', 0.0): 1, ('gon', 0.0): 1, ('vomit', 0.0): 1, ('famou', 0.0): 5, ('bowl', 0.0): 1, ('devast', 0.0): 1, ('titan', 0.0): 1, ('ae', 0.0): 1, ('mark', 0.0): 2, ('hair', 0.0): 21, ('shini', 0.0): 1, ('wavi', 0.0): 1, ('emo', 0.0): 2, ('germani', 0.0): 4, ('load', 0.0): 9, ('shed', 0.0): 2, ('ha', 0.0): 7, ('bheyp', 0.0): 1, ('ayemso', 0.0): 1, ('ear', 0.0): 5, ('swell', 0.0): 2, ('sm', 0.0): 7, ('fb', 0.0): 7, ('remind', 0.0): 3, ('abt', 0.0): 3, ('womad', 0.0): 1, ('wut', 0.0): 1, ('hell', 0.0): 11, ('viciou', 0.0): 1, ('circl', 0.0): 1, ('surpris', 0.0): 5, ('ticket', 0.0): 12, ('codi', 0.0): 1, ('simpson', 0.0): 1, ('concert', 0.0): 11, ('singapor', 0.0): 4, ('august', 0.0): 5, ('pooo', 0.0): 2, ('bh3', 0.0): 1, ('enter', 0.0): 1, ('pitchwar', 0.0): 1, ('chap', 0.0): 1, (\"mine'\", 0.0): 1, ('transcript', 0.0): 1, (\"apma'\", 0.0): 1, ('shoulder', 0.0): 2, ('bitch', 0.0): 11, ('competit', 0.0): 1, (\"it'll\", 0.0): 3, ('fine', 0.0): 6, ('timw', 0.0): 1, ('acc', 0.0): 8, ('rude', 0.0): 11, ('vitamin', 0.0): 1, ('e', 0.0): 9, ('oil', 0.0): 1, ('massag', 0.0): 5, ('everyday', 0.0): 7, ('healthier', 0.0): 1, ('easier', 0.0): 3, ('stretch', 0.0): 1, ('choos', 0.0): 7, ('blockjam', 0.0): 1, (\"schedule'\", 0.0): 1, ('whack', 0.0): 1, ('kik', 0.0): 69, ('thelock', 0.0): 1, ('76', 0.0): 1, ('sex', 0.0): 6, ('omegl', 0.0): 4, ('coupl', 0.0): 2, ('travel', 0.0): 11, ('hotgirl', 0.0): 2, ('2009', 0.0): 1, ('3', 0.0): 32, ('ghantay', 0.0): 1, ('light', 0.0): 8, ('nai', 0.0): 1, ('hay', 0.0): 8, ('deni', 0.0): 1, ('ruin', 0.0): 11, ('laguna', 0.0): 1, ('exit', 0.0): 2, ('gomen', 0.0): 1, ('heck', 0.0): 5, ('fair', 0.0): 12, ('grew', 0.0): 2, ('half', 0.0): 10, ('inch', 0.0): 2, ('two', 0.0): 19, ('problem', 0.0): 7, ('suuuper', 0.0): 1, ('65', 0.0): 1, ('sale', 0.0): 8, ('inact', 0.0): 8, ('orphan', 0.0): 1, ('black', 0.0): 12, ('earlier', 0.0): 9, ('whaaat', 0.0): 5, ('kaya', 0.0): 2, ('naaan', 0.0): 1, ('paus', 0.0): 1, ('randomli', 0.0): 1, ('app', 0.0): 13, ('3:30', 0.0): 1, ('walk', 0.0): 7, ('inglewood', 0.0): 1, ('ummm', 0.0): 4, ('anxieti', 0.0): 3, ('readi', 0.0): 12, ('also', 0.0): 19, ('charcoal', 0.0): 1, ('til', 0.0): 5, ('mid-end', 0.0): 1, ('aug', 0.0): 1, ('noooo', 0.0): 1, ('heard', 0.0): 6, ('rip', 0.0): 12, ('rodfanta', 0.0): 1, ('wasp', 0.0): 2, ('sting', 0.0): 1, ('avert', 0.0): 1, ('bug', 0.0): 3, ('(:', 0.0): 7, ('exo', 0.0): 2, ('seekli', 0.0): 1, ('riptito', 0.0): 1, ('manbearpig', 0.0): 1, ('cannot', 0.0): 7, ('grow', 0.0): 3, ('shorter', 0.0): 1, ('academ', 0.0): 1, ('free', 0.0): 19, ('exclus', 0.0): 2, ('unfair', 0.0): 7, ('esp', 0.0): 4, ('regard', 0.0): 1, ('current', 0.0): 7, ('bleak', 0.0): 1, ('german', 0.0): 1, ('chart', 0.0): 2, ('situat', 0.0): 2, ('entri', 0.0): 4, ('even', 0.0): 70, ('top', 0.0): 6, ('100', 0.0): 8, ('pfft', 0.0): 1, ('place', 0.0): 18, ('white', 0.0): 7, ('wash', 0.0): 1, ('polaroid', 0.0): 1, ('newbethvideo', 0.0): 1, ('greec', 0.0): 2, ('xur', 0.0): 2, ('imi', 0.0): 3, ('fill', 0.0): 1, ('♡', 0.0): 11, ('♥', 0.0): 22, ('xoxoxo', 0.0): 1, ('pictur', 0.0): 17, ('stud', 0.0): 1, ('hund', 0.0): 1, ('6', 0.0): 14, ('kikchat', 0.0): 9, ('amazon', 0.0): 5, ('3.4', 0.0): 1, ('yach', 0.0): 1, ('telat', 0.0): 1, ('huvvft', 0.0): 1, ('zoo', 0.0): 2, ('fieldtrip', 0.0): 1, ('touch', 0.0): 5, ('yan', 0.0): 1, ('posit', 0.0): 2, ('king', 0.0): 1, ('futur', 0.0): 4, ('sizw', 0.0): 1, ('write', 0.0): 13, ('20', 0.0): 9, ('result', 0.0): 3, ('km', 0.0): 2, ('four', 0.0): 4, ('shift', 0.0): 5, ('aaahhh', 0.0): 2, ('boredom', 0.0): 1, ('en', 0.0): 1, ('aint', 0.0): 7, ('who', 0.0): 1, ('sins', 0.0): 1, ('that', 0.0): 13, ('somehow', 0.0): 2, ('tini', 0.0): 4, ('ball', 0.0): 2, ('barbel', 0.0): 1, ('owww', 0.0): 2, ('amsterdam', 0.0): 1, ('luv', 0.0): 2, ('💖', 0.0): 4, ('ps', 0.0): 3, ('looong', 0.0): 1, ('especi', 0.0): 4, (':/', 0.0): 11, ('lap', 0.0): 1, ('litro', 0.0): 1, ('shepherd', 0.0): 2, ('lami', 0.0): 1, ('mayb', 0.0): 27, ('relax', 0.0): 3, ('lungomar', 0.0): 1, ('pesaro', 0.0): 1, ('giachietittiwed', 0.0): 1, ('igersoftheday', 0.0): 1, ('summertim', 0.0): 1, ('nose', 0.0): 7, ('bruis', 0.0): 1, ('lil', 0.0): 8, ('snake', 0.0): 3, ('journey', 0.0): 2, ('scarf', 0.0): 1, ('au', 0.0): 3, ('afford', 0.0): 7, ('fridayfeel', 0.0): 1, ('earli', 0.0): 12, ('money', 0.0): 24, ('chicken', 0.0): 5, ('woe', 0.0): 4, ('nigga', 0.0): 3, ('motn', 0.0): 1, ('make-up', 0.0): 1, ('justic', 0.0): 1, ('import', 0.0): 4, ('sit', 0.0): 5, ('mind', 0.0): 7, ('buy', 0.0): 17, ('limit', 0.0): 4, ('ver', 0.0): 1, ('normal', 0.0): 5, ('edit', 0.0): 7, ('huhuhu', 0.0): 3, ('stack', 0.0): 1, (\"m'ladi\", 0.0): 1, ('j8', 0.0): 1, ('j11', 0.0): 1, ('m20', 0.0): 1, ('jk', 0.0): 5, ('acad', 0.0): 1, ('schedul', 0.0): 9, ('nowww', 0.0): 1, ('cop', 0.0): 1, ('jame', 0.0): 4, ('window', 0.0): 6, ('hugh', 0.0): 2, ('paw', 0.0): 1, ('muddi', 0.0): 1, ('distract', 0.0): 1, ('heyi', 0.0): 1, ('otherwis', 0.0): 3, ('picnic', 0.0): 1, ('24', 0.0): 11, ('cupcak', 0.0): 2, ('talaga', 0.0): 1, ('best', 0.0): 22, ('femal', 0.0): 3, ('poppin', 0.0): 1, ('joc', 0.0): 1, ('playin', 0.0): 1, ('saw', 0.0): 19, ('fix', 0.0): 10, ('coldplay', 0.0): 1, ('media', 0.0): 1, ('player', 0.0): 3, ('fail', 0.0): 10, ('subj', 0.0): 1, ('sobrang', 0.0): 1, ('bv', 0.0): 1, ('zamn', 0.0): 1, ('line', 0.0): 8, ('afropunk', 0.0): 1, ('fest', 0.0): 1, ('brooklyn', 0.0): 2, ('id', 0.0): 5, ('put', 0.0): 14, ('50', 0.0): 5, ('madrid', 0.0): 7, ('shithous', 0.0): 1, ('cutest', 0.0): 2, ('danc', 0.0): 6, ('ur', 0.0): 26, ('arm', 0.0): 3, ('rais', 0.0): 1, ('hand', 0.0): 12, ('ladder', 0.0): 2, ('told', 0.0): 11, ('climb', 0.0): 3, ('success', 0.0): 4, ('nerv', 0.0): 1, ('wrack', 0.0): 1, ('test', 0.0): 8, ('booset', 0.0): 1, ('restart', 0.0): 1, ('assassin', 0.0): 1, ('creed', 0.0): 1, ('ii', 0.0): 1, ('heap', 0.0): 1, ('fell', 0.0): 10, ('daughter', 0.0): 1, ('begin', 0.0): 4, ('ps3', 0.0): 1, ('ankl', 0.0): 4, ('step', 0.0): 5, ('puddl', 0.0): 2, ('wear', 0.0): 5, ('slipper', 0.0): 1, ('eve', 0.0): 1, ('bbi', 0.0): 6, ('sararoc', 0.0): 1, ('angri', 0.0): 5, ('pretti', 0.0): 15, ('fnaf', 0.0): 1, ('holiday', 0.0): 20, ('cheer', 0.0): 6, ('😘', 0.0): 11, ('anywayhedidanicejob', 0.0): 1, ('😞', 0.0): 3, ('3am', 0.0): 2, ('other', 0.0): 7, ('local', 0.0): 3, ('cruis', 0.0): 1, ('done', 0.0): 24, ('doubl', 0.0): 4, ('wail', 0.0): 1, ('manual', 0.0): 2, ('wheelchair', 0.0): 1, ('check', 0.0): 19, ('fit', 0.0): 3, ('nh', 0.0): 3, ('26week', 0.0): 1, ('sbenu', 0.0): 1, ('sasin', 0.0): 1, ('team', 0.0): 14, ('anarchi', 0.0): 1, ('af', 0.0): 14, ('candl', 0.0): 1, ('forehead', 0.0): 4, ('medicin', 0.0): 3, ('welcom', 0.0): 5, ('oop', 0.0): 4, ('hoya', 0.0): 3, ('mah', 0.0): 2, ('a', 0.0): 1, ('nobodi', 0.0): 10, ('awhil', 0.0): 2, ('ago', 0.0): 20, ('b', 0.0): 10, ('hush', 0.0): 2, ('gurli', 0.0): 1, ('bring', 0.0): 9, ('purti', 0.0): 1, ('mouth', 0.0): 5, ('closer', 0.0): 2, ('shiver', 0.0): 1, ('solut', 0.0): 1, ('paid', 0.0): 8, ('properli', 0.0): 2, ('gol', 0.0): 1, ('pea', 0.0): 1, ('english', 0.0): 9, ('mental', 0.0): 4, ('tierd', 0.0): 2, ('third', 0.0): 1, (\"eye'\", 0.0): 1, ('thnkyouuu', 0.0): 1, ('carolin', 0.0): 1, ('neither', 0.0): 6, ('figur', 0.0): 6, ('mirror', 0.0): 1, ('highlight', 0.0): 2, ('pure', 0.0): 3, ('courag', 0.0): 1, ('bit', 0.0): 15, ('fishi', 0.0): 1, ('idek', 0.0): 1, ('apink', 0.0): 5, ('perform', 0.0): 8, ('bulet', 0.0): 1, ('gendut', 0.0): 1, ('noo', 0.0): 5, ('race', 0.0): 3, ('hotwheel', 0.0): 1, ('ms', 0.0): 1, ('patch', 0.0): 1, ('typic', 0.0): 2, ('ahaha', 0.0): 1, ('lay', 0.0): 2, ('wine', 0.0): 1, ('glass', 0.0): 3, (\"where'\", 0.0): 4, ('akon', 0.0): 1, ('somewher', 0.0): 5, ('nightmar', 0.0): 7, ('ya', 0.0): 15, ('mino', 0.0): 2, ('crazyyi', 0.0): 1, ('thooo', 0.0): 1, ('zz', 0.0): 1, ('airport', 0.0): 7, ('straight', 0.0): 4, ('soundcheck', 0.0): 1, ('hmm', 0.0): 4, ('antagonist', 0.0): 1, ('ob', 0.0): 1, ('phantasi', 0.0): 1, ('star', 0.0): 4, ('ip', 0.0): 1, ('issu', 0.0): 11, ('bruce', 0.0): 1, ('sleepdepriv', 0.0): 1, ('tiredashel', 0.0): 1, ('4aspot', 0.0): 1, (\"kinara'\", 0.0): 1, ('awami', 0.0): 1, ('question', 0.0): 9, ('niqqa', 0.0): 1, ('answer', 0.0): 14, ('mockingjay', 0.0): 1, ('slow', 0.0): 9, ('pb.contest', 0.0): 1, ('cycl', 0.0): 2, ('aarww', 0.0): 1, ('lmbo', 0.0): 1, ('dangit', 0.0): 1, ('ohmygod', 0.0): 1, ('scenario', 0.0): 1, ('tooo', 0.0): 2, ('duck', 0.0): 1, ('baechyyi', 0.0): 1, ('okayyy', 0.0): 1, ('noon', 0.0): 3, ('drag', 0.0): 5, ('serious', 0.0): 11, ('misundersrand', 0.0): 1, ('chal', 0.0): 1, ('raha', 0.0): 1, ('hai', 0.0): 11, ('yhm', 0.0): 1, ('edsa', 0.0): 2, ('jasmingarrick', 0.0): 2, ('kikmeguy', 0.0): 5, ('webcam', 0.0): 2, ('milf', 0.0): 1, ('nakamaforev', 0.0): 3, ('kiksex', 0.0): 7, (\"unicef'\", 0.0): 1, ('fu', 0.0): 1, ('alon', 0.0): 16, ('manag', 0.0): 13, ('stephen', 0.0): 1, ('street', 0.0): 2, ('35', 0.0): 1, ('min', 0.0): 7, ('appear', 0.0): 2, ('record', 0.0): 6, ('coz', 0.0): 4, ('frustrat', 0.0): 6, ('sent', 0.0): 9, ('interest', 0.0): 9, ('woza', 0.0): 1, ('promis', 0.0): 4, ('senight', 0.0): 1, ('468', 0.0): 1, ('kikmeboy', 0.0): 9, ('gay', 0.0): 6, ('teen', 0.0): 7, ('amateur', 0.0): 5, ('hotscratch', 0.0): 1, ('sell', 0.0): 8, ('sock', 0.0): 6, ('150-160', 0.0): 1, ('peso', 0.0): 1, ('gotta', 0.0): 8, ('pay', 0.0): 8, ('degrassi', 0.0): 1, ('4-6', 0.0): 1, ('bcz', 0.0): 1, ('kat', 0.0): 3, ('chem', 0.0): 2, ('onscreen', 0.0): 1, ('ofscreen', 0.0): 1, ('kinda', 0.0): 10, ('pak', 0.0): 4, ('class', 0.0): 10, ('monthli', 0.0): 1, ('roll', 0.0): 4, ('band', 0.0): 2, ('throw', 0.0): 2, ('ironi', 0.0): 2, ('rhisfor', 0.0): 1, ('500', 0.0): 2, ('bestoftheday', 0.0): 3, ('chat', 0.0): 9, ('camsex', 0.0): 5, ('unfollow', 0.0): 11, ('particular', 0.0): 1, ('support', 0.0): 26, ('bae', 0.0): 11, ('poopi', 0.0): 1, ('pip', 0.0): 1, ('post', 0.0): 12, ('felt', 0.0): 6, ('uff', 0.0): 1, ('1.300', 0.0): 1, ('credit', 0.0): 3, ('glue', 0.0): 1, ('factori', 0.0): 1, ('kuchar', 0.0): 1, ('fast', 0.0): 7, ('graduat', 0.0): 3, ('up', 0.0): 2, ('definit', 0.0): 3, ('uni', 0.0): 2, ('ee', 0.0): 1, ('tommi', 0.0): 1, ('georgia', 0.0): 2, ('bout', 0.0): 2, ('instant', 0.0): 1, ('transmiss', 0.0): 1, ('malik', 0.0): 1, ('orang', 0.0): 2, ('suma', 0.0): 1, ('shouldeeerr', 0.0): 1, ('outfit', 0.0): 5, ('age', 0.0): 8, ('repack', 0.0): 3, ('group', 0.0): 4, ('charl', 0.0): 1, ('grown', 0.0): 2, ('rememb', 0.0): 17, ('dy', 0.0): 1, ('rihanna', 0.0): 1, ('red', 0.0): 4, ('ging', 0.0): 2, ('boot', 0.0): 4, ('closest', 0.0): 3, ('nike', 0.0): 1, ('adida', 0.0): 1, ('inform', 0.0): 4, ('pro@illamasqua.com', 0.0): 1, ('set', 0.0): 13, ('ifeely', 0.0): 1, ('harder', 0.0): 2, ('usual', 0.0): 7, ('ratbaglat', 0.0): 1, ('second', 0.0): 5, ('semest', 0.0): 2, ('gin', 0.0): 1, ('gut', 0.0): 12, ('reynold', 0.0): 1, ('dessert', 0.0): 2, ('season', 0.0): 9, ('villag', 0.0): 1, ('differ', 0.0): 10, ('citi', 0.0): 11, ('unit', 0.0): 3, ('oppress', 0.0): 1, ('mass', 0.0): 2, ('wat', 0.0): 5, ('afghanistn', 0.0): 1, ('war', 0.0): 2, ('tore', 0.0): 1, ('sunggyu', 0.0): 5, ('injur', 0.0): 7, ('plaster', 0.0): 2, ('rtd', 0.0): 1, ('loui', 0.0): 4, ('harri', 0.0): 10, ('5so', 0.0): 7, ('crowd', 0.0): 1, ('stadium', 0.0): 4, ('welder', 0.0): 1, ('ghost', 0.0): 1, ('hogo', 0.0): 1, ('vishaya', 0.0): 1, ('adu', 0.0): 1, ('bjp', 0.0): 1, ('madatt', 0.0): 1, ('anta', 0.0): 1, ('vishwa', 0.0): 1, ('ne', 0.0): 3, ('illa', 0.0): 1, ('wua', 0.0): 1, ('picki', 0.0): 1, ('finger', 0.0): 8, ('favourit', 0.0): 9, ('mutual', 0.0): 2, ('gn', 0.0): 1, ('along', 0.0): 3, ('ass', 0.0): 9, ('thent', 0.0): 1, ('423', 0.0): 1, ('sabadodeganarseguidor', 0.0): 2, ('sexual', 0.0): 4, ('sync', 0.0): 2, ('plug.dj', 0.0): 1, ('peel', 0.0): 1, ('suspems', 0.0): 1, ('cope', 0.0): 3, ('offroad', 0.0): 1, ('adventur', 0.0): 1, ('there', 0.0): 5, ('harvest', 0.0): 1, ('machineri', 0.0): 1, ('inapropri', 0.0): 1, ('weav', 0.0): 2, ('nowher', 0.0): 3, ('decent', 0.0): 2, ('invest', 0.0): 2, ('scottish', 0.0): 1, ('footbal', 0.0): 3, ('dire', 0.0): 2, ('nomoney', 0.0): 1, ('nawf', 0.0): 1, ('sum', 0.0): 2, ('becho', 0.0): 1, ('danni', 0.0): 3, ('eng', 0.0): 2, (\"let'\", 0.0): 5, ('overli', 0.0): 2, ('lab', 0.0): 1, ('ty', 0.0): 3, ('zap', 0.0): 1, ('distress', 0.0): 1, ('shot', 0.0): 6, ('cinema', 0.0): 4, ('louisianashoot', 0.0): 1, ('laugh', 0.0): 7, ('har', 0.0): 3, (\"how'\", 0.0): 5, ('chum', 0.0): 1, ('ncc', 0.0): 1, ('ph', 0.0): 2, ('balik', 0.0): 1, ('naman', 0.0): 1, ('kayo', 0.0): 1, ('itong', 0.0): 1, ('shirt', 0.0): 3, ('thaaat', 0.0): 1, ('ctto', 0.0): 1, ('expir', 0.0): 3, ('bi', 0.0): 2, ('tough', 0.0): 2, ('11', 0.0): 4, ('3:33', 0.0): 2, ('jfc', 0.0): 1, ('bio', 0.0): 3, ('bodo', 0.0): 1, ('amat', 0.0): 1, ('quick', 0.0): 5, ('yelaaa', 0.0): 1, ('dublin', 0.0): 2, ('potter', 0.0): 1, ('marathon', 0.0): 3, ('balanc', 0.0): 2, ('warm', 0.0): 5, ('comic', 0.0): 5, ('pine', 0.0): 1, ('keybind', 0.0): 1, ('featur', 0.0): 4, ('wild', 0.0): 2, ('warfar', 0.0): 1, ('control', 0.0): 2, ('diagnos', 0.0): 1, ('wiv', 0.0): 1, (\"scheuermann'\", 0.0): 1, ('diseas', 0.0): 3, ('bone', 0.0): 1, ('rlyhurt', 0.0): 1, ('howdo', 0.0): 1, ('georgesampson', 0.0): 1, ('stand', 0.0): 6, ('signal', 0.0): 3, ('reckon', 0.0): 1, ('t20', 0.0): 1, ('action', 0.0): 2, ('taunton', 0.0): 1, ('vacat', 0.0): 3, ('excit', 0.0): 6, ('justiceforsandrabland', 0.0): 2, ('sandrabland', 0.0): 6, ('disturb', 0.0): 1, ('women', 0.0): 5, ('happpi', 0.0): 1, ('justinbieb', 0.0): 4, ('daianerufato', 0.0): 3, ('ilysm', 0.0): 3, ('2015', 0.0): 12, ('07:34', 0.0): 1, ('delphi', 0.0): 2, ('weak', 0.0): 2, ('dom', 0.0): 2, ('techniqu', 0.0): 1, ('minc', 0.0): 2, ('complet', 0.0): 9, ('symphoni', 0.0): 1, ('joe', 0.0): 3, ('co', 0.0): 6, ('wth', 0.0): 2, ('aisyhhh', 0.0): 1, ('bald', 0.0): 1, ('14', 0.0): 3, ('seungchan', 0.0): 1, ('aigooo', 0.0): 1, ('riri', 0.0): 1, ('origin', 0.0): 6, ('depend', 0.0): 2, ('vet', 0.0): 1, ('major', 0.0): 2, ('va', 0.0): 1, ('kept', 0.0): 2, ('lumin', 0.0): 1, ('follback', 0.0): 2, ('treat', 0.0): 5, ('v', 0.0): 6, ('product', 0.0): 4, ('letter', 0.0): 1, ('z', 0.0): 5, ('uniqu', 0.0): 2, ('refresh', 0.0): 1, ('popular', 0.0): 1, ('bebee', 0.0): 2, ('lt', 0.0): 1, ('inaccuraci', 0.0): 1, ('inaccur', 0.0): 1, ('worri', 0.0): 8, ('burn', 0.0): 4, ('rn', 0.0): 17, ('tragic', 0.0): 1, ('joy', 0.0): 2, ('sam', 0.0): 4, ('rush', 0.0): 2, ('toronto', 0.0): 1, ('stuart', 0.0): 1, (\"party'\", 0.0): 2, ('iyalaya', 0.0): 1, ('shade', 0.0): 3, ('round', 0.0): 3, ('clock', 0.0): 2, (';(', 0.0): 6, ('happier', 0.0): 1, ('h', 0.0): 8, ('ubusi', 0.0): 1, ('le', 0.0): 3, ('fifa', 0.0): 1, ('gymnast', 0.0): 1, ('aahhh', 0.0): 1, ('noggin', 0.0): 1, ('bump', 0.0): 1, ('feelslikeanidiot', 0.0): 1, ('pregnant', 0.0): 2, ('woman', 0.0): 5, ('dearli', 0.0): 1, ('sunshin', 0.0): 4, ('suk', 0.0): 2, ('pumpkin', 0.0): 1, ('scone', 0.0): 1, ('outnumb', 0.0): 1, ('vidcon', 0.0): 10, ('eri', 0.0): 1, ('geez', 0.0): 1, ('preciou', 0.0): 4, ('hive', 0.0): 1, ('vote', 0.0): 7, ('vietnam', 0.0): 1, ('decemb', 0.0): 2, ('dunt', 0.0): 1, ('ikr', 0.0): 3, ('sob', 0.0): 3, ('buff', 0.0): 1, ('leg', 0.0): 4, ('toni', 0.0): 1, ('deactiv', 0.0): 6, ('bra', 0.0): 2, (\"shady'\", 0.0): 1, ('isibaya', 0.0): 1, ('special', 0.0): 3, ('❤', 0.0): 21, ('️', 0.0): 19, ('😓', 0.0): 2, ('slept', 0.0): 5, ('colder', 0.0): 1, ('took', 0.0): 9, ('med', 0.0): 1, ('sausag', 0.0): 1, ('adio', 0.0): 1, ('cold', 0.0): 15, ('sore', 0.0): 9, ('ew', 0.0): 3, ('h8', 0.0): 1, ('messeng', 0.0): 2, ('shittier', 0.0): 1, ('leno', 0.0): 1, ('ident', 0.0): 1, ('crisi', 0.0): 2, ('roommat', 0.0): 1, ('knock', 0.0): 3, ('nighter', 0.0): 3, ('bird', 0.0): 2, ('flew', 0.0): 2, ('thru', 0.0): 2, ('derek', 0.0): 3, ('tour', 0.0): 7, ('wetherspoon', 0.0): 1, ('pub', 0.0): 1, ('polic', 0.0): 4, ('frank', 0.0): 2, ('ocean', 0.0): 4, ('releas', 0.0): 8, ('ff', 0.0): 4, ('lisah', 0.0): 2, ('kikm', 0.0): 8, ('eboni', 0.0): 2, ('weloveyounamjoon', 0.0): 1, ('gave', 0.0): 8, ('dress', 0.0): 6, ('polka', 0.0): 1, ('dot', 0.0): 2, ('ndi', 0.0): 1, ('yum', 0.0): 1, ('feed', 0.0): 3, ('leftov', 0.0): 2, ('side', 0.0): 6, ('cs', 0.0): 2, ('own', 0.0): 1, ('walnut', 0.0): 1, ('whip', 0.0): 1, ('wife', 0.0): 6, ('boah', 0.0): 1, ('madi', 0.0): 2, ('def', 0.0): 3, ('manga', 0.0): 1, ('giant', 0.0): 3, ('aminormalyet', 0.0): 1, ('cooki', 0.0): 2, ('breakfast', 0.0): 5, ('clutch', 0.0): 1, ('poorli', 0.0): 6, ('tummi', 0.0): 6, ('pj', 0.0): 1, ('groan', 0.0): 1, ('nou', 0.0): 1, ('adam', 0.0): 2, ('ken', 0.0): 1, ('sara', 0.0): 2, ('sister', 0.0): 4, ('accid', 0.0): 2, ('sort', 0.0): 7, ('mate', 0.0): 2, ('pick', 0.0): 12, ('rang', 0.0): 4, ('fk', 0.0): 2, ('freak', 0.0): 5, ('describ', 0.0): 1, ('eric', 0.0): 2, ('prydz', 0.0): 1, ('sister-in-law', 0.0): 1, ('instal', 0.0): 2, ('seat', 0.0): 4, ('bought', 0.0): 6, ('rear-end', 0.0): 1, (\"everyone'\", 0.0): 4, ('trash', 0.0): 2, ('boob', 0.0): 3, ('whilst', 0.0): 3, ('stair', 0.0): 1, ('childhood', 0.0): 1, ('toothsensit', 0.0): 4, ('size', 0.0): 9, ('ke', 0.0): 3, ('shem', 0.0): 2, ('trust', 0.0): 2, ('awel', 0.0): 1, ('drunk', 0.0): 2, ('weekendofmad', 0.0): 1, ('🍹', 0.0): 3, ('🍸', 0.0): 1, ('cb', 0.0): 1, ('dancer', 0.0): 1, ('choregraph', 0.0): 1, ('626-430-8715', 0.0): 1, ('messag', 0.0): 8, ('repli', 0.0): 14, ('hoe', 0.0): 1, ('xd', 0.0): 7, ('xiu', 0.0): 1, ('nk', 0.0): 1, ('gi', 0.0): 2, ('uss', 0.0): 1, ('eliss', 0.0): 1, ('ksoo', 0.0): 2, ('session', 0.0): 5, ('tat', 0.0): 1, ('bcoz', 0.0): 1, ('bet', 0.0): 10, ('rancho', 0.0): 1, ('imperi', 0.0): 1, ('de', 0.0): 1, ('silang', 0.0): 1, ('subdivis', 0.0): 1, ('center', 0.0): 1, ('39', 0.0): 1, ('cornwal', 0.0): 1, ('verit', 0.0): 1, ('prize', 0.0): 2, ('regular', 0.0): 3, ('workout', 0.0): 1, ('spin', 0.0): 1, ('base', 0.0): 1, ('upon', 0.0): 1, ('penni', 0.0): 1, ('ebook', 0.0): 1, ('фотосет', 0.0): 1, ('addicted-to-analsex', 0.0): 1, ('sweetbj', 0.0): 2, ('blowjob', 0.0): 1, ('mhhh', 0.0): 1, ('sed', 0.0): 1, ('sg', 0.0): 1, ('dinner', 0.0): 4, ('bless', 0.0): 2, ('mee', 0.0): 2, ('enviou', 0.0): 1, ('eonni', 0.0): 1, ('lovey', 0.0): 1, ('dovey', 0.0): 1, ('dongsaeng', 0.0): 1, ('workin', 0.0): 1, ('tuesday', 0.0): 4, ('schade', 0.0): 3, ('belfast', 0.0): 1, ('jealou', 0.0): 9, ('jacob', 0.0): 5, ('isco', 0.0): 4, ('peni', 0.0): 1, ('everi', 0.0): 16, ('convers', 0.0): 6, ('wonder', 0.0): 11, ('soul', 0.0): 5, ('nation', 0.0): 2, ('louisiana', 0.0): 4, ('lafayett', 0.0): 2, ('matteroftheheart', 0.0): 1, ('waduh', 0.0): 1, ('pant', 0.0): 3, ('suspend', 0.0): 2, ('believ', 0.0): 14, ('teenag', 0.0): 2, ('clich', 0.0): 1, ('youuu', 0.0): 5, ('rma', 0.0): 1, ('jersey', 0.0): 2, ('fake', 0.0): 4, ('jaclintil', 0.0): 1, ('model', 0.0): 9, ('likeforlik', 0.0): 7, ('mpoint', 0.0): 4, ('hotfmnoaidilforariana', 0.0): 2, ('ran', 0.0): 5, ('fuckkk', 0.0): 1, ('jump', 0.0): 3, ('justin', 0.0): 3, ('finish', 0.0): 14, ('sanum', 0.0): 1, ('llaollao', 0.0): 1, ('foood', 0.0): 1, ('ubericecream', 0.0): 14, ('glare', 0.0): 1, ('vine', 0.0): 3, ('tweetin', 0.0): 1, ('mood', 0.0): 3, ('elbow', 0.0): 1, ('choreo', 0.0): 1, ('offens', 0.0): 2, ('yeyi', 0.0): 1, ('hd', 0.0): 2, ('brow', 0.0): 1, ('kit', 0.0): 6, ('slightli', 0.0): 2, ('monday', 0.0): 10, ('sux', 0.0): 1, ('enjoy', 0.0): 9, ('nothaveld', 0.0): 1, ('765', 0.0): 1, ('edm', 0.0): 1, ('likeforfollow', 0.0): 3, ('hannib', 0.0): 3, ('mosquito', 0.0): 2, ('bite', 0.0): 5, ('kinki', 0.0): 1, ('hsould', 0.0): 1, ('justget', 0.0): 1, ('marri', 0.0): 2, ('la', 0.0): 11, ('shuffl', 0.0): 4, ('int', 0.0): 1, ('buckl', 0.0): 1, ('spring', 0.0): 1, ('millz', 0.0): 1, ('aski', 0.0): 2, ('awusasho', 0.0): 1, ('unlucki', 0.0): 2, ('driver', 0.0): 7, ('briefli', 0.0): 1, ('spot', 0.0): 4, ('144p', 0.0): 1, ('brook', 0.0): 1, ('crack', 0.0): 2, ('＠', 0.0): 5, ('maverickgam', 0.0): 4, ('07:32', 0.0): 1, ('07:25', 0.0): 1, ('max', 0.0): 3, ('file', 0.0): 2, ('extern', 0.0): 2, ('sd', 0.0): 1, ('via', 0.0): 1, ('airdroid', 0.0): 1, ('android', 0.0): 2, ('4.4+', 0.0): 1, ('googl', 0.0): 5, ('alright', 0.0): 3, ('cramp', 0.0): 2, ('&lt;/3', 0.0): 6, ('unstan', 0.0): 1, ('tay', 0.0): 2, ('ngeze', 0.0): 1, ('cocktaili', 0.0): 1, ('classi', 0.0): 1, ('07:24', 0.0): 1, ('✈', 0.0): 2, ('️2', 0.0): 1, ('raini', 0.0): 2, ('☔', 0.0): 2, ('peter', 0.0): 1, ('pen', 0.0): 1, ('spare', 0.0): 1, ('guest', 0.0): 2, ('barcelona', 0.0): 2, ('bilbao', 0.0): 1, ('booti', 0.0): 2, ('sharyl', 0.0): 1, ('shane', 0.0): 2, ('ta', 0.0): 1, ('giddi', 0.0): 1, ('d1', 0.0): 1, ('zipper', 0.0): 1, ('beyond', 0.0): 1, ('repair', 0.0): 4, ('iphon', 0.0): 5, ('upgrad', 0.0): 1, ('april', 0.0): 1, ('2016', 0.0): 1, ('cont', 0.0): 2, ('england', 0.0): 4, ('wore', 0.0): 2, ('greet', 0.0): 5, ('tempt', 0.0): 2, ('whole', 0.0): 16, ('pack', 0.0): 6, ('oreo', 0.0): 2, ('strength', 0.0): 1, ('wifi', 0.0): 5, ('network', 0.0): 4, ('within', 0.0): 3, ('lolipop', 0.0): 1, ('kebab', 0.0): 1, ('klappertart', 0.0): 1, ('cake', 0.0): 10, ('moodbost', 0.0): 2, ('shoot', 0.0): 6, ('unprepar', 0.0): 1, ('sri', 0.0): 1, ('dresscod', 0.0): 1, ('door', 0.0): 6, ('iam', 0.0): 2, ('dnt', 0.0): 1, ('stab', 0.0): 3, ('meh', 0.0): 3, ('wrocilam', 0.0): 1, ('otp', 0.0): 3, ('5', 0.0): 14, ('looww', 0.0): 1, ('recov', 0.0): 2, ('wayn', 0.0): 2, ('insur', 0.0): 3, ('loss', 0.0): 3, ('stolen', 0.0): 2, ('accident', 0.0): 1, ('damag', 0.0): 5, ('devic', 0.0): 3, ('warranti', 0.0): 1, ('centr', 0.0): 2, ('👌', 0.0): 1, ('lmfaoo', 0.0): 1, ('accur', 0.0): 2, ('fra', 0.0): 4, ('aliv', 0.0): 2, ('steel', 0.0): 2, ('otamendi', 0.0): 1, ('ny', 0.0): 2, ('🚖', 0.0): 1, ('🗽', 0.0): 1, ('🌃', 0.0): 1, ('stealth', 0.0): 2, ('bastard', 0.0): 2, ('inc', 0.0): 3, ('steam', 0.0): 2, ('therapi', 0.0): 1, ('exhaust', 0.0): 3, ('lie', 0.0): 7, ('total', 0.0): 11, ('block', 0.0): 11, ('choic', 0.0): 5, ('switzerland', 0.0): 1, ('kfc', 0.0): 1, ('common', 0.0): 4, ('th', 0.0): 5, ('wolrd', 0.0): 1, ('fyn', 0.0): 1, ('drop', 0.0): 10, ('state', 0.0): 4, ('3g', 0.0): 2, ('christ', 0.0): 1, ('scale', 0.0): 1, ('deck', 0.0): 1, ('chair', 0.0): 4, ('yk', 0.0): 1, ('resi', 0.0): 1, ('memori', 0.0): 5, ('nude', 0.0): 4, ('bruh', 0.0): 3, ('prepar', 0.0): 3, ('lock', 0.0): 2, ('view', 0.0): 7, ('fbc', 0.0): 3, ('mork', 0.0): 1, ('873', 0.0): 1, ('kikgirl', 0.0): 13, ('premiostumundo', 0.0): 2, ('hotspotwithdanri', 0.0): 1, ('hospit', 0.0): 3, ('food', 0.0): 18, ('sone', 0.0): 1, ('produc', 0.0): 1, ('potag', 0.0): 1, ('tomato', 0.0): 1, ('blight', 0.0): 1, ('sheffield', 0.0): 1, ('mych', 0.0): 1, ('shiiit', 0.0): 2, ('screenshot', 0.0): 4, ('prompt', 0.0): 1, ('areadi', 0.0): 1, ('similar', 0.0): 4, ('soulmat', 0.0): 1, ('canon', 0.0): 1, ('zzz', 0.0): 2, ('britain', 0.0): 1, ('😁', 0.0): 3, ('mana', 0.0): 2, ('hw', 0.0): 1, ('jouch', 0.0): 1, ('por', 0.0): 1, ('que', 0.0): 1, ('liceooo', 0.0): 1, ('30', 0.0): 3, ('minut', 0.0): 6, ('pass', 0.0): 13, ('ayala', 0.0): 1, ('tunnel', 0.0): 2, ('thatscold', 0.0): 1, ('80', 0.0): 1, ('snap', 0.0): 3, ('lourd', 0.0): 1, ('bang', 0.0): 3, ('anywher', 0.0): 4, ('water', 0.0): 8, ('road', 0.0): 1, ('showbox', 0.0): 1, ('naruto', 0.0): 1, ('cartoon', 0.0): 1, ('companion', 0.0): 2, ('skinni', 0.0): 3, ('fat', 0.0): 4, ('bare', 0.0): 6, ('dubai', 0.0): 3, ('calum', 0.0): 1, ('ashton', 0.0): 1, ('✧', 0.0): 8, ('｡', 0.0): 8, ('chelni', 0.0): 4, ('disappoint', 0.0): 13, ('everybodi', 0.0): 5, ('due', 0.0): 14, ('laribuggi', 0.0): 1, ('medic', 0.0): 1, ('nutella', 0.0): 1, (\"could'v\", 0.0): 3, ('siriu', 0.0): 1, ('goat', 0.0): 4, ('frudg', 0.0): 1, ('mike', 0.0): 1, ('cloth', 0.0): 6, ('stuff', 0.0): 11, ('sat', 0.0): 3, ('number', 0.0): 6, ('ring', 0.0): 1, ('bbz', 0.0): 1, ('angek', 0.0): 1, ('sbali', 0.0): 1, ('euuuwww', 0.0): 2, ('lunch', 0.0): 10, ('construct', 0.0): 3, ('worker', 0.0): 3, ('1k', 0.0): 3, ('style', 0.0): 4, ('nell', 0.0): 1, ('ik', 0.0): 2, ('death', 0.0): 3, ('jaysu', 0.0): 1, ('toast', 0.0): 1, ('insecur', 0.0): 2, ('buti', 0.0): 1, ('ure', 0.0): 2, ('poop', 0.0): 1, ('gorgeou', 0.0): 2, ('angel', 0.0): 2, ('rome', 0.0): 1, ('throat', 0.0): 10, ('llama', 0.0): 1, ('urself', 0.0): 2, ('getwellsoonamb', 0.0): 1, ('heath', 0.0): 2, ('ledger', 0.0): 1, ('appl', 0.0): 3, ('permiss', 0.0): 2, ('2-0', 0.0): 1, ('lead', 0.0): 3, ('supersport', 0.0): 1, ('milkshak', 0.0): 1, ('witcher', 0.0): 1, ('papertown', 0.0): 1, ('bale', 0.0): 1, ('9', 0.0): 5, ('méxico', 0.0): 1, ('bahay', 0.0): 1, ('bahayan', 0.0): 1, ('magisa', 0.0): 1, ('sadlyf', 0.0): 1, ('bunso', 0.0): 1, ('sleeep', 0.0): 4, ('astonvilla', 0.0): 1, ('berigaud', 0.0): 1, ('bakar', 0.0): 1, ('club', 0.0): 4, ('dear', 0.0): 11, ('allerg', 0.0): 4, ('depress', 0.0): 5, (\"blaine'\", 0.0): 1, ('acoust', 0.0): 2, ('version', 0.0): 5, ('excus', 0.0): 3, ('hernia', 0.0): 3, ('toxin', 0.0): 1, ('freedom', 0.0): 1, ('organ', 0.0): 2, ('ariel', 0.0): 1, ('slap', 0.0): 1, ('slam', 0.0): 1, ('bee', 0.0): 1, ('unknown', 0.0): 2, ('finddjderek', 0.0): 1, ('smell', 0.0): 3, ('uuughhh', 0.0): 1, ('grabe', 0.0): 5, ('ka', 0.0): 5, ('where', 0.0): 1, ('gf', 0.0): 3, ('james_yammouni', 0.0): 1, ('smi', 0.0): 1, ('nemesi', 0.0): 1, ('rule', 0.0): 1, ('doesnt', 0.0): 2, ('appeal', 0.0): 1, ('neeein', 0.0): 1, ('saaad', 0.0): 3, ('less', 0.0): 3, ('hang', 0.0): 7, ('creas', 0.0): 1, ('tan', 0.0): 3, ('dalla', 0.0): 4, ('suppos', 0.0): 7, ('infront', 0.0): 2, ('beato', 0.0): 1, ('tim', 0.0): 2, ('prob', 0.0): 5, ('minha', 0.0): 1, ('deleici', 0.0): 1, ('hr', 0.0): 2, ('pcb', 0.0): 1, ('ep', 0.0): 5, ('peregrin', 0.0): 1, ('8.40', 0.0): 1, ('pigeon', 0.0): 1, ('feet', 0.0): 3, ('tram', 0.0): 1, ('hav', 0.0): 2, ('spent', 0.0): 5, ('outsid', 0.0): 9, ('apt', 0.0): 1, ('build', 0.0): 3, ('key', 0.0): 3, ('bldg', 0.0): 1, ('wrote', 0.0): 3, ('dark', 0.0): 5, ('swan', 0.0): 1, ('fifth', 0.0): 2, ('mmmm', 0.0): 1, ('avi', 0.0): 4, ('nicki', 0.0): 1, ('fucjikg', 0.0): 1, ('disgust', 0.0): 6, ('buynotanapologyonitun', 0.0): 1, ('aval', 0.0): 1, ('denmark', 0.0): 1, ('nw', 0.0): 2, ('sch', 0.0): 2, ('share', 0.0): 11, ('jeslyn', 0.0): 1, ('72', 0.0): 4, ('root', 0.0): 2, ('kuch', 0.0): 1, ('nahi', 0.0): 1, ('hua', 0.0): 2, ('newbi', 0.0): 1, ('crap', 0.0): 3, ('miracl', 0.0): 1, ('4th', 0.0): 1, ('linda', 0.0): 1, ('click', 0.0): 1, ('pin', 0.0): 2, ('wing', 0.0): 3, ('epic', 0.0): 2, ('page', 0.0): 6, ('ang', 0.0): 8, ('ganda', 0.0): 1, ('💗', 0.0): 4, ('nux', 0.0): 1, ('hinanap', 0.0): 1, ('ako', 0.0): 1, ('uy', 0.0): 1, ('sched', 0.0): 1, ('anyar', 0.0): 1, ('entertain', 0.0): 2, ('typa', 0.0): 3, ('buddi', 0.0): 2, ('transpar', 0.0): 1, ('photoshop', 0.0): 2, ('planner', 0.0): 1, ('helppp', 0.0): 2, ('wearig', 0.0): 1, ('dri', 0.0): 2, ('alot', 0.0): 3, ('bu', 0.0): 5, ('prey', 0.0): 1, ('gross', 0.0): 5, ('drain', 0.0): 3, ('ausfailia', 0.0): 1, ('snow', 0.0): 3, ('footi', 0.0): 3, ('2nd', 0.0): 5, ('row', 0.0): 3, (\"m'\", 0.0): 2, ('kitkat', 0.0): 2, ('bday', 0.0): 7, ('😢', 0.0): 8, ('suger', 0.0): 1, ('olivia', 0.0): 2, ('audit', 0.0): 1, ('american', 0.0): 1, ('idol', 0.0): 2, ('injuri', 0.0): 2, ('appendix', 0.0): 1, ('burst', 0.0): 2, ('append', 0.0): 1, ('yeahh', 0.0): 2, ('fack', 0.0): 2, ('nhl', 0.0): 1, ('khami', 0.0): 2, ('favorit', 0.0): 4, ('rise', 0.0): 3, ('reaali', 0.0): 1, ('ja', 0.0): 2, ('naomi', 0.0): 1, ('modern', 0.0): 1, ('contemporari', 0.0): 1, ('slack', 0.0): 1, ('565', 0.0): 1, ('blond', 0.0): 2, ('jahat', 0.0): 3, ('discount', 0.0): 1, ('thorp', 0.0): 2, ('park', 0.0): 7, ('esnho', 0.0): 1, ('node', 0.0): 1, ('advanc', 0.0): 4, ('directx', 0.0): 1, ('workshop', 0.0): 1, ('p2', 0.0): 1, ('upload', 0.0): 2, ('remov', 0.0): 5, ('blackberri', 0.0): 1, ('shitti', 0.0): 1, ('mobil', 0.0): 2, ('povertyyouareevil', 0.0): 1, ('struggl', 0.0): 4, ('math', 0.0): 1, ('emm', 0.0): 1, ('data', 0.0): 6, ('elgin', 0.0): 1, ('vava', 0.0): 1, ('makati', 0.0): 1, ('💛', 0.0): 4, ('baon', 0.0): 1, ('soup', 0.0): 3, ('soak', 0.0): 1, ('bread', 0.0): 2, ('mush', 0.0): 1, (\"they'd\", 0.0): 2, ('matt', 0.0): 2, ('ouat', 0.0): 1, ('beach', 0.0): 5, ('blinkin', 0.0): 1, ('unblock', 0.0): 1, ('headack', 0.0): 1, ('tension', 0.0): 1, ('erit', 0.0): 1, ('perspect', 0.0): 1, ('wed', 0.0): 4, ('playlist', 0.0): 2, ('endlessli', 0.0): 1, ('blush', 0.0): 1, ('bat', 0.0): 1, ('kiddo', 0.0): 1, ('rumbel', 0.0): 1, ('overwhelm', 0.0): 1, ('thrown', 0.0): 2, ('irrespons', 0.0): 1, ('pakighinabi', 0.0): 1, ('pinkfinit', 0.0): 1, ('beb', 0.0): 2, ('migrain', 0.0): 2, ('almost', 0.0): 11, ('coyot', 0.0): 1, ('outta', 0.0): 1, ('mad', 0.0): 11, ('😒', 0.0): 3, ('headach', 0.0): 9, ('인피니트', 0.0): 2, ('save', 0.0): 6, ('baechu', 0.0): 1, ('calibraskaep', 0.0): 3, ('r', 0.0): 19, ('fanci', 0.0): 2, ('yt', 0.0): 3, ('purchas', 0.0): 2, ('elgato', 0.0): 1, ('ant', 0.0): 2, ('unexpect', 0.0): 2, ('bestfriend', 0.0): 9, ('faint', 0.0): 1, ('bp', 0.0): 1, ('appar', 0.0): 5, ('shower', 0.0): 3, ('subway', 0.0): 1, ('cool', 0.0): 5, ('prayer', 0.0): 2, ('fragil', 0.0): 1, ('huge', 0.0): 3, ('gap', 0.0): 1, ('plot', 0.0): 2, ('bungi', 0.0): 1, ('folk', 0.0): 1, ('raspberri', 0.0): 1, ('pi', 0.0): 1, ('shoe', 0.0): 2, ('woohyun', 0.0): 2, ('guilti', 0.0): 1, ('monica', 0.0): 2, ('davao', 0.0): 1, ('luckyyi', 0.0): 1, ('confid', 0.0): 1, ('eunha', 0.0): 1, ('misplac', 0.0): 1, ('den', 0.0): 1, ('dae', 0.0): 1, ('bap', 0.0): 1, ('likewis', 0.0): 1, ('liam', 0.0): 1, ('dylan', 0.0): 3, ('huehu', 0.0): 1, ('rice', 0.0): 1, ('krispi', 0.0): 1, ('marshmallow', 0.0): 2, ('srsli', 0.0): 7, ('birmingham', 0.0): 1, ('m5m6junction', 0.0): 1, ('soulsurvivor', 0.0): 1, ('stafford', 0.0): 1, ('progress', 0.0): 1, ('mixtur', 0.0): 1, (\"they'v\", 0.0): 4, ('practic', 0.0): 1, ('lage', 0.0): 1, ('ramd', 0.0): 1, ('lesbian', 0.0): 3, ('oralsex', 0.0): 4, ('munchkin', 0.0): 1, ('juja', 0.0): 1, ('murugan', 0.0): 1, ('handl', 0.0): 3, ('dia', 0.0): 2, ('bgtau', 0.0): 1, ('harap', 0.0): 1, ('bagi', 0.0): 1, ('aminn', 0.0): 1, ('fraand', 0.0): 1, ('😬', 0.0): 2, ('bigbang', 0.0): 2, ('steak', 0.0): 1, ('younger', 0.0): 2, ('sian', 0.0): 2, ('pizza', 0.0): 7, ('5am', 0.0): 5, ('nicoleapag', 0.0): 1, ('makeup', 0.0): 4, ('hellish', 0.0): 1, ('thirstyyi', 0.0): 1, ('chesti', 0.0): 1, ('dad', 0.0): 9, (\"nando'\", 0.0): 1, ('22', 0.0): 3, ('bow', 0.0): 2, ('queen', 0.0): 3, ('brave', 0.0): 1, ('hen', 0.0): 1, ('leed', 0.0): 9, ('rdd', 0.0): 1, ('dissip', 0.0): 1, ('. .', 0.0): 1, ('pump', 0.0): 2, ('capee', 0.0): 1, ('japan', 0.0): 2, ('random', 0.0): 1, ('young', 0.0): 5, ('outliv', 0.0): 1, ('x-ray', 0.0): 1, ('dental', 0.0): 1, ('spine', 0.0): 1, ('relief', 0.0): 1, ('popol', 0.0): 1, ('stomach', 0.0): 8, ('frog', 0.0): 2, ('brad', 0.0): 1, ('gen.ad', 0.0): 1, ('price', 0.0): 5, ('negoti', 0.0): 3, ('huhuhuhuhu', 0.0): 1, ('bbmadeinmanila', 0.0): 1, ('findavip', 0.0): 1, ('boyirl', 0.0): 1, ('yasss', 0.0): 1, ('6th', 0.0): 1, ('june', 0.0): 3, ('lain', 0.0): 1, ('diffici', 0.0): 1, ('custom', 0.0): 1, ('internet', 0.0): 9, ('near', 0.0): 9, ('speed', 0.0): 2, ('escap', 0.0): 1, ('rapist', 0.0): 1, ('commit', 0.0): 2, ('crime', 0.0): 1, ('bachpan', 0.0): 1, ('ki', 0.0): 2, ('yaadein', 0.0): 1, ('finnair', 0.0): 1, ('heathrow', 0.0): 1, ('norwegian', 0.0): 1, (':\\\\', 0.0): 1, ('batteri', 0.0): 3, ('upvot', 0.0): 4, ('keeno', 0.0): 1, ('whatthefuck', 0.0): 1, ('grotti', 0.0): 1, ('attent', 0.0): 1, ('seeker', 0.0): 1, ('moral', 0.0): 1, ('fern', 0.0): 1, ('mimi', 0.0): 1, ('bali', 0.0): 1, ('she', 0.0): 4, ('pleasee', 0.0): 3, ('brb', 0.0): 1, ('lowbat', 0.0): 1, ('otwolgrandtrail', 0.0): 4, ('funk', 0.0): 1, ('wewanticecream', 0.0): 1, ('sweat', 0.0): 2, ('eugh', 0.0): 1, ('speak', 0.0): 4, ('occasion', 0.0): 1, (\"izzy'\", 0.0): 1, ('dorm', 0.0): 1, ('choppi', 0.0): 1, ('paul', 0.0): 1, ('switch', 0.0): 4, (\"infinite'\", 0.0): 2, ('5:30', 0.0): 2, ('cayton', 0.0): 1, ('bay', 0.0): 2, ('emma', 0.0): 2, ('jen', 0.0): 1, ('darcey', 0.0): 1, ('connor', 0.0): 1, ('spoke', 0.0): 1, ('nail', 0.0): 2, ('biggest', 0.0): 3, ('blue', 0.0): 5, ('bottl', 0.0): 3, ('roommateexperi', 0.0): 1, ('yup', 0.0): 4, ('avoid', 0.0): 2, ('ic', 0.0): 1, ('te', 0.0): 1, ('auto-followback', 0.0): 1, ('asian', 0.0): 2, ('puppi', 0.0): 3, ('ljp', 0.0): 1, ('1/5', 0.0): 1, ('nowday', 0.0): 1, ('attach', 0.0): 2, ('beat', 0.0): 2, ('numb', 0.0): 1, ('dentist', 0.0): 3, ('misss', 0.0): 2, ('muchhh', 0.0): 1, ('youtub', 0.0): 5, ('rid', 0.0): 3, ('tab', 0.0): 2, ('uca', 0.0): 1, ('onto', 0.0): 2, ('track', 0.0): 3, ('bigtim', 0.0): 1, ('rumor', 0.0): 3, ('warmest', 0.0): 1, ('chin', 0.0): 2, ('tickl', 0.0): 1, ('♫', 0.0): 1, ('zikra', 0.0): 1, ('lusi', 0.0): 1, ('hasya', 0.0): 1, ('nugget', 0.0): 3, ('som', 0.0): 1, ('lu', 0.0): 1, ('olymp', 0.0): 1, (\"millie'\", 0.0): 1, ('guinea', 0.0): 1, ('lewi', 0.0): 1, ('748292', 0.0): 1, (\"we'll\", 0.0): 8, ('ano', 0.0): 2, ('22stan', 0.0): 1, ('24/7', 0.0): 2, ('thankyou', 0.0): 2, ('kanina', 0.0): 2, ('breakdown', 0.0): 2, ('mag', 0.0): 2, ('hatee', 0.0): 1, ('leas', 0.0): 1, ('written', 0.0): 2, ('hurri', 0.0): 4, ('attempt', 0.0): 1, ('6g', 0.0): 1, ('unsuccess', 0.0): 1, ('earlob', 0.0): 1, ('sue', 0.0): 1, ('dreari', 0.0): 1, ('denis', 0.0): 1, ('muriel', 0.0): 1, ('ahouré', 0.0): 1, ('pr', 0.0): 1, ('brand', 0.0): 1, ('imag', 0.0): 4, ('opportun', 0.0): 1, ('po', 0.0): 1, ('beg', 0.0): 2, (\"kath'd\", 0.0): 1, ('respond', 0.0): 2, ('chop', 0.0): 1, ('wbu', 0.0): 1, ('yess', 0.0): 2, ('kme', 0.0): 1, ('tom', 0.0): 4, ('cram', 0.0): 1, ('–', 0.0): 1, ('curiou', 0.0): 1, ('on-board', 0.0): 1, ('announc', 0.0): 3, ('trespass', 0.0): 1, ('fr', 0.0): 3, ('clandestin', 0.0): 1, ('muller', 0.0): 1, ('obviou', 0.0): 1, ('mufc', 0.0): 1, ('colour', 0.0): 4, ('stu', 0.0): 2, ('movie', 0.0): 1, ('buddyyi', 0.0): 1, ('feelgoodfriday', 0.0): 1, ('forest', 0.0): 1, ('6:30', 0.0): 1, ('babysit', 0.0): 1, ('opix', 0.0): 1, ('805', 0.0): 1, ('pilllow', 0.0): 1, ('fool', 0.0): 1, ('brag', 0.0): 1, ('skrillah', 0.0): 1, ('drown', 0.0): 2, ('gue', 0.0): 1, ('report', 0.0): 4, ('eventu', 0.0): 1, ('north', 0.0): 1, ('west', 0.0): 2, ('kitti', 0.0): 1, ('sjkao', 0.0): 1, ('mm', 0.0): 2, ('srri', 0.0): 1, ('honma', 0.0): 1, ('yeh', 0.0): 1, ('walay', 0.0): 1, ('bhi', 0.0): 2, ('bohat', 0.0): 1, ('wailay', 0.0): 1, ('hain', 0.0): 2, ('pre-season', 0.0): 1, ('friendli', 0.0): 3, ('pe', 0.0): 3, ('itna', 0.0): 2, ('shor', 0.0): 1, ('machaya', 0.0): 1, ('mein', 0.0): 1, ('samjha', 0.0): 1, ('cup', 0.0): 3, ('note', 0.0): 2, ('😄', 0.0): 1, ('👍', 0.0): 1, ('😔', 0.0): 7, ('sirkay', 0.0): 1, ('wali', 0.0): 1, ('pyaaz', 0.0): 1, ('daal', 0.0): 2, ('onion', 0.0): 1, ('vinegar', 0.0): 1, ('cook', 0.0): 3, ('tutori', 0.0): 1, ('soho', 0.0): 1, ('wobbl', 0.0): 1, ('server', 0.0): 4, ('ciao', 0.0): 1, ('masaan', 0.0): 1, ('muv', 0.0): 1, ('beast', 0.0): 2, ('hayst', 0.0): 1, ('cr', 0.0): 1, ('hnnn', 0.0): 1, ('fluffi', 0.0): 2, ('comeback', 0.0): 3, ('korea', 0.0): 1, ('wow', 0.0): 10, ('act', 0.0): 4, ('optimis', 0.0): 1, ('soniii', 0.0): 1, ('kahaaa', 0.0): 1, ('shave', 0.0): 3, ('tryna', 0.0): 3, ('healthi', 0.0): 2, ('freez', 0.0): 3, ('fml', 0.0): 4, ('jacket', 0.0): 1, ('sleepi', 0.0): 4, ('cyber', 0.0): 1, ('bulli', 0.0): 2, ('racial', 0.0): 2, ('scari', 0.0): 6, ('hall', 0.0): 1, ('stockholm', 0.0): 1, ('loool', 0.0): 3, ('bunch', 0.0): 3, ('among', 0.0): 1, ('__', 0.0): 2, ('busier', 0.0): 1, ('onward', 0.0): 1, ('ol', 0.0): 2, ('coincid', 0.0): 1, ('imac', 0.0): 1, ('launch', 0.0): 2, ('gram', 0.0): 1, ('nearer', 0.0): 1, ('blain', 0.0): 2, ('darren', 0.0): 2, ('layout', 0.0): 3, ('fuuuck', 0.0): 2, ('jesu', 0.0): 1, ('gishwh', 0.0): 1, ('exclud', 0.0): 1, ('unless', 0.0): 4, ('c', 0.0): 7, ('angelica', 0.0): 1, ('pull', 0.0): 5, ('colleg', 0.0): 5, ('movement', 0.0): 1, ('frou', 0.0): 1, ('vaccin', 0.0): 1, ('armor', 0.0): 2, ('legendari', 0.0): 1, ('cash', 0.0): 2, ('effort', 0.0): 2, ('nat', 0.0): 2, ('brake', 0.0): 1, ('grumpi', 0.0): 4, ('wreck', 0.0): 1, ('decis', 0.0): 2, ('gahhh', 0.0): 1, ('teribl', 0.0): 1, ('kilig', 0.0): 1, ('togeth', 0.0): 7, ('weaker', 0.0): 1, ('shravan', 0.0): 1, ('tv', 0.0): 4, ('stooop', 0.0): 1, ('gi-guilti', 0.0): 1, ('akooo', 0.0): 1, ('imveryverysorri', 0.0): 1, ('cd', 0.0): 1, ('grey', 0.0): 3, ('basenam', 0.0): 1, ('path', 0.0): 1, ('theme', 0.0): 2, ('cigar', 0.0): 1, ('speaker', 0.0): 1, ('volum', 0.0): 1, ('promethazin', 0.0): 1, ('zopiclon', 0.0): 1, ('addit', 0.0): 1, ('quetiapin', 0.0): 1, ('modifi', 0.0): 1, ('prescript', 0.0): 1, ('greska', 0.0): 1, ('macedonian', 0.0): 1, ('slovak', 0.0): 1, ('hike', 0.0): 1, ('certainli', 0.0): 2, ('browser', 0.0): 2, ('os', 0.0): 1, ('zokay', 0.0): 1, ('accent', 0.0): 1, ('b-but', 0.0): 1, ('gintama', 0.0): 1, ('shinsengumi', 0.0): 1, ('chapter', 0.0): 1, ('andi', 0.0): 1, ('crappl', 0.0): 1, ('agre', 0.0): 5, ('ftw', 0.0): 2, ('phandroid', 0.0): 1, ('tline', 0.0): 1, ('orchestra', 0.0): 1, ('ppl', 0.0): 5, ('rehears', 0.0): 1, ('bittersweet', 0.0): 1, ('eunji', 0.0): 1, ('bakit', 0.0): 4, ('121st', 0.0): 1, (\"yesterday'\", 0.0): 1, ('rt', 0.0): 8, ('ehdar', 0.0): 1, ('pegea', 0.0): 1, ('panga', 0.0): 1, ('dosto', 0.0): 1, ('nd', 0.0): 1, ('real_liam_payn', 0.0): 1, ('retweet', 0.0): 5, ('3/10', 0.0): 1, ('dmed', 0.0): 1, ('ad', 0.0): 1, ('yay', 0.0): 3, ('23', 0.0): 2, ('alreaddyyi', 0.0): 1, ('luceleva', 0.0): 1, ('21', 0.0): 1, ('porno', 0.0): 3, ('countrymus', 0.0): 4, ('sexysasunday', 0.0): 2, ('naeun', 0.0): 1, ('goal', 0.0): 5, (\"son'\", 0.0): 1, ('kidney', 0.0): 2, ('printer', 0.0): 1, ('ink', 0.0): 2, ('asham', 0.0): 3, ('ihatesomepeopl', 0.0): 1, ('tabl', 0.0): 2, ('0-2', 0.0): 1, ('brain', 0.0): 2, ('hard-wir', 0.0): 1, ('canadian', 0.0): 1, ('acn', 0.0): 2, ('gulo', 0.0): 1, ('kandekj', 0.0): 1, ('rize', 0.0): 1, ('meydan', 0.0): 1, ('experienc', 0.0): 2, ('fcking', 0.0): 1, ('crei', 0.0): 1, ('stabl', 0.0): 1, ('dormmat', 0.0): 1, ('pre', 0.0): 3, ('bo3', 0.0): 1, ('cod', 0.0): 2, ('redeem', 0.0): 1, ('invalid', 0.0): 1, ('wag', 0.0): 1, ('hopia', 0.0): 1, ('campaign', 0.0): 2, ('editor', 0.0): 1, ('reveal', 0.0): 2, ('booo', 0.0): 2, ('extens', 0.0): 1, ('rightnow', 0.0): 1, ('btu', 0.0): 1, ('karaok', 0.0): 1, ('licenc', 0.0): 1, ('apb', 0.0): 2, ('mbf', 0.0): 1, ('kpop', 0.0): 2, ('hahahaokay', 0.0): 1, ('basara', 0.0): 1, ('capcom', 0.0): 3, ('pc', 0.0): 2, ('url', 0.0): 2, ('web', 0.0): 2, ('site', 0.0): 6, ('design', 0.0): 3, ('grumbl', 0.0): 2, ('migrant', 0.0): 1, ('daddi', 0.0): 4, ('legit', 0.0): 1, ('australia', 0.0): 3, ('awsm', 0.0): 1, ('entir', 0.0): 5, ('tmw', 0.0): 1, ('uwu', 0.0): 1, ('jinki', 0.0): 1, ('taem', 0.0): 1, ('gif', 0.0): 2, ('cambridg', 0.0): 1, ('viath', 0.0): 1, ('brilliant', 0.0): 1, ('cypru', 0.0): 1, ('wet', 0.0): 10, ('30th', 0.0): 1, ('zayncomebackto', 0.0): 2, ('1d', 0.0): 6, ('senior', 0.0): 2, ('spazz', 0.0): 1, ('soobin', 0.0): 1, ('27', 0.0): 1, ('unmarri', 0.0): 1, ('float', 0.0): 3, ('pressur', 0.0): 3, ('winter', 0.0): 4, ('lifetim', 0.0): 2, ('hiondsh', 0.0): 1, ('58543', 0.0): 1, ('kikmenow', 0.0): 9, ('sexdat', 0.0): 2, (\"demi'\", 0.0): 1, ('junjou', 0.0): 2, ('romantica', 0.0): 1, ('cruel', 0.0): 1, ('privileg', 0.0): 2, ('mixtap', 0.0): 2, ('convinc', 0.0): 3, ('friex', 0.0): 1, ('taco', 0.0): 2, ('europ', 0.0): 2, ('shaylan', 0.0): 1, ('4:20', 0.0): 1, ('ylona', 0.0): 1, ('nah', 0.0): 4, ('notanapolog', 0.0): 3, ('ouh', 0.0): 1, ('tax', 0.0): 4, ('ohhh', 0.0): 2, ('nm', 0.0): 1, ('term', 0.0): 1, ('apolog', 0.0): 3, ('encanta', 0.0): 1, ('vale', 0.0): 1, ('osea', 0.0): 1, ('bea', 0.0): 1, ('♛', 0.0): 210, ('》', 0.0): 210, ('beli̇ev', 0.0): 35, ('wi̇ll', 0.0): 35, ('justi̇n', 0.0): 35, ('x15', 0.0): 35, ('350', 0.0): 4, ('ｓｅｅ', 0.0): 35, ('ｍｅ', 0.0): 35, ('40', 0.0): 3, ('dj', 0.0): 2, ('net', 0.0): 2, ('349', 0.0): 1, ('baek', 0.0): 1, ('tight', 0.0): 1, ('dunwan', 0.0): 1, ('suan', 0.0): 1, ('ba', 0.0): 3, ('haiz', 0.0): 1, ('otw', 0.0): 1, ('trade', 0.0): 3, ('venic', 0.0): 1, ('348', 0.0): 1, ('strong', 0.0): 6, ('adult', 0.0): 3, ('347', 0.0): 1, ('tree', 0.0): 3, ('hill', 0.0): 1, ('😕', 0.0): 1, ('com', 0.0): 1, ('insonia', 0.0): 1, ('346', 0.0): 1, ('rick', 0.0): 1, ('ross', 0.0): 1, ('wallet', 0.0): 4, ('empti', 0.0): 3, ('heartbreak', 0.0): 2, ('episod', 0.0): 11, ('345', 0.0): 1, ('milli', 0.0): 1, (':)', 0.0): 2, ('diff', 0.0): 1, ('persona', 0.0): 1, ('golden', 0.0): 1, ('scene', 0.0): 1, ('advert', 0.0): 1, ('determin', 0.0): 2, ('roseburi', 0.0): 1, ('familyhom', 0.0): 1, ('daw', 0.0): 2, ('344', 0.0): 1, ('monkey', 0.0): 1, ('yea', 0.0): 2, ('343', 0.0): 1, ('sweeti', 0.0): 2, ('erica', 0.0): 1, ('istg', 0.0): 1, ('lick', 0.0): 1, ('jackson', 0.0): 4, ('nsbzhdnxndamal', 0.0): 1, ('342', 0.0): 1, ('11:15', 0.0): 1, ('2hour', 0.0): 1, ('11:25', 0.0): 1, ('341', 0.0): 1, ('fandom', 0.0): 2, ('mahilig', 0.0): 1, ('mam-bulli', 0.0): 1, ('mtaani', 0.0): 1, ('tunaita', 0.0): 1, ('viazi', 0.0): 1, ('choma', 0.0): 1, ('laid', 0.0): 1, ('celebr', 0.0): 3, ('7am', 0.0): 1, ('jerk', 0.0): 1, ('lah', 0.0): 2, ('magic', 0.0): 1, ('menil', 0.0): 1, ('340', 0.0): 1, (\"kam'\", 0.0): 1, ('meee', 0.0): 1, ('diz', 0.0): 1, ('biooo', 0.0): 1, ('ay', 0.0): 1, ('taray', 0.0): 1, ('yumu-youtub', 0.0): 1, ('339', 0.0): 1, ('parijat', 0.0): 1, ('willmissyouparijat', 0.0): 1, ('abroad', 0.0): 2, ('jolli', 0.0): 1, ('scotland', 0.0): 2, ('338', 0.0): 1, ('mcnugget', 0.0): 1, ('sophi', 0.0): 5, ('feedback', 0.0): 4, ('met', 0.0): 7, ('caramello', 0.0): 2, ('koala', 0.0): 1, ('bar', 0.0): 1, ('suckmejimin', 0.0): 1, ('337', 0.0): 1, ('sucki', 0.0): 2, ('laughter', 0.0): 1, ('pou', 0.0): 1, ('goddamn', 0.0): 1, ('bark', 0.0): 1, ('nje', 0.0): 1, ('blast', 0.0): 1, ('hun', 0.0): 4, ('dbn', 0.0): 2, ('🎀', 0.0): 1, ('336', 0.0): 1, ('hardest', 0.0): 1, ('335', 0.0): 1, ('pledg', 0.0): 1, ('realiz', 0.0): 7, ('viber', 0.0): 1, ('mwah', 0.0): 1, ('estat', 0.0): 1, ('crush', 0.0): 1, ('lansi', 0.0): 1, ('334', 0.0): 1, ('hp', 0.0): 4, ('waah', 0.0): 1, ('miami', 0.0): 1, ('vandag', 0.0): 1, ('kgola', 0.0): 1, ('neng', 0.0): 1, ('eintlik', 0.0): 1, ('porn', 0.0): 2, ('4like', 0.0): 5, ('repost', 0.0): 2, ('333', 0.0): 3, ('magpi', 0.0): 1, ('22.05', 0.0): 1, ('15-24', 0.0): 1, ('05.15', 0.0): 1, ('coach', 0.0): 2, ('ador', 0.0): 1, ('chswiyfxcskcalum', 0.0): 1, ('nvm', 0.0): 2, ('lemm', 0.0): 1, ('quiet', 0.0): 3, ('foof', 0.0): 1, ('332', 0.0): 1, ('casilla', 0.0): 1, ('manchest', 0.0): 3, ('xi', 0.0): 1, ('rmtour', 0.0): 1, ('heavi', 0.0): 3, ('irl', 0.0): 2, ('blooper', 0.0): 2, ('huhuhuhu', 0.0): 1, ('na-tak', 0.0): 1, ('sorta', 0.0): 1, ('unfriend', 0.0): 1, ('greysonch', 0.0): 1, ('sandwich', 0.0): 4, ('bell', 0.0): 1, ('sebastian', 0.0): 1, ('rewatch', 0.0): 1, ('s4', 0.0): 1, ('ser', 0.0): 1, ('past', 0.0): 5, ('heart-break', 0.0): 1, ('outdat', 0.0): 1, ('m4', 0.0): 1, ('abandon', 0.0): 1, ('theater', 0.0): 1, ('smh', 0.0): 6, ('7-3', 0.0): 1, ('7.30-', 0.0): 1, ('ekk', 0.0): 1, ('giriboy', 0.0): 1, ('harriet', 0.0): 1, ('gegu', 0.0): 1, ('gray', 0.0): 1, ('truth', 0.0): 4, ('tbt', 0.0): 1, ('331', 0.0): 1, ('roof', 0.0): 2, ('indian', 0.0): 2, ('polit', 0.0): 3, ('blame', 0.0): 3, ('68', 0.0): 1, ('repres', 0.0): 1, ('corbyn', 0.0): 1, (\"labour'\", 0.0): 1, ('fortun', 0.0): 1, ('icecream', 0.0): 3, ('cuti', 0.0): 2, ('ry', 0.0): 1, ('lfccw', 0.0): 1, ('5ever', 0.0): 1, ('america', 0.0): 3, ('ontheroadagain', 0.0): 1, ('halaaang', 0.0): 1, ('reciev', 0.0): 1, ('flip', 0.0): 4, ('flop', 0.0): 1, ('caesarspalac', 0.0): 1, ('socialreward', 0.0): 1, ('requir', 0.0): 2, ('cali', 0.0): 1, ('fuckboy', 0.0): 1, ('330', 0.0): 1, ('deliveri', 0.0): 3, ('chrompet', 0.0): 1, ('easili', 0.0): 2, ('immun', 0.0): 1, ('system', 0.0): 3, ('lush', 0.0): 1, ('bathtub', 0.0): 1, ('php', 0.0): 1, ('mysql', 0.0): 1, ('libmysqlclient-dev', 0.0): 1, ('dev', 0.0): 2, ('pleasanton', 0.0): 1, ('wala', 0.0): 1, ('329', 0.0): 1, ('quickli', 0.0): 2, ('megan', 0.0): 1, ('heed', 0.0): 2, ('328', 0.0): 1, ('gwss', 0.0): 1, ('thankyouu', 0.0): 1, ('charad', 0.0): 1, ('becom', 0.0): 5, ('piano', 0.0): 2, ('327', 0.0): 1, ('complaint', 0.0): 2, ('yell', 0.0): 2, ('whatsoev', 0.0): 2, ('pete', 0.0): 1, ('wentz', 0.0): 1, ('shogi', 0.0): 1, ('blameshoghicp', 0.0): 1, ('classmat', 0.0): 1, ('troubl', 0.0): 1, ('fixedgearfrenzi', 0.0): 1, ('dispatch', 0.0): 1, ('theyr', 0.0): 2, ('hat', 0.0): 2, (\"shamuon'\", 0.0): 1, ('tokyo', 0.0): 1, ('toe', 0.0): 2, ('horrend', 0.0): 2, (\"someone'\", 0.0): 2, ('326', 0.0): 1, ('hasb', 0.0): 1, ('atti', 0.0): 1, ('muji', 0.0): 1, ('sirf', 0.0): 1, ('sensibl', 0.0): 1, ('etc', 0.0): 2, ('brum', 0.0): 1, ('cyclerevolut', 0.0): 1, ('caaannnttt', 0.0): 1, ('payment', 0.0): 3, ('overdrawn', 0.0): 1, ('tbf', 0.0): 1, ('complain', 0.0): 2, ('perfum', 0.0): 1, ('sampl', 0.0): 1, ('chanel', 0.0): 1, ('burberri', 0.0): 1, ('prada', 0.0): 1, ('325', 0.0): 1, ('noesss', 0.0): 1, ('topgear', 0.0): 1, ('worthi', 0.0): 1, ('bridesmaid', 0.0): 1, (\"tomorrow'\", 0.0): 2, ('gather', 0.0): 1, ('sudden', 0.0): 4, ('324', 0.0): 1, ('randomrestart', 0.0): 1, ('randomreboot', 0.0): 1, ('lumia', 0.0): 1, ('windowsphon', 0.0): 1, (\"microsoft'\", 0.0): 1, ('mañana', 0.0): 1, ('male', 0.0): 1, ('rap', 0.0): 1, ('sponsor', 0.0): 3, ('striker', 0.0): 2, ('lvg', 0.0): 1, ('behind', 0.0): 3, ('refurbish', 0.0): 1, ('cintiq', 0.0): 1, (\"finnick'\", 0.0): 1, ('askfinnick', 0.0): 1, ('contain', 0.0): 1, ('hairi', 0.0): 1, ('323', 0.0): 1, ('buri', 0.0): 1, ('omaygad', 0.0): 1, ('vic', 0.0): 1, ('surgeri', 0.0): 4, ('amber', 0.0): 8, ('tt.tt', 0.0): 1, ('hyper', 0.0): 2, ('vega', 0.0): 2, ('322', 0.0): 1, ('imiss', 0.0): 1, ('321', 0.0): 1, ('320', 0.0): 1, ('know.for', 0.0): 1, ('prepaid', 0.0): 1, ('none', 0.0): 4, ('319', 0.0): 1, ('grandma', 0.0): 1, (\"grandpa'\", 0.0): 1, ('farm', 0.0): 1, ('cow', 0.0): 1, ('sheep', 0.0): 1, ('hors', 0.0): 3, ('fruit', 0.0): 2, ('veget', 0.0): 1, ('puke', 0.0): 2, ('deliri', 0.0): 1, ('motilium', 0.0): 1, ('shite', 0.0): 1, ('318', 0.0): 1, ('schoolwork', 0.0): 1, (\"phoebe'\", 0.0): 1, ('317', 0.0): 1, ('pothol', 0.0): 1, ('316', 0.0): 1, ('notif', 0.0): 3, ('1,300', 0.0): 1, ('robyn', 0.0): 1, ('necklac', 0.0): 1, ('rachel', 0.0): 1, ('bhai', 0.0): 1, ('ramzan', 0.0): 1, ('crosss', 0.0): 1, ('clapham', 0.0): 1, ('investig', 0.0): 2, ('sth', 0.0): 1, ('essenti', 0.0): 1, ('photoshooot', 0.0): 1, ('austin', 0.0): 1, ('mahon', 0.0): 1, ('shut', 0.0): 3, ('andam', 0.0): 1, ('memor', 0.0): 1, ('cotton', 0.0): 1, ('candi', 0.0): 3, ('stock', 0.0): 3, ('swallow', 0.0): 1, ('snot', 0.0): 1, ('choke', 0.0): 1, ('taknottem', 0.0): 1, ('477', 0.0): 1, ('btob', 0.0): 2, ('percentag', 0.0): 1, ('shoshannavassil', 0.0): 1, ('swift', 0.0): 1, ('flat', 0.0): 3, ('a9', 0.0): 2, ('wsalelov', 0.0): 5, ('sexyjan', 0.0): 1, ('horni', 0.0): 2, ('goodmus', 0.0): 4, ('debut', 0.0): 3, ('lart', 0.0): 1, ('sew', 0.0): 1, ('skyfal', 0.0): 1, ('premier', 0.0): 1, ('yummi', 0.0): 2, ('manteca', 0.0): 1, (\"she'd\", 0.0): 2, ('probabl', 0.0): 8, ('shiatsu', 0.0): 1, ('heat', 0.0): 1, ('risk', 0.0): 3, ('edward', 0.0): 1, ('hopper', 0.0): 1, ('eyyah', 0.0): 1, ('utd', 0.0): 2, ('born', 0.0): 1, ('1-0', 0.0): 1, ('cart', 0.0): 1, ('shop', 0.0): 10, ('log', 0.0): 2, ('aaa', 0.0): 2, ('waifu', 0.0): 1, ('break', 0.0): 8, ('breakup', 0.0): 3, ('bother', 0.0): 3, ('bia', 0.0): 1, ('syndrom', 0.0): 1, ('shi', 0.0): 1, ('bias', 0.0): 1, ('pixel', 0.0): 2, ('weh', 0.0): 2, ('area', 0.0): 4, ('maymay', 0.0): 1, ('magpaalam', 0.0): 1, ('tf', 0.0): 3, ('subtitl', 0.0): 1, ('oitnb', 0.0): 1, ('backstori', 0.0): 1, ('jeremi', 0.0): 1, ('kyle', 0.0): 1, ('gimm', 0.0): 2, ('meal', 0.0): 3, ('neat-o', 0.0): 1, ('wru', 0.0): 1, ('scissor', 0.0): 1, ('creation', 0.0): 1, ('public', 0.0): 1, ('amtir', 0.0): 1, ('imysm', 0.0): 2, ('tut', 0.0): 1, ('trop', 0.0): 2, ('tard', 0.0): 1, ('deadlin', 0.0): 1, ('31', 0.0): 2, ('st', 0.0): 3, ('child', 0.0): 4, ('oct', 0.0): 2, ('bush', 0.0): 2, ('premiun', 0.0): 1, ('notcool', 0.0): 1, ('2/3', 0.0): 2, ('lahat', 0.0): 2, ('ng', 0.0): 4, ('araw', 0.0): 1, ('nage', 0.0): 1, ('gyu', 0.0): 4, ('lmfaooo', 0.0): 2, ('download', 0.0): 3, ('leagu', 0.0): 1, ('mashup', 0.0): 1, ('eu', 0.0): 1, ('lc', 0.0): 1, ('typo', 0.0): 2, ('itali', 0.0): 1, ('yass', 0.0): 1, ('christma', 0.0): 2, ('rel', 0.0): 1, ('yr', 0.0): 3, ('sydney', 0.0): 1, ('mb', 0.0): 1, ('perf', 0.0): 2, ('programm', 0.0): 1, ('bff', 0.0): 2, ('hashtag', 0.0): 1, ('omfg', 0.0): 4, ('exercis', 0.0): 2, ('combat', 0.0): 1, ('dosent', 0.0): 1, (\"sod'\", 0.0): 1, ('20min', 0.0): 1, ('request', 0.0): 2, ('yahoo', 0.0): 2, ('yodel', 0.0): 2, ('jokingli', 0.0): 1, ('regret', 0.0): 5, ('starbuck', 0.0): 3, ('lynettelow', 0.0): 1, ('interraci', 0.0): 3, (\"today'\", 0.0): 3, ('tgif', 0.0): 1, ('gahd', 0.0): 1, ('26th', 0.0): 1, ('discov', 0.0): 1, ('12.00', 0.0): 1, ('obyun', 0.0): 1, ('unni', 0.0): 4, ('wayhh', 0.0): 1, ('preval', 0.0): 1, ('controversi', 0.0): 1, ('🍵', 0.0): 2, ('☕', 0.0): 1, ('tube', 0.0): 1, ('strike', 0.0): 3, ('meck', 0.0): 1, ('mcfc', 0.0): 1, ('fresh', 0.0): 1, ('ucan', 0.0): 1, ('anxiou', 0.0): 1, ('poc', 0.0): 1, ('specif', 0.0): 2, ('sinhala', 0.0): 1, ('billionair', 0.0): 1, ('1645', 0.0): 1, ('island', 0.0): 3, ('1190', 0.0): 1, ('maldiv', 0.0): 1, ('dheena', 0.0): 1, ('fasgadah', 0.0): 1, ('alvadhaau', 0.0): 1, ('countdown', 0.0): 1, ('function', 0.0): 3, ('desktop', 0.0): 1, ('evelineconrad', 0.0): 1, ('facetim', 0.0): 4, ('kikmsn', 0.0): 2, ('selfshot', 0.0): 2, ('panda', 0.0): 1, ('backkk', 0.0): 1, ('transfer', 0.0): 3, ('dan', 0.0): 2, ('dull', 0.0): 1, ('overcast', 0.0): 1, ('folder', 0.0): 1, ('truck', 0.0): 2, ('missin', 0.0): 2, ('hangin', 0.0): 1, ('wiff', 0.0): 1, ('dept', 0.0): 1, ('cherri', 0.0): 1, ('bakewel', 0.0): 1, ('collect', 0.0): 3, ('teal', 0.0): 1, ('sect', 0.0): 1, ('tennunb', 0.0): 1, ('rather', 0.0): 4, ('skip', 0.0): 1, ('doomsday', 0.0): 1, ('neglect', 0.0): 1, ('posti', 0.0): 1, ('goodnight', 0.0): 1, ('donat', 0.0): 3, ('ship', 0.0): 6, ('bellami', 0.0): 1, ('raven', 0.0): 2, ('clark', 0.0): 1, ('helmi', 0.0): 1, ('uh', 0.0): 5, ('cnt', 0.0): 1, ('whereisthesun', 0.0): 1, ('summerismiss', 0.0): 1, ('longgg', 0.0): 1, ('ridicul', 0.0): 4, ('stocko', 0.0): 1, ('lucozad', 0.0): 1, ('explos', 0.0): 1, ('beh', 0.0): 2, ('half-rememb', 0.0): 1, (\"melody'\", 0.0): 1, ('recal', 0.0): 2, ('level', 0.0): 3, ('target', 0.0): 1, ('difficult', 0.0): 4, ('mile', 0.0): 1, ('pfb', 0.0): 1, ('nate', 0.0): 2, ('expo', 0.0): 2, ('jisoo', 0.0): 1, ('chloe', 0.0): 2, ('anon', 0.0): 2, ('mager', 0.0): 1, ('wi', 0.0): 1, ('knw', 0.0): 1, ('wht', 0.0): 1, ('distant', 0.0): 1, ('buffer', 0.0): 2, ('insan', 0.0): 1, ('charli', 0.0): 1, ('finland', 0.0): 3, ('gana', 0.0): 1, ('studio', 0.0): 3, ('arch', 0.0): 1, ('lyin', 0.0): 1, ('kian', 0.0): 3, ('supercar', 0.0): 1, ('gurgaon', 0.0): 1, ('locat', 0.0): 7, ('9:15', 0.0): 1, ('satir', 0.0): 1, ('gener', 0.0): 2, ('peanut', 0.0): 3, ('butter', 0.0): 1, ('garden', 0.0): 2, ('beer', 0.0): 1, ('viner', 0.0): 1, ('palembang', 0.0): 1, ('sorrryyi', 0.0): 1, ('fani', 0.0): 1, ('hahahahaha', 0.0): 2, ('boner', 0.0): 1, ('merci', 0.0): 1, ('yuki', 0.0): 1, ('2500k', 0.0): 1, ('mari', 0.0): 1, ('jake', 0.0): 1, ('gyllenha', 0.0): 1, ('impact', 0.0): 1, (\"ledger'\", 0.0): 1, ('btw', 0.0): 5, ('cough', 0.0): 4, ('hunni', 0.0): 1, ('b4', 0.0): 1, ('deplet', 0.0): 1, ('mbasa', 0.0): 1, ('client', 0.0): 3, ('ray', 0.0): 1, ('aah', 0.0): 1, ('type', 0.0): 2, ('suit', 0.0): 5, ('pa-copi', 0.0): 1, ('proper', 0.0): 2, ('biom', 0.0): 1, ('mosqu', 0.0): 1, ('smelli', 0.0): 1, ('taxi', 0.0): 4, ('emptier', 0.0): 1, (\"ciara'\", 0.0): 1, (\"everything'\", 0.0): 1, ('clip', 0.0): 2, ('tall', 0.0): 2, ('gladli', 0.0): 1, ('intent', 0.0): 1, ('amb', 0.0): 1, (\"harry'\", 0.0): 2, ('jean', 0.0): 2, ('mayday', 0.0): 1, ('parad', 0.0): 2, ('lyf', 0.0): 1, ('13th', 0.0): 1, ('anim', 0.0): 4, ('kingdom', 0.0): 1, ('chri', 0.0): 7, ('brown', 0.0): 4, ('riski', 0.0): 1, ('cologn', 0.0): 1, ('duo', 0.0): 3, ('ballad', 0.0): 2, ('bish', 0.0): 2, ('intern', 0.0): 2, ('brought', 0.0): 1, ('yumyum', 0.0): 1, (\"cathy'\", 0.0): 1, ('missyou', 0.0): 1, ('rubi', 0.0): 2, ('rose', 0.0): 2, ('tou', 0.0): 1, ('main', 0.0): 1, ('pora', 0.0): 1, ('stalk', 0.0): 3, ('karlia', 0.0): 1, ('khatam', 0.0): 2, ('bandi', 0.0): 1, ('👑', 0.0): 1, ('pyaari', 0.0): 1, ('gawd', 0.0): 1, ('understood', 0.0): 1, ('review', 0.0): 3, ('massi', 0.0): 1, ('thatselfiethough', 0.0): 1, ('loop', 0.0): 1, ('ofc', 0.0): 1, ('pict', 0.0): 1, ('caught', 0.0): 1, ('aishhh', 0.0): 1, ('viewer', 0.0): 1, ('exam', 0.0): 5, ('sighsss', 0.0): 1, ('burnt', 0.0): 2, ('toffe', 0.0): 2, ('honesti', 0.0): 1, ('cheatday', 0.0): 1, ('protein', 0.0): 1, ('sissi', 0.0): 1, ('tote', 0.0): 1, ('slowli', 0.0): 1, ('church', 0.0): 2, ('pll', 0.0): 1, ('sel', 0.0): 1, ('beth', 0.0): 2, ('serbia', 0.0): 1, ('serbian', 0.0): 1, ('selen', 0.0): 1, ('motav', 0.0): 1, ('💋', 0.0): 2, ('zayyyn', 0.0): 1, ('momma', 0.0): 1, ('happend', 0.0): 1, ('imper', 0.0): 1, ('trmdhesit', 0.0): 1, ('pana', 0.0): 1, ('quickest', 0.0): 2, ('blood', 0.0): 5, ('sake', 0.0): 1, ('hamstr', 0.0): 1, ('rodwel', 0.0): 1, ('trace', 0.0): 1, ('artist', 0.0): 4, ('tp', 0.0): 1, ('powder', 0.0): 1, ('wider', 0.0): 1, ('honestli', 0.0): 4, ('comfort', 0.0): 3, ('bruno', 0.0): 1, ('1.8', 0.0): 1, ('ed', 0.0): 7, ('croke', 0.0): 2, ('deal', 0.0): 6, ('toll', 0.0): 1, ('packag', 0.0): 1, ('shape', 0.0): 1, ('unluckiest', 0.0): 1, ('bettor', 0.0): 1, ('nstp', 0.0): 1, ('sem', 0.0): 2, ('chipotl', 0.0): 1, ('chick-fil-a', 0.0): 1, ('stole', 0.0): 3, ('evet', 0.0): 1, ('ramadhan', 0.0): 1, ('eid', 0.0): 4, ('stexpert', 0.0): 1, ('ripstegi', 0.0): 1, ('nickyyi', 0.0): 1, ('¿', 0.0): 1, ('centralis', 0.0): 1, ('discontinu', 0.0): 1, ('sniff', 0.0): 1, (\"i't\", 0.0): 1, ('glad', 0.0): 2, ('fab', 0.0): 2, ('theres', 0.0): 1, ('cred', 0.0): 1, ('t_t', 0.0): 1, ('elimin', 0.0): 1, ('teamzip', 0.0): 1, ('smtm', 0.0): 1, ('assingn', 0.0): 1, ('editi', 0.0): 1, ('nakaka', 0.0): 1, ('beastmod', 0.0): 1, ('gaaawd', 0.0): 1, ('jane', 0.0): 1, ('mango', 0.0): 1, ('colombia', 0.0): 1, ('yot', 0.0): 1, ('labyo', 0.0): 1, ('pano', 0.0): 1, ('nalamannn', 0.0): 1, ('hardhead', 0.0): 1, ('cell', 0.0): 1, (\"zach'\", 0.0): 1, ('burger', 0.0): 2, ('xpress', 0.0): 1, ('hopkin', 0.0): 1, ('melatonin', 0.0): 1, ('2-4', 0.0): 1, ('nap', 0.0): 2, ('wide', 0.0): 2, ('task', 0.0): 1, ('9pm', 0.0): 1, ('hahaah', 0.0): 1, ('frequent', 0.0): 1, ('jail', 0.0): 2, ('weirddd', 0.0): 1, ('donghyuk', 0.0): 1, ('stan', 0.0): 1, ('bek', 0.0): 1, ('13', 0.0): 4, ('reynoldsgrl', 0.0): 1, ('ole', 0.0): 1, ('beardi', 0.0): 1, ('kaussi', 0.0): 1, ('bummer', 0.0): 3, ('fightingmciren', 0.0): 1, (\"michael'\", 0.0): 1, ('�', 0.0): 21, ('miser', 0.0): 2, ('💦', 0.0): 1, ('yoga', 0.0): 2, ('🌞', 0.0): 1, ('💃🏽', 0.0): 1, ('shouldv', 0.0): 1, ('saffron', 0.0): 1, ('peasant', 0.0): 1, ('wouldv', 0.0): 1, ('nfinit', 0.0): 1, ('admin_myung', 0.0): 1, ('slp', 0.0): 1, ('saddest', 0.0): 2, ('laomma', 0.0): 2, ('kebaya', 0.0): 1, ('bandung', 0.0): 1, ('indonesia', 0.0): 1, ('7df89150', 0.0): 1, ('whatsapp', 0.0): 2, ('62', 0.0): 1, ('08962464174', 0.0): 1, ('laomma_coutur', 0.0): 1, ('haizzz', 0.0): 1, ('urghhh', 0.0): 1, ('working-on-a-tight-schedul', 0.0): 1, ('ganbarimasu', 0.0): 1, ('livid', 0.0): 1, ('whammi', 0.0): 1, ('quuuee', 0.0): 1, ('friooo', 0.0): 1, ('ladi', 0.0): 4, ('stereo', 0.0): 1, ('chwang', 0.0): 1, ('lorm', 0.0): 1, ('823', 0.0): 1, ('rp', 0.0): 1, ('indiemus', 0.0): 10, ('unhappi', 0.0): 2, ('jennyjean', 0.0): 1, ('elfindelmundo', 0.0): 2, ('lolzz', 0.0): 1, ('dat', 0.0): 4, ('corey', 0.0): 1, ('appreci', 0.0): 2, ('weekli', 0.0): 2, ('mahirap', 0.0): 1, ('nash', 0.0): 1, ('gosh', 0.0): 6, ('noodl', 0.0): 1, ('veeerri', 0.0): 1, ('rted', 0.0): 2, ('orig', 0.0): 1, ('starholicxx', 0.0): 1, ('07:17', 0.0): 2, ('@the', 0.0): 1, ('notr', 0.0): 1, ('hwi', 0.0): 1, ('niall', 0.0): 5, ('fraud', 0.0): 1, ('diplomaci', 0.0): 1, ('fittest', 0.0): 1, ('zero', 0.0): 1, ('toler', 0.0): 2, ('gurl', 0.0): 1, ('notion', 0.0): 1, ('pier', 0.0): 1, ('approach', 0.0): 1, ('rattl', 0.0): 1, ('robe', 0.0): 1, ('emphasi', 0.0): 1, ('vocal', 0.0): 1, ('chose', 0.0): 1, ('erm', 0.0): 1, ('abby.can', 0.0): 1, ('persuad', 0.0): 1, ('lyric', 0.0): 1, (\"emily'\", 0.0): 1, ('odd', 0.0): 3, ('possibl', 0.0): 8, ('elect', 0.0): 2, ('kamiss', 0.0): 1, ('mwa', 0.0): 1, ('mommi', 0.0): 3, ('scream', 0.0): 1, ('fight', 0.0): 2, ('cafe', 0.0): 2, ('melbourn', 0.0): 1, ('anyonnee', 0.0): 1, ('loner', 0.0): 1, ('fricken', 0.0): 2, ('rito', 0.0): 1, ('friendzon', 0.0): 1, ('panel', 0.0): 1, ('repeat', 0.0): 2, ('audienc', 0.0): 1, ('hsm', 0.0): 1, ('canario', 0.0): 1, ('hotel', 0.0): 8, ('ukiss', 0.0): 1, ('faith', 0.0): 2, ('kurt', 0.0): 1, (\"fatma'm\", 0.0): 1, ('alex', 0.0): 4, ('swag', 0.0): 1, ('lmfao', 0.0): 2, ('flapjack', 0.0): 1, ('countthecost', 0.0): 1, ('ihop', 0.0): 1, ('infra', 0.0): 1, ('lq', 0.0): 1, ('knive', 0.0): 1, ('sotir', 0.0): 1, ('mybrainneedstoshutoff', 0.0): 1, ('macci', 0.0): 1, ('chees', 0.0): 7, ('25', 0.0): 2, ('tend', 0.0): 1, ('510', 0.0): 1, ('silicon', 0.0): 1, ('cover', 0.0): 2, ('kbye', 0.0): 1, ('ini', 0.0): 1, ('anytim', 0.0): 1, ('citizen', 0.0): 1, ('compar', 0.0): 2, ('rank', 0.0): 1, ('mcountdown', 0.0): 2, ('5h', 0.0): 1, ('thapelo', 0.0): 1, ('op', 0.0): 1, ('civ', 0.0): 1, ('wooden', 0.0): 1, ('mic', 0.0): 1, ('embarrass', 0.0): 2, ('translat', 0.0): 3, ('daili', 0.0): 3, ('mecha-totem', 0.0): 1, ('nak', 0.0): 1, ('tgk', 0.0): 1, ('townsss', 0.0): 1, ('jokid', 0.0): 1, ('rent', 0.0): 2, ('degre', 0.0): 1, ('inconsider', 0.0): 2, ('softbal', 0.0): 1, ('appli', 0.0): 1, ('tomcat', 0.0): 1, ('chel', 0.0): 1, ('jemma', 0.0): 1, ('detail', 0.0): 4, ('list', 0.0): 4, ('matchi', 0.0): 2, ('elsa', 0.0): 1, ('postpon', 0.0): 1, ('karin', 0.0): 1, ('honey', 0.0): 2, ('vist', 0.0): 1, ('unhealthi', 0.0): 1, ('propa', 0.0): 1, ('knockin', 0.0): 1, ('bacon', 0.0): 1, ('market', 0.0): 2, ('pre-holiday', 0.0): 1, ('diet', 0.0): 1, ('meani', 0.0): 1, ('deathbybaconsmel', 0.0): 1, ('init', 0.0): 2, ('destin', 0.0): 1, ('victoria', 0.0): 2, ('luna', 0.0): 1, ('krystal', 0.0): 1, ('sarajevo', 0.0): 1, ('haix', 0.0): 2, ('sp', 0.0): 1, ('student', 0.0): 4, ('wii', 0.0): 2, ('bayonetta', 0.0): 1, ('101', 0.0): 1, ('doabl', 0.0): 1, ('drove', 0.0): 1, ('agenc', 0.0): 1, ('story.miss', 0.0): 1, ('everon', 0.0): 1, ('jp', 0.0): 1, ('mamabear', 0.0): 1, ('imintoh', 0.0): 1, ('underr', 0.0): 1, (\"slovakia'\", 0.0): 1, ('d:', 0.0): 6, ('saklap', 0.0): 1, ('grade', 0.0): 2, ('rizal', 0.0): 1, ('lib', 0.0): 1, ('discuss', 0.0): 1, ('advisori', 0.0): 1, ('period', 0.0): 2, ('dit', 0.0): 1, ('du', 0.0): 1, ('harsh', 0.0): 2, ('ohgod', 0.0): 1, ('abligaverin', 0.0): 2, ('photooftheday', 0.0): 2, ('sexygirlbypreciouslemmi', 0.0): 3, ('ripsandrabland', 0.0): 1, ('edel', 0.0): 1, ('salam', 0.0): 1, ('mubark', 0.0): 1, ('dong', 0.0): 3, ('tammirossm', 0.0): 4, ('speck', 0.0): 1, ('abbymil', 0.0): 2, ('18', 0.0): 8, ('ion', 0.0): 1, ('5min', 0.0): 1, ('hse', 0.0): 1, ('noob', 0.0): 1, ('nxt', 0.0): 1, ('2week', 0.0): 1, ('300', 0.0): 3, ('fck', 0.0): 2, ('nae', 0.0): 2, ('deep', 0.0): 3, ('human', 0.0): 3, ('whit', 0.0): 1, ('van', 0.0): 4, ('bristol', 0.0): 1, ('subserv', 0.0): 1, ('si', 0.0): 4, ('oo', 0.0): 1, ('tub', 0.0): 1, ('penyfan', 0.0): 1, ('forecast', 0.0): 2, ('breconbeacon', 0.0): 1, ('tittheir', 0.0): 1, ('42', 0.0): 1, ('hotti', 0.0): 3, ('uu', 0.0): 2, ('rough', 0.0): 1, ('fuzzi', 0.0): 1, ('san', 0.0): 3, ('antonio', 0.0): 1, ('kang', 0.0): 1, ('junhe', 0.0): 1, ('couldv', 0.0): 1, ('pz', 0.0): 1, ('somerset', 0.0): 1, ('given', 0.0): 2, ('sunburnt', 0.0): 1, ('safer', 0.0): 1, ('k3g', 0.0): 1, ('input', 0.0): 1, ('gamestomp', 0.0): 1, ('desc', 0.0): 1, (\"angelo'\", 0.0): 1, ('yna', 0.0): 1, ('psygustokita', 0.0): 2, ('fiver', 0.0): 1, ('toward', 0.0): 1, ('sakho', 0.0): 1, ('threat', 0.0): 1, ('goalscor', 0.0): 1, ('10:59', 0.0): 1, ('11.00', 0.0): 1, ('sham', 0.0): 1, ('tricki', 0.0): 1, ('baao', 0.0): 1, ('nisrina', 0.0): 1, ('crazi', 0.0): 8, ('ladygaga', 0.0): 1, (\"you'\", 0.0): 2, ('pari', 0.0): 2, ('marrish', 0.0): 1, (\"otp'\", 0.0): 1, ('6:15', 0.0): 1, ('edomnt', 0.0): 1, ('qih', 0.0): 1, ('shxb', 0.0): 1, ('1000', 0.0): 1, ('chilton', 0.0): 1, ('mother', 0.0): 2, ('obsess', 0.0): 1, ('creepi', 0.0): 2, ('josh', 0.0): 1, ('boohoo', 0.0): 1, ('fellow', 0.0): 2, ('tweep', 0.0): 1, ('roar', 0.0): 1, ('victori', 0.0): 1, ('tweepsmatchout', 0.0): 1, ('nein', 0.0): 3, ('404', 0.0): 1, ('midnight', 0.0): 2, ('willlow', 0.0): 1, ('hbd', 0.0): 1, ('sowwi', 0.0): 1, ('3000', 0.0): 1, ('grind', 0.0): 1, ('gear', 0.0): 1, ('0.001', 0.0): 1, ('meant', 0.0): 6, ('portrait', 0.0): 1, ('mode', 0.0): 2, ('fact', 0.0): 4, ('11:11', 0.0): 4, ('shanzay', 0.0): 1, ('salabrati', 0.0): 1, ('journo', 0.0): 1, ('lure', 0.0): 1, ('gang', 0.0): 1, ('twist', 0.0): 1, ('mashaket', 0.0): 1, ('pet', 0.0): 2, ('bapak', 0.0): 1, ('royal', 0.0): 2, ('prima', 0.0): 1, ('mune', 0.0): 1, ('874', 0.0): 1, ('plisss', 0.0): 1, ('elf', 0.0): 1, ('teenchoic', 0.0): 5, ('choiceinternationalartist', 0.0): 5, ('superjunior', 0.0): 5, (\"he'll\", 0.0): 1, ('sunway', 0.0): 1, ('petal', 0.0): 1, ('jaya', 0.0): 1, ('selangor', 0.0): 1, ('glow', 0.0): 1, ('huhuu', 0.0): 1, ('congratul', 0.0): 2, ('margo', 0.0): 1, ('konga', 0.0): 1, ('ni', 0.0): 4, ('wa', 0.0): 2, ('ode', 0.0): 1, ('disvirgin', 0.0): 1, ('bride', 0.0): 3, ('yulin', 0.0): 1, ('meat', 0.0): 1, ('festiv', 0.0): 2, ('imma', 0.0): 2, ('syawal', 0.0): 1, ('lapar', 0.0): 1, ('foundat', 0.0): 1, ('clash', 0.0): 2, ('facil', 0.0): 1, ('dh', 0.0): 2, ('chalet', 0.0): 1, ('suay', 0.0): 1, ('anot', 0.0): 1, ('bugger', 0.0): 1, ('एक', 0.0): 1, ('बार', 0.0): 1, ('फिर', 0.0): 1, ('सेँ', 0.0): 1, ('धोखा', 0.0): 1, ('chandauli', 0.0): 1, ('majhwar', 0.0): 1, ('railway', 0.0): 1, ('tito', 0.0): 2, ('tita', 0.0): 1, ('cousin', 0.0): 3, ('critic', 0.0): 1, ('condit', 0.0): 1, ('steal', 0.0): 1, ('narco', 0.0): 1, ('regen', 0.0): 1, ('unfav', 0.0): 2, ('benadryl', 0.0): 1, ('offlin', 0.0): 1, ('arent', 0.0): 1, ('msg', 0.0): 1, ('yg', 0.0): 1, ('gg', 0.0): 3, ('sxrew', 0.0): 1, ('dissappear', 0.0): 1, ('swap', 0.0): 1, ('bleed', 0.0): 1, ('ishal', 0.0): 1, ('mi', 0.0): 2, ('thaank', 0.0): 1, ('jhezz', 0.0): 1, ('sneak', 0.0): 3, ('soft', 0.0): 1, ('defenc', 0.0): 1, ('defens', 0.0): 1, ('nrltigersroost', 0.0): 1, ('indiana', 0.0): 2, ('hibb', 0.0): 1, ('biblethump', 0.0): 1, ('rlyyi', 0.0): 1, ('septum', 0.0): 1, ('pierc', 0.0): 2, ('goood', 0.0): 1, ('hiya', 0.0): 1, ('fire', 0.0): 1, ('venom', 0.0): 1, ('carriag', 0.0): 1, ('pink', 0.0): 1, ('fur-trim', 0.0): 1, ('stetson', 0.0): 1, ('error', 0.0): 4, ('59', 0.0): 1, ('xue', 0.0): 1, ('midori', 0.0): 1, ('sakit', 0.0): 2, ('mateo', 0.0): 1, ('hawk', 0.0): 2, ('bartend', 0.0): 1, ('surf', 0.0): 1, ('despair', 0.0): 1, ('insta', 0.0): 1, ('promo', 0.0): 1, ('iwantin', 0.0): 1, ('___', 0.0): 2, ('fault', 0.0): 3, ('goodluck', 0.0): 1, ('pocket', 0.0): 1, ('help@veryhq.co.uk', 0.0): 1, ('benedictervent', 0.0): 1, ('content', 0.0): 1, ('221b', 0.0): 1, ('popcorn', 0.0): 3, ('joyc', 0.0): 1, ('ooop', 0.0): 1, ('spotifi', 0.0): 1, ('paalam', 0.0): 1, ('sazbal', 0.0): 1, ('incid', 0.0): 1, ('aaahh', 0.0): 1, ('gooo', 0.0): 1, (\"stomach'\", 0.0): 1, ('growl', 0.0): 1, ('beard', 0.0): 1, ('nooop', 0.0): 1, ('🎉', 0.0): 3, ('ding', 0.0): 3, ('hundr', 0.0): 1, ('meg', 0.0): 1, (\"verity'\", 0.0): 1, ('rupert', 0.0): 1, ('amin', 0.0): 1, ('studi', 0.0): 2, ('pleaaas', 0.0): 1, ('👆🏻', 0.0): 2, ('woaah', 0.0): 1, ('solvo', 0.0): 1, ('twin', 0.0): 2, (\"friday'\", 0.0): 1, ('lego', 0.0): 1, ('barefoot', 0.0): 1, ('twelvyy', 0.0): 1, ('boaz', 0.0): 1, ('myhil', 0.0): 1, ('takeov', 0.0): 1, ('wba', 0.0): 1, (\"taeyeon'\", 0.0): 1, ('derp', 0.0): 1, ('pd', 0.0): 1, ('zoom', 0.0): 2, (\"sunny'\", 0.0): 1, ('besst', 0.0): 1, ('plagu', 0.0): 1, ('pit', 0.0): 1, ('rich', 0.0): 1, ('sight', 0.0): 1, ('frail', 0.0): 1, ('lotteri', 0.0): 1, ('ride', 0.0): 2, ('twurkin', 0.0): 1, ('razzist', 0.0): 1, ('tumblr', 0.0): 1, ('shek', 0.0): 1, ('609', 0.0): 1, ('mugshot', 0.0): 1, ('attend', 0.0): 3, ('plsss', 0.0): 4, ('taissa', 0.0): 1, ('farmiga', 0.0): 1, ('robert', 0.0): 1, ('qualiti', 0.0): 1, ('daniel', 0.0): 1, ('latest', 0.0): 3, ('softwar', 0.0): 1, ('restor', 0.0): 2, ('momo', 0.0): 2, ('pharma', 0.0): 1, ('immov', 0.0): 1, ('messi', 0.0): 1, ('ansh', 0.0): 1, ('f1', 0.0): 1, ('billion', 0.0): 1, ('rand', 0.0): 1, ('bein', 0.0): 1, ('tla', 0.0): 1, ('tweng', 0.0): 1, ('gene', 0.0): 1, ('up.com', 0.0): 1, ('counti', 0.0): 2, ('cooler', 0.0): 1, ('minhyuk', 0.0): 1, ('gold', 0.0): 2, ('1900', 0.0): 1, ('😪', 0.0): 3, ('yu', 0.0): 1, ('hz', 0.0): 2, ('selena', 0.0): 2, ('emta', 0.0): 1, ('hatigii', 0.0): 1, ('b2aa', 0.0): 1, ('yayyy', 0.0): 1, ('anesthesia', 0.0): 1, ('penrith', 0.0): 1, ('emu', 0.0): 1, ('plain', 0.0): 1, ('staff', 0.0): 3, ('untouch', 0.0): 1, ('brienn', 0.0): 1, ('lsh', 0.0): 1, ('gunna', 0.0): 1, ('former', 0.0): 1, ('darn', 0.0): 1, ('allah', 0.0): 4, ('pakistan', 0.0): 2, ('juudiciari', 0.0): 1, (\"horton'\", 0.0): 1, ('dunkin', 0.0): 1, ('socialis', 0.0): 1, ('cara', 0.0): 1, (\"delevingne'\", 0.0): 1, ('fear', 0.0): 1, ('drug', 0.0): 1, ('lace', 0.0): 1, ('fank', 0.0): 1, ('takfaham', 0.0): 1, ('ufff', 0.0): 1, ('sr', 0.0): 2, ('dard', 0.0): 1, ('katekyn', 0.0): 1, ('ehh', 0.0): 1, ('yeahhh', 0.0): 2, ('hacharatt', 0.0): 1, ('niwll', 0.0): 1, ('defin', 0.0): 1, ('wit', 0.0): 2, ('goa', 0.0): 1, ('lini', 0.0): 1, ('kasi', 0.0): 3, ('rhd', 0.0): 1, ('1st', 0.0): 3, ('wae', 0.0): 1, ('subsid', 0.0): 1, ('20th', 0.0): 1, ('anniversari', 0.0): 1, ('youngja', 0.0): 1, ('harumph', 0.0): 1, ('soggi', 0.0): 1, ('weed', 0.0): 1, ('ireland', 0.0): 3, ('sakura', 0.0): 1, ('flavour', 0.0): 1, ('chokki', 0.0): 1, ('🌸', 0.0): 1, ('unavail', 0.0): 2, ('richard', 0.0): 2, ('laptop', 0.0): 2, ('satya', 0.0): 1, ('aditya', 0.0): 1, ('🍜', 0.0): 3, ('vibrat', 0.0): 1, ('an', 0.0): 2, ('cu', 0.0): 1, ('dhaka', 0.0): 1, ('jam', 0.0): 1, ('shall', 0.0): 2, ('cornetto', 0.0): 3, ('noseble', 0.0): 1, ('nintendo', 0.0): 3, ('wew', 0.0): 1, ('ramo', 0.0): 1, ('ground', 0.0): 2, ('shawn', 0.0): 1, ('mend', 0.0): 1, ('l', 0.0): 2, ('dinghi', 0.0): 1, ('skye', 0.0): 1, ('store', 0.0): 3, ('descript', 0.0): 2, ('colleagu', 0.0): 2, ('gagal', 0.0): 2, ('txt', 0.0): 1, ('sim', 0.0): 1, ('nooot', 0.0): 1, ('notch', 0.0): 1, ('tht', 0.0): 2, ('starv', 0.0): 4, ('\\U000fe196', 0.0): 1, ('pyjama', 0.0): 1, ('swifti', 0.0): 1, ('sorna', 0.0): 1, ('lurgi', 0.0): 1, ('jim', 0.0): 2, ('6gb', 0.0): 1, ('fenestoscop', 0.0): 1, ('etienn', 0.0): 1, ('bandana', 0.0): 3, ('bigger', 0.0): 2, ('vagina', 0.0): 1, ('suriya', 0.0): 1, ('dangl', 0.0): 1, ('mjhe', 0.0): 2, ('aaj', 0.0): 1, ('tak', 0.0): 3, ('kisi', 0.0): 1, ('kiya', 0.0): 1, ('eyesight', 0.0): 1, ('25x30', 0.0): 1, ('aftenoon', 0.0): 1, ('booor', 0.0): 1, ('uuu', 0.0): 1, ('boyfriend', 0.0): 8, ('freebiefriday', 0.0): 1, ('garag', 0.0): 1, ('michael', 0.0): 1, ('obvious', 0.0): 1, ('denim', 0.0): 1, ('somebodi', 0.0): 1, ('ce', 0.0): 1, ('gw', 0.0): 1, ('anatomi', 0.0): 1, ('no1', 0.0): 1, (\"morisette'\", 0.0): 1, ('flash', 0.0): 1, ('non-trial', 0.0): 1, ('sayhernam', 0.0): 1, ('lootcrat', 0.0): 1, ('item', 0.0): 1, ('inca', 0.0): 1, ('trail', 0.0): 1, ('sandboard', 0.0): 1, ('derbi', 0.0): 1, ('coffe', 0.0): 1, ('unabl', 0.0): 3, ('signatur', 0.0): 1, ('dish', 0.0): 1, ('unfamiliar', 0.0): 1, ('kitchen', 0.0): 3, ('coldest', 0.0): 1, (\"old'\", 0.0): 1, ('14518344', 0.0): 1, ('61', 0.0): 1, ('thirdwheel', 0.0): 1, ('lovebird', 0.0): 1, ('nth', 0.0): 1, ('imo', 0.0): 1, ('familiar', 0.0): 1, ('@juliettemaughan', 0.0): 1, ('copi', 0.0): 1, ('sensiesha', 0.0): 1, ('eldest', 0.0): 1, ('netbal', 0.0): 1, ('😟', 0.0): 1, ('keedz', 0.0): 1, ('taybigail', 0.0): 1, ('jordan', 0.0): 1, ('tournament', 0.0): 1, ('goin', 0.0): 1, ('ps4', 0.0): 3, ('kink', 0.0): 1, ('charger', 0.0): 1, ('streak', 0.0): 1, ('scorch', 0.0): 1, ('srski', 0.0): 1, ('tdc', 0.0): 1, ('egypt', 0.0): 1, ('in-sensit', 0.0): 1, ('cooper', 0.0): 3, ('invit', 0.0): 1, ('donna', 0.0): 1, ('thurston', 0.0): 1, ('collin', 0.0): 1, ('quietli', 0.0): 2, ('kennel', 0.0): 1, ('911', 0.0): 1, ('pluckersss', 0.0): 1, ('gion', 0.0): 1, ('886', 0.0): 1, ('nsfw', 0.0): 1, ('kidschoiceaward', 0.0): 1, ('ming', 0.0): 1, ('pbr', 0.0): 1, ('shoutout', 0.0): 1, ('periscop', 0.0): 1, ('ut', 0.0): 1, ('shawti', 0.0): 1, ('naw', 0.0): 4, (\"sterling'\", 0.0): 1, ('9muse', 0.0): 1, ('hrryok', 0.0): 2, ('asap', 0.0): 2, ('wnt', 0.0): 1, ('9:30', 0.0): 1, ('9:48', 0.0): 1, ('9/11', 0.0): 1, ('bueno', 0.0): 1, ('receptionist', 0.0): 1, ('ella', 0.0): 2, ('goe', 0.0): 4, ('ketchup', 0.0): 1, ('tasteless', 0.0): 1, ('deantd', 0.0): 1, ('justgotkanekifi', 0.0): 1, ('notgonnabeactivefor', 0.0): 1, ('2weeksdontmissittoomuch', 0.0): 1, ('2013', 0.0): 1, ('disney', 0.0): 2, ('vlog', 0.0): 1, ('swim', 0.0): 1, ('turtl', 0.0): 2, ('cnn', 0.0): 2, ('straplin', 0.0): 1, ('theatr', 0.0): 1, ('guncontrol', 0.0): 1, ('stung', 0.0): 2, ('tweak', 0.0): 1, (\"thát'\", 0.0): 1, ('powerpoint', 0.0): 1, ('present', 0.0): 5, ('diner', 0.0): 1, ('no-no', 0.0): 1, ('hind', 0.0): 1, ('circuit', 0.0): 1, ('secondari', 0.0): 1, ('sodder', 0.0): 1, ('perhap', 0.0): 2, ('mobitel', 0.0): 1, ('colin', 0.0): 1, ('playstat', 0.0): 2, ('charg', 0.0): 4, ('exp', 0.0): 1, ('misspelt', 0.0): 1, ('wan', 0.0): 1, ('hyungwon', 0.0): 2, ('alarm', 0.0): 1, ('needicecreamnow', 0.0): 1, ('shake', 0.0): 1, ('repeatedli', 0.0): 1, ('nu-uh', 0.0): 1, ('jace', 0.0): 1, ('mostest', 0.0): 1, ('vip', 0.0): 1, ('urgh', 0.0): 1, ('consol', 0.0): 1, (\"grigson'\", 0.0): 1, ('carrot', 0.0): 1, ('&gt;:-(', 0.0): 4, ('sunburn', 0.0): 1, ('ughh', 0.0): 2, ('enabl', 0.0): 1, ('otter', 0.0): 1, ('protect', 0.0): 1, ('argh', 0.0): 1, ('pon', 0.0): 1, ('otl', 0.0): 2, ('sleepov', 0.0): 2, ('jess', 0.0): 2, ('bebe', 0.0): 1, ('fabina', 0.0): 1, (\"barrista'\", 0.0): 1, ('plant', 0.0): 3, ('pup', 0.0): 2, ('brolli', 0.0): 1, ('mere', 0.0): 2, ('nhi', 0.0): 1, ('dey', 0.0): 2, ('serv', 0.0): 1, ('kepo', 0.0): 1, ('bitin', 0.0): 1, ('pretzel', 0.0): 1, ('bb17', 0.0): 1, ('bblf', 0.0): 1, ('fuckin', 0.0): 1, ('vanilla', 0.0): 1, ('latt', 0.0): 1, ('skulker', 0.0): 1, ('thread', 0.0): 1, ('hungrrryyi', 0.0): 1, ('icloud', 0.0): 1, ('ipod', 0.0): 3, ('hallyu', 0.0): 1, ('buuut', 0.0): 1, ('über', 0.0): 1, ('oki', 0.0): 2, ('8p', 0.0): 1, ('champagn', 0.0): 1, ('harlo', 0.0): 1, ('torrentialrain', 0.0): 1, ('lloyd', 0.0): 1, ('asshol', 0.0): 1, ('clearli', 0.0): 2, ('knowww', 0.0): 2, ('runni', 0.0): 1, ('sehun', 0.0): 1, ('sweater', 0.0): 1, ('intoler', 0.0): 2, ('xenophob', 0.0): 1, ('wtfff', 0.0): 1, ('tone', 0.0): 1, ('wasnt', 0.0): 1, ('1pm', 0.0): 2, ('fantasi', 0.0): 1, ('newer', 0.0): 1, ('pish', 0.0): 1, ('comparison', 0.0): 1, ('remast', 0.0): 1, ('fe14', 0.0): 1, ('icon', 0.0): 2, ('strawberri', 0.0): 1, ('loos', 0.0): 1, ('kapatidkongpogi', 0.0): 1, ('steph', 0.0): 1, ('mel', 0.0): 1, ('longest', 0.0): 1, ('carmen', 0.0): 1, ('login', 0.0): 1, ('respons', 0.0): 3, ('00128835', 0.0): 1, ('wingstop', 0.0): 1, ('budg', 0.0): 1, ('fuq', 0.0): 1, ('ilhoon', 0.0): 1, ('ganteng', 0.0): 1, ('simpl', 0.0): 1, ('getthescoop', 0.0): 1, ('hearess', 0.0): 1, ('677', 0.0): 1, ('txt_shot', 0.0): 1, ('standbi', 0.0): 1, ('inatal', 0.0): 1, ('zenmat', 0.0): 1, ('namecheck', 0.0): 1, ('whistl', 0.0): 1, ('junmyeon', 0.0): 1, ('ddi', 0.0): 1, ('arini', 0.0): 1, ('je', 0.0): 1, ('bright', 0.0): 2, ('igbo', 0.0): 1, ('blamehoney', 0.0): 1, ('whhr', 0.0): 1, ('juan', 0.0): 1, ('snuggl', 0.0): 1, ('internship', 0.0): 1, ('usag', 0.0): 1, ('warn', 0.0): 1, ('vertigo', 0.0): 1, ('panic', 0.0): 1, ('attack', 0.0): 4, ('dual', 0.0): 1, ('carriageway', 0.0): 1, ('aragalang', 0.0): 1, ('08', 0.0): 1, ('tam', 0.0): 1, ('bose', 0.0): 1, ('theo', 0.0): 1, ('anymoree', 0.0): 1, ('rubbish', 0.0): 1, ('cactu', 0.0): 1, ('sorrri', 0.0): 1, ('bowel', 0.0): 1, ('nasti', 0.0): 2, ('tumour', 0.0): 1, ('faster', 0.0): 1, ('puffi', 0.0): 1, ('eyelid', 0.0): 1, ('musica', 0.0): 1, ('dota', 0.0): 1, ('4am', 0.0): 1, ('campsit', 0.0): 1, ('miah', 0.0): 1, ('hahay', 0.0): 1, ('churro', 0.0): 1, ('montana', 0.0): 2, ('reign', 0.0): 1, ('exampl', 0.0): 1, ('inflat', 0.0): 1, ('sic', 0.0): 1, ('reset', 0.0): 1, ('entlerbountli', 0.0): 1, ('tinder', 0.0): 3, ('dirtykik', 0.0): 2, ('sexcam', 0.0): 3, ('spray', 0.0): 1, ('industri', 0.0): 1, ('swollen', 0.0): 1, ('distanc', 0.0): 2, ('jojo', 0.0): 1, ('postcod', 0.0): 1, ('kafi', 0.0): 1, ('din', 0.0): 1, ('mene', 0.0): 1, ('aj', 0.0): 1, ('koi', 0.0): 1, ('rewert', 0.0): 1, ('bunta', 0.0): 1, ('warnaaa', 0.0): 1, ('tortur', 0.0): 2, ('field', 0.0): 1, ('wall', 0.0): 2, ('iran', 0.0): 1, ('irand', 0.0): 1, ('us-iran', 0.0): 1, ('nuclear', 0.0): 1, (\"mit'\", 0.0): 1, ('expert', 0.0): 1, ('sever', 0.0): 3, ('li', 0.0): 1, ('s2e12', 0.0): 1, ('rumpi', 0.0): 1, ('gallon', 0.0): 1, ('ryan', 0.0): 1, ('secret', 0.0): 2, ('dandia', 0.0): 1, ('rbi', 0.0): 1, ('cage', 0.0): 2, ('parrot', 0.0): 1, ('1li', 0.0): 1, ('commiss', 0.0): 1, ('cag', 0.0): 1, ('stripe', 0.0): 2, ('gujarat', 0.0): 1, ('tear', 0.0): 3, ('ily.melani', 0.0): 1, ('unlik', 0.0): 2, ('talent', 0.0): 2, ('deepxcap', 0.0): 1, ('doin', 0.0): 3, ('5:08', 0.0): 1, ('thesi', 0.0): 11, ('belieb', 0.0): 2, ('gtg', 0.0): 1, ('compet', 0.0): 1, ('vv', 0.0): 1, ('respect', 0.0): 5, ('opt-out', 0.0): 1, ('vam', 0.0): 1, ('spece', 0.0): 1, ('ell', 0.0): 1, ('articl', 0.0): 1, ('sexyameli', 0.0): 1, ('fineandyu', 0.0): 1, ('gd', 0.0): 1, ('flesh', 0.0): 1, ('daft', 0.0): 1, ('imsorri', 0.0): 1, ('aku', 0.0): 1, ('chelsea', 0.0): 2, ('koe', 0.0): 1, ('emyu', 0.0): 1, ('confetti', 0.0): 1, ('bf', 0.0): 2, ('sini', 0.0): 1, ('dipoppo', 0.0): 1, ('hop', 0.0): 2, ('bestweekend', 0.0): 1, ('okay-ish', 0.0): 1, ('html', 0.0): 1, ('geneva', 0.0): 1, ('patml', 0.0): 1, ('482', 0.0): 1, ('orgasm', 0.0): 3, ('abouti', 0.0): 1, ('797', 0.0): 1, ('reaalli', 0.0): 1, ('aldub', 0.0): 1, ('nila', 0.0): 1, ('smart', 0.0): 1, ('meter', 0.0): 1, ('display', 0.0): 1, ('unansw', 0.0): 1, ('bri', 0.0): 1, ('magcon', 0.0): 1, ('sinuend', 0.0): 1, ('kak', 0.0): 1, ('laper', 0.0): 2, ('rage', 0.0): 1, ('loser', 0.0): 1, ('brendon', 0.0): 1, (\"urie'\", 0.0): 1, ('sumer', 0.0): 1, ('repackag', 0.0): 1, (\":'d\", 0.0): 1, ('matthew', 0.0): 1, ('yongb', 0.0): 1, ('sued', 0.0): 1, ('suprem', 0.0): 1, ('warm-up', 0.0): 1, ('arriv', 0.0): 4, ('brill', 0.0): 1, ('120', 0.0): 1, ('rub', 0.0): 1, ('belli', 0.0): 1, ('jannatul', 0.0): 1, ('ferdou', 0.0): 1, ('ekta', 0.0): 1, ('kharap', 0.0): 1, ('manush', 0.0): 1, ('mart', 0.0): 2, ('gua', 0.0): 1, ('can', 0.0): 1, (\"khloe'\", 0.0): 1, ('nhe', 0.0): 1, ('yar', 0.0): 1, ('minkyuk', 0.0): 1, ('hol', 0.0): 1, ('isol', 0.0): 1, ('hk', 0.0): 1, ('sensor', 0.0): 1, ('broker', 0.0): 1, ('wna', 0.0): 1, ('flaviana', 0.0): 1, ('chickmt', 0.0): 1, ('123', 0.0): 1, ('letsfootbal', 0.0): 2, ('atk', 0.0): 2, ('greymind', 0.0): 2, ('43', 0.0): 2, ('gayl', 0.0): 2, ('cricket', 0.0): 3, ('2-3', 0.0): 2, ('mood-dump', 0.0): 1, ('livestream', 0.0): 1, ('gotten', 0.0): 1, ('felton', 0.0): 1, ('veriti', 0.0): 1, (\"standen'\", 0.0): 1, ('shortli', 0.0): 1, ('😆', 0.0): 2, ('takoyaki', 0.0): 1, ('piti', 0.0): 1, ('aisyah', 0.0): 1, ('ffvi', 0.0): 1, ('youtu.be/2_gpctsojkw', 0.0): 1, ('donutsss', 0.0): 1, ('50p', 0.0): 1, ('grate', 0.0): 1, ('spars', 0.0): 1, ('dd', 0.0): 1, ('lagi', 0.0): 1, ('rider', 0.0): 1, ('pride', 0.0): 1, ('hueee', 0.0): 1, ('password', 0.0): 1, ('thingi', 0.0): 1, ('georg', 0.0): 1, ('afraid', 0.0): 2, ('chew', 0.0): 2, ('toy', 0.0): 1, ('stella', 0.0): 1, ('threw', 0.0): 2, ('theaccidentalcoupl', 0.0): 1, ('smooth', 0.0): 1, ('handov', 0.0): 1, ('spick', 0.0): 1, ('bebii', 0.0): 1, ('happenend', 0.0): 1, ('dr', 0.0): 1, ('balm', 0.0): 1, ('hmph', 0.0): 1, ('bubba', 0.0): 2, ('floor', 0.0): 3, ('georgi', 0.0): 1, ('oi', 0.0): 1, ('bengali', 0.0): 1, ('masterchef', 0.0): 1, ('whatchya', 0.0): 1, ('petrol', 0.0): 1, ('diesel', 0.0): 1, ('wardrob', 0.0): 1, ('awe', 0.0): 1, ('cock', 0.0): 1, ('nyquil', 0.0): 1, ('poootek', 0.0): 1, ('1,500', 0.0): 1, ('bobbl', 0.0): 1, ('leak', 0.0): 1, ('thermo', 0.0): 1, ('classic', 0.0): 1, ('ti5', 0.0): 1, ('12th', 0.0): 1, ('skate', 0.0): 1, ('tae', 0.0): 1, ('kita', 0.0): 4, ('ia', 0.0): 1, ('pkwalasawa', 0.0): 1, ('india', 0.0): 1, ('corrupt', 0.0): 2, ('access', 0.0): 2, ('anything.sur', 0.0): 1, ('info', 0.0): 6, ('octob', 0.0): 1, ('mubank', 0.0): 2, ('ene', 0.0): 2, ('3k', 0.0): 1, ('zehr', 0.0): 1, ('khani', 0.0): 1, ('groceri', 0.0): 1, ('hubba', 0.0): 1, ('bubbl', 0.0): 1, ('gum', 0.0): 2, ('closet', 0.0): 1, ('jhalak', 0.0): 1, ('. ..', 0.0): 2, ('bakwa', 0.0): 1, ('. ...', 0.0): 1, ('seehiah', 0.0): 1, ('goy', 0.0): 1, ('nacho', 0.0): 1, ('braid', 0.0): 2, ('initi', 0.0): 1, ('ruth', 0.0): 1, ('boong', 0.0): 1, ('recommend', 0.0): 3, ('gta', 0.0): 1, ('cwnt', 0.0): 1, ('trivia', 0.0): 1, ('belat', 0.0): 1, ('rohingya', 0.0): 1, ('muslim', 0.0): 2, ('indict', 0.0): 1, ('traffick', 0.0): 1, ('thailand', 0.0): 1, ('asia', 0.0): 1, ('rumbl', 0.0): 1, ('kumbl', 0.0): 1, ('scold', 0.0): 1, ('phrase', 0.0): 1, ('includ', 0.0): 1, ('tag', 0.0): 2, ('melt', 0.0): 1, ('tfw', 0.0): 1, ('jest', 0.0): 1, ('offend', 0.0): 2, ('sleepingwithsiren', 0.0): 1, ('17th', 0.0): 1, ('bringmethehorizon', 0.0): 1, ('18th', 0.0): 2, ('carva', 0.0): 1, ('regularli', 0.0): 2, ('sympathi', 0.0): 1, ('revamp', 0.0): 1, ('headphon', 0.0): 1, ('cunt', 0.0): 1, ('wacha', 0.0): 1, ('niend', 0.0): 1, ('bravo', 0.0): 1, ('2hr', 0.0): 1, ('13m', 0.0): 1, ('kk', 0.0): 2, ('calibraksaep', 0.0): 2, ('darlin', 0.0): 1, ('stun', 0.0): 1, (\"doedn't\", 0.0): 1, ('meaning', 0.0): 1, ('horrif', 0.0): 2, ('scoup', 0.0): 2, ('paypal', 0.0): 3, ('sweedi', 0.0): 1, ('nam', 0.0): 1, (\"sacconejoly'\", 0.0): 1, ('bethesda', 0.0): 1, ('fallout', 0.0): 1, ('minecon', 0.0): 1, ('perfect', 0.0): 2, ('katee', 0.0): 1, ('iloveyouu', 0.0): 1, ('linux', 0.0): 1, ('nawww', 0.0): 1, ('chikka', 0.0): 1, ('ug', 0.0): 1, ('rata', 0.0): 1, ('soonest', 0.0): 1, ('mwamwa', 0.0): 1, ('faggot', 0.0): 1, ('doubt', 0.0): 2, ('fyi', 0.0): 1, ('profil', 0.0): 1, ('nicest', 0.0): 1, ('mehendi', 0.0): 1, ('dash', 0.0): 1, ('bookmark', 0.0): 1, ('whay', 0.0): 1, ('shaa', 0.0): 1, ('prami', 0.0): 1, ('😚', 0.0): 4, ('ngee', 0.0): 1, ('ann', 0.0): 1, ('crikey', 0.0): 2, ('snit', 0.0): 1, ('nathanielhinanakit', 0.0): 1, ('naya', 0.0): 1, ('spinni', 0.0): 1, ('wheel', 0.0): 2, ('albeit', 0.0): 1, ('athlet', 0.0): 1, ('gfriend', 0.0): 2, ('yung', 0.0): 2, ('fugli', 0.0): 1, ('💞', 0.0): 4, ('jongda', 0.0): 1, ('hardli', 0.0): 2, ('tlist', 0.0): 1, ('budget', 0.0): 1, ('pabebegirl', 0.0): 1, ('pabeb', 0.0): 2, ('alter', 0.0): 1, ('sandra', 0.0): 2, ('bland', 0.0): 2, ('storifi', 0.0): 1, ('abbi', 0.0): 2, ('mtvhottest', 0.0): 1, ('gaga', 0.0): 1, ('rib', 0.0): 1, ('😵', 0.0): 1, ('hulkamania', 0.0): 1, ('unlov', 0.0): 1, ('lazi', 0.0): 3, ('ihhh', 0.0): 1, ('stackar', 0.0): 1, ('basil', 0.0): 1, ('remedi', 0.0): 1, ('ov', 0.0): 2, ('raiz', 0.0): 1, ('nvr', 0.0): 1, ('gv', 0.0): 1, ('up.wt', 0.0): 1, ('wt', 0.0): 1, ('imran', 0.0): 2, ('achiev', 0.0): 1, ('thr', 0.0): 1, ('soln', 0.0): 1, (\"sister'\", 0.0): 1, ('hong', 0.0): 1, ('kong', 0.0): 1, ('31st', 0.0): 1, ('pipe', 0.0): 1, ('sept', 0.0): 2, ('lawn', 0.0): 1, (\"cupid'\", 0.0): 1, ('torn', 0.0): 1, ('retain', 0.0): 1, ('clown', 0.0): 2, ('lipstick', 0.0): 1, ('haiss', 0.0): 1, ('todayi', 0.0): 1, ('thoo', 0.0): 1, ('everday', 0.0): 1, ('hangout', 0.0): 2, ('steven', 0.0): 2, ('william', 0.0): 1, ('umboh', 0.0): 1, ('goodafternoon', 0.0): 1, ('jadin', 0.0): 1, ('thiz', 0.0): 1, ('iz', 0.0): 1, ('emeg', 0.0): 1, ('kennat', 0.0): 1, ('reunit', 0.0): 1, ('abi', 0.0): 1, ('arctic', 0.0): 1, ('chicsirif', 0.0): 1, ('structur', 0.0): 1, ('cumbia', 0.0): 1, ('correct', 0.0): 1, ('badlif', 0.0): 1, ('4-5', 0.0): 2, ('kaslkdja', 0.0): 1, ('3wk', 0.0): 1, ('flower', 0.0): 1, ('feverfew', 0.0): 1, ('weddingflow', 0.0): 1, ('diyflow', 0.0): 1, ('fitn', 0.0): 1, ('worth', 0.0): 4, ('wolverin', 0.0): 1, ('khan', 0.0): 1, ('innoc', 0.0): 1, ('🙏🏻', 0.0): 1, ('🎂', 0.0): 2, ('memem', 0.0): 2, ('krystoria', 0.0): 1, ('snob', 0.0): 1, ('zumba', 0.0): 1, ('greekcrisi', 0.0): 1, ('remain', 0.0): 1, ('dutch', 0.0): 1, ('legibl', 0.0): 2, ('isra', 0.0): 1, ('passport', 0.0): 1, ('froze', 0.0): 1, ('theori', 0.0): 1, ('23rd', 0.0): 1, ('24th', 0.0): 1, ('stomachach', 0.0): 1, ('slice', 0.0): 1, ('ཀ', 0.0): 1, ('again', 0.0): 1, ('otani', 0.0): 1, ('3-0', 0.0): 1, ('3rd', 0.0): 3, ('bottom', 0.0): 2, ('niaaa', 0.0): 1, ('2/4', 0.0): 1, ('scheme', 0.0): 2, ('fckin', 0.0): 1, ('hii', 0.0): 1, ('vin', 0.0): 1, ('plss', 0.0): 1, ('rpli', 0.0): 1, ('rat', 0.0): 3, ('bollywood', 0.0): 1, ('mac', 0.0): 1, ('backup', 0.0): 2, ('lune', 0.0): 1, ('robinhood', 0.0): 1, ('robinhoodi', 0.0): 1, ('🚙', 0.0): 1, ('💚', 0.0): 1, ('docopenhagen', 0.0): 1, ('setter', 0.0): 1, ('swipe', 0.0): 1, ('bbygurl', 0.0): 1, ('neil', 0.0): 1, ('caribbean', 0.0): 1, ('6yr', 0.0): 1, ('jabongatpumaurbanstamped', 0.0): 2, ('takraw', 0.0): 1, ('fersure', 0.0): 1, ('angi', 0.0): 1, ('sheriff', 0.0): 1, ('aaag', 0.0): 1, (\"i'mo\", 0.0): 1, ('sulk', 0.0): 1, ('selfish', 0.0): 1, ('trick', 0.0): 2, ('nonc', 0.0): 1, ('pad', 0.0): 1, ('bison', 0.0): 1, ('motiv', 0.0): 2, (\"q'don\", 0.0): 1, ('cheat', 0.0): 2, ('stomp', 0.0): 1, ('aaaaaaaaah', 0.0): 1, ('kany', 0.0): 1, ('mama', 0.0): 1, ('jdjdjdjd', 0.0): 1, (\"jimin'\", 0.0): 1, ('fancaf', 0.0): 1, ('waffl', 0.0): 1, ('87.7', 0.0): 1, ('2fm', 0.0): 1, ('himseek', 0.0): 1, ('kissm', 0.0): 1, ('akua', 0.0): 1, ('glo', 0.0): 1, ('cori', 0.0): 1, ('monteith', 0.0): 1, ('often', 0.0): 1, ('hashbrown', 0.0): 1, ('💘', 0.0): 2, ('pg', 0.0): 1, ('msc', 0.0): 1, ('hierro', 0.0): 1, ('shirleycam', 0.0): 1, ('phonesex', 0.0): 2, ('pal', 0.0): 1, ('111', 0.0): 1, ('gilet', 0.0): 1, ('cheek', 0.0): 1, ('squishi', 0.0): 1, ('lahhh', 0.0): 1, ('eon', 0.0): 1, ('sunris', 0.0): 1, ('beeti', 0.0): 1, ('697', 0.0): 1, ('kikkomansabor', 0.0): 1, ('getaway', 0.0): 1, ('crimin', 0.0): 1, ('amiibo', 0.0): 1, ('batman', 0.0): 1, ('habe', 0.0): 1, ('siannn', 0.0): 1, ('march', 0.0): 1, ('2017', 0.0): 1, ('chuckin', 0.0): 1, ('ampsha', 0.0): 1, ('nia', 0.0): 1, ('strap', 0.0): 1, ('dz9055', 0.0): 1, ('entlead', 0.0): 1, ('590', 0.0): 1, ('twice', 0.0): 5, ('07:02', 0.0): 1, ('ifsc', 0.0): 1, ('mayor', 0.0): 1, ('biodivers', 0.0): 1, ('taxonom', 0.0): 1, ('collabor', 0.0): 1, ('speci', 0.0): 1, ('discoveri', 0.0): 1, ('collar', 0.0): 1, ('3:03', 0.0): 1, ('belt', 0.0): 1, ('smith', 0.0): 2, ('eyelin', 0.0): 1, ('therefor', 0.0): 1, ('netherland', 0.0): 1, ('el', 0.0): 1, ('jeb', 0.0): 1, ('blacklivesmatt', 0.0): 1, ('slogan', 0.0): 1, ('msnbc', 0.0): 1, ('jebbush', 0.0): 1, ('famish', 0.0): 1, ('marino', 0.0): 1, ('qualifi', 0.0): 2, ('suzi', 0.0): 1, ('skirt', 0.0): 1, ('tama', 0.0): 1, ('warrior', 0.0): 2, ('wound', 0.0): 1, ('iraq', 0.0): 1, ('be', 0.0): 2, ('camara', 0.0): 1, ('coveral', 0.0): 1, ('happili', 0.0): 1, ('sneezi', 0.0): 1, ('rogerwatch', 0.0): 1, ('stalker', 0.0): 1, ('velvet', 0.0): 1, ('tradit', 0.0): 1, (\"people'\", 0.0): 1, ('beheaviour', 0.0): 1, (\"robert'\", 0.0): 1, ('.\\n.', 0.0): 2, ('aaron', 0.0): 1, ('jelous', 0.0): 1, ('mtg', 0.0): 1, ('thoughtseiz', 0.0): 1, ('playabl', 0.0): 1, ('oldi', 0.0): 1, ('goodi', 0.0): 1, ('mcg', 0.0): 1, ('inspirit', 0.0): 1, ('shine', 0.0): 1, ('ise', 0.0): 1, ('assum', 0.0): 2, ('waist', 0.0): 2, ('guin', 0.0): 1, ('venu', 0.0): 1, ('evil', 0.0): 1, ('pepper', 0.0): 1, ('thessidew', 0.0): 1, ('877', 0.0): 1, ('genesi', 0.0): 1, ('mexico', 0.0): 2, ('novemb', 0.0): 1, ('mash', 0.0): 1, ('whattsap', 0.0): 1, ('inuyasha', 0.0): 2, ('outfwith', 0.0): 1, ('myungsoo', 0.0): 1, ('organis', 0.0): 1, ('satisfi', 0.0): 1, ('wah', 0.0): 1, ('challo', 0.0): 1, ('pliss', 0.0): 1, ('juliana', 0.0): 1, ('enrol', 0.0): 1, ('darlen', 0.0): 1, ('emoji', 0.0): 2, ('brisban', 0.0): 1, ('merlin', 0.0): 1, ('nawwwe', 0.0): 1, ('hyperbulli', 0.0): 1, ('tong', 0.0): 1, ('nga', 0.0): 1, ('seatmat', 0.0): 1, ('rajud', 0.0): 1, ('barkada', 0.0): 1, ('ore', 0.0): 1, ('kayla', 0.0): 1, ('ericavan', 0.0): 1, ('jong', 0.0): 1, ('dongwoo', 0.0): 1, ('photocard', 0.0): 1, ('wh', 0.0): 1, ('dw', 0.0): 1, ('tumor', 0.0): 1, ('vivian', 0.0): 1, ('mmsmalubhangsakit', 0.0): 1, ('jillcruz', 0.0): 2, ('lgbt', 0.0): 3, ('qt', 0.0): 1, ('19th', 0.0): 1, ('toss', 0.0): 1, ('co-work', 0.0): 1, ('mia', 0.0): 1, ('push', 0.0): 4, ('dare', 0.0): 2, ('unsettl', 0.0): 1, ('gh', 0.0): 1, ('18c', 0.0): 1, ('rlli', 0.0): 2, ('hamster', 0.0): 2, ('sheeran', 0.0): 2, ('preform', 0.0): 2, ('monash', 0.0): 1, ('hitmark', 0.0): 1, ('glitch', 0.0): 1, ('safaa', 0.0): 1, (\"selena'\", 0.0): 1, ('galat', 0.0): 1, ('tum', 0.0): 1, ('ab', 0.0): 5, ('non', 0.0): 1, ('lrka', 0.0): 1, ('bna', 0.0): 1, ('kia', 0.0): 1, ('bhook', 0.0): 1, ('jai', 0.0): 1, ('social', 0.0): 2, ('afterschool', 0.0): 1, ('bilal', 0.0): 1, ('ashraf', 0.0): 1, ('icu', 0.0): 1, ('thanksss', 0.0): 1, ('annnd', 0.0): 1, ('winchest', 0.0): 1, ('{:', 0.0): 1, ('grepe', 0.0): 1, ('grepein', 0.0): 1, ('panem', 0.0): 1, ('lover', 0.0): 1, ('sulli', 0.0): 1, ('cpm', 0.0): 1, ('condemn', 0.0): 1, ('✔', 0.0): 1, ('occur', 0.0): 1, ('unagi', 0.0): 1, ('7elw', 0.0): 1, ('mesh', 0.0): 1, ('beyt', 0.0): 1, ('3a2ad', 0.0): 1, ('fluent', 0.0): 1, ('varsiti', 0.0): 1, ('sengenza', 0.0): 1, ('context', 0.0): 1, ('movnat', 0.0): 1, ('yield', 0.0): 1, ('nbhero', 0.0): 1, (\"it'd\", 0.0): 1, ('background', 0.0): 1, ('agov', 0.0): 1, ('brasileirao', 0.0): 2, ('abus', 0.0): 1, ('unpar', 0.0): 1, ('bianca', 0.0): 1, ('bun', 0.0): 1, ('dislik', 0.0): 1, ('burdensom', 0.0): 1, ('clear', 0.0): 2, ('amelia', 0.0): 1, ('melon', 0.0): 2, ('useless', 0.0): 1, ('soccer', 0.0): 2, ('interview', 0.0): 2, ('thursday', 0.0): 1, ('nevermind', 0.0): 1, ('jeon', 0.0): 1, ('claw', 0.0): 1, ('thigh', 0.0): 2, ('traction', 0.0): 1, ('damnit', 0.0): 1, ('pri', 0.0): 1, ('pv', 0.0): 2, ('reliv', 0.0): 1, ('nyc', 0.0): 2, ('klm', 0.0): 1, ('11am', 0.0): 1, (\"mcd'\", 0.0): 1, ('hung', 0.0): 1, ('bam', 0.0): 1, ('seventh', 0.0): 1, ('splendour', 0.0): 1, ('swedish', 0.0): 1, ('metal', 0.0): 1, ('häirførc', 0.0): 1, ('givecodpieceach', 0.0): 1, ('alic', 0.0): 3, ('stile', 0.0): 1, ('explain', 0.0): 3, ('ili', 0.0): 1, ('pragu', 0.0): 1, ('sadi', 0.0): 1, ('charact', 0.0): 1, ('915', 0.0): 1, ('hayee', 0.0): 2, ('patwari', 0.0): 1, ('mam', 0.0): 1, (\"ik'\", 0.0): 1, ('vision', 0.0): 2, ('ga', 0.0): 1, ('awhhh', 0.0): 1, ('nalang', 0.0): 1, ('hehe', 0.0): 1, ('albanian', 0.0): 1, ('curs', 0.0): 2, ('tava', 0.0): 1, ('chara', 0.0): 1, ('teteh', 0.0): 1, ('verri', 0.0): 1, ('shatter', 0.0): 2, ('sb', 0.0): 1, ('nawe', 0.0): 1, ('bulldog', 0.0): 1, ('macho', 0.0): 1, ('puriti', 0.0): 1, ('kwento', 0.0): 1, ('nakakapikon', 0.0): 1, ('nagbabasa', 0.0): 1, ('blog', 0.0): 2, ('cancer', 0.0): 1, (':-\\\\', 0.0): 1, ('jonatha', 0.0): 4, ('beti', 0.0): 4, ('sogok', 0.0): 1, ('premium', 0.0): 2, ('instrument', 0.0): 1, ('howev', 0.0): 1, ('dastardli', 0.0): 1, ('swine', 0.0): 1, ('envelop', 0.0): 1, ('pipol', 0.0): 1, ('tad', 0.0): 1, ('wiper', 0.0): 2, ('supposedli', 0.0): 1, ('kernel', 0.0): 1, ('intel', 0.0): 1, ('mega', 0.0): 1, ('bent', 0.0): 1, ('socket', 0.0): 1, ('pcgame', 0.0): 1, ('pcupgrad', 0.0): 1, ('brainwash', 0.0): 2, ('smosh', 0.0): 1, ('plawnew', 0.0): 1, ('837', 0.0): 1, ('aswel', 0.0): 1, ('litter', 0.0): 1, ('mensch', 0.0): 1, ('sepanx', 0.0): 1, ('pci', 0.0): 1, ('caerphilli', 0.0): 1, ('omw', 0.0): 1, ('😍', 0.0): 1, ('hahdhdhshh', 0.0): 1, ('growinguppoor', 0.0): 1, ('🇺🇸', 0.0): 2, (\"bangtan'\", 0.0): 1, ('taimoor', 0.0): 1, ('meray', 0.0): 1, ('dost', 0.0): 1, ('tya', 0.0): 1, ('refollow', 0.0): 1, ('dumb', 0.0): 2, ('butt', 0.0): 1, ('pissbabi', 0.0): 1, ('plank', 0.0): 1, ('inconsist', 0.0): 1, ('moor', 0.0): 1, ('bin', 0.0): 1, ('osx', 0.0): 1, ('chrome', 0.0): 1, ('voiceov', 0.0): 1, ('devo', 0.0): 1, ('hulkhogan', 0.0): 1, ('unpleas', 0.0): 1, ('daaamn', 0.0): 1, ('dada', 0.0): 1, ('fulli', 0.0): 1, ('spike', 0.0): 1, (\"panic'\", 0.0): 1, ('22nd', 0.0): 1, ('south', 0.0): 2, ('africa', 0.0): 2, ('190', 0.0): 2, ('lizardz', 0.0): 1, ('deepli', 0.0): 1, ('emerg', 0.0): 1, ('engin', 0.0): 1, ('dormtel', 0.0): 1, ('scho', 0.0): 1, ('siya', 0.0): 1, ('onee', 0.0): 1, ('carri', 0.0): 1, ('7pm', 0.0): 1, ('feta', 0.0): 1, ('blaaaz', 0.0): 1, ('nausea', 0.0): 1, ('awar', 0.0): 1, ('top-up', 0.0): 1, ('sharknado', 0.0): 1, ('erni', 0.0): 1, ('ezoo', 0.0): 1, ('lilybutl', 0.0): 1, ('seduc', 0.0): 2, ('powai', 0.0): 1, ('neighbor', 0.0): 1, ('delhi', 0.0): 1, ('unsaf', 0.0): 1, ('halo', 0.0): 1, ('fred', 0.0): 1, ('gaon', 0.0): 1, ('infnt', 0.0): 1, ('elig', 0.0): 1, ('acub', 0.0): 1, (\"why'd\", 0.0): 1, ('bullshit', 0.0): 2, ('hanaaa', 0.0): 1, ('jn', 0.0): 1, ('tau', 0.0): 1, ('basta', 0.0): 1, ('sext', 0.0): 1, ('addm', 0.0): 1, ('hotmusicdeloco', 0.0): 2, ('dhi', 0.0): 1, ('👉', 0.0): 1, ('8ball', 0.0): 1, ('fakmarey', 0.0): 1, ('doo', 0.0): 2, ('six', 0.0): 3, ('flag', 0.0): 1, ('fulltim', 0.0): 1, ('awkward', 0.0): 1, ('beet', 0.0): 1, ('juic', 0.0): 1, ('dci', 0.0): 1, ('granddad', 0.0): 1, ('minion', 0.0): 3, ('bucket', 0.0): 1, ('kapan', 0.0): 1, ('udah', 0.0): 1, ('dihapu', 0.0): 1, ('hilang', 0.0): 1, ('dari', 0.0): 1, ('muka', 0.0): 1, ('bumi', 0.0): 1, ('narrow', 0.0): 1, ('gona', 0.0): 2, ('chello', 0.0): 1, ('gate', 0.0): 1, ('guard', 0.0): 1, ('crepe', 0.0): 1, ('forsaken', 0.0): 1, ('kanin', 0.0): 1, ('hypixel', 0.0): 1, ('grrr', 0.0): 1, ('thestruggleisr', 0.0): 1, ('geek', 0.0): 1, ('gamer', 0.0): 2, ('afterbirth', 0.0): 1, (\"apink'\", 0.0): 1, ('overperhatian', 0.0): 1, ('son', 0.0): 1, ('pox', 0.0): 1, ('ahm', 0.0): 1, ('karli', 0.0): 1, ('kloss', 0.0): 1, ('goofi', 0.0): 1, ('pcd', 0.0): 1, ('antagonis', 0.0): 1, ('writer', 0.0): 1, ('nudg', 0.0): 1, ('delv', 0.0): 1, ('grandad', 0.0): 1, (\"gray'\", 0.0): 1, ('followk', 0.0): 1, ('suggest', 0.0): 2, ('pace', 0.0): 1, ('maker', 0.0): 1, ('molli', 0.0): 1, ('higher', 0.0): 1, ('ceremoni', 0.0): 1, ('christin', 0.0): 1, ('moodi', 0.0): 1, ('throwback', 0.0): 1, ('fav', 0.0): 3, ('barb', 0.0): 1, ('creasi', 0.0): 1, ('deputi', 0.0): 1, ('tast', 0.0): 1, (\"banana'\", 0.0): 1, ('saludo', 0.0): 1, ('dissapoint', 0.0): 1, ('😫', 0.0): 1, ('&lt;--', 0.0): 1, (\"bae'\", 0.0): 1, ('pimpl', 0.0): 2, ('amount', 0.0): 2, ('tdi', 0.0): 1, ('pamela', 0.0): 1, ('mini', 0.0): 1, ('mast', 0.0): 1, ('intermitt', 0.0): 1, ('servic', 0.0): 3, ('janniecam', 0.0): 1, ('musicbiz', 0.0): 1, ('braxton', 0.0): 1, ('pro', 0.0): 2, ('urban', 0.0): 1, ('unpreced', 0.0): 1, ('tebow', 0.0): 1, ('okaaay', 0.0): 1, ('sayanggg', 0.0): 1, ('housework', 0.0): 1, ('bust', 0.0): 2, ('disneyland', 0.0): 1, ('thoma', 0.0): 1, ('tommyy', 0.0): 1, ('billi', 0.0): 1, ('kevin', 0.0): 1, ('clifton', 0.0): 1, ('strictli', 0.0): 1, ('nsc', 0.0): 1, ('mat', 0.0): 1, ('0', 0.0): 1, ('awhh', 0.0): 1, ('ram', 0.0): 2, ('voucher', 0.0): 1, ('smadvow', 0.0): 1, ('544', 0.0): 1, ('acdc', 0.0): 1, ('aker', 0.0): 1, ('gmail', 0.0): 1, ('sprevelink', 0.0): 1, ('633', 0.0): 1, ('lana', 0.0): 2, ('loveyoutilltheendcart', 0.0): 1, ('sfv', 0.0): 1, ('6/7', 0.0): 1, ('winner', 0.0): 1, ('20/1', 0.0): 1, ('david', 0.0): 1, ('rosi', 0.0): 1, ('hayoung', 0.0): 1, ('nlb', 0.0): 1, ('@_', 0.0): 1, ('tayo', 0.0): 1, ('forth', 0.0): 1, ('suspect', 0.0): 1, ('mening', 0.0): 1, ('viral', 0.0): 1, ('tonsil', 0.0): 1, ('😷', 0.0): 1, ('😝', 0.0): 1, ('babyy', 0.0): 2, ('cushion', 0.0): 1, ('😿', 0.0): 1, ('💓', 0.0): 2, ('weigh', 0.0): 1, ('keen', 0.0): 1, ('petrofac', 0.0): 1, (';-)', 0.0): 1, ('wig', 0.0): 1, (\"mark'\", 0.0): 1, ('pathet', 0.0): 1, ('burden.say', 0.0): 1, ('itchi', 0.0): 1, ('cheaper', 0.0): 1, ('malaysia', 0.0): 1, ('130', 0.0): 1, ('snapchattimg', 0.0): 1, ('😏', 0.0): 4, ('sin', 0.0): 1, ('lor', 0.0): 1, ('dedic', 0.0): 1, ('worriedli', 0.0): 1, ('stare', 0.0): 1, ('toneadi', 0.0): 1, ('46532', 0.0): 1, ('snapdirti', 0.0): 1, ('sheskindahot', 0.0): 1, ('corps', 0.0): 1, ('taeni', 0.0): 1, ('fyeah', 0.0): 1, ('andromeda', 0.0): 1, ('yunni', 0.0): 1, ('whdjwksja', 0.0): 1, ('ziam', 0.0): 1, ('100k', 0.0): 1, ('spoil', 0.0): 1, ('curtain', 0.0): 1, ('watchabl', 0.0): 1, ('migrin', 0.0): 1, ('gdce', 0.0): 1, ('gamescom', 0.0): 1, (\"do't\", 0.0): 1, ('parcel', 0.0): 1, ('num', 0.0): 1, ('oooouch', 0.0): 1, ('pinki', 0.0): 1, ('👣', 0.0): 1, ('podiatrist', 0.0): 1, ('gusto', 0.0): 1, (\"rodic'\", 0.0): 1, (\"one'\", 0.0): 1, ('adoohh', 0.0): 1, ('b-butt', 0.0): 1, ('tigermilk', 0.0): 1, ('east', 0.0): 1, ('dulwich', 0.0): 1, ('intens', 0.0): 1, ('kagami', 0.0): 1, ('kuroko', 0.0): 1, ('sana', 0.0): 2, ('makita', 0.0): 1, ('spooki', 0.0): 1, ('smol', 0.0): 1, ('bean', 0.0): 1, ('fagan', 0.0): 1, ('meadowhal', 0.0): 1, ('lola', 0.0): 1, ('nadalaw', 0.0): 1, ('labyu', 0.0): 1, ('jot', 0.0): 1, ('ivypowel', 0.0): 1, ('homeslic', 0.0): 1, ('33', 0.0): 2, ('emoticon', 0.0): 2, ('eyebrow', 0.0): 1, ('prettylook', 0.0): 1, ('whitney', 0.0): 1, ('houston', 0.0): 1, ('aur', 0.0): 1, ('shamil', 0.0): 1, ('tonn', 0.0): 1, ('statu', 0.0): 1, ('→', 0.0): 1, ('suddenli', 0.0): 2, ('alli', 0.0): 2, ('wrap', 0.0): 1, ('neck', 0.0): 1, ('heartbroken', 0.0): 1, ('chover', 0.0): 1, ('cebu', 0.0): 1, ('lechon', 0.0): 1, ('kitten', 0.0): 2, ('jannygreen', 0.0): 2, ('suicid', 0.0): 2, ('forgiv', 0.0): 1, ('conno', 0.0): 1, ('brooo', 0.0): 1, ('rout', 0.0): 1, ('lovebox', 0.0): 1, ('prod', 0.0): 1, ('osad', 0.0): 1, ('scam', 0.0): 1, ('itb', 0.0): 1, ('omigod', 0.0): 1, ('ehem', 0.0): 1, ('ala', 0.0): 1, ('yeke', 0.0): 1, ('jumpa', 0.0): 1, ('😋', 0.0): 1, ('ape', 0.0): 1, ('1.2', 0.0): 1, ('map', 0.0): 1, ('namin', 0.0): 1, ('govt', 0.0): 1, ('e-petit', 0.0): 1, ('pretend', 0.0): 1, ('irk', 0.0): 1, ('ruess', 0.0): 1, ('program', 0.0): 1, ('aigoo', 0.0): 1, ('doujin', 0.0): 1, ('killua', 0.0): 1, ('ginggon', 0.0): 1, ('guys.al', 0.0): 1, ('ytd', 0.0): 1, ('pdapaghimok', 0.0): 1, ('flexibl', 0.0): 1, ('sheet', 0.0): 1, ('nanaman', 0.0): 1, ('pinay', 0.0): 1, ('pie', 0.0): 1, ('jadi', 0.0): 1, ('langsung', 0.0): 1, ('flasback', 0.0): 1, ('franc', 0.0): 1, (':|', 0.0): 1, ('lo', 0.0): 1, ('nicknam', 0.0): 1, ('involv', 0.0): 1, ('scrape', 0.0): 1, ('pile', 0.0): 1, ('sare', 0.0): 1, ('bandar', 0.0): 1, ('varg', 0.0): 1, ('hammer', 0.0): 1, ('lolo', 0.0): 1, ('xbsbabnb', 0.0): 1, ('stilll', 0.0): 1, ('apma', 0.0): 2, ('leadership', 0.0): 1, ('wakeupgop', 0.0): 1, ('mv', 0.0): 1, ('bull', 0.0): 1, ('trafficcc', 0.0): 1, ('oscar', 0.0): 1, ('pornographi', 0.0): 1, ('slutsham', 0.0): 1, ('ect', 0.0): 1, ('poland', 0.0): 1, ('faraway', 0.0): 1, ('700', 0.0): 1, ('800', 0.0): 1, ('cgi', 0.0): 1, ('pun', 0.0): 1, (\"x'\", 0.0): 1, ('osaka', 0.0): 1, ('junior', 0.0): 1, ('aytona', 0.0): 1, ('hala', 0.0): 1, ('mathird', 0.0): 1, ('jkjk', 0.0): 1, ('backtrack', 0.0): 1, ('util', 0.0): 1, ('pat', 0.0): 1, ('jay', 0.0): 2, ('broh', 0.0): 1, ('calll', 0.0): 1, ('icaru', 0.0): 1, ('awn', 0.0): 1, ('bach', 0.0): 1, ('court', 0.0): 1, ('landlord', 0.0): 1, (\"mp'\", 0.0): 1, ('dame', 0.0): 1, ('gossip', 0.0): 1, ('purpl', 0.0): 2, ('tie', 0.0): 1, ('ishii', 0.0): 1, ('clara', 0.0): 1, ('yile', 0.0): 1, ('whatev', 0.0): 1, ('stil', 0.0): 1, ('sidharth', 0.0): 1, ('ndabenhl', 0.0): 1, ('doggi', 0.0): 1, ('antag', 0.0): 1, ('41', 0.0): 1, ('thu', 0.0): 1, ('jenner', 0.0): 1, ('troubleshoot', 0.0): 1, (\"convo'\", 0.0): 1, ('dem', 0.0): 1, ('tix', 0.0): 2, ('automat', 0.0): 1, ('redirect', 0.0): 1, ('gigi', 0.0): 1, ('carter', 0.0): 1, ('corn', 0.0): 2, ('chip', 0.0): 2, ('nnnooo', 0.0): 1, ('cz', 0.0): 1, ('gorilla', 0.0): 1, ('hbm', 0.0): 1, ('humid', 0.0): 1, ('admir', 0.0): 1, ('consist', 0.0): 1, ('jason', 0.0): 1, (\"shackell'\", 0.0): 1, ('podcast', 0.0): 1, ('envi', 0.0): 1, ('twer', 0.0): 1, ('782', 0.0): 1, ('hahaahahahaha', 0.0): 1, ('sm1', 0.0): 1, ('mutil', 0.0): 1, ('robot', 0.0): 1, ('destroy', 0.0): 1, ('freakin', 0.0): 1, ('haestarr', 0.0): 1, ('😀', 0.0): 3, ('audio', 0.0): 1, ('snippet', 0.0): 1, ('brotherhood', 0.0): 1, ('mefd', 0.0): 1, ('diana', 0.0): 1, ('master', 0.0): 1, ('led', 0.0): 1, ('award', 0.0): 1, ('meowkd', 0.0): 1, ('complic', 0.0): 1, (\"c'mon\", 0.0): 1, (\"swimmer'\", 0.0): 1, ('leh', 0.0): 1, ('corner', 0.0): 1, ('didnot', 0.0): 1, ('usanel', 0.0): 2, ('nathan', 0.0): 1, ('micha', 0.0): 1, ('fave', 0.0): 2, ('creep', 0.0): 1, ('throughout', 0.0): 1, ('whose', 0.0): 1, ('ave', 0.0): 1, ('tripl', 0.0): 1, ('lectur', 0.0): 1, ('2-5', 0.0): 1, ('jaw', 0.0): 1, ('quarter', 0.0): 1, ('soni', 0.0): 1, ('followmeaaron', 0.0): 1, ('tzelumxoxo', 0.0): 1, ('drank', 0.0): 1, ('mew', 0.0): 1, ('indic', 0.0): 1, ('ouliv', 0.0): 1, ('70748', 0.0): 1, ('viernesderolenahot', 0.0): 1, ('longmorn', 0.0): 1, ('tobermori', 0.0): 1, ('32', 0.0): 1, ('tail', 0.0): 1, ('recuerda', 0.0): 1, ('tanto', 0.0): 1, ('bath', 0.0): 1, ('muna', 0.0): 1, ('await', 0.0): 1, ('urslef', 0.0): 1, ('lime', 0.0): 1, ('truckload', 0.0): 1, ('favour', 0.0): 2, ('spectat', 0.0): 1, ('sail', 0.0): 1, (\"w'end\", 0.0): 1, ('bbc', 0.0): 1, ('‘', 0.0): 1, ('foil', 0.0): 1, ('ac45', 0.0): 1, ('catamaran', 0.0): 1, ('peli', 0.0): 1, ('829', 0.0): 1, ('sextaatequemfimseguesdvcomvalentino', 0.0): 1, ('befor', 0.0): 1, ('valu', 0.0): 1, ('cinnamon', 0.0): 1, ('mtap', 0.0): 1, ('peng', 0.0): 1, ('frozen', 0.0): 1, ('bagu', 0.0): 1, ('emang', 0.0): 1, ('engg', 0.0): 1, ('cmc', 0.0): 1, ('mage', 0.0): 1, ('statement', 0.0): 1, ('moodsw', 0.0): 1, ('termin', 0.0): 1, ('men', 0.0): 1, ('peep', 0.0): 1, ('multipl', 0.0): 1, ('mef', 0.0): 1, ('rebound', 0.0): 1, ('pooor', 0.0): 1, ('2am', 0.0): 1, ('perpetu', 0.0): 1, ('bitchfac', 0.0): 1, ('clever', 0.0): 1, ('iceland', 0.0): 1, ('zayn_come_back_we_miss_y', 0.0): 1, ('pmsl', 0.0): 1, ('mianh', 0.0): 1, ('milkeu', 0.0): 1, ('lrt', 0.0): 1, ('bambam', 0.0): 1, ('soda', 0.0): 1, ('payback', 0.0): 1, ('87000', 0.0): 1, ('jobe', 0.0): 1, ('muchi', 0.0): 1, ('🎈', 0.0): 1, ('bathroom', 0.0): 1, ('lagg', 0.0): 1, ('banget', 0.0): 1, ('novel', 0.0): 1, (\"there'd\", 0.0): 1, ('invis', 0.0): 1, ('scuttl', 0.0): 1, ('worm', 0.0): 1, ('bauuukkk', 0.0): 1, ('jessica', 0.0): 1, ('5:15', 0.0): 1, ('argument', 0.0): 1, ('couldnt', 0.0): 2, ('yepp', 0.0): 1, ('😺', 0.0): 1, ('💒', 0.0): 1, ('💎', 0.0): 1, ('feelin', 0.0): 1, ('biscuit', 0.0): 1, ('slather', 0.0): 1, ('jsut', 0.0): 1, ('belov', 0.0): 1, ('grandmoth', 0.0): 1, ('princess', 0.0): 2, ('babee', 0.0): 1, ('demn', 0.0): 1, ('hotaisndonwyvauwjoqhsjsnaihsuswtf', 0.0): 1, ('sia', 0.0): 1, ('niram', 0.0): 1, ('geng', 0.0): 1, ('fikri', 0.0): 1, ('tirtagangga', 0.0): 1, ('char', 0.0): 1, ('font', 0.0): 2, ('riprishikeshwari', 0.0): 1, ('creamist', 0.0): 1, ('challeng', 0.0): 1, ('substitut', 0.0): 1, ('skin', 0.0): 1, ('cplt', 0.0): 1, ('cp', 0.0): 1, ('hannah', 0.0): 1, ('💙', 0.0): 1, ('💪', 0.0): 1, ('opu', 0.0): 1, ('inner', 0.0): 1, ('pleasur', 0.0): 1, ('bbq', 0.0): 1, ('lolliv', 0.0): 1, ('split', 0.0): 3, ('collat', 0.0): 2, ('spilt', 0.0): 2, ('quitkarwaoyaaro', 0.0): 1, ('deacti̇v', 0.0): 1, ('2.5', 0.0): 1, ('g2a', 0.0): 1, ('sherep', 0.0): 1, ('nemen', 0.0): 1, ('behey', 0.0): 1, ('motherfuck', 0.0): 1, ('tattoo', 0.0): 1, ('reec', 0.0): 1, ('vm', 0.0): 1, ('deth', 0.0): 2, ('lest', 0.0): 1, ('gp', 0.0): 1, ('departur', 0.0): 1, ('wipe', 0.0): 1, ('yuck', 0.0): 1, ('ystrday', 0.0): 1, ('seolhyun', 0.0): 1, ('drama', 0.0): 1, ('spici', 0.0): 1, ('owl', 0.0): 1, ('mumbai', 0.0): 1, (\"pj'\", 0.0): 1, ('wallpap', 0.0): 1, ('cba', 0.0): 1, ('hotter', 0.0): 1, ('rec', 0.0): 1, ('gotdamn', 0.0): 1, ('baaack', 0.0): 1, ('honest', 0.0): 1, ('srw', 0.0): 1, ('mobag', 0.0): 1, ('dunno', 0.0): 1, ('stroke', 0.0): 1, ('gnr', 0.0): 1, ('backstag', 0.0): 1, ('slash', 0.0): 1, ('prolli', 0.0): 1, ('bunni', 0.0): 1, ('sooner', 0.0): 1, ('analyst', 0.0): 1, ('expedia', 0.0): 1, ('bellevu', 0.0): 1, ('prison', 0.0): 1, ('alcohol', 0.0): 1, ('huhuh', 0.0): 1, ('heartburn', 0.0): 1, ('awalmu', 0.0): 1, ('njareeem', 0.0): 1, ('maggi', 0.0): 1, ('psycho', 0.0): 1, ('wahhh', 0.0): 1, ('abudhabi', 0.0): 1, ('hiby', 0.0): 1, ('shareyoursumm', 0.0): 1, ('b8', 0.0): 1, ('must.b', 0.0): 1, ('dairi', 0.0): 1, ('produxt', 0.0): 1, ('lactos', 0.0): 2, ('midland', 0.0): 1, ('knacker', 0.0): 1, ('footag', 0.0): 1, ('lifeless', 0.0): 1, ('shell', 0.0): 1, ('44', 0.0): 1, ('7782', 0.0): 1, ('pengen', 0.0): 1, ('girlll', 0.0): 1, ('tsunami', 0.0): 1, ('indi', 0.0): 1, ('nick', 0.0): 1, ('tirad', 0.0): 1, ('stoop', 0.0): 1, ('lower', 0.0): 1, ('role', 0.0): 1, ('thunder', 0.0): 1, ('paradis', 0.0): 1, ('habit', 0.0): 1, ('facad', 0.0): 1, ('democraci', 0.0): 1, ('brat', 0.0): 1, ('tb', 0.0): 1, (\"o'\", 0.0): 1, ('bade', 0.0): 1, ('fursat', 0.0): 1, ('usey', 0.0): 2, ('banaya', 0.0): 1, ('uppar', 0.0): 1, ('waal', 0.0): 1, ('ney', 0.0): 1, ('afso', 0.0): 1, ('hums', 0.0): 1, ('dur', 0.0): 1, ('wo', 0.0): 1, (\"who'd\", 0.0): 1, ('naruhina', 0.0): 1, ('namee', 0.0): 1, ('haiqal', 0.0): 1, ('360hr', 0.0): 1, ('picc', 0.0): 1, ('instor', 0.0): 1, ('pre-vot', 0.0): 1, ('5th', 0.0): 1, ('usernam', 0.0): 1, ('minho', 0.0): 1, ('durian', 0.0): 1, ('strudel', 0.0): 1, ('tsk', 0.0): 1, ('marin', 0.0): 1, ('kailan', 0.0): 1, ('separ', 0.0): 1, ('payday', 0.0): 1, ('payhour', 0.0): 1, ('immedi', 0.0): 1, ('natur', 0.0): 1, ('pre-ord', 0.0): 1, ('fwm', 0.0): 1, ('guppi', 0.0): 1, ('poorkid', 0.0): 1, ('lack', 0.0): 1, ('misunderstood', 0.0): 1, ('cuddli', 0.0): 1, ('scratch', 0.0): 1, ('thumb', 0.0): 1, ('compens', 0.0): 1, ('kirkiri', 0.0): 1, ('phase', 0.0): 1, ('wonho', 0.0): 1, ('visual', 0.0): 1, (\"='(\", 0.0): 1, ('mission', 0.0): 1, ('pap', 0.0): 1, ('danzel', 0.0): 1, ('craft', 0.0): 1, ('devil', 0.0): 1, ('phil', 0.0): 1, ('sheff', 0.0): 1, ('york', 0.0): 1, ('visa', 0.0): 1, ('gim', 0.0): 1, ('bench', 0.0): 1, ('harm', 0.0): 1, ('yolo', 0.0): 1, ('bloat', 0.0): 1, ('olli', 0.0): 1, ('alterni', 0.0): 1, ('earth', 0.0): 1, ('influenc', 0.0): 1, ('overal', 0.0): 1, ('continent', 0.0): 1, ('🔫', 0.0): 1, ('tank', 0.0): 1, ('thirsti', 0.0): 1, ('konami', 0.0): 1, ('polici', 0.0): 1, ('ranti', 0.0): 1, ('atm', 0.0): 1, ('pervers', 0.0): 1, ('bylfnnz', 0.0): 1, ('ban', 0.0): 1, ('failsatlif', 0.0): 1, ('press', 0.0): 1, ('duper', 0.0): 1, ('waaah', 0.0): 1, ('jaebum', 0.0): 1, ('ahmad', 0.0): 1, ('maslan', 0.0): 1, ('hull', 0.0): 1, ('misser', 0.0): 1}\n\n\nUnfortunately, this does not help much to understand the data. It would be better to visualize this output to gain better insights.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Lab on Building and Visualizing word frequencies"
    ]
  },
  {
    "objectID": "posts/c1lab2/index.html#table-of-word-counts",
    "href": "posts/c1lab2/index.html#table-of-word-counts",
    "title": "Building and Visualizing word frequencies",
    "section": "Table of word counts",
    "text": "Table of word counts\nWe will select a set of words that we would like to visualize. It is better to store this temporary information in a table that is very easy to use later.\n\n# select some words to appear in the report. we will assume that each word is unique (i.e. no duplicates)\nkeys = ['happi', 'merri', 'nice', 'good', 'bad', 'sad', 'mad', 'best', 'pretti',\n        '❤', ':)', ':(', '😒', '😬', '😄', '😍', '♛',\n        'song', 'idea', 'power', 'play', 'magnific']\n\n# list representing our table of word counts.\n\n# each element consist of a sublist with this pattern: [&lt;word&gt;, &lt;positive_count&gt;, &lt;negative_count&gt;]\ndata = []\n\n# loop through our selected words\nfor word in keys:\n    \n    # initialize positive and negative counts\n    pos = 0\n    neg = 0\n    \n    # retrieve number of positive counts\n    if (word, 1) in freqs:\n        pos = freqs[(word, 1)]\n        \n    # retrieve number of negative counts\n    if (word, 0) in freqs:\n        neg = freqs[(word, 0)]\n        \n    # append the word counts to the table\n    data.append([word, pos, neg])\n    \ndata\n\n[['happi', 211, 25],\n ['merri', 1, 0],\n ['nice', 98, 19],\n ['good', 238, 101],\n ['bad', 18, 73],\n ['sad', 5, 123],\n ['mad', 4, 11],\n ['best', 65, 22],\n ['pretti', 20, 15],\n ['❤', 29, 21],\n [':)', 3568, 2],\n [':(', 1, 4571],\n ['😒', 1, 3],\n ['😬', 0, 2],\n ['😄', 5, 1],\n ['😍', 2, 1],\n ['♛', 0, 210],\n ['song', 22, 27],\n ['idea', 26, 10],\n ['power', 7, 6],\n ['play', 46, 48],\n ['magnific', 2, 0]]\n\n\nWe can then use a scatter plot to inspect this table visually. Instead of plotting the raw counts, we will plot it in the logarithmic scale to take into account the wide discrepancies between the raw counts (e.g. :) has 3568 counts in the positive while only 2 in the negative). The red line marks the boundary between positive and negative areas. Words close to the red line can be classified as neutral.\n\nfig, ax = plt.subplots(figsize = (8, 8))\n\n# convert positive raw counts to logarithmic scale. we add 1 to avoid log(0)\nx = np.log([x[1] + 1 for x in data])  \n\n# do the same for the negative counts\ny = np.log([x[2] + 1 for x in data]) \n\n# Plot a dot for each pair of words\nax.scatter(x, y)  \n\n# assign axis labels\nplt.xlabel(\"Log Positive count\")\nplt.ylabel(\"Log Negative count\")\n\n# Add the word as the label at the same position as you added the points just before\nfor i in range(0, len(data)):\n    ax.annotate(data[i][0], (x[i], y[i]), fontsize=12)\nax.plot([0, 9], [0, 9], color = 'red') # Plot the red line that divides the 2 areas.\nplt.show()\n\nText(0.5, 0, 'Log Positive count')\n\n\nText(0, 0.5, 'Log Negative count')\n\n\nText(5.356586274672012, 3.258096538021482, 'happi')\n\n\nText(0.6931471805599453, 0.0, 'merri')\n\n\nText(4.59511985013459, 2.995732273553991, 'nice')\n\n\nText(5.476463551931511, 4.624972813284271, 'good')\n\n\nText(2.9444389791664403, 4.30406509320417, 'bad')\n\n\nText(1.791759469228055, 4.820281565605037, 'sad')\n\n\nText(1.6094379124341003, 2.4849066497880004, 'mad')\n\n\nText(4.189654742026425, 3.1354942159291497, 'best')\n\n\nText(3.044522437723423, 2.772588722239781, 'pretti')\n\n\nText(3.4011973816621555, 3.091042453358316, '❤')\n\n\nText(8.18004072349016, 1.0986122886681098, ':)')\n\n\nText(0.6931471805599453, 8.427706024914702, ':(')\n\n\nText(0.6931471805599453, 1.3862943611198906, '😒')\n\n\nText(0.0, 1.0986122886681098, '😬')\n\n\nText(1.791759469228055, 0.6931471805599453, '😄')\n\n\nText(1.0986122886681098, 0.6931471805599453, '😍')\n\n\nText(0.0, 5.351858133476067, '♛')\n\n\nText(3.1354942159291497, 3.332204510175204, 'song')\n\n\nText(3.295836866004329, 2.3978952727983707, 'idea')\n\n\nText(2.0794415416798357, 1.9459101490553132, 'power')\n\n\nText(3.8501476017100584, 3.8918202981106265, 'play')\n\n\nText(1.0986122886681098, 0.0, 'magnific')\n\n\n/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 128556 (\\N{GRIMACING FACE}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n\n\n\n\n\n\n\n\n\nThis chart is straightforward to interpret. It shows that emoticons :) and :( are very important for sentiment analysis. Thus, we should not let preprocessing steps get rid of these symbols! Furthermore, what is the meaning of the crown symbol? It seems to be very negative! That’s all for this lab! We’ve seen how to build a word frequency dictionary and this will come in handy when extracting the features of a list of tweets. Next up, we will be reviewing Logistic Regression. Keep it up!",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Lab on Building and Visualizing word frequencies"
    ]
  },
  {
    "objectID": "posts/c2w3/assignment.html",
    "href": "posts/c2w3/assignment.html",
    "title": "Language Models: Auto-Complete",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure 1\nIn this assignment, you will build an auto-complete system. Auto-complete system is something you may see every day - When you google something, you often have suggestions to help you complete your search. - When you are writing an email, you get suggestions telling you possible endings to your sentence.\nBy the end of this assignment, you will develop a prototype of such a system."
  },
  {
    "objectID": "posts/c2w3/assignment.html#outline",
    "href": "posts/c2w3/assignment.html#outline",
    "title": "Language Models: Auto-Complete",
    "section": "Outline",
    "text": "Outline\n\n1 Load and Preprocess Data\n1.1: Load the data\n1.2 Pre-process the data\n\nExercise 01\nExercise 02\nExercise 03\nExercise 04\nExercise 05\nExercise 06\nExercise 07\n\n2 Develop n-gram based language models\n\nExercise 08\nExercise 09\n\n\n3 Perplexity\n\nExercise 10\n\n4 Build an auto-complete system\n\nExercise 11\n\n\nA key building block for an auto-complete system is a language model. A language model assigns the probability to a sequence of words, in a way that more “likely” sequences receive higher scores. For example, &gt;“I have a pen” is expected to have a higher probability than &gt;“I am a pen” since the first one seems to be a more natural sentence in the real world.\nYou can take advantage of this probability calculation to develop an auto-complete system.\nSuppose the user typed &gt;“I eat scrambled” Then you can find a word x such that “I eat scrambled x” receives the highest probability. If x = “eggs”, the sentence would be &gt;“I eat scrambled eggs”\nWhile a variety of language models have been developed, this assignment uses N-grams, a simple but powerful method for language modeling. - N-grams are also used in machine translation and speech recognition.\nHere are the steps of this assignment:\n\nLoad and preprocess data\n\nLoad and tokenize data.\nSplit the sentences into train and test sets.\nReplace words with a low frequency by an unknown marker &lt;unk&gt;.\n\nDevelop N-gram based language models\n\nCompute the count of n-grams from a given data set.\nEstimate the conditional probability of a next word with k-smoothing.\n\nEvaluate the N-gram models by computing the perplexity score.\nUse your own model to suggest an upcoming word given your sentence.\n\n\nimport math\nimport random\nimport numpy as np\nimport pandas as pd\nimport nltk\nnltk.data.path.append('.')\n\n ## Part 1: Load and Preprocess Data\n ### Part 1.1: Load the data You will use twitter data. Load the data and view the first few sentences by running the next cell.\nNotice that data is a long string that contains many many tweets. Observe that there is a line break “” between tweets.\n\nwith open(\"en_US.twitter.txt\", \"r\") as f:\n    data = f.read()\nprint(\"Data type:\", type(data))\nprint(\"Number of letters:\", len(data))\nprint(\"First 300 letters of the data\")\nprint(\"-------\")\ndisplay(data[0:300])\nprint(\"-------\")\n\nprint(\"Last 300 letters of the data\")\nprint(\"-------\")\ndisplay(data[-300:])\nprint(\"-------\")\n\nData type: &lt;class 'str'&gt;\nNumber of letters: 3335477\nFirst 300 letters of the data\n-------\n\n\n\"How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\\nWhen you meet someone special... you'll know. Your heart will beat more rapidly and you'll smile for no reason.\\nthey've decided its more fun if I don't.\\nSo Tired D; Played Lazer Tag & Ran A \"\n\n\n-------\nLast 300 letters of the data\n-------\n\n\n\"ust had one a few weeks back....hopefully we will be back soon! wish you the best yo\\nColombia is with an 'o'...“: We now ship to 4 countries in South America (fist pump). Please welcome Columbia to the Stunner Family”\\n#GutsiestMovesYouCanMake Giving a cat a bath.\\nCoffee after 5 was a TERRIBLE idea.\\n\"\n\n\n-------\n\n\n ### Part 1.2 Pre-process the data\nPreprocess this data with the following steps:\n\nSplit data into sentences using “” as the delimiter.\nSplit each sentence into tokens. Note that in this assignment we use “token” and “words” interchangeably.\nAssign sentences into train or test sets.\nFind tokens that appear at least N times in the training data.\nReplace tokens that appear less than N times by &lt;unk&gt;\n\nNote: we omit validation data in this exercise. - In real applications, we should hold a part of data as a validation set and use it to tune our training. - We skip this process for simplicity.\n ### Exercise 01\nSplit data into sentences.\n\n\nHints\n\n\n\n\nUse str.split\n\n\n\n\n# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n### GRADED_FUNCTION: split_to_sentences ###\ndef split_to_sentences(data):\n    \"\"\"\n    Split data by linebreak \"\\n\"\n    \n    Args:\n        data: str\n    \n    Returns:\n        A list of sentences\n    \"\"\"\n    ### START CODE HERE (Replace instances of 'None' with your code) ###\n    sentences = None\n    ### END CODE HERE ###\n    \n    # Additional clearning (This part is already implemented)\n    # - Remove leading and trailing spaces from each sentence\n    # - Drop sentences if they are empty strings.\n    sentences = [s.strip() for s in sentences]\n    sentences = [s for s in sentences if len(s) &gt; 0]\n    \n    return sentences    \n\n\n# test your code\nx = \"\"\"\nI have a pen.\\nI have an apple. \\nAh\\nApple pen.\\n\n\"\"\"\nprint(x)\n\nsplit_to_sentences(x)\n\n\nI have a pen.\nI have an apple. \nAh\nApple pen.\n\n\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[4], line 7\n      2 x = \"\"\"\n      3 I have a pen.\\nI have an apple. \\nAh\\nApple pen.\\n\n      4 \"\"\"\n      5 print(x)\n----&gt; 7 split_to_sentences(x)\n\nCell In[3], line 20, in split_to_sentences(data)\n     14 sentences = None\n     15 ### END CODE HERE ###\n     16 \n     17 # Additional clearning (This part is already implemented)\n     18 # - Remove leading and trailing spaces from each sentence\n     19 # - Drop sentences if they are empty strings.\n---&gt; 20 sentences = [s.strip() for s in sentences]\n     21 sentences = [s for s in sentences if len(s) &gt; 0]\n     23 return sentences\n\nTypeError: 'NoneType' object is not iterable\n\n\n\nExpected answer:\n['I have a pen.', 'I have an apple.', 'Ah', 'Apple pen.']\n ### Exercise 02 The next step is to tokenize sentences (split a sentence into a list of words). - Convert all tokens into lower case so that words which are capitalized (for example, at the start of a sentence) in the original text are treated the same as the lowercase versions of the words. - Append each tokenized list of words into a list of tokenized sentences.\n\n\nHints\n\n\n\n\nUse str.lower to convert strings to lowercase.\n\n\nPlease use nltk.word_tokenize to split sentences into tokens.\n\n\nIf you used str.split insteaad of nltk.word_tokenize, there are additional edge cases to handle, such as the punctuation (comma, period) that follows a word.\n\n\n\n\n# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n### GRADED_FUNCTION: tokenize_sentences ###\ndef tokenize_sentences(sentences):\n    \"\"\"\n    Tokenize sentences into tokens (words)\n    \n    Args:\n        sentences: List of strings\n    \n    Returns:\n        List of lists of tokens\n    \"\"\"\n    \n    # Initialize the list of lists of tokenized sentences\n    tokenized_sentences = []\n    ### START CODE HERE (Replace instances of 'None' with your code) ###\n    \n    # Go through each sentence\n    for sentence in None:\n        \n        # Convert to lowercase letters\n        sentence = None\n        \n        # Convert into a list of words\n        tokenized = None\n        \n        # append the list of words to the list of lists\n        None\n    \n    ### END CODE HERE ###\n    \n    return tokenized_sentences\n\n\n# test your code\nsentences = [\"Sky is blue.\", \"Leaves are green.\", \"Roses are red.\"]\ntokenize_sentences(sentences)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[6], line 3\n      1 # test your code\n      2 sentences = [\"Sky is blue.\", \"Leaves are green.\", \"Roses are red.\"]\n----&gt; 3 tokenize_sentences(sentences)\n\nCell In[5], line 19, in tokenize_sentences(sentences)\n     15 tokenized_sentences = []\n     16 ### START CODE HERE (Replace instances of 'None' with your code) ###\n     17 \n     18 # Go through each sentence\n---&gt; 19 for sentence in None:\n     20     \n     21     # Convert to lowercase letters\n     22     sentence = None\n     24     # Convert into a list of words\n\nTypeError: 'NoneType' object is not iterable\n\n\n\n\nExpected output\n[['sky', 'is', 'blue', '.'],\n ['leaves', 'are', 'green', '.'],\n ['roses', 'are', 'red', '.']]\n ### Exercise 03\nUse the two functions that you have just implemented to get the tokenized data. - split the data into sentences - tokenize those sentences\n\n# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n### GRADED_FUNCTION: get_tokenized_data ###\ndef get_tokenized_data(data):\n    \"\"\"\n    Make a list of tokenized sentences\n    \n    Args:\n        data: String\n    \n    Returns:\n        List of lists of tokens\n    \"\"\"\n    ### START CODE HERE (Replace instances of 'None' with your code) ###\n    \n    # Get the sentences by splitting up the data\n    sentences = None\n    \n    # Get the list of lists of tokens by tokenizing the sentences\n    tokenized_sentences = None\n    \n    ### END CODE HERE ###\n    \n    return tokenized_sentences\n\n\n# test your function\nx = \"Sky is blue.\\nLeaves are green\\nRoses are red.\"\nget_tokenized_data(x)\n\n\nExpected outcome\n[['sky', 'is', 'blue', '.'],\n ['leaves', 'are', 'green'],\n ['roses', 'are', 'red', '.']]\n\n\n\nSplit into train and test sets\nNow run the cell below to split data into training and test sets.\n\ntokenized_data = get_tokenized_data(data)\nrandom.seed(87)\nrandom.shuffle(tokenized_data)\n\ntrain_size = int(len(tokenized_data) * 0.8)\ntrain_data = tokenized_data[0:train_size]\ntest_data = tokenized_data[train_size:]\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[9], line 3\n      1 tokenized_data = get_tokenized_data(data)\n      2 random.seed(87)\n----&gt; 3 random.shuffle(tokenized_data)\n      5 train_size = int(len(tokenized_data) * 0.8)\n      6 train_data = tokenized_data[0:train_size]\n\nFile /usr/lib/python3.10/random.py:391, in Random.shuffle(self, x, random)\n    389 if random is None:\n    390     randbelow = self._randbelow\n--&gt; 391     for i in reversed(range(1, len(x))):\n    392         # pick an element in x[:i+1] with which to exchange x[i]\n    393         j = randbelow(i + 1)\n    394         x[i], x[j] = x[j], x[i]\n\nTypeError: object of type 'NoneType' has no len()\n\n\n\n\nprint(\"{} data are split into {} train and {} test set\".format(\n    len(tokenized_data), len(train_data), len(test_data)))\n\nprint(\"First training sample:\")\nprint(train_data[0])\n      \nprint(\"First test sample\")\nprint(test_data[0])\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[10], line 2\n      1 print(\"{} data are split into {} train and {} test set\".format(\n----&gt; 2     len(tokenized_data), len(train_data), len(test_data)))\n      4 print(\"First training sample:\")\n      5 print(train_data[0])\n\nTypeError: object of type 'NoneType' has no len()\n\n\n\n\nExpected output\n47961 data are split into 38368 train and 9593 test set\nFirst training sample:\n['i', 'personally', 'would', 'like', 'as', 'our', 'official', 'glove', 'of', 'the', 'team', 'local', 'company', 'and', 'quality', 'production']\nFirst test sample\n['that', 'picture', 'i', 'just', 'seen', 'whoa', 'dere', '!', '!', '&gt;', '&gt;', '&gt;', '&gt;', '&gt;', '&gt;', '&gt;']\n ### Exercise 04\nYou won’t use all the tokens (words) appearing in the data for training. Instead, you will use the more frequently used words.\n- You will focus on the words that appear at least N times in the data. - First count how many times each word appears in the data.\nYou will need a double for-loop, one for sentences and the other for tokens within a sentence.\n\n\nHints\n\n\n\n\nIf you decide to import and use defaultdict, remember to cast the dictionary back to a regular ‘dict’ before returning it.\n\n\n\n\n# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n### GRADED_FUNCTION: count_words ###\ndef count_words(tokenized_sentences):\n    \"\"\"\n    Count the number of word appearence in the tokenized sentences\n    \n    Args:\n        tokenized_sentences: List of lists of strings\n    \n    Returns:\n        dict that maps word (str) to the frequency (int)\n    \"\"\"\n        \n    word_counts = {}\n    ### START CODE HERE (Replace instances of 'None' with your code) ###\n    \n    # Loop through each sentence\n    for sentence in None: # complete this line\n        \n        # Go through each token in the sentence\n        for token in None: # complete this line\n\n            # If the token is not in the dictionary yet, set the count to 1\n            if None: # complete this line\n                word_counts[token] = None\n            \n            # If the token is already in the dictionary, increment the count by 1\n            else:\n                word_counts[token] += None\n\n    ### END CODE HERE ###\n    \n    return word_counts\n\n\n# test your code\ntokenized_sentences = [['sky', 'is', 'blue', '.'],\n                       ['leaves', 'are', 'green', '.'],\n                       ['roses', 'are', 'red', '.']]\ncount_words(tokenized_sentences)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[12], line 5\n      1 # test your code\n      2 tokenized_sentences = [['sky', 'is', 'blue', '.'],\n      3                        ['leaves', 'are', 'green', '.'],\n      4                        ['roses', 'are', 'red', '.']]\n----&gt; 5 count_words(tokenized_sentences)\n\nCell In[11], line 18, in count_words(tokenized_sentences)\n     14 word_counts = {}\n     15 ### START CODE HERE (Replace instances of 'None' with your code) ###\n     16 \n     17 # Loop through each sentence\n---&gt; 18 for sentence in None: # complete this line\n     19     \n     20     # Go through each token in the sentence\n     21     for token in None: # complete this line\n     22 \n     23         # If the token is not in the dictionary yet, set the count to 1\n     24         if None: # complete this line\n\nTypeError: 'NoneType' object is not iterable\n\n\n\n\n\nExpected output\nNote that the order may differ.\n{'sky': 1,\n 'is': 1,\n 'blue': 1,\n '.': 3,\n 'leaves': 1,\n 'are': 2,\n 'green': 1,\n 'roses': 1,\n 'red': 1}\n\n\n\nHandling ‘Out of Vocabulary’ words\nIf your model is performing autocomplete, but encounters a word that it never saw during training, it won’t have an input word to help it determine the next word to suggest. The model will not be able to predict the next word because there are no counts for the current word. - This ‘new’ word is called an ‘unknown word’, or out of vocabulary (OOV) words. - The percentage of unknown words in the test set is called the  OOV  rate.\nTo handle unknown words during prediction, use a special token to represent all unknown words ‘unk’. - Modify the training data so that it has some ‘unknown’ words to train on. - Words to convert into “unknown” words are those that do not occur very frequently in the training set. - Create a list of the most frequent words in the training set, called the  closed vocabulary . - Convert all the other words that are not part of the closed vocabulary to the token ‘unk’.\n ### Exercise 05\nYou will now create a function that takes in a text document and a threshold count_threshold. - Any word whose count is greater than or equal to the threshold count_threshold is kept in the closed vocabulary. - Returns the word closed vocabulary list.\n\n# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n### GRADED_FUNCTION: get_words_with_nplus_frequency ###\ndef get_words_with_nplus_frequency(tokenized_sentences, count_threshold):\n    \"\"\"\n    Find the words that appear N times or more\n    \n    Args:\n        tokenized_sentences: List of lists of sentences\n        count_threshold: minimum number of occurrences for a word to be in the closed vocabulary.\n    \n    Returns:\n        List of words that appear N times or more\n    \"\"\"\n    # Initialize an empty list to contain the words that\n    # appear at least 'minimum_freq' times.\n    closed_vocab = []\n    \n    # Get the word couts of the tokenized sentences\n    # Use the function that you defined earlier to count the words\n    word_counts = count_words(tokenized_sentences)\n    \n    ### START CODE HERE (Replace instances of 'None' with your code) ###\n\n    # for each word and its count\n    for word, cnt in None: # complete this line\n        \n        # check that the word's count\n        # is at least as great as the minimum count\n        if None:\n            \n            # append the word to the list\n            None\n    ### END CODE HERE ###\n    \n    return closed_vocab\n\n\n# test your code\ntokenized_sentences = [['sky', 'is', 'blue', '.'],\n                       ['leaves', 'are', 'green', '.'],\n                       ['roses', 'are', 'red', '.']]\ntmp_closed_vocab = get_words_with_nplus_frequency(tokenized_sentences, count_threshold=2)\nprint(f\"Closed vocabulary:\")\nprint(tmp_closed_vocab)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[14], line 5\n      1 # test your code\n      2 tokenized_sentences = [['sky', 'is', 'blue', '.'],\n      3                        ['leaves', 'are', 'green', '.'],\n      4                        ['roses', 'are', 'red', '.']]\n----&gt; 5 tmp_closed_vocab = get_words_with_nplus_frequency(tokenized_sentences, count_threshold=2)\n      6 print(f\"Closed vocabulary:\")\n      7 print(tmp_closed_vocab)\n\nCell In[13], line 20, in get_words_with_nplus_frequency(tokenized_sentences, count_threshold)\n     16 closed_vocab = []\n     18 # Get the word couts of the tokenized sentences\n     19 # Use the function that you defined earlier to count the words\n---&gt; 20 word_counts = count_words(tokenized_sentences)\n     22 ### START CODE HERE (Replace instances of 'None' with your code) ###\n     23 \n     24 # for each word and its count\n     25 for word, cnt in None: # complete this line\n     26     \n     27     # check that the word's count\n     28     # is at least as great as the minimum count\n\nCell In[11], line 18, in count_words(tokenized_sentences)\n     14 word_counts = {}\n     15 ### START CODE HERE (Replace instances of 'None' with your code) ###\n     16 \n     17 # Loop through each sentence\n---&gt; 18 for sentence in None: # complete this line\n     19     \n     20     # Go through each token in the sentence\n     21     for token in None: # complete this line\n     22 \n     23         # If the token is not in the dictionary yet, set the count to 1\n     24         if None: # complete this line\n\nTypeError: 'NoneType' object is not iterable\n\n\n\n\nExpected output\nClosed vocabulary:\n['.', 'are']\n ### Exercise 06\nThe words that appear count_threshold times or more are in the closed vocabulary. - All other words are regarded as unknown. - Replace words not in the closed vocabulary with the token &lt;unk&gt;.\n\n# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n### GRADED_FUNCTION: replace_oov_words_by_unk ###\ndef replace_oov_words_by_unk(tokenized_sentences, vocabulary, unknown_token=\"&lt;unk&gt;\"):\n    \"\"\"\n    Replace words not in the given vocabulary with '&lt;unk&gt;' token.\n    \n    Args:\n        tokenized_sentences: List of lists of strings\n        vocabulary: List of strings that we will use\n        unknown_token: A string representing unknown (out-of-vocabulary) words\n    \n    Returns:\n        List of lists of strings, with words not in the vocabulary replaced\n    \"\"\"\n    \n    # Place vocabulary into a set for faster search\n    vocabulary = set(vocabulary)\n    \n    # Initialize a list that will hold the sentences\n    # after less frequent words are replaced by the unknown token\n    replaced_tokenized_sentences = []\n    \n    # Go through each sentence\n    for sentence in tokenized_sentences:\n        \n        # Initialize the list that will contain\n        # a single sentence with \"unknown_token\" replacements\n        replaced_sentence = []\n        ### START CODE HERE (Replace instances of 'None' with your code) ###\n\n        # for each token in the sentence\n        for token in None: # complete this line\n            \n            # Check if the token is in the closed vocabulary\n            if token in None: # complete this line\n                # If so, append the word to the replaced_sentence\n                None\n            else:\n                # otherwise, append the unknown token instead\n                None\n        ### END CODE HERE ###\n        \n        # Append the list of tokens to the list of lists\n        replaced_tokenized_sentences.append(replaced_sentence)\n        \n    return replaced_tokenized_sentences\n\n\ntokenized_sentences = [[\"dogs\", \"run\"], [\"cats\", \"sleep\"]]\nvocabulary = [\"dogs\", \"sleep\"]\ntmp_replaced_tokenized_sentences = replace_oov_words_by_unk(tokenized_sentences, vocabulary)\nprint(f\"Original sentence:\")\nprint(tokenized_sentences)\nprint(f\"tokenized_sentences with less frequent words converted to '&lt;unk&gt;':\")\nprint(tmp_replaced_tokenized_sentences)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[16], line 3\n      1 tokenized_sentences = [[\"dogs\", \"run\"], [\"cats\", \"sleep\"]]\n      2 vocabulary = [\"dogs\", \"sleep\"]\n----&gt; 3 tmp_replaced_tokenized_sentences = replace_oov_words_by_unk(tokenized_sentences, vocabulary)\n      4 print(f\"Original sentence:\")\n      5 print(tokenized_sentences)\n\nCell In[15], line 32, in replace_oov_words_by_unk(tokenized_sentences, vocabulary, unknown_token)\n     28 replaced_sentence = []\n     29 ### START CODE HERE (Replace instances of 'None' with your code) ###\n     30 \n     31 # for each token in the sentence\n---&gt; 32 for token in None: # complete this line\n     33     \n     34     # Check if the token is in the closed vocabulary\n     35     if token in None: # complete this line\n     36         # If so, append the word to the replaced_sentence\n     37         None\n\nTypeError: 'NoneType' object is not iterable\n\n\n\n\n\n\nExpected answer\nOriginal sentence:\n[['dogs', 'run'], ['cats', 'sleep']]\ntokenized_sentences with less frequent words converted to '&lt;unk&gt;':\n[['dogs', '&lt;unk&gt;'], ['&lt;unk&gt;', 'sleep']]\n ### Exercise 07\nNow we are ready to process our data by combining the functions that you just implemented.\n\nFind tokens that appear at least count_threshold times in the training data.\nReplace tokens that appear less than count_threshold times by “&lt;unk&gt;” both for training and test data.\n\n\n# UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n### GRADED_FUNCTION: preprocess_data ###\ndef preprocess_data(train_data, test_data, count_threshold):\n    \"\"\"\n    Preprocess data, i.e.,\n        - Find tokens that appear at least N times in the training data.\n        - Replace tokens that appear less than N times by \"&lt;unk&gt;\" both for training and test data.        \n    Args:\n        train_data, test_data: List of lists of strings.\n        count_threshold: Words whose count is less than this are \n                      treated as unknown.\n    \n    Returns:\n        Tuple of\n        - training data with low frequent words replaced by \"&lt;unk&gt;\"\n        - test data with low frequent words replaced by \"&lt;unk&gt;\"\n        - vocabulary of words that appear n times or more in the training data\n    \"\"\"\n    ### START CODE HERE (Replace instances of 'None' with your code) ###\n\n    # Get the closed vocabulary using the train data\n    vocabulary = None\n    \n    # For the train data, replace less common words with \"&lt;unk&gt;\"\n    train_data_replaced = None\n    \n    # For the test data, replace less common words with \"&lt;unk&gt;\"\n    test_data_replaced = None\n    \n    ### END CODE HERE ###\n    return train_data_replaced, test_data_replaced, vocabulary\n\n\n# test your code\ntmp_train = [['sky', 'is', 'blue', '.'],\n     ['leaves', 'are', 'green']]\ntmp_test = [['roses', 'are', 'red', '.']]\n\ntmp_train_repl, tmp_test_repl, tmp_vocab = preprocess_data(tmp_train, \n                                                           tmp_test, \n                                                           count_threshold = 1)\n\nprint(\"tmp_train_repl\")\nprint(tmp_train_repl)\nprint()\nprint(\"tmp_test_repl\")\nprint(tmp_test_repl)\nprint()\nprint(\"tmp_vocab\")\nprint(tmp_vocab)\n\ntmp_train_repl\nNone\n\ntmp_test_repl\nNone\n\ntmp_vocab\nNone\n\n\n\nExpected outcome\ntmp_train_repl\n[['sky', 'is', 'blue', '.'], ['leaves', 'are', 'green']]\n\ntmp_test_repl\n[['&lt;unk&gt;', 'are', '&lt;unk&gt;', '.']]\n\ntmp_vocab\n['sky', 'is', 'blue', '.', 'leaves', 'are', 'green']\n\n\n\nPreprocess the train and test data\nRun the cell below to complete the preprocessing both for training and test sets.\n\nminimum_freq = 2\ntrain_data_processed, test_data_processed, vocabulary = preprocess_data(train_data, \n                                                                        test_data, \n                                                                        minimum_freq)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[19], line 2\n      1 minimum_freq = 2\n----&gt; 2 train_data_processed, test_data_processed, vocabulary = preprocess_data(train_data, \n      3                                                                         test_data, \n      4                                                                         minimum_freq)\n\nNameError: name 'train_data' is not defined\n\n\n\n\nprint(\"First preprocessed training sample:\")\nprint(train_data_processed[0])\nprint()\nprint(\"First preprocessed test sample:\")\nprint(test_data_processed[0])\nprint()\nprint(\"First 10 vocabulary:\")\nprint(vocabulary[0:10])\nprint()\nprint(\"Size of vocabulary:\", len(vocabulary))\n\nFirst preprocessed training sample:\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[20], line 2\n      1 print(\"First preprocessed training sample:\")\n----&gt; 2 print(train_data_processed[0])\n      3 print()\n      4 print(\"First preprocessed test sample:\")\n\nNameError: name 'train_data_processed' is not defined\n\n\n\n\nExpected output\nFirst preprocessed training sample:\n['i', 'personally', 'would', 'like', 'as', 'our', 'official', 'glove', 'of', 'the', 'team', 'local', 'company', 'and', 'quality', 'production']\n\nFirst preprocessed test sample:\n['that', 'picture', 'i', 'just', 'seen', 'whoa', 'dere', '!', '!', '&gt;', '&gt;', '&gt;', '&gt;', '&gt;', '&gt;', '&gt;']\n\nFirst 10 vocabulary:\n['i', 'personally', 'would', 'like', 'as', 'our', 'official', 'glove', 'of', 'the']\n\nSize of vocabulary: 14821\nYou are done with the preprocessing section of the assignment. Objects train_data_processed, test_data_processed, and vocabulary will be used in the rest of the exercises.\n ## Part 2: Develop n-gram based language models\nIn this section, you will develop the n-grams language model. - Assume the probability of the next word depends only on the previous n-gram. - The previous n-gram is the series of the previous ‘n’ words.\nThe conditional probability for the word at position ‘t’ in the sentence, given that the words preceding it are w_{t-1}, w_{t-2} \\cdots w_{t-n} is:\n P(w_t | w_{t-1}\\dots w_{t-n}) \\tag{1}\nYou can estimate this probability by counting the occurrences of these series of words in the training data. - The probability can be estimated as a ratio, where - The numerator is the number of times word ‘t’ appears after words t-1 through t-n appear in the training data. - The denominator is the number of times word t-1 through t-n appears in the training data.\n \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_n)}{C(w_{t-1}\\dots w_{t-n})} \\tag{2} \n\nThe function C(\\cdots) denotes the number of occurence of the given sequence.\n\\hat{P} means the estimation of P.\nNotice that denominator of the equation (2) is the number of occurence of the previous n words, and the numerator is the same sequence followed by the word w_t.\n\nLater, you will modify the equation (2) by adding k-smoothing, which avoids errors when any counts are zero.\nThe equation (2) tells us that to estimate probabilities based on n-grams, you need the counts of n-grams (for denominator) and (n+1)-grams (for numerator).\n ### Exercise 08 Next, you will implement a function that computes the counts of n-grams for an arbitrary number n.\nWhen computing the counts for n-grams, prepare the sentence beforehand by prepending n-1 starting markers “&lt;s&gt;” to indicate the beginning of the sentence.\n- For example, in the bi-gram model (N=2), a sequence with two start tokens “&lt;s&gt;&lt;s&gt;” should predict the first word of a sentence. - So, if the sentence is “I like food”, modify it to be “&lt;s&gt;&lt;s&gt; I like food”. - Also prepare the sentence for counting by appending an end token “&lt;e&gt;” so that the model can predict when to finish a sentence.\nTechnical note: In this implementation, you will store the counts as a dictionary. - The key of each key-value pair in the dictionary is a tuple of n words (and not a list) - The value in the key-value pair is the number of occurrences.\n- The reason for using a tuple as a key instead of a list is because a list in Python is a mutable object (it can be changed after it is first created). A tuple is “immutable”, so it cannot be altered after it is first created. This makes a tuple suitable as a data type for the key in a dictionary.\n\n\nHints\n\n\n\n\nTo prepend or append, you can create lists and concatenate them using the + operator\n\n\nTo create a list of a repeated value, you can follow this syntax: [‘a’] * 3 to get [‘a’,‘a’,‘a’]\n\n\nTo set the range for index ‘i’, think of this example: An n-gram where n=2 (bigram), and the sentence is length N=5 (including two start tokens and one end token). So the index positions are [0,1,2,3,4]. The largest index ‘i’ where a bigram can start is at position i=3, because the word tokens at position 3 and 4 will form the bigram.\n\n\nRemember that the range() function excludes the value that is used for the maximum of the range.  range(3)  produces (0,1,2) but excludes 3.\n\n\n\n\n# UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n### GRADED FUNCTION: count_n_grams ###\ndef count_n_grams(data, n, start_token='&lt;s&gt;', end_token = '&lt;e&gt;'):\n    \"\"\"\n    Count all n-grams in the data\n    \n    Args:\n        data: List of lists of words\n        n: number of words in a sequence\n    \n    Returns:\n        A dictionary that maps a tuple of n-words to its frequency\n    \"\"\"\n    \n    # Initialize dictionary of n-grams and their counts\n    n_grams = {}\n\n    ### START CODE HERE (Replace instances of 'None' with your code) ###\n    \n    # Go through each sentence in the data\n    for sentence in None: # complete this line\n        \n        # prepend start token n times, and  append &lt;e&gt; one time\n        sentence = None\n        \n        # convert list to tuple\n        # So that the sequence of words can be used as\n        # a key in the dictionary\n        sentence = None\n        \n        # Use 'i' to indicate the start of the n-gram\n        # from index 0\n        # to the last index where the end of the n-gram\n        # is within the sentence.\n        \n        for i in range(None): # complete this line\n\n            # Get the n-gram from i to i+n\n            n_gram = None\n\n            # check if the n-gram is in the dictionary\n            if n_gram in None: # complete this line\n            \n                # Increment the count for this n-gram\n                n_grams[n_gram] += None\n            else:\n                # Initialize this n-gram count to 1\n                n_grams[n_gram] = None\n    \n            ### END CODE HERE ###\n    return n_grams\n\n\n# test your code\n# CODE REVIEW COMMENT: Outcome does not match expected outcome\nsentences = [['i', 'like', 'a', 'cat'],\n             ['this', 'dog', 'is', 'like', 'a', 'cat']]\nprint(\"Uni-gram:\")\nprint(count_n_grams(sentences, 1))\nprint(\"Bi-gram:\")\nprint(count_n_grams(sentences, 2))\n\nUni-gram:\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[22], line 6\n      3 sentences = [['i', 'like', 'a', 'cat'],\n      4              ['this', 'dog', 'is', 'like', 'a', 'cat']]\n      5 print(\"Uni-gram:\")\n----&gt; 6 print(count_n_grams(sentences, 1))\n      7 print(\"Bi-gram:\")\n      8 print(count_n_grams(sentences, 2))\n\nCell In[21], line 21, in count_n_grams(data, n, start_token, end_token)\n     16 n_grams = {}\n     18 ### START CODE HERE (Replace instances of 'None' with your code) ###\n     19 \n     20 # Go through each sentence in the data\n---&gt; 21 for sentence in None: # complete this line\n     22     \n     23     # prepend start token n times, and  append &lt;e&gt; one time\n     24     sentence = None\n     26     # convert list to tuple\n     27     # So that the sequence of words can be used as\n     28     # a key in the dictionary\n\nTypeError: 'NoneType' object is not iterable\n\n\n\nExpected outcome:\nUni-gram:\n{('&lt;s&gt;',): 2, ('i',): 1, ('like',): 2, ('a',): 2, ('cat',): 2, ('&lt;e&gt;',): 2, ('this',): 1, ('dog',): 1, ('is',): 1}\nBi-gram:\n{('&lt;s&gt;', '&lt;s&gt;'): 2, ('&lt;s&gt;', 'i'): 1, ('i', 'like'): 1, ('like', 'a'): 2, ('a', 'cat'): 2, ('cat', '&lt;e&gt;'): 2, ('&lt;s&gt;', 'this'): 1, ('this', 'dog'): 1, ('dog', 'is'): 1, ('is', 'like'): 1}\n ### Exercise 09\nNext, estimate the probability of a word given the prior ‘n’ words using the n-gram counts.\n \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_n)}{C(w_{t-1}\\dots w_{t-n})} \\tag{2} \nThis formula doesn’t work when a count of an n-gram is zero.. - Suppose we encounter an n-gram that did not occur in the training data.\n- Then, the equation (2) cannot be evaluated (it becomes zero divided by zero).\nA way to handle zero counts is to add k-smoothing.\n- K-smoothing adds a positive constant k to each numerator and k \\times |V| in the denominator, where |V| is the number of words in the vocabulary.\n \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_n) + k}{C(w_{t-1}\\dots w_{t-n}) + k|V|} \\tag{3} \nFor n-grams that have a zero count, the equation (3) becomes \\frac{1}{|V|}. - This means that any n-gram with zero count has the same probability of \\frac{1}{|V|}.\nDefine a function that computes the probability estimate (3) from n-gram counts and a constant k.\n\nThe function takes in a dictionary ‘n_gram_counts’, where the key is the n-gram and the value is the count of that n-gram.\nThe function also takes another dictionary n_plus1_gram_counts, which you’ll use to find the count for the previous n-gram plus the current word.\n\n\n\nHints\n\n\n\n\nTo define a tuple containing a single value, add a comma after that value. For example: (‘apple’,) is a tuple containing a single string ‘apple’\n\n\nTo concatenate two tuples, use the ‘+’ operator\n\n\n words \n\n\n\n\n# UNQ_C9 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n### GRADED FUNCTION: estimate_probability ###\ndef estimate_probability(word, previous_n_gram, \n                         n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n    \"\"\"\n    Estimate the probabilities of a next word using the n-gram counts with k-smoothing\n    \n    Args:\n        word: next word\n        previous_n_gram: A sequence of words of length n\n        n_gram_counts: Dictionary of counts of n-grams\n        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n        vocabulary_size: number of words in the vocabulary\n        k: positive constant, smoothing parameter\n    \n    Returns:\n        A probability\n    \"\"\"\n    # convert list to tuple to use it as a dictionary key\n    previous_n_gram = tuple(previous_n_gram)\n    \n    ### START CODE HERE (Replace instances of 'None' with your code) ###\n    \n    # Set the denominator\n    # If the previous n-gram exists in the dictionary of n-gram counts,\n    # Get its count.  Otherwise set the count to zero\n    # Use the dictionary that has counts for n-grams\n    previous_n_gram_count = None\n        \n    # Calculate the denominator using the count of the previous n gram\n    # and apply k-smoothing\n    denominator = None\n\n    # Define n plus 1 gram as the previous n-gram plus the current word as a tuple\n    n_plus1_gram = None\n  \n    # Set the count to the count in the dictionary,\n    # otherwise 0 if not in the dictionary\n    # use the dictionary that has counts for the n-gram plus current word\n    n_plus1_gram_count = None\n        \n    # Define the numerator use the count of the n-gram plus current word,\n    # and apply smoothing\n    numerator = None\n\n    # Calculate the probability as the numerator divided by denominator\n    probability = None\n    \n    ### END CODE HERE ###\n    \n    return probability\n\n\n# test your code\nsentences = [['i', 'like', 'a', 'cat'],\n             ['this', 'dog', 'is', 'like', 'a', 'cat']]\nunique_words = list(set(sentences[0] + sentences[1]))\n\nunigram_counts = count_n_grams(sentences, 1)\nbigram_counts = count_n_grams(sentences, 2)\ntmp_prob = estimate_probability(\"cat\", \"a\", unigram_counts, bigram_counts, len(unique_words), k=1)\n\nprint(f\"The estimated probability of word 'cat' given the previous n-gram 'a' is: {tmp_prob:.4f}\")\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[24], line 6\n      2 sentences = [['i', 'like', 'a', 'cat'],\n      3              ['this', 'dog', 'is', 'like', 'a', 'cat']]\n      4 unique_words = list(set(sentences[0] + sentences[1]))\n----&gt; 6 unigram_counts = count_n_grams(sentences, 1)\n      7 bigram_counts = count_n_grams(sentences, 2)\n      8 tmp_prob = estimate_probability(\"cat\", \"a\", unigram_counts, bigram_counts, len(unique_words), k=1)\n\nCell In[21], line 21, in count_n_grams(data, n, start_token, end_token)\n     16 n_grams = {}\n     18 ### START CODE HERE (Replace instances of 'None' with your code) ###\n     19 \n     20 # Go through each sentence in the data\n---&gt; 21 for sentence in None: # complete this line\n     22     \n     23     # prepend start token n times, and  append &lt;e&gt; one time\n     24     sentence = None\n     26     # convert list to tuple\n     27     # So that the sequence of words can be used as\n     28     # a key in the dictionary\n\nTypeError: 'NoneType' object is not iterable\n\n\n\n\n\nExpected output\nThe estimated probability of word 'cat' given the previous n-gram 'a' is: 0.3333\n\n\n\nEstimate probabilities for all words\nThe function defined below loops over all words in vocabulary to calculate probabilities for all possible words. - This function is provided for you.\n\ndef estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0):\n    \"\"\"\n    Estimate the probabilities of next words using the n-gram counts with k-smoothing\n    \n    Args:\n        previous_n_gram: A sequence of words of length n\n        n_gram_counts: Dictionary of counts of (n+1)-grams\n        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n        vocabulary: List of words\n        k: positive constant, smoothing parameter\n    \n    Returns:\n        A dictionary mapping from next words to the probability.\n    \"\"\"\n    \n    # convert list to tuple to use it as a dictionary key\n    previous_n_gram = tuple(previous_n_gram)\n    \n    # add &lt;e&gt; &lt;unk&gt; to the vocabulary\n    # &lt;s&gt; is not needed since it should not appear as the next word\n    vocabulary = vocabulary + [\"&lt;e&gt;\", \"&lt;unk&gt;\"]\n    vocabulary_size = len(vocabulary)\n    \n    probabilities = {}\n    for word in vocabulary:\n        probability = estimate_probability(word, previous_n_gram, \n                                           n_gram_counts, n_plus1_gram_counts, \n                                           vocabulary_size, k=k)\n        probabilities[word] = probability\n\n    return probabilities\n\n\n# test your code\nsentences = [['i', 'like', 'a', 'cat'],\n             ['this', 'dog', 'is', 'like', 'a', 'cat']]\nunique_words = list(set(sentences[0] + sentences[1]))\nunigram_counts = count_n_grams(sentences, 1)\nbigram_counts = count_n_grams(sentences, 2)\nestimate_probabilities(\"a\", unigram_counts, bigram_counts, unique_words, k=1)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[26], line 5\n      2 sentences = [['i', 'like', 'a', 'cat'],\n      3              ['this', 'dog', 'is', 'like', 'a', 'cat']]\n      4 unique_words = list(set(sentences[0] + sentences[1]))\n----&gt; 5 unigram_counts = count_n_grams(sentences, 1)\n      6 bigram_counts = count_n_grams(sentences, 2)\n      7 estimate_probabilities(\"a\", unigram_counts, bigram_counts, unique_words, k=1)\n\nCell In[21], line 21, in count_n_grams(data, n, start_token, end_token)\n     16 n_grams = {}\n     18 ### START CODE HERE (Replace instances of 'None' with your code) ###\n     19 \n     20 # Go through each sentence in the data\n---&gt; 21 for sentence in None: # complete this line\n     22     \n     23     # prepend start token n times, and  append &lt;e&gt; one time\n     24     sentence = None\n     26     # convert list to tuple\n     27     # So that the sequence of words can be used as\n     28     # a key in the dictionary\n\nTypeError: 'NoneType' object is not iterable\n\n\n\n\nExpected output\n{'cat': 0.2727272727272727,\n 'i': 0.09090909090909091,\n 'this': 0.09090909090909091,\n 'a': 0.09090909090909091,\n 'is': 0.09090909090909091,\n 'like': 0.09090909090909091,\n 'dog': 0.09090909090909091,\n '&lt;e&gt;': 0.09090909090909091,\n '&lt;unk&gt;': 0.09090909090909091}\n\n# Additional test\ntrigram_counts = count_n_grams(sentences, 3)\nestimate_probabilities([\"&lt;s&gt;\", \"&lt;s&gt;\"], bigram_counts, trigram_counts, unique_words, k=1)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[27], line 2\n      1 # Additional test\n----&gt; 2 trigram_counts = count_n_grams(sentences, 3)\n      3 estimate_probabilities([\"&lt;s&gt;\", \"&lt;s&gt;\"], bigram_counts, trigram_counts, unique_words, k=1)\n\nCell In[21], line 21, in count_n_grams(data, n, start_token, end_token)\n     16 n_grams = {}\n     18 ### START CODE HERE (Replace instances of 'None' with your code) ###\n     19 \n     20 # Go through each sentence in the data\n---&gt; 21 for sentence in None: # complete this line\n     22     \n     23     # prepend start token n times, and  append &lt;e&gt; one time\n     24     sentence = None\n     26     # convert list to tuple\n     27     # So that the sequence of words can be used as\n     28     # a key in the dictionary\n\nTypeError: 'NoneType' object is not iterable\n\n\n\n\n\nExpected output\n{'cat': 0.09090909090909091,\n 'i': 0.18181818181818182,\n 'this': 0.18181818181818182,\n 'a': 0.09090909090909091,\n 'is': 0.09090909090909091,\n 'like': 0.09090909090909091,\n 'dog': 0.09090909090909091,\n '&lt;e&gt;': 0.09090909090909091,\n '&lt;unk&gt;': 0.09090909090909091}\n\n\n\nCount and probability matrices\nAs we have seen so far, the n-gram counts computed above are sufficient for computing the probabilities of the next word.\n- It can be more intuitive to present them as count or probability matrices. - The functions defined in the next cells return count or probability matrices. - This function is provided for you.\n\ndef make_count_matrix(n_plus1_gram_counts, vocabulary):\n    # add &lt;e&gt; &lt;unk&gt; to the vocabulary\n    # &lt;s&gt; is omitted since it should not appear as the next word\n    vocabulary = vocabulary + [\"&lt;e&gt;\", \"&lt;unk&gt;\"]\n    \n    # obtain unique n-grams\n    n_grams = []\n    for n_plus1_gram in n_plus1_gram_counts.keys():\n        n_gram = n_plus1_gram[0:-1]\n        n_grams.append(n_gram)\n    n_grams = list(set(n_grams))\n    \n    # mapping from n-gram to row\n    row_index = {n_gram:i for i, n_gram in enumerate(n_grams)}\n    # mapping from next word to column\n    col_index = {word:j for j, word in enumerate(vocabulary)}\n    \n    nrow = len(n_grams)\n    ncol = len(vocabulary)\n    count_matrix = np.zeros((nrow, ncol))\n    for n_plus1_gram, count in n_plus1_gram_counts.items():\n        n_gram = n_plus1_gram[0:-1]\n        word = n_plus1_gram[-1]\n        if word not in vocabulary:\n            continue\n        i = row_index[n_gram]\n        j = col_index[word]\n        count_matrix[i, j] = count\n    \n    count_matrix = pd.DataFrame(count_matrix, index=n_grams, columns=vocabulary)\n    return count_matrix\n\n\nsentences = [['i', 'like', 'a', 'cat'],\n                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\nunique_words = list(set(sentences[0] + sentences[1]))\nbigram_counts = count_n_grams(sentences, 2)\n\nprint('bigram counts')\ndisplay(make_count_matrix(bigram_counts, unique_words))\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[29], line 4\n      1 sentences = [['i', 'like', 'a', 'cat'],\n      2                  ['this', 'dog', 'is', 'like', 'a', 'cat']]\n      3 unique_words = list(set(sentences[0] + sentences[1]))\n----&gt; 4 bigram_counts = count_n_grams(sentences, 2)\n      6 print('bigram counts')\n      7 display(make_count_matrix(bigram_counts, unique_words))\n\nCell In[21], line 21, in count_n_grams(data, n, start_token, end_token)\n     16 n_grams = {}\n     18 ### START CODE HERE (Replace instances of 'None' with your code) ###\n     19 \n     20 # Go through each sentence in the data\n---&gt; 21 for sentence in None: # complete this line\n     22     \n     23     # prepend start token n times, and  append &lt;e&gt; one time\n     24     sentence = None\n     26     # convert list to tuple\n     27     # So that the sequence of words can be used as\n     28     # a key in the dictionary\n\nTypeError: 'NoneType' object is not iterable\n\n\n\n\nExpected output\nbigram counts\n          cat    i   this   a  is   like  dog  &lt;e&gt;   &lt;unk&gt;\n(&lt;s&gt;,)    0.0   1.0  1.0  0.0  0.0  0.0   0.0  0.0    0.0\n(a,)      2.0   0.0  0.0  0.0  0.0  0.0   0.0  0.0    0.0\n(this,)   0.0   0.0  0.0  0.0  0.0  0.0   1.0  0.0    0.0\n(like,)   0.0   0.0  0.0  2.0  0.0  0.0   0.0  0.0    0.0\n(dog,)    0.0   0.0  0.0  0.0  1.0  0.0   0.0  0.0    0.0\n(cat,)    0.0   0.0  0.0  0.0  0.0  0.0   0.0  2.0    0.0\n(is,)     0.0   0.0  0.0  0.0  0.0  1.0   0.0  0.0    0.0\n(i,)      0.0   0.0  0.0  0.0  0.0  1.0   0.0  0.0    0.0\n\n# Show trigram counts\nprint('\\ntrigram counts')\ntrigram_counts = count_n_grams(sentences, 3)\ndisplay(make_count_matrix(trigram_counts, unique_words))\n\n\ntrigram counts\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[30], line 3\n      1 # Show trigram counts\n      2 print('\\ntrigram counts')\n----&gt; 3 trigram_counts = count_n_grams(sentences, 3)\n      4 display(make_count_matrix(trigram_counts, unique_words))\n\nCell In[21], line 21, in count_n_grams(data, n, start_token, end_token)\n     16 n_grams = {}\n     18 ### START CODE HERE (Replace instances of 'None' with your code) ###\n     19 \n     20 # Go through each sentence in the data\n---&gt; 21 for sentence in None: # complete this line\n     22     \n     23     # prepend start token n times, and  append &lt;e&gt; one time\n     24     sentence = None\n     26     # convert list to tuple\n     27     # So that the sequence of words can be used as\n     28     # a key in the dictionary\n\nTypeError: 'NoneType' object is not iterable\n\n\n\n\n\nExpected output\ntrigram counts\n              cat    i   this   a  is   like  dog  &lt;e&gt;   &lt;unk&gt;\n(dog, is)     0.0   0.0  0.0  0.0  0.0  1.0   0.0  0.0    0.0\n(this, dog)   0.0   0.0  0.0  0.0  1.0  0.0   0.0  0.0    0.0\n(a, cat)      0.0   0.0  0.0  0.0  0.0  0.0   0.0  2.0    0.0\n(like, a)     2.0   0.0  0.0  0.0  0.0  0.0   0.0  0.0    0.0\n(is, like)    0.0   0.0  0.0  1.0  0.0  0.0   0.0  0.0    0.0\n(&lt;s&gt;, i)      0.0   0.0  0.0  0.0  0.0  1.0   0.0  0.0    0.0\n(i, like)     0.0   0.0  0.0  1.0  0.0  0.0   0.0  0.0    0.0\n(&lt;s&gt;, &lt;s&gt;)    0.0   1.0  1.0  0.0  0.0  0.0   0.0  0.0    0.0\n(&lt;s&gt;, this)   0.0   0.0  0.0  0.0  0.0  0.0   1.0  0.0    0.0\nThe following function calculates the probabilities of each word given the previous n-gram, and stores this in matrix form. - This function is provided for you.\n\ndef make_probability_matrix(n_plus1_gram_counts, vocabulary, k):\n    count_matrix = make_count_matrix(n_plus1_gram_counts, unique_words)\n    count_matrix += k\n    prob_matrix = count_matrix.div(count_matrix.sum(axis=1), axis=0)\n    return prob_matrix\n\n\nsentences = [['i', 'like', 'a', 'cat'],\n                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\nunique_words = list(set(sentences[0] + sentences[1]))\nbigram_counts = count_n_grams(sentences, 2)\nprint(\"bigram probabilities\")\ndisplay(make_probability_matrix(bigram_counts, unique_words, k=1))\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[32], line 4\n      1 sentences = [['i', 'like', 'a', 'cat'],\n      2                  ['this', 'dog', 'is', 'like', 'a', 'cat']]\n      3 unique_words = list(set(sentences[0] + sentences[1]))\n----&gt; 4 bigram_counts = count_n_grams(sentences, 2)\n      5 print(\"bigram probabilities\")\n      6 display(make_probability_matrix(bigram_counts, unique_words, k=1))\n\nCell In[21], line 21, in count_n_grams(data, n, start_token, end_token)\n     16 n_grams = {}\n     18 ### START CODE HERE (Replace instances of 'None' with your code) ###\n     19 \n     20 # Go through each sentence in the data\n---&gt; 21 for sentence in None: # complete this line\n     22     \n     23     # prepend start token n times, and  append &lt;e&gt; one time\n     24     sentence = None\n     26     # convert list to tuple\n     27     # So that the sequence of words can be used as\n     28     # a key in the dictionary\n\nTypeError: 'NoneType' object is not iterable\n\n\n\n\nprint(\"trigram probabilities\")\ntrigram_counts = count_n_grams(sentences, 3)\ndisplay(make_probability_matrix(trigram_counts, unique_words, k=1))\n\ntrigram probabilities\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[33], line 2\n      1 print(\"trigram probabilities\")\n----&gt; 2 trigram_counts = count_n_grams(sentences, 3)\n      3 display(make_probability_matrix(trigram_counts, unique_words, k=1))\n\nCell In[21], line 21, in count_n_grams(data, n, start_token, end_token)\n     16 n_grams = {}\n     18 ### START CODE HERE (Replace instances of 'None' with your code) ###\n     19 \n     20 # Go through each sentence in the data\n---&gt; 21 for sentence in None: # complete this line\n     22     \n     23     # prepend start token n times, and  append &lt;e&gt; one time\n     24     sentence = None\n     26     # convert list to tuple\n     27     # So that the sequence of words can be used as\n     28     # a key in the dictionary\n\nTypeError: 'NoneType' object is not iterable\n\n\n\nConfirm that you obtain the same results as for the estimate_probabilities function that you implemented.\n ## Part 3: Perplexity\nIn this section, you will generate the perplexity score to evaluate your model on the test set. - You will also use back-off when needed. - Perplexity is used as an evaluation metric of your language model. - To calculate the the perplexity score of the test set on an n-gram model, use:\n PP(W) =\\sqrt[N]{ \\prod_{t=n+1}^N \\frac{1}{P(w_t | w_{t-n} \\cdots w_{t-1})} } \\tag{4}\n\nwhere N is the length of the sentence.\nn is the number of words in the n-gram (e.g. 2 for a bigram).\nIn math, the numbering starts at one and not zero.\n\nIn code, array indexing starts at zero, so the code will use ranges for t according to this formula:\n PP(W) =\\sqrt[N]{ \\prod_{t=n}^{N-1} \\frac{1}{P(w_t | w_{t-n} \\cdots w_{t-1})} } \\tag{4.1}\nThe higher the probabilities are, the lower the perplexity will be. - The more the n-grams tell us about the sentence, the lower the perplexity score will be.\n ### Exercise 10 Compute the perplexity score given an N-gram count matrix and a sentence.\n\n\nHints\n\n\n\n\nRemember that range(2,4) produces the integers [2, 3] (and excludes 4).\n\n\n\n\n# UNQ_C10 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: calculate_perplexity\ndef calculate_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n    \"\"\"\n    Calculate perplexity for a list of sentences\n    \n    Args:\n        sentence: List of strings\n        n_gram_counts: Dictionary of counts of (n+1)-grams\n        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n        vocabulary_size: number of unique words in the vocabulary\n        k: Positive smoothing constant\n    \n    Returns:\n        Perplexity score\n    \"\"\"\n    # length of previous words\n    n = len(list(n_gram_counts.keys())[0]) \n    \n    # prepend &lt;s&gt; and append &lt;e&gt;\n    sentence = [\"&lt;s&gt;\"] * n + sentence + [\"&lt;e&gt;\"]\n    \n    # Cast the sentence from a list to a tuple\n    sentence = tuple(sentence)\n    \n    # length of sentence (after adding &lt;s&gt; and &lt;e&gt; tokens)\n    N = len(sentence)\n    \n    # The variable p will hold the product\n    # that is calculated inside the n-root\n    # Update this in the code below\n    product_pi = 1.0\n    \n    ### START CODE HERE (Replace instances of 'None' with your code) ###\n    \n    # Index t ranges from n to N - 1, inclusive on both ends\n    for t in range(None, None): # complete this line\n\n        # get the n-gram preceding the word at position t\n        n_gram = None\n        \n        # get the word at position t\n        word = None\n        \n        # Estimate the probability of the word given the n-gram\n        # using the n-gram counts, n-plus1-gram counts,\n        # vocabulary size, and smoothing constant\n        probability = None\n        \n        # Update the product of the probabilities\n        # This 'product_pi' is a cumulative product \n        # of the (1/P) factors that are calculated in the loop\n        product_pi *= None\n\n    # Take the Nth root of the product\n    perplexity = None\n    \n    ### END CODE HERE ### \n    return perplexity\n\n\n# test your code\n\nsentences = [['i', 'like', 'a', 'cat'],\n                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\nunique_words = list(set(sentences[0] + sentences[1]))\n\nunigram_counts = count_n_grams(sentences, 1)\nbigram_counts = count_n_grams(sentences, 2)\n\n\nperplexity_train1 = calculate_perplexity(sentences[0],\n                                         unigram_counts, bigram_counts,\n                                         len(unique_words), k=1.0)\nprint(f\"Perplexity for first train sample: {perplexity_train1:.4f}\")\n\ntest_sentence = ['i', 'like', 'a', 'dog']\nperplexity_test = calculate_perplexity(test_sentence,\n                                       unigram_counts, bigram_counts,\n                                       len(unique_words), k=1.0)\nprint(f\"Perplexity for test sample: {perplexity_test:.4f}\")\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[35], line 7\n      3 sentences = [['i', 'like', 'a', 'cat'],\n      4                  ['this', 'dog', 'is', 'like', 'a', 'cat']]\n      5 unique_words = list(set(sentences[0] + sentences[1]))\n----&gt; 7 unigram_counts = count_n_grams(sentences, 1)\n      8 bigram_counts = count_n_grams(sentences, 2)\n     11 perplexity_train1 = calculate_perplexity(sentences[0],\n     12                                          unigram_counts, bigram_counts,\n     13                                          len(unique_words), k=1.0)\n\nCell In[21], line 21, in count_n_grams(data, n, start_token, end_token)\n     16 n_grams = {}\n     18 ### START CODE HERE (Replace instances of 'None' with your code) ###\n     19 \n     20 # Go through each sentence in the data\n---&gt; 21 for sentence in None: # complete this line\n     22     \n     23     # prepend start token n times, and  append &lt;e&gt; one time\n     24     sentence = None\n     26     # convert list to tuple\n     27     # So that the sequence of words can be used as\n     28     # a key in the dictionary\n\nTypeError: 'NoneType' object is not iterable\n\n\n\n\n\n\nExpected Output\nPerplexity for first train sample: 2.8040\nPerplexity for test sample: 3.9654\n Note:  If your sentence is really long, there will be underflow when multiplying many fractions. - To handle longer sentences, modify your implementation to take the sum of the log of the probabilities.\n ## Part 4: Build an auto-complete system\nIn this section, you will combine the language models developed so far to implement an auto-complete system.\n ### Exercise 11 Compute probabilities for all possible next words and suggest the most likely one. - This function also take an optional argument start_with, which specifies the first few letters of the next words.\n\n\nHints\n\n\n\n\nestimate_probabilities returns a dictionary where the key is a word and the value is the word’s probability.\n\n\nUse str1.startswith(str2) to determine if a string starts with the letters of another string. For example, ‘learning’.startswith(‘lea’) returns True, whereas ‘learning’.startswith(‘ear’) returns False. There are two additional parameters in str.startswith(), but you can use the default values for those parameters in this case.\n\n\n\n\n# UNQ_C11 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: suggest_a_word\ndef suggest_a_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0, start_with=None):\n    \"\"\"\n    Get suggestion for the next word\n    \n    Args:\n        previous_tokens: The sentence you input where each token is a word. Must have length &gt; n \n        n_gram_counts: Dictionary of counts of (n+1)-grams\n        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n        vocabulary: List of words\n        k: positive constant, smoothing parameter\n        start_with: If not None, specifies the first few letters of the next word\n        \n    Returns:\n        A tuple of \n          - string of the most likely next word\n          - corresponding probability\n    \"\"\"\n    \n    # length of previous words\n    n = len(list(n_gram_counts.keys())[0]) \n    \n    # From the words that the user already typed\n    # get the most recent 'n' words as the previous n-gram\n    previous_n_gram = previous_tokens[-n:]\n\n    # Estimate the probabilities that each word in the vocabulary\n    # is the next word,\n    # given the previous n-gram, the dictionary of n-gram counts,\n    # the dictionary of n plus 1 gram counts, and the smoothing constant\n    probabilities = estimate_probabilities(previous_n_gram,\n                                           n_gram_counts, n_plus1_gram_counts,\n                                           vocabulary, k=k)\n    \n    # Initialize suggested word to None\n    # This will be set to the word with highest probability\n    suggestion = None\n    \n    # Initialize the highest word probability to 0\n    # this will be set to the highest probability \n    # of all words to be suggested\n    max_prob = 0\n    \n    ### START CODE HERE (Replace instances of 'None' with your code) ###\n    \n    # For each word and its probability in the probabilities dictionary:\n    for word, prob in None: # complete this line\n        \n        # If the optional start_with string is set\n        if None: # complete this line\n            \n            # Check if the beginning of word does not match with the letters in 'start_with'\n            if None: # complete this line\n\n                # if they don't match, skip this word (move onto the next word)\n                None # complete this line\n        \n        # Check if this word's probability\n        # is greater than the current maximum probability\n        if None: # complete this line\n            \n            # If so, save this word as the best suggestion (so far)\n            suggestion = None\n            \n            # Save the new maximum probability\n            max_prob = None\n\n    ### END CODE HERE\n    \n    return suggestion, max_prob\n\n\n# test your code\nsentences = [['i', 'like', 'a', 'cat'],\n             ['this', 'dog', 'is', 'like', 'a', 'cat']]\nunique_words = list(set(sentences[0] + sentences[1]))\n\nunigram_counts = count_n_grams(sentences, 1)\nbigram_counts = count_n_grams(sentences, 2)\n\nprevious_tokens = [\"i\", \"like\"]\ntmp_suggest1 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0)\nprint(f\"The previous words are 'i like',\\n\\tand the suggested word is `{tmp_suggest1[0]}` with a probability of {tmp_suggest1[1]:.4f}\")\n\nprint()\n# test your code when setting the starts_with\ntmp_starts_with = 'c'\ntmp_suggest2 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0, start_with=tmp_starts_with)\nprint(f\"The previous words are 'i like', the suggestion must start with `{tmp_starts_with}`\\n\\tand the suggested word is `{tmp_suggest2[0]}` with a probability of {tmp_suggest2[1]:.4f}\")\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[37], line 6\n      2 sentences = [['i', 'like', 'a', 'cat'],\n      3              ['this', 'dog', 'is', 'like', 'a', 'cat']]\n      4 unique_words = list(set(sentences[0] + sentences[1]))\n----&gt; 6 unigram_counts = count_n_grams(sentences, 1)\n      7 bigram_counts = count_n_grams(sentences, 2)\n      9 previous_tokens = [\"i\", \"like\"]\n\nCell In[21], line 21, in count_n_grams(data, n, start_token, end_token)\n     16 n_grams = {}\n     18 ### START CODE HERE (Replace instances of 'None' with your code) ###\n     19 \n     20 # Go through each sentence in the data\n---&gt; 21 for sentence in None: # complete this line\n     22     \n     23     # prepend start token n times, and  append &lt;e&gt; one time\n     24     sentence = None\n     26     # convert list to tuple\n     27     # So that the sequence of words can be used as\n     28     # a key in the dictionary\n\nTypeError: 'NoneType' object is not iterable\n\n\n\n\n\nExpected output\nThe previous words are 'i like',\n    and the suggested word is `a` with a probability of 0.2727\n\nThe previous words are 'i like', the suggestion must start with `c`\n    and the suggested word is `cat` with a probability of 0.0909\n\n\nGet multiple suggestions\nThe function defined below loop over varioud n-gram models to get multiple suggestions.\n\ndef get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n    model_counts = len(n_gram_counts_list)\n    suggestions = []\n    for i in range(model_counts-1):\n        n_gram_counts = n_gram_counts_list[i]\n        n_plus1_gram_counts = n_gram_counts_list[i+1]\n        \n        suggestion = suggest_a_word(previous_tokens, n_gram_counts,\n                                    n_plus1_gram_counts, vocabulary,\n                                    k=k, start_with=start_with)\n        suggestions.append(suggestion)\n    return suggestions\n\n\n# test your code\nsentences = [['i', 'like', 'a', 'cat'],\n             ['this', 'dog', 'is', 'like', 'a', 'cat']]\nunique_words = list(set(sentences[0] + sentences[1]))\n\nunigram_counts = count_n_grams(sentences, 1)\nbigram_counts = count_n_grams(sentences, 2)\ntrigram_counts = count_n_grams(sentences, 3)\nquadgram_counts = count_n_grams(sentences, 4)\nqintgram_counts = count_n_grams(sentences, 5)\n\nn_gram_counts_list = [unigram_counts, bigram_counts, trigram_counts, quadgram_counts, qintgram_counts]\nprevious_tokens = [\"i\", \"like\"]\ntmp_suggest3 = get_suggestions(previous_tokens, n_gram_counts_list, unique_words, k=1.0)\n\nprint(f\"The previous words are 'i like', the suggestions are:\")\ndisplay(tmp_suggest3)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[39], line 6\n      2 sentences = [['i', 'like', 'a', 'cat'],\n      3              ['this', 'dog', 'is', 'like', 'a', 'cat']]\n      4 unique_words = list(set(sentences[0] + sentences[1]))\n----&gt; 6 unigram_counts = count_n_grams(sentences, 1)\n      7 bigram_counts = count_n_grams(sentences, 2)\n      8 trigram_counts = count_n_grams(sentences, 3)\n\nCell In[21], line 21, in count_n_grams(data, n, start_token, end_token)\n     16 n_grams = {}\n     18 ### START CODE HERE (Replace instances of 'None' with your code) ###\n     19 \n     20 # Go through each sentence in the data\n---&gt; 21 for sentence in None: # complete this line\n     22     \n     23     # prepend start token n times, and  append &lt;e&gt; one time\n     24     sentence = None\n     26     # convert list to tuple\n     27     # So that the sequence of words can be used as\n     28     # a key in the dictionary\n\nTypeError: 'NoneType' object is not iterable\n\n\n\n\n\nSuggest multiple words using n-grams of varying length\nCongratulations! You have developed all building blocks for implementing your own auto-complete systems.\nLet’s see this with n-grams of varying lengths (unigrams, bigrams, trigrams, 4-grams…6-grams).\n\nn_gram_counts_list = []\nfor n in range(1, 6):\n    print(\"Computing n-gram counts with n =\", n, \"...\")\n    n_model_counts = count_n_grams(train_data_processed, n)\n    n_gram_counts_list.append(n_model_counts)\n\nComputing n-gram counts with n = 1 ...\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[40], line 4\n      2 for n in range(1, 6):\n      3     print(\"Computing n-gram counts with n =\", n, \"...\")\n----&gt; 4     n_model_counts = count_n_grams(train_data_processed, n)\n      5     n_gram_counts_list.append(n_model_counts)\n\nNameError: name 'train_data_processed' is not defined\n\n\n\n\nprevious_tokens = [\"i\", \"am\", \"to\"]\ntmp_suggest4 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n\nprint(f\"The previous words are {previous_tokens}, the suggestions are:\")\ndisplay(tmp_suggest4)\n\nThe previous words are ['i', 'am', 'to'], the suggestions are:\n\n\n[]\n\n\n\nprevious_tokens = [\"i\", \"want\", \"to\", \"go\"]\ntmp_suggest5 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n\nprint(f\"The previous words are {previous_tokens}, the suggestions are:\")\ndisplay(tmp_suggest5)\n\nThe previous words are ['i', 'want', 'to', 'go'], the suggestions are:\n\n\n[]\n\n\n\nprevious_tokens = [\"hey\", \"how\", \"are\"]\ntmp_suggest6 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n\nprint(f\"The previous words are {previous_tokens}, the suggestions are:\")\ndisplay(tmp_suggest6)\n\nThe previous words are ['hey', 'how', 'are'], the suggestions are:\n\n\n[]\n\n\n\nprevious_tokens = [\"hey\", \"how\", \"are\", \"you\"]\ntmp_suggest7 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n\nprint(f\"The previous words are {previous_tokens}, the suggestions are:\")\ndisplay(tmp_suggest7)\n\nThe previous words are ['hey', 'how', 'are', 'you'], the suggestions are:\n\n\n[]\n\n\n\nprevious_tokens = [\"hey\", \"how\", \"are\", \"you\"]\ntmp_suggest8 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=\"d\")\n\nprint(f\"The previous words are {previous_tokens}, the suggestions are:\")\ndisplay(tmp_suggest8)\n\nThe previous words are ['hey', 'how', 'are', 'you'], the suggestions are:\n\n\n[]"
  },
  {
    "objectID": "posts/c2w3/lab01.html",
    "href": "posts/c2w3/lab01.html",
    "title": "N-grams Corpus preprocessing",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure 1"
  },
  {
    "objectID": "posts/c2w3/lab01.html#n-grams",
    "href": "posts/c2w3/lab01.html#n-grams",
    "title": "N-grams Corpus preprocessing",
    "section": "N-grams",
    "text": "N-grams\n\nSentence to n-gram\nThe next step is to build n-grams from the tokenized sentences.\nA sliding window of size n-words can generate the n-grams. The window scans the list of words starting at the sentence beginning, moving by a step of one word until it reaches the end of the sentence.\nHere is an example method that prints all trigrams in the given sentence.\n\ndef sentence_to_trigram(tokenized_sentence):\n    \"\"\"\n    Prints all trigrams in the given tokenized sentence.\n    \n    Args:\n        tokenized_sentence: The words list.\n    \n    Returns:\n        No output\n    \"\"\"\n    # note that the last position of i is 3rd to the end\n    for i in range(len(tokenized_sentence) - 3 + 1):\n        # the sliding window starts at position i and contains 3 words\n        trigram = tokenized_sentence[i : i + 3]\n        print(trigram)\n\ntokenized_sentence = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n\nprint(f'List all trigrams of sentence: {tokenized_sentence}\\n')\nsentence_to_trigram(tokenized_sentence)\n\nList all trigrams of sentence: ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n\n['i', 'am', 'happy']\n['am', 'happy', 'because']\n['happy', 'because', 'i']\n['because', 'i', 'am']\n['i', 'am', 'learning']\n['am', 'learning', '.']\n\n\n ### Prefix of an n-gram\nAs you saw in the lecture, the n-gram probability is often calculated based on the (n-1)-gram counts. The prefix is needed in the formula to calculate the probability of an n-gram.\n\\begin{equation*}\nP(w_n|w_1^{n-1})=\\frac{C(w_1^n)}{C(w_1^{n-1})}\n\\end{equation*}\nThe following code shows how to get an (n-1)-gram prefix from n-gram on an example of getting trigram from a 4-gram.\n\n# get trigram prefix from a 4-gram\nfourgram = ['i', 'am', 'happy','because']\ntrigram = fourgram[0:-1] # Get the elements from 0, included, up to the last element, not included.\nprint(trigram)\n\n['i', 'am', 'happy']\n\n\n\n\nStart and end of sentence word &lt;s&gt; and &lt;/s&gt;\nYou could see in the lecture that we must add some special characters at the beginning and the end of each sentence:\n\n&lt;s&gt; at beginning\n&lt;/s&gt; at the end\n\nFor n-grams, we must prepend n-1 of characters at the begining of the sentence.\nLet us have a look at how you can implement this in code.\n\n# when working with trigrams, you need to prepend 2 &lt;s&gt; and append one &lt;/s&gt;\nn = 3\ntokenized_sentence = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\ntokenized_sentence = [\"&lt;s&gt;\"] * (n - 1) + tokenized_sentence + [\"&lt;/s&gt;\"]\nprint(tokenized_sentence)\n\n['&lt;s&gt;', '&lt;s&gt;', 'i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.', '&lt;/s&gt;']\n\n\nThat’s all for the lab for “N-gram” lesson of week 3."
  },
  {
    "objectID": "posts/c2w3/index.html",
    "href": "posts/c2w3/index.html",
    "title": "Autocomplete and Language Models",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nFigure 2\nThese are my notes for Week 3 notes for the Natural Language Processing with Probabilistic Models Course in the Natural Language Processing Specialization Offered by DeepLearning.AI on Coursera.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete and Language Models"
    ]
  },
  {
    "objectID": "posts/c2w3/index.html#tldr---autocomplete-and-language-models",
    "href": "posts/c2w3/index.html#tldr---autocomplete-and-language-models",
    "title": "Autocomplete and Language Models",
    "section": "TL;DR - Autocomplete and Language Models",
    "text": "TL;DR - Autocomplete and Language Models\n\n\n\nLanguage Models in a nutshell\n\n\nThis week we learn how to model a language using N-grams. Starting from the definition of conditional probability we develop the probabilities of sequences of words. Next we add start and end of sentences tokens to our word sequence model. Next we add tokens to represent out of vocabulary words. Then we tackle sparsity by implementing smoothing and backoff. Finally we consider how to evaluate our language model.\nIn the labs we preprocess a corpus to create an N-gram language model. We then build the language model and evaluate it using perplexity.\nIn the assignment for this week, we build a language model to generate autocomplete a text fragment. We will also evaluate the perplexity of the model.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete and Language Models"
    ]
  },
  {
    "objectID": "posts/c2w3/index.html#n-grams-overview",
    "href": "posts/c2w3/index.html#n-grams-overview",
    "title": "Autocomplete and Language Models",
    "section": "N-Grams Overview",
    "text": "N-Grams Overview\nRecall how Firth suggested a distributional view of semantics?\nPreviously we used a vector space model to represent this idea. Now we dive deeper and develop a model for distributional semantics using probabilities of sequences of words. Once we can estimate these probabilities we can predict the next word in a sentence.\nN-gram refer to howe we model the a sequence of N words. For example, a bigram model would model the probability of a word given the previous word. A trigram model would model the probability of a word given the previous bigram. And so on. These probabilities are derived by counting frequencies in a corpus of texts.\nN-grams are fundamental and give you a foundation that will allow you to understand more complicated models in the specialization. They form the theoretical under pinning for the next two courses.\nN-grams models allow us to predict the probabilities of certain words happening in a specific sequence. Using that, you can build an auto-correct or even a search suggestion tool.\nOther applications of N-gram language modeling include:\n\nSpeech recognition\nSpelling correction\nAugmentative communication\n\n\n\n\n\n\n\n\ncaption\n\n\n\n\nFigure 3: Description\n\n\nThis week you are going to learn to:\n\nProcess a text corpus to N-gram language model\nHandle out of vocabulary words\nImplement smoothing for previously unseen N-grams\nLanguage model evaluation",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete and Language Models"
    ]
  },
  {
    "objectID": "posts/c2w3/index.html#n-grams-and-probabilities",
    "href": "posts/c2w3/index.html#n-grams-and-probabilities",
    "title": "Autocomplete and Language Models",
    "section": "N-grams and Probabilities",
    "text": "N-grams and Probabilities\nBefore we start computing probabilities of certain sequences, we need to first define what is an N-gram language model:\n\n\n\n\n\n\n\ncaption\n\n\n\n\nFigure 4: Description\n\n\nNow given the those definitions, we can label a sentence as follows:\n\n\n\n\n\n\n\ncaption\n\n\n\n\nFigure 5: Description\n\n\nIn other notation you can write:\n\nw_1^m = w_1 w_2 w_3 ... w_m\n \nw_1^3 = w_1 w_2 w_3\n \nw_{m-2}^m = w_{m-2} w_{m-1} w_m\n\nGiven the following corpus: I am happy because I am learning.\nSize of corpus m = 7.\n\nP(I) = \\frac{1}{7}\n\n\nP(happy) = \\frac{1}{7}\n\nTo generalize, the probability of a unigram is\n\nP(w) = \\frac{C(w)}{m}\n\nWhere C(w) is the count of the word in the corpus and m is the size of the corpus.\n\nBigram Probability:\n\n\n\n\n\n\n\ncaption\n\n\n\n\nFigure 6: Description\n\n\n\n\nTrigram Probability:\nTo compute the probability of a trigram: \nP(w_3 | w_1^2) = \\frac{C(w_1^2 w_3)}{C(w_1^2)}\n\n\nC(w_1^2 w_3) = C(w_1 w_2 w_3) = C(w_1^3)\n\nN-gram Probability:\n\nP(w_n | w_1^{n-1}) = \\frac{C(w_1^{n-1} w_n)}{C(w_1^{n-1})}\n\n\nC(w_1^{n-1} w_n) = C(w_1^n)",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete and Language Models"
    ]
  },
  {
    "objectID": "posts/c2w3/index.html#sequence-probabilities",
    "href": "posts/c2w3/index.html#sequence-probabilities",
    "title": "Autocomplete and Language Models",
    "section": "Sequence Probabilities",
    "text": "Sequence Probabilities\nYou just saw how to compute sequence probabilities, their short comings, and finally how to approximate N-gram probabilities. In doing so, you try to approximate the probability of a sentence. For example, what is the probability of the following sentence: The teacher drinks tea. To compute it, you will make use of the following:\n\nP(B|A) = \\frac{P(A,B)}{P(A)} \\Rightarrow P(A,B) = P(A)P(B|A)\n\n\nP(A,B,C,D) = P(A)P(B|A)P(C|A,B)P(D|A,B,C)\n\nTo compute the probability of a sequence, you can compute the following:\n\nP(\\text{The teacher drinks tea}) = P(\\text{The})P(\\text{teacher}|\\text{The})P(\\text{drinks}|\\text{The teacher})P(\\text{tea}|\\text{The teacher drinks})\n\nOne of the main issues with computing the probabilities above is the corpus rarely contains the exact same phrases as the ones you computed your probabilities on. Hence, you can easily end up getting a probability of 0. The Markov assumption indicates that only the last word matters. Hence:\n\n\\text{Bigram} \\qquad P(w_n | w_1^{n-1}) \\approx P(w_n | w_{n-1})\n\n\n\\text{Trigram} \\qquad P(w_n | w_1^{n-1}) \\approx P(w_n | w_{n-2}^{n-1})\n\n\n\\text{N-gram} \\qquad P(w_n | w_1^{n-1}) \\approx P(w_n | w_{n-N+1}^{n-1})\n\nYou can model the entire sentence as follows:\n\nP(w_1^n) = P(w_1)P(w_2|w_1)P(w_3|w_2)P(w_4|w_3) ... P(w_n|w_{n-1}) = \\prod_{i=1}^{n} P(w_i|w_{i-1})\n\n\nP(w_1^n) = P(w_1)P(w_2|w_1)P(w_3|w_2)P(w_4|w_3) ... P(w_n|w_{n-1}) = \\prod_{i=1}^{n} P(w_i|w_{i-1})",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete and Language Models"
    ]
  },
  {
    "objectID": "posts/c2w3/index.html#starting-and-ending-sentences",
    "href": "posts/c2w3/index.html#starting-and-ending-sentences",
    "title": "Autocomplete and Language Models",
    "section": "Starting and Ending Sentences",
    "text": "Starting and Ending Sentences\nWe usually start and end a sentence with the following tokens respectively: &lt;s&gt; &lt;/s&gt;.\nWhen computing probabilities using a unigram, you can append an &lt;s&gt; in the beginning of the sentence. To generalize to an N-gram language model, you can add N-1 start tokens &lt;s&gt;.\nFor the end of sentence token &lt;/s&gt;, you only need one even if it is an N-gram. Here is an example:\n\n\n\n\n\n\n\ncaption\n\n\n\n\nFigure 7: Description\n\n\nMake sure you know how to compute the probabilities above!",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete and Language Models"
    ]
  },
  {
    "objectID": "posts/c2w3/index.html#lecture-notebook-corpus-preprocessing-for-n-grams",
    "href": "posts/c2w3/index.html#lecture-notebook-corpus-preprocessing-for-n-grams",
    "title": "Autocomplete and Language Models",
    "section": "Lecture notebook: Corpus preprocessing for N-grams",
    "text": "Lecture notebook: Corpus preprocessing for N-grams\nlab 1 Corpus preprocessing for N-grams",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete and Language Models"
    ]
  },
  {
    "objectID": "posts/c2w3/index.html#the-n-gram-language-model",
    "href": "posts/c2w3/index.html#the-n-gram-language-model",
    "title": "Autocomplete and Language Models",
    "section": "The N-gram Language Model",
    "text": "The N-gram Language Model\n\n\n\n\n\n\n\nCount matrix\n\n\n\n\nFigure 8: Count Matrix\n\n\n\n\n\n\n\n\nProbability matrix\n\n\n\n\nFigure 9: Probability Matrix\n\n\n\n\n\n\n\n\nLanguage model\n\n\n\n\nFigure 10: Language model\n\n\n\n\n\n\n\n\nLog probability\n\n\n\n\nFigure 11: Log probability\n\n\n\n\n\nWe covered a lot of concepts in the previous video. You have seen:\n\nCount matrix c.f Figure 8\nProbability matrix c.f Figure 9\nLanguage model c.f. Figure 10\nLog probability c.f Figure 11 to avoid underflow\nGenerative language model c.f. Figure 12\n\nIn the count matrix:\n\nRows correspond to the unique corpus N-1 grams.\nColumns correspond to the unique corpus words.\n\nHere is an example of the count matrix of a bigram.\nTo convert it into a probability matrix, you can use the following formula:\n\nP(w_n \\mid w^{n-1}_{n-N+1}) = \\frac{C(w^{n-1}_{n-N+1}, w_n)}{C(w^{n-1}_{n-N+1})}\n\\tag{1}\n\nsum(row) = \\sum_{w \\in V} C(w^{n-1}_{n-N+1}, w) = C(w^{n-1}_{n-N+1})\n\\tag{2}\nNow given the probability matrix, you can generate the language model. You can compute the sentence probability and the next word prediction.\nTo compute the probability of a sequence, you needed to compute:\n\n\\begin{align*}\nP(w_1^n) &= P(w_1)P(w_2|w_1)P(w_3|w_2)P(w_4|w_3) \\ldots P(w_n|w_{n-1})  \\\\\n&= \\prod_{i=1}^{n} P(w_i | w_{i-1})\n\\end{align*}\n\\tag{3}\nTo avoid underflow, you can multiply by the log.\n\n\\begin{align*}\n\\log P(w_1^n) &= \\log P(w_1) + \\log P(w_2|w_1) + \\log P(w_3|w_2) + \\ldots+ \\log P(w_n|w_{n-1}) \\\\\n& = \\sum_{i=1}^{n} \\log P(w_i|w_{i-1})\n\\end{align*}\n\\tag{4}\nFinally, you can create a generative language model.\n\n\n\n\n\n\n\nGenerative language model\n\n\n\n\nFigure 12: Generative language model",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete and Language Models"
    ]
  },
  {
    "objectID": "posts/c2w3/index.html#lecture-notebook-building-the-language-model",
    "href": "posts/c2w3/index.html#lecture-notebook-building-the-language-model",
    "title": "Autocomplete and Language Models",
    "section": "Lecture notebook: Building the language model",
    "text": "Lecture notebook: Building the language model\nlab 2 Building the language model",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete and Language Models"
    ]
  },
  {
    "objectID": "posts/c2w3/index.html#language-model-evaluation",
    "href": "posts/c2w3/index.html#language-model-evaluation",
    "title": "Autocomplete and Language Models",
    "section": "Language Model Evaluation",
    "text": "Language Model Evaluation\nSplitting the Data We will now discuss the train/val/test splits and perplexity.\n\nTrain/Val/Test splits\n\n\n\nSmaller Corpora:\nLarger Corpora:\n\n\n\n\ntrain\n80%\n98%\n\n\ntest\n10%\n1%\n\n\nval\n10%\n1%\n\n\n\nThere are two main methods for splitting the data:\n\n\n\n\n\n\n\ncaption\n\n\n\n\nFigure 13: Count Matrix",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete and Language Models"
    ]
  },
  {
    "objectID": "posts/c2w3/index.html#perplexity",
    "href": "posts/c2w3/index.html#perplexity",
    "title": "Autocomplete and Language Models",
    "section": "Perplexity",
    "text": "Perplexity\nPerplexity is used to tell us whether a set of sentences look like they were written by humans rather than by a simple program choosing words at random. A text that is written by humans is more likely to have lower perplexity, where a text generated by random word choice would have a higher perplexity.\nConcretely, here are the formulas to calculate perplexity.\n\nPP(W) = P(s_1, s_2, \\ldots, s_m)^{-\\frac{1}{m}}\n\n\nPP(W) = \\sqrt{ \\prod_{i=1}^{m} \\prod_{j=1}^{|s_i|} \\frac{1}{P(w_j^{(i)} | w_{j-1}^{(i)})}}\n\n​w_j^{(i)} corresponds to the jth word in the ith sentence. If you were to concatenate all the sentences then w_i is the ith word in the test set. To compute the log perplexity, you go from:\n\nPP(W) = \\sqrt{ \\prod_{i=1}^{m} \\frac{1}{P(w_i | w_{i-1})}}\n\nTo\n\nlog PP(W) = -\\frac{1}{m} \\sum_{i=1}^{m} \\log_2 P(w_i | w_{i-1})",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete and Language Models"
    ]
  },
  {
    "objectID": "posts/c2w3/index.html#out-of-vocabulary-words",
    "href": "posts/c2w3/index.html#out-of-vocabulary-words",
    "title": "Autocomplete and Language Models",
    "section": "Out of Vocabulary Words",
    "text": "Out of Vocabulary Words\nMany times, you will be dealing with unknown words in the corpus. So how do you choose your vocabulary? What is a vocabulary?\nA vocabulary is a set of unique words supported by your language model. In some tasks like speech recognition or question answering, you will encounter and generate words only from a fixed set of words. Hence, a closed vocabulary.\nOpen vocabulary means that you may encounter words from outside the vocabulary, like a name of a new city in the training set. Here is one recipe that would allow you to handle unknown words.\n\nCreate vocabulary V\nReplace any word in corpus and not in V by &lt;UNK&gt;\nCount the probabilities with &lt;UNK&gt; as with any other word\n\n\n\n\n\n\n\n\ncaption\n\n\n\n\nFigure 14: Count Matrix\n\n\nThe example above shows how you can use min_frequency and replace all the words that show up fewer times than min_frequency by UNK. You can then treat UNK as a regular word.\n\nCriteria to create the vocabulary\n\nMin word frequency f\nMax |V|, include words by frequency\nUse &lt;UNK&gt; sparingly (Why?)\nPerplexity - only compare LMs with the same V",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete and Language Models"
    ]
  },
  {
    "objectID": "posts/c2w3/index.html#out-of-vocabulary-words-1",
    "href": "posts/c2w3/index.html#out-of-vocabulary-words-1",
    "title": "Autocomplete and Language Models",
    "section": "Out of Vocabulary Words",
    "text": "Out of Vocabulary Words",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete and Language Models"
    ]
  },
  {
    "objectID": "posts/c2w3/index.html#smoothing",
    "href": "posts/c2w3/index.html#smoothing",
    "title": "Autocomplete and Language Models",
    "section": "Smoothing",
    "text": "Smoothing\n\n\n\n\n\n\n\nProblem\n\n\n\n\nFigure 15: Missing N-grams\n\n\nThe three main concepts covered here are dealing with missing n-grams, smoothing, and Backoff and interpolation.\n\n\n\n\n\n\n\nSmoothing\n\n\n\n\nFigure 16: Add One Smoothing, Add K Smoothing\n\n\n\nP(w_n | w^{n-1}_{n-N+1}) = \\frac{C(w^{n-1}_{n-N+1}, w_n)}{C(w^{n-1}_{n-N+1})} \\qquad \\text{can be 0}\n\\tag{5}\nHence we can add-1 smoothing as follows to fix that problem:\n\nP(w_n | w_{n-1}) = \\frac{C(w_{n-1}, w_n) + 1}{\\sum_{w \\in V} (C(w_{n-1}, w) + 1)} = \\frac{C(w_{n-1}, w_n) + 1}{C(w_{n-1}) + V} \\qquad\n\\tag{6}\nAdd-k smoothing is very similar:\n\nP(w_n | w_{n-1}) = \\frac{C(w_{n-1}, w_n) + k}{\\sum_{w \\in V} (C(w_{n-1}, w) + k)} =\\frac{C(w_{n-1}, w_n) + k}{C(w_{n-1}) + k\\times V} \\qquad\n\\tag{7}\n\n\n\n\n\n\n\nBackoff\n\n\n\n\nFigure 17: Backoff\n\n\nWhen using back-off:\n\nIf N-gram missing =&gt; use (N-1)-gram, …: Using the lower level N-grams (i.e. (N-1)-gram, (N-2)-gram, down to unigram) distorts the probability distribution. Especially for smaller corpora, some probability needs to be discounted from higher level N-grams to use it for lower level N-grams.\nProbability discounting e.g. Katz backoff: makes use of discounting.\n“Stupid” backoff: If the higher order N-gram probability is missing, the lower order N-gram probability is used, just multiplied by a constant. A constant of about 0.4 was experimentally shown to work well.\n\nHere is a visualization of the backoff process.\nYou can also use interpolation when computing probabilities as follows:\n\n\\hat{P}(w_n | w_{n-2} W_{n-1}) = \\lambda_1 \\times P(w_n | w_{n-2} w_{n-1}) + \\lambda_2 \\times P(w_n | w_{n-1}) + \\lambda_3 \\times P(w_n) \\qquad\n\\tag{8}\nWhere\n\n\\sum_i \\lambda_i = 1",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete and Language Models"
    ]
  },
  {
    "objectID": "posts/c2w3/index.html#week-summary",
    "href": "posts/c2w3/index.html#week-summary",
    "title": "Autocomplete and Language Models",
    "section": "Week Summary",
    "text": "Week Summary\nThis week you learned the following concepts\n\nN-Grams and probabilities\nApproximate sentence probability from N-Grams\nBuild a language model from a corpus\nFix missing information\nOut of vocabulary words with &lt;UNK&gt;\nMissing N-Gram in corpus with smoothing, backoff and interpolation\nEvaluate language model with perplexity\nCoding assignment!",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete and Language Models"
    ]
  },
  {
    "objectID": "posts/c2w3/index.html#lecture-notebook-language-model-generalization",
    "href": "posts/c2w3/index.html#lecture-notebook-language-model-generalization",
    "title": "Autocomplete and Language Models",
    "section": "Lecture notebook: Language model generalization",
    "text": "Lecture notebook: Language model generalization\nAutocomplete",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete and Language Models"
    ]
  },
  {
    "objectID": "posts/c3w4/index.html",
    "href": "posts/c3w4/index.html",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "",
    "text": "course banner\n\n\nresources:\n\n(Chadha 2020) Aman Chadha’s Notes\nIbrahim Jelliti’s Notes\n\n\n\n\n\n\n\nReferences\n\nChadha, Aman. 2020. “Distilled Notes for the Natural Language Processing Specialization on Coursera (Offered by Deeplearning.ai).” https://www.aman.ai. www.aman.ai.\n\nCitationBibTeX citation:@online{2020,\n  author = {},\n  title = {Neural {Networks} for {Sentiment} {Analysis}},\n  date = {2020-10-23},\n  url = {https://orenbochman.github.io/notes-nlp/posts/c3w4/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“Neural Networks for Sentiment Analysis.” 2020. October 23,\n2020. https://orenbochman.github.io/notes-nlp/posts/c3w4/.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Chat Bots"
    ]
  },
  {
    "objectID": "posts/c4w1/index.html",
    "href": "posts/c4w1/index.html",
    "title": "Week 1 Neural Machine Translation",
    "section": "",
    "text": "deeplearning.ai",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation"
    ]
  },
  {
    "objectID": "posts/c4w1/index.html#sec-sec-tldr-neural-network-engineering",
    "href": "posts/c4w1/index.html#sec-sec-tldr-neural-network-engineering",
    "title": "Week 1 Neural Machine Translation",
    "section": "TLDR: Neural Network engineering",
    "text": "TLDR: Neural Network engineering\n\nHow do we learn to align two sequences?\n\nExplain how an Encoder/Decoder model works\nApply word alignment for machine translation\nTrain a Neural Machine Translation model with Attention\nDevelop intuition for how teacher forcing helps a translation model checks its predictions\nUse BLEU score and ROUGE score to evaluate machine-generated text quality\nDescribe several decoding methods including MBR and Beam search",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation"
    ]
  },
  {
    "objectID": "posts/c4w1/index.html#sec-sec-tldr-neural-network-engineering-2",
    "href": "posts/c4w1/index.html#sec-sec-tldr-neural-network-engineering-2",
    "title": "Week 1 Neural Machine Translation",
    "section": "TLDR: Neural Network engineering",
    "text": "TLDR: Neural Network engineering\n\nHow to learn to align two sequences?\nUsing an tl.attention layer. While it doesn’t reorder the input, attention works by assigning to each item of the input a subset of the input.\n\n\nHow to utilize arbitrary functions on a TRAX Neural Net?\nUsing a functional layers\n    def Addition(): # this is a closure\n    \n        layer_name = \"Addition\"  # the name \n        # Custom function for the custom layer\n        def func(x, y):\n            return x + y\n    \n        return tl.Fn(layer_name, func) # returning an tl.Fn object with name and function\n\n\nHow to duplicate or delete input in TRAX neural networks?\nSimple serial architectures map input to output. When a more complex setup is needed like in the deep imagenets by google this is done using a tl.Select combinator. This is actually something I’ve seen in some papers on image processing papers before residual architectures became more popular - which is also covered in the next item.\ntl.Select([0,1,0,1])\nwhich pops items 0,1 from the stack and pushes them in twice. This replicating the inputs on the stack.\nNow lets suppose a layer gets 3 tensors + the output of anther layer. Which means there will be 4 tensors on the stack. To ignore say the second item we use select. But we also want to consume a layer’s output so we can indicate this using the second parameter which tell the select it has three in coming tensors.\ntl.Select([0,2],n_in=3)\n\n\nHow to make a residual connections in a TRAX Neural Nets?\nUse the tl.Residual combinator.\ntl.Residual( tl.The_Layer_To_Bypass() )\n\n\nHow to sample sequence states with noise AKA Temperature Based Sampling ?\nUse tl.logsoftmax_sample() to create Temperature Based Sampling or Greedy Decoding based on the temperature parameter.\nSetting temperature to 0 will return the maximal likelihood estimate - this is called Greedy Decoding. Larger values will add noise to the distribution allowing sampling of items with lower probabilities. the implementation is like this:\ndef logsoftmax_sample(log_probs, temperature=1.0):  \n\"\"\"Returns a sample from a log-softmax output, with temperature.\nArgs:\n    log_probs: Logarithms of probabilities (often coming from LogSofmax)\n    temperature: For scaling before sampling (1.0 = default, 0.0 = pick argmax)\n\"\"\"\n\n# This is equivalent to sampling from a softmax with temperature.\nu = np.random.uniform(low=1e-6, high=1.0 - 1e-6, size=log_probs.shape)\ng = -np.log(-np.log(u))\nreturn np.argmax(log_probs + g * temperature, axis=-1)",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation"
    ]
  },
  {
    "objectID": "posts/c4w1/index.html#sec-sec-additional-coding-notes",
    "href": "posts/c4w1/index.html#sec-sec-additional-coding-notes",
    "title": "Week 1 Neural Machine Translation",
    "section": "Additional coding notes:",
    "text": "Additional coding notes:\n\nHow to reshape a test Tensor so it has a (size 0) batch dimension at the front?\nThis is needed when inspecting single test inputs instead of working with a batch. The model is expecting to process batches of inputs like it saw during training - we therefore need to add a dimension at the start.\npadded_with_batch = fastnp.expand_dims(fastnp.array(padded),axis=0)\n\n# get log probabilities from the last token output\nlog_probs = output[0,-1,:] \n\n\nHow to use calculate Jacquard Similarity\nwhich is the intersection over union?\ndef jaccard_similarity(candidate, reference):\n    \"\"\"Returns the Jacquard similarity between two token lists\n    Args:\n        candidate (list of int): tokenized version of the candidate translation\n        reference (list of int): tokenized version of the reference translation\n    Returns:\n        float: overlap between the two token lists\n    \"\"\"\n    \n    # convert the lists to a set to get the unique tokens\n    can_unigram_set, ref_unigram_set = set(candidate), set(reference)  \n    \n    # get the set of tokens common to both candidate and reference\n    joint_elems = can_unigram_set.intersection(ref_unigram_set)\n    \n    # get the set of all tokens found in either candidate or reference\n    all_elems = can_unigram_set.union(ref_unigram_set)\n    \n    # divide the number of joint elements by the number of all elements\n    overlap = len(joint_elems) / len(all_elems)\n    \n    return overlap",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation"
    ]
  },
  {
    "objectID": "posts/c4w1/index.html#sec-sec-seq2seq-model",
    "href": "posts/c4w1/index.html#sec-sec-seq2seq-model",
    "title": "Week 1 Neural Machine Translation",
    "section": "Seq2Seq model",
    "text": "Seq2Seq model\n\nIntroduced by Google in 2014\nMaps variable-length sequences to fixed-length memory\nLSTMs and GRUs are typically used to overcome the vanishing gradient problem\n\n\n\n\n\nencoder decoder architecture\n\nTherefore, attention mechanisms have become critical for sequence modeling in various tasks, allowing modeling of dependencies without caring too much about their distance in the input or output sequences.\nin this encoder decoder architecture the yellow block in the middle is the final hidden state produced by the encoder. It’s essentials a compressed representation of the sequence in this case the English sentence. The problem with RNN is they tend to have a bias for representing more recent data.\nOne approach to overcome this issue is to provide the decoder with the attention layer.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation"
    ]
  },
  {
    "objectID": "posts/c4w1/index.html#sec-w1v4-attention",
    "href": "posts/c4w1/index.html#sec-w1v4-attention",
    "title": "Week 1 Neural Machine Translation",
    "section": "W1V4: Attention",
    "text": "W1V4: Attention\nThe attention mechanism uses encoded representations of both the input or the encoder hidden states and the outputs or the decoder hidden states. The keys and values are pairs. Both of dimension NN, where NN is the input sequence length and comes from the encoder hidden states. Keys and values have their own respective matrices, but the matrices have the same shape and are often the same. While the queries come from the decoder hidden states. One way you can think of it is as follows. Imagine that you are translating English into German. You can represent the word embeddings in the English language as keys and values. The queries will then be the German equivalent. You can then calculate the dot product between the query and the key. Note that similar vectors have higher dot products and non-similar vectors will have lower dot products. The intuition here is that you want to identify the corresponding words in the queries that are similar to the keys. This would allow your model to “look” or focus on the right place when translating each word.\nWe then run a Softmax:\n\nsoftmax(QK^T )  \n\\tag{1}\nThat gives a distribution of numbers between 0 and 1.\nWe then would multiply the output by V. Remember V in this example was the same as our keys, corresponding to the English word embeddings. Hence the equation becomes\n\nsoftmax(QK^T )V  \n {#sec-softmax-formula-2}\nIn the matrix, the lighter square shows where the model is actually looking when making the translation of that word. This mapping isn’t necessarily be one to one. The lighting just tells you to what extent is each word contributing to the input that’s fed into the decoder. As you can see several words can contribute to translating another word, depending on the weights (output) of the softmax that we use to create the new input. a picture of attention in translation with English to German An important thing to keep in mind is that the model should be flexible enough to connect each English word with its relevant German word, even if they don’t appear in the same position in their respective sentences. In other words, it should be flexible enough to handle differences in grammar and word ordering in different languages.\nIn a situation like the one just mentioned, where the grammar of foreign language requires a difference word order than the other, the attention is so flexible enough to find the connection. The first four tokens, the agreements on the, are pretty straightforward, but then the grammatical structure between French and English changes. Now instead of looking at the corresponding fifth token to translate the French word zone, the attention knows to look further down at the eighth token, which corresponds to the English word area, glorious and necessary. It’s pretty amazing, was a little matrix multiplication can do.\nSo attention is a layer of calculations that let your model focus on the most important parts of the sequence for each step. Queries, values, and keys are representations of the encoder and decoder hidden states. And they’re used to retrieve information inside the attention layer by calculating the similarity between the decoder queries and the encoder key- value pairs.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation"
    ]
  },
  {
    "objectID": "posts/c4w1/index.html#sec-evaluation-metrics",
    "href": "posts/c4w1/index.html#sec-evaluation-metrics",
    "title": "Week 1 Neural Machine Translation",
    "section": "Evaluation metrics for Machine Translation",
    "text": "Evaluation metrics for Machine Translation\n\nBLEU\n\nThe authors of (Papineni et al. 2002) introduced the BLEU score.\nThe closer the BLEU score is to 1, the better a model preforms.\nThe closer to 0, the worse it does.\n\nTo get the BLEU score, the candidates and the references are usually based on an average of unigrams, bigrams, trigrams or even four-gram precision. For example using uni-grams:\n\n\n\n\nscreenshot_of_outline_slide\n\nYou would sum over the unique n-gram counts in the candidate and divide by the total number of words in the candidate. The same concept could apply to unigrams, bigrams, etc. One issue with the BLEU score is that it doesn’t take into account semantics, so it doesn’t take into account the order of the n-grams in the sentence.\n\nBLEU = BP\\Bigl(\\prod_{i=1}^{4}\\text{precision}_i\\Bigr)^{(1/4)}\n\\tag{2}\nwith the Brevity Penalty and precision defined as:\n\nBP = min\\Bigl(1, e^{(1-({ref}/{cand}))}\\Bigr)\n\\tag{3}\n\n\\text{Precision}_i = \\frac {\\sum_{snt \\in{cand}}\\sum_{i\\in{snt}}min\\Bigl(m^{i}_{cand}, m^{i}_{ref}\\Bigr)}{w^{i}_{t}}\n\\tag{4}\nwhere:\n\nm^{i}_{cand}, is the count of i-gram in candidate matching the reference translation.\nm^{i}_{ref}, is the count of i-gram in the reference translation.\nw^{i}_{t}, is the total number of i-grams in candidate translation.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation"
    ]
  },
  {
    "objectID": "posts/c4w1/index.html#sec-rouge",
    "href": "posts/c4w1/index.html#sec-rouge",
    "title": "Week 1 Neural Machine Translation",
    "section": "ROUGE",
    "text": "ROUGE\n(Lin 2004) introduced a similar method for evaluation called the ROUGE score which calculates precision and recall for machine texts by counting the n-gram overlap between the machine texts and a reference text. Here is an example that calculates recall: \n\nRouge_{recall} = \\sum  \\frac{(\\{\\text{prediction n-grams}\\} \\cap \\{ \\text{test n-grams}\\})}{\\vert{ \\text{test n-grams}}\\vert }\n\\tag{5}\nRouge also allows you to compute precision as follows:\n\n\n\n\nprecision in ROUGE\n\n \\text{ROUGE}_{\\text{precision}} = \\sum \\frac{(\\{\\text{prediction n-grams}\\} \\cap \\{ \\text{test ngrams}\\})}{\\vert\\{ \\text{vocab}\\}\\vert}\n\\tag{6}\nThe ROUGE-N refers to the overlap of N-grams between the actual system and the reference summaries. The F-score metric combines Recall and precision into one metric.\n\nF_{score}= 2 \\times \\frac{(\\text{precision} \\times \\text{recall})}{(\\text{precision} + \\text{recall})}\n\\tag{7}",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation"
    ]
  },
  {
    "objectID": "posts/c4w1/index.html#sec-random-sampling",
    "href": "posts/c4w1/index.html#sec-random-sampling",
    "title": "Week 1 Neural Machine Translation",
    "section": "Random sampling",
    "text": "Random sampling\nRandom sampling for decoding involves drawing a word from the softmax distribution. To explore the latent space it is possible to introduce a temperature variable which controls the randomness of the sample.\ndef logsoftmax_sample(log_probs, temperature=1.0):  \n  \"\"\"Returns a sample from a log-softmax output, with temperature.\n  Args:\n    log_probs: Logarithms of probabilities (often coming from LogSofmax)\n    temperature: For scaling before sampling (1.0 = default, 0.0 = pick argmax)\n  \"\"\"\n  # This is equivalent to sampling from a softmax with temperature.\n  u = np.random.uniform(low=1e-6, high=1.0 - 1e-6, size=log_probs.shape)\n  g = -np.log(-np.log(u))\n  return np.argmax(log_probs + g * temperature, axis=-1)",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation"
    ]
  },
  {
    "objectID": "posts/c4w1/index.html#sec-beam-search",
    "href": "posts/c4w1/index.html#sec-beam-search",
    "title": "Week 1 Neural Machine Translation",
    "section": "Beam Search",
    "text": "Beam Search\nThe beam search algorithm is a limited (best-first search). The parameter for the beam width limits the choices considered at each step.\n\n\n\n\nBeam Search",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation"
    ]
  },
  {
    "objectID": "posts/c4w1/index.html#sec-minimum-bayes-risk",
    "href": "posts/c4w1/index.html#sec-minimum-bayes-risk",
    "title": "Week 1 Neural Machine Translation",
    "section": "Minimum Bayes Risk",
    "text": "Minimum Bayes Risk\nMBR (Minimum Bayes Risk) Compares many samples against one another. To implement MBR:\n\nGenerate several random samples.\nCompare each sample against all the others and assign a similarity score using ROUGE.\nSelect the sample with the highest similarity: the golden one.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation"
    ]
  },
  {
    "objectID": "posts/c4w1/index.html#sec-summary",
    "href": "posts/c4w1/index.html#sec-summary",
    "title": "Week 1 Neural Machine Translation",
    "section": "Summary",
    "text": "Summary\n\nMaximal Probability is a baseline - but not a particularly good one when the data is noisy.\nRandom sampling with temperature is better.\nBeam search uses conditional probabilities and the parameter.\nMBR takes several samples and compares them against each other to find the golden one.\n\nnote: although not mentioned in the next week’s notes Beam Search is useful for improving the summarization task. We can extract a golden summary from a number of samples using MBR. ROUGE-N is the preferred metric for evaluating summarization\n\nReferences\n\n-​ (Peters et al. 2018)\n(Alammar 2024)\n\n\n\nAlammar, Jay. 2024. “The Illustrated Transformer.” https://jalammar.github.io/illustrated-transformer.\n\n\nLin, Chin-Yew. 2004. “ROUGE: A Package for Automatic Evaluation of Summaries.” In Text Summarization Branches Out, 74–81. Barcelona, Spain: Association for Computational Linguistics. https://aclanthology.org/W04-1013.\n\n\nPapineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. “Bleu: A Method for Automatic Evaluation of Machine Translation.” In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, 311–18. https://www.aclweb.org/anthology/P02-1040.pdf.\n\n\nPeters, Matthew E., Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. “Deep Contextualized Word Representations.” CoRR abs/1802.05365. http://arxiv.org/abs/1802.05365.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation"
    ]
  },
  {
    "objectID": "posts/c2w4/assignment.html",
    "href": "posts/c2w4/assignment.html",
    "title": "Assignment 4: Word Embeddings",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure 1\nWelcome to the fourth (and last) programming assignment of Course 2!\nIn this assignment, you will practice how to compute word embeddings and use them for sentiment analysis. - To implement sentiment analysis, you can go beyond counting the number of positive words and negative words. - You can find a way to represent each word numerically, by a vector. - The vector could then represent syntactic (i.e. parts of speech) and semantic (i.e. meaning) structures.\nIn this assignment, you will explore a classic way of generating word embeddings or representations. - You will implement a famous model called the continuous bag of words (CBOW) model.\nBy completing this assignment you will:\nKnowing how to train these models will give you a better understanding of word vectors, which are building blocks to many applications in natural language processing."
  },
  {
    "objectID": "posts/c2w4/assignment.html#outline",
    "href": "posts/c2w4/assignment.html#outline",
    "title": "Assignment 4: Word Embeddings",
    "section": "Outline",
    "text": "Outline\n\n1 The Continuous bag of words model\n2 Training the Model\n\n2.0 Initialize the model\n\nExercise 01\n\n2.1 Softmax Function\n\nExercise 02\n\n2.2 Forward Propagation\n\nExercise 03\n\n2.3 Cost Function\n2.4 Backproagation\n\nExercise 04\n\n2.5 Gradient Descent\n\nExercise 05\n\n\n3 Visualizing the word vectors\n\n # 1. The Continuous bag of words model\nLet’s take a look at the following sentence: &gt;‘I am happy because I am learning’.\n\nIn continuous bag of words (CBOW) modeling, we try to predict the center word given a few context words (the words around the center word).\nFor example, if you were to choose a context half-size of say C = 2, then you would try to predict the word happy given the context that includes 2 words before and 2 words after the center word:\n\n\nC words before: [I, am]\n\n\nC words after: [because, I]\n\n\nIn other words:\n\ncontext = [I,am, because, I] target = happy\nThe structure of your model will look like this:\n\n\n\n\n\n\n\ncourse banner\n\n\n\n\nFigure 2: Figure 1\n\n\nWhere \\bar x is the average of all the one hot vectors of the context words.\n\n\n\n\n\n\n\ncourse banner\n\n\n\n\nFigure 3: Figure 2\n\n\nOnce you have encoded all the context words, you can use \\bar x as the input to your model.\nThe architecture you will be implementing is as follows:\n\\begin{align}\nh &= W_1 \\  X + b_1  \\tag{1} \\\\\na &= ReLU(h)  \\tag{2} \\\\\nz &= W_2 \\  a + b_2   \\tag{3} \\\\\n\\hat y &= softmax(z)   \\tag{4} \\\\\n\\end{align}\n\n# Import Python libraries and helper functions (in utils2) \nimport nltk\nfrom nltk.tokenize import word_tokenize\nimport numpy as np\nfrom collections import Counter\nfrom utils2 import sigmoid, get_batches, compute_pca, get_dict\n\n\n# Download sentence tokenizer\nnltk.data.path.append('.')\n\n\n# Load, tokenize and process the data\nimport re                                                           #  Load the Regex-modul\nwith open('shakespeare.txt') as f:\n    data = f.read()                                                 #  Read in the data\ndata = re.sub(r'[,!?;-]', '.',data)                                 #  Punktuations are replaced by .\ndata = nltk.word_tokenize(data)                                     #  Tokenize string to words\ndata = [ ch.lower() for ch in data if ch.isalpha() or ch == '.']    #  Lower case and drop non-alphabetical tokens\nprint(\"Number of tokens:\", len(data),'\\n', data[:15])               #  print data sample\n\nNumber of tokens: 61011 \n ['o', 'for', 'a', 'muse', 'of', 'fire', '.', 'that', 'would', 'ascend', 'the', 'brightest', 'heaven', 'of', 'invention']\n\n\n\n# Compute the frequency distribution of the words in the dataset (vocabulary)\nfdist = nltk.FreqDist(word for word in data)\nprint(\"Size of vocabulary: \",len(fdist) )\nprint(\"Most frequent tokens: \",fdist.most_common(20) ) # print the 20 most frequent words and their freq.\n\nSize of vocabulary:  5784\nMost frequent tokens:  [('.', 9630), ('the', 1521), ('and', 1394), ('i', 1259), ('to', 1159), ('of', 1093), ('my', 857), ('that', 781), ('in', 771), ('a', 752), ('you', 748), ('is', 630), ('not', 559), ('for', 467), ('it', 460), ('with', 441), ('his', 434), ('but', 417), ('me', 417), ('your', 397)]\n\n\n\nMapping words to indices and indices to words\nWe provide a helper function to create a dictionary that maps words to indices and indices to words.\n\n# get_dict creates two dictionaries, converting words to indices and viceversa.\nword2Ind, Ind2word = get_dict(data)\nV = len(word2Ind)\nprint(\"Size of vocabulary: \", V)\n\nSize of vocabulary:  5784\n\n\n\n# example of word to index mapping\nprint(\"Index of the word 'king' :  \",word2Ind['king'] )\nprint(\"Word which has index 2743:  \",Ind2word[2743] )\n\nIndex of the word 'king' :   2747\nWord which has index 2743:   kind\n\n\n # 2 Training the Model\n\n\nInitializing the model\nYou will now initialize two matrices and two vectors. - The first matrix (W_1) is of dimension N \\times V, where V is the number of words in your vocabulary and N is the dimension of your word vector. - The second matrix (W_2) is of dimension V \\times N. - Vector b_1 has dimensions N\\times 1 - Vector b_2 has dimensions V\\times 1. - b_1 and b_2 are the bias vectors of the linear layers from matrices W_1 and W_2.\nThe overall structure of the model will look as in Figure 1, but at this stage we are just initializing the parameters.\n ### Exercise 01 Please use numpy.random.rand to generate matrices that are initialized with random values from a uniform distribution, ranging between 0 and 1.\nNote: In the next cell you will encounter a random seed. Please DO NOT modify this seed so your solution can be tested correctly.\n\n# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: initialize_model\ndef initialize_model(N,V, random_seed=1):\n    '''\n    Inputs: \n        N:  dimension of hidden vector \n        V:  dimension of vocabulary\n        random_seed: random seed for consistent results in the unit tests\n     Outputs: \n        W1, W2, b1, b2: initialized weights and biases\n    '''\n    \n    np.random.seed(random_seed)\n    \n    ### START CODE HERE (Replace instances of 'None' with your code) ###\n    # W1 has shape (N,V)\n    W1 = None\n    # W2 has shape (V,N)\n    W2 = None\n    # b1 has shape (N,1)\n    b1 = None\n    # b2 has shape (V,1)\n    b2 = None\n    ### END CODE HERE ###\n\n    return W1, W2, b1, b2\n\n\n# Test your function example.\ntmp_N = 4\ntmp_V = 10\ntmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\nassert tmp_W1.shape == ((tmp_N,tmp_V))\nassert tmp_W2.shape == ((tmp_V,tmp_N))\nprint(f\"tmp_W1.shape: {tmp_W1.shape}\")\nprint(f\"tmp_W2.shape: {tmp_W2.shape}\")\nprint(f\"tmp_b1.shape: {tmp_b1.shape}\")\nprint(f\"tmp_b2.shape: {tmp_b2.shape}\")\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[8], line 5\n      3 tmp_V = 10\n      4 tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n----&gt; 5 assert tmp_W1.shape == ((tmp_N,tmp_V))\n      6 assert tmp_W2.shape == ((tmp_V,tmp_N))\n      7 print(f\"tmp_W1.shape: {tmp_W1.shape}\")\n\nAttributeError: 'NoneType' object has no attribute 'shape'\n\n\n\n\nExpected Output\ntmp_W1.shape: (4, 10)\ntmp_W2.shape: (10, 4)\ntmp_b1.shape: (4, 1)\ntmp_b2.shape: (10, 1)\n ### 2.1 Softmax Before we can start training the model, we need to implement the softmax function as defined in equation 5:\n  \\text{softmax}(z_i) = \\frac{e^{z_i} }{\\sum_{i=0}^{V-1} e^{z_i} }  \\tag{5} \n\nArray indexing in code starts at 0.\nV is the number of words in the vocabulary (which is also the number of rows of z).\ni goes from 0 to |V| - 1.\n\n ### Exercise 02 Instructions: Implement the softmax function below.\n\nAssume that the input z to softmax is a 2D array\nEach training example is represented by a column of shape (V, 1) in this 2D array.\nThere may be more than one column, in the 2D array, because you can put in a batch of examples to increase efficiency. Let’s call the batch size lowercase m, so the z array has shape (V, m)\nWhen taking the sum from i=1 \\cdots V-1, take the sum for each column (each example) separately.\n\nPlease use - numpy.exp - numpy.sum (set the axis so that you take the sum of each column in z)\n\n# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: softmax\ndef softmax(z):\n    '''\n    Inputs: \n        z: output scores from the hidden layer\n    Outputs: \n        yhat: prediction (estimate of y)\n    '''\n    \n    ### START CODE HERE (Replace instances of 'None' with your own code) ###\n    \n    # Calculate yhat (softmax)\n    yhat = None\n    \n    ### END CODE HERE ###\n    \n    return yhat\n\n\n# Test the function\ntmp = np.array([[1,2,3],\n                [1,1,1]\n               ])\ntmp_sm = softmax(tmp)\ndisplay(tmp_sm)\n\nNone\n\n\n\n\nExpected Ouput\narray([[0.5       , 0.73105858, 0.88079708],\n       [0.5       , 0.26894142, 0.11920292]])\n ### 2.2 Forward propagation\n ### Exercise 03 Implement the forward propagation z according to equations (1) to (3). \n\\begin{align}\nh &= W_1 \\  X + b_1  \\tag{1} \\\\\na &= ReLU(h)  \\tag{2} \\\\\nz &= W_2 \\  a + b_2   \\tag{3} \\\\\n\\end{align}\nFor that, you will use as activation the Rectified Linear Unit (ReLU) given by:\nf(h)=\\max (0,h) \\tag{6}\n\n\nHints\n\n\n\n\nYou can use numpy.maximum(x1,x2) to get the maximum of two values\n\n\nUse numpy.dot(A,B) to matrix multiply A and B\n\n\n\n\n# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: forward_prop\ndef forward_prop(x, W1, W2, b1, b2):\n    '''\n    Inputs: \n        x:  average one hot vector for the context \n        W1, W2, b1, b2:  matrices and biases to be learned\n     Outputs: \n        z:  output score vector\n    '''\n    \n    ### START CODE HERE (Replace instances of 'None' with your own code) ###\n    \n    # Calculate h\n    h = None\n    \n    # Apply the relu on h (store result in h)\n    h = None\n    \n    # Calculate z\n    z = None\n    \n    ### END CODE HERE ###\n\n    return z, h\n\n\n# Test the function\n\n# Create some inputs\ntmp_N = 2\ntmp_V = 3\ntmp_x = np.array([[0,1,0]]).T\ntmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(N=tmp_N,V=tmp_V, random_seed=1)\n\nprint(f\"x has shape {tmp_x.shape}\")\nprint(f\"N is {tmp_N} and vocabulary size V is {tmp_V}\")\n\n# call function\ntmp_z, tmp_h = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\nprint(\"call forward_prop\")\nprint()\n# Look at output\nprint(f\"z has shape {tmp_z.shape}\")\nprint(\"z has values:\")\nprint(tmp_z)\n\nprint()\n\nprint(f\"h has shape {tmp_h.shape}\")\nprint(\"h has values:\")\nprint(tmp_h)\n\nx has shape (3, 1)\nN is 2 and vocabulary size V is 3\ncall forward_prop\n\n\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[12], line 17\n     15 print()\n     16 # Look at output\n---&gt; 17 print(f\"z has shape {tmp_z.shape}\")\n     18 print(\"z has values:\")\n     19 print(tmp_z)\n\nAttributeError: 'NoneType' object has no attribute 'shape'\n\n\n\n\n\nExpected output\nx has shape (3, 1)\nN is 2 and vocabulary size V is 3\ncall forward_prop\n\nz has shape (3, 1)\nz has values:\n[[0.55379268]\n [1.58960774]\n [1.50722933]]\n\nh has shape (2, 1)\nh has values:\n[[0.92477674]\n [1.02487333]]\n ## 2.3 Cost function\n\nWe have implemented the cross-entropy cost function for you.\n\n\n# compute_cost: cross-entropy cost functioN\ndef compute_cost(y, yhat, batch_size):\n    # cost function \n    logprobs = np.multiply(np.log(yhat),y) + np.multiply(np.log(1 - yhat), 1 - y)\n    cost = - 1/batch_size * np.sum(logprobs)\n    cost = np.squeeze(cost)\n    return cost\n\n\n# Test the function\ntmp_C = 2\ntmp_N = 50\ntmp_batch_size = 4\ntmp_word2Ind, tmp_Ind2word = get_dict(data)\ntmp_V = len(word2Ind)\n\ntmp_x, tmp_y = next(get_batches(data, tmp_word2Ind, tmp_V,tmp_C, tmp_batch_size))\n        \nprint(f\"tmp_x.shape {tmp_x.shape}\")\nprint(f\"tmp_y.shape {tmp_y.shape}\")\n\ntmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n\nprint(f\"tmp_W1.shape {tmp_W1.shape}\")\nprint(f\"tmp_W2.shape {tmp_W2.shape}\")\nprint(f\"tmp_b1.shape {tmp_b1.shape}\")\nprint(f\"tmp_b2.shape {tmp_b2.shape}\")\n\ntmp_z, tmp_h = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\nprint(f\"tmp_z.shape: {tmp_z.shape}\")\nprint(f\"tmp_h.shape: {tmp_h.shape}\")\n\ntmp_yhat = softmax(tmp_z)\nprint(f\"tmp_yhat.shape: {tmp_yhat.shape}\")\n\ntmp_cost = compute_cost(tmp_y, tmp_yhat, tmp_batch_size)\nprint(\"call compute_cost\")\nprint(f\"tmp_cost {tmp_cost:.4f}\")\n\ntmp_x.shape (5784, 4)\ntmp_y.shape (5784, 4)\n\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[14], line 15\n     11 print(f\"tmp_y.shape {tmp_y.shape}\")\n     13 tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n---&gt; 15 print(f\"tmp_W1.shape {tmp_W1.shape}\")\n     16 print(f\"tmp_W2.shape {tmp_W2.shape}\")\n     17 print(f\"tmp_b1.shape {tmp_b1.shape}\")\n\nAttributeError: 'NoneType' object has no attribute 'shape'\n\n\n\n\n\nExpected output\ntmp_x.shape (5778, 4)\ntmp_y.shape (5778, 4)\ntmp_W1.shape (50, 5778)\ntmp_W2.shape (5778, 50)\ntmp_b1.shape (50, 1)\ntmp_b2.shape (5778, 1)\ntmp_z.shape: (5778, 4)\ntmp_h.shape: (50, 4)\ntmp_yhat.shape: (5778, 4)\ncall compute_cost\ntmp_cost 9.9560\n ## 2.4 Training the Model - Backpropagation\n ### Exercise 04 Now that you have understood how the CBOW model works, you will train it.  You created a function for the forward propagation. Now you will implement a function that computes the gradients to backpropagate the errors.\n\n# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: back_prop\ndef back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size):\n    '''\n    Inputs: \n        x:  average one hot vector for the context \n        yhat: prediction (estimate of y)\n        y:  target vector\n        h:  hidden vector (see eq. 1)\n        W1, W2, b1, b2:  matrices and biases  \n        batch_size: batch size \n     Outputs: \n        grad_W1, grad_W2, grad_b1, grad_b2:  gradients of matrices and biases   \n    '''\n    ### START CODE HERE (Replace instanes of 'None' with your code) ###\n    \n    # Compute l1 as W2^T (Yhat - Y)\n    # Re-use it whenever you see W2^T (Yhat - Y) used to compute a gradient\n    l1 = None\n    # Apply relu to l1\n    l1 = None\n    # Compute the gradient of W1\n    grad_W1 = None\n    # Compute the gradient of W2\n    grad_W2 = None\n    # Compute the gradient of b1\n    grad_b1 = None\n    # Compute the gradient of b2\n    grad_b2 = None\n    ### END CODE HERE ###\n    \n    return grad_W1, grad_W2, grad_b1, grad_b2\n\n\n# Test the function\ntmp_C = 2\ntmp_N = 50\ntmp_batch_size = 4\ntmp_word2Ind, tmp_Ind2word = get_dict(data)\ntmp_V = len(word2Ind)\n\n# get a batch of data\ntmp_x, tmp_y = next(get_batches(data, tmp_word2Ind, tmp_V,tmp_C, tmp_batch_size))\n\nprint(\"get a batch of data\")\nprint(f\"tmp_x.shape {tmp_x.shape}\")\nprint(f\"tmp_y.shape {tmp_y.shape}\")\n\nprint()\nprint(\"Initialize weights and biases\")\ntmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n\nprint(f\"tmp_W1.shape {tmp_W1.shape}\")\nprint(f\"tmp_W2.shape {tmp_W2.shape}\")\nprint(f\"tmp_b1.shape {tmp_b1.shape}\")\nprint(f\"tmp_b2.shape {tmp_b2.shape}\")\n\nprint()\nprint(\"Forwad prop to get z and h\")\ntmp_z, tmp_h = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\nprint(f\"tmp_z.shape: {tmp_z.shape}\")\nprint(f\"tmp_h.shape: {tmp_h.shape}\")\n\nprint()\nprint(\"Get yhat by calling softmax\")\ntmp_yhat = softmax(tmp_z)\nprint(f\"tmp_yhat.shape: {tmp_yhat.shape}\")\n\ntmp_m = (2*tmp_C)\ntmp_grad_W1, tmp_grad_W2, tmp_grad_b1, tmp_grad_b2 = back_prop(tmp_x, tmp_yhat, tmp_y, tmp_h, tmp_W1, tmp_W2, tmp_b1, tmp_b2, tmp_batch_size)\n\nprint()\nprint(\"call back_prop\")\nprint(f\"tmp_grad_W1.shape {tmp_grad_W1.shape}\")\nprint(f\"tmp_grad_W2.shape {tmp_grad_W2.shape}\")\nprint(f\"tmp_grad_b1.shape {tmp_grad_b1.shape}\")\nprint(f\"tmp_grad_b2.shape {tmp_grad_b2.shape}\")\n\nget a batch of data\ntmp_x.shape (5784, 4)\ntmp_y.shape (5784, 4)\n\nInitialize weights and biases\n\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[16], line 19\n     16 print(\"Initialize weights and biases\")\n     17 tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n---&gt; 19 print(f\"tmp_W1.shape {tmp_W1.shape}\")\n     20 print(f\"tmp_W2.shape {tmp_W2.shape}\")\n     21 print(f\"tmp_b1.shape {tmp_b1.shape}\")\n\nAttributeError: 'NoneType' object has no attribute 'shape'\n\n\n\n\n\nExpected output\nget a batch of data\ntmp_x.shape (5778, 4)\ntmp_y.shape (5778, 4)\n\nInitialize weights and biases\ntmp_W1.shape (50, 5778)\ntmp_W2.shape (5778, 50)\ntmp_b1.shape (50, 1)\ntmp_b2.shape (5778, 1)\n\nForwad prop to get z and h\ntmp_z.shape: (5778, 4)\ntmp_h.shape: (50, 4)\n\nGet yhat by calling softmax\ntmp_yhat.shape: (5778, 4)\n\ncall back_prop\ntmp_grad_W1.shape (50, 5778)\ntmp_grad_W2.shape (5778, 50)\ntmp_grad_b1.shape (50, 1)\ntmp_grad_b2.shape (5778, 1)\n ## Gradient Descent\n ### Exercise 05 Now that you have implemented a function to compute the gradients, you will implement batch gradient descent over your training set.\nHint: For that, you will use initialize_model and the back_prop functions which you just created (and the compute_cost function). You can also use the provided get_batches helper function:\nfor x, y in get_batches(data, word2Ind, V, C, batch_size):\n...\nAlso: print the cost after each batch is processed (use batch size = 128)\n\n# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: gradient_descent\ndef gradient_descent(data, word2Ind, N, V, num_iters, alpha=0.03):\n    \n    '''\n    This is the gradient_descent function\n    \n      Inputs: \n        data:      text\n        word2Ind:  words to Indices\n        N:         dimension of hidden vector  \n        V:         dimension of vocabulary \n        num_iters: number of iterations  \n     Outputs: \n        W1, W2, b1, b2:  updated matrices and biases   \n\n    '''\n    W1, W2, b1, b2 = initialize_model(N,V, random_seed=282)\n    batch_size = 128\n    iters = 0\n    C = 2\n    for x, y in get_batches(data, word2Ind, V, C, batch_size):\n        ### START CODE HERE (Replace instances of 'None' with your own code) ###\n        # Get z and h\n        z, h = None\n        # Get yhat\n        yhat = None\n        # Get cost\n        cost = None\n        if ( (iters+1) % 10 == 0):\n            print(f\"iters: {iters + 1} cost: {cost:.6f}\")\n        # Get gradients\n        grad_W1, grad_W2, grad_b1, grad_b2 = None\n        \n        # Update weights and biases\n        W1 = None \n        W2 = None\n        b1 = None\n        b2 = None\n        \n        ### END CODE HERE ###\n        \n        iters += 1 \n        if iters == num_iters: \n            break\n        if iters % 100 == 0:\n            alpha *= 0.66\n            \n    return W1, W2, b1, b2\n\n\n# test your function\nC = 2\nN = 50\nword2Ind, Ind2word = get_dict(data)\nV = len(word2Ind)\nnum_iters = 150\nprint(\"Call gradient_descent\")\nW1, W2, b1, b2 = gradient_descent(data, word2Ind, N, V, num_iters)\n\nCall gradient_descent\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[18], line 8\n      6 num_iters = 150\n      7 print(\"Call gradient_descent\")\n----&gt; 8 W1, W2, b1, b2 = gradient_descent(data, word2Ind, N, V, num_iters)\n\nCell In[17], line 25, in gradient_descent(data, word2Ind, N, V, num_iters, alpha)\n     21 C = 2\n     22 for x, y in get_batches(data, word2Ind, V, C, batch_size):\n     23     ### START CODE HERE (Replace instances of 'None' with your own code) ###\n     24     # Get z and h\n---&gt; 25     z, h = None\n     26     # Get yhat\n     27     yhat = None\n\nTypeError: cannot unpack non-iterable NoneType object\n\n\n\n\n\nExpected Output\niters: 10 cost: 0.789141\niters: 20 cost: 0.105543\niters: 30 cost: 0.056008\niters: 40 cost: 0.038101\niters: 50 cost: 0.028868\niters: 60 cost: 0.023237\niters: 70 cost: 0.019444\niters: 80 cost: 0.016716\niters: 90 cost: 0.014660\niters: 100 cost: 0.013054\niters: 110 cost: 0.012133\niters: 120 cost: 0.011370\niters: 130 cost: 0.010698\niters: 140 cost: 0.010100\niters: 150 cost: 0.009566\nYour numbers may differ a bit depending on which version of Python you’re using.\n ## 3.0 Visualizing the word vectors\nIn this part you will visualize the word vectors trained using the function you just coded above.\n\n# visualizing the word vectors here\nfrom matplotlib import pyplot\n%config InlineBackend.figure_format = 'svg'\nwords = ['king', 'queen','lord','man', 'woman','dog','wolf',\n         'rich','happy','sad']\n\nembs = (W1.T + W2)/2.0\n \n# given a list of words and the embeddings, it returns a matrix with all the embeddings\nidx = [word2Ind[word] for word in words]\nX = embs[idx, :]\nprint(X.shape, idx)  # X.shape:  Number of words of dimension N each \n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[19], line 7\n      3 get_ipython().run_line_magic('config', \"InlineBackend.figure_format = 'svg'\")\n      4 words = ['king', 'queen','lord','man', 'woman','dog','wolf',\n      5          'rich','happy','sad']\n----&gt; 7 embs = (W1.T + W2)/2.0\n      9 # given a list of words and the embeddings, it returns a matrix with all the embeddings\n     10 idx = [word2Ind[word] for word in words]\n\nNameError: name 'W1' is not defined\n\n\n\n\nresult= compute_pca(X, 2)\npyplot.scatter(result[:, 0], result[:, 1])\nfor i, word in enumerate(words):\n    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\npyplot.show()\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[20], line 1\n----&gt; 1 result= compute_pca(X, 2)\n      2 pyplot.scatter(result[:, 0], result[:, 1])\n      3 for i, word in enumerate(words):\n\nNameError: name 'X' is not defined\n\n\n\nYou can see that man and king are next to each other. However, we have to be careful with the interpretation of this projected word vectors, since the PCA depends on the projection – as shown in the following illustration.\n\nresult= compute_pca(X, 4)\npyplot.scatter(result[:, 3], result[:, 1])\nfor i, word in enumerate(words):\n    pyplot.annotate(word, xy=(result[i, 3], result[i, 1]))\npyplot.show()\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[21], line 1\n----&gt; 1 result= compute_pca(X, 4)\n      2 pyplot.scatter(result[:, 3], result[:, 1])\n      3 for i, word in enumerate(words):\n\nNameError: name 'X' is not defined"
  },
  {
    "objectID": "posts/c2w4/lab03.html",
    "href": "posts/c2w4/lab03.html",
    "title": "Word Embeddings: Training the CBOW model",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure 1\nIn previous lecture notebooks you saw how to prepare data before feeding it to a continuous bag-of-words model, the model itself, its architecture and activation functions. This notebook will walk you through:\nWhich are concepts necessary to understand how the training of the model works.\nLet’s dive into it!\nimport numpy as np\nfrom utils2 import get_dict"
  },
  {
    "objectID": "posts/c2w4/lab03.html#forward-propagation",
    "href": "posts/c2w4/lab03.html#forward-propagation",
    "title": "Word Embeddings: Training the CBOW model",
    "section": "Forward propagation",
    "text": "Forward propagation\nLet’s dive into the neural network itself, which is shown below with all the dimensions and formulas you’ll need.\n\n Figure 2\n\nSet N equal to 3. Remember that N is a hyperparameter of the CBOW model that represents the size of the word embedding vectors, as well as the size of the hidden layer.\nAlso set V equal to 5, which is the size of the vocabulary we have used so far.\n\n# Define the size of the word embedding vectors and save it in the variable 'N'\nN = 3\n\n# Define V. Remember this was the size of the vocabulary in the previous lecture notebooks\nV = 5\n\n\nInitialization of the weights and biases\nBefore you start training the neural network, you need to initialize the weight matrices and bias vectors with random values.\nIn the assignment you will implement a function to do this yourself using numpy.random.rand. In this notebook, we’ve pre-populated these matrices and vectors for you.\n\n# Define first matrix of weights\nW1 = np.array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n               [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n               [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n\n# Define second matrix of weights\nW2 = np.array([[-0.22182064, -0.43008631,  0.13310965],\n               [ 0.08476603,  0.08123194,  0.1772054 ],\n               [ 0.1871551 , -0.06107263, -0.1790735 ],\n               [ 0.07055222, -0.02015138,  0.36107434],\n               [ 0.33480474, -0.39423389, -0.43959196]])\n\n# Define first vector of biases\nb1 = np.array([[ 0.09688219],\n               [ 0.29239497],\n               [-0.27364426]])\n\n# Define second vector of biases\nb2 = np.array([[ 0.0352008 ],\n               [-0.36393384],\n               [-0.12775555],\n               [-0.34802326],\n               [-0.07017815]])\n\nCheck that the dimensions of these matrices match those shown in the figure above.\n\nprint(f'V (vocabulary size): {V}')\nprint(f'N (embedding size / size of the hidden layer): {N}')\nprint(f'size of W1: {W1.shape} (NxV)')\nprint(f'size of b1: {b1.shape} (Nx1)')\nprint(f'size of W2: {W2.shape} (VxN)')\nprint(f'size of b2: {b2.shape} (Vx1)')\n\nV (vocabulary size): 5\nN (embedding size / size of the hidden layer): 3\nsize of W1: (3, 5) (NxV)\nsize of b1: (3, 1) (Nx1)\nsize of W2: (5, 3) (VxN)\nsize of b2: (5, 1) (Vx1)\n\n\nBefore moving forward, you will need some functions and variables defined in previous notebooks. They can be found next. Be sure you understand everything that is going on in the next cell, if not consider doing a refresh of the first lecture notebook.\n\n# Define the tokenized version of the corpus\nwords = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n\n# Get 'word2Ind' and 'Ind2word' dictionaries for the tokenized corpus\nword2Ind, Ind2word = get_dict(words)\n\n# Define the 'get_windows' function as seen in a previous notebook\ndef get_windows(words, C):\n    i = C\n    while i &lt; len(words) - C:\n        center_word = words[i]\n        context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n        yield context_words, center_word\n        i += 1\n\n# Define the 'word_to_one_hot_vector' function as seen in a previous notebook\ndef word_to_one_hot_vector(word, word2Ind, V):\n    one_hot_vector = np.zeros(V)\n    one_hot_vector[word2Ind[word]] = 1\n    return one_hot_vector\n\n# Define the 'context_words_to_vector' function as seen in a previous notebook\ndef context_words_to_vector(context_words, word2Ind, V):\n    context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n    context_words_vectors = np.mean(context_words_vectors, axis=0)\n    return context_words_vectors\n\n# Define the generator function 'get_training_example' as seen in a previous notebook\ndef get_training_example(words, C, word2Ind, V):\n    for context_words, center_word in get_windows(words, C):\n        yield context_words_to_vector(context_words, word2Ind, V), word_to_one_hot_vector(center_word, word2Ind, V)\n\n\n\nTraining example\nRun the next cells to get the first training example, made of the vector representing the context words “i am because i”, and the target which is the one-hot vector representing the center word “happy”.\n\nYou don’t need to worry about the Python syntax, but there are some explanations below if you want to know what’s happening behind the scenes.\n\n\n# Save generator object in the 'training_examples' variable with the desired arguments\ntraining_examples = get_training_example(words, 2, word2Ind, V)\n\n\nget_training_examples, which uses the yield keyword, is known as a generator. When run, it builds an iterator, which is a special type of object that… you can iterate on (using a for loop for instance), to retrieve the successive values that the function generates.\nIn this case get_training_examples yields training examples, and iterating on training_examples will return the successive training examples.\n\n\n# Get first values from generator\nx_array, y_array = next(training_examples)\n\n\nnext is another special keyword, which gets the next available value from an iterator. Here, you’ll get the very first value, which is the first training example. If you run this cell again, you’ll get the next value, and so on until the iterator runs out of values to return.\nIn this notebook next is used because you will only be performing one iteration of training. In this week’s assignment with the full training over several iterations you’ll use regular for loops with the iterator that supplies the training examples.\n\nThe vector representing the context words, which will be fed into the neural network, is:\n\n# Print context words vector\nx_array\n\narray([0.25, 0.25, 0.  , 0.5 , 0.  ])\n\n\nThe one-hot vector representing the center word to be predicted is:\n\n# Print one hot vector of center word\ny_array\n\narray([0., 0., 1., 0., 0.])\n\n\nNow convert these vectors into matrices (or 2D arrays) to be able to perform matrix multiplication on the right types of objects, as explained in a previous notebook.\n\n# Copy vector\nx = x_array.copy()\n\n# Reshape it\nx.shape = (V, 1)\n\n# Print it\nprint(f'x:\\n{x}\\n')\n\n# Copy vector\ny = y_array.copy()\n\n# Reshape it\ny.shape = (V, 1)\n\n# Print it\nprint(f'y:\\n{y}')\n\nx:\n[[0.25]\n [0.25]\n [0.  ]\n [0.5 ]\n [0.  ]]\n\ny:\n[[0.]\n [0.]\n [1.]\n [0.]\n [0.]]\n\n\nNow you will need the activation functions seen before. Again, if this feel unfamiliar consider checking the previous lecture notebook.\n\n# Define the 'relu' function as seen in the previous lecture notebook\ndef relu(z):\n    result = z.copy()\n    result[result &lt; 0] = 0\n    return result\n\n# Define the 'softmax' function as seen in the previous lecture notebook\ndef softmax(z):\n    e_z = np.exp(z)\n    sum_e_z = np.sum(e_z)\n    return e_z / sum_e_z\n\n\n\nValues of the hidden layer\nNow that you have initialized all the variables that you need for forward propagation, you can calculate the values of the hidden layer using the following formulas:\n\\begin{align}\n\\mathbf{z_1} = \\mathbf{W_1}\\mathbf{x} + \\mathbf{b_1}  \\tag{1} \\\\\n\\mathbf{h} = \\mathrm{ReLU}(\\mathbf{z_1})  \\tag{2} \\\\\n\\end{align}\nFirst, you can calculate the value of \\mathbf{z_1}.\n\n# Compute z1 (values of first hidden layer before applying the ReLU function)\nz1 = np.dot(W1, x) + b1\n\n\n np.dot is numpy’s function for matrix multiplication.\n\nAs expected you get an N by 1 matrix, or column vector with N elements, where N is equal to the embedding size, which is 3 in this example.\n\n# Print z1\nz1\n\narray([[ 0.36483875],\n       [ 0.63710329],\n       [-0.3236647 ]])\n\n\nYou can now take the ReLU of \\mathbf{z_1} to get \\mathbf{h}, the vector with the values of the hidden layer.\n\n# Compute h (z1 after applying ReLU function)\nh = relu(z1)\n\n# Print h\nh\n\narray([[0.36483875],\n       [0.63710329],\n       [0.        ]])\n\n\nApplying ReLU means that the negative element of \\mathbf{z_1} has been replaced with a zero.\n\n\nValues of the output layer\nHere are the formulas you need to calculate the values of the output layer, represented by the vector \\mathbf{\\hat y}:\n\\begin{align}\n\\mathbf{z_2} &= \\mathbf{W_2}\\mathbf{h} + \\mathbf{b_2}   \\tag{3} \\\\\n\\mathbf{\\hat y} &= \\mathrm{softmax}(\\mathbf{z_2})   \\tag{4} \\\\\n\\end{align}\nFirst, calculate \\mathbf{z_2}.\n\n# Compute z2 (values of the output layer before applying the softmax function)\nz2 = np.dot(W2, h) + b2\n\n# Print z2\nz2\n\narray([[-0.31973737],\n       [-0.28125477],\n       [-0.09838369],\n       [-0.33512159],\n       [-0.19919612]])\n\n\nExpected output:\narray([[-0.31973737],\n       [-0.28125477],\n       [-0.09838369],\n       [-0.33512159],\n       [-0.19919612]])\nThis is a V by 1 matrix, where V is the size of the vocabulary, which is 5 in this example.\nNow calculate the value of \\mathbf{\\hat y}.\n\n# Compute y_hat (z2 after applying softmax function)\ny_hat = softmax(z2)\n\n# Print y_hat\ny_hat\n\narray([[0.18519074],\n       [0.19245626],\n       [0.23107446],\n       [0.18236353],\n       [0.20891502]])\n\n\nExpected output:\narray([[0.18519074],\n       [0.19245626],\n       [0.23107446],\n       [0.18236353],\n       [0.20891502]])\nAs you’ve performed the calculations with random matrices and vectors (apart from the input vector), the output of the neural network is essentially random at this point. The learning process will adjust the weights and biases to match the actual targets better.\nThat being said, what word did the neural network predict?\n\n\nSolution\n\n\nThe neural network predicted the word “happy”: the largest element of \\mathbf{\\hat y} is the third one, and the third word of the vocabulary is “happy”.\n\n\nHere’s how you could implement this in Python:\n\n\nprint(Ind2word[np.argmax(y_hat)])\n\nWell done, you’ve completed the forward propagation phase!"
  },
  {
    "objectID": "posts/c2w4/lab03.html#cross-entropy-loss",
    "href": "posts/c2w4/lab03.html#cross-entropy-loss",
    "title": "Word Embeddings: Training the CBOW model",
    "section": "Cross-entropy loss",
    "text": "Cross-entropy loss\nNow that you have the network’s prediction, you can calculate the cross-entropy loss to determine how accurate the prediction was compared to the actual target.\n\nRemember that you are working on a single training example, not on a batch of examples, which is why you are using loss and not cost, which is the generalized form of loss.\n\nFirst let’s recall what the prediction was.\n\n# Print prediction\ny_hat\n\narray([[0.18519074],\n       [0.19245626],\n       [0.23107446],\n       [0.18236353],\n       [0.20891502]])\n\n\nAnd the actual target value is:\n\n# Print target value\ny\n\narray([[0.],\n       [0.],\n       [1.],\n       [0.],\n       [0.]])\n\n\nThe formula for cross-entropy loss is:\n J=-\\sum\\limits_{k=1}^{V}y_k\\log{\\hat{y}_k} \\tag{6}\nTry implementing the cross-entropy loss function so you get more familiar working with numpy\nHere are a some hints if you’re stuck.\n\ndef cross_entropy_loss(y_predicted, y_actual):\n    # Fill the loss variable with your code\n    loss = np.sum(-np.log(y_predicted)*y_actual)\n    return loss\n\n\n\nHint 1\n\n&lt;p&gt;To multiply two numpy matrices (such as &lt;code&gt;y&lt;/code&gt; and &lt;code&gt;y_hat&lt;/code&gt;) element-wise, you can simply use the &lt;code&gt;*&lt;/code&gt; operator.&lt;/p&gt;\n\n\nHint 2\n\n\nOnce you have a vector equal to the element-wise multiplication of y and y_hat, you can use np.sum to calculate the sum of the elements of this vector.\n\n\n\nSolution\n\n\nloss = np.sum(-np.log(y_hat)*y)\n\nDon’t forget to run the cell containing the cross_entropy_loss function once it is solved.\nNow use this function to calculate the loss with the actual values of \\mathbf{y} and \\mathbf{\\hat y}.\n\n# Print value of cross entropy loss for prediction and target value\ncross_entropy_loss(y_hat, y)\n\nnp.float64(1.4650152923611108)\n\n\nExpected output:\n1.4650152923611106\nThis value is neither good nor bad, which is expected as the neural network hasn’t learned anything yet.\nThe actual learning will start during the next phase: backpropagation."
  },
  {
    "objectID": "posts/c2w4/lab03.html#backpropagation",
    "href": "posts/c2w4/lab03.html#backpropagation",
    "title": "Word Embeddings: Training the CBOW model",
    "section": "Backpropagation",
    "text": "Backpropagation\nThe formulas that you will implement for backpropagation are the following.\n\\begin{align}\n\\frac{\\partial J}{\\partial \\mathbf{W_1}} &= \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right )\\mathbf{x}^\\top \\tag{7}\\\\\n\\frac{\\partial J}{\\partial \\mathbf{W_2}} &= (\\mathbf{\\hat{y}} - \\mathbf{y})\\mathbf{h^\\top} \\tag{8}\\\\\n\\frac{\\partial J}{\\partial \\mathbf{b_1}} &= \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right ) \\tag{9}\\\\\n\\frac{\\partial J}{\\partial \\mathbf{b_2}} &= \\mathbf{\\hat{y}} - \\mathbf{y} \\tag{10}\n\\end{align}\n\nNote: these formulas are slightly simplified compared to the ones in the lecture as you’re working on a single training example, whereas the lecture provided the formulas for a batch of examples. In the assignment you’ll be implementing the latter.\n\nLet’s start with an easy one.\nCalculate the partial derivative of the loss function with respect to \\mathbf{b_2}, and store the result in grad_b2.\n\\frac{\\partial J}{\\partial \\mathbf{b_2}} = \\mathbf{\\hat{y}} - \\mathbf{y} \\tag{10}\n\n# Compute vector with partial derivatives of loss function with respect to b2\ngrad_b2 = y_hat - y\n\n# Print this vector\ngrad_b2\n\narray([[ 0.18519074],\n       [ 0.19245626],\n       [-0.76892554],\n       [ 0.18236353],\n       [ 0.20891502]])\n\n\nExpected output:\narray([[ 0.18519074],\n       [ 0.19245626],\n       [-0.76892554],\n       [ 0.18236353],\n       [ 0.20891502]])\nNext, calculate the partial derivative of the loss function with respect to \\mathbf{W_2}, and store the result in grad_W2.\n\\frac{\\partial J}{\\partial \\mathbf{W_2}} = (\\mathbf{\\hat{y}} - \\mathbf{y})\\mathbf{h^\\top} \\tag{8}\n\nHint: use .T to get a transposed matrix, e.g. h.T returns \\mathbf{h^\\top}.\n\n\n# Compute matrix with partial derivatives of loss function with respect to W2\ngrad_W2 = np.dot(y_hat - y, h.T)\n\n# Print matrix\ngrad_W2\n\narray([[ 0.06756476,  0.11798563,  0.        ],\n       [ 0.0702155 ,  0.12261452,  0.        ],\n       [-0.28053384, -0.48988499,  0.        ],\n       [ 0.06653328,  0.1161844 ,  0.        ],\n       [ 0.07622029,  0.13310045,  0.        ]])\n\n\nExpected output:\narray([[ 0.06756476,  0.11798563,  0.        ],\n       [ 0.0702155 ,  0.12261452,  0.        ],\n       [-0.28053384, -0.48988499,  0.        ],\n       [ 0.06653328,  0.1161844 ,  0.        ],\n       [ 0.07622029,  0.13310045,  0.        ]])\nNow calculate the partial derivative with respect to \\mathbf{b_1} and store the result in grad_b1.\n\\frac{\\partial J}{\\partial \\mathbf{b_1}} = \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right ) \\tag{9}\n\n# Compute vector with partial derivatives of loss function with respect to b1\ngrad_b1 = relu(np.dot(W2.T, y_hat - y))\n\n# Print vector\ngrad_b1\n\narray([[0.        ],\n       [0.        ],\n       [0.17045858]])\n\n\nExpected output:\narray([[0.        ],\n       [0.        ],\n       [0.17045858]])\nFinally, calculate the partial derivative of the loss with respect to \\mathbf{W_1}, and store it in grad_W1.\n\\frac{\\partial J}{\\partial \\mathbf{W_1}} = \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right )\\mathbf{x}^\\top \\tag{7}\n\n# Compute matrix with partial derivatives of loss function with respect to W1\ngrad_W1 = np.dot(relu(np.dot(W2.T, y_hat - y)), x.T)\n\n# Print matrix\ngrad_W1\n\narray([[0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.04261464, 0.04261464, 0.        , 0.08522929, 0.        ]])\n\n\nExpected output:\narray([[0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.04261464, 0.04261464, 0.        , 0.08522929, 0.        ]])\nBefore moving on to gradient descent, double-check that all the matrices have the expected dimensions.\n\nprint(f'V (vocabulary size): {V}')\nprint(f'N (embedding size / size of the hidden layer): {N}')\nprint(f'size of grad_W1: {grad_W1.shape} (NxV)')\nprint(f'size of grad_b1: {grad_b1.shape} (Nx1)')\nprint(f'size of grad_W2: {grad_W2.shape} (VxN)')\nprint(f'size of grad_b2: {grad_b2.shape} (Vx1)')\n\nV (vocabulary size): 5\nN (embedding size / size of the hidden layer): 3\nsize of grad_W1: (3, 5) (NxV)\nsize of grad_b1: (3, 1) (Nx1)\nsize of grad_W2: (5, 3) (VxN)\nsize of grad_b2: (5, 1) (Vx1)"
  },
  {
    "objectID": "posts/c2w4/lab03.html#gradient-descent",
    "href": "posts/c2w4/lab03.html#gradient-descent",
    "title": "Word Embeddings: Training the CBOW model",
    "section": "Gradient descent",
    "text": "Gradient descent\nDuring the gradient descent phase, you will update the weights and biases by subtracting \\alpha times the gradient from the original matrices and vectors, using the following formulas.\n\\begin{align}\n\\mathbf{W_1} &:= \\mathbf{W_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_1}} \\tag{11}\\\\\n\\mathbf{W_2} &:= \\mathbf{W_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_2}} \\tag{12}\\\\\n\\mathbf{b_1} &:= \\mathbf{b_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_1}} \\tag{13}\\\\\n\\mathbf{b_2} &:= \\mathbf{b_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_2}} \\tag{14}\\\\\n\\end{align}\nFirst, let set a value for \\alpha.\n\n# Define alpha\nalpha = 0.03\n\nThe updated weight matrix \\mathbf{W_1} will be:\n\n# Compute updated W1\nW1_new = W1 - alpha * grad_W1\n\nLet’s compare the previous and new values of \\mathbf{W_1}:\n\nprint('old value of W1:')\nprint(W1)\nprint()\nprint('new value of W1:')\nprint(W1_new)\n\nold value of W1:\n[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]\n [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]\n [ 0.26637602 -0.23846886 -0.37770863 -0.11399446  0.34008124]]\n\nnew value of W1:\n[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]\n [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]\n [ 0.26509758 -0.2397473  -0.37770863 -0.11655134  0.34008124]]\n\n\nThe difference is very subtle (hint: take a closer look at the last row), which is why it takes a fair amount of iterations to train the neural network until it reaches optimal weights and biases starting from random values.\nNow calculate the new values of \\mathbf{W_2} (to be stored in W2_new), \\mathbf{b_1} (in b1_new), and \\mathbf{b_2} (in b2_new).\n\\begin{align}\n\\mathbf{W_2} &:= \\mathbf{W_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_2}} \\tag{12}\\\\\n\\mathbf{b_1} &:= \\mathbf{b_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_1}} \\tag{13}\\\\\n\\mathbf{b_2} &:= \\mathbf{b_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_2}} \\tag{14}\\\\\n\\end{align}\n\n# Compute updated W2\nW2_new = W2 - alpha * grad_W2\n\n# Compute updated b1\nb1_new = b1 - alpha * grad_b1\n\n# Compute updated b2\nb2_new = b2 - alpha * grad_b2\n\n\nprint('W2_new')\nprint(W2_new)\nprint()\nprint('b1_new')\nprint(b1_new)\nprint()\nprint('b2_new')\nprint(b2_new)\n\nW2_new\n[[-0.22384758 -0.43362588  0.13310965]\n [ 0.08265956  0.0775535   0.1772054 ]\n [ 0.19557112 -0.04637608 -0.1790735 ]\n [ 0.06855622 -0.02363691  0.36107434]\n [ 0.33251813 -0.3982269  -0.43959196]]\n\nb1_new\n[[ 0.09688219]\n [ 0.29239497]\n [-0.27875802]]\n\nb2_new\n[[ 0.02964508]\n [-0.36970753]\n [-0.10468778]\n [-0.35349417]\n [-0.0764456 ]]\n\n\nExpected output:\nW2_new\n[[-0.22384758 -0.43362588  0.13310965]\n [ 0.08265956  0.0775535   0.1772054 ]\n [ 0.19557112 -0.04637608 -0.1790735 ]\n [ 0.06855622 -0.02363691  0.36107434]\n [ 0.33251813 -0.3982269  -0.43959196]]\n\nb1_new\n[[ 0.09688219]\n [ 0.29239497]\n [-0.27875802]]\n\nb2_new\n[[ 0.02964508]\n [-0.36970753]\n [-0.10468778]\n [-0.35349417]\n [-0.0764456 ]]\nCongratulations, you have completed one iteration of training using one training example!\nYou’ll need many more iterations to fully train the neural network, and you can optimize the learning process by training on batches of examples, as described in the lecture. You will get to do this during this week’s assignment.\n\nHow this practice relates to and differs from the upcoming graded assignment\n\nIn the assignment, for each iteration of training you will use batches of examples instead of a single example. The formulas for forward propagation and backpropagation will be modified accordingly, and you will use cross-entropy cost instead of cross-entropy loss.\nYou will also complete several iterations of training, until you reach an acceptably low cross-entropy cost, at which point you can extract good word embeddings from the weight matrices."
  },
  {
    "objectID": "posts/c2w4/lab02.html",
    "href": "posts/c2w4/lab02.html",
    "title": "Word Embeddings: Intro to CBOW model, activation functions and working with Numpy",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure 1\nIn this lecture notebook you will be given an introduction to the continuous bag-of-words model, its activation functions and some considerations when working with Numpy.\nLet’s dive into it!\nimport numpy as np"
  },
  {
    "objectID": "posts/c2w4/lab02.html#activation-functions",
    "href": "posts/c2w4/lab02.html#activation-functions",
    "title": "Word Embeddings: Intro to CBOW model, activation functions and working with Numpy",
    "section": "Activation functions",
    "text": "Activation functions\nLet’s start by implementing the activation functions, ReLU and softmax.\n\nReLU\nReLU is used to calculate the values of the hidden layer, in the following formulas:\n\\begin{align}\n\\mathbf{z_1} &= \\mathbf{W_1}\\mathbf{x} + \\mathbf{b_1}  \\tag{1} \\\\\n\\mathbf{h} &= \\mathrm{ReLU}(\\mathbf{z_1})  \\tag{2} \\\\\n\\end{align}\nLet’s fix a value for \\mathbf{z_1} as a working example.\n\n# Define a random seed so all random outcomes can be reproduced\nnp.random.seed(10)\n\n# Define a 5X1 column vector using numpy\nz_1 = 10*np.random.rand(5, 1)-5\n\n# Print the vector\nz_1\n\narray([[ 2.71320643],\n       [-4.79248051],\n       [ 1.33648235],\n       [ 2.48803883],\n       [-0.01492988]])\n\n\nNotice that using numpy’s random.rand function returns a numpy array filled with values taken from a uniform distribution over [0, 1). Numpy allows vectorization so each value is multiplied by 10 and then substracted 5.\nTo get the ReLU of this vector, you want all the negative values to become zeros.\nFirst create a copy of this vector.\n\n# Create copy of vector and save it in the 'h' variable\nh = z_1.copy()\n\nNow determine which of its values are negative.\n\n# Determine which values met the criteria (this is possible because of vectorization)\nh &lt; 0\n\narray([[False],\n       [ True],\n       [False],\n       [False],\n       [ True]])\n\n\nYou can now simply set all of the values which are negative to 0.\n\n# Slice the array or vector. This is the same as applying ReLU to it\nh[h &lt; 0] = 0\n\nAnd that’s it: you have the ReLU of \\mathbf{z_1}!\n\n# Print the vector after ReLU\nh\n\narray([[2.71320643],\n       [0.        ],\n       [1.33648235],\n       [2.48803883],\n       [0.        ]])\n\n\nNow implement ReLU as a function.\n\n# Define the 'relu' function that will include the steps previously seen\ndef relu(z):\n    result = z.copy()\n    result[result &lt; 0] = 0\n    return result\n\nAnd check that it’s working.\n\n# Define a new vector and save it in the 'z' variable\nz = np.array([[-1.25459881], [ 4.50714306], [ 2.31993942], [ 0.98658484], [-3.4398136 ]])\n\n# Apply ReLU to it\nrelu(z)\n\narray([[0.        ],\n       [4.50714306],\n       [2.31993942],\n       [0.98658484],\n       [0.        ]])\n\n\nExpected output:\narray([[0.        ],\n       [4.50714306],\n       [2.31993942],\n       [0.98658484],\n       [0.        ]])\n\n\nSoftmax\nThe second activation function that you need is softmax. This function is used to calculate the values of the output layer of the neural network, using the following formulas:\n\\begin{align}\n\\mathbf{z_2} &= \\mathbf{W_2}\\mathbf{h} + \\mathbf{b_2}   \\tag{3} \\\\\n\\mathbf{\\hat y} &= \\mathrm{softmax}(\\mathbf{z_2})   \\tag{4} \\\\\n\\end{align}\nTo calculate softmax of a vector \\mathbf{z}, the i-th component of the resulting vector is given by:\n \\textrm{softmax}(\\textbf{z})_i = \\frac{e^{z_i} }{\\sum\\limits_{j=1}^{V} e^{z_j} }  \\tag{5} \nLet’s work through an example.\n\n# Define a new vector and save it in the 'z' variable\nz = np.array([9, 8, 11, 10, 8.5])\n\n# Print the vector\nz\n\narray([ 9. ,  8. , 11. , 10. ,  8.5])\n\n\nYou’ll need to calculate the exponentials of each element, both for the numerator and for the denominator.\n\n# Save exponentials of the values in a new vector\ne_z = np.exp(z)\n\n# Print the vector with the exponential values\ne_z\n\narray([ 8103.08392758,  2980.95798704, 59874.1417152 , 22026.46579481,\n        4914.7688403 ])\n\n\nThe denominator is equal to the sum of these exponentials.\n\n# Save the sum of the exponentials\nsum_e_z = np.sum(e_z)\n\n# Print sum of exponentials\nsum_e_z\n\nnp.float64(97899.41826492078)\n\n\nAnd the value of the first element of \\textrm{softmax}(\\textbf{z}) is given by:\n\n# Print softmax value of the first element in the original vector\ne_z[0]/sum_e_z\n\nnp.float64(0.08276947985173956)\n\n\nThis is for one element. You can use numpy’s vectorized operations to calculate the values of all the elements of the \\textrm{softmax}(\\textbf{z}) vector in one go.\nImplement the softmax function.\n\n# Define the 'softmax' function that will include the steps previously seen\ndef softmax(z):\n    e_z = np.exp(z)\n    sum_e_z = np.sum(e_z)\n    return e_z / sum_e_z\n\nNow check that it works.\n\n# Print softmax values for original vector\nsoftmax([9, 8, 11, 10, 8.5])\n\narray([0.08276948, 0.03044919, 0.61158833, 0.22499077, 0.05020223])\n\n\nExpected output:\narray([0.08276948, 0.03044919, 0.61158833, 0.22499077, 0.05020223])\nNotice that the sum of all these values is equal to 1.\n\n# Assert that the sum of the softmax values is equal to 1\nnp.sum(softmax([9, 8, 11, 10, 8.5])) == 1\n\nnp.True_"
  },
  {
    "objectID": "posts/c2w4/lab02.html#dimensions-1-d-arrays-vs-2-d-column-vectors",
    "href": "posts/c2w4/lab02.html#dimensions-1-d-arrays-vs-2-d-column-vectors",
    "title": "Word Embeddings: Intro to CBOW model, activation functions and working with Numpy",
    "section": "Dimensions: 1-D arrays vs 2-D column vectors",
    "text": "Dimensions: 1-D arrays vs 2-D column vectors\nBefore moving on to implement forward propagation, backpropagation, and gradient descent in the next lecture notebook, let’s have a look at the dimensions of the vectors you’ve been handling until now.\nCreate a vector of length V filled with zeros.\n\n# Define V. Remember this was the size of the vocabulary in the previous lecture notebook\nV = 5\n\n# Define vector of length V filled with zeros\nx_array = np.zeros(V)\n\n# Print vector\nx_array\n\narray([0., 0., 0., 0., 0.])\n\n\nThis is a 1-dimensional array, as revealed by the .shape property of the array.\n\n# Print vector's shape\nx_array.shape\n\n(5,)\n\n\nTo perform matrix multiplication in the next steps, you actually need your column vectors to be represented as a matrix with one column. In numpy, this matrix is represented as a 2-dimensional array.\nThe easiest way to convert a 1D vector to a 2D column matrix is to set its .shape property to the number of rows and one column, as shown in the next cell.\n\n# Copy vector\nx_column_vector = x_array.copy()\n\n# Reshape copy of vector\nx_column_vector.shape = (V, 1)  # alternatively ... = (x_array.shape[0], 1)\n\n# Print vector\nx_column_vector\n\narray([[0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.]])\n\n\nThe shape of the resulting “vector” is:\n\n# Print vector's shape\nx_column_vector.shape\n\n(5, 1)\n\n\nSo you now have a 5x1 matrix that you can use to perform standard matrix multiplication.\nCongratulations on finishing this lecture notebook! Hopefully you now have a better understanding of the activation functions used in the continuous bag-of-words model, as well as a clearer idea of how to leverage Numpy’s power for these types of mathematical computations.\nIn the next lecture notebook you will get a comprehensive dive into:\n\nForward propagation.\nCross-entropy loss.\nBackpropagation.\nGradient descent.\n\nSee you next time!"
  },
  {
    "objectID": "posts/c2w4/index.html",
    "href": "posts/c2w4/index.html",
    "title": "Word embeddings with neural networks",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nFigure 2\nThese are my notes for Week 4 notes for the Natural Language Processing with Probabilistic Models Course in the Natural Language Processing Specialization Offered by DeepLearning.AI on Coursera.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#overview",
    "href": "posts/c2w4/index.html#overview",
    "title": "Word embeddings with neural networks",
    "section": "Overview",
    "text": "Overview\nWord embeddings are used in most NLP applications. Whenever you are dealing with text, you first have to find a way to encode the words as numbers. Word embedding are a very common technique that allows you to do so. Here are a few applications of word embeddings that you should be able to implement by the time you complete the specialization.\n\n\n\n\n\n\n\napplications\n\n\n\n\nFigure 3: Basic Applications of word embeddings\n\n\n\n\n\n\n\n\napplications\n\n\n\n\nFigure 4: Advanced applications of word embeddings\n\n\n\nBy the end of this week you will be able to:\n\nIdentify the key concepts of word representations\nGenerate word embeddings\nPrepare text for machine learning\nImplement the continuous bag-of-words model",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#basic-word-representations",
    "href": "posts/c2w4/index.html#basic-word-representations",
    "title": "Word embeddings with neural networks",
    "section": "Basic Word Representations",
    "text": "Basic Word Representations\n\n\n\n\n\n\n\nFigure 5: One-hot vectors\n\n\n\n\n\n\n\n\nFigure 6: One-hot vectors\n\n\n\nBasic word representations could be classified into the following:\n\nIntegers\nOne-hot vectors\nWord embeddings\n\nTo the left, you have an example where you use integers to represent a word. The issue there is that there is no reason why one word corresponds to a bigger number than another. To fix this problem we introduce one hot vectors (diagram on the right). To implement one hot vectors, you have to initialize a vector of zeros of dimension V and then put a 1 in the index corresponding to the word you are representing.\nThe pros of one-hot vectors: simple and require no implied ordering.\nThe cons of one-hot vectors: huge and encode no meaning.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#word-embeddings",
    "href": "posts/c2w4/index.html#word-embeddings",
    "title": "Word embeddings with neural networks",
    "section": "Word Embeddings",
    "text": "Word Embeddings\n\n\n\n\n\n\n\nFigure 7: Meaning as vectors in 1D\n\n\n\n\n\n\n\n\nFigure 8: Meaning as vectors in 2D\n\n\n\nFrom the plot above, you can see that when encoding a word in 2D, similar words tend to be found next to each other. Perhaps the first coordinate represents whether a word is positive or negative. The second coordinate tell you whether the word is abstract or concrete. This is just an example, in the real world you will find embeddings with hundreds of dimensions. You can think of each coordinate as a number telling you something about the word.\nThe pros:\n\nLow dimensions (less than V)\nAllow you to encode meaning",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#how-to-create-word-embeddings",
    "href": "posts/c2w4/index.html#how-to-create-word-embeddings",
    "title": "Word embeddings with neural networks",
    "section": "How to Create Word Embeddings?",
    "text": "How to Create Word Embeddings?\n\n\n\n\n\n\n\nFigure 9: Meaning as vectors in 2D\n\n\nTo create word embeddings you always need a corpus of text, and an embedding method.\nThe context of a word tells you what type of words tend to occur near that specific word. The context is important as this is what will give meaning to each word embedding.\nEmbeddings There are many types of possible methods that allow you to learn the word embeddings. The machine learning model performs a learning task, and the main by-products of this task are the word embeddings. The task could be to learn to predict a word based on the surrounding words in a sentence of the corpus, as in the case of the continuous bag-of-words.\nThe task is self-supervised: it is both unsupervised in the sense that the input data — the corpus — is unlabelled, and supervised in the sense that the data itself provides the necessary context which would ordinarily make up the labels.\nWhen training word vectors, there are some parameters you need to tune. (i.e. the dimension of the word vector)",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#word-embedding-methods",
    "href": "posts/c2w4/index.html#word-embedding-methods",
    "title": "Word embeddings with neural networks",
    "section": "Word Embedding Methods",
    "text": "Word Embedding Methods\nClassical Methods\n\nword2vec (Google, 2013)\nContinuous bag-of-words (CBOW): the model learns to predict the center word given some context words.\nContinuous skip-gram / Skip-gram with negative sampling (SGNS): the model learns to predict the words surrounding a given input word.\nGlobal Vectors (GloVe) (Stanford, 2014): factorizes the logarithm of the corpus’s word co-occurrence matrix, similar to the count matrix you’ve used before.\nfastText (Facebook, 2016): based on the skip-gram model and takes into account the structure of words by representing words as an n-gram of characters. It supports out-of-vocabulary (OOV) words.\n\nDeep learning, contextual embeddings\nIn these more advanced models, words have different embeddings depending on their context. You can download pre-trained embeddings for the following models.\n\nBERT (Google, 2018):\nELMo (Allen Institute for AI, 2018)\nGPT-2 (OpenAI, 2018)",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#continuous-bag-of-words-model",
    "href": "posts/c2w4/index.html#continuous-bag-of-words-model",
    "title": "Word embeddings with neural networks",
    "section": "Continuous Bag-of-Words Model",
    "text": "Continuous Bag-of-Words Model\n\n\n\n\n\n\n\nFigure 10: Meaning as vectors in 2D\n\n\n\n\n\n\n\n\nFigure 11: Meaning as vectors in 2D\n\n\n\n\n\n\n\n\nFigure 12: Meaning as vectors in 2D\n\n\n\n\nContinuous Bag of Words Model To create word embeddings, you need a corpus and a learning algorithm. The by-product of this task would be a set of word embeddings. In the case of the continuous bag-of-words model, the objective of the task is to predict a missing word based on the surrounding words.\nHere is a visualization that shows you how the models works.\nAs you can see, the window size in the image above is 5. The context size, C, is 2. C usually tells you how many words before or after the center word the model will use to make the prediction. Here is another visualization that shows an overview of the model.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#cleaning-and-tokenization",
    "href": "posts/c2w4/index.html#cleaning-and-tokenization",
    "title": "Word embeddings with neural networks",
    "section": "Cleaning and Tokenization",
    "text": "Cleaning and Tokenization\n\n\n\n\n\n\n\nFigure 13: Meaning as vectors in 2D\n\n\n\n\n\n\n\n\nFigure 14: Meaning as vectors in 2D\n\n\n\nBefore implementing any natural language processing algorithm, you might want to clean the data and tokenize it. Here are a few things to keep track of when handling your data.\nYou can clean data using python as follows:\nYou can add as many conditions as you want in the lines corresponding to the green rectangle above.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#sliding-window-of-words-in-python",
    "href": "posts/c2w4/index.html#sliding-window-of-words-in-python",
    "title": "Word embeddings with neural networks",
    "section": "Sliding Window of words in Python",
    "text": "Sliding Window of words in Python\n\n\n\n\n\n\n\nFigure 15: Sliding Window of words in Python\n\n\nThe code above shows you a function which takes in two parameters.\nWords: a list of words.\nC: the context size.\nWe first start by setting i to C. Then we single out the center_word, and the context_words. We then yield those and increment i.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#transforming-words-into-vectors",
    "href": "posts/c2w4/index.html#transforming-words-into-vectors",
    "title": "Word embeddings with neural networks",
    "section": "Transforming Words into Vectors",
    "text": "Transforming Words into Vectors\n\n\n\n\n\n\n\nFigure 16: Transforming Words into Vectors\n\n\nAs you can see, we started with one-hot vectors for the context words and and we transform them into a single vector by taking an average. As a result you end up having the following vectors that you can use for your training.\n\n\n\n\n\n\n\nFigure 17: Sliding Window of words in Python",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#lecture-notebook---data-preparation",
    "href": "posts/c2w4/index.html#lecture-notebook---data-preparation",
    "title": "Word embeddings with neural networks",
    "section": "Lecture Notebook - Data Preparation",
    "text": "Lecture Notebook - Data Preparation",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#architecture-of-the-cbow-model",
    "href": "posts/c2w4/index.html#architecture-of-the-cbow-model",
    "title": "Word embeddings with neural networks",
    "section": "Architecture of the CBOW Model",
    "text": "Architecture of the CBOW Model\nThe architecture for the CBOW model could be described as follows\n\n\n\n\n\n\n\nFigure 18: Architecture for the CBOW Model\n\n\nYou have an input, X, which is the average of all context vectors. You then multiply it by W_1 and add b1. The result goes through a ReLU function to give you your hidden layer. That layer is then multiplied by W_2 and you add b_2. The result goes through a softmax which gives you a distribution over V, vocabulary words. You pick the vocabulary word that corresponds to the arg-max of the output.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#architecture-of-the-cbow-model-dimensions",
    "href": "posts/c2w4/index.html#architecture-of-the-cbow-model-dimensions",
    "title": "Word embeddings with neural networks",
    "section": "Architecture of the CBOW Model: Dimensions",
    "text": "Architecture of the CBOW Model: Dimensions\nThe equations for the previous model are:\n\nz_1 = W_1 x + b_1\n\n\nh = ReLU(z_1)\n\n\nz_2 = W_2 h + b_2\n\n\n\\hat{y} = softmax(z_2)\n\nHere, you can see the dimensions:\n\n\n\n\n\n\n\nFigure 19: Architecture for the CBOW Model\n\n\nMake sure you go through the matrix multiplications and understand why the dimensions make sense.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#architecture-of-the-cbow-model-dimensions-2",
    "href": "posts/c2w4/index.html#architecture-of-the-cbow-model-dimensions-2",
    "title": "Word embeddings with neural networks",
    "section": "Architecture of the CBOW Model: Dimensions 2",
    "text": "Architecture of the CBOW Model: Dimensions 2\nWhen dealing with batch input, you can stack the examples as columns. You can then proceed to multiply the matrices as follows:\n\n\n\n\n\n\n\nFigure 20: Dimensions Batch Input\n\n\nIn the diagram above, you can see the dimensions of each matrix. Note that your \\hat{Y} is of dimension V by m. Each column is the prediction of the column corresponding to the context words. So the first column in \\hat{Y} is the prediction corresponding to the first column of X.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#architecture-of-the-cbow-model-activation-functions",
    "href": "posts/c2w4/index.html#architecture-of-the-cbow-model-activation-functions",
    "title": "Word embeddings with neural networks",
    "section": "Architecture of the CBOW Model: Activation Functions",
    "text": "Architecture of the CBOW Model: Activation Functions\n\nReLU funciton\nThe rectified linear unit (ReLU), is one of the most popular activation functions. When you feed a vector, namely x, into a ReLU function. You end up taking x=max(0,x). This is a drawing that shows ReLU.\n\n\n\n\n\n\n\nFigure 21: Dimensions Batch Input\n\n\n\n\nSoftmax function\nThe softmax function takes a vector and transforms it into a probability distribution. For example, given the following vector z, you can transform it into a probability distribution as follows.\n\n\n\n\n\n\n\nFigure 22: Dimensions Batch Input\n\n\nAs you can see, you can compute \n\\hat{y} = \\frac{e^{z_i}}{\\sum_{j=1}^V e^{z_j}}",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#lecture-notebook---intro-to-cbow-model",
    "href": "posts/c2w4/index.html#lecture-notebook---intro-to-cbow-model",
    "title": "Word embeddings with neural networks",
    "section": "Lecture Notebook - Intro to CBOW model",
    "text": "Lecture Notebook - Intro to CBOW model",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#training-a-cbow-model-cost-function",
    "href": "posts/c2w4/index.html#training-a-cbow-model-cost-function",
    "title": "Word embeddings with neural networks",
    "section": "Training a CBOW Model: Cost Function",
    "text": "Training a CBOW Model: Cost Function\nThe cost function for the CBOW model is a cross-entropy loss defined as:\n\nJ = -\\sum_{k=1}^V y_k log(\\hat{y}_k)\n {eq-cbow-cost}\nHere is an example where you use the equation above.\n\n\n\n\n\n\n\nFigure 23: Dimensions Batch Input\n\n\nWhy is the cost 4.61 in the example above?",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#training-a-cbow-model-forward-propagation",
    "href": "posts/c2w4/index.html#training-a-cbow-model-forward-propagation",
    "title": "Word embeddings with neural networks",
    "section": "Training a CBOW Model: Forward Propagation",
    "text": "Training a CBOW Model: Forward Propagation\nTraining a CBOW Model: Forward Propagation Forward propagation is defined as:\n\nZ_1 = W_1 X + B_1\n\n\nH = ReLU(Z_1)\n\n\nZ_2 = W_2 H + B_2\n\n\n\\hat{Y} = softmax(Z_2)\n\nIn the image below you start from the left and you forward propagate all the way to the right.\n\n\n\n\n\n\n\nFigure 24: Dimensions Batch Input\n\n\nTo calculate the loss of a batch, you have to compute the following:\n\nJ_{batch} = -\\frac{1}{m} \\sum_{i=1}^m \\sum_{j=1}^V y_j(i) log(\\hat{y}^j(i))\n\nGiven, your predicted center word matrix, and actual center word matrix, you can compute the loss.\n\n\n\n\n\n\n\nFigure 25: Dimensions Batch Input",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#training-a-cbow-model-backpropagation-and-gradient-descent",
    "href": "posts/c2w4/index.html#training-a-cbow-model-backpropagation-and-gradient-descent",
    "title": "Word embeddings with neural networks",
    "section": "Training a CBOW Model: Backpropagation and Gradient Descent",
    "text": "Training a CBOW Model: Backpropagation and Gradient Descent\n\n\n\n\n\n\n\nFigure 26: Dimensions Batch Input\n\n\n\n\n\n\n\n\nFigure 27: Dimensions Batch Input\n\n\n\nTraining a CBOW Model: Backpropagation and Gradient Descent Backpropagation: calculate partial derivatives of cost with respect to weights and biases.\nWhen computing the back-prop in this model, you need to compute the following:\n\n\\frac{\\partial J_{batch}}{\\partial W_1}, \\frac{\\partial J_{batch}}{\\partial W_2}, \\frac{\\partial J_{batch}}{\\partial B_1}, \\frac{\\partial J_{batch}}{\\partial B_2}\n\nGradient descent: update weights and biases\nNow to update the weights you can iterate as follows:\n\nW_1 := W_1 - \\alpha \\frac{\\partial J_{batch}}{\\partial W_1}\n\n\nW_2 := W_2 - \\alpha \\frac{\\partial J_{batch}}{\\partial W_2}\n\n\nB_1 := B_1 - \\alpha \\frac{\\partial J_{batch}}{\\partial B_1}\n\n\nB_2 := B_2 - \\alpha \\frac{\\partial J_{batch}}{\\partial B_2}\n\nA smaller alpha allows for more gradual updates to the weights and biases, whereas a larger number allows for a faster update of the weights. If α is too large, you might not learn anything, if it is too small, your model will take forever to train.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#lecture-notebook---training-the-cbow-model",
    "href": "posts/c2w4/index.html#lecture-notebook---training-the-cbow-model",
    "title": "Word embeddings with neural networks",
    "section": "Lecture Notebook - Training the CBOW model",
    "text": "Lecture Notebook - Training the CBOW model",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#extracting-word-embedding-vectors",
    "href": "posts/c2w4/index.html#extracting-word-embedding-vectors",
    "title": "Word embeddings with neural networks",
    "section": "Extracting Word Embedding Vectors",
    "text": "Extracting Word Embedding Vectors\nThere are two options to extract word embeddings after training the continuous bag of words model. You can use W_1 as follows:\n\n\n\n\n\n\n\nFigure 28: Dimensions Batch Input\n\n\nIf you were to use W_1, each column will correspond to the embeddings of a specific word. You can also use W_2 as follows:\n\n\n\n\n\n\n\nFigure 29: Dimensions Batch Input\n\n\nThe final option is to take an average of both matrices as follows:\n\n\n\n\n\n\n\nFigure 30: Dimensions Batch Input",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#lecture-notebook---word-embeddings",
    "href": "posts/c2w4/index.html#lecture-notebook---word-embeddings",
    "title": "Word embeddings with neural networks",
    "section": "Lecture Notebook - Word Embeddings",
    "text": "Lecture Notebook - Word Embeddings",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#lecture-notebook-word-embeddings-step-by-step",
    "href": "posts/c2w4/index.html#lecture-notebook-word-embeddings-step-by-step",
    "title": "Word embeddings with neural networks",
    "section": "Lecture notebook: Word embeddings step by step",
    "text": "Lecture notebook: Word embeddings step by step",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#evaluating-word-embeddings-intrinsic-evaluation",
    "href": "posts/c2w4/index.html#evaluating-word-embeddings-intrinsic-evaluation",
    "title": "Word embeddings with neural networks",
    "section": "Evaluating Word Embeddings: Intrinsic Evaluation",
    "text": "Evaluating Word Embeddings: Intrinsic Evaluation\nIntrinsic evaluation allows you to test relationships between words. It allows you to capture semantic analogies as, “France” is to “Paris” as “Italy” is to &lt;?&gt; and also syntactic analogies as “seen” is to “saw” as “been” is to &lt;?&gt;.\nAmbiguous cases could be much harder to track: ::: {#fig-28 .column-margin} \nDimensions Batch Input :::\nHere are a few ways that allow to use intrinsic evaluation.\n\n\n\n\n\n\n\nFigure 31: Dimensions Batch Input\n\n\n\n\n\n\n\n\nFigure 32: Dimensions Batch Input",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#evaluating-word-embeddings-extrinsic-evaluation",
    "href": "posts/c2w4/index.html#evaluating-word-embeddings-extrinsic-evaluation",
    "title": "Word embeddings with neural networks",
    "section": "Evaluating Word Embeddings: Extrinsic Evaluation",
    "text": "Evaluating Word Embeddings: Extrinsic Evaluation\nExtrinsic evaluation tests word embeddings on external tasks like named entity recognition, parts-of-speech tagging, etc.\n\nEvaluates actual usefulness of embeddings\nTime Consuming\nMore difficult to trouble shoot\n\nSo now you know both intrinsic and extrinsic evaluation.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#conclusion",
    "href": "posts/c2w4/index.html#conclusion",
    "title": "Word embeddings with neural networks",
    "section": "Conclusion",
    "text": "Conclusion",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c1w2/lab01.html",
    "href": "posts/c1w2/lab01.html",
    "title": "Visualizing Naive Bayes",
    "section": "",
    "text": "course banner\n\nIn this lab, we will cover an essential part of data analysis that has not been included in the lecture videos. As we stated in the previous module, data visualization gives insight into the expected performance of any model.\nIn the following exercise, you are going to make a visual inspection of the tweets dataset using the Naïve Bayes features. We will see how we can understand the log-likelihood ratio explained in the videos as a pair of numerical features that can be fed in a machine learning algorithm.\nAt the end of this lab, we will introduce the concept of confidence ellipse as a tool for representing the Naïve Bayes model visually.\n\nimport numpy as np # Library for linear algebra and math utils\nimport pandas as pd # Dataframe library\n\nimport matplotlib.pyplot as plt # Library for plots\nfrom utils import confidence_ellipse # Function to add confidence ellipses to charts\n\n## Calculate the likelihoods for each tweet\nFor each tweet, we have calculated the likelihood of the tweet to be positive and the likelihood to be negative. We have calculated in different columns the numerator and denominator of the likelihood ratio introduced previously.\nlog \\frac{P(tweet|pos)}{P(tweet|neg)} = log(P(tweet|pos)) - log(P(tweet|neg))  positive = log(P(tweet|pos)) = \\sum_{i=0}^{n}{log P(W_i|pos)} negative = log(P(tweet|neg)) = \\sum_{i=0}^{n}{log P(W_i|neg)}\nWe did not include the code because this is part of this week’s assignment. The ‘bayes_features.csv’ file contains the final result of this process.\nThe cell below loads the table in a dataframe. Dataframes are data structures that simplify the manipulation of data, allowing filtering, slicing, joining, and summarization.\n\ndata = pd.read_csv('bayes_features.csv'); # Load the data from the csv file\n\ndata.head(5) # Print the first 5 tweets features. Each row represents a tweet\n\n\n\n\n\n\n\n\npositive\nnegative\nsentiment\n\n\n\n\n0\n-45.763393\n-63.351354\n1.0\n\n\n1\n-105.491568\n-114.204862\n1.0\n\n\n2\n-57.028078\n-67.216467\n1.0\n\n\n3\n-10.055885\n-18.589057\n1.0\n\n\n4\n-125.749270\n-138.334845\n1.0\n\n\n\n\n\n\n\n\n# Plot the samples using columns 1 and 2 of the matrix\n\nfig, ax = plt.subplots(figsize = (8, 8)) #Create a new figure with a custom size\n\ncolors = ['red', 'green'] # Define a color palete\n\n# Color base on sentiment\nax.scatter(data.positive, data.negative, \n    c=[colors[int(k)] for k in data.sentiment], s = 0.1, marker='*')  # Plot a dot for each tweet\n\n# Custom limits for this chart\nplt.xlim(-250,0)\nplt.ylim(-250,0)\n\nplt.xlabel(\"Positive\") # x-axis label\nplt.ylabel(\"Negative\") # y-axis label\n\nText(0.5, 0, 'Positive')\n\n\nText(0, 0.5, 'Negative')\n\n\n\n\n\n\n\n\n\n\nUsing Confidence Ellipses to interpret Naïve Bayes\nIn this section, we will use the confidence ellipse to give us an idea of what the Naïve Bayes model see.\nA confidence ellipse is a way to visualize a 2D random variable. It is a better way than plotting the points over a cartesian plane because, with big datasets, the points can overlap badly and hide the real distribution of the data. Confidence ellipses summarize the information of the dataset with only four parameters:\n\nCenter: It is the numerical mean of the attributes\nHeight and width: Related with the variance of each attribute. The user must specify the desired amount of standard deviations used to plot the ellipse.\nAngle: Related with the covariance among attributes.\n\nThe parameter n_std stands for the number of standard deviations bounded by the ellipse. Remember that for normal random distributions:\n\nAbout 68% of the area under the curve falls within 1 standard deviation around the mean.\nAbout 95% of the area under the curve falls within 2 standard deviations around the mean.\nAbout 99.7% of the area under the curve falls within 3 standard deviations around the mean.\n\n\nIn the next chart, we will plot the data and its corresponding confidence ellipses using 2 std and 3 std.\n\n# Plot the samples using columns 1 and 2 of the matrix\nfig, ax = plt.subplots(figsize = (8, 8))\n\ncolors = ['red', 'green'] # Define a color palete\n\n# Color base on sentiment\n\nax.scatter(data.positive, data.negative, c=[colors[int(k)] for k in data.sentiment], s = 0.1, marker='*')  # Plot a dot for tweet\n\n# Custom limits for this chart\nplt.xlim(-200,40)  \nplt.ylim(-200,40)\n\nplt.xlabel(\"Positive\") # x-axis label\nplt.ylabel(\"Negative\") # y-axis label\n\ndata_pos = data[data.sentiment == 1] # Filter only the positive samples\ndata_neg = data[data.sentiment == 0] # Filter only the negative samples\n\n# Print confidence ellipses of 2 std\nconfidence_ellipse(data_pos.positive, data_pos.negative, ax, n_std=2, edgecolor='black', label=r'$2\\sigma$' )\nconfidence_ellipse(data_neg.positive, data_neg.negative, ax, n_std=2, edgecolor='orange')\n\n# Print confidence ellipses of 3 std\nconfidence_ellipse(data_pos.positive, data_pos.negative, ax, n_std=3, edgecolor='black', linestyle=':', label=r'$3\\sigma$')\nconfidence_ellipse(data_neg.positive, data_neg.negative, ax, n_std=3, edgecolor='orange', linestyle=':')\nax.legend()\n\nplt.show()\n\nText(0.5, 0, 'Positive')\n\n\nText(0, 0.5, 'Negative')\n\n\n\n\n\n\n\n\n\nIn the next cell, we will modify the features of the samples with positive sentiment (1), in a way that the two distributions overlap. In this case, the Naïve Bayes method will produce a lower accuracy than with the original data.\n\ndata2 = data.copy() # Copy the whole data frame\n\n# The following 2 lines only modify the entries in the data frame where sentiment == 1\ndata2.negative[data.sentiment == 1] =  data2.negative * 1.5 + 50 # Modify the negative attribute\ndata2.positive[data.sentiment == 1] =  data2.positive / 1.5 - 50 # Modify the positive attribute \n\n/tmp/ipykernel_130410/2253601370.py:4: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  data2.negative[data.sentiment == 1] =  data2.negative * 1.5 + 50 # Modify the negative attribute\n/tmp/ipykernel_130410/2253601370.py:5: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  data2.positive[data.sentiment == 1] =  data2.positive / 1.5 - 50 # Modify the positive attribute\n\n\nNow let us plot the two distributions and the confidence ellipses\n\n# Plot the samples using columns 1 and 2 of the matrix\nfig, ax = plt.subplots(figsize = (8, 8))\n\ncolors = ['red', 'green'] # Define a color palete\n\n# Color base on sentiment\n\n#data.negative[data.sentiment == 1] =  data.negative * 2\n\nax.scatter(data2.positive, data2.negative, c=[colors[int(k)] for k in data2.sentiment], s = 0.1, marker='*')  # Plot a dot for tweet\n# Custom limits for this chart\nplt.xlim(-200,40)  \nplt.ylim(-200,40)\n\nplt.xlabel(\"Positive\") # x-axis label\nplt.ylabel(\"Negative\") # y-axis label\n\ndata_pos = data2[data2.sentiment == 1] # Filter only the positive samples\ndata_neg = data[data2.sentiment == 0] # Filter only the negative samples\n\n# Print confidence ellipses of 2 std\nconfidence_ellipse(data_pos.positive, data_pos.negative, ax, n_std=2, edgecolor='black', label=r'$2\\sigma$' )\nconfidence_ellipse(data_neg.positive, data_neg.negative, ax, n_std=2, edgecolor='orange')\n\n# Print confidence ellipses of 3 std\nconfidence_ellipse(data_pos.positive, data_pos.negative, ax, n_std=3, edgecolor='black', linestyle=':', label=r'$3\\sigma$')\nconfidence_ellipse(data_neg.positive, data_neg.negative, ax, n_std=3, edgecolor='orange', linestyle=':')\nax.legend()\n\nplt.show()\n\nText(0.5, 0, 'Positive')\n\n\nText(0, 0.5, 'Negative')\n\n\n\n\n\n\n\n\n\nTo give away: Understanding the data allows us to predict if the method will perform well or not. Alternatively, it will allow us to understand why it worked well or bad.\n\n\n\n\nCitationBibTeX citation:@online{2025,\n  author = {},\n  title = {Visualizing {Naive} {Bayes}},\n  date = {2025-01-31},\n  url = {https://orenbochman.github.io/notes-nlp/posts/c1w2/lab01.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“Visualizing Naive Bayes.” 2025. January 31, 2025. https://orenbochman.github.io/notes-nlp/posts/c1w2/lab01.html."
  },
  {
    "objectID": "posts/c1w2/index.html",
    "href": "posts/c1w2/index.html",
    "title": "Probability and Bayes Rule",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nFigure 1\nSince I majored in Mathematics, I glossed over many details when I took the initial notes, since the course caters to all levels of students. When I migrated the notes from OneNotes to the web, I updated and reviewed the current course material and at times additional notes by (Chadha2020NLP?) and (Jelliti 2020). I have tried to add my own insights from other sources, books I read or others courses I have taken.\nThe following two results are due to (NIPS2001_7b7a53e2?) by way of Naive_Bayes_classifier wikipedia article. Another detail that can help you make sense of this lesson is the following result relating Naïve Bayes to Logistic Regression which we covered last week. In the case of discrete inputs like indicator or frequency features for discrete events, naive Bayes classifiers form a generative-discriminative pair with multinomial logistic regression classifiers: each naive Bayes classifier can be considered a way of fitting a probability model that optimizes the joint likelihood p (C,x), while logistic regression fits the same probability model to optimize the conditional p(C ∣ x).",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Probability and Bayes Rule"
    ]
  },
  {
    "objectID": "posts/c1w2/index.html#probability-of-a-randomly-selected-tweets-sentiment",
    "href": "posts/c1w2/index.html#probability-of-a-randomly-selected-tweets-sentiment",
    "title": "Probability and Bayes Rule",
    "section": "Probability of a randomly selected tweet’s sentiment",
    "text": "Probability of a randomly selected tweet’s sentiment\n\nTo calculate a probability of a certain event happening, you take the count of that specific event and divide it by the sum of all events.\nFurthermore, the sum of all probabilities has to equal 1. If we pick a tweet at random, what is the probability of it being +? We define an event A: “A tweet is positive” and calculate its probability\n\n\nP(A) = P(+) = \\frac{N_{+}}{N}=\\frac{13}{20}=0.65\n\nAnd since probabilities add up to one:\n\nP(-) = 1- P(+)=0.35",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Probability and Bayes Rule"
    ]
  },
  {
    "objectID": "posts/c1w2/index.html#probabiliy-for-a-specific-words-sentiment",
    "href": "posts/c1w2/index.html#probabiliy-for-a-specific-words-sentiment",
    "title": "Probability and Bayes Rule",
    "section": "Probabiliy for a specific word’s sentiment",
    "text": "Probabiliy for a specific word’s sentiment\nWithin that corpus, the word happy is sometimes labeled + and in other cases, -. This indicates that some negative tweets contain the word happy. Shown below is a graphical representation of this “overlap”. Let’s explore how we may represent this graphically using a venn diagram and then derive a probability-based representation.\n\n\n\n\nTweets with “Happy”\n\nFirst, we need to estimate the probability of the event B: “tweets containing the word happy”\n\nP(B) = P(Happy)=\\frac{N_{happy}}{N}=\\frac{4}{20}=0.2\n\n\n\n\n\nVenn diagram for defining probabilities from events\n\nTo compute the probability of 2 events happening like happy and + in the picture you would be looking at the intersection, or overlap of the two events, In this case, the red and the blue boxes overlap in three boxes, So the answer is: \nP(A \\cap B) = P(A,B) = \\frac{2}{20}\n\nThe Event “A is labeled +”, - The probability of events A shown as P(A) is calculated as the ratio between the count of positive tweets and the corpus divided by the total number of tweets in the corpus.\n\n\n\n\n\n\n\n specific tweets color coded per the Venn diagram\n\n\n\n\n\n\n\n\nDefinition of conditional probability\n\n\n\nConditional probability is the probability of an outcome B when we already know for certain that an event A has already happened. Notation: \nP(B|A)\n\n\n\n\nand there more + than - more specifically our prior knowledge is that : \n  \\frac{P(+)}{P(−)}=\\frac{13}{7}\n\nthe likelihood of a tweet with happy being + is\nthe challenge arises from some words being in both + and - tweets Conditional probabilities help us reduce the sample search space by restricting it to a specific event which is a given. We should understand the difference between P(A|B) and P(B|A)",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Probability and Bayes Rule"
    ]
  },
  {
    "objectID": "posts/c1w2/index.html#what-is-phappy",
    "href": "posts/c1w2/index.html#what-is-phappy",
    "title": "Probability and Bayes Rule",
    "section": "what is P(+|happy)",
    "text": "what is P(+|happy)\n\nWe start with the Venn diagram for the P(A|B). \nWhere we restricted the diagram to just A the subset of happy tweets.\nAnd we just want those tweets that are also + i.e. (B).\nall we need is to plug in the counts from our count chart. \nwhich we now estimate \nP(A \\mid B) = P(Positive \\mid happy) = \\frac{3}{4} = 0.75",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Probability and Bayes Rule"
    ]
  },
  {
    "objectID": "posts/c1w2/index.html#what-is-phappy-1",
    "href": "posts/c1w2/index.html#what-is-phappy-1",
    "title": "Probability and Bayes Rule",
    "section": "what is P(happy|+)",
    "text": "what is P(happy|+)\n\nWe start with the Venn diagram for the P(B|A)\nwhere we have restricted the diagram to just B the subset of + tweets. \nand we just want from those the tweets that are also happy i.e. (A).\nand the counts for P(B|A) \nwhich we now estimate \nP(B \\mid A) = P(happy \\mid Positive) = \\frac{3}{13} = 0.231",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Probability and Bayes Rule"
    ]
  },
  {
    "objectID": "posts/c1w2/index.html#naïve-bayes-introduction",
    "href": "posts/c1w2/index.html#naïve-bayes-introduction",
    "title": "Probability and Bayes Rule",
    "section": "Naïve Bayes Introduction",
    "text": "Naïve Bayes Introduction\nHere is a sample corpus\n\n\n\n\n\n\n\n\n+ tweets\n- tweets\n\n\n\n\nI am happy because I am learning NLP\nI am sad, I am not learning NLP\n\n\nI am happy\nI am sad\n\n\n\nAnd these are the class frequencies and probabilities Table of tweets\n\n\nimport pandas as pd\nimport string \nraw_tweets=[\n  \"I am happy because I am learning NLP\",\n  \"I am sad, I am not learning NLP\",\n  \"I am happy, not sad\",\n  \"I am sad, not happy\",\n]\ndef clean(tweet:str):\n  return  tweet.translate(str.maketrans('', '', string.punctuation)).lower()\ntweets = [clean(tweet) for tweet in raw_tweets]\nlabels=['+','-','+','-']\ndf = pd.DataFrame({'tweets': tweets, 'labels': labels})\ndf\n\n\n\n\n\n\n\n\ntweets\nlabels\n\n\n\n\n0\ni am happy because i am learning nlp\n+\n\n\n1\ni am sad i am not learning nlp\n-\n\n\n2\ni am happy not sad\n+\n\n\n3\ni am sad not happy\n-\n\n\n\n\n\n\n\n\nimport numpy as np\nfrom collections import Counter\np_freq,n_freq = Counter(), Counter()\n#print( df[df.labels == '+']['tweets'].to_list())\n[p_freq.update(tweet.split()) for tweet in df[df.labels == '+']['tweets'].to_list()]\n[n_freq.update(tweet.split()) for tweet in df[df.labels == '-']['tweets'].to_list()]\nprint(p_freq)\nprint(n_freq)\nvocab = list(set(p_freq.keys()).union(set(n_freq.keys())))\npos_freq = [p_freq[word] for word in vocab ]\nneg_freq = [n_freq[word] for word in vocab ]\nvocab_df=pd.DataFrame({'vocab':vocab,'pos_freq':pos_freq,'neg_freq':neg_freq})\nvocab_df['p_pos']=vocab_df.pos_freq/vocab_df.pos_freq.sum()\nvocab_df['p_neg']=vocab_df.neg_freq/vocab_df.neg_freq.sum()\nvocab_df['p_pos_sm']=(vocab_df.pos_freq+1)/(vocab_df.pos_freq.sum()+vocab_df.shape[1])\nvocab_df['p_neg_sm']=(vocab_df.neg_freq+1)/(vocab_df.neg_freq.sum()+vocab_df.shape[1])\nvocab_df['ratio']= vocab_df.p_pos_sm/vocab_df.p_neg_sm\nvocab_df['lambda']= np.log(vocab_df.p_pos_sm/vocab_df.p_neg_sm)\npd.set_option('display.float_format', '{:.2f}'.format)\nvocab_df\nprint(vocab_df.shape)\n\n[None, None]\n\n\n[None, None]\n\n\nCounter({'i': 3, 'am': 3, 'happy': 2, 'because': 1, 'learning': 1, 'nlp': 1, 'not': 1, 'sad': 1})\nCounter({'i': 3, 'am': 3, 'sad': 2, 'not': 2, 'learning': 1, 'nlp': 1, 'happy': 1})\n\n\n\n\n\n\n\n\n\nvocab\npos_freq\nneg_freq\np_pos\np_neg\np_pos_sm\np_neg_sm\nratio\nlambda\n\n\n\n\n0\nnot\n1\n2\n0.08\n0.15\n0.11\n0.16\n0.70\n-0.35\n\n\n1\ni\n3\n3\n0.23\n0.23\n0.22\n0.21\n1.06\n0.05\n\n\n2\nam\n3\n3\n0.23\n0.23\n0.22\n0.21\n1.06\n0.05\n\n\n3\nhappy\n2\n1\n0.15\n0.08\n0.17\n0.11\n1.58\n0.46\n\n\n4\nlearning\n1\n1\n0.08\n0.08\n0.11\n0.11\n1.06\n0.05\n\n\n5\nbecause\n1\n0\n0.08\n0.00\n0.11\n0.05\n2.11\n0.75\n\n\n6\nsad\n1\n2\n0.08\n0.15\n0.11\n0.16\n0.70\n-0.35\n\n\n7\nnlp\n1\n1\n0.08\n0.08\n0.11\n0.11\n1.06\n0.05\n\n\n\n\n\n\n\n(8, 9)\n\n\n\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\ntable = [[\"Sun\",696000,1989100000],\n         [\"Earth\",6371,5973.6],\n         [\"Moon\",1737,73.5],\n         [\"Mars\",3390,641.85]]\nMarkdown(tabulate(\n  table, \n  headers=[\"Planet\",\"R (km)\", \"mass (x 10^29 kg)\"]\n))\n\n\n\nTable 1: Planets\n\n\n\n\n\n\nPlanet\nR (km)\nmass (x 10^29 kg)\n\n\n\n\nSun\n696000\n1.9891e+09\n\n\nEarth\n6371\n5973.6\n\n\nMoon\n1737\n73.5\n\n\nMars\n3390\n641.85\n\n\n\n\n\n\n\n\n\n\n\nTable 2: Frequency Table {#tbl-first} | word | + | - | |———-|:—-:|:—-:| | I | 0.24 | 0.25 | | am | 0.24 | 0.25 | | happy | 0.15 | 0.08 | | because | 0.08 | 0.00 | | learning | 0.08 | 0.08 | | NLP | 0.08 | 0.08 | | sad | 0.08 | 0.17 | | not | 0.08 | 0.17 | : P(w_i|class) table {#tbl-second} Probabilities\n\n\n\n\n\n\n\nword\n+\n-\n\n\n\n\nI\n3\n3\n\n\nam\n3\n3\n\n\nhappy\n2\n1\n\n\nbecause\n1\n0\n\n\nlearning\n1\n1\n\n\nNLP\n1\n1\n\n\nsad\n1\n2\n\n\nnot\n1\n2\n\n\nNclass\n13\n12\n\n\n\n\n\n\n\n\nLet’s motivate the Naïve Bayes inference condition rule for binary classification:\nTo build a classifier, we will first start by creating conditional probabilities given the table;\n\n\n\n\nNaïve Bayes\n\n\nWe want to find if given our prior knowledge of P(+) and P(-) if a new tweet has + or - sentiment.\nTo do that we will estimate p(+|T) and p(-|T) and then decide based on which is greater than 0.5.\n\n\n\n\n\nTable of probabilities\n\nWe can use the Bayes rule:\n\np(+|T) = \\frac{ p(T|+) \\times p(+) }{ p(T) }\n\nand\n\np(-|T) = \\frac{ p(T|-) \\times p(-) }{ p(T) }\n\nwhere:\n\np(+|T) is the posterior probability of a label + given tweet T\np(+) is our prior knowledge\np(T|+) is the likelihood of tweet T being +.\n{p(T)}\n\nThe term p(T) is in both terms and can be eliminated. However, it will cancel out when we use the ratio for the inference. This lets us compute the following table of probabilities; word am learning NLP Pos 0.24 0.08 0.08 Neg 0.25 0.08 0.08 .17 Naïve Bayes is the simplest probabilistic graphical model which comes with an independence assumption for the features.\n\np(T|+) = \\prod^m_{i=1}P(w_i|+) \\implies p(+|T)=\\frac{P(+)}{P(T)} \\prod^m_{i=1}P(w_i|+)\n\nand\n\np(T|−) = \\prod^m_{i=1}P(w_i|−) \\implies p(−|T) =  \\frac{P(−)}{P(T)} \\prod^m_{i=1} P(w_i|−)\n\nOnce you have the probabilities, you can compute the likelihood score as follows:\nTweet: I am happy today: I am learning. - Since there is no entry for today in our conditional probabilities table, this implies that this word is not in your vocabulary. So we’ll ignore its contribution to the overall score. - All the neutral words in the tweet such as I and am cancel out in the expression, as shown in the figure below.\n\n   \\prod^m_{i=1} \\frac{P(w_i|+)}{P(w_i|-)}= \\frac {0.14}{0.10} =1.4 &gt; 1\n\n\nA score greater than 1 indicates that the class is positive, otherwise, it is negative.\n\n\nP(+|T) &gt; P(−|T)\n\nthen we infer that the T has + sentiment. dividing by the right term we get the inference rule:\n\n\\frac{P(+|T)}{P(−|T)} &gt; 1\n which expands to : \n  \\frac {P(+|T)}{P(−|T)} = \\frac {P(+)}{P(-)}\\prod^m_{i=1} \\frac {P(w_i|+)}{P(w_i|−)} &gt; 1\n\nThis is the inference rule for naïve Bayes.\nNote: Naïve Bayes is a model which assumes all features are independent, so the basic component here is:\n\n\\frac{P(w_i|+)}{P(w_i|-)} &gt; 1\n the ratio of the probability that a word appears in a positive tweet and that it appears in a negative tweet",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Probability and Bayes Rule"
    ]
  },
  {
    "objectID": "posts/c1w2/index.html#additive-smoothing",
    "href": "posts/c1w2/index.html#additive-smoothing",
    "title": "Probability and Bayes Rule",
    "section": "Additive smoothing:",
    "text": "Additive smoothing:\n\np_{addative}(w_i|class)=\\frac{ freq(w,class)+\\delta}{ N_{class} + \\delta \\times V}",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Probability and Bayes Rule"
    ]
  },
  {
    "objectID": "posts/c1w2/index.html#more-alternatives-to-laplacian-smoothing",
    "href": "posts/c1w2/index.html#more-alternatives-to-laplacian-smoothing",
    "title": "Probability and Bayes Rule",
    "section": "More alternatives to Laplacian smoothing",
    "text": "More alternatives to Laplacian smoothing\n\n\n\n\nGood Turing smoothing\n\n\nKneser-Ney smoothing (NEY19941?) which corrects better for smaller data sets. \nGood-Turing smoothing (Good 1953) which uses order statistics to give even better estimates.\n\nwith a survey of the subject here: Chen and Goodman (1996)",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Probability and Bayes Rule"
    ]
  },
  {
    "objectID": "posts/c1w2/index.html#sources-of-errors-in-naïve-bayes",
    "href": "posts/c1w2/index.html#sources-of-errors-in-naïve-bayes",
    "title": "Probability and Bayes Rule",
    "section": "Sources of Errors in Naïve Bayes",
    "text": "Sources of Errors in Naïve Bayes\n\nError Analysis\nBad sentiment classifications are due to:\n\npreprocessing dropping punctuation that encodes emotion like a sad smiley.\nWord order can contribute to meaning - breaking the independence assumption of our model\nPronouns removed as stop words - may encode emotion\nSarcasm can confound the model\nEuphemisms are also a challenge",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Probability and Bayes Rule"
    ]
  },
  {
    "objectID": "posts/c1w3/assignment.html",
    "href": "posts/c1w3/assignment.html",
    "title": "Assignment 3: Hello Vectors",
    "section": "",
    "text": "course banner\nWelcome to this week’s programming assignment on exploring word vectors. In natural language processing, we represent each word as a vector consisting of numbers. The vector encodes the meaning of the word. These numbers (or weights) for each word are learned using various machine learning models, which we will explore in more detail later in this specialization. Rather than make you code the machine learning models from scratch, we will show you how to use them. In the real world, you can always load the trained word vectors, and you will almost never have to train them from scratch. In this assignment, you will:"
  },
  {
    "objectID": "posts/c1w3/assignment.html#predict-the-countries-from-capitals",
    "href": "posts/c1w3/assignment.html#predict-the-countries-from-capitals",
    "title": "Assignment 3: Hello Vectors",
    "section": "1.0 Predict the Countries from Capitals",
    "text": "1.0 Predict the Countries from Capitals\nIn the lectures, we have illustrated the word analogies by finding the capital of a country from the country. We have changed the problem a bit in this part of the assignment. You are asked to predict the countries that corresponds to some capitals. You are playing trivia against some second grader who just took their geography test and knows all the capitals by heart. Thanks to NLP, you will be able to answer the questions properly. In other words, you will write a program that can give you the country by its capital. That way you are pretty sure you will win the trivia game. We will start by exploring the data set.\n\n\n\nmap\n\n\n\n1.1 Importing the data\nAs usual, you start by importing some essential Python libraries and then load the dataset. The dataset will be loaded as a Pandas DataFrame, which is very a common method in data science. This may take a few minutes because of the large size of the data.\n\n# Run this cell to import packages.\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom utils import get_vectors\n\n\ndata = pd.read_csv('capitals.txt', delimiter=' ')\ndata.columns = ['city1', 'country1', 'city2', 'country2']\n\n# print first five elements in the DataFrame\ndata.head(5)\n\n\n\n\n\n\n\n\ncity1\ncountry1\ncity2\ncountry2\n\n\n\n\n0\nAthens\nGreece\nBangkok\nThailand\n\n\n1\nAthens\nGreece\nBeijing\nChina\n\n\n2\nAthens\nGreece\nBerlin\nGermany\n\n\n3\nAthens\nGreece\nBern\nSwitzerland\n\n\n4\nAthens\nGreece\nCairo\nEgypt\n\n\n\n\n\n\n\n\n\n\nTo Run This Code On Your Own Machine:\nNote that because the original google news word embedding dataset is about 3.64 gigabytes, the workspace is not able to handle the full file set. So we’ve downloaded the full dataset, extracted a sample of the words that we’re going to analyze in this assignment, and saved it in a pickle file called word_embeddings_capitals.p\nIf you want to download the full dataset on your own and choose your own set of word embeddings, please see the instructions and some helper code.\n\nDownload the dataset from this page.\nSearch in the page for ‘GoogleNews-vectors-negative300.bin.gz’ and click the link to download.\n\nCopy-paste the code below and run it on your local machine after downloading the dataset to the same directory as the notebook.\nimport nltk\nfrom gensim.models import KeyedVectors\n\n\nembeddings = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary = True)\nf = open('capitals.txt', 'r').read()\nset_words = set(nltk.word_tokenize(f))\nselect_words = words = ['king', 'queen', 'oil', 'gas', 'happy', 'sad', 'city', 'town', 'village', 'country', 'continent', 'petroleum', 'joyful']\nfor w in select_words:\n    set_words.add(w)\n\ndef get_word_embeddings(embeddings):\n\n    word_embeddings = {}\n    for word in embeddings.vocab:\n        if word in set_words:\n            word_embeddings[word] = embeddings[word]\n    return word_embeddings\n\n\n# Testing your function\nword_embeddings = get_word_embeddings(embeddings)\nprint(len(word_embeddings))\npickle.dump( word_embeddings, open( \"word_embeddings_subset.p\", \"wb\" ) )\n\nNow we will load the word embeddings as a Python dictionary. As stated, these have already been obtained through a machine learning algorithm.\n\nword_embeddings = pickle.load(open(\"word_embeddings_subset.p\", \"rb\"))\nlen(word_embeddings)  # there should be 243 words that will be used in this assignment\n\n243\n\n\nEach of the word embedding is a 300-dimensional vector.\n\nprint(\"dimension: {}\".format(word_embeddings['Spain'].shape[0]))\n\ndimension: 300\n\n\n\n\nPredict relationships among words\nNow you will write a function that will use the word embeddings to predict relationships among words. * The function will take as input three words. * The first two are related to each other. * It will predict a 4th word which is related to the third word in a similar manner as the two first words are related to each other. * As an example, “Athens is to Greece as Bangkok is to ______”? * You will write a program that is capable of finding the fourth word. * We will give you a hint to show you how to compute this.\nA similar analogy would be the following:\n\nYou will implement a function that can tell you the capital of a country. You should use the same methodology shown in the figure above. To do this, compute you’ll first compute cosine similarity metric or the Euclidean distance.\n\n\n1.2 Cosine Similarity\nThe cosine similarity function is:\n\\cos (\\theta)=\\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|}=\\frac{\\sum_{i=1}^{n} A_{i} B_{i}}{\\sqrt{\\sum_{i=1}^{n} A_{i}^{2}} \\sqrt{\\sum_{i=1}^{n} B_{i}^{2}}}\\tag{1}\nA and B represent the word vectors and A_i or B_i represent index i of that vector. & Note that if A and B are identical, you will get cos(\\theta) = 1. * Otherwise, if they are the total opposite, meaning, A= -B, then you would get cos(\\theta) = -1. * If you get cos(\\theta) =0, that means that they are orthogonal (or perpendicular). * Numbers between 0 and 1 indicate a similarity score. * Numbers between -1-0 indicate a dissimilarity score.\nInstructions: Implement a function that takes in two word vectors and computes the cosine distance.\n\n\nHints\n\n\n\n\nPython’s NumPy library  adds support for linear algebra operations (e.g., dot product, vector norm …).\n\n\nUse  numpy.dot .\n\n\nUse numpy.linalg.norm .\n\n\n\n\n# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef cosine_similarity(A, B):\n    '''\n    Input:\n        A: a numpy array which corresponds to a word vector\n        B: A numpy array which corresponds to a word vector\n    Output:\n        cos: numerical number representing the cosine similarity between A and B.\n    '''\n\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    \n    dot = None\n    norma = None\n    normb = None \n    cos = None\n\n    ### END CODE HERE ###\n    return cos\n\n\n# feel free to try different words\nking = word_embeddings['king']\nqueen = word_embeddings['queen']\n\ncosine_similarity(king, queen)\n\nExpected Output:\n\\approx 0.6510956\n\n\n1.3 Euclidean distance\nYou will now implement a function that computes the similarity between two vectors using the Euclidean distance. Euclidean distance is defined as:\n \\begin{aligned} d(\\mathbf{A}, \\mathbf{B})=d(\\mathbf{B}, \\mathbf{A}) &=\\sqrt{\\left(A_{1}-B_{1}\\right)^{2}+\\left(A_{2}-B_{2}\\right)^{2}+\\cdots+\\left(A_{n}-B_{n}\\right)^{2}} \\\\ &=\\sqrt{\\sum_{i=1}^{n}\\left(A_{i}-B_{i}\\right)^{2}} \\end{aligned}\n\nn is the number of elements in the vector\nA and B are the corresponding word vectors.\nThe more similar the words, the more likely the Euclidean distance will be close to 0.\n\nInstructions: Write a function that computes the Euclidean distance between two vectors.\n\n\nHints\n\n\n\n\nUse  numpy.linalg.norm .\n\n\n\n\n# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef euclidean(A, B):\n    \"\"\"\n    Input:\n        A: a numpy array which corresponds to a word vector\n        B: A numpy array which corresponds to a word vector\n    Output:\n        d: numerical number representing the Euclidean distance between A and B.\n    \"\"\"\n\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n\n    # euclidean distance\n\n    d = None\n\n    ### END CODE HERE ###\n\n    return d\n\n\n# Test your function\neuclidean(king, queen)\n\nExpected Output:\n2.4796925\n\n\n1.4 Finding the country of each capital\nNow, you will use the previous functions to compute similarities between vectors, and use these to find the capital cities of countries. You will write a function that takes in three words, and the embeddings dictionary. Your task is to find the capital cities. For example, given the following words:\n\n1: Athens 2: Greece 3: Baghdad,\n\nyour task is to predict the country 4: Iraq.\nInstructions:\n\nTo predict the capital you might want to look at the King - Man + Woman = Queen example above, and implement that scheme into a mathematical function, using the word embeddings and a similarity function.\nIterate over the embeddings dictionary and compute the cosine similarity score between your vector and the current word embedding.\nYou should add a check to make sure that the word you return is not any of the words that you fed into your function. Return the one with the highest score.\n\n\n# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef get_country(city1, country1, city2, embeddings):\n    \"\"\"\n    Input:\n        city1: a string (the capital city of country1)\n        country1: a string (the country of capital1)\n        city2: a string (the capital city of country2)\n        embeddings: a dictionary where the keys are words and values are their embeddings\n    Output:\n        countries: a dictionary with the most likely country and its similarity score\n    \"\"\"\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n\n    # store the city1, country 1, and city 2 in a set called group\n    group = set((None, None, None))\n\n    # get embeddings of city 1\n    city1_emb = None\n\n    # get embedding of country 1\n    country1_emb = None\n\n    # get embedding of city 2\n    city2_emb = None\n\n    # get embedding of country 2 (it's a combination of the embeddings of country 1, city 1 and city 2)\n    # Remember: King - Man + Woman = Queen\n    vec = None\n\n    # Initialize the similarity to -1 (it will be replaced by a similarities that are closer to +1)\n    similarity = -1\n\n    # initialize country to an empty string\n    country = ''\n\n    # loop through all words in the embeddings dictionary\n    for word in embeddings.keys():\n\n        # first check that the word is not already in the 'group'\n        if word not in group:\n\n            # get the word embedding\n            word_emb = None\n\n            # calculate cosine similarity between embedding of country 2 and the word in the embeddings dictionary\n            cur_similarity = None\n\n            # if the cosine similarity is more similar than the previously best similarity...\n            if cur_similarity &gt; similarity:\n\n                # update the similarity to the new, better similarity\n                similarity = None\n\n                # store the country as a tuple, which contains the word and the similarity\n                country = (None, None)\n\n    ### END CODE HERE ###\n\n    return country\n\n\n# Testing your function, note to make it more robust you can return the 5 most similar words.\nget_country('Athens', 'Greece', 'Cairo', word_embeddings)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[10], line 2\n      1 # Testing your function, note to make it more robust you can return the 5 most similar words.\n----&gt; 2 get_country('Athens', 'Greece', 'Cairo', word_embeddings)\n\nCell In[9], line 49, in get_country(city1, country1, city2, embeddings)\n     46 cur_similarity = None\n     48 # if the cosine similarity is more similar than the previously best similarity...\n---&gt; 49 if cur_similarity &gt; similarity:\n     50 \n     51     # update the similarity to the new, better similarity\n     52     similarity = None\n     54     # store the country as a tuple, which contains the word and the similarity\n\nTypeError: '&gt;' not supported between instances of 'NoneType' and 'int'\n\n\n\nExpected Output:\n(‘Egypt’, 0.7626821)\n\n\n1.5 Model Accuracy\nNow you will test your new function on the dataset and check the accuracy of the model:\n\\text{Accuracy}=\\frac{\\text{Correct \\# of predictions}}{\\text{Total \\# of predictions}}\nInstructions: Write a program that can compute the accuracy on the dataset provided for you. You have to iterate over every row to get the corresponding words and feed them into you get_country function above.\n\n\nHints\n\n\n\n\nUse  pandas.DataFrame.iterrows .\n\n\n\n\n# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef get_accuracy(word_embeddings, data):\n    '''\n    Input:\n        word_embeddings: a dictionary where the key is a word and the value is its embedding\n        data: a pandas dataframe containing all the country and capital city pairs\n    \n    Output:\n        accuracy: the accuracy of the model\n    '''\n\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    # initialize num correct to zero\n    num_correct = 0\n\n    # loop through the rows of the dataframe\n    for i, row in data.iterrows():\n\n        # get city1\n        city1 = None\n\n        # get country1\n        country1 = None\n\n        # get city2\n        city2 =  None\n\n        # get country2\n        country2 = None\n\n        # use get_country to find the predicted country2\n        predicted_country2, _ = None\n\n        # if the predicted country2 is the same as the actual country2...\n        if predicted_country2 == country2:\n            # increment the number of correct by 1\n            num_correct += None\n\n    # get the number of rows in the data dataframe (length of dataframe)\n    m = len(data)\n\n    # calculate the accuracy by dividing the number correct by m\n    accuracy = None\n\n    ### END CODE HERE ###\n    return accuracy\n\nNOTE: The cell below takes about 30 SECONDS to run.\n\naccuracy = get_accuracy(word_embeddings, data)\nprint(f\"Accuracy is {accuracy:.2f}\")\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[12], line 1\n----&gt; 1 accuracy = get_accuracy(word_embeddings, data)\n      2 print(f\"Accuracy is {accuracy:.2f}\")\n\nCell In[11], line 32, in get_accuracy(word_embeddings, data)\n     29 country2 = None\n     31 # use get_country to find the predicted country2\n---&gt; 32 predicted_country2, _ = None\n     34 # if the predicted country2 is the same as the actual country2...\n     35 if predicted_country2 == country2:\n     36     # increment the number of correct by 1\n\nTypeError: cannot unpack non-iterable NoneType object\n\n\n\nExpected Output:\n\\approx 0.92"
  },
  {
    "objectID": "posts/c1w3/lab02.html",
    "href": "posts/c1w3/lab02.html",
    "title": "Manipulating word embeddings",
    "section": "",
    "text": "course banner\nIn this week’s assignment, you are going to use a pre-trained word embedding for finding word analogies and equivalence. This exercise can be used as an Intrinsic Evaluation for the word embedding performance. In this notebook, you will apply linear algebra operations using NumPy to find analogies between words manually. This will help you to prepare for this week’s assignment.\nimport pandas as pd # Library for Dataframes \nimport numpy as np # Library for math functions\nimport pickle # Python object serialization library. Not secure\n\nword_embeddings = pickle.load( open( \"./data/word_embeddings_subset.p\", \"rb\" ) )\nlen(word_embeddings) # there should be 243 words that will be used in this assignment\n\n243\nNow that the model is loaded, we can take a look at the word representations. First, note that word_embeddings is a dictionary. Each word is the key to the entry, and the value is its corresponding vector presentation. Remember that square brackets allow access to any entry if the key exists.\ncountryVector = word_embeddings['country'] # Get the vector representation for the word 'country'\nprint(type(countryVector)) # Print the type of the vector. Note it is a numpy array\nprint(countryVector) # Print the values of the vector.  \n\n&lt;class 'numpy.ndarray'&gt;\n[-0.08007812  0.13378906  0.14355469  0.09472656 -0.04736328 -0.02355957\n -0.00854492 -0.18652344  0.04589844 -0.08154297 -0.03442383 -0.11621094\n  0.21777344 -0.10351562 -0.06689453  0.15332031 -0.19335938  0.26367188\n -0.13671875 -0.05566406  0.07470703 -0.00070953  0.09375    -0.14453125\n  0.04296875 -0.01916504 -0.22558594 -0.12695312 -0.0168457   0.05224609\n  0.0625     -0.1484375  -0.01965332  0.17578125  0.10644531 -0.04760742\n -0.10253906 -0.28515625  0.10351562  0.20800781 -0.07617188 -0.04345703\n  0.08642578  0.08740234  0.11767578  0.20996094 -0.07275391  0.1640625\n -0.01135254  0.0025177   0.05810547 -0.03222656  0.06884766  0.046875\n  0.10107422  0.02148438 -0.16210938  0.07128906 -0.16210938  0.05981445\n  0.05102539 -0.05566406  0.06787109 -0.03759766  0.04345703 -0.03173828\n -0.03417969 -0.01116943  0.06201172 -0.08007812 -0.14941406  0.11914062\n  0.02575684  0.00302124  0.04711914 -0.17773438  0.04101562  0.05541992\n  0.00598145  0.03027344 -0.07666016 -0.109375    0.02832031 -0.10498047\n  0.0100708  -0.03149414 -0.22363281 -0.03125    -0.01147461  0.17285156\n  0.08056641 -0.10888672 -0.09570312 -0.21777344 -0.07910156 -0.10009766\n  0.06396484 -0.11962891  0.18652344 -0.02062988 -0.02172852  0.29296875\n -0.00793457  0.0324707  -0.15136719  0.00227356 -0.03540039 -0.13378906\n  0.0546875  -0.03271484 -0.01855469 -0.10302734 -0.13378906  0.11425781\n  0.16699219  0.01361084 -0.02722168 -0.2109375   0.07177734  0.08691406\n -0.09960938  0.01422119 -0.18261719  0.00741577  0.01965332  0.00738525\n -0.03271484 -0.15234375 -0.26367188 -0.14746094  0.03320312 -0.03344727\n -0.01000977  0.01855469  0.00183868 -0.10498047  0.09667969  0.07910156\n  0.11181641  0.13085938 -0.08740234 -0.1328125   0.05004883  0.19824219\n  0.0612793   0.16210938  0.06933594  0.01281738  0.01550293  0.01531982\n  0.11474609  0.02758789  0.13769531 -0.08349609  0.01123047 -0.20507812\n -0.12988281 -0.16699219  0.20410156 -0.03588867 -0.10888672  0.0534668\n  0.15820312 -0.20410156  0.14648438 -0.11572266  0.01855469 -0.13574219\n  0.24121094  0.12304688 -0.14550781  0.17578125  0.11816406 -0.30859375\n  0.10888672 -0.22363281  0.19335938 -0.15722656 -0.07666016 -0.09082031\n -0.19628906 -0.23144531 -0.09130859 -0.14160156  0.06347656  0.03344727\n -0.03369141  0.06591797  0.06201172  0.3046875   0.16796875 -0.11035156\n -0.03833008 -0.02563477 -0.09765625  0.04467773 -0.0534668   0.11621094\n -0.15039062 -0.16308594 -0.15527344  0.04638672  0.11572266 -0.06640625\n -0.04516602  0.02331543 -0.08105469 -0.0255127  -0.07714844  0.0016861\n  0.15820312  0.00994873 -0.06445312  0.15722656 -0.03112793  0.10644531\n -0.140625    0.23535156 -0.11279297  0.16015625  0.00061798 -0.1484375\n  0.02307129 -0.109375    0.05444336 -0.14160156  0.11621094  0.03710938\n  0.14746094 -0.04199219 -0.01391602 -0.03881836  0.02783203  0.10205078\n  0.07470703  0.20898438 -0.04223633 -0.04150391 -0.00588989 -0.14941406\n -0.04296875 -0.10107422 -0.06176758  0.09472656  0.22265625 -0.02307129\n  0.04858398 -0.15527344 -0.02282715 -0.04174805  0.16699219 -0.09423828\n  0.14453125  0.11132812  0.04223633 -0.16699219  0.10253906  0.16796875\n  0.12597656 -0.11865234 -0.0213623  -0.08056641  0.24316406  0.15527344\n  0.16503906  0.00854492 -0.12255859  0.08691406 -0.11914062 -0.02941895\n  0.08349609 -0.03100586  0.13964844 -0.05151367  0.00765991 -0.04443359\n -0.04980469 -0.03222656 -0.00952148 -0.10888672 -0.10302734 -0.15722656\n  0.19335938  0.04858398  0.015625   -0.08105469 -0.11621094 -0.01989746\n  0.05737305  0.06103516 -0.14550781  0.06738281 -0.24414062 -0.07714844\n  0.04760742 -0.07519531 -0.14941406 -0.04418945  0.09716797  0.06738281]\nIt is important to note that we store each vector as a NumPy array. It allows us to use the linear algebra operations on it.\nThe vectors have a size of 300, while the vocabulary size of Google News is around 3 million words!\n#Get the vector for a given word:\ndef vec(w):\n    return word_embeddings[w]"
  },
  {
    "objectID": "posts/c1w3/lab02.html#operating-on-word-embeddings",
    "href": "posts/c1w3/lab02.html#operating-on-word-embeddings",
    "title": "Manipulating word embeddings",
    "section": "Operating on word embeddings",
    "text": "Operating on word embeddings\nRemember that understanding the data is one of the most critical steps in Data Science. Word embeddings are the result of machine learning processes and will be part of the input for further processes. These word embedding needs to be validated or at least understood because the performance of the derived model will strongly depend on its quality.\nWord embeddings are multidimensional arrays, usually with hundreds of attributes that pose a challenge for its interpretation.\nIn this notebook, we will visually inspect the word embedding of some words using a pair of attributes. Raw attributes are not the best option for the creation of such charts but will allow us to illustrate the mechanical part in Python.\nIn the next cell, we make a beautiful plot for the word embeddings of some words. Even if plotting the dots gives an idea of the words, the arrow representations help to visualize the vector’s alignment as well.\n\nimport matplotlib.pyplot as plt # Import matplotlib\n%matplotlib inline\n\nwords = ['oil', 'gas', 'happy', 'sad', 'city', 'town', 'village', 'country', 'continent', 'petroleum', 'joyful']\n\nbag2d = np.array([vec(word) for word in words]) # Convert each word to its vector representation\n\nfig, ax = plt.subplots(figsize = (10, 10)) # Create custom size image\n\ncol1 = 3 # Select the column for the x axis\ncol2 = 2 # Select the column for the y axis\n\n# Print an arrow for each word\nfor word in bag2d:\n    ax.arrow(0, 0, word[col1], word[col2], head_width=0.005, head_length=0.005, fc='r', ec='r', width = 1e-5)\n\n    \nax.scatter(bag2d[:, col1], bag2d[:, col2]); # Plot a dot for each word\n\n# Add the word label over each dot in the scatter plot\nfor i in range(0, len(words)):\n    ax.annotate(words[i], (bag2d[i, col1], bag2d[i, col2]))\n\n\nplt.show()\n\nText(0.06396484375, -0.279296875, 'oil')\n\n\nText(0.01080322265625, -0.138671875, 'gas')\n\n\nText(0.025390625, 0.00160980224609375, 'happy')\n\n\nText(-0.044677734375, 0.06689453125, 'sad')\n\n\nText(-0.0400390625, 0.18359375, 'city')\n\n\nText(-0.1611328125, 0.030029296875, 'town')\n\n\nText(-0.2158203125, 0.09033203125, 'village')\n\n\nText(0.0947265625, 0.1435546875, 'country')\n\n\nText(0.1279296875, 0.2392578125, 'continent')\n\n\nText(0.10888671875, -0.22265625, 'petroleum')\n\n\nText(0.083984375, 0.1298828125, 'joyful')\n\n\n\n\n\n\n\n\n\nNote that similar words like ‘village’ and ‘town’ or ‘petroleum’, ‘oil’, and ‘gas’ tend to point in the same direction. Also, note that ‘sad’ and ‘happy’ looks close to each other; however, the vectors point in opposite directions.\nIn this chart, one can figure out the angles and distances between the words. Some words are close in both kinds of distance metrics."
  },
  {
    "objectID": "posts/c1w3/lab02.html#word-distance",
    "href": "posts/c1w3/lab02.html#word-distance",
    "title": "Manipulating word embeddings",
    "section": "Word distance",
    "text": "Word distance\nNow plot the words ‘sad’, ‘happy’, ‘town’, and ‘village’. In this same chart, display the vector from ‘village’ to ‘town’ and the vector from ‘sad’ to ‘happy’. Let us use NumPy for these linear algebra operations.\n\nwords = ['sad', 'happy', 'town', 'village']\n\nbag2d = np.array([vec(word) for word in words]) # Convert each word to its vector representation\n\nfig, ax = plt.subplots(figsize = (10, 10)) # Create custom size image\n\ncol1 = 3 # Select the column for the x axe\ncol2 = 2 # Select the column for the y axe\n\n# Print an arrow for each word\nfor word in bag2d:\n    ax.arrow(0, 0, word[col1], word[col2], head_width=0.0005, head_length=0.0005, fc='r', ec='r', width = 1e-5)\n    \n# print the vector difference between village and town\nvillage = vec('village')\ntown = vec('town')\ndiff = town - village\nax.arrow(village[col1], village[col2], diff[col1], diff[col2], fc='b', ec='b', width = 1e-5)\n\n# print the vector difference between village and town\nsad = vec('sad')\nhappy = vec('happy')\ndiff = happy - sad\nax.arrow(sad[col1], sad[col2], diff[col1], diff[col2], fc='b', ec='b', width = 1e-5)\n\n\nax.scatter(bag2d[:, col1], bag2d[:, col2]); # Plot a dot for each word\n\n# Add the word label over each dot in the scatter plot\nfor i in range(0, len(words)):\n    ax.annotate(words[i], (bag2d[i, col1], bag2d[i, col2]))\n\n\nplt.show()\n\nText(-0.044677734375, 0.06689453125, 'sad')\n\n\nText(0.025390625, 0.00160980224609375, 'happy')\n\n\nText(-0.1611328125, 0.030029296875, 'town')\n\n\nText(-0.2158203125, 0.09033203125, 'village')"
  },
  {
    "objectID": "posts/c1w3/lab02.html#linear-algebra-on-word-embeddings",
    "href": "posts/c1w3/lab02.html#linear-algebra-on-word-embeddings",
    "title": "Manipulating word embeddings",
    "section": "Linear algebra on word embeddings",
    "text": "Linear algebra on word embeddings\nIn the lectures, we saw the analogies between words using algebra on word embeddings. Let us see how to do it in Python with Numpy.\nTo start, get the norm of a word in the word embedding.\n\nprint(np.linalg.norm(vec('town'))) # Print the norm of the word town\nprint(np.linalg.norm(vec('sad'))) # Print the norm of the word sad\n\n2.3858097\n2.9004838"
  },
  {
    "objectID": "posts/c1w3/lab02.html#predicting-capitals",
    "href": "posts/c1w3/lab02.html#predicting-capitals",
    "title": "Manipulating word embeddings",
    "section": "Predicting capitals",
    "text": "Predicting capitals\nNow, applying vector difference and addition, one can create a vector representation for a new word. For example, we can say that the vector difference between ‘France’ and ‘Paris’ represents the concept of Capital.\nOne can move from the city of Madrid in the direction of the concept of Capital, and obtain something close to the corresponding country to which Madrid is the Capital.\n\ncapital = vec('France') - vec('Paris')\ncountry = vec('Madrid') + capital\n\nprint(country[0:5]) # Print the first 5 values of the vector\n\n[-0.02905273 -0.2475586   0.53952026  0.20581055 -0.14862823]\n\n\nWe can observe that the vector ‘country’ that we expected to be the same as the vector for Spain is not exactly it.\n\ndiff = country - vec('Spain')\nprint(diff[0:10])\n\n[-0.06054688 -0.06494141  0.37643433  0.08129883 -0.13007355 -0.00952148\n -0.03417969 -0.00708008  0.09790039 -0.01867676]\n\n\nSo, we have to look for the closest words in the embedding that matches the candidate country. If the word embedding works as expected, the most similar word must be ‘Spain’. Let us define a function that helps us to do it. We will store our word embedding as a DataFrame, which facilitate the lookup operations based on the numerical vectors.\n\n# Create a dataframe out of the dictionary embedding. This facilitate the algebraic operations\nkeys = word_embeddings.keys()\ndata = []\nfor key in keys:\n    data.append(word_embeddings[key])\n\nembedding = pd.DataFrame(data=data, index=keys)\n# Define a function to find the closest word to a vector:\ndef find_closest_word(v, k = 1):\n    # Calculate the vector difference from each word to the input vector\n    diff = embedding.values - v \n    # Get the norm of each difference vector. \n    # It means the squared euclidean distance from each word to the input vector\n    delta = np.sum(diff * diff, axis=1)\n    # Find the index of the minimun distance in the array\n    i = np.argmin(delta)\n    # Return the row name for this item\n    return embedding.iloc[i].name\n\n\n# Print some rows of the embedding as a Dataframe\nembedding.head(10)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n\n\n\n\ncountry\n-0.080078\n0.133789\n0.143555\n0.094727\n-0.047363\n-0.023560\n-0.008545\n-0.186523\n0.045898\n-0.081543\n...\n-0.145508\n0.067383\n-0.244141\n-0.077148\n0.047607\n-0.075195\n-0.149414\n-0.044189\n0.097168\n0.067383\n\n\ncity\n-0.010071\n0.057373\n0.183594\n-0.040039\n-0.029785\n-0.079102\n0.071777\n0.013306\n-0.143555\n0.011292\n...\n0.024292\n-0.168945\n-0.062988\n0.117188\n-0.020508\n0.030273\n-0.247070\n-0.122559\n0.076172\n-0.234375\n\n\nChina\n-0.073242\n0.135742\n0.108887\n0.083008\n-0.127930\n-0.227539\n0.151367\n-0.045654\n-0.065430\n0.034424\n...\n0.140625\n0.087402\n0.152344\n0.079590\n0.006348\n-0.037842\n-0.183594\n0.137695\n0.093750\n-0.079590\n\n\nIraq\n0.191406\n0.125000\n-0.065430\n0.060059\n-0.285156\n-0.102539\n0.117188\n-0.351562\n-0.095215\n0.200195\n...\n-0.100586\n-0.077148\n-0.123047\n0.193359\n-0.153320\n0.089355\n-0.173828\n-0.054688\n0.302734\n0.105957\n\n\noil\n-0.139648\n0.062256\n-0.279297\n0.063965\n0.044434\n-0.154297\n-0.184570\n-0.498047\n0.047363\n0.110840\n...\n-0.195312\n-0.345703\n0.217773\n-0.091797\n0.051025\n0.061279\n0.194336\n0.204102\n0.235352\n-0.051025\n\n\ntown\n0.123535\n0.159180\n0.030029\n-0.161133\n0.015625\n0.111816\n0.039795\n-0.196289\n-0.039307\n0.067871\n...\n-0.007935\n-0.091797\n-0.265625\n0.029297\n0.089844\n-0.049805\n-0.202148\n-0.079590\n0.068848\n-0.164062\n\n\nCanada\n-0.136719\n-0.154297\n0.269531\n0.273438\n0.086914\n-0.076172\n-0.018677\n0.006256\n0.077637\n-0.211914\n...\n0.105469\n0.030762\n-0.039307\n0.183594\n-0.117676\n0.191406\n0.074219\n0.020996\n0.285156\n-0.257812\n\n\nLondon\n-0.267578\n0.092773\n-0.238281\n0.115234\n-0.006836\n0.221680\n-0.251953\n-0.055420\n0.020020\n0.149414\n...\n-0.008667\n-0.008484\n-0.053223\n0.197266\n-0.296875\n0.064453\n0.091797\n0.058350\n0.022583\n-0.101074\n\n\nEngland\n-0.198242\n0.115234\n0.062500\n-0.058350\n0.226562\n0.045898\n-0.062256\n-0.202148\n0.080566\n0.021606\n...\n0.135742\n0.109375\n-0.121582\n0.008545\n-0.171875\n0.086914\n0.070312\n0.003281\n0.069336\n0.056152\n\n\nAustralia\n0.048828\n-0.194336\n-0.041504\n0.084473\n-0.114258\n-0.208008\n-0.164062\n-0.269531\n0.079102\n0.275391\n...\n0.021118\n0.171875\n0.042236\n0.221680\n-0.239258\n-0.106934\n0.030884\n0.006622\n0.051270\n-0.135742\n\n\n\n\n10 rows × 300 columns\n\n\n\nNow let us find the name that corresponds to our numerical country:\n\nfind_closest_word(country)\n\n'Spain'"
  },
  {
    "objectID": "posts/c1w3/lab02.html#predicting-other-countries",
    "href": "posts/c1w3/lab02.html#predicting-other-countries",
    "title": "Manipulating word embeddings",
    "section": "Predicting other Countries",
    "text": "Predicting other Countries\n\nfind_closest_word(vec('Italy') - vec('Rome') + vec('Madrid'))\n\n'Spain'\n\n\n\nprint(find_closest_word(vec('Berlin') + capital))\nprint(find_closest_word(vec('Beijing') + capital))\n\nGermany\nChina\n\n\nHowever, it does not always work.\n\nprint(find_closest_word(vec('Lisbon') + capital))\n\nLisbon"
  },
  {
    "objectID": "posts/c1w3/lab02.html#represent-a-sentence-as-a-vector",
    "href": "posts/c1w3/lab02.html#represent-a-sentence-as-a-vector",
    "title": "Manipulating word embeddings",
    "section": "Represent a sentence as a vector",
    "text": "Represent a sentence as a vector\nA whole sentence can be represented as a vector by summing all the word vectors that conform to the sentence. Let us see.\n\ndoc = \"Spain petroleum city king\"\nvdoc = [vec(x) for x in doc.split(\" \")]\ndoc2vec = np.sum(vdoc, axis = 0)\ndoc2vec\n\narray([ 2.87475586e-02,  1.03759766e-01,  1.32629395e-01,  3.33007812e-01,\n       -2.61230469e-02, -5.95703125e-01, -1.25976562e-01, -1.01306152e+00,\n       -2.18544006e-01,  6.60705566e-01, -2.58300781e-01, -2.09960938e-02,\n       -7.71484375e-02, -3.07128906e-01, -5.94726562e-01,  2.00561523e-01,\n       -1.04980469e-02, -1.10748291e-01,  4.82177734e-02,  6.38977051e-01,\n        2.36083984e-01, -2.69775391e-01,  3.90625000e-02,  4.16503906e-01,\n        2.83416748e-01, -7.25097656e-02, -3.12988281e-01,  1.05712891e-01,\n        3.22265625e-02,  2.38403320e-01,  3.88183594e-01, -7.51953125e-02,\n       -1.26281738e-01,  6.60644531e-01, -7.89794922e-01, -7.04345703e-02,\n       -1.14379883e-01, -4.78515625e-02,  4.76318359e-01,  5.31127930e-01,\n        8.10546875e-02, -1.17553711e-01,  1.02050781e+00,  5.59814453e-01,\n       -1.17187500e-01,  1.21826172e-01, -5.51574707e-01,  1.44531250e-01,\n       -7.66113281e-01,  5.36102295e-01, -2.80029297e-01,  3.85986328e-01,\n       -2.39135742e-01, -2.86865234e-02, -5.10498047e-01,  2.59658813e-01,\n       -7.52929688e-01,  4.32128906e-02, -7.17773438e-02, -1.26708984e-01,\n        4.40673828e-02,  5.12939453e-01, -5.15808105e-01,  1.20117188e-01,\n       -5.52978516e-02, -3.92089844e-01, -3.15917969e-01,  1.57226562e-01,\n       -3.19702148e-01,  1.75170898e-01, -3.81835938e-01, -2.07031250e-01,\n       -4.72717285e-02, -2.79296875e-01, -3.29040527e-01, -1.69067383e-01,\n        1.61132812e-02,  1.71569824e-01,  5.73730469e-02, -2.44140625e-03,\n        8.34960938e-02, -1.58203125e-01, -3.10119629e-01,  5.28564453e-02,\n        8.60595703e-02,  5.12695312e-02, -7.22900391e-01,  4.97924805e-01,\n       -5.85937500e-03,  4.49951172e-01,  3.82446289e-01, -2.80029297e-01,\n       -3.28125000e-01, -6.27441406e-02, -4.81933594e-01,  1.93176270e-02,\n       -1.69326782e-01, -4.28649902e-01,  5.39062500e-01, -1.28417969e-01,\n       -8.83789062e-02,  5.13916016e-01,  9.13085938e-02, -1.60156250e-01,\n        6.86035156e-02, -9.74121094e-02, -3.70712280e-01, -3.27270508e-01,\n        1.77978516e-01, -4.65332031e-01,  1.70410156e-01,  9.08203125e-02,\n        2.76857376e-01, -1.69677734e-01,  3.27728271e-01, -3.12500000e-02,\n       -2.20809937e-01, -3.46679688e-01,  4.67407227e-01,  5.31860352e-01,\n       -1.30615234e-01, -2.36816406e-02, -6.56250000e-01, -5.79589844e-01,\n       -2.05810547e-01, -3.03222656e-01,  1.94259644e-01, -7.28515625e-01,\n       -4.92522240e-01, -5.37109375e-01, -3.47656250e-01,  1.08642578e-01,\n       -1.41601562e-01, -2.07031250e-01,  2.52441406e-01, -7.78808594e-02,\n       -5.02441406e-01,  1.53808594e-02,  8.64257812e-02,  2.59765625e-01,\n        6.64062500e-02, -7.12890625e-01, -1.45751953e-01,  7.56835938e-03,\n        4.87792969e-01,  1.39160156e-01,  1.15722656e-01,  1.28662109e-01,\n       -4.75585938e-01,  2.21191406e-01,  3.25317383e-01,  1.06323242e-01,\n       -6.11083984e-01, -3.59619141e-01,  6.54296875e-02, -2.41699219e-01,\n       -6.29882812e-02, -1.62109375e-01,  4.26269531e-01, -4.38354492e-01,\n        1.93725586e-01,  4.89562988e-01,  5.31494141e-01, -7.29370117e-02,\n        1.77246094e-01,  9.39941406e-02,  2.92236328e-01, -2.74047852e-01,\n        2.63366699e-02,  4.36035156e-01, -3.76953125e-01,  3.10546875e-01,\n        4.87304688e-01, -2.43041992e-01,  1.21612549e-02, -3.80371094e-01,\n        3.80493164e-01, -6.22436523e-01, -3.98071289e-01,  1.24206543e-01,\n       -8.20312500e-01, -2.72583008e-01, -6.21582031e-01, -4.87060547e-01,\n        3.06671143e-01, -2.61230469e-01,  5.12451172e-01,  5.55694580e-01,\n        5.66894531e-01,  7.33886719e-01, -1.75781250e-01,  4.13574219e-01,\n       -2.54272461e-01,  1.32507324e-01, -4.78515625e-01,  4.63256836e-01,\n       -6.21948242e-02, -1.80664062e-01, -5.46386719e-01, -6.31103516e-01,\n       -1.47949219e-01, -3.15185547e-01, -7.12890625e-02, -7.67578125e-01,\n        3.92272949e-01, -1.97753906e-01,  2.23144531e-01, -5.07324219e-01,\n        8.39843750e-02, -4.98657227e-02,  1.01074219e-01,  2.07885742e-01,\n       -2.77343750e-01,  1.03027344e-01, -1.38671875e-01,  2.87353516e-01,\n       -4.81895447e-01, -1.66748047e-01, -1.47277832e-01,  3.61633301e-01,\n        6.38504028e-02, -6.69189453e-01,  1.95312500e-03, -7.34375000e-01,\n       -1.28158569e-01,  9.76562500e-04, -7.08007812e-02,  3.72558594e-01,\n        8.31176758e-01,  5.94482422e-01,  5.37109375e-02, -3.00140381e-01,\n       -4.53857422e-01,  1.11511230e-01, -1.32812500e-01,  1.25732422e-01,\n        3.39843750e-01, -2.48352051e-01, -1.62353516e-02, -2.84667969e-01,\n        4.70703125e-01, -4.48242188e-01,  8.50753784e-02,  2.69042969e-01,\n        3.98254395e-03, -3.53759766e-01, -3.90625000e-02, -3.22753906e-01,\n       -6.90917969e-02, -4.13818359e-02,  1.35314941e-01, -8.50396156e-02,\n        1.28417969e-01,  6.15966797e-01,  3.55957031e-01, -6.05468750e-02,\n       -2.25463867e-01, -2.62207031e-01, -2.72949219e-01, -5.16113281e-01,\n        1.59179688e-01,  2.74902344e-01, -7.61718750e-02, -3.41796875e-03,\n        4.37500000e-01,  2.98583984e-01, -4.40795898e-01, -3.43261719e-01,\n        1.73583984e-01,  3.32092285e-01, -2.12646484e-01,  5.76171875e-01,\n        2.06787109e-01, -7.91015625e-02,  5.79695702e-02, -1.01806641e-01,\n       -7.06787109e-01, -3.40576172e-02, -4.11865234e-01,  9.82666016e-02,\n       -1.70410156e-01, -4.18212891e-01,  8.39233398e-01, -1.15722656e-01,\n        1.28173828e-01, -2.07763672e-01, -4.08203125e-01, -1.77612305e-01,\n        1.01196289e-01,  4.24072266e-01, -5.26428223e-02, -5.58593750e-01,\n        1.12304688e-02, -1.12060547e-01, -9.42382812e-02,  2.35595703e-02,\n       -3.92578125e-01, -7.12890625e-02,  5.69824219e-01,  9.81445312e-02],\n      dtype=float32)\n\n\n\nfind_closest_word(doc2vec)\n\n'petroleum'\n\n\nCongratulations! You have finished the introduction to word embeddings manipulation!"
  },
  {
    "objectID": "posts/c1lab3/index.html",
    "href": "posts/c1lab3/index.html",
    "title": "Visualizing tweets and the Logistic Regression model",
    "section": "",
    "text": "course banner\nObjectives:\nSteps:",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Lab on visualizing tweets and the Logistic Regression model"
    ]
  },
  {
    "objectID": "posts/c1lab3/index.html#import-the-required-libraries",
    "href": "posts/c1lab3/index.html#import-the-required-libraries",
    "title": "Visualizing tweets and the Logistic Regression model",
    "section": "Import the required libraries",
    "text": "Import the required libraries\nWe will be using NLTK, an opensource NLP library, for collecting, handling, and processing Twitter data. In this lab, we will use the example dataset that comes alongside with NLTK. This dataset has been manually annotated and serves to establish baselines for models quickly. So, to start, let’s import the required libraries.\n\nimport nltk                         # NLP toolbox\nfrom os import getcwd\nimport pandas as pd                 # Library for Dataframes \nfrom nltk.corpus import twitter_samples \nimport matplotlib.pyplot as plt     # Library for visualization\nimport numpy as np                  # Library for math functions\nfrom utils import process_tweet, build_freqs # Our functions for NLP",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Lab on visualizing tweets and the Logistic Regression model"
    ]
  },
  {
    "objectID": "posts/c1lab3/index.html#load-the-nltk-sample-dataset",
    "href": "posts/c1lab3/index.html#load-the-nltk-sample-dataset",
    "title": "Visualizing tweets and the Logistic Regression model",
    "section": "Load the NLTK sample dataset",
    "text": "Load the NLTK sample dataset\nTo complete this lab, you need the sample dataset of the previous lab. Here, we assume the files are already available, and we only need to load into Python lists.\n\n# select the set of positive and negative tweets\nall_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')\ntweets = all_positive_tweets + all_negative_tweets ## Concatenate the lists. \nlabels = np.append(np.ones((len(all_positive_tweets),1)), np.zeros((len(all_negative_tweets),1)), axis = 0)\n\n# split the data into two pieces, one for training and one for testing (validation set) \ntrain_pos  = all_positive_tweets[:4000]\ntrain_neg  = all_negative_tweets[:4000]\ntrain_x = train_pos + train_neg \nprint(\"Number of tweets: \", len(train_x))\n\nNumber of tweets:  8000",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Lab on visualizing tweets and the Logistic Regression model"
    ]
  },
  {
    "objectID": "posts/c1lab3/index.html#load-a-pretrained-logistic-regression-model",
    "href": "posts/c1lab3/index.html#load-a-pretrained-logistic-regression-model",
    "title": "Visualizing tweets and the Logistic Regression model",
    "section": "Load a pretrained Logistic Regression model",
    "text": "Load a pretrained Logistic Regression model\nIn the same way, as part of this week’s assignment, a Logistic regression model must be trained. The next cell contains the resulting model from such training. Notice that a list of 3 numeric values represents the whole model, that we have called theta \\theta.\n\ntheta = [7e-08, 0.0005239, -0.00055517]",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Lab on visualizing tweets and the Logistic Regression model"
    ]
  },
  {
    "objectID": "posts/c1lab3/index.html#plot-the-samples-in-a-scatter-plot",
    "href": "posts/c1lab3/index.html#plot-the-samples-in-a-scatter-plot",
    "title": "Visualizing tweets and the Logistic Regression model",
    "section": "Plot the samples in a scatter plot",
    "text": "Plot the samples in a scatter plot\nThe vector theta represents a plane that split our feature space into two parts. Samples located over that plane are considered positive, and samples located under that plane are considered negative. Remember that we have a 3D feature space, i.e., each tweet is represented as a vector comprised of three values: [bias, positive_sum, negative_sum], always having bias = 1. If we ignore the bias term, we can plot each tweet in a cartesian plane, using positive_sum and negative_sum. In the cell below, we do precisely this. Additionally, we color each tweet, depending on its class. Positive tweets will be green and negative tweets will be red.\n\n# Plot the samples using columns 1 and 2 of the matrix\nfig, ax = plt.subplots(figsize = (8, 8))\ncolors = ['red', 'green']\n\n# Color based on the sentiment Y\nax.scatter(X[:,1], X[:,2], c=[colors[int(k)] for k in Y], s = 0.1)  # Plot a dot for each pair of words\nplt.xlabel(\"Positive\")\nplt.ylabel(\"Negative\")\n\nText(0.5, 0, 'Positive')\n\n\nText(0, 0.5, 'Negative')\n\n\n\n\n\n\n\n\n\nFrom the plot, it is evident that the features that we have chosen to represent tweets as numerical vectors allow an almost perfect separation between positive and negative tweets. So you can expect a very high accuracy for this model! ## Plot the model alongside the data We will draw a gray line to show the cutoff between the positive and negative regions. In other words, the gray line marks the line where  z = \\theta * x = 0. To draw this line, we have to solve the above equation in terms of one of the independent variables.  z = \\theta * x = 0  x = [1, pos, neg]   z(\\theta, x) = \\theta_0+ \\theta_1 * pos + \\theta_2 * neg = 0   neg = (-\\theta_0 - \\theta_1 * pos) / \\theta_2  The red and green lines that point in the direction of the corresponding sentiment are calculated using a perpendicular line to the separation line calculated in the previous equations(neg function). It must point in the same direction as the derivative of the Logit function, but the magnitude may differ. It is only for a visual representation of the model. direction = pos * \\theta_2 / \\theta_1\n\n# Equation for the separation plane\n\n# It give a value in the negative axe as a function of a positive value\n\n# f(pos, neg, W) = w0 + w1 * pos + w2 * neg = 0\n\n# s(pos, W) = (w0 - w1 * pos) / w2\ndef neg(theta, pos):\n    return (-theta[0] - pos * theta[1]) / theta[2]\n\n# Equation for the direction of the sentiments change\n\n# We don't care about the magnitude of the change. We are only interested \n\n# in the direction. So this direction is just a perpendicular function to the \n\n# separation plane\n\n# df(pos, W) = pos * w2 / w1\ndef direction(theta, pos):\n    return    pos * theta[2] / theta[1]\n\nThe green line in the chart points in the direction where z &gt; 0 and the red line points in the direction where z &lt; 0. The direction of these lines are given by the weights \\theta_1 and \\theta_2\n\n# Plot the samples using columns 1 and 2 of the matrix\nfig, ax = plt.subplots(figsize = (8, 8))\ncolors = ['red', 'green']\n\n# Color base on the sentiment Y\nax.scatter(X[:,1], X[:,2], c=[colors[int(k)] for k in Y], s = 0.1)  # Plot a dot for each pair of words\nplt.xlabel(\"Positive\")\nplt.ylabel(\"Negative\")\n\n# Now lets represent the logistic regression model in this chart. \nmaxpos = np.max(X[:,1])\noffset = 5000 # The pos value for the direction vectors origin\n\n# Plot a gray line that divides the 2 areas.\nax.plot([0,  maxpos], [neg(theta, 0),   neg(theta, maxpos)], color = 'gray') \n\n# Plot a green line pointing to the positive direction\nax.arrow(offset, neg(theta, offset), offset, direction(theta, offset), head_width=500, head_length=500, fc='g', ec='g')\n\n# Plot a red line pointing to the negative direction\nax.arrow(offset, neg(theta, offset), -offset, -direction(theta, offset), head_width=500, head_length=500, fc='r', ec='r')\nplt.show()\n\nText(0.5, 0, 'Positive')\n\n\nText(0, 0.5, 'Negative')\n\n\n\n\n\n\n\n\n\nNote that more critical than the Logistic regression itself, are the features extracted from tweets that allow getting the right results in this exercise. That is all, folks. Hopefully, now you understand better what the Logistic regression model represents, and why it works that well for this specific problem.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Lab on visualizing tweets and the Logistic Regression model"
    ]
  },
  {
    "objectID": "posts/c2w2/lab01.html",
    "href": "posts/c2w2/lab01.html",
    "title": "Parts-of-Speech Tagging - First Steps: Working with text files, Creating a Vocabulary and Handling Unknown Words",
    "section": "",
    "text": "course banner\nIn this lecture notebook you will create a vocabulary from a tagged dataset and learn how to deal with words that are not present in this vocabulary when working with other text sources.\nAside from this you will also learn how to:\nimport string\nfrom collections import defaultdict"
  },
  {
    "objectID": "posts/c2w2/lab01.html#processing-new-text-sources",
    "href": "posts/c2w2/lab01.html#processing-new-text-sources",
    "title": "Parts-of-Speech Tagging - First Steps: Working with text files, Creating a Vocabulary and Handling Unknown Words",
    "section": "Processing new text sources",
    "text": "Processing new text sources\n\nDealing with unknown words\nNow that you have a vocabulary, you will use it when processing new text sources. A new text will have words that do not appear in the current vocabulary. To tackle this, you can simply classify each new word as an unknown one, but you can do better by creating a function that tries to classify the type of each unknown word and assign it a corresponding unknown token.\nThis function will do the following checks and return an appropriate token:\n\nCheck if the unknown word contains any character that is a digit\n\nreturn --unk_digit--\n\nCheck if the unknown word contains any punctuation character\n\nreturn --unk_punct--\n\nCheck if the unknown word contains any upper-case character\n\nreturn --unk_upper--\n\nCheck if the unknown word ends with a suffix that could indicate it is a noun, verb, adjective or adverb\n\nreturn --unk_noun--, --unk_verb--, --unk_adj--, --unk_adv-- respectively\n\n\nIf a word fails to fall under any condition then its token will be a plain --unk--. The conditions will be evaluated in the same order as listed here. So if a word contains a punctuation character but does not contain digits, it will fall under the second condition. To achieve this behaviour some if/elif statements can be used along with early returns.\nThis function is implemented next. Notice that the any() function is being heavily used. It returns True if at least one of the cases it evaluates is True.\n\ndef assign_unk(word):\n    \"\"\"\n    Assign tokens to unknown words\n    \"\"\"\n    \n    # Punctuation characters\n    # Try printing them out in a new cell!\n    punct = set(string.punctuation)\n    \n    # Suffixes\n    noun_suffix = [\"action\", \"age\", \"ance\", \"cy\", \"dom\", \"ee\", \"ence\", \"er\", \"hood\", \"ion\", \"ism\", \"ist\", \"ity\", \"ling\", \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ship\", \"ty\"]\n    verb_suffix = [\"ate\", \"ify\", \"ise\", \"ize\"]\n    adj_suffix = [\"able\", \"ese\", \"ful\", \"i\", \"ian\", \"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ly\", \"ous\"]\n    adv_suffix = [\"ward\", \"wards\", \"wise\"]\n\n    # Loop the characters in the word, check if any is a digit\n    if any(char.isdigit() for char in word):\n        return \"--unk_digit--\"\n\n    # Loop the characters in the word, check if any is a punctuation character\n    elif any(char in punct for char in word):\n        return \"--unk_punct--\"\n\n    # Loop the characters in the word, check if any is an upper case character\n    elif any(char.isupper() for char in word):\n        return \"--unk_upper--\"\n\n    # Check if word ends with any noun suffix\n    elif any(word.endswith(suffix) for suffix in noun_suffix):\n        return \"--unk_noun--\"\n\n    # Check if word ends with any verb suffix\n    elif any(word.endswith(suffix) for suffix in verb_suffix):\n        return \"--unk_verb--\"\n\n    # Check if word ends with any adjective suffix\n    elif any(word.endswith(suffix) for suffix in adj_suffix):\n        return \"--unk_adj--\"\n\n    # Check if word ends with any adverb suffix\n    elif any(word.endswith(suffix) for suffix in adv_suffix):\n        return \"--unk_adv--\"\n    \n    # If none of the previous criteria is met, return plain unknown\n    return \"--unk--\"\n\nA POS tagger will always encounter words that are not within the vocabulary that is being used. By augmenting the dataset to include these unknown word tokens you are helping the tagger to have a better idea of the appropriate tag for these words.\n\n\nGetting the correct tag for a word\nAll that is left is to implement a function that will get the correct tag for a particular word taking special considerations for unknown words. Since the dataset provides each word and tag within the same line and a word being known depends on the vocabulary used, these two elements should be arguments to this function.\nThis function should check if a line is empty and if so, it should return a placeholder word and tag, --n-- and --s-- respectively.\nIf not, it should process the line to return the correct word and tag pair, considering if a word is unknown in which scenario the function assign_unk() should be used.\nThe function is implemented next. Notice That the split() method can be used without specifying the delimiter, in which case it will default to any whitespace.\n\ndef get_word_tag(line, vocab):\n    # If line is empty return placeholders for word and tag\n    if not line.split():\n        word = \"--n--\"\n        tag = \"--s--\"\n    else:\n        # Split line to separate word and tag\n        word, tag = line.split()\n        # Check if word is not in vocabulary\n        if word not in vocab: \n            # Handle unknown word\n            word = assign_unk(word)\n    return word, tag\n\nNow you can try this function with some examples to test that it is working as intended:\n\nget_word_tag('\\n', vocab)\n\n('--n--', '--s--')\n\n\nSince this line only includes a newline character it returns a placeholder word and tag.\n\nget_word_tag('In\\tIN\\n', vocab)\n\n('In', 'IN')\n\n\nThis one is a valid line and the function does a fair job at returning the correct (word, tag) pair.\n\nget_word_tag('tardigrade\\tNN\\n', vocab)\n\n('--unk--', 'NN')\n\n\nThis line includes a noun that is not present in the vocabulary.\nThe assign_unk function fails to detect that it is a noun so it returns an unknown token.\n\nget_word_tag('scrutinize\\tVB\\n', vocab)\n\n('--unk_verb--', 'VB')\n\n\nThis line includes a verb that is not present in the vocabulary.\nIn this case the assign_unk is able to detect that it is a verb so it returns an unknown verb token.\nCongratulations on finishing this lecture notebook! Now you should be more familiar with working with text data and have a better understanding of how a basic POS tagger works.\nKeep it up!"
  },
  {
    "objectID": "posts/c2w2/index.html",
    "href": "posts/c2w2/index.html",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nFigure 2",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Part of Speech Tagging and Hidden Markov Models"
    ]
  },
  {
    "objectID": "posts/c2w2/index.html#part-of-speech-tagging",
    "href": "posts/c2w2/index.html#part-of-speech-tagging",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Part of Speech Tagging",
    "text": "Part of Speech Tagging\nPart of Speech Tagging (POS) is the process of assigning a part of speech to a word. By doing so, you will learn the following:\n\nMarkov Chains\nHidden Markov Models\nViterbi algorithm\n\nHere is a concrete example:\n\n\n\n\n\n\n\nautocorrect\n\n\n\n\nFigure 3: Learning Objectives\n\n\nYou can use part of speech tagging for:\n\nIdentifying named entities\nSpeech recognition\nCoreference Resolution\n\nYou can use the probabilities of POS tags happening near one another to come up with the most reasonable output",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Part of Speech Tagging and Hidden Markov Models"
    ]
  },
  {
    "objectID": "posts/c2w2/index.html#lab1-working-with-text-data",
    "href": "posts/c2w2/index.html#lab1-working-with-text-data",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Lab1: Working with text data",
    "text": "Lab1: Working with text data\nWorking with text data",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Part of Speech Tagging and Hidden Markov Models"
    ]
  },
  {
    "objectID": "posts/c2w2/index.html#markov-chains",
    "href": "posts/c2w2/index.html#markov-chains",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Markov Chains",
    "text": "Markov Chains\n\n\n\n\n\n\n\nPOS Tagging\n\n\n\n\nFigure 4: POS Tagging\n\n\n\n\n\n\n\n\nFSM\n\n\n\n\nFigure 5: FSM representation for POS tagging\n\n\n\nThe circles of the graph represent the states of your model. A state refers to a certain condition of the present moment. You can think of these as the POS tags of the current word.\n\nQ={q_1, q_2, q_3} \\qquad \\text{ is the set of all states in your model. }",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Part of Speech Tagging and Hidden Markov Models"
    ]
  },
  {
    "objectID": "posts/c2w2/index.html#markov-chains-and-pos-tags",
    "href": "posts/c2w2/index.html#markov-chains-and-pos-tags",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Markov Chains and POS Tags",
    "text": "Markov Chains and POS Tags\nTo help identify the parts of speech for every word, you need to build a transition matrix that gives you the probabilities from one state to another.\n\n\n\n\n\n\n\nFSM\n\n\n\n\nFigure 6: Learning Objectives\n\n\nIn the diagram above, the blue circles correspond to the part of speech tags, and the arrows correspond to the transition probabilities from one part of speech to another. You can populate the table on the right from the diagram on the left. The first row in your A matrix corresponds to the initial distribution among all the states. According to the table, the sentence has a 40% chance to start as a noun, 10% chance to start with a verb, and a 50% chance to start with another part of speech tag.\nIn more general notation, you can write the transition matrix A, given some states Q, as follows:\n\n\n\n\n\n\n\nFSM\n\n\n\n\nFigure 7: Learning Objectives",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Part of Speech Tagging and Hidden Markov Models"
    ]
  },
  {
    "objectID": "posts/c2w2/index.html#hidden-markov-models",
    "href": "posts/c2w2/index.html#hidden-markov-models",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Hidden Markov Models",
    "text": "Hidden Markov Models\nIn the previous video, I showed you an example with a simple markov model. The transition probabilities allowed you to identify the transition probability from one POS to another. We will now explore hidden markov models. In hidden markov models you make use of emission probabilities that give you the probability to go from one state (POS tag) to a specific word.\n\n\n\n\n\n\n\nState Transition Graph\n\n\n\n\nFigure 8: Emission probabilities\n\n\nFor example, given that you are in a verb state, you can go to other words with certain probabilities. This emission matrix B, will be used with your transition matrix A, to help you identify the part of speech of a word in a sentence. To populate your matrix B, you can just have a labelled dataset and compute the probabilities of going from a POS to each word in your vocabulary. Here is a recap of what you have seen so far:\n\n\n\n\n\n\n\nHMM\n\n\n\n\nFigure 9: HMM Summary\n\n\nNote that the sum of each row in your A and B matrix has to be 1. Next, I will show you how you can calculate the probabilities inside these matrices.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Part of Speech Tagging and Hidden Markov Models"
    ]
  },
  {
    "objectID": "posts/c2w2/index.html#calculating-probabilities",
    "href": "posts/c2w2/index.html#calculating-probabilities",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Calculating Probabilities",
    "text": "Calculating Probabilities\nHere is a visual representation on how to calculate the probabilities:\n\n\n\n\n\n\n\nTransition Probabilities\n\n\n\n\nFigure 10: Transition Probabilities\n\n\nThe number of times that blue is followed by purple is 2 out of 3. We will use the same logic to populate our transition and emission matrices. In the transition matrix we will count the number of times tag t_{(i−1)},t{(i)} show up near each other and divide by the total number of times t_{(i−1)} shows up. (which is the same as the number of times it shows up followed by anything else).\n\n\n\n\n\n\n\nTransition Probabilities\n\n\n\n\nFigure 11: Transition Probabilities\n\n\n\ncalculate co-occurrence of tag pairs \nC(t_{(i-1)},t_{(i)})\n\\tag{1}\ncalculate the probabilities using the counts \nP(t_{(i)}|t_{(i-1)}) = \\frac{C(t_{(i)}),t_{(i-1)},}{\\sum_{i=1}^{N} C(t_{(i-1)})}\n\\tag{2}\n\nWhere\nC(t_{(i−1)} ,t_{(i)}) is the count of times tag t_{(i-1)} shows up before tag i.\nFrom this you can compute the probability that a tag shows up after another tag.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Part of Speech Tagging and Hidden Markov Models"
    ]
  },
  {
    "objectID": "posts/c2w2/index.html#populating-the-transition-matrix",
    "href": "posts/c2w2/index.html#populating-the-transition-matrix",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Populating the Transition Matrix",
    "text": "Populating the Transition Matrix\nTo populate the transition matrix you have to keep track of the number of times each tag shows up before another tag.\n\n\n\n\n\n\n\nTransition Probabilities\n\n\n\n\nFigure 12: Transition Probabilities\n\n\nIn the table above, you can see that green corresponds to nouns (NN), purple corresponds to verbs (VB), and blue corresponds to other (O). Orange (π) corresponds to the initial state. The numbers inside the matrix correspond to the number of times a part of speech tag shows up right after another one.\nTo go from O to NN or in other words to calculate P(O∣NN) you have to compute the following:\n\n\n\n\n\n\n\nTransition Probabilities\n\n\n\n\nFigure 13: Transition Probabilities\n\n\nTo generalize:\n\nP(t_{(i)} \\mid t_{(i-1)}) = \\frac{C(t_{(i)},t_{(i-1)})}{\\sum_{j=1}^{N} C(t_{(i-1)},t_{(j)})}\n\\tag{3}\nWhere:\n\nC(t_{(i)},t_{(i-1)}) is the count of times tag t_{(i-1)} shows up before tag i.\n\nUnfortunately, sometimes you might not see two POS tags in front each other. This will give you a probability of 0. To solve this issue, you will “smooth” it as follows:\n\n\n\n\n\n\n\nTransition Probabilities\n\n\n\n\nFigure 14: Transition Probabilities\n\n\nThe \\epsilon allows you to not have any two sequences showing up with 0 probability. Why is this important?",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Part of Speech Tagging and Hidden Markov Models"
    ]
  },
  {
    "objectID": "posts/c2w2/index.html#populating-the-emission-matrix",
    "href": "posts/c2w2/index.html#populating-the-emission-matrix",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Populating the Emission Matrix",
    "text": "Populating the Emission Matrix\nTo populate the emission matrix, you have to keep track of the words associated with their parts of speech tags.\n\n\n\n\n\n\n\nThe Emission Matrix\n\n\n\n\nFigure 15: The Emission Matrix\n\n\nTo populate the matrix, we will also use smoothing as we have previously used:\n\nP(w_i \\mid t_i) = \\frac{C(t_i,w_i)+\\epsilon}{\\sum_{j=1}^{N} C(t_i,w_j)+ N \\times \\epsilon} = \\frac{C(t_i,w_i)+\\epsilon}{\\sum_{j=1}^{N} C(t_i)+N\\times \\epsilon}\n\nWhere C(t_i,w_i) is the count associated with how many times the tag t_i is associated with the word w_i. The epsilon above is the smoothing parameter. In the next video, we will talk about the Viterbi algorithm and discuss how you can use the transition and emission matrix to come up with probabilities.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Part of Speech Tagging and Hidden Markov Models"
    ]
  },
  {
    "objectID": "posts/c2w2/index.html#lab2-working-with-text-data",
    "href": "posts/c2w2/index.html#lab2-working-with-text-data",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Lab2: Working with text data",
    "text": "Lab2: Working with text data\nWorking with text data",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Part of Speech Tagging and Hidden Markov Models"
    ]
  },
  {
    "objectID": "posts/c2w2/index.html#the-viterbi-algorithm",
    "href": "posts/c2w2/index.html#the-viterbi-algorithm",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "The Viterbi Algorithm",
    "text": "The Viterbi Algorithm\nThe Viterbi algorithm makes use of the transition probabilities and the emission probabilities as follows.\n\n\n\n\n\n\n\nThe Emission Matrix\n\n\n\n\nFigure 16: The Viterbi Algorithm\n\n\nTo go from π to O you need to multiply the corresponding transition probability (0.3) and the corresponding emission probability (0.5), which gives you 0.15. You keep doing that for all the words, until you get the probability of an entire sequence.\n\n\n\n\n\n\n\nThe Emission Matrix\n\n\n\n\nFigure 17: The Viterbi Algorithm\n\n\nYou can then see how you will just pick the sequence with the highest probability. We will show you a systematic way to accomplish this (Viterbi!).",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Part of Speech Tagging and Hidden Markov Models"
    ]
  },
  {
    "objectID": "posts/c2w2/index.html#viterbi-initialization",
    "href": "posts/c2w2/index.html#viterbi-initialization",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Viterbi Initialization",
    "text": "Viterbi Initialization\nYou will now populate a matrix C of dimension (num_tags, num_words). This matrix will have the probabilities that will tell you what part of speech each word belongs to.\n\n\n\n\n\n\n\nThe Emission Matrix\n\n\n\n\nFigure 18: The Viterbi Initialization\n\n\nNow to populate the first column, you just multiply the initial π distribution, for each tag, times b_{i,cindex(w_1)}, which is the emission probability of the word 1 given the tag i. Where the i, corresponds to the tag of the initial distribution and the cindex(w_1), is the index of word 1 in the emission matrix. And that’s it, you are done with populating the first column of your new C matrix. You will now need to keep track what part of speech you are coming from. Hence we introduce a matrix D, which allows you to store the labels that represent the different states you are going through when finding the most likely sequence of POS tags for the given sequence of words w_2 ,…,w_k. At first you set the first column to 0, because you are not coming from any POS tag.\n\n\n\n\n\n\n\nThe Emission Matrix\n\n\n\n\nFigure 19: The Viterbi Initialization\n\n\nThese two matrices will make more sense in the next videos.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Part of Speech Tagging and Hidden Markov Models"
    ]
  },
  {
    "objectID": "posts/c2w2/index.html#viterbi-forward-pass",
    "href": "posts/c2w2/index.html#viterbi-forward-pass",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Viterbi: Forward Pass",
    "text": "Viterbi: Forward Pass\nThis will be best illustrated with an example:\n\n\n\n\n\n\n\nViterbi: Forward Pass\n\n\n\n\nFigure 20: Viterbi: Forward Pass\n\n\nSo to populate a cell (i.e. 1,2) in the image above, you have to take the max of [kth cells in the previous column, times the corresponding transition probability of the kth POS to the first POS times the emission probability of the first POS and the current word you are looking at]. You do that for all the cells. Take a paper and a pencil, and make sure you understand how it is done.\nThe general rule is c_{ij}= max_k c_{k,j-1} \\times a_{k,i} \\times b_{i,cindex(w_j)}\nNow to populate the D matrix, you will keep track of the argmax of where you came from as follows:\nNote that the only difference between c_{ij} and d_{ij}, is that in the former you compute the probability and in the latter you keep track of the index of the row where that probability came from. So you keep track of which k was used to get that max probability.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Part of Speech Tagging and Hidden Markov Models"
    ]
  },
  {
    "objectID": "posts/c2w2/index.html#viterbi-backward-pass",
    "href": "posts/c2w2/index.html#viterbi-backward-pass",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Viterbi: Backward Pass",
    "text": "Viterbi: Backward Pass\nGreat, now that you know how to compute A, B, C, and D, we will put it all together and show you how to construct the path that will give you the part of speech tags for your sentence.\n\n\n\n\n\n\n\nViterbi: Forward Pass\n\n\n\n\nFigure 21: Viterbi: Forward Pass\n\n\nThe equation above just gives you the index of the highest row in the last column of C. Once you have that, you can go ahead and start using your D matrix as follows:\n\n\n\n\n\n\n\nViterbi: Forward Pass\n\n\n\n\nFigure 22: Viterbi: Forward Pass\n\n\nNote that since we started at index one, hence the last word (w5) is t_1. Then we go to the first row of D and what ever that number is, it indicated the row of the next part of speech tag. Then next part of speech tag indicates the row of the next and so forth. This allows you to reconstruct the POS tags for your sentence.\nYou will be implementing this in this week’s programming assignment.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Part of Speech Tagging and Hidden Markov Models"
    ]
  },
  {
    "objectID": "posts/c2w2/index.html#assignment",
    "href": "posts/c2w2/index.html#assignment",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Assignment",
    "text": "Assignment\nPart-of-speech (POS) tagging",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Part of Speech Tagging and Hidden Markov Models"
    ]
  },
  {
    "objectID": "posts/c1w1/lab03.html",
    "href": "posts/c1w1/lab03.html",
    "title": "Visualizing tweets and the Logistic Regression model",
    "section": "",
    "text": "course banner\nObjectives: Visualize and interpret the logistic regression model\nSteps: * Plot tweets in a scatter plot using their positive and negative sums. * Plot the output of the logistic regression model in the same plot as a solid line"
  },
  {
    "objectID": "posts/c1w1/lab03.html#import-the-required-libraries",
    "href": "posts/c1w1/lab03.html#import-the-required-libraries",
    "title": "Visualizing tweets and the Logistic Regression model",
    "section": "Import the required libraries",
    "text": "Import the required libraries\nWe will be using NLTK, an opensource NLP library, for collecting, handling, and processing Twitter data. In this lab, we will use the example dataset that comes alongside with NLTK. This dataset has been manually annotated and serves to establish baselines for models quickly.\nSo, to start, let’s import the required libraries.\n\nimport nltk                         # NLP toolbox\nfrom os import getcwd\nimport pandas as pd                 # Library for Dataframes \nfrom nltk.corpus import twitter_samples \nimport matplotlib.pyplot as plt     # Library for visualization\nimport numpy as np                  # Library for math functions\n\nfrom utils import process_tweet, build_freqs # Our functions for NLP"
  },
  {
    "objectID": "posts/c1w1/lab03.html#load-the-nltk-sample-dataset",
    "href": "posts/c1w1/lab03.html#load-the-nltk-sample-dataset",
    "title": "Visualizing tweets and the Logistic Regression model",
    "section": "Load the NLTK sample dataset",
    "text": "Load the NLTK sample dataset\nTo complete this lab, you need the sample dataset of the previous lab. Here, we assume the files are already available, and we only need to load into Python lists.\n\n# select the set of positive and negative tweets\nall_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')\n\ntweets = all_positive_tweets + all_negative_tweets ## Concatenate the lists. \nlabels = np.append(np.ones((len(all_positive_tweets),1)), np.zeros((len(all_negative_tweets),1)), axis = 0)\n\n# split the data into two pieces, one for training and one for testing (validation set) \ntrain_pos  = all_positive_tweets[:4000]\ntrain_neg  = all_negative_tweets[:4000]\n\ntrain_x = train_pos + train_neg \n\nprint(\"Number of tweets: \", len(train_x))\n\nNumber of tweets:  8000"
  },
  {
    "objectID": "posts/c1w1/lab03.html#load-a-pretrained-logistic-regression-model",
    "href": "posts/c1w1/lab03.html#load-a-pretrained-logistic-regression-model",
    "title": "Visualizing tweets and the Logistic Regression model",
    "section": "Load a pretrained Logistic Regression model",
    "text": "Load a pretrained Logistic Regression model\nIn the same way, as part of this week’s assignment, a Logistic regression model must be trained. The next cell contains the resulting model from such training. Notice that a list of 3 numeric values represents the whole model, that we have called theta \\theta.\n\ntheta = [7e-08, 0.0005239, -0.00055517]"
  },
  {
    "objectID": "posts/c1w1/lab03.html#plot-the-samples-in-a-scatter-plot",
    "href": "posts/c1w1/lab03.html#plot-the-samples-in-a-scatter-plot",
    "title": "Visualizing tweets and the Logistic Regression model",
    "section": "Plot the samples in a scatter plot",
    "text": "Plot the samples in a scatter plot\nThe vector theta represents a plane that split our feature space into two parts. Samples located over that plane are considered positive, and samples located under that plane are considered negative. Remember that we have a 3D feature space, i.e., each tweet is represented as a vector comprised of three values: [bias, positive_sum, negative_sum], always having bias = 1.\nIf we ignore the bias term, we can plot each tweet in a cartesian plane, using positive_sum and negative_sum. In the cell below, we do precisely this. Additionally, we color each tweet, depending on its class. Positive tweets will be green and negative tweets will be red.\n\n# Plot the samples using columns 1 and 2 of the matrix\nfig, ax = plt.subplots(figsize = (8, 8))\n\ncolors = ['red', 'green']\n\n# Color based on the sentiment Y\nax.scatter(X[:,1], X[:,2], c=[colors[int(k)] for k in Y], s = 0.1)  # Plot a dot for each pair of words\nplt.xlabel(\"Positive\")\nplt.ylabel(\"Negative\")\n\nText(0.5, 0, 'Positive')\n\n\nText(0, 0.5, 'Negative')\n\n\n\n\n\n\n\n\n\nFrom the plot, it is evident that the features that we have chosen to represent tweets as numerical vectors allow an almost perfect separation between positive and negative tweets. So you can expect a very high accuracy for this model!"
  },
  {
    "objectID": "posts/c1w1/lab03.html#plot-the-model-alongside-the-data",
    "href": "posts/c1w1/lab03.html#plot-the-model-alongside-the-data",
    "title": "Visualizing tweets and the Logistic Regression model",
    "section": "Plot the model alongside the data",
    "text": "Plot the model alongside the data\nWe will draw a gray line to show the cutoff between the positive and negative regions. In other words, the gray line marks the line where  z = \\theta * x = 0. To draw this line, we have to solve the above equation in terms of one of the independent variables.\n z = \\theta * x = 0  x = [1, pos, neg]   z(\\theta, x) = \\theta_0+ \\theta_1 * pos + \\theta_2 * neg = 0   neg = (-\\theta_0 - \\theta_1 * pos) / \\theta_2 \nThe red and green lines that point in the direction of the corresponding sentiment are calculated using a perpendicular line to the separation line calculated in the previous equations(neg function). It must point in the same direction as the derivative of the Logit function, but the magnitude may differ. It is only for a visual representation of the model.\ndirection = pos * \\theta_2 / \\theta_1\n\n# Equation for the separation plane\n# It give a value in the negative axe as a function of a positive value\n# f(pos, neg, W) = w0 + w1 * pos + w2 * neg = 0\n# s(pos, W) = (w0 - w1 * pos) / w2\ndef neg(theta, pos):\n    return (-theta[0] - pos * theta[1]) / theta[2]\n\n# Equation for the direction of the sentiments change\n# We don't care about the magnitude of the change. We are only interested \n# in the direction. So this direction is just a perpendicular function to the \n# separation plane\n# df(pos, W) = pos * w2 / w1\ndef direction(theta, pos):\n    return    pos * theta[2] / theta[1]\n\nThe green line in the chart points in the direction where z &gt; 0 and the red line points in the direction where z &lt; 0. The direction of these lines are given by the weights \\theta_1 and \\theta_2\n\n# Plot the samples using columns 1 and 2 of the matrix\nfig, ax = plt.subplots(figsize = (8, 8))\n\ncolors = ['red', 'green']\n\n# Color base on the sentiment Y\nax.scatter(X[:,1], X[:,2], c=[colors[int(k)] for k in Y], s = 0.1)  # Plot a dot for each pair of words\nplt.xlabel(\"Positive\")\nplt.ylabel(\"Negative\")\n\n# Now lets represent the logistic regression model in this chart. \nmaxpos = np.max(X[:,1])\n\noffset = 5000 # The pos value for the direction vectors origin\n\n# Plot a gray line that divides the 2 areas.\nax.plot([0,  maxpos], [neg(theta, 0),   neg(theta, maxpos)], color = 'gray') \n\n# Plot a green line pointing to the positive direction\nax.arrow(offset, neg(theta, offset), offset, direction(theta, offset), head_width=500, head_length=500, fc='g', ec='g')\n# Plot a red line pointing to the negative direction\nax.arrow(offset, neg(theta, offset), -offset, -direction(theta, offset), head_width=500, head_length=500, fc='r', ec='r')\n\nplt.show()\n\nText(0.5, 0, 'Positive')\n\n\nText(0, 0.5, 'Negative')\n\n\n\n\n\n\n\n\n\nNote that more critical than the Logistic regression itself, are the features extracted from tweets that allow getting the right results in this exercise.\nThat is all, folks. Hopefully, now you understand better what the Logistic regression model represents, and why it works that well for this specific problem."
  },
  {
    "objectID": "posts/c1w1/lab02.html",
    "href": "posts/c1w1/lab02.html",
    "title": "Building and Visualizing word frequencies",
    "section": "",
    "text": "course banner\nIn this lab, we will focus on the build_freqs() helper function and visualizing a dataset fed into it. In our goal of tweet sentiment analysis, this function will build a dictionary where we can lookup how many times a word appears in the lists of positive or negative tweets. This will be very helpful when extracting the features of the dataset in the week’s programming assignment. Let’s see how this function is implemented under the hood in this notebook."
  },
  {
    "objectID": "posts/c1w1/lab02.html#setup",
    "href": "posts/c1w1/lab02.html#setup",
    "title": "Building and Visualizing word frequencies",
    "section": "Setup",
    "text": "Setup\nLet’s import the required libraries for this lab:\n\nimport nltk                                  # Python library for NLP\nfrom nltk.corpus import twitter_samples      # sample Twitter dataset from NLTK\nimport matplotlib.pyplot as plt              # visualization library\nimport numpy as np                           # library for scientific computing and matrix operations\n\n\nImport some helper functions that we provided in the utils.py file:\n\nprocess_tweet(): Cleans the text, tokenizes it into separate words, removes stopwords, and converts words to stems.\nbuild_freqs(): This counts how often a word in the ‘corpus’ (the entire set of tweets) was associated with a positive label 1 or a negative label 0. It then builds the freqs dictionary, where each key is a (word,label) tuple, and the value is the count of its frequency within the corpus of tweets.\n\n\n# download the stopwords for the process_tweet function\nnltk.download('stopwords')\n\n# import our convenience functions\nfrom utils import process_tweet, build_freqs\n\n[nltk_data] Downloading package stopwords to /home/oren/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue"
  },
  {
    "objectID": "posts/c1w1/lab02.html#load-the-nltk-sample-dataset",
    "href": "posts/c1w1/lab02.html#load-the-nltk-sample-dataset",
    "title": "Building and Visualizing word frequencies",
    "section": "Load the NLTK sample dataset",
    "text": "Load the NLTK sample dataset\nAs in the previous lab, we will be using the Twitter dataset from NLTK.\n\n# select the lists of positive and negative tweets\nall_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')\n\n# concatenate the lists, 1st part is the positive tweets followed by the negative\ntweets = all_positive_tweets + all_negative_tweets\n\n# let's see how many tweets we have\nprint(\"Number of tweets: \", len(tweets))\n\nNumber of tweets:  10000\n\n\nNext, we will build a labels array that matches the sentiments of our tweets. This data type works pretty much like a regular list but is optimized for computations and manipulation. The labels array will be composed of 10000 elements. The first 5000 will be filled with 1 labels denoting positive sentiments, and the next 5000 will be 0 labels denoting the opposite. We can do this easily with a series of operations provided by the numpy library:\n\nnp.ones() - create an array of 1’s\nnp.zeros() - create an array of 0’s\nnp.append() - concatenate arrays\n\n\n# make a numpy array representing labels of the tweets\nlabels = np.append(np.ones((len(all_positive_tweets))), np.zeros((len(all_negative_tweets))))"
  },
  {
    "objectID": "posts/c1w1/lab02.html#dictionaries",
    "href": "posts/c1w1/lab02.html#dictionaries",
    "title": "Building and Visualizing word frequencies",
    "section": "Dictionaries",
    "text": "Dictionaries\nIn Python, a dictionary is a mutable and indexed collection. It stores items as key-value pairs and uses hash tables underneath to allow practically constant time lookups. In NLP, dictionaries are essential because it enables fast retrieval of items or containment checks even with thousands of entries in the collection.\n\nDefinition\nA dictionary in Python is declared using curly brackets. Look at the next example:\n\ndictionary = {'key1': 1, 'key2': 2}\n\nThe former line defines a dictionary with two entries. Keys and values can be almost any type (with a few restriction on keys), and in this case, we used strings. We can also use floats, integers, tuples, etc.\n\n\nAdding or editing entries\nNew entries can be inserted into dictionaries using square brackets. If the dictionary already contains the specified key, its value is overwritten.\n\n# Add a new entry\ndictionary['key3'] = -5\n\n# Overwrite the value of key1\ndictionary['key1'] = 0\n\nprint(dictionary)\n\n{'key1': 0, 'key2': 2, 'key3': -5}\n\n\n\n\nAccessing values and lookup keys\nPerforming dictionary lookups and retrieval are common tasks in NLP. There are two ways to do this:\n\nUsing square bracket notation: This form is allowed if the lookup key is in the dictionary. It produces an error otherwise.\nUsing the get() method: This allows us to set a default value if the dictionary key does not exist.\n\nLet us see these in action:\n\n# Square bracket lookup when the key exist\nprint(dictionary['key2'])\n\n2\n\n\nHowever, if the key is missing, the operation produce an error\n\n# The output of this line is intended to produce a KeyError\nprint(dictionary['key8'])\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nCell In[8], line 2\n      1 # The output of this line is intended to produce a KeyError\n----&gt; 2 print(dictionary['key8'])\n\nKeyError: 'key8'\n\n\n\nWhen using a square bracket lookup, it is common to use an if-else block to check for containment first (with the keyword in) before getting the item. On the other hand, you can use the .get() method if you want to set a default value when the key is not found. Let’s compare these in the cells below:\n\n# This prints a value\nif 'key1' in dictionary:\n    print(\"item found: \", dictionary['key1'])\nelse:\n    print('key1 is not defined')\n\n# Same as what you get with get\nprint(\"item found: \", dictionary.get('key1', -1))\n\nitem found:  0\nitem found:  0\n\n\n\n# This prints a message because the key is not found\nif 'key7' in dictionary:\n    print(dictionary['key7'])\nelse:\n    print('key does not exist!')\n\n# This prints -1 because the key is not found and we set the default to -1\nprint(dictionary.get('key7', -1))\n\nkey does not exist!\n-1"
  },
  {
    "objectID": "posts/c1w1/lab02.html#word-frequency-dictionary",
    "href": "posts/c1w1/lab02.html#word-frequency-dictionary",
    "title": "Building and Visualizing word frequencies",
    "section": "Word frequency dictionary",
    "text": "Word frequency dictionary\nNow that we know the building blocks, let’s finally take a look at the build_freqs() function in utils.py. This is the function that creates the dictionary containing the word counts from each corpus.\ndef build_freqs(tweets, ys):\n    \"\"\"Build frequencies.\n    Input:\n        tweets: a list of tweets\n        ys: an m x 1 array with the sentiment label of each tweet\n            (either 0 or 1)\n    Output:\n        freqs: a dictionary mapping each (word, sentiment) pair to its\n        frequency\n    \"\"\"\n    # Convert np array to list since zip needs an iterable.\n    # The squeeze is necessary or the list ends up with one element.\n    # Also note that this is just a NOP if ys is already a list.\n    yslist = np.squeeze(ys).tolist()\n\n    # Start with an empty dictionary and populate it by looping over all tweets\n    # and over all processed words in each tweet.\n    freqs = {}\n    for y, tweet in zip(yslist, tweets):\n        for word in process_tweet(tweet):\n            pair = (word, y)\n            if pair in freqs:\n                freqs[pair] += 1\n            else:\n                freqs[pair] = 1    \n    return freqs\nYou can also do the for loop like this to make it a bit more compact:\n    for y, tweet in zip(yslist, tweets):\n        for word in process_tweet(tweet):\n            pair = (word, y)\n            freqs[pair] = freqs.get(pair, 0) + 1\nAs shown above, each key is a 2-element tuple containing a (word, y) pair. The word is an element in a processed tweet while y is an integer representing the corpus: 1 for the positive tweets and 0 for the negative tweets. The value associated with this key is the number of times that word appears in the specified corpus. For example:\n# \"folowfriday\" appears 25 times in the positive tweets\n('followfriday', 1.0): 25\n\n# \"shame\" appears 19 times in the negative tweets\n('shame', 0.0): 19 \nNow, it is time to use the dictionary returned by the build_freqs() function. First, let us feed our tweets and labels lists then print a basic report:\n\n# create frequency dictionary\nfreqs = build_freqs(tweets, labels)\n\n# check data type\nprint(f'type(freqs) = {type(freqs)}')\n\n# check length of the dictionary\nprint(f'len(freqs) = {len(freqs)}')\n\ntype(freqs) = &lt;class 'dict'&gt;\nlen(freqs) = 13065\n\n\nNow print the frequency of each word depending on its class.\n\nprint(freqs)\n\n{('followfriday', 1.0): 25, ('top', 1.0): 32, ('engag', 1.0): 7, ('member', 1.0): 16, ('commun', 1.0): 33, ('week', 1.0): 83, (':)', 1.0): 3568, ('hey', 1.0): 76, ('jame', 1.0): 7, ('odd', 1.0): 2, (':/', 1.0): 5, ('pleas', 1.0): 97, ('call', 1.0): 37, ('contact', 1.0): 7, ('centr', 1.0): 2, ('02392441234', 1.0): 1, ('abl', 1.0): 8, ('assist', 1.0): 1, ('mani', 1.0): 33, ('thank', 1.0): 620, ('listen', 1.0): 16, ('last', 1.0): 47, ('night', 1.0): 68, ('bleed', 1.0): 2, ('amaz', 1.0): 51, ('track', 1.0): 5, ('scotland', 1.0): 2, ('congrat', 1.0): 21, ('yeaaah', 1.0): 1, ('yipppi', 1.0): 1, ('accnt', 1.0): 2, ('verifi', 1.0): 2, ('rqst', 1.0): 1, ('succeed', 1.0): 1, ('got', 1.0): 69, ('blue', 1.0): 9, ('tick', 1.0): 1, ('mark', 1.0): 1, ('fb', 1.0): 6, ('profil', 1.0): 2, ('15', 1.0): 5, ('day', 1.0): 246, ('one', 1.0): 129, ('irresist', 1.0): 2, ('flipkartfashionfriday', 1.0): 17, ('like', 1.0): 233, ('keep', 1.0): 68, ('love', 1.0): 400, ('custom', 1.0): 4, ('wait', 1.0): 70, ('long', 1.0): 36, ('hope', 1.0): 141, ('enjoy', 1.0): 75, ('happi', 1.0): 211, ('friday', 1.0): 116, ('lwwf', 1.0): 1, ('second', 1.0): 10, ('thought', 1.0): 29, ('’', 1.0): 21, ('enough', 1.0): 18, ('time', 1.0): 127, ('dd', 1.0): 1, ('new', 1.0): 143, ('short', 1.0): 7, ('enter', 1.0): 9, ('system', 1.0): 2, ('sheep', 1.0): 1, ('must', 1.0): 18, ('buy', 1.0): 11, ('jgh', 1.0): 4, ('go', 1.0): 148, ('bayan', 1.0): 1, (':d', 1.0): 629, ('bye', 1.0): 7, ('act', 1.0): 8, ('mischiev', 1.0): 1, ('etl', 1.0): 1, ('layer', 1.0): 1, ('in-hous', 1.0): 1, ('wareh', 1.0): 1, ('app', 1.0): 16, ('katamari', 1.0): 1, ('well', 1.0): 81, ('…', 1.0): 38, ('name', 1.0): 18, ('impli', 1.0): 1, (':p', 1.0): 138, ('influenc', 1.0): 18, ('big', 1.0): 33, ('...', 1.0): 289, ('juici', 1.0): 3, ('selfi', 1.0): 12, ('follow', 1.0): 381, ('perfect', 1.0): 24, ('alreadi', 1.0): 28, ('know', 1.0): 145, (\"what'\", 1.0): 17, ('great', 1.0): 171, ('opportun', 1.0): 23, ('junior', 1.0): 2, ('triathlet', 1.0): 1, ('age', 1.0): 2, ('12', 1.0): 5, ('13', 1.0): 6, ('gatorad', 1.0): 1, ('seri', 1.0): 5, ('get', 1.0): 206, ('entri', 1.0): 4, ('lay', 1.0): 4, ('greet', 1.0): 5, ('card', 1.0): 8, ('rang', 1.0): 3, ('print', 1.0): 3, ('today', 1.0): 108, ('job', 1.0): 41, (':-)', 1.0): 692, (\"friend'\", 1.0): 3, ('lunch', 1.0): 5, ('yummm', 1.0): 1, ('nostalgia', 1.0): 1, ('tb', 1.0): 2, ('ku', 1.0): 1, ('id', 1.0): 8, ('conflict', 1.0): 1, ('help', 1.0): 41, (\"here'\", 1.0): 25, ('screenshot', 1.0): 3, ('work', 1.0): 110, ('hi', 1.0): 173, ('liv', 1.0): 2, ('hello', 1.0): 59, ('need', 1.0): 78, ('someth', 1.0): 28, ('u', 1.0): 175, ('fm', 1.0): 2, ('twitter', 1.0): 29, ('—', 1.0): 27, ('sure', 1.0): 58, ('thing', 1.0): 69, ('dm', 1.0): 39, ('x', 1.0): 72, (\"i'v\", 1.0): 35, ('heard', 1.0): 9, ('four', 1.0): 5, ('season', 1.0): 9, ('pretti', 1.0): 20, ('dope', 1.0): 2, ('penthous', 1.0): 1, ('obv', 1.0): 1, ('gobigorgohom', 1.0): 1, ('fun', 1.0): 58, (\"y'all\", 1.0): 3, ('yeah', 1.0): 47, ('suppos', 1.0): 7, ('lol', 1.0): 64, ('chat', 1.0): 13, ('bit', 1.0): 20, ('youth', 1.0): 19, ('💅🏽', 1.0): 1, ('💋', 1.0): 2, ('seen', 1.0): 10, ('year', 1.0): 43, ('rest', 1.0): 12, ('goe', 1.0): 7, ('quickli', 1.0): 3, ('bed', 1.0): 16, ('music', 1.0): 21, ('fix', 1.0): 10, ('dream', 1.0): 20, ('spiritu', 1.0): 1, ('ritual', 1.0): 1, ('festiv', 1.0): 8, ('népal', 1.0): 1, ('begin', 1.0): 4, ('line-up', 1.0): 4, ('left', 1.0): 13, ('see', 1.0): 184, ('sarah', 1.0): 4, ('send', 1.0): 22, ('us', 1.0): 109, ('email', 1.0): 26, ('bitsy@bitdefender.com', 1.0): 1, (\"we'll\", 1.0): 20, ('asap', 1.0): 5, ('kik', 1.0): 22, ('hatessuc', 1.0): 1, ('32429', 1.0): 1, ('kikm', 1.0): 1, ('lgbt', 1.0): 2, ('tinder', 1.0): 1, ('nsfw', 1.0): 1, ('akua', 1.0): 1, ('cumshot', 1.0): 1, ('come', 1.0): 70, ('hous', 1.0): 7, ('nsn_supplement', 1.0): 1, ('effect', 1.0): 4, ('press', 1.0): 1, ('releas', 1.0): 11, ('distribut', 1.0): 1, ('result', 1.0): 2, ('link', 1.0): 18, ('remov', 1.0): 3, ('pressreleas', 1.0): 1, ('newsdistribut', 1.0): 1, ('bam', 1.0): 44, ('bestfriend', 1.0): 50, ('lot', 1.0): 87, ('warsaw', 1.0): 44, ('&lt;3', 1.0): 134, ('x46', 1.0): 1, ('everyon', 1.0): 58, ('watch', 1.0): 46, ('documentari', 1.0): 1, ('earthl', 1.0): 2, ('youtub', 1.0): 13, ('support', 1.0): 27, ('buuut', 1.0): 1, ('oh', 1.0): 53, ('look', 1.0): 137, ('forward', 1.0): 29, ('visit', 1.0): 30, ('next', 1.0): 48, ('letsgetmessi', 1.0): 1, ('jo', 1.0): 1, ('make', 1.0): 99, ('feel', 1.0): 46, ('better', 1.0): 52, ('never', 1.0): 36, ('anyon', 1.0): 11, ('kpop', 1.0): 1, ('flesh', 1.0): 1, ('good', 1.0): 238, ('girl', 1.0): 44, ('best', 1.0): 65, ('wish', 1.0): 37, ('reason', 1.0): 13, ('epic', 1.0): 2, ('soundtrack', 1.0): 1, ('shout', 1.0): 12, ('ad', 1.0): 14, ('video', 1.0): 34, ('playlist', 1.0): 5, ('would', 1.0): 84, ('dear', 1.0): 17, ('jordan', 1.0): 1, ('okay', 1.0): 39, ('fake', 1.0): 2, ('gameplay', 1.0): 2, (';)', 1.0): 27, ('haha', 1.0): 53, ('im', 1.0): 51, ('kid', 1.0): 18, ('stuff', 1.0): 13, ('exactli', 1.0): 6, ('product', 1.0): 12, ('line', 1.0): 6, ('etsi', 1.0): 1, ('shop', 1.0): 16, ('check', 1.0): 52, ('vacat', 1.0): 6, ('recharg', 1.0): 1, ('normal', 1.0): 6, ('charger', 1.0): 2, ('asleep', 1.0): 9, ('talk', 1.0): 45, ('sooo', 1.0): 6, ('someon', 1.0): 34, ('text', 1.0): 18, ('ye', 1.0): 77, ('bet', 1.0): 6, (\"he'll\", 1.0): 4, ('fit', 1.0): 3, ('hear', 1.0): 33, ('speech', 1.0): 1, ('piti', 1.0): 3, ('green', 1.0): 3, ('garden', 1.0): 7, ('midnight', 1.0): 1, ('sun', 1.0): 6, ('beauti', 1.0): 50, ('canal', 1.0): 1, ('dasvidaniya', 1.0): 1, ('till', 1.0): 18, ('scout', 1.0): 1, ('sg', 1.0): 1, ('futur', 1.0): 13, ('wlan', 1.0): 1, ('pro', 1.0): 5, ('confer', 1.0): 1, ('asia', 1.0): 1, ('chang', 1.0): 24, ('lollipop', 1.0): 1, ('🍭', 1.0): 1, ('nez', 1.0): 1, ('agnezmo', 1.0): 1, ('oley', 1.0): 1, ('mama', 1.0): 1, ('stand', 1.0): 8, ('stronger', 1.0): 1, ('god', 1.0): 20, ('misti', 1.0): 1, ('babi', 1.0): 20, ('cute', 1.0): 26, ('woohoo', 1.0): 3, (\"can't\", 1.0): 43, ('sign', 1.0): 11, ('yet', 1.0): 13, ('still', 1.0): 48, ('think', 1.0): 63, ('mka', 1.0): 5, ('liam', 1.0): 8, ('access', 1.0): 3, ('welcom', 1.0): 73, ('stat', 1.0): 60, ('arriv', 1.0): 67, ('1', 1.0): 75, ('unfollow', 1.0): 63, ('via', 1.0): 69, ('surpris', 1.0): 10, ('figur', 1.0): 5, ('happybirthdayemilybett', 1.0): 1, ('sweet', 1.0): 19, ('talent', 1.0): 5, ('2', 1.0): 58, ('plan', 1.0): 27, ('drain', 1.0): 1, ('gotta', 1.0): 5, ('timezon', 1.0): 1, ('parent', 1.0): 5, ('proud', 1.0): 12, ('least', 1.0): 16, ('mayb', 1.0): 18, ('sometim', 1.0): 13, ('grade', 1.0): 4, ('al', 1.0): 4, ('grand', 1.0): 4, ('manila_bro', 1.0): 2, ('chosen', 1.0): 1, ('let', 1.0): 68, ('around', 1.0): 17, ('..', 1.0): 128, ('side', 1.0): 15, ('world', 1.0): 27, ('eh', 1.0): 2, ('take', 1.0): 43, ('care', 1.0): 18, ('final', 1.0): 30, ('fuck', 1.0): 26, ('weekend', 1.0): 75, ('real', 1.0): 21, ('x45', 1.0): 1, ('join', 1.0): 23, ('hushedcallwithfraydo', 1.0): 1, ('gift', 1.0): 8, ('yeahhh', 1.0): 1, ('hushedpinwithsammi', 1.0): 2, ('event', 1.0): 8, ('might', 1.0): 27, ('luv', 1.0): 6, ('realli', 1.0): 79, ('appreci', 1.0): 31, ('share', 1.0): 46, ('wow', 1.0): 22, ('tom', 1.0): 5, ('gym', 1.0): 4, ('monday', 1.0): 9, ('invit', 1.0): 17, ('scope', 1.0): 5, ('friend', 1.0): 61, ('nude', 1.0): 2, ('sleep', 1.0): 45, ('birthday', 1.0): 74, ('want', 1.0): 96, ('t-shirt', 1.0): 3, ('cool', 1.0): 38, ('haw', 1.0): 1, ('phela', 1.0): 1, ('mom', 1.0): 10, ('obvious', 1.0): 2, ('princ', 1.0): 1, ('charm', 1.0): 1, ('stage', 1.0): 2, ('luck', 1.0): 30, ('tyler', 1.0): 2, ('hipster', 1.0): 1, ('glass', 1.0): 5, ('marti', 1.0): 2, ('glad', 1.0): 43, ('done', 1.0): 54, ('afternoon', 1.0): 10, ('read', 1.0): 34, ('kahfi', 1.0): 1, ('finish', 1.0): 17, ('ohmyg', 1.0): 1, ('yaya', 1.0): 3, ('dub', 1.0): 2, ('stalk', 1.0): 2, ('ig', 1.0): 3, ('gondooo', 1.0): 1, ('moo', 1.0): 2, ('tologooo', 1.0): 1, ('becom', 1.0): 10, ('detail', 1.0): 10, ('zzz', 1.0): 1, ('xx', 1.0): 42, ('physiotherapi', 1.0): 1, ('hashtag', 1.0): 5, ('💪', 1.0): 1, ('monica', 1.0): 1, ('miss', 1.0): 27, ('sound', 1.0): 23, ('morn', 1.0): 101, (\"that'\", 1.0): 67, ('x43', 1.0): 1, ('definit', 1.0): 23, ('tri', 1.0): 44, ('tonight', 1.0): 20, ('took', 1.0): 8, ('advic', 1.0): 6, ('treviso', 1.0): 1, ('concert', 1.0): 24, ('citi', 1.0): 27, ('countri', 1.0): 23, (\"i'll\", 1.0): 90, ('start', 1.0): 61, ('fine', 1.0): 10, ('gorgeou', 1.0): 12, ('xo', 1.0): 2, ('oven', 1.0): 3, ('roast', 1.0): 2, ('garlic', 1.0): 1, ('oliv', 1.0): 1, ('oil', 1.0): 4, ('dri', 1.0): 5, ('tomato', 1.0): 1, ('basil', 1.0): 1, ('centuri', 1.0): 1, ('tuna', 1.0): 1, ('right', 1.0): 47, ('back', 1.0): 98, ('atchya', 1.0): 1, ('even', 1.0): 35, ('almost', 1.0): 10, ('chanc', 1.0): 6, ('cheer', 1.0): 20, ('po', 1.0): 4, ('ice', 1.0): 6, ('cream', 1.0): 6, ('agre', 1.0): 16, ('100', 1.0): 8, ('heheheh', 1.0): 2, ('that', 1.0): 13, ('point', 1.0): 13, ('stay', 1.0): 25, ('home', 1.0): 31, ('soon', 1.0): 47, ('promis', 1.0): 6, ('web', 1.0): 4, ('whatsapp', 1.0): 5, ('volta', 1.0): 1, ('funcionar', 1.0): 1, ('com', 1.0): 2, ('iphon', 1.0): 7, ('jailbroken', 1.0): 1, ('later', 1.0): 16, ('34', 1.0): 3, ('min', 1.0): 9, ('leia', 1.0): 1, ('appear', 1.0): 3, ('hologram', 1.0): 1, ('r2d2', 1.0): 1, ('w', 1.0): 18, ('messag', 1.0): 10, ('obi', 1.0): 1, ('wan', 1.0): 3, ('sit', 1.0): 8, ('luke', 1.0): 6, ('inter', 1.0): 1, ('3', 1.0): 31, ('ucl', 1.0): 1, ('arsen', 1.0): 2, ('small', 1.0): 4, ('team', 1.0): 29, ('pass', 1.0): 12, ('🚂', 1.0): 1, ('dewsburi', 1.0): 2, ('railway', 1.0): 1, ('station', 1.0): 4, ('dew', 1.0): 1, ('west', 1.0): 3, ('yorkshir', 1.0): 2, ('430', 1.0): 1, ('smh', 1.0): 2, ('9:25', 1.0): 1, ('live', 1.0): 26, ('strang', 1.0): 4, ('imagin', 1.0): 5, ('megan', 1.0): 1, ('masaantoday', 1.0): 6, ('a4', 1.0): 3, ('shweta', 1.0): 1, ('tripathi', 1.0): 1, ('5', 1.0): 17, ('20', 1.0): 6, ('kurta', 1.0): 3, ('half', 1.0): 7, ('number', 1.0): 13, ('wsalelov', 1.0): 16, ('ah', 1.0): 13, ('larri', 1.0): 3, ('anyway', 1.0): 16, ('kinda', 1.0): 13, ('goood', 1.0): 4, ('life', 1.0): 49, ('enn', 1.0): 1, ('could', 1.0): 32, ('warmup', 1.0): 1, ('15th', 1.0): 2, ('bath', 1.0): 7, ('dum', 1.0): 2, ('andar', 1.0): 1, ('ram', 1.0): 1, ('sampath', 1.0): 1, ('sona', 1.0): 1, ('mohapatra', 1.0): 1, ('samantha', 1.0): 1, ('edward', 1.0): 1, ('mein', 1.0): 1, ('tulan', 1.0): 1, ('razi', 1.0): 2, ('wah', 1.0): 2, ('josh', 1.0): 1, ('alway', 1.0): 67, ('smile', 1.0): 62, ('pictur', 1.0): 12, ('16.20', 1.0): 1, ('giveitup', 1.0): 1, ('given', 1.0): 3, ('ga', 1.0): 3, ('subsidi', 1.0): 1, ('initi', 1.0): 4, ('propos', 1.0): 3, ('delight', 1.0): 7, ('yesterday', 1.0): 7, ('x42', 1.0): 1, ('lmaoo', 1.0): 2, ('song', 1.0): 22, ('ever', 1.0): 23, ('shall', 1.0): 6, ('littl', 1.0): 31, ('throwback', 1.0): 3, ('outli', 1.0): 1, ('island', 1.0): 5, ('cheung', 1.0): 1, ('chau', 1.0): 1, ('mui', 1.0): 1, ('wo', 1.0): 1, ('total', 1.0): 9, ('differ', 1.0): 11, ('kfckitchentour', 1.0): 2, ('kitchen', 1.0): 4, ('clean', 1.0): 1, (\"i'm\", 1.0): 183, ('cusp', 1.0): 1, ('test', 1.0): 7, ('water', 1.0): 8, ('reward', 1.0): 1, ('arummzz', 1.0): 2, (\"let'\", 1.0): 23, ('drive', 1.0): 11, ('travel', 1.0): 20, ('yogyakarta', 1.0): 3, ('jeep', 1.0): 3, ('indonesia', 1.0): 4, ('instamood', 1.0): 3, ('wanna', 1.0): 30, ('skype', 1.0): 3, ('may', 1.0): 22, ('nice', 1.0): 98, ('friendli', 1.0): 2, ('pretend', 1.0): 2, ('film', 1.0): 9, ('congratul', 1.0): 15, ('winner', 1.0): 4, ('cheesydelight', 1.0): 1, ('contest', 1.0): 6, ('address', 1.0): 10, ('guy', 1.0): 60, ('market', 1.0): 5, ('24/7', 1.0): 1, ('14', 1.0): 1, ('hour', 1.0): 27, ('leav', 1.0): 12, ('without', 1.0): 12, ('delay', 1.0): 2, ('actual', 1.0): 19, ('easi', 1.0): 9, ('guess', 1.0): 14, ('train', 1.0): 10, ('wd', 1.0): 1, ('shift', 1.0): 5, ('engin', 1.0): 2, ('etc', 1.0): 2, ('sunburn', 1.0): 1, ('peel', 1.0): 2, ('blog', 1.0): 31, ('huge', 1.0): 11, ('warm', 1.0): 6, ('☆', 1.0): 3, ('complet', 1.0): 11, ('triangl', 1.0): 2, ('northern', 1.0): 1, ('ireland', 1.0): 2, ('sight', 1.0): 1, ('smthng', 1.0): 2, ('fr', 1.0): 3, ('hug', 1.0): 13, ('xoxo', 1.0): 3, ('uu', 1.0): 1, ('jaann', 1.0): 1, ('topnewfollow', 1.0): 2, ('connect', 1.0): 14, ('wonder', 1.0): 35, ('made', 1.0): 53, ('fluffi', 1.0): 1, ('insid', 1.0): 8, ('pirouett', 1.0): 1, ('moos', 1.0): 1, ('trip', 1.0): 14, ('philli', 1.0): 1, ('decemb', 1.0): 3, (\"i'd\", 1.0): 20, ('dude', 1.0): 6, ('x41', 1.0): 1, ('question', 1.0): 17, ('flaw', 1.0): 1, ('pain', 1.0): 9, ('negat', 1.0): 1, ('strength', 1.0): 3, ('went', 1.0): 12, ('solo', 1.0): 4, ('move', 1.0): 12, ('fav', 1.0): 13, ('nirvana', 1.0): 1, ('smell', 1.0): 2, ('teen', 1.0): 3, ('spirit', 1.0): 3, ('rip', 1.0): 3, ('ami', 1.0): 4, ('winehous', 1.0): 1, ('coupl', 1.0): 9, ('tomhiddleston', 1.0): 1, ('elizabetholsen', 1.0): 1, ('yaytheylookgreat', 1.0): 1, ('goodnight', 1.0): 24, ('vid', 1.0): 11, ('wake', 1.0): 12, ('gonna', 1.0): 21, ('shoot', 1.0): 6, ('itti', 1.0): 2, ('bitti', 1.0): 2, ('teeni', 1.0): 2, ('bikini', 1.0): 3, ('much', 1.0): 89, ('4th', 1.0): 4, ('togeth', 1.0): 7, ('end', 1.0): 20, ('xfile', 1.0): 1, ('content', 1.0): 4, ('rain', 1.0): 21, ('fabul', 1.0): 5, ('fantast', 1.0): 13, ('♡', 1.0): 20, ('jb', 1.0): 1, ('forev', 1.0): 5, ('belieb', 1.0): 3, ('nighti', 1.0): 1, ('bug', 1.0): 3, ('bite', 1.0): 1, ('bracelet', 1.0): 2, ('idea', 1.0): 26, ('foundri', 1.0): 1, ('game', 1.0): 27, ('sens', 1.0): 7, ('pic', 1.0): 27, ('ef', 1.0): 1, ('phone', 1.0): 19, ('woot', 1.0): 2, ('derek', 1.0): 1, ('use', 1.0): 44, ('parkshar', 1.0): 1, ('gloucestershir', 1.0): 1, ('aaaahhh', 1.0): 1, ('man', 1.0): 23, ('traffic', 1.0): 2, ('stress', 1.0): 8, ('reliev', 1.0): 1, (\"how'r\", 1.0): 1, ('arbeloa', 1.0): 1, ('turn', 1.0): 15, ('17', 1.0): 4, ('omg', 1.0): 15, ('say', 1.0): 61, ('europ', 1.0): 1, ('rise', 1.0): 2, ('find', 1.0): 23, ('hard', 1.0): 12, ('believ', 1.0): 9, ('uncount', 1.0): 1, ('coz', 1.0): 3, ('unlimit', 1.0): 1, ('cours', 1.0): 18, ('teamposit', 1.0): 1, ('aldub', 1.0): 2, ('☕', 1.0): 3, ('rita', 1.0): 2, ('info', 1.0): 13, (\"we'd\", 1.0): 4, ('way', 1.0): 46, ('boy', 1.0): 21, ('x40', 1.0): 1, ('true', 1.0): 22, ('sethi', 1.0): 2, ('high', 1.0): 7, ('exe', 1.0): 1, ('skeem', 1.0): 1, ('saam', 1.0): 1, ('peopl', 1.0): 48, ('polit', 1.0): 2, ('izzat', 1.0): 1, ('wese', 1.0): 1, ('trust', 1.0): 9, ('khawateen', 1.0): 1, ('k', 1.0): 9, ('sath', 1.0): 2, ('mana', 1.0): 1, ('kar', 1.0): 1, ('deya', 1.0): 1, ('sort', 1.0): 9, ('smart', 1.0): 5, ('hair', 1.0): 12, ('tbh', 1.0): 5, ('jacob', 1.0): 2, ('g', 1.0): 10, ('upgrad', 1.0): 6, ('tee', 1.0): 2, ('famili', 1.0): 19, ('person', 1.0): 19, ('two', 1.0): 22, ('convers', 1.0): 6, ('onlin', 1.0): 7, ('mclaren', 1.0): 1, ('fridayfeel', 1.0): 5, ('tgif', 1.0): 10, ('squar', 1.0): 1, ('enix', 1.0): 1, ('bissmillah', 1.0): 1, ('ya', 1.0): 23, ('allah', 1.0): 3, (\"we'r\", 1.0): 29, ('socent', 1.0): 1, ('startup', 1.0): 2, ('drop', 1.0): 9, ('your', 1.0): 3, ('arnd', 1.0): 1, ('town', 1.0): 5, ('basic', 1.0): 4, ('piss', 1.0): 3, ('cup', 1.0): 4, ('also', 1.0): 35, ('terribl', 1.0): 2, ('complic', 1.0): 1, ('discuss', 1.0): 3, ('snapchat', 1.0): 36, ('lynettelow', 1.0): 1, ('kikmenow', 1.0): 3, ('snapm', 1.0): 2, ('hot', 1.0): 24, ('amazon', 1.0): 1, ('kikmeguy', 1.0): 3, ('defin', 1.0): 2, ('grow', 1.0): 7, ('sport', 1.0): 4, ('rt', 1.0): 12, ('rakyat', 1.0): 1, ('write', 1.0): 13, ('sinc', 1.0): 15, ('mention', 1.0): 24, ('fli', 1.0): 5, ('fish', 1.0): 3, ('promot', 1.0): 5, ('post', 1.0): 21, ('cyber', 1.0): 1, ('ourdaughtersourprid', 1.0): 5, ('mypapamyprid', 1.0): 2, ('papa', 1.0): 2, ('coach', 1.0): 2, ('posit', 1.0): 8, ('kha', 1.0): 1, ('atleast', 1.0): 2, ('x39', 1.0): 1, ('mango', 1.0): 1, (\"lassi'\", 1.0): 1, (\"monty'\", 1.0): 1, ('marvel', 1.0): 2, ('though', 1.0): 19, ('suspect', 1.0): 3, ('meant', 1.0): 3, ('24', 1.0): 4, ('hr', 1.0): 2, ('touch', 1.0): 15, ('kepler', 1.0): 4, ('452b', 1.0): 5, ('chalna', 1.0): 1, ('hai', 1.0): 11, ('thankyou', 1.0): 14, ('hazel', 1.0): 1, ('food', 1.0): 6, ('brooklyn', 1.0): 1, ('pta', 1.0): 2, ('awak', 1.0): 10, ('okayi', 1.0): 2, ('awww', 1.0): 15, ('ha', 1.0): 23, ('doc', 1.0): 1, ('splendid', 1.0): 1, ('spam', 1.0): 1, ('folder', 1.0): 1, ('amount', 1.0): 1, ('nigeria', 1.0): 1, ('claim', 1.0): 1, ('rted', 1.0): 1, ('leg', 1.0): 5, ('hurt', 1.0): 8, ('bad', 1.0): 18, ('mine', 1.0): 14, ('saturday', 1.0): 8, ('thaaank', 1.0): 1, ('puhon', 1.0): 1, ('happinesss', 1.0): 1, ('tnc', 1.0): 1, ('prior', 1.0): 1, ('notif', 1.0): 2, ('fat', 1.0): 1, ('co', 1.0): 1, ('probabl', 1.0): 9, ('ate', 1.0): 4, ('yuna', 1.0): 2, ('tamesid', 1.0): 1, ('´', 1.0): 3, ('googl', 1.0): 6, ('account', 1.0): 19, ('scouser', 1.0): 1, ('everyth', 1.0): 13, ('zoe', 1.0): 2, ('mate', 1.0): 7, ('liter', 1.0): 6, (\"they'r\", 1.0): 12, ('samee', 1.0): 1, ('edgar', 1.0): 1, ('updat', 1.0): 13, ('log', 1.0): 4, ('bring', 1.0): 17, ('abe', 1.0): 1, ('meet', 1.0): 34, ('x38', 1.0): 1, ('sigh', 1.0): 3, ('dreamili', 1.0): 1, ('pout', 1.0): 1, ('eye', 1.0): 14, ('quacketyquack', 1.0): 7, ('funni', 1.0): 19, ('happen', 1.0): 16, ('phil', 1.0): 1, ('em', 1.0): 3, ('del', 1.0): 1, ('rodder', 1.0): 1, ('els', 1.0): 10, ('play', 1.0): 46, ('newest', 1.0): 1, ('gamejam', 1.0): 1, ('irish', 1.0): 2, ('literatur', 1.0): 2, ('inaccess', 1.0): 2, (\"kareena'\", 1.0): 2, ('fan', 1.0): 30, ('brain', 1.0): 13, ('dot', 1.0): 11, ('braindot', 1.0): 11, ('fair', 1.0): 5, ('rush', 1.0): 1, ('either', 1.0): 11, ('brandi', 1.0): 1, ('18', 1.0): 5, ('carniv', 1.0): 1, ('men', 1.0): 10, ('put', 1.0): 17, ('mask', 1.0): 3, ('xavier', 1.0): 1, ('forneret', 1.0): 1, ('jennif', 1.0): 1, ('site', 1.0): 9, ('free', 1.0): 37, ('50.000', 1.0): 3, ('8', 1.0): 10, ('ball', 1.0): 7, ('pool', 1.0): 5, ('coin', 1.0): 5, ('edit', 1.0): 7, ('trish', 1.0): 1, ('♥', 1.0): 19, ('grate', 1.0): 5, ('three', 1.0): 10, ('comment', 1.0): 8, ('wakeup', 1.0): 1, ('besid', 1.0): 2, ('dirti', 1.0): 2, ('sex', 1.0): 6, ('lmaooo', 1.0): 1, ('😤', 1.0): 2, ('loui', 1.0): 4, (\"he'\", 1.0): 11, ('throw', 1.0): 3, ('caus', 1.0): 15, ('inspir', 1.0): 7, ('ff', 1.0): 48, ('twoof', 1.0): 3, ('gr8', 1.0): 1, ('wkend', 1.0): 3, ('kind', 1.0): 24, ('exhaust', 1.0): 2, ('word', 1.0): 20, ('cheltenham', 1.0): 1, ('area', 1.0): 4, ('kale', 1.0): 1, ('crisp', 1.0): 1, ('ruin', 1.0): 5, ('x37', 1.0): 1, ('open', 1.0): 12, ('worldwid', 1.0): 2, ('outta', 1.0): 1, ('sfvbeta', 1.0): 1, ('vantast', 1.0): 1, ('xcylin', 1.0): 1, ('bundl', 1.0): 1, ('show', 1.0): 28, ('internet', 1.0): 2, ('price', 1.0): 4, ('realisticli', 1.0): 1, ('pay', 1.0): 8, ('net', 1.0): 1, ('educ', 1.0): 1, ('power', 1.0): 7, ('weapon', 1.0): 1, ('nelson', 1.0): 1, ('mandela', 1.0): 1, ('recent', 1.0): 9, ('j', 1.0): 3, ('chenab', 1.0): 1, ('flow', 1.0): 5, ('pakistan', 1.0): 2, ('incredibleindia', 1.0): 1, ('teenchoic', 1.0): 10, ('choiceinternationalartist', 1.0): 9, ('superjunior', 1.0): 9, ('caught', 1.0): 4, ('first', 1.0): 50, ('salmon', 1.0): 3, ('super-blend', 1.0): 1, ('project', 1.0): 6, ('youth@bipolaruk.org.uk', 1.0): 1, ('awesom', 1.0): 42, ('stream', 1.0): 14, ('alma', 1.0): 1, ('mater', 1.0): 1, ('highschoolday', 1.0): 1, ('clientvisit', 1.0): 1, ('faith', 1.0): 3, ('christian', 1.0): 1, ('school', 1.0): 9, ('lizaminnelli', 1.0): 1, ('upcom', 1.0): 2, ('uk', 1.0): 4, ('😄', 1.0): 5, ('singl', 1.0): 6, ('hill', 1.0): 4, ('everi', 1.0): 26, ('beat', 1.0): 10, ('wrong', 1.0): 10, ('readi', 1.0): 25, ('natur', 1.0): 1, ('pefumeri', 1.0): 1, ('workshop', 1.0): 3, ('neal', 1.0): 1, ('yard', 1.0): 1, ('covent', 1.0): 1, ('tomorrow', 1.0): 40, ('fback', 1.0): 27, ('indo', 1.0): 1, ('harmo', 1.0): 1, ('americano', 1.0): 1, ('rememb', 1.0): 16, ('aww', 1.0): 10, ('head', 1.0): 14, ('saw', 1.0): 19, ('dark', 1.0): 6, ('handshom', 1.0): 1, ('juga', 1.0): 1, ('hurray', 1.0): 1, ('hate', 1.0): 13, ('cant', 1.0): 15, ('decid', 1.0): 4, ('save', 1.0): 12, ('list', 1.0): 15, ('hiya', 1.0): 4, ('exec', 1.0): 1, ('loryn.good@lincs-chamber.co.uk', 1.0): 1, ('photo', 1.0): 19, ('thx', 1.0): 15, ('4', 1.0): 24, ('china', 1.0): 2, ('homosexu', 1.0): 1, ('hyungbot', 1.0): 1, ('give', 1.0): 48, ('fam', 1.0): 5, ('mind', 1.0): 23, ('timetunnel', 1.0): 1, ('1982', 1.0): 1, ('quit', 1.0): 13, ('radio', 1.0): 5, ('set', 1.0): 11, ('heart', 1.0): 11, ('hiii', 1.0): 2, ('jack', 1.0): 3, ('ili', 1.0): 5, ('✨', 1.0): 4, ('domino', 1.0): 1, ('pub', 1.0): 1, ('heat', 1.0): 1, ('prob', 1.0): 5, ('sorri', 1.0): 22, ('hastili', 1.0): 1, ('type', 1.0): 6, ('came', 1.0): 7, ('pakistani', 1.0): 1, ('x36', 1.0): 1, ('3point', 1.0): 1, ('dreamteam', 1.0): 1, ('gooo', 1.0): 2, ('bailey', 1.0): 2, ('pbb', 1.0): 4, ('737gold', 1.0): 3, ('drank', 1.0): 2, ('old', 1.0): 13, ('gotten', 1.0): 2, ('1/2', 1.0): 1, ('welsh', 1.0): 1, ('wale', 1.0): 3, ('yippe', 1.0): 1, ('💟', 1.0): 4, ('bro', 1.0): 24, ('lord', 1.0): 4, ('michael', 1.0): 4, (\"u'r\", 1.0): 1, ('ure', 1.0): 1, ('bigot', 1.0): 1, ('usual', 1.0): 6, ('front', 1.0): 4, ('squat', 1.0): 1, ('dobar', 1.0): 1, ('dan', 1.0): 5, ('brand', 1.0): 8, ('heavi', 1.0): 2, ('musicolog', 1.0): 1, ('2015', 1.0): 16, ('spend', 1.0): 2, ('marathon', 1.0): 1, ('iflix', 1.0): 2, ('offici', 1.0): 10, ('graduat', 1.0): 3, ('cri', 1.0): 9, ('__', 1.0): 1, ('yep', 1.0): 9, ('expert', 1.0): 4, ('bisexu', 1.0): 1, ('minal', 1.0): 1, ('aidzin', 1.0): 1, ('yo', 1.0): 7, ('pi', 1.0): 1, ('cook', 1.0): 2, ('book', 1.0): 21, ('dinner', 1.0): 7, ('tough', 1.0): 2, ('choic', 1.0): 8, ('other', 1.0): 12, ('chill', 1.0): 6, ('smu', 1.0): 1, ('oval', 1.0): 1, ('basketbal', 1.0): 1, ('player', 1.0): 4, ('whahahaha', 1.0): 1, ('soamaz', 1.0): 1, ('moment', 1.0): 12, ('onto', 1.0): 3, ('a5', 1.0): 1, ('wardrob', 1.0): 2, ('user', 1.0): 3, ('teamr', 1.0): 1, ('appar', 1.0): 6, ('depend', 1.0): 2, ('greatli', 1.0): 1, ('design', 1.0): 21, ('ahhh', 1.0): 1, ('7th', 1.0): 1, ('cinepambata', 1.0): 1, ('mechan', 1.0): 1, ('form', 1.0): 4, ('download', 1.0): 10, ('ur', 1.0): 38, ('swisher', 1.0): 1, ('cop', 1.0): 1, ('ducktail', 1.0): 1, ('surreal', 1.0): 3, ('exposur', 1.0): 1, ('sotw', 1.0): 1, ('halesowen', 1.0): 1, ('blackcountryfair', 1.0): 1, ('street', 1.0): 1, ('assess', 1.0): 1, ('mental', 1.0): 4, ('bodi', 1.0): 15, ('ooz', 1.0): 1, ('appeal', 1.0): 1, ('amassiveoverdoseofship', 1.0): 1, ('latest', 1.0): 5, ('isi', 1.0): 1, ('chan', 1.0): 1, ('c', 1.0): 9, ('note', 1.0): 6, ('pkwalasawa', 1.0): 1, ('gemma', 1.0): 1, ('orlean', 1.0): 1, ('fever', 1.0): 2, ('geskenya', 1.0): 1, ('obamainkenya', 1.0): 1, ('magicalkenya', 1.0): 1, ('greatkenya', 1.0): 1, ('allgoodthingsk', 1.0): 1, ('anim', 1.0): 6, ('umaru', 1.0): 1, ('singer', 1.0): 4, ('ship', 1.0): 8, ('order', 1.0): 17, ('room', 1.0): 5, ('car', 1.0): 6, ('gone', 1.0): 5, ('hahaha', 1.0): 14, ('stori', 1.0): 11, ('relat', 1.0): 4, ('label', 1.0): 1, ('worst', 1.0): 3, ('batch', 1.0): 1, ('princip', 1.0): 1, ('due', 1.0): 3, ('march', 1.0): 1, ('wooftast', 1.0): 2, ('receiv', 1.0): 8, ('necessari', 1.0): 1, ('regret', 1.0): 4, ('rn', 1.0): 4, ('whatev', 1.0): 5, ('hat', 1.0): 1, ('success', 1.0): 6, ('abstin', 1.0): 1, ('wtf', 1.0): 3, (\"there'\", 1.0): 11, ('thrown', 1.0): 1, ('middl', 1.0): 2, ('repeat', 1.0): 3, ('relentlessli', 1.0): 1, ('approxim', 1.0): 1, ('oldschool', 1.0): 1, ('runescap', 1.0): 1, ('daaay', 1.0): 1, ('jumma_mubarik', 1.0): 1, ('frnd', 1.0): 1, ('stay_bless', 1.0): 1, ('bless', 1.0): 12, ('pussycat', 1.0): 1, ('main', 1.0): 7, ('launch', 1.0): 4, ('pretoria', 1.0): 1, ('fahrinahmad', 1.0): 1, ('tengkuaaronshah', 1.0): 1, ('eksperimencinta', 1.0): 1, ('tykkäsin', 1.0): 1, ('videosta', 1.0): 1, ('month', 1.0): 13, ('hoodi', 1.0): 2, ('eeep', 1.0): 1, ('yay', 1.0): 16, ('sohappyrightnow', 1.0): 1, ('mmm', 1.0): 1, ('azz-set', 1.0): 1, ('babe', 1.0): 9, ('feedback', 1.0): 11, ('gain', 1.0): 6, ('valu', 1.0): 2, ('peac', 1.0): 8, ('refresh', 1.0): 5, ('manthan', 1.0): 1, ('tune', 1.0): 5, ('fresh', 1.0): 6, ('mother', 1.0): 5, ('determin', 1.0): 2, ('maxfreshmov', 1.0): 2, ('loneliest', 1.0): 1, ('tattoo', 1.0): 3, ('friday.and', 1.0): 1, ('magnific', 1.0): 2, ('e', 1.0): 5, ('achiev', 1.0): 2, ('rashmi', 1.0): 1, ('dedic', 1.0): 2, ('happyfriday', 1.0): 6, ('nearli', 1.0): 4, ('retweet', 1.0): 35, ('alert', 1.0): 1, ('da', 1.0): 5, ('dang', 1.0): 2, ('rad', 1.0): 2, ('fanart', 1.0): 1, ('massiv', 1.0): 1, ('niamh', 1.0): 1, ('fennel', 1.0): 1, ('journal', 1.0): 1, ('land', 1.0): 2, ('copi', 1.0): 5, ('past', 1.0): 7, ('tweet', 1.0): 61, ('yesss', 1.0): 5, ('ariana', 1.0): 2, ('selena', 1.0): 2, ('gomez', 1.0): 1, ('tomlinson', 1.0): 1, ('payn', 1.0): 1, ('caradelevingn', 1.0): 1, ('🌷', 1.0): 1, ('trade', 1.0): 3, ('tire', 1.0): 5, ('nope', 1.0): 7, ('appli', 1.0): 6, ('iamca', 1.0): 1, ('found', 1.0): 15, ('afti', 1.0): 1, ('goodmorn', 1.0): 8, ('prokabaddi', 1.0): 1, ('koel', 1.0): 1, ('mallick', 1.0): 1, ('recit', 1.0): 4, ('nation', 1.0): 3, ('anthem', 1.0): 1, ('6', 1.0): 23, ('yournaturallead', 1.0): 1, ('youngnaturallead', 1.0): 1, ('mon', 1.0): 3, ('27juli', 1.0): 1, ('cumbria', 1.0): 1, ('flockstar', 1.0): 1, ('thur', 1.0): 2, ('30juli', 1.0): 1, ('itv', 1.0): 1, ('sleeptight', 1.0): 1, ('haveagoodday', 1.0): 1, ('septemb', 1.0): 5, ('perhap', 1.0): 3, ('bb', 1.0): 4, ('full', 1.0): 19, ('album', 1.0): 6, ('fulli', 1.0): 2, ('intend', 1.0): 1, ('possibl', 1.0): 7, ('attack', 1.0): 3, ('&gt;:d', 1.0): 4, ('bird', 1.0): 4, ('teamadmicro', 1.0): 1, ('fridaydownpour', 1.0): 1, ('clear', 1.0): 4, ('rohit', 1.0): 1, ('queen', 1.0): 8, ('otwolgrandtrail', 1.0): 3, ('sheer', 1.0): 1, ('fact', 1.0): 8, ('obama', 1.0): 1, ('innumer', 1.0): 1, ('presid', 1.0): 2, ('ni', 1.0): 3, ('shauri', 1.0): 1, ('yako', 1.0): 1, ('memotohat', 1.0): 1, ('sunday', 1.0): 9, ('pamper', 1.0): 2, (\"t'wa\", 1.0): 1, ('cabincrew', 1.0): 1, ('interview', 1.0): 5, ('langkawi', 1.0): 1, ('1st', 1.0): 1, ('august', 1.0): 7, ('fulfil', 1.0): 5, ('fantasi', 1.0): 6, ('👉', 1.0): 6, ('ex-tweleb', 1.0): 1, ('apart', 1.0): 2, ('makeov', 1.0): 1, ('brilliantli', 1.0): 1, ('happyyi', 1.0): 1, ('birthdaaayyy', 1.0): 2, ('kill', 1.0): 3, ('interest', 1.0): 20, ('internship', 1.0): 3, ('program', 1.0): 5, ('sadli', 1.0): 1, ('career', 1.0): 3, ('page', 1.0): 9, ('issu', 1.0): 10, ('sad', 1.0): 5, ('overwhelmingli', 1.0): 1, ('aha', 1.0): 2, ('beaut', 1.0): 2, ('♬', 1.0): 2, ('win', 1.0): 16, ('deo', 1.0): 1, ('faaabul', 1.0): 1, ('freebiefriday', 1.0): 4, ('aluminiumfre', 1.0): 1, ('stayfresh', 1.0): 1, ('john', 1.0): 6, ('worri', 1.0): 18, ('navig', 1.0): 1, ('thnk', 1.0): 1, ('progrmr', 1.0): 1, ('9pm', 1.0): 1, ('9am', 1.0): 2, ('hardli', 1.0): 1, ('rose', 1.0): 4, ('emot', 1.0): 3, ('poetri', 1.0): 1, ('frequentfly', 1.0): 1, ('break', 1.0): 10, ('apolog', 1.0): 4, ('kb', 1.0): 1, ('londondairi', 1.0): 1, ('icecream', 1.0): 2, ('experi', 1.0): 7, ('cover', 1.0): 9, ('sin', 1.0): 1, ('excit', 1.0): 33, (\":')\", 1.0): 2, ('xxx', 1.0): 15, ('jim', 1.0): 1, ('chuckl', 1.0): 1, ('cake', 1.0): 10, ('doh', 1.0): 1, ('500', 1.0): 2, ('subscrib', 1.0): 2, ('reach', 1.0): 1, ('scorch', 1.0): 1, ('summer', 1.0): 17, ('younger', 1.0): 4, ('woman', 1.0): 4, ('stamina', 1.0): 1, ('expect', 1.0): 6, ('anyth', 1.0): 22, ('less', 1.0): 8, ('tweeti', 1.0): 1, ('fab', 1.0): 12, ('dont', 1.0): 13, ('--&gt;', 1.0): 2, ('10', 1.0): 16, ('loner', 1.0): 3, ('introduc', 1.0): 3, ('vs', 1.0): 4, ('alter', 1.0): 1, ('understand', 1.0): 6, ('spread', 1.0): 8, ('problem', 1.0): 19, ('supa', 1.0): 1, ('dupa', 1.0): 1, ('near', 1.0): 6, ('dartmoor', 1.0): 1, ('gold', 1.0): 7, ('colour', 1.0): 4, ('ok', 1.0): 38, ('someday', 1.0): 4, ('r', 1.0): 14, ('dii', 1.0): 1, ('n', 1.0): 17, ('forget', 1.0): 17, ('si', 1.0): 4, ('smf', 1.0): 1, ('ft', 1.0): 4, ('japanes', 1.0): 3, ('import', 1.0): 5, ('kitti', 1.0): 1, ('match', 1.0): 6, ('stationari', 1.0): 1, ('draw', 1.0): 6, ('close', 1.0): 14, ('broken', 1.0): 3, ('specialis', 1.0): 4, ('thermal', 1.0): 4, ('imag', 1.0): 6, ('survey', 1.0): 4, ('–', 1.0): 14, ('south', 1.0): 2, ('korea', 1.0): 3, ('scamper', 1.0): 1, ('slept', 1.0): 4, ('alarm', 1.0): 1, (\"ain't\", 1.0): 5, ('mad', 1.0): 4, ('chweina', 1.0): 1, ('xd', 1.0): 4, ('jotzh', 1.0): 1, ('wast', 1.0): 7, ('place', 1.0): 21, ('worth', 1.0): 11, ('coat', 1.0): 3, ('beforehand', 1.0): 1, ('tho', 1.0): 12, ('foh', 1.0): 2, ('outsid', 1.0): 5, ('holiday', 1.0): 11, ('menac', 1.0): 1, ('jojo', 1.0): 2, ('ta', 1.0): 2, ('accept', 1.0): 1, ('admin', 1.0): 2, ('lukri', 1.0): 1, ('😘', 1.0): 10, ('momma', 1.0): 2, ('bear', 1.0): 2, ('❤', 1.0): 29, ('️', 1.0): 20, ('redid', 1.0): 1, ('8th', 1.0): 1, ('v.ball', 1.0): 1, ('atm', 1.0): 4, ('build', 1.0): 8, ('pack', 1.0): 8, ('suitcas', 1.0): 2, ('hang-copi', 1.0): 1, ('translat', 1.0): 1, (\"dostoevsky'\", 1.0): 1, ('voucher', 1.0): 2, ('bugatti', 1.0): 1, ('bra', 1.0): 3, ('مطعم_هاشم', 1.0): 1, ('yummi', 1.0): 3, ('a7la', 1.0): 1, ('bdayt', 1.0): 1, ('mnwreeen', 1.0): 1, ('jazz', 1.0): 2, ('truck', 1.0): 1, ('x34', 1.0): 1, ('speak', 1.0): 8, ('pbevent', 1.0): 1, ('hq', 1.0): 1, ('add', 1.0): 22, ('yoona', 1.0): 1, ('hairpin', 1.0): 1, ('otp', 1.0): 1, ('collect', 1.0): 7, ('mastership', 1.0): 1, ('honey', 1.0): 4, ('paindo', 1.0): 1, ('await', 1.0): 1, ('report', 1.0): 3, ('manni', 1.0): 1, ('asshol', 1.0): 3, ('brijresid', 1.0): 1, ('structur', 1.0): 1, ('156', 1.0): 1, ('unit', 1.0): 3, ('encompass', 1.0): 1, ('bhk', 1.0): 1, ('flat', 1.0): 2, ('91', 1.0): 2, ('975-580-', 1.0): 1, ('444', 1.0): 1, ('honor', 1.0): 2, ('curri', 1.0): 2, ('clash', 1.0): 1, ('milano', 1.0): 1, ('👌', 1.0): 1, ('followback', 1.0): 6, (':-d', 1.0): 5, ('legit', 1.0): 1, ('loser', 1.0): 5, ('gass', 1.0): 1, ('dead', 1.0): 4, ('starsquad', 1.0): 4, ('⭐', 1.0): 3, ('news', 1.0): 25, ('utc', 1.0): 1, ('flume', 1.0): 1, ('kaytranada', 1.0): 1, ('alunageorg', 1.0): 1, ('ticket', 1.0): 12, ('km', 1.0): 1, ('certainti', 1.0): 1, ('solv', 1.0): 2, ('faster', 1.0): 3, ('👊', 1.0): 1, ('hurri', 1.0): 5, ('totem', 1.0): 1, ('somewher', 1.0): 5, ('alic', 1.0): 4, ('dog', 1.0): 6, ('cat', 1.0): 5, ('goodwynsgoodi', 1.0): 1, ('ugh', 1.0): 1, ('fade', 1.0): 2, ('moan', 1.0): 1, ('leed', 1.0): 1, ('jozi', 1.0): 1, ('wasnt', 1.0): 2, ('fifth', 1.0): 2, ('avail', 1.0): 10, ('tix', 1.0): 2, ('pa', 1.0): 2, ('ba', 1.0): 2, ('ng', 1.0): 2, ('atl', 1.0): 1, ('coldplay', 1.0): 1, ('favorit', 1.0): 14, ('scientist', 1.0): 1, ('yellow', 1.0): 2, ('atla', 1.0): 1, ('yein', 1.0): 1, ('selo', 1.0): 1, ('jabongatpumaurbanstamped', 1.0): 4, ('an', 1.0): 3, ('7', 1.0): 8, ('waiter', 1.0): 1, ('bill', 1.0): 5, ('sir', 1.0): 12, ('titl', 1.0): 2, ('pocket', 1.0): 1, ('wrip', 1.0): 1, ('jean', 1.0): 1, ('conni', 1.0): 2, ('crew', 1.0): 3, ('staff', 1.0): 2, ('sweetan', 1.0): 1, ('ask', 1.0): 37, ('mum', 1.0): 2, ('beg', 1.0): 2, ('soprano', 1.0): 1, ('ukrain', 1.0): 2, ('x33', 1.0): 1, ('olli', 1.0): 2, ('disney.art', 1.0): 1, ('elmoprinssi', 1.0): 1, ('salsa', 1.0): 1, ('danc', 1.0): 2, ('tell', 1.0): 25, ('truth', 1.0): 4, ('pl', 1.0): 8, ('4-6', 1.0): 1, ('2nd', 1.0): 5, ('blogiversari', 1.0): 1, ('review', 1.0): 9, ('cuti', 1.0): 6, ('bohol', 1.0): 1, ('briliant', 1.0): 1, ('v', 1.0): 9, ('key', 1.0): 3, ('annual', 1.0): 1, ('far', 1.0): 19, ('spin', 1.0): 2, ('voic', 1.0): 3, ('\\U000fe334', 1.0): 1, ('yeheyi', 1.0): 1, ('pinya', 1.0): 1, ('whoooah', 1.0): 1, ('tranc', 1.0): 1, ('lover', 1.0): 4, ('subject', 1.0): 7, ('physic', 1.0): 1, ('stop', 1.0): 15, ('ब', 1.0): 1, ('matter', 1.0): 6, ('jungl', 1.0): 1, ('accommod', 1.0): 1, ('secret', 1.0): 9, ('behind', 1.0): 3, ('sandroforceo', 1.0): 2, ('ceo', 1.0): 11, ('1month', 1.0): 11, ('swag', 1.0): 1, ('mia', 1.0): 1, ('workinprogress', 1.0): 1, ('choos', 1.0): 2, ('finnigan', 1.0): 1, ('loyal', 1.0): 2, ('royal', 1.0): 2, ('fotoset', 1.0): 1, ('reus', 1.0): 1, ('seem', 1.0): 10, ('somebodi', 1.0): 1, ('sell', 1.0): 1, ('young', 1.0): 3, ('muntu', 1.0): 1, ('anoth', 1.0): 23, ('gem', 1.0): 2, ('falco', 1.0): 1, ('supersmash', 1.0): 1, ('hotnsexi', 1.0): 1, ('friskyfriday', 1.0): 1, ('beach', 1.0): 4, ('movi', 1.0): 24, ('crop', 1.0): 2, ('nash', 1.0): 1, ('tissu', 1.0): 1, ('chocol', 1.0): 7, ('tea', 1.0): 6, ('hannib', 1.0): 3, ('episod', 1.0): 5, ('hotb', 1.0): 1, ('bush', 1.0): 2, ('classicassur', 1.0): 1, ('thrill', 1.0): 2, ('intern', 1.0): 2, ('assign', 1.0): 1, ('aerial', 1.0): 1, ('camera', 1.0): 6, ('oper', 1.0): 1, ('boom', 1.0): 3, ('hong', 1.0): 1, ('kong', 1.0): 1, ('ferri', 1.0): 1, ('central', 1.0): 2, ('girlfriend', 1.0): 4, ('after-work', 1.0): 1, ('drink', 1.0): 8, ('dj', 1.0): 3, ('resto', 1.0): 1, ('drinkt', 1.0): 1, ('koffi', 1.0): 1, ('a6', 1.0): 1, ('stargat', 1.0): 1, ('atlanti', 1.0): 1, ('muaahhh', 1.0): 1, ('ohh', 1.0): 3, ('hii', 1.0): 2, ('🙈', 1.0): 1, ('di', 1.0): 5, ('nagsend', 1.0): 1, ('yung', 1.0): 1, ('ko', 1.0): 4, ('&lt;/3', 1.0): 1, ('ulit', 1.0): 3, ('🎉', 1.0): 5, ('🎈', 1.0): 1, ('ugli', 1.0): 4, ('legget', 1.0): 1, ('qui', 1.0): 1, ('per', 1.0): 1, ('la', 1.0): 8, ('mar', 1.0): 1, ('encourag', 1.0): 3, ('employ', 1.0): 5, ('board', 1.0): 5, ('sticker', 1.0): 1, ('sponsor', 1.0): 4, ('prize', 1.0): 3, ('(:', 1.0): 1, ('milo', 1.0): 1, ('aurini', 1.0): 1, ('juicebro', 1.0): 1, ('pillar', 1.0): 2, ('respect', 1.0): 2, ('boii', 1.0): 1, ('smashingbook', 1.0): 1, ('bibl', 1.0): 2, ('ill', 1.0): 6, ('sick', 1.0): 4, ('lamo', 1.0): 1, ('fangirl', 1.0): 3, ('platon', 1.0): 1, ('scienc', 1.0): 5, ('resid', 1.0): 2, ('servicewithasmil', 1.0): 1, ('bloodlin', 1.0): 1, ('huski', 1.0): 1, ('obituari', 1.0): 1, ('advert', 1.0): 1, ('goofingaround', 1.0): 1, ('bollywood', 1.0): 1, ('giveaway', 1.0): 6, ('dah', 1.0): 2, ('noth', 1.0): 15, ('bitter', 1.0): 2, ('anger', 1.0): 1, ('hatr', 1.0): 2, ('toward', 1.0): 2, ('pure', 1.0): 2, ('indiffer', 1.0): 1, ('suit', 1.0): 5, ('zach', 1.0): 1, ('codi', 1.0): 2, ('deliv', 1.0): 3, ('ac', 1.0): 1, ('excel', 1.0): 6, ('produc', 1.0): 1, ('boggl', 1.0): 1, ('fatigu', 1.0): 1, ('baareeq', 1.0): 1, ('gamedev', 1.0): 2, ('hobbi', 1.0): 1, ('tweenie_fox', 1.0): 1, ('click', 1.0): 3, ('accessori', 1.0): 1, ('tamang', 1.0): 1, ('hinala', 1.0): 1, ('niam', 1.0): 1, ('selfiee', 1.0): 1, ('especi', 1.0): 4, ('lass', 1.0): 1, ('ale', 1.0): 1, ('swim', 1.0): 3, ('bout', 1.0): 3, ('goodby', 1.0): 5, ('feminist', 1.0): 1, ('fought', 1.0): 1, ('snobbi', 1.0): 1, ('bitch', 1.0): 3, ('carolin', 1.0): 2, ('mighti', 1.0): 1, ('🔥', 1.0): 1, ('threw', 1.0): 2, ('hbd', 1.0): 1, ('follback', 1.0): 19, ('jog', 1.0): 1, ('remot', 1.0): 2, ('newli', 1.0): 1, ('ebay', 1.0): 2, ('store', 1.0): 15, ('disneyinfin', 1.0): 1, ('starwar', 1.0): 1, ('charact', 1.0): 3, ('preorder', 1.0): 1, ('starter', 1.0): 1, ('hit', 1.0): 13, ('snap', 1.0): 4, ('homi', 1.0): 3, ('bought', 1.0): 4, ('skin', 1.0): 8, ('bday', 1.0): 11, ('chant', 1.0): 2, ('jai', 1.0): 1, ('itali', 1.0): 2, ('fast', 1.0): 4, ('heeeyyy', 1.0): 1, ('woah', 1.0): 3, ('★', 1.0): 5, ('😊', 1.0): 11, ('whenev', 1.0): 4, ('ang', 1.0): 2, ('kiss', 1.0): 4, ('philippin', 1.0): 2, ('packag', 1.0): 3, ('bruis', 1.0): 1, ('rib', 1.0): 2, ('😀', 1.0): 2, ('😁', 1.0): 6, ('😂', 1.0): 17, ('😃', 1.0): 1, ('😅', 1.0): 1, ('😉', 1.0): 2, ('tombraid', 1.0): 1, ('hype', 1.0): 1, ('thejuiceinthemix', 1.0): 1, ('rela', 1.0): 1, ('low', 1.0): 6, ('prioriti', 1.0): 1, ('harri', 1.0): 5, ('bc', 1.0): 9, ('collaps', 1.0): 2, ('chaotic', 1.0): 1, ('cosa', 1.0): 1, ('&lt;---', 1.0): 2, ('alliter', 1.0): 1, ('oppayaa', 1.0): 1, (\"how'\", 1.0): 4, ('natgeo', 1.0): 1, ('lick', 1.0): 1, ('elbow', 1.0): 2, ('. .', 1.0): 2, ('“', 1.0): 7, ('emu', 1.0): 1, ('stoke', 1.0): 1, ('woke', 1.0): 5, (\"people'\", 1.0): 3, ('approv', 1.0): 6, (\"god'\", 1.0): 2, ('jisung', 1.0): 1, ('sunshin', 1.0): 7, ('mm', 1.0): 6, ('nicola', 1.0): 1, ('brighten', 1.0): 2, ('helen', 1.0): 3, ('brian', 1.0): 3, ('2-3', 1.0): 1, ('australia', 1.0): 5, ('ol', 1.0): 2, ('bone', 1.0): 1, ('creak', 1.0): 1, ('abuti', 1.0): 1, ('tweetland', 1.0): 1, ('android', 1.0): 3, ('xma', 1.0): 2, ('skyblock', 1.0): 1, ('bcaus', 1.0): 1, ('2009', 1.0): 1, ('die', 1.0): 10, ('twitch', 1.0): 5, ('sympathi', 1.0): 1, ('laugh', 1.0): 5, ('unniee', 1.0): 1, ('nuka', 1.0): 1, ('penacova', 1.0): 1, ('djset', 1.0): 1, ('edm', 1.0): 1, ('kizomba', 1.0): 1, ('latinhous', 1.0): 1, ('housemus', 1.0): 3, ('portug', 1.0): 1, ('wild', 1.0): 2, ('ride', 1.0): 6, ('anytim', 1.0): 6, ('tast', 1.0): 5, ('yer', 1.0): 2, ('mtn', 1.0): 2, ('maganda', 1.0): 1, ('mistress', 1.0): 2, ('saphir', 1.0): 1, ('busi', 1.0): 19, ('4000', 1.0): 1, ('instagram', 1.0): 7, ('among', 1.0): 5, ('coconut', 1.0): 1, ('sambal', 1.0): 1, ('mussel', 1.0): 1, ('recip', 1.0): 5, ('kalin', 1.0): 1, ('mixcloud', 1.0): 1, ('sarcasm', 1.0): 2, ('chelsea', 1.0): 3, ('he', 1.0): 2, ('useless', 1.0): 2, ('thursday', 1.0): 2, ('hang', 1.0): 3, ('hehe', 1.0): 10, ('said', 1.0): 16, ('benson', 1.0): 1, ('facebook', 1.0): 5, ('solid', 1.0): 1, ('16/17', 1.0): 1, ('30', 1.0): 3, ('°', 1.0): 1, ('😜', 1.0): 2, ('maryhick', 1.0): 1, ('kikmeboy', 1.0): 7, ('photooftheday', 1.0): 4, ('musicbiz', 1.0): 2, ('sheskindahot', 1.0): 1, ('fleekil', 1.0): 1, ('mbalula', 1.0): 1, ('africa', 1.0): 1, ('mexican', 1.0): 1, ('scar', 1.0): 1, ('offic', 1.0): 8, ('donut', 1.0): 2, ('foiegra', 1.0): 2, ('despit', 1.0): 2, ('weather', 1.0): 9, ('wed', 1.0): 5, ('toni', 1.0): 2, ('stark', 1.0): 1, ('incred', 1.0): 7, ('poem', 1.0): 2, ('bubbl', 1.0): 3, ('dale', 1.0): 1, ('billion', 1.0): 1, ('magic', 1.0): 5, ('op', 1.0): 3, ('cast', 1.0): 1, ('vote', 1.0): 9, ('elect', 1.0): 1, ('jcreport', 1.0): 1, ('piggin', 1.0): 1, ('botan', 1.0): 2, ('soap', 1.0): 4, ('late', 1.0): 13, ('upload', 1.0): 5, ('freshli', 1.0): 1, ('3week', 1.0): 1, ('heal', 1.0): 1, ('tobi-bro', 1.0): 1, ('isp', 1.0): 1, ('steel', 1.0): 1, ('wednesday', 1.0): 1, ('swear', 1.0): 3, ('met', 1.0): 4, ('earlier', 1.0): 4, ('cam', 1.0): 3, ('😭', 1.0): 2, ('except', 1.0): 2, (\"masha'allah\", 1.0): 1, ('french', 1.0): 5, ('wwat', 1.0): 2, ('franc', 1.0): 5, ('yaaay', 1.0): 3, ('beirut', 1.0): 2, ('coffe', 1.0): 11, ('panda', 1.0): 6, ('eonni', 1.0): 2, ('favourit', 1.0): 13, ('soda', 1.0): 1, ('fuller', 1.0): 1, ('shit', 1.0): 13, ('healthi', 1.0): 2, ('💓', 1.0): 2, ('rettweet', 1.0): 3, ('mvg', 1.0): 1, ('valuabl', 1.0): 1, ('madrid', 1.0): 3, ('sore', 1.0): 6, ('bergerac', 1.0): 1, ('u21', 1.0): 1, ('individu', 1.0): 2, ('adam', 1.0): 1, (\"beach'\", 1.0): 1, ('suicid', 1.0): 1, ('squad', 1.0): 1, ('fond', 1.0): 1, ('christoph', 1.0): 2, ('cocki', 1.0): 1, ('prove', 1.0): 3, (\"attitude'\", 1.0): 1, ('improv', 1.0): 3, ('suggest', 1.0): 6, ('date', 1.0): 12, ('inde', 1.0): 10, ('intellig', 1.0): 3, ('strong', 1.0): 7, ('cs', 1.0): 2, ('certain', 1.0): 2, ('exam', 1.0): 5, ('forgot', 1.0): 3, ('home-bas', 1.0): 1, ('knee', 1.0): 4, ('sale', 1.0): 3, ('fleur', 1.0): 1, ('dress', 1.0): 10, ('readystock_hijabmart', 1.0): 1, ('idr', 1.0): 2, ('325.000', 1.0): 1, ('200.000', 1.0): 1, ('tompolo', 1.0): 1, ('aim', 1.0): 1, ('cannot', 1.0): 4, ('buyer', 1.0): 3, ('disappoint', 1.0): 1, ('paper', 1.0): 4, ('slack', 1.0): 1, ('crack', 1.0): 1, ('particularli', 1.0): 2, ('strike', 1.0): 1, ('31', 1.0): 1, ('mam', 1.0): 2, ('feytyaz', 1.0): 1, ('instant', 1.0): 1, ('stiffen', 1.0): 1, ('ricky_feb', 1.0): 1, ('grindea', 1.0): 1, ('courier', 1.0): 1, ('crypt', 1.0): 1, ('arma', 1.0): 1, ('record', 1.0): 5, ('gosh', 1.0): 2, ('limbo', 1.0): 1, ('orchard', 1.0): 1, ('art', 1.0): 10, ('super', 1.0): 15, ('karachi', 1.0): 2, ('ka', 1.0): 4, ('venic', 1.0): 1, ('sever', 1.0): 3, ('part', 1.0): 15, ('wit', 1.0): 2, ('accumul', 1.0): 1, ('maroon', 1.0): 1, ('cocktail', 1.0): 4, ('0-100', 1.0): 1, ('quick', 1.0): 7, ('1100d', 1.0): 1, ('auto-focu', 1.0): 1, ('manual', 1.0): 2, ('vein', 1.0): 1, ('crackl', 1.0): 1, ('glaze', 1.0): 1, ('layout', 1.0): 3, ('bomb', 1.0): 4, ('social', 1.0): 4, ('websit', 1.0): 8, ('pake', 1.0): 1, ('joim', 1.0): 1, ('feed', 1.0): 4, ('troop', 1.0): 1, ('mail', 1.0): 3, ('ladolcevitainluxembourg@hotmail.com', 1.0): 1, ('prrequest', 1.0): 1, ('journorequest', 1.0): 1, ('the_madstork', 1.0): 1, ('shaun', 1.0): 1, ('bot', 1.0): 4, ('chloe', 1.0): 2, ('actress', 1.0): 3, ('away', 1.0): 13, ('wick', 1.0): 9, ('hola', 1.0): 1, ('juan', 1.0): 1, ('houston', 1.0): 1, ('tx', 1.0): 2, ('jenni', 1.0): 1, (\"year'\", 1.0): 2, ('stumbl', 1.0): 1, ('upon', 1.0): 1, ('prob.nic', 1.0): 1, ('choker', 1.0): 1, ('btw', 1.0): 12, ('seouljin', 1.0): 1, ('photoset', 1.0): 3, ('sadomasochistsparadis', 1.0): 1, ('wynter', 1.0): 1, ('bottom', 1.0): 3, ('outtak', 1.0): 1, ('sadomasochist', 1.0): 1, ('paradis', 1.0): 1, ('ty', 1.0): 8, ('bbi', 1.0): 3, ('clip', 1.0): 1, ('lose', 1.0): 6, ('cypher', 1.0): 1, ('amen', 1.0): 2, ('x32', 1.0): 1, ('plant', 1.0): 4, ('allow', 1.0): 4, ('corner', 1.0): 3, ('addict', 1.0): 4, ('gurl', 1.0): 1, ('suck', 1.0): 9, ('special', 1.0): 8, ('owe', 1.0): 1, ('daniel', 1.0): 2, ('ape', 1.0): 1, ('saar', 1.0): 1, ('ahead', 1.0): 4, ('vers', 1.0): 1, ('butterfli', 1.0): 1, ('bonu', 1.0): 2, ('fill', 1.0): 5, ('tear', 1.0): 1, ('laughter', 1.0): 2, ('5so', 1.0): 6, ('yummmyyi', 1.0): 1, ('eat', 1.0): 6, ('dosa', 1.0): 1, ('easier', 1.0): 2, ('unless', 1.0): 3, ('achi', 1.0): 2, ('youuu', 1.0): 2, ('bawi', 1.0): 1, ('ako', 1.0): 1, ('queenesth', 1.0): 1, ('sharp', 1.0): 2, ('yess', 1.0): 1, ('poldi', 1.0): 1, ('cimbom', 1.0): 1, ('buddi', 1.0): 7, ('bruhhh', 1.0): 1, ('daddi', 1.0): 2, ('”', 1.0): 5, ('knowledg', 1.0): 2, ('attent', 1.0): 4, ('1tb', 1.0): 1, ('bank', 1.0): 1, ('credit', 1.0): 4, ('depart', 1.0): 2, ('anz', 1.0): 1, ('extrem', 1.0): 3, ('offshor', 1.0): 1, ('absolut', 1.0): 9, ('classic', 1.0): 3, ('gottolovebank', 1.0): 1, ('yup', 1.0): 6, ('in-shaa-allah', 1.0): 1, ('dua', 1.0): 1, ('thru', 1.0): 2, ('aameen', 1.0): 2, ('4/5', 1.0): 1, ('coca', 1.0): 1, ('cola', 1.0): 1, ('fanta', 1.0): 1, ('pepsi', 1.0): 1, ('sprite', 1.0): 1, ('all', 1.0): 1, ('sweeeti', 1.0): 1, (';-)', 1.0): 3, ('welcometweet', 1.0): 2, ('psygustokita', 1.0): 4, ('setup', 1.0): 1, ('wet', 1.0): 3, ('feet', 1.0): 3, ('carpet', 1.0): 1, ('judgment', 1.0): 1, ('hypocrit', 1.0): 1, ('narcissist', 1.0): 1, ('jumpsuit', 1.0): 1, ('bt', 1.0): 2, ('denim', 1.0): 1, ('verg', 1.0): 1, ('owl', 1.0): 1, ('constant', 1.0): 1, ('run', 1.0): 12, ('sia', 1.0): 1, ('count', 1.0): 7, ('brilliant', 1.0): 9, ('teacher', 1.0): 1, ('compar', 1.0): 2, ('religion', 1.0): 1, ('rant', 1.0): 1, ('student', 1.0): 6, ('bencher', 1.0): 1, ('1/5', 1.0): 1, ('porsch', 1.0): 1, ('paddock', 1.0): 1, ('budapestgp', 1.0): 1, ('johnyherbert', 1.0): 1, ('roll', 1.0): 5, ('porschesupercup', 1.0): 1, ('koyal', 1.0): 1, ('melodi', 1.0): 1, ('unexpect', 1.0): 4, ('creat', 1.0): 8, ('memori', 1.0): 3, ('35', 1.0): 1, ('ep', 1.0): 3, ('catch', 1.0): 10, ('wirh', 1.0): 1, ('arc', 1.0): 1, ('x31', 1.0): 1, ('wolv', 1.0): 2, ('desir', 1.0): 1, ('ameen', 1.0): 1, ('kca', 1.0): 1, ('votejkt', 1.0): 1, ('48id', 1.0): 1, ('helpinggroupdm', 1.0): 1, ('quot', 1.0): 6, ('weird', 1.0): 5, ('dp', 1.0): 1, ('wife', 1.0): 5, ('poor', 1.0): 4, ('chick', 1.0): 1, ('guid', 1.0): 3, ('zonzofox', 1.0): 3, ('bhaiya', 1.0): 1, ('brother', 1.0): 4, ('lucki', 1.0): 10, ('patti', 1.0): 1, ('elabor', 1.0): 1, ('kuch', 1.0): 1, ('rate', 1.0): 1, ('merdeka', 1.0): 1, ('palac', 1.0): 2, ('hotel', 1.0): 5, ('plusmil', 1.0): 1, ('servic', 1.0): 7, ('hahahaa', 1.0): 1, ('mean', 1.0): 25, ('nex', 1.0): 2, ('safe', 1.0): 5, ('gwd', 1.0): 1, ('she', 1.0): 2, ('okok', 1.0): 1, ('33', 1.0): 4, ('idiot', 1.0): 1, ('chaerin', 1.0): 1, ('unni', 1.0): 1, ('viabl', 1.0): 1, ('altern', 1.0): 3, ('nowaday', 1.0): 2, ('ip', 1.0): 1, ('tombow', 1.0): 1, ('abt', 1.0): 2, ('friyay', 1.0): 2, ('smug', 1.0): 1, ('marrickvil', 1.0): 1, ('public', 1.0): 3, ('ten', 1.0): 1, ('ago', 1.0): 8, ('eighteen', 1.0): 1, ('auvssscr', 1.0): 1, ('ncaaseason', 1.0): 1, ('slow', 1.0): 2, ('popsicl', 1.0): 1, ('soft', 1.0): 2, ('melt', 1.0): 1, ('mouth', 1.0): 2, ('thankyouuu', 1.0): 1, ('dianna', 1.0): 1, ('ngga', 1.0): 1, ('usah', 1.0): 1, ('dipikirin', 1.0): 1, ('elah', 1.0): 1, ('easili', 1.0): 1, (\"who'\", 1.0): 9, ('entp', 1.0): 1, ('killin', 1.0): 1, ('meme', 1.0): 1, ('worthi', 1.0): 1, ('shot', 1.0): 6, ('emon', 1.0): 1, ('decent', 1.0): 2, ('outdoor', 1.0): 1, ('rave', 1.0): 1, ('dv', 1.0): 1, ('aku', 1.0): 1, ('bakal', 1.0): 1, ('liat', 1.0): 1, ('kak', 1.0): 2, ('merri', 1.0): 1, ('tv', 1.0): 5, ('outfit', 1.0): 3, ('---&gt;', 1.0): 1, ('fashionfriday', 1.0): 1, ('angle.nelson', 1.0): 1, ('cheap', 1.0): 1, ('mymonsoonstori', 1.0): 2, ('tree', 1.0): 2, ('lotion', 1.0): 1, ('moistur', 1.0): 1, ('monsoon', 1.0): 1, ('whoop', 1.0): 6, ('romant', 1.0): 2, ('valencia', 1.0): 1, ('daaru', 1.0): 1, ('parti', 1.0): 12, ('chaddi', 1.0): 1, ('wonderful.great', 1.0): 1, ('trim', 1.0): 1, ('pube', 1.0): 1, ('es', 1.0): 2, ('mi', 1.0): 5, ('tio', 1.0): 1, ('sinaloa', 1.0): 1, ('arr', 1.0): 1, ('stylish', 1.0): 1, ('trendi', 1.0): 1, ('kim', 1.0): 5, ('fabfriday', 1.0): 2, ('facetim', 1.0): 4, ('calum', 1.0): 3, ('constantli', 1.0): 1, ('announc', 1.0): 1, ('filbarbarian', 1.0): 1, ('beer', 1.0): 3, ('arm', 1.0): 3, ('testicl', 1.0): 1, ('light', 1.0): 13, ('katerina', 1.0): 1, ('maniataki', 1.0): 1, ('ahh', 1.0): 5, ('alright', 1.0): 6, ('worthwhil', 1.0): 3, ('judg', 1.0): 2, ('tech', 1.0): 2, ('window', 1.0): 7, ('stupid', 1.0): 8, ('plugin', 1.0): 1, ('bass', 1.0): 1, ('slap', 1.0): 1, ('6pm', 1.0): 1, ('door', 1.0): 3, ('vip', 1.0): 1, ('gener', 1.0): 4, ('seat', 1.0): 2, ('earli', 1.0): 9, ('london', 1.0): 9, ('toptravelcentar', 1.0): 1, ('ttctop', 1.0): 1, ('lux', 1.0): 1, ('luxurytravel', 1.0): 1, ('beograd', 1.0): 1, ('srbija', 1.0): 1, ('putovanja', 1.0): 1, ('wendi', 1.0): 2, ('provid', 1.0): 4, ('drainag', 1.0): 1, ('homebound', 1.0): 1, ('hahahay', 1.0): 1, ('yeeeah', 1.0): 1, ('moar', 1.0): 2, ('kitteh', 1.0): 1, ('incom', 1.0): 1, ('tower', 1.0): 2, ('yippee', 1.0): 1, ('scrummi', 1.0): 1, ('bio', 1.0): 5, ('mcpe', 1.0): 1, ('-&gt;', 1.0): 1, ('vainglori', 1.0): 1, ('driver', 1.0): 1, ('6:01', 1.0): 1, ('lilydal', 1.0): 1, ('fss', 1.0): 1, ('rais', 1.0): 3, ('magicalmysterytour', 1.0): 1, ('chek', 1.0): 2, ('rule', 1.0): 2, ('weebli', 1.0): 1, ('donetsk', 1.0): 1, ('earth', 1.0): 7, ('personalis', 1.0): 1, ('wrap', 1.0): 2, ('stationeri', 1.0): 1, ('adrian', 1.0): 1, ('parcel', 1.0): 2, ('tuesday', 1.0): 7, ('pri', 1.0): 3, ('80', 1.0): 3, ('wz', 1.0): 1, ('pattern', 1.0): 1, ('cut', 1.0): 3, ('buttonhol', 1.0): 1, ('4mi', 1.0): 1, ('famou', 1.0): 1, ('client', 1.0): 1, ('p', 1.0): 3, ('aliv', 1.0): 2, ('trial', 1.0): 1, ('spm', 1.0): 1, ('dinooo', 1.0): 1, ('cardio', 1.0): 1, ('steak', 1.0): 1, ('cue', 1.0): 1, ('laptop', 1.0): 1, ('guinea', 1.0): 1, ('pig', 1.0): 1, ('salamat', 1.0): 1, ('sa', 1.0): 6, ('mga', 1.0): 1, ('nag.greet', 1.0): 1, ('guis', 1.0): 1, ('godbless', 1.0): 2, ('crush', 1.0): 3, ('appl', 1.0): 4, ('deserv', 1.0): 11, ('charl', 1.0): 1, ('workhard', 1.0): 1, ('model', 1.0): 7, ('forrit', 1.0): 1, ('bread', 1.0): 2, ('bacon', 1.0): 2, ('butter', 1.0): 2, ('afang', 1.0): 2, ('soup', 1.0): 2, ('semo', 1.0): 2, ('brb', 1.0): 1, ('forc', 1.0): 2, ('doesnt', 1.0): 5, ('tato', 1.0): 1, ('bulat', 1.0): 1, ('concern', 1.0): 1, ('snake', 1.0): 1, ('perform', 1.0): 3, ('con', 1.0): 1, ('todayyy', 1.0): 1, ('max', 1.0): 2, ('gaza', 1.0): 1, ('bbb', 1.0): 1, ('pc', 1.0): 3, ('22', 1.0): 2, ('legal', 1.0): 1, ('ditch', 1.0): 2, ('tori', 1.0): 1, ('bajrangibhaijaanhighestweek', 1.0): 6, (\"s'okay\", 1.0): 1, ('andi', 1.0): 2, ('you-and', 1.0): 1, ('return', 1.0): 3, ('tuitutil', 1.0): 1, ('bud', 1.0): 2, ('learn', 1.0): 8, ('takeaway', 1.0): 1, ('instead', 1.0): 7, ('1hr', 1.0): 1, ('genial', 1.0): 1, ('competit', 1.0): 1, ('yosh', 1.0): 1, ('procrastin', 1.0): 1, ('plu', 1.0): 4, ('kfc', 1.0): 2, ('itun', 1.0): 1, ('dedicatedfan', 1.0): 1, ('💜', 1.0): 7, ('daft', 1.0): 1, ('teeth', 1.0): 1, ('troubl', 1.0): 1, ('huxley', 1.0): 1, ('basket', 1.0): 2, ('ben', 1.0): 2, ('sent', 1.0): 8, ('gamer', 1.0): 3, ('activ', 1.0): 5, ('120', 1.0): 2, ('distanc', 1.0): 2, ('suitabl', 1.0): 1, ('stockholm', 1.0): 1, ('zack', 1.0): 1, ('destroy', 1.0): 1, ('heel', 1.0): 2, ('claw', 1.0): 1, ('q', 1.0): 2, ('blond', 1.0): 2, ('box', 1.0): 3, ('cheerio', 1.0): 1, ('seed', 1.0): 4, ('cutest', 1.0): 2, ('ffback', 1.0): 2, ('spotifi', 1.0): 3, (\"we'v\", 1.0): 7, ('vc', 1.0): 1, ('tgp', 1.0): 1, ('race', 1.0): 5, ('averag', 1.0): 2, (\"joe'\", 1.0): 1, ('bluejay', 1.0): 1, ('vinylbear', 1.0): 1, ('pal', 1.0): 1, ('furbabi', 1.0): 1, ('luff', 1.0): 1, ('mega', 1.0): 4, ('retail', 1.0): 4, ('boot', 1.0): 2, ('whsmith', 1.0): 1, ('ps3', 1.0): 1, ('shannon', 1.0): 1, ('na', 1.0): 9, ('redecor', 1.0): 1, ('bob', 1.0): 3, ('elli', 1.0): 4, ('mairi', 1.0): 1, ('workout', 1.0): 6, ('impair', 1.0): 1, ('uggghhh', 1.0): 1, ('dam', 1.0): 2, ('dun', 1.0): 2, ('eczema', 1.0): 1, ('suffer', 1.0): 4, ('ndee', 1.0): 1, ('pleasur', 1.0): 14, ('publiliu', 1.0): 1, ('syru', 1.0): 1, ('fear', 1.0): 1, ('death', 1.0): 3, ('dread', 1.0): 1, ('fell', 1.0): 3, ('fuk', 1.0): 1, ('unblock', 1.0): 1, ('tweak', 1.0): 2, ('php', 1.0): 1, ('fall', 1.0): 10, ('oomf', 1.0): 1, ('pippa', 1.0): 1, ('hschool', 1.0): 1, ('bu', 1.0): 3, ('cardi', 1.0): 1, ('everyday', 1.0): 3, ('everytim', 1.0): 3, ('hk', 1.0): 1, (\"why'd\", 1.0): 1, ('acorn', 1.0): 1, ('origin', 1.0): 7, ('c64', 1.0): 1, ('cpu', 1.0): 1, ('consider', 1.0): 1, ('advanc', 1.0): 1, ('onair', 1.0): 1, ('bay', 1.0): 1, ('hold', 1.0): 6, ('river', 1.0): 3, ('0878 0388', 1.0): 1, ('1033', 1.0): 1, ('0272 3306', 1.0): 1, ('70', 1.0): 5, ('rescu', 1.0): 1, ('mutt', 1.0): 1, ('confirm', 1.0): 3, ('deliveri', 1.0): 3, ('switch', 1.0): 2, ('lap', 1.0): 1, ('optim', 1.0): 1, ('lu', 1.0): 1, (':|', 1.0): 1, ('tweetofthedecad', 1.0): 1, ('class', 1.0): 5, ('happiest', 1.0): 2, ('bbmme', 1.0): 3, ('pin', 1.0): 4, ('7df9e60a', 1.0): 1, ('bbm', 1.0): 2, ('bbmpin', 1.0): 2, ('addmeonbbm', 1.0): 1, ('addm', 1.0): 1, (\"today'\", 1.0): 3, ('menu', 1.0): 1, ('marri', 1.0): 3, ('glenn', 1.0): 1, ('what', 1.0): 4, ('height', 1.0): 1, (\"sculptor'\", 1.0): 1, ('ti5', 1.0): 1, ('dota', 1.0): 3, ('nudg', 1.0): 1, ('spot', 1.0): 5, ('tasti', 1.0): 1, ('hilli', 1.0): 1, ('cycl', 1.0): 6, ('england', 1.0): 4, ('scotlandismass', 1.0): 1, ('gen', 1.0): 2, ('vikk', 1.0): 1, ('fna', 1.0): 1, ('mombasa', 1.0): 1, ('tukutanemombasa', 1.0): 1, ('100reasonstovisitmombasa', 1.0): 1, ('karibumombasa', 1.0): 1, ('hanbin', 1.0): 1, ('certainli', 1.0): 4, ('goosnight', 1.0): 1, ('kindli', 1.0): 4, ('familiar', 1.0): 2, ('jealou', 1.0): 4, ('tent', 1.0): 2, ('yea', 1.0): 2, ('cozi', 1.0): 1, ('phenomen', 1.0): 2, ('collab', 1.0): 2, ('gave', 1.0): 4, ('birth', 1.0): 1, ('behav', 1.0): 2, ('monster', 1.0): 1, ('spree', 1.0): 4, ('000', 1.0): 1, ('tank', 1.0): 6, ('outstand', 1.0): 1, ('donat', 1.0): 3, ('h', 1.0): 4, ('contestkiduniya', 1.0): 2, ('mfundo', 1.0): 1, ('och', 1.0): 1, ('hun', 1.0): 4, ('inner', 1.0): 2, ('nerd', 1.0): 2, ('tame', 1.0): 2, ('insidi', 1.0): 1, ('logic', 1.0): 1, ('math', 1.0): 1, ('channel', 1.0): 5, ('continu', 1.0): 4, ('doubt', 1.0): 3, ('300', 1.0): 2, ('sub', 1.0): 2, ('200', 1.0): 3, ('forgiven', 1.0): 1, ('manner', 1.0): 1, ('yhooo', 1.0): 1, ('ngi', 1.0): 1, ('mood', 1.0): 7, ('push', 1.0): 1, ('limit', 1.0): 6, ('obakeng', 1.0): 1, ('goat', 1.0): 1, ('alhamdullilah', 1.0): 1, ('pebbl', 1.0): 1, ('engross', 1.0): 1, ('bing', 1.0): 2, ('scream', 1.0): 2, ('whole', 1.0): 7, ('wide', 1.0): 2, ('🌎', 1.0): 2, ('😧', 1.0): 1, ('wat', 1.0): 2, ('muahhh', 1.0): 1, ('pausetim', 1.0): 1, ('drift', 1.0): 1, ('loos', 1.0): 3, ('campaign', 1.0): 4, ('kickstart', 1.0): 1, ('articl', 1.0): 9, ('jenna', 1.0): 1, ('bellybutton', 1.0): 5, ('inni', 1.0): 4, ('outi', 1.0): 4, ('havent', 1.0): 4, ('delish', 1.0): 1, ('joselito', 1.0): 1, ('freya', 1.0): 1, ('nth', 1.0): 1, ('latepost', 1.0): 1, ('lupet', 1.0): 1, ('mo', 1.0): 2, ('eric', 1.0): 3, ('askaman', 1.0): 1, ('150', 1.0): 1, ('0345', 1.0): 2, ('454', 1.0): 1, ('111', 1.0): 1, ('webz', 1.0): 1, ('oop', 1.0): 5, (\"they'll\", 1.0): 6, ('realis', 1.0): 2, ('anymor', 1.0): 3, ('carmel', 1.0): 1, ('decis', 1.0): 5, ('matt', 1.0): 6, ('@commoncultur', 1.0): 1, ('@connorfranta', 1.0): 1, ('honestli', 1.0): 3, ('explain', 1.0): 3, ('relationship', 1.0): 4, ('pick', 1.0): 15, ('tessnzach', 1.0): 1, ('paperboy', 1.0): 1, ('honest', 1.0): 3, ('reassur', 1.0): 1, ('guysss', 1.0): 3, ('mubank', 1.0): 2, (\"dongwoo'\", 1.0): 1, ('bright', 1.0): 2, ('tommorow', 1.0): 3, ('newyork', 1.0): 1, ('lolll', 1.0): 1, ('twinx', 1.0): 1, ('16', 1.0): 2, ('path', 1.0): 1, ('firmansyahbl', 1.0): 1, ('procedur', 1.0): 1, ('grim', 1.0): 1, ('fandango', 1.0): 1, ('ordinari', 1.0): 1, ('extraordinari', 1.0): 1, ('bo', 1.0): 2, ('birmingham', 1.0): 1, ('oracl', 1.0): 1, ('samosa', 1.0): 1, ('firebal', 1.0): 1, ('shoe', 1.0): 4, ('serv', 1.0): 1, ('sushi', 1.0): 2, ('shoeshi', 1.0): 1, ('�', 1.0): 2, ('lymond', 1.0): 1, ('philippa', 1.0): 2, ('novel', 1.0): 1, ('tara', 1.0): 3, ('. . .', 1.0): 2, ('aur', 1.0): 2, ('han', 1.0): 1, ('imran', 1.0): 3, ('khan', 1.0): 7, ('63', 1.0): 1, ('agaaain', 1.0): 1, ('doli', 1.0): 1, ('siregar', 1.0): 1, ('ninh', 1.0): 1, ('size', 1.0): 5, ('geekiest', 1.0): 1, ('geek', 1.0): 2, ('wallet', 1.0): 3, ('request', 1.0): 4, ('media', 1.0): 4, ('ralli', 1.0): 1, ('rotat', 1.0): 3, ('direct', 1.0): 3, ('eek', 1.0): 1, ('red', 1.0): 6, ('beij', 1.0): 1, ('meni', 1.0): 1, ('tebrik', 1.0): 1, ('etdi', 1.0): 1, ('700', 1.0): 1, ('💗', 1.0): 2, ('rod', 1.0): 1, ('embrac', 1.0): 1, ('actor', 1.0): 1, ('aplomb', 1.0): 1, ('foreveralon', 1.0): 2, ('mysumm', 1.0): 1, ('01482', 1.0): 1, ('333505', 1.0): 1, ('hahahaha', 1.0): 2, ('wear', 1.0): 6, ('uniform', 1.0): 1, ('evil', 1.0): 1, ('owww', 1.0): 1, ('choo', 1.0): 1, ('chweet', 1.0): 1, ('shorthair', 1.0): 1, ('oscar', 1.0): 1, ('realiz', 1.0): 7, ('harmoni', 1.0): 1, ('deneriveri', 1.0): 1, ('506', 1.0): 1, ('kiksext', 1.0): 5, ('kikkomansabor', 1.0): 2, ('killer', 1.0): 1, ('henessydiari', 1.0): 1, ('journey', 1.0): 4, ('band', 1.0): 4, ('plz', 1.0): 5, ('convo', 1.0): 3, ('11', 1.0): 5, ('vault', 1.0): 1, ('expand', 1.0): 2, ('vinni', 1.0): 1, ('money', 1.0): 9, ('hahahahaha', 1.0): 2, ('50cent', 1.0): 1, ('repay', 1.0): 1, ('debt', 1.0): 2, ('evet', 1.0): 1, ('wifi', 1.0): 3, ('lifestyl', 1.0): 1, ('qatarday', 1.0): 1, ('. ..', 1.0): 3, ('🌞', 1.0): 3, ('girli', 1.0): 1, ('india', 1.0): 4, ('innov', 1.0): 1, ('volunt', 1.0): 2, ('saran', 1.0): 1, ('drama', 1.0): 3, ('genr', 1.0): 1, ('romanc', 1.0): 1, ('comedi', 1.0): 1, ('leannerin', 1.0): 1, ('19', 1.0): 7, ('porno', 1.0): 1, ('l4l', 1.0): 3, ('weloveyounamjoon', 1.0): 1, ('homey', 1.0): 1, ('kenya', 1.0): 1, ('roller', 1.0): 2, ('coaster', 1.0): 1, ('aspect', 1.0): 1, ('najam', 1.0): 1, ('confess', 1.0): 2, ('pricelessantiqu', 1.0): 1, ('takesonetoknowon', 1.0): 1, ('extra', 1.0): 5, ('ucount', 1.0): 1, ('ji', 1.0): 3, ('turkish', 1.0): 1, ('knew', 1.0): 8, ('crap', 1.0): 1, ('burn', 1.0): 3, ('80x', 1.0): 1, ('airlin', 1.0): 1, ('sexi', 1.0): 10, ('yello', 1.0): 1, ('gail', 1.0): 1, ('yael', 1.0): 1, ('lesson', 1.0): 4, ('en', 1.0): 1, ('mano', 1.0): 1, ('hand', 1.0): 4, ('manag', 1.0): 6, ('prettiest', 1.0): 1, ('reader', 1.0): 4, ('dnt', 1.0): 1, ('ideal', 1.0): 2, ('weekli', 1.0): 2, ('idol', 1.0): 3, ('pose', 1.0): 2, ('shortlist', 1.0): 1, ('dominion', 1.0): 2, ('picnic', 1.0): 2, ('tmrw', 1.0): 3, ('nobodi', 1.0): 2, ('jummamubarak', 1.0): 1, ('shower', 1.0): 3, ('shalwarkameez', 1.0): 1, ('itter', 1.0): 1, ('offer', 1.0): 8, ('jummapray', 1.0): 1, ('af', 1.0): 8, ('display', 1.0): 1, ('enabl', 1.0): 1, ('compani', 1.0): 4, ('peep', 1.0): 4, ('tweep', 1.0): 2, ('folow', 1.0): 1, ('2k', 1.0): 1, ('ohhh', 1.0): 4, ('teaser', 1.0): 2, ('airec', 1.0): 1, ('009', 1.0): 1, ('acid', 1.0): 1, ('mous', 1.0): 2, ('31st', 1.0): 2, ('includ', 1.0): 5, ('robin', 1.0): 1, ('rough', 1.0): 4, ('control', 1.0): 1, ('remix', 1.0): 5, ('fave', 1.0): 3, ('toss', 1.0): 1, ('ladi', 1.0): 8, ('🐑', 1.0): 1, ('librari', 1.0): 3, ('mr2', 1.0): 1, ('climb', 1.0): 1, ('cuddl', 1.0): 1, ('jilla', 1.0): 1, ('headlin', 1.0): 1, ('2017', 1.0): 1, ('jumma', 1.0): 5, ('mubarik', 1.0): 2, ('spent', 1.0): 2, ('congratz', 1.0): 1, ('contribut', 1.0): 3, ('2.0', 1.0): 2, ('yuppiiee', 1.0): 1, ('alienthought', 1.0): 1, ('happyalien', 1.0): 1, ('crowd', 1.0): 2, ('loudest', 1.0): 2, ('gari', 1.0): 1, ('particular', 1.0): 1, ('attract', 1.0): 1, ('supprt', 1.0): 1, ('savag', 1.0): 1, ('cleans', 1.0): 1, ('scam', 1.0): 1, ('ridden', 1.0): 1, ('vyapam', 1.0): 2, ('renam', 1.0): 1, ('wave', 1.0): 2, ('couch', 1.0): 1, ('dodg', 1.0): 1, ('explan', 1.0): 2, ('bag', 1.0): 4, ('sanza', 1.0): 1, ('yaa', 1.0): 3, ('slr', 1.0): 1, ('som', 1.0): 1, ('honour', 1.0): 1, ('heheh', 1.0): 1, ('view', 1.0): 16, ('explor', 1.0): 2, ('wayanadan', 1.0): 1, ('forest', 1.0): 1, ('wayanad', 1.0): 1, ('srijith', 1.0): 1, ('whisper', 1.0): 1, ('lie', 1.0): 4, ('pokemon', 1.0): 1, ('dazzl', 1.0): 1, ('urself', 1.0): 2, ('doubl', 1.0): 2, ('flare', 1.0): 1, ('black', 1.0): 4, ('9', 1.0): 3, ('51', 1.0): 1, ('brows', 1.0): 1, ('bore', 1.0): 9, ('femal', 1.0): 2, ('tour', 1.0): 8, ('delv', 1.0): 2, ('muchhh', 1.0): 1, ('tmr', 1.0): 1, ('breakfast', 1.0): 4, ('gl', 1.0): 1, (\"tonight'\", 1.0): 2, ('):', 1.0): 7, ('litey', 1.0): 1, ('manuella', 1.0): 1, ('abhi', 1.0): 2, ('tak', 1.0): 2, ('nhi', 1.0): 2, ('dekhi', 1.0): 1, ('promo', 1.0): 3, ('se', 1.0): 4, ('xpax', 1.0): 1, ('lisa', 1.0): 2, ('aboard', 1.0): 3, ('institut', 1.0): 1, ('nc', 1.0): 2, ('chees', 1.0): 4, ('overload', 1.0): 1, ('pizza', 1.0): 1, ('•', 1.0): 3, ('mcfloat', 1.0): 1, ('fudg', 1.0): 3, ('sanda', 1.0): 1, ('munchkin', 1.0): 1, (\"d'd\", 1.0): 1, ('granni', 1.0): 1, ('baller', 1.0): 1, ('lil', 1.0): 4, ('chain', 1.0): 1, ('everybodi', 1.0): 1, ('ought', 1.0): 1, ('jay', 1.0): 3, ('events@breastcancernow.org', 1.0): 1, ('79x', 1.0): 1, ('champion', 1.0): 1, ('letter', 1.0): 2, ('uniqu', 1.0): 2, ('affaraid', 1.0): 1, ('dearslim', 1.0): 2, ('role', 1.0): 2, ('billi', 1.0): 2, ('lab', 1.0): 1, ('ovh', 1.0): 2, ('maxi', 1.0): 2, ('bunch', 1.0): 1, ('acc', 1.0): 2, ('sprit', 1.0): 1, ('you', 1.0): 1, ('til', 1.0): 2, ('hammi', 1.0): 1, ('freedom', 1.0): 2, ('pistol', 1.0): 1, ('unlock', 1.0): 1, ('bemeapp', 1.0): 1, ('thumb', 1.0): 1, ('beme', 1.0): 1, ('bemecod', 1.0): 1, ('proudtobem', 1.0): 1, ('round', 1.0): 2, ('calm', 1.0): 5, ('kepo', 1.0): 1, ('luckili', 1.0): 1, ('clearli', 1.0): 2, ('دعمم', 1.0): 1, ('للعودة', 1.0): 1, ('للحياة', 1.0): 1, ('heiyo', 1.0): 2, ('dudafti', 1.0): 1, ('breaktym', 1.0): 1, ('fatal', 1.0): 1, ('danger', 1.0): 1, ('term', 1.0): 2, ('health', 1.0): 2, ('outrag', 1.0): 1, ('645k', 1.0): 1, ('muna', 1.0): 1, ('magstart', 1.0): 1, ('salut', 1.0): 3, ('→', 1.0): 1, ('thq', 1.0): 1, ('contin', 1.0): 1, ('thalaivar', 1.0): 1, ('£', 1.0): 7, ('heiya', 1.0): 2, ('grab', 1.0): 3, ('30.000', 1.0): 2, ('av', 1.0): 1, ('gd', 1.0): 3, ('wknd', 1.0): 1, ('ear', 1.0): 12, (\"y'day\", 1.0): 1, ('hxh', 1.0): 1, ('badass', 1.0): 2, ('killua', 1.0): 1, ('scene', 1.0): 2, ('78x', 1.0): 1, ('unappreci', 1.0): 1, ('graciou', 1.0): 1, ('nailedit', 1.0): 1, ('ourdisneyinfin', 1.0): 1, ('mari', 1.0): 3, ('jillmil', 1.0): 1, ('webcam', 1.0): 2, ('elfindelmundo', 1.0): 1, ('mainli', 1.0): 1, ('favour', 1.0): 1, ('dancetast', 1.0): 1, ('satyajit', 1.0): 1, (\"ray'\", 1.0): 1, ('porosh', 1.0): 1, ('pathor', 1.0): 1, ('situat', 1.0): 3, ('goldbug', 1.0): 1, ('wine', 1.0): 3, ('bottl', 1.0): 2, ('spill', 1.0): 2, ('jazmin', 1.0): 3, ('bonilla', 1.0): 3, ('15000', 1.0): 1, ('star', 1.0): 9, ('hollywood', 1.0): 3, ('rofl', 1.0): 3, ('shade', 1.0): 1, ('grey', 1.0): 1, ('netsec', 1.0): 1, ('kev', 1.0): 1, ('sister', 1.0): 6, ('told', 1.0): 6, ('unlist', 1.0): 1, ('hickey', 1.0): 1, ('dad', 1.0): 5, ('hock', 1.0): 1, ('mamma', 1.0): 1, ('human', 1.0): 5, ('be', 1.0): 1, ('mere', 1.0): 1, ('holist', 1.0): 1, ('cosmovis', 1.0): 1, ('narrow-mind', 1.0): 1, ('charg', 1.0): 3, ('cess', 1.0): 1, ('alix', 1.0): 1, ('quan', 1.0): 1, ('tip', 1.0): 5, ('naaahhh', 1.0): 1, ('duh', 1.0): 2, ('emesh', 1.0): 1, ('hilari', 1.0): 4, ('kath', 1.0): 3, ('kia', 1.0): 1, ('@vauk', 1.0): 1, ('tango', 1.0): 1, ('tracerequest', 1.0): 2, ('dassi', 1.0): 1, ('fwm', 1.0): 1, ('selamat', 1.0): 1, ('nichola', 1.0): 2, ('malta', 1.0): 1, ('gto', 1.0): 1, ('tomorrowland', 1.0): 1, ('incal', 1.0): 1, ('shob', 1.0): 1, ('incomplet', 1.0): 1, ('barkada', 1.0): 1, ('silverston', 1.0): 1, ('pull', 1.0): 1, ('bookstor', 1.0): 1, ('ganna', 1.0): 1, ('hillari', 1.0): 1, ('clinton', 1.0): 1, ('court', 1.0): 2, ('notic', 1.0): 11, ('slice', 1.0): 2, ('life-so', 1.0): 1, ('hidden', 1.0): 1, ('untap', 1.0): 1, ('mca', 1.0): 2, ('gettin', 1.0): 1, ('hella', 1.0): 1, ('wana', 1.0): 1, ('bandz', 1.0): 1, ('hell', 1.0): 4, ('donington', 1.0): 1, ('park', 1.0): 8, ('24/25', 1.0): 1, ('x30', 1.0): 1, ('merci', 1.0): 1, ('bien', 1.0): 1, ('pitbul', 1.0): 1, ('777x', 1.0): 1, ('fri', 1.0): 3, ('annyeong', 1.0): 1, ('oppa', 1.0): 7, ('indonesian', 1.0): 1, ('elf', 1.0): 3, ('flight', 1.0): 2, ('bf', 1.0): 2, ('jennyjean', 1.0): 1, ('kikchat', 1.0): 1, ('sabadodeganarseguidor', 1.0): 1, ('sexysasunday', 1.0): 2, ('marseil', 1.0): 1, ('ganda', 1.0): 1, ('fnaf', 1.0): 5, ('steam', 1.0): 1, ('assur', 1.0): 2, ('current', 1.0): 7, ('goin', 1.0): 1, ('sweeti', 1.0): 4, ('strongest', 1.0): 1, (\"spot'\", 1.0): 1, ('barnstapl', 1.0): 1, ('bideford', 1.0): 1, ('abit', 1.0): 1, ('road', 1.0): 5, ('rocro', 1.0): 1, ('13glodyysbro', 1.0): 1, ('hire', 1.0): 1, ('2ne1', 1.0): 1, ('aspetti', 1.0): 1, ('chicken', 1.0): 4, ('chip', 1.0): 3, ('cupboard', 1.0): 1, ('empti', 1.0): 2, ('jami', 1.0): 2, ('ian', 1.0): 2, ('latin', 1.0): 5, ('asian', 1.0): 5, ('version', 1.0): 8, ('va', 1.0): 1, ('642', 1.0): 1, ('kikgirl', 1.0): 5, ('orgasm', 1.0): 1, ('phonesex', 1.0): 1, ('spacer', 1.0): 1, ('felic', 1.0): 1, ('smoak', 1.0): 1, ('👓', 1.0): 1, ('💘', 1.0): 3, ('children', 1.0): 3, ('psychopath', 1.0): 1, ('spoil', 1.0): 1, ('dimpl', 1.0): 1, ('contempl', 1.0): 1, ('indi', 1.0): 2, ('rout', 1.0): 4, ('jsl', 1.0): 1, ('76x', 1.0): 1, ('gotcha', 1.0): 1, ('kina', 1.0): 1, ('donna', 1.0): 3, ('reachabl', 1.0): 1, ('jk', 1.0): 1, ('s02e04', 1.0): 1, ('air', 1.0): 7, ('naggi', 1.0): 1, ('anal', 1.0): 1, ('child', 1.0): 3, ('vidcon', 1.0): 2, ('anxiou', 1.0): 1, ('shake', 1.0): 2, ('10:30', 1.0): 1, ('smoke', 1.0): 3, ('white', 1.0): 4, ('grandpa', 1.0): 4, ('prolli', 1.0): 1, ('stash', 1.0): 2, ('closer-chas', 1.0): 1, ('spec', 1.0): 1, ('leagu', 1.0): 3, ('chase', 1.0): 1, ('wall', 1.0): 3, ('angel', 1.0): 4, ('mochamichel', 1.0): 1, ('iph', 1.0): 4, ('0ne', 1.0): 4, ('simpli', 1.0): 3, ('bi0', 1.0): 8, ('x29', 1.0): 1, ('there', 1.0): 2, ('background', 1.0): 2, ('maggi', 1.0): 1, ('afraid', 1.0): 3, ('mull', 1.0): 1, ('nil', 1.0): 1, ('glasgow', 1.0): 2, ('netbal', 1.0): 1, ('thistl', 1.0): 1, ('thistlelov', 1.0): 1, ('minecraft', 1.0): 7, ('drew', 1.0): 3, ('delici', 1.0): 3, ('muddl', 1.0): 1, ('racket', 1.0): 2, ('isol', 1.0): 1, ('fa', 1.0): 1, ('particip', 1.0): 2, ('icecreammast', 1.0): 1, ('group', 1.0): 10, ('huhu', 1.0): 3, ('shet', 1.0): 1, ('desk', 1.0): 1, ('o_o', 1.0): 1, ('orz', 1.0): 1, ('problemmm', 1.0): 1, ('75x', 1.0): 1, ('english', 1.0): 4, ('yeeaayi', 1.0): 1, ('alhamdulillah', 1.0): 1, ('amin', 1.0): 1, ('weed', 1.0): 1, ('crowdfund', 1.0): 1, ('goal', 1.0): 2, ('walk', 1.0): 12, ('hellooo', 1.0): 2, ('select', 1.0): 1, ('lynn', 1.0): 1, ('buffer', 1.0): 2, ('button', 1.0): 2, ('compos', 1.0): 1, ('fridayfun', 1.0): 1, ('non-filipina', 1.0): 1, ('ejayst', 1.0): 1, ('state', 1.0): 2, ('le', 1.0): 2, ('stan', 1.0): 1, ('lee', 1.0): 2, ('discoveri', 1.0): 1, ('cousin', 1.0): 5, ('1400', 1.0): 1, ('yr', 1.0): 2, ('teleport', 1.0): 1, ('shahid', 1.0): 1, ('afridi', 1.0): 1, ('tou', 1.0): 1, ('mahnor', 1.0): 1, ('baloch', 1.0): 1, ('nikki', 1.0): 2, ('flower', 1.0): 4, ('blackfli', 1.0): 1, ('courgett', 1.0): 1, ('wont', 1.0): 5, ('affect', 1.0): 2, ('fruit', 1.0): 5, ('italian', 1.0): 1, ('netfilx', 1.0): 1, ('unmarri', 1.0): 1, ('finger', 1.0): 6, ('rock', 1.0): 10, ('wielli', 1.0): 1, ('paul', 1.0): 2, ('barcod', 1.0): 1, ('charlott', 1.0): 1, ('thta', 1.0): 1, ('trailblazerhonor', 1.0): 1, ('labour', 1.0): 3, ('leader', 1.0): 3, ('alot', 1.0): 2, ('agayhippiehippi', 1.0): 1, ('exercis', 1.0): 2, ('ginger', 1.0): 1, ('x28', 1.0): 1, ('teach', 1.0): 2, ('awar', 1.0): 1, ('::', 1.0): 4, ('portsmouth', 1.0): 1, ('sonal', 1.0): 1, ('hungri', 1.0): 2, ('hmmm', 1.0): 4, ('pedant', 1.0): 1, ('98', 1.0): 1, ('kit', 1.0): 2, ('ack', 1.0): 1, ('hih', 1.0): 1, ('choir', 1.0): 1, ('rosidbinr', 1.0): 1, ('duke', 1.0): 2, ('earl', 1.0): 1, ('tau', 1.0): 1, ('orayt', 1.0): 1, ('knw', 1.0): 1, ('block', 1.0): 3, ('dikha', 1.0): 1, ('reh', 1.0): 1, ('adolf', 1.0): 1, ('hitler', 1.0): 1, ('obstacl', 1.0): 1, ('exist', 1.0): 2, ('surrend', 1.0): 2, ('terrif', 1.0): 1, ('advaddict', 1.0): 1, ('_15', 1.0): 1, ('jimin', 1.0): 1, ('notanapolog', 1.0): 3, ('map', 1.0): 2, ('inform', 1.0): 5, ('0.7', 1.0): 1, ('motherfuck', 1.0): 1, (\"david'\", 1.0): 1, ('damn', 1.0): 3, ('colleg', 1.0): 2, ('24th', 1.0): 3, ('steroid', 1.0): 1, ('alansmithpart', 1.0): 1, ('servu', 1.0): 1, ('bonasio', 1.0): 1, (\"doido'\", 1.0): 1, ('task', 1.0): 2, ('deleg', 1.0): 1, ('aaahhh', 1.0): 1, ('jen', 1.0): 2, ('virgin', 1.0): 5, ('non-mapbox', 1.0): 1, ('restrict', 1.0): 1, ('mapbox', 1.0): 1, ('basemap', 1.0): 1, ('contractu', 1.0): 1, ('research', 1.0): 1, ('seafood', 1.0): 1, ('weltum', 1.0): 1, ('teh', 1.0): 1, ('deti', 1.0): 1, ('huh', 1.0): 2, ('=d', 1.0): 2, ('annoy', 1.0): 2, ('katmtan', 1.0): 1, ('swan', 1.0): 1, ('fandom', 1.0): 3, ('blurri', 1.0): 1, ('besok', 1.0): 1, ('b', 1.0): 8, ('urgent', 1.0): 3, ('within', 1.0): 4, ('dorset', 1.0): 1, ('goddess', 1.0): 1, ('blast', 1.0): 1, ('shitfac', 1.0): 1, ('soul', 1.0): 4, ('sing', 1.0): 5, ('disney', 1.0): 1, ('doug', 1.0): 3, ('28', 1.0): 2, ('bnte', 1.0): 1, ('hain', 1.0): 2, (';p', 1.0): 1, ('shiiitt', 1.0): 1, ('case', 1.0): 9, ('rm35', 1.0): 1, ('negooo', 1.0): 1, ('male', 1.0): 1, ('madelin', 1.0): 1, ('nun', 1.0): 1, ('mornin', 1.0): 2, ('yapster', 1.0): 1, ('pli', 1.0): 1, ('icon', 1.0): 2, ('alchemist', 1.0): 1, ('x27', 1.0): 1, ('dayz', 1.0): 1, ('preview', 1.0): 1, ('thug', 1.0): 1, ('lmao', 1.0): 3, ('sharethelov', 1.0): 2, ('highvalu', 1.0): 2, ('halsey', 1.0): 1, ('30th', 1.0): 1, ('anniversari', 1.0): 5, ('folk', 1.0): 10, ('bae', 1.0): 6, ('repli', 1.0): 5, ('complain', 1.0): 3, ('rude', 1.0): 3, ('bond', 1.0): 4, ('nigg', 1.0): 1, ('readingr', 1.0): 1, ('wordoftheweek', 1.0): 1, ('wotw', 1.0): 1, ('4:18', 1.0): 1, ('est', 1.0): 1, ('earn', 1.0): 1, ('jess', 1.0): 2, ('surri', 1.0): 1, ('botani', 1.0): 1, ('gel', 1.0): 1, ('alison', 1.0): 1, ('lsa', 1.0): 1, ('respons', 1.0): 7, ('fron', 1.0): 1, ('debbi', 1.0): 1, ('carol', 1.0): 2, ('patient', 1.0): 4, ('discharg', 1.0): 1, ('loung', 1.0): 1, ('walmart', 1.0): 1, ('balanc', 1.0): 2, ('studi', 1.0): 6, ('hayley', 1.0): 2, ('shoulder', 1.0): 1, ('pad', 1.0): 2, ('mount', 1.0): 1, ('inquisitor', 1.0): 1, ('cosplay', 1.0): 4, ('cosplayprogress', 1.0): 1, ('mike', 1.0): 3, ('dunno', 1.0): 2, ('insecur', 1.0): 2, ('nh', 1.0): 1, ('devolut', 1.0): 1, ('patriot', 1.0): 1, ('halla', 1.0): 1, ('ark', 1.0): 1, (\"jiyeon'\", 1.0): 1, ('buzz', 1.0): 2, ('burnt', 1.0): 1, ('mist', 1.0): 4, ('opi', 1.0): 1, ('avoplex', 1.0): 1, ('nail', 1.0): 3, ('cuticl', 1.0): 1, ('replenish', 1.0): 1, ('15ml', 1.0): 1, ('seriou', 1.0): 2, ('submiss', 1.0): 1, ('lb', 1.0): 2, ('cherish', 1.0): 2, ('flip', 1.0): 1, ('learnt', 1.0): 2, ('backflip', 1.0): 2, ('jumpgiant', 1.0): 1, ('foampit', 1.0): 1, ('usa', 1.0): 3, ('pamer', 1.0): 1, ('thk', 1.0): 1, ('actuallythough', 1.0): 1, ('craft', 1.0): 2, ('session', 1.0): 3, ('mehtab', 1.0): 1, ('aunti', 1.0): 1, ('gc', 1.0): 1, ('yeeew', 1.0): 1, ('pre', 1.0): 3, ('lan', 1.0): 1, ('yeey', 1.0): 1, ('arrang', 1.0): 1, ('doodl', 1.0): 2, ('comic', 1.0): 1, ('summon', 1.0): 1, ('none', 1.0): 1, ('🙅', 1.0): 1, ('lycra', 1.0): 1, ('vincent', 1.0): 1, ('couldnt', 1.0): 1, ('roy', 1.0): 1, ('bg', 1.0): 1, ('img', 1.0): 1, ('circl', 1.0): 1, ('font', 1.0): 1, ('deathofgrass', 1.0): 1, ('loan', 1.0): 2, ('lawnmow', 1.0): 1, ('popular', 1.0): 2, ('charismat', 1.0): 1, ('man.h', 1.0): 1, ('thrive', 1.0): 1, ('economi', 1.0): 1, ('burst', 1.0): 2, ('georgi', 1.0): 1, ('x26', 1.0): 1, ('million', 1.0): 4, ('fl', 1.0): 1, ('kindest', 1.0): 2, ('iceland', 1.0): 1, ('crazi', 1.0): 4, ('landscap', 1.0): 2, ('yok', 1.0): 1, ('lah', 1.0): 1, ('concordia', 1.0): 1, ('reunit', 1.0): 1, ('xxxibmchll', 1.0): 1, ('sea', 1.0): 4, ('prettier', 1.0): 2, ('imitatia', 1.0): 1, ('oe', 1.0): 1, ('michel', 1.0): 1, ('comeback', 1.0): 1, ('gross', 1.0): 1, ('treat', 1.0): 5, ('equal', 1.0): 2, ('injustic', 1.0): 1, ('femin', 1.0): 1, ('ineedfeminismbecaus', 1.0): 1, ('forgotten', 1.0): 3, ('stuck', 1.0): 4, ('recommend', 1.0): 4, ('redhead', 1.0): 1, ('wacki', 1.0): 1, ('rather', 1.0): 5, ('waytoliveahappylif', 1.0): 1, ('hoxton', 1.0): 1, ('holborn', 1.0): 1, ('karen', 1.0): 2, ('wag', 1.0): 2, ('bum', 1.0): 1, ('wwooo', 1.0): 1, ('nite', 1.0): 3, ('laiten', 1.0): 1, ('arond', 1.0): 1, ('1:30', 1.0): 1, ('consid', 1.0): 3, ('matur', 1.0): 3, ('journeyp', 1.0): 2, ('foam', 1.0): 1, (\"lady'\", 1.0): 1, ('mob', 1.0): 1, ('fals', 1.0): 1, ('bulletin', 1.0): 1, ('spring', 1.0): 1, ('fiesta', 1.0): 1, ('nois', 1.0): 2, ('awuuu', 1.0): 1, ('aich', 1.0): 1, ('sept', 1.0): 2, ('rudramadevi', 1.0): 1, ('anushka', 1.0): 1, ('gunashekar', 1.0): 1, ('harryxhood', 1.0): 1, ('upset', 1.0): 1, ('ooh', 1.0): 1, ('humanist', 1.0): 1, ('magazin', 1.0): 2, ('usernam', 1.0): 1, ('rape', 1.0): 1, ('csrrace', 1.0): 1, ('lack', 1.0): 6, ('hygien', 1.0): 1, ('tose', 1.0): 1, ('cloth', 1.0): 1, ('temperatur', 1.0): 1, ('planet', 1.0): 2, ('brave', 1.0): 2, ('ge', 1.0): 1, ('2015kenya', 1.0): 1, ('ryan', 1.0): 4, ('tidi', 1.0): 2, ('hagergang', 1.0): 1, ('chanhun', 1.0): 1, ('photoshoot', 1.0): 1, ('afteral', 1.0): 1, ('sadkaay', 1.0): 1, ('thark', 1.0): 1, ('peak', 1.0): 1, ('heatwav', 1.0): 1, ('lower', 1.0): 1, ('standard', 1.0): 2, ('x25', 1.0): 1, ('recruit', 1.0): 2, ('doom', 1.0): 1, ('nasti', 1.0): 1, ('affili', 1.0): 1, ('&gt;:)', 1.0): 2, ('64', 1.0): 2, ('74', 1.0): 1, ('40', 1.0): 4, ('00', 1.0): 1, ('hall', 1.0): 2, ('ted', 1.0): 3, ('pixgram', 1.0): 2, ('creativ', 1.0): 2, ('slideshow', 1.0): 1, ('nibbl', 1.0): 2, ('ivi', 1.0): 1, ('sho', 1.0): 1, ('superpow', 1.0): 2, ('obsess', 1.0): 2, ('oth', 1.0): 1, ('third', 1.0): 2, ('ngarepfollbackdarinabilahjkt', 1.0): 1, ('48', 1.0): 1, ('sunglass', 1.0): 1, ('jacki', 1.0): 2, ('sunni', 1.0): 6, ('style', 1.0): 5, ('jlo', 1.0): 1, ('jlover', 1.0): 1, ('turkey', 1.0): 1, ('goodafternoon', 1.0): 2, ('collag', 1.0): 2, ('furri', 1.0): 2, ('bruce', 1.0): 2, ('kunoriforceo', 1.0): 8, ('aayegi', 1.0): 1, ('tim', 1.0): 2, ('wiw', 1.0): 1, ('bip', 1.0): 1, ('zareen', 1.0): 1, ('daisi', 1.0): 1, (\"b'coz\", 1.0): 1, ('kart', 1.0): 1, ('mak', 1.0): 1, ('∗', 1.0): 2, ('lega', 1.0): 1, ('spag', 1.0): 1, ('boat', 1.0): 2, ('outboard', 1.0): 1, ('spell', 1.0): 4, ('reboard', 1.0): 1, ('fire', 1.0): 2, ('offboard', 1.0): 1, ('sn16', 1.0): 1, ('9dg', 1.0): 1, ('bnf', 1.0): 1, ('50', 1.0): 1, ('jason', 1.0): 1, ('rob', 1.0): 2, ('feb', 1.0): 1, ('victoriasecret', 1.0): 1, ('finland', 1.0): 1, ('helsinki', 1.0): 1, ('airport', 1.0): 3, ('plane', 1.0): 2, ('beyond', 1.0): 4, ('ont', 1.0): 1, ('tii', 1.0): 1, ('lng', 1.0): 2, ('yan', 1.0): 2, (\"u'll\", 1.0): 2, ('steve', 1.0): 2, ('bell', 1.0): 1, ('prescott', 1.0): 1, ('leadership', 1.0): 2, ('cartoon', 1.0): 1, ('upsid', 1.0): 2, ('statement', 1.0): 1, ('selamathariraya', 1.0): 1, ('lovesummertim', 1.0): 1, ('dumont', 1.0): 1, ('jax', 1.0): 1, ('jone', 1.0): 1, ('awesomee', 1.0): 1, ('x24', 1.0): 1, ('geoff', 1.0): 1, ('amazingli', 1.0): 1, ('talant', 1.0): 1, ('vsco', 1.0): 2, ('thanki', 1.0): 2, ('hash', 1.0): 1, ('tag', 1.0): 5, ('ifimeetanalien', 1.0): 1, ('bff', 1.0): 4, ('section', 1.0): 3, ('follbaaack', 1.0): 1, ('az', 1.0): 1, ('cauliflow', 1.0): 1, ('attempt', 1.0): 1, ('prinsesa', 1.0): 1, ('yaaah', 1.0): 2, ('law', 1.0): 3, ('toy', 1.0): 2, ('sonaaa', 1.0): 1, ('beautiful', 1.0): 2, (\"josephine'\", 1.0): 1, ('mirror', 1.0): 3, ('cretaperfect', 1.0): 2, ('4me', 1.0): 2, ('cretaperfectsuv', 1.0): 2, ('creta', 1.0): 1, ('load', 1.0): 1, ('telecom', 1.0): 2, ('judi', 1.0): 1, ('superb', 1.0): 1, ('slightli', 1.0): 1, ('rakna', 1.0): 1, ('ew', 1.0): 1, ('whose', 1.0): 1, ('fifa', 1.0): 1, ('lineup', 1.0): 1, ('surviv', 1.0): 2, ('p90x', 1.0): 1, ('p90', 1.0): 1, ('dishoom', 1.0): 2, ('rajnigandha', 1.0): 1, ('minju', 1.0): 1, ('rapper', 1.0): 1, ('lead', 1.0): 2, ('vocal', 1.0): 1, ('yujin', 1.0): 1, ('visual', 1.0): 2, ('makna', 1.0): 1, ('jane', 1.0): 2, ('hah', 1.0): 4, ('hawk', 1.0): 2, ('greatest', 1.0): 2, ('histori', 1.0): 2, ('along', 1.0): 6, ('talkback', 1.0): 1, ('process', 1.0): 4, ('featur', 1.0): 4, ('mostli', 1.0): 1, (\"cinema'\", 1.0): 1, ('defend', 1.0): 2, ('fashion', 1.0): 2, ('atroc', 1.0): 1, ('pandimension', 1.0): 1, ('manifest', 1.0): 1, ('argo', 1.0): 1, ('ring', 1.0): 4, ('640', 1.0): 1, ('nad', 1.0): 1, ('plezzz', 1.0): 1, ('asthma', 1.0): 1, ('inhal', 1.0): 1, ('breath', 1.0): 3, ('goodluck', 1.0): 1, ('hunger', 1.0): 1, ('mockingjay', 1.0): 1, ('thehungergam', 1.0): 1, ('ador', 1.0): 4, ('x23', 1.0): 1, ('reina', 1.0): 1, ('felt', 1.0): 3, ('excus', 1.0): 2, ('attend', 1.0): 2, ('whn', 1.0): 1, ('andr', 1.0): 1, ('mamayang', 1.0): 1, ('11pm', 1.0): 1, ('1d', 1.0): 2, ('89.9', 1.0): 1, ('powi', 1.0): 1, ('shropshir', 1.0): 1, ('border', 1.0): 1, (\"school'\", 1.0): 1, ('san', 1.0): 2, ('diego', 1.0): 1, ('jump', 1.0): 2, ('sourc', 1.0): 3, ('appeas', 1.0): 1, ('¦', 1.0): 1, ('aj', 1.0): 1, ('action', 1.0): 1, ('grunt', 1.0): 1, ('sc', 1.0): 1, ('anti-christ', 1.0): 1, ('m8', 1.0): 1, ('ju', 1.0): 1, ('halfway', 1.0): 1, ('ex', 1.0): 2, ('postiv', 1.0): 2, ('opinion', 1.0): 3, ('avi', 1.0): 1, ('dare', 1.0): 4, ('corridor', 1.0): 1, ('👯', 1.0): 2, ('neither', 1.0): 2, ('rundown', 1.0): 1, ('yah', 1.0): 4, ('leviboard', 1.0): 1, ('kleper', 1.0): 1, (':(', 1.0): 1, ('impecc', 1.0): 2, ('setokido', 1.0): 1, ('shoulda', 1.0): 3, ('hippo', 1.0): 1, ('materialist', 1.0): 1, ('showpo', 1.0): 1, ('cough', 1.0): 6, ('@artofsleepingin', 1.0): 1, ('x22', 1.0): 1, ('☺', 1.0): 5, ('makesm', 1.0): 1, ('santorini', 1.0): 1, ('escap', 1.0): 2, ('beatport', 1.0): 1, ('👊🏻', 1.0): 1, ('trmdhesit', 1.0): 2, ('manuel', 1.0): 1, ('vall', 1.0): 1, ('king', 1.0): 3, ('seven', 1.0): 2, ('kingdom', 1.0): 2, ('andal', 1.0): 1, ('taught', 1.0): 1, ('hide', 1.0): 3, ('privaci', 1.0): 1, ('wise', 1.0): 1, ('natsuki', 1.0): 1, ('often', 1.0): 2, ('catchi', 1.0): 1, ('neil', 1.0): 2, ('emir', 1.0): 2, ('brill', 1.0): 1, ('urquhart', 1.0): 1, ('castl', 1.0): 1, ('simpl', 1.0): 2, ('shatter', 1.0): 2, ('contrast', 1.0): 1, ('educampakl', 1.0): 1, ('rotorua', 1.0): 1, ('pehli', 1.0): 1, ('phir', 1.0): 1, ('somi', 1.0): 1, ('burfday', 1.0): 1, ('univers', 1.0): 3, ('santo', 1.0): 1, ('toma', 1.0): 1, ('norh', 1.0): 1, ('dialogu', 1.0): 2, ('chainsaw', 1.0): 2, ('amus', 1.0): 1, ('awe', 1.0): 1, ('protect', 1.0): 2, ('pop', 1.0): 5, ('2ish', 1.0): 1, ('fahad', 1.0): 1, ('bhai', 1.0): 3, ('iqrar', 1.0): 1, ('waseem', 1.0): 1, ('abroad', 1.0): 2, ('movie', 1.0): 1, ('chef', 1.0): 1, ('grogol', 1.0): 1, ('long-dist', 1.0): 1, ('rhi', 1.0): 1, ('pwrfl', 1.0): 1, ('benefit', 1.0): 2, ('b2b', 1.0): 1, ('b2c', 1.0): 1, (\"else'\", 1.0): 2, ('soo', 1.0): 2, ('enterprison', 1.0): 1, ('schoolsoutforsumm', 1.0): 1, ('fellow', 1.0): 4, ('juggl', 1.0): 1, ('purrtho', 1.0): 1, ('catho', 1.0): 1, ('catami', 1.0): 1, ('fourfivesecond', 1.0): 4, ('deaf', 1.0): 4, ('drug', 1.0): 1, ('alcohol', 1.0): 1, ('apexi', 1.0): 3, ('crystal', 1.0): 3, ('meth', 1.0): 1, ('champagn', 1.0): 1, ('fc', 1.0): 1, ('streamer', 1.0): 1, ('juic', 1.0): 1, ('correct', 1.0): 1, ('portrait', 1.0): 1, ('izumi', 1.0): 1, ('fugiwara', 1.0): 1, ('clonmel', 1.0): 1, ('vibrant', 1.0): 1, ('estim', 1.0): 1, ('server', 1.0): 2, ('quiet', 1.0): 1, ('yey', 1.0): 1, (\"insha'allah\", 1.0): 1, ('wil', 1.0): 1, ('x21', 1.0): 1, ('trend', 1.0): 3, ('akshaymostlovedsuperstarev', 1.0): 1, ('indirect', 1.0): 1, ('askurban', 1.0): 1, ('lyka', 1.0): 2, ('nap', 1.0): 4, ('aff', 1.0): 1, ('unam', 1.0): 1, ('jonginuh', 1.0): 1, ('forecast', 1.0): 2, ('10am', 1.0): 2, ('5am', 1.0): 1, ('sooth', 1.0): 1, ('vii', 1.0): 1, ('sweetheart', 1.0): 1, ('freak', 1.0): 3, ('zayn', 1.0): 3, ('fucker', 1.0): 1, ('pet', 1.0): 2, ('illustr', 1.0): 1, ('wohoo', 1.0): 1, ('gleam', 1.0): 1, ('paint', 1.0): 4, ('deal', 1.0): 2, ('prime', 1.0): 2, ('minist', 1.0): 2, ('sunjam', 1.0): 1, ('industri', 1.0): 1, ('present', 1.0): 7, ('practic', 1.0): 3, ('proactiv', 1.0): 1, ('environ', 1.0): 1, ('unreal', 1.0): 1, ('zain', 1.0): 1, ('zac', 1.0): 1, ('isaac', 1.0): 1, ('oss', 1.0): 1, ('frank', 1.0): 1, ('iero', 1.0): 1, ('phase', 1.0): 2, ('david', 1.0): 1, ('beginn', 1.0): 1, ('shine', 1.0): 3, ('sunflow', 1.0): 2, ('tommarow', 1.0): 1, ('yall', 1.0): 2, ('rank', 1.0): 2, ('birthdaymonth', 1.0): 1, ('vianey', 1.0): 1, ('juli', 1.0): 11, ('birthdaygirl', 1.0): 1, (\"town'\", 1.0): 1, ('andrew', 1.0): 2, ('checkout', 1.0): 2, ('otwol', 1.0): 1, ('awhil', 1.0): 1, ('x20', 1.0): 1, ('all-tim', 1.0): 1, ('julia', 1.0): 1, ('robert', 1.0): 1, ('awwhh', 1.0): 1, ('bulldog', 1.0): 1, ('unfortun', 1.0): 2, ('02079', 1.0): 1, ('490', 1.0): 1, ('132', 1.0): 1, ('born', 1.0): 2, ('fightstickfriday', 1.0): 1, ('extravag', 1.0): 2, ('tearout', 1.0): 1, ('selekt', 1.0): 1, ('yoot', 1.0): 1, ('cross', 1.0): 3, ('gudday', 1.0): 1, ('dave', 1.0): 5, ('haileyhelp', 1.0): 1, ('eid', 1.0): 2, ('mubarak', 1.0): 5, ('brotheeerrr', 1.0): 1, ('adventur', 1.0): 5, ('tokyo', 1.0): 2, ('kansai', 1.0): 1, ('l', 1.0): 4, ('upp', 1.0): 2, ('om', 1.0): 1, ('60', 1.0): 1, ('minut', 1.0): 7, ('data', 1.0): 1, ('jesu', 1.0): 5, ('amsterdam', 1.0): 2, ('3rd', 1.0): 3, ('nextweek', 1.0): 1, ('booti', 1.0): 2, ('bcuz', 1.0): 1, ('step', 1.0): 3, ('option', 1.0): 3, ('stabl', 1.0): 1, ('sturdi', 1.0): 1, ('lukkke', 1.0): 1, ('again.ensoi', 1.0): 1, ('tc', 1.0): 1, ('madam', 1.0): 1, ('siddi', 1.0): 1, ('unknown', 1.0): 2, ('roomi', 1.0): 1, ('gn', 1.0): 2, ('gf', 1.0): 2, ('consent', 1.0): 1, ('mister', 1.0): 2, ('vine', 1.0): 2, ('peyton', 1.0): 1, ('nagato', 1.0): 1, ('yuki-chan', 1.0): 1, ('shoushitsu', 1.0): 1, ('archdbanterburi', 1.0): 3, ('experttradesmen', 1.0): 1, ('banter', 1.0): 1, ('quiz', 1.0): 1, ('tradetalk', 1.0): 1, ('floof', 1.0): 1, ('face', 1.0): 13, ('muahah', 1.0): 1, ('x19', 1.0): 1, ('anticip', 1.0): 1, ('jd', 1.0): 1, ('laro', 1.0): 1, ('tayo', 1.0): 1, ('answer', 1.0): 8, ('ht', 1.0): 1, ('angelica', 1.0): 1, ('anghel', 1.0): 1, ('aa', 1.0): 3, ('kkk', 1.0): 1, ('macbook', 1.0): 1, ('rehears', 1.0): 1, ('youthcelebr', 1.0): 1, ('mute', 1.0): 1, ('29th', 1.0): 1, ('gohf', 1.0): 4, ('vegetarian', 1.0): 1, (\"she'll\", 1.0): 1, ('gooday', 1.0): 3, ('101', 1.0): 3, ('12000', 1.0): 1, ('oshieer', 1.0): 1, ('realreview', 1.0): 1, ('happycustom', 1.0): 1, ('realoshi', 1.0): 1, ('dealsuthaonotebachao', 1.0): 1, ('bigger', 1.0): 2, ('dime', 1.0): 1, ('uhuh', 1.0): 1, ('🎵', 1.0): 3, ('code', 1.0): 4, ('pleasant', 1.0): 2, ('on-board', 1.0): 1, ('raheel', 1.0): 1, ('flyhigh', 1.0): 1, ('bother', 1.0): 2, ('everett', 1.0): 1, ('taylor', 1.0): 1, ('ha-ha', 1.0): 1, ('peachyloan', 1.0): 1, ('fridayfreebi', 1.0): 1, ('noe', 1.0): 1, ('yisss', 1.0): 1, ('bindingofissac', 1.0): 1, ('xboxon', 1.0): 1, ('consol', 1.0): 1, ('justin', 1.0): 2, ('gladli', 1.0): 1, ('son', 1.0): 4, ('morocco', 1.0): 1, ('peru', 1.0): 1, ('nxt', 1.0): 1, ('bp', 1.0): 1, ('resort', 1.0): 1, ('x18', 1.0): 1, ('havuuulovey', 1.0): 1, ('uuu', 1.0): 1, ('possitv', 1.0): 1, ('hopey', 1.0): 1, ('throwbackfriday', 1.0): 1, ('christen', 1.0): 1, ('ki', 1.0): 1, ('yaad', 1.0): 1, ('gayi', 1.0): 1, ('opossum', 1.0): 1, ('belat', 1.0): 5, ('yeahh', 1.0): 2, ('kuffar', 1.0): 1, ('comput', 1.0): 5, ('cell', 1.0): 1, ('diarrhea', 1.0): 1, ('immigr', 1.0): 1, ('lice', 1.0): 1, ('goictiv', 1.0): 1, ('70685', 1.0): 1, ('tagsforlik', 1.0): 4, ('trapmus', 1.0): 1, ('hotmusicdeloco', 1.0): 1, ('kinick', 1.0): 1, ('01282', 1.0): 2, ('452096', 1.0): 1, ('shadi', 1.0): 1, ('reserv', 1.0): 3, ('tkt', 1.0): 1, ('likewis', 1.0): 4, ('overgener', 1.0): 1, ('ikr', 1.0): 1, ('😍', 1.0): 2, ('consumer', 1.0): 1, ('fic', 1.0): 2, ('ouch', 1.0): 2, ('slip', 1.0): 1, ('disc', 1.0): 1, ('thw', 1.0): 1, ('chute', 1.0): 1, ('chalut', 1.0): 1, ('replay', 1.0): 1, ('iplay', 1.0): 1, ('11am', 1.0): 3, ('unneed', 1.0): 1, ('megamoh', 1.0): 1, ('7/29', 1.0): 1, ('tool', 1.0): 2, ('zealand', 1.0): 1, ('pile', 1.0): 2, ('dump', 1.0): 1, ('couscou', 1.0): 3, (\"women'\", 1.0): 2, ('fiction', 1.0): 1, ('wahahaah', 1.0): 1, ('x17', 1.0): 1, ('orhan', 1.0): 1, ('pamuk', 1.0): 1, ('hero', 1.0): 3, ('canopi', 1.0): 1, ('mapl', 1.0): 2, ('syrup', 1.0): 1, ('farm', 1.0): 2, ('stephani', 1.0): 2, ('💖', 1.0): 2, ('congrtaualt', 1.0): 1, ('philea', 1.0): 1, ('club', 1.0): 4, ('inc', 1.0): 1, ('photograph', 1.0): 2, ('phonegraph', 1.0): 1, ('srsli', 1.0): 1, ('10:17', 1.0): 1, ('ripaaa', 1.0): 1, ('banat', 1.0): 1, ('ray', 1.0): 1, ('dept', 1.0): 1, ('hospit', 1.0): 3, ('grt', 1.0): 1, ('infograph', 1.0): 1, (\"o'clock\", 1.0): 2, ('habit', 1.0): 1, ('1dfor', 1.0): 1, ('roadtrip', 1.0): 1, ('19:30', 1.0): 1, ('ifc', 1.0): 1, ('whip', 1.0): 1, ('lilsisbro', 1.0): 1, ('pre-ord', 1.0): 2, (\"pixar'\", 1.0): 2, ('steelbook', 1.0): 1, ('hmm', 1.0): 2, ('pegel', 1.0): 1, ('lemess', 1.0): 1, ('kyle', 1.0): 2, ('paypal', 1.0): 1, ('oct', 1.0): 1, ('tud', 1.0): 1, ('jst', 1.0): 2, ('humphrey', 1.0): 1, ('yell', 1.0): 2, ('erm', 1.0): 1, ('breach', 1.0): 1, ('lemon', 1.0): 2, ('yogurt', 1.0): 2, ('pot', 1.0): 1, ('discov', 1.0): 2, ('liquoric', 1.0): 1, ('pud', 1.0): 1, ('cajun', 1.0): 1, ('spice', 1.0): 1, ('yum', 1.0): 2, ('cajunchicken', 1.0): 1, ('infinit', 1.0): 2, ('fight', 1.0): 4, ('gern', 1.0): 1, ('cikaaa', 1.0): 1, ('maaf', 1.0): 1, ('telat', 1.0): 1, ('ngucapinnya', 1.0): 1, ('maaay', 1.0): 1, ('x16', 1.0): 1, ('viparita', 1.0): 1, ('karani', 1.0): 1, ('legsupthewal', 1.0): 1, ('unwind', 1.0): 1, ('coco', 1.0): 3, ('comfi', 1.0): 1, ('jalulu', 1.0): 1, ('rosh', 1.0): 1, ('gla', 1.0): 1, ('pallavi', 1.0): 1, ('nairobi', 1.0): 1, ('hrdstellobama', 1.0): 1, ('region', 1.0): 2, ('civil', 1.0): 1, ('societi', 1.0): 2, ('globe', 1.0): 1, ('hajur', 1.0): 1, ('yayi', 1.0): 2, (\"must'v\", 1.0): 1, ('nerv', 1.0): 1, ('prelim', 1.0): 1, ('costacc', 1.0): 1, ('nwb', 1.0): 1, ('shud', 1.0): 1, ('cold', 1.0): 2, ('hmu', 1.0): 2, ('cala', 1.0): 1, ('brush', 1.0): 1, ('ego', 1.0): 1, ('wherev', 1.0): 1, ('interact', 1.0): 2, ('dongsaeng', 1.0): 1, ('chorong', 1.0): 1, ('friendship', 1.0): 1, ('impress', 1.0): 3, ('dragon', 1.0): 2, ('duck', 1.0): 5, ('mix', 1.0): 5, ('cheetah', 1.0): 1, ('wagga', 1.0): 2, ('coursework', 1.0): 1, ('lorna', 1.0): 1, ('scan', 1.0): 1, ('x12', 1.0): 2, ('canva', 1.0): 2, ('iqbal', 1.0): 1, ('ima', 1.0): 1, ('hon', 1.0): 1, ('aja', 1.0): 1, ('besi', 1.0): 1, ('chati', 1.0): 1, ('phulani', 1.0): 1, ('swasa', 1.0): 1, ('bahari', 1.0): 1, ('jiba', 1.0): 1, ('mumbai', 1.0): 1, ('gujarat', 1.0): 1, ('distrub', 1.0): 1, ('otherwis', 1.0): 5, ('190cr', 1.0): 1, ('inspit', 1.0): 1, ('highest', 1.0): 1, ('holder', 1.0): 1, ('threaten', 1.0): 1, ('daili', 1.0): 2, ('basi', 1.0): 1, ('vr', 1.0): 1, ('angelo', 1.0): 1, ('quezon', 1.0): 1, ('sweatpant', 1.0): 1, ('farbridg', 1.0): 1, ('segalakatakata', 1.0): 1, ('nixu', 1.0): 1, ('begun', 1.0): 1, ('flint', 1.0): 1, ('🍰', 1.0): 5, ('separ', 1.0): 1, ('criticis', 1.0): 1, ('gestur', 1.0): 1, ('pedal', 1.0): 1, ('stroke', 1.0): 1, ('caro', 1.0): 1, ('deposit', 1.0): 1, ('secur', 1.0): 2, ('shock', 1.0): 1, ('coff', 1.0): 2, ('tenerina', 1.0): 1, ('auguri', 1.0): 1, ('iso', 1.0): 1, ('certif', 1.0): 1, ('paralyz', 1.0): 1, ('anxieti', 1.0): 1, (\"it'd\", 1.0): 1, ('develop', 1.0): 3, ('spain', 1.0): 2, ('def', 1.0): 1, ('bantim', 1.0): 1, ('fail', 1.0): 5, ('2ban', 1.0): 1, ('x15', 1.0): 1, ('awkward', 1.0): 2, ('ab', 1.0): 1, ('gale', 1.0): 1, ('founder', 1.0): 1, ('loveyaaah', 1.0): 1, ('⅛', 1.0): 1, ('⅞', 1.0): 1, ('∞', 1.0): 1, ('specialist', 1.0): 1, ('aw', 1.0): 3, ('babyyi', 1.0): 1, ('djstruthmat', 1.0): 1, ('re-cap', 1.0): 1, ('flickr', 1.0): 1, ('tack', 1.0): 2, ('zephbot', 1.0): 1, ('hhahahahaha', 1.0): 1, ('blew', 1.0): 2, ('entir', 1.0): 2, ('vega', 1.0): 3, ('strip', 1.0): 1, ('hahahahahhaha', 1.0): 1, (\"callie'\", 1.0): 1, ('puppi', 1.0): 1, ('owner', 1.0): 2, ('callinganimalabusehotlineasap', 1.0): 1, ('gorefiend', 1.0): 1, ('mythic', 1.0): 1, ('remind', 1.0): 6, ('9:00', 1.0): 1, ('▪', 1.0): 2, ('️bea', 1.0): 1, ('miller', 1.0): 2, ('lockscreen', 1.0): 1, ('mbf', 1.0): 1, ('keesh', 1.0): 1, (\"yesterday'\", 1.0): 1, ('groupi', 1.0): 1, ('bebe', 1.0): 1, ('sizam', 1.0): 1, ('color', 1.0): 5, ('invoic', 1.0): 1, ('kanina', 1.0): 1, ('pong', 1.0): 1, ('umaga', 1.0): 1, ('browser', 1.0): 1, ('typic', 1.0): 2, ('pleass', 1.0): 5, ('leeteuk', 1.0): 1, ('pearl', 1.0): 1, ('thusi', 1.0): 1, ('pour', 1.0): 1, ('milk', 1.0): 2, ('tgv', 1.0): 1, ('pari', 1.0): 5, ('austerlitz', 1.0): 1, ('bloi', 1.0): 1, ('mile', 1.0): 3, ('chateau', 1.0): 1, ('de', 1.0): 1, ('marai', 1.0): 1, ('taxi', 1.0): 1, ('x14', 1.0): 1, ('nom', 1.0): 1, ('enji', 1.0): 1, ('hater', 1.0): 3, ('purchas', 1.0): 2, ('specially-mark', 1.0): 1, ('custard', 1.0): 1, ('sm', 1.0): 1, ('on-pack', 1.0): 1, ('instruct', 1.0): 1, ('tile', 1.0): 1, ('downstair', 1.0): 1, ('kelli', 1.0): 1, ('greek', 1.0): 2, ('petra', 1.0): 1, ('shadowplayloui', 1.0): 1, ('mutual', 1.0): 2, ('cuz', 1.0): 4, ('liveonstream', 1.0): 1, ('lani', 1.0): 1, ('graze', 1.0): 1, ('pride', 1.0): 1, ('bristolart', 1.0): 1, ('in-app', 1.0): 1, ('ensur', 1.0): 1, ('item', 1.0): 2, ('screw', 1.0): 1, ('amber', 1.0): 2, ('43', 1.0): 1, ('hpc', 1.0): 1, ('wip', 1.0): 2, ('sw', 1.0): 1, ('newsround', 1.0): 1, ('hound', 1.0): 1, ('7:40', 1.0): 1, ('ada', 1.0): 1, ('racist', 1.0): 1, ('hulk', 1.0): 1, ('tight', 1.0): 2, ('prayer', 1.0): 3, ('pardon', 1.0): 1, ('phl', 1.0): 1, ('abu', 1.0): 2, ('dhabi', 1.0): 1, ('hihihi', 1.0): 1, ('teamjanuaryclaim', 1.0): 1, ('godonna', 1.0): 1, ('msg', 1.0): 2, ('bowwowchicawowwow', 1.0): 1, ('settl', 1.0): 1, ('dkt', 1.0): 1, ('porch', 1.0): 1, ('uber', 1.0): 2, ('mobil', 1.0): 4, ('applic', 1.0): 3, ('giggl', 1.0): 2, ('bare', 1.0): 3, ('wind', 1.0): 2, ('kahlil', 1.0): 1, ('gibran', 1.0): 1, ('flash', 1.0): 1, ('stiff', 1.0): 1, ('upper', 1.0): 1, ('lip', 1.0): 1, ('britain', 1.0): 1, ('latmon', 1.0): 1, ('endeavour', 1.0): 1, ('ann', 1.0): 2, ('joy', 1.0): 4, ('os', 1.0): 1, ('exploit', 1.0): 1, ('ign', 1.0): 2, ('au', 1.0): 1, ('pubcast', 1.0): 1, ('tengaman', 1.0): 1, ('21', 1.0): 2, ('celebratio', 1.0): 1, ('women', 1.0): 1, ('instal', 1.0): 2, ('glorifi', 1.0): 1, ('infirm', 1.0): 1, ('silli', 1.0): 1, ('suav', 1.0): 1, ('gentlemen', 1.0): 1, ('monthli', 1.0): 1, ('mileag', 1.0): 1, ('target', 1.0): 2, ('samsung', 1.0): 1, ('qualiti', 1.0): 3, ('ey', 1.0): 1, ('beth', 1.0): 2, ('gangster', 1.0): 1, (\"athena'\", 1.0): 1, ('fanci', 1.0): 1, ('wellington', 1.0): 1, ('rich', 1.0): 2, ('christina', 1.0): 1, ('newslett', 1.0): 1, ('zy', 1.0): 1, ('olur', 1.0): 1, ('x13', 1.0): 1, ('flawless', 1.0): 1, ('reaction', 1.0): 2, ('hayli', 1.0): 1, ('edwin', 1.0): 1, ('elvena', 1.0): 1, ('emc', 1.0): 1, ('rubber', 1.0): 3, ('swearword', 1.0): 1, ('infect', 1.0): 1, ('10:16', 1.0): 1, ('wrote', 1.0): 3, ('gan', 1.0): 1, ('brotherhood', 1.0): 1, ('wolf', 1.0): 5, ('pill', 1.0): 1, ('nocturn', 1.0): 1, ('rrp', 1.0): 1, ('18.99', 1.0): 1, ('13.99', 1.0): 1, ('jah', 1.0): 1, ('wobbl', 1.0): 1, ('retard', 1.0): 1, ('50notif', 1.0): 1, ('check-up', 1.0): 1, ('pun', 1.0): 1, ('elit', 1.0): 1, ('camillu', 1.0): 1, ('pleasee', 1.0): 1, ('spare', 1.0): 1, ('tyre', 1.0): 2, ('joke', 1.0): 3, ('ahahah', 1.0): 1, ('shame', 1.0): 1, ('abandon', 1.0): 1, ('disagre', 1.0): 2, ('nowher', 1.0): 2, ('contradict', 1.0): 1, ('chao', 1.0): 1, ('contain', 1.0): 1, ('cranium', 1.0): 1, ('sneaker', 1.0): 1, ('nike', 1.0): 1, ('nikeorigin', 1.0): 1, ('nikeindonesia', 1.0): 1, ('pierojogg', 1.0): 1, ('skoy', 1.0): 1, ('winter', 1.0): 2, ('falkland', 1.0): 1, ('jamie-le', 1.0): 1, ('congraaat', 1.0): 1, ('hooh', 1.0): 1, ('chrome', 1.0): 1, ('storm', 1.0): 1, ('thunderstorm', 1.0): 1, ('circuscircu', 1.0): 1, ('omgg', 1.0): 1, ('tdi', 1.0): 1, ('(-:', 1.0): 2, ('peter', 1.0): 1, ('expel', 1.0): 2, ('boughi', 1.0): 1, ('kernel', 1.0): 1, ('paralysi', 1.0): 1, ('liza', 1.0): 1, ('lol.hook', 1.0): 1, ('vampir', 1.0): 2, ('diari', 1.0): 3, ('twice', 1.0): 1, ('thanq', 1.0): 2, ('goodwil', 1.0): 1, ('vandr', 1.0): 1, ('ash', 1.0): 1, ('debat', 1.0): 3, ('solar', 1.0): 1, ('6-5', 1.0): 1, ('shown', 1.0): 1, ('ek', 1.0): 1, ('taco', 1.0): 2, ('mexico', 1.0): 2, ('viva', 1.0): 1, ('méxico', 1.0): 1, ('burger', 1.0): 3, ('thebestangkapuso', 1.0): 1, ('lighter', 1.0): 1, ('tooth', 1.0): 2, ('korean', 1.0): 2, ('netizen', 1.0): 1, ('crueler', 1.0): 1, ('eleph', 1.0): 1, ('marula', 1.0): 1, ('tdif', 1.0): 1, ('shoutout', 1.0): 1, ('shortli', 1.0): 1, ('itsamarvelth', 1.0): 1, (\"japan'\", 1.0): 1, ('artist', 1.0): 1, ('homework', 1.0): 1, ('marco', 1.0): 1, ('herb', 1.0): 1, ('pm', 1.0): 3, ('self', 1.0): 1, ('esteem', 1.0): 1, ('patienc', 1.0): 1, ('sobtian', 1.0): 1, ('cowork', 1.0): 1, ('deathli', 1.0): 1, ('hallow', 1.0): 1, ('supernatur', 1.0): 1, ('consult', 1.0): 1, ('himach', 1.0): 1, ('2.25', 1.0): 1, ('asham', 1.0): 1, ('where.do.i.start', 1.0): 1, ('moviemarathon', 1.0): 1, ('skill', 1.0): 4, ('shadow', 1.0): 1, ('own', 1.0): 1, ('pair', 1.0): 3, (\"it'll\", 1.0): 6, ('cortez', 1.0): 1, ('superstar', 1.0): 1, ('tthank', 1.0): 1, ('colin', 1.0): 1, ('luxuou', 1.0): 1, ('tarryn', 1.0): 1, ('hbdme', 1.0): 1, ('yeeeyyy', 1.0): 1, ('barsostay', 1.0): 1, ('males', 1.0): 1, ('independ', 1.0): 1, ('sum', 1.0): 1, ('debacl', 1.0): 1, ('perfectli', 1.0): 1, ('longer', 1.0): 2, ('amyjackson', 1.0): 1, ('omegl', 1.0): 2, ('countrymus', 1.0): 1, ('five', 1.0): 2, (\"night'\", 1.0): 2, (\"freddy'\", 1.0): 2, ('demo', 1.0): 2, ('pump', 1.0): 2, ('fanboy', 1.0): 1, ('thegrandad', 1.0): 1, ('sidni', 1.0): 1, ('remarriag', 1.0): 1, ('occas', 1.0): 1, ('languag', 1.0): 1, ('java', 1.0): 1, (\"php'\", 1.0): 1, ('notion', 1.0): 1, ('refer', 1.0): 1, ('confus', 1.0): 3, ('ohioan', 1.0): 1, ('stick', 1.0): 2, ('doctor', 1.0): 3, ('offlin', 1.0): 1, ('thesim', 1.0): 1, ('mb', 1.0): 1, ('meaningless', 1.0): 1, ('common', 1.0): 1, ('celebr', 1.0): 9, ('muertosatfring', 1.0): 1, ('emul', 1.0): 1, ('brought', 1.0): 1, ('enemi', 1.0): 2, ('relax', 1.0): 3, ('ou', 1.0): 1, ('pink', 1.0): 2, ('cc', 1.0): 2, ('meooowww', 1.0): 1, ('barkkkiiidee', 1.0): 1, ('bark', 1.0): 1, ('x11', 1.0): 1, ('routin', 1.0): 4, ('alek', 1.0): 1, ('awh', 1.0): 2, ('kumpul', 1.0): 1, ('cantik', 1.0): 1, ('ganteng', 1.0): 1, ('kresna', 1.0): 1, ('jelli', 1.0): 1, ('simon', 1.0): 1, ('lesley', 1.0): 3, ('blood', 1.0): 2, ('panti', 1.0): 1, ('lion', 1.0): 1, ('artworkbyli', 1.0): 1, ('judo', 1.0): 1, ('daredevil', 1.0): 2, ('despond', 1.0): 1, ('re-watch', 1.0): 1, ('welcoma.hav', 1.0): 1, ('favor', 1.0): 5, ('tridon', 1.0): 1, ('21pic', 1.0): 1, ('master', 1.0): 3, ('nim', 1.0): 1, (\"there'r\", 1.0): 1, ('22pic', 1.0): 1, ('kebun', 1.0): 1, ('ubud', 1.0): 1, ('ladyposs', 1.0): 1, ('xoxoxo', 1.0): 1, ('sneak', 1.0): 3, ('peek', 1.0): 2, ('inbox', 1.0): 1, ('happyweekend', 1.0): 1, ('therealgolden', 1.0): 1, ('47', 1.0): 1, ('girlfriendsmya', 1.0): 1, ('ppl', 1.0): 2, ('closest', 1.0): 1, ('njoy', 1.0): 1, ('followingg', 1.0): 1, ('privat', 1.0): 1, ('pusher', 1.0): 1, ('stun', 1.0): 4, ('wooohooo', 1.0): 1, ('cuss', 1.0): 1, ('teenag', 1.0): 1, ('ace', 1.0): 1, ('sauc', 1.0): 3, ('livi', 1.0): 1, ('fowl', 1.0): 1, ('oliviafowl', 1.0): 1, ('891', 1.0): 1, ('burnout', 1.0): 1, ('johnforceo', 1.0): 1, ('matthew', 1.0): 1, ('provok', 1.0): 1, ('indiankultur', 1.0): 1, ('oppos', 1.0): 1, ('biker', 1.0): 1, ('lyk', 1.0): 1, ('gud', 1.0): 4, ('weight', 1.0): 6, ('bcu', 1.0): 1, ('rubbish', 1.0): 1, ('veggi', 1.0): 2, ('steph', 1.0): 1, ('nj', 1.0): 1, ('x10', 1.0): 1, ('cohes', 1.0): 1, ('gossip', 1.0): 2, ('alex', 1.0): 3, ('heswifi', 1.0): 1, ('7am', 1.0): 1, ('wub', 1.0): 1, ('cerbchan', 1.0): 1, ('jarraaa', 1.0): 1, ('morrrn', 1.0): 1, ('snooz', 1.0): 1, ('clicksco', 1.0): 1, ('gay', 1.0): 4, ('lesbian', 1.0): 2, ('rigid', 1.0): 1, ('theocrat', 1.0): 1, ('wing', 1.0): 1, ('fundamentalist', 1.0): 1, ('islamist', 1.0): 1, ('brianaaa', 1.0): 1, ('brianazabrocki', 1.0): 1, ('sky', 1.0): 2, ('batb', 1.0): 1, ('clap', 1.0): 3, ('whilst', 1.0): 1, ('aki', 1.0): 1, ('thencerest', 1.0): 2, ('547', 1.0): 2, ('indiemus', 1.0): 5, ('sexyjudi', 1.0): 3, ('pussi', 1.0): 4, ('sexo', 1.0): 3, ('humid', 1.0): 1, ('87', 1.0): 1, ('sloppi', 1.0): 1, (\"second'\", 1.0): 1, ('stock', 1.0): 3, ('marmit', 1.0): 2, ('x9', 1.0): 1, ('nic', 1.0): 3, ('taft', 1.0): 1, ('finalist', 1.0): 1, ('lotteri', 1.0): 1, ('award', 1.0): 3, ('usagi', 1.0): 1, ('looov', 1.0): 1, ('wowww', 1.0): 2, ('💙', 1.0): 8, ('💚', 1.0): 8, ('💕', 1.0): 12, ('lepa', 1.0): 1, ('sembuh', 1.0): 1, ('sibuk', 1.0): 1, ('balik', 1.0): 1, ('kin', 1.0): 1, ('gotham', 1.0): 1, ('sunnyday', 1.0): 1, ('dudett', 1.0): 1, ('cost', 1.0): 1, ('flippin', 1.0): 1, ('fortun', 1.0): 1, ('divinediscont', 1.0): 1, (';}', 1.0): 1, ('amnot', 1.0): 1, ('autofollow', 1.0): 3, ('teamfollowback', 1.0): 4, ('geer', 1.0): 1, ('bat', 1.0): 2, ('mz', 1.0): 1, ('yang', 1.0): 2, ('deennya', 1.0): 1, ('jehwan', 1.0): 1, ('11:00', 1.0): 1, ('ashton', 1.0): 1, ('✧', 1.0): 12, ('｡', 1.0): 4, ('chelni', 1.0): 2, ('datz', 1.0): 1, ('jeremi', 1.0): 1, ('fmt', 1.0): 1, ('dat', 1.0): 3, ('heartbeat', 1.0): 1, ('clutch', 1.0): 1, ('🐢', 1.0): 2, ('besteverdoctorwhoepisod', 1.0): 1, ('relev', 1.0): 1, ('puke', 1.0): 1, ('proper', 1.0): 1, ('x8', 1.0): 1, ('sublimin', 1.0): 1, ('eatmeat', 1.0): 1, ('brewproject', 1.0): 1, ('lovenafianna', 1.0): 1, ('mr', 1.0): 7, ('lewi', 1.0): 1, ('clock', 1.0): 1, ('3:02', 1.0): 2, ('muslim', 1.0): 1, ('prophet', 1.0): 1, ('غردلي', 1.0): 4, ('is.h', 1.0): 1, ('mistak', 1.0): 4, ('understood', 1.0): 1, ('politician', 1.0): 1, ('argu', 1.0): 1, ('intellect', 1.0): 1, ('shiva', 1.0): 1, ('mp3', 1.0): 1, ('standrew', 1.0): 1, ('sandcastl', 1.0): 1, ('ewok', 1.0): 1, ('nate', 1.0): 2, ('brawl', 1.0): 1, ('rear', 1.0): 1, ('nake', 1.0): 1, ('choke', 1.0): 1, ('heck', 1.0): 1, ('gun', 1.0): 2, ('associ', 1.0): 1, ('um', 1.0): 1, ('endow', 1.0): 1, ('ai', 1.0): 1, ('sikandar', 1.0): 1, ('pti', 1.0): 1, ('standwdik', 1.0): 1, ('westandwithik', 1.0): 1, ('starbuck', 1.0): 2, ('logo', 1.0): 2, ('renew', 1.0): 1, ('chariti', 1.0): 1, ('جمعة_مباركة', 1.0): 1, ('hoki', 1.0): 1, ('biz', 1.0): 1, ('non', 1.0): 1, ('america', 1.0): 1, ('california', 1.0): 1, ('01:16', 1.0): 1, ('45gameplay', 1.0): 2, ('ilovey', 1.0): 2, ('vex', 1.0): 1, ('iger', 1.0): 1, ('leicaq', 1.0): 1, ('leica', 1.0): 1, ('dudee', 1.0): 1, ('persona', 1.0): 1, ('yepp', 1.0): 1, ('5878e503', 1.0): 1, ('x7', 1.0): 1, ('greg', 1.0): 1, ('posey', 1.0): 1, ('miami', 1.0): 1, ('james_yammouni', 1.0): 1, ('breakdown', 1.0): 1, ('materi', 1.0): 2, ('thorin', 1.0): 1, ('hunt', 1.0): 1, ('choroo', 1.0): 1, ('nahi', 1.0): 2, ('aztec', 1.0): 1, ('princess', 1.0): 2, ('raini', 1.0): 1, ('kingfish', 1.0): 1, ('chinua', 1.0): 1, ('acheb', 1.0): 1, ('intellectu', 1.0): 2, ('liquid', 1.0): 1, ('melbournetrip', 1.0): 1, ('taxikitchen', 1.0): 1, ('nooow', 1.0): 2, ('mcdo', 1.0): 1, ('everywher', 1.0): 2, ('dreamer', 1.0): 1, ('tanisha', 1.0): 1, ('1nonli', 1.0): 1, ('attitud', 1.0): 1, ('kindl', 1.0): 2, ('flame', 1.0): 1, ('convict', 1.0): 1, ('bar', 1.0): 1, ('repath', 1.0): 2, ('adi', 1.0): 1, ('stefani', 1.0): 1, ('sg1', 1.0): 1, ('lightbox', 1.0): 1, ('ran', 1.0): 2, ('incorrect', 1.0): 1, ('apologist', 1.0): 1, ('x6', 1.0): 1, ('vuli', 1.0): 1, ('01:15', 1.0): 1, ('batman', 1.0): 1, ('pearson', 1.0): 1, ('reput', 1.0): 2, ('nikkei', 1.0): 1, ('woodford', 1.0): 1, ('vscocam', 1.0): 1, ('vscoph', 1.0): 1, ('vscogood', 1.0): 1, ('vscophil', 1.0): 1, ('vscocousin', 1.0): 1, ('yaap', 1.0): 1, ('urwelc', 1.0): 1, ('neon', 1.0): 1, ('pant', 1.0): 1, ('haaa', 1.0): 1, ('will', 1.0): 2, ('auspost', 1.0): 1, ('openfollow', 1.0): 1, ('rp', 1.0): 2, ('eng', 1.0): 1, ('yūjō-cosplay', 1.0): 1, ('luxembourg', 1.0): 1, ('bunni', 1.0): 1, ('broadcast', 1.0): 1, ('needa', 1.0): 1, ('gal', 1.0): 3, ('bend', 1.0): 3, ('heaven', 1.0): 2, ('score', 1.0): 2, ('januari', 1.0): 1, ('hanabutl', 1.0): 1, ('kikhorni', 1.0): 1, ('interraci', 1.0): 1, ('makeup', 1.0): 1, ('chu', 1.0): 1, (\"weekend'\", 1.0): 1, ('punt', 1.0): 1, ('horserac', 1.0): 1, ('hors', 1.0): 2, ('horseracingtip', 1.0): 1, ('guitar', 1.0): 1, ('cocoar', 1.0): 1, ('brief', 1.0): 1, ('introduct', 1.0): 1, ('earliest', 1.0): 1, ('indian', 1.0): 1, ('subcontin', 1.0): 1, ('bfr', 1.0): 1, ('maurya', 1.0): 1, ('jordanian', 1.0): 1, ('00962778381', 1.0): 1, ('838', 1.0): 1, ('tenyai', 1.0): 1, ('hee', 1.0): 2, ('ss', 1.0): 1, ('semi', 1.0): 1, ('atp', 1.0): 2, ('wimbledon', 1.0): 2, ('feder', 1.0): 1, ('nadal', 1.0): 1, ('monfil', 1.0): 1, ('handsom', 1.0): 2, ('cilic', 1.0): 3, ('firm', 1.0): 1, ('potenti', 1.0): 3, ('nyc', 1.0): 1, ('chillin', 1.0): 2, ('tail', 1.0): 2, ('kitten', 1.0): 1, ('garret', 1.0): 1, ('baz', 1.0): 1, ('leo', 1.0): 2, ('xst', 1.0): 1, ('centrifug', 1.0): 1, ('etern', 1.0): 3, ('forgiv', 1.0): 2, ('kangin', 1.0): 1, ('بندر', 1.0): 1, ('العنزي', 1.0): 1, ('kristin', 1.0): 1, ('cass', 1.0): 1, ('surajettan', 1.0): 1, ('kashi', 1.0): 1, ('ashwathi', 1.0): 1, ('mommi', 1.0): 2, ('tirth', 1.0): 1, ('brambhatt', 1.0): 1, ('snooker', 1.0): 1, ('compens', 1.0): 1, ('theoper', 1.0): 1, ('479', 1.0): 1, ('premiostumundo', 1.0): 2, ('philosoph', 1.0): 1, ('x5', 1.0): 1, ('graphic', 1.0): 2, ('level', 1.0): 1, ('aug', 1.0): 3, ('excl', 1.0): 1, ('raw', 1.0): 1, ('weeni', 1.0): 1, ('annoyingbabi', 1.0): 1, ('lazi', 1.0): 2, ('cosi', 1.0): 1, ('client_amends_edit', 1.0): 1, ('_5_final_final_fin', 1.0): 1, ('pdf', 1.0): 1, ('mauliat', 1.0): 1, ('ito', 1.0): 2, ('okkay', 1.0): 1, ('knock', 1.0): 3, (\"soloist'\", 1.0): 1, ('ryu', 1.0): 1, ('saera', 1.0): 1, ('pinkeu', 1.0): 1, ('angri', 1.0): 3, ('screencap', 1.0): 1, ('jonghyun', 1.0): 1, ('seungyeon', 1.0): 1, ('cnblue', 1.0): 1, ('mbc', 1.0): 1, ('wgm', 1.0): 1, ('masa', 1.0): 2, ('entrepreneurship', 1.0): 1, ('empow', 1.0): 1, ('limpopo', 1.0): 1, ('pict', 1.0): 1, ('norapowel', 1.0): 1, ('hornykik', 1.0): 2, ('livesex', 1.0): 1, ('pumpkin', 1.0): 1, ('thrice', 1.0): 1, ('patron', 1.0): 1, ('ventur', 1.0): 1, ('deathcur', 1.0): 1, ('boob', 1.0): 1, ('blame', 1.0): 1, ('dine', 1.0): 1, ('modern', 1.0): 1, ('grill', 1.0): 1, ('disk', 1.0): 1, ('nt4', 1.0): 1, ('iirc', 1.0): 1, ('ux', 1.0): 1, ('refin', 1.0): 1, ('zdp', 1.0): 1, ('didnt', 1.0): 2, ('justic', 1.0): 1, ('daw', 1.0): 1, ('tine', 1.0): 1, ('gensan', 1.0): 1, ('frightl', 1.0): 1, ('undead', 1.0): 1, ('plush', 1.0): 1, ('cushion', 1.0): 1, ('nba', 1.0): 3, ('2k15', 1.0): 3, ('mypark', 1.0): 3, ('chronicl', 1.0): 4, ('gryph', 1.0): 3, ('volum', 1.0): 3, ('ellen', 1.0): 1, ('degener', 1.0): 1, ('shirt', 1.0): 1, ('mint', 1.0): 1, ('superdri', 1.0): 1, ('berangkaat', 1.0): 1, ('lagiii', 1.0): 1, ('siguro', 1.0): 1, ('un', 1.0): 1, ('kesa', 1.0): 1, ('lotsa', 1.0): 2, ('organis', 1.0): 2, ('4am', 1.0): 1, ('fingers-cross', 1.0): 1, ('deep', 1.0): 1, ('htaccess', 1.0): 1, ('file', 1.0): 2, ('adf', 1.0): 1, ('womad', 1.0): 1, ('gran', 1.0): 1, ('canaria', 1.0): 1, ('gig', 1.0): 1, ('twist', 1.0): 1, ('youv', 1.0): 1, ('teamnatur', 1.0): 1, ('huni', 1.0): 1, ('yayayayay', 1.0): 1, ('yt', 1.0): 2, ('convent', 1.0): 1, ('brighton', 1.0): 1, ('slay', 1.0): 1, ('nicknam', 1.0): 1, ('babygirl', 1.0): 1, ('regard', 1.0): 2, ('himmat', 1.0): 1, ('karain', 1.0): 2, ('baat', 1.0): 1, ('meri', 1.0): 1, ('hotee-mi', 1.0): 1, ('uncl', 1.0): 1, ('tongu', 1.0): 1, ('pronounc', 1.0): 1, ('nativ', 1.0): 1, ('american', 1.0): 2, ('proverb', 1.0): 1, ('lovabl', 1.0): 1, ('yesha', 1.0): 1, ('montoya', 1.0): 1, ('eagerli', 1.0): 1, ('payment', 1.0): 1, ('suprem', 1.0): 1, ('leon', 1.0): 1, ('ks', 1.0): 2, ('randi', 1.0): 1, ('9bi', 1.0): 1, ('physiqu', 1.0): 1, ('shave', 1.0): 1, ('uncut', 1.0): 1, ('boi', 1.0): 1, ('cheapest', 1.0): 1, ('regular', 1.0): 3, ('printer', 1.0): 3, ('nz', 1.0): 1, ('larg', 1.0): 4, ('format', 1.0): 1, ('10/10', 1.0): 1, ('senior', 1.0): 1, ('raid', 1.0): 2, ('conserv', 1.0): 1, ('batteri', 1.0): 1, ('comfort', 1.0): 2, ('swt', 1.0): 1, ('reservations@sandsbeach.eu', 1.0): 1, ('localgaragederbi', 1.0): 1, ('campu', 1.0): 1, ('subgam', 1.0): 1, ('faceit', 1.0): 1, ('snpcaht', 1.0): 1, ('hakhakhak', 1.0): 1, ('t___t', 1.0): 1, (\"kyungsoo'\", 1.0): 1, ('3d', 1.0): 2, ('properti', 1.0): 2, ('agent', 1.0): 1, ('accur', 1.0): 1, ('descript', 1.0): 1, ('theori', 1.0): 1, ('x4', 1.0): 1, ('15.90', 1.0): 1, ('yvett', 1.0): 1, ('author', 1.0): 2, ('mwf', 1.0): 1, ('programm', 1.0): 1, ('taal', 1.0): 1, ('lake', 1.0): 1, ('2emt', 1.0): 1, ('«', 1.0): 2, ('scurri', 1.0): 1, ('agil', 1.0): 1, ('solut', 1.0): 1, ('sme', 1.0): 1, ('omar', 1.0): 1, ('biggest', 1.0): 5, ('kamaal', 1.0): 1, ('amm', 1.0): 1, ('3am', 1.0): 1, ('hopehousekid', 1.0): 1, ('pitmantrain', 1.0): 1, ('walkersmithway', 1.0): 1, ('keepitloc', 1.0): 2, ('sehun', 1.0): 1, ('se100lead', 1.0): 1, ('unev', 1.0): 1, ('sofa', 1.0): 1, ('surf', 1.0): 1, ('cunt', 1.0): 1, ('rescoop', 1.0): 1, ('multiraci', 1.0): 1, ('fk', 1.0): 1, ('narrow', 1.0): 1, ('warlock', 1.0): 1, ('balloon', 1.0): 3, ('mj', 1.0): 1, ('madison', 1.0): 1, ('beonknockknock', 1.0): 1, ('con-gradu', 1.0): 1, ('gent', 1.0): 1, ('bitchfac', 1.0): 1, ('😒', 1.0): 1, ('organ', 1.0): 1, ('12pm', 1.0): 2, ('york', 1.0): 2, ('nearest', 1.0): 1, ('lendal', 1.0): 1, ('pikami', 1.0): 1, ('captur', 1.0): 1, ('fulton', 1.0): 1, ('sheen', 1.0): 1, ('baloney', 1.0): 1, ('unvarnish', 1.0): 1, ('laid', 1.0): 2, ('thick', 1.0): 1, ('blarney', 1.0): 1, ('flatteri', 1.0): 1, ('thin', 1.0): 1, ('sachin', 1.0): 1, ('unimport', 1.0): 1, ('context', 1.0): 1, ('dampen', 1.0): 1, ('yu', 1.0): 1, ('rocket', 1.0): 1, ('narendra', 1.0): 1, ('modi', 1.0): 1, ('aaaand', 1.0): 1, (\"team'\", 1.0): 1, ('macauley', 1.0): 1, ('howev', 1.0): 3, ('x3', 1.0): 1, ('wheeen', 1.0): 1, ('heechul', 1.0): 1, ('toast', 1.0): 2, ('coffee-weekday', 1.0): 1, ('9-11', 1.0): 1, ('sail', 1.0): 1, (\"friday'\", 1.0): 1, ('commerci', 1.0): 1, ('insur', 1.0): 1, ('requir', 1.0): 2, ('lookfortheo', 1.0): 1, ('cl', 1.0): 1, ('thou', 1.0): 1, ('april', 1.0): 2, ('airforc', 1.0): 1, ('clark', 1.0): 1, ('field', 1.0): 1, ('pampanga', 1.0): 1, ('troll', 1.0): 1, ('⚡', 1.0): 1, ('brow', 1.0): 1, ('oili', 1.0): 1, ('maricarljanah', 1.0): 1, ('6:15', 1.0): 1, ('degre', 1.0): 3, ('fahrenheit', 1.0): 1, ('🍸', 1.0): 7, ('╲', 1.0): 4, ('─', 1.0): 8, ('╱', 1.0): 5, ('🍤', 1.0): 4, ('╭', 1.0): 4, ('╮', 1.0): 4, ('┓', 1.0): 2, ('┳', 1.0): 1, ('┣', 1.0): 1, ('╰', 1.0): 3, ('╯', 1.0): 3, ('┗', 1.0): 2, ('┻', 1.0): 1, ('stool', 1.0): 1, ('toppl', 1.0): 1, ('findyourfit', 1.0): 1, ('prefer', 1.0): 2, ('whomosexu', 1.0): 1, ('stack', 1.0): 1, ('pandora', 1.0): 3, ('digitalexet', 1.0): 1, ('digitalmarket', 1.0): 1, ('sociamedia', 1.0): 1, ('nb', 1.0): 1, ('bom', 1.0): 1, ('dia', 1.0): 1, ('todo', 1.0): 1, ('forklift', 1.0): 1, ('warehous', 1.0): 1, ('worker', 1.0): 1, ('lsceen', 1.0): 1, ('immatur', 1.0): 1, ('gandhi', 1.0): 1, ('grassi', 1.0): 1, ('feetblog', 1.0): 2, ('daughter', 1.0): 3, ('4yr', 1.0): 1, ('old-porridg', 1.0): 1, ('fiend', 1.0): 1, ('2nite', 1.0): 1, ('comp', 1.0): 1, ('vike', 1.0): 1, ('t20blast', 1.0): 1, ('np', 1.0): 1, ('tax', 1.0): 1, ('ooohh', 1.0): 1, ('petjam', 1.0): 1, ('virtual', 1.0): 2, ('pounc', 1.0): 1, ('bentek', 1.0): 1, ('agn', 1.0): 1, ('socialmedia@dpdgroup.co.uk', 1.0): 1, ('sam', 1.0): 3, ('fruiti', 1.0): 1, ('vodka', 1.0): 2, ('sellyourcarin', 1.0): 2, ('5word', 1.0): 2, ('chaloniklo', 1.0): 2, ('pic.twitter.com/jxz2lbv6o', 1.0): 1, (\"paperwhite'\", 1.0): 1, ('laser-lik', 1.0): 1, ('focu', 1.0): 1, ('ghost', 1.0): 3, ('tagsforlikesapp', 1.0): 2, ('instagood', 1.0): 2, ('tbt', 1.0): 1, ('socket', 1.0): 1, ('spanner', 1.0): 1, ('😴', 1.0): 1, ('pglcsgo', 1.0): 1, ('x2', 1.0): 1, ('tend', 1.0): 1, ('crave', 1.0): 1, ('slower', 1.0): 1, ('sjw', 1.0): 1, ('cakehamp', 1.0): 1, ('glow', 1.0): 2, ('yayyy', 1.0): 1, ('merced', 1.0): 1, ('hood', 1.0): 1, ('badg', 1.0): 1, ('host', 1.0): 1, ('drone', 1.0): 1, ('blow', 1.0): 1, ('ignor', 1.0): 1, ('retali', 1.0): 1, ('bolling', 1.0): 1, (\"where'\", 1.0): 1, ('denmark', 1.0): 1, ('whitey', 1.0): 1, ('cultur', 1.0): 2, ('course', 1.0): 1, ('intro', 1.0): 2, ('graphicdesign', 1.0): 1, ('videograph', 1.0): 1, ('space', 1.0): 2, (\"ted'\", 1.0): 1, ('bogu', 1.0): 1, ('1000', 1.0): 1, ('hahahaaah', 1.0): 1, ('owli', 1.0): 1, ('afternon', 1.0): 1, ('whangarei', 1.0): 1, ('kati', 1.0): 2, ('paulin', 1.0): 1, ('traffick', 1.0): 1, ('wors', 1.0): 3, ('henc', 1.0): 1, ('express', 1.0): 1, ('wot', 1.0): 1, ('hand-lett', 1.0): 1, ('roof', 1.0): 1, ('eas', 1.0): 1, ('2/2', 1.0): 1, ('sour', 1.0): 1, ('dough', 1.0): 1, ('egypt', 1.0): 1, ('hubbi', 1.0): 2, ('sakin', 1.0): 1, ('six', 1.0): 1, ('christma', 1.0): 2, ('avril', 1.0): 1, ('n04j', 1.0): 1, ('25', 1.0): 1, ('prosecco', 1.0): 1, ('pech', 1.0): 1, ('micro', 1.0): 1, ('catspj', 1.0): 1, ('4:15', 1.0): 1, ('lazyweekend', 1.0): 1, ('overdu', 1.0): 1, ('mice', 1.0): 1, ('💃', 1.0): 3, ('jurass', 1.0): 1, ('ding', 1.0): 1, ('nila', 1.0): 1, ('8)', 1.0): 1, ('cooki', 1.0): 1, ('shir', 1.0): 1, ('0', 1.0): 3, ('hale', 1.0): 1, ('cheshir', 1.0): 1, ('decor', 1.0): 1, ('lemm', 1.0): 2, ('rec', 1.0): 1, ('ingat', 1.0): 1, ('din', 1.0): 2, ('mono', 1.0): 1, ('kathryn', 1.0): 1, ('jr', 1.0): 1, ('hsr', 1.0): 1, ('base', 1.0): 3, ('major', 1.0): 1, ('sugarrush', 1.0): 1, ('knit', 1.0): 1, ('partli', 1.0): 1, ('homegirl', 1.0): 1, ('nanci', 1.0): 1, ('fenja', 1.0): 1, ('aapk', 1.0): 1, ('benchmark', 1.0): 1, ('ke', 1.0): 1, ('hisaab', 1.0): 1, ('ho', 1.0): 1, ('gaya', 1.0): 1, ('ofc', 1.0): 1, ('rtss', 1.0): 1, ('hwait', 1.0): 1, ('titanfal', 1.0): 1, ('xbox', 1.0): 2, ('ultim', 1.0): 2, ('gastronomi', 1.0): 1, ('newblogpost', 1.0): 1, ('foodiefriday', 1.0): 1, ('foodi', 1.0): 1, ('yoghurt', 1.0): 1, ('pancak', 1.0): 2, ('sabah', 1.0): 3, ('kapima', 1.0): 1, ('gelen', 1.0): 1, ('guzel', 1.0): 1, ('bir', 1.0): 1, ('hediy', 1.0): 1, ('thanx', 1.0): 1, ('💞', 1.0): 2, ('visa', 1.0): 1, ('parisa', 1.0): 1, ('epiphani', 1.0): 1, ('lit', 1.0): 1, ('em-con', 1.0): 1, ('swore', 1.0): 1, ('0330 333 7234', 1.0): 1, ('kianweareproud', 1.0): 1, ('distract', 1.0): 1, ('dayofarch', 1.0): 1, ('10-20', 1.0): 1, ('bapu', 1.0): 1, ('ivypowel', 1.0): 1, ('newmus', 1.0): 1, ('sexchat', 1.0): 1, ('🍅', 1.0): 1, ('pathway', 1.0): 1, ('balkan', 1.0): 1, ('gypsi', 1.0): 1, ('mayhem', 1.0): 1, ('burek', 1.0): 1, ('meat', 1.0): 1, ('gibanica', 1.0): 1, ('pie', 1.0): 1, ('surrey', 1.0): 1, ('afterward', 1.0): 1, ('10.30', 1.0): 1, ('tempor', 1.0): 1, ('void', 1.0): 1, ('stem', 1.0): 1, ('sf', 1.0): 1, ('ykr', 1.0): 1, ('sparki', 1.0): 1, ('40mm', 1.0): 1, ('3.5', 1.0): 1, ('gr', 1.0): 1, ('rockfish', 1.0): 1, ('topwat', 1.0): 1, ('twitlong', 1.0): 1, ('me.so', 1.0): 1, ('jummah', 1.0): 3, ('durood', 1.0): 1, ('pak', 1.0): 1, ('cjradacomateada', 1.0): 2, ('supris', 1.0): 1, ('debut', 1.0): 1, ('shipper', 1.0): 1, ('asid', 1.0): 1, ('housem', 1.0): 1, ('737bigatingconcert', 1.0): 1, ('jedzjabłka', 1.0): 1, ('pijjabłka', 1.0): 1, ('polish', 1.0): 1, ('cider', 1.0): 1, ('mustread', 1.0): 1, ('cricket', 1.0): 1, ('5pm', 1.0): 1, ('queri', 1.0): 2, ('abbi', 1.0): 1, ('sumedh', 1.0): 1, ('sunnah', 1.0): 2, ('عن', 1.0): 2, ('quad', 1.0): 1, ('bike', 1.0): 1, ('carri', 1.0): 2, ('proprieti', 1.0): 1, ('chronic', 1.0): 1, ('superday', 1.0): 1, ('chocolatey', 1.0): 1, ('yasu', 1.0): 1, ('ooooh', 1.0): 1, ('hallo', 1.0): 2, ('dylan', 1.0): 2, ('laura', 1.0): 1, ('patric', 1.0): 2, ('keepin', 1.0): 1, ('mohr', 1.0): 1, ('guest', 1.0): 1, (\"o'neal\", 1.0): 1, ('tk', 1.0): 1, ('lua', 1.0): 1, ('stone', 1.0): 2, ('quicker', 1.0): 1, ('diet', 1.0): 1, ('sosweet', 1.0): 1, ('nominier', 1.0): 1, ('und', 1.0): 1, ('hardcor', 1.0): 1, ('😌', 1.0): 1, ('ff__special', 1.0): 1, ('acha', 1.0): 2, ('banda', 1.0): 1, ('✌', 1.0): 1, ('bhi', 1.0): 2, ('krta', 1.0): 1, ('beautifully-craft', 1.0): 1, ('mockingbird', 1.0): 1, ('diploma', 1.0): 1, ('blend', 1.0): 3, ('numbero', 1.0): 1, ('lolz', 1.0): 1, ('ambros', 1.0): 1, ('gwinett', 1.0): 1, ('bierc', 1.0): 1, ('ravag', 1.0): 1, ('illadvis', 1.0): 1, ('marriag', 1.0): 1, ('stare', 1.0): 1, ('cynic', 1.0): 2, ('yahuda', 1.0): 1, ('nosmet', 1.0): 1, ('poni', 1.0): 1, ('cuuut', 1.0): 1, (\"f'ing\", 1.0): 1, ('vacant', 1.0): 1, ('hauc', 1.0): 1, ('lovesss', 1.0): 1, ('hiss', 1.0): 1, ('overnight', 1.0): 1, ('cornish', 1.0): 1, ('all-clear', 1.0): 1, ('raincoat', 1.0): 1, ('measur', 1.0): 1, ('wealth', 1.0): 1, ('invest', 1.0): 2, ('garbi', 1.0): 1, ('wash', 1.0): 2, ('refuel', 1.0): 1, ('dunedin', 1.0): 1, ('kall', 1.0): 1, ('rakhi', 1.0): 1, ('12th', 1.0): 2, ('repres', 1.0): 3, ('slovenia', 1.0): 1, ('fridg', 1.0): 2, ('ludlow', 1.0): 1, ('28th', 1.0): 1, ('selway', 1.0): 1, ('submit', 1.0): 1, ('spanish', 1.0): 2, ('90210', 1.0): 1, ('oitnb', 1.0): 1, ('prepar', 1.0): 3, ('condit', 1.0): 1, ('msged', 1.0): 1, ('chiquito', 1.0): 1, ('ohaha', 1.0): 1, ('delhi', 1.0): 1, ('95', 1.0): 1, ('webtogsaward', 1.0): 1, ('grace', 1.0): 2, ('sheffield', 1.0): 1, ('tramlin', 1.0): 1, ('tl', 1.0): 2, ('hack', 1.0): 1, ('lad', 1.0): 1, ('beeepin', 1.0): 1, ('duper', 1.0): 1, ('handl', 1.0): 1, ('critiqu', 1.0): 1, ('contectu', 1.0): 1, ('ultor', 1.0): 2, ('mamaya', 1.0): 1, ('loiyal', 1.0): 1, ('para', 1.0): 1, ('truthfulwordsof', 1.0): 1, ('beanatividad', 1.0): 1, ('nknkkpagpapakumbaba', 1.0): 1, ('birthdaypres', 1.0): 1, ('compliment', 1.0): 1, ('swerv', 1.0): 1, ('goodtim', 1.0): 1, ('sinist', 1.0): 1, ('scare', 1.0): 1, ('tryna', 1.0): 1, ('anonym', 1.0): 1, ('dipsatch', 1.0): 1, ('aunt', 1.0): 1, ('dagga', 1.0): 1, ('burket', 1.0): 1, ('2am', 1.0): 1, ('twine', 1.0): 1, (\"diane'\", 1.0): 1, ('happybirthday', 1.0): 1, ('thanksss', 1.0): 1, ('randomli', 1.0): 1, ('buckinghampalac', 1.0): 1, ('chibi', 1.0): 1, ('maker', 1.0): 1, ('timog', 1.0): 1, ('18th', 1.0): 1, ('otw', 1.0): 1, ('kami', 1.0): 1, ('feelinggood', 1.0): 1, ('demand', 1.0): 2, ('naman', 1.0): 1, ('barkin', 1.0): 1, ('yeap', 1.0): 2, ('onkey', 1.0): 1, ('umma', 1.0): 1, ('pervert', 1.0): 1, ('onyu', 1.0): 1, ('appa', 1.0): 1, ('luci', 1.0): 1, ('horribl', 1.0): 1, ('quantum', 1.0): 1, ('greater', 1.0): 1, ('blockchain', 1.0): 1, ('nowplay', 1.0): 1, ('loftey', 1.0): 1, ('routt', 1.0): 1, ('assia', 1.0): 1, ('.\\n.\\n.', 1.0): 1, ('joint', 1.0): 1, ('futurereleas', 1.0): 1, (\"look'\", 1.0): 1, ('scari', 1.0): 1, ('murder', 1.0): 1, ('mysteri', 1.0): 1, ('comma', 1.0): 1, (\"j'\", 1.0): 1, ('hunni', 1.0): 2, ('diva', 1.0): 1, ('emili', 1.0): 3, ('nathan', 1.0): 1, ('medit', 1.0): 1, ('alumni', 1.0): 1, ('mba', 1.0): 1, ('foto', 1.0): 1, ('what-is-your-fashion', 1.0): 1, ('lorenangel', 1.0): 1, ('kw', 1.0): 2, ('tellanoldjokeday', 1.0): 1, ('reqd', 1.0): 1, ('specul', 1.0): 1, ('consist', 1.0): 4, ('tropic', 1.0): 1, ('startupph', 1.0): 1, ('zodiac', 1.0): 1, ('rapunzel', 1.0): 1, ('therver', 1.0): 1, ('85552', 1.0): 1, ('bestoftheday', 1.0): 1, ('oralsex', 1.0): 1, ('carli', 1.0): 1, ('happili', 1.0): 1, ('contract', 1.0): 1, ('matsu_bouzu', 1.0): 1, ('sonic', 1.0): 2, ('videogam', 1.0): 1, ('harana', 1.0): 1, ('belfast', 1.0): 1, ('danni', 1.0): 1, ('rare', 1.0): 1, ('sponsorship', 1.0): 1, ('aswel', 1.0): 1, ('gigi', 1.0): 1, ('nick', 1.0): 1, ('austin', 1.0): 1, ('youll', 1.0): 1, ('weak', 1.0): 4, ('10,000', 1.0): 1, ('bravo', 1.0): 1, ('iamamonst', 1.0): 1, ('rxthedailysurveyvot', 1.0): 1, ('broke', 1.0): 1, ('ass', 1.0): 1, ('roux', 1.0): 1, ('walkin', 1.0): 1, ('audienc', 1.0): 2, ('pfb', 1.0): 1, ('jute', 1.0): 1, ('walangmakakapigilsakin', 1.0): 1, ('lori', 1.0): 1, ('ehm', 1.0): 1, ('trick', 1.0): 1, ('baekhyun', 1.0): 1, ('eyesmil', 1.0): 1, ('borrow', 1.0): 1, ('knive', 1.0): 1, ('thek', 1.0): 1, ('eventu', 1.0): 1, ('reaapear', 1.0): 1, ('kno', 1.0): 1, ('whet', 1.0): 1, ('gratti', 1.0): 1, ('shorter', 1.0): 1, ('tweetin', 1.0): 1, ('inshallah', 1.0): 1, ('banana', 1.0): 1, ('raspberri', 1.0): 2, ('healthylifestyl', 1.0): 1, ('aint', 1.0): 2, ('skate', 1.0): 1, ('analyz', 1.0): 1, ('varieti', 1.0): 1, ('4:13', 1.0): 1, ('insomnia', 1.0): 1, ('medic', 1.0): 1, ('opposit', 1.0): 1, ('everlast', 1.0): 1, ('yoga', 1.0): 1, ('massag', 1.0): 2, ('osteopath', 1.0): 1, ('trainer', 1.0): 1, ('sharm', 1.0): 1, ('al_master_band', 1.0): 1, ('tbc', 1.0): 1, ('unives', 1.0): 1, ('architectur', 1.0): 1, ('random', 1.0): 1, ('isnt', 1.0): 1, ('typo', 1.0): 1, ('snark', 1.0): 1, ('lession', 1.0): 1, ('drunk', 1.0): 1, ('bruuh', 1.0): 1, ('2week', 1.0): 1, ('50europ', 1.0): 1, ('🇫🇷', 1.0): 4, ('iov', 1.0): 1, ('accord', 1.0): 1, ('mne', 1.0): 1, ('pchelok', 1.0): 1, ('ja', 1.0): 1, ('=:', 1.0): 2, ('sweetest', 1.0): 1, ('comet', 1.0): 1, ('ahah', 1.0): 1, ('candi', 1.0): 2, ('axio', 1.0): 1, ('rabbit', 1.0): 2, ('nutshel', 1.0): 1, ('taken', 1.0): 1, ('letshavecocktailsafternuclai', 1.0): 1, ('malik', 1.0): 1, ('umair', 1.0): 1, ('canon', 1.0): 1, ('gang', 1.0): 1, ('grind', 1.0): 1, ('thoracicbridg', 1.0): 1, ('5minut', 1.0): 1, ('nonscript', 1.0): 1, ('password', 1.0): 1, ('shoshannavassil', 1.0): 1, ('addmeonsnapchat', 1.0): 1, ('dmme', 1.0): 1, ('mpoint', 1.0): 2, ('soph', 1.0): 1, ('anot', 1.0): 1, ('liao', 1.0): 2, ('ord', 1.0): 1, ('lor', 1.0): 1, ('sibei', 1.0): 1, ('xialan', 1.0): 1, ('thnx', 1.0): 1, ('malfunct', 1.0): 1, ('clown', 1.0): 1, ('joker', 1.0): 1, ('\\U000fec00', 1.0): 1, ('nigth', 1.0): 1, ('estoy', 1.0): 1, ('escuchando', 1.0): 1, ('elsewher', 1.0): 1, ('bipolar', 1.0): 1, ('hahahahahahahahahahahahahaha', 1.0): 1, ('yoohoo', 1.0): 1, ('bajrangibhaijaanstorm', 1.0): 1, ('superhappi', 1.0): 1, ('doll', 1.0): 1, ('energi', 1.0): 1, ('f', 1.0): 3, (\"m'dear\", 1.0): 1, ('emma', 1.0): 2, ('alrd', 1.0): 1, ('dhan', 1.0): 2, ('satguru', 1.0): 1, ('tera', 1.0): 1, ('aasra', 1.0): 1, ('pita', 1.0): 1, ('keeo', 1.0): 1, ('darl', 1.0): 2, ('akarshan', 1.0): 1, ('sweetpea', 1.0): 1, ('gluten', 1.0): 1, ('pastri', 1.0): 2, ('highfiv', 1.0): 1, ('artsi', 1.0): 1, ('verbal', 1.0): 1, ('kaaa', 1.0): 1, ('oxford', 1.0): 2, ('wahoo', 1.0): 1, ('anchor', 1.0): 1, ('partnership', 1.0): 1, ('robbenisland', 1.0): 1, ('whale', 1.0): 1, ('aquat', 1.0): 1, ('safari', 1.0): 1, ('garru', 1.0): 1, ('liara', 1.0): 1, ('appoint', 1.0): 1, ('burnley', 1.0): 1, ('453', 1.0): 1, ('110', 1.0): 2, ('49', 1.0): 1, ('footbal', 1.0): 1, ('fm15', 1.0): 1, ('fmfamili', 1.0): 1, ('aamir', 1.0): 1, ('difficult', 1.0): 1, ('medium', 1.0): 1, ('nva', 1.0): 1, ('minuet', 1.0): 1, ('gamec', 1.0): 1, ('headrest', 1.0): 1, ('pit', 1.0): 1, ('spoken', 1.0): 1, ('advis', 1.0): 1, ('paypoint', 1.0): 1, ('deepthroat', 1.0): 1, ('truli', 1.0): 3, ('bee', 1.0): 2, ('upward', 1.0): 1, ('bound', 1.0): 1, ('movingonup', 1.0): 1, ('aitor', 1.0): 1, ('sn', 1.0): 1, ('ps4', 1.0): 2, ('jawad', 1.0): 1, ('presal', 1.0): 1, ('betcha', 1.0): 1, ('dumb', 1.0): 2, ('butt', 1.0): 1, ('qualki', 1.0): 1, ('808', 1.0): 1, ('milf', 1.0): 1, ('4like', 1.0): 1, ('sexysaturday', 1.0): 1, ('vw', 1.0): 1, ('umpfff', 1.0): 1, ('ca', 1.0): 1, ('domg', 1.0): 1, ('nanti', 1.0): 1, ('difollow', 1.0): 1, ('stubborn', 1.0): 1, ('nothavingit', 1.0): 1, ('klee', 1.0): 1, ('hem', 1.0): 1, ('congrad', 1.0): 1, ('accomplish', 1.0): 1, ('kfcroleplay', 1.0): 3, ('tregaron', 1.0): 1, ('boar', 1.0): 1, ('sweati', 1.0): 1, ('glyon', 1.0): 1, ('🚮', 1.0): 1, (\"tee'\", 1.0): 1, ('johnni', 1.0): 1, ('utub', 1.0): 1, (\"video'\", 1.0): 1, ('loss', 1.0): 1, ('combin', 1.0): 2, ('pigeon', 1.0): 1, ('fingerscross', 1.0): 1, ('photobomb', 1.0): 1, ('90', 1.0): 1, ('23', 1.0): 1, ('gimm', 1.0): 1, ('definetli', 1.0): 1, ('exit', 1.0): 1, ('bom-dia', 1.0): 1, ('apod', 1.0): 1, ('ultraviolet', 1.0): 1, ('m31', 1.0): 1, ('jul', 1.0): 1, ('oooh', 1.0): 1, ('yawn', 1.0): 1, ('ftw', 1.0): 1, ('maman', 1.0): 1, ('afterznoon', 1.0): 1, ('tweeep', 1.0): 1, ('abp', 1.0): 2, ('kiya', 1.0): 1, ('van', 1.0): 1, ('olymp', 1.0): 1, ('😷', 1.0): 1, ('classi', 1.0): 1, ('attach', 1.0): 1, ('equip', 1.0): 1, ('bobbl', 1.0): 1, ('anu', 1.0): 1, ('mh3', 1.0): 1, ('patch', 1.0): 1, ('psp', 1.0): 1, ('huffpost', 1.0): 1, ('tribut', 1.0): 1, ('h_eartshapedbox', 1.0): 1, ('magictrikband', 1.0): 1, ('magictrik', 1.0): 2, ('roommat', 1.0): 1, ('tami', 1.0): 1, ('b3dk', 1.0): 1, ('7an', 1.0): 1, ('ank', 1.0): 1, ('purpos', 1.0): 1, ('struggl', 1.0): 1, ('eagl', 1.0): 1, ('oceana', 1.0): 1, ('idk', 1.0): 3, ('med', 1.0): 1, ('fridayfauxpa', 1.0): 1, ('subtl', 1.0): 1, ('hint', 1.0): 1, ('prim', 1.0): 1, ('algorithm', 1.0): 1, ('iii', 1.0): 1, ('rosa', 1.0): 1, ('yvw', 1.0): 1, ('here', 1.0): 1, ('boost', 1.0): 1, ('unforgett', 1.0): 1, ('humor', 1.0): 1, (\"mum'\", 1.0): 1, ('hahahhaah', 1.0): 1, ('sombrero', 1.0): 1, ('lost', 1.0): 2, ('spammer', 1.0): 1, ('proceed', 1.0): 1, ('entertain', 1.0): 1, ('100k', 1.0): 1, ('mileston', 1.0): 1, ('judith', 1.0): 1, ('district', 1.0): 1, ('council', 1.0): 1, ('midar', 1.0): 1, ('gender', 1.0): 1, ('ilysm', 1.0): 1, ('zen', 1.0): 1, ('neat', 1.0): 1, ('rider', 1.0): 1, ('fyi', 1.0): 1, ('dig', 1.0): 2, ('👱🏽', 1.0): 1, ('👽', 1.0): 1, ('🌳', 1.0): 1, ('suspici', 1.0): 1, ('calori', 1.0): 1, ('harder', 1.0): 1, ('jessica', 1.0): 1, ('carina', 1.0): 1, ('francisco', 1.0): 1, ('teret', 1.0): 1, ('potassium', 1.0): 1, ('rehydr', 1.0): 1, ('drinkitallup', 1.0): 1, ('thirstquench', 1.0): 1, ('tapir', 1.0): 1, ('calf', 1.0): 1, ('mealtim', 1.0): 1, ('uhc', 1.0): 1, ('scale', 1.0): 1, ('network', 1.0): 1, ('areal', 1.0): 1, ('extremesport', 1.0): 1, ('quadbik', 1.0): 1, ('bloggersrequir', 1.0): 1, ('bloggersw', 1.0): 1, ('brainer', 1.0): 1, ('mse', 1.0): 1, ('fund', 1.0): 1, ('nooowww', 1.0): 1, ('lile', 1.0): 1, ('tid', 1.0): 1, ('tmi', 1.0): 1, ('deploy', 1.0): 1, ('jule', 1.0): 1, ('betti', 1.0): 1, ('hddc', 1.0): 1, ('salman', 1.0): 1, ('pthht', 1.0): 1, ('lfc', 1.0): 3, ('tope', 1.0): 1, ('xxoo', 1.0): 2, ('russia', 1.0): 2, ('silver-wash', 1.0): 1, ('fritillari', 1.0): 1, ('moon', 1.0): 1, ('ap', 1.0): 2, ('trash', 1.0): 2, ('clever', 1.0): 1, (\"thank'\", 1.0): 1, ('keven', 1.0): 1, ('pastim', 1.0): 1, ('ashramcal', 1.0): 1, ('ontrack', 1.0): 1, ('german', 1.0): 1, ('subtitl', 1.0): 1, ('pinter', 1.0): 1, ('morninggg', 1.0): 1, ('🐶', 1.0): 1, ('pete', 1.0): 1, ('awesome-o', 1.0): 1, ('multipl', 1.0): 1, ('cya', 1.0): 1, ('harrog', 1.0): 1, ('jet', 1.0): 1, ('supplier', 1.0): 1, ('req', 1.0): 1, ('fridayloug', 1.0): 1, ('4thstreetmus', 1.0): 1, ('hawaii', 1.0): 1, ('kick', 1.0): 1, ('deepli', 1.0): 1, ('john@timney.eclipse.co.uk', 1.0): 1, ('thousand', 1.0): 2, ('newspap', 1.0): 1, ('lew', 1.0): 1, ('nah', 1.0): 1, ('fallout', 1.0): 2, ('technic', 1.0): 1, ('gunderson', 1.0): 1, ('europa', 1.0): 1, ('thoroughli', 1.0): 1, ('script', 1.0): 1, ('overtak', 1.0): 1, ('motorway', 1.0): 1, ('thu', 1.0): 1, ('niteflirt', 1.0): 1, ('hbu', 1.0): 2, ('bowl', 1.0): 1, ('chri', 1.0): 2, ('niall', 1.0): 2, ('94', 1.0): 1, ('ik', 1.0): 1, ('stydia', 1.0): 1, ('nawazuddin', 1.0): 1, ('siddiqu', 1.0): 1, ('nomnomnom', 1.0): 1, ('dukefreebiefriday', 1.0): 1, ('z', 1.0): 1, ('insyaallah', 1.0): 1, ('ham', 1.0): 1, ('villa', 1.0): 1, ('brum', 1.0): 1, ('deni', 1.0): 1, ('vagina', 1.0): 1, ('rli', 1.0): 1, ('izzi', 1.0): 1, ('mitch', 1.0): 1, ('minn', 1.0): 1, ('recently.websit', 1.0): 1, ('coolingtow', 1.0): 1, ('soon.thank', 1.0): 1, ('showinginterest', 1.0): 1, ('multicolor', 1.0): 1, ('wid', 1.0): 1, ('wedg', 1.0): 1, ('motiv', 1.0): 1, ('nnnnot', 1.0): 1, (\"gf'\", 1.0): 1, ('bluesidemenxix', 1.0): 1, ('ardent', 1.0): 1, ('mooorn', 1.0): 1, ('wuppert', 1.0): 1, ('fridayfunday', 1.0): 1, ('re-sign', 1.0): 1, ('chalkhil', 1.0): 1, ('midday', 1.0): 1, ('carter', 1.0): 1, ('remedi', 1.0): 1, ('atrack', 1.0): 1, ('christ', 1.0): 1, ('badminton', 1.0): 1, (\"littl'un\", 1.0): 1, ('ikprideofpak', 1.0): 1, ('janjua', 1.0): 1, ('pimpl', 1.0): 1, ('forehead', 1.0): 1, ('volcano', 1.0): 1, ('mag', 1.0): 1, ('miryenda', 1.0): 1, (\"technology'\", 1.0): 1, ('touchétoday', 1.0): 1, ('idownload', 1.0): 1, ('25ish', 1.0): 1, ('snowbal', 1.0): 1, ('nd', 1.0): 1, ('expir', 1.0): 1, ('6gb', 1.0): 1, ('loveu', 1.0): 1, ('morefuninthephilippin', 1.0): 1, ('laho', 1.0): 1, ('caramoan', 1.0): 1, ('kareem', 1.0): 1, ('surah', 1.0): 1, ('kahaf', 1.0): 1, ('melani', 1.0): 1, ('bosch', 1.0): 1, ('machin', 1.0): 1, (\"week'\", 1.0): 1, ('refollow', 1.0): 1, ('😎', 1.0): 1, ('💁🏻', 1.0): 1, ('relaps', 1.0): 1, ('prada', 1.0): 2, ('punjabiswillgetit', 1.0): 1, ('hitter', 1.0): 1, ('mass', 1.0): 2, ('shoud', 1.0): 1, ('1:12', 1.0): 1, ('ughtm', 1.0): 1, ('545', 1.0): 1, ('kissm', 1.0): 1, ('likeforfollow', 1.0): 1, ('overwhelm', 1.0): 1, ('groupmat', 1.0): 1, ('75', 1.0): 2, ('kyunk', 1.0): 1, ('aitchison', 1.0): 1, ('curvi', 1.0): 1, ('mont', 1.0): 1, ('doa', 1.0): 1, ('header', 1.0): 1, ('speaker', 1.0): 3, ('avoid', 1.0): 1, ('laboratori', 1.0): 1, ('idc', 1.0): 1, ('fuckin', 1.0): 2, ('wooo', 1.0): 2, ('neobyt', 1.0): 1, ('pirat', 1.0): 1, ('takedown', 1.0): 1, ('indirag', 1.0): 1, ('judiciari', 1.0): 1, ('commit', 1.0): 4, ('govt', 1.0): 1, ('polici', 1.0): 1, ('rbi', 1.0): 1, ('similar', 1.0): 1, (\"thought'\", 1.0): 1, ('progress', 1.0): 1, ('transfer', 1.0): 1, ('gg', 1.0): 1, ('defenit', 1.0): 1, ('nofx', 1.0): 1, ('friskyfiday', 1.0): 1, ('yipee', 1.0): 1, ('shed', 1.0): 1, ('incent', 1.0): 1, ('vege', 1.0): 1, ('marin', 1.0): 1, ('gz', 1.0): 1, ('rajeev', 1.0): 1, ('hvng', 1.0): 1, ('funfil', 1.0): 1, ('friday.it', 1.0): 1, ('ws', 1.0): 1, ('reali', 1.0): 1, ('diff', 1.0): 1, ('kabir.fel', 1.0): 1, ('dresden', 1.0): 1, ('germani', 1.0): 1, ('plot', 1.0): 1, ('tdf', 1.0): 1, ('🍷', 1.0): 2, ('☀', 1.0): 2, ('🚲', 1.0): 2, ('minion', 1.0): 2, ('slot', 1.0): 1, (\"b'day\", 1.0): 1, ('isabella', 1.0): 1, ('okeyyy', 1.0): 1, ('vddd', 1.0): 1, (');', 1.0): 1, ('selfee', 1.0): 1, ('insta', 1.0): 1, ('🙆', 1.0): 1, ('🙌', 1.0): 1, ('😛', 1.0): 1, ('🐒', 1.0): 1, ('😝', 1.0): 1, ('hhahhaaa', 1.0): 1, ('jeez', 1.0): 1, ('teamcannib', 1.0): 1, ('teamspacewhalingisthebest', 1.0): 1, ('fitfa', 1.0): 1, ('identifi', 1.0): 1, ('pharmaci', 1.0): 1, ('verylaterealis', 1.0): 1, ('iwishiknewbett', 1.0): 1, ('satisfi', 1.0): 1, ('ess-aych-eye-te', 1.0): 1, ('supposedli', 1.0): 1, ('👍', 1.0): 1, ('immedi', 1.0): 1, (\"foxy'\", 1.0): 1, ('instrument', 1.0): 1, ('alon', 1.0): 2, ('goldcoast', 1.0): 1, ('lelomustfal', 1.0): 1, ('meal', 1.0): 1, ('5g', 1.0): 1, ('liker', 1.0): 1, ('newdress', 1.0): 1, ('resist', 1.0): 1, ('fot', 1.0): 1, ('troy', 1.0): 1, ('twitterfollowerswhatsup', 1.0): 1, ('happyfriedday', 1.0): 1, ('keepsafealway', 1.0): 1, ('loveyeah', 1.0): 1, ('emojasp_her', 1.0): 1, ('vanilla', 1.0): 1, ('sidemen', 1.0): 1, ('yaaayyy', 1.0): 1, ('friendaaa', 1.0): 1, ('bulb', 1.0): 5, ('corn', 1.0): 6, ('1tbps4', 1.0): 1, ('divin', 1.0): 1, ('wheeli', 1.0): 1, ('bin', 1.0): 1, ('ubericecream', 1.0): 1, ('messengerforaday', 1.0): 1, ('kyli', 1.0): 1, ('toilet', 1.0): 1, ('ikaw', 1.0): 1, ('musta', 1.0): 1, ('cheatmat', 1.0): 1, ('kyuhyun', 1.0): 1, ('ghanton', 1.0): 1, ('easy.get', 1.0): 1, ('5:30', 1.0): 1, ('therein', 1.0): 1, ('majalah', 1.0): 1, ('dominiqu', 1.0): 1, ('lamp', 1.0): 1, ('a-foot', 1.0): 1, ('revamp', 1.0): 1, ('brainchild', 1.0): 1, ('confid', 1.0): 1, ('confin', 1.0): 1, ('colorado', 1.0): 1, ('goodyear', 1.0): 1, ('upto', 1.0): 1, ('cashback', 1.0): 1, ('yourewelcom', 1.0): 1, ('nightli', 1.0): 1, ('simpin', 1.0): 1, ('sketchbook', 1.0): 1, ('4wild', 1.0): 1, ('colorpencil', 1.0): 1, ('cray', 1.0): 1, ('6:30', 1.0): 1, ('imma', 1.0): 3, ('ob', 1.0): 1, ('11h', 1.0): 1, ('kino', 1.0): 1, ('adult', 1.0): 1, ('kardamena', 1.0): 1, ('samo', 1.0): 1, ('greec', 1.0): 1, ('caesar', 1.0): 1, ('salad', 1.0): 1, ('tad', 1.0): 1, ('bland', 1.0): 1, ('respond', 1.0): 1, ('okk', 1.0): 1, ('den', 1.0): 1, ('allov', 1.0): 1, ('hangout', 1.0): 1, ('whoever', 1.0): 1, ('tourist', 1.0): 1, ('♌', 1.0): 1, ('kutiyapanti', 1.0): 1, ('profession', 1.0): 1, ('boomshot', 1.0): 1, ('fuh', 1.0): 1, ('yeeey', 1.0): 1, ('donot', 1.0): 1, ('expos', 1.0): 1, ('lipstick', 1.0): 1, ('cran', 1.0): 1, ('prayr', 1.0): 1, ('හෙල', 1.0): 1, ('හවුල', 1.0): 1, ('onemochaonelov', 1.0): 1, ('southpaw', 1.0): 1, ('geniu', 1.0): 1, ('stroma', 1.0): 1, ('🔴', 1.0): 1, ('younow', 1.0): 1, ('jonah', 1.0): 1, ('jareddd', 1.0): 1, ('postcod', 1.0): 1, ('talkmobil', 1.0): 1, ('huha', 1.0): 1, ('transform', 1.0): 1, ('sword', 1.0): 3, ('misread', 1.0): 1, ('richard', 1.0): 1, ('ibiza', 1.0): 1, ('birthdaymoneyforjesusjuic', 1.0): 1, ('ytb', 1.0): 1, ('tutori', 1.0): 1, ('construct', 1.0): 2, ('critic', 1.0): 1, ('ganesha', 1.0): 1, ('textur', 1.0): 1, ('photographi', 1.0): 1, ('hinduism', 1.0): 1, ('hindugod', 1.0): 1, ('elephantgod', 1.0): 1, ('selfish', 1.0): 1, ('bboy', 1.0): 1, ('cardgam', 1.0): 1, ('pixelart', 1.0): 1, ('gamedesign', 1.0): 1, ('indiedev', 1.0): 1, ('pixel_daili', 1.0): 1, ('plateau', 1.0): 1, ('laguna', 1.0): 1, ('tha', 1.0): 4, ('bahot', 1.0): 1, ('baje', 1.0): 1, ('raat', 1.0): 1, ('liya', 1.0): 1, ('hath', 1.0): 1, ('ghant', 1.0): 1, ('itna', 1.0): 2, ('bana', 1.0): 1, ('paya', 1.0): 1, ('uta', 1.0): 1, ('manga', 1.0): 1, ('jamuna', 1.0): 1, ('\\\\:', 1.0): 1, ('swiftma', 1.0): 1, ('trion', 1.0): 1, ('forum', 1.0): 1, ('b-day', 1.0): 1, ('disgust', 1.0): 1, ('commodor', 1.0): 1, ('annabel', 1.0): 1, ('bridg', 1.0): 1, ('quest', 1.0): 1, ('borderland', 1.0): 1, ('wanderrook', 1.0): 1, ('gm', 1.0): 1, ('preciou', 1.0): 2, ('mizz', 1.0): 1, ('bleedgreen', 1.0): 1, ('✌🏻', 1.0): 1, ('sophia', 1.0): 1, ('chicago', 1.0): 1, ('honeymoon', 1.0): 1, (\"da'esh\", 1.0): 1, ('co-ord', 1.0): 1, ('fsa', 1.0): 1, ('estat', 1.0): 1, (\"when'\", 1.0): 1, ('dusti', 1.0): 1, ('tunisia', 1.0): 2, (\"class'\", 1.0): 1, ('irrit', 1.0): 1, ('fiverr', 1.0): 1, ('gina', 1.0): 1, ('soproud', 1.0): 1, ('enought', 1.0): 1, ('hole', 1.0): 1, ('melbourneburg', 1.0): 1, ('arianna', 1.0): 1, ('esai', 1.0): 1, ('rotterdam', 1.0): 1, ('jordi', 1.0): 1, ('clasi', 1.0): 1, ('horni', 1.0): 1, ('salon', 1.0): 1, ('bleach', 1.0): 1, ('olaplex', 1.0): 1, ('damag', 1.0): 1, ('teamwork', 1.0): 1, ('zitecofficestori', 1.0): 1, ('다쇼', 1.0): 1, ('colleagu', 1.0): 1, ('eb', 1.0): 1, (\"t'would\", 1.0): 1, ('tweetup', 1.0): 1, ('detect', 1.0): 1, ('jonathancreek', 1.0): 1, ('dvr', 1.0): 1, ('kat', 1.0): 1, ('rarer', 1.0): 1, ('okkk', 1.0): 1, ('frend', 1.0): 1, ('milt', 1.0): 1, ('mario', 1.0): 1, ('rewatch', 1.0): 1, ('1600', 1.0): 1, ('sige', 1.0): 1, ('punta', 1.0): 1, ('kayo', 1.0): 1, ('nooo', 1.0): 1, ('prompt', 1.0): 1, ('t-mobil', 1.0): 1, ('orang', 1.0): 1, ('ee', 1.0): 1, ('teapot', 1.0): 1, ('hotter', 1.0): 1, ('»', 1.0): 1, ('londoutrad', 1.0): 1, ('kal', 1.0): 1, ('wayward', 1.0): 1, ('pine', 1.0): 1, ('muscl', 1.0): 1, ('ilikeit', 1.0): 1, ('belong', 1.0): 1, ('watford', 1.0): 1, ('enterpris', 1.0): 1, ('cube', 1.0): 1, ('particp', 1.0): 1, ('saudi', 1.0): 1, ('arabia', 1.0): 1, ('recogn', 1.0): 1, ('fanbas', 1.0): 3, ('bailona', 1.0): 3, ('responsibilti', 1.0): 1, ('sunlight', 1.0): 1, ('tiger', 1.0): 1, ('elev', 1.0): 1, ('horror', 1.0): 1, ('bitchesss', 1.0): 1, ('shitti', 1.0): 1, ('squash', 1.0): 1, ('becca', 1.0): 1, ('delta', 1.0): 1, ('nut', 1.0): 1, ('yun', 1.0): 1, ('joe', 1.0): 1, ('dirt', 1.0): 1, ('sharon', 1.0): 1, ('medicin', 1.0): 1, ('ttyl', 1.0): 1, ('gav', 1.0): 1, ('linda', 1.0): 1, ('3hr', 1.0): 1, ('tym', 1.0): 2, ('dieback', 1.0): 1, ('endit', 1.0): 1, ('minecon', 1.0): 1, ('sere', 1.0): 1, ('joerin', 1.0): 1, ('joshan', 1.0): 1, ('tandem', 1.0): 1, ('ligao', 1.0): 1, ('albay', 1.0): 1, ('bcyc', 1.0): 1, ('lnh', 1.0): 1, ('sat', 1.0): 1, ('honorari', 1.0): 1, ('alac', 1.0): 1, ('skelo_ghost', 1.0): 1, ('madadagdagan', 1.0): 1, ('bmc', 1.0): 1, ('11:11', 1.0): 2, ('embarrass', 1.0): 1, ('entropi', 1.0): 1, ('evolut', 1.0): 2, ('loop', 1.0): 1, ('eva', 1.0): 1, ('camden', 1.0): 1, ('uhh', 1.0): 1, ('scoup', 1.0): 1, ('jren', 1.0): 1, ('nuest', 1.0): 1, ('lovelayyy', 1.0): 1, ('kidney', 1.0): 1, ('neuer', 1.0): 1, ('spray', 1.0): 1, ('donnae.strydom@westerncape.gov.za', 1.0): 1, ('uni', 1.0): 1, ('uff', 1.0): 1, ('karhi', 1.0): 1, ('thi', 1.0): 1, ('juaquin', 1.0): 1, ('v3nzor99', 1.0): 1, ('shell', 1.0): 1, ('heyi', 1.0): 1, ('flavor', 1.0): 1, ('thakyou', 1.0): 1, ('beatriz', 1.0): 1, ('cancel', 1.0): 1, ('puff', 1.0): 1, ('egg', 1.0): 2, ('tart', 1.0): 1, ('chai', 1.0): 1, ('mtr', 1.0): 1, ('alyssa', 1.0): 1, ('rub', 1.0): 1, ('tummi', 1.0): 1, ('zelda', 1.0): 1, ('ive', 1.0): 1, ('🎂', 1.0): 1, ('jiva', 1.0): 1, ('🍹', 1.0): 1, ('🍻', 1.0): 1, ('mubbarak', 1.0): 1, ('deborah', 1.0): 1, ('coupon', 1.0): 1, ('colourdeb', 1.0): 1, ('purpl', 1.0): 1, (\"chippy'\", 1.0): 1, ('vessel', 1.0): 1, ('ps', 1.0): 2, ('vintag', 1.0): 1, ('✫', 1.0): 4, ('˚', 1.0): 4, ('·', 1.0): 4, ('✵', 1.0): 4, ('⊹', 1.0): 4, ('1710', 1.0): 1, ('gooffeanotter', 1.0): 1, ('kiksex', 1.0): 1, ('mugshot', 1.0): 1, ('token', 1.0): 1, ('maritimen', 1.0): 1, ('rh', 1.0): 1, ('tatton', 1.0): 1, ('jump_julia', 1.0): 1, ('malema', 1.0): 1, ('fren', 1.0): 1, ('nuf', 1.0): 1, ('teas', 1.0): 1, ('alien', 1.0): 2, ('closer', 1.0): 1, ('monitor', 1.0): 1, ('kimmi', 1.0): 1, (\"channel'\", 1.0): 1, ('planetbollywoodnew', 1.0): 1, ('epi', 1.0): 1, ('tricki', 1.0): 1, ('be-shak', 1.0): 1, ('chenoweth', 1.0): 1, ('oodl', 1.0): 1, ('hailey', 1.0): 1, ('craźi', 1.0): 1, ('sęxxxÿ', 1.0): 1, ('cøôl', 1.0): 1, ('runway', 1.0): 1, ('gooodnight', 1.0): 1, ('iv', 1.0): 1, ('ri', 1.0): 1, ('jayci', 1.0): 1, ('karaok', 1.0): 1, ('ltsw', 1.0): 1, ('giant', 1.0): 1, ('1709', 1.0): 1, ('refus', 1.0): 1, ('collagen', 1.0): 1, ('2win', 1.0): 1, ('hopetowin', 1.0): 1, ('inventori', 1.0): 1, ('loveforfood', 1.0): 1, ('foodforthought', 1.0): 1, ('thoughtfortheday', 1.0): 1, ('carp', 1.0): 1, ('diem', 1.0): 1, ('nath', 1.0): 1, ('ning', 1.0): 1, ('although', 1.0): 1, ('harm', 1.0): 1, ('stormi', 1.0): 1, ('sync', 1.0): 1, ('devic', 1.0): 1, ('mess', 1.0): 1, ('nylon', 1.0): 1, ('gvb', 1.0): 1, ('cd', 1.0): 1, ('mountain.titl', 1.0): 1, ('unto', 1.0): 1, ('theworldwouldchang', 1.0): 1, ('categori', 1.0): 1, ('mah', 1.0): 1, ('panel', 1.0): 1, (\"i'am\", 1.0): 1, ('80-1', 1.0): 1, ('1708', 1.0): 1, ('neenkin', 1.0): 1, ('masterpiec', 1.0): 1, ('debit', 1.0): 1, ('beagl', 1.0): 1, ('♫', 1.0): 1, ('feat', 1.0): 1, ('charli', 1.0): 1, ('puth', 1.0): 1, ('wiz', 1.0): 1, ('khalifa', 1.0): 1, ('svu', 1.0): 1, ('darker', 1.0): 1, ('berni', 1.0): 1, ('henri', 1.0): 1, ('trap', 1.0): 1, ('tommi', 1.0): 1, (\"vivian'\", 1.0): 1, ('transpar', 1.0): 1, ('bitcoin', 1.0): 1, ('insight', 1.0): 1, ('ping', 1.0): 1, ('masquerad', 1.0): 1, ('zorroreturm', 1.0): 1, ('1707', 1.0): 1, ('pk', 1.0): 1, ('hay', 1.0): 1, ('jacquelin', 1.0): 1, ('passion', 1.0): 1, ('full-fledg', 1.0): 1, ('workplac', 1.0): 1, ('venu', 1.0): 1, ('lago', 1.0): 1, ('luxord', 1.0): 1, ('potato', 1.0): 1, ('hundr', 1.0): 1, ('cite', 1.0): 1, ('academ', 1.0): 1, ('pokiri', 1.0): 1, ('1nenokkadin', 1.0): 1, ('heritag', 1.0): 1, ('wood', 1.0): 1, ('beleaf', 1.0): 1, ('spnfamili', 1.0): 1, ('spn', 1.0): 1, ('alwayskeepfight', 1.0): 1, ('jaredpadalecki', 1.0): 1, ('jensenackl', 1.0): 1, ('peasant', 1.0): 2, ('ahahha', 1.0): 1, ('distant', 1.0): 1, ('shout-out', 1.0): 1, ('adulthood', 1.0): 1, ('hopeless', 0.0): 2, ('tmr', 0.0): 3, (':(', 0.0): 4571, ('everyth', 0.0): 17, ('kid', 0.0): 20, ('section', 0.0): 3, ('ikea', 0.0): 1, ('cute', 0.0): 43, ('shame', 0.0): 19, (\"i'm\", 0.0): 343, ('nearli', 0.0): 3, ('19', 0.0): 8, ('2', 0.0): 41, ('month', 0.0): 23, ('heart', 0.0): 27, ('slide', 0.0): 1, ('wast', 0.0): 5, ('basket', 0.0): 1, ('“', 0.0): 15, ('hate', 0.0): 57, ('japanes', 0.0): 4, ('call', 0.0): 29, ('bani', 0.0): 2, ('”', 0.0): 11, ('dang', 0.0): 2, ('start', 0.0): 44, ('next', 0.0): 40, ('week', 0.0): 56, ('work', 0.0): 133, ('oh', 0.0): 92, ('god', 0.0): 15, ('babi', 0.0): 47, ('face', 0.0): 20, ('make', 0.0): 102, ('smile', 0.0): 10, ('neighbour', 0.0): 1, ('motor', 0.0): 1, ('ask', 0.0): 29, ('said', 0.0): 33, ('updat', 0.0): 11, ('search', 0.0): 3, ('sialan', 0.0): 1, ('athabasca', 0.0): 2, ('glacier', 0.0): 2, ('1948', 0.0): 1, (':-(', 0.0): 493, ('jasper', 0.0): 1, ('jaspernationalpark', 0.0): 1, ('alberta', 0.0): 1, ('explorealberta', 0.0): 1, ('…', 0.0): 16, ('realli', 0.0): 131, ('good', 0.0): 101, ('g', 0.0): 8, ('idea', 0.0): 10, ('never', 0.0): 57, ('go', 0.0): 224, ('meet', 0.0): 31, ('mare', 0.0): 1, ('ivan', 0.0): 1, ('happi', 0.0): 25, ('trip', 0.0): 11, ('keep', 0.0): 34, ('safe', 0.0): 5, ('see', 0.0): 124, ('soon', 0.0): 45, ('tire', 0.0): 50, ('hahahah', 0.0): 3, ('knee', 0.0): 2, ('replac', 0.0): 4, ('get', 0.0): 232, ('day', 0.0): 149, ('ouch', 0.0): 3, ('relat', 0.0): 2, ('sweet', 0.0): 7, ('n', 0.0): 21, ('sour', 0.0): 2, ('kind', 0.0): 11, ('bi-polar', 0.0): 1, ('peopl', 0.0): 75, ('life', 0.0): 33, ('...', 0.0): 331, ('cuz', 0.0): 4, ('full', 0.0): 16, ('pleass', 0.0): 2, ('im', 0.0): 129, ('sure', 0.0): 31, ('tho', 0.0): 28, ('feel', 0.0): 158, ('stupid', 0.0): 8, (\"can't\", 0.0): 180, ('seem', 0.0): 15, ('grasp', 0.0): 1, ('basic', 0.0): 2, ('digit', 0.0): 8, ('paint', 0.0): 3, ('noth', 0.0): 26, (\"i'v\", 0.0): 77, ('research', 0.0): 1, ('help', 0.0): 54, ('lord', 0.0): 2, ('lone', 0.0): 9, ('someon', 0.0): 57, ('talk', 0.0): 45, ('guy', 0.0): 62, ('girl', 0.0): 28, ('assign', 0.0): 5, ('project', 0.0): 3, ('😩', 0.0): 14, ('want', 0.0): 246, ('play', 0.0): 48, ('video', 0.0): 23, ('game', 0.0): 28, ('watch', 0.0): 77, ('movi', 0.0): 24, ('choreograph', 0.0): 1, ('hard', 0.0): 35, ('email', 0.0): 10, ('link', 0.0): 12, ('still', 0.0): 124, ('say', 0.0): 63, ('longer', 0.0): 12, ('avail', 0.0): 13, ('cri', 0.0): 46, ('bc', 0.0): 50, ('miss', 0.0): 301, ('mingm', 0.0): 1, ('much', 0.0): 139, ('sorri', 0.0): 148, ('mom', 0.0): 13, ('far', 0.0): 18, ('away', 0.0): 28, (\"we'r\", 0.0): 30, ('truli', 0.0): 5, ('flight', 0.0): 6, ('friend', 0.0): 39, ('happen', 0.0): 51, ('sad', 0.0): 123, ('dog', 0.0): 17, ('pee', 0.0): 2, ('’', 0.0): 27, ('bag', 0.0): 8, ('take', 0.0): 49, ('newwin', 0.0): 1, ('15', 0.0): 10, ('doushit', 0.0): 1, ('late', 0.0): 27, ('suck', 0.0): 23, ('sick', 0.0): 43, ('plan', 0.0): 17, ('first', 0.0): 27, ('gundam', 0.0): 1, ('night', 0.0): 46, ('nope', 0.0): 6, ('dollar', 0.0): 1, ('😭', 0.0): 29, ('listen', 0.0): 18, ('back', 0.0): 122, ('old', 0.0): 16, ('show', 0.0): 26, ('know', 0.0): 131, ('weird', 0.0): 10, ('got', 0.0): 104, ('u', 0.0): 193, ('leav', 0.0): 42, ('might', 0.0): 11, ('give', 0.0): 36, ('pale', 0.0): 2, ('imit', 0.0): 1, ('went', 0.0): 32, ('sea', 0.0): 1, ('massiv', 0.0): 4, ('fuck', 0.0): 58, ('rash', 0.0): 1, ('bodi', 0.0): 12, ('pain', 0.0): 21, ('thing', 0.0): 52, ('ever', 0.0): 30, ('home', 0.0): 63, ('hi', 0.0): 34, ('absent', 0.0): 1, ('gran', 0.0): 2, ('knew', 0.0): 6, ('care', 0.0): 20, ('tell', 0.0): 26, ('love', 0.0): 152, ('wish', 0.0): 91, ('would', 0.0): 70, ('sequel', 0.0): 1, ('busi', 0.0): 28, ('sa', 0.0): 15, ('school', 0.0): 32, ('time', 0.0): 166, ('yah', 0.0): 3, ('xx', 0.0): 18, ('ouucchhh', 0.0): 1, ('one', 0.0): 148, ('wisdom', 0.0): 2, ('teeth', 0.0): 6, ('come', 0.0): 91, ('frighten', 0.0): 1, ('case', 0.0): 6, ('pret', 0.0): 1, ('wkwkw', 0.0): 1, ('verfi', 0.0): 1, ('activ', 0.0): 6, ('forget', 0.0): 8, ('follow', 0.0): 262, ('member', 0.0): 6, ('thank', 0.0): 107, ('join', 0.0): 8, ('goodby', 0.0): 14, ('´', 0.0): 4, ('chain', 0.0): 1, ('—', 0.0): 26, ('sentir-s', 0.0): 1, ('incompleta', 0.0): 1, ('okay', 0.0): 38, ('..', 0.0): 108, ('wednesday', 0.0): 5, ('marvel', 0.0): 1, ('thwart', 0.0): 1, ('awh', 0.0): 3, (\"what'\", 0.0): 15, ('chanc', 0.0): 16, ('zant', 0.0): 1, ('need', 0.0): 106, ('someth', 0.0): 28, ('x', 0.0): 39, (\"when'\", 0.0): 1, ('birthday', 0.0): 23, ('worst', 0.0): 14, ('part', 0.0): 11, ('bad', 0.0): 73, ('audraesar', 0.0): 1, ('sushi', 0.0): 3, ('pic', 0.0): 15, ('tl', 0.0): 8, ('drive', 0.0): 16, ('craaazzyy', 0.0): 2, ('pop', 0.0): 3, ('like', 0.0): 228, ('helium', 0.0): 1, ('balloon', 0.0): 1, ('climatechang', 0.0): 5, ('cc', 0.0): 6, (\"california'\", 0.0): 1, ('power', 0.0): 6, ('influenti', 0.0): 1, ('air', 0.0): 3, ('pollut', 0.0): 1, ('watchdog', 0.0): 1, ('califor', 0.0): 1, ('elhaida', 0.0): 1, ('rob', 0.0): 2, ('juri', 0.0): 1, ('came', 0.0): 16, ('10th', 0.0): 1, ('televot', 0.0): 1, ('idaho', 0.0): 2, ('restrict', 0.0): 2, ('fish', 0.0): 2, ('despit', 0.0): 2, ('region', 0.0): 2, ('drought-link', 0.0): 1, ('die-of', 0.0): 1, ('abrupt', 0.0): 1, ('climat', 0.0): 1, ('chang', 0.0): 27, ('may', 0.0): 16, ('doom', 0.0): 2, ('mammoth', 0.0): 1, ('megafauna', 0.0): 1, ('sc', 0.0): 3, (\"australia'\", 0.0): 1, ('dirtiest', 0.0): 2, ('station', 0.0): 3, ('consid', 0.0): 5, ('clean', 0.0): 6, ('energi', 0.0): 3, ('biomass', 0.0): 1, (\"ain't\", 0.0): 5, ('easi', 0.0): 6, ('green', 0.0): 7, ('golf', 0.0): 1, ('cours', 0.0): 7, ('california', 0.0): 1, ('ulti', 0.0): 1, ('well', 0.0): 56, ('mine', 0.0): 12, ('gonna', 0.0): 51, ('sexi', 0.0): 14, ('prexi', 0.0): 1, ('kindergarten', 0.0): 1, ('hungri', 0.0): 19, ('cant', 0.0): 47, ('find', 0.0): 53, ('book', 0.0): 20, ('sane', 0.0): 1, ('liter', 0.0): 15, ('three', 0.0): 7, ('loung', 0.0): 1, ('event', 0.0): 4, ('turn', 0.0): 17, ('boss', 0.0): 5, ('hozier', 0.0): 1, (\"that'\", 0.0): 61, ('true', 0.0): 22, ('soooner', 0.0): 1, ('ahh', 0.0): 7, ('fam', 0.0): 3, ('respectlost', 0.0): 1, ('hypercholesteloremia', 0.0): 1, ('ok', 0.0): 33, ('look', 0.0): 100, ('gift', 0.0): 11, ('calibraska', 0.0): 1, ('actual', 0.0): 24, ('genuin', 0.0): 2, ('contend', 0.0): 1, ('head', 0.0): 23, ('alway', 0.0): 56, ('hurt', 0.0): 41, ('stay', 0.0): 24, ('lmao', 0.0): 13, ('older', 0.0): 5, ('sound', 0.0): 19, ('upset', 0.0): 11, ('infinit', 0.0): 10, ('ao', 0.0): 1, ('stick', 0.0): 1, ('8th', 0.0): 1, ('either', 0.0): 13, ('seriou', 0.0): 8, ('yun', 0.0): 1, ('eh', 0.0): 4, ('room', 0.0): 11, ('way', 0.0): 42, ('hot', 0.0): 15, ('havent', 0.0): 11, ('found', 0.0): 11, ('handsom', 0.0): 2, ('jack', 0.0): 3, ('draw', 0.0): 2, ('shit', 0.0): 36, ('cut', 0.0): 14, ('encor', 0.0): 4, ('4thwin', 0.0): 4, ('baymax', 0.0): 1, ('french', 0.0): 4, ('mixer', 0.0): 1, ('💜', 0.0): 6, ('wft', 0.0): 1, ('awesom', 0.0): 5, ('replay', 0.0): 1, ('parti', 0.0): 15, ('promot', 0.0): 3, ('music', 0.0): 16, ('bank', 0.0): 9, ('short', 0.0): 11, ('boy', 0.0): 18, ('order', 0.0): 16, ('receiv', 0.0): 7, ('hub', 0.0): 1, ('nearest', 0.0): 1, ('deliv', 0.0): 3, ('today', 0.0): 108, ('1/2', 0.0): 3, ('mum', 0.0): 14, ('loud', 0.0): 2, ('final', 0.0): 35, ('parasyt', 0.0): 1, ('alll', 0.0): 1, ('zayniscomingbackonjuli', 0.0): 23, ('26', 0.0): 24, ('bye', 0.0): 8, ('era', 0.0): 1, ('。', 0.0): 3, ('ω', 0.0): 1, ('」', 0.0): 2, ('∠', 0.0): 2, ('):', 0.0): 6, ('nathann', 0.0): 1, ('💕', 0.0): 7, ('hug', 0.0): 29, ('😊', 0.0): 9, ('beauti', 0.0): 11, ('dieididieieiei', 0.0): 1, ('stage', 0.0): 15, ('mean', 0.0): 43, ('hello', 0.0): 13, ('lion', 0.0): 3, ('think', 0.0): 75, ('screw', 0.0): 4, ('netflix', 0.0): 5, ('chill', 0.0): 7, ('di', 0.0): 7, ('ervin', 0.0): 1, ('ohh', 0.0): 8, ('yeah', 0.0): 41, ('hope', 0.0): 102, ('accept', 0.0): 2, ('offer', 0.0): 10, ('desper', 0.0): 2, ('year', 0.0): 46, ('snapchat', 0.0): 79, ('amargolonnard', 0.0): 2, ('kikhorni', 0.0): 13, ('snapm', 0.0): 4, ('tagsforlik', 0.0): 5, ('batalladelosgallo', 0.0): 2, ('webcamsex', 0.0): 4, ('ugh', 0.0): 26, ('stream', 0.0): 24, ('duti', 0.0): 3, (\"u'v\", 0.0): 1, ('gone', 0.0): 24, ('alien', 0.0): 1, ('aww', 0.0): 21, ('wanna', 0.0): 94, ('sorka', 0.0): 1, ('funer', 0.0): 4, ('text', 0.0): 15, ('phone', 0.0): 34, ('sunni', 0.0): 1, ('nonexist', 0.0): 1, ('wowza', 0.0): 1, ('fah', 0.0): 1, ('taylor', 0.0): 3, ('crop', 0.0): 1, ('boo', 0.0): 5, ('count', 0.0): 7, ('new', 0.0): 51, ('guitar', 0.0): 1, ('jonghyun', 0.0): 1, ('hyung', 0.0): 1, ('pleas', 0.0): 275, ('predict', 0.0): 2, ('sj', 0.0): 3, ('nomin', 0.0): 1, ('vs', 0.0): 4, ('pl', 0.0): 45, ('dude', 0.0): 12, ('calm', 0.0): 3, ('brace', 0.0): 5, ('sir', 0.0): 5, ('plu', 0.0): 4, ('4', 0.0): 18, ('shock', 0.0): 3, ('omggg', 0.0): 2, ('yall', 0.0): 4, ('deserv', 0.0): 8, ('whenev', 0.0): 3, ('spend', 0.0): 8, ('smoke', 0.0): 3, ('end', 0.0): 40, ('fall', 0.0): 16, ('asleep', 0.0): 25, ('1', 0.0): 26, ('point', 0.0): 14, ('close', 0.0): 20, ('grand', 0.0): 1, ('whyyi', 0.0): 7, ('long', 0.0): 38, ('must', 0.0): 15, ('annoy', 0.0): 11, ('evan', 0.0): 1, ('option', 0.0): 3, ('opt', 0.0): 1, (\"who'\", 0.0): 7, ('giveaway', 0.0): 3, ('muster', 0.0): 1, ('merch', 0.0): 4, ('ah', 0.0): 18, ('funni', 0.0): 6, ('drink', 0.0): 7, ('savanna', 0.0): 1, ('straw', 0.0): 1, ('ignor', 0.0): 16, ('yester', 0.0): 1, ('afternoon', 0.0): 3, ('sleep', 0.0): 90, ('ye', 0.0): 48, ('sadli', 0.0): 11, ('when', 0.0): 2, ('album', 0.0): 16, ('last', 0.0): 72, ('chocol', 0.0): 8, ('consum', 0.0): 1, ('werk', 0.0): 1, ('morn', 0.0): 31, ('foreal', 0.0): 1, ('wesen', 0.0): 1, ('uwes', 0.0): 1, ('mj', 0.0): 1, ('😂', 0.0): 24, ('catch', 0.0): 9, ('onlin', 0.0): 20, ('enough', 0.0): 24, ('haha', 0.0): 30, (\"he'\", 0.0): 23, ('bosen', 0.0): 1, ('die', 0.0): 21, ('egg', 0.0): 4, ('benni', 0.0): 1, ('sometim', 0.0): 16, ('followback', 0.0): 6, ('huhu', 0.0): 17, ('understand', 0.0): 15, ('badli', 0.0): 12, ('scare', 0.0): 16, ('&gt;:(', 0.0): 47, ('al', 0.0): 4, ('kati', 0.0): 3, ('zaz', 0.0): 1, ('ami', 0.0): 2, ('lot', 0.0): 27, ('diari', 0.0): 1, ('read', 0.0): 20, ('rehash', 0.0): 1, ('websit', 0.0): 7, ('mushroom', 0.0): 1, ('piec', 0.0): 4, ('except', 0.0): 5, ('reach', 0.0): 3, ('anyway', 0.0): 12, ('vicki', 0.0): 1, ('omg', 0.0): 63, ('wtf', 0.0): 13, ('lip', 0.0): 3, ('virgin', 0.0): 2, ('your', 0.0): 8, ('45', 0.0): 1, ('hahah', 0.0): 6, ('ninasti', 0.0): 1, ('tsktsk', 0.0): 1, ('oppa', 0.0): 4, ('wont', 0.0): 9, ('dick', 0.0): 5, ('kawaii', 0.0): 1, ('manli', 0.0): 1, ('xbox', 0.0): 3, ('alreadi', 0.0): 52, ('comfi', 0.0): 1, ('bed', 0.0): 12, ('youu', 0.0): 2, ('sigh', 0.0): 13, ('lol', 0.0): 43, ('potato', 0.0): 1, ('fri', 0.0): 7, ('guess', 0.0): 14, (\"y'all\", 0.0): 2, ('ugli', 0.0): 9, ('asf', 0.0): 1, ('huh', 0.0): 7, ('eish', 0.0): 1, ('ive', 0.0): 11, ('quit', 0.0): 9, ('lost', 0.0): 25, ('twitter', 0.0): 30, ('mojo', 0.0): 1, ('dont', 0.0): 53, ('mara', 0.0): 1, ('neh', 0.0): 2, ('fever', 0.0): 7, ('&lt;3', 0.0): 25, ('poor', 0.0): 35, ('bb', 0.0): 7, ('abl', 0.0): 22, ('associ', 0.0): 1, ('councillor', 0.0): 1, ('confer', 0.0): 2, ('weekend', 0.0): 25, ('skype', 0.0): 6, ('account', 0.0): 20, ('hack', 0.0): 8, ('contact', 0.0): 7, ('creat', 0.0): 2, ('tweet', 0.0): 35, ('spree', 0.0): 4, ('na', 0.0): 29, ('sholong', 0.0): 1, ('reject', 0.0): 7, ('propos', 0.0): 2, ('gee', 0.0): 1, ('fli', 0.0): 10, ('gidi', 0.0): 1, ('pamper', 0.0): 1, ('lago', 0.0): 1, ('ehn', 0.0): 1, ('arrest', 0.0): 1, ('girlfriend', 0.0): 2, ('he', 0.0): 3, ('nice', 0.0): 19, ('person', 0.0): 15, ('idk', 0.0): 26, ('anybodi', 0.0): 7, ('song', 0.0): 27, ('disappear', 0.0): 1, ('itun', 0.0): 3, ('daze', 0.0): 1, ('confus', 0.0): 8, ('surviv', 0.0): 5, ('fragment', 0.0): 1, (\"would'v\", 0.0): 2, ('forc', 0.0): 2, ('horribl', 0.0): 9, ('weather', 0.0): 29, ('us', 0.0): 43, ('could', 0.0): 69, ('walao', 0.0): 1, ('kb', 0.0): 1, ('send', 0.0): 12, ('ill', 0.0): 16, ('djderek', 0.0): 1, ('mani', 0.0): 29, ('fun', 0.0): 32, ('gig', 0.0): 3, ('absolut', 0.0): 6, ('legend', 0.0): 3, ('wait', 0.0): 43, ('till', 0.0): 8, ('saturday', 0.0): 10, ('homework', 0.0): 2, ('pa', 0.0): 8, ('made', 0.0): 23, ('da', 0.0): 5, ('greek', 0.0): 2, ('tragedi', 0.0): 1, ('rain', 0.0): 43, ('gym', 0.0): 6, ('💪🏻', 0.0): 1, ('🐒', 0.0): 1, ('what', 0.0): 8, ('wrong', 0.0): 33, ('struck', 0.0): 1, ('anymor', 0.0): 20, ('belgium', 0.0): 4, ('fabian', 0.0): 2, ('delph', 0.0): 6, ('fallen', 0.0): 3, ('hide', 0.0): 4, ('drake', 0.0): 1, ('silent', 0.0): 1, ('hear', 0.0): 33, ('rest', 0.0): 21, ('peac', 0.0): 5, ('mo', 0.0): 4, ('tonight', 0.0): 24, ('t20blast', 0.0): 1, ('ahhh', 0.0): 5, ('wake', 0.0): 21, ('mumma', 0.0): 2, ('7', 0.0): 16, ('dead', 0.0): 10, ('tomorrow', 0.0): 34, (\"i'll\", 0.0): 41, ('high', 0.0): 8, ('low', 0.0): 8, ('pray', 0.0): 13, ('appropri', 0.0): 1, ('. . .', 0.0): 2, ('awak', 0.0): 10, ('woke', 0.0): 14, ('upp', 0.0): 1, ('dm', 0.0): 23, ('luke', 0.0): 6, ('hey', 0.0): 26, ('babe', 0.0): 19, ('across', 0.0): 4, ('hindi', 0.0): 1, ('reaction', 0.0): 1, ('5s', 0.0): 1, ('run', 0.0): 15, ('space', 0.0): 5, ('tbh', 0.0): 14, ('disabl', 0.0): 2, ('pension', 0.0): 1, ('ptsd', 0.0): 1, ('imposs', 0.0): 4, ('physic', 0.0): 7, ('financi', 0.0): 2, ('nooo', 0.0): 16, ('broke', 0.0): 9, ('soo', 0.0): 3, ('amaz', 0.0): 16, ('toghet', 0.0): 1, ('around', 0.0): 20, ('p', 0.0): 5, ('hold', 0.0): 9, ('anoth', 0.0): 27, ('septemb', 0.0): 2, ('21st', 0.0): 2, ('snsd', 0.0): 2, ('interact', 0.0): 2, ('anna', 0.0): 5, ('akana', 0.0): 1, ('askip', 0.0): 1, (\"t'exist\", 0.0): 1, ('channel', 0.0): 6, ('owner', 0.0): 1, ('decid', 0.0): 10, ('broadcast', 0.0): 6, ('kei', 0.0): 2, ('rate', 0.0): 4, ('se', 0.0): 2, ('notic', 0.0): 26, ('exist', 0.0): 2, ('traffic', 0.0): 5, ('terribl', 0.0): 12, ('eye', 0.0): 12, ('small', 0.0): 9, ('kate', 0.0): 2, ('spade', 0.0): 1, ('pero', 0.0): 3, ('walang', 0.0): 1, ('maganda', 0.0): 1, ('aw', 0.0): 42, ('seen', 0.0): 23, ('agesss', 0.0): 1, ('add', 0.0): 26, ('corinehurleigh', 0.0): 1, ('snapchatm', 0.0): 6, ('instagram', 0.0): 4, ('addmeonsnapchat', 0.0): 2, ('sf', 0.0): 3, ('quot', 0.0): 6, ('kiksext', 0.0): 6, ('bum', 0.0): 2, ('zara', 0.0): 1, ('trouser', 0.0): 1, ('effect', 0.0): 4, ('spanish', 0.0): 1, (\"it'okay\", 0.0): 1, ('health', 0.0): 2, ('luck', 0.0): 6, ('freed', 0.0): 1, ('rock', 0.0): 3, ('orcalov', 0.0): 1, ('tri', 0.0): 65, ('big', 0.0): 21, ('cuddl', 0.0): 8, ('lew', 0.0): 1, ('kiss', 0.0): 4, ('em', 0.0): 1, ('crave', 0.0): 8, ('banana', 0.0): 4, ('crumbl', 0.0): 1, ('mcflurri', 0.0): 1, ('cabl', 0.0): 1, ('car', 0.0): 17, ('brother', 0.0): 10, (\"venus'\", 0.0): 1, ('concept', 0.0): 4, ('rli', 0.0): 5, ('tea', 0.0): 7, ('tagal', 0.0): 2, (\"we'v\", 0.0): 3, ('appoint', 0.0): 1, (\"i'd\", 0.0): 11, ('sinc', 0.0): 35, (\"there'\", 0.0): 18, ('milk', 0.0): 3, ('left', 0.0): 26, ('cereal', 0.0): 2, ('film', 0.0): 6, ('date', 0.0): 7, ('previou', 0.0): 2, ('73', 0.0): 2, ('user', 0.0): 1, ('everywher', 0.0): 6, ('fansign', 0.0): 1, ('photo', 0.0): 15, ('expens', 0.0): 7, ('zzzz', 0.0): 1, ('let', 0.0): 37, ('sun', 0.0): 10, ('yet', 0.0): 33, (\"bff'\", 0.0): 1, ('extrem', 0.0): 3, ('stress', 0.0): 10, ('anyth', 0.0): 19, ('win', 0.0): 27, (\"deosn't\", 0.0): 1, ('liverpool', 0.0): 2, ('pool', 0.0): 3, ('though', 0.0): 57, ('bro', 0.0): 3, ('great', 0.0): 22, ('news', 0.0): 21, ('self', 0.0): 1, ('esteem', 0.0): 1, ('lowest', 0.0): 1, ('better', 0.0): 36, ('tacki', 0.0): 1, ('taken', 0.0): 9, ('man', 0.0): 32, ('lucki', 0.0): 16, ('charm', 0.0): 1, ('haaretz', 0.0): 1, ('israel', 0.0): 1, ('syria', 0.0): 2, ('continu', 0.0): 1, ('develop', 0.0): 5, ('chemic', 0.0): 1, ('weapon', 0.0): 2, ('offici', 0.0): 3, ('wsj', 0.0): 2, ('rep', 0.0): 1, ('bt', 0.0): 4, ('mr', 0.0): 9, ('wong', 0.0): 1, ('confisc', 0.0): 1, ('art', 0.0): 4, ('thought', 0.0): 31, ('icepack', 0.0): 1, ('dose', 0.0): 2, ('killer', 0.0): 2, ('board', 0.0): 1, ('whimper', 0.0): 1, ('fan', 0.0): 17, ('senpai', 0.0): 1, ('buttsex', 0.0): 1, ('joke', 0.0): 8, ('headlin', 0.0): 1, (\"dn't\", 0.0): 1, ('brk', 0.0): 1, (\":'(\", 0.0): 13, ('hit', 0.0): 7, ('voic', 0.0): 9, ('falsetto', 0.0): 1, ('zone', 0.0): 2, ('leannerin', 0.0): 1, ('hornykik', 0.0): 17, ('loveofmylif', 0.0): 2, ('dmme', 0.0): 2, ('pussi', 0.0): 2, ('newmus', 0.0): 3, ('sexo', 0.0): 2, ('s2', 0.0): 1, ('spain', 0.0): 4, ('delay', 0.0): 5, ('kill', 0.0): 22, ('singl', 0.0): 10, ('untruth', 0.0): 1, ('cross', 0.0): 4, ('countri', 0.0): 6, ('ij', 0.0): 1, ('💥', 0.0): 1, ('✨', 0.0): 1, ('💫', 0.0): 1, ('bear', 0.0): 2, ('littl', 0.0): 21, ('apart', 0.0): 7, ('live', 0.0): 37, ('soshi', 0.0): 1, ('didnt', 0.0): 24, ('buttt', 0.0): 2, ('congrat', 0.0): 2, ('sunday', 0.0): 8, ('friday', 0.0): 12, ('shoulda', 0.0): 1, ('move', 0.0): 12, ('w', 0.0): 22, ('caus', 0.0): 16, (\"they'r\", 0.0): 14, ('heyyy', 0.0): 1, ('yeol', 0.0): 2, ('solo', 0.0): 6, ('dancee', 0.0): 1, ('inter', 0.0): 1, ('nemanja', 0.0): 1, ('vidic', 0.0): 1, ('roma', 0.0): 1, (\"mom'\", 0.0): 2, ('linguist', 0.0): 1, (\"dad'\", 0.0): 1, ('comput', 0.0): 6, ('scientist', 0.0): 1, ('dumbest', 0.0): 1, ('famili', 0.0): 9, ('broken', 0.0): 11, ('ice', 0.0): 35, ('cream', 0.0): 32, ('pour', 0.0): 1, ('crash', 0.0): 6, ('scienc', 0.0): 1, ('resourc', 0.0): 1, ('vehicl', 0.0): 5, ('ate', 0.0): 10, ('ayex', 0.0): 1, ('eat', 0.0): 27, ('swear', 0.0): 6, ('lamon', 0.0): 1, ('scroll', 0.0): 1, ('curv', 0.0): 2, ('😉', 0.0): 1, ('cement', 0.0): 1, ('cast', 0.0): 5, ('10.3', 0.0): 1, ('k', 0.0): 9, ('sign', 0.0): 9, ('zayn', 0.0): 8, ('bot', 0.0): 1, ('plz', 0.0): 3, ('mention', 0.0): 9, ('jmu', 0.0): 1, ('camp', 0.0): 7, ('teas', 0.0): 3, ('sweetest', 0.0): 1, ('awuna', 0.0): 1, ('mbulelo', 0.0): 1, ('match', 0.0): 7, ('pig', 0.0): 2, ('although', 0.0): 5, ('crackl', 0.0): 1, ('nois', 0.0): 3, ('plug', 0.0): 2, ('fuse', 0.0): 1, ('dammit', 0.0): 3, ('tip', 0.0): 2, ('carlton', 0.0): 2, ('aflblueshawk', 0.0): 2, (\"alex'\", 0.0): 1, ('hous', 0.0): 16, ('motorsport', 0.0): 1, ('seri', 0.0): 3, ('disc', 0.0): 1, ('right', 0.0): 51, ('cheeki', 0.0): 1, ('j', 0.0): 1, ('instead', 0.0): 4, ('seo', 0.0): 1, ('nl', 0.0): 1, ('bud', 0.0): 1, ('christi', 0.0): 1, ('xo', 0.0): 1, ('niec', 0.0): 1, ('summer', 0.0): 19, ('bloodi', 0.0): 2, ('sandwhich', 0.0): 1, ('buset', 0.0): 1, ('discrimin', 0.0): 4, ('five', 0.0): 5, ('learn', 0.0): 5, ('pregnanc', 0.0): 2, ('foot', 0.0): 5, ('f', 0.0): 4, ('matern', 0.0): 1, ('kick', 0.0): 6, ('domesticviol', 0.0): 1, ('law', 0.0): 4, ('domest', 0.0): 1, ('violenc', 0.0): 2, ('victim', 0.0): 4, ('98fm', 0.0): 1, ('exactli', 0.0): 5, ('unfortun', 0.0): 21, ('yesterday', 0.0): 13, ('uk', 0.0): 9, ('govern', 0.0): 1, ('sapiosexu', 0.0): 1, ('damn', 0.0): 29, ('beta', 0.0): 4, ('12', 0.0): 8, ('hour', 0.0): 35, ('world', 0.0): 17, ('hulk', 0.0): 3, ('hogan', 0.0): 3, ('scrub', 0.0): 1, ('wwe', 0.0): 2, ('histori', 0.0): 2, ('iren', 0.0): 4, ('mistak', 0.0): 6, ('naa', 0.0): 1, ('sold', 0.0): 6, ('h_my_k', 0.0): 1, ('lose', 0.0): 7, ('valentin', 0.0): 2, ('et', 0.0): 3, (\"r'ship\", 0.0): 1, ('btwn', 0.0): 1, ('homo', 0.0): 2, ('biphob', 0.0): 2, ('comment', 0.0): 4, ('certain', 0.0): 6, ('disciplin', 0.0): 2, ('incl', 0.0): 2, ('european', 0.0): 3, ('lang', 0.0): 6, ('lit', 0.0): 2, ('educ', 0.0): 2, ('fresherstofin', 0.0): 1, ('💔', 0.0): 3, ('dream', 0.0): 24, ('gettin', 0.0): 2, ('realist', 0.0): 4, ('thx', 0.0): 1, ('real', 0.0): 21, ('isnt', 0.0): 7, ('prefer', 0.0): 4, ('benzema', 0.0): 2, ('hahahahahaah', 0.0): 1, ('donno', 0.0): 1, ('korean', 0.0): 2, ('languag', 0.0): 5, ('russian', 0.0): 2, ('waaa', 0.0): 1, ('eidwithgrof', 0.0): 1, ('boreddd', 0.0): 1, ('mug', 0.0): 3, ('piss', 0.0): 3, ('tiddler', 0.0): 1, ('silli', 0.0): 2, ('least', 0.0): 15, ('card', 0.0): 7, ('chorong', 0.0): 1, ('leader', 0.0): 1, ('에이핑크', 0.0): 3, ('더쇼', 0.0): 4, ('clan', 0.0): 1, ('slot', 0.0): 2, ('open', 0.0): 16, ('pfff', 0.0): 1, ('privat', 0.0): 2, ('bugbounti', 0.0): 1, ('self-xss', 0.0): 1, ('host', 0.0): 2, ('header', 0.0): 3, ('poison', 0.0): 3, ('code', 0.0): 8, ('execut', 0.0): 1, ('ktksbye', 0.0): 1, ('connect', 0.0): 3, ('compani', 0.0): 3, ('alert', 0.0): 2, ('cancel', 0.0): 10, ('uber', 0.0): 3, ('everyon', 0.0): 26, ('els', 0.0): 4, ('offic', 0.0): 7, ('ahahah', 0.0): 1, ('petit', 0.0): 1, ('relationship', 0.0): 4, ('height', 0.0): 2, ('cost', 0.0): 1, ('600', 0.0): 2, ('£', 0.0): 6, ('secur', 0.0): 4, ('odoo', 0.0): 2, ('8', 0.0): 11, ('partner', 0.0): 2, ('commun', 0.0): 2, ('spirit', 0.0): 3, ('jgh', 0.0): 2, ('effin', 0.0): 1, ('facebook', 0.0): 4, ('anyon', 0.0): 17, (\"else'\", 0.0): 1, ('box', 0.0): 8, ('ap', 0.0): 3, ('stori', 0.0): 13, ('london', 0.0): 12, ('imagin', 0.0): 2, ('elsewher', 0.0): 1, ('someday', 0.0): 1, ('ben', 0.0): 3, ('provid', 0.0): 3, ('name', 0.0): 15, ('branch', 0.0): 1, ('visit', 0.0): 12, ('address', 0.0): 3, ('concern', 0.0): 3, ('welsh', 0.0): 1, ('pod', 0.0): 1, ('juli', 0.0): 12, ('laura', 0.0): 4, ('insid', 0.0): 10, ('train', 0.0): 12, ('d;', 0.0): 1, ('talk-kama', 0.0): 1, ('hawako', 0.0): 1, ('waa', 0.0): 1, ('kimaaani', 0.0): 1, ('prisss', 0.0): 1, ('baggag', 0.0): 2, ('claim', 0.0): 3, ('plane', 0.0): 2, ('niamh', 0.0): 1, ('forev', 0.0): 10, ('hmmm', 0.0): 2, ('sugar', 0.0): 3, ('rare', 0.0): 1, ('paper', 0.0): 16, ('town', 0.0): 14, ('score', 0.0): 3, ('stuck', 0.0): 8, ('agh', 0.0): 2, ('middl', 0.0): 7, ('undercoverboss', 0.0): 1, ('تكفى', 0.0): 1, ('10', 0.0): 8, ('job', 0.0): 13, ('cat', 0.0): 17, ('forgotten', 0.0): 3, ('yep', 0.0): 5, ('stop', 0.0): 43, ('ach', 0.0): 2, ('wrist', 0.0): 1, ('nake', 0.0): 3, ('forgot', 0.0): 14, ('bracelet', 0.0): 3, ('ligo', 0.0): 1, ('dozen', 0.0): 1, ('parent', 0.0): 8, ('children', 0.0): 2, ('shark', 0.0): 2, ('selfi', 0.0): 6, ('heartach', 0.0): 1, ('zayniscomingback', 0.0): 3, ('mix', 0.0): 2, ('sweden', 0.0): 1, ('breath', 0.0): 4, ('moment', 0.0): 14, ('word', 0.0): 16, ('elmhurst', 0.0): 1, ('fc', 0.0): 1, ('etid', 0.0): 1, (\"chillin'with\", 0.0): 1, ('father', 0.0): 2, ('istanya', 0.0): 1, ('2suppli', 0.0): 1, ('extra', 0.0): 3, ('infrastructur', 0.0): 2, ('teacher', 0.0): 2, ('doctor', 0.0): 4, ('nurs', 0.0): 2, ('paramed', 0.0): 1, ('countless', 0.0): 1, ('2cope', 0.0): 1, ('bore', 0.0): 23, ('plea', 0.0): 2, ('arian', 0.0): 1, ('hahahaha', 0.0): 6, ('slr', 0.0): 1, ('kendal', 0.0): 1, ('kyli', 0.0): 3, (\"kylie'\", 0.0): 1, ('manila', 0.0): 3, ('jeebu', 0.0): 1, ('reabsorbt', 0.0): 1, ('tooth', 0.0): 2, ('abscess', 0.0): 1, ('threaten', 0.0): 2, ('affect', 0.0): 1, ('front', 0.0): 6, ('crown', 0.0): 1, ('ooouch', 0.0): 1, ('barney', 0.0): 1, (\"be'\", 0.0): 1, ('yo', 0.0): 4, ('later', 0.0): 14, ('realis', 0.0): 6, ('problemat', 0.0): 1, ('expect', 0.0): 5, ('proud', 0.0): 8, ('mess', 0.0): 7, ('maa', 0.0): 2, ('without', 0.0): 25, ('bangalor', 0.0): 1, ('awww', 0.0): 23, ('lui', 0.0): 1, ('manzano', 0.0): 1, ('shaaa', 0.0): 1, ('super', 0.0): 11, ('7th', 0.0): 1, ('conven', 0.0): 1, ('2:30', 0.0): 2, ('pm', 0.0): 8, ('forward', 0.0): 6, ('delet', 0.0): 5, ('turkey', 0.0): 1, ('bomb', 0.0): 3, ('isi', 0.0): 1, ('allow', 0.0): 9, ('usa', 0.0): 2, ('use', 0.0): 43, ('airfield', 0.0): 1, ('jet', 0.0): 1, (\"jack'\", 0.0): 1, ('spam', 0.0): 6, ('sooo', 0.0): 16, ('☺', 0.0): 3, (\"mommy'\", 0.0): 1, ('reason', 0.0): 8, ('overweight', 0.0): 1, ('sigeg', 0.0): 1, ('habhab', 0.0): 1, ('masud', 0.0): 1, ('kaha', 0.0): 1, ('ko', 0.0): 10, ('akong', 0.0): 1, ('un', 0.0): 1, ('hella', 0.0): 4, ('matter', 0.0): 4, ('pala', 0.0): 1, ('hahaha', 0.0): 11, ('lesson', 0.0): 1, ('dolphin', 0.0): 1, ('xxx', 0.0): 12, ('holi', 0.0): 2, ('anythin', 0.0): 1, ('trend', 0.0): 6, ('radio', 0.0): 4, ('sing', 0.0): 5, ('bewar', 0.0): 1, ('agonis', 0.0): 1, ('experi', 0.0): 2, ('ahead', 0.0): 3, ('modimo', 0.0): 1, ('ho', 0.0): 3, ('tseba', 0.0): 1, ('wena', 0.0): 1, ('fela', 0.0): 1, ('emot', 0.0): 8, ('hubbi', 0.0): 1, ('delight', 0.0): 1, ('return', 0.0): 6, ('bill', 0.0): 6, ('nowt', 0.0): 1, ('wors', 0.0): 8, ('willi', 0.0): 1, ('gon', 0.0): 1, ('vomit', 0.0): 1, ('famou', 0.0): 5, ('bowl', 0.0): 1, ('devast', 0.0): 1, ('titan', 0.0): 1, ('ae', 0.0): 1, ('mark', 0.0): 2, ('hair', 0.0): 21, ('shini', 0.0): 1, ('wavi', 0.0): 1, ('emo', 0.0): 2, ('germani', 0.0): 4, ('load', 0.0): 9, ('shed', 0.0): 2, ('ha', 0.0): 7, ('bheyp', 0.0): 1, ('ayemso', 0.0): 1, ('ear', 0.0): 5, ('swell', 0.0): 2, ('sm', 0.0): 7, ('fb', 0.0): 7, ('remind', 0.0): 3, ('abt', 0.0): 3, ('womad', 0.0): 1, ('wut', 0.0): 1, ('hell', 0.0): 11, ('viciou', 0.0): 1, ('circl', 0.0): 1, ('surpris', 0.0): 5, ('ticket', 0.0): 12, ('codi', 0.0): 1, ('simpson', 0.0): 1, ('concert', 0.0): 11, ('singapor', 0.0): 4, ('august', 0.0): 5, ('pooo', 0.0): 2, ('bh3', 0.0): 1, ('enter', 0.0): 1, ('pitchwar', 0.0): 1, ('chap', 0.0): 1, (\"mine'\", 0.0): 1, ('transcript', 0.0): 1, (\"apma'\", 0.0): 1, ('shoulder', 0.0): 2, ('bitch', 0.0): 11, ('competit', 0.0): 1, (\"it'll\", 0.0): 3, ('fine', 0.0): 6, ('timw', 0.0): 1, ('acc', 0.0): 8, ('rude', 0.0): 11, ('vitamin', 0.0): 1, ('e', 0.0): 9, ('oil', 0.0): 1, ('massag', 0.0): 5, ('everyday', 0.0): 7, ('healthier', 0.0): 1, ('easier', 0.0): 3, ('stretch', 0.0): 1, ('choos', 0.0): 7, ('blockjam', 0.0): 1, (\"schedule'\", 0.0): 1, ('whack', 0.0): 1, ('kik', 0.0): 69, ('thelock', 0.0): 1, ('76', 0.0): 1, ('sex', 0.0): 6, ('omegl', 0.0): 4, ('coupl', 0.0): 2, ('travel', 0.0): 11, ('hotgirl', 0.0): 2, ('2009', 0.0): 1, ('3', 0.0): 32, ('ghantay', 0.0): 1, ('light', 0.0): 8, ('nai', 0.0): 1, ('hay', 0.0): 8, ('deni', 0.0): 1, ('ruin', 0.0): 11, ('laguna', 0.0): 1, ('exit', 0.0): 2, ('gomen', 0.0): 1, ('heck', 0.0): 5, ('fair', 0.0): 12, ('grew', 0.0): 2, ('half', 0.0): 10, ('inch', 0.0): 2, ('two', 0.0): 19, ('problem', 0.0): 7, ('suuuper', 0.0): 1, ('65', 0.0): 1, ('sale', 0.0): 8, ('inact', 0.0): 8, ('orphan', 0.0): 1, ('black', 0.0): 12, ('earlier', 0.0): 9, ('whaaat', 0.0): 5, ('kaya', 0.0): 2, ('naaan', 0.0): 1, ('paus', 0.0): 1, ('randomli', 0.0): 1, ('app', 0.0): 13, ('3:30', 0.0): 1, ('walk', 0.0): 7, ('inglewood', 0.0): 1, ('ummm', 0.0): 4, ('anxieti', 0.0): 3, ('readi', 0.0): 12, ('also', 0.0): 19, ('charcoal', 0.0): 1, ('til', 0.0): 5, ('mid-end', 0.0): 1, ('aug', 0.0): 1, ('noooo', 0.0): 1, ('heard', 0.0): 6, ('rip', 0.0): 12, ('rodfanta', 0.0): 1, ('wasp', 0.0): 2, ('sting', 0.0): 1, ('avert', 0.0): 1, ('bug', 0.0): 3, ('(:', 0.0): 7, ('exo', 0.0): 2, ('seekli', 0.0): 1, ('riptito', 0.0): 1, ('manbearpig', 0.0): 1, ('cannot', 0.0): 7, ('grow', 0.0): 3, ('shorter', 0.0): 1, ('academ', 0.0): 1, ('free', 0.0): 19, ('exclus', 0.0): 2, ('unfair', 0.0): 7, ('esp', 0.0): 4, ('regard', 0.0): 1, ('current', 0.0): 7, ('bleak', 0.0): 1, ('german', 0.0): 1, ('chart', 0.0): 2, ('situat', 0.0): 2, ('entri', 0.0): 4, ('even', 0.0): 70, ('top', 0.0): 6, ('100', 0.0): 8, ('pfft', 0.0): 1, ('place', 0.0): 18, ('white', 0.0): 7, ('wash', 0.0): 1, ('polaroid', 0.0): 1, ('newbethvideo', 0.0): 1, ('greec', 0.0): 2, ('xur', 0.0): 2, ('imi', 0.0): 3, ('fill', 0.0): 1, ('♡', 0.0): 11, ('♥', 0.0): 22, ('xoxoxo', 0.0): 1, ('pictur', 0.0): 17, ('stud', 0.0): 1, ('hund', 0.0): 1, ('6', 0.0): 14, ('kikchat', 0.0): 9, ('amazon', 0.0): 5, ('3.4', 0.0): 1, ('yach', 0.0): 1, ('telat', 0.0): 1, ('huvvft', 0.0): 1, ('zoo', 0.0): 2, ('fieldtrip', 0.0): 1, ('touch', 0.0): 5, ('yan', 0.0): 1, ('posit', 0.0): 2, ('king', 0.0): 1, ('futur', 0.0): 4, ('sizw', 0.0): 1, ('write', 0.0): 13, ('20', 0.0): 9, ('result', 0.0): 3, ('km', 0.0): 2, ('four', 0.0): 4, ('shift', 0.0): 5, ('aaahhh', 0.0): 2, ('boredom', 0.0): 1, ('en', 0.0): 1, ('aint', 0.0): 7, ('who', 0.0): 1, ('sins', 0.0): 1, ('that', 0.0): 13, ('somehow', 0.0): 2, ('tini', 0.0): 4, ('ball', 0.0): 2, ('barbel', 0.0): 1, ('owww', 0.0): 2, ('amsterdam', 0.0): 1, ('luv', 0.0): 2, ('💖', 0.0): 4, ('ps', 0.0): 3, ('looong', 0.0): 1, ('especi', 0.0): 4, (':/', 0.0): 11, ('lap', 0.0): 1, ('litro', 0.0): 1, ('shepherd', 0.0): 2, ('lami', 0.0): 1, ('mayb', 0.0): 27, ('relax', 0.0): 3, ('lungomar', 0.0): 1, ('pesaro', 0.0): 1, ('giachietittiwed', 0.0): 1, ('igersoftheday', 0.0): 1, ('summertim', 0.0): 1, ('nose', 0.0): 7, ('bruis', 0.0): 1, ('lil', 0.0): 8, ('snake', 0.0): 3, ('journey', 0.0): 2, ('scarf', 0.0): 1, ('au', 0.0): 3, ('afford', 0.0): 7, ('fridayfeel', 0.0): 1, ('earli', 0.0): 12, ('money', 0.0): 24, ('chicken', 0.0): 5, ('woe', 0.0): 4, ('nigga', 0.0): 3, ('motn', 0.0): 1, ('make-up', 0.0): 1, ('justic', 0.0): 1, ('import', 0.0): 4, ('sit', 0.0): 5, ('mind', 0.0): 7, ('buy', 0.0): 17, ('limit', 0.0): 4, ('ver', 0.0): 1, ('normal', 0.0): 5, ('edit', 0.0): 7, ('huhuhu', 0.0): 3, ('stack', 0.0): 1, (\"m'ladi\", 0.0): 1, ('j8', 0.0): 1, ('j11', 0.0): 1, ('m20', 0.0): 1, ('jk', 0.0): 5, ('acad', 0.0): 1, ('schedul', 0.0): 9, ('nowww', 0.0): 1, ('cop', 0.0): 1, ('jame', 0.0): 4, ('window', 0.0): 6, ('hugh', 0.0): 2, ('paw', 0.0): 1, ('muddi', 0.0): 1, ('distract', 0.0): 1, ('heyi', 0.0): 1, ('otherwis', 0.0): 3, ('picnic', 0.0): 1, ('24', 0.0): 11, ('cupcak', 0.0): 2, ('talaga', 0.0): 1, ('best', 0.0): 22, ('femal', 0.0): 3, ('poppin', 0.0): 1, ('joc', 0.0): 1, ('playin', 0.0): 1, ('saw', 0.0): 19, ('fix', 0.0): 10, ('coldplay', 0.0): 1, ('media', 0.0): 1, ('player', 0.0): 3, ('fail', 0.0): 10, ('subj', 0.0): 1, ('sobrang', 0.0): 1, ('bv', 0.0): 1, ('zamn', 0.0): 1, ('line', 0.0): 8, ('afropunk', 0.0): 1, ('fest', 0.0): 1, ('brooklyn', 0.0): 2, ('id', 0.0): 5, ('put', 0.0): 14, ('50', 0.0): 5, ('madrid', 0.0): 7, ('shithous', 0.0): 1, ('cutest', 0.0): 2, ('danc', 0.0): 6, ('ur', 0.0): 26, ('arm', 0.0): 3, ('rais', 0.0): 1, ('hand', 0.0): 12, ('ladder', 0.0): 2, ('told', 0.0): 11, ('climb', 0.0): 3, ('success', 0.0): 4, ('nerv', 0.0): 1, ('wrack', 0.0): 1, ('test', 0.0): 8, ('booset', 0.0): 1, ('restart', 0.0): 1, ('assassin', 0.0): 1, ('creed', 0.0): 1, ('ii', 0.0): 1, ('heap', 0.0): 1, ('fell', 0.0): 10, ('daughter', 0.0): 1, ('begin', 0.0): 4, ('ps3', 0.0): 1, ('ankl', 0.0): 4, ('step', 0.0): 5, ('puddl', 0.0): 2, ('wear', 0.0): 5, ('slipper', 0.0): 1, ('eve', 0.0): 1, ('bbi', 0.0): 6, ('sararoc', 0.0): 1, ('angri', 0.0): 5, ('pretti', 0.0): 15, ('fnaf', 0.0): 1, ('holiday', 0.0): 20, ('cheer', 0.0): 6, ('😘', 0.0): 11, ('anywayhedidanicejob', 0.0): 1, ('😞', 0.0): 3, ('3am', 0.0): 2, ('other', 0.0): 7, ('local', 0.0): 3, ('cruis', 0.0): 1, ('done', 0.0): 24, ('doubl', 0.0): 4, ('wail', 0.0): 1, ('manual', 0.0): 2, ('wheelchair', 0.0): 1, ('check', 0.0): 19, ('fit', 0.0): 3, ('nh', 0.0): 3, ('26week', 0.0): 1, ('sbenu', 0.0): 1, ('sasin', 0.0): 1, ('team', 0.0): 14, ('anarchi', 0.0): 1, ('af', 0.0): 14, ('candl', 0.0): 1, ('forehead', 0.0): 4, ('medicin', 0.0): 3, ('welcom', 0.0): 5, ('oop', 0.0): 4, ('hoya', 0.0): 3, ('mah', 0.0): 2, ('a', 0.0): 1, ('nobodi', 0.0): 10, ('awhil', 0.0): 2, ('ago', 0.0): 20, ('b', 0.0): 10, ('hush', 0.0): 2, ('gurli', 0.0): 1, ('bring', 0.0): 9, ('purti', 0.0): 1, ('mouth', 0.0): 5, ('closer', 0.0): 2, ('shiver', 0.0): 1, ('solut', 0.0): 1, ('paid', 0.0): 8, ('properli', 0.0): 2, ('gol', 0.0): 1, ('pea', 0.0): 1, ('english', 0.0): 9, ('mental', 0.0): 4, ('tierd', 0.0): 2, ('third', 0.0): 1, (\"eye'\", 0.0): 1, ('thnkyouuu', 0.0): 1, ('carolin', 0.0): 1, ('neither', 0.0): 6, ('figur', 0.0): 6, ('mirror', 0.0): 1, ('highlight', 0.0): 2, ('pure', 0.0): 3, ('courag', 0.0): 1, ('bit', 0.0): 15, ('fishi', 0.0): 1, ('idek', 0.0): 1, ('apink', 0.0): 5, ('perform', 0.0): 8, ('bulet', 0.0): 1, ('gendut', 0.0): 1, ('noo', 0.0): 5, ('race', 0.0): 3, ('hotwheel', 0.0): 1, ('ms', 0.0): 1, ('patch', 0.0): 1, ('typic', 0.0): 2, ('ahaha', 0.0): 1, ('lay', 0.0): 2, ('wine', 0.0): 1, ('glass', 0.0): 3, (\"where'\", 0.0): 4, ('akon', 0.0): 1, ('somewher', 0.0): 5, ('nightmar', 0.0): 7, ('ya', 0.0): 15, ('mino', 0.0): 2, ('crazyyi', 0.0): 1, ('thooo', 0.0): 1, ('zz', 0.0): 1, ('airport', 0.0): 7, ('straight', 0.0): 4, ('soundcheck', 0.0): 1, ('hmm', 0.0): 4, ('antagonist', 0.0): 1, ('ob', 0.0): 1, ('phantasi', 0.0): 1, ('star', 0.0): 4, ('ip', 0.0): 1, ('issu', 0.0): 11, ('bruce', 0.0): 1, ('sleepdepriv', 0.0): 1, ('tiredashel', 0.0): 1, ('4aspot', 0.0): 1, (\"kinara'\", 0.0): 1, ('awami', 0.0): 1, ('question', 0.0): 9, ('niqqa', 0.0): 1, ('answer', 0.0): 14, ('mockingjay', 0.0): 1, ('slow', 0.0): 9, ('pb.contest', 0.0): 1, ('cycl', 0.0): 2, ('aarww', 0.0): 1, ('lmbo', 0.0): 1, ('dangit', 0.0): 1, ('ohmygod', 0.0): 1, ('scenario', 0.0): 1, ('tooo', 0.0): 2, ('duck', 0.0): 1, ('baechyyi', 0.0): 1, ('okayyy', 0.0): 1, ('noon', 0.0): 3, ('drag', 0.0): 5, ('serious', 0.0): 11, ('misundersrand', 0.0): 1, ('chal', 0.0): 1, ('raha', 0.0): 1, ('hai', 0.0): 11, ('yhm', 0.0): 1, ('edsa', 0.0): 2, ('jasmingarrick', 0.0): 2, ('kikmeguy', 0.0): 5, ('webcam', 0.0): 2, ('milf', 0.0): 1, ('nakamaforev', 0.0): 3, ('kiksex', 0.0): 7, (\"unicef'\", 0.0): 1, ('fu', 0.0): 1, ('alon', 0.0): 16, ('manag', 0.0): 13, ('stephen', 0.0): 1, ('street', 0.0): 2, ('35', 0.0): 1, ('min', 0.0): 7, ('appear', 0.0): 2, ('record', 0.0): 6, ('coz', 0.0): 4, ('frustrat', 0.0): 6, ('sent', 0.0): 9, ('interest', 0.0): 9, ('woza', 0.0): 1, ('promis', 0.0): 4, ('senight', 0.0): 1, ('468', 0.0): 1, ('kikmeboy', 0.0): 9, ('gay', 0.0): 6, ('teen', 0.0): 7, ('amateur', 0.0): 5, ('hotscratch', 0.0): 1, ('sell', 0.0): 8, ('sock', 0.0): 6, ('150-160', 0.0): 1, ('peso', 0.0): 1, ('gotta', 0.0): 8, ('pay', 0.0): 8, ('degrassi', 0.0): 1, ('4-6', 0.0): 1, ('bcz', 0.0): 1, ('kat', 0.0): 3, ('chem', 0.0): 2, ('onscreen', 0.0): 1, ('ofscreen', 0.0): 1, ('kinda', 0.0): 10, ('pak', 0.0): 4, ('class', 0.0): 10, ('monthli', 0.0): 1, ('roll', 0.0): 4, ('band', 0.0): 2, ('throw', 0.0): 2, ('ironi', 0.0): 2, ('rhisfor', 0.0): 1, ('500', 0.0): 2, ('bestoftheday', 0.0): 3, ('chat', 0.0): 9, ('camsex', 0.0): 5, ('unfollow', 0.0): 11, ('particular', 0.0): 1, ('support', 0.0): 26, ('bae', 0.0): 11, ('poopi', 0.0): 1, ('pip', 0.0): 1, ('post', 0.0): 12, ('felt', 0.0): 6, ('uff', 0.0): 1, ('1.300', 0.0): 1, ('credit', 0.0): 3, ('glue', 0.0): 1, ('factori', 0.0): 1, ('kuchar', 0.0): 1, ('fast', 0.0): 7, ('graduat', 0.0): 3, ('up', 0.0): 2, ('definit', 0.0): 3, ('uni', 0.0): 2, ('ee', 0.0): 1, ('tommi', 0.0): 1, ('georgia', 0.0): 2, ('bout', 0.0): 2, ('instant', 0.0): 1, ('transmiss', 0.0): 1, ('malik', 0.0): 1, ('orang', 0.0): 2, ('suma', 0.0): 1, ('shouldeeerr', 0.0): 1, ('outfit', 0.0): 5, ('age', 0.0): 8, ('repack', 0.0): 3, ('group', 0.0): 4, ('charl', 0.0): 1, ('grown', 0.0): 2, ('rememb', 0.0): 17, ('dy', 0.0): 1, ('rihanna', 0.0): 1, ('red', 0.0): 4, ('ging', 0.0): 2, ('boot', 0.0): 4, ('closest', 0.0): 3, ('nike', 0.0): 1, ('adida', 0.0): 1, ('inform', 0.0): 4, ('pro@illamasqua.com', 0.0): 1, ('set', 0.0): 13, ('ifeely', 0.0): 1, ('harder', 0.0): 2, ('usual', 0.0): 7, ('ratbaglat', 0.0): 1, ('second', 0.0): 5, ('semest', 0.0): 2, ('gin', 0.0): 1, ('gut', 0.0): 12, ('reynold', 0.0): 1, ('dessert', 0.0): 2, ('season', 0.0): 9, ('villag', 0.0): 1, ('differ', 0.0): 10, ('citi', 0.0): 11, ('unit', 0.0): 3, ('oppress', 0.0): 1, ('mass', 0.0): 2, ('wat', 0.0): 5, ('afghanistn', 0.0): 1, ('war', 0.0): 2, ('tore', 0.0): 1, ('sunggyu', 0.0): 5, ('injur', 0.0): 7, ('plaster', 0.0): 2, ('rtd', 0.0): 1, ('loui', 0.0): 4, ('harri', 0.0): 10, ('5so', 0.0): 7, ('crowd', 0.0): 1, ('stadium', 0.0): 4, ('welder', 0.0): 1, ('ghost', 0.0): 1, ('hogo', 0.0): 1, ('vishaya', 0.0): 1, ('adu', 0.0): 1, ('bjp', 0.0): 1, ('madatt', 0.0): 1, ('anta', 0.0): 1, ('vishwa', 0.0): 1, ('ne', 0.0): 3, ('illa', 0.0): 1, ('wua', 0.0): 1, ('picki', 0.0): 1, ('finger', 0.0): 8, ('favourit', 0.0): 9, ('mutual', 0.0): 2, ('gn', 0.0): 1, ('along', 0.0): 3, ('ass', 0.0): 9, ('thent', 0.0): 1, ('423', 0.0): 1, ('sabadodeganarseguidor', 0.0): 2, ('sexual', 0.0): 4, ('sync', 0.0): 2, ('plug.dj', 0.0): 1, ('peel', 0.0): 1, ('suspems', 0.0): 1, ('cope', 0.0): 3, ('offroad', 0.0): 1, ('adventur', 0.0): 1, ('there', 0.0): 5, ('harvest', 0.0): 1, ('machineri', 0.0): 1, ('inapropri', 0.0): 1, ('weav', 0.0): 2, ('nowher', 0.0): 3, ('decent', 0.0): 2, ('invest', 0.0): 2, ('scottish', 0.0): 1, ('footbal', 0.0): 3, ('dire', 0.0): 2, ('nomoney', 0.0): 1, ('nawf', 0.0): 1, ('sum', 0.0): 2, ('becho', 0.0): 1, ('danni', 0.0): 3, ('eng', 0.0): 2, (\"let'\", 0.0): 5, ('overli', 0.0): 2, ('lab', 0.0): 1, ('ty', 0.0): 3, ('zap', 0.0): 1, ('distress', 0.0): 1, ('shot', 0.0): 6, ('cinema', 0.0): 4, ('louisianashoot', 0.0): 1, ('laugh', 0.0): 7, ('har', 0.0): 3, (\"how'\", 0.0): 5, ('chum', 0.0): 1, ('ncc', 0.0): 1, ('ph', 0.0): 2, ('balik', 0.0): 1, ('naman', 0.0): 1, ('kayo', 0.0): 1, ('itong', 0.0): 1, ('shirt', 0.0): 3, ('thaaat', 0.0): 1, ('ctto', 0.0): 1, ('expir', 0.0): 3, ('bi', 0.0): 2, ('tough', 0.0): 2, ('11', 0.0): 4, ('3:33', 0.0): 2, ('jfc', 0.0): 1, ('bio', 0.0): 3, ('bodo', 0.0): 1, ('amat', 0.0): 1, ('quick', 0.0): 5, ('yelaaa', 0.0): 1, ('dublin', 0.0): 2, ('potter', 0.0): 1, ('marathon', 0.0): 3, ('balanc', 0.0): 2, ('warm', 0.0): 5, ('comic', 0.0): 5, ('pine', 0.0): 1, ('keybind', 0.0): 1, ('featur', 0.0): 4, ('wild', 0.0): 2, ('warfar', 0.0): 1, ('control', 0.0): 2, ('diagnos', 0.0): 1, ('wiv', 0.0): 1, (\"scheuermann'\", 0.0): 1, ('diseas', 0.0): 3, ('bone', 0.0): 1, ('rlyhurt', 0.0): 1, ('howdo', 0.0): 1, ('georgesampson', 0.0): 1, ('stand', 0.0): 6, ('signal', 0.0): 3, ('reckon', 0.0): 1, ('t20', 0.0): 1, ('action', 0.0): 2, ('taunton', 0.0): 1, ('vacat', 0.0): 3, ('excit', 0.0): 6, ('justiceforsandrabland', 0.0): 2, ('sandrabland', 0.0): 6, ('disturb', 0.0): 1, ('women', 0.0): 5, ('happpi', 0.0): 1, ('justinbieb', 0.0): 4, ('daianerufato', 0.0): 3, ('ilysm', 0.0): 3, ('2015', 0.0): 12, ('07:34', 0.0): 1, ('delphi', 0.0): 2, ('weak', 0.0): 2, ('dom', 0.0): 2, ('techniqu', 0.0): 1, ('minc', 0.0): 2, ('complet', 0.0): 9, ('symphoni', 0.0): 1, ('joe', 0.0): 3, ('co', 0.0): 6, ('wth', 0.0): 2, ('aisyhhh', 0.0): 1, ('bald', 0.0): 1, ('14', 0.0): 3, ('seungchan', 0.0): 1, ('aigooo', 0.0): 1, ('riri', 0.0): 1, ('origin', 0.0): 6, ('depend', 0.0): 2, ('vet', 0.0): 1, ('major', 0.0): 2, ('va', 0.0): 1, ('kept', 0.0): 2, ('lumin', 0.0): 1, ('follback', 0.0): 2, ('treat', 0.0): 5, ('v', 0.0): 6, ('product', 0.0): 4, ('letter', 0.0): 1, ('z', 0.0): 5, ('uniqu', 0.0): 2, ('refresh', 0.0): 1, ('popular', 0.0): 1, ('bebee', 0.0): 2, ('lt', 0.0): 1, ('inaccuraci', 0.0): 1, ('inaccur', 0.0): 1, ('worri', 0.0): 8, ('burn', 0.0): 4, ('rn', 0.0): 17, ('tragic', 0.0): 1, ('joy', 0.0): 2, ('sam', 0.0): 4, ('rush', 0.0): 2, ('toronto', 0.0): 1, ('stuart', 0.0): 1, (\"party'\", 0.0): 2, ('iyalaya', 0.0): 1, ('shade', 0.0): 3, ('round', 0.0): 3, ('clock', 0.0): 2, (';(', 0.0): 6, ('happier', 0.0): 1, ('h', 0.0): 8, ('ubusi', 0.0): 1, ('le', 0.0): 3, ('fifa', 0.0): 1, ('gymnast', 0.0): 1, ('aahhh', 0.0): 1, ('noggin', 0.0): 1, ('bump', 0.0): 1, ('feelslikeanidiot', 0.0): 1, ('pregnant', 0.0): 2, ('woman', 0.0): 5, ('dearli', 0.0): 1, ('sunshin', 0.0): 4, ('suk', 0.0): 2, ('pumpkin', 0.0): 1, ('scone', 0.0): 1, ('outnumb', 0.0): 1, ('vidcon', 0.0): 10, ('eri', 0.0): 1, ('geez', 0.0): 1, ('preciou', 0.0): 4, ('hive', 0.0): 1, ('vote', 0.0): 7, ('vietnam', 0.0): 1, ('decemb', 0.0): 2, ('dunt', 0.0): 1, ('ikr', 0.0): 3, ('sob', 0.0): 3, ('buff', 0.0): 1, ('leg', 0.0): 4, ('toni', 0.0): 1, ('deactiv', 0.0): 6, ('bra', 0.0): 2, (\"shady'\", 0.0): 1, ('isibaya', 0.0): 1, ('special', 0.0): 3, ('❤', 0.0): 21, ('️', 0.0): 19, ('😓', 0.0): 2, ('slept', 0.0): 5, ('colder', 0.0): 1, ('took', 0.0): 9, ('med', 0.0): 1, ('sausag', 0.0): 1, ('adio', 0.0): 1, ('cold', 0.0): 15, ('sore', 0.0): 9, ('ew', 0.0): 3, ('h8', 0.0): 1, ('messeng', 0.0): 2, ('shittier', 0.0): 1, ('leno', 0.0): 1, ('ident', 0.0): 1, ('crisi', 0.0): 2, ('roommat', 0.0): 1, ('knock', 0.0): 3, ('nighter', 0.0): 3, ('bird', 0.0): 2, ('flew', 0.0): 2, ('thru', 0.0): 2, ('derek', 0.0): 3, ('tour', 0.0): 7, ('wetherspoon', 0.0): 1, ('pub', 0.0): 1, ('polic', 0.0): 4, ('frank', 0.0): 2, ('ocean', 0.0): 4, ('releas', 0.0): 8, ('ff', 0.0): 4, ('lisah', 0.0): 2, ('kikm', 0.0): 8, ('eboni', 0.0): 2, ('weloveyounamjoon', 0.0): 1, ('gave', 0.0): 8, ('dress', 0.0): 6, ('polka', 0.0): 1, ('dot', 0.0): 2, ('ndi', 0.0): 1, ('yum', 0.0): 1, ('feed', 0.0): 3, ('leftov', 0.0): 2, ('side', 0.0): 6, ('cs', 0.0): 2, ('own', 0.0): 1, ('walnut', 0.0): 1, ('whip', 0.0): 1, ('wife', 0.0): 6, ('boah', 0.0): 1, ('madi', 0.0): 2, ('def', 0.0): 3, ('manga', 0.0): 1, ('giant', 0.0): 3, ('aminormalyet', 0.0): 1, ('cooki', 0.0): 2, ('breakfast', 0.0): 5, ('clutch', 0.0): 1, ('poorli', 0.0): 6, ('tummi', 0.0): 6, ('pj', 0.0): 1, ('groan', 0.0): 1, ('nou', 0.0): 1, ('adam', 0.0): 2, ('ken', 0.0): 1, ('sara', 0.0): 2, ('sister', 0.0): 4, ('accid', 0.0): 2, ('sort', 0.0): 7, ('mate', 0.0): 2, ('pick', 0.0): 12, ('rang', 0.0): 4, ('fk', 0.0): 2, ('freak', 0.0): 5, ('describ', 0.0): 1, ('eric', 0.0): 2, ('prydz', 0.0): 1, ('sister-in-law', 0.0): 1, ('instal', 0.0): 2, ('seat', 0.0): 4, ('bought', 0.0): 6, ('rear-end', 0.0): 1, (\"everyone'\", 0.0): 4, ('trash', 0.0): 2, ('boob', 0.0): 3, ('whilst', 0.0): 3, ('stair', 0.0): 1, ('childhood', 0.0): 1, ('toothsensit', 0.0): 4, ('size', 0.0): 9, ('ke', 0.0): 3, ('shem', 0.0): 2, ('trust', 0.0): 2, ('awel', 0.0): 1, ('drunk', 0.0): 2, ('weekendofmad', 0.0): 1, ('🍹', 0.0): 3, ('🍸', 0.0): 1, ('cb', 0.0): 1, ('dancer', 0.0): 1, ('choregraph', 0.0): 1, ('626-430-8715', 0.0): 1, ('messag', 0.0): 8, ('repli', 0.0): 14, ('hoe', 0.0): 1, ('xd', 0.0): 7, ('xiu', 0.0): 1, ('nk', 0.0): 1, ('gi', 0.0): 2, ('uss', 0.0): 1, ('eliss', 0.0): 1, ('ksoo', 0.0): 2, ('session', 0.0): 5, ('tat', 0.0): 1, ('bcoz', 0.0): 1, ('bet', 0.0): 10, ('rancho', 0.0): 1, ('imperi', 0.0): 1, ('de', 0.0): 1, ('silang', 0.0): 1, ('subdivis', 0.0): 1, ('center', 0.0): 1, ('39', 0.0): 1, ('cornwal', 0.0): 1, ('verit', 0.0): 1, ('prize', 0.0): 2, ('regular', 0.0): 3, ('workout', 0.0): 1, ('spin', 0.0): 1, ('base', 0.0): 1, ('upon', 0.0): 1, ('penni', 0.0): 1, ('ebook', 0.0): 1, ('фотосет', 0.0): 1, ('addicted-to-analsex', 0.0): 1, ('sweetbj', 0.0): 2, ('blowjob', 0.0): 1, ('mhhh', 0.0): 1, ('sed', 0.0): 1, ('sg', 0.0): 1, ('dinner', 0.0): 4, ('bless', 0.0): 2, ('mee', 0.0): 2, ('enviou', 0.0): 1, ('eonni', 0.0): 1, ('lovey', 0.0): 1, ('dovey', 0.0): 1, ('dongsaeng', 0.0): 1, ('workin', 0.0): 1, ('tuesday', 0.0): 4, ('schade', 0.0): 3, ('belfast', 0.0): 1, ('jealou', 0.0): 9, ('jacob', 0.0): 5, ('isco', 0.0): 4, ('peni', 0.0): 1, ('everi', 0.0): 16, ('convers', 0.0): 6, ('wonder', 0.0): 11, ('soul', 0.0): 5, ('nation', 0.0): 2, ('louisiana', 0.0): 4, ('lafayett', 0.0): 2, ('matteroftheheart', 0.0): 1, ('waduh', 0.0): 1, ('pant', 0.0): 3, ('suspend', 0.0): 2, ('believ', 0.0): 14, ('teenag', 0.0): 2, ('clich', 0.0): 1, ('youuu', 0.0): 5, ('rma', 0.0): 1, ('jersey', 0.0): 2, ('fake', 0.0): 4, ('jaclintil', 0.0): 1, ('model', 0.0): 9, ('likeforlik', 0.0): 7, ('mpoint', 0.0): 4, ('hotfmnoaidilforariana', 0.0): 2, ('ran', 0.0): 5, ('fuckkk', 0.0): 1, ('jump', 0.0): 3, ('justin', 0.0): 3, ('finish', 0.0): 14, ('sanum', 0.0): 1, ('llaollao', 0.0): 1, ('foood', 0.0): 1, ('ubericecream', 0.0): 14, ('glare', 0.0): 1, ('vine', 0.0): 3, ('tweetin', 0.0): 1, ('mood', 0.0): 3, ('elbow', 0.0): 1, ('choreo', 0.0): 1, ('offens', 0.0): 2, ('yeyi', 0.0): 1, ('hd', 0.0): 2, ('brow', 0.0): 1, ('kit', 0.0): 6, ('slightli', 0.0): 2, ('monday', 0.0): 10, ('sux', 0.0): 1, ('enjoy', 0.0): 9, ('nothaveld', 0.0): 1, ('765', 0.0): 1, ('edm', 0.0): 1, ('likeforfollow', 0.0): 3, ('hannib', 0.0): 3, ('mosquito', 0.0): 2, ('bite', 0.0): 5, ('kinki', 0.0): 1, ('hsould', 0.0): 1, ('justget', 0.0): 1, ('marri', 0.0): 2, ('la', 0.0): 11, ('shuffl', 0.0): 4, ('int', 0.0): 1, ('buckl', 0.0): 1, ('spring', 0.0): 1, ('millz', 0.0): 1, ('aski', 0.0): 2, ('awusasho', 0.0): 1, ('unlucki', 0.0): 2, ('driver', 0.0): 7, ('briefli', 0.0): 1, ('spot', 0.0): 4, ('144p', 0.0): 1, ('brook', 0.0): 1, ('crack', 0.0): 2, ('＠', 0.0): 5, ('maverickgam', 0.0): 4, ('07:32', 0.0): 1, ('07:25', 0.0): 1, ('max', 0.0): 3, ('file', 0.0): 2, ('extern', 0.0): 2, ('sd', 0.0): 1, ('via', 0.0): 1, ('airdroid', 0.0): 1, ('android', 0.0): 2, ('4.4+', 0.0): 1, ('googl', 0.0): 5, ('alright', 0.0): 3, ('cramp', 0.0): 2, ('&lt;/3', 0.0): 6, ('unstan', 0.0): 1, ('tay', 0.0): 2, ('ngeze', 0.0): 1, ('cocktaili', 0.0): 1, ('classi', 0.0): 1, ('07:24', 0.0): 1, ('✈', 0.0): 2, ('️2', 0.0): 1, ('raini', 0.0): 2, ('☔', 0.0): 2, ('peter', 0.0): 1, ('pen', 0.0): 1, ('spare', 0.0): 1, ('guest', 0.0): 2, ('barcelona', 0.0): 2, ('bilbao', 0.0): 1, ('booti', 0.0): 2, ('sharyl', 0.0): 1, ('shane', 0.0): 2, ('ta', 0.0): 1, ('giddi', 0.0): 1, ('d1', 0.0): 1, ('zipper', 0.0): 1, ('beyond', 0.0): 1, ('repair', 0.0): 4, ('iphon', 0.0): 5, ('upgrad', 0.0): 1, ('april', 0.0): 1, ('2016', 0.0): 1, ('cont', 0.0): 2, ('england', 0.0): 4, ('wore', 0.0): 2, ('greet', 0.0): 5, ('tempt', 0.0): 2, ('whole', 0.0): 16, ('pack', 0.0): 6, ('oreo', 0.0): 2, ('strength', 0.0): 1, ('wifi', 0.0): 5, ('network', 0.0): 4, ('within', 0.0): 3, ('lolipop', 0.0): 1, ('kebab', 0.0): 1, ('klappertart', 0.0): 1, ('cake', 0.0): 10, ('moodbost', 0.0): 2, ('shoot', 0.0): 6, ('unprepar', 0.0): 1, ('sri', 0.0): 1, ('dresscod', 0.0): 1, ('door', 0.0): 6, ('iam', 0.0): 2, ('dnt', 0.0): 1, ('stab', 0.0): 3, ('meh', 0.0): 3, ('wrocilam', 0.0): 1, ('otp', 0.0): 3, ('5', 0.0): 14, ('looww', 0.0): 1, ('recov', 0.0): 2, ('wayn', 0.0): 2, ('insur', 0.0): 3, ('loss', 0.0): 3, ('stolen', 0.0): 2, ('accident', 0.0): 1, ('damag', 0.0): 5, ('devic', 0.0): 3, ('warranti', 0.0): 1, ('centr', 0.0): 2, ('👌', 0.0): 1, ('lmfaoo', 0.0): 1, ('accur', 0.0): 2, ('fra', 0.0): 4, ('aliv', 0.0): 2, ('steel', 0.0): 2, ('otamendi', 0.0): 1, ('ny', 0.0): 2, ('🚖', 0.0): 1, ('🗽', 0.0): 1, ('🌃', 0.0): 1, ('stealth', 0.0): 2, ('bastard', 0.0): 2, ('inc', 0.0): 3, ('steam', 0.0): 2, ('therapi', 0.0): 1, ('exhaust', 0.0): 3, ('lie', 0.0): 7, ('total', 0.0): 11, ('block', 0.0): 11, ('choic', 0.0): 5, ('switzerland', 0.0): 1, ('kfc', 0.0): 1, ('common', 0.0): 4, ('th', 0.0): 5, ('wolrd', 0.0): 1, ('fyn', 0.0): 1, ('drop', 0.0): 10, ('state', 0.0): 4, ('3g', 0.0): 2, ('christ', 0.0): 1, ('scale', 0.0): 1, ('deck', 0.0): 1, ('chair', 0.0): 4, ('yk', 0.0): 1, ('resi', 0.0): 1, ('memori', 0.0): 5, ('nude', 0.0): 4, ('bruh', 0.0): 3, ('prepar', 0.0): 3, ('lock', 0.0): 2, ('view', 0.0): 7, ('fbc', 0.0): 3, ('mork', 0.0): 1, ('873', 0.0): 1, ('kikgirl', 0.0): 13, ('premiostumundo', 0.0): 2, ('hotspotwithdanri', 0.0): 1, ('hospit', 0.0): 3, ('food', 0.0): 18, ('sone', 0.0): 1, ('produc', 0.0): 1, ('potag', 0.0): 1, ('tomato', 0.0): 1, ('blight', 0.0): 1, ('sheffield', 0.0): 1, ('mych', 0.0): 1, ('shiiit', 0.0): 2, ('screenshot', 0.0): 4, ('prompt', 0.0): 1, ('areadi', 0.0): 1, ('similar', 0.0): 4, ('soulmat', 0.0): 1, ('canon', 0.0): 1, ('zzz', 0.0): 2, ('britain', 0.0): 1, ('😁', 0.0): 3, ('mana', 0.0): 2, ('hw', 0.0): 1, ('jouch', 0.0): 1, ('por', 0.0): 1, ('que', 0.0): 1, ('liceooo', 0.0): 1, ('30', 0.0): 3, ('minut', 0.0): 6, ('pass', 0.0): 13, ('ayala', 0.0): 1, ('tunnel', 0.0): 2, ('thatscold', 0.0): 1, ('80', 0.0): 1, ('snap', 0.0): 3, ('lourd', 0.0): 1, ('bang', 0.0): 3, ('anywher', 0.0): 4, ('water', 0.0): 8, ('road', 0.0): 1, ('showbox', 0.0): 1, ('naruto', 0.0): 1, ('cartoon', 0.0): 1, ('companion', 0.0): 2, ('skinni', 0.0): 3, ('fat', 0.0): 4, ('bare', 0.0): 6, ('dubai', 0.0): 3, ('calum', 0.0): 1, ('ashton', 0.0): 1, ('✧', 0.0): 8, ('｡', 0.0): 8, ('chelni', 0.0): 4, ('disappoint', 0.0): 13, ('everybodi', 0.0): 5, ('due', 0.0): 14, ('laribuggi', 0.0): 1, ('medic', 0.0): 1, ('nutella', 0.0): 1, (\"could'v\", 0.0): 3, ('siriu', 0.0): 1, ('goat', 0.0): 4, ('frudg', 0.0): 1, ('mike', 0.0): 1, ('cloth', 0.0): 6, ('stuff', 0.0): 11, ('sat', 0.0): 3, ('number', 0.0): 6, ('ring', 0.0): 1, ('bbz', 0.0): 1, ('angek', 0.0): 1, ('sbali', 0.0): 1, ('euuuwww', 0.0): 2, ('lunch', 0.0): 10, ('construct', 0.0): 3, ('worker', 0.0): 3, ('1k', 0.0): 3, ('style', 0.0): 4, ('nell', 0.0): 1, ('ik', 0.0): 2, ('death', 0.0): 3, ('jaysu', 0.0): 1, ('toast', 0.0): 1, ('insecur', 0.0): 2, ('buti', 0.0): 1, ('ure', 0.0): 2, ('poop', 0.0): 1, ('gorgeou', 0.0): 2, ('angel', 0.0): 2, ('rome', 0.0): 1, ('throat', 0.0): 10, ('llama', 0.0): 1, ('urself', 0.0): 2, ('getwellsoonamb', 0.0): 1, ('heath', 0.0): 2, ('ledger', 0.0): 1, ('appl', 0.0): 3, ('permiss', 0.0): 2, ('2-0', 0.0): 1, ('lead', 0.0): 3, ('supersport', 0.0): 1, ('milkshak', 0.0): 1, ('witcher', 0.0): 1, ('papertown', 0.0): 1, ('bale', 0.0): 1, ('9', 0.0): 5, ('méxico', 0.0): 1, ('bahay', 0.0): 1, ('bahayan', 0.0): 1, ('magisa', 0.0): 1, ('sadlyf', 0.0): 1, ('bunso', 0.0): 1, ('sleeep', 0.0): 4, ('astonvilla', 0.0): 1, ('berigaud', 0.0): 1, ('bakar', 0.0): 1, ('club', 0.0): 4, ('dear', 0.0): 11, ('allerg', 0.0): 4, ('depress', 0.0): 5, (\"blaine'\", 0.0): 1, ('acoust', 0.0): 2, ('version', 0.0): 5, ('excus', 0.0): 3, ('hernia', 0.0): 3, ('toxin', 0.0): 1, ('freedom', 0.0): 1, ('organ', 0.0): 2, ('ariel', 0.0): 1, ('slap', 0.0): 1, ('slam', 0.0): 1, ('bee', 0.0): 1, ('unknown', 0.0): 2, ('finddjderek', 0.0): 1, ('smell', 0.0): 3, ('uuughhh', 0.0): 1, ('grabe', 0.0): 5, ('ka', 0.0): 5, ('where', 0.0): 1, ('gf', 0.0): 3, ('james_yammouni', 0.0): 1, ('smi', 0.0): 1, ('nemesi', 0.0): 1, ('rule', 0.0): 1, ('doesnt', 0.0): 2, ('appeal', 0.0): 1, ('neeein', 0.0): 1, ('saaad', 0.0): 3, ('less', 0.0): 3, ('hang', 0.0): 7, ('creas', 0.0): 1, ('tan', 0.0): 3, ('dalla', 0.0): 4, ('suppos', 0.0): 7, ('infront', 0.0): 2, ('beato', 0.0): 1, ('tim', 0.0): 2, ('prob', 0.0): 5, ('minha', 0.0): 1, ('deleici', 0.0): 1, ('hr', 0.0): 2, ('pcb', 0.0): 1, ('ep', 0.0): 5, ('peregrin', 0.0): 1, ('8.40', 0.0): 1, ('pigeon', 0.0): 1, ('feet', 0.0): 3, ('tram', 0.0): 1, ('hav', 0.0): 2, ('spent', 0.0): 5, ('outsid', 0.0): 9, ('apt', 0.0): 1, ('build', 0.0): 3, ('key', 0.0): 3, ('bldg', 0.0): 1, ('wrote', 0.0): 3, ('dark', 0.0): 5, ('swan', 0.0): 1, ('fifth', 0.0): 2, ('mmmm', 0.0): 1, ('avi', 0.0): 4, ('nicki', 0.0): 1, ('fucjikg', 0.0): 1, ('disgust', 0.0): 6, ('buynotanapologyonitun', 0.0): 1, ('aval', 0.0): 1, ('denmark', 0.0): 1, ('nw', 0.0): 2, ('sch', 0.0): 2, ('share', 0.0): 11, ('jeslyn', 0.0): 1, ('72', 0.0): 4, ('root', 0.0): 2, ('kuch', 0.0): 1, ('nahi', 0.0): 1, ('hua', 0.0): 2, ('newbi', 0.0): 1, ('crap', 0.0): 3, ('miracl', 0.0): 1, ('4th', 0.0): 1, ('linda', 0.0): 1, ('click', 0.0): 1, ('pin', 0.0): 2, ('wing', 0.0): 3, ('epic', 0.0): 2, ('page', 0.0): 6, ('ang', 0.0): 8, ('ganda', 0.0): 1, ('💗', 0.0): 4, ('nux', 0.0): 1, ('hinanap', 0.0): 1, ('ako', 0.0): 1, ('uy', 0.0): 1, ('sched', 0.0): 1, ('anyar', 0.0): 1, ('entertain', 0.0): 2, ('typa', 0.0): 3, ('buddi', 0.0): 2, ('transpar', 0.0): 1, ('photoshop', 0.0): 2, ('planner', 0.0): 1, ('helppp', 0.0): 2, ('wearig', 0.0): 1, ('dri', 0.0): 2, ('alot', 0.0): 3, ('bu', 0.0): 5, ('prey', 0.0): 1, ('gross', 0.0): 5, ('drain', 0.0): 3, ('ausfailia', 0.0): 1, ('snow', 0.0): 3, ('footi', 0.0): 3, ('2nd', 0.0): 5, ('row', 0.0): 3, (\"m'\", 0.0): 2, ('kitkat', 0.0): 2, ('bday', 0.0): 7, ('😢', 0.0): 8, ('suger', 0.0): 1, ('olivia', 0.0): 2, ('audit', 0.0): 1, ('american', 0.0): 1, ('idol', 0.0): 2, ('injuri', 0.0): 2, ('appendix', 0.0): 1, ('burst', 0.0): 2, ('append', 0.0): 1, ('yeahh', 0.0): 2, ('fack', 0.0): 2, ('nhl', 0.0): 1, ('khami', 0.0): 2, ('favorit', 0.0): 4, ('rise', 0.0): 3, ('reaali', 0.0): 1, ('ja', 0.0): 2, ('naomi', 0.0): 1, ('modern', 0.0): 1, ('contemporari', 0.0): 1, ('slack', 0.0): 1, ('565', 0.0): 1, ('blond', 0.0): 2, ('jahat', 0.0): 3, ('discount', 0.0): 1, ('thorp', 0.0): 2, ('park', 0.0): 7, ('esnho', 0.0): 1, ('node', 0.0): 1, ('advanc', 0.0): 4, ('directx', 0.0): 1, ('workshop', 0.0): 1, ('p2', 0.0): 1, ('upload', 0.0): 2, ('remov', 0.0): 5, ('blackberri', 0.0): 1, ('shitti', 0.0): 1, ('mobil', 0.0): 2, ('povertyyouareevil', 0.0): 1, ('struggl', 0.0): 4, ('math', 0.0): 1, ('emm', 0.0): 1, ('data', 0.0): 6, ('elgin', 0.0): 1, ('vava', 0.0): 1, ('makati', 0.0): 1, ('💛', 0.0): 4, ('baon', 0.0): 1, ('soup', 0.0): 3, ('soak', 0.0): 1, ('bread', 0.0): 2, ('mush', 0.0): 1, (\"they'd\", 0.0): 2, ('matt', 0.0): 2, ('ouat', 0.0): 1, ('beach', 0.0): 5, ('blinkin', 0.0): 1, ('unblock', 0.0): 1, ('headack', 0.0): 1, ('tension', 0.0): 1, ('erit', 0.0): 1, ('perspect', 0.0): 1, ('wed', 0.0): 4, ('playlist', 0.0): 2, ('endlessli', 0.0): 1, ('blush', 0.0): 1, ('bat', 0.0): 1, ('kiddo', 0.0): 1, ('rumbel', 0.0): 1, ('overwhelm', 0.0): 1, ('thrown', 0.0): 2, ('irrespons', 0.0): 1, ('pakighinabi', 0.0): 1, ('pinkfinit', 0.0): 1, ('beb', 0.0): 2, ('migrain', 0.0): 2, ('almost', 0.0): 11, ('coyot', 0.0): 1, ('outta', 0.0): 1, ('mad', 0.0): 11, ('😒', 0.0): 3, ('headach', 0.0): 9, ('인피니트', 0.0): 2, ('save', 0.0): 6, ('baechu', 0.0): 1, ('calibraskaep', 0.0): 3, ('r', 0.0): 19, ('fanci', 0.0): 2, ('yt', 0.0): 3, ('purchas', 0.0): 2, ('elgato', 0.0): 1, ('ant', 0.0): 2, ('unexpect', 0.0): 2, ('bestfriend', 0.0): 9, ('faint', 0.0): 1, ('bp', 0.0): 1, ('appar', 0.0): 5, ('shower', 0.0): 3, ('subway', 0.0): 1, ('cool', 0.0): 5, ('prayer', 0.0): 2, ('fragil', 0.0): 1, ('huge', 0.0): 3, ('gap', 0.0): 1, ('plot', 0.0): 2, ('bungi', 0.0): 1, ('folk', 0.0): 1, ('raspberri', 0.0): 1, ('pi', 0.0): 1, ('shoe', 0.0): 2, ('woohyun', 0.0): 2, ('guilti', 0.0): 1, ('monica', 0.0): 2, ('davao', 0.0): 1, ('luckyyi', 0.0): 1, ('confid', 0.0): 1, ('eunha', 0.0): 1, ('misplac', 0.0): 1, ('den', 0.0): 1, ('dae', 0.0): 1, ('bap', 0.0): 1, ('likewis', 0.0): 1, ('liam', 0.0): 1, ('dylan', 0.0): 3, ('huehu', 0.0): 1, ('rice', 0.0): 1, ('krispi', 0.0): 1, ('marshmallow', 0.0): 2, ('srsli', 0.0): 7, ('birmingham', 0.0): 1, ('m5m6junction', 0.0): 1, ('soulsurvivor', 0.0): 1, ('stafford', 0.0): 1, ('progress', 0.0): 1, ('mixtur', 0.0): 1, (\"they'v\", 0.0): 4, ('practic', 0.0): 1, ('lage', 0.0): 1, ('ramd', 0.0): 1, ('lesbian', 0.0): 3, ('oralsex', 0.0): 4, ('munchkin', 0.0): 1, ('juja', 0.0): 1, ('murugan', 0.0): 1, ('handl', 0.0): 3, ('dia', 0.0): 2, ('bgtau', 0.0): 1, ('harap', 0.0): 1, ('bagi', 0.0): 1, ('aminn', 0.0): 1, ('fraand', 0.0): 1, ('😬', 0.0): 2, ('bigbang', 0.0): 2, ('steak', 0.0): 1, ('younger', 0.0): 2, ('sian', 0.0): 2, ('pizza', 0.0): 7, ('5am', 0.0): 5, ('nicoleapag', 0.0): 1, ('makeup', 0.0): 4, ('hellish', 0.0): 1, ('thirstyyi', 0.0): 1, ('chesti', 0.0): 1, ('dad', 0.0): 9, (\"nando'\", 0.0): 1, ('22', 0.0): 3, ('bow', 0.0): 2, ('queen', 0.0): 3, ('brave', 0.0): 1, ('hen', 0.0): 1, ('leed', 0.0): 9, ('rdd', 0.0): 1, ('dissip', 0.0): 1, ('. .', 0.0): 1, ('pump', 0.0): 2, ('capee', 0.0): 1, ('japan', 0.0): 2, ('random', 0.0): 1, ('young', 0.0): 5, ('outliv', 0.0): 1, ('x-ray', 0.0): 1, ('dental', 0.0): 1, ('spine', 0.0): 1, ('relief', 0.0): 1, ('popol', 0.0): 1, ('stomach', 0.0): 8, ('frog', 0.0): 2, ('brad', 0.0): 1, ('gen.ad', 0.0): 1, ('price', 0.0): 5, ('negoti', 0.0): 3, ('huhuhuhuhu', 0.0): 1, ('bbmadeinmanila', 0.0): 1, ('findavip', 0.0): 1, ('boyirl', 0.0): 1, ('yasss', 0.0): 1, ('6th', 0.0): 1, ('june', 0.0): 3, ('lain', 0.0): 1, ('diffici', 0.0): 1, ('custom', 0.0): 1, ('internet', 0.0): 9, ('near', 0.0): 9, ('speed', 0.0): 2, ('escap', 0.0): 1, ('rapist', 0.0): 1, ('commit', 0.0): 2, ('crime', 0.0): 1, ('bachpan', 0.0): 1, ('ki', 0.0): 2, ('yaadein', 0.0): 1, ('finnair', 0.0): 1, ('heathrow', 0.0): 1, ('norwegian', 0.0): 1, (':\\\\', 0.0): 1, ('batteri', 0.0): 3, ('upvot', 0.0): 4, ('keeno', 0.0): 1, ('whatthefuck', 0.0): 1, ('grotti', 0.0): 1, ('attent', 0.0): 1, ('seeker', 0.0): 1, ('moral', 0.0): 1, ('fern', 0.0): 1, ('mimi', 0.0): 1, ('bali', 0.0): 1, ('she', 0.0): 4, ('pleasee', 0.0): 3, ('brb', 0.0): 1, ('lowbat', 0.0): 1, ('otwolgrandtrail', 0.0): 4, ('funk', 0.0): 1, ('wewanticecream', 0.0): 1, ('sweat', 0.0): 2, ('eugh', 0.0): 1, ('speak', 0.0): 4, ('occasion', 0.0): 1, (\"izzy'\", 0.0): 1, ('dorm', 0.0): 1, ('choppi', 0.0): 1, ('paul', 0.0): 1, ('switch', 0.0): 4, (\"infinite'\", 0.0): 2, ('5:30', 0.0): 2, ('cayton', 0.0): 1, ('bay', 0.0): 2, ('emma', 0.0): 2, ('jen', 0.0): 1, ('darcey', 0.0): 1, ('connor', 0.0): 1, ('spoke', 0.0): 1, ('nail', 0.0): 2, ('biggest', 0.0): 3, ('blue', 0.0): 5, ('bottl', 0.0): 3, ('roommateexperi', 0.0): 1, ('yup', 0.0): 4, ('avoid', 0.0): 2, ('ic', 0.0): 1, ('te', 0.0): 1, ('auto-followback', 0.0): 1, ('asian', 0.0): 2, ('puppi', 0.0): 3, ('ljp', 0.0): 1, ('1/5', 0.0): 1, ('nowday', 0.0): 1, ('attach', 0.0): 2, ('beat', 0.0): 2, ('numb', 0.0): 1, ('dentist', 0.0): 3, ('misss', 0.0): 2, ('muchhh', 0.0): 1, ('youtub', 0.0): 5, ('rid', 0.0): 3, ('tab', 0.0): 2, ('uca', 0.0): 1, ('onto', 0.0): 2, ('track', 0.0): 3, ('bigtim', 0.0): 1, ('rumor', 0.0): 3, ('warmest', 0.0): 1, ('chin', 0.0): 2, ('tickl', 0.0): 1, ('♫', 0.0): 1, ('zikra', 0.0): 1, ('lusi', 0.0): 1, ('hasya', 0.0): 1, ('nugget', 0.0): 3, ('som', 0.0): 1, ('lu', 0.0): 1, ('olymp', 0.0): 1, (\"millie'\", 0.0): 1, ('guinea', 0.0): 1, ('lewi', 0.0): 1, ('748292', 0.0): 1, (\"we'll\", 0.0): 8, ('ano', 0.0): 2, ('22stan', 0.0): 1, ('24/7', 0.0): 2, ('thankyou', 0.0): 2, ('kanina', 0.0): 2, ('breakdown', 0.0): 2, ('mag', 0.0): 2, ('hatee', 0.0): 1, ('leas', 0.0): 1, ('written', 0.0): 2, ('hurri', 0.0): 4, ('attempt', 0.0): 1, ('6g', 0.0): 1, ('unsuccess', 0.0): 1, ('earlob', 0.0): 1, ('sue', 0.0): 1, ('dreari', 0.0): 1, ('denis', 0.0): 1, ('muriel', 0.0): 1, ('ahouré', 0.0): 1, ('pr', 0.0): 1, ('brand', 0.0): 1, ('imag', 0.0): 4, ('opportun', 0.0): 1, ('po', 0.0): 1, ('beg', 0.0): 2, (\"kath'd\", 0.0): 1, ('respond', 0.0): 2, ('chop', 0.0): 1, ('wbu', 0.0): 1, ('yess', 0.0): 2, ('kme', 0.0): 1, ('tom', 0.0): 4, ('cram', 0.0): 1, ('–', 0.0): 1, ('curiou', 0.0): 1, ('on-board', 0.0): 1, ('announc', 0.0): 3, ('trespass', 0.0): 1, ('fr', 0.0): 3, ('clandestin', 0.0): 1, ('muller', 0.0): 1, ('obviou', 0.0): 1, ('mufc', 0.0): 1, ('colour', 0.0): 4, ('stu', 0.0): 2, ('movie', 0.0): 1, ('buddyyi', 0.0): 1, ('feelgoodfriday', 0.0): 1, ('forest', 0.0): 1, ('6:30', 0.0): 1, ('babysit', 0.0): 1, ('opix', 0.0): 1, ('805', 0.0): 1, ('pilllow', 0.0): 1, ('fool', 0.0): 1, ('brag', 0.0): 1, ('skrillah', 0.0): 1, ('drown', 0.0): 2, ('gue', 0.0): 1, ('report', 0.0): 4, ('eventu', 0.0): 1, ('north', 0.0): 1, ('west', 0.0): 2, ('kitti', 0.0): 1, ('sjkao', 0.0): 1, ('mm', 0.0): 2, ('srri', 0.0): 1, ('honma', 0.0): 1, ('yeh', 0.0): 1, ('walay', 0.0): 1, ('bhi', 0.0): 2, ('bohat', 0.0): 1, ('wailay', 0.0): 1, ('hain', 0.0): 2, ('pre-season', 0.0): 1, ('friendli', 0.0): 3, ('pe', 0.0): 3, ('itna', 0.0): 2, ('shor', 0.0): 1, ('machaya', 0.0): 1, ('mein', 0.0): 1, ('samjha', 0.0): 1, ('cup', 0.0): 3, ('note', 0.0): 2, ('😄', 0.0): 1, ('👍', 0.0): 1, ('😔', 0.0): 7, ('sirkay', 0.0): 1, ('wali', 0.0): 1, ('pyaaz', 0.0): 1, ('daal', 0.0): 2, ('onion', 0.0): 1, ('vinegar', 0.0): 1, ('cook', 0.0): 3, ('tutori', 0.0): 1, ('soho', 0.0): 1, ('wobbl', 0.0): 1, ('server', 0.0): 4, ('ciao', 0.0): 1, ('masaan', 0.0): 1, ('muv', 0.0): 1, ('beast', 0.0): 2, ('hayst', 0.0): 1, ('cr', 0.0): 1, ('hnnn', 0.0): 1, ('fluffi', 0.0): 2, ('comeback', 0.0): 3, ('korea', 0.0): 1, ('wow', 0.0): 10, ('act', 0.0): 4, ('optimis', 0.0): 1, ('soniii', 0.0): 1, ('kahaaa', 0.0): 1, ('shave', 0.0): 3, ('tryna', 0.0): 3, ('healthi', 0.0): 2, ('freez', 0.0): 3, ('fml', 0.0): 4, ('jacket', 0.0): 1, ('sleepi', 0.0): 4, ('cyber', 0.0): 1, ('bulli', 0.0): 2, ('racial', 0.0): 2, ('scari', 0.0): 6, ('hall', 0.0): 1, ('stockholm', 0.0): 1, ('loool', 0.0): 3, ('bunch', 0.0): 3, ('among', 0.0): 1, ('__', 0.0): 2, ('busier', 0.0): 1, ('onward', 0.0): 1, ('ol', 0.0): 2, ('coincid', 0.0): 1, ('imac', 0.0): 1, ('launch', 0.0): 2, ('gram', 0.0): 1, ('nearer', 0.0): 1, ('blain', 0.0): 2, ('darren', 0.0): 2, ('layout', 0.0): 3, ('fuuuck', 0.0): 2, ('jesu', 0.0): 1, ('gishwh', 0.0): 1, ('exclud', 0.0): 1, ('unless', 0.0): 4, ('c', 0.0): 7, ('angelica', 0.0): 1, ('pull', 0.0): 5, ('colleg', 0.0): 5, ('movement', 0.0): 1, ('frou', 0.0): 1, ('vaccin', 0.0): 1, ('armor', 0.0): 2, ('legendari', 0.0): 1, ('cash', 0.0): 2, ('effort', 0.0): 2, ('nat', 0.0): 2, ('brake', 0.0): 1, ('grumpi', 0.0): 4, ('wreck', 0.0): 1, ('decis', 0.0): 2, ('gahhh', 0.0): 1, ('teribl', 0.0): 1, ('kilig', 0.0): 1, ('togeth', 0.0): 7, ('weaker', 0.0): 1, ('shravan', 0.0): 1, ('tv', 0.0): 4, ('stooop', 0.0): 1, ('gi-guilti', 0.0): 1, ('akooo', 0.0): 1, ('imveryverysorri', 0.0): 1, ('cd', 0.0): 1, ('grey', 0.0): 3, ('basenam', 0.0): 1, ('path', 0.0): 1, ('theme', 0.0): 2, ('cigar', 0.0): 1, ('speaker', 0.0): 1, ('volum', 0.0): 1, ('promethazin', 0.0): 1, ('zopiclon', 0.0): 1, ('addit', 0.0): 1, ('quetiapin', 0.0): 1, ('modifi', 0.0): 1, ('prescript', 0.0): 1, ('greska', 0.0): 1, ('macedonian', 0.0): 1, ('slovak', 0.0): 1, ('hike', 0.0): 1, ('certainli', 0.0): 2, ('browser', 0.0): 2, ('os', 0.0): 1, ('zokay', 0.0): 1, ('accent', 0.0): 1, ('b-but', 0.0): 1, ('gintama', 0.0): 1, ('shinsengumi', 0.0): 1, ('chapter', 0.0): 1, ('andi', 0.0): 1, ('crappl', 0.0): 1, ('agre', 0.0): 5, ('ftw', 0.0): 2, ('phandroid', 0.0): 1, ('tline', 0.0): 1, ('orchestra', 0.0): 1, ('ppl', 0.0): 5, ('rehears', 0.0): 1, ('bittersweet', 0.0): 1, ('eunji', 0.0): 1, ('bakit', 0.0): 4, ('121st', 0.0): 1, (\"yesterday'\", 0.0): 1, ('rt', 0.0): 8, ('ehdar', 0.0): 1, ('pegea', 0.0): 1, ('panga', 0.0): 1, ('dosto', 0.0): 1, ('nd', 0.0): 1, ('real_liam_payn', 0.0): 1, ('retweet', 0.0): 5, ('3/10', 0.0): 1, ('dmed', 0.0): 1, ('ad', 0.0): 1, ('yay', 0.0): 3, ('23', 0.0): 2, ('alreaddyyi', 0.0): 1, ('luceleva', 0.0): 1, ('21', 0.0): 1, ('porno', 0.0): 3, ('countrymus', 0.0): 4, ('sexysasunday', 0.0): 2, ('naeun', 0.0): 1, ('goal', 0.0): 5, (\"son'\", 0.0): 1, ('kidney', 0.0): 2, ('printer', 0.0): 1, ('ink', 0.0): 2, ('asham', 0.0): 3, ('ihatesomepeopl', 0.0): 1, ('tabl', 0.0): 2, ('0-2', 0.0): 1, ('brain', 0.0): 2, ('hard-wir', 0.0): 1, ('canadian', 0.0): 1, ('acn', 0.0): 2, ('gulo', 0.0): 1, ('kandekj', 0.0): 1, ('rize', 0.0): 1, ('meydan', 0.0): 1, ('experienc', 0.0): 2, ('fcking', 0.0): 1, ('crei', 0.0): 1, ('stabl', 0.0): 1, ('dormmat', 0.0): 1, ('pre', 0.0): 3, ('bo3', 0.0): 1, ('cod', 0.0): 2, ('redeem', 0.0): 1, ('invalid', 0.0): 1, ('wag', 0.0): 1, ('hopia', 0.0): 1, ('campaign', 0.0): 2, ('editor', 0.0): 1, ('reveal', 0.0): 2, ('booo', 0.0): 2, ('extens', 0.0): 1, ('rightnow', 0.0): 1, ('btu', 0.0): 1, ('karaok', 0.0): 1, ('licenc', 0.0): 1, ('apb', 0.0): 2, ('mbf', 0.0): 1, ('kpop', 0.0): 2, ('hahahaokay', 0.0): 1, ('basara', 0.0): 1, ('capcom', 0.0): 3, ('pc', 0.0): 2, ('url', 0.0): 2, ('web', 0.0): 2, ('site', 0.0): 6, ('design', 0.0): 3, ('grumbl', 0.0): 2, ('migrant', 0.0): 1, ('daddi', 0.0): 4, ('legit', 0.0): 1, ('australia', 0.0): 3, ('awsm', 0.0): 1, ('entir', 0.0): 5, ('tmw', 0.0): 1, ('uwu', 0.0): 1, ('jinki', 0.0): 1, ('taem', 0.0): 1, ('gif', 0.0): 2, ('cambridg', 0.0): 1, ('viath', 0.0): 1, ('brilliant', 0.0): 1, ('cypru', 0.0): 1, ('wet', 0.0): 10, ('30th', 0.0): 1, ('zayncomebackto', 0.0): 2, ('1d', 0.0): 6, ('senior', 0.0): 2, ('spazz', 0.0): 1, ('soobin', 0.0): 1, ('27', 0.0): 1, ('unmarri', 0.0): 1, ('float', 0.0): 3, ('pressur', 0.0): 3, ('winter', 0.0): 4, ('lifetim', 0.0): 2, ('hiondsh', 0.0): 1, ('58543', 0.0): 1, ('kikmenow', 0.0): 9, ('sexdat', 0.0): 2, (\"demi'\", 0.0): 1, ('junjou', 0.0): 2, ('romantica', 0.0): 1, ('cruel', 0.0): 1, ('privileg', 0.0): 2, ('mixtap', 0.0): 2, ('convinc', 0.0): 3, ('friex', 0.0): 1, ('taco', 0.0): 2, ('europ', 0.0): 2, ('shaylan', 0.0): 1, ('4:20', 0.0): 1, ('ylona', 0.0): 1, ('nah', 0.0): 4, ('notanapolog', 0.0): 3, ('ouh', 0.0): 1, ('tax', 0.0): 4, ('ohhh', 0.0): 2, ('nm', 0.0): 1, ('term', 0.0): 1, ('apolog', 0.0): 3, ('encanta', 0.0): 1, ('vale', 0.0): 1, ('osea', 0.0): 1, ('bea', 0.0): 1, ('♛', 0.0): 210, ('》', 0.0): 210, ('beli̇ev', 0.0): 35, ('wi̇ll', 0.0): 35, ('justi̇n', 0.0): 35, ('x15', 0.0): 35, ('350', 0.0): 4, ('ｓｅｅ', 0.0): 35, ('ｍｅ', 0.0): 35, ('40', 0.0): 3, ('dj', 0.0): 2, ('net', 0.0): 2, ('349', 0.0): 1, ('baek', 0.0): 1, ('tight', 0.0): 1, ('dunwan', 0.0): 1, ('suan', 0.0): 1, ('ba', 0.0): 3, ('haiz', 0.0): 1, ('otw', 0.0): 1, ('trade', 0.0): 3, ('venic', 0.0): 1, ('348', 0.0): 1, ('strong', 0.0): 6, ('adult', 0.0): 3, ('347', 0.0): 1, ('tree', 0.0): 3, ('hill', 0.0): 1, ('😕', 0.0): 1, ('com', 0.0): 1, ('insonia', 0.0): 1, ('346', 0.0): 1, ('rick', 0.0): 1, ('ross', 0.0): 1, ('wallet', 0.0): 4, ('empti', 0.0): 3, ('heartbreak', 0.0): 2, ('episod', 0.0): 11, ('345', 0.0): 1, ('milli', 0.0): 1, (':)', 0.0): 2, ('diff', 0.0): 1, ('persona', 0.0): 1, ('golden', 0.0): 1, ('scene', 0.0): 1, ('advert', 0.0): 1, ('determin', 0.0): 2, ('roseburi', 0.0): 1, ('familyhom', 0.0): 1, ('daw', 0.0): 2, ('344', 0.0): 1, ('monkey', 0.0): 1, ('yea', 0.0): 2, ('343', 0.0): 1, ('sweeti', 0.0): 2, ('erica', 0.0): 1, ('istg', 0.0): 1, ('lick', 0.0): 1, ('jackson', 0.0): 4, ('nsbzhdnxndamal', 0.0): 1, ('342', 0.0): 1, ('11:15', 0.0): 1, ('2hour', 0.0): 1, ('11:25', 0.0): 1, ('341', 0.0): 1, ('fandom', 0.0): 2, ('mahilig', 0.0): 1, ('mam-bulli', 0.0): 1, ('mtaani', 0.0): 1, ('tunaita', 0.0): 1, ('viazi', 0.0): 1, ('choma', 0.0): 1, ('laid', 0.0): 1, ('celebr', 0.0): 3, ('7am', 0.0): 1, ('jerk', 0.0): 1, ('lah', 0.0): 2, ('magic', 0.0): 1, ('menil', 0.0): 1, ('340', 0.0): 1, (\"kam'\", 0.0): 1, ('meee', 0.0): 1, ('diz', 0.0): 1, ('biooo', 0.0): 1, ('ay', 0.0): 1, ('taray', 0.0): 1, ('yumu-youtub', 0.0): 1, ('339', 0.0): 1, ('parijat', 0.0): 1, ('willmissyouparijat', 0.0): 1, ('abroad', 0.0): 2, ('jolli', 0.0): 1, ('scotland', 0.0): 2, ('338', 0.0): 1, ('mcnugget', 0.0): 1, ('sophi', 0.0): 5, ('feedback', 0.0): 4, ('met', 0.0): 7, ('caramello', 0.0): 2, ('koala', 0.0): 1, ('bar', 0.0): 1, ('suckmejimin', 0.0): 1, ('337', 0.0): 1, ('sucki', 0.0): 2, ('laughter', 0.0): 1, ('pou', 0.0): 1, ('goddamn', 0.0): 1, ('bark', 0.0): 1, ('nje', 0.0): 1, ('blast', 0.0): 1, ('hun', 0.0): 4, ('dbn', 0.0): 2, ('🎀', 0.0): 1, ('336', 0.0): 1, ('hardest', 0.0): 1, ('335', 0.0): 1, ('pledg', 0.0): 1, ('realiz', 0.0): 7, ('viber', 0.0): 1, ('mwah', 0.0): 1, ('estat', 0.0): 1, ('crush', 0.0): 1, ('lansi', 0.0): 1, ('334', 0.0): 1, ('hp', 0.0): 4, ('waah', 0.0): 1, ('miami', 0.0): 1, ('vandag', 0.0): 1, ('kgola', 0.0): 1, ('neng', 0.0): 1, ('eintlik', 0.0): 1, ('porn', 0.0): 2, ('4like', 0.0): 5, ('repost', 0.0): 2, ('333', 0.0): 3, ('magpi', 0.0): 1, ('22.05', 0.0): 1, ('15-24', 0.0): 1, ('05.15', 0.0): 1, ('coach', 0.0): 2, ('ador', 0.0): 1, ('chswiyfxcskcalum', 0.0): 1, ('nvm', 0.0): 2, ('lemm', 0.0): 1, ('quiet', 0.0): 3, ('foof', 0.0): 1, ('332', 0.0): 1, ('casilla', 0.0): 1, ('manchest', 0.0): 3, ('xi', 0.0): 1, ('rmtour', 0.0): 1, ('heavi', 0.0): 3, ('irl', 0.0): 2, ('blooper', 0.0): 2, ('huhuhuhu', 0.0): 1, ('na-tak', 0.0): 1, ('sorta', 0.0): 1, ('unfriend', 0.0): 1, ('greysonch', 0.0): 1, ('sandwich', 0.0): 4, ('bell', 0.0): 1, ('sebastian', 0.0): 1, ('rewatch', 0.0): 1, ('s4', 0.0): 1, ('ser', 0.0): 1, ('past', 0.0): 5, ('heart-break', 0.0): 1, ('outdat', 0.0): 1, ('m4', 0.0): 1, ('abandon', 0.0): 1, ('theater', 0.0): 1, ('smh', 0.0): 6, ('7-3', 0.0): 1, ('7.30-', 0.0): 1, ('ekk', 0.0): 1, ('giriboy', 0.0): 1, ('harriet', 0.0): 1, ('gegu', 0.0): 1, ('gray', 0.0): 1, ('truth', 0.0): 4, ('tbt', 0.0): 1, ('331', 0.0): 1, ('roof', 0.0): 2, ('indian', 0.0): 2, ('polit', 0.0): 3, ('blame', 0.0): 3, ('68', 0.0): 1, ('repres', 0.0): 1, ('corbyn', 0.0): 1, (\"labour'\", 0.0): 1, ('fortun', 0.0): 1, ('icecream', 0.0): 3, ('cuti', 0.0): 2, ('ry', 0.0): 1, ('lfccw', 0.0): 1, ('5ever', 0.0): 1, ('america', 0.0): 3, ('ontheroadagain', 0.0): 1, ('halaaang', 0.0): 1, ('reciev', 0.0): 1, ('flip', 0.0): 4, ('flop', 0.0): 1, ('caesarspalac', 0.0): 1, ('socialreward', 0.0): 1, ('requir', 0.0): 2, ('cali', 0.0): 1, ('fuckboy', 0.0): 1, ('330', 0.0): 1, ('deliveri', 0.0): 3, ('chrompet', 0.0): 1, ('easili', 0.0): 2, ('immun', 0.0): 1, ('system', 0.0): 3, ('lush', 0.0): 1, ('bathtub', 0.0): 1, ('php', 0.0): 1, ('mysql', 0.0): 1, ('libmysqlclient-dev', 0.0): 1, ('dev', 0.0): 2, ('pleasanton', 0.0): 1, ('wala', 0.0): 1, ('329', 0.0): 1, ('quickli', 0.0): 2, ('megan', 0.0): 1, ('heed', 0.0): 2, ('328', 0.0): 1, ('gwss', 0.0): 1, ('thankyouu', 0.0): 1, ('charad', 0.0): 1, ('becom', 0.0): 5, ('piano', 0.0): 2, ('327', 0.0): 1, ('complaint', 0.0): 2, ('yell', 0.0): 2, ('whatsoev', 0.0): 2, ('pete', 0.0): 1, ('wentz', 0.0): 1, ('shogi', 0.0): 1, ('blameshoghicp', 0.0): 1, ('classmat', 0.0): 1, ('troubl', 0.0): 1, ('fixedgearfrenzi', 0.0): 1, ('dispatch', 0.0): 1, ('theyr', 0.0): 2, ('hat', 0.0): 2, (\"shamuon'\", 0.0): 1, ('tokyo', 0.0): 1, ('toe', 0.0): 2, ('horrend', 0.0): 2, (\"someone'\", 0.0): 2, ('326', 0.0): 1, ('hasb', 0.0): 1, ('atti', 0.0): 1, ('muji', 0.0): 1, ('sirf', 0.0): 1, ('sensibl', 0.0): 1, ('etc', 0.0): 2, ('brum', 0.0): 1, ('cyclerevolut', 0.0): 1, ('caaannnttt', 0.0): 1, ('payment', 0.0): 3, ('overdrawn', 0.0): 1, ('tbf', 0.0): 1, ('complain', 0.0): 2, ('perfum', 0.0): 1, ('sampl', 0.0): 1, ('chanel', 0.0): 1, ('burberri', 0.0): 1, ('prada', 0.0): 1, ('325', 0.0): 1, ('noesss', 0.0): 1, ('topgear', 0.0): 1, ('worthi', 0.0): 1, ('bridesmaid', 0.0): 1, (\"tomorrow'\", 0.0): 2, ('gather', 0.0): 1, ('sudden', 0.0): 4, ('324', 0.0): 1, ('randomrestart', 0.0): 1, ('randomreboot', 0.0): 1, ('lumia', 0.0): 1, ('windowsphon', 0.0): 1, (\"microsoft'\", 0.0): 1, ('mañana', 0.0): 1, ('male', 0.0): 1, ('rap', 0.0): 1, ('sponsor', 0.0): 3, ('striker', 0.0): 2, ('lvg', 0.0): 1, ('behind', 0.0): 3, ('refurbish', 0.0): 1, ('cintiq', 0.0): 1, (\"finnick'\", 0.0): 1, ('askfinnick', 0.0): 1, ('contain', 0.0): 1, ('hairi', 0.0): 1, ('323', 0.0): 1, ('buri', 0.0): 1, ('omaygad', 0.0): 1, ('vic', 0.0): 1, ('surgeri', 0.0): 4, ('amber', 0.0): 8, ('tt.tt', 0.0): 1, ('hyper', 0.0): 2, ('vega', 0.0): 2, ('322', 0.0): 1, ('imiss', 0.0): 1, ('321', 0.0): 1, ('320', 0.0): 1, ('know.for', 0.0): 1, ('prepaid', 0.0): 1, ('none', 0.0): 4, ('319', 0.0): 1, ('grandma', 0.0): 1, (\"grandpa'\", 0.0): 1, ('farm', 0.0): 1, ('cow', 0.0): 1, ('sheep', 0.0): 1, ('hors', 0.0): 3, ('fruit', 0.0): 2, ('veget', 0.0): 1, ('puke', 0.0): 2, ('deliri', 0.0): 1, ('motilium', 0.0): 1, ('shite', 0.0): 1, ('318', 0.0): 1, ('schoolwork', 0.0): 1, (\"phoebe'\", 0.0): 1, ('317', 0.0): 1, ('pothol', 0.0): 1, ('316', 0.0): 1, ('notif', 0.0): 3, ('1,300', 0.0): 1, ('robyn', 0.0): 1, ('necklac', 0.0): 1, ('rachel', 0.0): 1, ('bhai', 0.0): 1, ('ramzan', 0.0): 1, ('crosss', 0.0): 1, ('clapham', 0.0): 1, ('investig', 0.0): 2, ('sth', 0.0): 1, ('essenti', 0.0): 1, ('photoshooot', 0.0): 1, ('austin', 0.0): 1, ('mahon', 0.0): 1, ('shut', 0.0): 3, ('andam', 0.0): 1, ('memor', 0.0): 1, ('cotton', 0.0): 1, ('candi', 0.0): 3, ('stock', 0.0): 3, ('swallow', 0.0): 1, ('snot', 0.0): 1, ('choke', 0.0): 1, ('taknottem', 0.0): 1, ('477', 0.0): 1, ('btob', 0.0): 2, ('percentag', 0.0): 1, ('shoshannavassil', 0.0): 1, ('swift', 0.0): 1, ('flat', 0.0): 3, ('a9', 0.0): 2, ('wsalelov', 0.0): 5, ('sexyjan', 0.0): 1, ('horni', 0.0): 2, ('goodmus', 0.0): 4, ('debut', 0.0): 3, ('lart', 0.0): 1, ('sew', 0.0): 1, ('skyfal', 0.0): 1, ('premier', 0.0): 1, ('yummi', 0.0): 2, ('manteca', 0.0): 1, (\"she'd\", 0.0): 2, ('probabl', 0.0): 8, ('shiatsu', 0.0): 1, ('heat', 0.0): 1, ('risk', 0.0): 3, ('edward', 0.0): 1, ('hopper', 0.0): 1, ('eyyah', 0.0): 1, ('utd', 0.0): 2, ('born', 0.0): 1, ('1-0', 0.0): 1, ('cart', 0.0): 1, ('shop', 0.0): 10, ('log', 0.0): 2, ('aaa', 0.0): 2, ('waifu', 0.0): 1, ('break', 0.0): 8, ('breakup', 0.0): 3, ('bother', 0.0): 3, ('bia', 0.0): 1, ('syndrom', 0.0): 1, ('shi', 0.0): 1, ('bias', 0.0): 1, ('pixel', 0.0): 2, ('weh', 0.0): 2, ('area', 0.0): 4, ('maymay', 0.0): 1, ('magpaalam', 0.0): 1, ('tf', 0.0): 3, ('subtitl', 0.0): 1, ('oitnb', 0.0): 1, ('backstori', 0.0): 1, ('jeremi', 0.0): 1, ('kyle', 0.0): 1, ('gimm', 0.0): 2, ('meal', 0.0): 3, ('neat-o', 0.0): 1, ('wru', 0.0): 1, ('scissor', 0.0): 1, ('creation', 0.0): 1, ('public', 0.0): 1, ('amtir', 0.0): 1, ('imysm', 0.0): 2, ('tut', 0.0): 1, ('trop', 0.0): 2, ('tard', 0.0): 1, ('deadlin', 0.0): 1, ('31', 0.0): 2, ('st', 0.0): 3, ('child', 0.0): 4, ('oct', 0.0): 2, ('bush', 0.0): 2, ('premiun', 0.0): 1, ('notcool', 0.0): 1, ('2/3', 0.0): 2, ('lahat', 0.0): 2, ('ng', 0.0): 4, ('araw', 0.0): 1, ('nage', 0.0): 1, ('gyu', 0.0): 4, ('lmfaooo', 0.0): 2, ('download', 0.0): 3, ('leagu', 0.0): 1, ('mashup', 0.0): 1, ('eu', 0.0): 1, ('lc', 0.0): 1, ('typo', 0.0): 2, ('itali', 0.0): 1, ('yass', 0.0): 1, ('christma', 0.0): 2, ('rel', 0.0): 1, ('yr', 0.0): 3, ('sydney', 0.0): 1, ('mb', 0.0): 1, ('perf', 0.0): 2, ('programm', 0.0): 1, ('bff', 0.0): 2, ('hashtag', 0.0): 1, ('omfg', 0.0): 4, ('exercis', 0.0): 2, ('combat', 0.0): 1, ('dosent', 0.0): 1, (\"sod'\", 0.0): 1, ('20min', 0.0): 1, ('request', 0.0): 2, ('yahoo', 0.0): 2, ('yodel', 0.0): 2, ('jokingli', 0.0): 1, ('regret', 0.0): 5, ('starbuck', 0.0): 3, ('lynettelow', 0.0): 1, ('interraci', 0.0): 3, (\"today'\", 0.0): 3, ('tgif', 0.0): 1, ('gahd', 0.0): 1, ('26th', 0.0): 1, ('discov', 0.0): 1, ('12.00', 0.0): 1, ('obyun', 0.0): 1, ('unni', 0.0): 4, ('wayhh', 0.0): 1, ('preval', 0.0): 1, ('controversi', 0.0): 1, ('🍵', 0.0): 2, ('☕', 0.0): 1, ('tube', 0.0): 1, ('strike', 0.0): 3, ('meck', 0.0): 1, ('mcfc', 0.0): 1, ('fresh', 0.0): 1, ('ucan', 0.0): 1, ('anxiou', 0.0): 1, ('poc', 0.0): 1, ('specif', 0.0): 2, ('sinhala', 0.0): 1, ('billionair', 0.0): 1, ('1645', 0.0): 1, ('island', 0.0): 3, ('1190', 0.0): 1, ('maldiv', 0.0): 1, ('dheena', 0.0): 1, ('fasgadah', 0.0): 1, ('alvadhaau', 0.0): 1, ('countdown', 0.0): 1, ('function', 0.0): 3, ('desktop', 0.0): 1, ('evelineconrad', 0.0): 1, ('facetim', 0.0): 4, ('kikmsn', 0.0): 2, ('selfshot', 0.0): 2, ('panda', 0.0): 1, ('backkk', 0.0): 1, ('transfer', 0.0): 3, ('dan', 0.0): 2, ('dull', 0.0): 1, ('overcast', 0.0): 1, ('folder', 0.0): 1, ('truck', 0.0): 2, ('missin', 0.0): 2, ('hangin', 0.0): 1, ('wiff', 0.0): 1, ('dept', 0.0): 1, ('cherri', 0.0): 1, ('bakewel', 0.0): 1, ('collect', 0.0): 3, ('teal', 0.0): 1, ('sect', 0.0): 1, ('tennunb', 0.0): 1, ('rather', 0.0): 4, ('skip', 0.0): 1, ('doomsday', 0.0): 1, ('neglect', 0.0): 1, ('posti', 0.0): 1, ('goodnight', 0.0): 1, ('donat', 0.0): 3, ('ship', 0.0): 6, ('bellami', 0.0): 1, ('raven', 0.0): 2, ('clark', 0.0): 1, ('helmi', 0.0): 1, ('uh', 0.0): 5, ('cnt', 0.0): 1, ('whereisthesun', 0.0): 1, ('summerismiss', 0.0): 1, ('longgg', 0.0): 1, ('ridicul', 0.0): 4, ('stocko', 0.0): 1, ('lucozad', 0.0): 1, ('explos', 0.0): 1, ('beh', 0.0): 2, ('half-rememb', 0.0): 1, (\"melody'\", 0.0): 1, ('recal', 0.0): 2, ('level', 0.0): 3, ('target', 0.0): 1, ('difficult', 0.0): 4, ('mile', 0.0): 1, ('pfb', 0.0): 1, ('nate', 0.0): 2, ('expo', 0.0): 2, ('jisoo', 0.0): 1, ('chloe', 0.0): 2, ('anon', 0.0): 2, ('mager', 0.0): 1, ('wi', 0.0): 1, ('knw', 0.0): 1, ('wht', 0.0): 1, ('distant', 0.0): 1, ('buffer', 0.0): 2, ('insan', 0.0): 1, ('charli', 0.0): 1, ('finland', 0.0): 3, ('gana', 0.0): 1, ('studio', 0.0): 3, ('arch', 0.0): 1, ('lyin', 0.0): 1, ('kian', 0.0): 3, ('supercar', 0.0): 1, ('gurgaon', 0.0): 1, ('locat', 0.0): 7, ('9:15', 0.0): 1, ('satir', 0.0): 1, ('gener', 0.0): 2, ('peanut', 0.0): 3, ('butter', 0.0): 1, ('garden', 0.0): 2, ('beer', 0.0): 1, ('viner', 0.0): 1, ('palembang', 0.0): 1, ('sorrryyi', 0.0): 1, ('fani', 0.0): 1, ('hahahahaha', 0.0): 2, ('boner', 0.0): 1, ('merci', 0.0): 1, ('yuki', 0.0): 1, ('2500k', 0.0): 1, ('mari', 0.0): 1, ('jake', 0.0): 1, ('gyllenha', 0.0): 1, ('impact', 0.0): 1, (\"ledger'\", 0.0): 1, ('btw', 0.0): 5, ('cough', 0.0): 4, ('hunni', 0.0): 1, ('b4', 0.0): 1, ('deplet', 0.0): 1, ('mbasa', 0.0): 1, ('client', 0.0): 3, ('ray', 0.0): 1, ('aah', 0.0): 1, ('type', 0.0): 2, ('suit', 0.0): 5, ('pa-copi', 0.0): 1, ('proper', 0.0): 2, ('biom', 0.0): 1, ('mosqu', 0.0): 1, ('smelli', 0.0): 1, ('taxi', 0.0): 4, ('emptier', 0.0): 1, (\"ciara'\", 0.0): 1, (\"everything'\", 0.0): 1, ('clip', 0.0): 2, ('tall', 0.0): 2, ('gladli', 0.0): 1, ('intent', 0.0): 1, ('amb', 0.0): 1, (\"harry'\", 0.0): 2, ('jean', 0.0): 2, ('mayday', 0.0): 1, ('parad', 0.0): 2, ('lyf', 0.0): 1, ('13th', 0.0): 1, ('anim', 0.0): 4, ('kingdom', 0.0): 1, ('chri', 0.0): 7, ('brown', 0.0): 4, ('riski', 0.0): 1, ('cologn', 0.0): 1, ('duo', 0.0): 3, ('ballad', 0.0): 2, ('bish', 0.0): 2, ('intern', 0.0): 2, ('brought', 0.0): 1, ('yumyum', 0.0): 1, (\"cathy'\", 0.0): 1, ('missyou', 0.0): 1, ('rubi', 0.0): 2, ('rose', 0.0): 2, ('tou', 0.0): 1, ('main', 0.0): 1, ('pora', 0.0): 1, ('stalk', 0.0): 3, ('karlia', 0.0): 1, ('khatam', 0.0): 2, ('bandi', 0.0): 1, ('👑', 0.0): 1, ('pyaari', 0.0): 1, ('gawd', 0.0): 1, ('understood', 0.0): 1, ('review', 0.0): 3, ('massi', 0.0): 1, ('thatselfiethough', 0.0): 1, ('loop', 0.0): 1, ('ofc', 0.0): 1, ('pict', 0.0): 1, ('caught', 0.0): 1, ('aishhh', 0.0): 1, ('viewer', 0.0): 1, ('exam', 0.0): 5, ('sighsss', 0.0): 1, ('burnt', 0.0): 2, ('toffe', 0.0): 2, ('honesti', 0.0): 1, ('cheatday', 0.0): 1, ('protein', 0.0): 1, ('sissi', 0.0): 1, ('tote', 0.0): 1, ('slowli', 0.0): 1, ('church', 0.0): 2, ('pll', 0.0): 1, ('sel', 0.0): 1, ('beth', 0.0): 2, ('serbia', 0.0): 1, ('serbian', 0.0): 1, ('selen', 0.0): 1, ('motav', 0.0): 1, ('💋', 0.0): 2, ('zayyyn', 0.0): 1, ('momma', 0.0): 1, ('happend', 0.0): 1, ('imper', 0.0): 1, ('trmdhesit', 0.0): 1, ('pana', 0.0): 1, ('quickest', 0.0): 2, ('blood', 0.0): 5, ('sake', 0.0): 1, ('hamstr', 0.0): 1, ('rodwel', 0.0): 1, ('trace', 0.0): 1, ('artist', 0.0): 4, ('tp', 0.0): 1, ('powder', 0.0): 1, ('wider', 0.0): 1, ('honestli', 0.0): 4, ('comfort', 0.0): 3, ('bruno', 0.0): 1, ('1.8', 0.0): 1, ('ed', 0.0): 7, ('croke', 0.0): 2, ('deal', 0.0): 6, ('toll', 0.0): 1, ('packag', 0.0): 1, ('shape', 0.0): 1, ('unluckiest', 0.0): 1, ('bettor', 0.0): 1, ('nstp', 0.0): 1, ('sem', 0.0): 2, ('chipotl', 0.0): 1, ('chick-fil-a', 0.0): 1, ('stole', 0.0): 3, ('evet', 0.0): 1, ('ramadhan', 0.0): 1, ('eid', 0.0): 4, ('stexpert', 0.0): 1, ('ripstegi', 0.0): 1, ('nickyyi', 0.0): 1, ('¿', 0.0): 1, ('centralis', 0.0): 1, ('discontinu', 0.0): 1, ('sniff', 0.0): 1, (\"i't\", 0.0): 1, ('glad', 0.0): 2, ('fab', 0.0): 2, ('theres', 0.0): 1, ('cred', 0.0): 1, ('t_t', 0.0): 1, ('elimin', 0.0): 1, ('teamzip', 0.0): 1, ('smtm', 0.0): 1, ('assingn', 0.0): 1, ('editi', 0.0): 1, ('nakaka', 0.0): 1, ('beastmod', 0.0): 1, ('gaaawd', 0.0): 1, ('jane', 0.0): 1, ('mango', 0.0): 1, ('colombia', 0.0): 1, ('yot', 0.0): 1, ('labyo', 0.0): 1, ('pano', 0.0): 1, ('nalamannn', 0.0): 1, ('hardhead', 0.0): 1, ('cell', 0.0): 1, (\"zach'\", 0.0): 1, ('burger', 0.0): 2, ('xpress', 0.0): 1, ('hopkin', 0.0): 1, ('melatonin', 0.0): 1, ('2-4', 0.0): 1, ('nap', 0.0): 2, ('wide', 0.0): 2, ('task', 0.0): 1, ('9pm', 0.0): 1, ('hahaah', 0.0): 1, ('frequent', 0.0): 1, ('jail', 0.0): 2, ('weirddd', 0.0): 1, ('donghyuk', 0.0): 1, ('stan', 0.0): 1, ('bek', 0.0): 1, ('13', 0.0): 4, ('reynoldsgrl', 0.0): 1, ('ole', 0.0): 1, ('beardi', 0.0): 1, ('kaussi', 0.0): 1, ('bummer', 0.0): 3, ('fightingmciren', 0.0): 1, (\"michael'\", 0.0): 1, ('�', 0.0): 21, ('miser', 0.0): 2, ('💦', 0.0): 1, ('yoga', 0.0): 2, ('🌞', 0.0): 1, ('💃🏽', 0.0): 1, ('shouldv', 0.0): 1, ('saffron', 0.0): 1, ('peasant', 0.0): 1, ('wouldv', 0.0): 1, ('nfinit', 0.0): 1, ('admin_myung', 0.0): 1, ('slp', 0.0): 1, ('saddest', 0.0): 2, ('laomma', 0.0): 2, ('kebaya', 0.0): 1, ('bandung', 0.0): 1, ('indonesia', 0.0): 1, ('7df89150', 0.0): 1, ('whatsapp', 0.0): 2, ('62', 0.0): 1, ('08962464174', 0.0): 1, ('laomma_coutur', 0.0): 1, ('haizzz', 0.0): 1, ('urghhh', 0.0): 1, ('working-on-a-tight-schedul', 0.0): 1, ('ganbarimasu', 0.0): 1, ('livid', 0.0): 1, ('whammi', 0.0): 1, ('quuuee', 0.0): 1, ('friooo', 0.0): 1, ('ladi', 0.0): 4, ('stereo', 0.0): 1, ('chwang', 0.0): 1, ('lorm', 0.0): 1, ('823', 0.0): 1, ('rp', 0.0): 1, ('indiemus', 0.0): 10, ('unhappi', 0.0): 2, ('jennyjean', 0.0): 1, ('elfindelmundo', 0.0): 2, ('lolzz', 0.0): 1, ('dat', 0.0): 4, ('corey', 0.0): 1, ('appreci', 0.0): 2, ('weekli', 0.0): 2, ('mahirap', 0.0): 1, ('nash', 0.0): 1, ('gosh', 0.0): 6, ('noodl', 0.0): 1, ('veeerri', 0.0): 1, ('rted', 0.0): 2, ('orig', 0.0): 1, ('starholicxx', 0.0): 1, ('07:17', 0.0): 2, ('@the', 0.0): 1, ('notr', 0.0): 1, ('hwi', 0.0): 1, ('niall', 0.0): 5, ('fraud', 0.0): 1, ('diplomaci', 0.0): 1, ('fittest', 0.0): 1, ('zero', 0.0): 1, ('toler', 0.0): 2, ('gurl', 0.0): 1, ('notion', 0.0): 1, ('pier', 0.0): 1, ('approach', 0.0): 1, ('rattl', 0.0): 1, ('robe', 0.0): 1, ('emphasi', 0.0): 1, ('vocal', 0.0): 1, ('chose', 0.0): 1, ('erm', 0.0): 1, ('abby.can', 0.0): 1, ('persuad', 0.0): 1, ('lyric', 0.0): 1, (\"emily'\", 0.0): 1, ('odd', 0.0): 3, ('possibl', 0.0): 8, ('elect', 0.0): 2, ('kamiss', 0.0): 1, ('mwa', 0.0): 1, ('mommi', 0.0): 3, ('scream', 0.0): 1, ('fight', 0.0): 2, ('cafe', 0.0): 2, ('melbourn', 0.0): 1, ('anyonnee', 0.0): 1, ('loner', 0.0): 1, ('fricken', 0.0): 2, ('rito', 0.0): 1, ('friendzon', 0.0): 1, ('panel', 0.0): 1, ('repeat', 0.0): 2, ('audienc', 0.0): 1, ('hsm', 0.0): 1, ('canario', 0.0): 1, ('hotel', 0.0): 8, ('ukiss', 0.0): 1, ('faith', 0.0): 2, ('kurt', 0.0): 1, (\"fatma'm\", 0.0): 1, ('alex', 0.0): 4, ('swag', 0.0): 1, ('lmfao', 0.0): 2, ('flapjack', 0.0): 1, ('countthecost', 0.0): 1, ('ihop', 0.0): 1, ('infra', 0.0): 1, ('lq', 0.0): 1, ('knive', 0.0): 1, ('sotir', 0.0): 1, ('mybrainneedstoshutoff', 0.0): 1, ('macci', 0.0): 1, ('chees', 0.0): 7, ('25', 0.0): 2, ('tend', 0.0): 1, ('510', 0.0): 1, ('silicon', 0.0): 1, ('cover', 0.0): 2, ('kbye', 0.0): 1, ('ini', 0.0): 1, ('anytim', 0.0): 1, ('citizen', 0.0): 1, ('compar', 0.0): 2, ('rank', 0.0): 1, ('mcountdown', 0.0): 2, ('5h', 0.0): 1, ('thapelo', 0.0): 1, ('op', 0.0): 1, ('civ', 0.0): 1, ('wooden', 0.0): 1, ('mic', 0.0): 1, ('embarrass', 0.0): 2, ('translat', 0.0): 3, ('daili', 0.0): 3, ('mecha-totem', 0.0): 1, ('nak', 0.0): 1, ('tgk', 0.0): 1, ('townsss', 0.0): 1, ('jokid', 0.0): 1, ('rent', 0.0): 2, ('degre', 0.0): 1, ('inconsider', 0.0): 2, ('softbal', 0.0): 1, ('appli', 0.0): 1, ('tomcat', 0.0): 1, ('chel', 0.0): 1, ('jemma', 0.0): 1, ('detail', 0.0): 4, ('list', 0.0): 4, ('matchi', 0.0): 2, ('elsa', 0.0): 1, ('postpon', 0.0): 1, ('karin', 0.0): 1, ('honey', 0.0): 2, ('vist', 0.0): 1, ('unhealthi', 0.0): 1, ('propa', 0.0): 1, ('knockin', 0.0): 1, ('bacon', 0.0): 1, ('market', 0.0): 2, ('pre-holiday', 0.0): 1, ('diet', 0.0): 1, ('meani', 0.0): 1, ('deathbybaconsmel', 0.0): 1, ('init', 0.0): 2, ('destin', 0.0): 1, ('victoria', 0.0): 2, ('luna', 0.0): 1, ('krystal', 0.0): 1, ('sarajevo', 0.0): 1, ('haix', 0.0): 2, ('sp', 0.0): 1, ('student', 0.0): 4, ('wii', 0.0): 2, ('bayonetta', 0.0): 1, ('101', 0.0): 1, ('doabl', 0.0): 1, ('drove', 0.0): 1, ('agenc', 0.0): 1, ('story.miss', 0.0): 1, ('everon', 0.0): 1, ('jp', 0.0): 1, ('mamabear', 0.0): 1, ('imintoh', 0.0): 1, ('underr', 0.0): 1, (\"slovakia'\", 0.0): 1, ('d:', 0.0): 6, ('saklap', 0.0): 1, ('grade', 0.0): 2, ('rizal', 0.0): 1, ('lib', 0.0): 1, ('discuss', 0.0): 1, ('advisori', 0.0): 1, ('period', 0.0): 2, ('dit', 0.0): 1, ('du', 0.0): 1, ('harsh', 0.0): 2, ('ohgod', 0.0): 1, ('abligaverin', 0.0): 2, ('photooftheday', 0.0): 2, ('sexygirlbypreciouslemmi', 0.0): 3, ('ripsandrabland', 0.0): 1, ('edel', 0.0): 1, ('salam', 0.0): 1, ('mubark', 0.0): 1, ('dong', 0.0): 3, ('tammirossm', 0.0): 4, ('speck', 0.0): 1, ('abbymil', 0.0): 2, ('18', 0.0): 8, ('ion', 0.0): 1, ('5min', 0.0): 1, ('hse', 0.0): 1, ('noob', 0.0): 1, ('nxt', 0.0): 1, ('2week', 0.0): 1, ('300', 0.0): 3, ('fck', 0.0): 2, ('nae', 0.0): 2, ('deep', 0.0): 3, ('human', 0.0): 3, ('whit', 0.0): 1, ('van', 0.0): 4, ('bristol', 0.0): 1, ('subserv', 0.0): 1, ('si', 0.0): 4, ('oo', 0.0): 1, ('tub', 0.0): 1, ('penyfan', 0.0): 1, ('forecast', 0.0): 2, ('breconbeacon', 0.0): 1, ('tittheir', 0.0): 1, ('42', 0.0): 1, ('hotti', 0.0): 3, ('uu', 0.0): 2, ('rough', 0.0): 1, ('fuzzi', 0.0): 1, ('san', 0.0): 3, ('antonio', 0.0): 1, ('kang', 0.0): 1, ('junhe', 0.0): 1, ('couldv', 0.0): 1, ('pz', 0.0): 1, ('somerset', 0.0): 1, ('given', 0.0): 2, ('sunburnt', 0.0): 1, ('safer', 0.0): 1, ('k3g', 0.0): 1, ('input', 0.0): 1, ('gamestomp', 0.0): 1, ('desc', 0.0): 1, (\"angelo'\", 0.0): 1, ('yna', 0.0): 1, ('psygustokita', 0.0): 2, ('fiver', 0.0): 1, ('toward', 0.0): 1, ('sakho', 0.0): 1, ('threat', 0.0): 1, ('goalscor', 0.0): 1, ('10:59', 0.0): 1, ('11.00', 0.0): 1, ('sham', 0.0): 1, ('tricki', 0.0): 1, ('baao', 0.0): 1, ('nisrina', 0.0): 1, ('crazi', 0.0): 8, ('ladygaga', 0.0): 1, (\"you'\", 0.0): 2, ('pari', 0.0): 2, ('marrish', 0.0): 1, (\"otp'\", 0.0): 1, ('6:15', 0.0): 1, ('edomnt', 0.0): 1, ('qih', 0.0): 1, ('shxb', 0.0): 1, ('1000', 0.0): 1, ('chilton', 0.0): 1, ('mother', 0.0): 2, ('obsess', 0.0): 1, ('creepi', 0.0): 2, ('josh', 0.0): 1, ('boohoo', 0.0): 1, ('fellow', 0.0): 2, ('tweep', 0.0): 1, ('roar', 0.0): 1, ('victori', 0.0): 1, ('tweepsmatchout', 0.0): 1, ('nein', 0.0): 3, ('404', 0.0): 1, ('midnight', 0.0): 2, ('willlow', 0.0): 1, ('hbd', 0.0): 1, ('sowwi', 0.0): 1, ('3000', 0.0): 1, ('grind', 0.0): 1, ('gear', 0.0): 1, ('0.001', 0.0): 1, ('meant', 0.0): 6, ('portrait', 0.0): 1, ('mode', 0.0): 2, ('fact', 0.0): 4, ('11:11', 0.0): 4, ('shanzay', 0.0): 1, ('salabrati', 0.0): 1, ('journo', 0.0): 1, ('lure', 0.0): 1, ('gang', 0.0): 1, ('twist', 0.0): 1, ('mashaket', 0.0): 1, ('pet', 0.0): 2, ('bapak', 0.0): 1, ('royal', 0.0): 2, ('prima', 0.0): 1, ('mune', 0.0): 1, ('874', 0.0): 1, ('plisss', 0.0): 1, ('elf', 0.0): 1, ('teenchoic', 0.0): 5, ('choiceinternationalartist', 0.0): 5, ('superjunior', 0.0): 5, (\"he'll\", 0.0): 1, ('sunway', 0.0): 1, ('petal', 0.0): 1, ('jaya', 0.0): 1, ('selangor', 0.0): 1, ('glow', 0.0): 1, ('huhuu', 0.0): 1, ('congratul', 0.0): 2, ('margo', 0.0): 1, ('konga', 0.0): 1, ('ni', 0.0): 4, ('wa', 0.0): 2, ('ode', 0.0): 1, ('disvirgin', 0.0): 1, ('bride', 0.0): 3, ('yulin', 0.0): 1, ('meat', 0.0): 1, ('festiv', 0.0): 2, ('imma', 0.0): 2, ('syawal', 0.0): 1, ('lapar', 0.0): 1, ('foundat', 0.0): 1, ('clash', 0.0): 2, ('facil', 0.0): 1, ('dh', 0.0): 2, ('chalet', 0.0): 1, ('suay', 0.0): 1, ('anot', 0.0): 1, ('bugger', 0.0): 1, ('एक', 0.0): 1, ('बार', 0.0): 1, ('फिर', 0.0): 1, ('सेँ', 0.0): 1, ('धोखा', 0.0): 1, ('chandauli', 0.0): 1, ('majhwar', 0.0): 1, ('railway', 0.0): 1, ('tito', 0.0): 2, ('tita', 0.0): 1, ('cousin', 0.0): 3, ('critic', 0.0): 1, ('condit', 0.0): 1, ('steal', 0.0): 1, ('narco', 0.0): 1, ('regen', 0.0): 1, ('unfav', 0.0): 2, ('benadryl', 0.0): 1, ('offlin', 0.0): 1, ('arent', 0.0): 1, ('msg', 0.0): 1, ('yg', 0.0): 1, ('gg', 0.0): 3, ('sxrew', 0.0): 1, ('dissappear', 0.0): 1, ('swap', 0.0): 1, ('bleed', 0.0): 1, ('ishal', 0.0): 1, ('mi', 0.0): 2, ('thaank', 0.0): 1, ('jhezz', 0.0): 1, ('sneak', 0.0): 3, ('soft', 0.0): 1, ('defenc', 0.0): 1, ('defens', 0.0): 1, ('nrltigersroost', 0.0): 1, ('indiana', 0.0): 2, ('hibb', 0.0): 1, ('biblethump', 0.0): 1, ('rlyyi', 0.0): 1, ('septum', 0.0): 1, ('pierc', 0.0): 2, ('goood', 0.0): 1, ('hiya', 0.0): 1, ('fire', 0.0): 1, ('venom', 0.0): 1, ('carriag', 0.0): 1, ('pink', 0.0): 1, ('fur-trim', 0.0): 1, ('stetson', 0.0): 1, ('error', 0.0): 4, ('59', 0.0): 1, ('xue', 0.0): 1, ('midori', 0.0): 1, ('sakit', 0.0): 2, ('mateo', 0.0): 1, ('hawk', 0.0): 2, ('bartend', 0.0): 1, ('surf', 0.0): 1, ('despair', 0.0): 1, ('insta', 0.0): 1, ('promo', 0.0): 1, ('iwantin', 0.0): 1, ('___', 0.0): 2, ('fault', 0.0): 3, ('goodluck', 0.0): 1, ('pocket', 0.0): 1, ('help@veryhq.co.uk', 0.0): 1, ('benedictervent', 0.0): 1, ('content', 0.0): 1, ('221b', 0.0): 1, ('popcorn', 0.0): 3, ('joyc', 0.0): 1, ('ooop', 0.0): 1, ('spotifi', 0.0): 1, ('paalam', 0.0): 1, ('sazbal', 0.0): 1, ('incid', 0.0): 1, ('aaahh', 0.0): 1, ('gooo', 0.0): 1, (\"stomach'\", 0.0): 1, ('growl', 0.0): 1, ('beard', 0.0): 1, ('nooop', 0.0): 1, ('🎉', 0.0): 3, ('ding', 0.0): 3, ('hundr', 0.0): 1, ('meg', 0.0): 1, (\"verity'\", 0.0): 1, ('rupert', 0.0): 1, ('amin', 0.0): 1, ('studi', 0.0): 2, ('pleaaas', 0.0): 1, ('👆🏻', 0.0): 2, ('woaah', 0.0): 1, ('solvo', 0.0): 1, ('twin', 0.0): 2, (\"friday'\", 0.0): 1, ('lego', 0.0): 1, ('barefoot', 0.0): 1, ('twelvyy', 0.0): 1, ('boaz', 0.0): 1, ('myhil', 0.0): 1, ('takeov', 0.0): 1, ('wba', 0.0): 1, (\"taeyeon'\", 0.0): 1, ('derp', 0.0): 1, ('pd', 0.0): 1, ('zoom', 0.0): 2, (\"sunny'\", 0.0): 1, ('besst', 0.0): 1, ('plagu', 0.0): 1, ('pit', 0.0): 1, ('rich', 0.0): 1, ('sight', 0.0): 1, ('frail', 0.0): 1, ('lotteri', 0.0): 1, ('ride', 0.0): 2, ('twurkin', 0.0): 1, ('razzist', 0.0): 1, ('tumblr', 0.0): 1, ('shek', 0.0): 1, ('609', 0.0): 1, ('mugshot', 0.0): 1, ('attend', 0.0): 3, ('plsss', 0.0): 4, ('taissa', 0.0): 1, ('farmiga', 0.0): 1, ('robert', 0.0): 1, ('qualiti', 0.0): 1, ('daniel', 0.0): 1, ('latest', 0.0): 3, ('softwar', 0.0): 1, ('restor', 0.0): 2, ('momo', 0.0): 2, ('pharma', 0.0): 1, ('immov', 0.0): 1, ('messi', 0.0): 1, ('ansh', 0.0): 1, ('f1', 0.0): 1, ('billion', 0.0): 1, ('rand', 0.0): 1, ('bein', 0.0): 1, ('tla', 0.0): 1, ('tweng', 0.0): 1, ('gene', 0.0): 1, ('up.com', 0.0): 1, ('counti', 0.0): 2, ('cooler', 0.0): 1, ('minhyuk', 0.0): 1, ('gold', 0.0): 2, ('1900', 0.0): 1, ('😪', 0.0): 3, ('yu', 0.0): 1, ('hz', 0.0): 2, ('selena', 0.0): 2, ('emta', 0.0): 1, ('hatigii', 0.0): 1, ('b2aa', 0.0): 1, ('yayyy', 0.0): 1, ('anesthesia', 0.0): 1, ('penrith', 0.0): 1, ('emu', 0.0): 1, ('plain', 0.0): 1, ('staff', 0.0): 3, ('untouch', 0.0): 1, ('brienn', 0.0): 1, ('lsh', 0.0): 1, ('gunna', 0.0): 1, ('former', 0.0): 1, ('darn', 0.0): 1, ('allah', 0.0): 4, ('pakistan', 0.0): 2, ('juudiciari', 0.0): 1, (\"horton'\", 0.0): 1, ('dunkin', 0.0): 1, ('socialis', 0.0): 1, ('cara', 0.0): 1, (\"delevingne'\", 0.0): 1, ('fear', 0.0): 1, ('drug', 0.0): 1, ('lace', 0.0): 1, ('fank', 0.0): 1, ('takfaham', 0.0): 1, ('ufff', 0.0): 1, ('sr', 0.0): 2, ('dard', 0.0): 1, ('katekyn', 0.0): 1, ('ehh', 0.0): 1, ('yeahhh', 0.0): 2, ('hacharatt', 0.0): 1, ('niwll', 0.0): 1, ('defin', 0.0): 1, ('wit', 0.0): 2, ('goa', 0.0): 1, ('lini', 0.0): 1, ('kasi', 0.0): 3, ('rhd', 0.0): 1, ('1st', 0.0): 3, ('wae', 0.0): 1, ('subsid', 0.0): 1, ('20th', 0.0): 1, ('anniversari', 0.0): 1, ('youngja', 0.0): 1, ('harumph', 0.0): 1, ('soggi', 0.0): 1, ('weed', 0.0): 1, ('ireland', 0.0): 3, ('sakura', 0.0): 1, ('flavour', 0.0): 1, ('chokki', 0.0): 1, ('🌸', 0.0): 1, ('unavail', 0.0): 2, ('richard', 0.0): 2, ('laptop', 0.0): 2, ('satya', 0.0): 1, ('aditya', 0.0): 1, ('🍜', 0.0): 3, ('vibrat', 0.0): 1, ('an', 0.0): 2, ('cu', 0.0): 1, ('dhaka', 0.0): 1, ('jam', 0.0): 1, ('shall', 0.0): 2, ('cornetto', 0.0): 3, ('noseble', 0.0): 1, ('nintendo', 0.0): 3, ('wew', 0.0): 1, ('ramo', 0.0): 1, ('ground', 0.0): 2, ('shawn', 0.0): 1, ('mend', 0.0): 1, ('l', 0.0): 2, ('dinghi', 0.0): 1, ('skye', 0.0): 1, ('store', 0.0): 3, ('descript', 0.0): 2, ('colleagu', 0.0): 2, ('gagal', 0.0): 2, ('txt', 0.0): 1, ('sim', 0.0): 1, ('nooot', 0.0): 1, ('notch', 0.0): 1, ('tht', 0.0): 2, ('starv', 0.0): 4, ('\\U000fe196', 0.0): 1, ('pyjama', 0.0): 1, ('swifti', 0.0): 1, ('sorna', 0.0): 1, ('lurgi', 0.0): 1, ('jim', 0.0): 2, ('6gb', 0.0): 1, ('fenestoscop', 0.0): 1, ('etienn', 0.0): 1, ('bandana', 0.0): 3, ('bigger', 0.0): 2, ('vagina', 0.0): 1, ('suriya', 0.0): 1, ('dangl', 0.0): 1, ('mjhe', 0.0): 2, ('aaj', 0.0): 1, ('tak', 0.0): 3, ('kisi', 0.0): 1, ('kiya', 0.0): 1, ('eyesight', 0.0): 1, ('25x30', 0.0): 1, ('aftenoon', 0.0): 1, ('booor', 0.0): 1, ('uuu', 0.0): 1, ('boyfriend', 0.0): 8, ('freebiefriday', 0.0): 1, ('garag', 0.0): 1, ('michael', 0.0): 1, ('obvious', 0.0): 1, ('denim', 0.0): 1, ('somebodi', 0.0): 1, ('ce', 0.0): 1, ('gw', 0.0): 1, ('anatomi', 0.0): 1, ('no1', 0.0): 1, (\"morisette'\", 0.0): 1, ('flash', 0.0): 1, ('non-trial', 0.0): 1, ('sayhernam', 0.0): 1, ('lootcrat', 0.0): 1, ('item', 0.0): 1, ('inca', 0.0): 1, ('trail', 0.0): 1, ('sandboard', 0.0): 1, ('derbi', 0.0): 1, ('coffe', 0.0): 1, ('unabl', 0.0): 3, ('signatur', 0.0): 1, ('dish', 0.0): 1, ('unfamiliar', 0.0): 1, ('kitchen', 0.0): 3, ('coldest', 0.0): 1, (\"old'\", 0.0): 1, ('14518344', 0.0): 1, ('61', 0.0): 1, ('thirdwheel', 0.0): 1, ('lovebird', 0.0): 1, ('nth', 0.0): 1, ('imo', 0.0): 1, ('familiar', 0.0): 1, ('@juliettemaughan', 0.0): 1, ('copi', 0.0): 1, ('sensiesha', 0.0): 1, ('eldest', 0.0): 1, ('netbal', 0.0): 1, ('😟', 0.0): 1, ('keedz', 0.0): 1, ('taybigail', 0.0): 1, ('jordan', 0.0): 1, ('tournament', 0.0): 1, ('goin', 0.0): 1, ('ps4', 0.0): 3, ('kink', 0.0): 1, ('charger', 0.0): 1, ('streak', 0.0): 1, ('scorch', 0.0): 1, ('srski', 0.0): 1, ('tdc', 0.0): 1, ('egypt', 0.0): 1, ('in-sensit', 0.0): 1, ('cooper', 0.0): 3, ('invit', 0.0): 1, ('donna', 0.0): 1, ('thurston', 0.0): 1, ('collin', 0.0): 1, ('quietli', 0.0): 2, ('kennel', 0.0): 1, ('911', 0.0): 1, ('pluckersss', 0.0): 1, ('gion', 0.0): 1, ('886', 0.0): 1, ('nsfw', 0.0): 1, ('kidschoiceaward', 0.0): 1, ('ming', 0.0): 1, ('pbr', 0.0): 1, ('shoutout', 0.0): 1, ('periscop', 0.0): 1, ('ut', 0.0): 1, ('shawti', 0.0): 1, ('naw', 0.0): 4, (\"sterling'\", 0.0): 1, ('9muse', 0.0): 1, ('hrryok', 0.0): 2, ('asap', 0.0): 2, ('wnt', 0.0): 1, ('9:30', 0.0): 1, ('9:48', 0.0): 1, ('9/11', 0.0): 1, ('bueno', 0.0): 1, ('receptionist', 0.0): 1, ('ella', 0.0): 2, ('goe', 0.0): 4, ('ketchup', 0.0): 1, ('tasteless', 0.0): 1, ('deantd', 0.0): 1, ('justgotkanekifi', 0.0): 1, ('notgonnabeactivefor', 0.0): 1, ('2weeksdontmissittoomuch', 0.0): 1, ('2013', 0.0): 1, ('disney', 0.0): 2, ('vlog', 0.0): 1, ('swim', 0.0): 1, ('turtl', 0.0): 2, ('cnn', 0.0): 2, ('straplin', 0.0): 1, ('theatr', 0.0): 1, ('guncontrol', 0.0): 1, ('stung', 0.0): 2, ('tweak', 0.0): 1, (\"thát'\", 0.0): 1, ('powerpoint', 0.0): 1, ('present', 0.0): 5, ('diner', 0.0): 1, ('no-no', 0.0): 1, ('hind', 0.0): 1, ('circuit', 0.0): 1, ('secondari', 0.0): 1, ('sodder', 0.0): 1, ('perhap', 0.0): 2, ('mobitel', 0.0): 1, ('colin', 0.0): 1, ('playstat', 0.0): 2, ('charg', 0.0): 4, ('exp', 0.0): 1, ('misspelt', 0.0): 1, ('wan', 0.0): 1, ('hyungwon', 0.0): 2, ('alarm', 0.0): 1, ('needicecreamnow', 0.0): 1, ('shake', 0.0): 1, ('repeatedli', 0.0): 1, ('nu-uh', 0.0): 1, ('jace', 0.0): 1, ('mostest', 0.0): 1, ('vip', 0.0): 1, ('urgh', 0.0): 1, ('consol', 0.0): 1, (\"grigson'\", 0.0): 1, ('carrot', 0.0): 1, ('&gt;:-(', 0.0): 4, ('sunburn', 0.0): 1, ('ughh', 0.0): 2, ('enabl', 0.0): 1, ('otter', 0.0): 1, ('protect', 0.0): 1, ('argh', 0.0): 1, ('pon', 0.0): 1, ('otl', 0.0): 2, ('sleepov', 0.0): 2, ('jess', 0.0): 2, ('bebe', 0.0): 1, ('fabina', 0.0): 1, (\"barrista'\", 0.0): 1, ('plant', 0.0): 3, ('pup', 0.0): 2, ('brolli', 0.0): 1, ('mere', 0.0): 2, ('nhi', 0.0): 1, ('dey', 0.0): 2, ('serv', 0.0): 1, ('kepo', 0.0): 1, ('bitin', 0.0): 1, ('pretzel', 0.0): 1, ('bb17', 0.0): 1, ('bblf', 0.0): 1, ('fuckin', 0.0): 1, ('vanilla', 0.0): 1, ('latt', 0.0): 1, ('skulker', 0.0): 1, ('thread', 0.0): 1, ('hungrrryyi', 0.0): 1, ('icloud', 0.0): 1, ('ipod', 0.0): 3, ('hallyu', 0.0): 1, ('buuut', 0.0): 1, ('über', 0.0): 1, ('oki', 0.0): 2, ('8p', 0.0): 1, ('champagn', 0.0): 1, ('harlo', 0.0): 1, ('torrentialrain', 0.0): 1, ('lloyd', 0.0): 1, ('asshol', 0.0): 1, ('clearli', 0.0): 2, ('knowww', 0.0): 2, ('runni', 0.0): 1, ('sehun', 0.0): 1, ('sweater', 0.0): 1, ('intoler', 0.0): 2, ('xenophob', 0.0): 1, ('wtfff', 0.0): 1, ('tone', 0.0): 1, ('wasnt', 0.0): 1, ('1pm', 0.0): 2, ('fantasi', 0.0): 1, ('newer', 0.0): 1, ('pish', 0.0): 1, ('comparison', 0.0): 1, ('remast', 0.0): 1, ('fe14', 0.0): 1, ('icon', 0.0): 2, ('strawberri', 0.0): 1, ('loos', 0.0): 1, ('kapatidkongpogi', 0.0): 1, ('steph', 0.0): 1, ('mel', 0.0): 1, ('longest', 0.0): 1, ('carmen', 0.0): 1, ('login', 0.0): 1, ('respons', 0.0): 3, ('00128835', 0.0): 1, ('wingstop', 0.0): 1, ('budg', 0.0): 1, ('fuq', 0.0): 1, ('ilhoon', 0.0): 1, ('ganteng', 0.0): 1, ('simpl', 0.0): 1, ('getthescoop', 0.0): 1, ('hearess', 0.0): 1, ('677', 0.0): 1, ('txt_shot', 0.0): 1, ('standbi', 0.0): 1, ('inatal', 0.0): 1, ('zenmat', 0.0): 1, ('namecheck', 0.0): 1, ('whistl', 0.0): 1, ('junmyeon', 0.0): 1, ('ddi', 0.0): 1, ('arini', 0.0): 1, ('je', 0.0): 1, ('bright', 0.0): 2, ('igbo', 0.0): 1, ('blamehoney', 0.0): 1, ('whhr', 0.0): 1, ('juan', 0.0): 1, ('snuggl', 0.0): 1, ('internship', 0.0): 1, ('usag', 0.0): 1, ('warn', 0.0): 1, ('vertigo', 0.0): 1, ('panic', 0.0): 1, ('attack', 0.0): 4, ('dual', 0.0): 1, ('carriageway', 0.0): 1, ('aragalang', 0.0): 1, ('08', 0.0): 1, ('tam', 0.0): 1, ('bose', 0.0): 1, ('theo', 0.0): 1, ('anymoree', 0.0): 1, ('rubbish', 0.0): 1, ('cactu', 0.0): 1, ('sorrri', 0.0): 1, ('bowel', 0.0): 1, ('nasti', 0.0): 2, ('tumour', 0.0): 1, ('faster', 0.0): 1, ('puffi', 0.0): 1, ('eyelid', 0.0): 1, ('musica', 0.0): 1, ('dota', 0.0): 1, ('4am', 0.0): 1, ('campsit', 0.0): 1, ('miah', 0.0): 1, ('hahay', 0.0): 1, ('churro', 0.0): 1, ('montana', 0.0): 2, ('reign', 0.0): 1, ('exampl', 0.0): 1, ('inflat', 0.0): 1, ('sic', 0.0): 1, ('reset', 0.0): 1, ('entlerbountli', 0.0): 1, ('tinder', 0.0): 3, ('dirtykik', 0.0): 2, ('sexcam', 0.0): 3, ('spray', 0.0): 1, ('industri', 0.0): 1, ('swollen', 0.0): 1, ('distanc', 0.0): 2, ('jojo', 0.0): 1, ('postcod', 0.0): 1, ('kafi', 0.0): 1, ('din', 0.0): 1, ('mene', 0.0): 1, ('aj', 0.0): 1, ('koi', 0.0): 1, ('rewert', 0.0): 1, ('bunta', 0.0): 1, ('warnaaa', 0.0): 1, ('tortur', 0.0): 2, ('field', 0.0): 1, ('wall', 0.0): 2, ('iran', 0.0): 1, ('irand', 0.0): 1, ('us-iran', 0.0): 1, ('nuclear', 0.0): 1, (\"mit'\", 0.0): 1, ('expert', 0.0): 1, ('sever', 0.0): 3, ('li', 0.0): 1, ('s2e12', 0.0): 1, ('rumpi', 0.0): 1, ('gallon', 0.0): 1, ('ryan', 0.0): 1, ('secret', 0.0): 2, ('dandia', 0.0): 1, ('rbi', 0.0): 1, ('cage', 0.0): 2, ('parrot', 0.0): 1, ('1li', 0.0): 1, ('commiss', 0.0): 1, ('cag', 0.0): 1, ('stripe', 0.0): 2, ('gujarat', 0.0): 1, ('tear', 0.0): 3, ('ily.melani', 0.0): 1, ('unlik', 0.0): 2, ('talent', 0.0): 2, ('deepxcap', 0.0): 1, ('doin', 0.0): 3, ('5:08', 0.0): 1, ('thesi', 0.0): 11, ('belieb', 0.0): 2, ('gtg', 0.0): 1, ('compet', 0.0): 1, ('vv', 0.0): 1, ('respect', 0.0): 5, ('opt-out', 0.0): 1, ('vam', 0.0): 1, ('spece', 0.0): 1, ('ell', 0.0): 1, ('articl', 0.0): 1, ('sexyameli', 0.0): 1, ('fineandyu', 0.0): 1, ('gd', 0.0): 1, ('flesh', 0.0): 1, ('daft', 0.0): 1, ('imsorri', 0.0): 1, ('aku', 0.0): 1, ('chelsea', 0.0): 2, ('koe', 0.0): 1, ('emyu', 0.0): 1, ('confetti', 0.0): 1, ('bf', 0.0): 2, ('sini', 0.0): 1, ('dipoppo', 0.0): 1, ('hop', 0.0): 2, ('bestweekend', 0.0): 1, ('okay-ish', 0.0): 1, ('html', 0.0): 1, ('geneva', 0.0): 1, ('patml', 0.0): 1, ('482', 0.0): 1, ('orgasm', 0.0): 3, ('abouti', 0.0): 1, ('797', 0.0): 1, ('reaalli', 0.0): 1, ('aldub', 0.0): 1, ('nila', 0.0): 1, ('smart', 0.0): 1, ('meter', 0.0): 1, ('display', 0.0): 1, ('unansw', 0.0): 1, ('bri', 0.0): 1, ('magcon', 0.0): 1, ('sinuend', 0.0): 1, ('kak', 0.0): 1, ('laper', 0.0): 2, ('rage', 0.0): 1, ('loser', 0.0): 1, ('brendon', 0.0): 1, (\"urie'\", 0.0): 1, ('sumer', 0.0): 1, ('repackag', 0.0): 1, (\":'d\", 0.0): 1, ('matthew', 0.0): 1, ('yongb', 0.0): 1, ('sued', 0.0): 1, ('suprem', 0.0): 1, ('warm-up', 0.0): 1, ('arriv', 0.0): 4, ('brill', 0.0): 1, ('120', 0.0): 1, ('rub', 0.0): 1, ('belli', 0.0): 1, ('jannatul', 0.0): 1, ('ferdou', 0.0): 1, ('ekta', 0.0): 1, ('kharap', 0.0): 1, ('manush', 0.0): 1, ('mart', 0.0): 2, ('gua', 0.0): 1, ('can', 0.0): 1, (\"khloe'\", 0.0): 1, ('nhe', 0.0): 1, ('yar', 0.0): 1, ('minkyuk', 0.0): 1, ('hol', 0.0): 1, ('isol', 0.0): 1, ('hk', 0.0): 1, ('sensor', 0.0): 1, ('broker', 0.0): 1, ('wna', 0.0): 1, ('flaviana', 0.0): 1, ('chickmt', 0.0): 1, ('123', 0.0): 1, ('letsfootbal', 0.0): 2, ('atk', 0.0): 2, ('greymind', 0.0): 2, ('43', 0.0): 2, ('gayl', 0.0): 2, ('cricket', 0.0): 3, ('2-3', 0.0): 2, ('mood-dump', 0.0): 1, ('livestream', 0.0): 1, ('gotten', 0.0): 1, ('felton', 0.0): 1, ('veriti', 0.0): 1, (\"standen'\", 0.0): 1, ('shortli', 0.0): 1, ('😆', 0.0): 2, ('takoyaki', 0.0): 1, ('piti', 0.0): 1, ('aisyah', 0.0): 1, ('ffvi', 0.0): 1, ('youtu.be/2_gpctsojkw', 0.0): 1, ('donutsss', 0.0): 1, ('50p', 0.0): 1, ('grate', 0.0): 1, ('spars', 0.0): 1, ('dd', 0.0): 1, ('lagi', 0.0): 1, ('rider', 0.0): 1, ('pride', 0.0): 1, ('hueee', 0.0): 1, ('password', 0.0): 1, ('thingi', 0.0): 1, ('georg', 0.0): 1, ('afraid', 0.0): 2, ('chew', 0.0): 2, ('toy', 0.0): 1, ('stella', 0.0): 1, ('threw', 0.0): 2, ('theaccidentalcoupl', 0.0): 1, ('smooth', 0.0): 1, ('handov', 0.0): 1, ('spick', 0.0): 1, ('bebii', 0.0): 1, ('happenend', 0.0): 1, ('dr', 0.0): 1, ('balm', 0.0): 1, ('hmph', 0.0): 1, ('bubba', 0.0): 2, ('floor', 0.0): 3, ('georgi', 0.0): 1, ('oi', 0.0): 1, ('bengali', 0.0): 1, ('masterchef', 0.0): 1, ('whatchya', 0.0): 1, ('petrol', 0.0): 1, ('diesel', 0.0): 1, ('wardrob', 0.0): 1, ('awe', 0.0): 1, ('cock', 0.0): 1, ('nyquil', 0.0): 1, ('poootek', 0.0): 1, ('1,500', 0.0): 1, ('bobbl', 0.0): 1, ('leak', 0.0): 1, ('thermo', 0.0): 1, ('classic', 0.0): 1, ('ti5', 0.0): 1, ('12th', 0.0): 1, ('skate', 0.0): 1, ('tae', 0.0): 1, ('kita', 0.0): 4, ('ia', 0.0): 1, ('pkwalasawa', 0.0): 1, ('india', 0.0): 1, ('corrupt', 0.0): 2, ('access', 0.0): 2, ('anything.sur', 0.0): 1, ('info', 0.0): 6, ('octob', 0.0): 1, ('mubank', 0.0): 2, ('ene', 0.0): 2, ('3k', 0.0): 1, ('zehr', 0.0): 1, ('khani', 0.0): 1, ('groceri', 0.0): 1, ('hubba', 0.0): 1, ('bubbl', 0.0): 1, ('gum', 0.0): 2, ('closet', 0.0): 1, ('jhalak', 0.0): 1, ('. ..', 0.0): 2, ('bakwa', 0.0): 1, ('. ...', 0.0): 1, ('seehiah', 0.0): 1, ('goy', 0.0): 1, ('nacho', 0.0): 1, ('braid', 0.0): 2, ('initi', 0.0): 1, ('ruth', 0.0): 1, ('boong', 0.0): 1, ('recommend', 0.0): 3, ('gta', 0.0): 1, ('cwnt', 0.0): 1, ('trivia', 0.0): 1, ('belat', 0.0): 1, ('rohingya', 0.0): 1, ('muslim', 0.0): 2, ('indict', 0.0): 1, ('traffick', 0.0): 1, ('thailand', 0.0): 1, ('asia', 0.0): 1, ('rumbl', 0.0): 1, ('kumbl', 0.0): 1, ('scold', 0.0): 1, ('phrase', 0.0): 1, ('includ', 0.0): 1, ('tag', 0.0): 2, ('melt', 0.0): 1, ('tfw', 0.0): 1, ('jest', 0.0): 1, ('offend', 0.0): 2, ('sleepingwithsiren', 0.0): 1, ('17th', 0.0): 1, ('bringmethehorizon', 0.0): 1, ('18th', 0.0): 2, ('carva', 0.0): 1, ('regularli', 0.0): 2, ('sympathi', 0.0): 1, ('revamp', 0.0): 1, ('headphon', 0.0): 1, ('cunt', 0.0): 1, ('wacha', 0.0): 1, ('niend', 0.0): 1, ('bravo', 0.0): 1, ('2hr', 0.0): 1, ('13m', 0.0): 1, ('kk', 0.0): 2, ('calibraksaep', 0.0): 2, ('darlin', 0.0): 1, ('stun', 0.0): 1, (\"doedn't\", 0.0): 1, ('meaning', 0.0): 1, ('horrif', 0.0): 2, ('scoup', 0.0): 2, ('paypal', 0.0): 3, ('sweedi', 0.0): 1, ('nam', 0.0): 1, (\"sacconejoly'\", 0.0): 1, ('bethesda', 0.0): 1, ('fallout', 0.0): 1, ('minecon', 0.0): 1, ('perfect', 0.0): 2, ('katee', 0.0): 1, ('iloveyouu', 0.0): 1, ('linux', 0.0): 1, ('nawww', 0.0): 1, ('chikka', 0.0): 1, ('ug', 0.0): 1, ('rata', 0.0): 1, ('soonest', 0.0): 1, ('mwamwa', 0.0): 1, ('faggot', 0.0): 1, ('doubt', 0.0): 2, ('fyi', 0.0): 1, ('profil', 0.0): 1, ('nicest', 0.0): 1, ('mehendi', 0.0): 1, ('dash', 0.0): 1, ('bookmark', 0.0): 1, ('whay', 0.0): 1, ('shaa', 0.0): 1, ('prami', 0.0): 1, ('😚', 0.0): 4, ('ngee', 0.0): 1, ('ann', 0.0): 1, ('crikey', 0.0): 2, ('snit', 0.0): 1, ('nathanielhinanakit', 0.0): 1, ('naya', 0.0): 1, ('spinni', 0.0): 1, ('wheel', 0.0): 2, ('albeit', 0.0): 1, ('athlet', 0.0): 1, ('gfriend', 0.0): 2, ('yung', 0.0): 2, ('fugli', 0.0): 1, ('💞', 0.0): 4, ('jongda', 0.0): 1, ('hardli', 0.0): 2, ('tlist', 0.0): 1, ('budget', 0.0): 1, ('pabebegirl', 0.0): 1, ('pabeb', 0.0): 2, ('alter', 0.0): 1, ('sandra', 0.0): 2, ('bland', 0.0): 2, ('storifi', 0.0): 1, ('abbi', 0.0): 2, ('mtvhottest', 0.0): 1, ('gaga', 0.0): 1, ('rib', 0.0): 1, ('😵', 0.0): 1, ('hulkamania', 0.0): 1, ('unlov', 0.0): 1, ('lazi', 0.0): 3, ('ihhh', 0.0): 1, ('stackar', 0.0): 1, ('basil', 0.0): 1, ('remedi', 0.0): 1, ('ov', 0.0): 2, ('raiz', 0.0): 1, ('nvr', 0.0): 1, ('gv', 0.0): 1, ('up.wt', 0.0): 1, ('wt', 0.0): 1, ('imran', 0.0): 2, ('achiev', 0.0): 1, ('thr', 0.0): 1, ('soln', 0.0): 1, (\"sister'\", 0.0): 1, ('hong', 0.0): 1, ('kong', 0.0): 1, ('31st', 0.0): 1, ('pipe', 0.0): 1, ('sept', 0.0): 2, ('lawn', 0.0): 1, (\"cupid'\", 0.0): 1, ('torn', 0.0): 1, ('retain', 0.0): 1, ('clown', 0.0): 2, ('lipstick', 0.0): 1, ('haiss', 0.0): 1, ('todayi', 0.0): 1, ('thoo', 0.0): 1, ('everday', 0.0): 1, ('hangout', 0.0): 2, ('steven', 0.0): 2, ('william', 0.0): 1, ('umboh', 0.0): 1, ('goodafternoon', 0.0): 1, ('jadin', 0.0): 1, ('thiz', 0.0): 1, ('iz', 0.0): 1, ('emeg', 0.0): 1, ('kennat', 0.0): 1, ('reunit', 0.0): 1, ('abi', 0.0): 1, ('arctic', 0.0): 1, ('chicsirif', 0.0): 1, ('structur', 0.0): 1, ('cumbia', 0.0): 1, ('correct', 0.0): 1, ('badlif', 0.0): 1, ('4-5', 0.0): 2, ('kaslkdja', 0.0): 1, ('3wk', 0.0): 1, ('flower', 0.0): 1, ('feverfew', 0.0): 1, ('weddingflow', 0.0): 1, ('diyflow', 0.0): 1, ('fitn', 0.0): 1, ('worth', 0.0): 4, ('wolverin', 0.0): 1, ('khan', 0.0): 1, ('innoc', 0.0): 1, ('🙏🏻', 0.0): 1, ('🎂', 0.0): 2, ('memem', 0.0): 2, ('krystoria', 0.0): 1, ('snob', 0.0): 1, ('zumba', 0.0): 1, ('greekcrisi', 0.0): 1, ('remain', 0.0): 1, ('dutch', 0.0): 1, ('legibl', 0.0): 2, ('isra', 0.0): 1, ('passport', 0.0): 1, ('froze', 0.0): 1, ('theori', 0.0): 1, ('23rd', 0.0): 1, ('24th', 0.0): 1, ('stomachach', 0.0): 1, ('slice', 0.0): 1, ('ཀ', 0.0): 1, ('again', 0.0): 1, ('otani', 0.0): 1, ('3-0', 0.0): 1, ('3rd', 0.0): 3, ('bottom', 0.0): 2, ('niaaa', 0.0): 1, ('2/4', 0.0): 1, ('scheme', 0.0): 2, ('fckin', 0.0): 1, ('hii', 0.0): 1, ('vin', 0.0): 1, ('plss', 0.0): 1, ('rpli', 0.0): 1, ('rat', 0.0): 3, ('bollywood', 0.0): 1, ('mac', 0.0): 1, ('backup', 0.0): 2, ('lune', 0.0): 1, ('robinhood', 0.0): 1, ('robinhoodi', 0.0): 1, ('🚙', 0.0): 1, ('💚', 0.0): 1, ('docopenhagen', 0.0): 1, ('setter', 0.0): 1, ('swipe', 0.0): 1, ('bbygurl', 0.0): 1, ('neil', 0.0): 1, ('caribbean', 0.0): 1, ('6yr', 0.0): 1, ('jabongatpumaurbanstamped', 0.0): 2, ('takraw', 0.0): 1, ('fersure', 0.0): 1, ('angi', 0.0): 1, ('sheriff', 0.0): 1, ('aaag', 0.0): 1, (\"i'mo\", 0.0): 1, ('sulk', 0.0): 1, ('selfish', 0.0): 1, ('trick', 0.0): 2, ('nonc', 0.0): 1, ('pad', 0.0): 1, ('bison', 0.0): 1, ('motiv', 0.0): 2, (\"q'don\", 0.0): 1, ('cheat', 0.0): 2, ('stomp', 0.0): 1, ('aaaaaaaaah', 0.0): 1, ('kany', 0.0): 1, ('mama', 0.0): 1, ('jdjdjdjd', 0.0): 1, (\"jimin'\", 0.0): 1, ('fancaf', 0.0): 1, ('waffl', 0.0): 1, ('87.7', 0.0): 1, ('2fm', 0.0): 1, ('himseek', 0.0): 1, ('kissm', 0.0): 1, ('akua', 0.0): 1, ('glo', 0.0): 1, ('cori', 0.0): 1, ('monteith', 0.0): 1, ('often', 0.0): 1, ('hashbrown', 0.0): 1, ('💘', 0.0): 2, ('pg', 0.0): 1, ('msc', 0.0): 1, ('hierro', 0.0): 1, ('shirleycam', 0.0): 1, ('phonesex', 0.0): 2, ('pal', 0.0): 1, ('111', 0.0): 1, ('gilet', 0.0): 1, ('cheek', 0.0): 1, ('squishi', 0.0): 1, ('lahhh', 0.0): 1, ('eon', 0.0): 1, ('sunris', 0.0): 1, ('beeti', 0.0): 1, ('697', 0.0): 1, ('kikkomansabor', 0.0): 1, ('getaway', 0.0): 1, ('crimin', 0.0): 1, ('amiibo', 0.0): 1, ('batman', 0.0): 1, ('habe', 0.0): 1, ('siannn', 0.0): 1, ('march', 0.0): 1, ('2017', 0.0): 1, ('chuckin', 0.0): 1, ('ampsha', 0.0): 1, ('nia', 0.0): 1, ('strap', 0.0): 1, ('dz9055', 0.0): 1, ('entlead', 0.0): 1, ('590', 0.0): 1, ('twice', 0.0): 5, ('07:02', 0.0): 1, ('ifsc', 0.0): 1, ('mayor', 0.0): 1, ('biodivers', 0.0): 1, ('taxonom', 0.0): 1, ('collabor', 0.0): 1, ('speci', 0.0): 1, ('discoveri', 0.0): 1, ('collar', 0.0): 1, ('3:03', 0.0): 1, ('belt', 0.0): 1, ('smith', 0.0): 2, ('eyelin', 0.0): 1, ('therefor', 0.0): 1, ('netherland', 0.0): 1, ('el', 0.0): 1, ('jeb', 0.0): 1, ('blacklivesmatt', 0.0): 1, ('slogan', 0.0): 1, ('msnbc', 0.0): 1, ('jebbush', 0.0): 1, ('famish', 0.0): 1, ('marino', 0.0): 1, ('qualifi', 0.0): 2, ('suzi', 0.0): 1, ('skirt', 0.0): 1, ('tama', 0.0): 1, ('warrior', 0.0): 2, ('wound', 0.0): 1, ('iraq', 0.0): 1, ('be', 0.0): 2, ('camara', 0.0): 1, ('coveral', 0.0): 1, ('happili', 0.0): 1, ('sneezi', 0.0): 1, ('rogerwatch', 0.0): 1, ('stalker', 0.0): 1, ('velvet', 0.0): 1, ('tradit', 0.0): 1, (\"people'\", 0.0): 1, ('beheaviour', 0.0): 1, (\"robert'\", 0.0): 1, ('.\\n.', 0.0): 2, ('aaron', 0.0): 1, ('jelous', 0.0): 1, ('mtg', 0.0): 1, ('thoughtseiz', 0.0): 1, ('playabl', 0.0): 1, ('oldi', 0.0): 1, ('goodi', 0.0): 1, ('mcg', 0.0): 1, ('inspirit', 0.0): 1, ('shine', 0.0): 1, ('ise', 0.0): 1, ('assum', 0.0): 2, ('waist', 0.0): 2, ('guin', 0.0): 1, ('venu', 0.0): 1, ('evil', 0.0): 1, ('pepper', 0.0): 1, ('thessidew', 0.0): 1, ('877', 0.0): 1, ('genesi', 0.0): 1, ('mexico', 0.0): 2, ('novemb', 0.0): 1, ('mash', 0.0): 1, ('whattsap', 0.0): 1, ('inuyasha', 0.0): 2, ('outfwith', 0.0): 1, ('myungsoo', 0.0): 1, ('organis', 0.0): 1, ('satisfi', 0.0): 1, ('wah', 0.0): 1, ('challo', 0.0): 1, ('pliss', 0.0): 1, ('juliana', 0.0): 1, ('enrol', 0.0): 1, ('darlen', 0.0): 1, ('emoji', 0.0): 2, ('brisban', 0.0): 1, ('merlin', 0.0): 1, ('nawwwe', 0.0): 1, ('hyperbulli', 0.0): 1, ('tong', 0.0): 1, ('nga', 0.0): 1, ('seatmat', 0.0): 1, ('rajud', 0.0): 1, ('barkada', 0.0): 1, ('ore', 0.0): 1, ('kayla', 0.0): 1, ('ericavan', 0.0): 1, ('jong', 0.0): 1, ('dongwoo', 0.0): 1, ('photocard', 0.0): 1, ('wh', 0.0): 1, ('dw', 0.0): 1, ('tumor', 0.0): 1, ('vivian', 0.0): 1, ('mmsmalubhangsakit', 0.0): 1, ('jillcruz', 0.0): 2, ('lgbt', 0.0): 3, ('qt', 0.0): 1, ('19th', 0.0): 1, ('toss', 0.0): 1, ('co-work', 0.0): 1, ('mia', 0.0): 1, ('push', 0.0): 4, ('dare', 0.0): 2, ('unsettl', 0.0): 1, ('gh', 0.0): 1, ('18c', 0.0): 1, ('rlli', 0.0): 2, ('hamster', 0.0): 2, ('sheeran', 0.0): 2, ('preform', 0.0): 2, ('monash', 0.0): 1, ('hitmark', 0.0): 1, ('glitch', 0.0): 1, ('safaa', 0.0): 1, (\"selena'\", 0.0): 1, ('galat', 0.0): 1, ('tum', 0.0): 1, ('ab', 0.0): 5, ('non', 0.0): 1, ('lrka', 0.0): 1, ('bna', 0.0): 1, ('kia', 0.0): 1, ('bhook', 0.0): 1, ('jai', 0.0): 1, ('social', 0.0): 2, ('afterschool', 0.0): 1, ('bilal', 0.0): 1, ('ashraf', 0.0): 1, ('icu', 0.0): 1, ('thanksss', 0.0): 1, ('annnd', 0.0): 1, ('winchest', 0.0): 1, ('{:', 0.0): 1, ('grepe', 0.0): 1, ('grepein', 0.0): 1, ('panem', 0.0): 1, ('lover', 0.0): 1, ('sulli', 0.0): 1, ('cpm', 0.0): 1, ('condemn', 0.0): 1, ('✔', 0.0): 1, ('occur', 0.0): 1, ('unagi', 0.0): 1, ('7elw', 0.0): 1, ('mesh', 0.0): 1, ('beyt', 0.0): 1, ('3a2ad', 0.0): 1, ('fluent', 0.0): 1, ('varsiti', 0.0): 1, ('sengenza', 0.0): 1, ('context', 0.0): 1, ('movnat', 0.0): 1, ('yield', 0.0): 1, ('nbhero', 0.0): 1, (\"it'd\", 0.0): 1, ('background', 0.0): 1, ('agov', 0.0): 1, ('brasileirao', 0.0): 2, ('abus', 0.0): 1, ('unpar', 0.0): 1, ('bianca', 0.0): 1, ('bun', 0.0): 1, ('dislik', 0.0): 1, ('burdensom', 0.0): 1, ('clear', 0.0): 2, ('amelia', 0.0): 1, ('melon', 0.0): 2, ('useless', 0.0): 1, ('soccer', 0.0): 2, ('interview', 0.0): 2, ('thursday', 0.0): 1, ('nevermind', 0.0): 1, ('jeon', 0.0): 1, ('claw', 0.0): 1, ('thigh', 0.0): 2, ('traction', 0.0): 1, ('damnit', 0.0): 1, ('pri', 0.0): 1, ('pv', 0.0): 2, ('reliv', 0.0): 1, ('nyc', 0.0): 2, ('klm', 0.0): 1, ('11am', 0.0): 1, (\"mcd'\", 0.0): 1, ('hung', 0.0): 1, ('bam', 0.0): 1, ('seventh', 0.0): 1, ('splendour', 0.0): 1, ('swedish', 0.0): 1, ('metal', 0.0): 1, ('häirførc', 0.0): 1, ('givecodpieceach', 0.0): 1, ('alic', 0.0): 3, ('stile', 0.0): 1, ('explain', 0.0): 3, ('ili', 0.0): 1, ('pragu', 0.0): 1, ('sadi', 0.0): 1, ('charact', 0.0): 1, ('915', 0.0): 1, ('hayee', 0.0): 2, ('patwari', 0.0): 1, ('mam', 0.0): 1, (\"ik'\", 0.0): 1, ('vision', 0.0): 2, ('ga', 0.0): 1, ('awhhh', 0.0): 1, ('nalang', 0.0): 1, ('hehe', 0.0): 1, ('albanian', 0.0): 1, ('curs', 0.0): 2, ('tava', 0.0): 1, ('chara', 0.0): 1, ('teteh', 0.0): 1, ('verri', 0.0): 1, ('shatter', 0.0): 2, ('sb', 0.0): 1, ('nawe', 0.0): 1, ('bulldog', 0.0): 1, ('macho', 0.0): 1, ('puriti', 0.0): 1, ('kwento', 0.0): 1, ('nakakapikon', 0.0): 1, ('nagbabasa', 0.0): 1, ('blog', 0.0): 2, ('cancer', 0.0): 1, (':-\\\\', 0.0): 1, ('jonatha', 0.0): 4, ('beti', 0.0): 4, ('sogok', 0.0): 1, ('premium', 0.0): 2, ('instrument', 0.0): 1, ('howev', 0.0): 1, ('dastardli', 0.0): 1, ('swine', 0.0): 1, ('envelop', 0.0): 1, ('pipol', 0.0): 1, ('tad', 0.0): 1, ('wiper', 0.0): 2, ('supposedli', 0.0): 1, ('kernel', 0.0): 1, ('intel', 0.0): 1, ('mega', 0.0): 1, ('bent', 0.0): 1, ('socket', 0.0): 1, ('pcgame', 0.0): 1, ('pcupgrad', 0.0): 1, ('brainwash', 0.0): 2, ('smosh', 0.0): 1, ('plawnew', 0.0): 1, ('837', 0.0): 1, ('aswel', 0.0): 1, ('litter', 0.0): 1, ('mensch', 0.0): 1, ('sepanx', 0.0): 1, ('pci', 0.0): 1, ('caerphilli', 0.0): 1, ('omw', 0.0): 1, ('😍', 0.0): 1, ('hahdhdhshh', 0.0): 1, ('growinguppoor', 0.0): 1, ('🇺🇸', 0.0): 2, (\"bangtan'\", 0.0): 1, ('taimoor', 0.0): 1, ('meray', 0.0): 1, ('dost', 0.0): 1, ('tya', 0.0): 1, ('refollow', 0.0): 1, ('dumb', 0.0): 2, ('butt', 0.0): 1, ('pissbabi', 0.0): 1, ('plank', 0.0): 1, ('inconsist', 0.0): 1, ('moor', 0.0): 1, ('bin', 0.0): 1, ('osx', 0.0): 1, ('chrome', 0.0): 1, ('voiceov', 0.0): 1, ('devo', 0.0): 1, ('hulkhogan', 0.0): 1, ('unpleas', 0.0): 1, ('daaamn', 0.0): 1, ('dada', 0.0): 1, ('fulli', 0.0): 1, ('spike', 0.0): 1, (\"panic'\", 0.0): 1, ('22nd', 0.0): 1, ('south', 0.0): 2, ('africa', 0.0): 2, ('190', 0.0): 2, ('lizardz', 0.0): 1, ('deepli', 0.0): 1, ('emerg', 0.0): 1, ('engin', 0.0): 1, ('dormtel', 0.0): 1, ('scho', 0.0): 1, ('siya', 0.0): 1, ('onee', 0.0): 1, ('carri', 0.0): 1, ('7pm', 0.0): 1, ('feta', 0.0): 1, ('blaaaz', 0.0): 1, ('nausea', 0.0): 1, ('awar', 0.0): 1, ('top-up', 0.0): 1, ('sharknado', 0.0): 1, ('erni', 0.0): 1, ('ezoo', 0.0): 1, ('lilybutl', 0.0): 1, ('seduc', 0.0): 2, ('powai', 0.0): 1, ('neighbor', 0.0): 1, ('delhi', 0.0): 1, ('unsaf', 0.0): 1, ('halo', 0.0): 1, ('fred', 0.0): 1, ('gaon', 0.0): 1, ('infnt', 0.0): 1, ('elig', 0.0): 1, ('acub', 0.0): 1, (\"why'd\", 0.0): 1, ('bullshit', 0.0): 2, ('hanaaa', 0.0): 1, ('jn', 0.0): 1, ('tau', 0.0): 1, ('basta', 0.0): 1, ('sext', 0.0): 1, ('addm', 0.0): 1, ('hotmusicdeloco', 0.0): 2, ('dhi', 0.0): 1, ('👉', 0.0): 1, ('8ball', 0.0): 1, ('fakmarey', 0.0): 1, ('doo', 0.0): 2, ('six', 0.0): 3, ('flag', 0.0): 1, ('fulltim', 0.0): 1, ('awkward', 0.0): 1, ('beet', 0.0): 1, ('juic', 0.0): 1, ('dci', 0.0): 1, ('granddad', 0.0): 1, ('minion', 0.0): 3, ('bucket', 0.0): 1, ('kapan', 0.0): 1, ('udah', 0.0): 1, ('dihapu', 0.0): 1, ('hilang', 0.0): 1, ('dari', 0.0): 1, ('muka', 0.0): 1, ('bumi', 0.0): 1, ('narrow', 0.0): 1, ('gona', 0.0): 2, ('chello', 0.0): 1, ('gate', 0.0): 1, ('guard', 0.0): 1, ('crepe', 0.0): 1, ('forsaken', 0.0): 1, ('kanin', 0.0): 1, ('hypixel', 0.0): 1, ('grrr', 0.0): 1, ('thestruggleisr', 0.0): 1, ('geek', 0.0): 1, ('gamer', 0.0): 2, ('afterbirth', 0.0): 1, (\"apink'\", 0.0): 1, ('overperhatian', 0.0): 1, ('son', 0.0): 1, ('pox', 0.0): 1, ('ahm', 0.0): 1, ('karli', 0.0): 1, ('kloss', 0.0): 1, ('goofi', 0.0): 1, ('pcd', 0.0): 1, ('antagonis', 0.0): 1, ('writer', 0.0): 1, ('nudg', 0.0): 1, ('delv', 0.0): 1, ('grandad', 0.0): 1, (\"gray'\", 0.0): 1, ('followk', 0.0): 1, ('suggest', 0.0): 2, ('pace', 0.0): 1, ('maker', 0.0): 1, ('molli', 0.0): 1, ('higher', 0.0): 1, ('ceremoni', 0.0): 1, ('christin', 0.0): 1, ('moodi', 0.0): 1, ('throwback', 0.0): 1, ('fav', 0.0): 3, ('barb', 0.0): 1, ('creasi', 0.0): 1, ('deputi', 0.0): 1, ('tast', 0.0): 1, (\"banana'\", 0.0): 1, ('saludo', 0.0): 1, ('dissapoint', 0.0): 1, ('😫', 0.0): 1, ('&lt;--', 0.0): 1, (\"bae'\", 0.0): 1, ('pimpl', 0.0): 2, ('amount', 0.0): 2, ('tdi', 0.0): 1, ('pamela', 0.0): 1, ('mini', 0.0): 1, ('mast', 0.0): 1, ('intermitt', 0.0): 1, ('servic', 0.0): 3, ('janniecam', 0.0): 1, ('musicbiz', 0.0): 1, ('braxton', 0.0): 1, ('pro', 0.0): 2, ('urban', 0.0): 1, ('unpreced', 0.0): 1, ('tebow', 0.0): 1, ('okaaay', 0.0): 1, ('sayanggg', 0.0): 1, ('housework', 0.0): 1, ('bust', 0.0): 2, ('disneyland', 0.0): 1, ('thoma', 0.0): 1, ('tommyy', 0.0): 1, ('billi', 0.0): 1, ('kevin', 0.0): 1, ('clifton', 0.0): 1, ('strictli', 0.0): 1, ('nsc', 0.0): 1, ('mat', 0.0): 1, ('0', 0.0): 1, ('awhh', 0.0): 1, ('ram', 0.0): 2, ('voucher', 0.0): 1, ('smadvow', 0.0): 1, ('544', 0.0): 1, ('acdc', 0.0): 1, ('aker', 0.0): 1, ('gmail', 0.0): 1, ('sprevelink', 0.0): 1, ('633', 0.0): 1, ('lana', 0.0): 2, ('loveyoutilltheendcart', 0.0): 1, ('sfv', 0.0): 1, ('6/7', 0.0): 1, ('winner', 0.0): 1, ('20/1', 0.0): 1, ('david', 0.0): 1, ('rosi', 0.0): 1, ('hayoung', 0.0): 1, ('nlb', 0.0): 1, ('@_', 0.0): 1, ('tayo', 0.0): 1, ('forth', 0.0): 1, ('suspect', 0.0): 1, ('mening', 0.0): 1, ('viral', 0.0): 1, ('tonsil', 0.0): 1, ('😷', 0.0): 1, ('😝', 0.0): 1, ('babyy', 0.0): 2, ('cushion', 0.0): 1, ('😿', 0.0): 1, ('💓', 0.0): 2, ('weigh', 0.0): 1, ('keen', 0.0): 1, ('petrofac', 0.0): 1, (';-)', 0.0): 1, ('wig', 0.0): 1, (\"mark'\", 0.0): 1, ('pathet', 0.0): 1, ('burden.say', 0.0): 1, ('itchi', 0.0): 1, ('cheaper', 0.0): 1, ('malaysia', 0.0): 1, ('130', 0.0): 1, ('snapchattimg', 0.0): 1, ('😏', 0.0): 4, ('sin', 0.0): 1, ('lor', 0.0): 1, ('dedic', 0.0): 1, ('worriedli', 0.0): 1, ('stare', 0.0): 1, ('toneadi', 0.0): 1, ('46532', 0.0): 1, ('snapdirti', 0.0): 1, ('sheskindahot', 0.0): 1, ('corps', 0.0): 1, ('taeni', 0.0): 1, ('fyeah', 0.0): 1, ('andromeda', 0.0): 1, ('yunni', 0.0): 1, ('whdjwksja', 0.0): 1, ('ziam', 0.0): 1, ('100k', 0.0): 1, ('spoil', 0.0): 1, ('curtain', 0.0): 1, ('watchabl', 0.0): 1, ('migrin', 0.0): 1, ('gdce', 0.0): 1, ('gamescom', 0.0): 1, (\"do't\", 0.0): 1, ('parcel', 0.0): 1, ('num', 0.0): 1, ('oooouch', 0.0): 1, ('pinki', 0.0): 1, ('👣', 0.0): 1, ('podiatrist', 0.0): 1, ('gusto', 0.0): 1, (\"rodic'\", 0.0): 1, (\"one'\", 0.0): 1, ('adoohh', 0.0): 1, ('b-butt', 0.0): 1, ('tigermilk', 0.0): 1, ('east', 0.0): 1, ('dulwich', 0.0): 1, ('intens', 0.0): 1, ('kagami', 0.0): 1, ('kuroko', 0.0): 1, ('sana', 0.0): 2, ('makita', 0.0): 1, ('spooki', 0.0): 1, ('smol', 0.0): 1, ('bean', 0.0): 1, ('fagan', 0.0): 1, ('meadowhal', 0.0): 1, ('lola', 0.0): 1, ('nadalaw', 0.0): 1, ('labyu', 0.0): 1, ('jot', 0.0): 1, ('ivypowel', 0.0): 1, ('homeslic', 0.0): 1, ('33', 0.0): 2, ('emoticon', 0.0): 2, ('eyebrow', 0.0): 1, ('prettylook', 0.0): 1, ('whitney', 0.0): 1, ('houston', 0.0): 1, ('aur', 0.0): 1, ('shamil', 0.0): 1, ('tonn', 0.0): 1, ('statu', 0.0): 1, ('→', 0.0): 1, ('suddenli', 0.0): 2, ('alli', 0.0): 2, ('wrap', 0.0): 1, ('neck', 0.0): 1, ('heartbroken', 0.0): 1, ('chover', 0.0): 1, ('cebu', 0.0): 1, ('lechon', 0.0): 1, ('kitten', 0.0): 2, ('jannygreen', 0.0): 2, ('suicid', 0.0): 2, ('forgiv', 0.0): 1, ('conno', 0.0): 1, ('brooo', 0.0): 1, ('rout', 0.0): 1, ('lovebox', 0.0): 1, ('prod', 0.0): 1, ('osad', 0.0): 1, ('scam', 0.0): 1, ('itb', 0.0): 1, ('omigod', 0.0): 1, ('ehem', 0.0): 1, ('ala', 0.0): 1, ('yeke', 0.0): 1, ('jumpa', 0.0): 1, ('😋', 0.0): 1, ('ape', 0.0): 1, ('1.2', 0.0): 1, ('map', 0.0): 1, ('namin', 0.0): 1, ('govt', 0.0): 1, ('e-petit', 0.0): 1, ('pretend', 0.0): 1, ('irk', 0.0): 1, ('ruess', 0.0): 1, ('program', 0.0): 1, ('aigoo', 0.0): 1, ('doujin', 0.0): 1, ('killua', 0.0): 1, ('ginggon', 0.0): 1, ('guys.al', 0.0): 1, ('ytd', 0.0): 1, ('pdapaghimok', 0.0): 1, ('flexibl', 0.0): 1, ('sheet', 0.0): 1, ('nanaman', 0.0): 1, ('pinay', 0.0): 1, ('pie', 0.0): 1, ('jadi', 0.0): 1, ('langsung', 0.0): 1, ('flasback', 0.0): 1, ('franc', 0.0): 1, (':|', 0.0): 1, ('lo', 0.0): 1, ('nicknam', 0.0): 1, ('involv', 0.0): 1, ('scrape', 0.0): 1, ('pile', 0.0): 1, ('sare', 0.0): 1, ('bandar', 0.0): 1, ('varg', 0.0): 1, ('hammer', 0.0): 1, ('lolo', 0.0): 1, ('xbsbabnb', 0.0): 1, ('stilll', 0.0): 1, ('apma', 0.0): 2, ('leadership', 0.0): 1, ('wakeupgop', 0.0): 1, ('mv', 0.0): 1, ('bull', 0.0): 1, ('trafficcc', 0.0): 1, ('oscar', 0.0): 1, ('pornographi', 0.0): 1, ('slutsham', 0.0): 1, ('ect', 0.0): 1, ('poland', 0.0): 1, ('faraway', 0.0): 1, ('700', 0.0): 1, ('800', 0.0): 1, ('cgi', 0.0): 1, ('pun', 0.0): 1, (\"x'\", 0.0): 1, ('osaka', 0.0): 1, ('junior', 0.0): 1, ('aytona', 0.0): 1, ('hala', 0.0): 1, ('mathird', 0.0): 1, ('jkjk', 0.0): 1, ('backtrack', 0.0): 1, ('util', 0.0): 1, ('pat', 0.0): 1, ('jay', 0.0): 2, ('broh', 0.0): 1, ('calll', 0.0): 1, ('icaru', 0.0): 1, ('awn', 0.0): 1, ('bach', 0.0): 1, ('court', 0.0): 1, ('landlord', 0.0): 1, (\"mp'\", 0.0): 1, ('dame', 0.0): 1, ('gossip', 0.0): 1, ('purpl', 0.0): 2, ('tie', 0.0): 1, ('ishii', 0.0): 1, ('clara', 0.0): 1, ('yile', 0.0): 1, ('whatev', 0.0): 1, ('stil', 0.0): 1, ('sidharth', 0.0): 1, ('ndabenhl', 0.0): 1, ('doggi', 0.0): 1, ('antag', 0.0): 1, ('41', 0.0): 1, ('thu', 0.0): 1, ('jenner', 0.0): 1, ('troubleshoot', 0.0): 1, (\"convo'\", 0.0): 1, ('dem', 0.0): 1, ('tix', 0.0): 2, ('automat', 0.0): 1, ('redirect', 0.0): 1, ('gigi', 0.0): 1, ('carter', 0.0): 1, ('corn', 0.0): 2, ('chip', 0.0): 2, ('nnnooo', 0.0): 1, ('cz', 0.0): 1, ('gorilla', 0.0): 1, ('hbm', 0.0): 1, ('humid', 0.0): 1, ('admir', 0.0): 1, ('consist', 0.0): 1, ('jason', 0.0): 1, (\"shackell'\", 0.0): 1, ('podcast', 0.0): 1, ('envi', 0.0): 1, ('twer', 0.0): 1, ('782', 0.0): 1, ('hahaahahahaha', 0.0): 1, ('sm1', 0.0): 1, ('mutil', 0.0): 1, ('robot', 0.0): 1, ('destroy', 0.0): 1, ('freakin', 0.0): 1, ('haestarr', 0.0): 1, ('😀', 0.0): 3, ('audio', 0.0): 1, ('snippet', 0.0): 1, ('brotherhood', 0.0): 1, ('mefd', 0.0): 1, ('diana', 0.0): 1, ('master', 0.0): 1, ('led', 0.0): 1, ('award', 0.0): 1, ('meowkd', 0.0): 1, ('complic', 0.0): 1, (\"c'mon\", 0.0): 1, (\"swimmer'\", 0.0): 1, ('leh', 0.0): 1, ('corner', 0.0): 1, ('didnot', 0.0): 1, ('usanel', 0.0): 2, ('nathan', 0.0): 1, ('micha', 0.0): 1, ('fave', 0.0): 2, ('creep', 0.0): 1, ('throughout', 0.0): 1, ('whose', 0.0): 1, ('ave', 0.0): 1, ('tripl', 0.0): 1, ('lectur', 0.0): 1, ('2-5', 0.0): 1, ('jaw', 0.0): 1, ('quarter', 0.0): 1, ('soni', 0.0): 1, ('followmeaaron', 0.0): 1, ('tzelumxoxo', 0.0): 1, ('drank', 0.0): 1, ('mew', 0.0): 1, ('indic', 0.0): 1, ('ouliv', 0.0): 1, ('70748', 0.0): 1, ('viernesderolenahot', 0.0): 1, ('longmorn', 0.0): 1, ('tobermori', 0.0): 1, ('32', 0.0): 1, ('tail', 0.0): 1, ('recuerda', 0.0): 1, ('tanto', 0.0): 1, ('bath', 0.0): 1, ('muna', 0.0): 1, ('await', 0.0): 1, ('urslef', 0.0): 1, ('lime', 0.0): 1, ('truckload', 0.0): 1, ('favour', 0.0): 2, ('spectat', 0.0): 1, ('sail', 0.0): 1, (\"w'end\", 0.0): 1, ('bbc', 0.0): 1, ('‘', 0.0): 1, ('foil', 0.0): 1, ('ac45', 0.0): 1, ('catamaran', 0.0): 1, ('peli', 0.0): 1, ('829', 0.0): 1, ('sextaatequemfimseguesdvcomvalentino', 0.0): 1, ('befor', 0.0): 1, ('valu', 0.0): 1, ('cinnamon', 0.0): 1, ('mtap', 0.0): 1, ('peng', 0.0): 1, ('frozen', 0.0): 1, ('bagu', 0.0): 1, ('emang', 0.0): 1, ('engg', 0.0): 1, ('cmc', 0.0): 1, ('mage', 0.0): 1, ('statement', 0.0): 1, ('moodsw', 0.0): 1, ('termin', 0.0): 1, ('men', 0.0): 1, ('peep', 0.0): 1, ('multipl', 0.0): 1, ('mef', 0.0): 1, ('rebound', 0.0): 1, ('pooor', 0.0): 1, ('2am', 0.0): 1, ('perpetu', 0.0): 1, ('bitchfac', 0.0): 1, ('clever', 0.0): 1, ('iceland', 0.0): 1, ('zayn_come_back_we_miss_y', 0.0): 1, ('pmsl', 0.0): 1, ('mianh', 0.0): 1, ('milkeu', 0.0): 1, ('lrt', 0.0): 1, ('bambam', 0.0): 1, ('soda', 0.0): 1, ('payback', 0.0): 1, ('87000', 0.0): 1, ('jobe', 0.0): 1, ('muchi', 0.0): 1, ('🎈', 0.0): 1, ('bathroom', 0.0): 1, ('lagg', 0.0): 1, ('banget', 0.0): 1, ('novel', 0.0): 1, (\"there'd\", 0.0): 1, ('invis', 0.0): 1, ('scuttl', 0.0): 1, ('worm', 0.0): 1, ('bauuukkk', 0.0): 1, ('jessica', 0.0): 1, ('5:15', 0.0): 1, ('argument', 0.0): 1, ('couldnt', 0.0): 2, ('yepp', 0.0): 1, ('😺', 0.0): 1, ('💒', 0.0): 1, ('💎', 0.0): 1, ('feelin', 0.0): 1, ('biscuit', 0.0): 1, ('slather', 0.0): 1, ('jsut', 0.0): 1, ('belov', 0.0): 1, ('grandmoth', 0.0): 1, ('princess', 0.0): 2, ('babee', 0.0): 1, ('demn', 0.0): 1, ('hotaisndonwyvauwjoqhsjsnaihsuswtf', 0.0): 1, ('sia', 0.0): 1, ('niram', 0.0): 1, ('geng', 0.0): 1, ('fikri', 0.0): 1, ('tirtagangga', 0.0): 1, ('char', 0.0): 1, ('font', 0.0): 2, ('riprishikeshwari', 0.0): 1, ('creamist', 0.0): 1, ('challeng', 0.0): 1, ('substitut', 0.0): 1, ('skin', 0.0): 1, ('cplt', 0.0): 1, ('cp', 0.0): 1, ('hannah', 0.0): 1, ('💙', 0.0): 1, ('💪', 0.0): 1, ('opu', 0.0): 1, ('inner', 0.0): 1, ('pleasur', 0.0): 1, ('bbq', 0.0): 1, ('lolliv', 0.0): 1, ('split', 0.0): 3, ('collat', 0.0): 2, ('spilt', 0.0): 2, ('quitkarwaoyaaro', 0.0): 1, ('deacti̇v', 0.0): 1, ('2.5', 0.0): 1, ('g2a', 0.0): 1, ('sherep', 0.0): 1, ('nemen', 0.0): 1, ('behey', 0.0): 1, ('motherfuck', 0.0): 1, ('tattoo', 0.0): 1, ('reec', 0.0): 1, ('vm', 0.0): 1, ('deth', 0.0): 2, ('lest', 0.0): 1, ('gp', 0.0): 1, ('departur', 0.0): 1, ('wipe', 0.0): 1, ('yuck', 0.0): 1, ('ystrday', 0.0): 1, ('seolhyun', 0.0): 1, ('drama', 0.0): 1, ('spici', 0.0): 1, ('owl', 0.0): 1, ('mumbai', 0.0): 1, (\"pj'\", 0.0): 1, ('wallpap', 0.0): 1, ('cba', 0.0): 1, ('hotter', 0.0): 1, ('rec', 0.0): 1, ('gotdamn', 0.0): 1, ('baaack', 0.0): 1, ('honest', 0.0): 1, ('srw', 0.0): 1, ('mobag', 0.0): 1, ('dunno', 0.0): 1, ('stroke', 0.0): 1, ('gnr', 0.0): 1, ('backstag', 0.0): 1, ('slash', 0.0): 1, ('prolli', 0.0): 1, ('bunni', 0.0): 1, ('sooner', 0.0): 1, ('analyst', 0.0): 1, ('expedia', 0.0): 1, ('bellevu', 0.0): 1, ('prison', 0.0): 1, ('alcohol', 0.0): 1, ('huhuh', 0.0): 1, ('heartburn', 0.0): 1, ('awalmu', 0.0): 1, ('njareeem', 0.0): 1, ('maggi', 0.0): 1, ('psycho', 0.0): 1, ('wahhh', 0.0): 1, ('abudhabi', 0.0): 1, ('hiby', 0.0): 1, ('shareyoursumm', 0.0): 1, ('b8', 0.0): 1, ('must.b', 0.0): 1, ('dairi', 0.0): 1, ('produxt', 0.0): 1, ('lactos', 0.0): 2, ('midland', 0.0): 1, ('knacker', 0.0): 1, ('footag', 0.0): 1, ('lifeless', 0.0): 1, ('shell', 0.0): 1, ('44', 0.0): 1, ('7782', 0.0): 1, ('pengen', 0.0): 1, ('girlll', 0.0): 1, ('tsunami', 0.0): 1, ('indi', 0.0): 1, ('nick', 0.0): 1, ('tirad', 0.0): 1, ('stoop', 0.0): 1, ('lower', 0.0): 1, ('role', 0.0): 1, ('thunder', 0.0): 1, ('paradis', 0.0): 1, ('habit', 0.0): 1, ('facad', 0.0): 1, ('democraci', 0.0): 1, ('brat', 0.0): 1, ('tb', 0.0): 1, (\"o'\", 0.0): 1, ('bade', 0.0): 1, ('fursat', 0.0): 1, ('usey', 0.0): 2, ('banaya', 0.0): 1, ('uppar', 0.0): 1, ('waal', 0.0): 1, ('ney', 0.0): 1, ('afso', 0.0): 1, ('hums', 0.0): 1, ('dur', 0.0): 1, ('wo', 0.0): 1, (\"who'd\", 0.0): 1, ('naruhina', 0.0): 1, ('namee', 0.0): 1, ('haiqal', 0.0): 1, ('360hr', 0.0): 1, ('picc', 0.0): 1, ('instor', 0.0): 1, ('pre-vot', 0.0): 1, ('5th', 0.0): 1, ('usernam', 0.0): 1, ('minho', 0.0): 1, ('durian', 0.0): 1, ('strudel', 0.0): 1, ('tsk', 0.0): 1, ('marin', 0.0): 1, ('kailan', 0.0): 1, ('separ', 0.0): 1, ('payday', 0.0): 1, ('payhour', 0.0): 1, ('immedi', 0.0): 1, ('natur', 0.0): 1, ('pre-ord', 0.0): 1, ('fwm', 0.0): 1, ('guppi', 0.0): 1, ('poorkid', 0.0): 1, ('lack', 0.0): 1, ('misunderstood', 0.0): 1, ('cuddli', 0.0): 1, ('scratch', 0.0): 1, ('thumb', 0.0): 1, ('compens', 0.0): 1, ('kirkiri', 0.0): 1, ('phase', 0.0): 1, ('wonho', 0.0): 1, ('visual', 0.0): 1, (\"='(\", 0.0): 1, ('mission', 0.0): 1, ('pap', 0.0): 1, ('danzel', 0.0): 1, ('craft', 0.0): 1, ('devil', 0.0): 1, ('phil', 0.0): 1, ('sheff', 0.0): 1, ('york', 0.0): 1, ('visa', 0.0): 1, ('gim', 0.0): 1, ('bench', 0.0): 1, ('harm', 0.0): 1, ('yolo', 0.0): 1, ('bloat', 0.0): 1, ('olli', 0.0): 1, ('alterni', 0.0): 1, ('earth', 0.0): 1, ('influenc', 0.0): 1, ('overal', 0.0): 1, ('continent', 0.0): 1, ('🔫', 0.0): 1, ('tank', 0.0): 1, ('thirsti', 0.0): 1, ('konami', 0.0): 1, ('polici', 0.0): 1, ('ranti', 0.0): 1, ('atm', 0.0): 1, ('pervers', 0.0): 1, ('bylfnnz', 0.0): 1, ('ban', 0.0): 1, ('failsatlif', 0.0): 1, ('press', 0.0): 1, ('duper', 0.0): 1, ('waaah', 0.0): 1, ('jaebum', 0.0): 1, ('ahmad', 0.0): 1, ('maslan', 0.0): 1, ('hull', 0.0): 1, ('misser', 0.0): 1}\n\n\nUnfortunately, this does not help much to understand the data. It would be better to visualize this output to gain better insights."
  },
  {
    "objectID": "posts/c1w1/lab02.html#table-of-word-counts",
    "href": "posts/c1w1/lab02.html#table-of-word-counts",
    "title": "Building and Visualizing word frequencies",
    "section": "Table of word counts",
    "text": "Table of word counts\nWe will select a set of words that we would like to visualize. It is better to store this temporary information in a table that is very easy to use later.\n\n# select some words to appear in the report. we will assume that each word is unique (i.e. no duplicates)\nkeys = ['happi', 'merri', 'nice', 'good', 'bad', 'sad', 'mad', 'best', 'pretti',\n        '❤', ':)', ':(', '😒', '😬', '😄', '😍', '♛',\n        'song', 'idea', 'power', 'play', 'magnific']\n\n# list representing our table of word counts.\n# each element consist of a sublist with this pattern: [&lt;word&gt;, &lt;positive_count&gt;, &lt;negative_count&gt;]\ndata = []\n\n# loop through our selected words\nfor word in keys:\n    \n    # initialize positive and negative counts\n    pos = 0\n    neg = 0\n    \n    # retrieve number of positive counts\n    if (word, 1) in freqs:\n        pos = freqs[(word, 1)]\n        \n    # retrieve number of negative counts\n    if (word, 0) in freqs:\n        neg = freqs[(word, 0)]\n        \n    # append the word counts to the table\n    data.append([word, pos, neg])\n    \ndata\n\n[['happi', 211, 25],\n ['merri', 1, 0],\n ['nice', 98, 19],\n ['good', 238, 101],\n ['bad', 18, 73],\n ['sad', 5, 123],\n ['mad', 4, 11],\n ['best', 65, 22],\n ['pretti', 20, 15],\n ['❤', 29, 21],\n [':)', 3568, 2],\n [':(', 1, 4571],\n ['😒', 1, 3],\n ['😬', 0, 2],\n ['😄', 5, 1],\n ['😍', 2, 1],\n ['♛', 0, 210],\n ['song', 22, 27],\n ['idea', 26, 10],\n ['power', 7, 6],\n ['play', 46, 48],\n ['magnific', 2, 0]]\n\n\nWe can then use a scatter plot to inspect this table visually. Instead of plotting the raw counts, we will plot it in the logarithmic scale to take into account the wide discrepancies between the raw counts (e.g. :) has 3568 counts in the positive while only 2 in the negative). The red line marks the boundary between positive and negative areas. Words close to the red line can be classified as neutral.\n\nfig, ax = plt.subplots(figsize = (8, 8))\n\n# convert positive raw counts to logarithmic scale. we add 1 to avoid log(0)\nx = np.log([x[1] + 1 for x in data])  \n\n# do the same for the negative counts\ny = np.log([x[2] + 1 for x in data]) \n\n# Plot a dot for each pair of words\nax.scatter(x, y)  \n\n# assign axis labels\nplt.xlabel(\"Log Positive count\")\nplt.ylabel(\"Log Negative count\")\n\n# Add the word as the label at the same position as you added the points just before\nfor i in range(0, len(data)):\n    ax.annotate(data[i][0], (x[i], y[i]), fontsize=12)\n\nax.plot([0, 9], [0, 9], color = 'red') # Plot the red line that divides the 2 areas.\nplt.show()\n\nText(0.5, 0, 'Log Positive count')\n\n\nText(0, 0.5, 'Log Negative count')\n\n\nText(5.356586274672012, 3.258096538021482, 'happi')\n\n\nText(0.6931471805599453, 0.0, 'merri')\n\n\nText(4.59511985013459, 2.995732273553991, 'nice')\n\n\nText(5.476463551931511, 4.624972813284271, 'good')\n\n\nText(2.9444389791664403, 4.30406509320417, 'bad')\n\n\nText(1.791759469228055, 4.820281565605037, 'sad')\n\n\nText(1.6094379124341003, 2.4849066497880004, 'mad')\n\n\nText(4.189654742026425, 3.1354942159291497, 'best')\n\n\nText(3.044522437723423, 2.772588722239781, 'pretti')\n\n\nText(3.4011973816621555, 3.091042453358316, '❤')\n\n\nText(8.18004072349016, 1.0986122886681098, ':)')\n\n\nText(0.6931471805599453, 8.427706024914702, ':(')\n\n\nText(0.6931471805599453, 1.3862943611198906, '😒')\n\n\nText(0.0, 1.0986122886681098, '😬')\n\n\nText(1.791759469228055, 0.6931471805599453, '😄')\n\n\nText(1.0986122886681098, 0.6931471805599453, '😍')\n\n\nText(0.0, 5.351858133476067, '♛')\n\n\nText(3.1354942159291497, 3.332204510175204, 'song')\n\n\nText(3.295836866004329, 2.3978952727983707, 'idea')\n\n\nText(2.0794415416798357, 1.9459101490553132, 'power')\n\n\nText(3.8501476017100584, 3.8918202981106265, 'play')\n\n\nText(1.0986122886681098, 0.0, 'magnific')\n\n\n/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 128556 (\\N{GRIMACING FACE}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n\n\n\n\n\n\n\n\n\nThis chart is straightforward to interpret. It shows that emoticons :) and :( are very important for sentiment analysis. Thus, we should not let preprocessing steps get rid of these symbols!\nFurthermore, what is the meaning of the crown symbol? It seems to be very negative!\n\nThat’s all for this lab! We’ve seen how to build a word frequency dictionary and this will come in handy when extracting the features of a list of tweets. Next up, we will be reviewing Logistic Regression. Keep it up!"
  }
]