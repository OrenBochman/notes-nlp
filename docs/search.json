[
  {
    "objectID": "posts/c1w1/index.html",
    "href": "posts/c1w1/index.html",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nThis week‚Äôs slides\n\n\n\n\nFigure¬†1\nMy notes for Week 1 of the Natural Language Processing with Probabilistic Models Course in the Natural Language Processing Specialization Offered by DeepLearning.AI on Coursera",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c1w1/index.html#sec-supervised-ml-sentiment-analysis",
    "href": "posts/c1w1/index.html#sec-supervised-ml-sentiment-analysis",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "Supervised ML & Sentiment Analysis",
    "text": "Supervised ML & Sentiment Analysis\n\n\n\n\n\n\n\nFigure¬†3: classification overview\n\n\n\nIn supervised ML we get a dataframe with input features X and their corresponding ground truth label Y.\nThe goal is to minimize prediction error rates, AKA cost.\nTo do this, one runs the prediction function which takes in parameters and map the features of an input to an output label \\hat{Y}.\nThe optimal mapping from features to labels is when the difference between the expected values Y and the predicted values \\hat{Y} is minimized.\nThe cost function F does this by comparing how closely the output \\hat{Y} is to the label Y.\nUpdate the parameters and repeat the whole process until the cost is minimized.\nWe use the Sigmoid cost function:\n\n\n\n\n\n\n\n\nFigure¬†4: the Sigmoid cost function",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c1w1/index.html#sec-sentiment-analysis",
    "href": "posts/c1w1/index.html#sec-sentiment-analysis",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\n\n\n\n\n\n\n\nFigure¬†5: Sentiment Analysis\n\n\n\n\n\n\n\n\nSentiment Analysis - Motivation\n\n\n\n\n\nIf we are passionate about NLP here is a gem to get we started. This is a popular science with many ideas for additional classifiers using pronouns.\n\n\n\n\n\n\n\n\n\n\n\nVideo¬†1: The Secret Life of Pronouns: James Pennebaker at TEDxAustin\n\n\n\n\n\n\n\n\nVideo¬†2: Language of Truth and Lies: I-words\n\n\n\n\n\n\n\n\nVideo¬†3: LIWC-22 2022 Tutorial 1: Getting started with LIWC-22\n\n\n\n\n\n\n\n\nFigure¬†6: classification overview\n\n\n\n\n\n\nOne example of a Supervised machine learning classification task for sentiment analysis\nThe objective is to predict whether a tweet has a positive or negative sentiment. (If it is positive/optimistic or negative/pessimistic).\n\nTo perform sentiment analysis on a tweet, we need to:\n\nrepresent the text for example ‚ÄúI am happy because I am learning NLP‚Äù as features,\ntrain a logistic regression classifier\n\n\n1 for a positive sentiment\n0 for negative sentiment.\n\n\nand then we use it to classify the text.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c1w1/index.html#sec-vocabulary-feature-extraction",
    "href": "posts/c1w1/index.html#sec-vocabulary-feature-extraction",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "Vocabulary & Feature Extraction",
    "text": "Vocabulary & Feature Extraction\n\nSparse Representation\n\n\n\n\nProblems with sparse representation\n\nGiven a tweet, or some text, one can represent it as a vector. The vector has a dimension‚ÄØ|V|, where‚ÄØV‚ÄØcorresponds to the size of the vocabulary size. If we had the tweet ‚ÄúI am happy because I am learning NLP,‚Äù then we would put a 1 in the corresponding index for any word in the tweet, and a 0 otherwise.\n\n\n\n\n\n\n\nFigure¬†7: A sparse representation\n\n\nAs‚ÄØV‚ÄØgets larger, the vector becomes more sparse. Furthermore, we end up having many more features and end up training‚ÄØ‚ÄØ\\theta_0 \\ldots \\theta_n ‚ÄØparameters. This results in a larger training time. And the inference time increases as well.\n\n\nFeature Extraction based on class frequencies\n\n\n\nTable¬†1: Table of tweets\n\n\n\n\n\n\n\n\n\nPositive tweets\nNegative tweets\n\n\n\n\nI am happy because I am learning NLP\nI am sad, I am not learning NLP\n\n\nI am happy\nI am sad\n\n\n\n\n\n\nGiven a corpus with positive and negative tweets we can represent it as follows:\n\n\n\nTable¬†2: Word class table\n\n\n\n\n\nVocabulary\nPosFreq (1)\nNegFreq (O)\n\n\n\n\nI\n3\n3\n\n\nam\n3\n3\n\n\nhappy\n2\n0\n\n\nbecause\n1\n0\n\n\nlearning\n1\n1\n\n\nNLP\n1\n1\n\n\nsad\n0\n2\n\n\nnot\n0\n1\n\n\n\n\n\n\nfreqs: dictionary mapping from (word, class) to frequency :\n\n\\underbrace{X_m}_{\\textcolor{#7200ac}{\\text{Features of tweet M}}} =\\left[ \\underbrace{1}_{\\textcolor{#126ed5}{\\text{bias}}}\n,\\sum_w \\underbrace{\\textcolor{#da7801}{freqs}(w,\\textcolor{#2db15d}{1})}_{\\textcolor{#931e18}{\\text{Sum Pos.freqs}}} ,\\sum_w \\underbrace{\\textcolor{#da7801}{frequencies}(w,\\textcolor{#931e18}{0})}_{\\textcolor{#2db15d}{\\text{Sum Neg. frequencies}}}\n\\right]\n\\tag{1}\nwe have to encode each tweet as a vector. Previously, this vector was of dimension‚ÄØVV. Now, as we‚Äôll see in the upcoming videos, we‚Äôll represent it with a vector of dimension‚ÄØ33. We create a dictionary to map the word, it class, either positive or negative, to the number of times that word appeared in its corresponding class.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c1w1/index.html#sec-preprocessing",
    "href": "posts/c1w1/index.html#sec-preprocessing",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "Preprocessing",
    "text": "Preprocessing\n\n\n\n\n\n\n\nFigure¬†8: preprocessing - feature extraction\n\n\n\n\n\n\n\n\nFigure¬†9: preprocessing - stemming\n\n\n\nWhen preprocessing, we have to perform the following:\n\nEliminate handles and URLs\nTokenize the string into words.\nRemove stop words like ‚Äúand, is, a, on, etc.‚Äù\nStemming - converting each word to its stem. For example dancer, dancing, danced, becomes ‚Äòdanc‚Äô. We can use Porter‚Äôs Stemmer to take care of this.\nConvert all our words to lower case.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c1w1/index.html#sec-logistic-regression-intro",
    "href": "posts/c1w1/index.html#sec-logistic-regression-intro",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "Intro to Logistic Regression",
    "text": "Intro to Logistic Regression\n\n\n\n\n\n\nFlatten learning curve üìà with StatQuest\n\n\n\n\n\nIf we want to flatten our learning curve consider the following videos which will help us get more confident with logistic regression by building from the more familiar OLS regression. Here are three videos from StatQuest\n\n\n\n\nVideo¬†4\nVideo¬†5\nVideo¬†6\n\n\n\n\n\n\n\n\n\nVideo¬†4: StatQuest: Logistic Regression\n\n\n\n\n\n\n\n\nVideo¬†5: StatQuest: Logistic Regression Details Pt1: Coefficients\n\n\n\n\n\n\n\n\nVideo¬†6: StatQuest: Logistic Regression Details Pt 2: Maximum Likelihood",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c1w1/index.html#sec-training-logistic-regression",
    "href": "posts/c1w1/index.html#sec-training-logistic-regression",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "Training Logistic Regression",
    "text": "Training Logistic Regression\n\n\n\n\n\n\n\nFigure¬†10: Training algorithm - flow chart\n\n\nTo train our logistic regression function, we‚Äôll do the following: we initialize our parameter ‚ÄØ\\theta , that we can use in we Sigmoid, we then compute the gradient that we‚Äôll use to update‚ÄØ\\theta, and then calculate the cost. we keep doing so until good enough\n\n\n\n\n\n\n\nFigure¬†11: Training\n\n\nUsually we keep training until the cost converges. If we were to plot the number of iterations versus the cost, we should see something like this:",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c1w1/index.html#sec-testing-logistic-regression",
    "href": "posts/c1w1/index.html#sec-testing-logistic-regression",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "Testing Logistic Regression",
    "text": "Testing Logistic Regression\n\n\n\n\n\n\n\nFigure¬†12: Testing logistic regression\n\n\nTo test our model, we would run a subset of our data, known as the validation set, on our model to get predictions. The predictions are the outputs of the Sigmoid function.\nIf the output is‚ÄØ‚â• 0.5, we would assign it to a positive class, otherwise to the negative class.\nTo compute accuracy, we solve the following equation:\n\n\\text{accuracy} = \\sum_i \\frac{\\hat{y}^{(i)}= y^{(i)}_{val}}{m}\n\\tag{2}\nwhere:\nCross‚ÄØvalidation note:\n\nIn reality, given your‚ÄØX‚ÄØdata we would usually split it into three components.‚ÄØX_{train}, X_{val}, X_{test}.\nThe distribution usually varies depending on the size of our data set. However, a‚ÄØ80%, 10%, 10%‚ÄØsplit usually works.\n\nIn other words, we go over all our training examples,‚ÄØm‚ÄØof them, and then for every prediction, if it was wright we add a one. we then divide by‚ÄØm.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c1w1/index.html#sec-logistic-regression-cost-function",
    "href": "posts/c1w1/index.html#sec-logistic-regression-cost-function",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "Logistic Regression: Cost Function",
    "text": "Logistic Regression: Cost Function\n\n\n\n\n\n\n\nFigure¬†13: Cost function for logistic regression\n\n\n\n\n\n\n\n\nSigmoid\n\n\n\nWe should start by developing intuition about how the cost function is designed the way it is.\nThis is important because we‚Äôll meet the sigmoid in Neural Networks, job interviews and so best make a friend of it.\nIn (Hinton 2012) there is the full derivation of Sigmoid cost function. Also the sigmoid and Logistic regression are intimately related - we can‚Äôt have one without the other.\n\n\nIn plain english: ‚ÄúThe cost function is mean log loss across all training examples‚Äù\n\nJ(\\theta) = ‚àí\\frac{1}{m} \\sum^m_{i=1}[y^{(i)}\\log h(x^{(i)}, \\theta)+(1 ‚àíy^{(i)}) \\log (1‚àíh(x^{(i)}, \\theta))]\n\\tag{3}\nwhere:\n\nm is the count of rows of our training set.\ni indexes a single row in the dataset.\nx^{(i)} is the data for row i.\ny^{(i)} is the ground truth AKA label for rows i.\nh(x^{(i)},\\theta) is the model‚Äôs prediction for row i.\n\nWe‚Äôll derive the logistic regression cost function to get the gradients.\nwe can see in the figure:\n\nIf‚ÄØy = 1‚ÄØand our prediction is close to‚ÄØ0, we get a cost close to‚ÄØ ‚àû.\nThe same applies when y=0 and we predict ion is close to‚ÄØ1.\nOn the other hand if we get a prediction equal to the label, we get a cost of 0.\n\nIn either, case we are trying to minimize ‚ÄØJ(\\theta)\n\n\nMathematical Derivation\nTo see why the cost function is designed that way, let‚Äôs take a step back and write up a function that compresses the two cases into one case.\nIf\n\nP(y \\mid x(i), \\theta) =h(x^{(i)}, \\theta)^{y^{(i)}}1‚àíh(x^{(i)}, \\theta)^{1‚àíy^{(i)}}\n\\tag{4}\nThen the likelihood of the data set is given by:\nFrom the preceding, we can see that when‚ÄØy = 1, we get h(x^{(i)}, \\theta)^{y^{(i)}} and when y‚âà0 the term 1 ‚àí h(x^{(i)}, \\theta)^{(1‚àíy^{(i)})}, which makes sense, since the two probabilities equal to 1.\nIn either case, we want to maximize the function h(x^{(i)}, \\theta)^{y(i)} by making it as close to 1‚ÄØas possible.\nWhen‚ÄØy ‚âà 0 , we want‚ÄØthe term 1-h(x^{(i)}, \\theta)^{1‚àíy^{(i)}} ‚âà 0 which then \\implies  h(x^{(i)}, \\theta)^{y^{(i)}} ‚âà 1\nWhen‚ÄØy=1, we want h(x^{(i)}, \\theta)^{y^{(i)}} = 1\nNow we want to find a way to model the entire data set and not just one example. To do so, we‚Äôll define the likelihood as follows:\n\nL(\\theta) = \\prod^m_{i=1} h(\\theta, x^{(i)})^{y^{(i)}} (1‚àíh(\\theta, x^{(i)}))^{(1‚àíy^{(i)})}\n\\tag{5}\nNote that if we mess up the classification of one example, we end up messing up the overall likelihood score, which is exactly what we intended. We want to fit a model to the entire dataset where all data points are related. to\n\n\\lim_{m \\to \\infty} L(\\theta) = 0\n\\tag{6}\nIt goes close to zero, because both h(\\theta, x^{(i)})^{y^{(i)}} and (1‚àíh(\\theta, x^{(i)}))^{(1‚àíy^{(i)})} ‚ÄØare bounded by [0,1]. Since we are trying to maximize‚ÄØ h(\\theta, x^{(i)})‚ÄØin‚ÄØL(\\theta), we can introduce the log and just maximize the log of the function.\nIntroducing the‚ÄØlog, allows us to write the‚ÄØlog‚ÄØof a product as the sum of each‚ÄØlog. Here are two identities that will come in handy:\n\n  \\log(a*b*c) = \\log(a) + \\log(b) + \\log(c)  \n\nand\n\n  \\log(a^b) = b \\times \\log(a)\n\nGiven the two preceding identities, we can rewrite the equation as follows:   \n  \\begin{align*}  \n    \\max_{ h(x^{(i)},\\theta)}\\log L(\\theta) &= \\log \\prod^m_{i=1}h(x^{(i)}, \\theta)^{y^{(i)}}(1‚àíh(x^{(i)} ,\\theta))^{1‚àíy^{(i)}} \\\\\n    &= \\sum^m_{i=1} \\log h(x^{(i)}, \\theta)^{y^{(i)}} (1‚àíh(x^{(i)}, \\theta)^{1‚àíy^{(i)}})            \\\\\n    &= \\sum^m_{i=1} \\log h(x^{(i)}, \\theta)^{y^{(i)}} + \\log(1‚àíh(x^{(i)}, \\theta)^{1‚àíy^{(i)}} \\\\\n    &= \\sum^m_{i=1} y^{(i)}\\log h(x^{(i)}, \\theta) + (1‚àíy^{(i)}) \\log(1‚àíh(x^{(i)}, \\theta))\n  \\end{align*}\n Hence, we now divide by‚ÄØm, because we want to see the average cost.   \n  \\begin{align*}  \n    \\frac{1}{m} \\sum^m_{i=1} y^{(i)}\\log h(x^{(i)}, \\theta) + (1‚àíy^{(i)}) \\log(1‚àíh(x^{(i)}, \\theta))  \n  \\end{align*}\n\nRemember that we were maximizing‚ÄØh(\\theta, x(i)) ‚ÄØin the preceding equation. It turns out that maximizing an equation is the same as minimizing its negative. Think of‚ÄØx^2, feel free to plot it to see that for we yourself. Hence we add a negative sign and we end up minimizing the cost function as follows.\n  \n  \\begin{align*}  \n    J(\\theta)= ‚àí \\frac{1}{m} \\sum^m_{i=1} [y^{(i)} \\log h(x^{(i)}, \\theta) + ( 1 ‚àí y^{(i)}) \\log ( 1 ‚àí h(x^{(i)}, \\theta))]  \n  \\end{align*}\n\nA vectorized implementation is:\n\n\\begin{align*} & h = g(X\\theta)\\newline & J(\\theta)  = \\frac{1}{m} \\cdot \\left(-y^{T}\\log(h)-(1-y)^{T}\\log(1-h)\\right) \\end{align*}",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c1w1/index.html#logistic-regression-gradient",
    "href": "posts/c1w1/index.html#logistic-regression-gradient",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "Logistic Regression: Gradient",
    "text": "Logistic Regression: Gradient\n\n\\mathbf{w}(t+1) = \\mathbf{w}(t) - \\eta\\nabla E_{in}\n\nFor the case of logistic regression, the gradient of the error measure with respect to the weights, is calculated as:\n\n\\nabla E_{in}\\left(\\mathbf{w}\\right) = -\\frac{1}{N}\\sum\\limits_{n=1}^N \\frac{y_n\\mathbf{x_N}}{1 + \\exp\\left(y_n \\mathbf{w^T}(t)\\mathbf{x_n}\\right)}\n\nLet‚Äôs look into the gradient descent in more detail, as the gradient update rule is given without an explicitly derivation.\nThe general form Of gradient descent is defined\n\n  \\begin{align*} &\n    Repeat \\; \\lbrace \\newline & \\; \\theta_j := \\theta_j - \\alpha \\dfrac{\\partial}{\\partial \\theta_j}J(\\theta) \\newline & \\rbrace\n  \\end{align*}\n\nWe can work out the derivative part using Calculus to get:\n\n  \\begin{align*} &\n    Repeat \\; \\lbrace  \\theta_j := \\theta_j - \\frac{\\alpha}{m} \\sum_{i=1}^m ( h(x^{(i)}, \\theta) - y^{(i)}) x_j^{(i)} \\rbrace\n  \\end{align*}\n\nA vectorized formulation \n\\theta_j := \\theta_j - \\frac{\\alpha}{m} X^T ( H(X, \\theta) -Y)\n\n\nPartial derivative of J(\\theta)\n\n\\begin{align*}\n  h(x)'&=\\left(\\frac{1}{1+e^{-x}}\\right)'=\\frac{-(1+e^{-x})'}{(1+e^{-x})^2}=\\frac{-1'-(e^{-x})'}{(1+e^{-x})^2}=\\frac{0-(-x)'(e^{-x})}{(1+e^{-x})^2}=\\frac{-(-1)(e^{-x})}{(1+e^{-x})^2}=\\frac{e^{-x}}{(1+e^{-x})^2} \\newline &=\\left(\\frac{1}{1+e^{-x}}\\right)\\left(\\frac{e^{-x}}{1+e^{-x}}\\right)=h(x)\\left(\\frac{+1-1 + e^{-x}}{1+e^{-x}}\\right)=h(x)\\left(\\frac{1 + e^{-x}}{1+e^{-x}} - \\frac{1}{1+e^{-x}}\\right)=h(x)(1 - h(x))\n\\end{align*}\n\n\n\\begin{align*}\n\\frac{\\partial}{\\partial \\theta_j} J(\\theta) &= \\frac{\\partial}{\\partial \\theta_j} \\frac{-1}{m}\\sum_{i=1}^m \\left [ y^{(i)} log ( h(x^{(i)}, \\theta) ) + (1-y^{(i)}) log (1 -  h(x^{(i)}, \\theta)) \\right ] \\newline\n&= - \\frac{1}{m}\\sum_{i=1}^m \\left [     y^{(i)} \\frac{\\partial}{\\partial \\theta_j} log ( h(x^{(i)}, \\theta))   + (1-y^{(i)}) \\frac{\\partial}{\\partial \\theta_j} log (1 -  h(x^{(i)}, \\theta))\\right ] \\newline\n&= - \\frac{1}{m}\\sum_{i=1}^m \\left [ \\frac{y^{(i)} \\frac{\\partial}{\\partial \\theta_j}  h(x^{(i)}, \\theta)}{ h(x^{(i)}, \\theta)} + \\frac{(1-y^{(i)})\\frac{\\partial}{\\partial \\theta_j} (1 -  h(x^{(i)}, \\theta))}{1 -  h(x^{(i)}, \\theta)}\\right ] \\newline\n&= - \\frac{1}{m}\\sum_{i=1}^m \\left [ \\frac{y^{(i)} \\frac{\\partial}{\\partial \\theta_j}  h(x^{(i)}, \\theta)}{ h(x^{(i)}, \\theta)} + \\frac{(1-y^{(i)})\\frac{\\partial}{\\partial \\theta_j} (1 -  h(x^{(i)}, \\theta))}{1 -  h(x^{(i)}, \\theta)}\\right ] \\newline\n&= - \\frac{1}{m}\\sum_{i=1}^m \\left [ \\frac{y^{(i)}  h(x^{(i)}, \\theta) (1 -  h(x^{(i)}, \\theta)) \\frac{\\partial}{\\partial \\theta_j} \\theta^T x^{(i)}}{ h(x^{(i)}, \\theta)}   + \\frac{- (1-y^{(i)})  h(x^{(i)}, \\theta)(1 -  h(x^{(i)}, \\theta)) \\frac{\\partial}{\\partial \\theta_j} \\theta^T x^{(i)}}{1 -  h(x^{(i)}, \\theta)}\\right ] \\newline\n&= - \\frac{1}{m}\\sum_{i=1}^m \\left [ \\frac{y^{(i)}  h(x^{(i)}, \\theta) (1 -  h(x^{(i)}, \\theta)) \\frac{\\partial}{\\partial \\theta_j} \\theta^T x^{(i)}}{ h(x^{(i)}, \\theta)} - \\frac{(1-y^{(i)}) h(x^{(i)}, \\theta) (1 -  h(x^{(i)}, \\theta)) \\frac{\\partial}{\\partial \\theta_j} \\theta^T x^{(i)}}{1 -  h(x^{(i)}, \\theta)}\\right ] \\newline\n&= - \\frac{1}{m}\\sum_{i=1}^m \\left [ y^{(i)} (1 -  h(x^{(i)}, \\theta)) x^{(i)}_j - (1-y^{(i)})  h(x^{(i)}, \\theta) x^{(i)}_j\\right ] \\newline\n&= - \\frac{1}{m}\\sum_{i=1}^m \\left [ y^{(i)} (1 -  h(x^{(i)}, \\theta)) - (1-y^{(i)})  h(x^{(i)}, \\theta) \\right ] x^{(i)}_j \\newline\n&= - \\frac{1}{m}\\sum_{i=1}^m \\left [ y^{(i)} - y^{(i)}  h(x^{(i)}, \\theta) -  h(x^{(i)}, \\theta) + y^{(i)}  h(x^{(i)}, \\theta) \\right ] x^{(i)}_j \\newline\n&= - \\frac{1}{m}\\sum_{i=1}^m \\left [ y^{(i)} -  h(x^{(i)}, \\theta) \\right ] x^{(i)}_j \\newline\n&= \\frac{1}{m}\\sum_{i=1}^m \\left [ h(x^{(i)}, \\theta) - y^{(i)} \\right ] x^{(i)}_j\n\\end{align*}\n\\tag{7}\nFirst calculate derivative Of Sigmoid function (it be useful while finding partial derivative Of Note that we computed the partial derivative Of the Sigmoid function If We Were to derive , 9) with respect to O_j, we would get ‚Äî\nNote that used the chain rule there. We multiply by the derivative Of with respect to Now we are ready to find out resulting partial derivative\nThe Vectorized Version:\n\n\\nabla J(\\theta) = \\frac{1}{m} X^T \\cdot (H(X,\\theta)-Y)\n\\tag{8}\nCongratulations. we now know the full derivation Of logistic regression.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c1w1/index.html#resources",
    "href": "posts/c1w1/index.html#resources",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "Resources:",
    "text": "Resources:\n\nDerivative of cost function for Logistic Regression as explained on Math Stack Exchange\nAn Intuitive Explanation of Bayes‚Äô Theorem on Better Explained\n(Chadha 2020) Aman Chadha‚Äôs Notes\nIbrahim Jelliti‚Äôs Notes",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c1w1/lab01.html",
    "href": "posts/c1w1/lab01.html",
    "title": "Preprocessing",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure¬†1\nIn this lab, we will be exploring how to preprocess tweets for sentiment analysis. We will provide a function for preprocessing tweets during this week‚Äôs assignment, but it is still good to know what is going on under the hood. By the end of this lecture, you will see how to use the NLTK package to perform a preprocessing pipeline for Twitter datasets.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "L1 Preprocessing"
    ]
  },
  {
    "objectID": "posts/c1w1/lab01.html#setup",
    "href": "posts/c1w1/lab01.html#setup",
    "title": "Preprocessing",
    "section": "Setup",
    "text": "Setup\nYou will be doing sentiment analysis on tweets in the first two weeks of this course. To help with that, we will be using the Natural Language Toolkit (NLTK) package, an open-source Python library for natural language processing. It has modules for collecting, handling, and processing Twitter data, and you will be acquainted with them as we move along the course.\nFor this exercise, we will use a Twitter dataset that comes with NLTK. This dataset has been manually annotated and serves to establish baselines for models quickly. Let us import them now as well as a few other libraries we will be using.\n\nimport nltk                                # Python library for NLP\nfrom nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\nimport matplotlib.pyplot as plt            # library for visualization\nimport random                              # pseudo-random number generator",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "L1 Preprocessing"
    ]
  },
  {
    "objectID": "posts/c1w1/lab01.html#about-the-twitter-dataset",
    "href": "posts/c1w1/lab01.html#about-the-twitter-dataset",
    "title": "Preprocessing",
    "section": "About the Twitter dataset",
    "text": "About the Twitter dataset\nThe sample dataset from NLTK is separated into positive and negative tweets. It contains 5000 positive tweets and 5000 negative tweets exactly. The exact match between these classes is not a coincidence. The intention is to have a balanced dataset. That does not reflect the real distributions of positive and negative classes in live Twitter streams. It is just because balanced datasets simplify the design of most computational methods that are required for sentiment analysis. However, it is better to be aware that this balance of classes is artificial.\nThe dataset is already downloaded in the Coursera workspace. In a local computer however, you can download the data by doing:\n\n# downloads sample twitter dataset. uncomment the line below if running on a local machine.\n# nltk.download('twitter_samples')\n\nWe can load the text fields of the positive and negative tweets by using the module‚Äôs strings() method like this:\n\n# select the set of positive and negative tweets\nall_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')\n\nNext, we‚Äôll print a report with the number of positive and negative tweets. It is also essential to know the data structure of the datasets\n\nprint('Number of positive tweets: ', len(all_positive_tweets))\nprint('Number of negative tweets: ', len(all_negative_tweets))\n\nprint('\\nThe type of all_positive_tweets is: ', type(all_positive_tweets))\nprint('The type of a tweet entry is: ', type(all_negative_tweets[0]))\n\nNumber of positive tweets:  5000\nNumber of negative tweets:  5000\n\nThe type of all_positive_tweets is:  &lt;class 'list'&gt;\nThe type of a tweet entry is:  &lt;class 'str'&gt;\n\n\nWe can see that the data is stored in a list and as you might expect, individual tweets are stored as strings.\nYou can make a more visually appealing report by using Matplotlib‚Äôs pyplot library. Let us see how to create a pie chart to show the same information as above. This simple snippet will serve you in future visualizations of this kind of data.\n\n# Declare a figure with a custom size\nfig = plt.figure(figsize=(5, 5))\n\n# labels for the two classes\nlabels = 'Positives', 'Negative'\n\n# Sizes for each slide\nsizes = [len(all_positive_tweets), len(all_negative_tweets)] \n\n# Declare pie chart, where the slices will be ordered and plotted counter-clockwise:\nplt.pie(sizes, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\n\n# Equal aspect ratio ensures that pie is drawn as a circle.\nplt.axis('equal')  \n\n# Display the chart\nplt.show()\n\n([&lt;matplotlib.patches.Wedge at 0x758640264910&gt;,\n  &lt;matplotlib.patches.Wedge at 0x758640265630&gt;],\n [Text(-1.0999999999999959, -9.616505800409723e-08, 'Positives'),\n  Text(1.0999999999999832, 1.9233011600819372e-07, 'Negative')],\n [Text(-0.5999999999999978, -5.2453668002234845e-08, '50.0%'),\n  Text(0.5999999999999908, 1.0490733600446929e-07, '50.0%')])\n\n\n(np.float64(-1.100000000000005),\n np.float64(1.100000000000106),\n np.float64(-1.1),\n np.float64(1.1))",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "L1 Preprocessing"
    ]
  },
  {
    "objectID": "posts/c1w1/lab01.html#looking-at-raw-texts",
    "href": "posts/c1w1/lab01.html#looking-at-raw-texts",
    "title": "Preprocessing",
    "section": "Looking at raw texts",
    "text": "Looking at raw texts\nBefore anything else, we can print a couple of tweets from the dataset to see how they look. Understanding the data is responsible for 80% of the success or failure in data science projects. We can use this time to observe aspects we‚Äôd like to consider when preprocessing our data.\nBelow, you will print one random positive and one random negative tweet. We have added a color mark at the beginning of the string to further distinguish the two. (Warning: This is taken from a public dataset of real tweets and a very small portion has explicit content.)\n\n# print positive in greeen\nprint('\\033[92m' + all_positive_tweets[random.randint(0,5000)])\n\n# print negative in red\nprint('\\033[91m' + all_negative_tweets[random.randint(0,5000)])\n\nThank you Esai :-)\n‚ôõ‚ôõ‚ôõ\n„Äã„Äã„Äã„Äã \nI LOVE YOU SO MUCH.\nI BELƒ∞EVE THAT HE Wƒ∞LL FOLLOW.\nPLEASE FOLLOW ME PLEASE JUSTƒ∞N @justinbieber :( x15.340\n„Äã„Äã„Äã„ÄãÔº≥Ôº•Ôº• Ôº≠Ôº•\n‚ôõ‚ôõ‚ôõ\n\n\nOne observation you may have is the presence of emoticons and URLs in many of the tweets. This info will come in handy in the next steps.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "L1 Preprocessing"
    ]
  },
  {
    "objectID": "posts/c1w1/lab01.html#preprocess-raw-text-for-sentiment-analysis",
    "href": "posts/c1w1/lab01.html#preprocess-raw-text-for-sentiment-analysis",
    "title": "Preprocessing",
    "section": "Preprocess raw text for Sentiment analysis",
    "text": "Preprocess raw text for Sentiment analysis\nData preprocessing is one of the critical steps in any machine learning project. It includes cleaning and formatting the data before feeding into a machine learning algorithm. For NLP, the preprocessing steps are comprised of the following tasks:\n\nTokenizing the string\nLowercasing\nRemoving stop words and punctuation\nStemming\n\nThe videos explained each of these steps and why they are important. Let‚Äôs see how we can do these to a given tweet. We will choose just one and see how this is transformed by each preprocessing step.\n\n# Our selected sample. Complex enough to exemplify each step\ntweet = all_positive_tweets[2277]\nprint(tweet)\n\nMy beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off‚Ä¶ https://t.co/3tfYom0N1i\n\n\nLet‚Äôs import a few more libraries for this purpose.\n\n# download the stopwords from NLTK\nnltk.download('stopwords')\n\n[nltk_data] Downloading package stopwords to /home/oren/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue\n\n\n\nimport re                                  # library for regular expression operations\nimport string                              # for string operations\n\nfrom nltk.corpus import stopwords          # module for stop words that come with NLTK\nfrom nltk.stem import PorterStemmer        # module for stemming\nfrom nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n\n\nRemove hyperlinks, Twitter marks and styles\nSince we have a Twitter dataset, we‚Äôd like to remove some substrings commonly used on the platform like the hashtag, retweet marks, and hyperlinks. We‚Äôll use the re library to perform regular expression operations on our tweet. We‚Äôll define our search pattern and use the sub() method to remove matches by substituting with an empty character (i.e.¬†'')\n\nprint('\\033[92m' + tweet)\nprint('\\033[94m')\n\n# remove old style retweet text \"RT\"\ntweet2 = re.sub(r'^RT[\\s]+', '', tweet)\n\n# remove hyperlinks\ntweet2 = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet2)\n\n# remove hashtags\n# only removing the hash # sign from the word\ntweet2 = re.sub(r'#', '', tweet2)\n\nprint(tweet2)\n\nMy beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off‚Ä¶ https://t.co/3tfYom0N1i\n\nMy beautiful sunflowers on a sunny Friday morning off :) sunflowers favourites happy Friday off‚Ä¶ \n\n\n\n\nTokenize the string\nTo tokenize means to split the strings into individual words without blanks or tabs. In this same step, we will also convert each word in the string to lower case. The tokenize module from NLTK allows us to do these easily:\n\nprint()\nprint('\\033[92m' + tweet2)\nprint('\\033[94m')\n\n# instantiate tokenizer class\ntokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                               reduce_len=True)\n\n# tokenize tweets\ntweet_tokens = tokenizer.tokenize(tweet2)\n\nprint()\nprint('Tokenized string:')\nprint(tweet_tokens)\n\n\nMy beautiful sunflowers on a sunny Friday morning off :) sunflowers favourites happy Friday off‚Ä¶ \n\n\nTokenized string:\n['my', 'beautiful', 'sunflowers', 'on', 'a', 'sunny', 'friday', 'morning', 'off', ':)', 'sunflowers', 'favourites', 'happy', 'friday', 'off', '‚Ä¶']\n\n\n\n\nRemove stop words and punctuations\nThe next step is to remove stop words and punctuation. Stop words are words that don‚Äôt add significant meaning to the text. You‚Äôll see the list provided by NLTK when you run the cells below.\n\n#Import the english stop words list from NLTK\nstopwords_english = stopwords.words('english') \n\nprint('Stop words\\n')\nprint(stopwords_english)\n\nprint('\\nPunctuation\\n')\nprint(string.punctuation)\n\nStop words\n\n['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n\nPunctuation\n\n!\"#$%&'()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\n\n\nWe can see that the stop words list above contains some words that could be important in some contexts. These could be words like i, not, between, because, won, against. You might need to customize the stop words list for some applications. For our exercise, we will use the entire list.\nFor the punctuation, we saw earlier that certain groupings like ‚Äò:)‚Äô and ‚Äò‚Ä¶‚Äô should be retained when dealing with tweets because they are used to express emotions. In other contexts, like medical analysis, these should also be removed.\nTime to clean up our tokenized tweet!\n\nprint()\nprint('\\033[92m')\nprint(tweet_tokens)\nprint('\\033[94m')\n\ntweets_clean = []\n\nfor word in tweet_tokens: # Go through every word in your tokens list\n    if (word not in stopwords_english and  # remove stopwords\n        word not in string.punctuation):  # remove punctuation\n        tweets_clean.append(word)\n\nprint('removed stop words and punctuation:')\nprint(tweets_clean)\n\n\n\n['my', 'beautiful', 'sunflowers', 'on', 'a', 'sunny', 'friday', 'morning', 'off', ':)', 'sunflowers', 'favourites', 'happy', 'friday', 'off', '‚Ä¶']\n\nremoved stop words and punctuation:\n['beautiful', 'sunflowers', 'sunny', 'friday', 'morning', ':)', 'sunflowers', 'favourites', 'happy', 'friday', '‚Ä¶']\n\n\nPlease note that the words happy and sunny in this list are correctly spelled.\n\n\nStemming\nStemming is the process of converting a word to its most general form, or stem. This helps in reducing the size of our vocabulary.\nConsider the words: * learn * learning * learned * learnt\nAll these words are stemmed from its common root learn. However, in some cases, the stemming process produces words that are not correct spellings of the root word. For example, happi and sunni. That‚Äôs because it chooses the most common stem for related words. For example, we can look at the set of words that comprises the different forms of happy:\n\nhappy\nhappiness\nhappier\n\nWe can see that the prefix happi is more commonly used. We cannot choose happ because it is the stem of unrelated words like happen.\nNLTK has different modules for stemming and we will be using the PorterStemmer module which uses the Porter Stemming Algorithm. Let‚Äôs see how we can use it in the cell below.\n\nprint()\nprint('\\033[92m')\nprint(tweets_clean)\nprint('\\033[94m')\n\n# Instantiate stemming class\nstemmer = PorterStemmer() \n\n# Create an empty list to store the stems\ntweets_stem = [] \n\nfor word in tweets_clean:\n    stem_word = stemmer.stem(word)  # stemming word\n    tweets_stem.append(stem_word)  # append to the list\n\nprint('stemmed words:')\nprint(tweets_stem)\n\n\n\n['beautiful', 'sunflowers', 'sunny', 'friday', 'morning', ':)', 'sunflowers', 'favourites', 'happy', 'friday', '‚Ä¶']\n\nstemmed words:\n['beauti', 'sunflow', 'sunni', 'friday', 'morn', ':)', 'sunflow', 'favourit', 'happi', 'friday', '‚Ä¶']\n\n\nThat‚Äôs it! Now we have a set of words we can feed into to the next stage of our machine learning project.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "L1 Preprocessing"
    ]
  },
  {
    "objectID": "posts/c1w1/lab01.html#process_tweet",
    "href": "posts/c1w1/lab01.html#process_tweet",
    "title": "Preprocessing",
    "section": "process_tweet()",
    "text": "process_tweet()\nAs shown above, preprocessing consists of multiple steps before you arrive at the final list of words. We will not ask you to replicate these however. In the week‚Äôs assignment, you will use the function process_tweet(tweet) available in utils.py. We encourage you to open the file and you‚Äôll see that this function‚Äôs implementation is very similar to the steps above.\nTo obtain the same result as in the previous code cells, you will only need to call the function process_tweet(). Let‚Äôs do that in the next cell.\n\nfrom utils import process_tweet # Import the process_tweet function\n\n# choose the same tweet\ntweet = all_positive_tweets[2277]\n\nprint()\nprint('\\033[92m')\nprint(tweet)\nprint('\\033[94m')\n\n# call the imported function\ntweets_stem = process_tweet(tweet); # Preprocess a given tweet\n\nprint('preprocessed tweet:')\nprint(tweets_stem) # Print the result\n\n\n\nMy beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off‚Ä¶ https://t.co/3tfYom0N1i\n\npreprocessed tweet:\n['beauti', 'sunflow', 'sunni', 'friday', 'morn', ':)', 'sunflow', 'favourit', 'happi', 'friday', '‚Ä¶']\n\n\nThat‚Äôs it for this lab! You now know what is going on when you call the preprocessing helper function in this week‚Äôs assignment. Hopefully, this exercise has also given you some insights on how to tweak this for other types of text datasets.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "L1 Preprocessing"
    ]
  },
  {
    "objectID": "posts/c1w1/assignment.html",
    "href": "posts/c1w1/assignment.html",
    "title": "Assignment 1: Logistic Regression",
    "section": "",
    "text": "Figure¬†1: course banner\nWelcome to week one of this specialization. You will learn about logistic regression. Concretely, you will be implementing logistic regression for sentiment analysis on tweets. Given a tweet, you will decide if it has a positive sentiment or a negative one. Specifically you will:\nWe will be using a data set of tweets. Hopefully you will get more than 99% accuracy.\nRun the cell below to load in the packages.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "A1 Logistic Regression"
    ]
  },
  {
    "objectID": "posts/c1w1/assignment.html#import-functions-and-data",
    "href": "posts/c1w1/assignment.html#import-functions-and-data",
    "title": "Assignment 1: Logistic Regression",
    "section": "Import functions and data",
    "text": "Import functions and data\n\n# run this cell to import nltk\nimport nltk\nfrom os import getcwd\n\n\nImported functions\nDownload the data needed for this assignment. Check out the documentation for the twitter_samples dataset.\n\ntwitter_samples: if you‚Äôre running this notebook on your local computer, you will need to download it using:\n\nnltk.download('twitter_samples')\n\nstopwords: if you‚Äôre running this notebook on your local computer, you will need to download it using:\n\nnltk.download('stopwords')\n\nImport some helper functions that we provided in the utils.py file:\n\nprocess_tweet(): cleans the text, tokenizes it into separate words, removes stopwords, and converts words to stems.\nbuild_freqs(): this counts how often a word in the ‚Äòcorpus‚Äô (the entire set of tweets) was associated with a positive label ‚Äò1‚Äô or a negative label ‚Äò0‚Äô, then builds the freqs dictionary, where each key is a (word,label) tuple, and the value is the count of its frequency within the corpus of tweets.\n\n\n# add folder, tmp2, from our local workspace containing pre-downloaded corpora files to nltk's data path\n# this enables importing of these files without downloading it again when we refresh our workspace\n\nfilePath = f\"{getcwd()}/../tmp2/\"\nnltk.data.path.append(filePath)\n\n\nimport numpy as np\nimport pandas as pd\nfrom nltk.corpus import twitter_samples \n\nfrom utils import process_tweet, build_freqs\n\n\n\n\nPrepare the data\n\nThe twitter_samples contains subsets of 5,000 positive tweets, 5,000 negative tweets, and the full set of 10,000 tweets.\n\nIf you used all three datasets, we would introduce duplicates of the positive tweets and negative tweets.\n\nYou will select just the five thousand positive tweets and five thousand negative tweets.\n\n\n\n# select the set of positive and negative tweets\nall_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')\n\n\nTrain test split: 20% will be in the test set, and 80% in the training set.\n\n\n# split the data into two pieces, one for training and one for testing (validation set) \ntest_pos = all_positive_tweets[4000:]\ntrain_pos = all_positive_tweets[:4000]\ntest_neg = all_negative_tweets[4000:]\ntrain_neg = all_negative_tweets[:4000]\n\ntrain_x = train_pos + train_neg \ntest_x = test_pos + test_neg\n\n\nCreate the numpy array of positive labels and negative labels.\n\n\n# combine positive and negative labels\ntrain_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\ntest_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)\n\n\n# Print the shape train and test sets\nprint(\"train_y.shape = \" + str(train_y.shape))\nprint(\"test_y.shape = \" + str(test_y.shape))\n\ntrain_y.shape = (8000, 1)\ntest_y.shape = (2000, 1)\n\n\n\nCreate the frequency dictionary using the imported build_freqs() function.\n\nWe highly recommend that you open utils.py and read the build_freqs() function to understand what it is doing.\nTo view the file directory, go to the menu and click File-&gt;Open.\n\n\n    for y,tweet in zip(ys, tweets):\n        for word in process_tweet(tweet):\n            pair = (word, y)\n            if pair in freqs:\n                freqs[pair] += 1\n            else:\n                freqs[pair] = 1\n\nNotice how the outer for loop goes through each tweet, and the inner for loop steps through each word in a tweet.\nThe freqs dictionary is the frequency dictionary that‚Äôs being built.\nThe key is the tuple (word, label), such as (‚Äúhappy‚Äù,1) or (‚Äúhappy‚Äù,0). The value stored for each key is the count of how many times the word ‚Äúhappy‚Äù was associated with a positive label, or how many times ‚Äúhappy‚Äù was associated with a negative label.\n\n\n# create frequency dictionary\nfreqs = build_freqs(train_x, train_y)\n\n# check the output\nprint(\"type(freqs) = \" + str(type(freqs)))\nprint(\"len(freqs) = \" + str(len(freqs.keys())))\n\ntype(freqs) = &lt;class 'dict'&gt;\nlen(freqs) = 11337\n\n\n\nExpected output\ntype(freqs) = &lt;class 'dict'&gt;\nlen(freqs) = 11346\n\n\n\nProcess tweet\nThe given function process_tweet() tokenizes the tweet into individual words, removes stop words and applies stemming.\n\n# test the function below\nprint('This is an example of a positive tweet: \\n', train_x[0])\nprint('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[0]))\n\nThis is an example of a positive tweet: \n #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n\nThis is an example of the processed version of the tweet: \n ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n\n\n\nExpected output\nThis is an example of a positive tweet: \n #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n \nThis is an example of the processes version: \n ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "A1 Logistic Regression"
    ]
  },
  {
    "objectID": "posts/c1w1/assignment.html#instructions-implement-gradient-descent-function",
    "href": "posts/c1w1/assignment.html#instructions-implement-gradient-descent-function",
    "title": "Assignment 1: Logistic Regression",
    "section": "Instructions: Implement gradient descent function",
    "text": "Instructions: Implement gradient descent function\n\nThe number of iterations num_iters is the number of times that you‚Äôll use the entire training set.\nFor each iteration, you‚Äôll calculate the cost function using all training examples (there are m training examples), and for all features.\nInstead of updating a single weight \\theta_i at a time, we can update all the weights in the column vector:\n\\mathbf{\\theta} = \\begin{pmatrix}\n\\theta_0\n\\\\\n\\theta_1\n\\\\\n\\theta_2\n\\\\\n\\vdots\n\\\\\n\\theta_n\n\\end{pmatrix}\n\\mathbf{\\theta} has dimensions (n+1, 1), where ‚Äòn‚Äô is the number of features, and there is one more element for the bias term \\theta_0 (note that the corresponding feature value \\mathbf{x_0} is 1).\nThe ‚Äòlogits‚Äô, ‚Äòz‚Äô, are calculated by multiplying the feature matrix ‚Äòx‚Äô with the weight vector ‚Äòtheta‚Äô. z = \\mathbf{x}\\mathbf{\\theta}\n\n\\mathbf{x} has dimensions (m, n+1)\n\\mathbf{\\theta}: has dimensions (n+1, 1)\n\\mathbf{z}: has dimensions (m, 1)\n\nThe prediction ‚Äòh‚Äô, is calculated by applying the sigmoid to each element in ‚Äòz‚Äô: h(z) = sigmoid(z), and has dimensions (m,1).\nThe cost function J is calculated by taking the dot product of the vectors ‚Äòy‚Äô and ‚Äòlog(h)‚Äô. Since both ‚Äòy‚Äô and ‚Äòh‚Äô are column vectors (m,1), transpose the vector to the left, so that matrix multiplication of a row vector with column vector performs the dot product. J = \\frac{-1}{m} \\times \\left(\\mathbf{y}^T \\cdot log(\\mathbf{h}) + \\mathbf{(1-y)}^T \\cdot log(\\mathbf{1-h}) \\right)\nThe update of theta is also vectorized. Because the dimensions of \\mathbf{x} are (m, n+1), and both \\mathbf{h} and \\mathbf{y} are (m, 1), we need to transpose the \\mathbf{x} and place it on the left in order to perform matrix multiplication, which then yields the (n+1, 1) answer we need: \\mathbf{\\theta} = \\mathbf{\\theta} - \\frac{\\alpha}{m} \\times \\left( \\mathbf{x}^T \\cdot \\left( \\mathbf{h-y} \\right) \\right)\n\n\n\nHints\n\n\n\n\nuse np.dot for matrix multiplication.\n\n\nTo ensure that the fraction -1/m is a decimal value, cast either the numerator or denominator (or both), like float(1), or write 1. for the float version of 1.\n\n\n\n\n# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef gradientDescent(x, y, theta, alpha, num_iters):\n    '''\n    Input:\n        x: matrix of features which is (m,n+1)\n        y: corresponding labels of the input matrix x, dimensions (m,1)\n        theta: weight vector of dimension (n+1,1)\n        alpha: learning rate\n        num_iters: number of iterations you want to train your model for\n    Output:\n        J: the final cost\n        theta: your final weight vector\n    Hint: you might want to print the cost to make sure that it is going down.\n    '''\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    # get 'm', the number of rows in matrix x\n    m = None\n    \n    for i in range(0, num_iters):\n        \n        # get z, the dot product of x and theta\n        z = None\n        \n        # get the sigmoid of z\n        h = None\n        \n        # calculate the cost function\n        J = None\n\n        # update the weights theta\n        theta = None\n        \n    ### END CODE HERE ###\n    J = float(J)\n    return J, theta\n\n\n# Check the function\n# Construct a synthetic test case using numpy PRNG functions\nnp.random.seed(1)\n# X input is 10 x 3 with ones for the bias terms\ntmp_X = np.append(np.ones((10, 1)), np.random.rand(10, 2) * 2000, axis=1)\n# Y Labels are 10 x 1\ntmp_Y = (np.random.rand(10, 1) &gt; 0.35).astype(float)\n\n# Apply gradient descent\ntmp_J, tmp_theta = gradientDescent(tmp_X, tmp_Y, np.zeros((3, 1)), 1e-8, 700)\nprint(f\"The cost after training is {tmp_J:.8f}.\")\nprint(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(tmp_theta)]}\")\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[15], line 10\n      7 tmp_Y = (np.random.rand(10, 1) &gt; 0.35).astype(float)\n      9 # Apply gradient descent\n---&gt; 10 tmp_J, tmp_theta = gradientDescent(tmp_X, tmp_Y, np.zeros((3, 1)), 1e-8, 700)\n     11 print(f\"The cost after training is {tmp_J:.8f}.\")\n     12 print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(tmp_theta)]}\")\n\nCell In[14], line 34, in gradientDescent(x, y, theta, alpha, num_iters)\n     31     theta = None\n     33 ### END CODE HERE ###\n---&gt; 34 J = float(J)\n     35 return J, theta\n\nTypeError: float() argument must be a string or a real number, not 'NoneType'\n\n\n\n\nExpected output\nThe cost after training is 0.67094970.\nThe resulting vector of weights is [4.1e-07, 0.00035658, 7.309e-05]",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "A1 Logistic Regression"
    ]
  },
  {
    "objectID": "posts/c1w1/assignment.html#part-2-extracting-the-features",
    "href": "posts/c1w1/assignment.html#part-2-extracting-the-features",
    "title": "Assignment 1: Logistic Regression",
    "section": "Part 2: Extracting the features",
    "text": "Part 2: Extracting the features\n\nGiven a list of tweets, extract the features and store them in a matrix. You will extract two features.\n\nThe first feature is the number of positive words in a tweet.\nThe second feature is the number of negative words in a tweet.\n\nThen train your logistic regression classifier on these features.\nTest the classifier on a validation set.\n\n\nInstructions: Implement the extract_features function.\n\nThis function takes in a single tweet.\nProcess the tweet using the imported process_tweet() function and save the list of tweet words.\nLoop through each word in the list of processed words\n\nFor each word, check the freqs dictionary for the count when that word has a positive ‚Äò1‚Äô label. (Check for the key (word, 1.0)\nDo the same for the count for when the word is associated with the negative label ‚Äò0‚Äô. (Check for the key (word, 0.0).)\n\n\n\n\nHints\n\n\n\n\nMake sure you handle cases when the (word, label) key is not found in the dictionary.\n\n\nSearch the web for hints about using the .get() method of a Python dictionary. Here is an  example \n\n\n\n\n# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef extract_features(tweet, freqs):\n    '''\n    Input: \n        tweet: a list of words for one tweet\n        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n    Output: \n        x: a feature vector of dimension (1,3)\n    '''\n    # process_tweet tokenizes, stems, and removes stopwords\n    word_l = process_tweet(tweet)\n    \n    # 3 elements in the form of a 1 x 3 vector\n    x = np.zeros((1, 3)) \n    \n    #bias term is set to 1\n    x[0,0] = 1 \n    \n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    \n    # loop through each word in the list of words\n    for word in word_l:\n        \n        # increment the word count for the positive label 1\n        x[0,1] += None\n        \n        # increment the word count for the negative label 0\n        x[0,2] += None\n        \n    ### END CODE HERE ###\n    assert(x.shape == (1, 3))\n    return x\n\n\n# Check your function\n\n# test 1\n# test on training data\ntmp1 = extract_features(train_x[0], freqs)\nprint(tmp1)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[17], line 5\n      1 # Check your function\n      2 \n      3 # test 1\n      4 # test on training data\n----&gt; 5 tmp1 = extract_features(train_x[0], freqs)\n      6 print(tmp1)\n\nCell In[16], line 25, in extract_features(tweet, freqs)\n     19 ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n     20 \n     21 # loop through each word in the list of words\n     22 for word in word_l:\n     23     \n     24     # increment the word count for the positive label 1\n---&gt; 25     x[0,1] += None\n     27     # increment the word count for the negative label 0\n     28     x[0,2] += None\n\nTypeError: unsupported operand type(s) for +: 'float' and 'NoneType'\n\n\n\n\nExpected output\n[[1.00e+00 3.02e+03 6.10e+01]]\n\n# test 2:\n# check for when the words are not in the freqs dictionary\ntmp2 = extract_features('blorb bleeeeb bloooob', freqs)\nprint(tmp2)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[18], line 3\n      1 # test 2:\n      2 # check for when the words are not in the freqs dictionary\n----&gt; 3 tmp2 = extract_features('blorb bleeeeb bloooob', freqs)\n      4 print(tmp2)\n\nCell In[16], line 25, in extract_features(tweet, freqs)\n     19 ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n     20 \n     21 # loop through each word in the list of words\n     22 for word in word_l:\n     23     \n     24     # increment the word count for the positive label 1\n---&gt; 25     x[0,1] += None\n     27     # increment the word count for the negative label 0\n     28     x[0,2] += None\n\nTypeError: unsupported operand type(s) for +: 'float' and 'NoneType'\n\n\n\n\n\nExpected output\n[[1. 0. 0.]]",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "A1 Logistic Regression"
    ]
  },
  {
    "objectID": "posts/c1w1/assignment.html#part-3-training-your-model",
    "href": "posts/c1w1/assignment.html#part-3-training-your-model",
    "title": "Assignment 1: Logistic Regression",
    "section": "Part 3: Training Your Model",
    "text": "Part 3: Training Your Model\nTo train the model: * Stack the features for all training examples into a matrix X. * Call gradientDescent, which you‚Äôve implemented above.\nThis section is given to you. Please read it for understanding and run the cell.\n\n# collect the features 'x' and stack them into a matrix 'X'\nX = np.zeros((len(train_x), 3))\nfor i in range(len(train_x)):\n    X[i, :]= extract_features(train_x[i], freqs)\n\n# training labels corresponding to X\nY = train_y\n\n# Apply gradient descent\nJ, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\nprint(f\"The cost after training is {J:.8f}.\")\nprint(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[19], line 4\n      2 X = np.zeros((len(train_x), 3))\n      3 for i in range(len(train_x)):\n----&gt; 4     X[i, :]= extract_features(train_x[i], freqs)\n      6 # training labels corresponding to X\n      7 Y = train_y\n\nCell In[16], line 25, in extract_features(tweet, freqs)\n     19 ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n     20 \n     21 # loop through each word in the list of words\n     22 for word in word_l:\n     23     \n     24     # increment the word count for the positive label 1\n---&gt; 25     x[0,1] += None\n     27     # increment the word count for the negative label 0\n     28     x[0,2] += None\n\nTypeError: unsupported operand type(s) for +: 'float' and 'NoneType'\n\n\n\nExpected Output:\nThe cost after training is 0.24216529.\nThe resulting vector of weights is [7e-08, 0.0005239, -0.00055517]",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "A1 Logistic Regression"
    ]
  },
  {
    "objectID": "posts/c1w1/assignment.html#check-performance-using-the-test-set",
    "href": "posts/c1w1/assignment.html#check-performance-using-the-test-set",
    "title": "Assignment 1: Logistic Regression",
    "section": "Check performance using the test set",
    "text": "Check performance using the test set\nAfter training your model using the training set above, check how your model might perform on real, unseen data, by testing it against the test set.\n\nInstructions: Implement test_logistic_regression\n\nGiven the test data and the weights of your trained model, calculate the accuracy of your logistic regression model.\nUse your predict_tweet() function to make predictions on each tweet in the test set.\nIf the prediction is &gt; 0.5, set the model‚Äôs classification y_hat to 1, otherwise set the model‚Äôs classification y_hat to 0.\nA prediction is accurate when y_hat equals test_y. Sum up all the instances when they are equal and divide by m.\n\n\n\nHints\n\n\n\n\nUse np.asarray() to convert a list to a numpy array\n\n\nUse np.squeeze() to make an (m,1) dimensional array into an (m,) array\n\n\n\n\n# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef test_logistic_regression(test_x, test_y, freqs, theta):\n    \"\"\"\n    Input: \n        test_x: a list of tweets\n        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n        freqs: a dictionary with the frequency of each pair (or tuple)\n        theta: weight vector of dimension (3, 1)\n    Output: \n        accuracy: (# of tweets classified correctly) / (total # of tweets)\n    \"\"\"\n    \n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    \n    # the list for storing predictions\n    y_hat = []\n    \n    for tweet in test_x:\n        # get the label prediction for the tweet\n        y_pred = predict_tweet(tweet, freqs, theta)\n        \n        if y_pred &gt; 0.5:\n            # append 1.0 to the list\n            None\n        else:\n            # append 0 to the list\n            None\n\n    # With the above implementation, y_hat is a list, but test_y is (m,1) array\n    # convert both to one-dimensional arrays in order to compare them using the '==' operator\n    accuracy = None\n\n    ### END CODE HERE ###\n    \n    return accuracy\n\n\ntmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\nprint(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[24], line 1\n----&gt; 1 tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n      2 print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")\n\nNameError: name 'theta' is not defined\n\n\n\n\n\nExpected Output:\n0.9950\nPretty good!",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "A1 Logistic Regression"
    ]
  },
  {
    "objectID": "posts/c2w2/lab02.html",
    "href": "posts/c2w2/lab02.html",
    "title": "Parts-of-Speech Tagging - Working with tags and Numpy",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure¬†1\n\n\nIn this lecture notebook you will create a matrix using some tag information and then modify it using different approaches. This will serve as hands-on experience working with Numpy and as an introduction to some elements used for POS tagging.\n\nimport numpy as np\nimport pandas as pd\n\n\nSome information on tags\nFor this notebook you will be using a toy example including only three tags (or states). In a real world application there are many more tags which can be found here.\n\n# Define tags for Adverb, Noun and To (the preposition) , respectively\ntags = ['RB', 'NN', 'TO']\n\nIn this week‚Äôs assignment you will construct some dictionaries that provide useful information of the tags and words you will be working with.\nOne of these dictionaries is the transition_counts which counts the number of times a particular tag happened next to another. The keys of this dictionary have the form (previous_tag, tag) and the values are the frequency of occurrences.\nAnother one is the emission_counts dictionary which will count the number of times a particular pair of (tag, word) appeared in the training dataset.\nIn general think of transition when working with tags only and of emission when working with tags and words.\nIn this notebook you will be looking at the first one:\n\n# Define 'transition_counts' dictionary\n# Note: values are the same as the ones in the assignment\ntransition_counts = {\n    ('NN', 'NN'): 16241,\n    ('RB', 'RB'): 2263,\n    ('TO', 'TO'): 2,\n    ('NN', 'TO'): 5256,\n    ('RB', 'TO'): 855,\n    ('TO', 'NN'): 734,\n    ('NN', 'RB'): 2431,\n    ('RB', 'NN'): 358,\n    ('TO', 'RB'): 200\n}\n\nNotice that there are 9 combinations of the 3 tags used. Each tag can appear after the same tag so you should include those as well.\n\n\nUsing Numpy for matrix creation\nNow you will create a matrix that includes these frequencies using Numpy arrays:\n\n# Store the number of tags in the 'num_tags' variable\nnum_tags = len(tags)\n\n# Initialize a 3X3 numpy array with zeros\ntransition_matrix = np.zeros((num_tags, num_tags))\n\n# Print matrix\ntransition_matrix\n\narray([[0., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.]])\n\n\nVisually you can see the matrix has the correct dimensions. Don‚Äôt forget you can check this too using the shape attribute:\n\n# Print shape of the matrix\ntransition_matrix.shape\n\n(3, 3)\n\n\nBefore filling this matrix with the values of the transition_counts dictionary you should sort the tags so that their placement in the matrix is consistent:\n\n# Create sorted version of the tag's list\nsorted_tags = sorted(tags)\n\n# Print sorted list\nsorted_tags\n\n['NN', 'RB', 'TO']\n\n\nTo fill this matrix with the correct values you can use a double for loop. You could also use itertools.product to one line this double loop:\n\n# Loop rows\nfor i in range(num_tags):\n    # Loop columns\n    for j in range(num_tags):\n        # Define tag pair\n        tag_tuple = (sorted_tags[i], sorted_tags[j])\n        # Get frequency from transition_counts dict and assign to (i, j) position in the matrix\n        transition_matrix[i, j] = transition_counts.get(tag_tuple)\n\n# Print matrix\ntransition_matrix\n\narray([[1.6241e+04, 2.4310e+03, 5.2560e+03],\n       [3.5800e+02, 2.2630e+03, 8.5500e+02],\n       [7.3400e+02, 2.0000e+02, 2.0000e+00]])\n\n\nLooks like this worked fine. However the matrix can be hard to read as Numpy is more about efficiency, rather than presenting values in a pretty format.\nFor this you can use a Pandas DataFrame. In particular, a function that takes the matrix as input and prints out a pretty version of it will be very useful:\n\n# Define 'print_matrix' function\ndef print_matrix(matrix):\n    print(pd.DataFrame(matrix, index=sorted_tags, columns=sorted_tags))\n\nNotice that the tags are not a parameter of the function. This is because the sorted_tags list will not change in the rest of the notebook so it is safe to use the variable previously declared. To test this function simply run:\n\n# Print the 'transition_matrix' by calling the 'print_matrix' function\nprint_matrix(transition_matrix)\n\n         NN      RB      TO\nNN  16241.0  2431.0  5256.0\nRB    358.0  2263.0   855.0\nTO    734.0   200.0     2.0\n\n\nThat is a lot better, isn‚Äôt it?\nAs you may have already deducted this matrix is not symmetrical.\n\n\nWorking with Numpy for matrix manipulation\nNow that you got the matrix set up it is time to see how a matrix can be manipulated after being created.\nNumpy allows vectorized operations which means that operations that would normally include looping over the matrix can be done in a simpler manner. This is consistent with treating numpy arrays as matrices since you get support for common matrix operations. You can do matrix multiplication, scalar multiplication, vector addition and many more!\nFor instance try scaling each value in the matrix by a factor of \\frac{1}{10}. Normally you would loop over each value in the matrix, updating them accordingly. But in Numpy this is as easy as dividing the whole matrix by 10:\n\n# Scale transition matrix\ntransition_matrix = transition_matrix/10\n\n# Print scaled matrix\nprint_matrix(transition_matrix)\n\n        NN     RB     TO\nNN  1624.1  243.1  525.6\nRB    35.8  226.3   85.5\nTO    73.4   20.0    0.2\n\n\nAnother trickier example is to normalize each row so that each value is equal to \\frac{value}{sum \\,of \\,row}.\nThis can be easily done with vectorization. First you will compute the sum of each row:\n\n# Compute sum of row for each row\nrows_sum = transition_matrix.sum(axis=1, keepdims=True)\n\n# Print sum of rows\nrows_sum\n\narray([[2392.8],\n       [ 347.6],\n       [  93.6]])\n\n\nNotice that the sum() method was used. This method does exactly what its name implies. Since the sum of the rows was desired the axis was set to 1. In Numpy axis=1 refers to the columns so the sum is done by summing each column of a particular row, for each row.\nAlso the keepdims parameter was set to True so the resulting array had shape (3, 1) rather than (3,). This was done so that the axes were consistent with the desired operation.\nWhen working with Numpy, always remember to check the shape of the arrays you are working with, many unexpected errors happen because of axes not being consistent. The shape attribute is your friend for these cases.\n\n# Normalize transition matrix\ntransition_matrix = transition_matrix / rows_sum\n\n# Print normalized matrix\nprint_matrix(transition_matrix)\n\n          NN        RB        TO\nNN  0.678745  0.101596  0.219659\nRB  0.102992  0.651036  0.245972\nTO  0.784188  0.213675  0.002137\n\n\nNotice that the normalization that was carried out forces the sum of each row to be equal to 1. You can easily check this by running the sum method on the resulting matrix:\n\ntransition_matrix.sum(axis=1, keepdims=True)\n\narray([[1.],\n       [1.],\n       [1.]])\n\n\nFor a final example you are asked to modify each value of the diagonal of the matrix so that they are equal to the log of the sum of the current row plus the current value. When doing mathematical operations like this one don‚Äôt forget to import the math module.\nThis can be done using a standard for loop or vectorization. You‚Äôll see both in action:\n\nimport math\n\n# Copy transition matrix for for-loop example\nt_matrix_for = np.copy(transition_matrix)\n\n# Copy transition matrix for numpy functions example\nt_matrix_np = np.copy(transition_matrix)\n\n\nUsing a for-loop\n\n# Loop values in the diagonal\nfor i in range(num_tags):\n    t_matrix_for[i, i] =  t_matrix_for[i, i] + math.log(rows_sum[i])\n\n# Print matrix\nprint_matrix(t_matrix_for)\n\n          NN        RB        TO\nNN  8.458964  0.101596  0.219659\nRB  0.102992  6.502088  0.245972\nTO  0.784188  0.213675  4.541167\n\n\n/tmp/ipykernel_205346/84584535.py:3: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  t_matrix_for[i, i] =  t_matrix_for[i, i] + math.log(rows_sum[i])\n\n\n\n\nUsing vectorization\n\n# Save diagonal in a numpy array\nd = np.diag(t_matrix_np)\n\n# Print shape of diagonal\nd.shape\n\n(3,)\n\n\nYou can save the diagonal in a numpy array using Numpy‚Äôs diag() function. Notice that this array has shape (3,) so it is inconsistent with the dimensions of the rows_sum array which are (3, 1). You‚Äôll have to reshape before moving forward. For this you can use Numpy‚Äôs reshape() function, specifying the desired shape in a tuple:\n\n# Reshape diagonal numpy array\nd = np.reshape(d, (3,1))\n\n# Print shape of diagonal\nd.shape\n\n(3, 1)\n\n\nNow that the diagonal has the correct shape you can do the vectorized operation by applying the math.log() function to the rows_sum array and adding the diagonal.\nTo apply a function to each element of a numpy array use Numpy‚Äôs vectorize() function providing the desired function as a parameter. This function returns a vectorized function that accepts a numpy array as a parameter.\nTo update the original matrix you can use Numpy‚Äôs fill_diagonal() function.\n\n# Perform the vectorized operation\nd = d + np.vectorize(math.log)(rows_sum)\n\n# Use numpy's 'fill_diagonal' function to update the diagonal\nnp.fill_diagonal(t_matrix_np, d)\n\n# Print the matrix\nprint_matrix(t_matrix_np)\n\n          NN        RB        TO\nNN  8.458964  0.101596  0.219659\nRB  0.102992  6.502088  0.245972\nTO  0.784188  0.213675  4.541167\n\n\nTo perform a sanity check that both methods yield the same result you can compare both matrices. Notice that this operation is also vectorized so you will get the equality check for each element in both matrices:\n\n# Check for equality\nt_matrix_for == t_matrix_np\n\narray([[ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True]])\n\n\nCongratulations on finishing this lecture notebook! Now you should be more familiar with some elements used by a POS tagger such as the transition_counts dictionary and with working with Numpy.\nKeep it up!\n\n\n\n\n\nCitationBibTeX citation:@online{bochman2020,\n  author = {Bochman, Oren},\n  title = {Parts-of-Speech {Tagging} - {Working} with Tags and {Numpy}},\n  date = {2020-10-23},\n  url = {https://orenbochman.github.io/notes-nlp/posts/c2w2/lab02.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2020. ‚ÄúParts-of-Speech Tagging - Working with Tags\nand Numpy.‚Äù October 23, 2020. https://orenbochman.github.io/notes-nlp/posts/c2w2/lab02.html.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "L2 - Working with tags and Numpy"
    ]
  },
  {
    "objectID": "posts/c2w2/assignment.html",
    "href": "posts/c2w2/assignment.html",
    "title": "Assignment 2: Parts-of-Speech Tagging (POS)",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure¬†1\nWelcome to the second assignment of Course 2 in the Natural Language Processing specialization. This assignment will develop skills in part-of-speech (POS) tagging, the process of assigning a part-of-speech tag (Noun, Verb, Adjective‚Ä¶) to each word in an input text. Tagging is difficult because some words can represent more than one part of speech at different times. They are Ambiguous. Let‚Äôs look at the following example:\nDistinguishing the parts-of-speech of a word in a sentence will help we better understand the meaning of a sentence. This would be critically important in search queries. Identifying the proper noun, the organization, the stock symbol, or anything similar would greatly improve everything ranging from speech recognition to search. By completing this assignment, we will:",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "A2 - POS tagging"
    ]
  },
  {
    "objectID": "posts/c2w2/assignment.html#outline",
    "href": "posts/c2w2/assignment.html#outline",
    "title": "Assignment 2: Parts-of-Speech Tagging (POS)",
    "section": "Outline",
    "text": "Outline\n\n0 Data Sources\n1 POS Tagging\n\n1.1 Training\n\nExercise 01\n\n1.2 Testing\n\nExercise 02\n\n\n2 Hidden Markov Models\n\n2.1 Generating Matrices\n\nExercise 03\nExercise 04\n\n\n3 Viterbi Algorithm\n\n3.1 Initialization\n\nExercise 05\n\n3.2 Viterbi Forward\n\nExercise 06\n\n3.3 Viterbi Backward\n\nExercise 07\n\n\n4 Predicting on a data set\n\nExercise 08\n\n\n\n# Importing packages and loading in the data set \nfrom utils_pos import get_word_tag, preprocess  \nimport pandas as pd\nfrom collections import defaultdict\nimport math\nimport numpy as np",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "A2 - POS tagging"
    ]
  },
  {
    "objectID": "posts/c2w2/assignment.html#0",
    "href": "posts/c2w2/assignment.html#0",
    "title": "Assignment 2: Parts-of-Speech Tagging (POS)",
    "section": "Part 0: Data Sources",
    "text": "Part 0: Data Sources\nThis assignment will use two tagged data sets collected from the Wall Street Journal (WSJ).\nHere is an example ‚Äòtag-set‚Äô or Part of Speech designation describing the two or three letter tag and their meaning.\n\nOne data set (WSJ-2_21.pos) will be used for training.\nThe other (WSJ-24.pos) for testing.\nThe tagged training data has been preprocessed to form a vocabulary (hmm_vocab.txt).\nThe words in the vocabulary are words from the training set that were used two or more times.\nThe vocabulary is augmented with a set of ‚Äòunknown word tokens‚Äô, described below.\n\nThe training set will be used to create the emission, transmission and tag counts.\nThe test set (WSJ-24.pos) is read in to create y.\n\nThis contains both the test text and the true tag.\nThe test set has also been preprocessed to remove the tags to form test_words.txt.\nThis is read in and further processed to identify the end of sentences and handle words not in the vocabulary using functions provided in utils_pos.py.\nThis forms the list prep, the preprocessed text used to test our POS taggers.\n\nA POS tagger will necessarily encounter words that are not in its datasets.\n\nTo improve accuracy, these words are further analyzed during preprocessing to extract available hints as to their appropriate tag.\nFor example, the suffix ‚Äòize‚Äô is a hint that the word is a verb, as in ‚Äòfinal-ize‚Äô or ‚Äòcharacter-ize‚Äô.\nA set of unknown-tokens, such as ‚Äò‚Äìunk-verb‚Äì‚Äô or ‚Äò‚Äìunk-noun‚Äì‚Äô will replace the unknown words in both the training and test corpus and will appear in the emission, transmission and tag data structures.\n\n\nImplementation note:\n\nFor python 3.6 and beyond, dictionaries retain the insertion order.\nFurthermore, their hash-based lookup makes them suitable for rapid membership tests.\n\nIf di is a dictionary, key in di will return True if di has a key key, else False.\n\n\nThe dictionary vocab will utilize these features.\n\n# load in the training corpus\nwith open(\"WSJ_02-21.pos\", 'r') as f:\n    training_corpus = f.readlines()\n\nprint(f\"A few items of the training corpus list\")\nprint(training_corpus[0:5])\n\nA few items of the training corpus list\n['In\\tIN\\n', 'an\\tDT\\n', 'Oct.\\tNNP\\n', '19\\tCD\\n', 'review\\tNN\\n']\n\n\n\n# read the vocabulary data, split by each line of text, and save the list\nwith open(\"hmm_vocab.txt\", 'r') as f:\n    voc_l = f.read().split('\\n')\n\nprint(\"A few items of the vocabulary list\")\nprint(voc_l[0:50])\nprint()\nprint(\"A few items at the end of the vocabulary list\")\nprint(voc_l[-50:])\n\nA few items of the vocabulary list\n['!', '#', '$', '%', '&', \"'\", \"''\", \"'40s\", \"'60s\", \"'70s\", \"'80s\", \"'86\", \"'90s\", \"'N\", \"'S\", \"'d\", \"'em\", \"'ll\", \"'m\", \"'n'\", \"'re\", \"'s\", \"'til\", \"'ve\", '(', ')', ',', '-', '--', '--n--', '--unk--', '--unk_adj--', '--unk_adv--', '--unk_digit--', '--unk_noun--', '--unk_punct--', '--unk_upper--', '--unk_verb--', '.', '...', '0.01', '0.0108', '0.02', '0.03', '0.05', '0.1', '0.10', '0.12', '0.13', '0.15']\n\nA few items at the end of the vocabulary list\n['yards', 'yardstick', 'year', 'year-ago', 'year-before', 'year-earlier', 'year-end', 'year-on-year', 'year-round', 'year-to-date', 'year-to-year', 'yearlong', 'yearly', 'years', 'yeast', 'yelled', 'yelling', 'yellow', 'yen', 'yes', 'yesterday', 'yet', 'yield', 'yielded', 'yielding', 'yields', 'you', 'young', 'younger', 'youngest', 'youngsters', 'your', 'yourself', 'youth', 'youthful', 'yuppie', 'yuppies', 'zero', 'zero-coupon', 'zeroing', 'zeros', 'zinc', 'zip', 'zombie', 'zone', 'zones', 'zoning', '{', '}', '']\n\n\n\n# vocab: dictionary that has the index of the corresponding words\nvocab = {} \n\n# Get the index of the corresponding words. \nfor i, word in enumerate(sorted(voc_l)): \n    vocab[word] = i       \n    \nprint(\"Vocabulary dictionary, key is the word, value is a unique integer\")\ncnt = 0\nfor k,v in vocab.items():\n    print(f\"{k}:{v}\")\n    cnt += 1\n    if cnt &gt; 20:\n        break\n\nVocabulary dictionary, key is the word, value is a unique integer\n:0\n!:1\n#:2\n$:3\n%:4\n&:5\n':6\n'':7\n'40s:8\n'60s:9\n'70s:10\n'80s:11\n'86:12\n'90s:13\n'N:14\n'S:15\n'd:16\n'em:17\n'll:18\n'm:19\n'n':20\n\n\n\n# load in the test corpus\nwith open(\"WSJ_24.pos\", 'r') as f:\n    y = f.readlines()\n\nprint(\"A sample of the test corpus\")\nprint(y[0:10])\n\nA sample of the test corpus\n['The\\tDT\\n', 'economy\\tNN\\n', \"'s\\tPOS\\n\", 'temperature\\tNN\\n', 'will\\tMD\\n', 'be\\tVB\\n', 'taken\\tVBN\\n', 'from\\tIN\\n', 'several\\tJJ\\n', 'vantage\\tNN\\n']\n\n\n\n#corpus without tags, preprocessed\n_, prep = preprocess(vocab, \"test.words\")     \n\nprint('The length of the preprocessed test corpus: ', len(prep))\nprint('This is a sample of the test_corpus: ')\nprint(prep[0:10])\n\nThe length of the preprocessed test corpus:  34199\nThis is a sample of the test_corpus: \n['The', 'economy', \"'s\", 'temperature', 'will', 'be', 'taken', 'from', 'several', '--unk--']",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "A2 - POS tagging"
    ]
  },
  {
    "objectID": "posts/c2w2/assignment.html#1.1",
    "href": "posts/c2w2/assignment.html#1.1",
    "title": "Assignment 2: Parts-of-Speech Tagging (POS)",
    "section": "Part 1.1 - Training",
    "text": "Part 1.1 - Training\nWe will start with the simplest possible parts-of-speech tagger and we will build up to the state of the art.\nIn this section, we will find the words that are not ambiguous.\n\nFor example, the word is is a verb and it is not ambiguous.\nIn the WSJ corpus, 86% of the token are unambiguous (meaning they have only one tag)\nAbout 14\\% are ambiguous (meaning that they have more than one tag)\n\n\nBefore we start predicting the tags of each word, we will need to compute a few dictionaries that will help we to generate the tables.\n\nTransition counts\n\nThe first dictionary is the transition_counts dictionary which computes the number of times each tag happened next to another tag.\n\nThis dictionary will be used to compute: \nP(t_i |t_{i-1}) \\tag{1}\n\nThis is the probability of a tag at position i given the tag at position i-1.\nIn order for we to compute equation 1, we will create a transition_counts dictionary where - The keys are (prev_tag, tag) - The values are the number of times those two tags appeared in that order.\n\n\nEmission counts\nThe second dictionary we will compute is the emission_counts dictionary. This dictionary will be used to compute:\n\nP(w_i|t_i)\\tag{2}\n\nIn other words, we will use it to compute the probability of a word given its tag.\nIn order for we to compute equation 2, we will create an emission_counts dictionary where - The keys are (tag, word) - The values are the number of times that pair showed up in your training set.\n\n\nTag counts\nThe last dictionary we will compute is the tag_counts dictionary. - The key is the tag - The value is the number of times each tag appeared.\n\n\nExercise 01\nInstructions: Write a program that takes in the training_corpus and returns the three dictionaries mentioned above transition_counts, emission_counts, and tag_counts.\n\nemission_counts: maps (tag, word) to the number of times it happened.\ntransition_counts: maps (prev_tag, tag) to the number of times it has appeared.\ntag_counts: maps (tag) to the number of times it has occured.\n\nImplementation note: This routine utilises defaultdict, which is a subclass of dict.\n\nA standard Python dictionary throws a KeyError if we try to access an item with a key that is not currently in the dictionary.\nIn contrast, the defaultdict will create an item of the type of the argument, in this case an integer with the default value of 0.\nSee defaultdict.\n\n\n# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: create_dictionaries\ndef create_dictionaries(training_corpus, vocab):\n    \"\"\"\n    Input: \n        training_corpus: a corpus where each line has a word followed by its tag.\n        vocab: a dictionary where keys are words in vocabulary and value is an index\n    Output: \n        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts\n        transition_counts: a dictionary where the keys are (prev_tag, tag) and the values are the counts\n        tag_counts: a dictionary where the keys are the tags and the values are the counts\n    \"\"\"\n    \n    # initialize the dictionaries using defaultdict\n    emission_counts = defaultdict(int)\n    transition_counts = defaultdict(int)\n    tag_counts = defaultdict(int)\n    \n    # Initialize \"prev_tag\" (previous tag) with the start state, denoted by '--s--'\n    prev_tag = '--s--' \n    \n    # use 'i' to track the line number in the corpus\n    i = 0 \n    \n    # Each item in the training corpus contains a word and its POS tag\n    # Go through each word and its tag in the training corpus\n    for word_tag in training_corpus:\n        \n        # Increment the word_tag count\n        i += 1\n        \n        # Every 50,000 words, print the word count\n        if i % 50000 == 0:\n            print(f\"word count = {i}\")\n            \n        ### START CODE HERE (Replace instances of 'None' with your code) ###\n        # get the word and tag using the get_word_tag helper function (imported from utils_pos.py)\n        word, tag = None \n        \n        # Increment the transition count for the previous word and tag\n        transition_counts[(prev_tag, tag)] += None\n        \n        # Increment the emission count for the tag and word\n        emission_counts[(tag, word)] += None\n\n        # Increment the tag count\n        tag_counts[tag] += None\n\n        # Set the previous tag to this tag (for the next iteration of the loop)\n        prev_tag = None\n        \n        ### END CODE HERE ###\n        \n    return emission_counts, transition_counts, tag_counts\n\n\nemission_counts, transition_counts, tag_counts = create_dictionaries(training_corpus, vocab)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[116], line 1\n----&gt; 1 emission_counts, transition_counts, tag_counts = create_dictionaries(training_corpus, vocab)\n\nCell In[115], line 38, in create_dictionaries(training_corpus, vocab)\n     34     print(f\"word count = {i}\")\n     36 ### START CODE HERE (Replace instances of 'None' with your code) ###\n     37 # get the word and tag using the get_word_tag helper function (imported from utils_pos.py)\n---&gt; 38 word, tag = None \n     40 # Increment the transition count for the previous word and tag\n     41 transition_counts[(prev_tag, tag)] += None\n\nTypeError: cannot unpack non-iterable NoneType object\n\n\n\n\n# get all the POS states\nstates = sorted(tag_counts.keys())\nprint(f\"Number of POS tags (number of 'states'): {len(states)}\")\nprint(\"View these POS tags (states)\")\nprint(states)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[117], line 2\n      1 # get all the POS states\n----&gt; 2 states = sorted(tag_counts.keys())\n      3 print(f\"Number of POS tags (number of 'states'): {len(states)}\")\n      4 print(\"View these POS tags (states)\")\n\nNameError: name 'tag_counts' is not defined\n\n\n\n\nExpected Output\nNumber of POS tags (number of 'states'46\nView these states\n['#', '$', \"''\", '(', ')', ',', '--s--', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\nThe ‚Äòstates‚Äô are the Parts-of-speech designations found in the training data. They will also be referred to as ‚Äòtags‚Äô or POS in this assignment.\n\n‚ÄúNN‚Äù is noun, singular,\n‚ÄòNNS‚Äô is noun, plural.\nIn addition, there are helpful tags like ‚Äò‚Äìs‚Äì‚Äô which indicate a start of a sentence.\nWe can get a more complete description at Penn Treebank II tag set.\n\n\nprint(\"transition examples: \")\nfor ex in list(transition_counts.items())[:3]:\n    print(ex)\nprint()\n\nprint(\"emission examples: \")\nfor ex in list(emission_counts.items())[200:203]:\n    print (ex)\nprint()\n\nprint(\"ambiguous word example: \")\nfor tup,cnt in emission_counts.items():\n    if tup[1] == 'back': print (tup, cnt) \n\ntransition examples: \n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[118], line 2\n      1 print(\"transition examples: \")\n----&gt; 2 for ex in list(transition_counts.items())[:3]:\n      3     print(ex)\n      4 print()\n\nNameError: name 'transition_counts' is not defined\n\n\n\n\n\nExpected Output\ntransition examples: \n(('--s--', 'IN'), 5050)\n(('IN', 'DT'), 32364)\n(('DT', 'NNP'), 9044)\n\nemission examples: \n(('DT', 'any'), 721)\n(('NN', 'decrease'), 7)\n(('NN', 'insider-trading'), 5)\n\nambiguous word example: \n('RB', 'back') 304\n('VB', 'back') 20\n('RP', 'back') 84\n('JJ', 'back') 25\n('NN', 'back') 29\n('VBP', 'back') 4\n\n\n\nPart 1.2 - Testing\nNow we will test the accuracy of your parts-of-speech tagger using your emission_counts dictionary. - Given your preprocessed test corpus prep, we will assign a parts-of-speech tag to every word in that corpus. - Using the original tagged test corpus y, we will then compute what percent of the tags we got correct.\n\n\nExercise 02\nInstructions: Implement predict_pos that computes the accuracy of your model.\n\nThis is a warm up exercise.\nTo assign a part of speech to a word, assign the most frequent POS for that word in the training set.\nThen evaluate how well this approach works. Each time we predict based on the most frequent POS for the given word, check whether the actual POS of that word is the same. If so, the prediction was correct!\nCalculate the accuracy as the number of correct predictions divided by the total number of words for which we predicted the POS tag.\n\n\n# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: predict_pos\n\ndef predict_pos(prep, y, emission_counts, vocab, states):\n    '''\n    Input: \n        prep: a preprocessed version of 'y'. A list with the 'word' component of the tuples.\n        y: a corpus composed of a list of tuples where each tuple consists of (word, POS)\n        emission_counts: a dictionary where the keys are (tag,word) tuples and the value is the count\n        vocab: a dictionary where keys are words in vocabulary and value is an index\n        states: a sorted list of all possible tags for this assignment\n    Output: \n        accuracy: Number of times we classified a word correctly\n    '''\n    \n    # Initialize the number of correct predictions to zero\n    num_correct = 0\n    \n    # Get the (tag, word) tuples, stored as a set\n    all_words = set(emission_counts.keys())\n    \n    # Get the number of (word, POS) tuples in the corpus 'y'\n    total = len(y)\n    for word, y_tup in zip(prep, y): \n\n        # Split the (word, POS) string into a list of two items\n        y_tup_l = y_tup.split()\n        \n        # Verify that y_tup contain both word and POS\n        if len(y_tup_l) == 2:\n            \n            # Set the true POS label for this word\n            true_label = y_tup_l[1]\n\n        else:\n            # If the y_tup didn't contain word and POS, go to next word\n            continue\n    \n        count_final = 0\n        pos_final = ''\n        \n        # If the word is in the vocabulary...\n        if word in vocab:\n            for pos in states:\n\n            ### START CODE HERE (Replace instances of 'None' with your code) ###\n                        \n                # define the key as the tuple containing the POS and word\n                key = None\n\n                # check if the (pos, word) key exists in the emission_counts dictionary\n                if key in None: # complete this line\n\n                # get the emission count of the (pos,word) tuple \n                    count = None\n\n                    # keep track of the POS with the largest count\n                    if None: # complete this line\n\n                        # update the final count (largest count)\n                        count_final = None\n\n                        # update the final POS\n                        pos_final = None\n\n            # If the final POS (with the largest count) matches the true POS:\n            if None: # complete this line\n                \n                # Update the number of correct predictions\n                num_correct += None\n            \n    ### END CODE HERE ###\n    accuracy = num_correct / total\n    \n    return accuracy\n\n\naccuracy_predict_pos = predict_pos(prep, y, emission_counts, vocab, states)\nprint(f\"Accuracy of prediction using predict_pos is {accuracy_predict_pos:.4f}\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[120], line 1\n----&gt; 1 accuracy_predict_pos = predict_pos(prep, y, emission_counts, vocab, states)\n      2 print(f\"Accuracy of prediction using predict_pos is {accuracy_predict_pos:.4f}\")\n\nNameError: name 'emission_counts' is not defined\n\n\n\n\nExpected Output\nAccuracy of prediction using predict_pos is 0.8889\n88.9% is really good for this warm up exercise. With hidden markov models, we should be able to get 95% accuracy.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "A2 - POS tagging"
    ]
  },
  {
    "objectID": "posts/c2w2/assignment.html#2.1",
    "href": "posts/c2w2/assignment.html#2.1",
    "title": "Assignment 2: Parts-of-Speech Tagging (POS)",
    "section": "Part 2.1 Generating Matrices",
    "text": "Part 2.1 Generating Matrices\n\nCreating the ‚ÄòA‚Äô transition probabilities matrix\nNow that we have your emission_counts, transition_counts, and tag_counts, we will start implementing the Hidden Markov Model.\nThis will allow we to quickly construct the - A transition probabilities matrix. - and the B emission probabilities matrix.\nWe will also use some smoothing when computing these matrices.\nHere is an example of what the A transition matrix would look like (it is simplified to 5 tags for viewing. It is 46x46 in this assignment.):\n\n\n\n\n\n\n\n\n\n\n\n\n\nA\n‚Ä¶\nRBS\nRP\nSYM\nTO\nUH\n‚Ä¶\n\n\n\n\nRBS\n‚Ä¶\n2.217069e-06\n2.217069e-06\n2.217069e-06\n0.008870\n2.217069e-06\n‚Ä¶\n\n\nRP\n‚Ä¶\n3.756509e-07\n7.516775e-04\n3.756509e-07\n0.051089\n3.756509e-07\n‚Ä¶\n\n\nSYM\n‚Ä¶\n1.722772e-05\n1.722772e-05\n1.722772e-05\n0.000017\n1.722772e-05\n‚Ä¶\n\n\nTO\n‚Ä¶\n4.477336e-05\n4.472863e-08\n4.472863e-08\n0.000090\n4.477336e-05\n‚Ä¶\n\n\nUH\n‚Ä¶\n1.030439e-05\n1.030439e-05\n1.030439e-05\n0.061837\n3.092348e-02\n‚Ä¶\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\nNote that the matrix above was computed with smoothing.\nEach cell gives we the probability to go from one part of speech to another.\n\nIn other words, there is a 4.47e-8 chance of going from parts-of-speech TO to RP.\nThe sum of each row has to equal 1, because we assume that the next POS tag must be one of the available columns in the table.\n\nThe smoothing was done as follows:\n\nP(t_i | t_{i-1}) = \\frac{C(t_{i-1}, t_{i}) + \\alpha }{C(t_{i-1}) +\\alpha * N}\\tag{3}\n\n\nN is the total number of tags\nC(t_{i-1}, t_{i}) is the count of the tuple (previous POS, current POS) in transition_counts dictionary.\nC(t_{i-1}) is the count of the previous POS in the tag_counts dictionary.\n\\alpha is a smoothing parameter.\n\n\n\nExercise 03\nInstructions: Implement the create_transition_matrix below for all tags. Your task is to output a matrix that computes equation 3 for each cell in matrix A.\n\n# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: create_transition_matrix\ndef create_transition_matrix(alpha, tag_counts, transition_counts):\n    ''' \n    Input: \n        alpha: number used for smoothing\n        tag_counts: a dictionary mapping each tag to its respective count\n        transition_counts: transition count for the previous word and tag\n    Output:\n        A: matrix of dimension (num_tags,num_tags)\n    '''\n    # Get a sorted list of unique POS tags\n    all_tags = sorted(tag_counts.keys())\n    \n    # Count the number of unique POS tags\n    num_tags = len(all_tags)\n    \n    # Initialize the transition matrix 'A'\n    A = np.zeros((num_tags,num_tags))\n    \n    # Get the unique transition tuples (previous POS, current POS)\n    trans_keys = set(transition_counts.keys())\n    \n    ### START CODE HERE (Replace instances of 'None' with your code) ### \n    \n    # Go through each row of the transition matrix A\n    for i in range(num_tags):\n        \n        # Go through each column of the transition matrix A\n        for j in range(num_tags):\n\n            # Initialize the count of the (prev POS, current POS) to zero\n            count = 0\n        \n            # Define the tuple (prev POS, current POS)\n            # Get the tag at position i and tag at position j (from the all_tags list)\n            key = None\n\n            # Check if the (prev POS, current POS) tuple \n            # exists in the transition counts dictionary\n            if None: #complete this line\n                \n                # Get count from the transition_counts dictionary \n                # for the (prev POS, current POS) tuple\n                count = None\n                \n            # Get the count of the previous tag (index position i) from tag_counts\n            count_prev_tag = None\n            \n            # Apply smoothing using count of the tuple, alpha, \n            # count of previous tag, alpha, and total number of tags\n            A[i,j] = None\n\n    ### END CODE HERE ###\n    \n    return A\n\n\nalpha = 0.001\nA = create_transition_matrix(alpha, tag_counts, transition_counts)\n# Testing your function\nprint(f\"A at row 0, col 0: {A[0,0]:.9f}\")\nprint(f\"A at row 3, col 1: {A[3,1]:.4f}\")\n\nprint(\"View a subset of transition matrix A\")\nA_sub = pd.DataFrame(A[30:35,30:35], index=states[30:35], columns = states[30:35] )\nprint(A_sub)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[122], line 2\n      1 alpha = 0.001\n----&gt; 2 A = create_transition_matrix(alpha, tag_counts, transition_counts)\n      3 # Testing your function\n      4 print(f\"A at row 0, col 0: {A[0,0]:.9f}\")\n\nNameError: name 'tag_counts' is not defined\n\n\n\n\nExpected Output\nA at row 0, col 0: 0.000007040\nA at row 3, col 1: 0.1691\nView a subset of transition matrix A\n              RBS            RP           SYM        TO            UH\nRBS  2.217069e-06  2.217069e-06  2.217069e-06  0.008870  2.217069e-06\nRP   3.756509e-07  7.516775e-04  3.756509e-07  0.051089  3.756509e-07\nSYM  1.722772e-05  1.722772e-05  1.722772e-05  0.000017  1.722772e-05\nTO   4.477336e-05  4.472863e-08  4.472863e-08  0.000090  4.477336e-05\nUH   1.030439e-05  1.030439e-05  1.030439e-05  0.061837  3.092348e-02\n\n\n\nCreate the ‚ÄòB‚Äô emission probabilities matrix\nNow we will create the B transition matrix which computes the emission probability.\nWe will use smoothing as defined below:\n\nP(w_i | t_i) = \\frac{C(t_i, word_i)+ \\alpha}{C(t_{i}) +\\alpha * N}\\tag{4}\n\n\nC(t_i, word_i) is the number of times word_i was associated with tag_i in the training data (stored in emission_counts dictionary).\nC(t_i) is the number of times tag_i was in the training data (stored in tag_counts dictionary).\nN is the number of words in the vocabulary\n\\alpha is a smoothing parameter.\n\nThe matrix B is of dimension (num_tags, N), where num_tags is the number of possible parts-of-speech tags.\nHere is an example of the matrix, only a subset of tags and words are shown:\n\nB Emissions Probability Matrix (subset)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nB\n‚Ä¶\n725\nadroitly\nengineers\npromoted\nsynergy\n‚Ä¶\n\n\n\n\nCD\n‚Ä¶\n8.201296e-05\n2.732854e-08\n2.732854e-08\n2.732854e-08\n2.732854e-08\n‚Ä¶\n\n\nNN\n‚Ä¶\n7.521128e-09\n7.521128e-09\n7.521128e-09\n7.521128e-09\n2.257091e-05\n‚Ä¶\n\n\nNNS\n‚Ä¶\n1.670013e-08\n1.670013e-08\n4.676203e-04\n1.670013e-08\n1.670013e-08\n‚Ä¶\n\n\nVB\n‚Ä¶\n3.779036e-08\n3.779036e-08\n3.779036e-08\n3.779036e-08\n3.779036e-08\n‚Ä¶\n\n\nRB\n‚Ä¶\n3.226454e-08\n6.456135e-05\n3.226454e-08\n3.226454e-08\n3.226454e-08\n‚Ä¶\n\n\nRP\n‚Ä¶\n3.723317e-07\n3.723317e-07\n3.723317e-07\n3.723317e-07\n3.723317e-07\n‚Ä¶\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\n\n\nExercise 04\nInstructions: Implement the create_emission_matrix below that computes the B emission probabilities matrix. Your function takes in \\alpha, the smoothing parameter, tag_counts, which is a dictionary mapping each tag to its respective count, the emission_counts dictionary where the keys are (tag, word) and the values are the counts. Your task is to output a matrix that computes equation 4 for each cell in matrix B.\n\n# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: create_emission_matrix\n\ndef create_emission_matrix(alpha, tag_counts, emission_counts, vocab):\n    '''\n    Input: \n        alpha: tuning parameter used in smoothing \n        tag_counts: a dictionary mapping each tag to its respective count\n        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts\n        vocab: a dictionary where keys are words in vocabulary and value is an index.\n               within the function it'll be treated as a list\n    Output:\n        B: a matrix of dimension (num_tags, len(vocab))\n    '''\n    \n    # get the number of POS tag\n    num_tags = len(tag_counts)\n    \n    # Get a list of all POS tags\n    all_tags = sorted(tag_counts.keys())\n    \n    # Get the total number of unique words in the vocabulary\n    num_words = len(vocab)\n    \n    # Initialize the emission matrix B with places for\n    # tags in the rows and words in the columns\n    B = np.zeros((num_tags, num_words))\n    \n    # Get a set of all (POS, word) tuples \n    # from the keys of the emission_counts dictionary\n    emis_keys = set(list(emission_counts.keys()))\n    \n    ### START CODE HERE (Replace instances of 'None' with your code) ###\n    \n    # Go through each row (POS tags)\n    for i in None: # complete this line\n        \n        # Go through each column (words)\n        for j in None: # complete this line\n\n            # Initialize the emission count for the (POS tag, word) to zero\n            count = 0\n                    \n            # Define the (POS tag, word) tuple for this row and column\n            key =  None\n\n            # check if the (POS tag, word) tuple exists as a key in emission counts\n            if None: # complete this line\n        \n                # Get the count of (POS tag, word) from the emission_counts d\n                count = None\n                \n            # Get the count of the POS tag\n            count_tag = None\n                \n            # Apply smoothing and store the smoothed value \n            # into the emission matrix B for this row and column\n            B[i,j] = None\n\n    ### END CODE HERE ###\n    return B\n\n\n# creating your emission probability matrix. this takes a few minutes to run. \nB = create_emission_matrix(alpha, tag_counts, emission_counts, list(vocab))\n\nprint(f\"View Matrix position at row 0, column 0: {B[0,0]:.9f}\")\nprint(f\"View Matrix position at row 3, column 1: {B[3,1]:.9f}\")\n\n# Try viewing emissions for a few words in a sample dataframe\ncidx  = ['725','adroitly','engineers', 'promoted', 'synergy']\n\n# Get the integer ID for each word\ncols = [vocab[a] for a in cidx]\n\n# Choose POS tags to show in a sample dataframe\nrvals =['CD','NN','NNS', 'VB','RB','RP']\n\n# For each POS tag, get the row number from the 'states' list\nrows = [states.index(a) for a in rvals]\n\n# Get the emissions for the sample of words, and the sample of POS tags\nB_sub = pd.DataFrame(B[np.ix_(rows,cols)], index=rvals, columns = cidx )\nprint(B_sub)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[124], line 2\n      1 # creating your emission probability matrix. this takes a few minutes to run. \n----&gt; 2 B = create_emission_matrix(alpha, tag_counts, emission_counts, list(vocab))\n      4 print(f\"View Matrix position at row 0, column 0: {B[0,0]:.9f}\")\n      5 print(f\"View Matrix position at row 3, column 1: {B[3,1]:.9f}\")\n\nNameError: name 'tag_counts' is not defined\n\n\n\n\nExpected Output\nView Matrix position at row 0, column 0: 0.000006032\nView Matrix position at row 3, column 1: 0.000000720\n              725      adroitly     engineers      promoted       synergy\nCD   8.201296e-05  2.732854e-08  2.732854e-08  2.732854e-08  2.732854e-08\nNN   7.521128e-09  7.521128e-09  7.521128e-09  7.521128e-09  2.257091e-05\nNNS  1.670013e-08  1.670013e-08  4.676203e-04  1.670013e-08  1.670013e-08\nVB   3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08\nRB   3.226454e-08  6.456135e-05  3.226454e-08  3.226454e-08  3.226454e-08\nRP   3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "A2 - POS tagging"
    ]
  },
  {
    "objectID": "posts/c2w2/assignment.html#3.1",
    "href": "posts/c2w2/assignment.html#3.1",
    "title": "Assignment 2: Parts-of-Speech Tagging (POS)",
    "section": "Part 3.1: Initialization",
    "text": "Part 3.1: Initialization\nWe will start by initializing two matrices of the same dimension.\n\nbest_probs: Each cell contains the probability of going from one POS tag to a word in the corpus.\nbest_paths: A matrix that helps we trace through the best possible path in the corpus.\n\n\nExercise 05\nInstructions: Write a program below that initializes the best_probs and the best_paths matrix.\nBoth matrices will be initialized to zero except for column zero of best_probs.\n\nColumn zero of best_probs is initialized with the assumption that the first word of the corpus was preceded by a start token (‚Äú‚Äìs‚Äì‚Äù).\nThis allows we to reference the A matrix for the transition probability\n\nHere is how to initialize column 0 of best_probs:\n\nThe probability of the best path going from the start index to a given POS tag indexed by integer i is denoted by \\textrm{best\\_probs}[s_{idx}, i] .\nThis is estimated as the probability that the start tag transitions to the POS denoted by index i: \\mathbf{A}[s_{idx}, i] AND that the POS tag denoted by i emits the first word of the given corpus, which is \\mathbf{B}[i, vocab[corpus[0]]].\nNote that vocab[corpus[0]] refers to the first word of the corpus (the word at position 0 of the corpus).\nvocab is a dictionary that returns the unique integer that refers to that particular word.\n\nConceptually, it looks like this:\n\n\\textrm{best\\_probs}[s_{idx}, i] = \\mathbf{A}[s_{idx}, i] \\times \\mathbf{B}[i, corpus[0] ]\n\nIn order to avoid multiplying and storing small values on the computer, we‚Äôll take the log of the product, which becomes the sum of two logs:\nbest\\_probs[i,0] = log(A[s_{idx}, i]) + log(B[i, vocab[corpus[0]]])\nAlso, to avoid taking the log of 0 (which is defined as negative infinity), the code itself will just set best\\_probs[i,0] = float('-inf') when A[s_{idx}, i] == 0\nSo the implementation to initialize best\\_probs looks like this:\nif A[s_{idx}, i] &lt;&gt; 0 : best\\_probs[i,0] = log(A[s_{idx}, i]) + log(B[i, vocab[corpus[0]]])\nif A[s_{idx}, i] == 0 : best\\_probs[i,0] = float('-inf')\nPlease use math.log to compute the natural logarithm.\nThe example below shows the initialization assuming the corpus starts with the phrase ‚ÄúLoss tracks upward‚Äù.\n\nRepresent infinity and negative infinity like this:\nfloat('inf')\nfloat('-inf')\n\n# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: initialize\ndef initialize(states, tag_counts, A, B, corpus, vocab):\n    '''\n    Input: \n        states: a list of all possible parts-of-speech\n        tag_counts: a dictionary mapping each tag to its respective count\n        A: Transition Matrix of dimension (num_tags, num_tags)\n        B: Emission Matrix of dimension (num_tags, len(vocab))\n        corpus: a sequence of words whose POS is to be identified in a list \n        vocab: a dictionary where keys are words in vocabulary and value is an index\n    Output:\n        best_probs: matrix of dimension (num_tags, len(corpus)) of floats\n        best_paths: matrix of dimension (num_tags, len(corpus)) of integers\n    '''\n    # Get the total number of unique POS tags\n    num_tags = len(tag_counts)\n    \n    # Initialize best_probs matrix \n    # POS tags in the rows, number of words in the corpus as the columns\n    best_probs = np.zeros((num_tags, len(corpus)))\n    \n    # Initialize best_paths matrix\n    # POS tags in the rows, number of words in the corpus as columns\n    best_paths = np.zeros((num_tags, len(corpus)), dtype=int)\n    \n    # Define the start token\n    s_idx = states.index(\"--s--\")\n    ### START CODE HERE (Replace instances of 'None' with your code) ###\n    \n    # Go through each of the POS tags\n    for i in None: # complete this line\n        \n        # Handle the special case when the transition from start token to POS tag i is zero\n        if None: # complete this line\n            \n            # Initialize best_probs at POS tag 'i', column 0, to negative infinity\n            best_probs[i,0] = None\n        \n        # For all other cases when transition from start token to POS tag i is non-zero:\n        else:\n            \n            # Initialize best_probs at POS tag 'i', column 0\n            # Check the formula in the instructions above\n            best_probs[i,0] = None\n                        \n    ### END CODE HERE ### \n    return best_probs, best_paths\n\n\nbest_probs, best_paths = initialize(states, tag_counts, A, B, prep, vocab)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[126], line 1\n----&gt; 1 best_probs, best_paths = initialize(states, tag_counts, A, B, prep, vocab)\n\nNameError: name 'states' is not defined\n\n\n\n\n# Test the function\nprint(f\"best_probs[0,0]: {best_probs[0,0]:.4f}\") \nprint(f\"best_paths[2,3]: {best_paths[2,3]:.4f}\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[127], line 2\n      1 # Test the function\n----&gt; 2 print(f\"best_probs[0,0]: {best_probs[0,0]:.4f}\") \n      3 print(f\"best_paths[2,3]: {best_paths[2,3]:.4f}\")\n\nNameError: name 'best_probs' is not defined\n\n\n\n\nExpected Output\nbest_probs[0,0]: -22.6098\nbest_paths[2,3]: 0.0000",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "A2 - POS tagging"
    ]
  },
  {
    "objectID": "posts/c2w2/assignment.html#3.2",
    "href": "posts/c2w2/assignment.html#3.2",
    "title": "Assignment 2: Parts-of-Speech Tagging (POS)",
    "section": "Part 3.2 Viterbi Forward (prediction)",
    "text": "Part 3.2 Viterbi Forward (prediction)\nIn this part of the assignment, we will implement the viterbi_forward segment. In other words, we will populate your best_probs and best_paths matrices.\n\nWalk forward through the corpus.\nFor each word, compute a probability for each possible tag.\nUnlike the previous algorithm predict_pos (the ‚Äòwarm-up‚Äô exercise), this will include the path up to that (word,tag) combination.\n\nHere is an example with a three-word corpus ‚ÄúLoss tracks upward‚Äù:\n\nNote, in this example, only a subset of states (POS tags) are shown in the diagram below, for easier reading.\nIn the diagram below, the first word ‚ÄúLoss‚Äù is already initialized.\nThe algorithm will compute a probability for each of the potential tags in the second and future words.\n\nCompute the probability that the tag of the second work (‚Äòtracks‚Äô) is a verb, 3rd person singular present (VBZ).\n\nIn the best_probs matrix, go to the column of the second word (‚Äòtracks‚Äô), and row 40 (VBZ), this cell is highlighted in light orange in the diagram below.\nExamine each of the paths from the tags of the first word (‚ÄòLoss‚Äô) and choose the most likely path.\n\nAn example of the calculation for one of those paths is the path from (‚ÄòLoss‚Äô, NN) to (‚Äòtracks‚Äô, VBZ).\nThe log of the probability of the path up to and including the first word ‚ÄòLoss‚Äô having POS tag NN is -14.32. The best_probs matrix contains this value -14.32 in the column for ‚ÄòLoss‚Äô and row for ‚ÄòNN‚Äô.\nFind the probability that NN transitions to VBZ. To find this probability, go to the A transition matrix, and go to the row for ‚ÄòNN‚Äô and the column for ‚ÄòVBZ‚Äô. The value is 4.37e-02, which is circled in the diagram, so add -14.32 + log(4.37e-02).\nFind the log of the probability that the tag VBS would ‚Äòemit‚Äô the word ‚Äòtracks‚Äô. To find this, look at the ‚ÄòB‚Äô emission matrix in row ‚ÄòVBZ‚Äô and the column for the word ‚Äòtracks‚Äô. The value 4.61e-04 is circled in the diagram below. So add -14.32 + log(4.37e-02) + log(4.61e-04).\nThe sum of -14.32 + log(4.37e-02) + log(4.61e-04) is -25.13. Store -25.13 in the best_probs matrix at row ‚ÄòVBZ‚Äô and column ‚Äòtracks‚Äô (as seen in the cell that is highlighted in light orange in the diagram).\nAll other paths in best_probs are calculated. Notice that -25.13 is greater than all of the other values in column ‚Äòtracks‚Äô of matrix best_probs, and so the most likely path to ‚ÄòVBZ‚Äô is from ‚ÄòNN‚Äô. ‚ÄòNN‚Äô is in row 20 of the best_probs matrix, so 20 is the most likely path.\nStore the most likely path 20 in the best_paths table. This is highlighted in light orange in the diagram below.\n\nThe formula to compute the probability and path for the i^{th} word in the corpus, the prior word i-1 in the corpus, current POS tag j, and previous POS tag k is:\n\\mathrm{prob} = \\mathbf{best\\_prob}_{k, i-1} + \\mathrm{log}(\\mathbf{A}_{k, j}) + \\mathrm{log}(\\mathbf{B}_{j, vocab(corpus_{i})})\nwhere corpus_{i} is the word in the corpus at index i, and vocab is the dictionary that gets the unique integer that represents a given word.\n\\mathrm{path} = k\nwhere k is the integer representing the previous POS tag.\n\nExercise 06\nInstructions: Implement the viterbi_forward algorithm and store the best_path and best_prob for every possible tag for each word in the matrices best_probs and best_tags using the pseudo code below.\n`for each word in the corpus\nfor each POS tag type that this word may be\n\n    for POS tag type that the previous word could be\n    \n        compute the probability that the previous word had a given POS tag, that the current word has a given POS tag, and that the POS tag would emit this current word.\n        \n        retain the highest probability computed for the current word\n        \n        set best_probs to this highest probability\n        \n        set best_paths to the index 'k', representing the POS tag of the previous word which produced the highest probability `\nPlease use math.log to compute the natural logarithm.\n\n\n\nHints\n\n\n\n\nRemember that when accessing emission matrix B, the column index is the unique integer ID associated with the word. It can be accessed by using the ‚Äòvocab‚Äô dictionary, where the key is the word, and the value is the unique integer ID for that word.\n\n\n\n\n\n# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: viterbi_forward\ndef viterbi_forward(A, B, test_corpus, best_probs, best_paths, vocab):\n    '''\n    Input: \n        A, B: The transition and emission matrices respectively\n        test_corpus: a list containing a preprocessed corpus\n        best_probs: an initilized matrix of dimension (num_tags, len(corpus))\n        best_paths: an initilized matrix of dimension (num_tags, len(corpus))\n        vocab: a dictionary where keys are words in vocabulary and value is an index \n    Output: \n        best_probs: a completed matrix of dimension (num_tags, len(corpus))\n        best_paths: a completed matrix of dimension (num_tags, len(corpus))\n    '''\n    # Get the number of unique POS tags (which is the num of rows in best_probs)\n    num_tags = best_probs.shape[0]\n    \n    # Go through every word in the corpus starting from word 1\n    # Recall that word 0 was initialized in `initialize()`\n    for i in range(1, len(test_corpus)): \n        \n        # Print number of words processed, every 5000 words\n        if i % 5000 == 0:\n            print(\"Words processed: {:&gt;8}\".format(i))\n            \n        ### START CODE HERE (Replace instances of 'None' with your code EXCEPT the first 'best_path_i = None') ###\n        # For each unique POS tag that the current word can be\n        for j in None: # complete this line\n            \n            # Initialize best_prob for word i to negative infinity\n            best_prob_i = None\n            \n            # Initialize best_path for current word i to None\n            best_path_i = None\n\n            # For each POS tag that the previous word can be:\n            for k in None: # complete this line\n            \n                # Calculate the probability = \n                # best probs of POS tag k, previous word i-1 + \n                # log(prob of transition from POS k to POS j) + \n                # log(prob that emission of POS j is word i)\n                prob = None\n\n                # check if this path's probability is greater than\n                # the best probability up to and before this point\n                if None: # complete this line\n                    \n                    # Keep track of the best probability\n                    best_prob_i = None\n                    \n                    # keep track of the POS tag of the previous word\n                    # that is part of the best path.  \n                    # Save the index (integer) associated with \n                    # that previous word's POS tag\n                    best_path_i = None\n\n            # Save the best probability for the \n            # given current word's POS tag\n            # and the position of the current word inside the corpus\n            best_probs[j,i] = None\n            \n            # Save the unique integer ID of the previous POS tag\n            # into best_paths matrix, for the POS tag of the current word\n            # and the position of the current word inside the corpus.\n            best_paths[j,i] = None\n\n        ### END CODE HERE ###\n    return best_probs, best_paths\n\nRun the viterbi_forward function to fill in the best_probs and best_paths matrices.\nNote that this will take a few minutes to run. There are about 30,000 words to process.\n\n# this will take a few minutes to run =&gt; processes ~ 30,000 words\nbest_probs, best_paths = viterbi_forward(A, B, prep, best_probs, best_paths, vocab)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[129], line 2\n      1 # this will take a few minutes to run =&gt; processes ~ 30,000 words\n----&gt; 2 best_probs, best_paths = viterbi_forward(A, B, prep, best_probs, best_paths, vocab)\n\nNameError: name 'A' is not defined\n\n\n\n\n# Test this function \nprint(f\"best_probs[0,1]: {best_probs[0,1]:.4f}\") \nprint(f\"best_probs[0,4]: {best_probs[0,4]:.4f}\") \n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[130], line 2\n      1 # Test this function \n----&gt; 2 print(f\"best_probs[0,1]: {best_probs[0,1]:.4f}\") \n      3 print(f\"best_probs[0,4]: {best_probs[0,4]:.4f}\") \n\nNameError: name 'best_probs' is not defined\n\n\n\n\nExpected Output\nbest_probs[0,1]: -24.7822\nbest_probs[0,4]: -49.5601",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "A2 - POS tagging"
    ]
  },
  {
    "objectID": "posts/c2w2/assignment.html#3.3",
    "href": "posts/c2w2/assignment.html#3.3",
    "title": "Assignment 2: Parts-of-Speech Tagging (POS)",
    "section": "Part 3.3 Viterbi backward",
    "text": "Part 3.3 Viterbi backward\nNow we will implement the Viterbi backward algorithm.\n\nThe Viterbi backward algorithm gets the predictions of the POS tags for each word in the corpus using the best_paths and the best_probs matrices.\n\nThe example below shows how to walk backwards through the best_paths matrix to get the POS tags of each word in the corpus. Recall that this example corpus has three words: ‚ÄúLoss tracks upward‚Äù.\nPOS tag for ‚Äòupward‚Äô is RB\n\nSelect the the most likely POS tag for the last word in the corpus, ‚Äòupward‚Äô in the best_prob table.\nLook for the row in the column for ‚Äòupward‚Äô that has the largest probability.\nNotice that in row 28 of best_probs, the estimated probability is -34.99, which is larger than the other values in the column. So the most likely POS tag for ‚Äòupward‚Äô is RB an adverb, at row 28 of best_prob.\nThe variable z is an array that stores the unique integer ID of the predicted POS tags for each word in the corpus. In array z, at position 2, store the value 28 to indicate that the word ‚Äòupward‚Äô (at index 2 in the corpus), most likely has the POS tag associated with unique ID 28 (which is RB).\nThe variable pred contains the POS tags in string form. So pred at index 2 stores the string RB.\n\nPOS tag for ‚Äòtracks‚Äô is VBZ\n\nThe next step is to go backward one word in the corpus (‚Äòtracks‚Äô). Since the most likely POS tag for ‚Äòupward‚Äô is RB, which is uniquely identified by integer ID 28, go to the best_paths matrix in column 2, row 28. The value stored in best_paths, column 2, row 28 indicates the unique ID of the POS tag of the previous word. In this case, the value stored here is 40, which is the unique ID for POS tag VBZ (verb, 3rd person singular present).\nSo the previous word at index 1 of the corpus (‚Äòtracks‚Äô), most likely has the POS tag with unique ID 40, which is VBZ.\nIn array z, store the value 40 at position 1, and for array pred, store the string VBZ to indicate that the word ‚Äòtracks‚Äô most likely has POS tag VBZ.\n\nPOS tag for ‚ÄòLoss‚Äô is NN\n\nIn best_paths at column 1, the unique ID stored at row 40 is 20. 20 is the unique ID for POS tag NN.\nIn array z at position 0, store 20. In array pred at position 0, store NN.\n\n\n\nExercise 07\nImplement the viterbi_backward algorithm, which returns a list of predicted POS tags for each word in the corpus.\n\nNote that the numbering of the index positions starts at 0 and not 1.\nm is the number of words in the corpus.\n\nSo the indexing into the corpus goes from 0 to m - 1.\nAlso, the columns in best_probs and best_paths are indexed from 0 to m - 1\n\n\nIn Step 1:\nLoop through all the rows (POS tags) in the last entry of best_probs and find the row (POS tag) with the maximum value. Convert the unique integer ID to a tag (a string representation) using the list states.\nReferring to the three-word corpus described above:\n\nz[2] = 28: For the word ‚Äòupward‚Äô at position 2 in the corpus, the POS tag ID is 28. Store 28 in z at position 2.\nstates[28] is ‚ÄòRB‚Äô: The POS tag ID 28 refers to the POS tag ‚ÄòRB‚Äô.\npred[2] = 'RB': In array pred, store the POS tag for the word ‚Äòupward‚Äô.\n\nIn Step 2:\n\nStarting at the last column of best_paths, use best_probs to find the most likely POS tag for the last word in the corpus.\nThen use best_paths to find the most likely POS tag for the previous word.\nUpdate the POS tag for each word in z and in preds.\n\nReferring to the three-word example from above, read best_paths at column 2 and fill in z at position 1.\nz[1] = best_paths[z[2],2]\nThe small test following the routine prints the last few words of the corpus and their states to aid in debug.\n\n# UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: viterbi_backward\ndef viterbi_backward(best_probs, best_paths, corpus, states):\n    '''\n    This function returns the best path.\n    \n    '''\n    # Get the number of words in the corpus\n    # which is also the number of columns in best_probs, best_paths\n    m = best_paths.shape[1] \n    \n    # Initialize array z, same length as the corpus\n    z = [None] * m\n    \n    # Get the number of unique POS tags\n    num_tags = best_probs.shape[0]\n    \n    # Initialize the best probability for the last word\n    best_prob_for_last_word = float('-inf')\n    \n    # Initialize pred array, same length as corpus\n    pred = [None] * m\n    \n    ### START CODE HERE (Replace instances of 'None' with your code) ###\n    ## Step 1 ##\n    \n    # Go through each POS tag for the last word (last column of best_probs)\n    # in order to find the row (POS tag integer ID) \n    # with highest probability for the last word\n    for k in None: # complete this line\n\n        # If the probability of POS tag at row k \n        # is better than the previously best probability for the last word:\n        if None: # complete this line\n            \n            # Store the new best probability for the lsat word\n            best_prob_for_last_word = None\n    \n            # Store the unique integer ID of the POS tag\n            # which is also the row number in best_probs\n            z[m - 1] = None\n            \n    # Convert the last word's predicted POS tag\n    # from its unique integer ID into the string representation\n    # using the 'states' dictionary\n    # store this in the 'pred' array for the last word\n    pred[m - 1] = None\n    \n    ## Step 2 ##\n    # Find the best POS tags by walking backward through the best_paths\n    # From the last word in the corpus to the 0th word in the corpus\n    for i in range(None, None, None): # complete this line\n        \n        # Retrieve the unique integer ID of\n        # the POS tag for the word at position 'i' in the corpus\n        pos_tag_for_word_i = None\n        \n        # In best_paths, go to the row representing the POS tag of word i\n        # and the column representing the word's position in the corpus\n        # to retrieve the predicted POS for the word at position i-1 in the corpus\n        z[i - 1] = None\n        \n        # Get the previous word's POS tag in string form\n        # Use the 'states' dictionary, \n        # where the key is the unique integer ID of the POS tag,\n        # and the value is the string representation of that POS tag\n        pred[i - 1] = None\n        \n     ### END CODE HERE ###\n    return pred\n\n\n# Run and test your function\npred = viterbi_backward(best_probs, best_paths, prep, states)\nm=len(pred)\nprint('The prediction for pred[-7:m-1] is: \\n', prep[-7:m-1], \"\\n\", pred[-7:m-1], \"\\n\")\nprint('The prediction for pred[0:8] is: \\n', pred[0:7], \"\\n\", prep[0:7])\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[132], line 2\n      1 # Run and test your function\n----&gt; 2 pred = viterbi_backward(best_probs, best_paths, prep, states)\n      3 m=len(pred)\n      4 print('The prediction for pred[-7:m-1] is: \\n', prep[-7:m-1], \"\\n\", pred[-7:m-1], \"\\n\")\n\nNameError: name 'best_probs' is not defined\n\n\n\nExpected Output:\nThe prediction for pred[-7:m-1] is:  \n ['see', 'them', 'here', 'with', 'us', '.']  \n ['VB', 'PRP', 'RB', 'IN', 'PRP', '.']   \nThe prediction for pred[0:8] is:    \n ['DT', 'NN', 'POS', 'NN', 'MD', 'VB', 'VBN']   \n ['The', 'economy', \"'s\", 'temperature', 'will', 'be', 'taken'] \nNow we just have to compare the predicted labels to the true labels to evaluate your model on the accuracy metric!",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "A2 - POS tagging"
    ]
  },
  {
    "objectID": "posts/c1w3/lab02.html",
    "href": "posts/c1w3/lab02.html",
    "title": "Manipulating word embeddings",
    "section": "",
    "text": "Figure¬†1: course banner\nIn this week‚Äôs assignment, you are going to use a pre-trained word embedding for finding word analogies and equivalence. This exercise can be used as an Intrinsic Evaluation for the word embedding performance. In this notebook, you will apply linear algebra operations using NumPy to find analogies between words manually. This will help you to prepare for this week‚Äôs assignment.\nimport pandas as pd # Library for Dataframes \nimport numpy as np # Library for math functions\nimport pickle # Python object serialization library. Not secure\n\nword_embeddings = pickle.load( open( \"./data/word_embeddings_subset.p\", \"rb\" ) )\nlen(word_embeddings) # there should be 243 words that will be used in this assignment\n\n243\nNow that the model is loaded, we can take a look at the word representations. First, note that word_embeddings is a dictionary. Each word is the key to the entry, and the value is its corresponding vector presentation. Remember that square brackets allow access to any entry if the key exists.\ncountryVector = word_embeddings['country'] # Get the vector representation for the word 'country'\nprint(type(countryVector)) # Print the type of the vector. Note it is a numpy array\nprint(countryVector) # Print the values of the vector.  \n\n&lt;class 'numpy.ndarray'&gt;\n[-0.08007812  0.13378906  0.14355469  0.09472656 -0.04736328 -0.02355957\n -0.00854492 -0.18652344  0.04589844 -0.08154297 -0.03442383 -0.11621094\n  0.21777344 -0.10351562 -0.06689453  0.15332031 -0.19335938  0.26367188\n -0.13671875 -0.05566406  0.07470703 -0.00070953  0.09375    -0.14453125\n  0.04296875 -0.01916504 -0.22558594 -0.12695312 -0.0168457   0.05224609\n  0.0625     -0.1484375  -0.01965332  0.17578125  0.10644531 -0.04760742\n -0.10253906 -0.28515625  0.10351562  0.20800781 -0.07617188 -0.04345703\n  0.08642578  0.08740234  0.11767578  0.20996094 -0.07275391  0.1640625\n -0.01135254  0.0025177   0.05810547 -0.03222656  0.06884766  0.046875\n  0.10107422  0.02148438 -0.16210938  0.07128906 -0.16210938  0.05981445\n  0.05102539 -0.05566406  0.06787109 -0.03759766  0.04345703 -0.03173828\n -0.03417969 -0.01116943  0.06201172 -0.08007812 -0.14941406  0.11914062\n  0.02575684  0.00302124  0.04711914 -0.17773438  0.04101562  0.05541992\n  0.00598145  0.03027344 -0.07666016 -0.109375    0.02832031 -0.10498047\n  0.0100708  -0.03149414 -0.22363281 -0.03125    -0.01147461  0.17285156\n  0.08056641 -0.10888672 -0.09570312 -0.21777344 -0.07910156 -0.10009766\n  0.06396484 -0.11962891  0.18652344 -0.02062988 -0.02172852  0.29296875\n -0.00793457  0.0324707  -0.15136719  0.00227356 -0.03540039 -0.13378906\n  0.0546875  -0.03271484 -0.01855469 -0.10302734 -0.13378906  0.11425781\n  0.16699219  0.01361084 -0.02722168 -0.2109375   0.07177734  0.08691406\n -0.09960938  0.01422119 -0.18261719  0.00741577  0.01965332  0.00738525\n -0.03271484 -0.15234375 -0.26367188 -0.14746094  0.03320312 -0.03344727\n -0.01000977  0.01855469  0.00183868 -0.10498047  0.09667969  0.07910156\n  0.11181641  0.13085938 -0.08740234 -0.1328125   0.05004883  0.19824219\n  0.0612793   0.16210938  0.06933594  0.01281738  0.01550293  0.01531982\n  0.11474609  0.02758789  0.13769531 -0.08349609  0.01123047 -0.20507812\n -0.12988281 -0.16699219  0.20410156 -0.03588867 -0.10888672  0.0534668\n  0.15820312 -0.20410156  0.14648438 -0.11572266  0.01855469 -0.13574219\n  0.24121094  0.12304688 -0.14550781  0.17578125  0.11816406 -0.30859375\n  0.10888672 -0.22363281  0.19335938 -0.15722656 -0.07666016 -0.09082031\n -0.19628906 -0.23144531 -0.09130859 -0.14160156  0.06347656  0.03344727\n -0.03369141  0.06591797  0.06201172  0.3046875   0.16796875 -0.11035156\n -0.03833008 -0.02563477 -0.09765625  0.04467773 -0.0534668   0.11621094\n -0.15039062 -0.16308594 -0.15527344  0.04638672  0.11572266 -0.06640625\n -0.04516602  0.02331543 -0.08105469 -0.0255127  -0.07714844  0.0016861\n  0.15820312  0.00994873 -0.06445312  0.15722656 -0.03112793  0.10644531\n -0.140625    0.23535156 -0.11279297  0.16015625  0.00061798 -0.1484375\n  0.02307129 -0.109375    0.05444336 -0.14160156  0.11621094  0.03710938\n  0.14746094 -0.04199219 -0.01391602 -0.03881836  0.02783203  0.10205078\n  0.07470703  0.20898438 -0.04223633 -0.04150391 -0.00588989 -0.14941406\n -0.04296875 -0.10107422 -0.06176758  0.09472656  0.22265625 -0.02307129\n  0.04858398 -0.15527344 -0.02282715 -0.04174805  0.16699219 -0.09423828\n  0.14453125  0.11132812  0.04223633 -0.16699219  0.10253906  0.16796875\n  0.12597656 -0.11865234 -0.0213623  -0.08056641  0.24316406  0.15527344\n  0.16503906  0.00854492 -0.12255859  0.08691406 -0.11914062 -0.02941895\n  0.08349609 -0.03100586  0.13964844 -0.05151367  0.00765991 -0.04443359\n -0.04980469 -0.03222656 -0.00952148 -0.10888672 -0.10302734 -0.15722656\n  0.19335938  0.04858398  0.015625   -0.08105469 -0.11621094 -0.01989746\n  0.05737305  0.06103516 -0.14550781  0.06738281 -0.24414062 -0.07714844\n  0.04760742 -0.07519531 -0.14941406 -0.04418945  0.09716797  0.06738281]\nIt is important to note that we store each vector as a NumPy array. It allows us to use the linear algebra operations on it.\nThe vectors have a size of 300, while the vocabulary size of Google News is around 3 million words!\n#Get the vector for a given word:\ndef vec(w):\n    return word_embeddings[w]",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "L2 Manipulating word embeddings"
    ]
  },
  {
    "objectID": "posts/c1w3/lab02.html#operating-on-word-embeddings",
    "href": "posts/c1w3/lab02.html#operating-on-word-embeddings",
    "title": "Manipulating word embeddings",
    "section": "Operating on word embeddings",
    "text": "Operating on word embeddings\nRemember that understanding the data is one of the most critical steps in Data Science. Word embeddings are the result of machine learning processes and will be part of the input for further processes. These word embedding needs to be validated or at least understood because the performance of the derived model will strongly depend on its quality.\nWord embeddings are multidimensional arrays, usually with hundreds of attributes that pose a challenge for its interpretation.\nIn this notebook, we will visually inspect the word embedding of some words using a pair of attributes. Raw attributes are not the best option for the creation of such charts but will allow us to illustrate the mechanical part in Python.\nIn the next cell, we make a beautiful plot for the word embeddings of some words. Even if plotting the dots gives an idea of the words, the arrow representations help to visualize the vector‚Äôs alignment as well.\n\nimport matplotlib.pyplot as plt # Import matplotlib\n%matplotlib inline\n\nwords = ['oil', 'gas', 'happy', 'sad', 'city', 'town', 'village', 'country', 'continent', 'petroleum', 'joyful']\n\nbag2d = np.array([vec(word) for word in words]) # Convert each word to its vector representation\n\nfig, ax = plt.subplots(figsize = (10, 10)) # Create custom size image\n\ncol1 = 3 # Select the column for the x axis\ncol2 = 2 # Select the column for the y axis\n\n# Print an arrow for each word\nfor word in bag2d:\n    ax.arrow(0, 0, word[col1], word[col2], head_width=0.005, head_length=0.005, fc='r', ec='r', width = 1e-5)\n\n    \nax.scatter(bag2d[:, col1], bag2d[:, col2]); # Plot a dot for each word\n\n# Add the word label over each dot in the scatter plot\nfor i in range(0, len(words)):\n    ax.annotate(words[i], (bag2d[i, col1], bag2d[i, col2]))\n\n\nplt.show()\n\nText(0.06396484375, -0.279296875, 'oil')\n\n\nText(0.01080322265625, -0.138671875, 'gas')\n\n\nText(0.025390625, 0.00160980224609375, 'happy')\n\n\nText(-0.044677734375, 0.06689453125, 'sad')\n\n\nText(-0.0400390625, 0.18359375, 'city')\n\n\nText(-0.1611328125, 0.030029296875, 'town')\n\n\nText(-0.2158203125, 0.09033203125, 'village')\n\n\nText(0.0947265625, 0.1435546875, 'country')\n\n\nText(0.1279296875, 0.2392578125, 'continent')\n\n\nText(0.10888671875, -0.22265625, 'petroleum')\n\n\nText(0.083984375, 0.1298828125, 'joyful')\n\n\n\n\n\n\n\n\n\nNote that similar words like ‚Äòvillage‚Äô and ‚Äòtown‚Äô or ‚Äòpetroleum‚Äô, ‚Äòoil‚Äô, and ‚Äògas‚Äô tend to point in the same direction. Also, note that ‚Äòsad‚Äô and ‚Äòhappy‚Äô looks close to each other; however, the vectors point in opposite directions.\nIn this chart, one can figure out the angles and distances between the words. Some words are close in both kinds of distance metrics.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "L2 Manipulating word embeddings"
    ]
  },
  {
    "objectID": "posts/c1w3/lab02.html#word-distance",
    "href": "posts/c1w3/lab02.html#word-distance",
    "title": "Manipulating word embeddings",
    "section": "Word distance",
    "text": "Word distance\nNow plot the words ‚Äòsad‚Äô, ‚Äòhappy‚Äô, ‚Äòtown‚Äô, and ‚Äòvillage‚Äô. In this same chart, display the vector from ‚Äòvillage‚Äô to ‚Äòtown‚Äô and the vector from ‚Äòsad‚Äô to ‚Äòhappy‚Äô. Let us use NumPy for these linear algebra operations.\n\nwords = ['sad', 'happy', 'town', 'village']\n\nbag2d = np.array([vec(word) for word in words]) # Convert each word to its vector representation\n\nfig, ax = plt.subplots(figsize = (10, 10)) # Create custom size image\n\ncol1 = 3 # Select the column for the x axe\ncol2 = 2 # Select the column for the y axe\n\n# Print an arrow for each word\nfor word in bag2d:\n    ax.arrow(0, 0, word[col1], word[col2], head_width=0.0005, head_length=0.0005, fc='r', ec='r', width = 1e-5)\n    \n# print the vector difference between village and town\nvillage = vec('village')\ntown = vec('town')\ndiff = town - village\nax.arrow(village[col1], village[col2], diff[col1], diff[col2], fc='b', ec='b', width = 1e-5)\n\n# print the vector difference between village and town\nsad = vec('sad')\nhappy = vec('happy')\ndiff = happy - sad\nax.arrow(sad[col1], sad[col2], diff[col1], diff[col2], fc='b', ec='b', width = 1e-5)\n\n\nax.scatter(bag2d[:, col1], bag2d[:, col2]); # Plot a dot for each word\n\n# Add the word label over each dot in the scatter plot\nfor i in range(0, len(words)):\n    ax.annotate(words[i], (bag2d[i, col1], bag2d[i, col2]))\n\n\nplt.show()\n\nText(-0.044677734375, 0.06689453125, 'sad')\n\n\nText(0.025390625, 0.00160980224609375, 'happy')\n\n\nText(-0.1611328125, 0.030029296875, 'town')\n\n\nText(-0.2158203125, 0.09033203125, 'village')",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "L2 Manipulating word embeddings"
    ]
  },
  {
    "objectID": "posts/c1w3/lab02.html#linear-algebra-on-word-embeddings",
    "href": "posts/c1w3/lab02.html#linear-algebra-on-word-embeddings",
    "title": "Manipulating word embeddings",
    "section": "Linear algebra on word embeddings",
    "text": "Linear algebra on word embeddings\nIn the lectures, we saw the analogies between words using algebra on word embeddings. Let us see how to do it in Python with Numpy.\nTo start, get the norm of a word in the word embedding.\n\nprint(np.linalg.norm(vec('town'))) # Print the norm of the word town\nprint(np.linalg.norm(vec('sad'))) # Print the norm of the word sad\n\n2.3858097\n2.9004838",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "L2 Manipulating word embeddings"
    ]
  },
  {
    "objectID": "posts/c1w3/lab02.html#predicting-capitals",
    "href": "posts/c1w3/lab02.html#predicting-capitals",
    "title": "Manipulating word embeddings",
    "section": "Predicting capitals",
    "text": "Predicting capitals\nNow, applying vector difference and addition, one can create a vector representation for a new word. For example, we can say that the vector difference between ‚ÄòFrance‚Äô and ‚ÄòParis‚Äô represents the concept of Capital.\nOne can move from the city of Madrid in the direction of the concept of Capital, and obtain something close to the corresponding country to which Madrid is the Capital.\n\ncapital = vec('France') - vec('Paris')\ncountry = vec('Madrid') + capital\n\nprint(country[0:5]) # Print the first 5 values of the vector\n\n[-0.02905273 -0.2475586   0.53952026  0.20581055 -0.14862823]\n\n\nWe can observe that the vector ‚Äòcountry‚Äô that we expected to be the same as the vector for Spain is not exactly it.\n\ndiff = country - vec('Spain')\nprint(diff[0:10])\n\n[-0.06054688 -0.06494141  0.37643433  0.08129883 -0.13007355 -0.00952148\n -0.03417969 -0.00708008  0.09790039 -0.01867676]\n\n\nSo, we have to look for the closest words in the embedding that matches the candidate country. If the word embedding works as expected, the most similar word must be ‚ÄòSpain‚Äô. Let us define a function that helps us to do it. We will store our word embedding as a DataFrame, which facilitate the lookup operations based on the numerical vectors.\n\n# Create a dataframe out of the dictionary embedding. This facilitate the algebraic operations\nkeys = word_embeddings.keys()\ndata = []\nfor key in keys:\n    data.append(word_embeddings[key])\n\nembedding = pd.DataFrame(data=data, index=keys)\n# Define a function to find the closest word to a vector:\ndef find_closest_word(v, k = 1):\n    # Calculate the vector difference from each word to the input vector\n    diff = embedding.values - v \n    # Get the norm of each difference vector. \n    # It means the squared euclidean distance from each word to the input vector\n    delta = np.sum(diff * diff, axis=1)\n    # Find the index of the minimun distance in the array\n    i = np.argmin(delta)\n    # Return the row name for this item\n    return embedding.iloc[i].name\n\n\n# Print some rows of the embedding as a Dataframe\nembedding.head(10)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n\n\n\n\ncountry\n-0.080078\n0.133789\n0.143555\n0.094727\n-0.047363\n-0.023560\n-0.008545\n-0.186523\n0.045898\n-0.081543\n...\n-0.145508\n0.067383\n-0.244141\n-0.077148\n0.047607\n-0.075195\n-0.149414\n-0.044189\n0.097168\n0.067383\n\n\ncity\n-0.010071\n0.057373\n0.183594\n-0.040039\n-0.029785\n-0.079102\n0.071777\n0.013306\n-0.143555\n0.011292\n...\n0.024292\n-0.168945\n-0.062988\n0.117188\n-0.020508\n0.030273\n-0.247070\n-0.122559\n0.076172\n-0.234375\n\n\nChina\n-0.073242\n0.135742\n0.108887\n0.083008\n-0.127930\n-0.227539\n0.151367\n-0.045654\n-0.065430\n0.034424\n...\n0.140625\n0.087402\n0.152344\n0.079590\n0.006348\n-0.037842\n-0.183594\n0.137695\n0.093750\n-0.079590\n\n\nIraq\n0.191406\n0.125000\n-0.065430\n0.060059\n-0.285156\n-0.102539\n0.117188\n-0.351562\n-0.095215\n0.200195\n...\n-0.100586\n-0.077148\n-0.123047\n0.193359\n-0.153320\n0.089355\n-0.173828\n-0.054688\n0.302734\n0.105957\n\n\noil\n-0.139648\n0.062256\n-0.279297\n0.063965\n0.044434\n-0.154297\n-0.184570\n-0.498047\n0.047363\n0.110840\n...\n-0.195312\n-0.345703\n0.217773\n-0.091797\n0.051025\n0.061279\n0.194336\n0.204102\n0.235352\n-0.051025\n\n\ntown\n0.123535\n0.159180\n0.030029\n-0.161133\n0.015625\n0.111816\n0.039795\n-0.196289\n-0.039307\n0.067871\n...\n-0.007935\n-0.091797\n-0.265625\n0.029297\n0.089844\n-0.049805\n-0.202148\n-0.079590\n0.068848\n-0.164062\n\n\nCanada\n-0.136719\n-0.154297\n0.269531\n0.273438\n0.086914\n-0.076172\n-0.018677\n0.006256\n0.077637\n-0.211914\n...\n0.105469\n0.030762\n-0.039307\n0.183594\n-0.117676\n0.191406\n0.074219\n0.020996\n0.285156\n-0.257812\n\n\nLondon\n-0.267578\n0.092773\n-0.238281\n0.115234\n-0.006836\n0.221680\n-0.251953\n-0.055420\n0.020020\n0.149414\n...\n-0.008667\n-0.008484\n-0.053223\n0.197266\n-0.296875\n0.064453\n0.091797\n0.058350\n0.022583\n-0.101074\n\n\nEngland\n-0.198242\n0.115234\n0.062500\n-0.058350\n0.226562\n0.045898\n-0.062256\n-0.202148\n0.080566\n0.021606\n...\n0.135742\n0.109375\n-0.121582\n0.008545\n-0.171875\n0.086914\n0.070312\n0.003281\n0.069336\n0.056152\n\n\nAustralia\n0.048828\n-0.194336\n-0.041504\n0.084473\n-0.114258\n-0.208008\n-0.164062\n-0.269531\n0.079102\n0.275391\n...\n0.021118\n0.171875\n0.042236\n0.221680\n-0.239258\n-0.106934\n0.030884\n0.006622\n0.051270\n-0.135742\n\n\n\n\n10 rows √ó 300 columns\n\n\n\nNow let us find the name that corresponds to our numerical country:\n\nfind_closest_word(country)\n\n'Spain'",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "L2 Manipulating word embeddings"
    ]
  },
  {
    "objectID": "posts/c1w3/lab02.html#predicting-other-countries",
    "href": "posts/c1w3/lab02.html#predicting-other-countries",
    "title": "Manipulating word embeddings",
    "section": "Predicting other Countries",
    "text": "Predicting other Countries\n\nfind_closest_word(vec('Italy') - vec('Rome') + vec('Madrid'))\n\n'Spain'\n\n\n\nprint(find_closest_word(vec('Berlin') + capital))\nprint(find_closest_word(vec('Beijing') + capital))\n\nGermany\nChina\n\n\nHowever, it does not always work.\n\nprint(find_closest_word(vec('Lisbon') + capital))\n\nLisbon",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "L2 Manipulating word embeddings"
    ]
  },
  {
    "objectID": "posts/c1w3/lab02.html#represent-a-sentence-as-a-vector",
    "href": "posts/c1w3/lab02.html#represent-a-sentence-as-a-vector",
    "title": "Manipulating word embeddings",
    "section": "Represent a sentence as a vector",
    "text": "Represent a sentence as a vector\nA whole sentence can be represented as a vector by summing all the word vectors that conform to the sentence. Let us see.\n\ndoc = \"Spain petroleum city king\"\nvdoc = [vec(x) for x in doc.split(\" \")]\ndoc2vec = np.sum(vdoc, axis = 0)\ndoc2vec\n\narray([ 2.87475586e-02,  1.03759766e-01,  1.32629395e-01,  3.33007812e-01,\n       -2.61230469e-02, -5.95703125e-01, -1.25976562e-01, -1.01306152e+00,\n       -2.18544006e-01,  6.60705566e-01, -2.58300781e-01, -2.09960938e-02,\n       -7.71484375e-02, -3.07128906e-01, -5.94726562e-01,  2.00561523e-01,\n       -1.04980469e-02, -1.10748291e-01,  4.82177734e-02,  6.38977051e-01,\n        2.36083984e-01, -2.69775391e-01,  3.90625000e-02,  4.16503906e-01,\n        2.83416748e-01, -7.25097656e-02, -3.12988281e-01,  1.05712891e-01,\n        3.22265625e-02,  2.38403320e-01,  3.88183594e-01, -7.51953125e-02,\n       -1.26281738e-01,  6.60644531e-01, -7.89794922e-01, -7.04345703e-02,\n       -1.14379883e-01, -4.78515625e-02,  4.76318359e-01,  5.31127930e-01,\n        8.10546875e-02, -1.17553711e-01,  1.02050781e+00,  5.59814453e-01,\n       -1.17187500e-01,  1.21826172e-01, -5.51574707e-01,  1.44531250e-01,\n       -7.66113281e-01,  5.36102295e-01, -2.80029297e-01,  3.85986328e-01,\n       -2.39135742e-01, -2.86865234e-02, -5.10498047e-01,  2.59658813e-01,\n       -7.52929688e-01,  4.32128906e-02, -7.17773438e-02, -1.26708984e-01,\n        4.40673828e-02,  5.12939453e-01, -5.15808105e-01,  1.20117188e-01,\n       -5.52978516e-02, -3.92089844e-01, -3.15917969e-01,  1.57226562e-01,\n       -3.19702148e-01,  1.75170898e-01, -3.81835938e-01, -2.07031250e-01,\n       -4.72717285e-02, -2.79296875e-01, -3.29040527e-01, -1.69067383e-01,\n        1.61132812e-02,  1.71569824e-01,  5.73730469e-02, -2.44140625e-03,\n        8.34960938e-02, -1.58203125e-01, -3.10119629e-01,  5.28564453e-02,\n        8.60595703e-02,  5.12695312e-02, -7.22900391e-01,  4.97924805e-01,\n       -5.85937500e-03,  4.49951172e-01,  3.82446289e-01, -2.80029297e-01,\n       -3.28125000e-01, -6.27441406e-02, -4.81933594e-01,  1.93176270e-02,\n       -1.69326782e-01, -4.28649902e-01,  5.39062500e-01, -1.28417969e-01,\n       -8.83789062e-02,  5.13916016e-01,  9.13085938e-02, -1.60156250e-01,\n        6.86035156e-02, -9.74121094e-02, -3.70712280e-01, -3.27270508e-01,\n        1.77978516e-01, -4.65332031e-01,  1.70410156e-01,  9.08203125e-02,\n        2.76857376e-01, -1.69677734e-01,  3.27728271e-01, -3.12500000e-02,\n       -2.20809937e-01, -3.46679688e-01,  4.67407227e-01,  5.31860352e-01,\n       -1.30615234e-01, -2.36816406e-02, -6.56250000e-01, -5.79589844e-01,\n       -2.05810547e-01, -3.03222656e-01,  1.94259644e-01, -7.28515625e-01,\n       -4.92522240e-01, -5.37109375e-01, -3.47656250e-01,  1.08642578e-01,\n       -1.41601562e-01, -2.07031250e-01,  2.52441406e-01, -7.78808594e-02,\n       -5.02441406e-01,  1.53808594e-02,  8.64257812e-02,  2.59765625e-01,\n        6.64062500e-02, -7.12890625e-01, -1.45751953e-01,  7.56835938e-03,\n        4.87792969e-01,  1.39160156e-01,  1.15722656e-01,  1.28662109e-01,\n       -4.75585938e-01,  2.21191406e-01,  3.25317383e-01,  1.06323242e-01,\n       -6.11083984e-01, -3.59619141e-01,  6.54296875e-02, -2.41699219e-01,\n       -6.29882812e-02, -1.62109375e-01,  4.26269531e-01, -4.38354492e-01,\n        1.93725586e-01,  4.89562988e-01,  5.31494141e-01, -7.29370117e-02,\n        1.77246094e-01,  9.39941406e-02,  2.92236328e-01, -2.74047852e-01,\n        2.63366699e-02,  4.36035156e-01, -3.76953125e-01,  3.10546875e-01,\n        4.87304688e-01, -2.43041992e-01,  1.21612549e-02, -3.80371094e-01,\n        3.80493164e-01, -6.22436523e-01, -3.98071289e-01,  1.24206543e-01,\n       -8.20312500e-01, -2.72583008e-01, -6.21582031e-01, -4.87060547e-01,\n        3.06671143e-01, -2.61230469e-01,  5.12451172e-01,  5.55694580e-01,\n        5.66894531e-01,  7.33886719e-01, -1.75781250e-01,  4.13574219e-01,\n       -2.54272461e-01,  1.32507324e-01, -4.78515625e-01,  4.63256836e-01,\n       -6.21948242e-02, -1.80664062e-01, -5.46386719e-01, -6.31103516e-01,\n       -1.47949219e-01, -3.15185547e-01, -7.12890625e-02, -7.67578125e-01,\n        3.92272949e-01, -1.97753906e-01,  2.23144531e-01, -5.07324219e-01,\n        8.39843750e-02, -4.98657227e-02,  1.01074219e-01,  2.07885742e-01,\n       -2.77343750e-01,  1.03027344e-01, -1.38671875e-01,  2.87353516e-01,\n       -4.81895447e-01, -1.66748047e-01, -1.47277832e-01,  3.61633301e-01,\n        6.38504028e-02, -6.69189453e-01,  1.95312500e-03, -7.34375000e-01,\n       -1.28158569e-01,  9.76562500e-04, -7.08007812e-02,  3.72558594e-01,\n        8.31176758e-01,  5.94482422e-01,  5.37109375e-02, -3.00140381e-01,\n       -4.53857422e-01,  1.11511230e-01, -1.32812500e-01,  1.25732422e-01,\n        3.39843750e-01, -2.48352051e-01, -1.62353516e-02, -2.84667969e-01,\n        4.70703125e-01, -4.48242188e-01,  8.50753784e-02,  2.69042969e-01,\n        3.98254395e-03, -3.53759766e-01, -3.90625000e-02, -3.22753906e-01,\n       -6.90917969e-02, -4.13818359e-02,  1.35314941e-01, -8.50396156e-02,\n        1.28417969e-01,  6.15966797e-01,  3.55957031e-01, -6.05468750e-02,\n       -2.25463867e-01, -2.62207031e-01, -2.72949219e-01, -5.16113281e-01,\n        1.59179688e-01,  2.74902344e-01, -7.61718750e-02, -3.41796875e-03,\n        4.37500000e-01,  2.98583984e-01, -4.40795898e-01, -3.43261719e-01,\n        1.73583984e-01,  3.32092285e-01, -2.12646484e-01,  5.76171875e-01,\n        2.06787109e-01, -7.91015625e-02,  5.79695702e-02, -1.01806641e-01,\n       -7.06787109e-01, -3.40576172e-02, -4.11865234e-01,  9.82666016e-02,\n       -1.70410156e-01, -4.18212891e-01,  8.39233398e-01, -1.15722656e-01,\n        1.28173828e-01, -2.07763672e-01, -4.08203125e-01, -1.77612305e-01,\n        1.01196289e-01,  4.24072266e-01, -5.26428223e-02, -5.58593750e-01,\n        1.12304688e-02, -1.12060547e-01, -9.42382812e-02,  2.35595703e-02,\n       -3.92578125e-01, -7.12890625e-02,  5.69824219e-01,  9.81445312e-02],\n      dtype=float32)\n\n\n\nfind_closest_word(doc2vec)\n\n'petroleum'\n\n\nCongratulations! You have finished the introduction to word embeddings manipulation!",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "L2 Manipulating word embeddings"
    ]
  },
  {
    "objectID": "posts/c1w3/assignment.html",
    "href": "posts/c1w3/assignment.html",
    "title": "Assignment 3: Hello Vectors",
    "section": "",
    "text": "Figure¬†1: course banner\nWelcome to this week‚Äôs programming assignment on exploring word vectors. In natural language processing, we represent each word as a vector consisting of numbers. The vector encodes the meaning of the word. These numbers (or weights) for each word are learned using various machine learning models, which we will explore in more detail later in this specialization. Rather than make you code the machine learning models from scratch, we will show you how to use them. In the real world, you can always load the trained word vectors, and you will almost never have to train them from scratch. In this assignment, you will:",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "A3 Hello Vectors"
    ]
  },
  {
    "objectID": "posts/c1w3/assignment.html#predict-the-countries-from-capitals",
    "href": "posts/c1w3/assignment.html#predict-the-countries-from-capitals",
    "title": "Assignment 3: Hello Vectors",
    "section": "1.0 Predict the Countries from Capitals",
    "text": "1.0 Predict the Countries from Capitals\nIn the lectures, we have illustrated the word analogies by finding the capital of a country from the country. We have changed the problem a bit in this part of the assignment. You are asked to predict the countries that corresponds to some capitals. You are playing trivia against some second grader who just took their geography test and knows all the capitals by heart. Thanks to NLP, you will be able to answer the questions properly. In other words, you will write a program that can give you the country by its capital. That way you are pretty sure you will win the trivia game. We will start by exploring the data set.\n\n\n\nmap\n\n\n\n1.1 Importing the data\nAs usual, you start by importing some essential Python libraries and then load the dataset. The dataset will be loaded as a Pandas DataFrame, which is very a common method in data science. This may take a few minutes because of the large size of the data.\n\n# Run this cell to import packages.\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom utils import get_vectors\n\n\ndata = pd.read_csv('capitals.txt', delimiter=' ')\ndata.columns = ['city1', 'country1', 'city2', 'country2']\n\n# print first five elements in the DataFrame\ndata.head(5)\n\n\n\n\n\n\n\n\ncity1\ncountry1\ncity2\ncountry2\n\n\n\n\n0\nAthens\nGreece\nBangkok\nThailand\n\n\n1\nAthens\nGreece\nBeijing\nChina\n\n\n2\nAthens\nGreece\nBerlin\nGermany\n\n\n3\nAthens\nGreece\nBern\nSwitzerland\n\n\n4\nAthens\nGreece\nCairo\nEgypt\n\n\n\n\n\n\n\n\n\n\nTo Run This Code On Your Own Machine:\nNote that because the original google news word embedding dataset is about 3.64 gigabytes, the workspace is not able to handle the full file set. So we‚Äôve downloaded the full dataset, extracted a sample of the words that we‚Äôre going to analyze in this assignment, and saved it in a pickle file called word_embeddings_capitals.p\nIf you want to download the full dataset on your own and choose your own set of word embeddings, please see the instructions and some helper code.\n\nDownload the dataset from this page.\nSearch in the page for ‚ÄòGoogleNews-vectors-negative300.bin.gz‚Äô and click the link to download.\n\nCopy-paste the code below and run it on your local machine after downloading the dataset to the same directory as the notebook.\nimport nltk\nfrom gensim.models import KeyedVectors\n\n\nembeddings = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary = True)\nf = open('capitals.txt', 'r').read()\nset_words = set(nltk.word_tokenize(f))\nselect_words = words = ['king', 'queen', 'oil', 'gas', 'happy', 'sad', 'city', 'town', 'village', 'country', 'continent', 'petroleum', 'joyful']\nfor w in select_words:\n    set_words.add(w)\n\ndef get_word_embeddings(embeddings):\n\n    word_embeddings = {}\n    for word in embeddings.vocab:\n        if word in set_words:\n            word_embeddings[word] = embeddings[word]\n    return word_embeddings\n\n\n# Testing your function\nword_embeddings = get_word_embeddings(embeddings)\nprint(len(word_embeddings))\npickle.dump( word_embeddings, open( \"word_embeddings_subset.p\", \"wb\" ) )\n\nNow we will load the word embeddings as a Python dictionary. As stated, these have already been obtained through a machine learning algorithm.\n\nword_embeddings = pickle.load(open(\"word_embeddings_subset.p\", \"rb\"))\nlen(word_embeddings)  # there should be 243 words that will be used in this assignment\n\n243\n\n\nEach of the word embedding is a 300-dimensional vector.\n\nprint(\"dimension: {}\".format(word_embeddings['Spain'].shape[0]))\n\ndimension: 300\n\n\n\n\nPredict relationships among words\nNow you will write a function that will use the word embeddings to predict relationships among words. * The function will take as input three words. * The first two are related to each other. * It will predict a 4th word which is related to the third word in a similar manner as the two first words are related to each other. * As an example, ‚ÄúAthens is to Greece as Bangkok is to ______‚Äù? * You will write a program that is capable of finding the fourth word. * We will give you a hint to show you how to compute this.\nA similar analogy would be the following:\n\nYou will implement a function that can tell you the capital of a country. You should use the same methodology shown in the figure above. To do this, compute you‚Äôll first compute cosine similarity metric or the Euclidean distance.\n\n\n1.2 Cosine Similarity\nThe cosine similarity function is:\n\\cos (\\theta)=\\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|}=\\frac{\\sum_{i=1}^{n} A_{i} B_{i}}{\\sqrt{\\sum_{i=1}^{n} A_{i}^{2}} \\sqrt{\\sum_{i=1}^{n} B_{i}^{2}}}\\tag{1}\nA and B represent the word vectors and A_i or B_i represent index i of that vector. & Note that if A and B are identical, you will get cos(\\theta) = 1. * Otherwise, if they are the total opposite, meaning, A= -B, then you would get cos(\\theta) = -1. * If you get cos(\\theta) =0, that means that they are orthogonal (or perpendicular). * Numbers between 0 and 1 indicate a similarity score. * Numbers between -1-0 indicate a dissimilarity score.\nInstructions: Implement a function that takes in two word vectors and computes the cosine distance.\n\n\nHints\n\n\n\n\nPython‚Äôs NumPy library  adds support for linear algebra operations (e.g., dot product, vector norm ‚Ä¶).\n\n\nUse  numpy.dot .\n\n\nUse numpy.linalg.norm .\n\n\n\n\n# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef cosine_similarity(A, B):\n    '''\n    Input:\n        A: a numpy array which corresponds to a word vector\n        B: A numpy array which corresponds to a word vector\n    Output:\n        cos: numerical number representing the cosine similarity between A and B.\n    '''\n\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    \n    dot = None\n    norma = None\n    normb = None \n    cos = None\n\n    ### END CODE HERE ###\n    return cos\n\n\n# feel free to try different words\nking = word_embeddings['king']\nqueen = word_embeddings['queen']\n\ncosine_similarity(king, queen)\n\nExpected Output:\n\\approx 0.6510956\n\n\n1.3 Euclidean distance\nYou will now implement a function that computes the similarity between two vectors using the Euclidean distance. Euclidean distance is defined as:\n \\begin{aligned} d(\\mathbf{A}, \\mathbf{B})=d(\\mathbf{B}, \\mathbf{A}) &=\\sqrt{\\left(A_{1}-B_{1}\\right)^{2}+\\left(A_{2}-B_{2}\\right)^{2}+\\cdots+\\left(A_{n}-B_{n}\\right)^{2}} \\\\ &=\\sqrt{\\sum_{i=1}^{n}\\left(A_{i}-B_{i}\\right)^{2}} \\end{aligned}\n\nn is the number of elements in the vector\nA and B are the corresponding word vectors.\nThe more similar the words, the more likely the Euclidean distance will be close to 0.\n\nInstructions: Write a function that computes the Euclidean distance between two vectors.\n\n\nHints\n\n\n\n\nUse  numpy.linalg.norm .\n\n\n\n\n# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef euclidean(A, B):\n    \"\"\"\n    Input:\n        A: a numpy array which corresponds to a word vector\n        B: A numpy array which corresponds to a word vector\n    Output:\n        d: numerical number representing the Euclidean distance between A and B.\n    \"\"\"\n\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n\n    # euclidean distance\n\n    d = None\n\n    ### END CODE HERE ###\n\n    return d\n\n\n# Test your function\neuclidean(king, queen)\n\nExpected Output:\n2.4796925\n\n\n1.4 Finding the country of each capital\nNow, you will use the previous functions to compute similarities between vectors, and use these to find the capital cities of countries. You will write a function that takes in three words, and the embeddings dictionary. Your task is to find the capital cities. For example, given the following words:\n\n1: Athens 2: Greece 3: Baghdad,\n\nyour task is to predict the country 4: Iraq.\nInstructions:\n\nTo predict the capital you might want to look at the King - Man + Woman = Queen example above, and implement that scheme into a mathematical function, using the word embeddings and a similarity function.\nIterate over the embeddings dictionary and compute the cosine similarity score between your vector and the current word embedding.\nYou should add a check to make sure that the word you return is not any of the words that you fed into your function. Return the one with the highest score.\n\n\n# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef get_country(city1, country1, city2, embeddings):\n    \"\"\"\n    Input:\n        city1: a string (the capital city of country1)\n        country1: a string (the country of capital1)\n        city2: a string (the capital city of country2)\n        embeddings: a dictionary where the keys are words and values are their embeddings\n    Output:\n        countries: a dictionary with the most likely country and its similarity score\n    \"\"\"\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n\n    # store the city1, country 1, and city 2 in a set called group\n    group = set((None, None, None))\n\n    # get embeddings of city 1\n    city1_emb = None\n\n    # get embedding of country 1\n    country1_emb = None\n\n    # get embedding of city 2\n    city2_emb = None\n\n    # get embedding of country 2 (it's a combination of the embeddings of country 1, city 1 and city 2)\n    # Remember: King - Man + Woman = Queen\n    vec = None\n\n    # Initialize the similarity to -1 (it will be replaced by a similarities that are closer to +1)\n    similarity = -1\n\n    # initialize country to an empty string\n    country = ''\n\n    # loop through all words in the embeddings dictionary\n    for word in embeddings.keys():\n\n        # first check that the word is not already in the 'group'\n        if word not in group:\n\n            # get the word embedding\n            word_emb = None\n\n            # calculate cosine similarity between embedding of country 2 and the word in the embeddings dictionary\n            cur_similarity = None\n\n            # if the cosine similarity is more similar than the previously best similarity...\n            if cur_similarity &gt; similarity:\n\n                # update the similarity to the new, better similarity\n                similarity = None\n\n                # store the country as a tuple, which contains the word and the similarity\n                country = (None, None)\n\n    ### END CODE HERE ###\n\n    return country\n\n\n# Testing your function, note to make it more robust you can return the 5 most similar words.\nget_country('Athens', 'Greece', 'Cairo', word_embeddings)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[10], line 2\n      1 # Testing your function, note to make it more robust you can return the 5 most similar words.\n----&gt; 2 get_country('Athens', 'Greece', 'Cairo', word_embeddings)\n\nCell In[9], line 49, in get_country(city1, country1, city2, embeddings)\n     46 cur_similarity = None\n     48 # if the cosine similarity is more similar than the previously best similarity...\n---&gt; 49 if cur_similarity &gt; similarity:\n     50 \n     51     # update the similarity to the new, better similarity\n     52     similarity = None\n     54     # store the country as a tuple, which contains the word and the similarity\n\nTypeError: '&gt;' not supported between instances of 'NoneType' and 'int'\n\n\n\nExpected Output:\n(‚ÄòEgypt‚Äô, 0.7626821)\n\n\n1.5 Model Accuracy\nNow you will test your new function on the dataset and check the accuracy of the model:\n\\text{Accuracy}=\\frac{\\text{Correct \\# of predictions}}{\\text{Total \\# of predictions}}\nInstructions: Write a program that can compute the accuracy on the dataset provided for you. You have to iterate over every row to get the corresponding words and feed them into you get_country function above.\n\n\nHints\n\n\n\n\nUse  pandas.DataFrame.iterrows .\n\n\n\n\n# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef get_accuracy(word_embeddings, data):\n    '''\n    Input:\n        word_embeddings: a dictionary where the key is a word and the value is its embedding\n        data: a pandas dataframe containing all the country and capital city pairs\n    \n    Output:\n        accuracy: the accuracy of the model\n    '''\n\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    # initialize num correct to zero\n    num_correct = 0\n\n    # loop through the rows of the dataframe\n    for i, row in data.iterrows():\n\n        # get city1\n        city1 = None\n\n        # get country1\n        country1 = None\n\n        # get city2\n        city2 =  None\n\n        # get country2\n        country2 = None\n\n        # use get_country to find the predicted country2\n        predicted_country2, _ = None\n\n        # if the predicted country2 is the same as the actual country2...\n        if predicted_country2 == country2:\n            # increment the number of correct by 1\n            num_correct += None\n\n    # get the number of rows in the data dataframe (length of dataframe)\n    m = len(data)\n\n    # calculate the accuracy by dividing the number correct by m\n    accuracy = None\n\n    ### END CODE HERE ###\n    return accuracy\n\nNOTE: The cell below takes about 30 SECONDS to run.\n\naccuracy = get_accuracy(word_embeddings, data)\nprint(f\"Accuracy is {accuracy:.2f}\")\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[12], line 1\n----&gt; 1 accuracy = get_accuracy(word_embeddings, data)\n      2 print(f\"Accuracy is {accuracy:.2f}\")\n\nCell In[11], line 32, in get_accuracy(word_embeddings, data)\n     29 country2 = None\n     31 # use get_country to find the predicted country2\n---&gt; 32 predicted_country2, _ = None\n     34 # if the predicted country2 is the same as the actual country2...\n     35 if predicted_country2 == country2:\n     36     # increment the number of correct by 1\n\nTypeError: cannot unpack non-iterable NoneType object\n\n\n\nExpected Output:\n\\approx 0.92",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "A3 Hello Vectors"
    ]
  },
  {
    "objectID": "posts/c1w2/index.html",
    "href": "posts/c1w2/index.html",
    "title": "Probability and Bayes Rule",
    "section": "",
    "text": "Figure¬†1: course banner\n\n\n\n\n\n\n\n\nFigure¬†2\nMy notes for Week 2 of the Natural Language Processing with Probabilistic Models Course in the Natural Language Processing Specialization Offered by DeepLearning.AI on Coursera\nThe following two results are due to (Ng and Jordan 2001) by way of Naive_Bayes_classifier wikipedia article.\nAnother detail that can help we make sense of this lesson is the following result relating Na√Øve Bayes to Logistic Regression which we covered last week. In the case of discrete inputs like indicator or frequency features for discrete events, naive Bayes classifiers form a generative-discriminative pair with multinomial logistic regression classifiers: each naive Bayes classifier can be considered a way of fitting a probability model that optimizes the joint likelihood p (C,x), while logistic regression fits the same probability model to optimize the conditional p(C ‚à£ x).",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Probability & Bayes Rule",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c1w2/index.html#probability-of-a-randomly-selected-tweets-sentiment",
    "href": "posts/c1w2/index.html#probability-of-a-randomly-selected-tweets-sentiment",
    "title": "Probability and Bayes Rule",
    "section": "Probability of a randomly selected tweet‚Äôs sentiment",
    "text": "Probability of a randomly selected tweet‚Äôs sentiment\n\nTo calculate a probability of a certain event happening, we take the count of that specific event and divide it by the sum of all events.\nFurthermore, the sum of all probabilities has to equal 1. If we pick a tweet at random, what is the probability of it being +? We define an event A: ‚ÄúA tweet is positive‚Äù and calculate its probability\n\n\nP(A) = P(+) = \\frac{N_{+}}{N}=\\frac{13}{20}=0.65\n\nAnd since probabilities add up to one:\n\nP(-) = 1- P(+)=0.35",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Probability & Bayes Rule",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c1w2/index.html#probability-for-a-specific-words-sentiment",
    "href": "posts/c1w2/index.html#probability-for-a-specific-words-sentiment",
    "title": "Probability and Bayes Rule",
    "section": "Probability for a specific word‚Äôs sentiment",
    "text": "Probability for a specific word‚Äôs sentiment\nWithin that corpus, the word happy is sometimes labeled + and in other cases, -. This indicates that some negative tweets contain the word happy. Shown below is a graphical representation of this ‚Äúoverlap‚Äù. Let‚Äôs explore how we may represent this graphically using a venn diagram and then derive a probability-based representation.\n\n\n\n\nTweets with ‚ÄúHappy‚Äù\n\nFirst, we need to estimate the probability of the event B: ‚Äútweets containing the word happy‚Äù\n\nP(B) = P(\\text{happy})=\\frac{N_\\text{happy}}{N}=\\frac{4}{20}=0.2\n\n\n\n\n\nVenn diagram for defining probabilities from events\n\nTo compute the probability of 2 events happening like happy and + in the picture we would be looking at the intersection, or overlap of the two events, In this case, the red and the blue boxes overlap in three boxes, So the answer is: \nP(A \\cap B) = P(A,B) = \\frac{2}{20}\n\nThe Event ‚ÄúA is labeled +‚Äù, - The probability of events A shown as P(A) is calculated as the ratio between the count of positive tweets and the corpus divided by the total number of tweets in the corpus.\n\n\n\n\n\n\n\n specific tweets color coded per the Venn diagram\n\n\n\n\n\n\n\n\nDefinition of conditional probability\n\n\n\nConditional probability is the probability of an outcome B when we already know for certain that an event A has already happened. Notation: \nP(B|A)\n\n\n\n\nand there more + than - more specifically our prior knowledge is that : \n  \\frac{P(+)}{P(‚àí)}=\\frac{13}{7}\n\nthe likelihood of a tweet with happy being + is\nthe challenge arises from some words being in both + and - tweets Conditional probabilities help us reduce the sample search space by restricting it to a specific event which is a given. We should understand the difference between P(A|B) and P(B|A)",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Probability & Bayes Rule",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c1w2/index.html#what-is-phappy",
    "href": "posts/c1w2/index.html#what-is-phappy",
    "title": "Probability and Bayes Rule",
    "section": "what is P(+|happy)",
    "text": "what is P(+|happy)\n\nWe start with the Venn diagram for the P(A|B). \nWhere we restricted the diagram to just A the subset of happy tweets.\nAnd we just want those tweets that are also + i.e.¬†(B).\nall we need is to plug in the counts from our count chart. \nwhich we now estimate \nP(A \\mid B) = P(Positive \\mid happy) = \\frac{3}{4} = 0.75",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Probability & Bayes Rule",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c1w2/index.html#what-is-phappy-1",
    "href": "posts/c1w2/index.html#what-is-phappy-1",
    "title": "Probability and Bayes Rule",
    "section": "what is P(happy|+)",
    "text": "what is P(happy|+)\n\nWe start with the Venn diagram for the P(B|A)\nwhere we have restricted the diagram to just B the subset of + tweets. \nand we just want from those the tweets that are also happy i.e.¬†(A).\nand the counts for P(B|A) \nwhich we now estimate \nP(B \\mid A) = P(happy \\mid Positive) = \\frac{3}{13} = 0.231",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Probability & Bayes Rule",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c1w2/index.html#na√Øve-bayes-introduction",
    "href": "posts/c1w2/index.html#na√Øve-bayes-introduction",
    "title": "Probability and Bayes Rule",
    "section": "Na√Øve Bayes Introduction",
    "text": "Na√Øve Bayes Introduction\nHere is a sample corpus\n\n\n\n\nTable¬†1: And these are the class frequencies and probabilities\n\n\n\n\n\n\n\n\n\n+ tweets\n- tweets\n\n\n\n\nI am happy because I am learning NLP\nI am sad, I am not learning NLP\n\n\nI am happy\nI am sad\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport string \nraw_tweets=[\n  \"I am happy because I am learning NLP\",\n  \"I am sad, I am not learning NLP\",\n  \"I am happy, not sad\",\n  \"I am sad, not happy\",\n]\ndef clean(tweet:str):\n  return  tweet.translate(str.maketrans('', '', string.punctuation)).lower()\ntweets = [clean(tweet) for tweet in raw_tweets]\nlabels=['+','-','+','-']\ndf = pd.DataFrame({'tweets': tweets, 'labels': labels})\ndf\n\n\n\n\n\n\n\n\ntweets\nlabels\n\n\n\n\n0\ni am happy because i am learning nlp\n+\n\n\n1\ni am sad i am not learning nlp\n-\n\n\n2\ni am happy not sad\n+\n\n\n3\ni am sad not happy\n-\n\n\n\n\n\n\n\n\nimport numpy as np\nfrom collections import Counter\np_freq,n_freq = Counter(), Counter()\n#print( df[df.labels == '+']['tweets'].to_list())\n[p_freq.update(tweet.split()) for tweet in df[df.labels == '+']['tweets'].to_list()]\n[n_freq.update(tweet.split()) for tweet in df[df.labels == '-']['tweets'].to_list()]\nprint(p_freq)\nprint(n_freq)\nvocab = list(set(p_freq.keys()).union(set(n_freq.keys())))\npos_freq = [p_freq[word] for word in vocab ]\nneg_freq = [n_freq[word] for word in vocab ]\nvocab_df=pd.DataFrame({'vocab':vocab,'pos_freq':pos_freq,'neg_freq':neg_freq})\nvocab_df['p_pos']=vocab_df.pos_freq/vocab_df.pos_freq.sum()\nvocab_df['p_neg']=vocab_df.neg_freq/vocab_df.neg_freq.sum()\nvocab_df['p_pos_sm']=(vocab_df.pos_freq+1)/(vocab_df.pos_freq.sum()+vocab_df.shape[1])\nvocab_df['p_neg_sm']=(vocab_df.neg_freq+1)/(vocab_df.neg_freq.sum()+vocab_df.shape[1])\nvocab_df['ratio']= vocab_df.p_pos_sm/vocab_df.p_neg_sm\nvocab_df['lambda']= np.log(vocab_df.p_pos_sm/vocab_df.p_neg_sm)\npd.set_option('display.float_format', '{:.2f}'.format)\nvocab_df\nprint(vocab_df.shape)\n\n[None, None]\n\n\n[None, None]\n\n\nCounter({'i': 3, 'am': 3, 'happy': 2, 'because': 1, 'learning': 1, 'nlp': 1, 'not': 1, 'sad': 1})\nCounter({'i': 3, 'am': 3, 'sad': 2, 'not': 2, 'learning': 1, 'nlp': 1, 'happy': 1})\n\n\n\n\n\n\n\n\n\nvocab\npos_freq\nneg_freq\np_pos\np_neg\np_pos_sm\np_neg_sm\nratio\nlambda\n\n\n\n\n0\nnot\n1\n2\n0.08\n0.15\n0.11\n0.16\n0.70\n-0.35\n\n\n1\ni\n3\n3\n0.23\n0.23\n0.22\n0.21\n1.06\n0.05\n\n\n2\nam\n3\n3\n0.23\n0.23\n0.22\n0.21\n1.06\n0.05\n\n\n3\nhappy\n2\n1\n0.15\n0.08\n0.17\n0.11\n1.58\n0.46\n\n\n4\nlearning\n1\n1\n0.08\n0.08\n0.11\n0.11\n1.06\n0.05\n\n\n5\nbecause\n1\n0\n0.08\n0.00\n0.11\n0.05\n2.11\n0.75\n\n\n6\nsad\n1\n2\n0.08\n0.15\n0.11\n0.16\n0.70\n-0.35\n\n\n7\nnlp\n1\n1\n0.08\n0.08\n0.11\n0.11\n1.06\n0.05\n\n\n\n\n\n\n\n(8, 9)\n\n\n\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\ntable = [[\"Sun\",696000,1989100000],\n         [\"Earth\",6371,5973.6],\n         [\"Moon\",1737,73.5],\n         [\"Mars\",3390,641.85]]\nMarkdown(tabulate(\n  table, \n  headers=[\"Planet\",\"R (km)\", \"mass (x 10^29 kg)\"]\n))\n\n\n\nTable¬†2: Planets\n\n\n\n\n\n\nPlanet\nR (km)\nmass (x 10^29 kg)\n\n\n\n\nSun\n696000\n1.9891e+09\n\n\nEarth\n6371\n5973.6\n\n\nMoon\n1737\n73.5\n\n\nMars\n3390\n641.85\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable¬†3: Frequency Table\n\n\n\n\n\nword\n+\n-\n\n\n\n\nI\n3\n3\n\n\nam\n3\n3\n\n\nhappy\n2\n1\n\n\nbecause\n1\n0\n\n\nlearning\n1\n1\n\n\nNLP\n1\n1\n\n\nsad\n1\n2\n\n\nnot\n1\n2\n\n\n\n\n\n\n\n\n\n\n\nTable¬†4: Probabilities Table\n\n\n\n\n\nword\n+\n-\n\n\n\n\nI\n0.24\n0.25\n\n\nam\n0.24\n0.25\n\n\nhappy\n0.15\n0.08\n\n\nbecause\n0.08\n0.00\n\n\nlearning\n0.08\n0.08\n\n\nNLP\n0.08\n0.08\n\n\nsad\n0.08\n0.17\n\n\nnot\n0.08\n0.17\n\n\n\n\n\n\n\n\n\nLet‚Äôs motivate the Na√Øve Bayes inference condition rule‚ÄØfor binary classification:\nTo build a classifier, we will first start by creating conditional probabilities given the table;\n\n\n\n\nNa√Øve Bayes\n\n\nWe want to find if given our prior knowledge of P(+) and P(-) if a new tweet has + or - sentiment.\nTo do that we will estimate p(+|T) and p(-|T) and then decide based on which is greater than 0.5.\n\n\n\n\n\nTable of probabilities\n\nWe can use the Bayes rule:\n\np(+|T) = \\frac{ p(T|+) \\times p(+) }{ p(T) }\n\nand\n\np(-|T) = \\frac{ p(T|-) \\times p(-) }{ p(T) }\n\nwhere:\n\np(+|T) is the posterior probability of a label + given tweet T\np(+) is our prior knowledge\np(T|+) is the likelihood of tweet T being +.\n{p(T)}\n\nThe term p(T) is in both terms and can be eliminated. However, it will cancel out when we use the ratio for the inference. This lets us compute the following table of probabilities; word am learning NLP Pos 0.24 0.08 0.08 Neg 0.25 0.08 0.08 .17 Na√Øve Bayes is the simplest probabilistic graphical model which comes with an independence assumption for the features.\n\np(T|+) = \\prod^m_{i=1}P(w_i|+) \\implies p(+|T)=\\frac{P(+)}{P(T)} \\prod^m_{i=1}P(w_i|+)\n\nand\n\np(T|‚àí) = \\prod^m_{i=1}P(w_i|‚àí) \\implies p(‚àí|T) =  \\frac{P(‚àí)}{P(T)} \\prod^m_{i=1} P(w_i|‚àí)\n\nOnce we have the probabilities, we can compute the likelihood score as follows:\nTweet: I am happy today: I am learning. - Since there is no entry for today in our conditional probabilities table, this implies that this word is not in your vocabulary. So we‚Äôll ignore its contribution to the overall score. - All the neutral words in the tweet such as I and am cancel out in the expression, as shown in the figure below.\n\n   \\prod^m_{i=1} \\frac{P(w_i|+)}{P(w_i|-)}= \\frac {0.14}{0.10} =1.4 &gt; 1\n\n\nA score greater than 1 indicates that the class is positive, otherwise, it is negative.\n\n\nP(+|T) &gt; P(‚àí|T)\n\nthen we infer that the T has + sentiment. dividing by the right term we get the inference rule:\n\n\\frac{P(+|T)}{P(‚àí|T)} &gt; 1\n which expands to : \n  \\frac {P(+|T)}{P(‚àí|T)} = \\frac {P(+)}{P(-)}\\prod^m_{i=1} \\frac {P(w_i|+)}{P(w_i|‚àí)} &gt; 1\n\nThis is the inference rule for na√Øve Bayes.\nNote: Na√Øve Bayes is a model which assumes all features are independent, so the basic component here is:\n\n\\frac{P(w_i|+)}{P(w_i|-)} &gt; 1\n the ratio of the probability that a word appears in a positive tweet and that it appears in a negative tweet",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Probability & Bayes Rule",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c1w2/index.html#additive-smoothing",
    "href": "posts/c1w2/index.html#additive-smoothing",
    "title": "Probability and Bayes Rule",
    "section": "Additive smoothing:",
    "text": "Additive smoothing:\n\np_{addative}(w_i|class)=\\frac{ freq(w,class)+\\delta}{ N_{class} + \\delta \\times V}",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Probability & Bayes Rule",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c1w2/index.html#more-alternatives-to-laplacian-smoothing",
    "href": "posts/c1w2/index.html#more-alternatives-to-laplacian-smoothing",
    "title": "Probability and Bayes Rule",
    "section": "More alternatives to Laplacian smoothing",
    "text": "More alternatives to Laplacian smoothing\n\n\n\n\nGood Turing smoothing\n\n\nKneser-Ney smoothing c.f. (Ney, Essen, and Kneser 1994) which corrects better for smaller data sets. \nGood-Turing smoothing c.f. (Good 1953) which uses order statistics to give even better estimates.\nwith a survey of the subject here: (Chen and Goodman 1996)",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Probability & Bayes Rule",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c1w2/index.html#sources-of-errors-in-na√Øve-bayes",
    "href": "posts/c1w2/index.html#sources-of-errors-in-na√Øve-bayes",
    "title": "Probability and Bayes Rule",
    "section": "Sources of Errors in Na√Øve Bayes",
    "text": "Sources of Errors in Na√Øve Bayes\n\nError Analysis\nBad sentiment classifications are due to:\n\npreprocessing dropping punctuation that encodes emotion like a sad smiley.\nWord order can contribute to meaning - breaking the independence assumption of our model\nPronouns removed as stop words - may encode emotion\nSarcasm can confound the model\nEuphemisms are also a challenge",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Probability & Bayes Rule",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c1w2/lab01.html",
    "href": "posts/c1w2/lab01.html",
    "title": "Visualizing Naive Bayes",
    "section": "",
    "text": "Figure¬†1: course banner\n\n\nIn this lab, we will cover an essential part of data analysis that has not been included in the lecture videos. As we stated in the previous module, data visualization gives insight into the expected performance of any model.\nIn the following exercise, you are going to make a visual inspection of the tweets dataset using the Na√Øve Bayes features. We will see how we can understand the log-likelihood ratio explained in the videos as a pair of numerical features that can be fed in a machine learning algorithm.\nAt the end of this lab, we will introduce the concept of confidence ellipse as a tool for representing the Na√Øve Bayes model visually.\n\nimport numpy as np # Library for linear algebra and math utils\nimport pandas as pd # Dataframe library\n\nimport matplotlib.pyplot as plt # Library for plots\nfrom utils import confidence_ellipse # Function to add confidence ellipses to charts\n\n## Calculate the likelihoods for each tweet\nFor each tweet, we have calculated the likelihood of the tweet to be positive and the likelihood to be negative. We have calculated in different columns the numerator and denominator of the likelihood ratio introduced previously.\n\nlog \\frac{P(tweet|pos)}{P(tweet|neg)} = log(P(tweet|pos)) - log(P(tweet|neg))\n\n\npositive = log(P(tweet|pos)) = \\sum_{i=0}^{n}{log P(W_i|pos)}\n\n\nnegative = log(P(tweet|neg)) = \\sum_{i=0}^{n}{log P(W_i|neg)}\n\nWe did not include the code because this is part of this week‚Äôs assignment. The ‚Äòbayes_features.csv‚Äô file contains the final result of this process.\nThe cell below loads the table in a dataframe. Dataframes are data structures that simplify the manipulation of data, allowing filtering, slicing, joining, and summarization.\n\ndata = pd.read_csv('bayes_features.csv'); # Load the data from the csv file\n\ndata.head(5) # Print the first 5 tweets features. Each row represents a tweet\n\n\n\n\n\n\n\n\npositive\nnegative\nsentiment\n\n\n\n\n0\n-45.763393\n-63.351354\n1.0\n\n\n1\n-105.491568\n-114.204862\n1.0\n\n\n2\n-57.028078\n-67.216467\n1.0\n\n\n3\n-10.055885\n-18.589057\n1.0\n\n\n4\n-125.749270\n-138.334845\n1.0\n\n\n\n\n\n\n\n\n# Plot the samples using columns 1 and 2 of the matrix\n\nfig, ax = plt.subplots(figsize = (8, 8)) #Create a new figure with a custom size\n\ncolors = ['red', 'green'] # Define a color palete\n\n# Color base on sentiment\nax.scatter(data.positive, data.negative, \n    c=[colors[int(k)] for k in data.sentiment], s = 0.1, marker='*')  # Plot a dot for each tweet\n\n# Custom limits for this chart\nplt.xlim(-250,0)\nplt.ylim(-250,0)\n\nplt.xlabel(\"Positive\") # x-axis label\nplt.ylabel(\"Negative\") # y-axis label\n\nText(0.5, 0, 'Positive')\n\n\nText(0, 0.5, 'Negative')\n\n\n\n\n\n\n\n\n\n\nUsing Confidence Ellipses to interpret Na√Øve Bayes\nIn this section, we will use the confidence ellipse to give us an idea of what the Na√Øve Bayes model see.\nA confidence ellipse is a way to visualize a 2D random variable. It is a better way than plotting the points over a cartesian plane because, with big datasets, the points can overlap badly and hide the real distribution of the data. Confidence ellipses summarize the information of the dataset with only four parameters:\n\nCenter: It is the numerical mean of the attributes\nHeight and width: Related with the variance of each attribute. The user must specify the desired amount of standard deviations used to plot the ellipse.\nAngle: Related with the covariance among attributes.\n\nThe parameter n_std stands for the number of standard deviations bounded by the ellipse. Remember that for normal random distributions:\n\nAbout 68% of the area under the curve falls within 1 standard deviation around the mean.\nAbout 95% of the area under the curve falls within 2 standard deviations around the mean.\nAbout 99.7% of the area under the curve falls within 3 standard deviations around the mean.\n\nstandard normal\nIn the next chart, we will plot the data and its corresponding confidence ellipses using 2 std and 3 std.\n\n# Plot the samples using columns 1 and 2 of the matrix\nfig, ax = plt.subplots(figsize = (8, 8))\n\ncolors = ['red', 'green'] # Define a color palete\n\n# Color base on sentiment\n\nax.scatter(data.positive, data.negative, c=[colors[int(k)] for k in data.sentiment], s = 0.1, marker='*')  # Plot a dot for tweet\n\n# Custom limits for this chart\nplt.xlim(-200,40)  \nplt.ylim(-200,40)\n\nplt.xlabel(\"Positive\") # x-axis label\nplt.ylabel(\"Negative\") # y-axis label\n\ndata_pos = data[data.sentiment == 1] # Filter only the positive samples\ndata_neg = data[data.sentiment == 0] # Filter only the negative samples\n\n# Print confidence ellipses of 2 std\nconfidence_ellipse(data_pos.positive, data_pos.negative, ax, n_std=2, edgecolor='black', label=r'$2\\sigma$' )\nconfidence_ellipse(data_neg.positive, data_neg.negative, ax, n_std=2, edgecolor='orange')\n\n# Print confidence ellipses of 3 std\nconfidence_ellipse(data_pos.positive, data_pos.negative, ax, n_std=3, edgecolor='black', linestyle=':', label=r'$3\\sigma$')\nconfidence_ellipse(data_neg.positive, data_neg.negative, ax, n_std=3, edgecolor='orange', linestyle=':')\nax.legend()\n\nplt.show()\n\nText(0.5, 0, 'Positive')\n\n\nText(0, 0.5, 'Negative')\n\n\n\n\n\n\n\n\n\nIn the next cell, we will modify the features of the samples with positive sentiment (1), in a way that the two distributions overlap. In this case, the Na√Øve Bayes method will produce a lower accuracy than with the original data.\n\ndata2 = data.copy() # Copy the whole data frame\n\n# The following 2 lines only modify the entries in the data frame where sentiment == 1\ndata2.negative[data.sentiment == 1] =  data2.negative * 1.5 + 50 # Modify the negative attribute\ndata2.positive[data.sentiment == 1] =  data2.positive / 1.5 - 50 # Modify the positive attribute \n\n/tmp/ipykernel_130410/2253601370.py:4: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  data2.negative[data.sentiment == 1] =  data2.negative * 1.5 + 50 # Modify the negative attribute\n/tmp/ipykernel_130410/2253601370.py:5: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  data2.positive[data.sentiment == 1] =  data2.positive / 1.5 - 50 # Modify the positive attribute\n\n\nNow let us plot the two distributions and the confidence ellipses\n\n# Plot the samples using columns 1 and 2 of the matrix\nfig, ax = plt.subplots(figsize = (8, 8))\n\ncolors = ['red', 'green'] # Define a color palete\n\n# Color base on sentiment\n\n#data.negative[data.sentiment == 1] =  data.negative * 2\n\nax.scatter(data2.positive, data2.negative, c=[colors[int(k)] for k in data2.sentiment], s = 0.1, marker='*')  # Plot a dot for tweet\n# Custom limits for this chart\nplt.xlim(-200,40)  \nplt.ylim(-200,40)\n\nplt.xlabel(\"Positive\") # x-axis label\nplt.ylabel(\"Negative\") # y-axis label\n\ndata_pos = data2[data2.sentiment == 1] # Filter only the positive samples\ndata_neg = data[data2.sentiment == 0] # Filter only the negative samples\n\n# Print confidence ellipses of 2 std\nconfidence_ellipse(data_pos.positive, data_pos.negative, ax, n_std=2, edgecolor='black', label=r'$2\\sigma$' )\nconfidence_ellipse(data_neg.positive, data_neg.negative, ax, n_std=2, edgecolor='orange')\n\n# Print confidence ellipses of 3 std\nconfidence_ellipse(data_pos.positive, data_pos.negative, ax, n_std=3, edgecolor='black', linestyle=':', label=r'$3\\sigma$')\nconfidence_ellipse(data_neg.positive, data_neg.negative, ax, n_std=3, edgecolor='orange', linestyle=':')\nax.legend()\n\nplt.show()\n\nText(0.5, 0, 'Positive')\n\n\nText(0, 0.5, 'Negative')\n\n\n\n\n\n\n\n\n\nTo give away: Understanding the data allows us to predict if the method will perform well or not. Alternatively, it will allow us to understand why it worked well or bad.\n\n\n\n\nCitationBibTeX citation:@online{bochman2025,\n  author = {Bochman, Oren},\n  title = {Visualizing {Naive} {Bayes}},\n  date = {2025-02-05},\n  url = {https://orenbochman.github.io/notes-nlp/posts/c1w2/lab01.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2025. ‚ÄúVisualizing Naive Bayes.‚Äù February 5,\n2025. https://orenbochman.github.io/notes-nlp/posts/c1w2/lab01.html.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "L1 Visualizing Naive Bayes"
    ]
  },
  {
    "objectID": "posts/c2w4/lab05.html",
    "href": "posts/c2w4/lab05.html",
    "title": "Word Embeddings: Hands On",
    "section": "",
    "text": "course banner\nIn previous lecture notebooks we saw all the steps needed to train the CBOW model. This notebook will walk we through how to extract the word embedding vectors from a model.\nLet‚Äôs dive into it!\nimport numpy as np\nfrom utils2 import get_dict\nBefore moving on, we will be provided with some variables needed for further procedures, which should be familiar by now. Also a trained CBOW model will be simulated, the corresponding weights and biases are provided:\n# Define the tokenized version of the corpus\nwords = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n\n# Define V. Remember this is the size of the vocabulary\nV = 5\n\n# Get 'word2Ind' and 'Ind2word' dictionaries for the tokenized corpus\nword2Ind, Ind2word = get_dict(words)\n\n\n# Define first matrix of weights\nW1 = np.array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n               [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n               [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n\n# Define second matrix of weights\nW2 = np.array([[-0.22182064, -0.43008631,  0.13310965],\n               [ 0.08476603,  0.08123194,  0.1772054 ],\n               [ 0.1871551 , -0.06107263, -0.1790735 ],\n               [ 0.07055222, -0.02015138,  0.36107434],\n               [ 0.33480474, -0.39423389, -0.43959196]])\n\n# Define first vector of biases\nb1 = np.array([[ 0.09688219],\n               [ 0.29239497],\n               [-0.27364426]])\n\n# Define second vector of biases\nb2 = np.array([[ 0.0352008 ],\n               [-0.36393384],\n               [-0.12775555],\n               [-0.34802326],\n               [-0.07017815]])"
  },
  {
    "objectID": "posts/c2w4/lab05.html#extracting-word-embedding-vectors",
    "href": "posts/c2w4/lab05.html#extracting-word-embedding-vectors",
    "title": "Word Embeddings: Hands On",
    "section": "Extracting word embedding vectors",
    "text": "Extracting word embedding vectors\nOnce we have finished training the neural network, we have three options to get word embedding vectors for the words of your vocabulary, based on the weight matrices \\mathbf{W_1} and/or \\mathbf{W_2}.\n\nOption 1: extract embedding vectors from \\mathbf{W_1}\nThe first option is to take the columns of \\mathbf{W_1} as the embedding vectors of the words of the vocabulary, using the same order of the words as for the input and output vectors.\n\nNote: in this practice notebooks the values of the word embedding vectors are meaningless after a single iteration with just one training example, but here‚Äôs how we would proceed after the training process is complete.\n\nFor example \\mathbf{W_1} is this matrix:\n\n# Print W1\nW1\n\narray([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n       [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n       [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n\n\nThe first column, which is a 3-element vector, is the embedding vector of the first word of your vocabulary. The second column is the word embedding vector for the second word, and so on.\nThe first, second, etc. words are ordered as follows.\n\n# Print corresponding word for each index within vocabulary's range\nfor i in range(V):\n    print(Ind2word[i])\n\nam\nbecause\nhappy\ni\nlearning\n\n\nSo the word embedding vectors corresponding to each word are:\n\n# Loop through each word of the vocabulary\nfor word in word2Ind:\n    # Extract the column corresponding to the index of the word in the vocabulary\n    word_embedding_vector = W1[:, word2Ind[word]]\n    # Print word alongside word embedding vector\n    print(f'{word}: {word_embedding_vector}')\n\nam: [0.41687358 0.32735501 0.26637602]\nbecause: [ 0.08854191  0.22795148 -0.23846886]\nhappy: [-0.23495225 -0.23951958 -0.37770863]\ni: [ 0.28320538  0.4117634  -0.11399446]\nlearning: [ 0.41800106 -0.23924344  0.34008124]\n\n\n\n\nOption 2: extract embedding vectors from \\mathbf{W_2}\nThe second option is to take \\mathbf{W_2} transposed, and take its columns as the word embedding vectors just like we did for \\mathbf{W_1}.\n\n# Print transposed W2\nW2.T\n\narray([[-0.22182064,  0.08476603,  0.1871551 ,  0.07055222,  0.33480474],\n       [-0.43008631,  0.08123194, -0.06107263, -0.02015138, -0.39423389],\n       [ 0.13310965,  0.1772054 , -0.1790735 ,  0.36107434, -0.43959196]])\n\n\n\n# Loop through each word of the vocabulary\nfor word in word2Ind:\n    # Extract the column corresponding to the index of the word in the vocabulary\n    word_embedding_vector = W2.T[:, word2Ind[word]]\n    # Print word alongside word embedding vector\n    print(f'{word}: {word_embedding_vector}')\n\nam: [-0.22182064 -0.43008631  0.13310965]\nbecause: [0.08476603 0.08123194 0.1772054 ]\nhappy: [ 0.1871551  -0.06107263 -0.1790735 ]\ni: [ 0.07055222 -0.02015138  0.36107434]\nlearning: [ 0.33480474 -0.39423389 -0.43959196]\n\n\n\n\nOption 3: extract embedding vectors from \\mathbf{W_1} and \\mathbf{W_2}\nThe third option, which is the one we will use in this week‚Äôs assignment, uses the average of \\mathbf{W_1} and \\mathbf{W_2^\\top}.\nCalculate the average of \\mathbf{W_1} and \\mathbf{W_2^\\top}, and store the result in W3.\n\n# Compute W3 as the average of W1 and W2 transposed\nW3 = (W1+W2.T)/2\n\n# Print W3\nW3\n\narray([[ 0.09752647,  0.08665397, -0.02389858,  0.1768788 ,  0.3764029 ],\n       [-0.05136565,  0.15459171, -0.15029611,  0.19580601, -0.31673866],\n       [ 0.19974284, -0.03063173, -0.27839106,  0.12353994, -0.04975536]])\n\n\nExpected output:\narray([[ 0.09752647,  0.08665397, -0.02389858,  0.1768788 ,  0.3764029 ],\n       [-0.05136565,  0.15459171, -0.15029611,  0.19580601, -0.31673866],\n       [ 0.19974284, -0.03063173, -0.27839106,  0.12353994, -0.04975536]])\nExtracting the word embedding vectors works just like the two previous options, by taking the columns of the matrix you‚Äôve just created.\n\n# Loop through each word of the vocabulary\nfor word in word2Ind:\n    # Extract the column corresponding to the index of the word in the vocabulary\n    word_embedding_vector = W3[:, word2Ind[word]]\n    # Print word alongside word embedding vector\n    print(f'{word}: {word_embedding_vector}')\n\nam: [ 0.09752647 -0.05136565  0.19974284]\nbecause: [ 0.08665397  0.15459171 -0.03063173]\nhappy: [-0.02389858 -0.15029611 -0.27839106]\ni: [0.1768788  0.19580601 0.12353994]\nlearning: [ 0.3764029  -0.31673866 -0.04975536]\n\n\nNow we know 3 different options to get the word embedding vectors from a model!\n\n\nHow this practice relates to and differs from the upcoming graded assignment\n\nAfter extracting the word embedding vectors, we will use principal component analysis (PCA) to visualize the vectors, which will enable we to perform an intrinsic evaluation of the quality of the vectors, as explained in the lecture.\n\nCongratulations on finishing all lecture notebooks for this week!\nYou‚Äôre now ready to take on this week‚Äôs assignment!\nKeep it up!"
  },
  {
    "objectID": "posts/c2w4/lab02.html",
    "href": "posts/c2w4/lab02.html",
    "title": "Word Embeddings: Intro to CBOW model, activation functions and working with Numpy",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure¬†1\nIn this lecture notebook you will be given an introduction to the continuous bag-of-words model, its activation functions and some considerations when working with Numpy.\nLet‚Äôs dive into it!\nimport numpy as np",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "L2 - Intro to CBOW"
    ]
  },
  {
    "objectID": "posts/c2w4/lab02.html#activation-functions",
    "href": "posts/c2w4/lab02.html#activation-functions",
    "title": "Word Embeddings: Intro to CBOW model, activation functions and working with Numpy",
    "section": "Activation functions",
    "text": "Activation functions\nLet‚Äôs start by implementing the activation functions, ReLU and softmax.\n\nReLU\nReLU is used to calculate the values of the hidden layer, in the following formulas:\n\\begin{align}\n\\mathbf{z_1} &= \\mathbf{W_1}\\mathbf{x} + \\mathbf{b_1}  \\tag{1} \\\\\n\\mathbf{h} &= \\mathrm{ReLU}(\\mathbf{z_1})  \\tag{2} \\\\\n\\end{align}\nLet‚Äôs fix a value for \\mathbf{z_1} as a working example.\n\n# Define a random seed so all random outcomes can be reproduced\nnp.random.seed(10)\n\n# Define a 5X1 column vector using numpy\nz_1 = 10*np.random.rand(5, 1)-5\n\n# Print the vector\nz_1\n\narray([[ 2.71320643],\n       [-4.79248051],\n       [ 1.33648235],\n       [ 2.48803883],\n       [-0.01492988]])\n\n\nNotice that using numpy‚Äôs random.rand function returns a numpy array filled with values taken from a uniform distribution over [0, 1). Numpy allows vectorization so each value is multiplied by 10 and then substracted 5.\nTo get the ReLU of this vector, you want all the negative values to become zeros.\nFirst create a copy of this vector.\n\n# Create copy of vector and save it in the 'h' variable\nh = z_1.copy()\n\nNow determine which of its values are negative.\n\n# Determine which values met the criteria (this is possible because of vectorization)\nh &lt; 0\n\narray([[False],\n       [ True],\n       [False],\n       [False],\n       [ True]])\n\n\nYou can now simply set all of the values which are negative to 0.\n\n# Slice the array or vector. This is the same as applying ReLU to it\nh[h &lt; 0] = 0\n\nAnd that‚Äôs it: you have the ReLU of \\mathbf{z_1}!\n\n# Print the vector after ReLU\nh\n\narray([[2.71320643],\n       [0.        ],\n       [1.33648235],\n       [2.48803883],\n       [0.        ]])\n\n\nNow implement ReLU as a function.\n\n# Define the 'relu' function that will include the steps previously seen\ndef relu(z):\n    result = z.copy()\n    result[result &lt; 0] = 0\n    return result\n\nAnd check that it‚Äôs working.\n\n# Define a new vector and save it in the 'z' variable\nz = np.array([[-1.25459881], [ 4.50714306], [ 2.31993942], [ 0.98658484], [-3.4398136 ]])\n\n# Apply ReLU to it\nrelu(z)\n\narray([[0.        ],\n       [4.50714306],\n       [2.31993942],\n       [0.98658484],\n       [0.        ]])\n\n\nExpected output:\narray([[0.        ],\n       [4.50714306],\n       [2.31993942],\n       [0.98658484],\n       [0.        ]])\n\n\nSoftmax\nThe second activation function that you need is softmax. This function is used to calculate the values of the output layer of the neural network, using the following formulas:\n\\begin{align}\n\\mathbf{z_2} &= \\mathbf{W_2}\\mathbf{h} + \\mathbf{b_2}   \\tag{3} \\\\\n\\mathbf{\\hat y} &= \\mathrm{softmax}(\\mathbf{z_2})   \\tag{4} \\\\\n\\end{align}\nTo calculate softmax of a vector \\mathbf{z}, the i-th component of the resulting vector is given by:\n \\textrm{softmax}(\\textbf{z})_i = \\frac{e^{z_i} }{\\sum\\limits_{j=1}^{V} e^{z_j} }  \\tag{5} \nLet‚Äôs work through an example.\n\n# Define a new vector and save it in the 'z' variable\nz = np.array([9, 8, 11, 10, 8.5])\n\n# Print the vector\nz\n\narray([ 9. ,  8. , 11. , 10. ,  8.5])\n\n\nYou‚Äôll need to calculate the exponentials of each element, both for the numerator and for the denominator.\n\n# Save exponentials of the values in a new vector\ne_z = np.exp(z)\n\n# Print the vector with the exponential values\ne_z\n\narray([ 8103.08392758,  2980.95798704, 59874.1417152 , 22026.46579481,\n        4914.7688403 ])\n\n\nThe denominator is equal to the sum of these exponentials.\n\n# Save the sum of the exponentials\nsum_e_z = np.sum(e_z)\n\n# Print sum of exponentials\nsum_e_z\n\nnp.float64(97899.41826492078)\n\n\nAnd the value of the first element of \\textrm{softmax}(\\textbf{z}) is given by:\n\n# Print softmax value of the first element in the original vector\ne_z[0]/sum_e_z\n\nnp.float64(0.08276947985173956)\n\n\nThis is for one element. You can use numpy‚Äôs vectorized operations to calculate the values of all the elements of the \\textrm{softmax}(\\textbf{z}) vector in one go.\nImplement the softmax function.\n\n# Define the 'softmax' function that will include the steps previously seen\ndef softmax(z):\n    e_z = np.exp(z)\n    sum_e_z = np.sum(e_z)\n    return e_z / sum_e_z\n\nNow check that it works.\n\n# Print softmax values for original vector\nsoftmax([9, 8, 11, 10, 8.5])\n\narray([0.08276948, 0.03044919, 0.61158833, 0.22499077, 0.05020223])\n\n\nExpected output:\narray([0.08276948, 0.03044919, 0.61158833, 0.22499077, 0.05020223])\nNotice that the sum of all these values is equal to 1.\n\n# Assert that the sum of the softmax values is equal to 1\nnp.sum(softmax([9, 8, 11, 10, 8.5])) == 1\n\nnp.True_",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "L2 - Intro to CBOW"
    ]
  },
  {
    "objectID": "posts/c2w4/lab02.html#dimensions-1-d-arrays-vs-2-d-column-vectors",
    "href": "posts/c2w4/lab02.html#dimensions-1-d-arrays-vs-2-d-column-vectors",
    "title": "Word Embeddings: Intro to CBOW model, activation functions and working with Numpy",
    "section": "Dimensions: 1-D arrays vs 2-D column vectors",
    "text": "Dimensions: 1-D arrays vs 2-D column vectors\nBefore moving on to implement forward propagation, backpropagation, and gradient descent in the next lecture notebook, let‚Äôs have a look at the dimensions of the vectors you‚Äôve been handling until now.\nCreate a vector of length V filled with zeros.\n\n# Define V. Remember this was the size of the vocabulary in the previous lecture notebook\nV = 5\n\n# Define vector of length V filled with zeros\nx_array = np.zeros(V)\n\n# Print vector\nx_array\n\narray([0., 0., 0., 0., 0.])\n\n\nThis is a 1-dimensional array, as revealed by the .shape property of the array.\n\n# Print vector's shape\nx_array.shape\n\n(5,)\n\n\nTo perform matrix multiplication in the next steps, you actually need your column vectors to be represented as a matrix with one column. In numpy, this matrix is represented as a 2-dimensional array.\nThe easiest way to convert a 1D vector to a 2D column matrix is to set its .shape property to the number of rows and one column, as shown in the next cell.\n\n# Copy vector\nx_column_vector = x_array.copy()\n\n# Reshape copy of vector\nx_column_vector.shape = (V, 1)  # alternatively ... = (x_array.shape[0], 1)\n\n# Print vector\nx_column_vector\n\narray([[0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.]])\n\n\nThe shape of the resulting ‚Äúvector‚Äù is:\n\n# Print vector's shape\nx_column_vector.shape\n\n(5, 1)\n\n\nSo you now have a 5x1 matrix that you can use to perform standard matrix multiplication.\nCongratulations on finishing this lecture notebook! Hopefully you now have a better understanding of the activation functions used in the continuous bag-of-words model, as well as a clearer idea of how to leverage Numpy‚Äôs power for these types of mathematical computations.\nIn the next lecture notebook you will get a comprehensive dive into:\n\nForward propagation.\nCross-entropy loss.\nBackpropagation.\nGradient descent.\n\nSee you next time!",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "L2 - Intro to CBOW"
    ]
  },
  {
    "objectID": "posts/c2w4/lab03.html",
    "href": "posts/c2w4/lab03.html",
    "title": "Word Embeddings: Training the CBOW model",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure¬†1\nIn previous lecture notebooks you saw how to prepare data before feeding it to a continuous bag-of-words model, the model itself, its architecture and activation functions. This notebook will walk you through:\nWhich are concepts necessary to understand how the training of the model works.\nLet‚Äôs dive into it!\nimport numpy as np\nfrom utils2 import get_dict",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "L3 - Training the CBOW"
    ]
  },
  {
    "objectID": "posts/c2w4/lab03.html#forward-propagation",
    "href": "posts/c2w4/lab03.html#forward-propagation",
    "title": "Word Embeddings: Training the CBOW model",
    "section": "Forward propagation",
    "text": "Forward propagation\nLet‚Äôs dive into the neural network itself, which is shown below with all the dimensions and formulas you‚Äôll need.\n\n\n\n\n\n\n\nFigure¬†2\n\n\nSet N equal to 3. Remember that N is a hyperparameter of the CBOW model that represents the size of the word embedding vectors, as well as the size of the hidden layer.\nAlso set V equal to 5, which is the size of the vocabulary we have used so far.\n\n# Define the size of the word embedding vectors and save it in the variable 'N'\nN = 3\n\n# Define V. Remember this was the size of the vocabulary in the previous lecture notebooks\nV = 5\n\n\nInitialization of the weights and biases\nBefore you start training the neural network, you need to initialize the weight matrices and bias vectors with random values.\nIn the assignment you will implement a function to do this yourself using numpy.random.rand. In this notebook, we‚Äôve pre-populated these matrices and vectors for you.\n\n# Define first matrix of weights\nW1 = np.array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n               [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n               [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n\n# Define second matrix of weights\nW2 = np.array([[-0.22182064, -0.43008631,  0.13310965],\n               [ 0.08476603,  0.08123194,  0.1772054 ],\n               [ 0.1871551 , -0.06107263, -0.1790735 ],\n               [ 0.07055222, -0.02015138,  0.36107434],\n               [ 0.33480474, -0.39423389, -0.43959196]])\n\n# Define first vector of biases\nb1 = np.array([[ 0.09688219],\n               [ 0.29239497],\n               [-0.27364426]])\n\n# Define second vector of biases\nb2 = np.array([[ 0.0352008 ],\n               [-0.36393384],\n               [-0.12775555],\n               [-0.34802326],\n               [-0.07017815]])\n\nCheck that the dimensions of these matrices match those shown in the figure above.\n\nprint(f'V (vocabulary size): {V}')\nprint(f'N (embedding size / size of the hidden layer): {N}')\nprint(f'size of W1: {W1.shape} (NxV)')\nprint(f'size of b1: {b1.shape} (Nx1)')\nprint(f'size of W2: {W2.shape} (VxN)')\nprint(f'size of b2: {b2.shape} (Vx1)')\n\nV (vocabulary size): 5\nN (embedding size / size of the hidden layer): 3\nsize of W1: (3, 5) (NxV)\nsize of b1: (3, 1) (Nx1)\nsize of W2: (5, 3) (VxN)\nsize of b2: (5, 1) (Vx1)\n\n\nBefore moving forward, you will need some functions and variables defined in previous notebooks. They can be found next. Be sure you understand everything that is going on in the next cell, if not consider doing a refresh of the first lecture notebook.\n\n# Define the tokenized version of the corpus\nwords = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n\n# Get 'word2Ind' and 'Ind2word' dictionaries for the tokenized corpus\nword2Ind, Ind2word = get_dict(words)\n\n# Define the 'get_windows' function as seen in a previous notebook\ndef get_windows(words, C):\n    i = C\n    while i &lt; len(words) - C:\n        center_word = words[i]\n        context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n        yield context_words, center_word\n        i += 1\n\n# Define the 'word_to_one_hot_vector' function as seen in a previous notebook\ndef word_to_one_hot_vector(word, word2Ind, V):\n    one_hot_vector = np.zeros(V)\n    one_hot_vector[word2Ind[word]] = 1\n    return one_hot_vector\n\n# Define the 'context_words_to_vector' function as seen in a previous notebook\ndef context_words_to_vector(context_words, word2Ind, V):\n    context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n    context_words_vectors = np.mean(context_words_vectors, axis=0)\n    return context_words_vectors\n\n# Define the generator function 'get_training_example' as seen in a previous notebook\ndef get_training_example(words, C, word2Ind, V):\n    for context_words, center_word in get_windows(words, C):\n        yield context_words_to_vector(context_words, word2Ind, V), word_to_one_hot_vector(center_word, word2Ind, V)\n\n\n\nTraining example\nRun the next cells to get the first training example, made of the vector representing the context words ‚Äúi am because i‚Äù, and the target which is the one-hot vector representing the center word ‚Äúhappy‚Äù.\n\nYou don‚Äôt need to worry about the Python syntax, but there are some explanations below if you want to know what‚Äôs happening behind the scenes.\n\n\n# Save generator object in the 'training_examples' variable with the desired arguments\ntraining_examples = get_training_example(words, 2, word2Ind, V)\n\n\nget_training_examples, which uses the yield keyword, is known as a generator. When run, it builds an iterator, which is a special type of object that‚Ä¶ you can iterate on (using a for loop for instance), to retrieve the successive values that the function generates.\nIn this case get_training_examples yields training examples, and iterating on training_examples will return the successive training examples.\n\n\n# Get first values from generator\nx_array, y_array = next(training_examples)\n\n\nnext is another special keyword, which gets the next available value from an iterator. Here, you‚Äôll get the very first value, which is the first training example. If you run this cell again, you‚Äôll get the next value, and so on until the iterator runs out of values to return.\nIn this notebook next is used because you will only be performing one iteration of training. In this week‚Äôs assignment with the full training over several iterations you‚Äôll use regular for loops with the iterator that supplies the training examples.\n\nThe vector representing the context words, which will be fed into the neural network, is:\n\n# Print context words vector\nx_array\n\narray([0.25, 0.25, 0.  , 0.5 , 0.  ])\n\n\nThe one-hot vector representing the center word to be predicted is:\n\n# Print one hot vector of center word\ny_array\n\narray([0., 0., 1., 0., 0.])\n\n\nNow convert these vectors into matrices (or 2D arrays) to be able to perform matrix multiplication on the right types of objects, as explained in a previous notebook.\n\n# Copy vector\nx = x_array.copy()\n\n# Reshape it\nx.shape = (V, 1)\n\n# Print it\nprint(f'x:\\n{x}\\n')\n\n# Copy vector\ny = y_array.copy()\n\n# Reshape it\ny.shape = (V, 1)\n\n# Print it\nprint(f'y:\\n{y}')\n\nx:\n[[0.25]\n [0.25]\n [0.  ]\n [0.5 ]\n [0.  ]]\n\ny:\n[[0.]\n [0.]\n [1.]\n [0.]\n [0.]]\n\n\nNow you will need the activation functions seen before. Again, if this feel unfamiliar consider checking the previous lecture notebook.\n\n# Define the 'relu' function as seen in the previous lecture notebook\ndef relu(z):\n    result = z.copy()\n    result[result &lt; 0] = 0\n    return result\n\n# Define the 'softmax' function as seen in the previous lecture notebook\ndef softmax(z):\n    e_z = np.exp(z)\n    sum_e_z = np.sum(e_z)\n    return e_z / sum_e_z\n\n\n\nValues of the hidden layer\nNow that you have initialized all the variables that you need for forward propagation, you can calculate the values of the hidden layer using the following formulas:\n\n\\begin{align}\n\\mathbf{z_1} = \\mathbf{W_1}\\mathbf{x} + \\mathbf{b_1}  \\tag{1} \\\\\n\\mathbf{h} = \\mathrm{ReLU}(\\mathbf{z_1})  \\tag{2} \\\\\n\\end{align}\n\nFirst, you can calculate the value of \\mathbf{z_1}.\n\n# Compute z1 (values of first hidden layer before applying the ReLU function)\nz1 = np.dot(W1, x) + b1\n\n\n¬†np.dot is numpy‚Äôs function for matrix multiplication.\n\nAs expected you get an N by 1 matrix, or column vector with N elements, where N is equal to the embedding size, which is 3 in this example.\n\n# Print z1\nz1\n\narray([[ 0.36483875],\n       [ 0.63710329],\n       [-0.3236647 ]])\n\n\nYou can now take the ReLU of \\mathbf{z_1} to get \\mathbf{h}, the vector with the values of the hidden layer.\n\n# Compute h (z1 after applying ReLU function)\nh = relu(z1)\n\n# Print h\nh\n\narray([[0.36483875],\n       [0.63710329],\n       [0.        ]])\n\n\nApplying ReLU means that the negative element of \\mathbf{z_1} has been replaced with a zero.\n\n\nValues of the output layer\nHere are the formulas you need to calculate the values of the output layer, represented by the vector \\mathbf{\\hat y}:\n\n\\begin{align}\n\\mathbf{z_2} &= \\mathbf{W_2}\\mathbf{h} + \\mathbf{b_2}   \\tag{3} \\\\\n\\mathbf{\\hat y} &= \\mathrm{softmax}(\\mathbf{z_2})   \\tag{4} \\\\\n\\end{align}\n\nFirst, calculate \\mathbf{z_2}.\n\n# Compute z2 (values of the output layer before applying the softmax function)\nz2 = np.dot(W2, h) + b2\n\n# Print z2\nz2\n\narray([[-0.31973737],\n       [-0.28125477],\n       [-0.09838369],\n       [-0.33512159],\n       [-0.19919612]])\n\n\nExpected output:\narray([[-0.31973737],\n       [-0.28125477],\n       [-0.09838369],\n       [-0.33512159],\n       [-0.19919612]])\nThis is a V by 1 matrix, where V is the size of the vocabulary, which is 5 in this example.\nNow calculate the value of \\mathbf{\\hat y}.\n\n# Compute y_hat (z2 after applying softmax function)\ny_hat = softmax(z2)\n\n# Print y_hat\ny_hat\n\narray([[0.18519074],\n       [0.19245626],\n       [0.23107446],\n       [0.18236353],\n       [0.20891502]])\n\n\nExpected output:\narray([[0.18519074],\n       [0.19245626],\n       [0.23107446],\n       [0.18236353],\n       [0.20891502]])\nAs you‚Äôve performed the calculations with random matrices and vectors (apart from the input vector), the output of the neural network is essentially random at this point. The learning process will adjust the weights and biases to match the actual targets better.\nThat being said, what word did the neural network predict?\n\n\nSolution\n\n\nThe neural network predicted the word ‚Äúhappy‚Äù: the largest element of \\mathbf{\\hat y} is the third one, and the third word of the vocabulary is ‚Äúhappy‚Äù.\n\n\nHere‚Äôs how you could implement this in Python:\n\n\nprint(Ind2word[np.argmax(y_hat)])\n\n\nWell done, you‚Äôve completed the forward propagation phase!",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "L3 - Training the CBOW"
    ]
  },
  {
    "objectID": "posts/c2w4/lab03.html#cross-entropy-loss",
    "href": "posts/c2w4/lab03.html#cross-entropy-loss",
    "title": "Word Embeddings: Training the CBOW model",
    "section": "Cross-entropy loss",
    "text": "Cross-entropy loss\nNow that you have the network‚Äôs prediction, you can calculate the cross-entropy loss to determine how accurate the prediction was compared to the actual target.\n\nRemember that you are working on a single training example, not on a batch of examples, which is why you are using loss and not cost, which is the generalized form of loss.\n\nFirst let‚Äôs recall what the prediction was.\n\n# Print prediction\ny_hat\n\narray([[0.18519074],\n       [0.19245626],\n       [0.23107446],\n       [0.18236353],\n       [0.20891502]])\n\n\nAnd the actual target value is:\n\n# Print target value\ny\n\narray([[0.],\n       [0.],\n       [1.],\n       [0.],\n       [0.]])\n\n\nThe formula for cross-entropy loss is:\n J=-\\sum\\limits_{k=1}^{V}y_k\\log{\\hat{y}_k} \\tag{6}\nTry implementing the cross-entropy loss function so you get more familiar working with numpy\nHere are a some hints if you‚Äôre stuck.\n\ndef cross_entropy_loss(y_predicted, y_actual):\n    # Fill the loss variable with your code\n    loss = np.sum(-np.log(y_predicted)*y_actual)\n    return loss\n\n\n\nHint 1\n\n&lt;p&gt;To multiply two numpy matrices (such as &lt;code&gt;y&lt;/code&gt; and &lt;code&gt;y_hat&lt;/code&gt;) element-wise, you can simply use the &lt;code&gt;*&lt;/code&gt; operator.&lt;/p&gt;\n\n\n\nHint 2\n\n\nOnce you have a vector equal to the element-wise multiplication of y and y_hat, you can use np.sum to calculate the sum of the elements of this vector.\n\n\n\n\nSolution\n\n\nloss = np.sum(-np.log(y_hat)*y)\n\n\nDon‚Äôt forget to run the cell containing the cross_entropy_loss function once it is solved.\nNow use this function to calculate the loss with the actual values of \\mathbf{y} and \\mathbf{\\hat y}.\n\n# Print value of cross entropy loss for prediction and target value\ncross_entropy_loss(y_hat, y)\n\nnp.float64(1.4650152923611108)\n\n\nExpected output:\n1.4650152923611106\nThis value is neither good nor bad, which is expected as the neural network hasn‚Äôt learned anything yet.\nThe actual learning will start during the next phase: backpropagation.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "L3 - Training the CBOW"
    ]
  },
  {
    "objectID": "posts/c2w4/lab03.html#backpropagation",
    "href": "posts/c2w4/lab03.html#backpropagation",
    "title": "Word Embeddings: Training the CBOW model",
    "section": "Backpropagation",
    "text": "Backpropagation\nThe formulas that you will implement for backpropagation are the following.\n\n\\begin{align}\n\\frac{\\partial J}{\\partial \\mathbf{W_1}} &= \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right )\\mathbf{x}^\\top \\tag{7}\\\\\n\\frac{\\partial J}{\\partial \\mathbf{W_2}} &= (\\mathbf{\\hat{y}} - \\mathbf{y})\\mathbf{h^\\top} \\tag{8}\\\\\n\\frac{\\partial J}{\\partial \\mathbf{b_1}} &= \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right ) \\tag{9}\\\\\n\\frac{\\partial J}{\\partial \\mathbf{b_2}} &= \\mathbf{\\hat{y}} - \\mathbf{y} \\tag{10}\n\\end{align}\n\n\nNote: these formulas are slightly simplified compared to the ones in the lecture as you‚Äôre working on a single training example, whereas the lecture provided the formulas for a batch of examples. In the assignment you‚Äôll be implementing the latter.\n\nLet‚Äôs start with an easy one.\nCalculate the partial derivative of the loss function with respect to \\mathbf{b_2}, and store the result in grad_b2.\n\n\\frac{\\partial J}{\\partial \\mathbf{b_2}} = \\mathbf{\\hat{y}} - \\mathbf{y} \\tag{10}\n\n\n# Compute vector with partial derivatives of loss function with respect to b2\ngrad_b2 = y_hat - y\n\n# Print this vector\ngrad_b2\n\narray([[ 0.18519074],\n       [ 0.19245626],\n       [-0.76892554],\n       [ 0.18236353],\n       [ 0.20891502]])\n\n\nExpected output:\narray([[ 0.18519074],\n       [ 0.19245626],\n       [-0.76892554],\n       [ 0.18236353],\n       [ 0.20891502]])\nNext, calculate the partial derivative of the loss function with respect to \\mathbf{W_2}, and store the result in grad_W2.\n\n\\frac{\\partial J}{\\partial \\mathbf{W_2}} = (\\mathbf{\\hat{y}} - \\mathbf{y})\\mathbf{h^\\top} \\tag{8}\n\n\nHint: use .T to get a transposed matrix, e.g.¬†h.T returns \\mathbf{h^\\top}.\n\n\n# Compute matrix with partial derivatives of loss function with respect to W2\ngrad_W2 = np.dot(y_hat - y, h.T)\n\n# Print matrix\ngrad_W2\n\narray([[ 0.06756476,  0.11798563,  0.        ],\n       [ 0.0702155 ,  0.12261452,  0.        ],\n       [-0.28053384, -0.48988499,  0.        ],\n       [ 0.06653328,  0.1161844 ,  0.        ],\n       [ 0.07622029,  0.13310045,  0.        ]])\n\n\nExpected output:\narray([[ 0.06756476,  0.11798563,  0.        ],\n       [ 0.0702155 ,  0.12261452,  0.        ],\n       [-0.28053384, -0.48988499,  0.        ],\n       [ 0.06653328,  0.1161844 ,  0.        ],\n       [ 0.07622029,  0.13310045,  0.        ]])\nNow calculate the partial derivative with respect to \\mathbf{b_1} and store the result in grad_b1.\n\\frac{\\partial J}{\\partial \\mathbf{b_1}} = \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right ) \\tag{9}\n\n# Compute vector with partial derivatives of loss function with respect to b1\ngrad_b1 = relu(np.dot(W2.T, y_hat - y))\n\n# Print vector\ngrad_b1\n\narray([[0.        ],\n       [0.        ],\n       [0.17045858]])\n\n\nExpected output:\narray([[0.        ],\n       [0.        ],\n       [0.17045858]])\nFinally, calculate the partial derivative of the loss with respect to \\mathbf{W_1}, and store it in grad_W1.\n\n\\frac{\\partial J}{\\partial \\mathbf{W_1}} = \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right )\\mathbf{x}^\\top \\tag{7}\n\n\n# Compute matrix with partial derivatives of loss function with respect to W1\ngrad_W1 = np.dot(relu(np.dot(W2.T, y_hat - y)), x.T)\n\n# Print matrix\ngrad_W1\n\narray([[0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.04261464, 0.04261464, 0.        , 0.08522929, 0.        ]])\n\n\nExpected output:\narray([[0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.04261464, 0.04261464, 0.        , 0.08522929, 0.        ]])\nBefore moving on to gradient descent, double-check that all the matrices have the expected dimensions.\n\nprint(f'V (vocabulary size): {V}')\nprint(f'N (embedding size / size of the hidden layer): {N}')\nprint(f'size of grad_W1: {grad_W1.shape} (NxV)')\nprint(f'size of grad_b1: {grad_b1.shape} (Nx1)')\nprint(f'size of grad_W2: {grad_W2.shape} (VxN)')\nprint(f'size of grad_b2: {grad_b2.shape} (Vx1)')\n\nV (vocabulary size): 5\nN (embedding size / size of the hidden layer): 3\nsize of grad_W1: (3, 5) (NxV)\nsize of grad_b1: (3, 1) (Nx1)\nsize of grad_W2: (5, 3) (VxN)\nsize of grad_b2: (5, 1) (Vx1)",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "L3 - Training the CBOW"
    ]
  },
  {
    "objectID": "posts/c2w4/lab03.html#gradient-descent",
    "href": "posts/c2w4/lab03.html#gradient-descent",
    "title": "Word Embeddings: Training the CBOW model",
    "section": "Gradient descent",
    "text": "Gradient descent\nDuring the gradient descent phase, you will update the weights and biases by subtracting \\alpha times the gradient from the original matrices and vectors, using the following formulas.\n\n\\begin{align}\n\\mathbf{W_1} &:= \\mathbf{W_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_1}} \\tag{11}\\\\\n\\mathbf{W_2} &:= \\mathbf{W_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_2}} \\tag{12}\\\\\n\\mathbf{b_1} &:= \\mathbf{b_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_1}} \\tag{13}\\\\\n\\mathbf{b_2} &:= \\mathbf{b_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_2}} \\tag{14}\\\\\n\\end{align}\n\nFirst, let set a value for \\alpha.\n\n# Define alpha\nalpha = 0.03\n\nThe updated weight matrix \\mathbf{W_1} will be:\n\n# Compute updated W1\nW1_new = W1 - alpha * grad_W1\n\nLet‚Äôs compare the previous and new values of \\mathbf{W_1}:\n\nprint('old value of W1:')\nprint(W1)\nprint()\nprint('new value of W1:')\nprint(W1_new)\n\nold value of W1:\n[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]\n [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]\n [ 0.26637602 -0.23846886 -0.37770863 -0.11399446  0.34008124]]\n\nnew value of W1:\n[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]\n [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]\n [ 0.26509758 -0.2397473  -0.37770863 -0.11655134  0.34008124]]\n\n\nThe difference is very subtle (hint: take a closer look at the last row), which is why it takes a fair amount of iterations to train the neural network until it reaches optimal weights and biases starting from random values.\nNow calculate the new values of \\mathbf{W_2} (to be stored in W2_new), \\mathbf{b_1} (in b1_new), and \\mathbf{b_2} (in b2_new).\n\\begin{align}\n\\mathbf{W_2} &:= \\mathbf{W_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_2}} \\tag{12}\\\\\n\\mathbf{b_1} &:= \\mathbf{b_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_1}} \\tag{13}\\\\\n\\mathbf{b_2} &:= \\mathbf{b_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_2}} \\tag{14}\\\\\n\\end{align}\n\n# Compute updated W2\nW2_new = W2 - alpha * grad_W2\n\n# Compute updated b1\nb1_new = b1 - alpha * grad_b1\n\n# Compute updated b2\nb2_new = b2 - alpha * grad_b2\n\n\nprint('W2_new')\nprint(W2_new)\nprint()\nprint('b1_new')\nprint(b1_new)\nprint()\nprint('b2_new')\nprint(b2_new)\n\nW2_new\n[[-0.22384758 -0.43362588  0.13310965]\n [ 0.08265956  0.0775535   0.1772054 ]\n [ 0.19557112 -0.04637608 -0.1790735 ]\n [ 0.06855622 -0.02363691  0.36107434]\n [ 0.33251813 -0.3982269  -0.43959196]]\n\nb1_new\n[[ 0.09688219]\n [ 0.29239497]\n [-0.27875802]]\n\nb2_new\n[[ 0.02964508]\n [-0.36970753]\n [-0.10468778]\n [-0.35349417]\n [-0.0764456 ]]\n\n\nExpected output:\nW2_new\n[[-0.22384758 -0.43362588  0.13310965]\n [ 0.08265956  0.0775535   0.1772054 ]\n [ 0.19557112 -0.04637608 -0.1790735 ]\n [ 0.06855622 -0.02363691  0.36107434]\n [ 0.33251813 -0.3982269  -0.43959196]]\n\nb1_new\n[[ 0.09688219]\n [ 0.29239497]\n [-0.27875802]]\n\nb2_new\n[[ 0.02964508]\n [-0.36970753]\n [-0.10468778]\n [-0.35349417]\n [-0.0764456 ]]\nCongratulations, you have completed one iteration of training using one training example!\nYou‚Äôll need many more iterations to fully train the neural network, and you can optimize the learning process by training on batches of examples, as described in the lecture. You will get to do this during this week‚Äôs assignment.\n\nHow this practice relates to and differs from the upcoming graded assignment\n\nIn the assignment, for each iteration of training you will use batches of examples instead of a single example. The formulas for forward propagation and backpropagation will be modified accordingly, and you will use cross-entropy cost instead of cross-entropy loss.\nYou will also complete several iterations of training, until you reach an acceptably low cross-entropy cost, at which point you can extract good word embeddings from the weight matrices.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "L3 - Training the CBOW"
    ]
  },
  {
    "objectID": "posts/c2w4/assignment.html",
    "href": "posts/c2w4/assignment.html",
    "title": "Assignment 4: Word Embeddings",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure¬†1\nWelcome to the fourth (and last) programming assignment of Course 2!\nIn this assignment, you will practice how to compute word embeddings and use them for sentiment analysis. - To implement sentiment analysis, you can go beyond counting the number of positive words and negative words. - You can find a way to represent each word numerically, by a vector. - The vector could then represent syntactic (i.e.¬†parts of speech) and semantic (i.e.¬†meaning) structures.\nIn this assignment, you will explore a classic way of generating word embeddings or representations. - You will implement a famous model called the continuous bag of words (CBOW) model.\nBy completing this assignment you will:\nKnowing how to train these models will give you a better understanding of word vectors, which are building blocks to many applications in natural language processing.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "A4 - Word embeddings"
    ]
  },
  {
    "objectID": "posts/c2w4/assignment.html#outline",
    "href": "posts/c2w4/assignment.html#outline",
    "title": "Assignment 4: Word Embeddings",
    "section": "Outline",
    "text": "Outline\n\n1 The Continuous bag of words model\n2 Training the Model\n\n2.0 Initialize the model\n\nExercise 01\n\n2.1 Softmax Function\n\nExercise 02\n\n2.2 Forward Propagation\n\nExercise 03\n\n2.3 Cost Function\n2.4 Backproagation\n\nExercise 04\n\n2.5 Gradient Descent\n\nExercise 05\n\n\n3 Visualizing the word vectors\n\n # 1. The Continuous bag of words model\nLet‚Äôs take a look at the following sentence: &gt;‚ÄòI am happy because I am learning‚Äô.\n\nIn continuous bag of words (CBOW) modeling, we try to predict the center word given a few context words (the words around the center word).\nFor example, if you were to choose a context half-size of say C = 2, then you would try to predict the word happy given the context that includes 2 words before and 2 words after the center word:\n\n\nC words before: [I, am]\n\n\nC words after: [because, I]\n\n\nIn other words:\n\ncontext = [I,am, because, I] target = happy\nThe structure of your model will look like this:\n\n\n\n\n\n\n\ncourse banner\n\n\n\n\nFigure¬†2: Figure 1\n\n\nWhere \\bar x is the average of all the one hot vectors of the context words.\n\n\n\n\n\n\n\ncourse banner\n\n\n\n\nFigure¬†3: Figure 2\n\n\nOnce you have encoded all the context words, you can use \\bar x as the input to your model.\nThe architecture you will be implementing is as follows:\n\\begin{align}\nh &= W_1 \\  X + b_1  \\tag{1} \\\\\na &= ReLU(h)  \\tag{2} \\\\\nz &= W_2 \\  a + b_2   \\tag{3} \\\\\n\\hat y &= softmax(z)   \\tag{4} \\\\\n\\end{align}\n\n# Import Python libraries and helper functions (in utils2) \nimport nltk\nfrom nltk.tokenize import word_tokenize\nimport numpy as np\nfrom collections import Counter\nfrom utils2 import sigmoid, get_batches, compute_pca, get_dict\n\n\n# Download sentence tokenizer\nnltk.data.path.append('.')\n\n\n# Load, tokenize and process the data\nimport re                                                           #  Load the Regex-modul\nwith open('shakespeare.txt') as f:\n    data = f.read()                                                 #  Read in the data\ndata = re.sub(r'[,!?;-]', '.',data)                                 #  Punktuations are replaced by .\ndata = nltk.word_tokenize(data)                                     #  Tokenize string to words\ndata = [ ch.lower() for ch in data if ch.isalpha() or ch == '.']    #  Lower case and drop non-alphabetical tokens\nprint(\"Number of tokens:\", len(data),'\\n', data[:15])               #  print data sample\n\nNumber of tokens: 61011 \n ['o', 'for', 'a', 'muse', 'of', 'fire', '.', 'that', 'would', 'ascend', 'the', 'brightest', 'heaven', 'of', 'invention']\n\n\n\n# Compute the frequency distribution of the words in the dataset (vocabulary)\nfdist = nltk.FreqDist(word for word in data)\nprint(\"Size of vocabulary: \",len(fdist) )\nprint(\"Most frequent tokens: \",fdist.most_common(20) ) # print the 20 most frequent words and their freq.\n\nSize of vocabulary:  5784\nMost frequent tokens:  [('.', 9630), ('the', 1521), ('and', 1394), ('i', 1259), ('to', 1159), ('of', 1093), ('my', 857), ('that', 781), ('in', 771), ('a', 752), ('you', 748), ('is', 630), ('not', 559), ('for', 467), ('it', 460), ('with', 441), ('his', 434), ('but', 417), ('me', 417), ('your', 397)]\n\n\n\nMapping words to indices and indices to words\nWe provide a helper function to create a dictionary that maps words to indices and indices to words.\n\n# get_dict creates two dictionaries, converting words to indices and viceversa.\nword2Ind, Ind2word = get_dict(data)\nV = len(word2Ind)\nprint(\"Size of vocabulary: \", V)\n\nSize of vocabulary:  5784\n\n\n\n# example of word to index mapping\nprint(\"Index of the word 'king' :  \",word2Ind['king'] )\nprint(\"Word which has index 2743:  \",Ind2word[2743] )\n\nIndex of the word 'king' :   2747\nWord which has index 2743:   kind\n\n\n # 2 Training the Model\n\n\nInitializing the model\nYou will now initialize two matrices and two vectors. - The first matrix (W_1) is of dimension N \\times V, where V is the number of words in your vocabulary and N is the dimension of your word vector. - The second matrix (W_2) is of dimension V \\times N. - Vector b_1 has dimensions N\\times 1 - Vector b_2 has dimensions V\\times 1. - b_1 and b_2 are the bias vectors of the linear layers from matrices W_1 and W_2.\nThe overall structure of the model will look as in Figure 1, but at this stage we are just initializing the parameters.\n ### Exercise 01 Please use numpy.random.rand to generate matrices that are initialized with random values from a uniform distribution, ranging between 0 and 1.\nNote: In the next cell you will encounter a random seed. Please DO NOT modify this seed so your solution can be tested correctly.\n\n# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: initialize_model\ndef initialize_model(N,V, random_seed=1):\n    '''\n    Inputs: \n        N:  dimension of hidden vector \n        V:  dimension of vocabulary\n        random_seed: random seed for consistent results in the unit tests\n     Outputs: \n        W1, W2, b1, b2: initialized weights and biases\n    '''\n    \n    np.random.seed(random_seed)\n    \n    ### START CODE HERE (Replace instances of 'None' with your code) ###\n    # W1 has shape (N,V)\n    W1 = None\n    # W2 has shape (V,N)\n    W2 = None\n    # b1 has shape (N,1)\n    b1 = None\n    # b2 has shape (V,1)\n    b2 = None\n    ### END CODE HERE ###\n\n    return W1, W2, b1, b2\n\n\n# Test your function example.\ntmp_N = 4\ntmp_V = 10\ntmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\nassert tmp_W1.shape == ((tmp_N,tmp_V))\nassert tmp_W2.shape == ((tmp_V,tmp_N))\nprint(f\"tmp_W1.shape: {tmp_W1.shape}\")\nprint(f\"tmp_W2.shape: {tmp_W2.shape}\")\nprint(f\"tmp_b1.shape: {tmp_b1.shape}\")\nprint(f\"tmp_b2.shape: {tmp_b2.shape}\")\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[8], line 5\n      3 tmp_V = 10\n      4 tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n----&gt; 5 assert tmp_W1.shape == ((tmp_N,tmp_V))\n      6 assert tmp_W2.shape == ((tmp_V,tmp_N))\n      7 print(f\"tmp_W1.shape: {tmp_W1.shape}\")\n\nAttributeError: 'NoneType' object has no attribute 'shape'\n\n\n\n\nExpected Output\ntmp_W1.shape: (4, 10)\ntmp_W2.shape: (10, 4)\ntmp_b1.shape: (4, 1)\ntmp_b2.shape: (10, 1)\n ### 2.1 Softmax Before we can start training the model, we need to implement the softmax function as defined in equation 5:\n  \\text{softmax}(z_i) = \\frac{e^{z_i} }{\\sum_{i=0}^{V-1} e^{z_i} }  \\tag{5} \n\nArray indexing in code starts at 0.\nV is the number of words in the vocabulary (which is also the number of rows of z).\ni goes from 0 to |V| - 1.\n\n ### Exercise 02 Instructions: Implement the softmax function below.\n\nAssume that the input z to softmax is a 2D array\nEach training example is represented by a column of shape (V, 1) in this 2D array.\nThere may be more than one column, in the 2D array, because you can put in a batch of examples to increase efficiency. Let‚Äôs call the batch size lowercase m, so the z array has shape (V, m)\nWhen taking the sum from i=1 \\cdots V-1, take the sum for each column (each example) separately.\n\nPlease use - numpy.exp - numpy.sum (set the axis so that you take the sum of each column in z)\n\n# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: softmax\ndef softmax(z):\n    '''\n    Inputs: \n        z: output scores from the hidden layer\n    Outputs: \n        yhat: prediction (estimate of y)\n    '''\n    \n    ### START CODE HERE (Replace instances of 'None' with your own code) ###\n    \n    # Calculate yhat (softmax)\n    yhat = None\n    \n    ### END CODE HERE ###\n    \n    return yhat\n\n\n# Test the function\ntmp = np.array([[1,2,3],\n                [1,1,1]\n               ])\ntmp_sm = softmax(tmp)\ndisplay(tmp_sm)\n\nNone\n\n\n\n\nExpected Ouput\narray([[0.5       , 0.73105858, 0.88079708],\n       [0.5       , 0.26894142, 0.11920292]])\n ### 2.2 Forward propagation\n ### Exercise 03 Implement the forward propagation z according to equations (1) to (3). \n\\begin{align}\nh &= W_1 \\  X + b_1  \\tag{1} \\\\\na &= ReLU(h)  \\tag{2} \\\\\nz &= W_2 \\  a + b_2   \\tag{3} \\\\\n\\end{align}\nFor that, you will use as activation the Rectified Linear Unit (ReLU) given by:\nf(h)=\\max (0,h) \\tag{6}\n\n\nHints\n\n\n\n\nYou can use numpy.maximum(x1,x2) to get the maximum of two values\n\n\nUse numpy.dot(A,B) to matrix multiply A and B\n\n\n\n\n# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: forward_prop\ndef forward_prop(x, W1, W2, b1, b2):\n    '''\n    Inputs: \n        x:  average one hot vector for the context \n        W1, W2, b1, b2:  matrices and biases to be learned\n     Outputs: \n        z:  output score vector\n    '''\n    \n    ### START CODE HERE (Replace instances of 'None' with your own code) ###\n    \n    # Calculate h\n    h = None\n    \n    # Apply the relu on h (store result in h)\n    h = None\n    \n    # Calculate z\n    z = None\n    \n    ### END CODE HERE ###\n\n    return z, h\n\n\n# Test the function\n\n# Create some inputs\ntmp_N = 2\ntmp_V = 3\ntmp_x = np.array([[0,1,0]]).T\ntmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(N=tmp_N,V=tmp_V, random_seed=1)\n\nprint(f\"x has shape {tmp_x.shape}\")\nprint(f\"N is {tmp_N} and vocabulary size V is {tmp_V}\")\n\n# call function\ntmp_z, tmp_h = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\nprint(\"call forward_prop\")\nprint()\n# Look at output\nprint(f\"z has shape {tmp_z.shape}\")\nprint(\"z has values:\")\nprint(tmp_z)\n\nprint()\n\nprint(f\"h has shape {tmp_h.shape}\")\nprint(\"h has values:\")\nprint(tmp_h)\n\nx has shape (3, 1)\nN is 2 and vocabulary size V is 3\ncall forward_prop\n\n\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[12], line 17\n     15 print()\n     16 # Look at output\n---&gt; 17 print(f\"z has shape {tmp_z.shape}\")\n     18 print(\"z has values:\")\n     19 print(tmp_z)\n\nAttributeError: 'NoneType' object has no attribute 'shape'\n\n\n\n\n\nExpected output\nx has shape (3, 1)\nN is 2 and vocabulary size V is 3\ncall forward_prop\n\nz has shape (3, 1)\nz has values:\n[[0.55379268]\n [1.58960774]\n [1.50722933]]\n\nh has shape (2, 1)\nh has values:\n[[0.92477674]\n [1.02487333]]\n ## 2.3 Cost function\n\nWe have implemented the cross-entropy cost function for you.\n\n\n# compute_cost: cross-entropy cost functioN\ndef compute_cost(y, yhat, batch_size):\n    # cost function \n    logprobs = np.multiply(np.log(yhat),y) + np.multiply(np.log(1 - yhat), 1 - y)\n    cost = - 1/batch_size * np.sum(logprobs)\n    cost = np.squeeze(cost)\n    return cost\n\n\n# Test the function\ntmp_C = 2\ntmp_N = 50\ntmp_batch_size = 4\ntmp_word2Ind, tmp_Ind2word = get_dict(data)\ntmp_V = len(word2Ind)\n\ntmp_x, tmp_y = next(get_batches(data, tmp_word2Ind, tmp_V,tmp_C, tmp_batch_size))\n        \nprint(f\"tmp_x.shape {tmp_x.shape}\")\nprint(f\"tmp_y.shape {tmp_y.shape}\")\n\ntmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n\nprint(f\"tmp_W1.shape {tmp_W1.shape}\")\nprint(f\"tmp_W2.shape {tmp_W2.shape}\")\nprint(f\"tmp_b1.shape {tmp_b1.shape}\")\nprint(f\"tmp_b2.shape {tmp_b2.shape}\")\n\ntmp_z, tmp_h = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\nprint(f\"tmp_z.shape: {tmp_z.shape}\")\nprint(f\"tmp_h.shape: {tmp_h.shape}\")\n\ntmp_yhat = softmax(tmp_z)\nprint(f\"tmp_yhat.shape: {tmp_yhat.shape}\")\n\ntmp_cost = compute_cost(tmp_y, tmp_yhat, tmp_batch_size)\nprint(\"call compute_cost\")\nprint(f\"tmp_cost {tmp_cost:.4f}\")\n\ntmp_x.shape (5784, 4)\ntmp_y.shape (5784, 4)\n\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[14], line 15\n     11 print(f\"tmp_y.shape {tmp_y.shape}\")\n     13 tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n---&gt; 15 print(f\"tmp_W1.shape {tmp_W1.shape}\")\n     16 print(f\"tmp_W2.shape {tmp_W2.shape}\")\n     17 print(f\"tmp_b1.shape {tmp_b1.shape}\")\n\nAttributeError: 'NoneType' object has no attribute 'shape'\n\n\n\n\n\nExpected output\ntmp_x.shape (5778, 4)\ntmp_y.shape (5778, 4)\ntmp_W1.shape (50, 5778)\ntmp_W2.shape (5778, 50)\ntmp_b1.shape (50, 1)\ntmp_b2.shape (5778, 1)\ntmp_z.shape: (5778, 4)\ntmp_h.shape: (50, 4)\ntmp_yhat.shape: (5778, 4)\ncall compute_cost\ntmp_cost 9.9560\n ## 2.4 Training the Model - Backpropagation\n ### Exercise 04 Now that you have understood how the CBOW model works, you will train it.  You created a function for the forward propagation. Now you will implement a function that computes the gradients to backpropagate the errors.\n\n# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: back_prop\ndef back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size):\n    '''\n    Inputs: \n        x:  average one hot vector for the context \n        yhat: prediction (estimate of y)\n        y:  target vector\n        h:  hidden vector (see eq. 1)\n        W1, W2, b1, b2:  matrices and biases  \n        batch_size: batch size \n     Outputs: \n        grad_W1, grad_W2, grad_b1, grad_b2:  gradients of matrices and biases   \n    '''\n    ### START CODE HERE (Replace instanes of 'None' with your code) ###\n    \n    # Compute l1 as W2^T (Yhat - Y)\n    # Re-use it whenever you see W2^T (Yhat - Y) used to compute a gradient\n    l1 = None\n    # Apply relu to l1\n    l1 = None\n    # Compute the gradient of W1\n    grad_W1 = None\n    # Compute the gradient of W2\n    grad_W2 = None\n    # Compute the gradient of b1\n    grad_b1 = None\n    # Compute the gradient of b2\n    grad_b2 = None\n    ### END CODE HERE ###\n    \n    return grad_W1, grad_W2, grad_b1, grad_b2\n\n\n# Test the function\ntmp_C = 2\ntmp_N = 50\ntmp_batch_size = 4\ntmp_word2Ind, tmp_Ind2word = get_dict(data)\ntmp_V = len(word2Ind)\n\n# get a batch of data\ntmp_x, tmp_y = next(get_batches(data, tmp_word2Ind, tmp_V,tmp_C, tmp_batch_size))\n\nprint(\"get a batch of data\")\nprint(f\"tmp_x.shape {tmp_x.shape}\")\nprint(f\"tmp_y.shape {tmp_y.shape}\")\n\nprint()\nprint(\"Initialize weights and biases\")\ntmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n\nprint(f\"tmp_W1.shape {tmp_W1.shape}\")\nprint(f\"tmp_W2.shape {tmp_W2.shape}\")\nprint(f\"tmp_b1.shape {tmp_b1.shape}\")\nprint(f\"tmp_b2.shape {tmp_b2.shape}\")\n\nprint()\nprint(\"Forwad prop to get z and h\")\ntmp_z, tmp_h = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\nprint(f\"tmp_z.shape: {tmp_z.shape}\")\nprint(f\"tmp_h.shape: {tmp_h.shape}\")\n\nprint()\nprint(\"Get yhat by calling softmax\")\ntmp_yhat = softmax(tmp_z)\nprint(f\"tmp_yhat.shape: {tmp_yhat.shape}\")\n\ntmp_m = (2*tmp_C)\ntmp_grad_W1, tmp_grad_W2, tmp_grad_b1, tmp_grad_b2 = back_prop(tmp_x, tmp_yhat, tmp_y, tmp_h, tmp_W1, tmp_W2, tmp_b1, tmp_b2, tmp_batch_size)\n\nprint()\nprint(\"call back_prop\")\nprint(f\"tmp_grad_W1.shape {tmp_grad_W1.shape}\")\nprint(f\"tmp_grad_W2.shape {tmp_grad_W2.shape}\")\nprint(f\"tmp_grad_b1.shape {tmp_grad_b1.shape}\")\nprint(f\"tmp_grad_b2.shape {tmp_grad_b2.shape}\")\n\nget a batch of data\ntmp_x.shape (5784, 4)\ntmp_y.shape (5784, 4)\n\nInitialize weights and biases\n\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[16], line 19\n     16 print(\"Initialize weights and biases\")\n     17 tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n---&gt; 19 print(f\"tmp_W1.shape {tmp_W1.shape}\")\n     20 print(f\"tmp_W2.shape {tmp_W2.shape}\")\n     21 print(f\"tmp_b1.shape {tmp_b1.shape}\")\n\nAttributeError: 'NoneType' object has no attribute 'shape'\n\n\n\n\n\nExpected output\nget a batch of data\ntmp_x.shape (5778, 4)\ntmp_y.shape (5778, 4)\n\nInitialize weights and biases\ntmp_W1.shape (50, 5778)\ntmp_W2.shape (5778, 50)\ntmp_b1.shape (50, 1)\ntmp_b2.shape (5778, 1)\n\nForwad prop to get z and h\ntmp_z.shape: (5778, 4)\ntmp_h.shape: (50, 4)\n\nGet yhat by calling softmax\ntmp_yhat.shape: (5778, 4)\n\ncall back_prop\ntmp_grad_W1.shape (50, 5778)\ntmp_grad_W2.shape (5778, 50)\ntmp_grad_b1.shape (50, 1)\ntmp_grad_b2.shape (5778, 1)\n ## Gradient Descent\n ### Exercise 05 Now that you have implemented a function to compute the gradients, you will implement batch gradient descent over your training set.\nHint: For that, you will use initialize_model and the back_prop functions which you just created (and the compute_cost function). You can also use the provided get_batches helper function:\nfor x, y in get_batches(data, word2Ind, V, C, batch_size):\n...\nAlso: print the cost after each batch is processed (use batch size = 128)\n\n# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: gradient_descent\ndef gradient_descent(data, word2Ind, N, V, num_iters, alpha=0.03):\n    \n    '''\n    This is the gradient_descent function\n    \n      Inputs: \n        data:      text\n        word2Ind:  words to Indices\n        N:         dimension of hidden vector  \n        V:         dimension of vocabulary \n        num_iters: number of iterations  \n     Outputs: \n        W1, W2, b1, b2:  updated matrices and biases   \n\n    '''\n    W1, W2, b1, b2 = initialize_model(N,V, random_seed=282)\n    batch_size = 128\n    iters = 0\n    C = 2\n    for x, y in get_batches(data, word2Ind, V, C, batch_size):\n        ### START CODE HERE (Replace instances of 'None' with your own code) ###\n        # Get z and h\n        z, h = None\n        # Get yhat\n        yhat = None\n        # Get cost\n        cost = None\n        if ( (iters+1) % 10 == 0):\n            print(f\"iters: {iters + 1} cost: {cost:.6f}\")\n        # Get gradients\n        grad_W1, grad_W2, grad_b1, grad_b2 = None\n        \n        # Update weights and biases\n        W1 = None \n        W2 = None\n        b1 = None\n        b2 = None\n        \n        ### END CODE HERE ###\n        \n        iters += 1 \n        if iters == num_iters: \n            break\n        if iters % 100 == 0:\n            alpha *= 0.66\n            \n    return W1, W2, b1, b2\n\n\n# test your function\nC = 2\nN = 50\nword2Ind, Ind2word = get_dict(data)\nV = len(word2Ind)\nnum_iters = 150\nprint(\"Call gradient_descent\")\nW1, W2, b1, b2 = gradient_descent(data, word2Ind, N, V, num_iters)\n\nCall gradient_descent\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[18], line 8\n      6 num_iters = 150\n      7 print(\"Call gradient_descent\")\n----&gt; 8 W1, W2, b1, b2 = gradient_descent(data, word2Ind, N, V, num_iters)\n\nCell In[17], line 25, in gradient_descent(data, word2Ind, N, V, num_iters, alpha)\n     21 C = 2\n     22 for x, y in get_batches(data, word2Ind, V, C, batch_size):\n     23     ### START CODE HERE (Replace instances of 'None' with your own code) ###\n     24     # Get z and h\n---&gt; 25     z, h = None\n     26     # Get yhat\n     27     yhat = None\n\nTypeError: cannot unpack non-iterable NoneType object\n\n\n\n\n\nExpected Output\niters: 10 cost: 0.789141\niters: 20 cost: 0.105543\niters: 30 cost: 0.056008\niters: 40 cost: 0.038101\niters: 50 cost: 0.028868\niters: 60 cost: 0.023237\niters: 70 cost: 0.019444\niters: 80 cost: 0.016716\niters: 90 cost: 0.014660\niters: 100 cost: 0.013054\niters: 110 cost: 0.012133\niters: 120 cost: 0.011370\niters: 130 cost: 0.010698\niters: 140 cost: 0.010100\niters: 150 cost: 0.009566\nYour numbers may differ a bit depending on which version of Python you‚Äôre using.\n ## 3.0 Visualizing the word vectors\nIn this part you will visualize the word vectors trained using the function you just coded above.\n\n# visualizing the word vectors here\nfrom matplotlib import pyplot\n%config InlineBackend.figure_format = 'svg'\nwords = ['king', 'queen','lord','man', 'woman','dog','wolf',\n         'rich','happy','sad']\n\nembs = (W1.T + W2)/2.0\n \n# given a list of words and the embeddings, it returns a matrix with all the embeddings\nidx = [word2Ind[word] for word in words]\nX = embs[idx, :]\nprint(X.shape, idx)  # X.shape:  Number of words of dimension N each \n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[19], line 7\n      3 get_ipython().run_line_magic('config', \"InlineBackend.figure_format = 'svg'\")\n      4 words = ['king', 'queen','lord','man', 'woman','dog','wolf',\n      5          'rich','happy','sad']\n----&gt; 7 embs = (W1.T + W2)/2.0\n      9 # given a list of words and the embeddings, it returns a matrix with all the embeddings\n     10 idx = [word2Ind[word] for word in words]\n\nNameError: name 'W1' is not defined\n\n\n\n\nresult= compute_pca(X, 2)\npyplot.scatter(result[:, 0], result[:, 1])\nfor i, word in enumerate(words):\n    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\npyplot.show()\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[20], line 1\n----&gt; 1 result= compute_pca(X, 2)\n      2 pyplot.scatter(result[:, 0], result[:, 1])\n      3 for i, word in enumerate(words):\n\nNameError: name 'X' is not defined\n\n\n\nYou can see that man and king are next to each other. However, we have to be careful with the interpretation of this projected word vectors, since the PCA depends on the projection ‚Äì as shown in the following illustration.\n\nresult= compute_pca(X, 4)\npyplot.scatter(result[:, 3], result[:, 1])\nfor i, word in enumerate(words):\n    pyplot.annotate(word, xy=(result[i, 3], result[i, 1]))\npyplot.show()\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[21], line 1\n----&gt; 1 result= compute_pca(X, 4)\n      2 pyplot.scatter(result[:, 3], result[:, 1])\n      3 for i, word in enumerate(words):\n\nNameError: name 'X' is not defined",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "A4 - Word embeddings"
    ]
  },
  {
    "objectID": "posts/c3w2/index.html",
    "href": "posts/c3w2/index.html",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\n\nThis week‚Äôs slides\n\n\n\n\nFigure¬†1\nMy irreverent notes for Week 2 of the Natural Language Processing with Sequence Models Course in the Natural Language Processing Specialization Offered by DeepLearning.AI on Coursera",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c3w2/index.html#traditional-language-models",
    "href": "posts/c3w2/index.html#traditional-language-models",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "Traditional Language models",
    "text": "Traditional Language models\n\nIn the example above, the second sentence is the one that is most likely to take place as it has the highest probability of happening. To compute the probabilities, we can do the following:\n\nLarge N-grams capture dependencies between distant words and need a lot of space and RAM. Hence, we resort to using different types of alternatives.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c3w2/index.html#recurrent-neural-networks",
    "href": "posts/c3w2/index.html#recurrent-neural-networks",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "Recurrent Neural Networks",
    "text": "Recurrent Neural Networks\nPreviously, we tried using traditional language models, but it turns out they took a lot of space and RAM. For example, in the sentence below:\n\nAn N-gram (trigram) would only look at ‚Äúdid not‚Äù and would try to complete the sentence from there. As a result, the model will not be able to see the beginning of the sentence ‚ÄúI called her but she‚Äù. Probably the most likely word is have after ‚Äúdid not‚Äù. RNNs help us solve this problem by being able to track dependencies that are much further apart from each other. As the RNN makes its way through a text corpus, it picks up some information as follows:\n\nNote that as we feed in more information into the model, the previous word‚Äôs retention gets weaker, but it is still there. Look at the orange rectangle above and see how it becomes smaller as we make your way through the text. This shows that your model is capable of capturing dependencies and remembers a previous word although it is at the beginning of a sentence or paragraph. Another advantage of RNNs is that a lot of the computation shares parameters.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c3w2/index.html#application-of-rnns",
    "href": "posts/c3w2/index.html#application-of-rnns",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "Application of RNNs",
    "text": "Application of RNNs\nRNNs could be used in a variety of tasks ranging from machine translation to caption generation. There are many ways to implement an RNN model:\n\nOne to One: given some scores of a championship, we can predict the winner.\nOne to Many: given an image, we can predict what the caption is going to be.\nMany to One: given a tweet, we can predict the sentiment of that tweet.\nMany to Many: given an english sentence, we can translate it to its German equivalent.\n\nIn the next video, we will see the math in simple RNNs.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c3w2/index.html#math-in-simple-rnns",
    "href": "posts/c3w2/index.html#math-in-simple-rnns",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "Math in Simple RNNs",
    "text": "Math in Simple RNNs\nIt is best to explain the math behind a simple RNN with a diagram:\n\nNote that:\n\nh^{&lt;t&gt;} = g(W_{h}[h^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_h)\n\nIs the same as multiplying W_{hh} by h and W_{hx} by x. In other words, we can concatenate it as follows:\n\nh^{&lt;t&gt;} = g(W_{hh}[h^{&lt;t-1&gt;} \\oplus W_{hx}x^{&lt;t&gt;}] + b_h)\n\nFor the prediction at each time step, we can use the following:\n\n\\hat{y}^{&lt;t&gt;} = g(W_{yh}h^{&lt;t&gt;} + b_y)\n\nNote that we end up training W_{hh}, W_{hx}, W_{yh}, b_h, and b_y. Here is a visualization of the model.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c3w2/index.html#lab-lecture-notebook-hidden-state-activation",
    "href": "posts/c3w2/index.html#lab-lecture-notebook-hidden-state-activation",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "Lab: Lecture Notebook: Hidden State Activation",
    "text": "Lab: Lecture Notebook: Hidden State Activation\nHidden State Activation",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c3w2/index.html#cost-function-for-rnns",
    "href": "posts/c3w2/index.html#cost-function-for-rnns",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "Cost Function for RNNs",
    "text": "Cost Function for RNNs\nThe cost function used in an RNN is the cross entropy loss. If we were to visualize it\n\nwe are basically summing over the all the classes and then multiplying y_j times log(\\hat{y}_j). If we were to compute the loss over several time steps, use the following formula:\n\nJ = -\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{j=1}^{K} y_j^{&lt;t&gt;} log \\hat{y}^{&lt;t&gt;}\n\nwhere T is the number of time steps and K is the number of classes.\nNote that we are simply summing over all the time steps and dividing by T, to get the average cost in each time step. Hence, we are just taking an average through time.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c3w2/index.html#implementation-note",
    "href": "posts/c3w2/index.html#implementation-note",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "Implementation Note",
    "text": "Implementation Note\nThe scan function is built as follows:\n\nNote, that is basically what an RNN is doing. It takes the initializer, and returns a list of outputs (ys), and uses the current value, to get the next y and the next current value. These type of abstractions allow for much faster computation.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c3w2/index.html#lab-working-with-jax-numpy-and-calculating-perplexity",
    "href": "posts/c3w2/index.html#lab-working-with-jax-numpy-and-calculating-perplexity",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "Lab: Working with JAX NumPy and Calculating Perplexity",
    "text": "Lab: Working with JAX NumPy and Calculating Perplexity\nWorking with JAX NumPy and Calculating Perplexity",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c3w2/index.html#gated-recurrent-units",
    "href": "posts/c3w2/index.html#gated-recurrent-units",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "Gated Recurrent Units",
    "text": "Gated Recurrent Units\nGated recurrent units are very similar to vanilla RNNs, except that they have a ‚Äúrelevance‚Äù and ‚Äúupdate‚Äù gate that allow the model to update and get relevant information. I personally find it easier to understand by looking at the formulas:\n\nTo the left, we have the diagram and equations for a simple RNN. To the right, we explain the GRU. Note that we add 3 layers before computing h and y.\n\n\\begin{align*}\n\\Gamma_u &= \\sigma(W_u[h^{&lt;t-1&gt;}, x^{&lt;t&gt;}]+b_u) \\\\\n\\Gamma_r &= \\sigma(W_r[h^{&lt;t-1&gt;}, x^{&lt;t&gt;}]+b_r) \\\\\nh'^{&lt;t&gt;} &= \\tanh(W_h[\\Gamma_r*h^{&lt;t-1&gt;}, x^{&lt;t&gt;}]+b_h) \\\\\nh^{&lt;t&gt;} &= \\Gamma_u*h^{&lt;t-1&gt;} + (1-\\Gamma_u)*h'^{&lt;t&gt;}\n\\end{align*}\n\nThe first gate Œì_u allows we to decide how much we want to update the weights by. The second gate Œì_r, helps we find a relevance score. We can compute the new h by using the relevance gate. Finally we can compute h, using the update gate. GRUs ‚Äúdecide‚Äù how to update the hidden state. GRUs help preserve important information.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c3w2/index.html#lab-vanilla-rnns-grus-and-the-scan-function",
    "href": "posts/c3w2/index.html#lab-vanilla-rnns-grus-and-the-scan-function",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "Lab: Vanilla RNNs, GRUs and the scan function",
    "text": "Lab: Vanilla RNNs, GRUs and the scan function\nVanilla RNNs, GRUs and the scan function",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c3w2/index.html#lab-creating-a-gru-model-using-trax",
    "href": "posts/c3w2/index.html#lab-creating-a-gru-model-using-trax",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "Lab: Creating a GRU model using Trax",
    "text": "Lab: Creating a GRU model using Trax\nCreating a GRU model using Trax",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c3w2/index.html#deep-and-bi-directional-rnns",
    "href": "posts/c3w2/index.html#deep-and-bi-directional-rnns",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "Deep and Bi-directional RNNs",
    "text": "Deep and Bi-directional RNNs\nBi-directional RNNs are important, because knowing what is next in the sentence could give we more context about the sentence itself.\n\nSo we can see, in order to make a prediction \\hat{y}, we will use the hidden states from both directions and combine them to make one hidden state, we can then proceed as we would with a simple vanilla RNN. When implementing Deep RNNs, we would compute the following.\n\nNote that at layer l, we are using the input from the bottom a^{[;-1]} and the hidden state h^{&lt;l&gt;} That allows we to get your new h, and then to get your new a, we will train another weight matrix W_{a}, which we will multiply by the corresponding h add the bias and then run it through an activation layer.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c3w2/index.html#resources",
    "href": "posts/c3w2/index.html#resources",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "Resources:",
    "text": "Resources:\n\n(Chadha 2020) Aman Chadha‚Äôs Notes\nIbrahim Jelliti‚Äôs Notes",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c3w2/lab01.html",
    "href": "posts/c3w2/lab01.html",
    "title": "Hidden State Activation : Ungraded Lecture Notebook",
    "section": "",
    "text": "course banner\nIn this notebook you‚Äôll take another look at the hidden state activation function. It can be written in two different ways.\nI‚Äôll show you, step by step, how to implement each of them and then how to verify whether the results produced by each of them are same or not."
  },
  {
    "objectID": "posts/c3w2/lab01.html#background",
    "href": "posts/c3w2/lab01.html#background",
    "title": "Hidden State Activation : Ungraded Lecture Notebook",
    "section": "Background",
    "text": "Background\n\n\n\nvanilla rnn\n\n\nThis is the hidden state activation function for a vanilla RNN.\nh^{&lt;t&gt;}=g(W_{h}[h^{&lt;t-1&gt;},x^{&lt;t&gt;}] + b_h)\nWhich is another way of writing this:\nh^{&lt;t&gt;}=g(W_{hh}h^{&lt;t-1&gt;} \\oplus W_{hx}x^{&lt;t&gt;} + b_h)\nWhere\n\nW_{h} in the first formula is denotes the horizontal concatenation of W_{hh} and W_{hx} from the second formula.\nW_{h} in the first formula is then multiplied by [h^{&lt;t-1&gt;},x^{&lt;t&gt;}], another concatenation of parameters from the second formula but this time in a different direction, i.e vertical!\n\nLet us see what this means computationally."
  },
  {
    "objectID": "posts/c3w2/lab01.html#imports",
    "href": "posts/c3w2/lab01.html#imports",
    "title": "Hidden State Activation : Ungraded Lecture Notebook",
    "section": "Imports",
    "text": "Imports\n\nimport numpy as np"
  },
  {
    "objectID": "posts/c3w2/lab01.html#joining-concatenation",
    "href": "posts/c3w2/lab01.html#joining-concatenation",
    "title": "Hidden State Activation : Ungraded Lecture Notebook",
    "section": "Joining (Concatenation)",
    "text": "Joining (Concatenation)\n\nWeights\nA join along the vertical boundary is called a horizontal concatenation or horizontal stack.\nVisually, it looks like this:- W_h = \\left [ W_{hh} \\ | \\ W_{hx} \\right ]\nI‚Äôll show you two different ways to achieve this using numpy.\nNote: The values used to populate the arrays, below, have been chosen to aid in visual illustration only. They are NOT what you‚Äôd expect to use building a model, which would typically be random variables instead.\n\nTry using random initializations for the weight arrays.\n\n\n# Create some dummy data\n\nw_hh = np.full((3, 2), 1)  # illustration purposes only, returns an array of size 3x2 filled with all 1s\nw_hx = np.full((3, 3), 9)  # illustration purposes only, returns an array of size 3x3 filled with all 9s\n\n\n### START CODE HERE ###\n# Try using some random initializations, though it will obfuscate the join. eg: uncomment these lines\n# w_hh = np.random.standard_normal((3,2))\n# w_hx = np.random.standard_normal((3,3))\n### END CODE HERE ###\n\nprint(\"-- Data --\\n\")\nprint(\"w_hh :\")\nprint(w_hh)\nprint(\"w_hh shape :\", w_hh.shape, \"\\n\")\nprint(\"w_hx :\")\nprint(w_hx)\nprint(\"w_hx shape :\", w_hx.shape, \"\\n\")\n\n# Joining the arrays\nprint(\"-- Joining --\\n\")\n# Option 1: concatenate - horizontal\nw_h1 = np.concatenate((w_hh, w_hx), axis=1)\nprint(\"option 1 : concatenate\\n\")\nprint(\"w_h :\")\nprint(w_h1)\nprint(\"w_h shape :\", w_h1.shape, \"\\n\")\n\n# Option 2: hstack\nw_h2 = np.hstack((w_hh, w_hx))\nprint(\"option 2 : hstack\\n\")\nprint(\"w_h :\")\nprint(w_h2)\nprint(\"w_h shape :\", w_h2.shape)\n\n-- Data --\n\nw_hh :\n[[1 1]\n [1 1]\n [1 1]]\nw_hh shape : (3, 2) \n\nw_hx :\n[[9 9 9]\n [9 9 9]\n [9 9 9]]\nw_hx shape : (3, 3) \n\n-- Joining --\n\noption 1 : concatenate\n\nw_h :\n[[1 1 9 9 9]\n [1 1 9 9 9]\n [1 1 9 9 9]]\nw_h shape : (3, 5) \n\noption 2 : hstack\n\nw_h :\n[[1 1 9 9 9]\n [1 1 9 9 9]\n [1 1 9 9 9]]\nw_h shape : (3, 5)\n\n\n\n\nHidden State & Inputs\nJoining along a horizontal boundary is called a vertical concatenation or vertical stack. Visually it looks like this:\n[h^{&lt;t-1&gt;},x^{&lt;t&gt;}] = \\left[ \\frac{h^{&lt;t-1&gt;}}{x^{&lt;t&gt;}} \\right]\nI‚Äôll show you two different ways to achieve this using numpy.\nTry using random initializations for the hiddent state and input matrices.\n\n# Create some more dummy data\nh_t_prev = np.full((2, 1), 1)  # illustration purposes only, returns an array of size 2x1 filled with all 1s\nx_t = np.full((3, 1), 9)       # illustration purposes only, returns an array of size 3x1 filled with all 9s\n\n# Try using some random initializations, though it will obfuscate the join. eg: uncomment these lines\n\n### START CODE HERE ###\n# h_t_prev = np.random.standard_normal((2,1))\n# x_t = np.random.standard_normal((3,1))\n### END CODE HERE ###\n\nprint(\"-- Data --\\n\")\nprint(\"h_t_prev :\")\nprint(h_t_prev)\nprint(\"h_t_prev shape :\", h_t_prev.shape, \"\\n\")\nprint(\"x_t :\")\nprint(x_t)\nprint(\"x_t shape :\", x_t.shape, \"\\n\")\n\n# Joining the arrays\nprint(\"-- Joining --\\n\")\n\n# Option 1: concatenate - vertical\nax_1 = np.concatenate(\n    (h_t_prev, x_t), axis=0\n)  # note the difference in axis parameter vs earlier\nprint(\"option 1 : concatenate\\n\")\nprint(\"ax_1 :\")\nprint(ax_1)\nprint(\"ax_1 shape :\", ax_1.shape, \"\\n\")\n\n# Option 2: vstack\nax_2 = np.vstack((h_t_prev, x_t))\nprint(\"option 2 : vstack\\n\")\nprint(\"ax_2 :\")\nprint(ax_2)\nprint(\"ax_2 shape :\", ax_2.shape)\n\n-- Data --\n\nh_t_prev :\n[[1]\n [1]]\nh_t_prev shape : (2, 1) \n\nx_t :\n[[9]\n [9]\n [9]]\nx_t shape : (3, 1) \n\n-- Joining --\n\noption 1 : concatenate\n\nax_1 :\n[[1]\n [1]\n [9]\n [9]\n [9]]\nax_1 shape : (5, 1) \n\noption 2 : vstack\n\nax_2 :\n[[1]\n [1]\n [9]\n [9]\n [9]]\nax_2 shape : (5, 1)"
  },
  {
    "objectID": "posts/c3w2/lab01.html#verify-formulas",
    "href": "posts/c3w2/lab01.html#verify-formulas",
    "title": "Hidden State Activation : Ungraded Lecture Notebook",
    "section": "Verify Formulas",
    "text": "Verify Formulas\nNow you know how to do the concatenations, horizontal and vertical, lets verify if the two formulas produce the same result.\nFormula 1: h^{&lt;t&gt;}=g(W_{h}[h^{&lt;t-1&gt;},x^{&lt;t&gt;}] + b_h)\nFormula 2: h^{&lt;t&gt;}=g(W_{hh}h^{&lt;t-1&gt;} \\oplus W_{hx}x^{&lt;t&gt;} + b_h)\nTo prove:- Formula 1 \\Leftrightarrow Formula 2\nWe will ignore the bias term b_h and the activation function g(\\ ) because the transformation will be identical for each formula. So what we really want to compare is the result of the following parameters inside each formula:\n$W_{h}[h{},x{}] W_{hh}h^{} W_{hx}x^{} $\nWe‚Äôll see how to do this using matrix multiplication combined with the data and techniques (stacking/concatenating) from above.\n\nTry adding a sigmoid activation function and bias term to the checks for completeness.\n\n\n# Data\n\nw_hh = np.full((3, 2), 1)  # returns an array of size 3x2 filled with all 1s\nw_hx = np.full((3, 3), 9)  # returns an array of size 3x3 filled with all 9s\nh_t_prev = np.full((2, 1), 1)  # returns an array of size 2x1 filled with all 1s\nx_t = np.full((3, 1), 9)       # returns an array of size 3x1 filled with all 9s\n\n\n# If you want to randomize the values, uncomment the next 4 lines\n\n# w_hh = np.random.standard_normal((3,2))\n# w_hx = np.random.standard_normal((3,3))\n# h_t_prev = np.random.standard_normal((2,1))\n# x_t = np.random.standard_normal((3,1))\n\n# Results\nprint(\"-- Results --\")\n# Formula 1\nstack_1 = np.hstack((w_hh, w_hx))\nstack_2 = np.vstack((h_t_prev, x_t))\n\nprint(\"\\nFormula 1\")\nprint(\"Term1:\\n\",stack_1)\nprint(\"Term2:\\n\",stack_2)\nformula_1 = np.matmul(np.hstack((w_hh, w_hx)), np.vstack((h_t_prev, x_t)))\nprint(\"Output:\")\nprint(formula_1)\n\n# Formula 2\nmul_1 = np.matmul(w_hh, h_t_prev)\nmul_2 = np.matmul(w_hx, x_t)\nprint(\"\\nFormula 2\")\nprint(\"Term1:\\n\",mul_1)\nprint(\"Term2:\\n\",mul_2)\n\nformula_2 = np.matmul(w_hh, h_t_prev) + np.matmul(w_hx, x_t)\nprint(\"\\nOutput:\")\nprint(formula_2, \"\\n\")\n\n# Verification \n# np.allclose - to check if two arrays are elementwise equal upto certain tolerance, here  \n# https://numpy.org/doc/stable/reference/generated/numpy.allclose.html\n\nprint(\"-- Verify --\")\nprint(\"Results are the same :\", np.allclose(formula_1, formula_2))\n\n### START CODE HERE ###\n# # Try adding a sigmoid activation function and bias term as a final check\n# # Activation\n# def sigmoid(x):\n#     return 1 / (1 + np.exp(-x))\n\n# # Bias and check\n# b = np.random.standard_normal((formula_1.shape[0],1))\n# print(\"Formula 1 Output:\\n\",sigmoid(formula_1+b))\n# print(\"Formula 2 Output:\\n\",sigmoid(formula_2+b))\n\n# all_close = np.allclose(sigmoid(formula_1+b), sigmoid(formula_2+b))\n# print(\"Results after activation are the same :\",all_close)\n### END CODE HERE ###\n\n-- Results --\n\nFormula 1\nTerm1:\n [[1 1 9 9 9]\n [1 1 9 9 9]\n [1 1 9 9 9]]\nTerm2:\n [[1]\n [1]\n [9]\n [9]\n [9]]\nOutput:\n[[245]\n [245]\n [245]]\n\nFormula 2\nTerm1:\n [[2]\n [2]\n [2]]\nTerm2:\n [[243]\n [243]\n [243]]\n\nOutput:\n[[245]\n [245]\n [245]] \n\n-- Verify --\nResults are the same : True"
  },
  {
    "objectID": "posts/c3w2/lab01.html#summary",
    "href": "posts/c3w2/lab01.html#summary",
    "title": "Hidden State Activation : Ungraded Lecture Notebook",
    "section": "Summary",
    "text": "Summary\nThat‚Äôs it! We‚Äôve verified that the two formulas produce the same results, and seen how to combine matrices vertically and horizontally to make that happen. We now have all the intuition needed to understand the math notation of RNNs."
  },
  {
    "objectID": "posts/c3w2/lab04.html",
    "href": "posts/c3w2/lab04.html",
    "title": "Creating a GRU model using Trax: Ungraded Lecture Notebook",
    "section": "",
    "text": "course banner\nFor this lecture notebook you will be using Trax‚Äôs layers. These are the building blocks for creating neural networks with Trax.\nimport trax\nfrom trax import layers as tl\n\n2025-02-05 18:41:32.506060: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1738773692.522541  546416 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1738773692.526991  546416 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nTrax allows to define neural network architectures by stacking layers (similarly to other libraries such as Keras). For this the Serial() is often used as it is a combinator that allows to stack layers serially using function composition.\nNext you can see a simple vanilla NN architecture containing 1 hidden(dense) layer with 128 cells and output (dense) layer with 10 cells on which we apply the final layer of logsoftmax.\nmlp = tl.Serial(\n  tl.Dense(128),\n  tl.Relu(),\n  tl.Dense(10),\n  tl.LogSoftmax()\n)\nEach of the layers within the Serial combinator layer is considered a sublayer. Notice that unlike similar libraries, in Trax the activation functions are considered layers. To know more about the Serial layer check the docs here.\nYou can try printing this object:\nprint(mlp)\n\nSerial[\n  Dense_128\n  Serial[\n    Relu\n  ]\n  Dense_10\n  LogSoftmax\n]\nPrinting the model gives you the exact same information as the model‚Äôs definition itself.\nBy just looking at the definition you can clearly see what is going on inside the neural network. Trax is very straightforward in the way a network is defined, that is one of the things that makes it awesome!"
  },
  {
    "objectID": "posts/c3w2/lab04.html#gru-model",
    "href": "posts/c3w2/lab04.html#gru-model",
    "title": "Creating a GRU model using Trax: Ungraded Lecture Notebook",
    "section": "GRU MODEL",
    "text": "GRU MODEL\nTo create a GRU model you will need to be familiar with the following layers (Documentation link attached with each layer name): - ShiftRight Shifts the tensor to the right by padding on axis 1. The mode should be specified and it refers to the context in which the model is being used. Possible values are: ‚Äòtrain‚Äô, ‚Äòeval‚Äô or ‚Äòpredict‚Äô, predict mode is for fast inference. Defaults to ‚Äútrain‚Äù.\n\nEmbedding Maps discrete tokens to vectors. It will have shape (vocabulary length X dimension of output vectors). The dimension of output vectors (also called d_feature) is the number of elements in the word embedding.\nGRU The GRU layer. It leverages another Trax layer called GRUCell. The number of GRU units should be specified and should match the number of elements in the word embedding. If you want to stack two consecutive GRU layers, it can be done by using python‚Äôs list comprehension.\nDense Vanilla Dense layer.\nLogSoftMax Log Softmax function.\n\nPutting everything together the GRU model will look like this:\n\nmode = 'train'\nvocab_size = 256\nmodel_dimension = 512\nn_layers = 2\n\nGRU = tl.Serial(\n      tl.ShiftRight(mode=mode), # Do remember to pass the mode parameter if you are using it for interence/test as default is train \n      tl.Embedding(vocab_size=vocab_size, d_feature=model_dimension),\n      [tl.GRU(n_units=model_dimension) for _ in range(n_layers)], # You can play around n_layers if you want to stack more GRU layers together\n      tl.Dense(n_units=vocab_size),\n      tl.LogSoftmax()\n    )\n\nNext is a helper function that prints information for every layer (sublayer within Serial):\nTry changing the parameters defined before the GRU model and see how it changes!\n\ndef show_layers(model, layer_prefix=\"Serial.sublayers\"):\n    print(f\"Total layers: {len(model.sublayers)}\\n\")\n    for i in range(len(model.sublayers)):\n        print('========')\n        print(f'{layer_prefix}_{i}: {model.sublayers[i]}\\n')\n        \nshow_layers(GRU)\n\nTotal layers: 6\n\n========\nSerial.sublayers_0: Serial[\n  ShiftRight(1)\n]\n\n========\nSerial.sublayers_1: Embedding_256_512\n\n========\nSerial.sublayers_2: GRU_512\n\n========\nSerial.sublayers_3: GRU_512\n\n========\nSerial.sublayers_4: Dense_256\n\n========\nSerial.sublayers_5: LogSoftmax\n\n\n\nHope you are now more familiarized with creating GRU models using Trax.\nYou will train this model in this week‚Äôs assignment and see it in action.\nGRU and the trax minions will return, in this week‚Äôs endgame."
  },
  {
    "objectID": "posts/c4w2/index.html",
    "href": "posts/c4w2/index.html",
    "title": "Week 2 - Text Summarization",
    "section": "",
    "text": "deeplearning.ai\nFor the impatient - We‚Äôll start with NLP engineering insights from this week.\n:::",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c4w2/index.html#sec-transformers-vs-rnns",
    "href": "posts/c4w2/index.html#sec-transformers-vs-rnns",
    "title": "Week 2 - Text Summarization",
    "section": "Transformers vs RNNs",
    "text": "Transformers vs RNNs\nRNNs were a big breakthrough and became the state of the art (SOTA) for machine translation (MT).\nThis illustrates a typical RNN that is used to translate the English sentence ‚ÄúHow are you?‚Äù to its German equivalent, ‚ÄúWie sind Sie?‚Äù.\n\n\n\n\nrnn-non-parallel\n\n\n\n\nlstm\n\n\nThe LSTM which goes a long way to solving the vanishing gradient problems requires three times the memory and cpu steps a the vanilla RNN.\nHowever, as time went by and models got longer and deeper the biggest challenge with improving RNNs, became their use of sequential computation.\n\n\n\n\nseq2seq-steps\n\nWhich entailed that to process the word ‚Äúyou‚Äù, the RNN it has to first go through ‚Äúare‚Äù and then ‚Äúyou‚Äù. Two other issues with RNNs are the:\n\nInformation loss\nIt becomes harder to keep track of whether the subject is singular or plural as we move further away from the subject.\n\n\n\n\nTransformer\n\ntransformer architecture:\nin the encoder side - lookup layer - the source sequence is converted from one hot encoding to a distributed representation using an embedding. - this is converted to K V matrices in the decoder side\n\n\nVanishing Gradient Problem\nWhen gradients we back-propagate, the gradients can become really small and as a result.\nWith small gradient the model will learn very little.\n\n\n\n\npositional-encoding\n\nTransformers which are based on attention and don‚Äôt require any sequential computation per layer, only a single step is needed.\n\n\n\n\nsummary\n\nAdditionally, the gradient steps that need to be taken from the last output to the first input in a transformer is just one. For RNNs, the number of steps increases with longer sequences. Finally, transformers don‚Äôt suffer from vanishing gradients problems that are related to the length of the sequences.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c4w2/index.html#sec-transformer-applications",
    "href": "posts/c4w2/index.html#sec-transformer-applications",
    "title": "Week 2 - Text Summarization",
    "section": "Transformer Applications",
    "text": "Transformer Applications\n Transformers have essentially replaced RNN,LSTM and GRUs in sequence processing.\n\n\n\n\napplication-NLP\n\n\nApplications of Transformers\n\nText summarization\nAutocomplete\nNER\nQ&A\nTranslation\nChat Bots\nSentiment Analyses\nMarket Intelligence\nText Classification\nOCR\nSpell Checking\n\n\n\n\n\nsota\n\n\n\nSOTA Transformers\nTransformers Time Line:\n\nGPT-4:\nElmO\nBERT\nT5",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c4w2/index.html#sec-t5",
    "href": "posts/c4w2/index.html#sec-t5",
    "title": "Week 2 - Text Summarization",
    "section": "T5 - Text-To-Text Transfer Transformer",
    "text": "T5 - Text-To-Text Transfer Transformer\n\n\n\n\nt5\n\n\n\n\nt5\n\n\n(Raffel et al. 2019) introduced T5 which can do a number of tasks with a single model. While the earlier transformer models were able to score high in many different tasks without specific training. T5 is setup to handle different inputs and respond with output that is relevant to the requested task.\n\nT5 Classification tasks\nThese tasks are selected using the initial string: - Translate English into German - Cola sentence - Question\n\n\n\n\ntext-to-text-transformer\n\n\n\nT5 regression tasks\n\nStbs Sentence1 ‚Ä¶ Stbs Sentence2 ‚Ä¶\nSummarize:\n\nplay trivia against T5 here\n\n\n\n\ntransformers quiz\n\n\n\n\n\n\n\nWarning\n\n\n\nI found this one a little confusing\n\n\nWe are told that the transformers can do in one operation what RNN needed to do in many steps. Also when querying transformers it does one task at a time. It seem that this question is about the ability of multiple heads to do several tasks at once could not do this is not well understood.\n\n\n\n\nsummary of transformers",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c4w2/index.html#sec-dot-product-attention",
    "href": "posts/c4w2/index.html#sec-dot-product-attention",
    "title": "Week 2 - Text Summarization",
    "section": "Dot-Product Attention",
    "text": "Dot-Product Attention\n\n\n\n\noutline-of-dot-product-attention\n\nDot product attention was introduced in 2015 by Minh-Thang Luong, Hieu Pham, Christopher D. Manning in Effective Approaches to Attention-based Neural Machine Translation which is available at papers with code.\nLook at Review of Effective Approaches to Attention-based NMT\nDot product attention is the main operation in transformers. It is the dot product between the embedding of source and target sequences. The embedding used is a cross language embedding in which distance between equivalent across languages are minimized. This facilitates finding the cosine similarity using the dot product between words.\n\n\n\n\nintro-to-attention\n\nLet‚Äôs try to understand dot product attention intuitively by walking over its operations at the word level. The actual algorithm uses linear algebra to perform many operations at once which is fast but more abstract and therefore difficult to understand.\n\nUsing a pre-trained cross-language embedding encode:\n\neach German word vector q_i is placed as a column vector to form the query matrix Q,\neach English word once as k_i and once as v_i, column vectors in the key K and value V matrices. This is more of a preprocessing step.\n\nFor each German word we want to derive a continuous filter function on the English sequence to pick the most relevant words for translation. We build this filter for word q_i by taking its dot product q_i \\cdot k_i with every word vector from the english sequence these products are called the the attention weights.\nnext we convert the rudimentary filter to a probabilistic one by applying a softmax() which is just a differentiable function that converts the attention weights to probabilities by keeping them at the same relative sizes while ensuring they add to one.\nnow that we have a q-filter we want to apply it. This is done by taking the weighed sum of the english words using the attention weights.\n\n\n\\hat q_i = \\sum_{i} softmax(q_i \\cdot k_i) \\times v_i =  \\sum w_a(q_i) * v_i\n\n\nQuery, Key & Value\n\n\n\n\nqueries-keys-values\n\nI find it fascinating that the authors of Attention is all decided to motivate their work on a attention using the language of information retrieval. This makes understanding the concepts a little easier and has also lead to more recent work on LSTM to use this same language. In both case though the authors are interested in the ability of the neural network to use the wghiets that it has learned to process a long sequence of test and put together a coherent output based on distant and often sparse parts of the input. In a named entity recognition task the network has a fairly simple task to do - it needs to classify a few token based a few cues from the immediate context. But for the text summarization task the network has to understand the text and pick up the most salient bit while discarding the rest.\n\n\n\n\n\n\nQuery, Key & Value\n\n\n\nAttention uses three matrices which are formed as shown in the figure The Query Q, Key K and Value V are formed from the source and target (if there is no target then just from the source). Each word is converted into an embedding column vector and these are placed into the matracies as their columns.\nIn Video¬†1 Dr.¬†≈Åukasz Kaiser talks about attention and here he is talking about solving the problem of retrieving information from a long sequence. At around 16 minutes in he call Q a query vector and K and V a memory, of all the words we have seen, which we want to access.\n\nThe Query is the matrix formed from the column word vector for the German words.\nThe Key is the matrix formed from the column word vector for the English words.\nThe Value is the matrix formed from the column word vector for the English words.\n\nK and V are the same\n\n\nOnce these are called keys since we use them to are we doing a similarity lookup. And the second time they are called value because we use them in the activation when we apply the weights to them. The input and output sequences are mapped to an embedding layer to become the Q, K and V matrices.\n\n\n\n\n\n\n\nVideo¬†1: Lukasz Kaiser‚Äôs Masterclass on Attention is all you need\n\n\nGiven an input, we transform it into a new representation or a column vector. Depending on the task we are working on, we will end up getting queries, keys, and values. Each column corresponds to a word in the figure above. Hence, when we compute the following:\n\n\n\n\nattention-formula\n\n\nmultiply Q by V.\napply the softmax() to transform to a probability.\nmultiply the softmax by V\n\n\n\n\n\nattention-math\n\nThis is restating the above in a very confusing way. I looked at it many times before I figured out that the square brackets are the dimensions and that we have the following two formulas indicated schematically above:\n\nZ = W_A V\n\nwhere:\n\nZ has size of is a ‚ÄòQ length‚Äô \\times ‚ÄòEmbedding size‚Äô matrix\nor for coders [len(Q),D] dimensional array\n\n\nW_A = softmax(QK^T)\n\nThis concept implies that similar vectors are likely to have a higher score when we dot them with one another. We transform that score into a probability by using a softmax function. We can then multiply the output by\nWe can think of the keys and the values as being the same. Note that both K,V are of dimension L_k, D. Each query q_i picks the most similar key k_j.\n\n\n\n\nattention-formula\n\nQueries are the German words and the keys are the English words. Once we have the attention weights, we can just multiply it by V to get a weighted combination of the input.\n\n\n\n\nattention-quiz\n\n\n\n\nsummary-for-dot-product-attention\n\n\nanother interesting point made in the preceding talk is that dot product attention has O(n^2 *d) complexity but typically d &gt;&gt; n since d ~ 1000 while for n ~ 70. So transformers should perform better then an RNN whose complexity is O(n*d^2). And this is before the advantages of using an efficient transformer like reformer.\nIn (Kasai et al. 2021) there is a reversal of the trend from rnn to transformers. Here the latest results show a an idea of training big transformers and then converting them to RNN to improve performance. (One get an RNN by training a transformer.)",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c4w2/index.html#sec-causal-attention",
    "href": "posts/c4w2/index.html#sec-causal-attention",
    "title": "Week 2 - Text Summarization",
    "section": "V4: Causal Attention",
    "text": "V4: Causal Attention\n\nWe are interested in three main types of attention.\nWe‚Äôll see a brief overview of causal attention.\nWe‚Äôll discover some mathematical foundations behind the causal attention.\n\n\n\n\n\nthree forms of attention\n\nIn terms of use cases there are three types of attention mechanisms:\n\nScaled dot product attention:\n\nAKA Encoder-Decoder attention.\none sentence in the decoder look at to another one in the encoder.\nuse cases:\n\nseq2seq\nmachine translation.\n\n\n\n\nCausal Attention:\n\nAKA self attention.\nattention is all we need.\nIn the same sentence words attend to previous words.\nFuture words have not been generated yet.\nuse cases:\n\ngeneration\ntext generation\nsummarization.\n\n\n\n\nBi-directional self attention:\n\nIn one sentence words look both at previous and future words.\nuse cases:\n\nmachine comprehension.\nquestion answering\n\n\n\n\n\n\ncausal attention\n\nIn causal attention, queries and keys come from the same sentence. That is why it is often referred to as self-attention. In general, causal attention allows words to attend to other words that are related in various ways.\n\n\n\n\ncausal attention mask\n\nAt a high-level We have K Q V matrices. corresponding However, token should not attend to words in the future since these were not generated yet. Therefore the future token‚Äôs data is masked by adding a big negative number.\n\n\n\n\ncausal-attention-math-\n\nMathematically, it looks like this:\n\n\n\n\ncausal-attention-quiz\n\n\n\n\nsummary-for-causal-attention",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c4w2/index.html#sec-multi-head-attention",
    "href": "posts/c4w2/index.html#sec-multi-head-attention",
    "title": "Week 2 - Text Summarization",
    "section": "V5: Multi-head Attention",
    "text": "V5: Multi-head Attention\n Let‚Äôs summarize the intuition behind multi-head attention and scaled dot product attention.\n\n\n\n\nmuti-head-attention\n\nQ. What are multiple attention heads?\n\nMultiple attention heads are simply replicas of the attention mechanism. In this they are analogous to the multiple filters used in a convolutional neural networks (CNN).\nDuring training they specialize by learning different relationships between words.\nDuring inference the operate parallel and independently of each other.\n\n\n\n\n\noverview of muti-head attention\n\nThis is perhaps the most important slide - but it fails to show the critical part of the algorithm.\nLet‚Äôs suppose we have k attention heads. We see at the lowest level the K, Q and V being passed into passing through k linear layers. How is this accomplished and more important why. What is actually happening here is the opposite of concatenation. Instead of processing a query embedding from a space of d-dimensions we first split the embedding into k vectors of length D/k. We have now k vectors from a k D/k-dimensional subspace. We now perform a dot product attention on each of these subspaces.\n\nEach of these dot product attention is operating on a difference subspace. It sees different subsets of the data and therefore specializes. How do these heads specializes is anybody‚Äôs guess - unless we have a special embedding which has been processed using PCA or some other algorithm to ensure that each subspace corresponds to some interpretable subset of features.\n\n\n\n\nmuti-head attention scaled dot-product\n\nFor example if we used a 1024 dimension embedding which concatenates 4 representations.\n\n[0:256] is an embedding trained on a phonological task\n[256:512] is an embedding trained on a morphological task\n[513:768] is an embedding trained on a syntactical task\n[769:1024] is an embedding trained on a semantic task\n\nWe could devise a number of subspace sampling schemes to give the k different attention heads different areas of specializations.\n\nsample from a single sub-space\n4 heads sample from one subspace and 4 heads sample from 3 different sub-spaces\n5 heads sampling from 2 subspaces different sub-spaces and 3 from 1\n5 heads sampling from 2 subspaces different sub-spaces and 3 from three\n\nEach would specialize on a domain or on a interface between two domain or on all data but one domain. Language is rather redundant so they may be able to reconstruct most of the missing data - but at least they would specialize in a linguistically meaningful way.\nGiven a word, we take its embedding then we multiply it by the Q, K, V matrix to get the corresponding queries, keys and values. When we use multi-head attention, a head can learn different relationships between words from another head.\nHere‚Äôs one way to look at it:\n\nFirst, imagine that we have an embedding for a word. We multiply that embedding with Q to get q_1, K to get k_1, and V to get v_1\n\n\n\n\n\nmuti-head-attention-concatenation\n\n\n\n\nmuti-head-attention-math\n\n\n\n\nmuti-head-attention-fotmula\n\n\n\n\nmuti-head-attention-quiz\n\n\n\n\nmuti-head-attention-math\n\n\n\n\nmuti-head-attention-math\n\n\n\n\nmuti-head-attention-math\n\n\n\n\nmuti-head-attention-math\n\n\n\n\n\n\n\n\n\nNext, we feed it to the linear layer, once we go through the linear layer for each word, we need to calculate a score. After that, we end up having an embedding for each word. But we still need to get the score for how much of each word we are going to use. For example, this will tell we how similar two words are q_1 and k_1or even q_1 and k_2 by doing a simple q_1 \\dot k_1. We can take the softmax of those scores (the paper mentions that we have to divide by \\sqrt(d) to get a probability and then we multiply that by the value. That gives we the new representation of the word.) If we have many heads, we can concatenate them and then multiply again by a matrix that is of dimension (dim of each head by num heads - dim of each head) to get one final vector corresponding to each word.\n\nHere is step by step guide, first we get the Q, K, V matrices: Note that the computation above was done for one head. If we have several heads, concretely nn, then we will have Z_1, Z_2, \\ldots, Z_n. In which case, we can just concatenate them and multiply by a W_O matrix as follows:\nHence, the more heads we have, the more Zs we will end up concatenating and as a result, that will change the inner dimension of W_O, which will then project the combined embeddings into one final embedding.\n\n\n\n\nsummary-muti-head-attention",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c4w2/index.html#sec-transformer-decoder",
    "href": "posts/c4w2/index.html#sec-transformer-decoder",
    "title": "Week 2 - Text Summarization",
    "section": "V6: Transformer Decoder",
    "text": "V6: Transformer Decoder\n\n\n\n\noutline\n\nThere is a learning objective here!\nthe transformer decoder has two parts\n\na decoder block (with multi-head attention) - think feature acquisition.\na feed forward block - think non-parametric regression on the features.\n\n\n\n\n\ntransformer-decoder-overview\n\n\n\n\ntransformer-decoder-explaination\n\n\n\n\ntransformer-decoder-ff\n\n\n\n\ntransformer-decoder-explaination\n\n\n\n\ntransformer-decoder-summary",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c4w2/index.html#sec-cross-entropy-loss",
    "href": "posts/c4w2/index.html#sec-cross-entropy-loss",
    "title": "Week 2 - Text Summarization",
    "section": "Cross entropy loss",
    "text": "Cross entropy loss\nCross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label.\n\n\n\n\ninference\n\nAfter training GPT2 on summarization data we just treat it like a word model and mine it for summaries. We do this by supply it with an input and predicting the output token by token. A more sophisticated method might be to use a beam search.\nAn even more sophisticated method might be to use an information metric to reject sentences and back track or better yet to do negative sampling from the prediction distribution (i.e.¬†erase some prediction‚Äôs probabilities and renormalize)\nOne could do even better by providing hints, especially if we also head some kind of extractive model with a high-level of certainty about the most important sentence and their most significant concepts.\n\n\n\n\nquiz\n\nWe want the model to be penalized if it makes the wrong prediction. In this case it it does not predict the next word in the summary.\nThis may not be ideal for a number of reasons:\n\nthe Big world view ‚Äúwe are interested in a summary not the next word‚Äù what if the model is generating a semantically equivalent summary, in such a case it should not be penalized at all.\n\nIn a previous assignment we used a siamese network to check if two queries were equivalent. I think that allowing the network would be beneficial. (A loss that examines a generated sequence and compares it to the output.) But I don‚Äôt really know how to back-propagate the outcome for all the words. Well not exactly\nAs we are using teacher forcing we can take a position that we ignore all the mistakes the model made and give it a good output sequence and ask it for the next word. This then allows us to back prop the last word‚Äôs loss all by itself.\nIf we do this for each word in the output in sequence we should be able to reuse most of the calculations.\nThere are cases we have multiple summaries:\n\nFor a wikipedia article we often have all version from inception to the current day. This can provide multiple summaries and text along with an a golden version (the current summary). Oh and we may have a better summary in other languages but that is a different story.\nFor IMDB movie plots we often have a long synopsis and multiple shorter summaries. Also we may also have the book or screen play.\n\nI mention these two cases since Curriculum Learning may be able to assist us in training\n\n\n\n\nsummary\n\nI think these is much missing from this lesson about summerization. However there are a number of good source in papers as well as some lectures on YouTube.\nI have quickly summarized one and should link to it from here once it is published.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c4w2/index.html#sec-lab1-attention",
    "href": "posts/c4w2/index.html#sec-lab1-attention",
    "title": "Week 2 - Text Summarization",
    "section": "Lab1 : Attention",
    "text": "Lab1 : Attention\nThis was a numpy based realization of dot product and multi-head attention. Some of the main assignment required porting this to Jax.\nAttention lab",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c4w2/index.html#lab2-the-transformer-decoder",
    "href": "posts/c4w2/index.html#lab2-the-transformer-decoder",
    "title": "Week 2 - Text Summarization",
    "section": "Lab2 : The Transformer Decoder",
    "text": "Lab2 : The Transformer Decoder\nthis covered the transformer block\nTransformer block lab",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c4w2/index.html#assignment-transformer-summarizer",
    "href": "posts/c4w2/index.html#assignment-transformer-summarizer",
    "title": "Week 2 - Text Summarization",
    "section": "Assignment: Transformer Summarizer",
    "text": "Assignment: Transformer Summarizer\nAssignment\nThis long assignment primarily focused on dot product attention, multi-head attention and on building the transformer blocks. These were manageable as their theory had been explained in the lectures and their code had already been covered in the labs. It glosses over the parts involving data processing, training, evaluation and the actual summarization task. The summarization is accomplished using maximum likelihood estimate. A beam search might have yielded better results.\nThe date as described by:\n\nWe use the CNN/Daily Mail dataset (Hermann et al., 2015; Nallapati et al., 2016), which contains online news articles (781 tokens on average) paired with multi-sentence summaries (3.75 sentences or 56 tokens on average).\n\n\nGet To The Point ¬ß4 (Abigail et all 2017) We used the non anatomized version. However the earlier paper used a preprocessed version which replaced the named entities with token like $entity5. This is probably ideal in other situations like event processing where each event looks quite different unless one anatomizes them rendering them much more similar and hopefully helping the model generalize better by learning from the partitions induced by the equivalency relation.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c4w2/index.html#sec-expanding-the-lab-to-a-project",
    "href": "posts/c4w2/index.html#sec-expanding-the-lab-to-a-project",
    "title": "Week 2 - Text Summarization",
    "section": "Expanding the lab to a project:",
    "text": "Expanding the lab to a project:\nThis is one of the main areas I‚Äôd like to focus on for a project. I have in mind a tool for improving wikipedia article leads. Here is how I‚Äôd like to take this project to the next level:\n\nmore data\ntrain it on additional material:\n\npapers and abstracts.\nwikipedia articles (with good first paragraphs. )\nbooks and book summaries (could be problematic due to the book‚Äôs length)\nmovie scripts and outlines from IMDB\n\na Storyline\nsummary (paragraph)\na synopsis (longer)\n\n\n\n\nMore algorithms\n\nUsing a reformer to handle longer texts like books.\nBetter summarization using:\n\na beam search to build better summaries.\na bayesian search to avoid repetitions.\n\nuse curriculum learning to speed up training with\n\neasier examples first.\nmultiple summaries per text.\nlearning on anonymized NE before graduating to non-anonymized texts\n\nuse better method for evaluation of summary.\n\nPerhaps an f-score combining precision or recall on\nAttention Activation summed as a Coverage score for each token.\n\nuse of non zero loss-weights layer\n\ndrop to zero as training progresses.\ndepend on the actual length of source and output.\nuse tf-idf to make holes in the mask surrounding essential concepts.\n\n\n\n\nEvaluation\nuse sammy lib with\n\nrouge-n metric\nthe pyramid metric\n\n\n\nExtra features\n\npages for paper reviews\npages for research questions\npages to implement exra code/ experiments.\nvisualize the concepts/sentences/paragraphs/sections covered in the summary.\nestablish a hierarchy of what would go into a longer outline.\ndevelop a f-score metric combining precision and recall for the summary of the abstract.\nin academic writing one sentence per paragraphs should capture the main concept and it is generally the first second or the last. Is such a sentence is available identify it. This would be done by comparing each sentence with the rest of the paragraph.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c4w2/index.html#sec-open-question",
    "href": "posts/c4w2/index.html#sec-open-question",
    "title": "Week 2 - Text Summarization",
    "section": "Open question",
    "text": "Open question\nFor me the assignment raised a number of questions about what really going on here during training.\nI‚Äôll probably do this assignment again and look for some answers to my many questions. Once I have these I‚Äôll add them in the body of these notes.\n\nQuestions\n\nLoading and prepocessing the data:\n\nWhat is going on after we load the dataset - is there data augmentation?\nWhat this sub word vocab?\nHow to make my own sub word vocab?\nHow are out of vocab words being handled?\nCan we query the model about these beside running decode ?\nHow are these created - I saw several sizes of vocab.\n\nTraining\n\nTraining data seems to be a little mangled - there seems to be missing white space after the first token of the summaries, is there some way to fix this?\nIn not sure but why do we use teacher forcing during training?\n\n\nIt should speed training up, but the setup is unclear.\n\nEvaluation\n\nWhy are we not looking at a summarization metic like pyramid, rouge5 or good old precision and recall.\n\nInference\n\nHow can we tell the model thinks its done?\n\n\nwhen it output and  token\n\n\nHow to generate one sentence for each paragraph/section\n\n\nChop up the input and summarise each section.\nCreate an new dataset that bases it summaries on the last and first sentences of each paragraph. If that‚Äôs too long summarize again for each section.\nIntroduce a timed mask that hides [0:t*len/T] where T is total number of tokens being generated.\nmake the mask a Bayesian search mechanism that hides concepts in the output.\n\n\nHow to use multiple summaries like in IMDB?\n\n\nscore using the pyramid scheme or rogue.\n\n\nHow to make the model avoid repeating /rephrasing themselves?\n\n\nuse a metric on new information. for example Maximal marginal relevance. MMR = \\argmax [\\lambda Sim_1(s_i,Q)- (1 - \\lambda) \\max Sim_2(s_i,s_j)] where Q is the query and s are output sentences and try to bake this into the regularization.\na coverage vector seems to be a recommend method.\n\nVisualization\n\n\nIs there a easy way to see the activation for each word in the output?\nIs there a easy way to see which concepts are significant (not too common and not too rare)\nIs there a easy way to see which concepts are salient - aligned to near by concepts.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c4w2/index.html#references",
    "href": "posts/c4w2/index.html#references",
    "title": "Week 2 - Text Summarization",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c4w2/index.html#papers",
    "href": "posts/c4w2/index.html#papers",
    "title": "Week 2 - Text Summarization",
    "section": "Papers",
    "text": "Papers\n\nTransformers\n\nExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Raffel et al, 2019)\n\nReformer: The Efficient Transformer (Kitaev et al, 2020)\nAttention Is All We Need (Vaswani et al, 2017)\nDeep contextualized word representations (Peters et al, 2018)\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al, 2018)\nFinetuning Pretrained Transformers into RNNs (Kasai et all 2021)\n\n\n\nSummarization\n\nA trainable document summarizer. (Kupiec et al., 1995) extractive\nConstructing literature abstracts by computer: techniques and prospects. (Paice, 1990) extractive\nAutomatic text summarization: Past, present and future (Saggion and Poibeau, 2013) extractive\nAbstractive sentence summarization with attentive recurrent neural networks. (Chopra et al., 2016) abstractive summarization\nPointing the unknown words. (Nallapati et al., 2016) abstractive summarization\nA neural attention model for abstractive sentence summarization. (Rush et al.,2015;) abstractive summarization\nEfficient summarization with read-again and copy mechanism(Zeng et al., 2016) abstractive summarization\nGet To The Point: Summarization with Pointer-Generator Networks (Abigail et all 2017) Hybrid summarization. Note: Christopher D. Manning\nExtractive Summarization as Text Matching (Zhong et all 2020)",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c4w2/index.html#articles",
    "href": "posts/c4w2/index.html#articles",
    "title": "Week 2 - Text Summarization",
    "section": "Articles",
    "text": "Articles\n\nThe Illustrated Transformer (Alammar, 2018)\nThe Illustrated GPT-2 (Alammar, 2019)\nHow GPT3 Works - Visualizations and Animations (Alammar, 2020)\nAttention? Attention! (Lilian Weng, 2018)\nThe Transformer Family (Lilian Weng, 2020)\nTeacher forcing for RNNs",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c4w2/index.html#links",
    "href": "posts/c4w2/index.html#links",
    "title": "Week 2 - Text Summarization",
    "section": "Links",
    "text": "Links\n\nJax\nTrax\nTrax community on Gitter\nCNN daily mail dataset",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization"
    ]
  },
  {
    "objectID": "posts/c2w3/lab02.html",
    "href": "posts/c2w3/lab02.html",
    "title": "Building the language model",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure¬†1",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "L2 - Building the language model"
    ]
  },
  {
    "objectID": "posts/c2w3/lab03.html",
    "href": "posts/c2w3/lab03.html",
    "title": "Out of vocabulary words (OOV)",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure¬†1\n\n\n\nVocabulary\nIn the video about the out of vocabulary words, we saw that the first step in dealing with the unknown words is to decide which words belong to the vocabulary.\nIn the code assignment, we will try the method based on minimum frequency - all words appearing in the training set with frequency &gt;= minimum frequency are added to the vocabulary.\nHere is a code for the other method, where the target size of the vocabulary is known in advance and the vocabulary is filled with words based on their frequency in the training set.\n\n# build the vocabulary from M most frequent words\n# use Counter object from the collections library to find M most common words\nfrom collections import Counter\n\n# the target size of the vocabulary\nM = 3\n\n# pre-calculated word counts\n# Counter could be used to build this dictionary from the source corpus\nword_counts = {'happy': 5, 'because': 3, 'i': 2, 'am': 2, 'learning': 3, '.': 1}\n\nvocabulary = Counter(word_counts).most_common(M)\n\n# remove the frequencies and leave just the words\nvocabulary = [w[0] for w in vocabulary]\n\nprint(f\"the new vocabulary containing {M} most frequent words: {vocabulary}\\n\") \n\nthe new vocabulary containing 3 most frequent words: ['happy', 'because', 'learning']\n\n\n\nNow that the vocabulary is ready, we can use it to replace the OOV words with &lt;UNK&gt; as we saw in the lecture.\n\n# test if words in the input sentences are in the vocabulary, if OOV, print &lt;UNK&gt;\nsentence = ['am', 'i', 'learning']\noutput_sentence = []\nprint(f\"input sentence: {sentence}\")\n\nfor w in sentence:\n    # test if word w is in vocabulary\n    if w in vocabulary:\n        output_sentence.append(w)\n    else:\n        output_sentence.append('&lt;UNK&gt;')\n        \nprint(f\"output sentence: {output_sentence}\")\n\ninput sentence: ['am', 'i', 'learning']\noutput sentence: ['&lt;UNK&gt;', '&lt;UNK&gt;', 'learning']\n\n\nWhen building the vocabulary in the code assignment, we will need to know how to iterate through the word counts dictionary.\nHere is an example of a similar task showing how to go through all the word counts and print out only the words with the frequency equal to f.¬†\n\n# iterate through all word counts and print words with given frequency f\nf = 3\n\nword_counts = {'happy': 5, 'because': 3, 'i': 2, 'am': 2, 'learning':3, '.': 1}\n\nfor word, freq in word_counts.items():\n    if freq == f:\n        print(word)\n\nbecause\nlearning\n\n\nAs mentioned in the videos, if there are many &lt;UNK&gt; replacements in your train and test set, we may get a very low perplexity even though the model itself wouldn‚Äôt be very helpful.\nHere is a sample code showing this unwanted effect.\n\n# many &lt;unk&gt; low perplexity \ntraining_set = ['i', 'am', 'happy', 'because','i', 'am', 'learning', '.']\ntraining_set_unk = ['i', 'am', '&lt;UNK&gt;', '&lt;UNK&gt;','i', 'am', '&lt;UNK&gt;', '&lt;UNK&gt;']\n\ntest_set = ['i', 'am', 'learning']\ntest_set_unk = ['i', 'am', '&lt;UNK&gt;']\n\nM = len(test_set)\nprobability = 1\nprobability_unk = 1\n\n# pre-calculated probabilities\nbigram_probabilities = {('i', 'am'): 1.0, ('am', 'happy'): 0.5, ('happy', 'because'): 1.0, ('because', 'i'): 1.0, ('am', 'learning'): 0.5, ('learning', '.'): 1.0}\nbigram_probabilities_unk = {('i', 'am'): 1.0, ('am', '&lt;UNK&gt;'): 1.0, ('&lt;UNK&gt;', '&lt;UNK&gt;'): 0.5, ('&lt;UNK&gt;', 'i'): 0.25}\n\n# got through the test set and calculate its bigram probability\nfor i in range(len(test_set) - 2 + 1):\n    bigram = tuple(test_set[i: i + 2])\n    probability = probability * bigram_probabilities[bigram]\n        \n    bigram_unk = tuple(test_set_unk[i: i + 2])\n    probability_unk = probability_unk * bigram_probabilities_unk[bigram_unk]\n\n# calculate perplexity for both original test set and test set with &lt;UNK&gt;\nperplexity = probability ** (-1 / M)\nperplexity_unk = probability_unk ** (-1 / M)\n\nprint(f\"perplexity for the training set: {perplexity}\")\nprint(f\"perplexity for the training set with &lt;UNK&gt;: {perplexity_unk}\")\n\nperplexity for the training set: 1.2599210498948732\nperplexity for the training set with &lt;UNK&gt;: 1.0\n\n\n\n\nSmoothing\nAdd-k smoothing was described as a method for smoothing of the probabilities for previously unseen n-grams.\nHere is an example code that shows how to implement add-k smoothing but also highlights a disadvantage of this method. The downside is that n-grams not previously seen in the training dataset get too high probability.\nIn the code output bellow you‚Äôll see that a phrase that is in the training set gets the same probability as an unknown phrase.\n\ndef add_k_smooting_probability(k, vocabulary_size, n_gram_count, n_gram_prefix_count):\n    numerator = n_gram_count + k\n    denominator = n_gram_prefix_count + k * vocabulary_size\n    return numerator / denominator\n\ntrigram_probabilities = {('i', 'am', 'happy') : 2}\nbigram_probabilities = {( 'am', 'happy') : 10}\nvocabulary_size = 5\nk = 1\n\nprobability_known_trigram = add_k_smooting_probability(k, vocabulary_size, trigram_probabilities[('i', 'am', 'happy')], \n                           bigram_probabilities[( 'am', 'happy')])\n\nprobability_unknown_trigram = add_k_smooting_probability(k, vocabulary_size, 0, 0)\n\nprint(f\"probability_known_trigram: {probability_known_trigram}\")\nprint(f\"probability_unknown_trigram: {probability_unknown_trigram}\")\n\nprobability_known_trigram: 0.2\nprobability_unknown_trigram: 0.2\n\n\n\n\nBack-off\nBack-off is a model generalization method that leverages information from lower order n-grams in case information about the high order n-grams is missing. For example, if the probability of an trigram is missing, use bigram information and so on.\nHere we can see an example of a simple back-off technique.\n\n# pre-calculated probabilities of all types of n-grams\ntrigram_probabilities = {('i', 'am', 'happy'): 0}\nbigram_probabilities = {( 'am', 'happy'): 0.3}\nunigram_probabilities = {'happy': 0.4}\n\n# this is the input trigram we need to estimate\ntrigram = ('are', 'you', 'happy')\n\n# find the last bigram and unigram of the input\nbigram = trigram[1: 3]\nunigram = trigram[2]\nprint(f\"besides the trigram {trigram} we also use bigram {bigram} and unigram ({unigram})\\n\")\n\n# 0.4 is used as an example, experimentally found for web-scale corpuses when using the \"stupid\" back-off\nlambda_factor = 0.4\nprobability_hat_trigram = 0\n\n# search for first non-zero probability starting with trigram\n# to generalize this for any order of n-gram hierarchy, \n# we could loop through the probability dictionaries instead of if/else cascade\nif trigram not in trigram_probabilities or trigram_probabilities[trigram] == 0:\n    print(f\"probability for trigram {trigram} not found\")\n    \n    if bigram not in bigram_probabilities or bigram_probabilities[bigram] == 0:\n        print(f\"probability for bigram {bigram} not found\")\n        \n        if unigram in unigram_probabilities:\n            print(f\"probability for unigram {unigram} found\\n\")\n            probability_hat_trigram = lambda_factor * lambda_factor * unigram_probabilities[unigram]\n        else:\n            probability_hat_trigram = 0\n    else:\n        probability_hat_trigram = lambda_factor * bigram_probabilities[bigram]\nelse:\n    probability_hat_trigram = trigram_probabilities[trigram]\n\nprint(f\"probability for trigram {trigram} estimated as {probability_hat_trigram}\")\n\nbesides the trigram ('are', 'you', 'happy') we also use bigram ('you', 'happy') and unigram (happy)\n\nprobability for trigram ('are', 'you', 'happy') not found\nprobability for bigram ('you', 'happy') not found\nprobability for unigram happy found\n\nprobability for trigram ('are', 'you', 'happy') estimated as 0.06400000000000002\n\n\n\n\nInterpolation\nThe other method for using probabilities of lower order n-grams is the interpolation. In this case, we use weighted probabilities of n-grams of all orders every time, not just when high order information is missing.\nFor example, we always combine trigram, bigram and unigram probability. We can see how this in the following code snippet.\n\n# pre-calculated probabilities of all types of n-grams\ntrigram_probabilities = {('i', 'am', 'happy'): 0.15}\nbigram_probabilities = {( 'am', 'happy'): 0.3}\nunigram_probabilities = {'happy': 0.4}\n\n# the weights come from optimization on a validation set\nlambda_1 = 0.8\nlambda_2 = 0.15\nlambda_3 = 0.05\n\n# this is the input trigram we need to estimate\ntrigram = ('i', 'am', 'happy')\n\n# find the last bigram and unigram of the input\nbigram = trigram[1: 3]\nunigram = trigram[2]\nprint(f\"besides the trigram {trigram} we also use bigram {bigram} and unigram ({unigram})\\n\")\n\n# in the production code, we would need to check if the probability n-gram dictionary contains the n-gram\nprobability_hat_trigram = lambda_1 * trigram_probabilities[trigram] \n+ lambda_2 * bigram_probabilities[bigram]\n+ lambda_3 * unigram_probabilities[unigram]\n\nprint(f\"estimated probability of the input trigram {trigram} is {probability_hat_trigram}\")\n\nbesides the trigram ('i', 'am', 'happy') we also use bigram ('am', 'happy') and unigram (happy)\n\n\n\n0.045\n\n\n0.020000000000000004\n\n\nestimated probability of the input trigram ('i', 'am', 'happy') is 0.12\n\n\nThat‚Äôs it for week 3, we should be ready now for the code assignment.\n\n\n\n\nCitationBibTeX citation:@online{bochman2020,\n  author = {Bochman, Oren},\n  title = {Out of Vocabulary Words {(OOV)}},\n  date = {2020-10-25},\n  url = {https://orenbochman.github.io/notes-nlp/posts/c2w3/lab03.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2020. ‚ÄúOut of Vocabulary Words (OOV).‚Äù\nOctober 25, 2020. https://orenbochman.github.io/notes-nlp/posts/c2w3/lab03.html.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "L3 - Out of vocabulary words"
    ]
  },
  {
    "objectID": "posts/c4w4/index.html",
    "href": "posts/c4w4/index.html",
    "title": "Chat Bots",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nThis week‚Äôs slides\n\n\n\n\nSupplementary Figure¬†1\nMy notes for Week 4 of the Natural Language Processing with Attention Labels Course in the Natural Language Processing Specialization Offered by DeepLearning.AI on Coursera\nDeep learning and A.I. researchers push the field forward by looking for new techniques as well as refinements of old ideas to get better performance on tasks. In this lesson we cover reversible layers which allow us to leverage a time memory tradeoff to process book length sequences and handle contexts over a conversation.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots"
    ]
  },
  {
    "objectID": "posts/c4w4/index.html#sec-task-long",
    "href": "posts/c4w4/index.html#sec-task-long",
    "title": "Chat Bots",
    "section": "Tasks with Long Sequences",
    "text": "Tasks with Long Sequences\n\n\n\n\n\n\n\nFigure¬†1: Context Window\n\n\nThis week we are going to learn about tasks that require processing longer sequences:\n\nWriting books\nStorytelling and understanding\nBuilding intelligent agents for conversations like chat-bots.\n\nMore specifically we will understand how re-former model (AKA the reversible transformer) and reversible layers work.\nThis week we will learn about the bottlenecks in these larger transformer models, and solutions we can use to make them trainable for you. We will also learn about the. Here is what we will be building for your programming assignment: A chatbot!\nIn many ways a Chat bot is very similar to a Q&A system which we built last week and that is also similar to query based summarization another task we covered a week before that. The new challenge is to manage what parts of the new and old context we keep around as the dialogue progresses. Chatbot are smart A.I. agents and much of the techniques developed under the umbrella of knowledge-based AI is also relevant in developing these. For instance carrying out actions on behalf of the user.\nChatbots can also get a very simple ui via the web or as an mobile app, which is another area I have some experience. However an even more powerful paradigm here is the ability to interact using voice which has many additional benefit for example supporting people with disabilities and operating in hands-free mode.\nHere is a link to an AI Storytelling system.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots"
    ]
  },
  {
    "objectID": "posts/c4w4/index.html#sec-transformer-complexity",
    "href": "posts/c4w4/index.html#sec-transformer-complexity",
    "title": "Chat Bots",
    "section": "Video 2: Transformer Complexity",
    "text": "Video 2: Transformer Complexity\n\n\n\n\n\n\n\nFigure¬†2: week-4\n\n\nOne of the biggest issues with the transformers is that it takes time and a lot of memory when training. Concretely here are the numbers. If we have a sequence of length L , then we need L^2*N memory to handle the sequence. So if we have N layers, that means your model will take N times more time to complete. As L gets larger, the memory and the time quickly increases.\nPerhaps this is the reason people are looking into converting transformers into RNN after training.\n\n\n\n\n\n\n\nFigure¬†3: week-4\n\n\nWhen we are handling long sequences, we frequently don‚Äôt need to consider all L positions. We can just focus on an area of interest instead. For example, when translating a long text from one language to another, we don‚Äôt need to consider every word at once. We can instead focus on a single word being translated, and those immediately around it, by using attention.\nTo overcome the memory requirements we can recompute the activations. As long as we do it efficiently, we will be able to save a good amount of time and memory. We will learn this week how to do it. Instead of storing N layers, we will be able to recompute them when doing the back-propagation. That combined with local attention, will give we a much faster model that works at the same level as the transformer we learned about last week.\n\none area where we can make headway is working with a subsequence of interest.\nduring training we need to keep the activations in memory for the back propagation task. Clearly for inference we may be able to save on memory.\nthe alternative is to discard the activations as we go along and recalculate later. This can allows trading memory for compute time. However with larger models compute time is also a bottleneck.\n\n\n\n\n\n\n\n\nFigure¬†4: Approximate Nearest Neighbours",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots"
    ]
  },
  {
    "objectID": "posts/c4w4/index.html#sec-lsh-attention",
    "href": "posts/c4w4/index.html#sec-lsh-attention",
    "title": "Chat Bots",
    "section": "LSH Attention",
    "text": "LSH Attention\nIn Course 1, we covered how locality sensitive hashing (LSH) works. We learned about:\n\nKNN\nHash Tables and Hash Functions\nLocality Sensitive Hashing\nMultiple Planes\n\nHere are the steps to follow to compute LSH given some vectors, where the vectors may correspond to the transformed word embedding that your transformer outputs.\nAttention is used to try which query (q) and key (k) are the most similar. To do so, we hash q and the keys. This will put similar vectors in the same bucket that we can use. The drawing above shows the lines that separate the buckets. Those could be seen as the planes.\nFirst let‚Äôs recall how the standard attention mechanism is defined as follows:\n\nA(Q,K,V) = softmax(QK^T)V\n\\tag{1}\nOnce we hash Q and K we will then compute standard attention on the bins that we have created. We will repeat the same process several times to increase the probability of having the same key in the same bin as the query.\n\n\n\n\n\n\n\nFigure¬†5: week-4\n\n\n\nGiven the sequence of queries and keys, we hash them into buckets. Check out Course 1 Week 4 for a review of the hashing.\nWe will then sort them by bucket.\nWe split the buckets into chunks (this is a technical detail for parallel computing purposes).\nWe then compute the attention within the same bucket of the chunk we are looking at and the previous chunk. &gt; Q. Why do we need to look at the previous chunk?\nWe can see in the figure some buckets (both blue and yellow) have been split across two chunks. Looking at the previous chunk will let we attend to the full bucket.\n\nIn Winograd schemas the resolution of the ambiguous pronoun switches between the two variants of the sentence.\n\nthe animal didn‚Äôt cross the street because it was too tired / the animal didn‚Äôt cross the street because it was too wide / The city councilmen refused the demonstrators a permit because they feared violence. / The city councilmen refused the demonstrators a permit because they advocated violence. /",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots"
    ]
  },
  {
    "objectID": "posts/c4w4/index.html#sec-reversible",
    "href": "posts/c4w4/index.html#sec-reversible",
    "title": "Chat Bots",
    "section": "Motivation for Reversible Layers: Memory!",
    "text": "Motivation for Reversible Layers: Memory!\n\n\n\n\n\n\n\nFigure¬†6: Memory efficency\n\n\nFor example in this model:\n\n2 GB for the input\n2 GB are required to compute the Attention\n2 GB for the feed forward. There are 12 attention layers 12 feed forward layers. That is equal to 12 * 2 + 12*2 + 2 (for the input) = 50 GB. That is a lot of memory.\n\nIf N is the sequence length:\n\nTransformers need O(N^2) memory.\n\nEach layer of a transformers has an Attention block and feed-forward block. If we want to process, for example to train a document of length 1 million token with 12 layers we will need 50 GB of ram. As we use residual architecture during prediction we only need the current layers input and the output for the next layer. But during training we need to keep all the copies so we can back-propagate the errors.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots"
    ]
  },
  {
    "objectID": "posts/c4w4/index.html#sec-reversible-residual",
    "href": "posts/c4w4/index.html#sec-reversible-residual",
    "title": "Chat Bots",
    "section": "Reversible Residual Layers",
    "text": "Reversible Residual Layers",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots"
    ]
  },
  {
    "objectID": "posts/c4w4/index.html#sec-reformer",
    "href": "posts/c4w4/index.html#sec-reformer",
    "title": "Chat Bots",
    "section": "Reformer",
    "text": "Reformer\ncan run 1 million token in 16 gb",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots"
    ]
  },
  {
    "objectID": "posts/c4w4/index.html#sec-lab-2",
    "href": "posts/c4w4/index.html#sec-lab-2",
    "title": "Chat Bots",
    "section": "Lab 2: Reversible layers",
    "text": "Lab 2: Reversible layers\nFrom the trax documents a Residual, involves first a split and then a merge:\nreturn Serial(\n    Branch(shortcut, layer), # split \n    Add(),                   # merge\n)\nwhere:\n\nBranch(shortcut, layers): makes two copies of the single incoming data stream, passes one copy via the shortcut (typically a no-op), and processes the other copy via the given layers (applied in series). [ùëõ_{ùëñùëõ}=1, ùëõ_{ùëúùë¢ùë°}=2]\nAdd(): combines the two streams back into one by adding two tensors element-wise. [ùëõ_{ùëñùëõ}=2, ùëõ_{ùëúùë¢ùë°}=1]\n\nIn the Branch operation each layer in the input list copies as many inputs from the stack as it needs, and their outputs are successively combined on stack. Put another way, each element of the branch can have differing numbers of inputs and outputs. Let‚Äôs try a more complex example. To work these operations modify the stack by replicating the input needed as well as pushing the outputs (as specified using th out parameters).",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots"
    ]
  },
  {
    "objectID": "posts/c4w4/index.html#sec-references",
    "href": "posts/c4w4/index.html#sec-references",
    "title": "Chat Bots",
    "section": "References",
    "text": "References\n\nPractical and Optimal LSH for Angular Distance\n\n\nTokenization\n\nSentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (Kudo & Richardson 2018) sub-word tokenization\nSubword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates (Kudo 2018) sub-word tokenization\nNeural Machine Translation of Rare Words with Subword Units (Sennrich et all 2016) sub-word tokenization\nSubword tokenizers TF tutorial sub-word tokenization\n[https://blog.floydhub.com/tokenization-nlp/]\nSwivel: Improving Embeddings by Noticing What‚Äôs Missing (Shazeer, 2016)\n\n\n\nTransformers\n\n[Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer] (Raffel et al, 2019)\n\n[Reformer: The Efficient Transformer] (Kitaev et al, 2020)\nAttention Is All We Need (Vaswani et al, 2017) Vaswani et al. (2023)\n[Deep contextualized word representations] (Peters et al, 2018)\n[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding] (Devlin et al, 2018)\n[Finetuning Pretrained Transformers into RNNs] (Kasai et all 2021)\n[The Illustrated Transformer] (Alammar, 2018)\n[The Illustrated GPT-2] (Alammar, 2019)\n[How GPT3 Works - Visualizations and Animations] (Alammar, 2020)\nIn Weng (2018) the author covers many attention mechanism Attention? Attention!\n[The Transformer Family] (Lilian Weng, 2020)\nTeacher forcing for RNNs\n\n\n\nQuestion Answering Task:\n\nIn Rush (2015) , a paper titled A Neural Attention Model for Abstractive Sentence Summarization the authors discuss the summarization task.\n\nThe first two videos can be viewed on youtube.\n\n\n\n\n\n\n\nVideo¬†1: Christopher Manning in Stanford CS224N: NLP with Deep Learning | Winter 2019 | Lecture 10 On Question Answering.\n\n\n\n\n\n\n\n\nVideo¬†2: Christopher Manning and Danqi Chen in Stanford CS224N NLP with Deep Learning | Winter 2021 | Lecture 12 - Question Answering\n\n\n\n\n\n\n\n\nVideo¬†3",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots"
    ]
  },
  {
    "objectID": "posts/c4w4/index.html#links",
    "href": "posts/c4w4/index.html#links",
    "title": "Chat Bots",
    "section": "Links",
    "text": "Links\n\nJax\nTrax\nTrax community on Gitter\nCNN daily mail dataset\nLei Mao Machine Learning, Artificial Intelligence, Computer Science.\nByte Pair Encoding (Lei Mao 2021)\nExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\nReformer: The Efficient Transformer\nAttention Is All We Need\nDeep contextualized word representations\nThe Illustrated Transformer\nThe Illustrated GPT-2\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\nHow GPT3 Works - Visualizations and Animations\nAttention? Attention! (Lilian Weng, 2018)\nThe Transformer Family ‚Äú(Lilian Weng, 2020)‚Äù\nFinetuning Pretrained Transformers into RNNs ‚Äú(Kasai et all 2021)‚Äù",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots"
    ]
  },
  {
    "objectID": "posts/c3w1/lab02.html",
    "href": "posts/c3w1/lab02.html",
    "title": "Classes and subclasses",
    "section": "",
    "text": "In this notebook, I will show you the basics of classes and subclasses in Python. As you‚Äôve seen in the lectures from this week, Trax uses layer classes as building blocks for deep learning models, so it is important to understand how classes and subclasses behave in order to be able to build custom layers when needed.\nBy completing this notebook, you will:"
  },
  {
    "objectID": "posts/c3w1/lab02.html#the-__init__-method",
    "href": "posts/c3w1/lab02.html#the-__init__-method",
    "title": "Classes and subclasses",
    "section": "1.1 The __init__ method",
    "text": "1.1 The __init__ method\nWhen you want to assign values to the parameters of your class when an instance is created, it is necessary to define a special method: __init__. The __init__ method is called when you create an instance of a class. It can have multiple arguments to initialize the paramenters of your instance. In the next cell I will define My_Class with an __init__ method that takes the instance (self) and an argument y as inputs.\n\nclass My_Class: \n    def __init__(self, y): # The __init__ method takes as input the instance to be initialized and a variable y\n        self.x = y         # Sets parameter x to be equal to y\n\nIn this case, the parameter x of an instance from My_Class would take the value of an argument y. The argument self is used to pass information from the instance being created to the method __init__. In the next cell, create an instance instance_c, with x equal to 10.\n\n### START CODE HERE (1 line) ### \ninstance_c = My_Class(10)\n### END CODE HERE ###\nprint('Parameter x of instance_c: ' + str(instance_c.x))\n\nParameter x of instance_c: 10\n\n\nNote that in this case, you had to pass the argument y from the __init__ method to create an instance of My_Class."
  },
  {
    "objectID": "posts/c3w1/lab02.html#the-__call__-method",
    "href": "posts/c3w1/lab02.html#the-__call__-method",
    "title": "Classes and subclasses",
    "section": "1.2 The __call__ method",
    "text": "1.2 The __call__ method\nAnother important method is the __call__ method. It is performed whenever you call an initialized instance of a class. It can have multiple arguments and you can define it to do whatever you want like\n\nChange a parameter,\nPrint a message,\nCreate new variables, etc.\n\nIn the next cell, I‚Äôll define My_Class with the same __init__ method as before and with a __call__ method that adds z to parameter x and prints the result.\n\nclass My_Class: \n    def __init__(self, y): # The __init__ method takes as input the instance to be initialized and a variable y\n        self.x = y         # Sets parameter x to be equal to y\n    def __call__(self, z): # __call__ method with self and z as arguments\n        self.x += z        # Adds z to parameter x when called \n        print(self.x)\n\nLet‚Äôs create instance_d with x equal to 5.\n\ninstance_d = My_Class(5)\n\nAnd now, see what happens when instance_d is called with argument 10.\n\ninstance_d(10)\n\n15\n\n\nNow, you are ready to complete the following cell so any instance from My_Class:\n\nIs initialized taking two arguments y and z and assigns them to x_1 and x_2, respectively. And,\nWhen called, takes the values of the parameters x_1 and x_2, sums them, prints and returns the result.\n\n\nclass My_Class: \n    def __init__(self, y, z): #Initialization of x_1 and x_2 with arguments y and z\n        ### START CODE HERE (2 lines) ### \n        self.x_1 = y\n        self.x_2 = z\n        ### END CODE HERE ###\n    def __call__(self):       #When called, adds the values of parameters x_1 and x_2, prints and returns the result \n        ### START CODE HERE (1 line) ### \n        result = self.x_1 + self.x_2 \n        ### END CODE HERE ### \n        print(\"Addition of {} and {} is {}\".format(self.x_1,self.x_2,result))\n        return result\n\nRun the next cell to check your implementation. If everything is correct, you shouldn‚Äôt get any errors.\n\ninstance_e = My_Class(10,15)\ndef test_class_definition():\n    \n    assert instance_e.x_1 == 10, \"Check the value assigned to x_1\"\n    assert instance_e.x_2 == 15, \"Check the value assigned to x_2\"\n    assert instance_e() == 25, \"Check the __call__ method\"\n    \n    print(\"\\033[92mAll tests passed!\")\n    \ntest_class_definition()\n\nAddition of 10 and 15 is 25\nAll tests passed!"
  },
  {
    "objectID": "posts/c3w1/lab02.html#custom-methods",
    "href": "posts/c3w1/lab02.html#custom-methods",
    "title": "Classes and subclasses",
    "section": "1.3 Custom methods",
    "text": "1.3 Custom methods\nIn addition to the __init__ and __call__ methods, your classes can have custom-built methods to do whatever you want when called. To define a custom method, you have to indicate its input arguments, the instructions that you want it to perform and the values to return (if any). In the next cell, My_Class is defined with my_method that multiplies the values of x_1 and x_2, sums that product with an input w, and returns the result.\n\nclass My_Class: \n    def __init__(self, y, z): #Initialization of x_1 and x_2 with arguments y and z\n        self.x_1 = y\n        self.x_2 = z\n    def __call__(self):       #Performs an operation with x_1 and x_2, and returns the result\n        a = self.x_1 - 2*self.x_2 \n        return a\n    def my_method(self, w):   #Multiplies x_1 and x_2, adds argument w and returns the result\n        result = self.x_1*self.x_2 + w\n        return result\n\nCreate an instance instance_f of My_Class with any integer values that you want for x_1 and x_2. For that instance, see the result of calling My_method, with an argument w equal to 16.\n\n### START CODE HERE (1 line) ### \ninstance_f = My_Class(1,10)\n### END CODE HERE ### \nprint(\"Output of my_method:\",instance_f.my_method(16))\n\nOutput of my_method: 26\n\n\nAs you can corroborate in the previous cell, to call a custom method m, with arguments args, for an instance i you must write i.m(args). With that in mind, methods can call others within a class. In the following cell, try to define new_method which calls my_method with v as input argument. Try to do this on your own in the cell given below.\n\nclass My_Class: \n    def __init__(self, y, z):         #Initialization of x_1 and x_2 with arguments y and z\n        self.x_1 = None\n        self.x_2 = None\n    def __call__(self):               #Performs an operation with x_1 and x_2, and returns the result\n        a = None \n        return a\n    def my_method(self, w):           #Multiplies x_1 and x_2, adds argument w and returns the result\n        b = None\n        return b\n    def new_method(self, v):          #Calls My_method with argument v\n        ### START CODE HERE (1 line) ### \n        result = None\n        ### END CODE HERE ### \n        return result\n\nSPOILER ALERT Solution:\n\n# hidden-cell\nclass My_Class: \n    def __init__(self, y, z):      #Initialization of x_1 and x_2 with arguments y and z\n        self.x_1 = y\n        self.x_2 = z\n    def __call__(self):            #Performs an operation with x_1 and x_2, and returns the result\n        a = self.x_1 - 2*self.x_2 \n        return a\n    def my_method(self, w):        #Multiplies x_1 and x_2, adds argument w and returns the result\n        b = self.x_1*self.x_2 + w\n        return b\n    def new_method(self, v):       #Calls My_method with argument v\n        result = self.my_method(v)\n        return result\n\n\ninstance_g = My_Class(1,10)\nprint(\"Output of my_method:\",instance_g.my_method(16))\nprint(\"Output of new_method:\",instance_g.new_method(16))\n\nOutput of my_method: 26\nOutput of new_method: 26"
  },
  {
    "objectID": "posts/c3w1/lab02.html#inheritance",
    "href": "posts/c3w1/lab02.html#inheritance",
    "title": "Classes and subclasses",
    "section": "2.1 Inheritance",
    "text": "2.1 Inheritance\nWhen you define a subclass sub, every method and parameter is inherited from super class, including the __init__ and __call__ methods. This means that any instance from sub can use the methods defined in super. Run the following cell and see for yourself.\n\ninstance_sub_a = sub_c(1,10)\nprint('Parameter x_1 of instance_sub_a: ' + str(instance_sub_a.x_1))\nprint('Parameter x_2 of instance_sub_a: ' + str(instance_sub_a.x_2))\nprint(\"Output of my_method of instance_sub_a:\",instance_sub_a.my_method(16))\n\nParameter x_1 of instance_sub_a: 1\nParameter x_2 of instance_sub_a: 10\nOutput of my_method of instance_sub_a: 26\n\n\nAs you can see, sub_c does not have an initialization method __init__, it is inherited from My_class. However, you can overwrite any method you want by defining it again in the subclass. For instance, in the next cell define a class sub_c with a redefined my_Method that multiplies x_1 and x_2 but does not add any additional argument.\n\nclass sub_c(My_Class):           #Subclass sub_c from My_class\n    def my_method(self):         #Multiplies x_1 and x_2 and returns the result\n        ### START CODE HERE (1 line) ###\n        b = self.x_1*self.x_2 \n        ### END CODE HERE ###\n        return b\n\nTo check your implementation run the following cell.\n\ntest = sub_c(3,10)\nassert test.my_method() == 30, \"The method my_method should return the product between x_1 and x_2\"\n\nprint(\"Output of overridden my_method of test:\",test.my_method()) #notice we didn't pass any parameter to call my_method\n#print(\"Output of overridden my_method of test:\",test.my_method(16)) #try to see what happens if you call it with 1 argument\n\nOutput of overridden my_method of test: 30\n\n\nIn the next cell, two instances are created, one of My_Class and another one of sub_c. The instances are initialized with equal x_1 and x_2 parameters.\n\ny,z= 1,10\ninstance_sub_a = sub_c(y,z)\ninstance_a = My_Class(y,z)\nprint('My_method for an instance of sub_c returns: ' + str(instance_sub_a.my_method()))\nprint('My_method for an instance of My_Class returns: ' + str(instance_a.my_method(10)))\n\nMy_method for an instance of sub_c returns: 10\nMy_method for an instance of My_Class returns: 20\n\n\nAs you can see, even though sub_c is a subclass from My_Class and both instances are initialized with the same values, My_method returns different results for each instance because you overwrote My_method for sub_c.\nCongratulations! You just reviewed the basics behind classes and subclasses. Now you can define your own classes and subclasses, work with instances and overwrite inherited methods. The concepts within this notebook are more than enough to understand how layers in Trax work."
  },
  {
    "objectID": "posts/c3w1/lab03.html",
    "href": "posts/c3w1/lab03.html",
    "title": "Data generators",
    "section": "",
    "text": "In Python, a generator is a function that behaves like an iterator. It will return the next item. Here is a link to review python generators. In many AI applications, it is advantageous to have a data generator to handle loading and transforming data for different applications.\nYou will now implement a custom data generator, using a common pattern that you will use during all assignments of this course. In the following example, we use a set of samples a, to derive a new set of samples, with more elements than the original set.\nNote: Pay attention to the use of list lines_index and variable index to traverse the original list.\nimport random \nimport numpy as np\n\n# Example of traversing a list of indexes to create a circular list\na = [1, 2, 3, 4]\nb = [0] * 10\n\na_size = len(a)\nb_size = len(b)\nlines_index = [*range(a_size)] # is equivalent to [i for i in range(0,a_size)], the difference being the advantage of using * to pass values of range iterator to list directly\nindex = 0                      # similar to index in data_generator below\nfor i in range(b_size):        # `b` is longer than `a` forcing a wrap\n    # We wrap by resetting index to 0 so the sequences circle back at the end to point to the first index\n    if index &gt;= a_size:\n        index = 0\n    \n    b[i] = a[lines_index[index]]     #  `indexes_list[index]` point to a index of a. Store the result in b\n    index += 1\n    \nprint(b)\n\n[1, 2, 3, 4, 1, 2, 3, 4, 1, 2]"
  },
  {
    "objectID": "posts/c3w1/lab03.html#shuffling-the-data-order",
    "href": "posts/c3w1/lab03.html#shuffling-the-data-order",
    "title": "Data generators",
    "section": "Shuffling the data order",
    "text": "Shuffling the data order\nIn the next example, we will do the same as before, but shuffling the order of the elements in the output list. Note that here, our strategy of traversing using lines_index and index becomes very important, because we can simulate a shuffle in the input data, without doing that in reality.\n\n# Example of traversing a list of indexes to create a circular list\na = [1, 2, 3, 4]\nb = []\n\na_size = len(a)\nb_size = 10\nlines_index = [*range(a_size)]\nprint(\"Original order of index:\",lines_index)\n\n# if we shuffle the index_list we can change the order of our circular list\n# without modifying the order or our original data\nrandom.shuffle(lines_index) # Shuffle the order\nprint(\"Shuffled order of index:\",lines_index)\n\nprint(\"New value order for first batch:\",[a[index] for index in lines_index])\nbatch_counter = 1\nindex = 0                # similar to index in data_generator below\nfor i in range(b_size):  # `b` is longer than `a` forcing a wrap\n    # We wrap by resetting index to 0\n    if index &gt;= a_size:\n        index = 0\n        batch_counter += 1\n        random.shuffle(lines_index) # Re-shuffle the order\n        print(\"\\nShuffled Indexes for Batch No.{} :{}\".format(batch_counter,lines_index))\n        print(\"Values for Batch No.{} :{}\".format(batch_counter,[a[index] for index in lines_index]))\n    \n    b.append(a[lines_index[index]])     #  `indexes_list[index]` point to a index of a. Store the result in b\n    index += 1\nprint()    \nprint(\"Final value of b:\",b)\n\nOriginal order of index: [0, 1, 2, 3]\nShuffled order of index: [3, 2, 0, 1]\nNew value order for first batch: [4, 3, 1, 2]\n\nShuffled Indexes for Batch No.2 :[1, 2, 3, 0]\nValues for Batch No.2 :[2, 3, 4, 1]\n\nShuffled Indexes for Batch No.3 :[2, 0, 3, 1]\nValues for Batch No.3 :[3, 1, 4, 2]\n\nFinal value of b: [4, 3, 1, 2, 2, 3, 4, 1, 3, 1]\n\n\nNote: We call an epoch each time that an algorithm passes over all the training examples. Shuffling the examples for each epoch is known to reduce variance, making the models more general and overfit less.\n\nExercise\nInstructions: Implement a data generator function that takes in batch_size, x, y shuffle where x could be a large list of samples, and y is a list of the tags associated with those samples. Return a subset of those inputs in a tuple of two arrays (X,Y). Each is an array of dimension (batch_size). If shuffle=True, the data will be traversed in a random form.\nDetails:\nThis code as an outer loop\nwhile True:  \n...  \nyield((X,Y))  \nWhich runs continuously in the fashion of generators, pausing when yielding the next values. We will generate a batch_size output on each pass of this loop.\nIt has an inner loop that stores in temporal lists (X, Y) the data samples to be included in the next batch.\nThere are three slightly out of the ordinary features.\n\nThe first is the use of a list of a predefined size to store the data for each batch. Using a predefined size list reduces the computation time if the elements in the array are of a fixed size, like numbers. If the elements are of different sizes, it is better to use an empty array and append one element at a time during the loop.\nThe second is tracking the current location in the incoming lists of samples. Generators variables hold their values between invocations, so we create an index variable, initialize to zero, and increment by one for each sample included in a batch. However, we do not use the index to access the positions of the list of sentences directly. Instead, we use it to select one index from a list of indexes. In this way, we can change the order in which we traverse our original list, keeping untouched our original list.\nThe third also relates to wrapping. Because batch_size and the length of the input lists are not aligned, gathering a batch_size group of inputs may involve wrapping back to the beginning of the input loop. In our approach, it is just enough to reset the index to 0. We can re-shuffle the list of indexes to produce different batches each time.\n\n\ndef data_generator(batch_size, data_x, data_y, shuffle=True):\n    '''\n      Input: \n        batch_size - integer describing the batch size\n        data_x - list containing samples\n        data_y - list containing labels\n        shuffle - Shuffle the data order\n      Output:\n        a tuple containing 2 elements:\n        X - list of dim (batch_size) of samples\n        Y - list of dim (batch_size) of labels\n    '''\n    \n    data_lng = len(data_x) # len(data_x) must be equal to len(data_y)\n    index_list = [*range(data_lng)] # Create a list with the ordered indexes of sample data\n    \n    \n    # If shuffle is set to true, we traverse the list in a random way\n    if shuffle:\n        random.shuffle(index_list) # Inplace shuffle of the list\n    \n    index = 0 # Start with the first element\n    # START CODE HERE    \n    # Fill all the None values with code taking reference of what you learned so far\n    while True:\n        X = [0] * batch_size # We can create a list with batch_size elements. \n        Y = [0] * batch_size # We can create a list with batch_size elements. \n        \n        for i in range(batch_size):\n            \n            # Wrap the index each time that we reach the end of the list\n            if index &gt;= data_lng:\n                index = 0\n                # Shuffle the index_list if shuffle is true\n                if shuffle:\n                    rnd.shuffle(index_list) # re-shuffle the order\n            \n            X[i] = data_x[index_list[index]] # We set the corresponding element in x\n            Y[i] = data_y[index_list[index]] # We set the corresponding element in x\n    # END CODE HERE            \n            index += 1\n        \n        yield((X, Y))\n\nIf your function is correct, all the tests must pass.\n\ndef test_data_generator():\n    x = [1, 2, 3, 4]\n    y = [xi ** 2 for xi in x]\n    \n    generator = data_generator(3, x, y, shuffle=False)\n\n    assert np.allclose(next(generator), ([1, 2, 3], [1, 4, 9])),  \"First batch does not match\"\n    assert np.allclose(next(generator), ([4, 1, 2], [16, 1, 4])), \"Second batch does not match\"\n    assert np.allclose(next(generator), ([3, 4, 1], [9, 16, 1])), \"Third batch does not match\"\n    assert np.allclose(next(generator), ([2, 3, 4], [4, 9, 16])), \"Fourth batch does not match\"\n\n    print(\"\\033[92mAll tests passed!\")\n\ntest_data_generator()\n\nAll tests passed!\n\n\nIf you could not solve the exercise, just run the next code to see the answer.\n\nimport base64\n\nsolution = \"ZGVmIGRhdGFfZ2VuZXJhdG9yKGJhdGNoX3NpemUsIGRhdGFfeCwgZGF0YV95LCBzaHVmZmxlPVRydWUpOgoKICAgIGRhdGFfbG5nID0gbGVuKGRhdGFfeCkgIyBsZW4oZGF0YV94KSBtdXN0IGJlIGVxdWFsIHRvIGxlbihkYXRhX3kpCiAgICBpbmRleF9saXN0ID0gWypyYW5nZShkYXRhX2xuZyldICMgQ3JlYXRlIGEgbGlzdCB3aXRoIHRoZSBvcmRlcmVkIGluZGV4ZXMgb2Ygc2FtcGxlIGRhdGEKICAgIAogICAgIyBJZiBzaHVmZmxlIGlzIHNldCB0byB0cnVlLCB3ZSB0cmF2ZXJzZSB0aGUgbGlzdCBpbiBhIHJhbmRvbSB3YXkKICAgIGlmIHNodWZmbGU6CiAgICAgICAgcm5kLnNodWZmbGUoaW5kZXhfbGlzdCkgIyBJbnBsYWNlIHNodWZmbGUgb2YgdGhlIGxpc3QKICAgIAogICAgaW5kZXggPSAwICMgU3RhcnQgd2l0aCB0aGUgZmlyc3QgZWxlbWVudAogICAgd2hpbGUgVHJ1ZToKICAgICAgICBYID0gWzBdICogYmF0Y2hfc2l6ZSAjIFdlIGNhbiBjcmVhdGUgYSBsaXN0IHdpdGggYmF0Y2hfc2l6ZSBlbGVtZW50cy4gCiAgICAgICAgWSA9IFswXSAqIGJhdGNoX3NpemUgIyBXZSBjYW4gY3JlYXRlIGEgbGlzdCB3aXRoIGJhdGNoX3NpemUgZWxlbWVudHMuIAogICAgICAgIAogICAgICAgIGZvciBpIGluIHJhbmdlKGJhdGNoX3NpemUpOgogICAgICAgICAgICAKICAgICAgICAgICAgIyBXcmFwIHRoZSBpbmRleCBlYWNoIHRpbWUgdGhhdCB3ZSByZWFjaCB0aGUgZW5kIG9mIHRoZSBsaXN0CiAgICAgICAgICAgIGlmIGluZGV4ID49IGRhdGFfbG5nOgogICAgICAgICAgICAgICAgaW5kZXggPSAwCiAgICAgICAgICAgICAgICAjIFNodWZmbGUgdGhlIGluZGV4X2xpc3QgaWYgc2h1ZmZsZSBpcyB0cnVlCiAgICAgICAgICAgICAgICBpZiBzaHVmZmxlOgogICAgICAgICAgICAgICAgICAgIHJuZC5zaHVmZmxlKGluZGV4X2xpc3QpICMgcmUtc2h1ZmZsZSB0aGUgb3JkZXIKICAgICAgICAgICAgCiAgICAgICAgICAgIFhbaV0gPSBkYXRhX3hbaW5kZXhfbGlzdFtpbmRleF1dIAogICAgICAgICAgICBZW2ldID0gZGF0YV95W2luZGV4X2xpc3RbaW5kZXhdXSAKICAgICAgICAgICAgCiAgICAgICAgICAgIGluZGV4ICs9IDEKICAgICAgICAKICAgICAgICB5aWVsZCgoWCwgWSkp\"\n\n# Print the solution to the given assignment\nprint(base64.b64decode(solution).decode(\"utf-8\"))\n\ndef data_generator(batch_size, data_x, data_y, shuffle=True):\n\n    data_lng = len(data_x) # len(data_x) must be equal to len(data_y)\n    index_list = [*range(data_lng)] # Create a list with the ordered indexes of sample data\n    \n    # If shuffle is set to true, we traverse the list in a random way\n    if shuffle:\n        rnd.shuffle(index_list) # Inplace shuffle of the list\n    \n    index = 0 # Start with the first element\n    while True:\n        X = [0] * batch_size # We can create a list with batch_size elements. \n        Y = [0] * batch_size # We can create a list with batch_size elements. \n        \n        for i in range(batch_size):\n            \n            # Wrap the index each time that we reach the end of the list\n            if index &gt;= data_lng:\n                index = 0\n                # Shuffle the index_list if shuffle is true\n                if shuffle:\n                    rnd.shuffle(index_list) # re-shuffle the order\n            \n            X[i] = data_x[index_list[index]] \n            Y[i] = data_y[index_list[index]] \n            \n            index += 1\n        \n        yield((X, Y))\n\n\n\n\nHope you enjoyed this tutorial on data generators which will help you with the assignments in this course."
  },
  {
    "objectID": "posts/c1w4/index.html",
    "href": "posts/c1w4/index.html",
    "title": "Machine Translation and Document Search",
    "section": "",
    "text": "Figure¬†1: course banner\n\n\n\n\n\n\n\n\nFigure¬†2",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Machine Translation and Document Search via KNN"
    ]
  },
  {
    "objectID": "posts/c1w4/index.html#assignment",
    "href": "posts/c1w4/index.html#assignment",
    "title": "Machine Translation and Document Search",
    "section": "Assignment",
    "text": "Assignment\nAssignment\n\nReading: Bibliography\n\n(Jurafsky and Martin 2025) :Speech and Language Processing",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Machine Translation and Document Search via KNN"
    ]
  },
  {
    "objectID": "posts/c1w4/lab01.html",
    "href": "posts/c1w4/lab01.html",
    "title": "Vector manipulation in Python",
    "section": "",
    "text": "Figure¬†1: course banner\nIn this lab, you will have the opportunity to practice once again with the NumPy library. This time, we will explore some advanced operations with arrays and matrices.\nAt the end of the previous module, we used PCA to transform a set of many variables into a set of only two uncorrelated variables. This process was made through a transformation of the data called rotation.\nIn this week‚Äôs assignment, you will need to find a transformation matrix from English to French vector space embeddings. Such a transformation matrix is nothing else but a matrix that rotates and scales vector spaces.\nIn this notebook, we will explain in detail the rotation transformation."
  },
  {
    "objectID": "posts/c1w4/lab01.html#transforming-vectors",
    "href": "posts/c1w4/lab01.html#transforming-vectors",
    "title": "Vector manipulation in Python",
    "section": "Transforming vectors",
    "text": "Transforming vectors\nThere are three main vector transformations: * Scaling * Translation * Rotation\nIn previous notebooks, we have applied the first two kinds of transformations. Now, let us learn how to use a fundamental transformation on vectors called rotation.\nThe rotation operation changes the direction of a vector, letting unaffected its dimensionality and its norm. Let us explain with some examples.\nIn the following cells, we will define a NumPy matrix and a NumPy array. Soon we will explain how this is related to matrix rotation.\n\nimport numpy as np                     # Import numpy for array manipulation\nimport matplotlib.pyplot as plt        # Import matplotlib for charts\nfrom utils_nb import plot_vectors      # Function to plot vectors (arrows)\n\n\nExample 1\n\n# Create a 2 x 2 matrix\nR = np.array([[2, 0],\n              [0, -2]])\n\n\nx = np.array([[1, 1]]) # Create a 1 x 2 matrix\n\nThe dot product between a vector and a square matrix produces a rotation and a scaling of the original vector.\nRemember that our recommended way to get the dot product in Python is np.dot(a, b):\n\ny = np.dot(x, R) # Apply the dot product between x and R\ny\n\narray([[ 2, -2]])\n\n\nWe are going to use Pyplot to inspect the effect of the rotation on 2D vectors visually. For that, we have created a function plot_vectors() that takes care of all the intricate parts of the visual formatting. The code for this function is inside the utils_nb.py file.\nNow we can plot the vector \\vec x = [1, 1] in a cartesian plane. The cartesian plane will be centered at [0,0] and its x and y limits will be between [-4, +4]\n\nplot_vectors([x], axes=[4, 4], fname='transform_x.svg')\n\n\n\n\n\n\n\n\nNow, let‚Äôs plot in the same system our vector \\vec x = [1, 1] and its dot product with the matrix\nRo = \\begin{bmatrix} 2 & 0 \\\\ 0 & -2 \\end{bmatrix}\ny = x \\cdot Ro = [[-2, 2]]\n\nplot_vectors([x, y], axes=[4, 4], fname='transformx_and_y.svg')\n\n\n\n\n\n\n\n\nNote that the output vector y (blue) is transformed in another vector.\n\n\nExample 2\nWe are going to use Pyplot to inspect the effect of the rotation on 2D vectors visually. For that, we have created a function that takes care of all the intricate parts of the visual formatting. The following procedure plots an arrow within a Pyplot canvas.\nData that is composed of 2 real attributes is telling to belong to a $ RxR $ or $ R^2 $ space. Rotation matrices in R^2 rotate a given vector \\vec x by a counterclockwise angle \\theta in a fixed coordinate system. Rotation matrices are of the form:\nRo = \\begin{bmatrix} cos \\theta & -sin \\theta \\\\ sin \\theta & cos \\theta \\end{bmatrix}\nThe trigonometric functions in Numpy require the angle in radians, not in degrees. In the next cell, we define a rotation matrix that rotates vectors by 45^o.\n\nangle = 100 * (np.pi / 180) #convert degrees to radians\n\nRo = np.array([[np.cos(angle), -np.sin(angle)],\n              [np.sin(angle), np.cos(angle)]])\n\nx2 = np.array([2, 2]).reshape(1, -1) # make it a row vector\ny2 = np.dot(x2, Ro)\n\nprint('Rotation matrix')\nprint(Ro)\nprint('\\nRotated vector')\nprint(y2)\n\nprint('\\n x2 norm', np.linalg.norm(x2))\nprint('\\n y2 norm', np.linalg.norm(y2))\nprint('\\n Rotation matrix norm', np.linalg.norm(Ro))\n\nRotation matrix\n[[-0.17364818 -0.98480775]\n [ 0.98480775 -0.17364818]]\n\nRotated vector\n[[ 1.62231915 -2.31691186]]\n\n x2 norm 2.8284271247461903\n\n y2 norm 2.82842712474619\n\n Rotation matrix norm 1.414213562373095\n\n\n\nplot_vectors([x2, y2], fname='transform_02.svg')\n\n\n\n\n\n\n\n\nSome points to note:\n\nThe norm of the input vector is the same as the norm of the output vector. Rotations matrices do not modify the norm of the vector, only its direction.\nThe norm of any R^2 rotation matrix is always \\sqrt 2 = 1.414221"
  },
  {
    "objectID": "posts/c1w4/lab01.html#frobenius-norm",
    "href": "posts/c1w4/lab01.html#frobenius-norm",
    "title": "Vector manipulation in Python",
    "section": "Frobenius Norm",
    "text": "Frobenius Norm\nThe Frobenius norm is the generalization to R^2 of the already known norm function for vectors\n\\| \\vec a \\| = \\sqrt {{\\vec a} \\cdot {\\vec a}} \nFor a given R^2 matrix A, the frobenius norm is defined as:\n\\|\\mathrm{A}\\|_{F} \\equiv \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n}\\left|a_{i j}\\right|^{2}}\n\nA = np.array([[2, 2],\n              [2, 2]])\n\nnp.square() is a way to square each element of a matrix. It must be equivalent to use the * operator in Numpy arrays.\n\nA_squared = np.square(A)\nA_squared\n\narray([[4, 4],\n       [4, 4]])\n\n\nNow you can sum over the elements of the resulting array, and then get the square root of the sum.\n\nA_Frobenius = np.sqrt(np.sum(A_squared))\nA_Frobenius\n\nnp.float64(4.0)\n\n\nThat was the extended version of the np.linalg.norm() function. You can check that it yields the same result.\n\nprint('Frobenius norm of the Rotation matrix')\nprint(np.sqrt(np.sum(Ro * Ro)), '== ', np.linalg.norm(Ro))\n\nFrobenius norm of the Rotation matrix\n1.414213562373095 ==  1.414213562373095\n\n\nCongratulations!! We‚Äôve covered a few more matrix operations in this lab. This will come in handy in this week‚Äôs programming assignment!"
  },
  {
    "objectID": "posts/c3w3/index.html",
    "href": "posts/c3w3/index.html",
    "title": "LSTMs and Named Entity Recognition",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure¬†1\n\n\n\n\n\n\n\n\nFigure¬†2\nMy irreverent notes for Week 3 of the Natural Language Processing with Sequence Models Course in the Natural Language Processing Specialization Offered by DeepLearning.AI on Coursera",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c3w3/index.html#rnns-and-vanishing-gradients",
    "href": "posts/c3w3/index.html#rnns-and-vanishing-gradients",
    "title": "LSTMs and Named Entity Recognition",
    "section": "RNNs and Vanishing Gradients",
    "text": "RNNs and Vanishing Gradients\nAdvantages of RNNs RNNs allow us to capture dependancies within a short range and they take up less RAM than other n-gram models.\nDisadvantages of RNNs RNNs struggle with longer term dependencies and are very prone to vanishing or exploding gradients.\n\nNote that as we are back-propagating through time, we end up getting the following:\n\nNote that the sigmoid and tanh functions are bounded by 0 and 1 and -1 and 1 respectively. This eventually leads us to a problem. If we have many numbers that are less than |1|, then as we go through many layers, and we take the product of those numbers, we eventually end up getting a gradient that is very close to 0. This introduces the problem of vanishing gradients.\nSolutions to Vanishing Gradient Problems",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c3w3/index.html#optional-intro-to-optimization-in-deep-learning-gradient-descent",
    "href": "posts/c3w3/index.html#optional-intro-to-optimization-in-deep-learning-gradient-descent",
    "title": "LSTMs and Named Entity Recognition",
    "section": "(Optional) Intro to optimization in deep learning: Gradient Descent",
    "text": "(Optional) Intro to optimization in deep learning: Gradient Descent\nCheck out this blog from Paperspace.io if you‚Äôre interested in understanding in more depth some of the challenges in gradient descent.\n\n\nVisual Loss Landscapes For Neural Nets (Paper)",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c3w3/index.html#lab-lecture-notebook-vanishing-gradients",
    "href": "posts/c3w3/index.html#lab-lecture-notebook-vanishing-gradients",
    "title": "LSTMs and Named Entity Recognition",
    "section": "Lab: Lecture Notebook: Vanishing Gradients",
    "text": "Lab: Lecture Notebook: Vanishing Gradients\nVanishing Gradients",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c3w3/index.html#introduction-to-lstms",
    "href": "posts/c3w3/index.html#introduction-to-lstms",
    "title": "LSTMs and Named Entity Recognition",
    "section": "Introduction to LSTMs",
    "text": "Introduction to LSTMs\nThe LSTM allows your model to remember and forget certain inputs. It consists of a cell state and a hidden state with three gates. The gates allow the gradients to flow unchanged. We can think of the three gates as follows:\nInput gate: tells we how much information to input at any time point.\nForget gate: tells we how much information to forget at any time point.\nOutput gate: tells we how much information to pass over at any time point.\nThere are many applications we can use LSTMs for, such as:",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c3w3/index.html#optional-understanding-lstms",
    "href": "posts/c3w3/index.html#optional-understanding-lstms",
    "title": "LSTMs and Named Entity Recognition",
    "section": "(Optional) Understanding LSTMs",
    "text": "(Optional) Understanding LSTMs\nHere‚Äôs a classic post on LSTMs with intuitive explanations and diagrams, to complement this week‚Äôs material.\n\nUnderstanding LSTM Networks",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c3w3/index.html#lstm-architecture",
    "href": "posts/c3w3/index.html#lstm-architecture",
    "title": "LSTMs and Named Entity Recognition",
    "section": "LSTM Architecture",
    "text": "LSTM Architecture\nThe LSTM architecture could get complicated and don‚Äôt worry about it if we do not understand it. I personally prefer looking at the equation, but I will try to give we a visualization for now and later this week we will take a look at the equations.\n\nNote that there is the cell state and the hidden state, and then there is the output state. The forget gate is the first activation in the drawing above. It makes use of the previous hidden state h^{&lt;t_0&gt;} and the input x^{&lt;t_0&gt;}. The input gate makes use of the next two activations, the sigmoid and the tanh. Finally the output gate makes use of the last activation and the tanh right above it. This is just an overview of the architecture, we will dive into the details once we introduce the equations.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c3w3/index.html#introduction-to-named-entity-recognition",
    "href": "posts/c3w3/index.html#introduction-to-named-entity-recognition",
    "title": "LSTMs and Named Entity Recognition",
    "section": "Introduction to Named Entity Recognition",
    "text": "Introduction to Named Entity Recognition\nNamed Entity Recognition (NER) locates and extracts predefined entities from text. It allows we to find places, organizations, names, time and dates. Here is an example of the model we will be building:\n\nNER systems are being used in search efficiency, recommendation engines, customer service, automatic trading, and many more.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c3w3/index.html#lstm-equations-optional",
    "href": "posts/c3w3/index.html#lstm-equations-optional",
    "title": "LSTMs and Named Entity Recognition",
    "section": "LSTM equations (Optional)",
    "text": "LSTM equations (Optional)\nThese are the LSTM equations related to the gates I had previously spoken about:\n\nf = \\sigma(W_f[h_{t-1}; x_t] + b_f) \\qquad \\text{Forget}\n\\tag{1}\n\ni = \\sigma(W_i[h_{t-1}; x_t] + b_i) \\qquad \\text{Input}\n\\tag{2}\n\ng = \\tanh(W_g[h_{t-1}; x_t] + b_g) \\qquad \\text{Gate}\n\\tag{3}\n\nc_t = f \\odot c_{t-1} + i \\odot g \\qquad \\text{Cell State}\n\\tag{4}\n\no = \\sigma(W_o[h_{t-1}; x_t] + b_o) \\qquad \\text{Output}\n\\tag{5}\nWe can think of:\n\nThe forget gate as a gate that tells we how much information to forget,\nThe input gate, tells we how much information to pick up.\nThe gate gate as the gate containing information. This is multiplied by the input gate (which tells we how much of that information to keep).",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c3w3/index.html#training-ners-data-processing",
    "href": "posts/c3w3/index.html#training-ners-data-processing",
    "title": "LSTMs and Named Entity Recognition",
    "section": "Training NERs: Data Processing",
    "text": "Training NERs: Data Processing\nProcessing data is one of the most important tasks when training AI algorithms. For NER, we have to:\n\nConvert words and entity classes into arrays:\nPad with tokens: Set sequence length to a certain number and use the  token to fill empty spaces\nCreate a data generator:\n\nOnce we have that, we can assign each class a number, and each word a number.\n\n\n\n\nData Processing\n\nTraining an NER system:\n\nCreate a tensor for each input and its corresponding number\nPut them in a batch ==&gt; 64, 128, 256, 512 ‚Ä¶\nFeed it into an LSTM unit\nRun the output through a dense layer\nPredict using a log softmax over K classes\n\nHere is an example of the architecture:\n\nNote that this is just one example of an NER system. Different architectures are possible.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c3w3/index.html#long-short-term-memory-deep-learning-specialization-c5",
    "href": "posts/c3w3/index.html#long-short-term-memory-deep-learning-specialization-c5",
    "title": "LSTMs and Named Entity Recognition",
    "section": "Long Short-Term Memory (Deep Learning Specialization C5)",
    "text": "Long Short-Term Memory (Deep Learning Specialization C5)\nNote: this section is based on a transcript of the video from the Deep Learning Specialization.\n\n\n\n\n\n\n\nLSTM from GRU or LSTM from RNNs\n\n\n\nIn this video Andrew Ng explains the Long Short-Term Memory (LSTM) as if it was developed from the GRU rather than RNNs. Sure the GRU has one less equation but its just about as complicated as the LSTM ‚Ä¶ The people who came up with it had 20 years to understand LSTM before they figured these out. We on the other hand have covered neither RNNs not GRUs and have none of that intuition they provide.\nLSTM also have a number of variations, such as the peephole connection, the carousal connection, and the coupled forget and input gates. üò±\nIn the other course on Deep Learning, Ng builds things up using a number of videos starting with notation. RNNs, different types of RNNs, and then the LSTM. The vanishing gradient problem with RNN and then the GRUs.\nI find the notation used very annoying but at least it is explained in the other course and seems to be motivated by time series. In Rnns we process the data in two dimensions. One is for the sequence index used in the super script. The other is for applying multiple layers which we don‚Äôt seem to consider.\nRnns learn weights more weights as the sequence grows and though not clear these weights are shared across the RNN units.\n\n\n\n\n\n\n\n\nNotation\n\n\n\n\nX^{(i)&lt;t&gt;} is the input at time t for the ith example.\ny^{(i)&lt;t&gt;} is the output at time t for the ith example.\nT_x^{(i)} is the length of the input sequence for the ith example.\nT_y^{(i)} is the length of the output sequence for the ith example.\nis $T_x^(i) = T_y^(i) not necessarily. (e.g.¬†translation can be longer or shorter, while NER can be one to one.). They will be different for different examples.\nis T_x^(i) = T_x^(j) unlikely as the length of the input sequence will vary from example to example.\n\n\n\n\n\n\n\nGRU v.s. LSTM\n\n\n\n\nLSTM equations\n\n\n\n\nLSTM Schematic\n\n\n\n\nLSTM Rollout\n\n\n\n\nNow that we understand my caveat let‚Äôs try to understand by filling the gaps as we go along with what what Ng says and shows!\n\nIf we understand the math we are good to go. Code is just a translation of the math into a programming language. Once we understand the math there are deeper levels of understanding that we can get to but we can‚Äôt get there without understanding the math. There are three challenges to understanding the math!\n\nBoth the LSTM and GRU are RNNs so they translate sequences to sequences in the general case let‚Äôs imagine we are translating english to german. We start with some input x_0 and we output some out put y_0. For the next word we want to use the previous word to help us translate the next word. This is done by concatenating the current word to the previous (hidden) state. The first take away\nRNN have an internal nonlinearity a tanh but no gating mechanism. The non-linearity is applied to the hidden state concatenated to the input. The hidden state is thus the long term memory of the RNN. For NER we only need a short context to decide but for translation we need to be aware longer context, perhaps a few sentences back. [RNNS, GRUs and LSTMs also have the non-linearity at their core]. That‚Äôs the second take away about the math of LSTMS\nThe vanishing and exploding gradients are not the only issues in RNNs there is also a problem of accessing data from many time steps back. By accessing I mean backpropagating the gradients backwards enough time steps. In the LSTM there are not only pathways that let the state pass on unchanged they also allow the gradients to flow back unchanged similar to ResNets. So the two path from Hidden state to hidden state and from Internal State to internal state are what allows the LSTM to handle long term gradient gradients better than RNNS. This is particularly true for most cases where there is a ‚Äòshort circuit‚Äô allowing these state to persist. And has a stabilizing effect. That is the third take away from the math of LSTMs.\nThe update, forget, output gates is where the weight and biases are used, thus this is where the learning is taking place and this is happening in an element-wise manner. This is the fourth take away from the math of LSTMs.\nThese three gates also control how much of the new data is incorporated into the internal state c^{&lt;t&gt;} and the hidden state. This is referred to as the gating mechanism. And different variants of LSTM and GRUS make subtle changes to the gating. This is the fifth take away from the math of LSTMs.\nInformation flow in the LST is captured by the dependency between the equations is as follows:\n\nf_t, i_t, g_t, o_t the gate uses see the old stat and the new input.\nc_t the internal state sees f_t, i_t, and g_t and the old state c_t\nh_t sees o_t and c_t which depend on the previous hidden state h_{t-1} and the new input x_t. To sum up the gates only depend on the input an the previous hidden state. The internal state depends on the gates and the previous internal state. The next hidden state depends on the internal state and the output gate.. This is the sixth take away from the math of LSTMs.\n\n\n\n\n\\begin{aligned}\n\\textcolor{red}{f_t} &= \\textcolor{purple}{\\sigma}(\\textcolor{blue}{W_f}[h_{t-1}; x_t] + \\textcolor{blue}{b_f}) \\\\\n\\textcolor{red}{i_t} &= \\textcolor{purple}{\\sigma}(\\textcolor{blue}{W_i}[h_{t-1}; x_t] + \\textcolor{blue}{b_i}) \\\\\n\\textcolor{red}{g_t} &= \\textcolor{purple}{\\tanh}(\\textcolor{blue}{W_g}[h_{t-1}; x_t] + \\textcolor{blue}{b_g})  \\\\\n\\textcolor{red}{o_t} &= \\textcolor{purple}{\\sigma}(\\textcolor{blue}{W_o}[h_{t-1}; x_t] + \\textcolor{blue}{b_o}) \\\\\n\\textcolor{green}{c_t} &= \\textcolor{red}{f_t} \\textcolor{orange}{\\odot} \\textcolor{green}{c_{t-1}} + \\textcolor{red}{i_t} \\textcolor{orange}{\\odot} \\textcolor{red}{g_t}     \\\\\nh_t &= \\textcolor{red}{o_t} \\textcolor{orange}{\\odot} \\textcolor{purple}{\\tanh}(\\textcolor{green}{c_t})\n\\end{aligned}\n\\tag{6}\nkey:\n\nred for gates\nblue for weights\norange for element-wise operations\ngreen for the internal state\npurple for the non-linearity \n\nNext level of understanding is to consider the action of the gating machanism and the relation between internal state and hidden state.\n\n\nThe key things from this unit are that the GRU does not use a forget gate, but uses 1-\\Gamma_u to decide how much of the previous memory cell to keep. In the LSTM, the forget gate instead.\nThere are two aspects to understanding these RNNS.\nThe equations look like simultaneous equations, in reality they are they have a more complex structure as\nThe schematic are emphesise a two other aspects of the LSTM, information flow and gating mechanisms.\n\nhow the equations are wired up to control the information flow and - the idea that we have a gating mechanism that combines the long term memory a in the Hidden state and the uses the memory cell, rather than the hidden state.\n\nWe learned about the GRU, or gated recurrent units, and how that can allow we to learn very long range connections in a sequence. The other type of unit that allows we to do this very well is the LSTM or the long short term memory units, and this is even more powerful than the GRU. Let‚Äôs take a look.\nHere are the equations from the previous video for the GRU. And for the GRU, we had a^{&lt;t&gt;} = c^{&lt;t&gt;}, and two gates, the optic gate and the relevance gate, \\tilde{c}^{&lt;t&gt;}, which is a candidate for replacing the memory cell, and then we use the update gate, \\Gamma_u, to decide whether or not to update c^{&lt;t&gt;} using \\tilde{c}^{&lt;t&gt;}.\nThe LSTM is an even slightly more powerful and more general version of the GRU, and is due to Sepp Hochreiter and Jurgen Schmidhuber1, c.f. Hochreiter and Schmidhuber (1997). And this was a really seminal paper, a huge impact on sequence modelling.\n1¬†with many interesting talks onlineI think this paper is one of the more difficult to read. It goes quite along into theory of vanishing gradients. And so, I think more people have learned about the details of LSTM through maybe other places than from this particular paper even though I think this paper has had a wonderful impact on the Deep Learning community.\nBut these are the equations that govern the LSTM. So, we continue to the memory cell, c, and the candidate value for updating it, \\tilde{c}^{&lt;t&gt;}, will be this, and so on. Notice that for the LSTM, we will no longer have the case that a^{&lt;t&gt;} =c^{&lt;t&gt;}. So, this is what we use. And so, this is just like the equation on the left except that with now, more specially use a^{&lt;t&gt;} there or a^{&lt;t-1&gt;} instead of c^{&lt;t-1&gt;}. And we‚Äôre not using this gamma or this relevance gate. Although we could have a variation of the LSTM where we put that back in, but with the more common version of the LSTM, doesn‚Äôt bother with that. And then we will have an update gate, same as before. So, W updates and we‚Äôre going to use a^{&lt;t-1&gt;} here, x^{&lt;t&gt;} + b_u. And one new property of the LSTM is, instead of having one update gate control, both of these terms, we‚Äôre going to have two separate terms. So instead of gamma_u and one minus gamma_u, we‚Äôre going have \\Gamma_u here. And forget gate, which we‚Äôre going to call \\Gamma_f. So, this gate, \\Gamma_f, is going to be sigmoid of pretty much what you‚Äôd expect, x^{&lt;t&gt;}+ b_f. And then, we‚Äôre going to have a new output gate which is \\sigma(W_o)+ b_o. And then, the update value to the memory so will be c^{&lt;t&gt;}=\\Gamma_u. And this asterisk denotes element-wise multiplication. This is a vector-vector element-wise multiplication, plus, and instead of one minus gamma u, we‚Äôre going to have a separate forget gate, \\Gamma_f * c^{&lt;t-1&gt;}. So this gives the memory cell the option of keeping the old value c^{t-1} and then just adding to it, this new value, \\tilde{c}^{&lt;t&gt;}. So, use a separate update and forget gates. So, this stands for update, forget, and output gate.\nAnd then finally, instead of \na^{&lt;t&gt;} = c^{&lt;t&gt;} \\quad \\text{(GRU)} \\qquad a^{&lt;t&gt;} = \\Gamma_0 * \\tanh( c^{&lt;t&gt;}) \\quad \\text{(LSTM)}\n.\nSo, these are the equations that govern the LSTM and we can tell it has three gates instead of two. So, it‚Äôs a bit more complicated and it places the gates into slightly different places. So, here again are the equations governing the behavior of the LSTM.\nOnce again, it‚Äôs traditional to explain these things using pictures. So let me draw one here. And if these pictures are too complicated, don‚Äôt worry about it. I personally find the equations easier to understand than the picture. But I‚Äôll just show the picture here for the intuitions it conveys. The bigger picture here was very much inspired by a blog post due to Chris Ola, titled Understanding LSTM Network, and the diagram drawing here is quite similar to one that he drew in his blog post. But the key thing is to take away from this picture are maybe that we use a^{&lt;t-1&gt;} and x^{&lt;t&gt;} to compute all the gate values. In this picture, we have a^{&lt;t-1&gt;}, x^{&lt;t&gt;} coming together to compute the forget gate, to compute the update gates, and to compute output gate. And they also go through a tanh to compute \\tilde{c}^{&lt;t&gt;}. And then these values are combined in these complicated ways with element-wise multiplies and so on, to get c^{&lt;t&gt;} from the previous c^{&lt;t-1&gt;}. Now, one element of this is interesting as we have a bunch of these in parallel. So, that‚Äôs one of them and we connect them. We then connect these temporally. So it does the input x^{&lt;1&gt;} then x^{&lt;2&gt;}, x^{&lt;3&gt;}. So, we can take these units and just hold them up as follows, where the output a at the previous timestep is the input a at the next timestep, the c.¬†I‚Äôve simplified to diagrams a little bit in the bottom. And one cool thing about this you‚Äôll notice is that there‚Äôs this line at the top that shows how, so long as we set the forget and the update gate appropriately, it is relatively easy for the LSTM to have some value c_0 and have that be passed all the way to the right to have your, maybe, c^{&lt;3&gt;}=c^{&lt;0&gt;}. And this is why the LSTM, as well as the GRU, is very good at memorizing certain values even for a long time, for certain real values stored in the memory cell even for many, many timesteps.\nSo, that‚Äôs it for the LSTM.\nAs we can imagine, there are also a few variations on this that people use. Perhaps, the most common one is that instead of just having the gate values be dependent only on a_{t-1}, x^{&lt;t&gt;}, sometimes, people also sneak in there the values c_{t-1} as well. This is called a peephole connection, introduced in Gers and Schmidhuber (2000) Not a great name maybe but you‚Äôll see, peephole connection. What that means is that the gate values may depend not just on a^{&lt;t-1&gt;} and on x^{&lt;t&gt;}, but also on the previous memory cell value, and the peephole connection can go into all three of these gates‚Äô computations. So that‚Äôs one common variation we see of LSTMs. One technical detail is that these are, say, 100-dimensional vectors. So if we have a 100-dimensional hidden memory cell unit, and so is this. And the, say, fifth element of c^{t-1} affects only the fifth element of the corresponding gates, so that relationship is one-to-one, where not every element of the 100-dimensional c^{&lt;t-1&gt;} can affect all elements of the case. But instead, the first element of c^{&lt;t-1&gt;} affects the first element of the case, second element affects the second element, and so on. But if we ever read the paper and see someone talk about the peephole connection, that‚Äôs when they mean that c^{&lt;t-1&gt;} is used to affect the gate value as well. So, that‚Äôs it for the LSTM.\nWhen should we use a GRU? And when should we use an LSTM?\nThere isn‚Äôt widespread consensus in this. And even though I presented GRUs first, in the history of deep learning, LSTMs actually came much earlier, and then GRUs were relatively recent invention that were maybe derived as Pavia‚Äôs simplification of the more complicated LSTM model. Researchers have tried both of these models on many different problems, and on different problems, different algorithms will win out. So, there isn‚Äôt a universally-superior algorithm which is why I want to show we both of them. But I feel like when I am using these, the advantage of the GRU is that it‚Äôs a simpler model and so it is actually easier to build a much bigger network, it only has two gates, so computationally, it runs a bit faster. So, it scales the building somewhat bigger models but the LSTM is more powerful and more effective since it has three gates instead of two. If we want to pick one to use, I think LSTM has been the historically more proven choice. So, if we had to pick one, I think most people today will still use the LSTM as the default first thing to try. Although, I think in the last few years, GRUs had been gaining a lot of momentum and I feel like more and more teams are also using GRUs because they‚Äôre a bit simpler but often work just as well. It might be easier to scale them to even bigger problems. So, that‚Äôs it for LSTMs. Well, either GRUs or LSTMs, you‚Äôll be able to build neural network that can capture much longer range dependencies.\nThe correct version of the final equation in the output gate is here:\nhttps://www.coursera.org/learn/nlp-sequence-models/supplement/xdv6z/long-short-term-memory-lstm-correction",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c3w3/index.html#computing-accuracy",
    "href": "posts/c3w3/index.html#computing-accuracy",
    "title": "LSTMs and Named Entity Recognition",
    "section": "Computing Accuracy",
    "text": "Computing Accuracy\nTo compare the accuracy, just follow the following steps:\n\nPass test set through the model\nGet arg max across the prediction array\nMask padded tokens\nCompare with the true labels.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c3w3/index.html#reflections-on-this-unit",
    "href": "posts/c3w3/index.html#reflections-on-this-unit",
    "title": "LSTMs and Named Entity Recognition",
    "section": "Reflections on this unit",
    "text": "Reflections on this unit\n\n\n\n\n\n\nMain Research Questions\n\n\n\n\nWhat is the vanishing and exploding gradient problem in RNNs?\nHow can we measure the ability of RNN to access data from many time steps back?\nWhat is the nature of the hidden state in RNNs?\n\n\nshort term memory\n\n\nWhat is the nature of the internal state in RNNs?\n\n\nlong term memory\n\n\nHow are gradients updated in the LSTMs?\nwhat is the constant error carousel in LSTMs?\nhow does it solve the vanishing gradient problem?\nHow does gating work in LSTMs?\nAre the gates binary?\nwhat is the idea behind a peekhole LSTM \nwhat is the idea of the bLSTM \nAre all LSTMs stacked, cam we have a single layer LSTM?",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c3w3/index.html#resources",
    "href": "posts/c3w3/index.html#resources",
    "title": "LSTMs and Named Entity Recognition",
    "section": "Resources:",
    "text": "Resources:\n\n(Chadha 2020) Aman Chadha‚Äôs Notes\nIbrahim Jelliti‚Äôs Notes\nIntro to optimization in deep learning: Gradient Descent (Tutorial) Ayoosh Kathuria\nVisual Loss Landscapes For Neural Nets (Paper)\nArticle on Learning Rate Schedules by Hafidz Zulkifli.\nStochastic Weight Averaging (Paper)\nUnderstanding LSTM Networks\nThe Unreasonable Effectiveness of Recurrent Neural Networks by Andrej Karpathy",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c2w1/lab02.html",
    "href": "posts/c2w1/lab02.html",
    "title": "Candidates from String Edits",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure¬†1\n\n\nEstimated Time: 20 minutes\nCreate a list of candidate strings by applying an edit operation\n\nImports and Data\n\n# data\nword = 'dearz' # ü¶å\n\n\n\nSplits\nFind all the ways we can split a word into 2 parts !\n\n# splits with a loop\nsplits_a = []\nfor i in range(len(word)+1):\n    splits_a.append([word[:i],word[i:]])\n\nfor i in splits_a:\n    print(i)\n\n['', 'dearz']\n['d', 'earz']\n['de', 'arz']\n['dea', 'rz']\n['dear', 'z']\n['dearz', '']\n\n\n\n# same splits, done using a list comprehension\nsplits_b = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n\nfor i in splits_b:\n    print(i)\n\n('', 'dearz')\n('d', 'earz')\n('de', 'arz')\n('dea', 'rz')\n('dear', 'z')\n('dearz', '')\n\n\n\n\nDelete Edit\nDelete a letter from each string in the splits list.\nWhat this does is effectivly delete each possible letter from the original word being edited.\n\n# deletes with a loop\nsplits = splits_a\ndeletes = []\n\nprint('word : ', word)\nfor L,R in splits:\n    if R:\n        print(L + R[1:], ' &lt;-- delete ', R[0])\n\nword :  dearz\nearz  &lt;-- delete  d\ndarz  &lt;-- delete  e\nderz  &lt;-- delete  a\ndeaz  &lt;-- delete  r\ndear  &lt;-- delete  z\n\n\nIt‚Äôs worth taking a closer look at how this is excecuting a ‚Äòdelete‚Äô.\nTaking the first item from the splits list :\n\n# breaking it down\nprint('word : ', word)\none_split = splits[0]\nprint('first item from the splits list : ', one_split)\nL = one_split[0]\nR = one_split[1]\nprint('L : ', L)\nprint('R : ', R)\nprint('*** now implicit delete by excluding the leading letter ***')\nprint('L + R[1:] : ',L + R[1:], ' &lt;-- delete ', R[0])\n\nword :  dearz\nfirst item from the splits list :  ['', 'dearz']\nL :  \nR :  dearz\n*** now implicit delete by excluding the leading letter ***\nL + R[1:] :  earz  &lt;-- delete  d\n\n\nSo the end result transforms ‚Äòdearz‚Äô to ‚Äòearz‚Äô by deleting the first character.\nAnd we use a loop (code block above) or a list comprehension (code block below) to do this for the entire splits list.\n\n# deletes with a list comprehension\nsplits = splits_a\ndeletes = [L + R[1:] for L, R in splits if R]\n\nprint(deletes)\nprint('*** which is the same as ***')\nfor i in deletes:\n    print(i)\n\n['earz', 'darz', 'derz', 'deaz', 'dear']\n*** which is the same as ***\nearz\ndarz\nderz\ndeaz\ndear\n\n\n\n\nUngraded Exercise\nWe now have a list of candidate strings created after performing a delete edit.  Next step will be to filter this list for candidate words found in a vocabulary.  Given the example vocab below, can we think of a way to create a list of candidate words ?  Remember, we already have a list of candidate strings, some of which are certainly not actual words we might find in your vocabulary !   So from the above list earz, darz, derz, deaz, dear.  You‚Äôre really only interested in dear.\n\nvocab = ['dean','deer','dear','fries','and','coke']\nedits = list(deletes)\n\nprint('vocab : ', vocab)\nprint('edits : ', edits)\n\ncandidates=[]\n\n### START CODE HERE ###\ncandidates = set(vocab).intersection(edits)  # hint: 'set.intersection'\n### END CODE HERE ###\n\nprint('candidate words : ', candidates)\n\nvocab :  ['dean', 'deer', 'dear', 'fries', 'and', 'coke']\nedits :  ['earz', 'darz', 'derz', 'deaz', 'dear']\ncandidate words :  {'dear'}\n\n\nExpected Outcome:\nvocab : [‚Äòdean‚Äô, ‚Äòdeer‚Äô, ‚Äòdear‚Äô, ‚Äòfries‚Äô, ‚Äòand‚Äô, ‚Äòcoke‚Äô]\nedits : [‚Äòearz‚Äô, ‚Äòdarz‚Äô, ‚Äòderz‚Äô, ‚Äòdeaz‚Äô, ‚Äòdear‚Äô]\ncandidate words : {‚Äòdear‚Äô}\n\n\nSummary\nYou‚Äôve unpacked an integral part of the assignment by breaking down splits and edits, specifically looking at deletes here.\nImplementation of the other edit types (insert, replace, switch) follows a similar methodology and should now feel somewhat familiar when we see them.\nThis bit of the code isn‚Äôt as intuitive as other sections, so well done!\nWe should now feel confident facing some of the more technical parts of the assignment at the end of the week.\n\n\n\n\nCitationBibTeX citation:@online{bochman2025,\n  author = {Bochman, Oren},\n  title = {Candidates from {String} {Edits}},\n  date = {2025-02-05},\n  url = {https://orenbochman.github.io/notes-nlp/posts/c2w1/lab02.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2025. ‚ÄúCandidates from String Edits.‚Äù\nFebruary 5, 2025. https://orenbochman.github.io/notes-nlp/posts/c2w1/lab02.html.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "L2 - Candidates from String Edits"
    ]
  },
  {
    "objectID": "posts/c2w1/lab03.html",
    "href": "posts/c2w1/lab03.html",
    "title": "Assignment 1: Auto Correct",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure¬†1\nWelcome to the first assignment of Course 2. This assignment will give you a chance to brush up on your python and probability skills. In doing so, you will implement an auto-correct system that is very effective and useful."
  },
  {
    "objectID": "posts/c2w1/lab03.html#outline",
    "href": "posts/c2w1/lab03.html#outline",
    "title": "Assignment 1: Auto Correct",
    "section": "Outline",
    "text": "Outline\n\n0. Overview\n\n0.1 Edit Distance\n\n1. Data Preprocessing\n\n1.1 Exercise 1\n1.2 Exercise 2\n1.3 Exercise 3\n\n2. String Manipulation\n\n2.1 Exercise 4\n2.2 Exercise 5\n2.3 Exercise 6\n2.4 Exercise 7\n\n3. Combining the edits\n\n3.1 Exercise 8\n3.2 Exercise 9\n3.3 Exercise 10\n\n4. Minimum Edit Distance\n\n4.1 Exercise 11\n\n5. Backtrace (Optional)\n\n ## 0. Overview\nYou use autocorrect every day on your cell phone and computer. In this assignment, you will explore what really goes on behind the scenes. Of course, the model you are about to implement is not identical to the one used in your phone, but it is still quite good.\nBy completing this assignment you will learn how to:\n\nGet a word count given a corpus\nGet a word probability in the corpus\nManipulate strings\nFilter strings\nImplement Minimum edit distance to compare strings and to help find the optimal path for the edits.\nUnderstand how dynamic programming works\n\nSimilar systems are used everywhere. - For example, if you type in the word ‚ÄúI am lerningg‚Äù, chances are very high that you meant to write ‚Äúlearning‚Äù, as shown in Figure 1.\n\n Figure 1\n\n #### 0.1 Edit Distance\nIn this assignment, you will implement models that correct words that are 1 and 2 edit distances away. - We say two words are n edit distance away from each other when we need n edits to change one word into another.\nAn edit could consist of one of the following options:\n\nDelete (remove a letter): ‚Äòhat‚Äô =&gt; ‚Äòat, ha, ht‚Äô\nSwitch (swap 2 adjacent letters): ‚Äòeta‚Äô =&gt; ‚Äòeat, tea,‚Ä¶‚Äô\nReplace (change 1 letter to another): ‚Äòjat‚Äô =&gt; ‚Äòhat, rat, cat, mat, ‚Ä¶‚Äô\nInsert (add a letter): ‚Äòte‚Äô =&gt; ‚Äòthe, ten, ate, ‚Ä¶‚Äô\n\nYou will be using the four methods above to implement an Auto-correct. - To do so, you will need to compute probabilities that a certain word is correct given an input.\nThis auto-correct you are about to implement was first created by Peter Norvig in 2007. - His original article may be a useful reference for this assignment.\nThe goal of our spell check model is to compute the following probability:\nP(c|w) = \\frac{P(w|c)\\times P(c)}{P(w)} \\tag{Eqn-1}\nThe equation above is Bayes Rule. - Equation 1 says that the probability of a word being correct $P(c|w) $is equal to the probability of having a certain word w, given that it is correct P(w|c), multiplied by the probability of being correct in general P(C) divided by the probability of that word w appearing P(w) in general. - To compute equation 1, you will first import a data set and then create all the probabilities that you need using that data set.\n # Part 1: Data Preprocessing\n\nimport re\nfrom collections import Counter\nimport numpy as np\nimport pandas as pd\n\nAs in any other machine learning task, the first thing you have to do is process your data set. - Many courses load in pre-processed data for you. - However, in the real world, when you build these NLP systems, you load the datasets and process them. - So let‚Äôs get some real world practice in pre-processing the data!\nYour first task is to read in a file called ‚Äòshakespeare.txt‚Äô which is found in your file directory. To look at this file you can go to File ==&gt; Open.\n ### Exercise 1 Implement the function process_data which\n\nReads in a corpus (text file)\nChanges everything to lowercase\nReturns a list of words.\n\n\nOptions and Hints\n\nIf you would like more of a real-life practice, don‚Äôt open the ‚ÄòHints‚Äô below (yet) and try searching the web to derive your answer.\nIf you want a little help, click on the green ‚ÄúGeneral Hints‚Äù section by clicking on it with your mouse.\nIf you get stuck or are not getting the expected results, click on the green ‚ÄòDetailed Hints‚Äô section to get hints for each step that you‚Äôll take to complete this function.\n\n\n\nGeneral Hints\n\n\nGeneral Hints to get started\n\n\nPython input and output\n\n\nPython ‚Äòre‚Äô documentation \n\n\n\n\n\nDetailed Hints\n\n\nDetailed hints if you‚Äôre stuck\n\n\nUse ‚Äòwith‚Äô syntax to read a file\n\n\nDecide whether to use ‚Äòread()‚Äô or ‚Äôreadline(). What‚Äôs the difference?\n\n\nChoose whether to use either str.lower() or str.lowercase(). What is the difference?\n\n\nUse re.findall(pattern, string)\n\n\nLook for the ‚ÄúRaw String Notation‚Äù section in the Python ‚Äòre‚Äô documentation to understand the difference between r‚Äô‚Äò, r‚Äô‚Äô and ‚Äò\\W‚Äô.\n\n\nFor the pattern, decide between using ‚Äò‚Äô, ‚Äò‚Äô, ‚Äò+‚Äô or ‚Äò+‚Äô. What do you think are the differences?\n\n\n\n\n# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: process_data\ndef process_data(file_name):\n    \"\"\"\n    Input: \n        A file_name which is found in your current directory. You just have to read it in. \n    Output: \n        words: a list containing all the words in the corpus (text file you read) in lower case. \n    \"\"\"\n    words = [] # return this variable correctly\n\n    ### START CODE HERE ### \n    words = re.findall(r'\\w+',open(file_name).read().lower())\n    ### END CODE HERE ###\n    \n    return words\n\nNote, in the following cell, ‚Äòwords‚Äô is converted to a python set. This eliminates any duplicate entries.\n\n#DO NOT MODIFY THIS CELL\nword_l = process_data('shakespeare.txt')\nvocab = set(word_l)  # this will be your new vocabulary\nprint(f\"The first ten words in the text are: \\n{word_l[0:10]}\")\nprint(f\"There are {len(vocab)} unique words in the vocabulary.\")\n\nThe first ten words in the text are: \n['o', 'for', 'a', 'muse', 'of', 'fire', 'that', 'would', 'ascend', 'the']\nThere are 6116 unique words in the vocabulary.\n\n\n\n\nExpected Output\nThe first ten words in the text are: \n['o', 'for', 'a', 'muse', 'of', 'fire', 'that', 'would', 'ascend', 'the']\nThere are 6116 unique words in the vocabulary.\n ### Exercise 2\nImplement a get_count function that returns a dictionary - The dictionary‚Äôs keys are words - The value for each word is the number of times that word appears in the corpus.\nFor example, given the following sentence: ‚ÄúI am happy because I am learning‚Äù, your dictionary should return the following:\n\n\n\nKey \n\n\nValue \n\n\n\n\nI\n\n\n2\n\n\n\n\nam\n\n\n2\n\n\n\n\nhappy\n\n\n1\n\n\n\n\nbecause\n\n\n1\n\n\n\n\nlearning\n\n\n1\n\n\n\nInstructions: Implement a get_count which returns a dictionary where the key is a word and the value is the number of times the word appears in the list.\n\n\nHints\n\n\n\n\nTry implementing this using a for loop and a regular dictionary. This may be good practice for similar coding interview questions\n\n\nYou can also use defaultdict instead of a regualr dictionary, along with the for loop\n\n\nOtherwise, to skip using a for loop, you can use Python‚Äôs  Counter class\n\n\n\n\n# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# UNIT TEST COMMENT: Candidate for Table Driven Tests\n# GRADED FUNCTION: get_count\ndef get_count(word_l):\n    '''\n    Input:\n        word_l: a set of words representing the corpus. \n    Output:\n        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.\n    '''\n    \n    word_count_dict = {}  # fill this with word counts\n    ### START CODE HERE \n    word_count_dict = Counter(word_l)\n    ### END CODE HERE ### \n    return word_count_dict\n\n\n#DO NOT MODIFY THIS CELL\nword_count_dict = get_count(word_l)\nprint(f\"There are {len(word_count_dict)} key values pairs\")\nprint(f\"The count for the word 'thee' is {word_count_dict.get('thee',0)}\")\n\nThere are 6116 key values pairs\nThe count for the word 'thee' is 240\n\n\n\n\nExpected Output\nThere are 6116 key values pairs\nThe count for the word 'thee' is 240\n ### Exercise 3 Given the dictionary of word counts, compute the probability that each word will appear if randomly selected from the corpus of words.\nP(w_i) = \\frac{C(w_i)}{M} \\tag{Eqn-2} where\nC(w_i) is the total number of times w_i appears in the corpus.\nM is the total number of words in the corpus.\nFor example, the probability of the word ‚Äòam‚Äô in the sentence ‚ÄòI am happy because I am learning‚Äô is:\nP(am) = \\frac{C(w_i)}{M} = \\frac {2}{7} \\tag{Eqn-3}.\nInstructions: Implement get_probs function which gives you the probability that a word occurs in a sample. This returns a dictionary where the keys are words, and the value for each word is its probability in the corpus of words.\n\n\nHints\n\n\nGeneral advice\n\n\nUse dictionary.values()\n\n\nUse sum()\n\n\nThe cardinality (number of words in the corpus should be equal to len(word_l). You will calculate this same number, but using the word count dictionary.\n\n\nIf you‚Äôre using a for loop:\n\n\nUse dictionary.keys()\n\n\nIf you‚Äôre using a dictionary comprehension:\n\n\nUse dictionary.items()\n\n\n\n\n# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: get_probs\ndef get_probs(word_count_dict):\n    '''\n    Input:\n        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.\n    Output:\n        probs: A dictionary where keys are the words and the values are the probability that a word will occur. \n    '''\n    probs = {}  # return this variable correctly\n    \n    ### START CODE HERE ###\n    probs = {k:v/sum(word_count_dict.values()) for k, v in word_count_dict.items()}\n    ### END CODE HERE ###\n    return probs\n\n\n#DO NOT MODIFY THIS CELL\nprobs = get_probs(word_count_dict)\nprint(f\"Length of probs is {len(probs)}\")\nprint(f\"P('thee') is {probs['thee']:.4f}\")\n\nLength of probs is 6116\nP('thee') is 0.0045\n\n\n\n\nExpected Output\nLength of probs is 6116\nP('thee') is 0.0045\n # Part 2: String Manipulations\nNow, that you have computed P(w_i) for all the words in the corpus, you will write a few functions to manipulate strings so that you can edit the erroneous strings and return the right spellings of the words. In this section, you will implement four functions:\n\ndelete_letter: given a word, it returns all the possible strings that have one character removed.\nswitch_letter: given a word, it returns all the possible strings that have two adjacent letters switched.\nreplace_letter: given a word, it returns all the possible strings that have one character replaced by another different letter.\ninsert_letter: given a word, it returns all the possible strings that have an additional character inserted.\n\n\n\nList comprehensions\nString and list manipulation in python will often make use of a python feature called list comprehensions. The routines below will be described as using list comprehensions, but if you would rather implement them in another way, you are free to do so as long as the result is the same. Further, the following section will provide detailed instructions on how to use list comprehensions and how to implement the desired functions. If you are a python expert, feel free to skip the python hints and move to implementing the routines directly.\nPython List Comprehensions embed a looping structure inside of a list declaration, collapsing many lines of code into a single line. If you are not familiar with them, they seem slightly out of order relative to for loops.\n\n Figure 2\n\nThe diagram above shows that the components of a list comprehension are the same components you would find in a typical for loop that appends to a list, but in a different order. With that in mind, we‚Äôll continue the specifics of this assignment. We will be very descriptive for the first function, deletes(), and less so in later functions as you become familiar with list comprehensions.\n ### Exercise 4\nInstructions for delete_letter(): Implement a delete_letter() function that, given a word, returns a list of strings with one character deleted.\nFor example, given the word nice, it would return the set: {‚Äòice‚Äô, ‚Äònce‚Äô, ‚Äònic‚Äô, ‚Äònie‚Äô}.\nStep 1: Create a list of ‚Äòsplits‚Äô. This is all the ways you can split a word into Left and Right: For example,\n‚Äônice is split into : [('', 'nice'), ('n', 'ice'), ('ni', 'ce'), ('nic', 'e'), ('nice', '')] This is common to all four functions (delete, replace, switch, insert).\n\n Figure 3\n\nStep 2: This is specific to delete_letter. Here, we are generating all words that result from deleting one character.\nThis can be done in a single line with a list comprehension. You can make use of this type of syntax:\n[f(a,b) for a, b in splits if condition]\nFor our ‚Äònice‚Äô example you get: [‚Äòice‚Äô, ‚Äònce‚Äô, ‚Äònie‚Äô, ‚Äònic‚Äô]\n\n Figure 4\n\n\n\nLevels of assistance\nTry this exercise with these levels of assistance.\n- We hope that this will make it both a meaningful experience but also not a frustrating experience. - Start with level 1, then move onto level 2, and 3 as needed.\n- Level 1. Try to think this through and implement this yourself.\n- Level 2. Click on the \"Level 2 Hints\" section for some hints to get started.\n- Level 3. If you would prefer more guidance, please click on the \"Level 3 Hints\" cell for step by step instructions.\n\nIf you are still stuck, look at the images in the ‚Äúlist comprehensions‚Äù section above.\n\n\n\nLevel 2 Hints\n\n\n\n\n Use array slicing like my_string[0:2] \n\n\n Use list comprehensions or for loops \n\n\n\n\n\nLevel 3 Hints\n\n\n\n\nsplits: Use array slicing, like my_str[0:2], to separate a string into two pieces.\n\n\nDo this in a loop or list comprehension, so that you have a list of tuples.\n\nFor example, ‚Äúcake‚Äù can get split into ‚Äúca‚Äù and ‚Äúke‚Äù. They‚Äôre stored in a tuple (‚Äúca‚Äù,‚Äúke‚Äù), and the tuple is appended to a list. We‚Äôll refer to these as L and R, so the tuple is (L,R)\n\n&lt;li&gt;When choosing the range for your loop, if you input the word \"cans\" and generate the tuple  ('cans',''), make sure to include an if statement to check the length of that right-side string (R) in the tuple (L,R) &lt;/li&gt;\n&lt;li&gt;deletes: Go through the list of tuples and combine the two strings together. You can use the + operator to combine two strings&lt;/li&gt;\n&lt;li&gt;When combining the tuples, make sure that you leave out a middle character.&lt;/li&gt;\n&lt;li&gt;Use array slicing to leave out the first character of the right substring.&lt;/li&gt;\n\n\n\n# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# UNIT TEST COMMENT: Candidate for Table Driven Tests\n# GRADED FUNCTION: deletes\ndef delete_letter(word, verbose=False):\n    '''\n    Input:\n        word: the string/word for which you will generate all possible words \n                in the vocabulary which have 1 missing character\n    Output:\n        delete_l: a list of all possible strings obtained by deleting 1 character from word\n    '''\n    \n    delete_l = []\n    split_l = []\n    \n    ### START CODE HERE ###\n    split_l = [(word[:i], word[i:]) for i in range(len(word))]\n    delete_l = [l+r[1:] for l, r in split_l]\n    ### END CODE HERE ###\n\n    if verbose: print(f\"input word {word}, \\nsplit_l = {split_l}, \\ndelete_l = {delete_l}\")\n\n    return delete_l\n\n\ndelete_word_l = delete_letter(word=\"cans\",\n                        verbose=True)\n\ninput word cans, \nsplit_l = [('', 'cans'), ('c', 'ans'), ('ca', 'ns'), ('can', 's')], \ndelete_l = ['ans', 'cns', 'cas', 'can']\n\n\n\n\nExpected Output\nNote: You might get a slightly different result with split_l\n\ninput word cans, \nsplit_l = [('', 'cans'), ('c', 'ans'), ('ca', 'ns'), ('can', 's')], \ndelete_l = ['ans', 'cns', 'cas', 'can']\n\n\nNote 1\n\nNotice how it has the extra tuple ('cans', '').\nThis will be fine as long as you have checked the size of the right-side substring in tuple (L,R).\nCan you explain why this will give you the same result for the list of deletion strings (delete_l)?\n\ninput word cans, \nsplit_l = [('', 'cans'), ('c', 'ans'), ('ca', 'ns'), ('can', 's'), ('cans', '')], \ndelete_l = ['ans', 'cns', 'cas', 'can']\n\n\nNote 2\nIf you end up getting the same word as your input word, like this:\ninput word cans, \nsplit_l = [('', 'cans'), ('c', 'ans'), ('ca', 'ns'), ('can', 's'), ('cans', '')], \ndelete_l = ['ans', 'cns', 'cas', 'can', 'cans']\n\nCheck how you set the range.\nSee if you check the length of the string on the right-side of the split.\n\n\n# test # 2\nprint(f\"Number of outputs of delete_letter('at') is {len(delete_letter('at'))}\")\n\nNumber of outputs of delete_letter('at') is 2\n\n\n\n\nExpected output\nNumber of outputs of delete_letter('at') is 2\n ### Exercise 5\nInstructions for switch_letter(): Now implement a function that switches two letters in a word. It takes in a word and returns a list of all the possible switches of two letters that are adjacent to each other. - For example, given the word ‚Äòeta‚Äô, it returns {‚Äòeat‚Äô, ‚Äòtea‚Äô}, but does not return ‚Äòate‚Äô.\nStep 1: is the same as in delete_letter()\nStep 2: A list comprehension or for loop which forms strings by swapping adjacent letters. This is of the form:\n[f(L,R) for L, R in splits if condition] where ‚Äòcondition‚Äô will test the length of R in a given iteration. See below.\n\n Figure 5\n\n\n\nLevels of difficulty\nTry this exercise with these levels of difficulty.\n- Level 1. Try to think this through and implement this yourself. - Level 2. Click on the ‚ÄúLevel 2 Hints‚Äù section for some hints to get started. - Level 3. If you would prefer more guidance, please click on the ‚ÄúLevel 3 Hints‚Äù cell for step by step instructions.\n\n\nLevel 2 Hints\n\n\n\n\n Use array slicing like my_string[0:2] \n\n\n Use list comprehensions or for loops \n\n\nTo do a switch, think of the whole word as divided into 4 distinct parts. Write out ‚Äòcupcakes‚Äô on a piece of paper and see how you can split it into (‚Äòcupc‚Äô, ‚Äòk‚Äô, ‚Äòa‚Äô, ‚Äòes‚Äô)\n\n\n\n\n\nLevel 3 Hints\n\n\n\n\nsplits: Use array slicing, like my_str[0:2], to separate a string into two pieces.\n\n\nSplitting is the same as for delete_letter\n\n\nTo perform the switch, go through the list of tuples and combine four strings together. You can use the + operator to combine strings\n\n\nThe four strings will be the left substring from the split tuple, followed by the first (index 1) character of the right substring, then the zero-th character (index 0) of the right substring, and then the remaining part of the right substring.\n\n\nUnlike delete_letter, you will want to check that your right substring is at least a minimum length. To see why, review the previous hint bullet point (directly before this one).\n\n\n\n\n# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# UNIT TEST COMMENT: Candidate for Table Driven Tests\n# GRADED FUNCTION: switches\ndef switch_letter(word, verbose=False):\n    '''\n    Input:\n        word: input string\n     Output:\n        switches: a list of all possible strings with one adjacent charater switched\n    ''' \n    \n    switch_l = []\n    split_l = []\n    \n    ### START CODE HERE ###\n    split_l = [(word[:i], word[i:]) for i in range(len(word))]\n    switch_l = [l+r[1]+r[0]+r[2:] for l, r in split_l if len(r) &gt; 1]\n    ### END CODE HERE ###\n    \n    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nswitch_l = {switch_l}\") \n\n    return switch_l\n\n\nswitch_word_l = switch_letter(word=\"eta\",\n                         verbose=True)\n\nInput word = eta \nsplit_l = [('', 'eta'), ('e', 'ta'), ('et', 'a')] \nswitch_l = ['tea', 'eat']\n\n\n\n\nExpected output\nInput word = eta \nsplit_l = [('', 'eta'), ('e', 'ta'), ('et', 'a')] \nswitch_l = ['tea', 'eat']\n\n\nNote 1\nYou may get this:\nInput word = eta \nsplit_l = [('', 'eta'), ('e', 'ta'), ('et', 'a'), ('eta', '')] \nswitch_l = ['tea', 'eat']\n\nNotice how it has the extra tuple ('eta', '').\nThis is also correct.\nCan you think of why this is the case?\n\n\n\nNote 2\nIf you get an error\nIndexError: string index out of range\n\nPlease see if you have checked the length of the strings when switching characters.\n\n\n# test # 2\nprint(f\"Number of outputs of switch_letter('at') is {len(switch_letter('at'))}\")\n\nNumber of outputs of switch_letter('at') is 1\n\n\n\n\nExpected output\nNumber of outputs of switch_letter('at') is 1\n ### Exercise 6 Instructions for replace_letter(): Now implement a function that takes in a word and returns a list of strings with one replaced letter from the original word.\nStep 1: is the same as in delete_letter()\nStep 2: A list comprehension or for loop which form strings by replacing letters. This can be of the form:\n[f(a,b,c) for a, b in splits if condition for c in string] Note the use of the second for loop.\nIt is expected in this routine that one or more of the replacements will include the original word. For example, replacing the first letter of ‚Äòear‚Äô with ‚Äòe‚Äô will return ‚Äòear‚Äô.\nStep 3: Remove the original input letter from the output.\n\n\nHints\n\n\n\n\nTo remove a word from a list, first store its contents inside a set()\n\n\nUse set.discard(‚Äòthe_word‚Äô) to remove a word in a set (if the word does not exist in the set, then it will not throw a KeyError. Using set.remove(‚Äòthe_word‚Äô) throws a KeyError if the word does not exist in the set.\n\n\n\n\n# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# UNIT TEST COMMENT: Candidate for Table Driven Tests\n# GRADED FUNCTION: replaces\ndef replace_letter(word, verbose=False):\n    '''\n    Input:\n        word: the input string/word \n    Output:\n        replaces: a list of all possible strings where we replaced one letter from the original word. \n    ''' \n    \n    letters = 'abcdefghijklmnopqrstuvwxyz'\n    replace_l = []\n    split_l = []\n    \n    ### START CODE HERE ###\n    split_l = [(word[:i], word[i:]) for i in range(len(word))]\n    replace_set = set([l+replace+r[1:] for l, r in split_l if len(r) &gt; 0 for replace in letters])\n    replace_set.discard(word)\n    ### END CODE HERE ###\n    \n    # turn the set back into a list and sort it, for easier viewing\n    replace_l = sorted(list(replace_set))\n    \n    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nreplace_l {replace_l}\")   \n    \n    return replace_l\n\n\nreplace_l = replace_letter(word='can',\n                              verbose=True)\n\nInput word = can \nsplit_l = [('', 'can'), ('c', 'an'), ('ca', 'n')] \nreplace_l ['aan', 'ban', 'caa', 'cab', 'cac', 'cad', 'cae', 'caf', 'cag', 'cah', 'cai', 'caj', 'cak', 'cal', 'cam', 'cao', 'cap', 'caq', 'car', 'cas', 'cat', 'cau', 'cav', 'caw', 'cax', 'cay', 'caz', 'cbn', 'ccn', 'cdn', 'cen', 'cfn', 'cgn', 'chn', 'cin', 'cjn', 'ckn', 'cln', 'cmn', 'cnn', 'con', 'cpn', 'cqn', 'crn', 'csn', 'ctn', 'cun', 'cvn', 'cwn', 'cxn', 'cyn', 'czn', 'dan', 'ean', 'fan', 'gan', 'han', 'ian', 'jan', 'kan', 'lan', 'man', 'nan', 'oan', 'pan', 'qan', 'ran', 'san', 'tan', 'uan', 'van', 'wan', 'xan', 'yan', 'zan']\n\n\n\n\nExpected Output**:\nInput word = can \nsplit_l = [('', 'can'), ('c', 'an'), ('ca', 'n')] \nreplace_l ['aan', 'ban', 'caa', 'cab', 'cac', 'cad', 'cae', 'caf', 'cag', 'cah', 'cai', 'caj', 'cak', 'cal', 'cam', 'cao', 'cap', 'caq', 'car', 'cas', 'cat', 'cau', 'cav', 'caw', 'cax', 'cay', 'caz', 'cbn', 'ccn', 'cdn', 'cen', 'cfn', 'cgn', 'chn', 'cin', 'cjn', 'ckn', 'cln', 'cmn', 'cnn', 'con', 'cpn', 'cqn', 'crn', 'csn', 'ctn', 'cun', 'cvn', 'cwn', 'cxn', 'cyn', 'czn', 'dan', 'ean', 'fan', 'gan', 'han', 'ian', 'jan', 'kan', 'lan', 'man', 'nan', 'oan', 'pan', 'qan', 'ran', 'san', 'tan', 'uan', 'van', 'wan', 'xan', 'yan', 'zan']\n\nNote how the input word ‚Äòcan‚Äô should not be one of the output words.\n\n\n\nNote 1\nIf you get something like this:\nInput word = can \nsplit_l = [('', 'can'), ('c', 'an'), ('ca', 'n'), ('can', '')] \nreplace_l ['aan', 'ban', 'caa', 'cab', 'cac', 'cad', 'cae', 'caf', 'cag', 'cah', 'cai', 'caj', 'cak', 'cal', 'cam', 'cao', 'cap', 'caq', 'car', 'cas', 'cat', 'cau', 'cav', 'caw', 'cax', 'cay', 'caz', 'cbn', 'ccn', 'cdn', 'cen', 'cfn', 'cgn', 'chn', 'cin', 'cjn', 'ckn', 'cln', 'cmn', 'cnn', 'con', 'cpn', 'cqn', 'crn', 'csn', 'ctn', 'cun', 'cvn', 'cwn', 'cxn', 'cyn', 'czn', 'dan', 'ean', 'fan', 'gan', 'han', 'ian', 'jan', 'kan', 'lan', 'man', 'nan', 'oan', 'pan', 'qan', 'ran', 'san', 'tan', 'uan', 'van', 'wan', 'xan', 'yan', 'zan']\n\nNotice how split_l has an extra tuple ('can', ''), but the output is still the same, so this is okay.\n\n\n\nNote 2\nIf you get something like this:\nInput word = can \nsplit_l = [('', 'can'), ('c', 'an'), ('ca', 'n'), ('can', '')] \nreplace_l ['aan', 'ban', 'caa', 'cab', 'cac', 'cad', 'cae', 'caf', 'cag', 'cah', 'cai', 'caj', 'cak', 'cal', 'cam', 'cana', 'canb', 'canc', 'cand', 'cane', 'canf', 'cang', 'canh', 'cani', 'canj', 'cank', 'canl', 'canm', 'cann', 'cano', 'canp', 'canq', 'canr', 'cans', 'cant', 'canu', 'canv', 'canw', 'canx', 'cany', 'canz', 'cao', 'cap', 'caq', 'car', 'cas', 'cat', 'cau', 'cav', 'caw', 'cax', 'cay', 'caz', 'cbn', 'ccn', 'cdn', 'cen', 'cfn', 'cgn', 'chn', 'cin', 'cjn', 'ckn', 'cln', 'cmn', 'cnn', 'con', 'cpn', 'cqn', 'crn', 'csn', 'ctn', 'cun', 'cvn', 'cwn', 'cxn', 'cyn', 'czn', 'dan', 'ean', 'fan', 'gan', 'han', 'ian', 'jan', 'kan', 'lan', 'man', 'nan', 'oan', 'pan', 'qan', 'ran', 'san', 'tan', 'uan', 'van', 'wan', 'xan', 'yan', 'zan']\n\nNotice how there are strings that are 1 letter longer than the original word, such as cana.\nPlease check for the case when there is an empty string '', and if so, do not use that empty string when setting replace_l.\n\n\n# test # 2\nprint(f\"Number of outputs of switch_letter('at') is {len(switch_letter('at'))}\")\n\nNumber of outputs of switch_letter('at') is 1\n\n\n\n\nExpected output\nNumber of outputs of switch_letter('at') is 1\n ### Exercise 7\nInstructions for insert_letter(): Now implement a function that takes in a word and returns a list with a letter inserted at every offset.\nStep 1: is the same as in delete_letter()\nStep 2: This can be a list comprehension of the form:\n[f(a,b,c) for a, b in splits if condition for c in string]\n\n# UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# UNIT TEST COMMENT: Candidate for Table Driven Tests\n# GRADED FUNCTION: inserts\ndef insert_letter(word, verbose=False):\n    '''\n    Input:\n        word: the input string/word \n    Output:\n        inserts: a set of all possible strings with one new letter inserted at every offset\n    ''' \n    letters = 'abcdefghijklmnopqrstuvwxyz'\n    insert_l = []\n    split_l = []\n    \n    ### START CODE HERE ###\n    split_l = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n    insert_l = [l+replace+r for l, r in split_l for replace in letters]\n    ### END CODE HERE ###\n\n    if verbose: print(f\"Input word {word} \\nsplit_l = {split_l} \\ninsert_l = {insert_l}\")\n    \n    return insert_l\n\n\ninsert_l = insert_letter('at', True)\nprint(f\"Number of strings output by insert_letter('at') is {len(insert_l)}\")\n\nInput word at \nsplit_l = [('', 'at'), ('a', 't'), ('at', '')] \ninsert_l = ['aat', 'bat', 'cat', 'dat', 'eat', 'fat', 'gat', 'hat', 'iat', 'jat', 'kat', 'lat', 'mat', 'nat', 'oat', 'pat', 'qat', 'rat', 'sat', 'tat', 'uat', 'vat', 'wat', 'xat', 'yat', 'zat', 'aat', 'abt', 'act', 'adt', 'aet', 'aft', 'agt', 'aht', 'ait', 'ajt', 'akt', 'alt', 'amt', 'ant', 'aot', 'apt', 'aqt', 'art', 'ast', 'att', 'aut', 'avt', 'awt', 'axt', 'ayt', 'azt', 'ata', 'atb', 'atc', 'atd', 'ate', 'atf', 'atg', 'ath', 'ati', 'atj', 'atk', 'atl', 'atm', 'atn', 'ato', 'atp', 'atq', 'atr', 'ats', 'att', 'atu', 'atv', 'atw', 'atx', 'aty', 'atz']\nNumber of strings output by insert_letter('at') is 78\n\n\n\n\nExpected output\nInput word at \nsplit_l = [('', 'at'), ('a', 't'), ('at', '')] \ninsert_l = ['aat', 'bat', 'cat', 'dat', 'eat', 'fat', 'gat', 'hat', 'iat', 'jat', 'kat', 'lat', 'mat', 'nat', 'oat', 'pat', 'qat', 'rat', 'sat', 'tat', 'uat', 'vat', 'wat', 'xat', 'yat', 'zat', 'aat', 'abt', 'act', 'adt', 'aet', 'aft', 'agt', 'aht', 'ait', 'ajt', 'akt', 'alt', 'amt', 'ant', 'aot', 'apt', 'aqt', 'art', 'ast', 'att', 'aut', 'avt', 'awt', 'axt', 'ayt', 'azt', 'ata', 'atb', 'atc', 'atd', 'ate', 'atf', 'atg', 'ath', 'ati', 'atj', 'atk', 'atl', 'atm', 'atn', 'ato', 'atp', 'atq', 'atr', 'ats', 'att', 'atu', 'atv', 'atw', 'atx', 'aty', 'atz']\nNumber of strings output by insert_letter('at') is 78\n\n\nNote 1\nIf you get a split_l like this:\nInput word at \nsplit_l = [('', 'at'), ('a', 't')] \ninsert_l = ['aat', 'bat', 'cat', 'dat', 'eat', 'fat', 'gat', 'hat', 'iat', 'jat', 'kat', 'lat', 'mat', 'nat', 'oat', 'pat', 'qat', 'rat', 'sat', 'tat', 'uat', 'vat', 'wat', 'xat', 'yat', 'zat', 'aat', 'abt', 'act', 'adt', 'aet', 'aft', 'agt', 'aht', 'ait', 'ajt', 'akt', 'alt', 'amt', 'ant', 'aot', 'apt', 'aqt', 'art', 'ast', 'att', 'aut', 'avt', 'awt', 'axt', 'ayt', 'azt']\nNumber of strings output by insert_letter('at') is 52\n\nNotice that split_l is missing the extra tuple (‚Äòat‚Äô, ‚Äô‚Äô). For insertion, we actually WANT this tuple.\nThe function is not creating all the desired output strings.\nCheck the range that you use for the for loop.\n\n\n\nNote 2\nIf you see this:\nInput word at \nsplit_l = [('', 'at'), ('a', 't'), ('at', '')] \ninsert_l = ['aat', 'bat', 'cat', 'dat', 'eat', 'fat', 'gat', 'hat', 'iat', 'jat', 'kat', 'lat', 'mat', 'nat', 'oat', 'pat', 'qat', 'rat', 'sat', 'tat', 'uat', 'vat', 'wat', 'xat', 'yat', 'zat', 'aat', 'abt', 'act', 'adt', 'aet', 'aft', 'agt', 'aht', 'ait', 'ajt', 'akt', 'alt', 'amt', 'ant', 'aot', 'apt', 'aqt', 'art', 'ast', 'att', 'aut', 'avt', 'awt', 'axt', 'ayt', 'azt']\nNumber of strings output by insert_letter('at') is 52\n\nEven though you may have fixed the split_l so that it contains the tuple ('at', ''), notice that you‚Äôre still missing some output strings.\n\nNotice that it‚Äôs missing strings such as ‚Äòata‚Äô, ‚Äòatb‚Äô, ‚Äòatc‚Äô all the way to ‚Äòatz‚Äô.\n\nTo fix this, make sure that when you set insert_l, you allow the use of the empty string ''.\n\n\n# test # 2\nprint(f\"Number of outputs of insert_letter('at') is {len(insert_letter('at'))}\")\n\nNumber of outputs of insert_letter('at') is 78\n\n\n\n\nExpected output\nNumber of outputs of insert_letter('at') is 78"
  },
  {
    "objectID": "posts/rnd/index.html",
    "href": "posts/rnd/index.html",
    "title": "NLP Specialization",
    "section": "",
    "text": "So I aced the course and am migrating more of the notes to this site. I have a tendency to add more more notes. But it is probably better to keep most of the extra material separate.\nIn the intervening time there has been massive breakthroughs in NLP. I have also learned a bit about RL. And on the advice of Richard S. Sutton I think to create a research notebook on NLP."
  },
  {
    "objectID": "posts/rnd/index.html#orthogonal-action-items",
    "href": "posts/rnd/index.html#orthogonal-action-items",
    "title": "NLP Specialization",
    "section": "Orthogonal Action Items",
    "text": "Orthogonal Action Items\n\nLearning\n\nFlash cards to keep everything fresh.\nPodcasts\nCreate a small app to drill though material from all the notes repos using spaced repetition.\n\nFeynman Technique\n\nCreate notes on what I know about NLP. These note already cover 90%+\nMy research questions\nCreate a list of all the areas I want to learn about.\nCreate a list of all the papers I want to read. -\nCreate a list of all the courses I want to take.\nCreate a list of all the notebooks I want to review.\nCreate a list of all the projects I want to do.\n\nNext I want to collect all the paper reviews.\nThere are some additional courses online that can be follow ups to this one\nthere are a number of notebooks from different sources that might be brought here for reference and further work.\nI‚Äôve number of ideas for Wikipedia/Wikidata related projects to try.\nProjects around interlingual word embeddings and discourse atoms.\nCreate pure python libraries for Graph Based models (Viterbi, CYK, Earley, etc.) And better yet versions that can take advantage of GPUs SImd or Spark or Mojo"
  },
  {
    "objectID": "posts/rnd/index.html#sec-feynman",
    "href": "posts/rnd/index.html#sec-feynman",
    "title": "NLP Specialization",
    "section": "Feynman Technique",
    "text": "Feynman Technique\n\nNotes on stuff I know.\n\nThe stuff I know on search, tokenisers, baysian herircial model should be added\nI should have some notes on Juffansky\nI should make quick notes + podcasts on my favorite linguistics books including popular ones.\n\nJuffansky\nDavid Goldberg\nGuy Deutscher ??\n\nThe Unfolding of Language: An Evolutionary Tour of Mankind‚Äôs Greatest Invention\n\nReview\nPodcast\n\nThrough the Language Glass: Why the World Looks Different in Other Languages\n\nReview\nPodcast\n\n\nOrnan‚Äôs Morphology etc.\n\nReview\nPodcast\n\nSteven Pinker - Rubs me the wrond way but is a eloquent interloqutor\n\nThe language instinct.\netc\n\nJames W. Pennebaker\n\nSecret life of Pronouns - James W. Pennebaker"
  },
  {
    "objectID": "posts/rnd/index.html#sec-rq",
    "href": "posts/rnd/index.html#sec-rq",
    "title": "NLP Specialization",
    "section": "Research Questions",
    "text": "Research Questions\n\nThere are now nice tutorials by Andrej Karpathy on creating GPT clones.\n\nIt would be interesting to some of the material in his series: at Neural Networks: Zero to Hero\nThe tokenizers in a few notebooks c.f. video\n\nthis course is done now but I should like to get better and do follow up work and research. The best way now seems to create an online research notebook and the best way forward is to migrate the relevant material to thier own pages.\nReformer, the efficent transformer that was covered in the course is not used in modern LLM implementations, it would be interesting to\n\nunderstand its shortcomming based on progress made in later papers.\nunderstand if one can grab the weights of some LLM, load them into a reformer and enjoy the benefits of million token context windows."
  },
  {
    "objectID": "posts/review/papers/2025-02-03-xLSTM/index.html",
    "href": "posts/review/papers/2025-02-03-xLSTM/index.html",
    "title": "NLP Specialization",
    "section": "",
    "text": "Literature Review\n\n\n\n\n\n\n\nVideo¬†1: Review by AI Bites\n\n\n\n\n\n\n\n\nVideo¬†2: Review by Yannic Kilcher"
  },
  {
    "objectID": "posts/review/papers/2025-02-03-xLSTM/index.html#introduction",
    "href": "posts/review/papers/2025-02-03-xLSTM/index.html#introduction",
    "title": "NLP Specialization",
    "section": "Introduction",
    "text": "Introduction\n\n\n\n\n\n\nMotivation\n\n\n\nSo this is my first review of the RNN papers. I found that since the courses covering LSTM seemed to be over simplified and confusing, I should go back to the source. Since time is limited, it helps to review the papers in reverse chronological order. Each new papers explains the limitations that are generally not apparent when the paper is first published. The authors of the later papers also have the benefit of hindsight which they share in the obligatory literature review wherein they provide lots of context we cannot get by reading the original papers. Given all that there is also a benefit to reading the original paper - the real innovators while harder to understand are remarkable thinkers and often reveal motivations and deep insights that elude later interlocutors.\n\n\n\nPodcast & Other Reviews\nThis paper is a bit heavy on the technical side. But if we are still here, we may enjoy this podcast reviewing the paper. Though AI generated I found it quite useful to get oriented with the paper and understand that the experiments the authors run to validate this new architectures.\nWe also cover something I found absent from the Course on sequence models . The limitations of LSTMs as viewed by the authors when considered in the context of transformers and Mamba.\n\n\n\nAlthough this paper is recent there are a number of other people who cover it.\n\nIn Video¬†2 Yannic Kilcher covers the paper in his youtube channel. He does a great job of explaining the paper in a way that is easy to understand. Kilcher is all too human, he makes many mistakes, stresses opinions over facts. If we don‚Äôt mind all that he is a great resource for understanding the paper. I find he rubs me the wrong way but at least I find this motivating to do better in my own reviews!\n\n\n\n\n\n\n\nWhat can we expect from xLSTM?\n\n\n\n\n\nxLSTM has the potential to considerably impact other fields like Reinforcement Learning, Time Series Prediction, or the modeling of physical systems. ‚Äì (Beck et al. 2024)\n\nIn his book Understanding Media (McLuhan 1988) Marshal McLuhan introduced his Tetrad. The Tetrad is a mental model for understanding how a technological innovation like the xLSTM might disrupt society. To work the model we ask the following four questions that can be applied to any new technology to understand its impact. The four questions in this instance are:\n\nWhat does the xLSTM enhance or amplify?\n\n\nThe xLSTM enhances the LSTM by introducing exponential gating and novel memory structures, boosting its capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models.\n\n\nWhat does the xLSTM make obsolete or replace?\n\n\nThe xLSTM makes the traditional LSTM obsolete by addressing its limitations, such as the inability to revise storage decisions, limited storage capacities, and lack of parallelizability.\n\n\nWhat does the xLSTM retrieve that was previously obsolesced?\n\n\nThe xLSTM retrieves the LSTM‚Äôs relevance in large-scale language modeling by scaling it to billions of parameters and integrating modern LLM techniques while mitigating known limitations. 4. What does the xLSTM reverse or flip into when pushed to its limits?\n\n\nThe xLSTM, when pushed to its limits, can compete with current Large Language Models based on Transformer technology, offering a competitive alternative for sequence modeling tasks.\n\nThe xLSTM paper by (Beck et al. 2024) is a great example of the tetrad in action. The authors take the LSTM, a technology that has stood the test of time, and enhance it with exponential gating and novel memory structures. They address the limitations of traditional LSTMs and push the technology to new limits, creating a new architecture that can compete with state-of-the-art Transformers and State Space Models. Recent work by (Chen et al. 2024) suggest that Transformers and The State Space Models are actually limited in their own ways.\n\n\n\n\n\n\n\n\nTetrad\n\n\n\n\nSupplementary Figure¬†1"
  },
  {
    "objectID": "posts/review/papers/2025-02-03-xLSTM/index.html#abstract",
    "href": "posts/review/papers/2025-02-03-xLSTM/index.html#abstract",
    "title": "NLP Specialization",
    "section": "Abstract",
    "text": "Abstract\n\nIn the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining:\n\nsLSTM with a scalar memory, a scalar update, and new memory mixing,\n\nmLSTM that is fully parallelizable with a matrix memory and a covariance update rule.\n\n\nIntegrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling. ‚Äî (Beck et al. 2024)\n\n\n\n\n\n\n\n\narchitecture\n\n\n\n\nFigure¬†1: The extended LSTM (xLSTM) family. From left to right:\n1. The original LSTM memory cell with constant error carousel and gating.\n2. New sLSTM and mLSTM memory cells that introduce exponential gating. sLSTM offers a new memory mixing technique. mLSTM is fully parallelizable with a novel matrix memory cell state and new covariance update rule.\n3. mLSTM and sLSTM in residual blocks yield xLSTM blocks.\n4. Stacked xLSTM blocks give an xLSTM\n\n\n\n\n\n\n\n\nblocks\n\n\n\n\nFigure¬†2: LSTM limitations.\n- Left: Nearest Neighbor Search problem in terms of mean squared error (MSE). Given a reference vector, a sequence is scanned sequentially for the most similar vector with the objective to return its attached value at sequence end. LSTM struggles to revise a stored value when a more similar vector is found. Our new xLSTM overcomes this limitation by exponential gating. - Right: Rare Token Prediction. The perplexity (PPL) of token prediction on Wikitext-103, in partitions of token frequency. LSTM performs worse on predicting rare tokens because of its limited storage capacities, whereas our new xLSTM solves this problem via a matrix memory.\n\n\n\n\n\n\n\n\nblocks\n\n\n\n\nFigure¬†3: xLSTM blocks.\nLeft: A residual sLSTM block with post up-projection (like Transformers): The input is fed into an sLSTM ‚Äî with an optional convolution ‚Äî followed by a gated MLP.\nRight: A residual mLSTM block with pre up-projection (like State Space models): mLSTM is wrapped inside two MLPs, via a convolution, a learnable skip connection, and an output gate that acts componentwise.\n\n\n\n\n\n\n\n\nblocks\n\n\n\n\nFigure¬†4: xLSTM blocks.\nLeft: A residual sLSTM block with post up-projection (like Transformers): The input is fed into an sLSTM ‚Äî with an optional convolution ‚Äî followed by a gated MLP.\nRight: A residual mLSTM block with pre up-projection (like State Space models): mLSTM is wrapped inside two MLPs, via a convolution, a learnable skip connection, and an output gate that acts componentwise.\n\n\n\n\n\n\n\n\nblocks\n\n\n\n\nFigure¬†5: xLSTM blocks.\nLeft: A residual sLSTM block with post up-projection (like Transformers): The input is fed into an sLSTM ‚Äî with an optional convolution ‚Äî followed by a gated MLP.\nRight: A residual mLSTM block with pre up-projection (like State Space models): mLSTM is wrapped inside two MLPs, via a convolution, a learnable skip connection, and an output gate that acts componentwise."
  },
  {
    "objectID": "posts/review/papers/2025-02-03-xLSTM/index.html#sec-outline",
    "href": "posts/review/papers/2025-02-03-xLSTM/index.html#sec-outline",
    "title": "NLP Specialization",
    "section": "Paper Outline",
    "text": "Paper Outline\n\nIntroduction\n\nDescribes the constant error carousel and gating mechanisms of Long Short-Term Memory (LSTM). &gt; ‚ÄúIn the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories‚Ä¶‚Äù\nDiscusses three main limitations of LSTMs:\n\nThe inability to revise storage decisions.\nLimited storage capacities.\nLack of parallelizability due to memory mixing.\n\nHighlights the emergence of Transformers in language modeling due to these limitations. &gt; Transformer Era: The rise of Transformers with parallelizable self-attention has largely overshadowed LSTMs due to their superior scalability.\n\nExtended Long Short-Term Memory\n\nIntroduces Extended Long Short-Term Memory (xLSTM), which incorporates exponential gating and novel memory structures.\nPresents two new LSTM variants:\n\nsLSTM with a scalar memory, a scalar update, and new memory mixing.\nmLSTM with a matrix memory and a covariance update rule, which is fully parallelizable.\n\nDescribes the integration of these LSTM variants into residual block modules, resulting in xLSTM blocks.\nPresents xLSTM architectures that residually stack xLSTM blocks.\n\nRelated Work:\n\nMentions various linear attention methods to overcome the quadratic complexity of Transformer attention.\nNotes the popularity of State Space Models (SSMs) for language modeling.\nHighlights the use of Recurrent Neural Networks (RNNs) as a replacement for Transformers.\nMentions the use of gating in recent RNN and SSM approaches.\nNotes the use of covariance update rules1 to enhance storage capacities in various models.\n\nExperiments\n\nPresents the experimental evaluation of xLSTM compared to existing methods with a focus on language modeling.\nDiscusses the effectiveness of xLSTM on synthetic tasks, including:\n\nFormal languages.\nMulti-Query Associative Recall.\nLong Range Arena tasks.\n\nPresents a method comparison and ablation study, where different models are trained on 15 billion tokens from the SlimPajama dataset.\nAssesses the scaling behavior of different methods based on validation perplexity.\nConducts a large-scale language modeling experiment:\n\nTraining different model sizes on 300B tokens from SlimPajama.\nEvaluating models on length extrapolation.\nAssessing models on validation perplexity and performance on downstream tasks.\nEvaluating models on 571 text domains of the PALOMA language benchmark dataset.\nAssessing the scaling behavior with increased training data.\n\n\nLimitations\n\nHighlights limitations of xLSTM, including:\n\nLack of parallelizability for sLSTM due to memory mixing.\nUnoptimized CUDA kernels for mLSTM.\nHigh computational complexity of mLSTM‚Äôs matrix memory.\nMemory overload for longer contexts due to the independence of mLSTM‚Äôs memory from sequence length.\n\n\nConclusion\n\nConcludes that xLSTM performs favorably in language modeling compared to Transformers and State Space Models.\nSuggests that larger xLSTM models will be competitive with current Large Language Models based on Transformer technology.\nNotes the potential impact of xLSTM on other deep learning fields such as reinforcement learning, time series prediction, and physical system modeling.\n\n\n1¬†explain covariance"
  },
  {
    "objectID": "posts/review/papers/2025-02-03-xLSTM/index.html#briefing-xlstm---extended-long-short-term-memory",
    "href": "posts/review/papers/2025-02-03-xLSTM/index.html#briefing-xlstm---extended-long-short-term-memory",
    "title": "NLP Specialization",
    "section": "Briefing : xLSTM - Extended Long Short-Term Memory",
    "text": "Briefing : xLSTM - Extended Long Short-Term Memory\n\nIntroduction and Motivation:\nLSTM‚Äôs Legacy: The Long Short-Term Memory (LSTM), introduced in the 1990s, has been a cornerstone of deep learning, especially in sequence modeling and early Large Language Models (LLMs). LSTMs are known for their ‚Äúconstant error carousel‚Äù and gating mechanisms, designed to combat vanishing gradients in recurrent neural networks.\n\n‚ÄúIn the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories‚Ä¶‚Äù\n\nTransformer Era: The rise of Transformers with parallelizable self-attention has largely overshadowed LSTMs due to their superior scalability.\n\n‚ÄúHowever, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale.‚Äù xLSTM Question: This research aims to explore the capabilities of LSTMs when scaled to billions of parameters, by integrating modern LLM techniques while addressing known limitations.\n\n\n‚ÄúWe now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs?‚Äù\n\n\n\nKey Limitations of Traditional LSTMs:\nThe paper identifies three major limitations of traditional LSTMs:\n\nInability to Revise Storage Decisions: LSTMs struggle to update previously stored information when more relevant information arrives later in the sequence. The paper uses a ‚ÄúNearest Neighbor Search‚Äù task to demonstrate this.\n\n\n‚ÄúLSTM struggles to revise a stored value when a more similar vector is found‚Ä¶‚Äù\n\n\nLimited Storage Capacity: LSTMs store information in scalar cell states, which can lead to information compression and loss. This impacts performance on tasks requiring detailed memory, like predicting rare tokens.\n\n\n‚ÄúLSTM performs worse on predicting rare tokens because of its limited storage capacities‚Ä¶‚Äù\n\n\nLack of Parallelizability: LSTMs process data sequentially due to the recurrent connections between hidden states, hindering parallel computation.\n\n\n‚ÄúLack of parallelizability due to memory mixing, i.e., the hidden-hidden connections between hidden states from one time step to the next, which enforce sequential processing.‚Äù\n\n\nxLSTM Innovations:\n\nThe paper introduces Extended Long Short-Term Memory (xLSTM), designed to address the above limitations, by modifying the original LSTM using two main approaches:\nExponential Gating:Implemented to allow for better revision of storage decisions. Normalization and stabilization techniques are used to avoid overflows in calculations due to the exponential function.\n\n‚ÄúTo empower LSTMs with the ability to revise storage decisions, we introduce exponential gates (red) together with normalization and stabilization.‚Äù Novel Memory Structures: Two new LSTM variants are created with these structures:\n\nsLSTM: A scalar memory LSTM with a scalar update and new memory mixing technique (within heads, not across). Features multiple memory cells and multiple heads, with memory mixing within each head.\n\n‚ÄúThe new sLSTM (see Section 2.2) with a scalar memory, a scalar update, and memory mixing‚Ä¶‚Äù\n\nmLSTM: A matrix memory LSTM, fully parallelizable, with a covariance update rule for storing key-value pairs. Does not have memory mixing between cells or heads.\n\n‚Äúthe new mLSTM (see Section 2.3) with a matrix memory and a covariance (outer product) update rule, which is fully parallelizable.‚Äù\n\n\nsLSTM Details:\n\n\nExponential Gates: Introduces exponential activation functions for input and forget gates with normalization and a stabilizer state to prevent overflow.\nNormalizer State: A state used to normalize the hidden state output by accumulating a weighted sum of input and forget gates.\nMemory Mixing: sLSTM allows memory mixing between cells, and also introduces a new form of memory mixing through multiple heads, mixing between cells within a head, but not across heads.\n\n\n‚ÄúThe new sLSTM can have multiple heads with memory mixing within each head but not across heads. The introduction of heads for sLSTM together with exponential gating establishes a new way of memory mixing.‚Äù\n\n\nmLSTM Details:\n\nMatrix Memory: Replaces the scalar cell state with a matrix, increasing storage capacity.\nKey-Value Storage: The mLSTM memory is based around a key-value storage system inspired by Bidirectional Associative Memories (BAMs) using covariance updates, where keys and values are updated via an outer product.\n\n‚ÄúAt time t, we want to store a pair of vectors, the key k_t ‚àà R^d and the value v_t ‚àà R^d‚Ä¶ The covariance update rule‚Ä¶ for storing a key-value pair‚Ä¶‚Äù\n\nParallelization: mLSTM is fully parallelizable due to abandoning hidden-hidden recurrent connections and memory mixing. Computations can be performed efficiently on GPUs using standard matrix operations.\n\n‚Äú‚Ä¶the mLSTM‚Ä¶ which is fully parallelizable.‚Äù\n\n\nxLSTM Architecture:\n\nxLSTM Blocks: The sLSTM and mLSTM variants are integrated into residual blocks.\nsLSTM blocks use post up-projection (like Transformers).\nmLSTM blocks use pre up-projection (like State Space Models).\n\n‚ÄúIntegrating these new LSTM variants into residual block modules results in xLSTM blocks‚Ä¶‚Äù\n\nResidual Stacking: xLSTM architectures are created by residually stacking these blocks, using pre-LayerNorm backbones similar to current LLMs.\n\n‚ÄúAn xLSTM architecture is constructed by residually stacking build-ing blocks‚Ä¶‚Äù Cover‚Äôs Theorem: Used to justify mapping information into a higher-dimensional space for better separation of histories or contexts.\n\n\n‚ÄúWe resort to Cover‚Äôs Theorem (Cover, 1965), which states that in a higher dimensional space non-linearly embedded patterns can more likely be linearly separated than in the original space.‚Äù\n\n\nPerformance and Scaling:\n\nLinear Computation & Constant Memory: Unlike Transformers, xLSTM networks have linear computation and constant memory complexity with respect to sequence length.\n\n‚ÄúContrary to Transformers, xLSTM networks have a linear computation and a constant memory complexity with respect to the sequence length.‚Äù\n\nSynthetic Tasks: xLSTM showed improved performance over regular LSTMs on ‚ÄúNearest Neighbor Search‚Äù and ‚ÄúRare Token Prediction‚Äù synthetic tasks.\nSlimPajama Experiments: The xLSTM models were trained on 15B and 300B tokens from the SlimPajama dataset for language modelling evaluation.\nCompetitive Performance: xLSTMs demonstrate competitive performance against state-of-the-art Transformers and State Space Models on language modelling benchmarks, both in terms of next token prediction perplexity and downstream tasks.\n\n‚ÄúExponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.‚Äù\n\nAblation studies show importance of gating techniques.\n\nMemory & Speed:\n\nsLSTM: Not parallelizable due to memory mixing, although the authors have created a fast CUDA implementation (less than twice as slow as mLSTM).\n\n‚ÄúWhile mLSTM is parallelizable analog to FlashAttention‚Ä¶ sLSTM is not parallelizable due to the memory mixing‚Ä¶‚Äù\n\nmLSTM: Parallelizable, but currently slower than FlashAttention or Mamba due to non-optimized CUDA kernels. Matrix memory is computationally complex but does not require parameters and can be parallelized.\n\n‚ÄúThe memory of mLSTM does not require parameters but is computationally expensive through its d√ód matrix memory and d√ó d update‚Ä¶ the computations can be done in parallel on GPUs, therefore these computations have only a minor effect on the wall clock time.‚Äù\n\n\nLimitations:\n\nsLSTM Parallelization: sLSTM‚Äôs memory mixing is non-parallelizable.\n\n‚Äú(i) In contrast to mLSTM, memory mixing of the sLSTM prohibits parallelizable operations, and therefore does not allow a fast parallel implementation.‚Äù\n\nmLSTM CUDA Optimization: The mLSTM CUDA kernels are not fully optimized.\n\n‚Äú(ii) The CUDA kernels for mLSTM are not optimized, and therefore the current implementation is about 4 times slower than FlashAttention or the scan used in Mamba.‚Äù\n\nmLSTM Matrix Memory: High computational complexity for mLSTM due to matrix memory operations.\nForget Gate Initialization: Careful initialization of the forget gates is needed.\nLong Context Memory: The matrix memory is independent of sequence length, and might overload memory for long context sizes.\n\n‚Äú(v) Since the matrix memory is independent of the sequence length, increasing the sequence length might overload the memory for longer context sizes.‚Äù\n\nHyperparameter Optimization: xLSTM is not yet fully optimized due to computational load, indicating further performance gains are possible.\n\n‚Äú(vi) Due to the expensive computational load for large language experiments, we did neither fully optimize the architecture nor the hyperparameters, especially for larger xLSTM architectures.‚Äù\n\n\nRelated Work:\n\nThe paper highlights connections of its ideas with the following areas:\nGating: Many recent models such as HGRN, GLA, Gated State Space models, Mamba, and others also utilize gating mechanisms. Covariance Update Rule: The covariance update rule used in mLSTM is related to other methods such as Fast Weight Programmers, Retention, and Linear Transformers.\n\nConclusion:\n\nThe xLSTM presents a compelling alternative to Transformers in sequence modeling, especially in contexts where memory efficiency and longer sequences are required. By combining insights from the original LSTM with exponential gating and novel memory structures, the paper shows that LSTMs can be competitive at scale. While limitations exist in terms of parallelization and hyperparameter tuning, the research shows promising potential for further development. The authors posit further research and improvements could allow xLSTM to reach its full potential."
  },
  {
    "objectID": "posts/review/papers/2025-02-03-xLSTM/index.html#my-thoughts",
    "href": "posts/review/papers/2025-02-03-xLSTM/index.html#my-thoughts",
    "title": "NLP Specialization",
    "section": "My Thoughts",
    "text": "My Thoughts\n\n\n\n\n\n\n\nVideo¬†3: Review of the sigmoid function\n\n\n\n\n\n\n\n\nResearch questions\n\n\n\n\nHow does the constant error carousel mitigate the vanishing gradient problem in the LSTM?\n\nThe constant error carousel is a way to keep the error constant over time. This helps to prevent the vanishing gradient problem in the LSTM.\n\nHow are the gates in the original LSTM binary?\n\nSigmoid, saturation and a threshold at 0.5\n\nWhat is the long term memory in the LSTM?\n\nthe cell state c_{t-1}\n\nWhat is the short term memory in the LSTM?\n\nthe hidden state h_{t-1}\n\nHow far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs\n\n\n\n\n\n\n\n\n\n\nSupplementary Figure¬†2: limits of the sigmoid function\n\n\n\n\n\n\n\n\nbias\n\n\n\n\nSupplementary Figure¬†3: The inductive bias of the sigmoid decision function.\n\n\n\n\nBinary nature of non exponential Gating Mechanisms\nIf we try to understand why are the gating mechanisms in the original LSTM is described here as binary?\nIt helps to the properties of the sigmoid functions that are explained in Video¬†3.\n\nSupplementary Figure¬†1 shows that The sigmoid function has a domain of \\mathbb{R} and a range of (0,1). This means that the sigmoid function can only output values between 0 and 1.\nIt has a very sharp gradient around 0 if the bias is set to 0. This means that the sigmoid function acts as a binary function around 0.\nThe Sigmoid function tends to saturate. Now sigmoid function around 0 isn‚Äôt quite quite binary but outside of (-6,+6) it becomes saturated and acts very much as a binary function.\nIf we use it as a loss function or a decision function, the sigmoid function is used with a threshold of 0.5. This means that if the output of the sigmoid function is greater than 0.5, the decision is 1, and if it is less than 0.5, the decision is 0. That is how the sigmoid function can become binary.\nNote that we can use thresholding during inference to make the decision binary and we can keep them as continuous values during training so we can have a smooth gradient for back-propagation.\nEven without thresholding the sigmoid function can become binary this is due to the saturation of the sigmoid function. I recall that Hinton in his Coursera course on Neural Networks and Deep Learning mentioned that activation function that become saturated are problematic. I can say that at last I think I understand what he meant:\nI see that that sigmoid functions can fall off the manifold of the data and this can be a problem. This is why the ReLU function is used in the Gated Recurrent Unit (GRU) and the Transformer. Falling off the manifold of the data means that the sigmoid function becomes saturated (above 6 and below -6). At this point the back-propagation steps are no longer able to change the weight or bias since the gradient is effectively so learning stops.\n\nThis becomes a is a problem in two senses. The sigmoid unit is likely to be stuck in a setting that only representative of part of the training data. The rest of the network which is not stuck now has to learn to compensate for this.\nThis issues may well be the root cause LSTM limitation number 1 - the inability to revise storage decisions. It seems that the authors decided to use the exponential gating function to address this issues.\nThis is why the gating mechanisms in the original LSTM are binary. The sigmoid function is used to control the flow of information in the LSTM by deciding which information to keep and which to discard. The sigmoid function is used to decide how much of the new input information should be added to the cell state and how much of the old cell state should be retained. This binary nature of the sigmoid function is what allows the LSTM to control the flow of information and decide what to remember and what to forget."
  },
  {
    "objectID": "posts/review/papers/2025-02-03-xLSTM/index.html#the-paper",
    "href": "posts/review/papers/2025-02-03-xLSTM/index.html#the-paper",
    "title": "NLP Specialization",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper"
  },
  {
    "objectID": "posts/review/papers/2025-02-03-xLSTM/index.html#references",
    "href": "posts/review/papers/2025-02-03-xLSTM/index.html#references",
    "title": "NLP Specialization",
    "section": "References",
    "text": "References\n\nThe paper on open review has some additional insights from the authors\nXLSTM ‚Äî Extended Long Short-Term Memory Networks By Shrinivasan Sankar ‚Äî May 20, 2024"
  },
  {
    "objectID": "posts/review/papers/transforer-to-rnn/index.html",
    "href": "posts/review/papers/transforer-to-rnn/index.html",
    "title": "NLP Specialization",
    "section": "",
    "text": "Litrature review\n\n\n\n\n\n\n\nVideo¬†1: Transformer to RNN (T2RNN) Part-1 by Dr.¬†Niraj Kumar\n\n\n\n\n\n\n\n\nVideo¬†2: Transformer to RNN (T2RNN) Part-2 by Dr.¬†Niraj Kumar"
  },
  {
    "objectID": "posts/review/papers/transforer-to-rnn/index.html#abstract",
    "href": "posts/review/papers/transforer-to-rnn/index.html#abstract",
    "title": "NLP Specialization",
    "section": "Abstract",
    "text": "Abstract\n\nTransformers have outperformed recurrent neural networks (RNNs) in natural language generation. But this comes with a significant computational cost, as the attention mechanism‚Äôs complexity scales quadratically with sequence length. Efficient transformer variants have received increasing interest in recent works. Among them, a linear-complexity recurrent variant has proven well suited for autoregressive generation. It approximates the softmax attention with randomized or heuristic feature maps, but can be difficult to train and may yield suboptimal accuracy. This work aims to convert a pretrained transformer into its efficient recurrent counterpart, improving efficiency while maintaining accuracy. Specifically, we propose a swap-then-finetune procedure: in an off-the-shelf pretrained transformer, we replace the softmax attention with its linear-complexity recurrent alternative and then finetune. With a learned feature map, our approach provides an improved tradeoff between efficiency and accuracy over the standard transformer and other recurrent variants. We also show that the finetuning process has lower training cost relative to training these recurrent variants from scratch. As many models for natural language tasks are increasingly dependent on large-scale pretrained transformers, this work presents a viable approach to improving inference efficiency without repeating the expensive pretraining process."
  },
  {
    "objectID": "posts/review/papers/transforer-to-rnn/index.html#the-paper",
    "href": "posts/review/papers/transforer-to-rnn/index.html#the-paper",
    "title": "NLP Specialization",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog\n\n\nCitationBibTeX citation:@online{bochman2025,\n  author = {Bochman, Oren},\n  title = {About},\n  date = {2025-01-29},\n  url = {https://orenbochman.github.io/notes-nlp/about.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2025. ‚ÄúAbout.‚Äù January 29, 2025. https://orenbochman.github.io/notes-nlp/about.html."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NLP course notes and research notebooks",
    "section": "",
    "text": "Assignment 2: Deep N-grams\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nThursday, February 6, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nVanilla RNNs, GRUs and the scan function\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nThursday, February 6, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 1: Auto Correct\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nLogistic regression\n\n\nSentiment analysis task\n\n\nClassification & Vector Spaces\n\n\n\n\n\n\n\n\n\nWednesday, February 5, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nCandidates from String Edits\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nLogistic regression\n\n\nSentiment analysis task\n\n\nClassification & Vector Spaces\n\n\n\n\n\n\n\n\n\nWednesday, February 5, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding the vocabulary\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nLogistic regression\n\n\nSentiment analysis task\n\n\nClassification & Vector Spaces\n\n\n\n\n\n\n\n\n\nWednesday, February 5, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 4: Question duplicates\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nWednesday, February 5, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 1: Sentiment with Deep Neural Networks\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nWednesday, February 5, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 4: Word Embeddings\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nProbabilistic Models\n\n\n\n\n\n\n\n\n\nWednesday, February 5, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 2: Naive Bayes\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nLogistic regression\n\n\nSentiment analysis task\n\n\nClassification & Vector Spaces\n\n\n\n\n\n\n\n\n\nWednesday, February 5, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nCourse Overview\n\n\nFoundations and Core Concepts of PyTorch\n\n\n\n\n\n\n\n\nWednesday, February 5, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 3: Hello Vectors\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nLogistic regression\n\n\nSentiment analysis task\n\n\nClassification & Vector Spaces\n\n\n\n\n\n\n\n\n\nWednesday, February 5, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nLinear algebra in Python with NumPy\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nLogistic regression\n\n\nSentiment analysis task\n\n\nClassification & Vector Spaces\n\n\n\n\n\n\n\n\n\nWednesday, February 5, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nManipulating word embeddings\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nLogistic regression\n\n\nSentiment analysis task\n\n\nClassification & Vector Spaces\n\n\n\n\n\n\n\n\n\nWednesday, February 5, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 1: Logistic Regression\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nLogistic regression\n\n\nSentiment analysis task\n\n\nClassification & Vector Spaces\n\n\n\n\n\n\n\n\n\nWednesday, February 5, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing Naive Bayes\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nLogistic regression\n\n\nSentiment analysis task\n\n\nClassification & Vector Spaces\n\n\n\n\n\n\n\n\n\nWednesday, February 5, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nAbstract\n\n\n\n\n\n\n\n\n\n\n\nWednesday, February 5, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nAbstract\n\n\n\n\n\n\n\n\n\n\n\nMonday, February 3, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nTL;DR ‚Äì Scaling LSTMs to Billions of Parameters with xLSTM\n\n\n\n\n\n\n\n\n\n\n\nMonday, February 3, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nOrthogonal Action Items\n\n\n\n\n\n\n\n\n\n\n\nThursday, January 30, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nTL;DR ‚Äì ML, NLP and the secret life of pronouns\n\n\n\n\n\n\n\n\n\n\n\nThursday, January 30, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nChat Bots\n\n\nNLP with Attention Models\n\n\n\nAttention\n\n\nChat bot development\n\n\nCoursera\n\n\nIntelligent agents\n\n\nLocality sensitive hashing\n\n\nNLP with Attention Models\n\n\nNeural Machine Translation\n\n\nNLP\n\n\nNotes\n\n\nPositional encoding\n\n\nQuestion answering task\n\n\nReversible layers\n\n\nTeacher forcing\n\n\nTransformer\n\n\n\nThis week of the NLP Specialization, we explore Chatbot. We will be building Reformer model, an efficient transformer to create intelligent conversational agents.We will learn how to train this model on dialogue datasets and generate responses that mimic human-like conversations. Through hands-on exercises using JAX, we will gain practical experience in building chatbot that can understand and respond to user queries. We will master the skills to develop sophisticated chatbot applications using state-of-the-art NLP techniques\n\n\n\n\n\nTuesday, April 27, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion Answering\n\n\nNLP with Attention Models\n\n\n\nAttention\n\n\nCoursera\n\n\nDeep Learning Algorithms\n\n\nNLP\n\n\nNotes\n\n\nNLP with Attention Models\n\n\nNeural Machine Translation\n\n\nTransformer\n\n\nTeacher forcing\n\n\nPositional encoding\n\n\nQuestion answering task\n\n\n\nThis week we will dive into Neural Question Answering. We will build advanced models like T5 and BERT to accurately answer questions based on given contexts. We will fine-tune these models to optimize their performance. We will gain practical experience in building question-answering systems.\n\n\n\n\n\nWednesday, April 14, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 2 - Text Summarization\n\n\nNLP with Attention Models\n\n\n\nNLP with Attention Models\n\n\nNeural Machine Translation\n\n\nCoursera\n\n\nNotes\n\n\nDeep Learning Algorithms\n\n\nTransformer\n\n\nTeacher forcing\n\n\nPositional encoding\n\n\nGPT2\n\n\nTransformer decoder\n\n\nAttention\n\n\nDot product attention\n\n\nSelf attention\n\n\nCausal attention\n\n\nMulti-head attention\n\n\nSummarization task\n\n\n\nThis week we unlock the secrets of Neural Text Summarization. We will building a powerful Transformer model to extract crucial information and create concise summaries. Through hands-on exercises using JAX, we will learn techniques like beam search and length normalization to enhance the quality of our summaries. Through hands-on exercises we will train our model on a dataset of articles.\n\n\n\n\n\nWednesday, March 31, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 1 - Neural Machine Translation\n\n\nNLP with Attention Models\n\n\n\nAttention\n\n\nBeam search\n\n\nBLEU\n\n\nROUGE\n\n\nCoursera\n\n\nNLP with Attention Models\n\n\nNotes\n\n\nMachine translation task\n\n\nMBR\n\n\nNLP\n\n\nPositional encoding\n\n\nSeq2Seq\n\n\nTransformer\n\n\nTeacher forcing\n\n\nTranslation task\n\n\nWord alignment\n\n\n\nThis week we dive deep into the Neural Machine Translation. We‚Äôll learn about the encoder-decoder architecture, explore attention mechanisms that enable the model to focus on different parts of the input sequence during translation. In the hands-on exercises, we‚Äôll implement an attention model for English to German translation, train it on a dataset of sentence pairs, and evaluate its performance.\n\n\n\n\n\nSaturday, March 20, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nWord Embeddings: Hands On\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nProbabilistic Models\n\n\n\n\n\n\n\n\n\nWednesday, October 28, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nWord Embeddings: Ungraded Practice Notebook\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nProbabilistic Models\n\n\n\n\n\n\n\n\n\nTuesday, October 27, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nWord Embeddings: Training the CBOW model\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nProbabilistic Models\n\n\n\n\n\n\n\n\n\nMonday, October 26, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nData generators\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nSunday, October 25, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nClasses and subclasses\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nSunday, October 25, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nOut of vocabulary words (OOV)\n\n\nProbabilistic Models\n\n\n\nNLP\n\n\nCoursera\n\n\nNotes\n\n\nProbabilistic Models\n\n\n\n\n\n\n\n\n\nSunday, October 25, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nWord Embeddings: Intro to CBOW model, activation functions and working with Numpy\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nProbabilistic Models\n\n\n\n\n\n\n\n\n\nSunday, October 25, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nTrax : Ungraded Lecture Notebook\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nSaturday, October 24, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding the language model\n\n\nProbabilistic Models\n\n\n\nNLP\n\n\nCoursera\n\n\nNotes\n\n\nProbabilistic Models\n\n\n\n\n\n\n\n\n\nSaturday, October 24, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluate a Siamese model: Ungraded Lecture Notebook\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nSaturday, October 24, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nCreating a Siamese model using Trax: Ungraded Lecture Notebook\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nSaturday, October 24, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nModified Triplet Loss : Ungraded Lecture Notebook\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nSaturday, October 24, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nCreating a GRU model using Trax: Ungraded Lecture Notebook\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nSaturday, October 24, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nHidden State Activation : Ungraded Lecture Notebook\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nSaturday, October 24, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with JAX numpy and calculating perplexity: Ungraded Lecture Notebook\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nSaturday, October 24, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nWord Embeddings First Steps: Data Preparation\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nProbabilistic Models\n\n\n\n\n\n\n\n\n\nSaturday, October 24, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nAutocorrect and minimum edit distance\n\n\nProbabilistic Models\n\n\n\nNLP\n\n\nCoursera\n\n\nNotes\n\n\nProbabilistic Models\n\n\n\nAuto-correct text with minimum edit distances\n\n\n\n\n\nFriday, October 23, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nLSTMs and Named Entity Recognition\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nNotes\n\n\nSequence Models\n\n\n\nwe cover Neural networks for deep learning, then build a tweet classifier that places tweets into positive or negative sentiment categories, using a DNN.\n\n\n\n\n\nFriday, October 23, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 4 - Naive Machine Translation and LSH\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nWord Embeddings\n\n\nTranslation task\n\n\nSearch Task\n\n\n\n\n\n\n\n\n\nFriday, October 23, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nVector manipulation in Python\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nWord Embeddings\n\n\nTranslation task\n\n\nSearch Task\n\n\n\n\n\n\n\n\n\nFriday, October 23, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nHash functions and multiplanes\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nWord Embeddings\n\n\nTranslation task\n\n\nSearch Task\n\n\n\n\n\n\n\n\n\nFriday, October 23, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Translation and Document Search\n\n\nClassification & Vector Spaces\n\n\n\nNLP\n\n\nCoursera\n\n\nnotes\n\n\nWord Embeddings\n\n\nTranslation task\n\n\nSearch Task\n\n\n\nConcepts, code snippets, and slide commentaries for this week‚Äôs lesson of the Course notes from the deeplearning.ai natural language programming specialization.\n\n\n\n\n\nFriday, October 23, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Networks for Sentiment Analysis\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nNotes\n\n\nSequence Models\n\n\nSentiment analysis task\n\n\n\nwe cover Neural networks for deep learning, then build a tweet classifier that places tweets into positive or negative sentiment categories, using a DNN.\n\n\n\n\n\nFriday, October 23, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nLanguage Models: Auto-Complete\n\n\nProbabilistic Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nProbabilistic Models\n\n\n\n\n\n\n\n\n\nFriday, October 23, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nN-grams Corpus preprocessing\n\n\nProbabilistic Models\n\n\n\nNLP\n\n\nCoursera\n\n\nNotes\n\n\nProbabilistic Models\n\n\n\n\n\n\n\n\n\nFriday, October 23, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nAutocomplete and Language Models\n\n\nProbabilistic Models\n\n\n\nNLP\n\n\nCoursera\n\n\nNotes\n\n\nProbabilistic Models\n\n\n\nExtract features from text into numerical vectors, then build a binary classifier for tweets using logistic regression!\n\n\n\n\n\nFriday, October 23, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nSiamese Networks\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nnotes\n\n\nSequence Models\n\n\n\nwe cover Neural networks for deep learning, then build a tweet classifier that places tweets into positive or negative sentiment categories, using a DNN.\n\n\n\n\n\nFriday, October 23, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nRecurrent Neural Networks for Language Modeling\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nNotes\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nFriday, October 23, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nWord embeddings with neural networks\n\n\nProbabilistic Models\n\n\n\nNLP\n\n\nCoursera\n\n\nnotes\n\n\nProbabilistic Models\n\n\n\nExtract features from text into numerical vectors, then build a binary classifier for tweets using logistic regression!\n\n\n\n\n\nFriday, October 23, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nProbability and Bayes Rule\n\n\nClassification & Vector Spaces\n\n\n\nNLP\n\n\nCoursera\n\n\ncode\n\n\nConditional Probability\n\n\nBayes rule\n\n\nNa√Øve Bayes\n\n\nLaplace smoothing\n\n\nLog-likelihood\n\n\nclassification\n\n\nsentiment analysis task\n\n\n\nConcepts, code snippets, and slide commentaries for this week‚Äôs lesson of the Course notes from the deeplearning.ai natural language programming specialization.\n\n\n\n\n\nFriday, October 23, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nProbability and Bayes Rule\n\n\nClassification & Vector Spaces\n\n\n\nNLP\n\n\nCoursera\n\n\nClassification & Vector Spaces\n\n\nNotes\n\n\nNa√Øve Bayes\n\n\nSentiment analysis task\n\n\n\nThe theory behind Bayes‚Äô rule for conditional probabilities, and its application toward building a Naive Bayes tweet classifier\n\n\n\n\n\nFriday, October 23, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nVector Space Models\n\n\nClassification & Vector Spaces\n\n\n\nNLP\n\n\nCoursera\n\n\nNotes\n\n\nClassification & Vector Spaces\n\n\n\nVector space models capture semantic meaning and relationships between words. You‚Äôll learn how to create word vectors that capture dependencies between words, then visualize their relationships in two dimensions using PCA.\n\n\n\n\n\nFriday, October 23, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 2: Parts-of-Speech Tagging (POS)\n\n\nProbabilistic Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nProbabilistic Models\n\n\n\nDevelop skills in part-of-speech (POS) tagging, the process of assigning a part-of-speech tag (Noun, Verb, Adjective‚Ä¶) to each word in an input text.\n\n\n\n\n\nFriday, October 23, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nParts-of-Speech Tagging - First Steps: Working with text files, Creating a Vocabulary and Handling Unknown Words\n\n\nProbabilistic Models\n\n\n\nNLP\n\n\nCoursera\n\n\nlab\n\n\nProbabilistic Models\n\n\n\nExtract features from text into numerical vectors, then build a binary classifier for tweets using logistic regression!\n\n\n\n\n\nFriday, October 23, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nParts-of-Speech Tagging - Working with tags and Numpy\n\n\nProbabilistic Models\n\n\n\nNLP\n\n\nCoursera\n\n\nlab\n\n\nProbabilistic Models\n\n\n\nIn this lab we will create a matrix using some tag information and then modify it using different approaches. This will serve as hands-on experience working with Numpy\n\n\n\n\n\nFriday, October 23, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nPart of Speech Tagging and Hidden Markov Models\n\n\nProbabilistic Models\n\n\n\nNLP\n\n\nCoursera\n\n\nNotes\n\n\nProbabilistic Models\n\n\n\nExtract features from text into numerical vectors, then build a binary classifier for tweets using logistic regression!\n\n\n\n\n\nFriday, October 23, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing tweets and the Logistic Regression model\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nLogistic regression\n\n\nSentiment analysis task\n\n\nClassification & Vector Spaces\n\n\nNLTK\n\n\n\n\n\n\n\n\n\nFriday, October 23, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nPreprocessing\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nLogistic regression\n\n\nSentiment analysis task\n\n\nClassification & Vector Spaces\n\n\nNLTK\n\n\n\n\n\n\n\n\n\nFriday, October 23, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding and Visualizing word frequencies\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nWord frequencies\n\n\nNLTK\n\n\nClassification & Vector Spaces\n\n\nSentiment analysis task\n\n\n\n\n\n\n\n\n\nFriday, October 23, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nSentiment Analysis with Logistic Regression\n\n\nClassification & Vector Spaces\n\n\n\nClassification & Vector Spaces\n\n\nCoursera\n\n\nLogistic regression\n\n\nNLP\n\n\nNotes\n\n\nSentiment analysis task\n\n\n\nExtract features from text into numerical vectors, then build a binary classifier for tweets using logistic regression!\n\n\n\n\n\nFriday, October 23, 2020\n\n\nOren Bochman\n\n\n\n\n\n\nNo matching items\n\n\n  \n\nCitationBibTeX citation:@online{bochman2025,\n  author = {Bochman, Oren},\n  title = {NLP Course Notes and Research Notebooks},\n  date = {2025-01-30},\n  url = {https://orenbochman.github.io/notes-nlp/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2025. ‚ÄúNLP Course Notes and Research Notebooks\n.‚Äù January 30, 2025. https://orenbochman.github.io/notes-nlp/."
  },
  {
    "objectID": "posts/review/papers/LSH/index.html",
    "href": "posts/review/papers/LSH/index.html",
    "title": "NLP Specialization",
    "section": "",
    "text": "Litrature review\n\n\n\n\n\n\n\nVideo¬†1: Locality-Sensitive Hashing and Beyond\n\n\n\n\n\n\n\n\nVideo¬†2: Beyond Locality Sensitive Hashing; Alexandr Andoni\nIn the NLP specialization we have covered and used LSH a number of times in at least two courses. In one sense I have an understanding of what is going on when we implement this. In another sense this seems to be quite confusing. So I don‚Äôt fully understand some aspects of LSH.\nOne way to do better is to try and explain it to someone else. This is what I am trying to do here by going back to the source and trying to understand the paper, problems, motivation and finally isolate what it is that is hard to grasp.\nThis paper is a good introduction to LSH for the angular distance.\nIn (Andoni et al. 2015) the authors delve into Locality-Sensitive Hashing (LSH) for the angular distance. They show the existence of an LSH family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal running time exponent. Unlike earlier algorithms with this property (e.g., Spherical LSH), their algorithm is also practical, improving upon the well-studied hyperplane LSH in practice.\nFor now the two videos which explain in some depth this an the related paper - Optimal Data-Dependent Hashing for Approximate Near Neighbors will have to do.\nTime permitting I will try and dive deeper into this paper. ¬∑"
  },
  {
    "objectID": "posts/review/papers/LSH/index.html#abstract",
    "href": "posts/review/papers/LSH/index.html#abstract",
    "title": "NLP Specialization",
    "section": "Abstract",
    "text": "Abstract\n\nWe show the existence of a Locality-Sensitive Hashing (LSH) family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal running time exponent. Unlike earlier algorithms with this property (e.g., Spherical LSH [1 , 2]), our algorithm is also practical, improving upon the well-studied hyperplane LSH [3] in practice.\nWe also introduce a multiprobe version of this algorithm, and conduct experimental evaluation on real and synthetic data sets.\nWe complement the above positive results with a fine-grained lower bound for the quality of any LSH family for angular distance. Our lower bound implies that the above LSH family exhibits a trade-off between evaluation time and quality that is close to optimal for a natural class of LSH functions. ‚Äì (Andoni et al. 2015)"
  },
  {
    "objectID": "posts/review/papers/LSH/index.html#the-paper",
    "href": "posts/review/papers/LSH/index.html#the-paper",
    "title": "NLP Specialization",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper"
  },
  {
    "objectID": "posts/review/seret-life-of-pronouns/index.html",
    "href": "posts/review/seret-life-of-pronouns/index.html",
    "title": "NLP Specialization",
    "section": "",
    "text": "cover\nIn (Pennebaker 2013) ‚ÄúThe Secret Life of Pronouns: What Our Words Say About Us,‚Äù the author explores how the seemingly innocuous function words we use‚Äîpronouns, articles, prepositions, and auxiliary verbs‚Äîcan reveal a great deal about our personalities, emotions, social connections, and psychological states. James W. Pennebaker, the author and a social psychologist, uses computer-based text analysis methods to uncover these hidden patterns."
  },
  {
    "objectID": "posts/review/seret-life-of-pronouns/index.html#outline",
    "href": "posts/review/seret-life-of-pronouns/index.html#outline",
    "title": "NLP Specialization",
    "section": "Outline",
    "text": "Outline\n\nKey Questions and Themes:\n\nCan language reveal psychological states? The author is primarily interested in what people‚Äôs words say about their psychological states. The book investigates whether language features can reveal how people are thinking and whether this information can be used to change their thinking.\nHow do function words differ from content words? The book distinguishes between content and style of speech, arguing that function words, which reflect language style, reveal more about a person than content words. Function words are tied to the relationship between speaker and listener.\nDo men and women use words differently? The book explores gender differences in language use, noting that women are typically more attentive to the thoughts and feelings of others, while men are less so. It also investigates whether different writing styles reflect different personalities.\nCan language predict behavior? The book examines whether language use can predict behaviors such as success in college, risk of suicide, leadership styles, and likelihood of declaring war.\nHow can language be used as a tool for change? The author examines whether analyzing words can lead to improvements in relationships, teaching, and leadership. Additionally, the book looks at how writing about emotional upheavals can improve mental and physical health, and whether using certain types of words can influence the effectiveness of that writing.\nCan language reveal deception? The book explores linguistic markers of deception, such as the use of first-person singular pronouns and the mention of other individuals.\nCan language analysis help identify authors? The book presents methods for identifying authors using function words, punctuation, and obscure words.\n\nMain Examples and Studies:\n\nExpressive Writing: Early research focused on the benefits of writing about traumatic experiences, leading to the development of the LIWC (Linguistic Inquiry and Word Count) program. This program helps analyze word categories associated with healthy writing such as positive emotion words, cognitive words, and the ability to construct a story.\nThe Bottle and the Two People Pictures: Participants described pictures, and their descriptions were analyzed for function word use and content. For example, people who focused on shadows in the bottle picture tended to be more creative. The second picture revealed common themes such as social connections, clothing, and accessories.\nThinking Styles: The book describes how people‚Äôs function words can be statistically grouped into categories reflecting thinking styles, such as formal, analytic, and narrative. These thinking styles can predict how people relate to others and organize their worlds.\n9/11 Blog Analysis: The language used in blog posts was analyzed before and after the 9/11 attacks. Researchers noted an increase in negative emotion words, a decrease in positive emotion words, and a temporary increase in cognitive words.\nCollege Admissions Essays: The study examined whether the writing style in college admissions essays could predict college grades.\nThe Federalist Papers: The book discusses the use of function words to determine the authorship of the anonymous Federalist Papers.\nLanguage Style Matching (LSM): LSM is a technique to determine how similarly two people use function words. It is used in studies of couples, speed dating, and even the writings of historical figures to analyze the relationship between the authors.\nObama‚Äôs Pronoun Use: The book analyzes Barack Obama‚Äôs use of first-person singular pronouns. His relatively low use of I-words is interpreted as a sign of self-confidence.\n\nAdditional Insights:\n\nStealth Words: The book emphasizes the importance of function words, often called ‚Äústealth words,‚Äù which are often overlooked.\nThe Role of Computers: Computer technology has revolutionized the study of language, allowing researchers to analyze large datasets and uncover hidden patterns that would be impossible to see with manual analysis.\nLanguage as a Tool: Language can be used not only as a window into the inner workings of people, but also as a tool for guiding thinking and promoting change.\nInterdisciplinary Approach: The author‚Äôs work draws on multiple disciplines including psychology, linguistics, computer science, history, and political science."
  },
  {
    "objectID": "posts/c4w3/index.html",
    "href": "posts/c4w3/index.html",
    "title": "Question Answering",
    "section": "",
    "text": "course banner",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#sec-question-answering",
    "href": "posts/c4w3/index.html#sec-question-answering",
    "title": "Question Answering",
    "section": "Question Answering",
    "text": "Question Answering\n\n\n\n\n\n\n\nThis week‚Äôs slides\n\n\n\n\nSupplementary Figure¬†1\n\n\nMy notes for Week 3 of the Natural Language Processing with Attention Labels Course in the Natural Language Processing Specialization Offered by DeepLearning.AI on Coursera\n\n\n\n\n\n\nLearning Objectives:\n\n\n\n\nGain intuition for how transfer learning works in the context of NLP\nIdentify two approaches to transfer learning\nDiscuss the evolution of language models from CBOW to T5 and Bert\nFine-tune BERT on a dataset\nImplement context-based question answering with T5\nInterpret the GLUE benchmark",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#sec-overview",
    "href": "posts/c4w3/index.html#sec-overview",
    "title": "Question Answering",
    "section": "Overview",
    "text": "Overview\n In this week we are going to learn about transfer learning. More specifically we will understand how T5 and BERT actually work.\n\n\n\n\nquestion-answering\n\n\n\n\n\n\n\nDefinitions:\n\n\n\nQ&A comes in two forms:\ncontext based : given a document and a question the model extracts an answer or generates an answer\nclosed book : the model picks an answer from several options (classifier)\n\n\n\n\n\n\ntl\n\n\n\n\nclassical-training\n\n\n\n\ntransfer-learning\n\n\n\n We can see how a model initially trained on some type of sentiment classification, could now be used for question answering. One other model that has state of the art makes use of multi tasking. For example, the same model could be used for sentiment analysis, question answering, and many other things.\n\n\n\n\ngoals\n\nThese new types of models make use of a lot of data. For example the C4 (colossal cleaned crawled corpus) is about 800 GB when all of the english wikipedia is just 13 GB!\n\nC4 is a colossal, cleaned version of Common Crawl‚Äôs web crawl corpus. It was based on Common Crawl dataset. It was used to train the T5 text-to-text Transformer models. Introduced by Raffel et al. (2023) in a paper titled ‚ÄúExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer‚Äù The dataset can be downloaded in a pre-processed form from allennlp. C4 at papers with code",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#sec-video2-transfer-learning-nlp",
    "href": "posts/c4w3/index.html#sec-video2-transfer-learning-nlp",
    "title": "Question Answering",
    "section": "Transfer Learning in NLP",
    "text": "Transfer Learning in NLP\n\n\n\n\ntransfer-learning-options\n\n\n\n\ntl-general-purpose\n\n\n\n\ntl-features-vs-fine-tuning\n\n\n\n\ntl-fine-tuning\n\n\n\n\ntl-pretain-data-performance\n\n\n\n\ntl-pretain-data-supervision\n\n\n\n\ntl-pretain-unsupervised\n\n\n\n\ntl-pretrain-selfsupervised\n\n\n\n\ntl-pretrain-selfsupervised\n\n\n\n\ntl-per-task-fine-tuning\n\n\n\n\ntl-summary\n\n\n\n\n\n\n\n\n\n\n\nThere are three main advantages to transfer learning:\n\nReduce training time\nImprove predictions\nAllows we to use smaller datasets\n\nTwo methods that we can use for transfer learning are the following:\n\npre-training\nfine tuning\n\nIn feature based, we can train word embeddings by running a different model and then using those features (i.e.¬†word vectors) on a different task. When fine tuning, we can use the exact same model and just run it on a different task. Sometimes when fine tuning, we can keep the model weights fixed and just add a new layer that we will train. Other times we can slowly unfreeze the layers one at a time. We can also use unlabelled data when pre-training, by masking words and trying to predict which word was masked.\nFor example, in the drawing above we try to predict the word ‚Äúfriend‚Äù. This allows your model to get a grasp of the overall structure of the data and to help the model learn some relationships within the words of a sentence",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#sec-video3-elmo-gpt-bert-t5",
    "href": "posts/c4w3/index.html#sec-video3-elmo-gpt-bert-t5",
    "title": "Question Answering",
    "section": "ELMo, GPT, BERT, T5",
    "text": "ELMo, GPT, BERT, T5\n\n\n\n\noutline\n\n\n\n\n\n\nCBOW-fixed-window\n\nThe models mentioned in the previous video were discovered in the following order.\n\nCBOW in Word2Vec - Issue: Fixed window we want all the context\n\n2013 Word2Vec Google\nCBOW & Skip grams\n\n2014 Glove Stanfor GloVe: Global Vectors for Word ()\n\nElMo - Bidirectional LSTM\n\nSolves: fixed window size using a biderectional RNN\nIssue: weak long term dependency\n\nGPT2 - issue: unidirectional. only looks back\nBERT - just encoder - biderctional, multi mask learning\nT5 - Encoder Decoder - multi-task learning",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#cbow",
    "href": "posts/c4w3/index.html#cbow",
    "title": "Question Answering",
    "section": "CBOW",
    "text": "CBOW\n\n\n\n\nCBOW-issues\n\n\n\n\nELMo-solution\n\n\n\n\nELMo-RNN\n\n\n\n\nGPT-unidirectional\n\n\n\n\nBERT\n\n\n\n\nmulti-mask\n\n\n\n\nBERT-pre-training\n\n\n\n\nt5-encoder-decoder\n\n\n\n\n\n\n\n\nIn CBOW, we want to encode a word as a vector. To do this we used the context before the word and the context after the word and we use that model to learn and creates features for the word. CBOW however uses a fixed window C (for the context).\nthe main isused with CBOW are:\n\nit has a fixed window size\nno concept of order\n\nso what do we do when we need more context to model the concept we are looking at?\nWhat ElMo does, it uses a bi-directional LSTM, which is a version of an RNN that looks at the inputs from the left and the right. This has the added benefit that the context size is no longer constrained. But since it is an RNN it has problems propagating information as sequences grow longer.\nThen Open AI introduced GPT. GPT unfortunately is uni-directional but it makes use of transformers. Although ElMo was bi-directional, it suffered from some issues such as capturing longer-term dependencies.\nBERT was then introduced which stands for the Bi-directional Encoder Representation from Transformers.\nT5 was introduced which makes use of transfer learning and uses the same model to predict on many tasks.\n\nGPT was a transformer decoder\nBERT was a transformer encoder\nT5 is a decoder encoder\n\n\n\n\n\nt5-text-to-text\n\nHere is an illustration of how T5 works:\n\n\n\n\nquestion\n\n\n\n\nsummary\n\n\nSo we can now flesh out the table",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#sec-video4-bert",
    "href": "posts/c4w3/index.html#sec-video4-bert",
    "title": "Question Answering",
    "section": "BERT Bidirectional Encoder Representations from Transformers",
    "text": "BERT Bidirectional Encoder Representations from Transformers\n\n\n\n\nBERT-outline\n\n\n\n\nBERT-question\n\n\n\n\nBERT-summary\n\n\n\nlets dive deeper into BERT\nThere are two steps in the BERT framework: pre-training and fine-tuning. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For fine tuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. For example, in the figure above, we get the corresponding embeddings for the input words, we run it through a few transformer blocks, and then we make the prediction at each time point T_i.\nTraining procedures:\n\nChoose 15% of the tokens at random:\n\nmask them 80% of the time,\nreplace them with a random token 10% of the time,\nkeep as is 10% of the time. There could be multiple masked spans in a sentence. Next sentence prediction is also used when pre-training.\n\n\n\n\n\n\nBERT\n\n\n\n\nBERT-spec\n\n\nSpec and features:\n\n\n\n\nBERT-pre-training",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#sec-video5-bert-objective",
    "href": "posts/c4w3/index.html#sec-video5-bert-objective",
    "title": "Question Answering",
    "section": "BERT Objective",
    "text": "BERT Objective\n\n\n\n\nBERT-outline\n\nMLM - masked language modeling.\nThis is the main unsupervised procedure to train the model with context left and right. It‚Äôs not clear how the model handles multiple masked items.\nDoes it try to predict them all at once or each one by considering input as context and unknowns.\n\n\n\n\nBERT-the-input\n\nThe input embeddings are the sum of the token embeddings, the segmentation embeddings and the position embeddings. The input embeddings: we have a CLS token to indicate the beginning of the sentence and a sep to indicate the end of the sentence The segment embeddings: allows we to indicate whether it is sentence a or b. Positional embeddings: allows we to indicate the word‚Äôs position in the sentence.\n\n\n\n\nBERT-the-output\n\nThe C token in the image above could be used for classification purposes. The unlabeled sentence A/B pair will depend on what we are trying to predict, it could range from question answering to sentiment. (in which case the second sentence could be just empty).\n\n\n\n\nBERT-objectives\n\nThe BERT objective is defined as follows:\n\n\n\n\nBERT-summary",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#sec-video6-fine-tuning-bert",
    "href": "posts/c4w3/index.html#sec-video6-fine-tuning-bert",
    "title": "Question Answering",
    "section": "Fine tuning BERT",
    "text": "Fine tuning BERT\n\n\n\n\nBERT-fine-tuning-outline\n\nOnce we have a pre-trained model, we can fine tune it on different tasks.\n\n\n\n\ninputs\n\nFor example, given a hypothesis, we can identify the premise. Given a question, we can find the answer. We can also use it for named entity recognition. Here is a summary of the inputs.\n\nWe can replace sentences A/B\nParaphrase from sentence A\nQuestion/passage\nHypothesis premise pairs in entailment\nText and a √ò for classification/sequence tagging\nOutput tokens are fed into a layer for token level tasks otherwise use [CLS] embedding as input.\n\n\n\n\n\nsummary",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#transformer-t5",
    "href": "posts/c4w3/index.html#transformer-t5",
    "title": "Question Answering",
    "section": "Transformer: T5",
    "text": "Transformer: T5\n\n\n\n\nt5-outline\n\n\n\n\nt5-text-to-text\n\n\n\n\nT5-transformer\n\n\n\nOne of the major techniques that allowed the T5 model to reach state of the art is the concept of masking:\nFor example, we represent the ‚Äúfor inviting‚Äù with &lt;X&gt; and last with &lt;Y&gt; then the model predicts what the X should be and what the Y should be. This is exactly what we saw in the BERT loss. We can also mask out a few positions, not just one. The loss is only on the mask for BERT, for T5 it is on the target.\n\n\n\n\nT5-architecture\n\nSo we start with the basic encoder-decoder representation. There we have a fully visible attention in the encoder and then causal attention in the decoder. So light gray lines correspond to causal masking. And dark gray lines correspond to the fully visible masking.\nIn the middle we have the language model which consists of a single transformer layer stack. And it‚Äôs being fed the concatenation of the inputs and the target. So it uses causal masking throughout as we can see because they‚Äôre all gray lines. And we have X_1 going inside, we get X_2, X_2 goes into the model and we get X3 and so forth.\nTo the right, we have prefix language model which corresponds to allowing fully visible masking over the inputs as we can see with the dark arrows. And then causal masking in the rest.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#sec-video8-multi-task-training-strategy",
    "href": "posts/c4w3/index.html#sec-video8-multi-task-training-strategy",
    "title": "Question Answering",
    "section": "Lecture Multi-Task Training Strategy",
    "text": "Lecture Multi-Task Training Strategy\n\n\n\n\nT5-architecture\n\n\n\n\nT5-summary\n\n\n\n\nT5-multi-task-training\n\n\n\nThis is a reminder of how the T5 model works:\nWe can see that we only have to add a small prefix to the input and the model as a result will solve the task for you. There are many tasks that the t5 model can do for you. It is possible to formulate most NLP tasks in a ‚Äútext-to-text‚Äù format ‚Äì that is, a task where the model is fed some text for context or conditioning and is then asked to produce some output text. This framework provides a consistent training objective both for pre-training and fine-tuning. Specifically, the model is trained with a maximum likelihood objective (using ‚Äúteacher forcing‚Äù ) regardless of the task.\n\nTraining data strategies\n\nExamples-proportional mixing\n\nsample in proportion to the size of each task‚Äôs dataset\n\nTemperature scaled mixing\n\nadjust the ‚Äútemperature‚Äù‚Äù of the mixing rates. This temperature parameter allows we to weight certain examples more than others. To implement temperature scaling with temperature T, we raise each task‚Äôs mixing rate rm to the power of 1‚ÅÑT and renormalize the rates so that they sum to 1. When T = 1, this approach is equivalent to examples-proportional mixing and as T increases the proportions become closer to equal mixing\n\nEqual mixing\n\nIn this case, we sample examples from each task with equal probability. Specifically, each example in each batch is sampled uniformly at random from one of the datasets we train on.\n\n\n\n\n\n\nio-format\n\n\n\n\nmulti-task-training\n\n\n\n\ndata-training-strategy\n\n\n\n\nunfreezing-adapter-layers\n\n\n\n\nquestion\n\n\n\n\nfine-tuning\n\n\n\n\n\n\nWe can see how fine tuning on a specific task could work even though we were pre-training on different tasks.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#sec-video9-glue-benchmark",
    "href": "posts/c4w3/index.html#sec-video9-glue-benchmark",
    "title": "Question Answering",
    "section": "GLUE Benchmark",
    "text": "GLUE Benchmark\n\n\n\n\nGLUE-evaluation\n\n\n\n\nGLUE-tasks\n\n\n\n\nGLUE\n\n\n\nGeneral Language Understanding Evaluation (GLUE) is contains:\n\nA collection used to train, evaluate, analyze natural language understanding systems\nDatasets with different genres, and of different sizes and difficulties\nLeaderboard\n\nCurrently T5 is state of the art according to this GLUE benchmark and we will be implementing it for homework this week! This GLUE bench mark is used for research purposes, it is model agnostic, and relies on models that make use of transfer learning.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#sec-video10-question-answering",
    "href": "posts/c4w3/index.html#sec-video10-question-answering",
    "title": "Question Answering",
    "section": "Question Answering",
    "text": "Question Answering\n We will be implementing an encoder this week. Last week we implemented the decoder. So here it is:\n\n\n\n\nBERT-encoder-Block\n\n\n\n\nBERT-blocks\n\n\n\n\nq&a-data-example\n\n\n\n\nq&a-with-t5\n\n\n\n\nt5\n\n\n\n\nt5-question\n\n\n\n\n\n\nWe can see there is a feed forward and the encoder-block above. It makes use of two residual connections, layer normalization, and dropout.\nThe steps we will follow to implement it are:\n\nLoad a pre-trained model\nProcess data to get the required inputs and outputs: ‚Äúquestion: Q context: C‚Äù as input and ‚ÄúA‚Äù as target\nFine tune your model on the new task and input\nPredict using your own model",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#sec-programming-assignment-question-answering",
    "href": "posts/c4w3/index.html#sec-programming-assignment-question-answering",
    "title": "Question Answering",
    "section": "Programming Assignment: Question Answering",
    "text": "Programming Assignment: Question Answering",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#sec-lab-sentencepiece-and-bpe",
    "href": "posts/c4w3/index.html#sec-lab-sentencepiece-and-bpe",
    "title": "Question Answering",
    "section": "Lab: SentencePiece and BPE",
    "text": "Lab: SentencePiece and BPE\n\nNFKC Normalization\nunicode normalization - for accents, diacritics and friends\nfrom unicodedata import normalize\nnorm_eaccent = normalize('NFKC', '\\u00E9')\nnorm_e_accent = normalize('NFKC', '\\u0065\\u0301')\nprint(f'{norm_eaccent} = {norm_e_accent} : {norm_eaccent == norm_e_accent}')",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#lossless-tokenization",
    "href": "posts/c4w3/index.html#lossless-tokenization",
    "title": "Question Answering",
    "section": "lossless tokenization",
    "text": "lossless tokenization\nTo ensure this lossless tokenization it replaces white space with _ (U+2581).\ns_ = s.replace(' ', '\\u2581')\n\nSentencePiece\n\n\nBPE",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#sec-lab-bert-loss",
    "href": "posts/c4w3/index.html#sec-lab-bert-loss",
    "title": "Question Answering",
    "section": "Lab: BERT Loss",
    "text": "Lab: BERT Loss",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#sec-lab-t5",
    "href": "posts/c4w3/index.html#sec-lab-t5",
    "title": "Question Answering",
    "section": "Lab: T5",
    "text": "Lab: T5",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#representation.-pdf-bib",
    "href": "posts/c4w3/index.html#representation.-pdf-bib",
    "title": "Question Answering",
    "section": "Representation. [pdf] [bib]",
    "text": "Representation. [pdf] [bib]\n\n2017 fasttext Facebook CBOW\n\nmorphological via sub words Algorithm of fasttext is based on these two papers:[8]\nEnriching Word Vectors with Subword Information , Piotr Bojanowski, Edouard Grave, Armand Joulin and Tomas Mikolov, 2016\nBag of Tricks for Efficient Text Classification, Armand Joulin, Edouard Grave, Piotr Bojanowski, Tomas Mikolov, 2016\n\n2018 ELMO Allen Institute for AI ELMo - Character based Bidirectional LSTM - Issue: long term dependency is weak due to vanishing gradient and information loss.\nGPT Encoder only with left context\nBert uses\n2020 T5 uses a label to specify task uses task specific bidirectional lstm to build the embeddings\nBERT Decoder only\n\nInput Token embedding - the distributed representation of the tokens in one space S with Dim(S)=D\nSegment embedding - because the model cannot tell the segment apart\n\nPosition embedding because the model cannot discriminate the word position. \nNote we are trying to mimic RNN behavior but we don‚Äôt have recursion:\nNote these are added - they all live in S. Question: would putting S and P in their own dimensions more interpretable. Questions: how do we know the model does not have embeddings that are similar to E_A and E_0 Output CLS - classification token SEP - separator token convert to embedding C is used for next sentence prediction T_i are used for masked word prediction T\nCross entropy loss + Binary loss\n\ncross entropy loss to compare between two distribution from Softmax\n\nbinary loss - could use cross entropy on two cat.\nPretraining\n        before feeding data we mask 15% of the tokens.\nmask 80% of the time:\ntraining data generator chooses 15%. of these at random for prediction\nreplace with:\nmask .8 of the time a random word .1 of the time\noriginal world otherwise.\n\na sentence may have multiple masks.\n\nnext sentence prediction also used in pre training.\nwhy/how\n(s1,s2) true/false\n\n\nBERT_Base\n12 layers\n12 attention heads\n110 million parameters\nFine tuning BERT\nFine tuning\nT5 like BERT does Transfer learning + fine tuning. classification, MT, NE, Sentiment\nSo we can see over here we have fully visible attention in the encoder and then causal attention in the decoder. \nAnd then we have the general encoder-decoder representation just as \nnotation. \nSo light gray lines correspond to causal masking. \nAnd dark gray lines correspond to the fully visible masking. \nSo on the left as I said again, it's the standard encoder-decoder architecture. \nIn the middle over here what we have, \nwe have the language model which consists of a single transformer layer stack. \nAnd it's being fed the concatenation of the inputs and the target. \nSo it uses causal masking throughout as we can see because they're \nall gray lines. \nAnd we have X1 going inside over here, get at X2, \nX2 goes into the model X3 and so forth. \nNow over here to the right, \nwe have prefix language model which corresponds to allowing fully \nvisible masking over the inputs as we can see here in the dark arrows. \nAnd then causal masking in the rest.\nPlay video starting at :3:2 and follow transcript3:02\nSo as we can see over here, it's doing causal masking. \nSo the model architecture, it uses encoder/decoder stack. \nIt has 12 transformer blocks each. \nSo we can think of it as a dozen eggs and then 220 million parameters. \nSo in summary, you've seen prefix language model attention. \nYou've seen the model architecture for T5. \nAnd you've seen how the pre-training is done similar to birds, but \nwe just use mask language modeling here.\n\n\nencoder/decoder\n1212 transformer blocks 220 million parameters pre training 2^18 steps = 262144",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#tokenization",
    "href": "posts/c4w3/index.html#tokenization",
    "title": "Question Answering",
    "section": "Tokenization",
    "text": "Tokenization\n\nSentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (Kudo & Richardson 2018) sub-word tokenization\nSubword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates (Kudo 2018) sub-word tokenization\nNeural Machine Translation of Rare Words with Subword Units (Sennrich et all 2016) sub-word tokenization\nSubword tokenizers TF tutorial sub-word tokenization\n[https://blog.floydhub.com/tokenization-nlp/]\nSwivel: Improving Embeddings by Noticing What‚Äôs Missing (Shazeer, 2016)",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#transformers",
    "href": "posts/c4w3/index.html#transformers",
    "title": "Question Answering",
    "section": "Transformers",
    "text": "Transformers\n\nExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Raffel et al, 2019)\n\nReformer: The Efficient Transformer (Kitaev et al, 2020)\nAttention Is All We Need (Vaswani et al, 2017)\nDeep contextualized word representations (Peters et al, 2018)\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al, 2018)\nFinetuning Pretrained Transformers into RNNs (Kasai et all 2021)\nThe Illustrated Transformer (Alammar, 2018)\nThe Illustrated GPT-2 (Alammar, 2019)\nHow GPT3 Works - Visualizations and Animations (Alammar, 2020)\nAttention? Attention! (Lilian Weng, 2018)\nThe Transformer Family (Lilian Weng, 2020)\nTeacher forcing for RNNs\n\n\nQuestion Answering Task:\n\nTitle (Author et al., Year) note",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c4w3/index.html#links",
    "href": "posts/c4w3/index.html#links",
    "title": "Question Answering",
    "section": "Links",
    "text": "Links\n\nJax\nTrax\nTrax community on Gitter\nCNN daily mail dataset\n\nLei Mao Machine Learning, Artificial Intelligence, Computer Science. [Byte Pair Encoding (Lei Mao 2021)] (https://leimao.github.io/blog/Byte-Pair-Encoding/) videos: Q&A\n\n\nSubword tokenizers\n\n\nSwivel Embeddings\nhttps://youtu.be/hAvtJ516Mw4",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering"
    ]
  },
  {
    "objectID": "posts/c2w1/assignment.html",
    "href": "posts/c2w1/assignment.html",
    "title": "Assignment 1: Auto Correct",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure¬†1\nWelcome to the first assignment of Course 2. This assignment will give we a chance to brush up on your python and probability skills. In doing so, we will implement an auto-correct system that is very effective and useful.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "A1 - Auto Correct"
    ]
  },
  {
    "objectID": "posts/c2w1/assignment.html#outline",
    "href": "posts/c2w1/assignment.html#outline",
    "title": "Assignment 1: Auto Correct",
    "section": "Outline",
    "text": "Outline\n\n0. Overview\n\n0.1 Edit Distance\n\n1. Data Preprocessing\n\n1.1 Exercise 1\n1.2 Exercise 2\n1.3 Exercise 3\n\n2. String Manipulation\n\n2.1 Exercise 4\n2.2 Exercise 5\n2.3 Exercise 6\n2.4 Exercise 7\n\n3. Combining the edits\n\n3.1 Exercise 8\n3.2 Exercise 9\n3.3 Exercise 10\n\n4. Minimum Edit Distance\n\n4.1 Exercise 11\n\n5. Backtrace (Optional)",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "A1 - Auto Correct"
    ]
  },
  {
    "objectID": "posts/c2w1/lab01.html",
    "href": "posts/c2w1/lab01.html",
    "title": "NLP Course 2 Week 1 Lesson : Building The Model - Lecture Exercise 01",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure¬†1\n\n\nEstimated Time: 10 minutes\n\nVocabulary Creation\nCreate a tiny vocabulary from a tiny corpus\nIt‚Äôs time to start small !\n\nImports and Data\n\n# imports\nimport re # regular expression library; for tokenization of words\nfrom collections import Counter # collections library; counter: dict subclass for counting hashable objects\nimport matplotlib.pyplot as plt # for data visualization\n\n\n# the tiny corpus of text ! \ntext = 'red pink pink blue blue yellow ORANGE BLUE BLUE PINK' # üåà\nprint(text)\nprint('string length : ',len(text))\n\nred pink pink blue blue yellow ORANGE BLUE BLUE PINK\nstring length :  52\n\n\n\n\nPreprocessing\n\n# convert all letters to lower case\ntext_lowercase = text.lower()\nprint(text_lowercase)\nprint('string length : ',len(text_lowercase))\n\nred pink pink blue blue yellow orange blue blue pink\nstring length :  52\n\n\n\n# some regex to tokenize the string to words and return them in a list\nwords = re.findall(r'\\w+', text_lowercase)\nprint(words)\nprint('count : ',len(words))\n\n['red', 'pink', 'pink', 'blue', 'blue', 'yellow', 'orange', 'blue', 'blue', 'pink']\ncount :  10\n\n\n\n\nCreate Vocabulary\nOption 1 : A set of distinct words from the text\n\n# create vocab\nvocab = set(words)\nprint(vocab)\nprint('count : ',len(vocab))\n\n{'red', 'yellow', 'orange', 'pink', 'blue'}\ncount :  5\n\n\n\n\nAdd Information with Word Counts\nOption 2 : Two alternatives for including the word count as well\n\n# create vocab including word count\ncounts_a = dict()\nfor w in words:\n    counts_a[w] = counts_a.get(w,0)+1\nprint(counts_a)\nprint('count : ',len(counts_a))\n\n{'red': 1, 'pink': 3, 'blue': 4, 'yellow': 1, 'orange': 1}\ncount :  5\n\n\n\n# create vocab including word count using collections.Counter\ncounts_b = dict()\ncounts_b = Counter(words)\nprint(counts_b)\nprint('count : ',len(counts_b))\n\nCounter({'blue': 4, 'pink': 3, 'red': 1, 'yellow': 1, 'orange': 1})\ncount :  5\n\n\n\n# barchart of sorted word counts\nd = {'blue': counts_b['blue'], 'pink': counts_b['pink'], 'red': counts_b['red'], 'yellow': counts_b['yellow'], 'orange': counts_b['orange']}\nplt.bar(range(len(d)), list(d.values()), align='center', color=d.keys())\n_ = plt.xticks(range(len(d)), list(d.keys()))\n\n\n\n\n\n\n\n\n\n\nUngraded Exercise\nNote that counts_b, above, returned by collections.Counter is sorted by word count\nCan you modify the tiny corpus of text so that a new color appears between pink and red in counts_b ?\nDo you need to run all the cells again, or just specific ones ?\n\nprint('counts_b : ', counts_b)\nprint('count : ', len(counts_b))\n\ncounts_b :  Counter({'blue': 4, 'pink': 3, 'red': 1, 'yellow': 1, 'orange': 1})\ncount :  5\n\n\nExpected Outcome:\ncounts_b : Counter({‚Äòblue‚Äô: 4, ‚Äòpink‚Äô: 3, ‚Äòyour_new_color_here‚Äô: 2, red‚Äô: 1, ‚Äòyellow‚Äô: 1, ‚Äòorange‚Äô: 1})  count : 6\n\n\nSummary\nThis is a tiny example but the methodology scales very well.  In the assignment you will create a large vocabulary of thousands of words, from a corpus  of tens of thousands or words! But the mechanics are exactly the same.  The only extra things to pay attention to should be; run time, memory management and the vocab data structure.  So the choice of approach used in code blocks counts_a vs counts_b, above, will be important.\n\n\n\n\n\nCitationBibTeX citation:@online{bochman2025,\n  author = {Bochman, Oren},\n  title = {Building the Vocabulary},\n  date = {2025-02-05},\n  url = {https://orenbochman.github.io/notes-nlp/posts/c2w1/lab01.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2025. ‚ÄúBuilding the Vocabulary.‚Äù February 5,\n2025. https://orenbochman.github.io/notes-nlp/posts/c2w1/lab01.html.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "L1 - Building the vocabulary"
    ]
  },
  {
    "objectID": "posts/c2w1/index.html",
    "href": "posts/c2w1/index.html",
    "title": "Autocorrect and minimum edit distance",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\n\nFigure¬†1\nMy irreverent notes for Week 1 of the Natural Language Processing with Probabilistic Models Course in the Natural Language Processing Specialization Offered by DeepLearning.AI on Coursera",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c2w1/index.html#overview",
    "href": "posts/c2w1/index.html#overview",
    "title": "Autocorrect and minimum edit distance",
    "section": "Overview",
    "text": "Overview\nWe use auto-correct everyday. When we send your friend a text message, or when we make a mistake in a query, there is an autocorrect behind the scenes that corrects the sentence for you. This week we are also going to learn about minimum edit distance, which tells we the minimum amount of edits to change one word into another. In doing that, we will learn about dynamic programming which is an important programming concept which frequently comes up in interviews and could be used to solve a lot of optimization problems.\n\n\n\n\n\n\n\nautocorrect\n\n\n\n\nFigure¬†2: Learning Objectives",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c2w1/index.html#autocorrect",
    "href": "posts/c2w1/index.html#autocorrect",
    "title": "Autocorrect and minimum edit distance",
    "section": "Autocorrect",
    "text": "Autocorrect\nAutocorrects are used everywhere. We use them in your phones, tablets, and computers.\n\n\n\n\n\n\n\nfind candidates\n\n\n\n\nFigure¬†3: What is autocorrect\n\n\nTo implement autocorrect in this week‚Äôs assignment, we have to follow these steps:\n\nIdentify a misspelled word\nFind strings n edit distance away: (these could be random strings)\nFilter candidates: (keep only the real words from the previous steps)\nCalculate word probabilities: (choose the word that is most likely to occur in that context)\n\n\n\n\n\n\n\n\nfind candidates\n\n\n\n\nFigure¬†4: Find candidates\n\n\nBuilding the model:\n\nIdentify the misspelled word\n\n\nWhen identifying the misspelled word, we can check whether it is in the vocabulary. If we don‚Äôt find it, then it is probably a typo.\n\n\nFind strings n edit distance away\nFilter candidates\n\n\nIn this step, we want to take all the words generated above and then only keep the actual words that make sense and that we can find in your vocabulary.\n\n\n\n\n\n\n\n\nfilter\n\n\n\n\nFigure¬†5: Filter candidates",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c2w1/index.html#lab-building-the-vocabulary",
    "href": "posts/c2w1/index.html#lab-building-the-vocabulary",
    "title": "Autocorrect and minimum edit distance",
    "section": "Lab: Building the vocabulary",
    "text": "Lab: Building the vocabulary\nBuilding the vocabulary",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c2w1/index.html#building-the-model-ii",
    "href": "posts/c2w1/index.html#building-the-model-ii",
    "title": "Autocorrect and minimum edit distance",
    "section": "Building the model II",
    "text": "Building the model II\n\nCalculating word probabilities\n\n\n\n\n\n\n\n\nword probabilities\n\n\n\n\nFigure¬†6: calculating word probabilities\n\n\nNote that we are storing the count of words and then we can use that to generate the probabilities. For this week, we will be counting the probabilities of words occurring. If we want to build a slightly more sophisticated auto-correct we can keep track of two words occurring next to each other instead. We can then use the previous word to decide. For example which combo is more likely, there friend or their friend? For this week however we will be implementing the probabilities by just using the word frequencies.\nHere is a summary of everything we have seen before in the previous two videos.\n\n\n\n\n\n\n\nsummary\n\n\n\n\nFigure¬†7: summary of first four autocorrect steps",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c2w1/index.html#lab-building-the-vocabulary-1",
    "href": "posts/c2w1/index.html#lab-building-the-vocabulary-1",
    "title": "Autocorrect and minimum edit distance",
    "section": "Lab: Building the vocabulary",
    "text": "Lab: Building the vocabulary\nCandidates from Edits",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect and Dynamic Programming"
    ]
  },
  {
    "objectID": "posts/c2w1/index.html#minimum-edit-distance",
    "href": "posts/c2w1/index.html#minimum-edit-distance",
    "title": "Autocorrect and minimum edit distance",
    "section": "Minimum edit distance",
    "text": "Minimum edit distance\nMinimum edit distance allows we to:\n\nEvaluate similarity between two strings\nFind the minimum number of edits between two strings\nImplement spelling correction, document similarity, machine translation, DNA sequencing, and more\n\nRemember that the edits include:\n\nInsert (add a letter) ‚Äòto‚Äô: ‚Äòtop‚Äô, ‚Äòtwo‚Äô ‚Ä¶\nDelete (remove a letter) ‚Äòhat‚Äô: ‚Äòha‚Äô, ‚Äòat‚Äô, ‚Äòht‚Äô\nReplace (change 1 letter to another) ‚Äòjaw‚Äô: ‚Äòjar‚Äô, ‚Äòpaw‚Äô, ‚Ä¶\n\nHere is a concrete example where we calculate the cost (i.e.¬†edit distance) between two strings.\n\n\n\n\n\n\n\nMinimum edit distance\n\n\n\n\nFigure¬†8: Minimum edit distance\n\n\nNote that as your strings get larger it gets much harder to calculate the minimum edit distance. Hence we will now learn about the minimum edit distance algorithm!",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c2w1/index.html#minimum-edit-distance-algorithm",
    "href": "posts/c2w1/index.html#minimum-edit-distance-algorithm",
    "title": "Autocorrect and minimum edit distance",
    "section": "Minimum edit distance algorithm",
    "text": "Minimum edit distance algorithm\nWhen computing the minimum edit distance, we would start with a source word and transform it into the target word. Let‚Äôs look at the following example:\n\n\n\n\n\n\n\nMinimum edit distance\n\n\n\n\nFigure¬†9: Minimum edit distance algorithm\n\n\nTo go:\n\nfrom # ‚Üí # has a cost of 0\nfrom p ‚Üí # has a cost of 1, because that is the cost of a delete.\n\nfrom p ‚Üí s has a cost of 2, delete p and insert s.\n\nWe can keep going this way by populating one element at a time, but it turns out there is a faster way to do this.\nWee will learn about it next.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c2w1/index.html#minimum-edit-distance-algorithm-ii",
    "href": "posts/c2w1/index.html#minimum-edit-distance-algorithm-ii",
    "title": "Autocorrect and minimum edit distance",
    "section": "Minimum edit distance algorithm II",
    "text": "Minimum edit distance algorithm II\nTo populate the following table:\n\n\n\n\n\n\n\nMinimum edit distance\n\n\n\n\nFigure¬†10: Minimum edit distance algorithm\n\n\nThere are three equations:\nD[i,j] = D[i-1, j] + del_cost: this indicates we want to populate the current cell (i,j) by using the cost in the cell found directly above.\nD[i,j] = D[i, j-1] + ins_cost: this indicates we want to populate the current cell (i,j) by using the cost in the cell found directly to its left.\nD[i,j] = D[i-1, j-1] + rep_cost: the rep cost can be 2 or 0 depending if we are going to actually replace it or not.\nAt every time step we check the three possible paths where we can come from and we select the least expensive one. Once we are done, we get the following:\n\n\n\n\n\n\n\nMinimum edit distance\n\n\n\n\nFigure¬†11: Minimum edit distance algorithm",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c2w1/index.html#minimum-edit-distance-iii",
    "href": "posts/c2w1/index.html#minimum-edit-distance-iii",
    "title": "Autocorrect and minimum edit distance",
    "section": "Minimum edit distance III",
    "text": "Minimum edit distance III\n\n\n\n\n\n\n\nMinimum edit distance\n\n\n\n\nFigure¬†12: Minimum edit distance algorithm\n\n\nTo summarize, we have seen the levenshtein distance which specifies the cost per operation. If we need to reconstruct the path of how we got from one string to the other, we can use a backtrace. We should keep a simple pointer in each cell letting we know where we came from to get there. So we know the path taken across the table from the top left corner, to the bottom right corner. We can then reconstruct it.\nThis method for computation instead of brute force is a technique known as dynamic programming. We first solve the smallest subproblem first and then reusing that result we solve the next biggest subproblem, saving that result, reusing it again, and so on. This is exactly what we did by populating each cell from the top right to the bottom left. It‚Äôs a well-known technique in computer science!",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c2w1/index.html#assignment-autocorrect",
    "href": "posts/c2w1/index.html#assignment-autocorrect",
    "title": "Autocorrect and minimum edit distance",
    "section": "Assignment : Autocorrect",
    "text": "Assignment : Autocorrect\n\nlab 3\n\nNote: due to the honor code, this is the original assignment notebook, and not the solution.\nI hope to later add some projects which should include similar material.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c1w4/assignment.html",
    "href": "posts/c1w4/assignment.html",
    "title": "Assignment 4 - Naive Machine Translation and LSH",
    "section": "",
    "text": "Figure¬†1: course banner\nYou will now implement your first machine translation system and then you will see how locality sensitive hashing works. Let‚Äôs get started by importing the required functions!\nIf you are running this notebook in your local computer, don‚Äôt forget to download the twitter samples and stopwords from nltk.\nNOTE: The Exercise xx numbers in this assignment are inconsistent with the UNQ_Cx numbers."
  },
  {
    "objectID": "posts/c1w4/assignment.html#the-data",
    "href": "posts/c1w4/assignment.html#the-data",
    "title": "Assignment 4 - Naive Machine Translation and LSH",
    "section": "The data",
    "text": "The data\nThe full dataset for English embeddings is about 3.64 gigabytes, and the French embeddings are about 629 megabytes. To prevent the Coursera workspace from crashing, we‚Äôve extracted a subset of the embeddings for the words that you‚Äôll use in this assignment.\nIf you want to run this on your local computer and use the full dataset, you can download the * English embeddings from Google code archive word2vec look for GoogleNews-vectors-negative300.bin.gz * You‚Äôll need to unzip the file first. * and the French embeddings from cross_lingual_text_classification. * in the terminal, type (in one line) curl -o ./wiki.multi.fr.vec https://dl.fbaipublicfiles.com/arrival/vectors/wiki.multi.fr.vec\nThen copy-paste the code below and run it.\n# Use this code to download and process the full dataset on your local computer\n\nfrom gensim.models import KeyedVectors\n\nen_embeddings = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary = True)\nfr_embeddings = KeyedVectors.load_word2vec_format('./wiki.multi.fr.vec')\n\n\n# loading the english to french dictionaries\nen_fr_train = get_dict('en-fr.train.txt')\nprint('The length of the english to french training dictionary is', len(en_fr_train))\nen_fr_test = get_dict('en-fr.test.txt')\nprint('The length of the english to french test dictionary is', len(en_fr_train))\n\nenglish_set = set(en_embeddings.vocab)\nfrench_set = set(fr_embeddings.vocab)\nen_embeddings_subset = {}\nfr_embeddings_subset = {}\nfrench_words = set(en_fr_train.values())\n\nfor en_word in en_fr_train.keys():\n    fr_word = en_fr_train[en_word]\n    if fr_word in french_set and en_word in english_set:\n        en_embeddings_subset[en_word] = en_embeddings[en_word]\n        fr_embeddings_subset[fr_word] = fr_embeddings[fr_word]\n\n\nfor en_word in en_fr_test.keys():\n    fr_word = en_fr_test[en_word]\n    if fr_word in french_set and en_word in english_set:\n        en_embeddings_subset[en_word] = en_embeddings[en_word]\n        fr_embeddings_subset[fr_word] = fr_embeddings[fr_word]\n\n\npickle.dump( en_embeddings_subset, open( \"en_embeddings.p\", \"wb\" ) )\npickle.dump( fr_embeddings_subset, open( \"fr_embeddings.p\", \"wb\" ) )\n\nThe subset of data\nTo do the assignment on the Coursera workspace, we‚Äôll use the subset of word embeddings.\n\nen_embeddings_subset = pickle.load(open(\"en_embeddings.p\", \"rb\"))\nfr_embeddings_subset = pickle.load(open(\"fr_embeddings.p\", \"rb\"))\n\n\n\nLook at the data\n\nen_embeddings_subset: the key is an English word, and the vaule is a 300 dimensional array, which is the embedding for that word.\n\n'the': array([ 0.08007812,  0.10498047,  0.04980469,  0.0534668 , -0.06738281, ....\n\nfr_embeddings_subset: the key is an French word, and the vaule is a 300 dimensional array, which is the embedding for that word.\n\n'la': array([-6.18250e-03, -9.43867e-04, -8.82648e-03,  3.24623e-02,...\n\n\nLoad two dictionaries mapping the English to French words\n\nA training dictionary\nand a testing dictionary.\n\n\n# loading the english to french dictionaries\nen_fr_train = get_dict('en-fr.train.txt')\nprint('The length of the English to French training dictionary is', len(en_fr_train))\nen_fr_test = get_dict('en-fr.test.txt')\nprint('The length of the English to French test dictionary is', len(en_fr_train))\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[4], line 2\n      1 # loading the english to french dictionaries\n----&gt; 2 en_fr_train = get_dict('en-fr.train.txt')\n      3 print('The length of the English to French training dictionary is', len(en_fr_train))\n      4 en_fr_test = get_dict('en-fr.test.txt')\n\nNameError: name 'get_dict' is not defined\n\n\n\n\n\nLooking at the English French dictionary\n\nen_fr_train is a dictionary where the key is the English word and the value is the French translation of that English word.\n\n{'the': 'la',\n 'and': 'et',\n 'was': '√©tait',\n 'for': 'pour',\n\nen_fr_test is similar to en_fr_train, but is a test set. We won‚Äôt look at it until we get to testing."
  },
  {
    "objectID": "posts/c1w4/assignment.html#generate-embedding-and-transform-matrices",
    "href": "posts/c1w4/assignment.html#generate-embedding-and-transform-matrices",
    "title": "Assignment 4 - Naive Machine Translation and LSH",
    "section": "1.1 Generate embedding and transform matrices",
    "text": "1.1 Generate embedding and transform matrices\n #### Exercise 01: Translating English dictionary to French by using embeddings\nYou will now implement a function get_matrices, which takes the loaded data and returns matrices X and Y.\nInputs: - en_fr : English to French dictionary - en_embeddings : English to embeddings dictionary - fr_embeddings : French to embeddings dictionary\nReturns: - Matrix X and matrix Y, where each row in X is the word embedding for an english word, and the same row in Y is the word embedding for the French version of that English word.\n\n Figure 2\n\nUse the en_fr dictionary to ensure that the ith row in the X matrix corresponds to the ith row in the Y matrix.\nInstructions: Complete the function get_matrices(): * Iterate over English words in en_fr dictionary. * Check if the word have both English and French embedding.\n\n\nHints\n\n&lt;p&gt;\n    &lt;ul&gt;\n        &lt;li&gt;&lt;a href=\"https://realpython.com/python-sets/#set-size-and-membership\" &gt;Sets&lt;/a&gt; are useful data structures that can be used to check if an item is a member of a group.&lt;/li&gt;\n        &lt;li&gt;You can get words which are embedded into the language by using &lt;a href=\"https://www.w3schools.com/python/ref_dictionary_keys.asp\"&gt; keys&lt;/a&gt; method.&lt;/li&gt;\n        &lt;li&gt;Keep vectors in `X` and `Y` sorted in list. You can use &lt;a href=\"https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.ma.vstack.html\"&gt; np.vstack()&lt;/a&gt; to merge them into the numpy matrix. &lt;/li&gt;\n        &lt;li&gt;&lt;a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.vstack.html\"&gt;numpy.vstack&lt;/a&gt; stacks the items in a list as rows in a matrix.&lt;/li&gt;\n    &lt;/ul&gt;\n&lt;/p&gt;\n\n# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef get_matrices(en_fr, french_vecs, english_vecs):\n    \"\"\"\n    Input:\n        en_fr: English to French dictionary\n        french_vecs: French words to their corresponding word embeddings.\n        english_vecs: English words to their corresponding word embeddings.\n    Output: \n        X: a matrix where the columns are the English embeddings.\n        Y: a matrix where the columns correspong to the French embeddings.\n        R: the projection matrix that minimizes the F norm ||X R -Y||^2.\n    \"\"\"\n\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n\n    # X_l and Y_l are lists of the english and french word embeddings\n    X_l = list()\n    Y_l = list()\n\n    # get the english words (the keys in the dictionary) and store in a set()\n    english_set = None\n\n    # get the french words (keys in the dictionary) and store in a set()\n    french_set = None\n\n    # store the french words that are part of the english-french dictionary (these are the values of the dictionary)\n    french_words = set(en_fr.values())\n\n    # loop through all english, french word pairs in the english french dictionary\n    for en_word, fr_word in en_fr.items():\n\n        # check that the french word has an embedding and that the english word has an embedding\n        if fr_word in french_set and en_word in english_set:\n\n            # get the english embedding\n            en_vec = english_vecs[en_word]\n\n            # get the french embedding\n            fr_vec = None\n\n            # add the english embedding to the list\n            X_l.append(en_vec)\n\n            # add the french embedding to the list\n            None\n\n    # stack the vectors of X_l into a matrix X\n    X = None\n\n    # stack the vectors of Y_l into a matrix Y\n    Y = None\n    ### END CODE HERE ###\n\n    return X, Y\n\nNow we will use function get_matrices() to obtain sets X_train and Y_train of English and French word embeddings into the corresponding vector space models.\n\n# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n\n# getting the training set:\nX_train, Y_train = get_matrices(\n    en_fr_train, fr_embeddings_subset, en_embeddings_subset)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[6], line 6\n      1 # UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n      2 # You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n      3 \n      4 # getting the training set:\n      5 X_train, Y_train = get_matrices(\n----&gt; 6     en_fr_train, fr_embeddings_subset, en_embeddings_subset)\n\nNameError: name 'en_fr_train' is not defined"
  },
  {
    "objectID": "posts/c1w4/assignment.html#calculate-transformation-matrix-r",
    "href": "posts/c1w4/assignment.html#calculate-transformation-matrix-r",
    "title": "Assignment 4 - Naive Machine Translation and LSH",
    "section": "Calculate transformation matrix R",
    "text": "Calculate transformation matrix R\nUsing those the training set, find the transformation matrix \\mathbf{R} by calling the function align_embeddings().\nNOTE: The code cell below will take a few minutes to fully execute (~3 mins)\n\n# UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\nR_train = align_embeddings(X_train, Y_train, train_steps=400, learning_rate=0.8)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[11], line 3\n      1 # UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n      2 # You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n----&gt; 3 R_train = align_embeddings(X_train, Y_train, train_steps=400, learning_rate=0.8)\n\nNameError: name 'X_train' is not defined\n\n\n\n\nExpected Output\nloss at iteration 0 is: 963.0146\nloss at iteration 25 is: 97.8292\nloss at iteration 50 is: 26.8329\nloss at iteration 75 is: 9.7893\nloss at iteration 100 is: 4.3776\nloss at iteration 125 is: 2.3281\nloss at iteration 150 is: 1.4480\nloss at iteration 175 is: 1.0338\nloss at iteration 200 is: 0.8251\nloss at iteration 225 is: 0.7145\nloss at iteration 250 is: 0.6534\nloss at iteration 275 is: 0.6185\nloss at iteration 300 is: 0.5981\nloss at iteration 325 is: 0.5858\nloss at iteration 350 is: 0.5782\nloss at iteration 375 is: 0.5735"
  },
  {
    "objectID": "posts/c1w4/assignment.html#testing-the-translation",
    "href": "posts/c1w4/assignment.html#testing-the-translation",
    "title": "Assignment 4 - Naive Machine Translation and LSH",
    "section": "2.2 Testing the translation",
    "text": "2.2 Testing the translation\n\nk-Nearest neighbors algorithm\nk-Nearest neighbors algorithm * k-NN is a method which takes a vector as input and finds the other vectors in the dataset that are closest to it. * The ‚Äòk‚Äô is the number of ‚Äúnearest neighbors‚Äù to find (e.g.¬†k=2 finds the closest two neighbors).\n\n\nSearching for the translation embedding\nSince we‚Äôre approximating the translation function from English to French embeddings by a linear transformation matrix \\mathbf{R}, most of the time we won‚Äôt get the exact embedding of a French word when we transform embedding \\mathbf{e} of some particular English word into the French embedding space. * This is where k-NN becomes really useful! By using 1-NN with \\mathbf{eR} as input, we can search for an embedding \\mathbf{f} (as a row) in the matrix \\mathbf{Y} which is the closest to the transformed vector \\mathbf{eR}\n\n\nCosine similarity\nCosine similarity between vectors u and v calculated as the cosine of the angle between them. The formula is\n\\cos(u,v)=\\frac{u\\cdot v}{\\left\\|u\\right\\|\\left\\|v\\right\\|}\n\n\\cos(u,v) = 1 when u and v lie on the same line and have the same direction.\n\\cos(u,v) is -1 when they have exactly opposite directions.\n\\cos(u,v) is 0 when the vectors are orthogonal (perpendicular) to each other.\n\n\nNote: Distance and similarity are pretty much opposite things.\n\nWe can obtain distance metric from cosine similarity, but the cosine similarity can‚Äôt be used directly as the distance metric.\nWhen the cosine similarity increases (towards 1), the ‚Äúdistance‚Äù between the two vectors decreases (towards 0).\nWe can define the cosine distance between u and v as d_{\\text{cos}}(u,v)=1-\\cos(u,v)\n\n\nExercise 05: Complete the function nearest_neighbor()\nInputs: * Vector v, * A set of possible nearest neighbors candidates * k nearest neighbors to find. * The distance metric should be based on cosine similarity. * cosine_similarity function is already implemented and imported for you. It‚Äôs arguments are two vectors and it returns the cosine of the angle between them. * Iterate over rows in candidates, and save the result of similarities between current row and vector v in a python list. Take care that similarities are in the same order as row vectors of candidates. * Now you can use numpy argsort to sort the indices for the rows of candidates.\n\n\nHints\n\n\n\n\nnumpy.argsort sorts values from most negative to most positive (smallest to largest)\n\n\nThe candidates that are nearest to ‚Äòv‚Äô should have the highest cosine similarity\n\n\nTo get the last element of a list ‚Äòtmp‚Äô, the notation is tmp[-1:]\n\n\n\n\n# UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef nearest_neighbor(v, candidates, k=1):\n    \"\"\"\n    Input:\n      - v, the vector you are going find the nearest neighbor for\n      - candidates: a set of vectors where we will find the neighbors\n      - k: top k nearest neighbors to find\n    Output:\n      - k_idx: the indices of the top k closest vectors in sorted form\n    \"\"\"\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    similarity_l = []\n\n    # for each candidate vector...\n    for row in candidates:\n        # get the cosine similarity\n        cos_similarity = None\n\n        # append the similarity to the list\n        None\n        \n    # sort the similarity list and get the indices of the sorted list\n    sorted_ids = None\n\n    # get the indices of the k most similar candidate vectors\n    k_idx = None\n    ### END CODE HERE ###\n    return k_idx\n\n\n# UNQ_C9 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n\n# Test your implementation:\nv = np.array([1, 0, 1])\ncandidates = np.array([[1, 0, 5], [-2, 5, 3], [2, 0, 1], [6, -9, 5], [9, 9, 9]])\nprint(candidates[nearest_neighbor(v, candidates, 3)])\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[13], line 5\n      1 # UNQ_C9 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n      2 # You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n      3 \n      4 # Test your implementation:\n----&gt; 5 v = np.array([1, 0, 1])\n      6 candidates = np.array([[1, 0, 5], [-2, 5, 3], [2, 0, 1], [6, -9, 5], [9, 9, 9]])\n      7 print(candidates[nearest_neighbor(v, candidates, 3)])\n\nNameError: name 'np' is not defined\n\n\n\nExpected Output:\n[[9 9 9]  [1 0 5]  [2 0 1]]\n\n\n\nTest your translation and compute its accuracy\n Exercise 06: Complete the function test_vocabulary which takes in English embedding matrix X, French embedding matrix Y and the R matrix and returns the accuracy of translations from X to Y by R.\n\nIterate over transformed English word embeddings and check if the closest French word vector belongs to French word that is the actual translation.\nObtain an index of the closest French embedding by using nearest_neighbor (with argument k=1), and compare it to the index of the English embedding you have just transformed.\nKeep track of the number of times you get the correct translation.\nCalculate accuracy as \\text{accuracy}=\\frac{\\#(\\text{correct predictions})}{\\#(\\text{total predictions})}\n\n\n# UNQ_C10 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef test_vocabulary(X, Y, R):\n    '''\n    Input:\n        X: a matrix where the columns are the English embeddings.\n        Y: a matrix where the columns correspong to the French embeddings.\n        R: the transform matrix which translates word embeddings from\n        English to French word vector space.\n    Output:\n        accuracy: for the English to French capitals\n    '''\n\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    # The prediction is X times R\n    pred = None\n\n    # initialize the number correct to zero\n    num_correct = 0\n\n    # loop through each row in pred (each transformed embedding)\n    for i in range(len(pred)):\n        # get the index of the nearest neighbor of pred at row 'i'; also pass in the candidates in Y\n        pred_idx = None\n\n        # if the index of the nearest neighbor equals the row of i... \\\n        if pred_idx == i:\n            # increment the number correct by 1.\n            num_correct += None\n\n    # accuracy is the number correct divided by the number of rows in 'pred' (also number of rows in X)\n    accuracy = None\n\n    ### END CODE HERE ###\n\n    return accuracy\n\nLet‚Äôs see how is your translation mechanism working on the unseen data:\n\nX_val, Y_val = get_matrices(en_fr_test, fr_embeddings_subset, en_embeddings_subset)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[15], line 1\n----&gt; 1 X_val, Y_val = get_matrices(en_fr_test, fr_embeddings_subset, en_embeddings_subset)\n\nNameError: name 'en_fr_test' is not defined\n\n\n\n\n# UNQ_C11 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n\nacc = test_vocabulary(X_val, Y_val, R_train)  # this might take a minute or two\nprint(f\"accuracy on test set is {acc:.3f}\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[16], line 4\n      1 # UNQ_C11 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n      2 # You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n----&gt; 4 acc = test_vocabulary(X_val, Y_val, R_train)  # this might take a minute or two\n      5 print(f\"accuracy on test set is {acc:.3f}\")\n\nNameError: name 'X_val' is not defined\n\n\n\nExpected Output:\n0.557\nYou managed to translate words from one language to another language without ever seing them with almost 56% accuracy by using some basic linear algebra and learning a mapping of words from one language to another!"
  },
  {
    "objectID": "posts/c1w4/assignment.html#looking-up-the-tweets",
    "href": "posts/c1w4/assignment.html#looking-up-the-tweets",
    "title": "Assignment 4 - Naive Machine Translation and LSH",
    "section": "3.2 Looking up the tweets",
    "text": "3.2 Looking up the tweets\nNow you have a vector of dimension (m,d) where m is the number of tweets (10,000) and d is the dimension of the embeddings (300). Now you will input a tweet, and use cosine similarity to see which tweet in our corpus is similar to your tweet.\n\nmy_tweet = 'i am sad'\nprocess_tweet(my_tweet)\ntweet_embedding = get_document_embedding(my_tweet, en_embeddings_subset)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[23], line 2\n      1 my_tweet = 'i am sad'\n----&gt; 2 process_tweet(my_tweet)\n      3 tweet_embedding = get_document_embedding(my_tweet, en_embeddings_subset)\n\nNameError: name 'process_tweet' is not defined\n\n\n\n\n# UNQ_C16 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n\n# this gives you a similar tweet as your input.\n# this implementation is vectorized...\nidx = np.argmax(cosine_similarity(document_vecs, tweet_embedding))\nprint(all_tweets[idx])\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[24], line 6\n      1 # UNQ_C16 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n      2 # You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n      3 \n      4 # this gives you a similar tweet as your input.\n      5 # this implementation is vectorized...\n----&gt; 6 idx = np.argmax(cosine_similarity(document_vecs, tweet_embedding))\n      7 print(all_tweets[idx])\n\nNameError: name 'np' is not defined\n\n\n\n\nExpected Output\n@zoeeylim sad sad sad kid :( it's ok I help you watch the match HAHAHAHAHA"
  },
  {
    "objectID": "posts/c1w4/assignment.html#finding-the-most-similar-tweets-with-lsh",
    "href": "posts/c1w4/assignment.html#finding-the-most-similar-tweets-with-lsh",
    "title": "Assignment 4 - Naive Machine Translation and LSH",
    "section": "3.3 Finding the most similar tweets with LSH",
    "text": "3.3 Finding the most similar tweets with LSH\nYou will now implement locality sensitive hashing (LSH) to identify the most similar tweet. * Instead of looking at all 10,000 vectors, you can just search a subset to find its nearest neighbors.\nLet‚Äôs say your data points are plotted like this:\n\n Figure 3\n\nYou can divide the vector space into regions and search within one region for nearest neighbors of a given vector.\n\n Figure 4\n\n\nN_VECS = len(all_tweets)       # This many vectors.\nN_DIMS = len(ind2Tweet[1])     # Vector dimensionality.\nprint(f\"Number of vectors is {N_VECS} and each has {N_DIMS} dimensions.\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[25], line 1\n----&gt; 1 N_VECS = len(all_tweets)       # This many vectors.\n      2 N_DIMS = len(ind2Tweet[1])     # Vector dimensionality.\n      3 print(f\"Number of vectors is {N_VECS} and each has {N_DIMS} dimensions.\")\n\nNameError: name 'all_tweets' is not defined\n\n\n\n\nChoosing the number of planes\n\nEach plane divides the space to 2 parts.\nSo n planes divide the space into 2^{n} hash buckets.\nWe want to organize 10,000 document vectors into buckets so that every bucket has about ~16 vectors.\nFor that we need \\frac{10000}{16}=625 buckets.\nWe‚Äôre interested in n, number of planes, so that 2^{n}= 625. Now, we can calculate n=\\log_{2}625 = 9.29 \\approx 10.\n\n\n# The number of planes. We use log2(625) to have ~16 vectors/bucket.\nN_PLANES = 10\n# Number of times to repeat the hashing to improve the search.\nN_UNIVERSES = 25"
  },
  {
    "objectID": "posts/c1w4/assignment.html#getting-the-hash-number-for-a-vector",
    "href": "posts/c1w4/assignment.html#getting-the-hash-number-for-a-vector",
    "title": "Assignment 4 - Naive Machine Translation and LSH",
    "section": "3.4 Getting the hash number for a vector",
    "text": "3.4 Getting the hash number for a vector\nFor each vector, we need to get a unique number associated to that vector in order to assign it to a ‚Äúhash bucket‚Äù.\n\nHyperlanes in vector spaces\n\nIn 3-dimensional vector space, the hyperplane is a regular plane. In 2 dimensional vector space, the hyperplane is a line.\nGenerally, the hyperplane is subspace which has dimension 1 lower than the original vector space has.\nA hyperplane is uniquely defined by its normal vector.\nNormal vector n of the plane \\pi is the vector to which all vectors in the plane \\pi are orthogonal (perpendicular in 3 dimensional case).\n\n\n\nUsing Hyperplanes to split the vector space\nWe can use a hyperplane to split the vector space into 2 parts. * All vectors whose dot product with a plane‚Äôs normal vector is positive are on one side of the plane. * All vectors whose dot product with the plane‚Äôs normal vector is negative are on the other side of the plane.\n\n\nEncoding hash buckets\n\nFor a vector, we can take its dot product with all the planes, then encode this information to assign the vector to a single hash bucket.\nWhen the vector is pointing to the opposite side of the hyperplane than normal, encode it by 0.\nOtherwise, if the vector is on the same side as the normal vector, encode it by 1.\nIf you calculate the dot product with each plane in the same order for every vector, you‚Äôve encoded each vector‚Äôs unique hash ID as a binary number, like [0, 1, 1, ‚Ä¶ 0].\n\n\n\n\nExercise 09: Implementing hash buckets\nWe‚Äôve initialized hash table hashes for you. It is list of N_UNIVERSES matrices, each describes its own hash table. Each matrix has N_DIMS rows and N_PLANES columns. Every column of that matrix is a N_DIMS-dimensional normal vector for each of N_PLANES hyperplanes which are used for creating buckets of the particular hash table.\nExercise: Your task is to complete the function hash_value_of_vector which places vector v in the correct hash bucket.\n\nFirst multiply your vector v, with a corresponding plane. This will give you a vector of dimension (1,\\text{N_planes}).\nYou will then convert every element in that vector to 0 or 1.\nYou create a hash vector by doing the following: if the element is negative, it becomes a 0, otherwise you change it to a 1.\nYou then compute the unique number for the vector by iterating over N_PLANES\nThen you multiply 2^i times the corresponding bit (0 or 1).\nYou will then store that sum in the variable hash_value.\n\nIntructions: Create a hash for the vector in the function below. Use this formula:\n hash = \\sum_{i=0}^{N-1} \\left( 2^{i} \\times h_{i} \\right) \n\nCreate the sets of planes\n\nCreate multiple (25) sets of planes (the planes that divide up the region).\nYou can think of these as 25 separate ways of dividing up the vector space with a different set of planes.\nEach element of this list contains a matrix with 300 rows (the word vector have 300 dimensions), and 10 columns (there are 10 planes in each ‚Äúuniverse‚Äù).\n\n\nnp.random.seed(0)\nplanes_l = [np.random.normal(size=(N_DIMS, N_PLANES))\n            for _ in range(N_UNIVERSES)]\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[27], line 1\n----&gt; 1 np.random.seed(0)\n      2 planes_l = [np.random.normal(size=(N_DIMS, N_PLANES))\n      3             for _ in range(N_UNIVERSES)]\n\nNameError: name 'np' is not defined\n\n\n\n\n\nHints\n\n\n\n\nnumpy.squeeze() removes unused dimensions from an array; for instance, it converts a (10,1) 2D array into a (10,) 1D array\n\n\n\n\n# UNQ_C17 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef hash_value_of_vector(v, planes):\n    \"\"\"Create a hash for a vector; hash_id says which random hash to use.\n    Input:\n        - v:  vector of tweet. It's dimension is (1, N_DIMS)\n        - planes: matrix of dimension (N_DIMS, N_PLANES) - the set of planes that divide up the region\n    Output:\n        - res: a number which is used as a hash for your vector\n\n    \"\"\"\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    # for the set of planes,\n    # calculate the dot product between the vector and the matrix containing the planes\n    # remember that planes has shape (300, 10)\n    # The dot product will have the shape (1,10)\n    dot_product = None\n\n    # get the sign of the dot product (1,10) shaped vector\n    sign_of_dot_product = None\n\n    # set h to be false (eqivalent to 0 when used in operations) if the sign is negative,\n    # and true (equivalent to 1) if the sign is positive (1,10) shaped vector\n    h = None\n\n    # remove extra un-used dimensions (convert this from a 2D to a 1D array)\n    h = None\n\n    # initialize the hash value to 0\n    hash_value = 0\n\n    n_planes = planes.shape[1]\n    for i in range(n_planes):\n        # increment the hash value by 2^i * h_i\n        hash_value += None\n    ### END CODE HERE ###\n\n    # cast hash_value as an integer\n    hash_value = int(hash_value)\n\n    return hash_value\n\n\n# UNQ_C18 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n\nnp.random.seed(0)\nidx = 0\nplanes = planes_l[idx]  # get one 'universe' of planes to test the function\nvec = np.random.rand(1, 300)\nprint(f\" The hash value for this vector,\",\n      f\"and the set of planes at index {idx},\",\n      f\"is {hash_value_of_vector(vec, planes)}\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[29], line 4\n      1 # UNQ_C18 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n      2 # You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n----&gt; 4 np.random.seed(0)\n      5 idx = 0\n      6 planes = planes_l[idx]  # get one 'universe' of planes to test the function\n\nNameError: name 'np' is not defined\n\n\n\n\nExpected Output\nThe hash value for this vector, and the set of planes at index 0, is 768"
  },
  {
    "objectID": "posts/c1w4/assignment.html#creating-a-hash-table",
    "href": "posts/c1w4/assignment.html#creating-a-hash-table",
    "title": "Assignment 4 - Naive Machine Translation and LSH",
    "section": "3.5 Creating a hash table",
    "text": "3.5 Creating a hash table\n\n\nExercise 10\nGiven that you have a unique number for each vector (or tweet), You now want to create a hash table. You need a hash table, so that given a hash_id, you can quickly look up the corresponding vectors. This allows you to reduce your search by a significant amount of time.\n\n\n\nWe have given you the make_hash_table function, which maps the tweet vectors to a bucket and stores the vector there. It returns the hash_table and the id_table. The id_table allows you know which vector in a certain bucket corresponds to what tweet.\n\n\nHints\n\n\n\n\na dictionary comprehension, similar to a list comprehension, looks like this: {i:0 for i in range(10)}, where the key is ‚Äòi‚Äô and the value is zero for all key-value pairs.\n\n\n\n\n# UNQ_C19 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# This is the code used to create a hash table: feel free to read over it\ndef make_hash_table(vecs, planes):\n    \"\"\"\n    Input:\n        - vecs: list of vectors to be hashed.\n        - planes: the matrix of planes in a single \"universe\", with shape (embedding dimensions, number of planes).\n    Output:\n        - hash_table: dictionary - keys are hashes, values are lists of vectors (hash buckets)\n        - id_table: dictionary - keys are hashes, values are list of vectors id's\n                            (it's used to know which tweet corresponds to the hashed vector)\n    \"\"\"\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n\n    # number of planes is the number of columns in the planes matrix\n    num_of_planes = None\n\n    # number of buckets is 2^(number of planes)\n    num_buckets = None\n\n    # create the hash table as a dictionary.\n    # Keys are integers (0,1,2.. number of buckets)\n    # Values are empty lists\n    hash_table = None\n\n    # create the id table as a dictionary.\n    # Keys are integers (0,1,2... number of buckets)\n    # Values are empty lists\n    id_table = None\n\n    # for each vector in 'vecs'\n    for i, v in enumerate(vecs):\n        # calculate the hash value for the vector\n        h = None\n\n        # store the vector into hash_table at key h,\n        # by appending the vector v to the list at key h\n        None\n\n        # store the vector's index 'i' (each document is given a unique integer 0,1,2...)\n        # the key is the h, and the 'i' is appended to the list at key h\n        None\n\n    ### END CODE HERE ###\n\n    return hash_table, id_table\n\n\n# UNQ_C20 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n\nnp.random.seed(0)\nplanes = planes_l[0]  # get one 'universe' of planes to test the function\nvec = np.random.rand(1, 300)\ntmp_hash_table, tmp_id_table = make_hash_table(document_vecs, planes)\n\nprint(f\"The hash table at key 0 has {len(tmp_hash_table[0])} document vectors\")\nprint(f\"The id table at key 0 has {len(tmp_id_table[0])}\")\nprint(f\"The first 5 document indices stored at key 0 of are {tmp_id_table[0][0:5]}\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[31], line 4\n      1 # UNQ_C20 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n      2 # You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n----&gt; 4 np.random.seed(0)\n      5 planes = planes_l[0]  # get one 'universe' of planes to test the function\n      6 vec = np.random.rand(1, 300)\n\nNameError: name 'np' is not defined\n\n\n\n\nExpected output\nThe hash table at key 0 has 3 document vectors\nThe id table at key 0 has 3\nThe first 5 document indices stored at key 0 of are [3276, 3281, 3282]\n\n\n\n\n3.6 Creating all hash tables\nYou can now hash your vectors and store them in a hash table that would allow you to quickly look up and search for similar vectors. Run the cell below to create the hashes. By doing so, you end up having several tables which have all the vectors. Given a vector, you then identify the buckets in all the tables. You can then iterate over the buckets and consider much fewer vectors. The more buckets you use, the more accurate your lookup will be, but also the longer it will take.\n\n# Creating the hashtables\nhash_tables = []\nid_tables = []\nfor universe_id in range(N_UNIVERSES):  # there are 25 hashes\n    print('working on hash universe #:', universe_id)\n    planes = planes_l[universe_id]\n    hash_table, id_table = make_hash_table(document_vecs, planes)\n    hash_tables.append(hash_table)\n    id_tables.append(id_table)\n\nworking on hash universe #: 0\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[32], line 6\n      4 for universe_id in range(N_UNIVERSES):  # there are 25 hashes\n      5     print('working on hash universe #:', universe_id)\n----&gt; 6     planes = planes_l[universe_id]\n      7     hash_table, id_table = make_hash_table(document_vecs, planes)\n      8     hash_tables.append(hash_table)\n\nNameError: name 'planes_l' is not defined\n\n\n\n\n\nApproximate K-NN\n\n\n\nExercise 11\nImplement approximate K nearest neighbors using locality sensitive hashing, to search for documents that are similar to a given document at the index doc_id.\n\nInputs\n\ndoc_id is the index into the document list all_tweets.\nv is the document vector for the tweet in all_tweets at index doc_id.\nplanes_l is the list of planes (the global variable created earlier).\nk is the number of nearest neighbors to search for.\nnum_universes_to_use: to save time, we can use fewer than the total number of available universes. By default, it‚Äôs set to N_UNIVERSES, which is 25 for this assignment.\n\nThe approximate_knn function finds a subset of candidate vectors that are in the same ‚Äúhash bucket‚Äù as the input vector ‚Äòv‚Äô. Then it performs the usual k-nearest neighbors search on this subset (instead of searching through all 10,000 tweets).\n\n\nHints\n\n\n\n\nThere are many dictionaries used in this function. Try to print out planes_l, hash_tables, id_tables to understand how they are structured, what the keys represent, and what the values contain.\n\n\nTo remove an item from a list, use .remove()\n\n\nTo append to a list, use .append()\n\n\nTo add to a set, use .add()\n\n\n\n\n# UNQ_C21 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# This is the code used to do the fast nearest neighbor search. Feel free to go over it\ndef approximate_knn(doc_id, v, planes_l, k=1, num_universes_to_use=N_UNIVERSES):\n    \"\"\"Search for k-NN using hashes.\"\"\"\n    assert num_universes_to_use &lt;= N_UNIVERSES\n\n    # Vectors that will be checked as possible nearest neighbor\n    vecs_to_consider_l = list()\n\n    # list of document IDs\n    ids_to_consider_l = list()\n\n    # create a set for ids to consider, for faster checking if a document ID already exists in the set\n    ids_to_consider_set = set()\n\n    # loop through the universes of planes\n    for universe_id in range(num_universes_to_use):\n\n        # get the set of planes from the planes_l list, for this particular universe_id\n        planes = planes_l[universe_id]\n\n        # get the hash value of the vector for this set of planes\n        hash_value = hash_value_of_vector(v, planes)\n\n        # get the hash table for this particular universe_id\n        hash_table = hash_tables[universe_id]\n\n        # get the list of document vectors for this hash table, where the key is the hash_value\n        document_vectors_l = hash_table[hash_value]\n\n        # get the id_table for this particular universe_id\n        id_table = id_tables[universe_id]\n\n        # get the subset of documents to consider as nearest neighbors from this id_table dictionary\n        new_ids_to_consider = id_table[hash_value]\n\n        ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n\n        # remove the id of the document that we're searching\n        if doc_id in new_ids_to_consider:\n            None\n            print(f\"removed doc_id {doc_id} of input vector from new_ids_to_search\")\n\n        # loop through the subset of document vectors to consider\n        for i, new_id in enumerate(new_ids_to_consider):\n\n            # if the document ID is not yet in the set ids_to_consider...\n            if new_id not in ids_to_consider_set:\n                # access document_vectors_l list at index i to get the embedding\n                # then append it to the list of vectors to consider as possible nearest neighbors\n                document_vector_at_i = None\n                None\n\n                # append the new_id (the index for the document) to the list of ids to consider\n                None\n\n                # also add the new_id to the set of ids to consider\n                # (use this to check if new_id is not already in the IDs to consider)\n                None\n\n        ### END CODE HERE ###\n\n    # Now run k-NN on the smaller set of vecs-to-consider.\n    print(\"Fast considering %d vecs\" % len(vecs_to_consider_l))\n\n    # convert the vecs to consider set to a list, then to a numpy array\n    vecs_to_consider_arr = np.array(vecs_to_consider_l)\n\n    # call nearest neighbors on the reduced list of candidate vectors\n    nearest_neighbor_idx_l = nearest_neighbor(v, vecs_to_consider_arr, k=k)\n\n    # Use the nearest neighbor index list as indices into the ids to consider\n    # create a list of nearest neighbors by the document ids\n    nearest_neighbor_ids = [ids_to_consider_l[idx]\n                            for idx in nearest_neighbor_idx_l]\n\n    return nearest_neighbor_ids\n\n\n#document_vecs, ind2Tweet\ndoc_id = 0\ndoc_to_search = all_tweets[doc_id]\nvec_to_search = document_vecs[doc_id]\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[34], line 3\n      1 #document_vecs, ind2Tweet\n      2 doc_id = 0\n----&gt; 3 doc_to_search = all_tweets[doc_id]\n      4 vec_to_search = document_vecs[doc_id]\n\nNameError: name 'all_tweets' is not defined\n\n\n\n\n# UNQ_C22 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n\n# Sample\nnearest_neighbor_ids = approximate_knn(\n    doc_id, vec_to_search, planes_l, k=3, num_universes_to_use=5)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[35], line 6\n      1 # UNQ_C22 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n      2 # You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n      3 \n      4 # Sample\n      5 nearest_neighbor_ids = approximate_knn(\n----&gt; 6     doc_id, vec_to_search, planes_l, k=3, num_universes_to_use=5)\n\nNameError: name 'vec_to_search' is not defined\n\n\n\n\nprint(f\"Nearest neighbors for document {doc_id}\")\nprint(f\"Document contents: {doc_to_search}\")\nprint(\"\")\n\nfor neighbor_id in nearest_neighbor_ids:\n    print(f\"Nearest neighbor at document id {neighbor_id}\")\n    print(f\"document contents: {all_tweets[neighbor_id]}\")\n\nNearest neighbors for document 0\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[36], line 2\n      1 print(f\"Nearest neighbors for document {doc_id}\")\n----&gt; 2 print(f\"Document contents: {doc_to_search}\")\n      3 print(\"\")\n      5 for neighbor_id in nearest_neighbor_ids:\n\nNameError: name 'doc_to_search' is not defined"
  },
  {
    "objectID": "posts/c1w4/lab02.html",
    "href": "posts/c1w4/lab02.html",
    "title": "Hash functions and multiplanes",
    "section": "",
    "text": "Figure¬†1: course banner\nIn this lab, we are going to practice the most important concepts related to the hash functions explained in the videos. You will be using these in this week‚Äôs assignment.\nA key point for the lookup using hash functions is the calculation of the hash key or bucket id that we assign for a given entry. In this notebook, we will cover:"
  },
  {
    "objectID": "posts/c1w4/lab02.html#basic-hash-tables",
    "href": "posts/c1w4/lab02.html#basic-hash-tables",
    "title": "Hash functions and multiplanes",
    "section": "Basic Hash tables",
    "text": "Basic Hash tables\nHash tables are data structures that allow indexing data to make lookup tasks more efficient. In this part, you will see the implementation of the simplest hash function.\n\nimport numpy as np                # library for array and matrix manipulation\nimport pprint                     # utilities for console printing \nfrom utils_nb import plot_vectors # helper function to plot vectors\nimport matplotlib.pyplot as plt   # visualization library\n\npp = pprint.PrettyPrinter(indent=4) # Instantiate a pretty printer\n\nIn the next cell, we will define a straightforward hash function for integer numbers. The function will receive a list of integer numbers and the desired amount of buckets. The function will produce a hash table stored as a dictionary, where keys contain the hash keys, and the values will provide the hashed elements of the input list.\nThe hash function is just the remainder of the integer division between each element and the desired number of buckets.\n\ndef basic_hash_table(value_l, n_buckets):\n    \n    def hash_function(value, n_buckets):\n        return int(value) % n_buckets\n    \n    hash_table = {i:[] for i in range(n_buckets)} # Initialize all the buckets in the hash table as empty lists\n\n    for value in value_l:\n        hash_value = hash_function(value,n_buckets) # Get the hash key for the given value\n        hash_table[hash_value].append(value) # Add the element to the corresponding bucket\n    \n    return hash_table\n\nNow let‚Äôs see the hash table function in action. The pretty print function (pprint()) will produce a visually appealing output.\n\nvalue_l = [100, 10, 14, 17, 97] # Set of values to hash\nhash_table_example = basic_hash_table(value_l, n_buckets=10)\npp.pprint(hash_table_example)\n\n{   0: [100, 10],\n    1: [],\n    2: [],\n    3: [],\n    4: [14],\n    5: [],\n    6: [],\n    7: [17, 97],\n    8: [],\n    9: []}\n\n\nIn this case, the bucket key must be the rightmost digit of each number."
  },
  {
    "objectID": "posts/c1w4/lab02.html#planes",
    "href": "posts/c1w4/lab02.html#planes",
    "title": "Hash functions and multiplanes",
    "section": "Planes",
    "text": "Planes\nMultiplanes hash functions are other types of hash functions. Multiplanes hash functions are based on the idea of numbering every single region that is formed by the intersection of n planes. In the following code, we show the most basic forms of the multiplanes principle. First, with a single plane:\n\nP = np.array([[1, 1]]) # Define a single plane. \nfig, ax1 = plt.subplots(figsize=(8, 8)) # Create a plot\n\nplot_vectors([P], axes=[2, 2], ax=ax1) # Plot the plane P as a vector\n\n# Plot  random points. \nfor i in range(0, 10):\n        v1 = np.array(np.random.uniform(-2, 2, 2)) # Get a pair of random numbers between -4 and 4 \n        side_of_plane = np.sign(np.dot(P, v1.T)) \n        \n        # Color the points depending on the sign of the result of np.dot(P, point.T)\n        if side_of_plane == 1:\n            ax1.plot([v1[0]], [v1[1]], 'bo') # Plot blue points\n        else:\n            ax1.plot([v1[0]], [v1[1]], 'ro') # Plot red points\n\nplt.show()\n\n\n\n\n\n\n\n\nThe first thing to note is that the vector that defines the plane does not mark the boundary between the two sides of the plane. It marks the direction in which you find the ‚Äòpositive‚Äô side of the plane. Not intuitive at all!\nIf we want to plot the separation plane, we need to plot a line that is perpendicular to our vector P. We can get such a line using a 90^o rotation matrix.\nFeel free to change the direction of the plane P.\n\nP = np.array([[1, 2]])  # Define a single plane. You may change the direction\n\n# Get a new plane perpendicular to P. We use a rotation matrix\nPT = np.dot([[0, 1], [-1, 0]], P.T).T  \n\nfig, ax1 = plt.subplots(figsize=(8, 8)) # Create a plot with custom size\n\nplot_vectors([P], colors=['b'], axes=[2, 2], ax=ax1) # Plot the plane P as a vector\n\n# Plot the plane P as a 2 vectors. \n# We scale by 2 just to get the arrows outside the current box\nplot_vectors([PT * 4, PT * -4], colors=['k', 'k'], axes=[4, 4], ax=ax1)\n\n# Plot 20 random points. \nfor i in range(0, 20):\n        v1 = np.array(np.random.uniform(-4, 4, 2)) # Get a pair of random numbers between -4 and 4 \n        side_of_plane = np.sign(np.dot(P, v1.T)) # Get the sign of the dot product with P\n        # Color the points depending on the sign of the result of np.dot(P, point.T)\n        if side_of_plane == 1:\n            ax1.plot([v1[0]], [v1[1]], 'bo') # Plot a blue point\n        else:\n            ax1.plot([v1[0]], [v1[1]], 'ro') # Plot a red point\n\nplt.show()\n\n\n\n\n\n\n\n\nNow, let us see what is inside the code that color the points.\n\nP = np.array([[1, 1]])      # Single plane\nv1 = np.array([[1, 2]])     # Sample point 1\nv2 = np.array([[-1, 1]])    # Sample point 2\nv3 = np.array([[-2, -1]])   # Sample point 3\n\n\nnp.dot(P, v1.T)\n\narray([[3]])\n\n\n\nnp.dot(P, v2.T)\n\narray([[0]])\n\n\n\nnp.dot(P, v3.T)\n\narray([[-3]])\n\n\nThe function below checks in which side of the plane P is located the vector v\n\ndef side_of_plane(P, v):\n    dotproduct = np.dot(P, v.T) # Get the dot product P * v'\n    sign_of_dot_product = np.sign(dotproduct) # The sign of the elements of the dotproduct matrix \n    sign_of_dot_product_scalar = sign_of_dot_product.item() # The value of the first item\n    return sign_of_dot_product_scalar\n\n\nside_of_plane(P, v1) # In which side is [1, 2]\n\n1\n\n\n\nside_of_plane(P, v2) # In which side is [-1, 1]\n\n0\n\n\n\nside_of_plane(P, v3) # In which side is [-2, -1]\n\n-1"
  },
  {
    "objectID": "posts/c1w4/lab02.html#hash-function-with-multiple-planes",
    "href": "posts/c1w4/lab02.html#hash-function-with-multiple-planes",
    "title": "Hash functions and multiplanes",
    "section": "Hash Function with multiple planes",
    "text": "Hash Function with multiple planes\nIn the following section, we are going to define a hash function with a list of three custom planes in 2D.\n\nP1 = np.array([[1, 1]])   # First plane 2D\nP2 = np.array([[-1, 1]])  # Second plane 2D\nP3 = np.array([[-1, -1]]) # Third plane 2D\nP_l = [P1, P2, P3]  # List of arrays. It is the multi plane\n\n# Vector to search\nv = np.array([[2, 2]])\n\nThe next function creates a hash value based on a set of planes. The output value is a combination of the side of the plane where the vector is localized with respect to the collection of planes.\nWe can think of this list of planes as a set of basic hash functions, each of which can produce only 1 or 0 as output.\n\ndef hash_multi_plane(P_l, v):\n    hash_value = 0\n    for i, P in enumerate(P_l):\n        sign = side_of_plane(P,v)\n        hash_i = 1 if sign &gt;=0 else 0\n        hash_value += 2**i * hash_i\n    return hash_value\n\n\nhash_multi_plane(P_l, v) # Find the number of the plane that containes this value\n\n3"
  },
  {
    "objectID": "posts/c1w4/lab02.html#random-planes",
    "href": "posts/c1w4/lab02.html#random-planes",
    "title": "Hash functions and multiplanes",
    "section": "Random Planes",
    "text": "Random Planes\nIn the cell below, we create a set of three random planes\n\nnp.random.seed(0)\nnum_dimensions = 2 # is 300 in assignment\nnum_planes = 3 # is 10 in assignment\nrandom_planes_matrix = np.random.normal(\n                       size=(num_planes,\n                             num_dimensions))\nprint(random_planes_matrix)\n\n[[ 1.76405235  0.40015721]\n [ 0.97873798  2.2408932 ]\n [ 1.86755799 -0.97727788]]\n\n\n\nv = np.array([[2, 2]])\n\nThe next function is similar to the side_of_plane() function, but it evaluates more than a plane each time. The result is an array with the side of the plane of v, for the set of planes P\n\n# Side of the plane function. The result is a matrix\ndef side_of_plane_matrix(P, v):\n    dotproduct = np.dot(P, v.T)\n    sign_of_dot_product = np.sign(dotproduct) # Get a boolean value telling if the value in the cell is positive or negative\n    return sign_of_dot_product\n\nGet the side of the plane of the vector [2, 2] for the set of random planes.\n\nsides_l = side_of_plane_matrix(\n            random_planes_matrix, v)\nsides_l\n\narray([[1.],\n       [1.],\n       [1.]])\n\n\nNow, let us use the former function to define our multiplane hash function\n\ndef hash_multi_plane_matrix(P, v, num_planes):\n    sides_matrix = side_of_plane_matrix(P, v) # Get the side of planes for P and v\n    hash_value = 0\n    for i in range(num_planes):\n        sign = sides_matrix[i].item() # Get the value inside the matrix cell\n        hash_i = 1 if sign &gt;=0 else 0\n        hash_value += 2**i * hash_i # sum 2^i * hash_i\n        \n    return hash_value\n\nPrint the bucket hash for the vector v = [2, 2].\n\nhash_multi_plane_matrix(random_planes_matrix, v, num_planes)\n\n7\n\n\n\nNote\nThis showed you how to make one set of random planes. You will make multiple sets of random planes in order to make the approximate nearest neighbors more accurate."
  },
  {
    "objectID": "posts/c1w4/lab02.html#document-vectors",
    "href": "posts/c1w4/lab02.html#document-vectors",
    "title": "Hash functions and multiplanes",
    "section": "Document vectors",
    "text": "Document vectors\nBefore we finish this lab, remember that you can represent a document as a vector by adding up the word vectors for the words inside the document. In this example, our embedding contains only three words, each represented by a 3D array.\n\nword_embedding = {\"I\": np.array([1,0,1]),\n                   \"love\": np.array([-1,0,1]),\n                   \"learning\": np.array([1,0,1])\n                  }\nwords_in_document = ['I', 'love', 'learning', 'not_a_word']\ndocument_embedding = np.array([0,0,0])\nfor word in words_in_document:\n    document_embedding += word_embedding.get(word,0)\n    \nprint(document_embedding)\n\n[1 0 3]\n\n\nCongratulations! You‚Äôve now completed this lab on hash functions and multiplanes!"
  },
  {
    "objectID": "posts/c3w1/assignment.html",
    "href": "posts/c3w1/assignment.html",
    "title": "Assignment 1: Sentiment with Deep Neural Networks",
    "section": "",
    "text": "course banner\nWelcome to the first assignment of course 3. In this assignment, you will explore sentiment analysis using deep neural networks."
  },
  {
    "objectID": "posts/c3w1/assignment.html#outline",
    "href": "posts/c3w1/assignment.html#outline",
    "title": "Assignment 1: Sentiment with Deep Neural Networks",
    "section": "Outline",
    "text": "Outline\n\nPart 1: Import libraries and try out Trax\nPart 2: Importing the data\n\n2.1 Loading in the data\n2.2 Building the vocabulary\n2.3 Converting a tweet to a tensor\n\nExercise 01\n\n2.4 Creating a batch generator\n\nExercise 02\n\n\nPart 3: Defining classes\n\n3.1 ReLU class\n\nExercise 03\n\n3.2 Dense class\n\nExercise 04\n\n3.3 Model\n\nExercise 05\n\n\nPart 4: Training\n\n4.1 Training the model\n\nExercise 06\n\n4.2 Practice Making a prediction\n\nPart 5: Evaluation\n\n5.1 Computing the accuracy on a batch\n\nExercise 07\n\n5.2 Testing your model on Validation Data\n\nExercise 08\n\n\nPart 6: Testing with your own input\n\nIn course 1, you implemented Logistic regression and Naive Bayes for sentiment analysis. However if you were to give your old models an example like:\n\nThis movie was almost good.\n\nYour model would have predicted a positive sentiment for that review. However, that sentence has a negative sentiment and indicates that the movie was not good. To solve those kinds of misclassifications, you will write a program that uses deep neural networks to identify sentiment in text. By completing this assignment, you will:\n\nUnderstand how you can build/design a model using layers\nTrain a model using a training loop\nUse a binary cross-entropy loss function\nCompute the accuracy of your model\nPredict using your own input\n\nAs you can tell, this model follows a similar structure to the one you previously implemented in the second course of this specialization. - Indeed most of the deep nets you will be implementing will have a similar structure. The only thing that changes is the model architecture, the inputs, and the outputs. Before starting the assignment, we will introduce you to the Google library trax that we use for building and training models.\nNow we will show you how to compute the gradient of a certain function f by just using .grad(f).\n\nTrax source code can be found on Github: Trax\nThe Trax code also uses the JAX library: JAX"
  },
  {
    "objectID": "posts/c3w1/assignment.html#2.1",
    "href": "posts/c3w1/assignment.html#2.1",
    "title": "Assignment 1: Sentiment with Deep Neural Networks",
    "section": "2.1 Loading in the data",
    "text": "2.1 Loading in the data\nImport the data set.\n- You may recognize this from earlier assignments in the specialization. - Details of process_tweet function are available in utils.py file\n\n## DO NOT EDIT THIS CELL\n\n# Import functions from the utils.py file\n\nimport numpy as np\n\n# Load positive and negative tweets\nall_positive_tweets, all_negative_tweets = load_tweets()\n\n# View the total number of positive and negative tweets.\nprint(f\"The number of positive tweets: {len(all_positive_tweets)}\")\nprint(f\"The number of negative tweets: {len(all_negative_tweets)}\")\n\n# Split positive set into validation and training\nval_pos   = all_positive_tweets[4000:] # generating validation set for positive tweets\ntrain_pos  = all_positive_tweets[:4000]# generating training set for positive tweets\n\n# Split negative set into validation and training\nval_neg   = all_negative_tweets[4000:] # generating validation set for negative tweets\ntrain_neg  = all_negative_tweets[:4000] # generating training set for nagative tweets\n\n# Combine training data into one set\ntrain_x = train_pos + train_neg \n\n# Combine validation data into one set\nval_x  = val_pos + val_neg\n\n# Set the labels for the training set (1 for positive, 0 for negative)\ntrain_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n\n# Set the labels for the validation set (1 for positive, 0 for negative)\nval_y  = np.append(np.ones(len(val_pos)), np.zeros(len(val_neg)))\n\nprint(f\"length of train_x {len(train_x)}\")\nprint(f\"length of val_x {len(val_x)}\")\n\nThe number of positive tweets: 5000\nThe number of negative tweets: 5000\nlength of train_x 8000\nlength of val_x 2000\n\n\nNow import a function that processes tweets (we‚Äôve provided this in the utils.py file). - `process_tweets‚Äô removes unwanted characters e.g.¬†hashtag, hyperlinks, stock tickers from tweet. - It also returns a list of words (it tokenizes the original string).\n\n# Import a function that processes the tweets\n# from utils import process_tweet\n\n# Try out function that processes tweets\nprint(\"original tweet at training position 0\")\nprint(train_pos[0])\n\nprint(\"Tweet at training position 0 after processing:\")\nprocess_tweet(train_pos[0])\n\noriginal tweet at training position 0\n#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\nTweet at training position 0 after processing:\n\n\n['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n\n\nNotice that the function process_tweet keeps key words, removes the hash # symbol, and ignores usernames (words that begin with ‚Äò@‚Äô). It also returns a list of the words."
  },
  {
    "objectID": "posts/c3w1/assignment.html#2.2",
    "href": "posts/c3w1/assignment.html#2.2",
    "title": "Assignment 1: Sentiment with Deep Neural Networks",
    "section": "2.2 Building the vocabulary",
    "text": "2.2 Building the vocabulary\nNow build the vocabulary. - Map each word in each tweet to an integer (an ‚Äúindex‚Äù). - The following code does this for you, but please read it and understand what it‚Äôs doing. - Note that you will build the vocabulary based on the training data. - To do so, you will assign an index to everyword by iterating over your training set.\nThe vocabulary will also include some special tokens - __PAD__: padding - &lt;/e&gt;: end of line - __UNK__: a token representing any word that is not in the vocabulary.\n\n# Build the vocabulary\n# Unit Test Note - There is no test set here only train/val\n\n# Include special tokens \n# started with pad, end of line and unk tokens\nVocab = {'__PAD__': 0, '__&lt;/e&gt;__': 1, '__UNK__': 2} \n\n# Note that we build vocab using training data\nfor tweet in train_x: \n    processed_tweet = process_tweet(tweet)\n    for word in processed_tweet:\n        if word not in Vocab: \n            Vocab[word] = len(Vocab)\n    \nprint(\"Total words in vocab are\",len(Vocab))\ndisplay(Vocab)\n\nTotal words in vocab are 9088\n\n\n{'__PAD__': 0,\n '__&lt;/e&gt;__': 1,\n '__UNK__': 2,\n 'followfriday': 3,\n 'top': 4,\n 'engag': 5,\n 'member': 6,\n 'commun': 7,\n 'week': 8,\n ':)': 9,\n 'hey': 10,\n 'jame': 11,\n 'odd': 12,\n ':/': 13,\n 'pleas': 14,\n 'call': 15,\n 'contact': 16,\n 'centr': 17,\n '02392441234': 18,\n 'abl': 19,\n 'assist': 20,\n 'mani': 21,\n 'thank': 22,\n 'listen': 23,\n 'last': 24,\n 'night': 25,\n 'bleed': 26,\n 'amaz': 27,\n 'track': 28,\n 'scotland': 29,\n 'congrat': 30,\n 'yeaaah': 31,\n 'yipppi': 32,\n 'accnt': 33,\n 'verifi': 34,\n 'rqst': 35,\n 'succeed': 36,\n 'got': 37,\n 'blue': 38,\n 'tick': 39,\n 'mark': 40,\n 'fb': 41,\n 'profil': 42,\n '15': 43,\n 'day': 44,\n 'one': 45,\n 'irresist': 46,\n 'flipkartfashionfriday': 47,\n 'like': 48,\n 'keep': 49,\n 'love': 50,\n 'custom': 51,\n 'wait': 52,\n 'long': 53,\n 'hope': 54,\n 'enjoy': 55,\n 'happi': 56,\n 'friday': 57,\n 'lwwf': 58,\n 'second': 59,\n 'thought': 60,\n '‚Äô': 61,\n 'enough': 62,\n 'time': 63,\n 'dd': 64,\n 'new': 65,\n 'short': 66,\n 'enter': 67,\n 'system': 68,\n 'sheep': 69,\n 'must': 70,\n 'buy': 71,\n 'jgh': 72,\n 'go': 73,\n 'bayan': 74,\n ':d': 75,\n 'bye': 76,\n 'act': 77,\n 'mischiev': 78,\n 'etl': 79,\n 'layer': 80,\n 'in-hous': 81,\n 'wareh': 82,\n 'app': 83,\n 'katamari': 84,\n 'well': 85,\n '‚Ä¶': 86,\n 'name': 87,\n 'impli': 88,\n ':p': 89,\n 'influenc': 90,\n 'big': 91,\n '...': 92,\n 'juici': 93,\n 'selfi': 94,\n 'follow': 95,\n 'perfect': 96,\n 'alreadi': 97,\n 'know': 98,\n \"what'\": 99,\n 'great': 100,\n 'opportun': 101,\n 'junior': 102,\n 'triathlet': 103,\n 'age': 104,\n '12': 105,\n '13': 106,\n 'gatorad': 107,\n 'seri': 108,\n 'get': 109,\n 'entri': 110,\n 'lay': 111,\n 'greet': 112,\n 'card': 113,\n 'rang': 114,\n 'print': 115,\n 'today': 116,\n 'job': 117,\n ':-)': 118,\n \"friend'\": 119,\n 'lunch': 120,\n 'yummm': 121,\n 'nostalgia': 122,\n 'tb': 123,\n 'ku': 124,\n 'id': 125,\n 'conflict': 126,\n 'help': 127,\n \"here'\": 128,\n 'screenshot': 129,\n 'work': 130,\n 'hi': 131,\n 'liv': 132,\n 'hello': 133,\n 'need': 134,\n 'someth': 135,\n 'u': 136,\n 'fm': 137,\n 'twitter': 138,\n '‚Äî': 139,\n 'sure': 140,\n 'thing': 141,\n 'dm': 142,\n 'x': 143,\n \"i'v\": 144,\n 'heard': 145,\n 'four': 146,\n 'season': 147,\n 'pretti': 148,\n 'dope': 149,\n 'penthous': 150,\n 'obv': 151,\n 'gobigorgohom': 152,\n 'fun': 153,\n \"y'all\": 154,\n 'yeah': 155,\n 'suppos': 156,\n 'lol': 157,\n 'chat': 158,\n 'bit': 159,\n 'youth': 160,\n 'üíÖüèΩ': 161,\n 'üíã': 162,\n 'seen': 163,\n 'year': 164,\n 'rest': 165,\n 'goe': 166,\n 'quickli': 167,\n 'bed': 168,\n 'music': 169,\n 'fix': 170,\n 'dream': 171,\n 'spiritu': 172,\n 'ritual': 173,\n 'festiv': 174,\n 'n√©pal': 175,\n 'begin': 176,\n 'line-up': 177,\n 'left': 178,\n 'see': 179,\n 'sarah': 180,\n 'send': 181,\n 'us': 182,\n 'email': 183,\n 'bitsy@bitdefender.com': 184,\n \"we'll\": 185,\n 'asap': 186,\n 'kik': 187,\n 'hatessuc': 188,\n '32429': 189,\n 'kikm': 190,\n 'lgbt': 191,\n 'tinder': 192,\n 'nsfw': 193,\n 'akua': 194,\n 'cumshot': 195,\n 'come': 196,\n 'hous': 197,\n 'nsn_supplement': 198,\n 'effect': 199,\n 'press': 200,\n 'releas': 201,\n 'distribut': 202,\n 'result': 203,\n 'link': 204,\n 'remov': 205,\n 'pressreleas': 206,\n 'newsdistribut': 207,\n 'bam': 208,\n 'bestfriend': 209,\n 'lot': 210,\n 'warsaw': 211,\n '&lt;3': 212,\n 'x46': 213,\n 'everyon': 214,\n 'watch': 215,\n 'documentari': 216,\n 'earthl': 217,\n 'youtub': 218,\n 'support': 219,\n 'buuut': 220,\n 'oh': 221,\n 'look': 222,\n 'forward': 223,\n 'visit': 224,\n 'next': 225,\n 'letsgetmessi': 226,\n 'jo': 227,\n 'make': 228,\n 'feel': 229,\n 'better': 230,\n 'never': 231,\n 'anyon': 232,\n 'kpop': 233,\n 'flesh': 234,\n 'good': 235,\n 'girl': 236,\n 'best': 237,\n 'wish': 238,\n 'reason': 239,\n 'epic': 240,\n 'soundtrack': 241,\n 'shout': 242,\n 'ad': 243,\n 'video': 244,\n 'playlist': 245,\n 'would': 246,\n 'dear': 247,\n 'jordan': 248,\n 'okay': 249,\n 'fake': 250,\n 'gameplay': 251,\n ';)': 252,\n 'haha': 253,\n 'im': 254,\n 'kid': 255,\n 'stuff': 256,\n 'exactli': 257,\n 'product': 258,\n 'line': 259,\n 'etsi': 260,\n 'shop': 261,\n 'check': 262,\n 'vacat': 263,\n 'recharg': 264,\n 'normal': 265,\n 'charger': 266,\n 'asleep': 267,\n 'talk': 268,\n 'sooo': 269,\n 'someon': 270,\n 'text': 271,\n 'ye': 272,\n 'bet': 273,\n \"he'll\": 274,\n 'fit': 275,\n 'hear': 276,\n 'speech': 277,\n 'piti': 278,\n 'green': 279,\n 'garden': 280,\n 'midnight': 281,\n 'sun': 282,\n 'beauti': 283,\n 'canal': 284,\n 'dasvidaniya': 285,\n 'till': 286,\n 'scout': 287,\n 'sg': 288,\n 'futur': 289,\n 'wlan': 290,\n 'pro': 291,\n 'confer': 292,\n 'asia': 293,\n 'chang': 294,\n 'lollipop': 295,\n 'üç≠': 296,\n 'nez': 297,\n 'agnezmo': 298,\n 'oley': 299,\n 'mama': 300,\n 'stand': 301,\n 'stronger': 302,\n 'god': 303,\n 'misti': 304,\n 'babi': 305,\n 'cute': 306,\n 'woohoo': 307,\n \"can't\": 308,\n 'sign': 309,\n 'yet': 310,\n 'still': 311,\n 'think': 312,\n 'mka': 313,\n 'liam': 314,\n 'access': 315,\n 'welcom': 316,\n 'stat': 317,\n 'arriv': 318,\n '1': 319,\n 'unfollow': 320,\n 'via': 321,\n 'surpris': 322,\n 'figur': 323,\n 'happybirthdayemilybett': 324,\n 'sweet': 325,\n 'talent': 326,\n '2': 327,\n 'plan': 328,\n 'drain': 329,\n 'gotta': 330,\n 'timezon': 331,\n 'parent': 332,\n 'proud': 333,\n 'least': 334,\n 'mayb': 335,\n 'sometim': 336,\n 'grade': 337,\n 'al': 338,\n 'grand': 339,\n 'manila_bro': 340,\n 'chosen': 341,\n 'let': 342,\n 'around': 343,\n '..': 344,\n 'side': 345,\n 'world': 346,\n 'eh': 347,\n 'take': 348,\n 'care': 349,\n 'final': 350,\n 'fuck': 351,\n 'weekend': 352,\n 'real': 353,\n 'x45': 354,\n 'join': 355,\n 'hushedcallwithfraydo': 356,\n 'gift': 357,\n 'yeahhh': 358,\n 'hushedpinwithsammi': 359,\n 'event': 360,\n 'might': 361,\n 'luv': 362,\n 'realli': 363,\n 'appreci': 364,\n 'share': 365,\n 'wow': 366,\n 'tom': 367,\n 'gym': 368,\n 'monday': 369,\n 'invit': 370,\n 'scope': 371,\n 'friend': 372,\n 'nude': 373,\n 'sleep': 374,\n 'birthday': 375,\n 'want': 376,\n 't-shirt': 377,\n 'cool': 378,\n 'haw': 379,\n 'phela': 380,\n 'mom': 381,\n 'obvious': 382,\n 'princ': 383,\n 'charm': 384,\n 'stage': 385,\n 'luck': 386,\n 'tyler': 387,\n 'hipster': 388,\n 'glass': 389,\n 'marti': 390,\n 'glad': 391,\n 'done': 392,\n 'afternoon': 393,\n 'read': 394,\n 'kahfi': 395,\n 'finish': 396,\n 'ohmyg': 397,\n 'yaya': 398,\n 'dub': 399,\n 'stalk': 400,\n 'ig': 401,\n 'gondooo': 402,\n 'moo': 403,\n 'tologooo': 404,\n 'becom': 405,\n 'detail': 406,\n 'zzz': 407,\n 'xx': 408,\n 'physiotherapi': 409,\n 'hashtag': 410,\n 'üí™': 411,\n 'monica': 412,\n 'miss': 413,\n 'sound': 414,\n 'morn': 415,\n \"that'\": 416,\n 'x43': 417,\n 'definit': 418,\n 'tri': 419,\n 'tonight': 420,\n 'took': 421,\n 'advic': 422,\n 'treviso': 423,\n 'concert': 424,\n 'citi': 425,\n 'countri': 426,\n \"i'll\": 427,\n 'start': 428,\n 'fine': 429,\n 'gorgeou': 430,\n 'xo': 431,\n 'oven': 432,\n 'roast': 433,\n 'garlic': 434,\n 'oliv': 435,\n 'oil': 436,\n 'dri': 437,\n 'tomato': 438,\n 'basil': 439,\n 'centuri': 440,\n 'tuna': 441,\n 'right': 442,\n 'back': 443,\n 'atchya': 444,\n 'even': 445,\n 'almost': 446,\n 'chanc': 447,\n 'cheer': 448,\n 'po': 449,\n 'ice': 450,\n 'cream': 451,\n 'agre': 452,\n '100': 453,\n 'heheheh': 454,\n 'that': 455,\n 'point': 456,\n 'stay': 457,\n 'home': 458,\n 'soon': 459,\n 'promis': 460,\n 'web': 461,\n 'whatsapp': 462,\n 'volta': 463,\n 'funcionar': 464,\n 'com': 465,\n 'iphon': 466,\n 'jailbroken': 467,\n 'later': 468,\n '34': 469,\n 'min': 470,\n 'leia': 471,\n 'appear': 472,\n 'hologram': 473,\n 'r2d2': 474,\n 'w': 475,\n 'messag': 476,\n 'obi': 477,\n 'wan': 478,\n 'sit': 479,\n 'luke': 480,\n 'inter': 481,\n '3': 482,\n 'ucl': 483,\n 'arsen': 484,\n 'small': 485,\n 'team': 486,\n 'pass': 487,\n 'üöÇ': 488,\n 'dewsburi': 489,\n 'railway': 490,\n 'station': 491,\n 'dew': 492,\n 'west': 493,\n 'yorkshir': 494,\n '430': 495,\n 'smh': 496,\n '9:25': 497,\n 'live': 498,\n 'strang': 499,\n 'imagin': 500,\n 'megan': 501,\n 'masaantoday': 502,\n 'a4': 503,\n 'shweta': 504,\n 'tripathi': 505,\n '5': 506,\n '20': 507,\n 'kurta': 508,\n 'half': 509,\n 'number': 510,\n 'wsalelov': 511,\n 'ah': 512,\n 'larri': 513,\n 'anyway': 514,\n 'kinda': 515,\n 'goood': 516,\n 'life': 517,\n 'enn': 518,\n 'could': 519,\n 'warmup': 520,\n '15th': 521,\n 'bath': 522,\n 'dum': 523,\n 'andar': 524,\n 'ram': 525,\n 'sampath': 526,\n 'sona': 527,\n 'mohapatra': 528,\n 'samantha': 529,\n 'edward': 530,\n 'mein': 531,\n 'tulan': 532,\n 'razi': 533,\n 'wah': 534,\n 'josh': 535,\n 'alway': 536,\n 'smile': 537,\n 'pictur': 538,\n '16.20': 539,\n 'giveitup': 540,\n 'given': 541,\n 'ga': 542,\n 'subsidi': 543,\n 'initi': 544,\n 'propos': 545,\n 'delight': 546,\n 'yesterday': 547,\n 'x42': 548,\n 'lmaoo': 549,\n 'song': 550,\n 'ever': 551,\n 'shall': 552,\n 'littl': 553,\n 'throwback': 554,\n 'outli': 555,\n 'island': 556,\n 'cheung': 557,\n 'chau': 558,\n 'mui': 559,\n 'wo': 560,\n 'total': 561,\n 'differ': 562,\n 'kfckitchentour': 563,\n 'kitchen': 564,\n 'clean': 565,\n \"i'm\": 566,\n 'cusp': 567,\n 'test': 568,\n 'water': 569,\n 'reward': 570,\n 'arummzz': 571,\n \"let'\": 572,\n 'drive': 573,\n 'travel': 574,\n 'yogyakarta': 575,\n 'jeep': 576,\n 'indonesia': 577,\n 'instamood': 578,\n 'wanna': 579,\n 'skype': 580,\n 'may': 581,\n 'nice': 582,\n 'friendli': 583,\n 'pretend': 584,\n 'film': 585,\n 'congratul': 586,\n 'winner': 587,\n 'cheesydelight': 588,\n 'contest': 589,\n 'address': 590,\n 'guy': 591,\n 'market': 592,\n '24/7': 593,\n '14': 594,\n 'hour': 595,\n 'leav': 596,\n 'without': 597,\n 'delay': 598,\n 'actual': 599,\n 'easi': 600,\n 'guess': 601,\n 'train': 602,\n 'wd': 603,\n 'shift': 604,\n 'engin': 605,\n 'etc': 606,\n 'sunburn': 607,\n 'peel': 608,\n 'blog': 609,\n 'huge': 610,\n 'warm': 611,\n '‚òÜ': 612,\n 'complet': 613,\n 'triangl': 614,\n 'northern': 615,\n 'ireland': 616,\n 'sight': 617,\n 'smthng': 618,\n 'fr': 619,\n 'hug': 620,\n 'xoxo': 621,\n 'uu': 622,\n 'jaann': 623,\n 'topnewfollow': 624,\n 'connect': 625,\n 'wonder': 626,\n 'made': 627,\n 'fluffi': 628,\n 'insid': 629,\n 'pirouett': 630,\n 'moos': 631,\n 'trip': 632,\n 'philli': 633,\n 'decemb': 634,\n \"i'd\": 635,\n 'dude': 636,\n 'x41': 637,\n 'question': 638,\n 'flaw': 639,\n 'pain': 640,\n 'negat': 641,\n 'strength': 642,\n 'went': 643,\n 'solo': 644,\n 'move': 645,\n 'fav': 646,\n 'nirvana': 647,\n 'smell': 648,\n 'teen': 649,\n 'spirit': 650,\n 'rip': 651,\n 'ami': 652,\n 'winehous': 653,\n 'coupl': 654,\n 'tomhiddleston': 655,\n 'elizabetholsen': 656,\n 'yaytheylookgreat': 657,\n 'goodnight': 658,\n 'vid': 659,\n 'wake': 660,\n 'gonna': 661,\n 'shoot': 662,\n 'itti': 663,\n 'bitti': 664,\n 'teeni': 665,\n 'bikini': 666,\n 'much': 667,\n '4th': 668,\n 'togeth': 669,\n 'end': 670,\n 'xfile': 671,\n 'content': 672,\n 'rain': 673,\n 'fabul': 674,\n 'fantast': 675,\n '‚ô°': 676,\n 'jb': 677,\n 'forev': 678,\n 'belieb': 679,\n 'nighti': 680,\n 'bug': 681,\n 'bite': 682,\n 'bracelet': 683,\n 'idea': 684,\n 'foundri': 685,\n 'game': 686,\n 'sens': 687,\n 'pic': 688,\n 'ef': 689,\n 'phone': 690,\n 'woot': 691,\n 'derek': 692,\n 'use': 693,\n 'parkshar': 694,\n 'gloucestershir': 695,\n 'aaaahhh': 696,\n 'man': 697,\n 'traffic': 698,\n 'stress': 699,\n 'reliev': 700,\n \"how'r\": 701,\n 'arbeloa': 702,\n 'turn': 703,\n '17': 704,\n 'omg': 705,\n 'say': 706,\n 'europ': 707,\n 'rise': 708,\n 'find': 709,\n 'hard': 710,\n 'believ': 711,\n 'uncount': 712,\n 'coz': 713,\n 'unlimit': 714,\n 'cours': 715,\n 'teamposit': 716,\n 'aldub': 717,\n '‚òï': 718,\n 'rita': 719,\n 'info': 720,\n \"we'd\": 721,\n 'way': 722,\n 'boy': 723,\n 'x40': 724,\n 'true': 725,\n 'sethi': 726,\n 'high': 727,\n 'exe': 728,\n 'skeem': 729,\n 'saam': 730,\n 'peopl': 731,\n 'polit': 732,\n 'izzat': 733,\n 'wese': 734,\n 'trust': 735,\n 'khawateen': 736,\n 'k': 737,\n 'sath': 738,\n 'mana': 739,\n 'kar': 740,\n 'deya': 741,\n 'sort': 742,\n 'smart': 743,\n 'hair': 744,\n 'tbh': 745,\n 'jacob': 746,\n 'g': 747,\n 'upgrad': 748,\n 'tee': 749,\n 'famili': 750,\n 'person': 751,\n 'two': 752,\n 'convers': 753,\n 'onlin': 754,\n 'mclaren': 755,\n 'fridayfeel': 756,\n 'tgif': 757,\n 'squar': 758,\n 'enix': 759,\n 'bissmillah': 760,\n 'ya': 761,\n 'allah': 762,\n \"we'r\": 763,\n 'socent': 764,\n 'startup': 765,\n 'drop': 766,\n 'your': 767,\n 'arnd': 768,\n 'town': 769,\n 'basic': 770,\n 'piss': 771,\n 'cup': 772,\n 'also': 773,\n 'terribl': 774,\n 'complic': 775,\n 'discuss': 776,\n 'snapchat': 777,\n 'lynettelow': 778,\n 'kikmenow': 779,\n 'snapm': 780,\n 'hot': 781,\n 'amazon': 782,\n 'kikmeguy': 783,\n 'defin': 784,\n 'grow': 785,\n 'sport': 786,\n 'rt': 787,\n 'rakyat': 788,\n 'write': 789,\n 'sinc': 790,\n 'mention': 791,\n 'fli': 792,\n 'fish': 793,\n 'promot': 794,\n 'post': 795,\n 'cyber': 796,\n 'ourdaughtersourprid': 797,\n 'mypapamyprid': 798,\n 'papa': 799,\n 'coach': 800,\n 'posit': 801,\n 'kha': 802,\n 'atleast': 803,\n 'x39': 804,\n 'mango': 805,\n \"lassi'\": 806,\n \"monty'\": 807,\n 'marvel': 808,\n 'though': 809,\n 'suspect': 810,\n 'meant': 811,\n '24': 812,\n 'hr': 813,\n 'touch': 814,\n 'kepler': 815,\n '452b': 816,\n 'chalna': 817,\n 'hai': 818,\n 'thankyou': 819,\n 'hazel': 820,\n 'food': 821,\n 'brooklyn': 822,\n 'pta': 823,\n 'awak': 824,\n 'okayi': 825,\n 'awww': 826,\n 'ha': 827,\n 'doc': 828,\n 'splendid': 829,\n 'spam': 830,\n 'folder': 831,\n 'amount': 832,\n 'nigeria': 833,\n 'claim': 834,\n 'rted': 835,\n 'leg': 836,\n 'hurt': 837,\n 'bad': 838,\n 'mine': 839,\n 'saturday': 840,\n 'thaaank': 841,\n 'puhon': 842,\n 'happinesss': 843,\n 'tnc': 844,\n 'prior': 845,\n 'notif': 846,\n 'fat': 847,\n 'co': 848,\n 'probabl': 849,\n 'ate': 850,\n 'yuna': 851,\n 'tamesid': 852,\n '¬¥': 853,\n 'googl': 854,\n 'account': 855,\n 'scouser': 856,\n 'everyth': 857,\n 'zoe': 858,\n 'mate': 859,\n 'liter': 860,\n \"they'r\": 861,\n 'samee': 862,\n 'edgar': 863,\n 'updat': 864,\n 'log': 865,\n 'bring': 866,\n 'abe': 867,\n 'meet': 868,\n 'x38': 869,\n 'sigh': 870,\n 'dreamili': 871,\n 'pout': 872,\n 'eye': 873,\n 'quacketyquack': 874,\n 'funni': 875,\n 'happen': 876,\n 'phil': 877,\n 'em': 878,\n 'del': 879,\n 'rodder': 880,\n 'els': 881,\n 'play': 882,\n 'newest': 883,\n 'gamejam': 884,\n 'irish': 885,\n 'literatur': 886,\n 'inaccess': 887,\n \"kareena'\": 888,\n 'fan': 889,\n 'brain': 890,\n 'dot': 891,\n 'braindot': 892,\n 'fair': 893,\n 'rush': 894,\n 'either': 895,\n 'brandi': 896,\n '18': 897,\n 'carniv': 898,\n 'men': 899,\n 'put': 900,\n 'mask': 901,\n 'xavier': 902,\n 'forneret': 903,\n 'jennif': 904,\n 'site': 905,\n 'free': 906,\n '50.000': 907,\n '8': 908,\n 'ball': 909,\n 'pool': 910,\n 'coin': 911,\n 'edit': 912,\n 'trish': 913,\n '‚ô•': 914,\n 'grate': 915,\n 'three': 916,\n 'comment': 917,\n 'wakeup': 918,\n 'besid': 919,\n 'dirti': 920,\n 'sex': 921,\n 'lmaooo': 922,\n 'üò§': 923,\n 'loui': 924,\n \"he'\": 925,\n 'throw': 926,\n 'caus': 927,\n 'inspir': 928,\n 'ff': 929,\n 'twoof': 930,\n 'gr8': 931,\n 'wkend': 932,\n 'kind': 933,\n 'exhaust': 934,\n 'word': 935,\n 'cheltenham': 936,\n 'area': 937,\n 'kale': 938,\n 'crisp': 939,\n 'ruin': 940,\n 'x37': 941,\n 'open': 942,\n 'worldwid': 943,\n 'outta': 944,\n 'sfvbeta': 945,\n 'vantast': 946,\n 'xcylin': 947,\n 'bundl': 948,\n 'show': 949,\n 'internet': 950,\n 'price': 951,\n 'realisticli': 952,\n 'pay': 953,\n 'net': 954,\n 'educ': 955,\n 'power': 956,\n 'weapon': 957,\n 'nelson': 958,\n 'mandela': 959,\n 'recent': 960,\n 'j': 961,\n 'chenab': 962,\n 'flow': 963,\n 'pakistan': 964,\n 'incredibleindia': 965,\n 'teenchoic': 966,\n 'choiceinternationalartist': 967,\n 'superjunior': 968,\n 'caught': 969,\n 'first': 970,\n 'salmon': 971,\n 'super-blend': 972,\n 'project': 973,\n 'youth@bipolaruk.org.uk': 974,\n 'awesom': 975,\n 'stream': 976,\n 'alma': 977,\n 'mater': 978,\n 'highschoolday': 979,\n 'clientvisit': 980,\n 'faith': 981,\n 'christian': 982,\n 'school': 983,\n 'lizaminnelli': 984,\n 'upcom': 985,\n 'uk': 986,\n 'üòÑ': 987,\n 'singl': 988,\n 'hill': 989,\n 'everi': 990,\n 'beat': 991,\n 'wrong': 992,\n 'readi': 993,\n 'natur': 994,\n 'pefumeri': 995,\n 'workshop': 996,\n 'neal': 997,\n 'yard': 998,\n 'covent': 999,\n ...}\n\n\nThe dictionary Vocab will look like this:\n{'__PAD__': 0,\n '__&lt;/e&gt;__': 1,\n '__UNK__': 2,\n 'followfriday': 3,\n 'top': 4,\n 'engag': 5,\n ...\n\nEach unique word has a unique integer associated with it.\nThe total number of words in Vocab: 9088"
  },
  {
    "objectID": "posts/c3w1/assignment.html#2.3",
    "href": "posts/c3w1/assignment.html#2.3",
    "title": "Assignment 1: Sentiment with Deep Neural Networks",
    "section": "2.3 Converting a tweet to a tensor",
    "text": "2.3 Converting a tweet to a tensor\nWrite a function that will convert each tweet to a tensor (a list of unique integer IDs representing the processed tweet). - Note, the returned data type will be a regular Python list() - You won‚Äôt use TensorFlow in this function - You also won‚Äôt use a numpy array - You also won‚Äôt use trax.fastmath.numpy array - For words in the tweet that are not in the vocabulary, set them to the unique ID for the token __UNK__.\n\nExample\nInput a tweet:\n'@happypuppy, is Maria happy?'\nThe tweet_to_tensor will first conver the tweet into a list of tokens (including only relevant words)\n['maria', 'happi']\nThen it will convert each word into its unique integer\n[2, 56]\n\nNotice that the word ‚Äúmaria‚Äù is not in the vocabulary, so it is assigned the unique integer associated with the __UNK__ token, because it is considered ‚Äúunknown.‚Äù\n\n\n\nExercise 01\nInstructions: Write a program tweet_to_tensor that takes in a tweet and converts it to an array of numbers. You can use the Vocab dictionary you just found to help create the tensor.\n\nUse the vocab_dict parameter and not a global variable.\nDo not hard code the integer value for the __UNK__ token.\n\n\n\nHints\n\n\n\n\nMap each word in tweet to corresponding token in ‚ÄòVocab‚Äô\n\n\nUse Python‚Äôs Dictionary.get(key,value) so that the function returns a default value if the key is not found in the dictionary.\n\n\n\n\n\n# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: tweet_to_tensor\ndef tweet_to_tensor(tweet, vocab_dict, unk_token='__UNK__', verbose=False):\n    '''\n    Input: \n        tweet - A string containing a tweet\n        vocab_dict - The words dictionary\n        unk_token - The special string for unknown tokens\n        verbose - Print info durign runtime\n    Output:\n        tensor_l - A python list with\n        \n    '''  \n    \n    ### START CODE HERE (Replace instances of 'None' with your code) ###\n    # Process the tweet into a list of words\n    # where only important words are kept (stop words removed)\n    word_l = None\n    \n    if verbose:\n        print(\"List of words from the processed tweet:\")\n        print(word_l)\n        \n    # Initialize the list that will contain the unique integer IDs of each word\n    tensor_l = []\n    \n    # Get the unique integer ID of the __UNK__ token\n    unk_ID = None\n    \n    if verbose:\n        print(f\"The unique integer ID for the unk_token is {unk_ID}\")\n        \n    # for each word in the list:\n    for word in word_l:\n        \n        # Get the unique integer ID.\n        # If the word doesn't exist in the vocab dictionary,\n        # use the unique ID for __UNK__ instead.\n        word_ID = None\n    ### END CODE HERE ###\n        \n        # Append the unique integer ID to the tensor list.\n        tensor_l.append(word_ID) \n    \n    return tensor_l\n\n\nprint(\"Actual tweet is\\n\", val_pos[0])\nprint(\"\\nTensor of tweet:\\n\", tweet_to_tensor(val_pos[0], vocab_dict=Vocab))\n\nActual tweet is\n Bro:U wan cut hair anot,ur hair long Liao bo\nMe:since ord liao,take it easy lor treat as save $ leave it longer :)\nBro:LOL Sibei xialan\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[11], line 2\n      1 print(\"Actual tweet is\\n\", val_pos[0])\n----&gt; 2 print(\"\\nTensor of tweet:\\n\", tweet_to_tensor(val_pos[0], vocab_dict=Vocab))\n\nCell In[10], line 34, in tweet_to_tensor(tweet, vocab_dict, unk_token, verbose)\n     31     print(f\"The unique integer ID for the unk_token is {unk_ID}\")\n     33 # for each word in the list:\n---&gt; 34 for word in word_l:\n     35     \n     36     # Get the unique integer ID.\n     37     # If the word doesn't exist in the vocab dictionary,\n     38     # use the unique ID for __UNK__ instead.\n     39     word_ID = None\n     40 ### END CODE HERE ###\n     41     \n     42     # Append the unique integer ID to the tensor list.\n\nTypeError: 'NoneType' object is not iterable\n\n\n\n\nExpected output\nActual tweet is\n Bro:U wan cut hair anot,ur hair long Liao bo\nMe:since ord liao,take it easy lor treat as save $ leave it longer :)\nBro:LOL Sibei xialan\n\nTensor of tweet:\n [1065, 136, 479, 2351, 745, 8148, 1123, 745, 53, 2, 2672, 791, 2, 2, 349, 601, 2, 3489, 1017, 597, 4559, 9, 1065, 157, 2, 2]\n\n# test tweet_to_tensor\n\ndef test_tweet_to_tensor():\n    test_cases = [\n        \n        {\n            \"name\":\"simple_test_check\",\n            \"input\": [val_pos[1], Vocab],\n            \"expected\":[444, 2, 304, 567, 56, 9],\n            \"error\":\"The function gives bad output for val_pos[1]. Test failed\"\n        },\n        {\n            \"name\":\"datatype_check\",\n            \"input\":[val_pos[1], Vocab],\n            \"expected\":type([]),\n            \"error\":\"Datatype mismatch. Need only list not np.array\"\n        },\n        {\n            \"name\":\"without_unk_check\",\n            \"input\":[val_pos[1], Vocab],\n            \"expected\":6,\n            \"error\":\"Unk word check not done- Please check if you included mapping for unknown word\"\n        }\n    ]\n    count = 0\n    for test_case in test_cases:\n        \n        try:\n            if test_case['name'] == \"simple_test_check\":\n                assert test_case[\"expected\"] == tweet_to_tensor(*test_case['input'])\n                count += 1\n            if test_case['name'] == \"datatype_check\":\n                assert isinstance(tweet_to_tensor(*test_case['input']), test_case[\"expected\"])\n                count += 1\n            if test_case['name'] == \"without_unk_check\":\n                assert None not in tweet_to_tensor(*test_case['input'])\n                count += 1\n                \n            \n            \n        except:\n            print(test_case['error'])\n    if count == 3:\n        print(\"\\033[92m All tests passed\")\n    else:\n        print(count,\" Tests passed out of 3\")\ntest_tweet_to_tensor()            \n\nThe function gives bad output for val_pos[1]. Test failed\nDatatype mismatch. Need only list not np.array\nUnk word check not done- Please check if you included mapping for unknown word\n0  Tests passed out of 3"
  },
  {
    "objectID": "posts/c3w1/assignment.html#2.4",
    "href": "posts/c3w1/assignment.html#2.4",
    "title": "Assignment 1: Sentiment with Deep Neural Networks",
    "section": "2.4 Creating a batch generator",
    "text": "2.4 Creating a batch generator\nMost of the time in Natural Language Processing, and AI in general we use batches when training our data sets. - If instead of training with batches of examples, you were to train a model with one example at a time, it would take a very long time to train the model. - You will now build a data generator that takes in the positive/negative tweets and returns a batch of training examples. It returns the model inputs, the targets (positive or negative labels) and the weight for each target (ex: this allows us to can treat some examples as more important to get right than others, but commonly this will all be 1.0).\nOnce you create the generator, you could include it in a for loop\nfor batch_inputs, batch_targets, batch_example_weights in data_generator:\n    ...\nYou can also get a single batch like this:\nbatch_inputs, batch_targets, batch_example_weights = next(data_generator)\nThe generator returns the next batch each time it‚Äôs called. - This generator returns the data in a format (tensors) that you could directly use in your model. - It returns a triple: the inputs, targets, and loss weights: ‚Äì Inputs is a tensor that contains the batch of tweets we put into the model. ‚Äì Targets is the corresponding batch of labels that we train to generate. ‚Äì Loss weights here are just 1s with same shape as targets. Next week, you will use it to mask input padding.\n\nExercise 02\nImplement data_generator.\n\n# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED: Data generator\ndef data_generator(data_pos, data_neg, batch_size, loop, vocab_dict, shuffle=False):\n    '''\n    Input: \n        data_pos - Set of posstive examples\n        data_neg - Set of negative examples\n        batch_size - number of samples per batch. Must be even\n        loop - True or False\n        vocab_dict - The words dictionary\n        shuffle - Shuffle the data order\n    Yield:\n        inputs - Subset of positive and negative examples\n        targets - The corresponding labels for the subset\n        example_weights - An array specifying the importance of each example\n        \n    '''     \n### START GIVEN CODE ###\n    # make sure the batch size is an even number\n    # to allow an equal number of positive and negative samples\n    assert batch_size % 2 == 0\n    \n    # Number of positive examples in each batch is half of the batch size\n    # same with number of negative examples in each batch\n    n_to_take = batch_size // 2\n    \n    # Use pos_index to walk through the data_pos array\n    # same with neg_index and data_neg\n    pos_index = 0\n    neg_index = 0\n    \n    len_data_pos = len(data_pos)\n    len_data_neg = len(data_neg)\n    \n    # Get and array with the data indexes\n    pos_index_lines = list(range(len_data_pos))\n    neg_index_lines = list(range(len_data_neg))\n    \n    # shuffle lines if shuffle is set to True\n    if shuffle:\n        rnd.shuffle(pos_index_lines)\n        rnd.shuffle(neg_index_lines)\n        \n    stop = False\n    \n    # Loop indefinitely\n    while not stop:  \n        \n        # create a batch with positive and negative examples\n        batch = []\n        \n        # First part: Pack n_to_take positive examples\n        \n        # Start from pos_index and increment i up to n_to_take\n        for i in range(n_to_take):\n                    \n            # If the positive index goes past the positive dataset lenght,\n            if pos_index &gt;= len_data_pos: \n                \n                # If loop is set to False, break once we reach the end of the dataset\n                if not loop:\n                    stop = True;\n                    break;\n                \n                # If user wants to keep re-using the data, reset the index\n                pos_index = 0\n                \n                if shuffle:\n                    # Shuffle the index of the positive sample\n                    rnd.shuffle(pos_index_lines)\n                    \n            # get the tweet as pos_index\n            tweet = data_pos[pos_index_lines[pos_index]]\n            \n            # convert the tweet into tensors of integers representing the processed words\n            tensor = tweet_to_tensor(tweet, vocab_dict)\n            \n            # append the tensor to the batch list\n            batch.append(tensor)\n            \n            # Increment pos_index by one\n            pos_index = pos_index + 1\n\n### END GIVEN CODE ###\n            \n### START CODE HERE (Replace instances of 'None' with your code) ###\n\n        # Second part: Pack n_to_take negative examples\n    \n        # Using the same batch list, start from neg_index and increment i up to n_to_take\n        for i in range(None):\n            \n            # If the negative index goes past the negative dataset length,\n            if None\n                \n                # If loop is set to False, break once we reach the end of the dataset\n                if not loop:\n                    stop = True;\n                    break;\n                    \n                # If user wants to keep re-using the data, reset the index\n                neg_index = None\n                \n                if shuffle:\n                    # Shuffle the index of the negative sample\n                    None\n            # get the tweet as neg_index\n            tweet = None\n            \n            # convert the tweet into tensors of integers representing the processed words\n            tensor = None\n            \n            # append the tensor to the batch list\n            None\n            \n            # Increment neg_index by one\n            neg_index = None\n\n### END CODE HERE ###        \n\n### START GIVEN CODE ###\n        if stop:\n            break;\n\n        # Update the start index for positive data \n        # so that it's n_to_take positions after the current pos_index\n        pos_index += n_to_take\n        \n        # Update the start index for negative data \n        # so that it's n_to_take positions after the current neg_index\n        neg_index += n_to_take\n        \n        # Get the max tweet length (the length of the longest tweet) \n        # (you will pad all shorter tweets to have this length)\n        max_len = max([len(t) for t in batch]) \n        \n        \n        # Initialize the input_l, which will \n        # store the padded versions of the tensors\n        tensor_pad_l = []\n        # Pad shorter tweets with zeros\n        for tensor in batch:\n### END GIVEN CODE ###\n\n### START CODE HERE (Replace instances of 'None' with your code) ###\n            # Get the number of positions to pad for this tensor so that it will be max_len long\n            n_pad = None\n            \n            # Generate a list of zeros, with length n_pad\n            pad_l = None\n            \n            # concatenate the tensor and the list of padded zeros\n            tensor_pad = None\n            \n            # append the padded tensor to the list of padded tensors\n            None\n\n        # convert the list of padded tensors to a numpy array\n        # and store this as the model inputs\n        inputs = None\n  \n        # Generate the list of targets for the positive examples (a list of ones)\n        # The length is the number of positive examples in the batch\n        target_pos = None\n        \n        # Generate the list of targets for the negative examples (a list of zeros)\n        # The length is the number of negative examples in the batch\n        target_neg = None\n        \n        # Concatenate the positve and negative targets\n        target_l = None\n        \n        # Convert the target list into a numpy array\n        targets = None\n\n        # Example weights: Treat all examples equally importantly.It should return an np.array. Hint: Use np.ones_like()\n        example_weights = None\n        \n\n### END CODE HERE ###\n\n### GIVEN CODE ###\n        # note we use yield and not return\n        yield inputs, targets, example_weights\n\n\n  Cell In[13], line 94\n    if None\n           ^\nSyntaxError: expected ':'\n\n\n\n\nNow you can use your data generator to create a data generator for the training data, and another data generator for the validation data.\nWe will create a third data generator that does not loop, for testing the final accuracy of the model.\n\n# Set the random number generator for the shuffle procedure\nrnd.seed(30) \n\n# Create the training data generator\ndef train_generator(batch_size, shuffle = False):\n    return data_generator(train_pos, train_neg, batch_size, True, Vocab, shuffle)\n\n# Create the validation data generator\ndef val_generator(batch_size, shuffle = False):\n    return data_generator(val_pos, val_neg, batch_size, True, Vocab, shuffle)\n\n# Create the validation data generator\ndef test_generator(batch_size, shuffle = False):\n    return data_generator(val_pos, val_neg, batch_size, False, Vocab, shuffle)\n\n# Get a batch from the train_generator and inspect.\ninputs, targets, example_weights = next(train_generator(4, shuffle=True))\n\n# this will print a list of 4 tensors padded with zeros\nprint(f'Inputs: {inputs}')\nprint(f'Targets: {targets}')\nprint(f'Example Weights: {example_weights}')\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[14], line 17\n     14     return data_generator(val_pos, val_neg, batch_size, False, Vocab, shuffle)\n     16 # Get a batch from the train_generator and inspect.\n---&gt; 17 inputs, targets, example_weights = next(train_generator(4, shuffle=True))\n     19 # this will print a list of 4 tensors padded with zeros\n     20 print(f'Inputs: {inputs}')\n\nCell In[14], line 6, in train_generator(batch_size, shuffle)\n      5 def train_generator(batch_size, shuffle = False):\n----&gt; 6     return data_generator(train_pos, train_neg, batch_size, True, Vocab, shuffle)\n\nNameError: name 'data_generator' is not defined\n\n\n\n\n# Test the train_generator\n\n# Create a data generator for training data,\n# which produces batches of size 4 (for tensors and their respective targets)\ntmp_data_gen = train_generator(batch_size = 4)\n\n# Call the data generator to get one batch and its targets\ntmp_inputs, tmp_targets, tmp_example_weights = next(tmp_data_gen)\n\nprint(f\"The inputs shape is {tmp_inputs.shape}\")\nprint(f\"The targets shape is {tmp_targets.shape}\")\nprint(f\"The example weights shape is {tmp_example_weights.shape}\")\n\nfor i,t in enumerate(tmp_inputs):\n    print(f\"input tensor: {t}; target {tmp_targets[i]}; example weights {tmp_example_weights[i]}\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[15], line 5\n      1 # Test the train_generator\n      2 \n      3 # Create a data generator for training data,\n      4 # which produces batches of size 4 (for tensors and their respective targets)\n----&gt; 5 tmp_data_gen = train_generator(batch_size = 4)\n      7 # Call the data generator to get one batch and its targets\n      8 tmp_inputs, tmp_targets, tmp_example_weights = next(tmp_data_gen)\n\nCell In[14], line 6, in train_generator(batch_size, shuffle)\n      5 def train_generator(batch_size, shuffle = False):\n----&gt; 6     return data_generator(train_pos, train_neg, batch_size, True, Vocab, shuffle)\n\nNameError: name 'data_generator' is not defined\n\n\n\n\nExpected output\nThe inputs shape is (4, 14)\nThe targets shape is (4,)\nThe example weights shape is (4,)\ninput tensor: [3 4 5 6 7 8 9 0 0 0 0 0 0 0]; target 1; example weights 1\ninput tensor: [10 11 12 13 14 15 16 17 18 19 20  9 21 22]; target 1; example weights 1\ninput tensor: [5738 2901 3761    0    0    0    0    0    0    0    0    0    0    0]; target 0; example weights 1\ninput tensor: [ 858  256 3652 5739  307 4458  567 1230 2767  328 1202 3761    0    0]; target 0; example weights 1\nNow that you have your train/val generators, you can just call them and they will return tensors which correspond to your tweets in the first column and their corresponding labels in the second column. Now you can go ahead and start building your neural network."
  },
  {
    "objectID": "posts/c3w1/assignment.html#3.1",
    "href": "posts/c3w1/assignment.html#3.1",
    "title": "Assignment 1: Sentiment with Deep Neural Networks",
    "section": "3.1 ReLU class",
    "text": "3.1 ReLU class\nYou will now implement the ReLU activation function in a class below. The ReLU function looks as follows:\n\n\n\nReLU\n\n\n\n\\mathrm{ReLU}(x) = \\mathrm{max}(0,x)\n\n\nExercise 03\nInstructions: Implement the ReLU activation function below. Your function should take in a matrix or vector and it should transform all the negative numbers into 0 while keeping all the positive numbers intact.\n\n\nHints\n\n\n\n\nPlease use numpy.maximum(A,k) to find the maximum between each element in A and a scalar k\n\n\n\n\n\n# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: Relu\nclass Relu(Layer):\n    \"\"\"Relu activation function implementation\"\"\"\n    def forward(self, x):\n        '''\n        Input: \n            - x (a numpy array): the input\n        Output:\n            - activation (numpy array): all positive or 0 version of x\n        '''\n        ### START CODE HERE (Replace instances of 'None' with your code) ###\n        \n        activation = None\n\n        ### END CODE HERE ###\n        \n        return activation\n\n\n# Test your relu function\nx = np.array([[-2.0, -1.0, 0.0], [0.0, 1.0, 2.0]], dtype=float)\nrelu_layer = Relu()\nprint(\"Test data is:\")\nprint(x)\nprint(\"Output of Relu is:\")\nprint(relu_layer(x))\n\nTest data is:\n[[-2. -1.  0.]\n [ 0.  1.  2.]]\nOutput of Relu is:\nNone\n\n\n\nExpected Outout\nTest data is:\n[[-2. -1.  0.]\n [ 0.  1.  2.]]\nOutput of Relu is:\n[[0. 0. 0.]\n [0. 1. 2.]]"
  },
  {
    "objectID": "posts/c3w1/assignment.html#3.2",
    "href": "posts/c3w1/assignment.html#3.2",
    "title": "Assignment 1: Sentiment with Deep Neural Networks",
    "section": "3.2 Dense class",
    "text": "3.2 Dense class\n\nExercise\nImplement the forward function of the Dense class. - The forward function multiplies the input to the layer (x) by the weight matrix (W)\n\\mathrm{forward}(\\mathbf{x},\\mathbf{W}) = \\mathbf{xW} \n\nYou can use numpy.dot to perform the matrix multiplication.\n\nNote that for more efficient code execution, you will use the trax version of math, which includes a trax version of numpy and also random.\nImplement the weight initializer new_weights function - Weights are initialized with a random key. - The second parameter is a tuple for the desired shape of the weights (num_rows, num_cols) - The num of rows for weights should equal the number of columns in x, because for forward propagation, you will multiply x times weights.\nPlease use trax.fastmath.random.normal(key, shape, dtype=tf.float32) to generate random values for the weight matrix. The key difference between this function and the standard numpy randomness is the explicit use of random keys, which need to be passed. While it can look tedious at the first sight to pass the random key everywhere, you will learn in Course 4 why this is very helpful when implementing some advanced models. - key can be generated by calling random.get_prng(seed=) and passing in a number for the seed. - shape is a tuple with the desired shape of the weight matrix. - The number of rows in the weight matrix should equal the number of columns in the variable x. Since x may have 2 dimensions if it reprsents a single training example (row, col), or three dimensions (batch_size, row, col), get the last dimension from the tuple that holds the dimensions of x. - The number of columns in the weight matrix is the number of units chosen for that dense layer. Look at the __init__ function to see which variable stores the number of units. - dtype is the data type of the values in the generated matrix; keep the default of tf.float32. In this case, don‚Äôt explicitly set the dtype (just let it use the default value).\nSet the standard deviation of the random values to 0.1 - The values generated have a mean of 0 and standard deviation of 1. - Set the default standard deviation stdev to be 0.1 by multiplying the standard deviation to each of the values in the weight matrix.\n\n# use the fastmath module within trax\nfrom trax import fastmath\n\n# use the numpy module from trax\nnp = fastmath.numpy\n\n# use the fastmath.random module from trax\nrandom = fastmath.random\n\n\n# See how the fastmath.trax.random.normal function works\ntmp_key = random.get_prng(seed=1)\nprint(\"The random seed generated by random.get_prng\")\ndisplay(tmp_key)\n\nprint(\"choose a matrix with 2 rows and 3 columns\")\ntmp_shape=(2,3)\ndisplay(tmp_shape)\n\n# Generate a weight matrix\n# Note that you'll get an error if you try to set dtype to tf.float32, where tf is tensorflow\n# Just avoid setting the dtype and allow it to use the default data type\ntmp_weight = trax.fastmath.random.normal(key=tmp_key, shape=tmp_shape)\n\nprint(\"Weight matrix generated with a normal distribution with mean 0 and stdev of 1\")\ndisplay(tmp_weight)\n\nThe random seed generated by random.get_prng\n\n\nArray([0, 1], dtype=uint32)\n\n\nchoose a matrix with 2 rows and 3 columns\n\n\n(2, 3)\n\n\nWeight matrix generated with a normal distribution with mean 0 and stdev of 1\n\n\nArray([[-0.15443718,  0.08470728, -0.13598049],\n       [-0.15503626,  1.2666672 ,  0.14829758]], dtype=float32)\n\n\n\n\nExercise 04\nImplement the Dense class.\n\n# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: Dense\n\nclass Dense(Layer):\n    \"\"\"\n    A dense (fully-connected) layer.\n    \"\"\"\n\n    # __init__ is implemented for you\n    def __init__(self, n_units, init_stdev=0.1):\n        \n        # Set the number of units in this layer\n        self._n_units = n_units\n        self._init_stdev = init_stdev\n\n    # Please implement 'forward()'\n    def forward(self, x):\n\n### START CODE HERE (Replace instances of 'None' with your code) ###\n\n        # Matrix multiply x and the weight matrix\n        dense = None \n        \n### END CODE HERE ###\n        return dense\n\n    # init_weights\n    def init_weights_and_state(self, input_signature, random_key):\n        \n### START CODE HERE (Replace instances of 'None' with your code) ###\n        # The input_signature has a .shape attribute that gives the shape as a tuple\n        input_shape = None\n\n        # Generate the weight matrix from a normal distribution, \n        # and standard deviation of 'stdev'        \n        w = None\n        \n### END CODE HERE ###     \n        self.weights = w\n        return self.weights\n\n\n# Testing your Dense layer \ndense_layer = Dense(n_units=10)  #sets  number of units in dense layer\nrandom_key = random.get_prng(seed=0)  # sets random seed\nz = np.array([[2.0, 7.0, 25.0]]) # input array \n\ndense_layer.init(z, random_key)\nprint(\"Weights are\\n \",dense_layer.weights) #Returns randomly generated weights\nprint(\"Foward function output is \", dense_layer(z)) # Returns multiplied values of units and weights\n\nWeights are\n  None\nFoward function output is  None\n\n\n\nExpected Outout\nWeights are\n  [[-0.02837108  0.09368162 -0.10050076  0.14165013  0.10543301  0.09108126\n  -0.04265672  0.0986188  -0.05575325  0.00153249]\n [-0.20785688  0.0554837   0.09142365  0.05744595  0.07227863  0.01210617\n  -0.03237354  0.16234995  0.02450038 -0.13809784]\n [-0.06111237  0.01403724  0.08410042 -0.1094358  -0.10775021 -0.11396459\n  -0.05933381 -0.01557652 -0.03832145 -0.11144515]]\nFoward function output is  [[-3.0395496   0.9266802   2.5414743  -2.050473   -1.9769388  -2.582209\n  -1.7952735   0.94427425 -0.8980402  -3.7497487 ]]"
  },
  {
    "objectID": "posts/c3w1/assignment.html#3.3",
    "href": "posts/c3w1/assignment.html#3.3",
    "title": "Assignment 1: Sentiment with Deep Neural Networks",
    "section": "3.3 Model",
    "text": "3.3 Model\nNow you will implement a classifier using neural networks. Here is the model architecture you will be implementing.\n\n\n\nNN\n\n\nFor the model implementation, you will use the Trax layers library tl. Note that the second character of tl is the lowercase of letter L, not the number 1. Trax layers are very similar to the ones you implemented above, but in addition to trainable weights also have a non-trainable state. State is used in layers like batch normalization and for inference, you will learn more about it in course 4.\nFirst, look at the code of the Trax Dense layer and compare to your implementation above. - tl.Dense: Trax Dense layer implementation\nOne other important layer that you will use a lot is one that allows to execute one layer after another in sequence. - tl.Serial: Combinator that applies layers serially.\n- You can pass in the layers as arguments to Serial, separated by commas. - For example: tl.Serial(tl.Embeddings(...), tl.Mean(...), tl.Dense(...), tl.LogSoftmax(...))\nPlease use the help function to view documentation for each layer.\n\n# View documentation on tl.Dense\nhelp(tl.Dense)\n\nHelp on class Dense in module trax.layers.core:\n\nclass Dense(trax.layers.base.Layer)\n |  Dense(n_units, kernel_initializer=&lt;function ScaledInitializer.&lt;locals&gt;.Init at 0x7b3f64ea5fc0&gt;, bias_initializer=&lt;function RandomNormalInitializer.&lt;locals&gt;.&lt;lambda&gt; at 0x7b3f64ea6050&gt;, use_bias=True, use_bfloat16=False)\n |  \n |  A dense (a.k.a. fully-connected, affine) layer.\n |  \n |  Dense layers are the prototypical example of a trainable layer, i.e., a layer\n |  with trainable weights. Each node in a dense layer computes a weighted sum of\n |  all node values from the preceding layer and adds to that sum a node-specific\n |  bias term. The full layer computation is expressed compactly in linear\n |  algebra as an affine map `y = Wx + b`, where `W` is a matrix and `y`, `x`,\n |  and `b` are vectors. The layer is trained, or \"learns\", by updating the\n |  values in `W` and `b`.\n |  \n |  Less commonly, a dense layer can omit the bias term and be a pure linear map:\n |  `y = Wx`.\n |  \n |  Method resolution order:\n |      Dense\n |      trax.layers.base.Layer\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, n_units, kernel_initializer=&lt;function ScaledInitializer.&lt;locals&gt;.Init at 0x7b3f64ea5fc0&gt;, bias_initializer=&lt;function RandomNormalInitializer.&lt;locals&gt;.&lt;lambda&gt; at 0x7b3f64ea6050&gt;, use_bias=True, use_bfloat16=False)\n |      Returns a dense (fully connected) layer of width `n_units`.\n |      \n |      A dense layer maps collections of `R^m` vectors to `R^n`, where `n`\n |      (`= n_units`) is fixed at layer creation time, and `m` is set at layer\n |      initialization time.\n |      \n |      Args:\n |        n_units: Number of nodes in the layer, also known as the width of the\n |            layer.\n |        kernel_initializer: Function that creates a matrix of (random) initial\n |            connection weights `W` for the layer.\n |        bias_initializer: Function that creates a vector of (random) initial\n |            bias weights `b` for the layer.\n |        use_bias: If `True`, compute an affine map `y = Wx + b`; else compute\n |            a linear map `y = Wx`.\n |        use_bfloat16: If `True`, use bfloat16 weights instead of the default\n |          float32; this can save memory but may (rarely) lead to numerical issues.\n |  \n |  forward(self, x)\n |      Executes this layer as part of a forward pass through the model.\n |      \n |      Args:\n |        x: Tensor of same shape and dtype as the input signature used to\n |            initialize this layer.\n |      \n |      Returns:\n |        Tensor of same shape and dtype as the input, except the final dimension\n |        is the layer's `n_units` value.\n |  \n |  init_weights_and_state(self, input_signature)\n |      Randomly initializes this layer's weights.\n |      \n |      Weights are a `(w, b)` tuple for layers created with `use_bias=True` (the\n |      default case), or a `w` tensor for layers created with `use_bias=False`.\n |      \n |      Args:\n |        input_signature: `ShapeDtype` instance characterizing the input this layer\n |            should compute on.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from trax.layers.base.Layer:\n |  \n |  __call__(self, x, weights=None, state=None, rng=None)\n |      Makes layers callable; for use in tests or interactive settings.\n |      \n |      This convenience method helps library users play with, test, or otherwise\n |      probe the behavior of layers outside of a full training environment. It\n |      presents the layer as callable function from inputs to outputs, with the\n |      option of manually specifying weights and non-parameter state per individual\n |      call. For convenience, weights and non-parameter state are cached per layer\n |      instance, starting from default values of `EMPTY_WEIGHTS` and `EMPTY_STATE`,\n |      and acquiring non-empty values either by initialization or from values\n |      explicitly provided via the weights and state keyword arguments, in which\n |      case the old weights will be preserved, and the state will be updated.\n |      \n |      Args:\n |        x: Zero or more input tensors, packaged as described in the `Layer` class\n |            docstring.\n |        weights: Weights or `None`; if `None`, use self's cached weights value.\n |        state: State or `None`; if `None`, use self's cached state value.\n |        rng: Single-use random number generator (JAX PRNG key), or `None`;\n |            if `None`, use a default computed from an integer 0 seed.\n |      \n |      Returns:\n |        Zero or more output tensors, packaged as described in the `Layer` class\n |        docstring.\n |  \n |  __repr__(self)\n |      Renders this layer as a medium-detailed string, to help in debugging.\n |      \n |      Subclasses should aim for high-signal/low-noise when overriding this\n |      method.\n |      \n |      Returns:\n |        A high signal-to-noise string representing this layer.\n |  \n |  __setattr__(self, attr, value)\n |      Sets class attributes and protects from typos.\n |      \n |      In Trax layers, we only allow to set the following public attributes::\n |      \n |        - weights\n |        - state\n |        - rng\n |      \n |      This function prevents from setting other public attributes to avoid typos,\n |      for example, this is not possible and would be without this function::\n |      \n |        [typo]   layer.weighs = some_tensor\n |      \n |      If you need to set other public attributes in a derived class (which we\n |      do not recommend as in almost all cases it suffices to use a private\n |      attribute), override self._settable_attrs to include the attribute name.\n |      \n |      Args:\n |        attr: Name of the attribute to be set.\n |        value: Value to be assigned to the attribute.\n |  \n |  backward(self, inputs, output, grad, weights, state, new_state, rng)\n |      Custom backward pass to propagate gradients in a custom way.\n |      \n |      Args:\n |        inputs: Input tensors; can be a (possibly nested) tuple.\n |        output: The result of running this layer on inputs.\n |        grad: Gradient signal computed based on subsequent layers; its structure\n |            and shape must match output.\n |        weights: This layer's weights.\n |        state: This layer's state prior to the current forward pass.\n |        new_state: This layer's state after the current forward pass.\n |        rng: Single-use random number generator (JAX PRNG key).\n |      \n |      Returns:\n |        The custom gradient signal for the input. Note that we need to return\n |        a gradient for each argument of forward, so it will usually be a tuple\n |        of signals: the gradient for inputs and weights.\n |  \n |  init(self, input_signature, rng=None, use_cache=False)\n |      Initializes weights/state of this layer and its sublayers recursively.\n |      \n |      Initialization creates layer weights and state, for layers that use them.\n |      It derives the necessary array shapes and data types from the layer's input\n |      signature, which is itself just shape and data type information.\n |      \n |      For layers without weights or state, this method safely does nothing.\n |      \n |      This method is designed to create weights/state only once for each layer\n |      instance, even if the same layer instance occurs in multiple places in the\n |      network. This enables weight sharing to be implemented as layer sharing.\n |      \n |      Args:\n |        input_signature: `ShapeDtype` instance (if this layer takes one input)\n |            or list/tuple of `ShapeDtype` instances.\n |        rng: Single-use random number generator (JAX PRNG key), or `None`;\n |            if `None`, use a default computed from an integer 0 seed.\n |        use_cache: If `True`, and if this layer instance has already been\n |            initialized elsewhere in the network, then return special marker\n |            values -- tuple `(GET_WEIGHTS_FROM_CACHE, GET_STATE_FROM_CACHE)`.\n |            Else return this layer's newly initialized weights and state.\n |      \n |      Returns:\n |        A `(weights, state)` tuple.\n |  \n |  init_from_file(self, file_name, weights_only=False, input_signature=None)\n |      Initializes this layer and its sublayers from a pickled checkpoint.\n |      \n |      In the common case (`weights_only=False`), the file must be a gziped pickled\n |      dictionary containing items with keys `'flat_weights', `'flat_state'` and\n |      `'input_signature'`, which are used to initialize this layer.\n |      If `input_signature` is specified, it's used instead of the one in the file.\n |      If `weights_only` is `True`, the dictionary does not need to have the\n |      `'flat_state'` item and the state it not restored either.\n |      \n |      Args:\n |        file_name: Name/path of the pickled weights/state file.\n |        weights_only: If `True`, initialize only the layer's weights. Else\n |            initialize both weights and state.\n |        input_signature: Input signature to be used instead of the one from file.\n |      \n |      Returns:\n |        A `(weights, state)` tuple.\n |  \n |  output_signature(self, input_signature)\n |      Returns output signature this layer would give for `input_signature`.\n |  \n |  pure_fn(self, x, weights, state, rng, use_cache=False)\n |      Applies this layer as a pure function with no optional args.\n |      \n |      This method exposes the layer's computation as a pure function. This is\n |      especially useful for JIT compilation. Do not override, use `forward`\n |      instead.\n |      \n |      Args:\n |        x: Zero or more input tensors, packaged as described in the `Layer` class\n |            docstring.\n |        weights: A tuple or list of trainable weights, with one element for this\n |            layer if this layer has no sublayers, or one for each sublayer if\n |            this layer has sublayers. If a layer (or sublayer) has no trainable\n |            weights, the corresponding weights element is an empty tuple.\n |        state: Layer-specific non-parameter state that can update between batches.\n |        rng: Single-use random number generator (JAX PRNG key).\n |        use_cache: if `True`, cache weights and state in the layer object; used\n |          to implement layer sharing in combinators.\n |      \n |      Returns:\n |        A tuple of `(tensors, state)`. The tensors match the number (`n_out`)\n |        promised by this layer, and are packaged as described in the `Layer`\n |        class docstring.\n |  \n |  save_to_file(self, file_name, weights_only=False, input_signature=None)\n |      Saves this layer and its sublayers to a pickled checkpoint.\n |      \n |      Args:\n |        file_name: Name/path of the pickled weights/state file.\n |        weights_only: If `True`, save only the layer's weights. Else\n |            save both weights and state.\n |        input_signature: Input signature to be used.\n |  \n |  weights_and_state_signature(self, input_signature, unsafe=False)\n |      Return a pair containing the signatures of weights and state.\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties inherited from trax.layers.base.Layer:\n |  \n |  has_backward\n |      Returns `True` if this layer provides its own custom backward pass code.\n |      \n |      A layer subclass that provides custom backward pass code (for custom\n |      gradients) must override this method to return `True`.\n |  \n |  n_in\n |      Returns how many tensors this layer expects as input.\n |  \n |  n_out\n |      Returns how many tensors this layer promises as output.\n |  \n |  name\n |      Returns the name of this layer.\n |  \n |  sublayers\n |      Returns a tuple containing this layer's sublayers; may be empty.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from trax.layers.base.Layer:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  rng\n |      Returns this layer's current single-use random number generator.\n |      \n |      Code that wants to base random samples on this generator must explicitly\n |      split off new generators from it. (See, for example, the `rng` setter code\n |      below.)\n |  \n |  state\n |      Returns a tuple containing this layer's state; may be empty.\n |      \n |      If the layer has sublayers, the state by convention will be\n |      a tuple of length `len(sublayers)` containing sublayer states.\n |      Note that in this case self._state only marks which ones are shared.\n |  \n |  weights\n |      Returns this layer's weights.\n |      \n |      Depending on the layer, the weights can be in the form of:\n |      \n |        - an empty tuple\n |        - a tensor (ndarray)\n |        - a nested structure of tuples and tensors\n |      \n |      If the layer has sublayers, the weights by convention will be\n |      a tuple of length `len(sublayers)` containing the weights of sublayers.\n |      Note that in this case self._weights only marks which ones are shared.\n\n\n\n\n# View documentation on tl.Serial\nhelp(tl.Serial)\n\nHelp on class Serial in module trax.layers.combinators:\n\nclass Serial(trax.layers.base.Layer)\n |  Serial(*sublayers, name=None, sublayers_to_print=None)\n |  \n |  Combinator that applies layers serially (by function composition).\n |  \n |  This combinator is commonly used to construct deep networks, e.g., like this::\n |  \n |      mlp = tl.Serial(\n |        tl.Dense(128),\n |        tl.Relu(),\n |        tl.Dense(10),\n |      )\n |  \n |  A Serial combinator uses stack semantics to manage data for its sublayers.\n |  Each sublayer sees only the inputs it needs and returns only the outputs it\n |  has generated. The sublayers interact via the data stack. For instance, a\n |  sublayer k, following sublayer j, gets called with the data stack in the\n |  state left after layer j has applied. The Serial combinator then:\n |  \n |    - takes n_in items off the top of the stack (n_in = k.n_in) and calls\n |      layer k, passing those items as arguments; and\n |  \n |    - takes layer k's n_out return values (n_out = k.n_out) and pushes\n |      them onto the data stack.\n |  \n |  A Serial instance with no sublayers acts as a special-case (but useful)\n |  1-input 1-output no-op.\n |  \n |  Method resolution order:\n |      Serial\n |      trax.layers.base.Layer\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, *sublayers, name=None, sublayers_to_print=None)\n |      Creates a partially initialized, unconnected layer instance.\n |      \n |      Args:\n |        n_in: Number of inputs expected by this layer.\n |        n_out: Number of outputs promised by this layer.\n |        name: Class-like name for this layer; for use when printing this layer.\n |        sublayers_to_print: Sublayers to display when printing out this layer;\n |          if None (the default), display all sublayers.\n |  \n |  forward(self, xs)\n |      Executes this layer as part of a forward pass through the model.\n |  \n |  init_weights_and_state(self, input_signature)\n |      Initializes weights and state for inputs with the given signature.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from trax.layers.base.Layer:\n |  \n |  __call__(self, x, weights=None, state=None, rng=None)\n |      Makes layers callable; for use in tests or interactive settings.\n |      \n |      This convenience method helps library users play with, test, or otherwise\n |      probe the behavior of layers outside of a full training environment. It\n |      presents the layer as callable function from inputs to outputs, with the\n |      option of manually specifying weights and non-parameter state per individual\n |      call. For convenience, weights and non-parameter state are cached per layer\n |      instance, starting from default values of `EMPTY_WEIGHTS` and `EMPTY_STATE`,\n |      and acquiring non-empty values either by initialization or from values\n |      explicitly provided via the weights and state keyword arguments, in which\n |      case the old weights will be preserved, and the state will be updated.\n |      \n |      Args:\n |        x: Zero or more input tensors, packaged as described in the `Layer` class\n |            docstring.\n |        weights: Weights or `None`; if `None`, use self's cached weights value.\n |        state: State or `None`; if `None`, use self's cached state value.\n |        rng: Single-use random number generator (JAX PRNG key), or `None`;\n |            if `None`, use a default computed from an integer 0 seed.\n |      \n |      Returns:\n |        Zero or more output tensors, packaged as described in the `Layer` class\n |        docstring.\n |  \n |  __repr__(self)\n |      Renders this layer as a medium-detailed string, to help in debugging.\n |      \n |      Subclasses should aim for high-signal/low-noise when overriding this\n |      method.\n |      \n |      Returns:\n |        A high signal-to-noise string representing this layer.\n |  \n |  __setattr__(self, attr, value)\n |      Sets class attributes and protects from typos.\n |      \n |      In Trax layers, we only allow to set the following public attributes::\n |      \n |        - weights\n |        - state\n |        - rng\n |      \n |      This function prevents from setting other public attributes to avoid typos,\n |      for example, this is not possible and would be without this function::\n |      \n |        [typo]   layer.weighs = some_tensor\n |      \n |      If you need to set other public attributes in a derived class (which we\n |      do not recommend as in almost all cases it suffices to use a private\n |      attribute), override self._settable_attrs to include the attribute name.\n |      \n |      Args:\n |        attr: Name of the attribute to be set.\n |        value: Value to be assigned to the attribute.\n |  \n |  backward(self, inputs, output, grad, weights, state, new_state, rng)\n |      Custom backward pass to propagate gradients in a custom way.\n |      \n |      Args:\n |        inputs: Input tensors; can be a (possibly nested) tuple.\n |        output: The result of running this layer on inputs.\n |        grad: Gradient signal computed based on subsequent layers; its structure\n |            and shape must match output.\n |        weights: This layer's weights.\n |        state: This layer's state prior to the current forward pass.\n |        new_state: This layer's state after the current forward pass.\n |        rng: Single-use random number generator (JAX PRNG key).\n |      \n |      Returns:\n |        The custom gradient signal for the input. Note that we need to return\n |        a gradient for each argument of forward, so it will usually be a tuple\n |        of signals: the gradient for inputs and weights.\n |  \n |  init(self, input_signature, rng=None, use_cache=False)\n |      Initializes weights/state of this layer and its sublayers recursively.\n |      \n |      Initialization creates layer weights and state, for layers that use them.\n |      It derives the necessary array shapes and data types from the layer's input\n |      signature, which is itself just shape and data type information.\n |      \n |      For layers without weights or state, this method safely does nothing.\n |      \n |      This method is designed to create weights/state only once for each layer\n |      instance, even if the same layer instance occurs in multiple places in the\n |      network. This enables weight sharing to be implemented as layer sharing.\n |      \n |      Args:\n |        input_signature: `ShapeDtype` instance (if this layer takes one input)\n |            or list/tuple of `ShapeDtype` instances.\n |        rng: Single-use random number generator (JAX PRNG key), or `None`;\n |            if `None`, use a default computed from an integer 0 seed.\n |        use_cache: If `True`, and if this layer instance has already been\n |            initialized elsewhere in the network, then return special marker\n |            values -- tuple `(GET_WEIGHTS_FROM_CACHE, GET_STATE_FROM_CACHE)`.\n |            Else return this layer's newly initialized weights and state.\n |      \n |      Returns:\n |        A `(weights, state)` tuple.\n |  \n |  init_from_file(self, file_name, weights_only=False, input_signature=None)\n |      Initializes this layer and its sublayers from a pickled checkpoint.\n |      \n |      In the common case (`weights_only=False`), the file must be a gziped pickled\n |      dictionary containing items with keys `'flat_weights', `'flat_state'` and\n |      `'input_signature'`, which are used to initialize this layer.\n |      If `input_signature` is specified, it's used instead of the one in the file.\n |      If `weights_only` is `True`, the dictionary does not need to have the\n |      `'flat_state'` item and the state it not restored either.\n |      \n |      Args:\n |        file_name: Name/path of the pickled weights/state file.\n |        weights_only: If `True`, initialize only the layer's weights. Else\n |            initialize both weights and state.\n |        input_signature: Input signature to be used instead of the one from file.\n |      \n |      Returns:\n |        A `(weights, state)` tuple.\n |  \n |  output_signature(self, input_signature)\n |      Returns output signature this layer would give for `input_signature`.\n |  \n |  pure_fn(self, x, weights, state, rng, use_cache=False)\n |      Applies this layer as a pure function with no optional args.\n |      \n |      This method exposes the layer's computation as a pure function. This is\n |      especially useful for JIT compilation. Do not override, use `forward`\n |      instead.\n |      \n |      Args:\n |        x: Zero or more input tensors, packaged as described in the `Layer` class\n |            docstring.\n |        weights: A tuple or list of trainable weights, with one element for this\n |            layer if this layer has no sublayers, or one for each sublayer if\n |            this layer has sublayers. If a layer (or sublayer) has no trainable\n |            weights, the corresponding weights element is an empty tuple.\n |        state: Layer-specific non-parameter state that can update between batches.\n |        rng: Single-use random number generator (JAX PRNG key).\n |        use_cache: if `True`, cache weights and state in the layer object; used\n |          to implement layer sharing in combinators.\n |      \n |      Returns:\n |        A tuple of `(tensors, state)`. The tensors match the number (`n_out`)\n |        promised by this layer, and are packaged as described in the `Layer`\n |        class docstring.\n |  \n |  save_to_file(self, file_name, weights_only=False, input_signature=None)\n |      Saves this layer and its sublayers to a pickled checkpoint.\n |      \n |      Args:\n |        file_name: Name/path of the pickled weights/state file.\n |        weights_only: If `True`, save only the layer's weights. Else\n |            save both weights and state.\n |        input_signature: Input signature to be used.\n |  \n |  weights_and_state_signature(self, input_signature, unsafe=False)\n |      Return a pair containing the signatures of weights and state.\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties inherited from trax.layers.base.Layer:\n |  \n |  has_backward\n |      Returns `True` if this layer provides its own custom backward pass code.\n |      \n |      A layer subclass that provides custom backward pass code (for custom\n |      gradients) must override this method to return `True`.\n |  \n |  n_in\n |      Returns how many tensors this layer expects as input.\n |  \n |  n_out\n |      Returns how many tensors this layer promises as output.\n |  \n |  name\n |      Returns the name of this layer.\n |  \n |  sublayers\n |      Returns a tuple containing this layer's sublayers; may be empty.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from trax.layers.base.Layer:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  rng\n |      Returns this layer's current single-use random number generator.\n |      \n |      Code that wants to base random samples on this generator must explicitly\n |      split off new generators from it. (See, for example, the `rng` setter code\n |      below.)\n |  \n |  state\n |      Returns a tuple containing this layer's state; may be empty.\n |      \n |      If the layer has sublayers, the state by convention will be\n |      a tuple of length `len(sublayers)` containing sublayer states.\n |      Note that in this case self._state only marks which ones are shared.\n |  \n |  weights\n |      Returns this layer's weights.\n |      \n |      Depending on the layer, the weights can be in the form of:\n |      \n |        - an empty tuple\n |        - a tensor (ndarray)\n |        - a nested structure of tuples and tensors\n |      \n |      If the layer has sublayers, the weights by convention will be\n |      a tuple of length `len(sublayers)` containing the weights of sublayers.\n |      Note that in this case self._weights only marks which ones are shared.\n\n\n\n\ntl.Embedding: Layer constructor function for an embedding layer.\n\ntl.Embedding(vocab_size, d_feature).\nvocab_size is the number of unique words in the given vocabulary.\nd_feature is the number of elements in the word embedding (some choices for a word embedding size range from 150 to 300, for example).\n\n\n\n# View documentation for tl.Embedding\nhelp(tl.Embedding)\n\nHelp on class Embedding in module trax.layers.core:\n\nclass Embedding(trax.layers.base.Layer)\n |  Embedding(vocab_size, d_feature, use_bfloat16=False, kernel_initializer=&lt;function ScaledInitializer.&lt;locals&gt;.Init at 0x7b3f64ea63b0&gt;)\n |  \n |  Trainable layer that maps discrete tokens/IDs to vectors.\n |  \n |  Embedding layers are commonly used to map discrete data, like words in NLP,\n |  into vectors. Here is a canonical example::\n |  \n |      vocab_size = 5\n |      word_ids = np.array([1, 2, 3, 4], dtype=np.int32)  # word_ids &lt; vocab_size\n |      embedding_layer = tl.Embedding(vocab_size, 32)\n |      embedding_layer.init(trax.shapes.signature(word_ids))\n |      embedded = embedding_layer(word_ids)  # embedded.shape = (4, 32)\n |  \n |  Method resolution order:\n |      Embedding\n |      trax.layers.base.Layer\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, vocab_size, d_feature, use_bfloat16=False, kernel_initializer=&lt;function ScaledInitializer.&lt;locals&gt;.Init at 0x7b3f64ea63b0&gt;)\n |      Returns an embedding layer with given vocabulary size and vector size.\n |      \n |      The layer clips input values (token IDs) to the range `[0, vocab_size)`.\n |      That is, negative token IDs all clip to `0` before being mapped to a\n |      vector, and token IDs with value `vocab_size` or greater all clip to\n |      `vocab_size - 1` before being mapped to a vector.\n |      \n |      Args:\n |        vocab_size: Size of the input vocabulary. The layer will assign a unique\n |          vector to each id in `range(vocab_size)`.\n |        d_feature: Dimensionality/depth of the output vectors.\n |        use_bfloat16: If `True`, use bfloat16 weights instead of the default\n |          float32; this can save memory but may (rarely) lead to numerical issues.\n |        kernel_initializer: Function that creates (random) initial vectors for\n |          the embedding.\n |  \n |  forward(self, x)\n |      Returns embedding vectors corresponding to input token IDs.\n |      \n |      Args:\n |        x: Tensor of token IDs.\n |      \n |      Returns:\n |        Tensor of embedding vectors.\n |  \n |  init_weights_and_state(self, input_signature)\n |      Randomly initializes this layer's weights.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from trax.layers.base.Layer:\n |  \n |  __call__(self, x, weights=None, state=None, rng=None)\n |      Makes layers callable; for use in tests or interactive settings.\n |      \n |      This convenience method helps library users play with, test, or otherwise\n |      probe the behavior of layers outside of a full training environment. It\n |      presents the layer as callable function from inputs to outputs, with the\n |      option of manually specifying weights and non-parameter state per individual\n |      call. For convenience, weights and non-parameter state are cached per layer\n |      instance, starting from default values of `EMPTY_WEIGHTS` and `EMPTY_STATE`,\n |      and acquiring non-empty values either by initialization or from values\n |      explicitly provided via the weights and state keyword arguments, in which\n |      case the old weights will be preserved, and the state will be updated.\n |      \n |      Args:\n |        x: Zero or more input tensors, packaged as described in the `Layer` class\n |            docstring.\n |        weights: Weights or `None`; if `None`, use self's cached weights value.\n |        state: State or `None`; if `None`, use self's cached state value.\n |        rng: Single-use random number generator (JAX PRNG key), or `None`;\n |            if `None`, use a default computed from an integer 0 seed.\n |      \n |      Returns:\n |        Zero or more output tensors, packaged as described in the `Layer` class\n |        docstring.\n |  \n |  __repr__(self)\n |      Renders this layer as a medium-detailed string, to help in debugging.\n |      \n |      Subclasses should aim for high-signal/low-noise when overriding this\n |      method.\n |      \n |      Returns:\n |        A high signal-to-noise string representing this layer.\n |  \n |  __setattr__(self, attr, value)\n |      Sets class attributes and protects from typos.\n |      \n |      In Trax layers, we only allow to set the following public attributes::\n |      \n |        - weights\n |        - state\n |        - rng\n |      \n |      This function prevents from setting other public attributes to avoid typos,\n |      for example, this is not possible and would be without this function::\n |      \n |        [typo]   layer.weighs = some_tensor\n |      \n |      If you need to set other public attributes in a derived class (which we\n |      do not recommend as in almost all cases it suffices to use a private\n |      attribute), override self._settable_attrs to include the attribute name.\n |      \n |      Args:\n |        attr: Name of the attribute to be set.\n |        value: Value to be assigned to the attribute.\n |  \n |  backward(self, inputs, output, grad, weights, state, new_state, rng)\n |      Custom backward pass to propagate gradients in a custom way.\n |      \n |      Args:\n |        inputs: Input tensors; can be a (possibly nested) tuple.\n |        output: The result of running this layer on inputs.\n |        grad: Gradient signal computed based on subsequent layers; its structure\n |            and shape must match output.\n |        weights: This layer's weights.\n |        state: This layer's state prior to the current forward pass.\n |        new_state: This layer's state after the current forward pass.\n |        rng: Single-use random number generator (JAX PRNG key).\n |      \n |      Returns:\n |        The custom gradient signal for the input. Note that we need to return\n |        a gradient for each argument of forward, so it will usually be a tuple\n |        of signals: the gradient for inputs and weights.\n |  \n |  init(self, input_signature, rng=None, use_cache=False)\n |      Initializes weights/state of this layer and its sublayers recursively.\n |      \n |      Initialization creates layer weights and state, for layers that use them.\n |      It derives the necessary array shapes and data types from the layer's input\n |      signature, which is itself just shape and data type information.\n |      \n |      For layers without weights or state, this method safely does nothing.\n |      \n |      This method is designed to create weights/state only once for each layer\n |      instance, even if the same layer instance occurs in multiple places in the\n |      network. This enables weight sharing to be implemented as layer sharing.\n |      \n |      Args:\n |        input_signature: `ShapeDtype` instance (if this layer takes one input)\n |            or list/tuple of `ShapeDtype` instances.\n |        rng: Single-use random number generator (JAX PRNG key), or `None`;\n |            if `None`, use a default computed from an integer 0 seed.\n |        use_cache: If `True`, and if this layer instance has already been\n |            initialized elsewhere in the network, then return special marker\n |            values -- tuple `(GET_WEIGHTS_FROM_CACHE, GET_STATE_FROM_CACHE)`.\n |            Else return this layer's newly initialized weights and state.\n |      \n |      Returns:\n |        A `(weights, state)` tuple.\n |  \n |  init_from_file(self, file_name, weights_only=False, input_signature=None)\n |      Initializes this layer and its sublayers from a pickled checkpoint.\n |      \n |      In the common case (`weights_only=False`), the file must be a gziped pickled\n |      dictionary containing items with keys `'flat_weights', `'flat_state'` and\n |      `'input_signature'`, which are used to initialize this layer.\n |      If `input_signature` is specified, it's used instead of the one in the file.\n |      If `weights_only` is `True`, the dictionary does not need to have the\n |      `'flat_state'` item and the state it not restored either.\n |      \n |      Args:\n |        file_name: Name/path of the pickled weights/state file.\n |        weights_only: If `True`, initialize only the layer's weights. Else\n |            initialize both weights and state.\n |        input_signature: Input signature to be used instead of the one from file.\n |      \n |      Returns:\n |        A `(weights, state)` tuple.\n |  \n |  output_signature(self, input_signature)\n |      Returns output signature this layer would give for `input_signature`.\n |  \n |  pure_fn(self, x, weights, state, rng, use_cache=False)\n |      Applies this layer as a pure function with no optional args.\n |      \n |      This method exposes the layer's computation as a pure function. This is\n |      especially useful for JIT compilation. Do not override, use `forward`\n |      instead.\n |      \n |      Args:\n |        x: Zero or more input tensors, packaged as described in the `Layer` class\n |            docstring.\n |        weights: A tuple or list of trainable weights, with one element for this\n |            layer if this layer has no sublayers, or one for each sublayer if\n |            this layer has sublayers. If a layer (or sublayer) has no trainable\n |            weights, the corresponding weights element is an empty tuple.\n |        state: Layer-specific non-parameter state that can update between batches.\n |        rng: Single-use random number generator (JAX PRNG key).\n |        use_cache: if `True`, cache weights and state in the layer object; used\n |          to implement layer sharing in combinators.\n |      \n |      Returns:\n |        A tuple of `(tensors, state)`. The tensors match the number (`n_out`)\n |        promised by this layer, and are packaged as described in the `Layer`\n |        class docstring.\n |  \n |  save_to_file(self, file_name, weights_only=False, input_signature=None)\n |      Saves this layer and its sublayers to a pickled checkpoint.\n |      \n |      Args:\n |        file_name: Name/path of the pickled weights/state file.\n |        weights_only: If `True`, save only the layer's weights. Else\n |            save both weights and state.\n |        input_signature: Input signature to be used.\n |  \n |  weights_and_state_signature(self, input_signature, unsafe=False)\n |      Return a pair containing the signatures of weights and state.\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties inherited from trax.layers.base.Layer:\n |  \n |  has_backward\n |      Returns `True` if this layer provides its own custom backward pass code.\n |      \n |      A layer subclass that provides custom backward pass code (for custom\n |      gradients) must override this method to return `True`.\n |  \n |  n_in\n |      Returns how many tensors this layer expects as input.\n |  \n |  n_out\n |      Returns how many tensors this layer promises as output.\n |  \n |  name\n |      Returns the name of this layer.\n |  \n |  sublayers\n |      Returns a tuple containing this layer's sublayers; may be empty.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from trax.layers.base.Layer:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  rng\n |      Returns this layer's current single-use random number generator.\n |      \n |      Code that wants to base random samples on this generator must explicitly\n |      split off new generators from it. (See, for example, the `rng` setter code\n |      below.)\n |  \n |  state\n |      Returns a tuple containing this layer's state; may be empty.\n |      \n |      If the layer has sublayers, the state by convention will be\n |      a tuple of length `len(sublayers)` containing sublayer states.\n |      Note that in this case self._state only marks which ones are shared.\n |  \n |  weights\n |      Returns this layer's weights.\n |      \n |      Depending on the layer, the weights can be in the form of:\n |      \n |        - an empty tuple\n |        - a tensor (ndarray)\n |        - a nested structure of tuples and tensors\n |      \n |      If the layer has sublayers, the weights by convention will be\n |      a tuple of length `len(sublayers)` containing the weights of sublayers.\n |      Note that in this case self._weights only marks which ones are shared.\n\n\n\n\ntmp_embed = tl.Embedding(vocab_size=3, d_feature=2)\ndisplay(tmp_embed)\n\nEmbedding_3_2\n\n\n\ntl.Mean: Calculates means across an axis. In this case, please choose axis = 1 to get an average embedding vector (an embedding vector that is an average of all words in the vocabulary).\n\nFor example, if the embedding matrix is 300 elements and vocab size is 10,000 words, taking the mean of the embedding matrix along axis=1 will yield a vector of 300 elements.\n\n\n# view the documentation for tl.mean\nhelp(tl.Mean)\n\nHelp on function Mean in module trax.layers.core:\n\nMean(axis=-1, keepdims=False)\n    Returns a layer that computes mean values using one tensor axis.\n    \n    `Mean` uses one tensor axis to form groups of values and replaces each group\n    with the mean value of that group. The resulting values can either remain\n    in their own size 1 axis (`keepdims=True`), or that axis can be removed from\n    the overall tensor (default `keepdims=False`), lowering the rank of the\n    tensor by one.\n    \n    Args:\n      axis: Axis along which values are grouped for computing a mean.\n      keepdims: If `True`, keep the resulting size 1 axis as a separate tensor\n          axis; else, remove that axis.\n\n\n\n\n# Pretend the embedding matrix uses \n# 2 elements for embedding the meaning of a word\n# and has a vocabulary size of 3\n# So it has shape (2,3)\ntmp_embed = np.array([[1,2,3,],\n                    [4,5,6]\n                   ])\n\n# take the mean along axis 0\nprint(\"The mean along axis 0 creates a vector whose length equals the vocabulary size\")\ndisplay(np.mean(tmp_embed,axis=0))\n\nprint(\"The mean along axis 1 creates a vector whose length equals the number of elements in a word embedding\")\ndisplay(np.mean(tmp_embed,axis=1))\n\nThe mean along axis 0 creates a vector whose length equals the vocabulary size\n\n\nArray([2.5, 3.5, 4.5], dtype=float32)\n\n\nThe mean along axis 1 creates a vector whose length equals the number of elements in a word embedding\n\n\nArray([2., 5.], dtype=float32)\n\n\n\ntl.LogSoftmax: Implements log softmax function\nHere, you don‚Äôt need to set any parameters for LogSoftMax().\n\n\nhelp(tl.LogSoftmax)\n\nHelp on function LogSoftmax in module trax.layers.core:\n\nLogSoftmax(axis=-1)\n    Returns a layer that applies log softmax along one tensor axis.\n    \n    Note that the implementation actually computes x - LogSumExp(x),\n    which is mathematically equal to LogSoftmax(x).\n    \n    `LogSoftmax` acts on a group of values and normalizes them to look like a set\n    of log probability values. (Probability values must be non-negative, and as\n    a set must sum to 1. A group of log probability values can be seen as the\n    natural logarithm function applied to a set of probability values.)\n    \n    Args:\n      axis: Axis along which values are grouped for computing log softmax.\n\n\n\nOnline documentation\n\ntl.Dense\ntl.Serial\ntl.Embedding\ntl.Mean\ntl.LogSoftmax\n\n\nExercise 05\nImplement the classifier function.\n\n# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: classifier\ndef classifier(vocab_size=len(Vocab), embedding_dim=256, output_dim=2, mode='train'):\n        \n### START CODE HERE (Replace instances of 'None' with your code) ###\n    # create embedding layer\n    embed_layer = tl.Embedding(\n        vocab_size=None, # Size of the vocabulary\n        d_feature=None)  # Embedding dimension\n    \n    # Create a mean layer, to create an \"average\" word embedding\n    mean_layer = None\n    \n    # Create a dense layer, one unit for each output\n    dense_output_layer = tl.Dense(n_units = None)\n\n    \n    # Create the log softmax layer (no parameters needed)\n    log_softmax_layer = None\n    \n    # Use tl.Serial to combine all layers\n    # and create the classifier\n    # of type trax.layers.combinators.Serial\n    model = tl.Serial(\n      None, # embedding layer\n      None, # mean layer\n      None, # dense output layer \n      None # log softmax layer\n    )\n### END CODE HERE ###     \n    \n    # return the model of type\n    return model\n\n\ntmp_model = classifier()\n\n/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/combinators.py:437: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n  if self._mode == 'predict' and self._state[1] is not ():  # pylint: disable=literal-comparison\n/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/combinators.py:910: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n  if state[0] is ():  # pylint: disable=literal-comparison\n/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/combinators.py:437: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n  if self._mode == 'predict' and self._state[1] is not ():  # pylint: disable=literal-comparison\n/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/combinators.py:910: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n  if state[0] is ():  # pylint: disable=literal-comparison\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[30], line 1\n----&gt; 1 tmp_model = classifier()\n\nCell In[29], line 24, in classifier(vocab_size, embedding_dim, output_dim, mode)\n     19     log_softmax_layer = None\n     21     # Use tl.Serial to combine all layers\n     22     # and create the classifier\n     23     # of type trax.layers.combinators.Serial\n---&gt; 24     model = tl.Serial(\n     25       None, # embedding layer\n     26       None, # mean layer\n     27       None, # dense output layer \n     28       None # log softmax layer\n     29     )\n     30 ### END CODE HERE ###     \n     31     \n     32     # return the model of type\n     33     return model\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/combinators.py:59, in Serial.__init__(self, name, sublayers_to_print, *sublayers)\n     55 def __init__(self, *sublayers, name=None, sublayers_to_print=None):\n     56   super().__init__(\n     57       name=name, sublayers_to_print=sublayers_to_print)\n---&gt; 59   sublayers = _ensure_flat(sublayers)\n     60   self._sublayers = sublayers\n     61   self._n_layers = len(sublayers)\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/combinators.py:1110, in _ensure_flat(layers)\n   1108 for obj in layers:\n   1109   if not isinstance(obj, base.Layer):\n-&gt; 1110     raise ValueError(\n   1111         f'Found nonlayer object ({obj}) in layers: {layers}')\n   1112 return layers\n\nValueError: Found nonlayer object (None) in layers: [None, None, None, None]\n\n\n\n\nprint(type(tmp_model))\ndisplay(tmp_model)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[31], line 1\n----&gt; 1 print(type(tmp_model))\n      2 display(tmp_model)\n\nNameError: name 'tmp_model' is not defined\n\n\n\n\nExpected Outout\n&lt;class 'trax.layers.combinators.Serial'&gt;\nSerial[\n  Embedding_9088_256\n  Mean\n  Dense_2\n  LogSoftmax\n]"
  },
  {
    "objectID": "posts/c3w1/assignment.html#4.1",
    "href": "posts/c3w1/assignment.html#4.1",
    "title": "Assignment 1: Sentiment with Deep Neural Networks",
    "section": "4.1 Training the model",
    "text": "4.1 Training the model\nNow you are going to train your model.\nLet‚Äôs define the TrainTask, EvalTask and Loop in preparation to train the model.\n\nfrom trax.supervised import training\n\nbatch_size = 16\nrnd.seed(271)\n\ntrain_task = training.TrainTask(\n    labeled_data=train_generator(batch_size=batch_size, shuffle=True),\n    loss_layer=tl.CrossEntropyLoss(),\n    optimizer=trax.optimizers.Adam(0.01),\n    n_steps_per_checkpoint=10,\n)\n\neval_task = training.EvalTask(\n    labeled_data=val_generator(batch_size=batch_size, shuffle=True),\n    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n)\n\nmodel = classifier()\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[36], line 7\n      3 batch_size = 16\n      4 rnd.seed(271)\n      6 train_task = training.TrainTask(\n----&gt; 7     labeled_data=train_generator(batch_size=batch_size, shuffle=True),\n      8     loss_layer=tl.CrossEntropyLoss(),\n      9     optimizer=trax.optimizers.Adam(0.01),\n     10     n_steps_per_checkpoint=10,\n     11 )\n     13 eval_task = training.EvalTask(\n     14     labeled_data=val_generator(batch_size=batch_size, shuffle=True),\n     15     metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n     16 )\n     18 model = classifier()\n\nCell In[14], line 6, in train_generator(batch_size, shuffle)\n      5 def train_generator(batch_size, shuffle = False):\n----&gt; 6     return data_generator(train_pos, train_neg, batch_size, True, Vocab, shuffle)\n\nNameError: name 'data_generator' is not defined\n\n\n\nThis defines a model trained using tl.CrossEntropyLoss optimized with the trax.optimizers.Adam optimizer, all the while tracking the accuracy using tl.Accuracy metric. We also track tl.CrossEntropyLoss on the validation set.\nNow let‚Äôs make an output directory and train the model.\n\noutput_dir = '~/model/'\noutput_dir_expand = os.path.expanduser(output_dir)\nprint(output_dir_expand)\n\n/home/oren/model/\n\n\n\nExercise 06\nInstructions: Implement train_model to train the model (classifier that you wrote earlier) for the given number of training steps (n_steps) using TrainTask, EvalTask and Loop.\n\n# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: train_model\ndef train_model(classifier, train_task, eval_task, n_steps, output_dir):\n    '''\n    Input: \n        classifier - the model you are building\n        train_task - Training task\n        eval_task - Evaluation task\n        n_steps - the evaluation steps\n        output_dir - folder to save your files\n    Output:\n        trainer -  trax trainer\n    '''\n### START CODE HERE (Replace instances of 'None' with your code) ###\n    training_loop = training.Loop(\n                                None, # The learning model\n                                None, # The training task\n                                eval_task = None, # The evaluation task\n                                output_dir = None) # The output directory\n\n    training_loop.run(n_steps = None)\n### END CODE HERE ###\n\n    # Return the training_loop, since it has the model.\n    return training_loop\n\n\ntraining_loop = train_model(model, train_task, eval_task, 100, output_dir_expand)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[39], line 1\n----&gt; 1 training_loop = train_model(model, train_task, eval_task, 100, output_dir_expand)\n\nNameError: name 'model' is not defined\n\n\n\n\nExpected output (Approximately)\nStep      1: train CrossEntropyLoss |  0.88939196\nStep      1: eval  CrossEntropyLoss |  0.68833977\nStep      1: eval          Accuracy |  0.50000000\nStep     10: train CrossEntropyLoss |  0.61036736\nStep     10: eval  CrossEntropyLoss |  0.52182281\nStep     10: eval          Accuracy |  0.68750000\nStep     20: train CrossEntropyLoss |  0.34137666\nStep     20: eval  CrossEntropyLoss |  0.20654774\nStep     20: eval          Accuracy |  1.00000000\nStep     30: train CrossEntropyLoss |  0.20208922\nStep     30: eval  CrossEntropyLoss |  0.21594886\nStep     30: eval          Accuracy |  0.93750000\nStep     40: train CrossEntropyLoss |  0.19611198\nStep     40: eval  CrossEntropyLoss |  0.17582777\nStep     40: eval          Accuracy |  1.00000000\nStep     50: train CrossEntropyLoss |  0.11203773\nStep     50: eval  CrossEntropyLoss |  0.07589275\nStep     50: eval          Accuracy |  1.00000000\nStep     60: train CrossEntropyLoss |  0.09375446\nStep     60: eval  CrossEntropyLoss |  0.09290724\nStep     60: eval          Accuracy |  1.00000000\nStep     70: train CrossEntropyLoss |  0.08785903\nStep     70: eval  CrossEntropyLoss |  0.09610598\nStep     70: eval          Accuracy |  1.00000000\nStep     80: train CrossEntropyLoss |  0.08858261\nStep     80: eval  CrossEntropyLoss |  0.02319432\nStep     80: eval          Accuracy |  1.00000000\nStep     90: train CrossEntropyLoss |  0.05699894\nStep     90: eval  CrossEntropyLoss |  0.01778970\nStep     90: eval          Accuracy |  1.00000000\nStep    100: train CrossEntropyLoss |  0.03663783\nStep    100: eval  CrossEntropyLoss |  0.00210550\nStep    100: eval          Accuracy |  1.00000000"
  },
  {
    "objectID": "posts/c3w1/assignment.html#4.2",
    "href": "posts/c3w1/assignment.html#4.2",
    "title": "Assignment 1: Sentiment with Deep Neural Networks",
    "section": "4.2 Practice Making a prediction",
    "text": "4.2 Practice Making a prediction\nNow that you have trained a model, you can access it as training_loop.model object. We will actually use training_loop.eval_model and in the next weeks you will learn why we sometimes use a different model for evaluation, e.g., one without dropout. For now, make predictions with your model.\nUse the training data just to see how the prediction process works.\n- Later, you will use validation data to evaluate your model‚Äôs performance.\n\n# Create a generator object\ntmp_train_generator = train_generator(16)\n\n# get one batch\ntmp_batch = next(tmp_train_generator)\n\n# Position 0 has the model inputs (tweets as tensors)\n# position 1 has the targets (the actual labels)\ntmp_inputs, tmp_targets, tmp_example_weights = tmp_batch\n\nprint(f\"The batch is a tuple of length {len(tmp_batch)} because position 0 contains the tweets, and position 1 contains the targets.\") \nprint(f\"The shape of the tweet tensors is {tmp_inputs.shape} (num of examples, length of tweet tensors)\")\nprint(f\"The shape of the labels is {tmp_targets.shape}, which is the batch size.\")\nprint(f\"The shape of the example_weights is {tmp_example_weights.shape}, which is the same as inputs/targets size.\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[40], line 2\n      1 # Create a generator object\n----&gt; 2 tmp_train_generator = train_generator(16)\n      4 # get one batch\n      5 tmp_batch = next(tmp_train_generator)\n\nCell In[14], line 6, in train_generator(batch_size, shuffle)\n      5 def train_generator(batch_size, shuffle = False):\n----&gt; 6     return data_generator(train_pos, train_neg, batch_size, True, Vocab, shuffle)\n\nNameError: name 'data_generator' is not defined\n\n\n\n\n# feed the tweet tensors into the model to get a prediction\ntmp_pred = training_loop.eval_model(tmp_inputs)\nprint(f\"The prediction shape is {tmp_pred.shape}, num of tensor_tweets as rows\")\nprint(\"Column 0 is the probability of a negative sentiment (class 0)\")\nprint(\"Column 1 is the probability of a positive sentiment (class 1)\")\nprint()\nprint(\"View the prediction array\")\ntmp_pred\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[41], line 2\n      1 # feed the tweet tensors into the model to get a prediction\n----&gt; 2 tmp_pred = training_loop.eval_model(tmp_inputs)\n      3 print(f\"The prediction shape is {tmp_pred.shape}, num of tensor_tweets as rows\")\n      4 print(\"Column 0 is the probability of a negative sentiment (class 0)\")\n\nNameError: name 'training_loop' is not defined\n\n\n\nTo turn these probabilities into categories (negative or positive sentiment prediction), for each row:\n\nCompare the probabilities in each column.\nIf column 1 has a value greater than column 0, classify that as a positive tweet.\nOtherwise if column 1 is less than or equal to column 0, classify that example as a negative tweet.\n\n\n# turn probabilites into category predictions\ntmp_is_positive = tmp_pred[:,1] &gt; tmp_pred[:,0]\nfor i, p in enumerate(tmp_is_positive):\n    print(f\"Neg log prob {tmp_pred[i,0]:.4f}\\tPos log prob {tmp_pred[i,1]:.4f}\\t is positive? {p}\\t actual {tmp_targets[i]}\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[42], line 2\n      1 # turn probabilites into category predictions\n----&gt; 2 tmp_is_positive = tmp_pred[:,1] &gt; tmp_pred[:,0]\n      3 for i, p in enumerate(tmp_is_positive):\n      4     print(f\"Neg log prob {tmp_pred[i,0]:.4f}\\tPos log prob {tmp_pred[i,1]:.4f}\\t is positive? {p}\\t actual {tmp_targets[i]}\")\n\nNameError: name 'tmp_pred' is not defined\n\n\n\nNotice that since you are making a prediction using a training batch, it‚Äôs more likely that the model‚Äôs predictions match the actual targets (labels).\n\nEvery prediction that the tweet is positive is also matching the actual target of 1 (positive sentiment).\nSimilarly, all predictions that the sentiment is not positive matches the actual target of 0 (negative sentiment)\n\nOne more useful thing to know is how to compare if the prediction is matching the actual target (label).\n- The result of calculation is_positive is a boolean. - The target is a type trax.fastmath.numpy.int32 - If you expect to be doing division, you may prefer to work with decimal numbers with the data type type trax.fastmath.numpy.int32\n\n# View the array of booleans\nprint(\"Array of booleans\")\ndisplay(tmp_is_positive)\n\n# convert boolean to type int32\n# True is converted to 1\n# False is converted to 0\ntmp_is_positive_int = tmp_is_positive.astype(np.int32)\n\n\n# View the array of integers\nprint(\"Array of integers\")\ndisplay(tmp_is_positive_int)\n\n# convert boolean to type float32\ntmp_is_positive_float = tmp_is_positive.astype(np.float32)\n\n# View the array of floats\nprint(\"Array of floats\")\ndisplay(tmp_is_positive_float)\n\nArray of booleans\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[43], line 3\n      1 # View the array of booleans\n      2 print(\"Array of booleans\")\n----&gt; 3 display(tmp_is_positive)\n      5 # convert boolean to type int32\n      6 # True is converted to 1\n      7 # False is converted to 0\n      8 tmp_is_positive_int = tmp_is_positive.astype(np.int32)\n\nNameError: name 'tmp_is_positive' is not defined\n\n\n\n\ntmp_pred.shape\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[44], line 1\n----&gt; 1 tmp_pred.shape\n\nNameError: name 'tmp_pred' is not defined\n\n\n\nNote that Python usually does type conversion for you when you compare a boolean to an integer - True compared to 1 is True, otherwise any other integer is False. - False compared to 0 is True, otherwise any ohter integer is False.\n\nprint(f\"True == 1: {True == 1}\")\nprint(f\"True == 2: {True == 2}\")\nprint(f\"False == 0: {False == 0}\")\nprint(f\"False == 2: {False == 2}\")\n\nTrue == 1: True\nTrue == 2: False\nFalse == 0: True\nFalse == 2: False\n\n\nHowever, we recommend that you keep track of the data type of your variables to avoid unexpected outcomes. So it helps to convert the booleans into integers - Compare 1 to 1 rather than comparing True to 1.\nHopefully you are now familiar with what kinds of inputs and outputs the model uses when making a prediction. - This will help you implement a function that estimates the accuracy of the model‚Äôs predictions."
  },
  {
    "objectID": "posts/c3w1/assignment.html#5.1",
    "href": "posts/c3w1/assignment.html#5.1",
    "title": "Assignment 1: Sentiment with Deep Neural Networks",
    "section": "5.1 Computing the accuracy on a batch",
    "text": "5.1 Computing the accuracy on a batch\nYou will now write a function that evaluates your model on the validation set and returns the accuracy.\n\npreds contains the predictions.\n\nIts dimensions are (batch_size, output_dim). output_dim is two in this case. Column 0 contains the probability that the tweet belongs to class 0 (negative sentiment). Column 1 contains probability that it belongs to class 1 (positive sentiment).\nIf the probability in column 1 is greater than the probability in column 0, then interpret this as the model‚Äôs prediction that the example has label 1 (positive sentiment).\n\nOtherwise, if the probabilities are equal or the probability in column 0 is higher, the model‚Äôs prediction is 0 (negative sentiment).\n\ny contains the actual labels.\ny_weights contains the weights to give to predictions.\n\n\nExercise 07\nImplement compute_accuracy.\n\n# UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: compute_accuracy\ndef compute_accuracy(preds, y, y_weights):\n    \"\"\"\n    Input: \n        preds: a tensor of shape (dim_batch, output_dim) \n        y: a tensor of shape (dim_batch, output_dim) with the true labels\n        y_weights: a n.ndarray with the a weight for each example\n    Output: \n        accuracy: a float between 0-1 \n        weighted_num_correct (np.float32): Sum of the weighted correct predictions\n        sum_weights (np.float32): Sum of the weights\n    \"\"\"\n    ### START CODE HERE (Replace instances of 'None' with your code) ###\n    # Create an array of booleans, \n    # True if the probability of positive sentiment is greater than\n    # the probability of negative sentiment\n    # else False\n    is_pos =  None\n\n    # convert the array of booleans into an array of np.int32\n    is_pos_int = None\n    \n    # compare the array of predictions (as int32) with the target (labels) of type int32\n    correct = None\n\n    # Count the sum of the weights.\n    sum_weights = None\n    \n    # convert the array of correct predictions (boolean) into an arrayof np.float32\n    correct_float = None\n    \n    # Multiply each prediction with its corresponding weight.\n    weighted_correct_float = None\n\n    # Sum up the weighted correct predictions (of type np.float32), to go in the\n    # denominator.\n    weighted_num_correct = None\n \n    # Divide the number of weighted correct predictions by the sum of the\n    # weights.\n    accuracy = None\n\n    ### END CODE HERE ###\n    return accuracy, weighted_num_correct, sum_weights\n\n\n# test your function\ntmp_val_generator = val_generator(64)\n\n# get one batch\ntmp_batch = next(tmp_val_generator)\n\n# Position 0 has the model inputs (tweets as tensors)\n# position 1 has the targets (the actual labels)\ntmp_inputs, tmp_targets, tmp_example_weights = tmp_batch\n\n# feed the tweet tensors into the model to get a prediction\ntmp_pred = training_loop.eval_model(tmp_inputs)\n\ntmp_acc, tmp_num_correct, tmp_num_predictions = compute_accuracy(preds=tmp_pred, y=tmp_targets, y_weights=tmp_example_weights)\n\nprint(f\"Model's prediction accuracy on a single training batch is: {100 * tmp_acc}%\")\nprint(f\"Weighted number of correct predictions {tmp_num_correct}; weighted number of total observations predicted {tmp_num_predictions}\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[47], line 2\n      1 # test your function\n----&gt; 2 tmp_val_generator = val_generator(64)\n      4 # get one batch\n      5 tmp_batch = next(tmp_val_generator)\n\nCell In[14], line 10, in val_generator(batch_size, shuffle)\n      9 def val_generator(batch_size, shuffle = False):\n---&gt; 10     return data_generator(val_pos, val_neg, batch_size, True, Vocab, shuffle)\n\nNameError: name 'data_generator' is not defined\n\n\n\n\nExpected output (Approximately)\nModel's prediction accuracy on a single training batch is: 100.0%\nWeighted number of correct predictions 64.0; weighted number of total observations predicted 64"
  },
  {
    "objectID": "posts/c3w1/assignment.html#5.2",
    "href": "posts/c3w1/assignment.html#5.2",
    "title": "Assignment 1: Sentiment with Deep Neural Networks",
    "section": "5.2 Testing your model on Validation Data",
    "text": "5.2 Testing your model on Validation Data\nNow you will write test your model‚Äôs prediction accuracy on validation data.\nThis program will take in a data generator and your model.\n\nThe generator allows you to get batches of data. You can use it with a for loop:\n\nfor batch in iterator: \n   # do something with that batch\nbatch has dimensions (X, Y, weights).\n\nColumn 0 corresponds to the tweet as a tensor (input).\nColumn 1 corresponds to its target (actual label, positive or negative sentiment).\nColumn 2 corresponds to the weights associated (example weights)\nYou can feed the tweet into model and it will return the predictions for the batch.\n\n ### Exercise 08\nInstructions: - Compute the accuracy over all the batches in the validation iterator. - Make use of compute_accuracy, which you recently implemented, and return the overall accuracy.\n\n# UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: test_model\ndef test_model(generator, model):\n    '''\n    Input: \n        generator: an iterator instance that provides batches of inputs and targets\n        model: a model instance \n    Output: \n        accuracy: float corresponding to the accuracy\n    '''\n    \n    accuracy = 0.\n    total_num_correct = 0\n    total_num_pred = 0\n    \n    ### START CODE HERE (Replace instances of 'None' with your code) ###\n    for batch in generator: \n        \n        # Retrieve the inputs from the batch\n        inputs = None\n        \n        # Retrieve the targets (actual labels) from the batch\n        targets = None\n        \n        # Retrieve the example weight.\n        example_weight = None\n\n        # Make predictions using the inputs\n        pred = None\n        \n        # Calculate accuracy for the batch by comparing its predictions and targets\n        batch_accuracy, batch_num_correct, batch_num_pred = None\n        \n        # Update the total number of correct predictions\n        # by adding the number of correct predictions from this batch\n        total_num_correct += None\n        \n        # Update the total number of predictions \n        # by adding the number of predictions made for the batch\n        total_num_pred += None\n\n    # Calculate accuracy over all examples\n    accuracy = None\n    \n    ### END CODE HERE ###\n    return accuracy\n\n\n# DO NOT EDIT THIS CELL\n# testing the accuracy of your model: this takes around 20 seconds\nmodel = training_loop.eval_model\naccuracy = test_model(test_generator(16), model)\n\nprint(f'The accuracy of your model on the validation set is {accuracy:.4f}', )\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[49], line 3\n      1 # DO NOT EDIT THIS CELL\n      2 # testing the accuracy of your model: this takes around 20 seconds\n----&gt; 3 model = training_loop.eval_model\n      4 accuracy = test_model(test_generator(16), model)\n      6 print(f'The accuracy of your model on the validation set is {accuracy:.4f}', )\n\nNameError: name 'training_loop' is not defined\n\n\n\n\nExpected Output (Approximately)\nThe accuracy of your model on the validation set is 0.9931"
  },
  {
    "objectID": "posts/c3w1/lab01.html",
    "href": "posts/c3w1/lab01.html",
    "title": "Trax : Ungraded Lecture Notebook",
    "section": "",
    "text": "course banner\nIn this notebook you‚Äôll get to know about the Trax framework and learn about some of its basic building blocks."
  },
  {
    "objectID": "posts/c3w1/lab01.html#background",
    "href": "posts/c3w1/lab01.html#background",
    "title": "Trax : Ungraded Lecture Notebook",
    "section": "Background",
    "text": "Background\n\nWhy Trax and not TensorFlow or PyTorch?\nTensorFlow and PyTorch are both extensive frameworks that can do almost anything in deep learning. They offer a lot of flexibility, but that often means verbosity of syntax and extra time to code.\nTrax is much more concise. It runs on a TensorFlow backend but allows you to train models with 1 line commands. Trax also runs end to end, allowing you to get data, model and train all with a single terse statements. This means you can focus on learning, instead of spending hours on the idiosyncrasies of big framework implementation.\n\n\nWhy not Keras then?\nKeras is now part of Tensorflow itself from 2.0 onwards. Also, Trax is good for implementing new state of the art algorithms like Transformers, Reformers, BERT because it is actively maintained by Google Brain Team for advanced deep learning tasks. It runs smoothly on CPUs, GPUs and TPUs as well with comparatively lesser modifications in code.\n\n\nHow to Code in Trax\nBuilding models in Traxrelies on 2 key concepts:\n\nlayers and\ncombinators.\n\nTrax layers are simple objects that process data and perform computations. They can be chained together into composite layers using Trax combinators, allowing you to build layers and models of any complexity.\n\n\nTrax, JAX, TensorFlow and Tensor2Tensor\nYou already know that Trax uses Tensorflow as a backend, but it also uses the JAX library to speed up computation too. You can view JAX as an enhanced and optimized version of numpy.\nWatch out for assignments which import import trax.fastmath.numpy as np. If you see this line, remember that when calling np you are really calling Trax‚Äôs version of numpy that is compatible with JAX.\nAs a result of this, where you used to encounter the type numpy.ndarray now you will find the type jax.interpreters.xla.DeviceArray.\nTensor2Tensor is another name you might have heard. It started as an end to end solution much like how Trax is designed, but it grew unwieldy and complicated. So you can view Trax as the new improved version that operates much faster and simpler.\n\n\nResources\n\nTrax source code can be found on Github: Trax\nJAX library: JAX"
  },
  {
    "objectID": "posts/c3w1/lab01.html#installing-trax",
    "href": "posts/c3w1/lab01.html#installing-trax",
    "title": "Trax : Ungraded Lecture Notebook",
    "section": "Installing Trax",
    "text": "Installing Trax\nTrax has dependencies on JAX and some libraries like JAX which are yet to be supported in Windows but work well in Ubuntu and MacOS. We would suggest that if you are working on Windows, try to install Trax on WSL2.\nOfficial maintained documentation - trax-ml not to be confused with this TraX\n\n#!pip install trax==1.3.1 Use this version for this notebook"
  },
  {
    "objectID": "posts/c3w1/lab01.html#imports",
    "href": "posts/c3w1/lab01.html#imports",
    "title": "Trax : Ungraded Lecture Notebook",
    "section": "Imports",
    "text": "Imports\n\nimport numpy as np  # regular ol' numpy\n\nfrom trax import layers as tl  # core building block\nfrom trax import shapes  # data signatures: dimensionality and type\nfrom trax import fastmath  # uses jax, offers numpy on steroids\n\n2025-02-05 21:54:57.883692: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1738785297.899560  613536 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1738785297.904994  613536 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\n\n\n# Trax version 1.3.1 or better \n!pip list | grep trax\n\ntrax                         1.4.1"
  },
  {
    "objectID": "posts/c3w1/lab01.html#layers",
    "href": "posts/c3w1/lab01.html#layers",
    "title": "Trax : Ungraded Lecture Notebook",
    "section": "Layers",
    "text": "Layers\nLayers are the core building blocks in Trax or as mentioned in the lectures, they are the base classes.\nThey take inputs, compute functions/custom calculations and return outputs.\nYou can also inspect layer properties. Let me show you some examples.\n\nRelu Layer\nFirst I‚Äôll show you how to build a relu activation function as a layer. A layer like this is one of the simplest types. Notice there is no object initialization so it works just like a math function.\nNote: Activation functions are also layers in Trax, which might look odd if you have been using other frameworks for a longer time.\n\n# Layers\n# Create a relu trax layer\nrelu = tl.Relu()\n\n# Inspect properties\nprint(\"-- Properties --\")\nprint(\"name :\", relu.name)\nprint(\"expected inputs :\", relu.n_in)\nprint(\"promised outputs :\", relu.n_out, \"\\n\")\n\n# Inputs\nx = np.array([-2, -1, 0, 1, 2])\nprint(\"-- Inputs --\")\nprint(\"x :\", x, \"\\n\")\n\n# Outputs\ny = relu(x)\nprint(\"-- Outputs --\")\nprint(\"y :\", y)\n\n-- Properties --\nname : Serial\nexpected inputs : 1\npromised outputs : 1 \n\n-- Inputs --\nx : [-2 -1  0  1  2] \n\n-- Outputs --\ny : [0 0 0 1 2]\n\n\n\n\nConcatenate Layer\nNow I‚Äôll show you how to build a layer that takes 2 inputs. Notice the change in the expected inputs property from 1 to 2.\n\n# Create a concatenate trax layer\nconcat = tl.Concatenate()\nprint(\"-- Properties --\")\nprint(\"name :\", concat.name)\nprint(\"expected inputs :\", concat.n_in)\nprint(\"promised outputs :\", concat.n_out, \"\\n\")\n\n# Inputs\nx1 = np.array([-10, -20, -30])\nx2 = x1 / -10\nprint(\"-- Inputs --\")\nprint(\"x1 :\", x1)\nprint(\"x2 :\", x2, \"\\n\")\n\n# Outputs\ny = concat([x1, x2])\nprint(\"-- Outputs --\")\nprint(\"y :\", y)\n\n-- Properties --\nname : Concatenate\nexpected inputs : 2\npromised outputs : 1 \n\n-- Inputs --\nx1 : [-10 -20 -30]\nx2 : [1. 2. 3.] \n\n-- Outputs --\ny : [-10. -20. -30.   1.   2.   3.]"
  },
  {
    "objectID": "posts/c3w1/lab01.html#layers-are-configurable",
    "href": "posts/c3w1/lab01.html#layers-are-configurable",
    "title": "Trax : Ungraded Lecture Notebook",
    "section": "Layers are Configurable",
    "text": "Layers are Configurable\nYou can change the default settings of layers. For example, you can change the expected inputs for a concatenate layer from 2 to 3 using the optional parameter n_items.\n\n# Configure a concatenate layer\nconcat_3 = tl.Concatenate(n_items=3)  # configure the layer's expected inputs\nprint(\"-- Properties --\")\nprint(\"name :\", concat_3.name)\nprint(\"expected inputs :\", concat_3.n_in)\nprint(\"promised outputs :\", concat_3.n_out, \"\\n\")\n\n# Inputs\nx1 = np.array([-10, -20, -30])\nx2 = x1 / -10\nx3 = x2 * 0.99\nprint(\"-- Inputs --\")\nprint(\"x1 :\", x1)\nprint(\"x2 :\", x2)\nprint(\"x3 :\", x3, \"\\n\")\n\n# Outputs\ny = concat_3([x1, x2, x3])\nprint(\"-- Outputs --\")\nprint(\"y :\", y)\n\n-- Properties --\nname : Concatenate\nexpected inputs : 3\npromised outputs : 1 \n\n-- Inputs --\nx1 : [-10 -20 -30]\nx2 : [1. 2. 3.]\nx3 : [0.99 1.98 2.97] \n\n-- Outputs --\ny : [-10.   -20.   -30.     1.     2.     3.     0.99   1.98   2.97]\n\n\nNote: At any point,if you want to refer the function help/ look up the documentation or use help function.\n\n#help(tl.Concatenate) #Uncomment this to see the function docstring with explaination"
  },
  {
    "objectID": "posts/c3w1/lab01.html#layers-can-have-weights",
    "href": "posts/c3w1/lab01.html#layers-can-have-weights",
    "title": "Trax : Ungraded Lecture Notebook",
    "section": "Layers can have Weights",
    "text": "Layers can have Weights\nSome layer types include mutable weights and biases that are used in computation and training. Layers of this type require initialization before use.\nFor example the LayerNorm layer calculates normalized data, that is also scaled by weights and biases. During initialization you pass the data shape and data type of the inputs, so the layer can initialize compatible arrays of weights and biases.\n\n# Uncomment any of them to see information regarding the function\n# help(tl.LayerNorm)\n# help(shapes.signature)\n\n\n# Layer initialization\nnorm = tl.LayerNorm()\n# You first must know what the input data will look like\nx = np.array([0, 1, 2, 3], dtype=\"float\")\n\n# Use the input data signature to get shape and type for initializing weights and biases\nnorm.init(shapes.signature(x)) # We need to convert the input datatype from usual tuple to trax ShapeDtype\n\nprint(\"Normal shape:\",x.shape, \"Data Type:\",type(x.shape))\nprint(\"Shapes Trax:\",shapes.signature(x),\"Data Type:\",type(shapes.signature(x)))\n\n# Inspect properties\nprint(\"-- Properties --\")\nprint(\"name :\", norm.name)\nprint(\"expected inputs :\", norm.n_in)\nprint(\"promised outputs :\", norm.n_out)\n# Weights and biases\nprint(\"weights :\", norm.weights[0])\nprint(\"biases :\", norm.weights[1], \"\\n\")\n\n# Inputs\nprint(\"-- Inputs --\")\nprint(\"x :\", x)\n\n# Outputs\ny = norm(x)\nprint(\"-- Outputs --\")\nprint(\"y :\", y)\n\n/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/normalization.py:141: UserWarning: Explicitly requested dtype float64 requested in ones is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n  scale = jnp.ones(features, dtype=input_signature.dtype)\n/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/normalization.py:142: UserWarning: Explicitly requested dtype float64 requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n  bias = jnp.zeros(features, dtype=input_signature.dtype)\n\n\n((Array([1., 1., 1., 1.], dtype=float32),\n  Array([0., 0., 0., 0.], dtype=float32)),\n ())\n\n\nNormal shape: (4,) Data Type: &lt;class 'tuple'&gt;\nShapes Trax: ShapeDtype{shape:(4,), dtype:float64} Data Type: &lt;class 'trax.shapes.ShapeDtype'&gt;\n-- Properties --\nname : LayerNorm\nexpected inputs : 1\npromised outputs : 1\nweights : [1. 1. 1. 1.]\nbiases : [0. 0. 0. 0.] \n\n-- Inputs --\nx : [0. 1. 2. 3.]\n-- Outputs --\ny : [-1.3416404  -0.44721344  0.44721344  1.3416404 ]"
  },
  {
    "objectID": "posts/c3w1/lab01.html#custom-layers",
    "href": "posts/c3w1/lab01.html#custom-layers",
    "title": "Trax : Ungraded Lecture Notebook",
    "section": "Custom Layers",
    "text": "Custom Layers\nThis is where things start getting more interesting! You can create your own custom layers too and define custom functions for computations by using tl.Fn. Let me show you how.\n\nhelp(tl.Fn)\n\nHelp on function Fn in module trax.layers.base:\n\nFn(name, f, n_out=1)\n    Returns a layer with no weights that applies the function `f`.\n    \n    `f` can take and return any number of arguments, and takes only positional\n    arguments -- no default or keyword arguments. It often uses JAX-numpy (`jnp`).\n    The following, for example, would create a layer that takes two inputs and\n    returns two outputs -- element-wise sums and maxima:\n    \n        `Fn('SumAndMax', lambda x0, x1: (x0 + x1, jnp.maximum(x0, x1)), n_out=2)`\n    \n    The layer's number of inputs (`n_in`) is automatically set to number of\n    positional arguments in `f`, but you must explicitly set the number of\n    outputs (`n_out`) whenever it's not the default value 1.\n    \n    Args:\n      name: Class-like name for the resulting layer; for use in debugging.\n      f: Pure function from input tensors to output tensors, where each input\n          tensor is a separate positional arg, e.g., `f(x0, x1) --&gt; x0 + x1`.\n          Output tensors must be packaged as specified in the `Layer` class\n          docstring.\n      n_out: Number of outputs promised by the layer; default value 1.\n    \n    Returns:\n      Layer executing the function `f`.\n\n\n\n\n# Define a custom layer\n# In this example you will create a layer to calculate the input times 2\n\ndef TimesTwo():\n    layer_name = \"TimesTwo\" #don't forget to give your custom layer a name to identify\n\n    # Custom function for the custom layer\n    def func(x):\n        return x * 2\n\n    return tl.Fn(layer_name, func)\n\n\n# Test it\ntimes_two = TimesTwo()\n\n# Inspect properties\nprint(\"-- Properties --\")\nprint(\"name :\", times_two.name)\nprint(\"expected inputs :\", times_two.n_in)\nprint(\"promised outputs :\", times_two.n_out, \"\\n\")\n\n# Inputs\nx = np.array([1, 2, 3])\nprint(\"-- Inputs --\")\nprint(\"x :\", x, \"\\n\")\n\n# Outputs\ny = times_two(x)\nprint(\"-- Outputs --\")\nprint(\"y :\", y)\n\n-- Properties --\nname : TimesTwo\nexpected inputs : 1\npromised outputs : 1 \n\n-- Inputs --\nx : [1 2 3] \n\n-- Outputs --\ny : [2 4 6]"
  },
  {
    "objectID": "posts/c3w1/lab01.html#combinators",
    "href": "posts/c3w1/lab01.html#combinators",
    "title": "Trax : Ungraded Lecture Notebook",
    "section": "Combinators",
    "text": "Combinators\nYou can combine layers to build more complex layers. Trax provides a set of objects named combinator layers to make this happen. Combinators are themselves layers, so behavior commutes.\n\nSerial Combinator\nThis is the most common and easiest to use. For example could build a simple neural network by combining layers into a single layer using the Serial combinator. This new layer then acts just like a single layer, so you can inspect intputs, outputs and weights. Or even combine it into another layer! Combinators can then be used as trainable models. Try adding more layers\nNote:As you must have guessed, if there is serial combinator, there must be a parallel combinator as well. Do try to explore about combinators and other layers from the trax documentation and look at the repo to understand how these layers are written.\n\n# help(tl.Serial)\n# help(tl.Parallel)\n\n\n# Serial combinator\nserial = tl.Serial(\n    tl.LayerNorm(),         # normalize input\n    tl.Relu(),              # convert negative values to zero\n    times_two,              # the custom layer you created above, multiplies the input recieved from above by 2\n    \n    ### START CODE HERE\n#     tl.Dense(n_units=2),  # try adding more layers. eg uncomment these lines\n#     tl.Dense(n_units=1),  # Binary classification, maybe? uncomment at your own peril\n#     tl.LogSoftmax()       # Yes, LogSoftmax is also a layer\n    ### END CODE HERE\n)\n\n# Initialization\nx = np.array([-2, -1, 0, 1, 2]) #input\nserial.init(shapes.signature(x)) #initialising serial instance\n\nprint(\"-- Serial Model --\")\nprint(serial,\"\\n\")\nprint(\"-- Properties --\")\nprint(\"name :\", serial.name)\nprint(\"sublayers :\", serial.sublayers)\nprint(\"expected inputs :\", serial.n_in)\nprint(\"promised outputs :\", serial.n_out)\nprint(\"weights & biases:\", serial.weights, \"\\n\")\n\n# Inputs\nprint(\"-- Inputs --\")\nprint(\"x :\", x, \"\\n\")\n\n# Outputs\ny = serial(x)\nprint(\"-- Outputs --\")\nprint(\"y :\", y)\n\n/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/normalization.py:141: UserWarning: Explicitly requested dtype int64 requested in ones is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n  scale = jnp.ones(features, dtype=input_signature.dtype)\n/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/normalization.py:142: UserWarning: Explicitly requested dtype int64 requested in zeros is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n  bias = jnp.zeros(features, dtype=input_signature.dtype)\n\n\n(((Array([1, 1, 1, 1, 1], dtype=int32), Array([0, 0, 0, 0, 0], dtype=int32)),\n  ((), (), ()),\n  ()),\n ((), ((), (), ()), ()))\n\n\n-- Serial Model --\nSerial[\n  LayerNorm\n  Serial[\n    Relu\n  ]\n  TimesTwo\n] \n\n-- Properties --\nname : Serial\nsublayers : [LayerNorm, Serial[\n  Relu\n], TimesTwo]\nexpected inputs : 1\npromised outputs : 1\nweights & biases: ((Array([1, 1, 1, 1, 1], dtype=int32), Array([0, 0, 0, 0, 0], dtype=int32)), ((), (), ()), ()) \n\n-- Inputs --\nx : [-2 -1  0  1  2] \n\n-- Outputs --\ny : [0.        0.        0.        1.4142132 2.8284264]"
  },
  {
    "objectID": "posts/c3w1/lab01.html#jax",
    "href": "posts/c3w1/lab01.html#jax",
    "title": "Trax : Ungraded Lecture Notebook",
    "section": "JAX",
    "text": "JAX\nJust remember to lookout for which numpy you are using, the regular ol‚Äô numpy or Trax‚Äôs JAX compatible numpy. Both tend to use the alias np so watch those import blocks.\nNote:There are certain things which are still not possible in fastmath.numpy which can be done in numpy so you will see in assignments we will switch between them to get our work done.\n\n# Numpy vs fastmath.numpy have different data types\n# Regular ol' numpy\nx_numpy = np.array([1, 2, 3])\nprint(\"good old numpy : \", type(x_numpy), \"\\n\")\n\n# Fastmath and jax numpy\nx_jax = fastmath.numpy.array([1, 2, 3])\nprint(\"jax trax numpy : \", type(x_jax))\n\ngood old numpy :  &lt;class 'numpy.ndarray'&gt; \n\njax trax numpy :  &lt;class 'jaxlib.xla_extension.ArrayImpl'&gt;"
  },
  {
    "objectID": "posts/c3w1/lab01.html#summary",
    "href": "posts/c3w1/lab01.html#summary",
    "title": "Trax : Ungraded Lecture Notebook",
    "section": "Summary",
    "text": "Summary\nTrax is a concise framework, built on TensorFlow, for end to end machine learning. The key building blocks are layers and combinators. This notebook is just a taste, but sets you up with some key inuitions to take forward into the rest of the course and assignments where you will build end to end models."
  },
  {
    "objectID": "posts/c3w1/index.html",
    "href": "posts/c3w1/index.html",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nThis week‚Äôs slides\n\n\n\n\nFigure¬†1\nMy irreverent notes for Week 1 of the Natural Language Processing with Sequence Models Course in the Natural Language Processing Specialization Offered by DeepLearning.AI on Coursera",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c3w1/index.html#neural-networks-for-sentiment-analysis",
    "href": "posts/c3w1/index.html#neural-networks-for-sentiment-analysis",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "Neural Networks for Sentiment Analysis",
    "text": "Neural Networks for Sentiment Analysis\nPreviously in the course we did sentiment analysis with logistic regression and naive Bayes. Those models were in a sense more naive, and are not able to catch the sentiment off a tweet like: ‚ÄúI am not happy‚Äù or ‚ÄúIf only it was a good day‚Äù. When using a neural network to predict the sentiment of a sentence, we can use the following. Note that the image below has three outputs, in this case we might want to predict, ‚Äúpositive‚Äù, ‚Äúneutral‚Äù, or ‚Äúnegative‚Äù.\n\nNote that the network above has three layers. To go from one layer to another we can use a W matrix to propagate to the next layer. Hence, we call this concept of going from the input until the final layer, forward propagation. To represent a tweet, we can use the following:\nNote, that we add zeros for padding to match the size of the longest tweet.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c3w1/index.html#trax-neural-networks",
    "href": "posts/c3w1/index.html#trax-neural-networks",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "Trax: Neural Networks",
    "text": "Trax: Neural Networks\nTrax has several advantages:\n\nRuns fast on CPUs, GPUs and TPUs\nParallel computing\nRecord algebraic computations for gradient evaluation\n\nHere is an example of how we can code a neural network in Trax:",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c3w1/index.html#reading-optional-trax-and-jax-docs-and-code",
    "href": "posts/c3w1/index.html#reading-optional-trax-and-jax-docs-and-code",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "Reading: (Optional) Trax and JAX, docs and code",
    "text": "Reading: (Optional) Trax and JAX, docs and code\nOfficial Trax documentation maintained by the Google Brain team:\n\nhttps://trax-ml.readthedocs.io/en/latest/\n\nTrax source code on GitHub:\n\nhttps://github.com/google/trax\n\nJAX library:\n\nhttps://jax.readthedocs.io/en/latest/index.html",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c3w1/index.html#lab-introduction-to-trax",
    "href": "posts/c3w1/index.html#lab-introduction-to-trax",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "Lab: Introduction to Trax",
    "text": "Lab: Introduction to Trax\nIntroduction to Trax",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c3w1/index.html#trax-layers",
    "href": "posts/c3w1/index.html#trax-layers",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "Trax: Layers",
    "text": "Trax: Layers\nTrax makes use of classes. If we are not familiar with classes in python, don‚Äôt worry about it, here is an example.\n\nIn the example above, we can see that a class takes in an __init__ and a __call__ method.\nThese methods allow we to initialize your internal variables and allow we to execute your function when called.\nTo the right we can see how we can initialize your class. When we call MyClass(7) , we are setting the y variable to 7. Now when we call f(3) we are adding 7 + 3.\nWe can change the my_method function to do whatever we want, and we can have as many methods as we want in a class.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c3w1/index.html#lab-classes-and-subclasses",
    "href": "posts/c3w1/index.html#lab-classes-and-subclasses",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "Lab: Classes and Subclasses",
    "text": "Lab: Classes and Subclasses\nClasses and Subclasses",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c3w1/index.html#dense-and-relu-layer",
    "href": "posts/c3w1/index.html#dense-and-relu-layer",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "Dense and ReLU layer",
    "text": "Dense and ReLU layer\nThe Dense layer is the computation of the inner product between a set of trainable weights (weight matrix) and an input vector. The visualization of the dense layer could be seen in the image below.\n\nThe orange box shows the dense layer. An activation layer is the set of blue nodes. Concretely one of the most commonly used activation layers is the rectified linear unit (ReLU).\n\nReLU(x) is defined as max(0,x) for any input x.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c3w1/index.html#serial-layer",
    "href": "posts/c3w1/index.html#serial-layer",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "Serial Layer",
    "text": "Serial Layer\nA serial layer allows we to compose layers in a serial arrangement:\n\nIt is a composition of sublayers. These layers are usually dense layers followed by activation layers.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c3w1/index.html#other-layers",
    "href": "posts/c3w1/index.html#other-layers",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "Other Layers",
    "text": "Other Layers\nOther layers could include embedding layers and mean layers. For example, we can learn word embeddings for each word in your vocabulary as follows:\n\nThe mean layer allows we to take the average of the embeddings. We can visualize it as follows:\n\nThis layer does not have any trainable parameters.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c3w1/index.html#training",
    "href": "posts/c3w1/index.html#training",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "Training",
    "text": "Training\nIn Trax, the function grad allows we to compute the gradient. We can use it as follows:\n\nNow if we were to evaluate grad_f at a certain value, namely z, it would be the same as computing 6z+1. Now to do the training, it becomes very simple:\n\nWe simply compute the gradients by feeding in y.forward (the latest value of y), the weights, and the input x, and then it does the back-propagation for we in a single line. We can then have the loop that allows we to update the weights (i.e.¬†gradient descent!).",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c3w1/index.html#lab-data-generators",
    "href": "posts/c3w1/index.html#lab-data-generators",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "Lab: Data Generators",
    "text": "Lab: Data Generators\nData Generators",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c2w3/assignment.html",
    "href": "posts/c2w3/assignment.html",
    "title": "Language Models: Auto-Complete",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure¬†1\nIn this assignment, you will build an auto-complete system. Auto-complete system is something you may see every day - When you google something, you often have suggestions to help you complete your search. - When you are writing an email, you get suggestions telling you possible endings to your sentence.\nBy the end of this assignment, you will develop a prototype of such a system.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "A3 - Auto-Complete"
    ]
  },
  {
    "objectID": "posts/c2w3/assignment.html#outline",
    "href": "posts/c2w3/assignment.html#outline",
    "title": "Language Models: Auto-Complete",
    "section": "Outline",
    "text": "Outline\n\n1 Load and Preprocess Data\n1.1: Load the data\n1.2 Pre-process the data\n\nExercise 01\nExercise 02\nExercise 03\nExercise 04\nExercise 05\nExercise 06\nExercise 07\n\n2 Develop n-gram based language models\n\nExercise 08\nExercise 09\n\n\n3 Perplexity\n\nExercise 10\n\n4 Build an auto-complete system\n\nExercise 11\n\n\nA key building block for an auto-complete system is a language model. A language model assigns the probability to a sequence of words, in a way that more ‚Äúlikely‚Äù sequences receive higher scores. For example, &gt;‚ÄúI have a pen‚Äù is expected to have a higher probability than &gt;‚ÄúI am a pen‚Äù since the first one seems to be a more natural sentence in the real world.\nYou can take advantage of this probability calculation to develop an auto-complete system.\nSuppose the user typed &gt;‚ÄúI eat scrambled‚Äù Then you can find a word x such that ‚ÄúI eat scrambled x‚Äù receives the highest probability. If x = ‚Äúeggs‚Äù, the sentence would be &gt;‚ÄúI eat scrambled eggs‚Äù\nWhile a variety of language models have been developed, this assignment uses N-grams, a simple but powerful method for language modeling. - N-grams are also used in machine translation and speech recognition.\nHere are the steps of this assignment:\n\nLoad and preprocess data\n\nLoad and tokenize data.\nSplit the sentences into train and test sets.\nReplace words with a low frequency by an unknown marker &lt;unk&gt;.\n\nDevelop N-gram based language models\n\nCompute the count of n-grams from a given data set.\nEstimate the conditional probability of a next word with k-smoothing.\n\nEvaluate the N-gram models by computing the perplexity score.\nUse your own model to suggest an upcoming word given your sentence.\n\n\nimport math\nimport random\nimport numpy as np\nimport pandas as pd\nimport nltk\nnltk.data.path.append('.')\n\n ## Part 1: Load and Preprocess Data\n ### Part 1.1: Load the data You will use twitter data. Load the data and view the first few sentences by running the next cell.\nNotice that data is a long string that contains many many tweets. Observe that there is a line break ‚Äú‚Äù between tweets.\n\nwith open(\"en_US.twitter.txt\", \"r\") as f:\n    data = f.read()\nprint(\"Data type:\", type(data))\nprint(\"Number of letters:\", len(data))\nprint(\"First 300 letters of the data\")\nprint(\"-------\")\ndisplay(data[0:300])\nprint(\"-------\")\n\nprint(\"Last 300 letters of the data\")\nprint(\"-------\")\ndisplay(data[-300:])\nprint(\"-------\")\n\nData type: &lt;class 'str'&gt;\nNumber of letters: 3335477\nFirst 300 letters of the data\n-------\n\n\n\"How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\\nWhen you meet someone special... you'll know. Your heart will beat more rapidly and you'll smile for no reason.\\nthey've decided its more fun if I don't.\\nSo Tired D; Played Lazer Tag & Ran A \"\n\n\n-------\nLast 300 letters of the data\n-------\n\n\n\"ust had one a few weeks back....hopefully we will be back soon! wish you the best yo\\nColombia is with an 'o'...‚Äú: We now ship to 4 countries in South America (fist pump). Please welcome Columbia to the Stunner Family‚Äù\\n#GutsiestMovesYouCanMake Giving a cat a bath.\\nCoffee after 5 was a TERRIBLE idea.\\n\"\n\n\n-------\n\n\n ### Part 1.2 Pre-process the data\nPreprocess this data with the following steps:\n\nSplit data into sentences using ‚Äú‚Äù as the delimiter.\nSplit each sentence into tokens. Note that in this assignment we use ‚Äútoken‚Äù and ‚Äúwords‚Äù interchangeably.\nAssign sentences into train or test sets.\nFind tokens that appear at least N times in the training data.\nReplace tokens that appear less than N times by &lt;unk&gt;\n\nNote: we omit validation data in this exercise. - In real applications, we should hold a part of data as a validation set and use it to tune our training. - We skip this process for simplicity.\n ### Exercise 01\nSplit data into sentences.\n\n\nHints\n\n\n\n\nUse str.split\n\n\n\n\n# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n### GRADED_FUNCTION: split_to_sentences ###\ndef split_to_sentences(data):\n    \"\"\"\n    Split data by linebreak \"\\n\"\n    \n    Args:\n        data: str\n    \n    Returns:\n        A list of sentences\n    \"\"\"\n    ### START CODE HERE (Replace instances of 'None' with your code) ###\n    sentences = None\n    ### END CODE HERE ###\n    \n    # Additional clearning (This part is already implemented)\n    # - Remove leading and trailing spaces from each sentence\n    # - Drop sentences if they are empty strings.\n    sentences = [s.strip() for s in sentences]\n    sentences = [s for s in sentences if len(s) &gt; 0]\n    \n    return sentences    \n\n\n# test your code\nx = \"\"\"\nI have a pen.\\nI have an apple. \\nAh\\nApple pen.\\n\n\"\"\"\nprint(x)\n\nsplit_to_sentences(x)\n\n\nI have a pen.\nI have an apple. \nAh\nApple pen.\n\n\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[4], line 7\n      2 x = \"\"\"\n      3 I have a pen.\\nI have an apple. \\nAh\\nApple pen.\\n\n      4 \"\"\"\n      5 print(x)\n----&gt; 7 split_to_sentences(x)\n\nCell In[3], line 20, in split_to_sentences(data)\n     14 sentences = None\n     15 ### END CODE HERE ###\n     16 \n     17 # Additional clearning (This part is already implemented)\n     18 # - Remove leading and trailing spaces from each sentence\n     19 # - Drop sentences if they are empty strings.\n---&gt; 20 sentences = [s.strip() for s in sentences]\n     21 sentences = [s for s in sentences if len(s) &gt; 0]\n     23 return sentences\n\nTypeError: 'NoneType' object is not iterable\n\n\n\nExpected answer:\n['I have a pen.', 'I have an apple.', 'Ah', 'Apple pen.']\n ### Exercise 02 The next step is to tokenize sentences (split a sentence into a list of words). - Convert all tokens into lower case so that words which are capitalized (for example, at the start of a sentence) in the original text are treated the same as the lowercase versions of the words. - Append each tokenized list of words into a list of tokenized sentences.\n\n\nHints\n\n\n\n\nUse str.lower to convert strings to lowercase.\n\n\nPlease use nltk.word_tokenize to split sentences into tokens.\n\n\nIf you used str.split insteaad of nltk.word_tokenize, there are additional edge cases to handle, such as the punctuation (comma, period) that follows a word.\n\n\n\n\n# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n### GRADED_FUNCTION: tokenize_sentences ###\ndef tokenize_sentences(sentences):\n    \"\"\"\n    Tokenize sentences into tokens (words)\n    \n    Args:\n        sentences: List of strings\n    \n    Returns:\n        List of lists of tokens\n    \"\"\"\n    \n    # Initialize the list of lists of tokenized sentences\n    tokenized_sentences = []\n    ### START CODE HERE (Replace instances of 'None' with your code) ###\n    \n    # Go through each sentence\n    for sentence in None:\n        \n        # Convert to lowercase letters\n        sentence = None\n        \n        # Convert into a list of words\n        tokenized = None\n        \n        # append the list of words to the list of lists\n        None\n    \n    ### END CODE HERE ###\n    \n    return tokenized_sentences\n\n\n# test your code\nsentences = [\"Sky is blue.\", \"Leaves are green.\", \"Roses are red.\"]\ntokenize_sentences(sentences)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[6], line 3\n      1 # test your code\n      2 sentences = [\"Sky is blue.\", \"Leaves are green.\", \"Roses are red.\"]\n----&gt; 3 tokenize_sentences(sentences)\n\nCell In[5], line 19, in tokenize_sentences(sentences)\n     15 tokenized_sentences = []\n     16 ### START CODE HERE (Replace instances of 'None' with your code) ###\n     17 \n     18 # Go through each sentence\n---&gt; 19 for sentence in None:\n     20     \n     21     # Convert to lowercase letters\n     22     sentence = None\n     24     # Convert into a list of words\n\nTypeError: 'NoneType' object is not iterable\n\n\n\n\nExpected output\n[['sky', 'is', 'blue', '.'],\n ['leaves', 'are', 'green', '.'],\n ['roses', 'are', 'red', '.']]\n ### Exercise 03\nUse the two functions that you have just implemented to get the tokenized data. - split the data into sentences - tokenize those sentences\n\n# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n### GRADED_FUNCTION: get_tokenized_data ###\ndef get_tokenized_data(data):\n    \"\"\"\n    Make a list of tokenized sentences\n    \n    Args:\n        data: String\n    \n    Returns:\n        List of lists of tokens\n    \"\"\"\n    ### START CODE HERE (Replace instances of 'None' with your code) ###\n    \n    # Get the sentences by splitting up the data\n    sentences = None\n    \n    # Get the list of lists of tokens by tokenizing the sentences\n    tokenized_sentences = None\n    \n    ### END CODE HERE ###\n    \n    return tokenized_sentences\n\n\n# test your function\nx = \"Sky is blue.\\nLeaves are green\\nRoses are red.\"\nget_tokenized_data(x)\n\n\nExpected outcome\n[['sky', 'is', 'blue', '.'],\n ['leaves', 'are', 'green'],\n ['roses', 'are', 'red', '.']]\n\n\n\nSplit into train and test sets\nNow run the cell below to split data into training and test sets.\n\ntokenized_data = get_tokenized_data(data)\nrandom.seed(87)\nrandom.shuffle(tokenized_data)\n\ntrain_size = int(len(tokenized_data) * 0.8)\ntrain_data = tokenized_data[0:train_size]\ntest_data = tokenized_data[train_size:]\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[9], line 3\n      1 tokenized_data = get_tokenized_data(data)\n      2 random.seed(87)\n----&gt; 3 random.shuffle(tokenized_data)\n      5 train_size = int(len(tokenized_data) * 0.8)\n      6 train_data = tokenized_data[0:train_size]\n\nFile /usr/lib/python3.10/random.py:391, in Random.shuffle(self, x, random)\n    389 if random is None:\n    390     randbelow = self._randbelow\n--&gt; 391     for i in reversed(range(1, len(x))):\n    392         # pick an element in x[:i+1] with which to exchange x[i]\n    393         j = randbelow(i + 1)\n    394         x[i], x[j] = x[j], x[i]\n\nTypeError: object of type 'NoneType' has no len()\n\n\n\n\nprint(\"{} data are split into {} train and {} test set\".format(\n    len(tokenized_data), len(train_data), len(test_data)))\n\nprint(\"First training sample:\")\nprint(train_data[0])\n      \nprint(\"First test sample\")\nprint(test_data[0])\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[10], line 2\n      1 print(\"{} data are split into {} train and {} test set\".format(\n----&gt; 2     len(tokenized_data), len(train_data), len(test_data)))\n      4 print(\"First training sample:\")\n      5 print(train_data[0])\n\nTypeError: object of type 'NoneType' has no len()\n\n\n\n\nExpected output\n47961 data are split into 38368 train and 9593 test set\nFirst training sample:\n['i', 'personally', 'would', 'like', 'as', 'our', 'official', 'glove', 'of', 'the', 'team', 'local', 'company', 'and', 'quality', 'production']\nFirst test sample\n['that', 'picture', 'i', 'just', 'seen', 'whoa', 'dere', '!', '!', '&gt;', '&gt;', '&gt;', '&gt;', '&gt;', '&gt;', '&gt;']\n ### Exercise 04\nYou won‚Äôt use all the tokens (words) appearing in the data for training. Instead, you will use the more frequently used words.\n- You will focus on the words that appear at least N times in the data. - First count how many times each word appears in the data.\nYou will need a double for-loop, one for sentences and the other for tokens within a sentence.\n\n\nHints\n\n\n\n\nIf you decide to import and use defaultdict, remember to cast the dictionary back to a regular ‚Äòdict‚Äô before returning it.\n\n\n\n\n# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n### GRADED_FUNCTION: count_words ###\ndef count_words(tokenized_sentences):\n    \"\"\"\n    Count the number of word appearence in the tokenized sentences\n    \n    Args:\n        tokenized_sentences: List of lists of strings\n    \n    Returns:\n        dict that maps word (str) to the frequency (int)\n    \"\"\"\n        \n    word_counts = {}\n    ### START CODE HERE (Replace instances of 'None' with your code) ###\n    \n    # Loop through each sentence\n    for sentence in None: # complete this line\n        \n        # Go through each token in the sentence\n        for token in None: # complete this line\n\n            # If the token is not in the dictionary yet, set the count to 1\n            if None: # complete this line\n                word_counts[token] = None\n            \n            # If the token is already in the dictionary, increment the count by 1\n            else:\n                word_counts[token] += None\n\n    ### END CODE HERE ###\n    \n    return word_counts\n\n\n# test your code\ntokenized_sentences = [['sky', 'is', 'blue', '.'],\n                       ['leaves', 'are', 'green', '.'],\n                       ['roses', 'are', 'red', '.']]\ncount_words(tokenized_sentences)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[12], line 5\n      1 # test your code\n      2 tokenized_sentences = [['sky', 'is', 'blue', '.'],\n      3                        ['leaves', 'are', 'green', '.'],\n      4                        ['roses', 'are', 'red', '.']]\n----&gt; 5 count_words(tokenized_sentences)\n\nCell In[11], line 18, in count_words(tokenized_sentences)\n     14 word_counts = {}\n     15 ### START CODE HERE (Replace instances of 'None' with your code) ###\n     16 \n     17 # Loop through each sentence\n---&gt; 18 for sentence in None: # complete this line\n     19     \n     20     # Go through each token in the sentence\n     21     for token in None: # complete this line\n     22 \n     23         # If the token is not in the dictionary yet, set the count to 1\n     24         if None: # complete this line\n\nTypeError: 'NoneType' object is not iterable\n\n\n\n\n\nExpected output\nNote that the order may differ.\n{'sky': 1,\n 'is': 1,\n 'blue': 1,\n '.': 3,\n 'leaves': 1,\n 'are': 2,\n 'green': 1,\n 'roses': 1,\n 'red': 1}\n\n\n\nHandling ‚ÄòOut of Vocabulary‚Äô words\nIf your model is performing autocomplete, but encounters a word that it never saw during training, it won‚Äôt have an input word to help it determine the next word to suggest. The model will not be able to predict the next word because there are no counts for the current word. - This ‚Äònew‚Äô word is called an ‚Äòunknown word‚Äô, or out of vocabulary (OOV) words. - The percentage of unknown words in the test set is called the  OOV  rate.\nTo handle unknown words during prediction, use a special token to represent all unknown words ‚Äòunk‚Äô. - Modify the training data so that it has some ‚Äòunknown‚Äô words to train on. - Words to convert into ‚Äúunknown‚Äù words are those that do not occur very frequently in the training set. - Create a list of the most frequent words in the training set, called the  closed vocabulary . - Convert all the other words that are not part of the closed vocabulary to the token ‚Äòunk‚Äô.\n ### Exercise 05\nYou will now create a function that takes in a text document and a threshold count_threshold. - Any word whose count is greater than or equal to the threshold count_threshold is kept in the closed vocabulary. - Returns the word closed vocabulary list.\n\n# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n### GRADED_FUNCTION: get_words_with_nplus_frequency ###\ndef get_words_with_nplus_frequency(tokenized_sentences, count_threshold):\n    \"\"\"\n    Find the words that appear N times or more\n    \n    Args:\n        tokenized_sentences: List of lists of sentences\n        count_threshold: minimum number of occurrences for a word to be in the closed vocabulary.\n    \n    Returns:\n        List of words that appear N times or more\n    \"\"\"\n    # Initialize an empty list to contain the words that\n    # appear at least 'minimum_freq' times.\n    closed_vocab = []\n    \n    # Get the word couts of the tokenized sentences\n    # Use the function that you defined earlier to count the words\n    word_counts = count_words(tokenized_sentences)\n    \n    ### START CODE HERE (Replace instances of 'None' with your code) ###\n\n    # for each word and its count\n    for word, cnt in None: # complete this line\n        \n        # check that the word's count\n        # is at least as great as the minimum count\n        if None:\n            \n            # append the word to the list\n            None\n    ### END CODE HERE ###\n    \n    return closed_vocab\n\n\n# test your code\ntokenized_sentences = [['sky', 'is', 'blue', '.'],\n                       ['leaves', 'are', 'green', '.'],\n                       ['roses', 'are', 'red', '.']]\ntmp_closed_vocab = get_words_with_nplus_frequency(tokenized_sentences, count_threshold=2)\nprint(f\"Closed vocabulary:\")\nprint(tmp_closed_vocab)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[14], line 5\n      1 # test your code\n      2 tokenized_sentences = [['sky', 'is', 'blue', '.'],\n      3                        ['leaves', 'are', 'green', '.'],\n      4                        ['roses', 'are', 'red', '.']]\n----&gt; 5 tmp_closed_vocab = get_words_with_nplus_frequency(tokenized_sentences, count_threshold=2)\n      6 print(f\"Closed vocabulary:\")\n      7 print(tmp_closed_vocab)\n\nCell In[13], line 20, in get_words_with_nplus_frequency(tokenized_sentences, count_threshold)\n     16 closed_vocab = []\n     18 # Get the word couts of the tokenized sentences\n     19 # Use the function that you defined earlier to count the words\n---&gt; 20 word_counts = count_words(tokenized_sentences)\n     22 ### START CODE HERE (Replace instances of 'None' with your code) ###\n     23 \n     24 # for each word and its count\n     25 for word, cnt in None: # complete this line\n     26     \n     27     # check that the word's count\n     28     # is at least as great as the minimum count\n\nCell In[11], line 18, in count_words(tokenized_sentences)\n     14 word_counts = {}\n     15 ### START CODE HERE (Replace instances of 'None' with your code) ###\n     16 \n     17 # Loop through each sentence\n---&gt; 18 for sentence in None: # complete this line\n     19     \n     20     # Go through each token in the sentence\n     21     for token in None: # complete this line\n     22 \n     23         # If the token is not in the dictionary yet, set the count to 1\n     24         if None: # complete this line\n\nTypeError: 'NoneType' object is not iterable\n\n\n\n\nExpected output\nClosed vocabulary:\n['.', 'are']\n ### Exercise 06\nThe words that appear count_threshold times or more are in the closed vocabulary. - All other words are regarded as unknown. - Replace words not in the closed vocabulary with the token &lt;unk&gt;.\n\n# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n### GRADED_FUNCTION: replace_oov_words_by_unk ###\ndef replace_oov_words_by_unk(tokenized_sentences, vocabulary, unknown_token=\"&lt;unk&gt;\"):\n    \"\"\"\n    Replace words not in the given vocabulary with '&lt;unk&gt;' token.\n    \n    Args:\n        tokenized_sentences: List of lists of strings\n        vocabulary: List of strings that we will use\n        unknown_token: A string representing unknown (out-of-vocabulary) words\n    \n    Returns:\n        List of lists of strings, with words not in the vocabulary replaced\n    \"\"\"\n    \n    # Place vocabulary into a set for faster search\n    vocabulary = set(vocabulary)\n    \n    # Initialize a list that will hold the sentences\n    # after less frequent words are replaced by the unknown token\n    replaced_tokenized_sentences = []\n    \n    # Go through each sentence\n    for sentence in tokenized_sentences:\n        \n        # Initialize the list that will contain\n        # a single sentence with \"unknown_token\" replacements\n        replaced_sentence = []\n        ### START CODE HERE (Replace instances of 'None' with your code) ###\n\n        # for each token in the sentence\n        for token in None: # complete this line\n            \n            # Check if the token is in the closed vocabulary\n            if token in None: # complete this line\n                # If so, append the word to the replaced_sentence\n                None\n            else:\n                # otherwise, append the unknown token instead\n                None\n        ### END CODE HERE ###\n        \n        # Append the list of tokens to the list of lists\n        replaced_tokenized_sentences.append(replaced_sentence)\n        \n    return replaced_tokenized_sentences\n\n\ntokenized_sentences = [[\"dogs\", \"run\"], [\"cats\", \"sleep\"]]\nvocabulary = [\"dogs\", \"sleep\"]\ntmp_replaced_tokenized_sentences = replace_oov_words_by_unk(tokenized_sentences, vocabulary)\nprint(f\"Original sentence:\")\nprint(tokenized_sentences)\nprint(f\"tokenized_sentences with less frequent words converted to '&lt;unk&gt;':\")\nprint(tmp_replaced_tokenized_sentences)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[16], line 3\n      1 tokenized_sentences = [[\"dogs\", \"run\"], [\"cats\", \"sleep\"]]\n      2 vocabulary = [\"dogs\", \"sleep\"]\n----&gt; 3 tmp_replaced_tokenized_sentences = replace_oov_words_by_unk(tokenized_sentences, vocabulary)\n      4 print(f\"Original sentence:\")\n      5 print(tokenized_sentences)\n\nCell In[15], line 32, in replace_oov_words_by_unk(tokenized_sentences, vocabulary, unknown_token)\n     28 replaced_sentence = []\n     29 ### START CODE HERE (Replace instances of 'None' with your code) ###\n     30 \n     31 # for each token in the sentence\n---&gt; 32 for token in None: # complete this line\n     33     \n     34     # Check if the token is in the closed vocabulary\n     35     if token in None: # complete this line\n     36         # If so, append the word to the replaced_sentence\n     37         None\n\nTypeError: 'NoneType' object is not iterable\n\n\n\n\n\n\nExpected answer\nOriginal sentence:\n[['dogs', 'run'], ['cats', 'sleep']]\ntokenized_sentences with less frequent words converted to '&lt;unk&gt;':\n[['dogs', '&lt;unk&gt;'], ['&lt;unk&gt;', 'sleep']]\n ### Exercise 07\nNow we are ready to process our data by combining the functions that you just implemented.\n\nFind tokens that appear at least count_threshold times in the training data.\nReplace tokens that appear less than count_threshold times by ‚Äú&lt;unk&gt;‚Äù both for training and test data.\n\n\n# UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n### GRADED_FUNCTION: preprocess_data ###\ndef preprocess_data(train_data, test_data, count_threshold):\n    \"\"\"\n    Preprocess data, i.e.,\n        - Find tokens that appear at least N times in the training data.\n        - Replace tokens that appear less than N times by \"&lt;unk&gt;\" both for training and test data.        \n    Args:\n        train_data, test_data: List of lists of strings.\n        count_threshold: Words whose count is less than this are \n                      treated as unknown.\n    \n    Returns:\n        Tuple of\n        - training data with low frequent words replaced by \"&lt;unk&gt;\"\n        - test data with low frequent words replaced by \"&lt;unk&gt;\"\n        - vocabulary of words that appear n times or more in the training data\n    \"\"\"\n    ### START CODE HERE (Replace instances of 'None' with your code) ###\n\n    # Get the closed vocabulary using the train data\n    vocabulary = None\n    \n    # For the train data, replace less common words with \"&lt;unk&gt;\"\n    train_data_replaced = None\n    \n    # For the test data, replace less common words with \"&lt;unk&gt;\"\n    test_data_replaced = None\n    \n    ### END CODE HERE ###\n    return train_data_replaced, test_data_replaced, vocabulary\n\n\n# test your code\ntmp_train = [['sky', 'is', 'blue', '.'],\n     ['leaves', 'are', 'green']]\ntmp_test = [['roses', 'are', 'red', '.']]\n\ntmp_train_repl, tmp_test_repl, tmp_vocab = preprocess_data(tmp_train, \n                                                           tmp_test, \n                                                           count_threshold = 1)\n\nprint(\"tmp_train_repl\")\nprint(tmp_train_repl)\nprint()\nprint(\"tmp_test_repl\")\nprint(tmp_test_repl)\nprint()\nprint(\"tmp_vocab\")\nprint(tmp_vocab)\n\ntmp_train_repl\nNone\n\ntmp_test_repl\nNone\n\ntmp_vocab\nNone\n\n\n\nExpected outcome\ntmp_train_repl\n[['sky', 'is', 'blue', '.'], ['leaves', 'are', 'green']]\n\ntmp_test_repl\n[['&lt;unk&gt;', 'are', '&lt;unk&gt;', '.']]\n\ntmp_vocab\n['sky', 'is', 'blue', '.', 'leaves', 'are', 'green']\n\n\n\nPreprocess the train and test data\nRun the cell below to complete the preprocessing both for training and test sets.\n\nminimum_freq = 2\ntrain_data_processed, test_data_processed, vocabulary = preprocess_data(train_data, \n                                                                        test_data, \n                                                                        minimum_freq)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[19], line 2\n      1 minimum_freq = 2\n----&gt; 2 train_data_processed, test_data_processed, vocabulary = preprocess_data(train_data, \n      3                                                                         test_data, \n      4                                                                         minimum_freq)\n\nNameError: name 'train_data' is not defined\n\n\n\n\nprint(\"First preprocessed training sample:\")\nprint(train_data_processed[0])\nprint()\nprint(\"First preprocessed test sample:\")\nprint(test_data_processed[0])\nprint()\nprint(\"First 10 vocabulary:\")\nprint(vocabulary[0:10])\nprint()\nprint(\"Size of vocabulary:\", len(vocabulary))\n\nFirst preprocessed training sample:\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[20], line 2\n      1 print(\"First preprocessed training sample:\")\n----&gt; 2 print(train_data_processed[0])\n      3 print()\n      4 print(\"First preprocessed test sample:\")\n\nNameError: name 'train_data_processed' is not defined\n\n\n\n\nExpected output\nFirst preprocessed training sample:\n['i', 'personally', 'would', 'like', 'as', 'our', 'official', 'glove', 'of', 'the', 'team', 'local', 'company', 'and', 'quality', 'production']\n\nFirst preprocessed test sample:\n['that', 'picture', 'i', 'just', 'seen', 'whoa', 'dere', '!', '!', '&gt;', '&gt;', '&gt;', '&gt;', '&gt;', '&gt;', '&gt;']\n\nFirst 10 vocabulary:\n['i', 'personally', 'would', 'like', 'as', 'our', 'official', 'glove', 'of', 'the']\n\nSize of vocabulary: 14821\nYou are done with the preprocessing section of the assignment. Objects train_data_processed, test_data_processed, and vocabulary will be used in the rest of the exercises.\n ## Part 2: Develop n-gram based language models\nIn this section, you will develop the n-grams language model. - Assume the probability of the next word depends only on the previous n-gram. - The previous n-gram is the series of the previous ‚Äòn‚Äô words.\nThe conditional probability for the word at position ‚Äòt‚Äô in the sentence, given that the words preceding it are w_{t-1}, w_{t-2} \\cdots w_{t-n} is:\n P(w_t | w_{t-1}\\dots w_{t-n}) \\tag{1}\nYou can estimate this probability by counting the occurrences of these series of words in the training data. - The probability can be estimated as a ratio, where - The numerator is the number of times word ‚Äòt‚Äô appears after words t-1 through t-n appear in the training data. - The denominator is the number of times word t-1 through t-n appears in the training data.\n \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_n)}{C(w_{t-1}\\dots w_{t-n})} \\tag{2} \n\nThe function C(\\cdots) denotes the number of occurence of the given sequence.\n\\hat{P} means the estimation of P.\nNotice that denominator of the equation (2) is the number of occurence of the previous n words, and the numerator is the same sequence followed by the word w_t.\n\nLater, you will modify the equation (2) by adding k-smoothing, which avoids errors when any counts are zero.\nThe equation (2) tells us that to estimate probabilities based on n-grams, you need the counts of n-grams (for denominator) and (n+1)-grams (for numerator).\n ### Exercise 08 Next, you will implement a function that computes the counts of n-grams for an arbitrary number n.\nWhen computing the counts for n-grams, prepare the sentence beforehand by prepending n-1 starting markers ‚Äú&lt;s&gt;‚Äù to indicate the beginning of the sentence.\n- For example, in the bi-gram model (N=2), a sequence with two start tokens ‚Äú&lt;s&gt;&lt;s&gt;‚Äù should predict the first word of a sentence. - So, if the sentence is ‚ÄúI like food‚Äù, modify it to be ‚Äú&lt;s&gt;&lt;s&gt; I like food‚Äù. - Also prepare the sentence for counting by appending an end token ‚Äú&lt;e&gt;‚Äù so that the model can predict when to finish a sentence.\nTechnical note: In this implementation, you will store the counts as a dictionary. - The key of each key-value pair in the dictionary is a tuple of n words (and not a list) - The value in the key-value pair is the number of occurrences.\n- The reason for using a tuple as a key instead of a list is because a list in Python is a mutable object (it can be changed after it is first created). A tuple is ‚Äúimmutable‚Äù, so it cannot be altered after it is first created. This makes a tuple suitable as a data type for the key in a dictionary.\n\n\nHints\n\n\n\n\nTo prepend or append, you can create lists and concatenate them using the + operator\n\n\nTo create a list of a repeated value, you can follow this syntax: [‚Äòa‚Äô] * 3 to get [‚Äòa‚Äô,‚Äòa‚Äô,‚Äòa‚Äô]\n\n\nTo set the range for index ‚Äòi‚Äô, think of this example: An n-gram where n=2 (bigram), and the sentence is length N=5 (including two start tokens and one end token). So the index positions are [0,1,2,3,4]. The largest index ‚Äòi‚Äô where a bigram can start is at position i=3, because the word tokens at position 3 and 4 will form the bigram.\n\n\nRemember that the range() function excludes the value that is used for the maximum of the range.  range(3)  produces (0,1,2) but excludes 3.\n\n\n\n\n# UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n### GRADED FUNCTION: count_n_grams ###\ndef count_n_grams(data, n, start_token='&lt;s&gt;', end_token = '&lt;e&gt;'):\n    \"\"\"\n    Count all n-grams in the data\n    \n    Args:\n        data: List of lists of words\n        n: number of words in a sequence\n    \n    Returns:\n        A dictionary that maps a tuple of n-words to its frequency\n    \"\"\"\n    \n    # Initialize dictionary of n-grams and their counts\n    n_grams = {}\n\n    ### START CODE HERE (Replace instances of 'None' with your code) ###\n    \n    # Go through each sentence in the data\n    for sentence in None: # complete this line\n        \n        # prepend start token n times, and  append &lt;e&gt; one time\n        sentence = None\n        \n        # convert list to tuple\n        # So that the sequence of words can be used as\n        # a key in the dictionary\n        sentence = None\n        \n        # Use 'i' to indicate the start of the n-gram\n        # from index 0\n        # to the last index where the end of the n-gram\n        # is within the sentence.\n        \n        for i in range(None): # complete this line\n\n            # Get the n-gram from i to i+n\n            n_gram = None\n\n            # check if the n-gram is in the dictionary\n            if n_gram in None: # complete this line\n            \n                # Increment the count for this n-gram\n                n_grams[n_gram] += None\n            else:\n                # Initialize this n-gram count to 1\n                n_grams[n_gram] = None\n    \n            ### END CODE HERE ###\n    return n_grams\n\n\n# test your code\n# CODE REVIEW COMMENT: Outcome does not match expected outcome\nsentences = [['i', 'like', 'a', 'cat'],\n             ['this', 'dog', 'is', 'like', 'a', 'cat']]\nprint(\"Uni-gram:\")\nprint(count_n_grams(sentences, 1))\nprint(\"Bi-gram:\")\nprint(count_n_grams(sentences, 2))\n\nUni-gram:\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[22], line 6\n      3 sentences = [['i', 'like', 'a', 'cat'],\n      4              ['this', 'dog', 'is', 'like', 'a', 'cat']]\n      5 print(\"Uni-gram:\")\n----&gt; 6 print(count_n_grams(sentences, 1))\n      7 print(\"Bi-gram:\")\n      8 print(count_n_grams(sentences, 2))\n\nCell In[21], line 21, in count_n_grams(data, n, start_token, end_token)\n     16 n_grams = {}\n     18 ### START CODE HERE (Replace instances of 'None' with your code) ###\n     19 \n     20 # Go through each sentence in the data\n---&gt; 21 for sentence in None: # complete this line\n     22     \n     23     # prepend start token n times, and  append &lt;e&gt; one time\n     24     sentence = None\n     26     # convert list to tuple\n     27     # So that the sequence of words can be used as\n     28     # a key in the dictionary\n\nTypeError: 'NoneType' object is not iterable\n\n\n\nExpected outcome:\nUni-gram:\n{('&lt;s&gt;',): 2, ('i',): 1, ('like',): 2, ('a',): 2, ('cat',): 2, ('&lt;e&gt;',): 2, ('this',): 1, ('dog',): 1, ('is',): 1}\nBi-gram:\n{('&lt;s&gt;', '&lt;s&gt;'): 2, ('&lt;s&gt;', 'i'): 1, ('i', 'like'): 1, ('like', 'a'): 2, ('a', 'cat'): 2, ('cat', '&lt;e&gt;'): 2, ('&lt;s&gt;', 'this'): 1, ('this', 'dog'): 1, ('dog', 'is'): 1, ('is', 'like'): 1}\n ### Exercise 09\nNext, estimate the probability of a word given the prior ‚Äòn‚Äô words using the n-gram counts.\n \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_n)}{C(w_{t-1}\\dots w_{t-n})} \\tag{2} \nThis formula doesn‚Äôt work when a count of an n-gram is zero.. - Suppose we encounter an n-gram that did not occur in the training data.\n- Then, the equation (2) cannot be evaluated (it becomes zero divided by zero).\nA way to handle zero counts is to add k-smoothing.\n- K-smoothing adds a positive constant k to each numerator and k \\times |V| in the denominator, where |V| is the number of words in the vocabulary.\n \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_n) + k}{C(w_{t-1}\\dots w_{t-n}) + k|V|} \\tag{3} \nFor n-grams that have a zero count, the equation (3) becomes \\frac{1}{|V|}. - This means that any n-gram with zero count has the same probability of \\frac{1}{|V|}.\nDefine a function that computes the probability estimate (3) from n-gram counts and a constant k.\n\nThe function takes in a dictionary ‚Äòn_gram_counts‚Äô, where the key is the n-gram and the value is the count of that n-gram.\nThe function also takes another dictionary n_plus1_gram_counts, which you‚Äôll use to find the count for the previous n-gram plus the current word.\n\n\n\nHints\n\n\n\n\nTo define a tuple containing a single value, add a comma after that value. For example: (‚Äòapple‚Äô,) is a tuple containing a single string ‚Äòapple‚Äô\n\n\nTo concatenate two tuples, use the ‚Äò+‚Äô operator\n\n\n words \n\n\n\n\n# UNQ_C9 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n### GRADED FUNCTION: estimate_probability ###\ndef estimate_probability(word, previous_n_gram, \n                         n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n    \"\"\"\n    Estimate the probabilities of a next word using the n-gram counts with k-smoothing\n    \n    Args:\n        word: next word\n        previous_n_gram: A sequence of words of length n\n        n_gram_counts: Dictionary of counts of n-grams\n        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n        vocabulary_size: number of words in the vocabulary\n        k: positive constant, smoothing parameter\n    \n    Returns:\n        A probability\n    \"\"\"\n    # convert list to tuple to use it as a dictionary key\n    previous_n_gram = tuple(previous_n_gram)\n    \n    ### START CODE HERE (Replace instances of 'None' with your code) ###\n    \n    # Set the denominator\n    # If the previous n-gram exists in the dictionary of n-gram counts,\n    # Get its count.  Otherwise set the count to zero\n    # Use the dictionary that has counts for n-grams\n    previous_n_gram_count = None\n        \n    # Calculate the denominator using the count of the previous n gram\n    # and apply k-smoothing\n    denominator = None\n\n    # Define n plus 1 gram as the previous n-gram plus the current word as a tuple\n    n_plus1_gram = None\n  \n    # Set the count to the count in the dictionary,\n    # otherwise 0 if not in the dictionary\n    # use the dictionary that has counts for the n-gram plus current word\n    n_plus1_gram_count = None\n        \n    # Define the numerator use the count of the n-gram plus current word,\n    # and apply smoothing\n    numerator = None\n\n    # Calculate the probability as the numerator divided by denominator\n    probability = None\n    \n    ### END CODE HERE ###\n    \n    return probability\n\n\n# test your code\nsentences = [['i', 'like', 'a', 'cat'],\n             ['this', 'dog', 'is', 'like', 'a', 'cat']]\nunique_words = list(set(sentences[0] + sentences[1]))\n\nunigram_counts = count_n_grams(sentences, 1)\nbigram_counts = count_n_grams(sentences, 2)\ntmp_prob = estimate_probability(\"cat\", \"a\", unigram_counts, bigram_counts, len(unique_words), k=1)\n\nprint(f\"The estimated probability of word 'cat' given the previous n-gram 'a' is: {tmp_prob:.4f}\")\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[24], line 6\n      2 sentences = [['i', 'like', 'a', 'cat'],\n      3              ['this', 'dog', 'is', 'like', 'a', 'cat']]\n      4 unique_words = list(set(sentences[0] + sentences[1]))\n----&gt; 6 unigram_counts = count_n_grams(sentences, 1)\n      7 bigram_counts = count_n_grams(sentences, 2)\n      8 tmp_prob = estimate_probability(\"cat\", \"a\", unigram_counts, bigram_counts, len(unique_words), k=1)\n\nCell In[21], line 21, in count_n_grams(data, n, start_token, end_token)\n     16 n_grams = {}\n     18 ### START CODE HERE (Replace instances of 'None' with your code) ###\n     19 \n     20 # Go through each sentence in the data\n---&gt; 21 for sentence in None: # complete this line\n     22     \n     23     # prepend start token n times, and  append &lt;e&gt; one time\n     24     sentence = None\n     26     # convert list to tuple\n     27     # So that the sequence of words can be used as\n     28     # a key in the dictionary\n\nTypeError: 'NoneType' object is not iterable\n\n\n\n\n\nExpected output\nThe estimated probability of word 'cat' given the previous n-gram 'a' is: 0.3333\n\n\n\nEstimate probabilities for all words\nThe function defined below loops over all words in vocabulary to calculate probabilities for all possible words. - This function is provided for you.\n\ndef estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0):\n    \"\"\"\n    Estimate the probabilities of next words using the n-gram counts with k-smoothing\n    \n    Args:\n        previous_n_gram: A sequence of words of length n\n        n_gram_counts: Dictionary of counts of (n+1)-grams\n        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n        vocabulary: List of words\n        k: positive constant, smoothing parameter\n    \n    Returns:\n        A dictionary mapping from next words to the probability.\n    \"\"\"\n    \n    # convert list to tuple to use it as a dictionary key\n    previous_n_gram = tuple(previous_n_gram)\n    \n    # add &lt;e&gt; &lt;unk&gt; to the vocabulary\n    # &lt;s&gt; is not needed since it should not appear as the next word\n    vocabulary = vocabulary + [\"&lt;e&gt;\", \"&lt;unk&gt;\"]\n    vocabulary_size = len(vocabulary)\n    \n    probabilities = {}\n    for word in vocabulary:\n        probability = estimate_probability(word, previous_n_gram, \n                                           n_gram_counts, n_plus1_gram_counts, \n                                           vocabulary_size, k=k)\n        probabilities[word] = probability\n\n    return probabilities\n\n\n# test your code\nsentences = [['i', 'like', 'a', 'cat'],\n             ['this', 'dog', 'is', 'like', 'a', 'cat']]\nunique_words = list(set(sentences[0] + sentences[1]))\nunigram_counts = count_n_grams(sentences, 1)\nbigram_counts = count_n_grams(sentences, 2)\nestimate_probabilities(\"a\", unigram_counts, bigram_counts, unique_words, k=1)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[26], line 5\n      2 sentences = [['i', 'like', 'a', 'cat'],\n      3              ['this', 'dog', 'is', 'like', 'a', 'cat']]\n      4 unique_words = list(set(sentences[0] + sentences[1]))\n----&gt; 5 unigram_counts = count_n_grams(sentences, 1)\n      6 bigram_counts = count_n_grams(sentences, 2)\n      7 estimate_probabilities(\"a\", unigram_counts, bigram_counts, unique_words, k=1)\n\nCell In[21], line 21, in count_n_grams(data, n, start_token, end_token)\n     16 n_grams = {}\n     18 ### START CODE HERE (Replace instances of 'None' with your code) ###\n     19 \n     20 # Go through each sentence in the data\n---&gt; 21 for sentence in None: # complete this line\n     22     \n     23     # prepend start token n times, and  append &lt;e&gt; one time\n     24     sentence = None\n     26     # convert list to tuple\n     27     # So that the sequence of words can be used as\n     28     # a key in the dictionary\n\nTypeError: 'NoneType' object is not iterable\n\n\n\n\nExpected output\n{'cat': 0.2727272727272727,\n 'i': 0.09090909090909091,\n 'this': 0.09090909090909091,\n 'a': 0.09090909090909091,\n 'is': 0.09090909090909091,\n 'like': 0.09090909090909091,\n 'dog': 0.09090909090909091,\n '&lt;e&gt;': 0.09090909090909091,\n '&lt;unk&gt;': 0.09090909090909091}\n\n# Additional test\ntrigram_counts = count_n_grams(sentences, 3)\nestimate_probabilities([\"&lt;s&gt;\", \"&lt;s&gt;\"], bigram_counts, trigram_counts, unique_words, k=1)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[27], line 2\n      1 # Additional test\n----&gt; 2 trigram_counts = count_n_grams(sentences, 3)\n      3 estimate_probabilities([\"&lt;s&gt;\", \"&lt;s&gt;\"], bigram_counts, trigram_counts, unique_words, k=1)\n\nCell In[21], line 21, in count_n_grams(data, n, start_token, end_token)\n     16 n_grams = {}\n     18 ### START CODE HERE (Replace instances of 'None' with your code) ###\n     19 \n     20 # Go through each sentence in the data\n---&gt; 21 for sentence in None: # complete this line\n     22     \n     23     # prepend start token n times, and  append &lt;e&gt; one time\n     24     sentence = None\n     26     # convert list to tuple\n     27     # So that the sequence of words can be used as\n     28     # a key in the dictionary\n\nTypeError: 'NoneType' object is not iterable\n\n\n\n\n\nExpected output\n{'cat': 0.09090909090909091,\n 'i': 0.18181818181818182,\n 'this': 0.18181818181818182,\n 'a': 0.09090909090909091,\n 'is': 0.09090909090909091,\n 'like': 0.09090909090909091,\n 'dog': 0.09090909090909091,\n '&lt;e&gt;': 0.09090909090909091,\n '&lt;unk&gt;': 0.09090909090909091}\n\n\n\nCount and probability matrices\nAs we have seen so far, the n-gram counts computed above are sufficient for computing the probabilities of the next word.\n- It can be more intuitive to present them as count or probability matrices. - The functions defined in the next cells return count or probability matrices. - This function is provided for you.\n\ndef make_count_matrix(n_plus1_gram_counts, vocabulary):\n    # add &lt;e&gt; &lt;unk&gt; to the vocabulary\n    # &lt;s&gt; is omitted since it should not appear as the next word\n    vocabulary = vocabulary + [\"&lt;e&gt;\", \"&lt;unk&gt;\"]\n    \n    # obtain unique n-grams\n    n_grams = []\n    for n_plus1_gram in n_plus1_gram_counts.keys():\n        n_gram = n_plus1_gram[0:-1]\n        n_grams.append(n_gram)\n    n_grams = list(set(n_grams))\n    \n    # mapping from n-gram to row\n    row_index = {n_gram:i for i, n_gram in enumerate(n_grams)}\n    # mapping from next word to column\n    col_index = {word:j for j, word in enumerate(vocabulary)}\n    \n    nrow = len(n_grams)\n    ncol = len(vocabulary)\n    count_matrix = np.zeros((nrow, ncol))\n    for n_plus1_gram, count in n_plus1_gram_counts.items():\n        n_gram = n_plus1_gram[0:-1]\n        word = n_plus1_gram[-1]\n        if word not in vocabulary:\n            continue\n        i = row_index[n_gram]\n        j = col_index[word]\n        count_matrix[i, j] = count\n    \n    count_matrix = pd.DataFrame(count_matrix, index=n_grams, columns=vocabulary)\n    return count_matrix\n\n\nsentences = [['i', 'like', 'a', 'cat'],\n                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\nunique_words = list(set(sentences[0] + sentences[1]))\nbigram_counts = count_n_grams(sentences, 2)\n\nprint('bigram counts')\ndisplay(make_count_matrix(bigram_counts, unique_words))\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[29], line 4\n      1 sentences = [['i', 'like', 'a', 'cat'],\n      2                  ['this', 'dog', 'is', 'like', 'a', 'cat']]\n      3 unique_words = list(set(sentences[0] + sentences[1]))\n----&gt; 4 bigram_counts = count_n_grams(sentences, 2)\n      6 print('bigram counts')\n      7 display(make_count_matrix(bigram_counts, unique_words))\n\nCell In[21], line 21, in count_n_grams(data, n, start_token, end_token)\n     16 n_grams = {}\n     18 ### START CODE HERE (Replace instances of 'None' with your code) ###\n     19 \n     20 # Go through each sentence in the data\n---&gt; 21 for sentence in None: # complete this line\n     22     \n     23     # prepend start token n times, and  append &lt;e&gt; one time\n     24     sentence = None\n     26     # convert list to tuple\n     27     # So that the sequence of words can be used as\n     28     # a key in the dictionary\n\nTypeError: 'NoneType' object is not iterable\n\n\n\n\nExpected output\nbigram counts\n          cat    i   this   a  is   like  dog  &lt;e&gt;   &lt;unk&gt;\n(&lt;s&gt;,)    0.0   1.0  1.0  0.0  0.0  0.0   0.0  0.0    0.0\n(a,)      2.0   0.0  0.0  0.0  0.0  0.0   0.0  0.0    0.0\n(this,)   0.0   0.0  0.0  0.0  0.0  0.0   1.0  0.0    0.0\n(like,)   0.0   0.0  0.0  2.0  0.0  0.0   0.0  0.0    0.0\n(dog,)    0.0   0.0  0.0  0.0  1.0  0.0   0.0  0.0    0.0\n(cat,)    0.0   0.0  0.0  0.0  0.0  0.0   0.0  2.0    0.0\n(is,)     0.0   0.0  0.0  0.0  0.0  1.0   0.0  0.0    0.0\n(i,)      0.0   0.0  0.0  0.0  0.0  1.0   0.0  0.0    0.0\n\n# Show trigram counts\nprint('\\ntrigram counts')\ntrigram_counts = count_n_grams(sentences, 3)\ndisplay(make_count_matrix(trigram_counts, unique_words))\n\n\ntrigram counts\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[30], line 3\n      1 # Show trigram counts\n      2 print('\\ntrigram counts')\n----&gt; 3 trigram_counts = count_n_grams(sentences, 3)\n      4 display(make_count_matrix(trigram_counts, unique_words))\n\nCell In[21], line 21, in count_n_grams(data, n, start_token, end_token)\n     16 n_grams = {}\n     18 ### START CODE HERE (Replace instances of 'None' with your code) ###\n     19 \n     20 # Go through each sentence in the data\n---&gt; 21 for sentence in None: # complete this line\n     22     \n     23     # prepend start token n times, and  append &lt;e&gt; one time\n     24     sentence = None\n     26     # convert list to tuple\n     27     # So that the sequence of words can be used as\n     28     # a key in the dictionary\n\nTypeError: 'NoneType' object is not iterable\n\n\n\n\n\nExpected output\ntrigram counts\n              cat    i   this   a  is   like  dog  &lt;e&gt;   &lt;unk&gt;\n(dog, is)     0.0   0.0  0.0  0.0  0.0  1.0   0.0  0.0    0.0\n(this, dog)   0.0   0.0  0.0  0.0  1.0  0.0   0.0  0.0    0.0\n(a, cat)      0.0   0.0  0.0  0.0  0.0  0.0   0.0  2.0    0.0\n(like, a)     2.0   0.0  0.0  0.0  0.0  0.0   0.0  0.0    0.0\n(is, like)    0.0   0.0  0.0  1.0  0.0  0.0   0.0  0.0    0.0\n(&lt;s&gt;, i)      0.0   0.0  0.0  0.0  0.0  1.0   0.0  0.0    0.0\n(i, like)     0.0   0.0  0.0  1.0  0.0  0.0   0.0  0.0    0.0\n(&lt;s&gt;, &lt;s&gt;)    0.0   1.0  1.0  0.0  0.0  0.0   0.0  0.0    0.0\n(&lt;s&gt;, this)   0.0   0.0  0.0  0.0  0.0  0.0   1.0  0.0    0.0\nThe following function calculates the probabilities of each word given the previous n-gram, and stores this in matrix form. - This function is provided for you.\n\ndef make_probability_matrix(n_plus1_gram_counts, vocabulary, k):\n    count_matrix = make_count_matrix(n_plus1_gram_counts, unique_words)\n    count_matrix += k\n    prob_matrix = count_matrix.div(count_matrix.sum(axis=1), axis=0)\n    return prob_matrix\n\n\nsentences = [['i', 'like', 'a', 'cat'],\n                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\nunique_words = list(set(sentences[0] + sentences[1]))\nbigram_counts = count_n_grams(sentences, 2)\nprint(\"bigram probabilities\")\ndisplay(make_probability_matrix(bigram_counts, unique_words, k=1))\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[32], line 4\n      1 sentences = [['i', 'like', 'a', 'cat'],\n      2                  ['this', 'dog', 'is', 'like', 'a', 'cat']]\n      3 unique_words = list(set(sentences[0] + sentences[1]))\n----&gt; 4 bigram_counts = count_n_grams(sentences, 2)\n      5 print(\"bigram probabilities\")\n      6 display(make_probability_matrix(bigram_counts, unique_words, k=1))\n\nCell In[21], line 21, in count_n_grams(data, n, start_token, end_token)\n     16 n_grams = {}\n     18 ### START CODE HERE (Replace instances of 'None' with your code) ###\n     19 \n     20 # Go through each sentence in the data\n---&gt; 21 for sentence in None: # complete this line\n     22     \n     23     # prepend start token n times, and  append &lt;e&gt; one time\n     24     sentence = None\n     26     # convert list to tuple\n     27     # So that the sequence of words can be used as\n     28     # a key in the dictionary\n\nTypeError: 'NoneType' object is not iterable\n\n\n\n\nprint(\"trigram probabilities\")\ntrigram_counts = count_n_grams(sentences, 3)\ndisplay(make_probability_matrix(trigram_counts, unique_words, k=1))\n\ntrigram probabilities\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[33], line 2\n      1 print(\"trigram probabilities\")\n----&gt; 2 trigram_counts = count_n_grams(sentences, 3)\n      3 display(make_probability_matrix(trigram_counts, unique_words, k=1))\n\nCell In[21], line 21, in count_n_grams(data, n, start_token, end_token)\n     16 n_grams = {}\n     18 ### START CODE HERE (Replace instances of 'None' with your code) ###\n     19 \n     20 # Go through each sentence in the data\n---&gt; 21 for sentence in None: # complete this line\n     22     \n     23     # prepend start token n times, and  append &lt;e&gt; one time\n     24     sentence = None\n     26     # convert list to tuple\n     27     # So that the sequence of words can be used as\n     28     # a key in the dictionary\n\nTypeError: 'NoneType' object is not iterable\n\n\n\nConfirm that you obtain the same results as for the estimate_probabilities function that you implemented.\n ## Part 3: Perplexity\nIn this section, you will generate the perplexity score to evaluate your model on the test set. - You will also use back-off when needed. - Perplexity is used as an evaluation metric of your language model. - To calculate the the perplexity score of the test set on an n-gram model, use:\n PP(W) =\\sqrt[N]{ \\prod_{t=n+1}^N \\frac{1}{P(w_t | w_{t-n} \\cdots w_{t-1})} } \\tag{4}\n\nwhere N is the length of the sentence.\nn is the number of words in the n-gram (e.g.¬†2 for a bigram).\nIn math, the numbering starts at one and not zero.\n\nIn code, array indexing starts at zero, so the code will use ranges for t according to this formula:\n PP(W) =\\sqrt[N]{ \\prod_{t=n}^{N-1} \\frac{1}{P(w_t | w_{t-n} \\cdots w_{t-1})} } \\tag{4.1}\nThe higher the probabilities are, the lower the perplexity will be. - The more the n-grams tell us about the sentence, the lower the perplexity score will be.\n ### Exercise 10 Compute the perplexity score given an N-gram count matrix and a sentence.\n\n\nHints\n\n\n\n\nRemember that range(2,4) produces the integers [2, 3] (and excludes 4).\n\n\n\n\n# UNQ_C10 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: calculate_perplexity\ndef calculate_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n    \"\"\"\n    Calculate perplexity for a list of sentences\n    \n    Args:\n        sentence: List of strings\n        n_gram_counts: Dictionary of counts of (n+1)-grams\n        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n        vocabulary_size: number of unique words in the vocabulary\n        k: Positive smoothing constant\n    \n    Returns:\n        Perplexity score\n    \"\"\"\n    # length of previous words\n    n = len(list(n_gram_counts.keys())[0]) \n    \n    # prepend &lt;s&gt; and append &lt;e&gt;\n    sentence = [\"&lt;s&gt;\"] * n + sentence + [\"&lt;e&gt;\"]\n    \n    # Cast the sentence from a list to a tuple\n    sentence = tuple(sentence)\n    \n    # length of sentence (after adding &lt;s&gt; and &lt;e&gt; tokens)\n    N = len(sentence)\n    \n    # The variable p will hold the product\n    # that is calculated inside the n-root\n    # Update this in the code below\n    product_pi = 1.0\n    \n    ### START CODE HERE (Replace instances of 'None' with your code) ###\n    \n    # Index t ranges from n to N - 1, inclusive on both ends\n    for t in range(None, None): # complete this line\n\n        # get the n-gram preceding the word at position t\n        n_gram = None\n        \n        # get the word at position t\n        word = None\n        \n        # Estimate the probability of the word given the n-gram\n        # using the n-gram counts, n-plus1-gram counts,\n        # vocabulary size, and smoothing constant\n        probability = None\n        \n        # Update the product of the probabilities\n        # This 'product_pi' is a cumulative product \n        # of the (1/P) factors that are calculated in the loop\n        product_pi *= None\n\n    # Take the Nth root of the product\n    perplexity = None\n    \n    ### END CODE HERE ### \n    return perplexity\n\n\n# test your code\n\nsentences = [['i', 'like', 'a', 'cat'],\n                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\nunique_words = list(set(sentences[0] + sentences[1]))\n\nunigram_counts = count_n_grams(sentences, 1)\nbigram_counts = count_n_grams(sentences, 2)\n\n\nperplexity_train1 = calculate_perplexity(sentences[0],\n                                         unigram_counts, bigram_counts,\n                                         len(unique_words), k=1.0)\nprint(f\"Perplexity for first train sample: {perplexity_train1:.4f}\")\n\ntest_sentence = ['i', 'like', 'a', 'dog']\nperplexity_test = calculate_perplexity(test_sentence,\n                                       unigram_counts, bigram_counts,\n                                       len(unique_words), k=1.0)\nprint(f\"Perplexity for test sample: {perplexity_test:.4f}\")\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[35], line 7\n      3 sentences = [['i', 'like', 'a', 'cat'],\n      4                  ['this', 'dog', 'is', 'like', 'a', 'cat']]\n      5 unique_words = list(set(sentences[0] + sentences[1]))\n----&gt; 7 unigram_counts = count_n_grams(sentences, 1)\n      8 bigram_counts = count_n_grams(sentences, 2)\n     11 perplexity_train1 = calculate_perplexity(sentences[0],\n     12                                          unigram_counts, bigram_counts,\n     13                                          len(unique_words), k=1.0)\n\nCell In[21], line 21, in count_n_grams(data, n, start_token, end_token)\n     16 n_grams = {}\n     18 ### START CODE HERE (Replace instances of 'None' with your code) ###\n     19 \n     20 # Go through each sentence in the data\n---&gt; 21 for sentence in None: # complete this line\n     22     \n     23     # prepend start token n times, and  append &lt;e&gt; one time\n     24     sentence = None\n     26     # convert list to tuple\n     27     # So that the sequence of words can be used as\n     28     # a key in the dictionary\n\nTypeError: 'NoneType' object is not iterable\n\n\n\n\n\n\nExpected Output\nPerplexity for first train sample: 2.8040\nPerplexity for test sample: 3.9654\n Note:  If your sentence is really long, there will be underflow when multiplying many fractions. - To handle longer sentences, modify your implementation to take the sum of the log of the probabilities.\n ## Part 4: Build an auto-complete system\nIn this section, you will combine the language models developed so far to implement an auto-complete system.\n ### Exercise 11 Compute probabilities for all possible next words and suggest the most likely one. - This function also take an optional argument start_with, which specifies the first few letters of the next words.\n\n\nHints\n\n\n\n\nestimate_probabilities returns a dictionary where the key is a word and the value is the word‚Äôs probability.\n\n\nUse str1.startswith(str2) to determine if a string starts with the letters of another string. For example, ‚Äòlearning‚Äô.startswith(‚Äòlea‚Äô) returns True, whereas ‚Äòlearning‚Äô.startswith(‚Äòear‚Äô) returns False. There are two additional parameters in str.startswith(), but you can use the default values for those parameters in this case.\n\n\n\n\n# UNQ_C11 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: suggest_a_word\ndef suggest_a_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0, start_with=None):\n    \"\"\"\n    Get suggestion for the next word\n    \n    Args:\n        previous_tokens: The sentence you input where each token is a word. Must have length &gt; n \n        n_gram_counts: Dictionary of counts of (n+1)-grams\n        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n        vocabulary: List of words\n        k: positive constant, smoothing parameter\n        start_with: If not None, specifies the first few letters of the next word\n        \n    Returns:\n        A tuple of \n          - string of the most likely next word\n          - corresponding probability\n    \"\"\"\n    \n    # length of previous words\n    n = len(list(n_gram_counts.keys())[0]) \n    \n    # From the words that the user already typed\n    # get the most recent 'n' words as the previous n-gram\n    previous_n_gram = previous_tokens[-n:]\n\n    # Estimate the probabilities that each word in the vocabulary\n    # is the next word,\n    # given the previous n-gram, the dictionary of n-gram counts,\n    # the dictionary of n plus 1 gram counts, and the smoothing constant\n    probabilities = estimate_probabilities(previous_n_gram,\n                                           n_gram_counts, n_plus1_gram_counts,\n                                           vocabulary, k=k)\n    \n    # Initialize suggested word to None\n    # This will be set to the word with highest probability\n    suggestion = None\n    \n    # Initialize the highest word probability to 0\n    # this will be set to the highest probability \n    # of all words to be suggested\n    max_prob = 0\n    \n    ### START CODE HERE (Replace instances of 'None' with your code) ###\n    \n    # For each word and its probability in the probabilities dictionary:\n    for word, prob in None: # complete this line\n        \n        # If the optional start_with string is set\n        if None: # complete this line\n            \n            # Check if the beginning of word does not match with the letters in 'start_with'\n            if None: # complete this line\n\n                # if they don't match, skip this word (move onto the next word)\n                None # complete this line\n        \n        # Check if this word's probability\n        # is greater than the current maximum probability\n        if None: # complete this line\n            \n            # If so, save this word as the best suggestion (so far)\n            suggestion = None\n            \n            # Save the new maximum probability\n            max_prob = None\n\n    ### END CODE HERE\n    \n    return suggestion, max_prob\n\n\n# test your code\nsentences = [['i', 'like', 'a', 'cat'],\n             ['this', 'dog', 'is', 'like', 'a', 'cat']]\nunique_words = list(set(sentences[0] + sentences[1]))\n\nunigram_counts = count_n_grams(sentences, 1)\nbigram_counts = count_n_grams(sentences, 2)\n\nprevious_tokens = [\"i\", \"like\"]\ntmp_suggest1 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0)\nprint(f\"The previous words are 'i like',\\n\\tand the suggested word is `{tmp_suggest1[0]}` with a probability of {tmp_suggest1[1]:.4f}\")\n\nprint()\n# test your code when setting the starts_with\ntmp_starts_with = 'c'\ntmp_suggest2 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0, start_with=tmp_starts_with)\nprint(f\"The previous words are 'i like', the suggestion must start with `{tmp_starts_with}`\\n\\tand the suggested word is `{tmp_suggest2[0]}` with a probability of {tmp_suggest2[1]:.4f}\")\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[37], line 6\n      2 sentences = [['i', 'like', 'a', 'cat'],\n      3              ['this', 'dog', 'is', 'like', 'a', 'cat']]\n      4 unique_words = list(set(sentences[0] + sentences[1]))\n----&gt; 6 unigram_counts = count_n_grams(sentences, 1)\n      7 bigram_counts = count_n_grams(sentences, 2)\n      9 previous_tokens = [\"i\", \"like\"]\n\nCell In[21], line 21, in count_n_grams(data, n, start_token, end_token)\n     16 n_grams = {}\n     18 ### START CODE HERE (Replace instances of 'None' with your code) ###\n     19 \n     20 # Go through each sentence in the data\n---&gt; 21 for sentence in None: # complete this line\n     22     \n     23     # prepend start token n times, and  append &lt;e&gt; one time\n     24     sentence = None\n     26     # convert list to tuple\n     27     # So that the sequence of words can be used as\n     28     # a key in the dictionary\n\nTypeError: 'NoneType' object is not iterable\n\n\n\n\n\nExpected output\nThe previous words are 'i like',\n    and the suggested word is `a` with a probability of 0.2727\n\nThe previous words are 'i like', the suggestion must start with `c`\n    and the suggested word is `cat` with a probability of 0.0909\n\n\nGet multiple suggestions\nThe function defined below loop over varioud n-gram models to get multiple suggestions.\n\ndef get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n    model_counts = len(n_gram_counts_list)\n    suggestions = []\n    for i in range(model_counts-1):\n        n_gram_counts = n_gram_counts_list[i]\n        n_plus1_gram_counts = n_gram_counts_list[i+1]\n        \n        suggestion = suggest_a_word(previous_tokens, n_gram_counts,\n                                    n_plus1_gram_counts, vocabulary,\n                                    k=k, start_with=start_with)\n        suggestions.append(suggestion)\n    return suggestions\n\n\n# test your code\nsentences = [['i', 'like', 'a', 'cat'],\n             ['this', 'dog', 'is', 'like', 'a', 'cat']]\nunique_words = list(set(sentences[0] + sentences[1]))\n\nunigram_counts = count_n_grams(sentences, 1)\nbigram_counts = count_n_grams(sentences, 2)\ntrigram_counts = count_n_grams(sentences, 3)\nquadgram_counts = count_n_grams(sentences, 4)\nqintgram_counts = count_n_grams(sentences, 5)\n\nn_gram_counts_list = [unigram_counts, bigram_counts, trigram_counts, quadgram_counts, qintgram_counts]\nprevious_tokens = [\"i\", \"like\"]\ntmp_suggest3 = get_suggestions(previous_tokens, n_gram_counts_list, unique_words, k=1.0)\n\nprint(f\"The previous words are 'i like', the suggestions are:\")\ndisplay(tmp_suggest3)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[39], line 6\n      2 sentences = [['i', 'like', 'a', 'cat'],\n      3              ['this', 'dog', 'is', 'like', 'a', 'cat']]\n      4 unique_words = list(set(sentences[0] + sentences[1]))\n----&gt; 6 unigram_counts = count_n_grams(sentences, 1)\n      7 bigram_counts = count_n_grams(sentences, 2)\n      8 trigram_counts = count_n_grams(sentences, 3)\n\nCell In[21], line 21, in count_n_grams(data, n, start_token, end_token)\n     16 n_grams = {}\n     18 ### START CODE HERE (Replace instances of 'None' with your code) ###\n     19 \n     20 # Go through each sentence in the data\n---&gt; 21 for sentence in None: # complete this line\n     22     \n     23     # prepend start token n times, and  append &lt;e&gt; one time\n     24     sentence = None\n     26     # convert list to tuple\n     27     # So that the sequence of words can be used as\n     28     # a key in the dictionary\n\nTypeError: 'NoneType' object is not iterable\n\n\n\n\n\nSuggest multiple words using n-grams of varying length\nCongratulations! You have developed all building blocks for implementing your own auto-complete systems.\nLet‚Äôs see this with n-grams of varying lengths (unigrams, bigrams, trigrams, 4-grams‚Ä¶6-grams).\n\nn_gram_counts_list = []\nfor n in range(1, 6):\n    print(\"Computing n-gram counts with n =\", n, \"...\")\n    n_model_counts = count_n_grams(train_data_processed, n)\n    n_gram_counts_list.append(n_model_counts)\n\nComputing n-gram counts with n = 1 ...\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[40], line 4\n      2 for n in range(1, 6):\n      3     print(\"Computing n-gram counts with n =\", n, \"...\")\n----&gt; 4     n_model_counts = count_n_grams(train_data_processed, n)\n      5     n_gram_counts_list.append(n_model_counts)\n\nNameError: name 'train_data_processed' is not defined\n\n\n\n\nprevious_tokens = [\"i\", \"am\", \"to\"]\ntmp_suggest4 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n\nprint(f\"The previous words are {previous_tokens}, the suggestions are:\")\ndisplay(tmp_suggest4)\n\nThe previous words are ['i', 'am', 'to'], the suggestions are:\n\n\n[]\n\n\n\nprevious_tokens = [\"i\", \"want\", \"to\", \"go\"]\ntmp_suggest5 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n\nprint(f\"The previous words are {previous_tokens}, the suggestions are:\")\ndisplay(tmp_suggest5)\n\nThe previous words are ['i', 'want', 'to', 'go'], the suggestions are:\n\n\n[]\n\n\n\nprevious_tokens = [\"hey\", \"how\", \"are\"]\ntmp_suggest6 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n\nprint(f\"The previous words are {previous_tokens}, the suggestions are:\")\ndisplay(tmp_suggest6)\n\nThe previous words are ['hey', 'how', 'are'], the suggestions are:\n\n\n[]\n\n\n\nprevious_tokens = [\"hey\", \"how\", \"are\", \"you\"]\ntmp_suggest7 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n\nprint(f\"The previous words are {previous_tokens}, the suggestions are:\")\ndisplay(tmp_suggest7)\n\nThe previous words are ['hey', 'how', 'are', 'you'], the suggestions are:\n\n\n[]\n\n\n\nprevious_tokens = [\"hey\", \"how\", \"are\", \"you\"]\ntmp_suggest8 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=\"d\")\n\nprint(f\"The previous words are {previous_tokens}, the suggestions are:\")\ndisplay(tmp_suggest8)\n\nThe previous words are ['hey', 'how', 'are', 'you'], the suggestions are:\n\n\n[]",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "A3 - Auto-Complete"
    ]
  },
  {
    "objectID": "posts/c2w3/lab01.html",
    "href": "posts/c2w3/lab01.html",
    "title": "N-grams Corpus preprocessing",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure¬†1",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "L1 - N-grams Corpus preprocessing"
    ]
  },
  {
    "objectID": "posts/c2w3/lab01.html#n-grams",
    "href": "posts/c2w3/lab01.html#n-grams",
    "title": "N-grams Corpus preprocessing",
    "section": "N-grams",
    "text": "N-grams\n\nSentence to n-gram\nThe next step is to build n-grams from the tokenized sentences.\nA sliding window of size n-words can generate the n-grams. The window scans the list of words starting at the sentence beginning, moving by a step of one word until it reaches the end of the sentence.\nHere is an example method that prints all trigrams in the given sentence.\n\ndef sentence_to_trigram(tokenized_sentence):\n    \"\"\"\n    Prints all trigrams in the given tokenized sentence.\n    \n    Args:\n        tokenized_sentence: The words list.\n    \n    Returns:\n        No output\n    \"\"\"\n    # note that the last position of i is 3rd to the end\n    for i in range(len(tokenized_sentence) - 3 + 1):\n        # the sliding window starts at position i and contains 3 words\n        trigram = tokenized_sentence[i : i + 3]\n        print(trigram)\n\ntokenized_sentence = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n\nprint(f'List all trigrams of sentence: {tokenized_sentence}\\n')\nsentence_to_trigram(tokenized_sentence)\n\nList all trigrams of sentence: ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n\n['i', 'am', 'happy']\n['am', 'happy', 'because']\n['happy', 'because', 'i']\n['because', 'i', 'am']\n['i', 'am', 'learning']\n['am', 'learning', '.']\n\n\n ### Prefix of an n-gram\nAs you saw in the lecture, the n-gram probability is often calculated based on the (n-1)-gram counts. The prefix is needed in the formula to calculate the probability of an n-gram.\n\\begin{equation*}\nP(w_n|w_1^{n-1})=\\frac{C(w_1^n)}{C(w_1^{n-1})}\n\\end{equation*}\nThe following code shows how to get an (n-1)-gram prefix from n-gram on an example of getting trigram from a 4-gram.\n\n# get trigram prefix from a 4-gram\nfourgram = ['i', 'am', 'happy','because']\ntrigram = fourgram[0:-1] # Get the elements from 0, included, up to the last element, not included.\nprint(trigram)\n\n['i', 'am', 'happy']\n\n\n\n\nStart and end of sentence word &lt;s&gt; and &lt;/s&gt;\nYou could see in the lecture that we must add some special characters at the beginning and the end of each sentence:\n\n&lt;s&gt; at beginning\n&lt;/s&gt; at the end\n\nFor n-grams, we must prepend n-1 of characters at the begining of the sentence.\nLet us have a look at how you can implement this in code.\n\n# when working with trigrams, you need to prepend 2 &lt;s&gt; and append one &lt;/s&gt;\nn = 3\ntokenized_sentence = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\ntokenized_sentence = [\"&lt;s&gt;\"] * (n - 1) + tokenized_sentence + [\"&lt;/s&gt;\"]\nprint(tokenized_sentence)\n\n['&lt;s&gt;', '&lt;s&gt;', 'i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.', '&lt;/s&gt;']\n\n\nThat‚Äôs all for the lab for ‚ÄúN-gram‚Äù lesson of week 3.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "L1 - N-grams Corpus preprocessing"
    ]
  },
  {
    "objectID": "posts/c2w3/index.html",
    "href": "posts/c2w3/index.html",
    "title": "Autocomplete and Language Models",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure¬†1\n\n\n\n\n\n\n\n\nFigure¬†2\nThese are my notes for Week 3 notes for the Natural Language Processing with Probabilistic Models Course in the Natural Language Processing Specialization Offered by DeepLearning.AI on Coursera.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete and Language Models"
    ]
  },
  {
    "objectID": "posts/c2w3/index.html#tldr---autocomplete-and-language-models",
    "href": "posts/c2w3/index.html#tldr---autocomplete-and-language-models",
    "title": "Autocomplete and Language Models",
    "section": "TL;DR - Autocomplete and Language Models",
    "text": "TL;DR - Autocomplete and Language Models\n\n\n\nLanguage Models in a nutshell\n\n\nThis week we learn how to model a language using N-grams. Starting from the definition of conditional probability we develop the probabilities of sequences of words. Next we add start and end of sentences tokens to our word sequence model. Next we add tokens to represent out of vocabulary words. Then we tackle sparsity by implementing smoothing and backoff. Finally we consider how to evaluate our language model.\nIn the labs we preprocess a corpus to create an N-gram language model. We then build the language model and evaluate it using perplexity.\nIn the assignment for this week, we build a language model to generate autocomplete a text fragment. We will also evaluate the perplexity of the model.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete and Language Models"
    ]
  },
  {
    "objectID": "posts/c2w3/index.html#n-grams-overview",
    "href": "posts/c2w3/index.html#n-grams-overview",
    "title": "Autocomplete and Language Models",
    "section": "N-Grams Overview",
    "text": "N-Grams Overview\nRecall how Firth suggested a distributional view of semantics?\nPreviously we used a vector space model to represent this idea. Now we dive deeper and develop a model for distributional semantics using probabilities of sequences of words. Once we can estimate these probabilities we can predict the next word in a sentence.\nN-gram refer to howe we model the a sequence of N words. For example, a bigram model would model the probability of a word given the previous word. A trigram model would model the probability of a word given the previous bigram. And so on. These probabilities are derived by counting frequencies in a corpus of texts.\nN-grams are fundamental and give we a foundation that will allow we to understand more complicated models in the specialization. They form the theoretical under pinning for the next two courses.\nN-grams models allow us to predict the probabilities of certain words happening in a specific sequence. Using that, we can build an auto-correct or even a search suggestion tool.\nOther applications of N-gram language modeling include:\n\nSpeech recognition\nSpelling correction\nAugmentative communication\n\n\n\n\n\n\n\n\ncaption\n\n\n\n\nFigure¬†3: Description\n\n\nThis week we are going to learn to:\n\nProcess a text corpus to N-gram language model\nHandle out of vocabulary words\nImplement smoothing for previously unseen N-grams\nLanguage model evaluation",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete and Language Models"
    ]
  },
  {
    "objectID": "posts/c2w3/index.html#n-grams-and-probabilities",
    "href": "posts/c2w3/index.html#n-grams-and-probabilities",
    "title": "Autocomplete and Language Models",
    "section": "N-grams and Probabilities",
    "text": "N-grams and Probabilities\nBefore we start computing probabilities of certain sequences, we need to first define what is an N-gram language model:\n\n\n\n\n\n\n\ncaption\n\n\n\n\nFigure¬†4: Description\n\n\nNow given the those definitions, we can label a sentence as follows:\n\n\n\n\n\n\n\ncaption\n\n\n\n\nFigure¬†5: Description\n\n\nIn other notation we can write:\n\nw_1^m = w_1 w_2 w_3 ... w_m\n \nw_1^3 = w_1 w_2 w_3\n \nw_{m-2}^m = w_{m-2} w_{m-1} w_m\n\nGiven the following corpus: I am happy because I am learning.\nSize of corpus m = 7.\n\nP(I) = \\frac{1}{7}\n\n\nP(happy) = \\frac{1}{7}\n\nTo generalize, the probability of a unigram is\n\nP(w) = \\frac{C(w)}{m}\n\nWhere C(w) is the count of the word in the corpus and m is the size of the corpus.\n\nBigram Probability:\n\n\n\n\n\n\n\ncaption\n\n\n\n\nFigure¬†6: Description\n\n\n\n\nTrigram Probability:\nTo compute the probability of a trigram: \nP(w_3 | w_1^2) = \\frac{C(w_1^2 w_3)}{C(w_1^2)}\n\n\nC(w_1^2 w_3) = C(w_1 w_2 w_3) = C(w_1^3)\n\nN-gram Probability:\n\nP(w_n | w_1^{n-1}) = \\frac{C(w_1^{n-1} w_n)}{C(w_1^{n-1})}\n\n\nC(w_1^{n-1} w_n) = C(w_1^n)",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete and Language Models"
    ]
  },
  {
    "objectID": "posts/c2w3/index.html#sequence-probabilities",
    "href": "posts/c2w3/index.html#sequence-probabilities",
    "title": "Autocomplete and Language Models",
    "section": "Sequence Probabilities",
    "text": "Sequence Probabilities\nWe just saw how to compute sequence probabilities, their short comings, and finally how to approximate N-gram probabilities. In doing so, we try to approximate the probability of a sentence. For example, what is the probability of the following sentence: The teacher drinks tea. To compute it, we will make use of the following:\n\nP(B|A) = \\frac{P(A,B)}{P(A)} \\Rightarrow P(A,B) = P(A)P(B|A)\n\n\nP(A,B,C,D) = P(A)P(B|A)P(C|A,B)P(D|A,B,C)\n\nTo compute the probability of a sequence, we can compute the following:\n\nP(\\text{The teacher drinks tea}) = P(\\text{The})P(\\text{teacher}|\\text{The})P(\\text{drinks}|\\text{The teacher})P(\\text{tea}|\\text{The teacher drinks})\n\nOne of the main issues with computing the probabilities above is the corpus rarely contains the exact same phrases as the ones we computed your probabilities on. Hence, we can easily end up getting a probability of 0. The Markov assumption indicates that only the last word matters. Hence:\n\n\\text{Bigram} \\qquad P(w_n | w_1^{n-1}) \\approx P(w_n | w_{n-1})\n\n\n\\text{Trigram} \\qquad P(w_n | w_1^{n-1}) \\approx P(w_n | w_{n-2}^{n-1})\n\n\n\\text{N-gram} \\qquad P(w_n | w_1^{n-1}) \\approx P(w_n | w_{n-N+1}^{n-1})\n\nWe can model the entire sentence as follows:\n\nP(w_1^n) = P(w_1)P(w_2|w_1)P(w_3|w_2)P(w_4|w_3) ... P(w_n|w_{n-1}) = \\prod_{i=1}^{n} P(w_i|w_{i-1})\n\n\nP(w_1^n) = P(w_1)P(w_2|w_1)P(w_3|w_2)P(w_4|w_3) ... P(w_n|w_{n-1}) = \\prod_{i=1}^{n} P(w_i|w_{i-1})",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete and Language Models"
    ]
  },
  {
    "objectID": "posts/c2w3/index.html#starting-and-ending-sentences",
    "href": "posts/c2w3/index.html#starting-and-ending-sentences",
    "title": "Autocomplete and Language Models",
    "section": "Starting and Ending Sentences",
    "text": "Starting and Ending Sentences\nWe usually start and end a sentence with the following tokens respectively: &lt;s&gt; &lt;/s&gt;.\nWhen computing probabilities using a unigram, we can append an &lt;s&gt; in the beginning of the sentence. To generalize to an N-gram language model, we can add N-1 start tokens &lt;s&gt;.\nFor the end of sentence token &lt;/s&gt;, we only need one even if it is an N-gram. Here is an example:\n\n\n\n\n\n\n\ncaption\n\n\n\n\nFigure¬†7: Description\n\n\nMake sure we know how to compute the probabilities above!",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete and Language Models"
    ]
  },
  {
    "objectID": "posts/c2w3/index.html#lecture-notebook-corpus-preprocessing-for-n-grams",
    "href": "posts/c2w3/index.html#lecture-notebook-corpus-preprocessing-for-n-grams",
    "title": "Autocomplete and Language Models",
    "section": "Lecture notebook: Corpus preprocessing for N-grams",
    "text": "Lecture notebook: Corpus preprocessing for N-grams\nlab 1 Corpus preprocessing for N-grams",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete and Language Models"
    ]
  },
  {
    "objectID": "posts/c2w3/index.html#the-n-gram-language-model",
    "href": "posts/c2w3/index.html#the-n-gram-language-model",
    "title": "Autocomplete and Language Models",
    "section": "The N-gram Language Model",
    "text": "The N-gram Language Model\n\n\n\n\n\n\n\nCount matrix\n\n\n\n\nFigure¬†8: Count Matrix\n\n\n\n\n\n\n\n\nProbability matrix\n\n\n\n\nFigure¬†9: Probability Matrix\n\n\n\n\n\n\n\n\nLanguage model\n\n\n\n\nFigure¬†10: Language model\n\n\n\n\n\n\n\n\nLog probability\n\n\n\n\nFigure¬†11: Log probability\n\n\n\n\n\nWe covered a lot of concepts in the previous video. We have seen:\n\nCount matrix c.f Figure¬†8\nProbability matrix c.f Figure¬†9\nLanguage model c.f. Figure¬†10\nLog probability c.f Figure¬†11 to avoid underflow\nGenerative language model c.f. Figure¬†12\n\nIn the count matrix:\n\nRows correspond to the unique corpus N-1 grams.\nColumns correspond to the unique corpus words.\n\nHere is an example of the count matrix of a bigram.\nTo convert it into a probability matrix, we can use the following formula:\n\nP(w_n \\mid w^{n-1}_{n-N+1}) = \\frac{C(w^{n-1}_{n-N+1}, w_n)}{C(w^{n-1}_{n-N+1})}\n\\tag{1}\n\nsum(row) = \\sum_{w \\in V} C(w^{n-1}_{n-N+1}, w) = C(w^{n-1}_{n-N+1})\n\\tag{2}\nNow given the probability matrix, we can generate the language model. We can compute the sentence probability and the next word prediction.\nTo compute the probability of a sequence, we needed to compute:\n\n\\begin{align*}\nP(w_1^n) &= P(w_1)P(w_2|w_1)P(w_3|w_2)P(w_4|w_3) \\ldots P(w_n|w_{n-1})  \\\\\n&= \\prod_{i=1}^{n} P(w_i | w_{i-1})\n\\end{align*}\n\\tag{3}\nTo avoid underflow, we can multiply by the log.\n\n\\begin{align*}\n\\log P(w_1^n) &= \\log P(w_1) + \\log P(w_2|w_1) + \\log P(w_3|w_2) + \\ldots+ \\log P(w_n|w_{n-1}) \\\\\n& = \\sum_{i=1}^{n} \\log P(w_i|w_{i-1})\n\\end{align*}\n\\tag{4}\nFinally, we can create a generative language model.\n\n\n\n\n\n\n\nGenerative language model\n\n\n\n\nFigure¬†12: Generative language model",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete and Language Models"
    ]
  },
  {
    "objectID": "posts/c2w3/index.html#lecture-notebook-building-the-language-model",
    "href": "posts/c2w3/index.html#lecture-notebook-building-the-language-model",
    "title": "Autocomplete and Language Models",
    "section": "Lecture notebook: Building the language model",
    "text": "Lecture notebook: Building the language model\nlab 2 Building the language model",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete and Language Models"
    ]
  },
  {
    "objectID": "posts/c2w3/index.html#language-model-evaluation",
    "href": "posts/c2w3/index.html#language-model-evaluation",
    "title": "Autocomplete and Language Models",
    "section": "Language Model Evaluation",
    "text": "Language Model Evaluation\nSplitting the Data We will now discuss the train/val/test splits and perplexity.\n\nTrain/Val/Test splits\n\n\n\nSmaller Corpora:\nLarger Corpora:\n\n\n\n\ntrain\n80%\n98%\n\n\ntest\n10%\n1%\n\n\nval\n10%\n1%\n\n\n\nThere are two main methods for splitting the data:\n\n\n\n\n\n\n\ncaption\n\n\n\n\nFigure¬†13: Count Matrix",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete and Language Models"
    ]
  },
  {
    "objectID": "posts/c2w3/index.html#perplexity",
    "href": "posts/c2w3/index.html#perplexity",
    "title": "Autocomplete and Language Models",
    "section": "Perplexity",
    "text": "Perplexity\nPerplexity is used to tell us whether a set of sentences look like they were written by humans rather than by a simple program choosing words at random. A text that is written by humans is more likely to have lower perplexity, where a text generated by random word choice would have a higher perplexity.\nConcretely, here are the formulas to calculate perplexity.\n\nPP(W) = P(s_1, s_2, \\ldots, s_m)^{-\\frac{1}{m}}\n\n\nPP(W) = \\sqrt{ \\prod_{i=1}^{m} \\prod_{j=1}^{|s_i|} \\frac{1}{P(w_j^{(i)} | w_{j-1}^{(i)})}}\n\n‚Äãw_j^{(i)} corresponds to the jth word in the ith sentence. If we were to concatenate all the sentences then w_i is the ith word in the test set. To compute the log perplexity, we go from:\n\nPP(W) = \\sqrt{ \\prod_{i=1}^{m} \\frac{1}{P(w_i | w_{i-1})}}\n\nTo\n\nlog PP(W) = -\\frac{1}{m} \\sum_{i=1}^{m} \\log_2 P(w_i | w_{i-1})",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete and Language Models"
    ]
  },
  {
    "objectID": "posts/c2w3/index.html#out-of-vocabulary-words",
    "href": "posts/c2w3/index.html#out-of-vocabulary-words",
    "title": "Autocomplete and Language Models",
    "section": "Out of Vocabulary Words",
    "text": "Out of Vocabulary Words\nMany times, we will be dealing with unknown words in the corpus. So how do we choose your vocabulary? What is a vocabulary?\nA vocabulary is a set of unique words supported by your language model. In some tasks like speech recognition or question answering, we will encounter and generate words only from a fixed set of words. Hence, a closed vocabulary.\nOpen vocabulary means that we may encounter words from outside the vocabulary, like a name of a new city in the training set. Here is one recipe that would allow we to handle unknown words.\n\nCreate vocabulary V\nReplace any word in corpus and not in V by &lt;UNK&gt;\nCount the probabilities with &lt;UNK&gt; as with any other word\n\n\n\n\n\n\n\n\ncaption\n\n\n\n\nFigure¬†14: Count Matrix\n\n\nThe example above shows how we can use min_frequency and replace all the words that show up fewer times than min_frequency by UNK. We can then treat UNK as a regular word.\n\nCriteria to create the vocabulary\n\nMin word frequency f\nMax |V|, include words by frequency\nUse &lt;UNK&gt; sparingly (Why?)\nPerplexity - only compare LMs with the same V",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete and Language Models"
    ]
  },
  {
    "objectID": "posts/c2w3/index.html#out-of-vocabulary-words-1",
    "href": "posts/c2w3/index.html#out-of-vocabulary-words-1",
    "title": "Autocomplete and Language Models",
    "section": "Out of Vocabulary Words",
    "text": "Out of Vocabulary Words",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete and Language Models"
    ]
  },
  {
    "objectID": "posts/c2w3/index.html#smoothing",
    "href": "posts/c2w3/index.html#smoothing",
    "title": "Autocomplete and Language Models",
    "section": "Smoothing",
    "text": "Smoothing\n\n\n\n\n\n\n\nProblem\n\n\n\n\nFigure¬†15: Missing N-grams\n\n\nThe three main concepts covered here are dealing with missing n-grams, smoothing, and Backoff and interpolation.\n\n\n\n\n\n\n\nSmoothing\n\n\n\n\nFigure¬†16: Add One Smoothing, Add K Smoothing\n\n\n\nP(w_n | w^{n-1}_{n-N+1}) = \\frac{C(w^{n-1}_{n-N+1}, w_n)}{C(w^{n-1}_{n-N+1})} \\qquad \\text{can be 0}\n\\tag{5}\nHence we can add-1 smoothing as follows to fix that problem:\n\nP(w_n | w_{n-1}) = \\frac{C(w_{n-1}, w_n) + 1}{\\sum_{w \\in V} (C(w_{n-1}, w) + 1)} = \\frac{C(w_{n-1}, w_n) + 1}{C(w_{n-1}) + V} \\qquad\n\\tag{6}\nAdd-k smoothing is very similar:\n\nP(w_n | w_{n-1}) = \\frac{C(w_{n-1}, w_n) + k}{\\sum_{w \\in V} (C(w_{n-1}, w) + k)} =\\frac{C(w_{n-1}, w_n) + k}{C(w_{n-1}) + k\\times V} \\qquad\n\\tag{7}\n\n\n\n\n\n\n\nBackoff\n\n\n\n\nFigure¬†17: Backoff\n\n\nWhen using back-off:\n\nIf N-gram missing =&gt; use (N-1)-gram, ‚Ä¶: Using the lower level N-grams (i.e.¬†(N-1)-gram, (N-2)-gram, down to unigram) distorts the probability distribution. Especially for smaller corpora, some probability needs to be discounted from higher level N-grams to use it for lower level N-grams.\nProbability discounting e.g.¬†Katz backoff: makes use of discounting.\n‚ÄúStupid‚Äù backoff: If the higher order N-gram probability is missing, the lower order N-gram probability is used, just multiplied by a constant. A constant of about 0.4 was experimentally shown to work well.\n\nHere is a visualization of the backoff process.\nWe can also use interpolation when computing probabilities as follows:\n\n\\hat{P}(w_n | w_{n-2} W_{n-1}) = \\lambda_1 \\times P(w_n | w_{n-2} w_{n-1}) + \\lambda_2 \\times P(w_n | w_{n-1}) + \\lambda_3 \\times P(w_n) \\qquad\n\\tag{8}\nWhere\n\n\\sum_i \\lambda_i = 1",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete and Language Models"
    ]
  },
  {
    "objectID": "posts/c2w3/index.html#week-summary",
    "href": "posts/c2w3/index.html#week-summary",
    "title": "Autocomplete and Language Models",
    "section": "Week Summary",
    "text": "Week Summary\nThis week we learned the following concepts\n\nN-Grams and probabilities\nApproximate sentence probability from N-Grams\nBuild a language model from a corpus\nFix missing information\nOut of vocabulary words with &lt;UNK&gt;\nMissing N-Gram in corpus with smoothing, backoff and interpolation\nEvaluate language model with perplexity\nCoding assignment!",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete and Language Models"
    ]
  },
  {
    "objectID": "posts/c2w3/index.html#lecture-notebook-language-model-generalization",
    "href": "posts/c2w3/index.html#lecture-notebook-language-model-generalization",
    "title": "Autocomplete and Language Models",
    "section": "Lecture notebook: Language model generalization",
    "text": "Lecture notebook: Language model generalization\nAutocomplete",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete and Language Models"
    ]
  },
  {
    "objectID": "posts/c3w4/index.html",
    "href": "posts/c3w4/index.html",
    "title": "Siamese Networks",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure¬†1\n\n\n\n\n\n\n\n\nFigure¬†2\nMy irreverent notes for Week 4 of the Natural Language Processing with Sequence Models Course in the Natural Language Processing Specialization Offered by DeepLearning.AI on Coursera",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Chat Bots"
    ]
  },
  {
    "objectID": "posts/c3w4/index.html#siamese-network",
    "href": "posts/c3w4/index.html#siamese-network",
    "title": "Siamese Networks",
    "section": "Siamese Network",
    "text": "Siamese Network\nIt is best to describe what a Siamese network is through an example.\n\n\n\n\n\n\n\nFigure¬†3\n\n\nNote that in the first example above, the two sentences mean the same thing but have completely different words. While in the second case, the two sentences mean completely different things but they have very similar words.\nClassification: learns what makes an input what it is.\nSiamese Networks: learns what makes two inputs the same\nHere are a few applications of siamese networks:\n\n\n\n\n\n\n\nFigure¬†4",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Chat Bots"
    ]
  },
  {
    "objectID": "posts/c3w4/index.html#architecture",
    "href": "posts/c3w4/index.html#architecture",
    "title": "Siamese Networks",
    "section": "Architecture",
    "text": "Architecture\nThe model architecture of a typical siamese network could look as follows:\n\n\n\n\n\n\n\nFigure¬†5\n\n\nThese two sub-networks are sister-networks which come together to produce a similarity score. Not all Siamese networks will be designed to contain LSTMs. One thing to remember is that sub-networks share identical parameters. This means that we only need to train one set of weights and not two.\nThe output of each sub-network is a vector. We can then run the output through a cosine similarity function to get the similarity score. In the next video, we will talk about the cost function for such a network.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Chat Bots"
    ]
  },
  {
    "objectID": "posts/c3w4/index.html#lab-creating-a-siamese-model-using-trax",
    "href": "posts/c3w4/index.html#lab-creating-a-siamese-model-using-trax",
    "title": "Siamese Networks",
    "section": "Lab: Creating a Siamese Model using Trax",
    "text": "Lab: Creating a Siamese Model using Trax\nCreating a Siamese Model using Trax",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Chat Bots"
    ]
  },
  {
    "objectID": "posts/c3w4/index.html#cost-function",
    "href": "posts/c3w4/index.html#cost-function",
    "title": "Siamese Networks",
    "section": "Cost Function",
    "text": "Cost Function\nLet us take a close look at the following slide:\n\n\n\n\n\n\n\nFigure¬†6\n\n\nNote that when trying to compute the cost for a siamese network we use the triplet loss. The triplet loss usually consists of an Anchor and a Positive example. Note that the anchor and the positive example have a cosine similarity score that is very close to one. On the other hand, the anchor and the negative example have a cosine similarity score close to -1. Now we are ideally trying to optimize the following equation: ‚àícos(A,P)+cos(A,N)‚â§0\nNote that if cos(A,P)=1 is 1 and cos(A,N)=‚àí1, then the equation is definitely less than 0. However, as cos(A,P) deviates from 1 and cos(A,N) deviates from -1, then we can end up getting a cost that is &gt; 0. Here is a visualization that would help we understand what is going on. Feel free to play with different numbers.\n\n\n\n\n\n\n\nFigure¬†7",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Chat Bots"
    ]
  },
  {
    "objectID": "posts/c3w4/index.html#triplets",
    "href": "posts/c3w4/index.html#triplets",
    "title": "Siamese Networks",
    "section": "Triplets",
    "text": "Triplets\nWe will now build on top of our previous cost function. To get the full cost function we will add a margin.\n\n\n\n\n\n\n\nFigure¬†8\n\n\nNote that we added an Œ± in the equation above. This allows we to have a margin of ‚Äúsafety‚Äù. When computing the full cost, we take the max of that the outcome of ‚àícos(A,P)+cos(A,N)+Œ± and 0. Note, we do not want to take a negative number as a cost.\nHere is a quick summary:\n\nùú∂: controls how far cos(A,P) is from cos(A,N)\nEasy negative triplet: cos(A,N) &lt; cos(A,P)\nSemi-hard negative triplet: cos(A,N) &lt; cos(A,P) &lt; cos(A,N) + ùú∂\nHard negative triplet: cos(A,P) &lt; cos(A,N)",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Chat Bots"
    ]
  },
  {
    "objectID": "posts/c3w4/index.html#computing-the-cost-i",
    "href": "posts/c3w4/index.html#computing-the-cost-i",
    "title": "Siamese Networks",
    "section": "Computing the Cost I",
    "text": "Computing the Cost I\nTo compute the cost, we will prepare the batches as follows:\n\n\n\n\n\n\n\nFigure¬†9\n\n\nNote that each example, has a similar example to its right, but no other example means the same thing. We will now introduce hard negative mining.\n\n\n\n\n\n\n\nFigure¬†10\n\n\nEach horizontal vector corresponds to the encoding of the corresponding question. Now when we multiply the two matrices and compute the cosine, we get the following:\n\n\n\n\n\n\n\nFigure¬†11\n\n\nThe diagonal line corresponds to scores of similar sentences, (normally they should be positive). The off-diagonals correspond to cosine scores between the anchor and the negative examples.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Chat Bots"
    ]
  },
  {
    "objectID": "posts/c3w4/index.html#computing-the-cost-ii",
    "href": "posts/c3w4/index.html#computing-the-cost-ii",
    "title": "Siamese Networks",
    "section": "Computing the Cost II",
    "text": "Computing the Cost II\nNow that we have the matrix with cosine similarity scores, which is the product of two matrices, we go ahead and compute the cost.\n\n\n\n\n\n\n\nFigure¬†12\n\n\nWe now introduce two concepts, the mean_neg, which is the mean negative of all the other off diagonals in the row, and the closest_neg, which corresponds to the highest number in the off diagonals.\nCost = max(‚àícos(A,P)+cos(A,N)+Œ±,0)\nSo we will have two costs now:\nCost1 = max(‚àícos(A,P)+mean_n eg+Œ±,0)\nCost2 = max(‚àícos(A,P)+closest_n eg+Œ±,0) ‚Å° The full cost is defined as: Cost1 + Cost2.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Chat Bots"
    ]
  },
  {
    "objectID": "posts/c3w4/index.html#lab-lecture-notebook-modified-triplet-loss",
    "href": "posts/c3w4/index.html#lab-lecture-notebook-modified-triplet-loss",
    "title": "Siamese Networks",
    "section": "Lab: Lecture Notebook: Modified Triplet Loss",
    "text": "Lab: Lecture Notebook: Modified Triplet Loss\nModified Triplet Loss",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Chat Bots"
    ]
  },
  {
    "objectID": "posts/c3w4/index.html#one-shot-learning",
    "href": "posts/c3w4/index.html#one-shot-learning",
    "title": "Siamese Networks",
    "section": "One Shot Learning",
    "text": "One Shot Learning\nImagine we are working in a bank and we need to verify the signature of a check. We can either build a classifier with K possible signatures as an output or we can build a classifier that tells we whether two signatures are the same.\n\n\n\n\n\n\n\nFigure¬†13\n\n\nHence, we resort to one shot learning. Instead of retraining your model for every signature, we can just learn a similarity score as follows:\n\n\n\n\n\n\n\nFigure¬†14",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Chat Bots"
    ]
  },
  {
    "objectID": "posts/c3w4/index.html#training-testing",
    "href": "posts/c3w4/index.html#training-testing",
    "title": "Siamese Networks",
    "section": "Training / Testing",
    "text": "Training / Testing\nAfter preparing the batches of vectors, we can proceed to multiplying the two matrices.\nHere is a quick recap of the first step:\n\n\n\n\n\n\n\nFigure¬†15\n\n\nThe next step is to implement the siamese model as follows:\nFinally when testing:\n\nConvert two inputs into an array of numbers\nFeed it into your model\nCompare ùíó_1,ùíó_2 using cosine similarity\nTest against a threshold \\tau\n\n\n\n\n\n\n\n\nFigure¬†16",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Chat Bots"
    ]
  },
  {
    "objectID": "posts/c3w4/index.html#lab-evaluate-a-siamese-model",
    "href": "posts/c3w4/index.html#lab-evaluate-a-siamese-model",
    "title": "Siamese Networks",
    "section": "Lab: Evaluate a Siamese Model",
    "text": "Lab: Evaluate a Siamese Model\nEvaluate a Siamese Model",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Chat Bots"
    ]
  },
  {
    "objectID": "posts/c3w4/index.html#acknowledgments",
    "href": "posts/c3w4/index.html#acknowledgments",
    "title": "Siamese Networks",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\n(Chadha 2020) Aman Chadha‚Äôs Notes\nIbrahim Jelliti‚Äôs Notes",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Chat Bots"
    ]
  },
  {
    "objectID": "posts/c3w2/lab03.html",
    "href": "posts/c3w2/lab03.html",
    "title": "Vanilla RNNs, GRUs and the scan function",
    "section": "",
    "text": "course banner\nIn this notebook, you will learn how to define the forward method for vanilla RNNs and GRUs. Additionally, you will see how to define and use the function scan to compute forward propagation for RNNs.\nBy completing this notebook, you will:\nimport numpy as np\nfrom numpy import random\nfrom time import perf_counter\nAn implementation of the sigmoid function is provided below so you can use it in this notebook.\ndef sigmoid(x): # Sigmoid function\n    return 1.0 / (1.0 + np.exp(-x))"
  },
  {
    "objectID": "posts/c3w2/lab03.html#forward-method-for-vanilla-rnns",
    "href": "posts/c3w2/lab03.html#forward-method-for-vanilla-rnns",
    "title": "Vanilla RNNs, GRUs and the scan function",
    "section": "1.1 Forward method for vanilla RNNs",
    "text": "1.1 Forward method for vanilla RNNs\nThe vanilla RNN cell is quite straight forward. Its most general structure is presented in the next figure:\n\nAs you saw in the lecture videos, the computations made in a vanilla RNN cell are equivalent to the following equations:\n\\begin{equation}\nh^{&lt;t&gt;}=g(W_{h}[h^{&lt;t-1&gt;},x^{&lt;t&gt;}] + b_h)\n\\label{eq: htRNN}\n\\end{equation}\n\\begin{equation}\n\\hat{y}^{&lt;t&gt;}=g(W_{yh}h^{&lt;t&gt;} + b_y)\n\\label{eq: ytRNN}\n\\end{equation}\nwhere [h^{&lt;t-1&gt;},x^{&lt;t&gt;}] means that h^{&lt;t-1&gt;} and x^{&lt;t&gt;} are concatenated together. In the next cell we provide the implementation of the forward method for a vanilla RNN.\n\ndef forward_V_RNN(inputs, weights): # Forward propagation for a a single vanilla RNN cell\n    x, h_t = inputs\n\n    # weights.\n    wh, _, _, bh, _, _ = weights\n\n    # new hidden state\n    h_t = np.dot(wh, np.concatenate([h_t, x])) + bh\n    h_t = sigmoid(h_t)\n\n    return h_t, h_t\n\nAs you can see, we omitted the computation of \\hat{y}^{&lt;t&gt;}. This was done for the sake of simplicity, so you can focus on the way that hidden states are updated here and in the GRU cell."
  },
  {
    "objectID": "posts/c3w2/lab03.html#forward-method-for-grus",
    "href": "posts/c3w2/lab03.html#forward-method-for-grus",
    "title": "Vanilla RNNs, GRUs and the scan function",
    "section": "1.2 Forward method for GRUs",
    "text": "1.2 Forward method for GRUs\nA GRU cell have more computations than the ones that vanilla RNNs have. You can see this visually in the following diagram:\n\nAs you saw in the lecture videos, GRUs have relevance \\Gamma_r and update \\Gamma_u gates that control how the hidden state h^{&lt;t&gt;} is updated on every time step. With these gates, GRUs are capable of keeping relevant information in the hidden state even for long sequences. The equations needed for the forward method in GRUs are provided below:\n\\begin{equation}\n\\Gamma_r=\\sigma{(W_r[h^{&lt;t-1&gt;}, x^{&lt;t&gt;}]+b_r)}\n\\end{equation}\n\\begin{equation}\n\\Gamma_u=\\sigma{(W_u[h^{&lt;t-1&gt;}, x^{&lt;t&gt;}]+b_u)}\n\\end{equation}\n\\begin{equation}\nc^{&lt;t&gt;}=\\tanh{(W_h[\\Gamma_r*h^{&lt;t-1&gt;},x^{&lt;t&gt;}]+b_h)}\n\\end{equation}\n\\begin{equation}\nh^{&lt;t&gt;}=\\Gamma_u*c^{&lt;t&gt;}+(1-\\Gamma_u)*h^{&lt;t-1&gt;}\n\\end{equation}\nIn the next cell, please implement the forward method for a GRU cell by computing the update u and relevance r gates, and the candidate hidden state c.\n\ndef forward_GRU(inputs, weights): # Forward propagation for a single GRU cell\n    x, h_t = inputs\n\n    # weights.\n    wu, wr, wc, bu, br, bc = weights\n\n    # Update gate\n    ### START CODE HERE (1-2 lINES) ###\n    u = np.dot(wu, np.concatenate([h_t, x])) + bu\n    u = sigmoid(u)\n    ### END CODE HERE ###\n    \n    # Relevance gate\n    ### START CODE HERE (1-2 lINES) ###\n    r = np.dot(wr, np.concatenate([h_t, x])) + br\n    r = sigmoid(u)\n    ### END CODE HERE ###\n    \n    # Candidate hidden state \n    ### START CODE HERE (1-2 lINES) ###\n    c = np.dot(wc, np.concatenate([r * h_t, x])) + bc\n    c = np.tanh(c)\n    ### END CODE HERE ###\n    \n    # New Hidden state h_t\n    h_t = u* c + (1 - u)* h_t\n    return h_t, h_t\n\nRun the following cell to check your implementation.\n\nforward_GRU([X[1],h_0], weights)[0]\n\narray([[ 9.77779014e-01],\n       [-9.97986240e-01],\n       [-5.19958083e-01],\n       [-9.99999886e-01],\n       [-9.99707004e-01],\n       [-3.02197037e-04],\n       [-9.58733503e-01],\n       [ 2.10804828e-02],\n       [ 9.77365398e-05],\n       [ 9.99833090e-01],\n       [ 1.63200940e-08],\n       [ 8.51874303e-01],\n       [ 5.21399924e-02],\n       [ 2.15495959e-02],\n       [ 9.99878828e-01],\n       [ 9.77165472e-01]])\n\n\nExpected output:\narray([[ 9.77779014e-01],\n       [-9.97986240e-01],\n       [-5.19958083e-01],\n       [-9.99999886e-01],\n       [-9.99707004e-01],\n       [-3.02197037e-04],\n       [-9.58733503e-01],\n       [ 2.10804828e-02],\n       [ 9.77365398e-05],\n       [ 9.99833090e-01],\n       [ 1.63200940e-08],\n       [ 8.51874303e-01],\n       [ 5.21399924e-02],\n       [ 2.15495959e-02],\n       [ 9.99878828e-01],\n       [ 9.77165472e-01]])"
  },
  {
    "objectID": "posts/c3w2/lab02.html",
    "href": "posts/c3w2/lab02.html",
    "title": "Working with JAX numpy and calculating perplexity: Ungraded Lecture Notebook",
    "section": "",
    "text": "course banner\nNormally you would import numpy and rename it as np.\nHowever in this week‚Äôs assignment you will notice that this convention has been changed.\nNow standard numpy is not renamed and trax.fastmath.numpy is renamed as np.\nThe rationale behind this change is that you will be using Trax‚Äôs numpy (which is compatible with JAX) far more often. Trax‚Äôs numpy supports most of the same functions as the regular numpy so the change won‚Äôt be noticeable in most cases.\nimport jax\nprint(jax.__version__)\n\n0.5.0\nimport numpy\nimport trax\nimport trax.fastmath.numpy as np\n\n# Setting random seeds\nfrom trax import fastmath\nseed=32\nrng = fastmath.random.get_prng(seed)\n#trax.supervised.trainer_lib.init_random_number_generators(32)\nnumpy.random.seed(32)\n\n2025-02-05 18:35:59.720117: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1738773359.738712  542932 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1738773359.743780  542932 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nOne important change to take into consideration is that the types of the resulting objects will be different depending on the version of numpy. With regular numpy you get numpy.ndarray but with Trax‚Äôs numpy you will get jax.interpreters.xla.DeviceArray. These two types map to each other. So if you find some error logs mentioning DeviceArray type, don‚Äôt worry about it, treat it like you would treat an ndarray and march ahead.\nYou can get a randomized numpy array by using the numpy.random.random() function.\nThis is one of the functionalities that Trax‚Äôs numpy does not currently support in the same way as the regular numpy.\nnumpy_array = numpy.random.random((5,10))\nprint(f\"The regular numpy array looks like this:\\n\\n {numpy_array}\\n\")\nprint(f\"It is of type: {type(numpy_array)}\")\n\nThe regular numpy array looks like this:\n\n [[0.85888927 0.37271115 0.55512878 0.95565655 0.7366696  0.81620514\n  0.10108656 0.92848807 0.60910917 0.59655344]\n [0.09178413 0.34518624 0.66275252 0.44171349 0.55148779 0.70371249\n  0.58940123 0.04993276 0.56179184 0.76635847]\n [0.91090833 0.09290995 0.90252139 0.46096041 0.45201847 0.99942549\n  0.16242374 0.70937058 0.16062408 0.81077677]\n [0.03514717 0.53488673 0.16650012 0.30841038 0.04506241 0.23857613\n  0.67483453 0.78238275 0.69520163 0.32895445]\n [0.49403187 0.52412136 0.29854125 0.46310814 0.98478429 0.50113492\n  0.39807245 0.72790532 0.86333097 0.02616954]]\n\nIt is of type: &lt;class 'numpy.ndarray'&gt;\nYou can easily cast regular numpy arrays or lists into trax numpy arrays using the trax.fastmath.numpy.array() function:\ntrax_numpy_array = np.array(numpy_array)\nprint(f\"The trax numpy array looks like this:\\n\\n {trax_numpy_array}\\n\")\nprint(f\"It is of type: {type(trax_numpy_array)}\")\n\nThe trax numpy array looks like this:\n\n [[0.8588893  0.37271115 0.55512875 0.9556565  0.7366696  0.81620514\n  0.10108656 0.9284881  0.60910916 0.59655344]\n [0.09178413 0.34518623 0.6627525  0.44171348 0.5514878  0.70371246\n  0.58940125 0.04993276 0.56179184 0.7663585 ]\n [0.91090834 0.09290995 0.9025214  0.46096042 0.45201847 0.9994255\n  0.16242374 0.7093706  0.16062407 0.81077677]\n [0.03514718 0.5348867  0.16650012 0.30841038 0.04506241 0.23857613\n  0.67483455 0.7823827  0.69520164 0.32895446]\n [0.49403188 0.52412134 0.29854125 0.46310815 0.9847843  0.50113493\n  0.39807245 0.72790533 0.86333096 0.02616954]]\n\nIt is of type: &lt;class 'jaxlib.xla_extension.ArrayImpl'&gt;\nHope you now understand the differences (and similarities) between these two versions and numpy. Great!\nThe previous section was a quick look at Trax‚Äôs numpy. However this notebook also aims to teach you how you can calculate the perplexity of a trained model."
  },
  {
    "objectID": "posts/c3w2/lab02.html#calculating-perplexity",
    "href": "posts/c3w2/lab02.html#calculating-perplexity",
    "title": "Working with JAX numpy and calculating perplexity: Ungraded Lecture Notebook",
    "section": "Calculating Perplexity",
    "text": "Calculating Perplexity\nThe perplexity is a metric that measures how well a probability model predicts a sample and it is commonly used to evaluate language models. It is defined as:\nP(W) = \\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}}\nAs an implementation hack, you would usually take the log of that formula (to enable us to use the log probabilities we get as output of our RNN, convert exponents to products, and products into sums which makes computations less complicated and computationally more efficient). You should also take care of the padding, since you do not want to include the padding when calculating the perplexity (because we do not want to have a perplexity measure artificially good). The algebra behind this process is explained next:\nlog P(W) = {log\\big(\\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}}\\big)}\n = {log\\big({\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}}\\big)^{\\frac{1}{N}}}\n = {log\\big({\\prod_{i=1}^{N}{P(w_i| w_1,...,w_{n-1})}}\\big)^{-\\frac{1}{N}}}   = -\\frac{1}{N}{log\\big({\\prod_{i=1}^{N}{P(w_i| w_1,...,w_{n-1})}}\\big)}   = -\\frac{1}{N}{\\big({\\sum_{i=1}^{N}{logP(w_i| w_1,...,w_{n-1})}}\\big)} \nYou will be working with a real example from this week‚Äôs assignment. The example is made up of: - predictions : batch of tensors corresponding to lines of text predicted by the model. - targets : batch of actual tensors corresponding to lines of text.\n\nfrom trax import layers as tl\n\n# Load from .npy files\npredictions = numpy.load('predictions.npy')\ntargets = numpy.load('targets.npy')\n\n# Cast to jax.interpreters.xla.DeviceArray\npredictions = np.array(predictions)\ntargets = np.array(targets)\n\n# Print shapes\nprint(f'predictions has shape: {predictions.shape}')\nprint(f'targets has shape: {targets.shape}')\n\npredictions has shape: (32, 64, 256)\ntargets has shape: (32, 64)\n\n\nNotice that the predictions have an extra dimension with the same length as the size of the vocabulary used.\nBecause of this you will need a way of reshaping targets to match this shape. For this you can use trax.layers.one_hot().\nNotice that predictions.shape[-1] will return the size of the last dimension of predictions.\n\nreshaped_targets = tl.one_hot(targets, predictions.shape[-1]) #trax's one_hot function takes the input as one_hot(x, n_categories, dtype=optional)\nprint(f'reshaped_targets has shape: {reshaped_targets.shape}')\n\nreshaped_targets has shape: (32, 64, 256)\n\n\nBy calculating the product of the predictions and the reshaped targets and summing across the last dimension, the total log perplexity can be computed:\n\ntotal_log_ppx = np.sum(predictions * reshaped_targets, axis= -1)\n\nNow you will need to account for the padding so this metric is not artificially deflated (since a lower perplexity means a better model). For identifying which elements are padding and which are not, you can use np.equal() and get a tensor with 1s in the positions of actual values and 0s where there are paddings.\n\nnon_pad = 1.0 - np.equal(targets, 0)\nprint(f'non_pad has shape: {non_pad.shape}\\n')\nprint(f'non_pad looks like this: \\n\\n {non_pad}')\n\nnon_pad has shape: (32, 64)\n\nnon_pad looks like this: \n\n [[1. 1. 1. ... 0. 0. 0.]\n [1. 1. 1. ... 0. 0. 0.]\n [1. 1. 1. ... 0. 0. 0.]\n ...\n [1. 1. 1. ... 0. 0. 0.]\n [1. 1. 1. ... 0. 0. 0.]\n [1. 1. 1. ... 0. 0. 0.]]\n\n\nBy computing the product of the total log perplexity and the non_pad tensor we remove the effect of padding on the metric:\n\nreal_log_ppx = total_log_ppx * non_pad\nprint(f'real perplexity still has shape: {real_log_ppx.shape}')\n\nreal perplexity still has shape: (32, 64)\n\n\nYou can check the effect of filtering out the padding by looking at the two log perplexity tensors:\n\nprint(f'log perplexity tensor before filtering padding: \\n\\n {total_log_ppx}\\n')\nprint(f'log perplexity tensor after filtering padding: \\n\\n {real_log_ppx}')\n\nlog perplexity tensor before filtering padding: \n\n [[ -5.396545    -1.0311184   -0.66916656 ... -22.37673    -23.18771\n  -21.843483  ]\n [ -4.5857706   -1.1341286   -8.538033   ... -20.15686    -26.837097\n  -23.57502   ]\n [ -5.2223887   -1.2824144   -0.17312431 ... -21.328228   -19.854412\n  -33.88444   ]\n ...\n [ -5.396545   -17.291681    -4.360766   ... -20.825802   -21.065838\n  -22.443115  ]\n [ -5.9313164  -14.247417    -0.2637329  ... -26.743248   -18.38433\n  -22.355278  ]\n [ -5.670536    -0.10595131   0.         ... -23.332523   -28.087376\n  -23.878807  ]]\n\nlog perplexity tensor after filtering padding: \n\n [[ -5.396545    -1.0311184   -0.66916656 ...  -0.          -0.\n   -0.        ]\n [ -4.5857706   -1.1341286   -8.538033   ...  -0.          -0.\n   -0.        ]\n [ -5.2223887   -1.2824144   -0.17312431 ...  -0.          -0.\n   -0.        ]\n ...\n [ -5.396545   -17.291681    -4.360766   ...  -0.          -0.\n   -0.        ]\n [ -5.9313164  -14.247417    -0.2637329  ...  -0.          -0.\n   -0.        ]\n [ -5.670536    -0.10595131   0.         ...  -0.          -0.\n   -0.        ]]\n\n\nTo get a single average log perplexity across all the elements in the batch you can sum across both dimensions and divide by the number of elements. Notice that the result will be the negative of the real log perplexity of the model:\n\nlog_ppx = np.sum(real_log_ppx) / np.sum(non_pad)\nlog_ppx = -log_ppx\nprint(f'The log perplexity and perplexity of the model are respectively: {log_ppx} and {np.exp(log_ppx)}')\n\nThe log perplexity and perplexity of the model are respectively: 2.3281209468841553 and 10.258646965026855\n\n\nCongratulations on finishing this lecture notebook! Now you should have a clear understanding of how to work with Trax‚Äôs numpy and how to compute the perplexity to evaluate your language models. Keep it up!"
  },
  {
    "objectID": "posts/c4w1/index.html",
    "href": "posts/c4w1/index.html",
    "title": "Week 1 - Neural Machine Translation",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nThis week‚Äôs slides\n\n\n\n\nSupplementary Figure¬†1\nMy notes for Week 1 of the Natural Language Processing with Attention Labels Course in the Natural Language Processing Specialization Offered by DeepLearning.AI on Coursera",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation"
    ]
  },
  {
    "objectID": "posts/c4w1/index.html#sec-sec-seq2seq-model",
    "href": "posts/c4w1/index.html#sec-sec-seq2seq-model",
    "title": "Week 1 - Neural Machine Translation",
    "section": "Seq2Seq model",
    "text": "Seq2Seq model\n\nIntroduced by Google in 2014\nMaps variable-length sequences to fixed-length memory\nLSTMs and GRUs are typically used to overcome the vanishing gradient problem\n\n\n\n\n\nencoder decoder architecture\n\nTherefore, attention mechanisms have become critical for sequence modeling in various tasks, allowing modeling of dependencies without caring too much about their distance in the input or output sequences.\nin this encoder decoder architecture the yellow block in the middle is the final hidden state produced by the encoder. It‚Äôs essentials a compressed representation of the sequence in this case the English sentence. The problem with RNN is they tend to have a bias for representing more recent data.\nOne approach to overcome this issue is to provide the decoder with the attention layer.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation"
    ]
  },
  {
    "objectID": "posts/c4w1/index.html#sec-w1v4-attention",
    "href": "posts/c4w1/index.html#sec-w1v4-attention",
    "title": "Week 1 - Neural Machine Translation",
    "section": "W1V4: Attention",
    "text": "W1V4: Attention\nThe attention mechanism uses encoded representations of both the input or the encoder hidden states and the outputs or the decoder hidden states. The keys and values are pairs. Both of dimension NN, where NN is the input sequence length and comes from the encoder hidden states. Keys and values have their own respective matrices, but the matrices have the same shape and are often the same. While the queries come from the decoder hidden states. One way we can think of it is as follows. Imagine that we are translating English into German. We can represent the word embeddings in the English language as keys and values. The queries will then be the German equivalent. We can then calculate the dot product between the query and the key. Note that similar vectors have higher dot products and non-similar vectors will have lower dot products. The intuition here is that we want to identify the corresponding words in the queries that are similar to the keys. This would allow your model to ‚Äúlook‚Äù or focus on the right place when translating each word.\nWe then run a Softmax:\n\nsoftmax(QK^T )  \n\\tag{1}\nThat gives a distribution of numbers between 0 and 1.\nWe then would multiply the output by V. Remember V in this example was the same as our keys, corresponding to the English word embeddings. Hence the equation becomes\n\nsoftmax(QK^T )V  \n {#sec-softmax-formula-2}\nIn the matrix, the lighter square shows where the model is actually looking when making the translation of that word. This mapping isn‚Äôt necessarily be one to one. The lighting just tells we to what extent is each word contributing to the input that‚Äôs fed into the decoder. As we can see several words can contribute to translating another word, depending on the weights (output) of the softmax that we use to create the new input. a picture of attention in translation with English to German An important thing to keep in mind is that the model should be flexible enough to connect each English word with its relevant German word, even if they don‚Äôt appear in the same position in their respective sentences. In other words, it should be flexible enough to handle differences in grammar and word ordering in different languages.\nIn a situation like the one just mentioned, where the grammar of foreign language requires a difference word order than the other, the attention is so flexible enough to find the connection. The first four tokens, the agreements on the, are pretty straightforward, but then the grammatical structure between French and English changes. Now instead of looking at the corresponding fifth token to translate the French word zone, the attention knows to look further down at the eighth token, which corresponds to the English word area, glorious and necessary. It‚Äôs pretty amazing, was a little matrix multiplication can do.\nSo attention is a layer of calculations that let your model focus on the most important parts of the sequence for each step. Queries, values, and keys are representations of the encoder and decoder hidden states. And they‚Äôre used to retrieve information inside the attention layer by calculating the similarity between the decoder queries and the encoder key- value pairs.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation"
    ]
  },
  {
    "objectID": "posts/c4w1/index.html#sec-evaluation-metrics",
    "href": "posts/c4w1/index.html#sec-evaluation-metrics",
    "title": "Week 1 - Neural Machine Translation",
    "section": "Evaluation metrics for Machine Translation",
    "text": "Evaluation metrics for Machine Translation\n\nBLEU\n\nThe authors of (Papineni et al. 2002) introduced the BLEU score.\nThe closer the BLEU score is to 1, the better a model preforms.\nThe closer to 0, the worse it does.\n\nTo get the BLEU score, the candidates and the references are usually based on an average of unigrams, bigrams, trigrams or even four-gram precision. For example using uni-grams:\n\n\n\n\nscreenshot_of_outline_slide\n\nWe would sum over the unique n-gram counts in the candidate and divide by the total number of words in the candidate. The same concept could apply to unigrams, bigrams, etc. One issue with the BLEU score is that it doesn‚Äôt take into account semantics, so it doesn‚Äôt take into account the order of the n-grams in the sentence.\n\nBLEU = BP\\Bigl(\\prod_{i=1}^{4}\\text{precision}_i\\Bigr)^{(1/4)}\n\\tag{2}\nwith the Brevity Penalty and precision defined as:\n\nBP = min\\Bigl(1, e^{(1-({ref}/{cand}))}\\Bigr)\n\\tag{3}\n\n\\text{Precision}_i = \\frac {\\sum_{snt \\in{cand}}\\sum_{i\\in{snt}}min\\Bigl(m^{i}_{cand}, m^{i}_{ref}\\Bigr)}{w^{i}_{t}}\n\\tag{4}\nwhere:\n\nm^{i}_{cand}, is the count of i-gram in candidate matching the reference translation.\nm^{i}_{ref}, is the count of i-gram in the reference translation.\nw^{i}_{t}, is the total number of i-grams in candidate translation.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation"
    ]
  },
  {
    "objectID": "posts/c4w1/index.html#sec-rouge",
    "href": "posts/c4w1/index.html#sec-rouge",
    "title": "Week 1 - Neural Machine Translation",
    "section": "ROUGE",
    "text": "ROUGE\n(Lin 2004) introduced a similar method for evaluation called the ROUGE score which calculates precision and recall for machine texts by counting the n-gram overlap between the machine texts and a reference text. Here is an example that calculates recall: \n\nRouge_{recall} = \\sum  \\frac{(\\{\\text{prediction n-grams}\\} \\cap \\{ \\text{test n-grams}\\})}{\\vert{ \\text{test n-grams}}\\vert }\n\\tag{5}\nRouge also allows we to compute precision as follows:\n\n\n\n\nprecision in ROUGE\n\n \\text{ROUGE}_{\\text{precision}} = \\sum \\frac{(\\{\\text{prediction n-grams}\\} \\cap \\{ \\text{test ngrams}\\})}{\\vert\\{ \\text{vocab}\\}\\vert}\n\\tag{6}\nThe ROUGE-N refers to the overlap of N-grams between the actual system and the reference summaries. The F-score metric combines Recall and precision into one metric.\n\nF_{score}= 2 \\times \\frac{(\\text{precision} \\times \\text{recall})}{(\\text{precision} + \\text{recall})}\n\\tag{7}",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation"
    ]
  },
  {
    "objectID": "posts/c4w1/index.html#sec-random-sampling",
    "href": "posts/c4w1/index.html#sec-random-sampling",
    "title": "Week 1 - Neural Machine Translation",
    "section": "Random sampling",
    "text": "Random sampling\nRandom sampling for decoding involves drawing a word from the softmax distribution. To explore the latent space it is possible to introduce a temperature variable which controls the randomness of the sample.\ndef logsoftmax_sample(log_probs, temperature=1.0):  \n  \"\"\"Returns a sample from a log-softmax output, with temperature.\n  Args:\n    log_probs: Logarithms of probabilities (often coming from LogSofmax)\n    temperature: For scaling before sampling (1.0 = default, 0.0 = pick argmax)\n  \"\"\"\n  # This is equivalent to sampling from a softmax with temperature.\n  u = np.random.uniform(low=1e-6, high=1.0 - 1e-6, size=log_probs.shape)\n  g = -np.log(-np.log(u))\n  return np.argmax(log_probs + g * temperature, axis=-1)",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation"
    ]
  },
  {
    "objectID": "posts/c4w1/index.html#sec-beam-search",
    "href": "posts/c4w1/index.html#sec-beam-search",
    "title": "Week 1 - Neural Machine Translation",
    "section": "Beam Search",
    "text": "Beam Search\nThe beam search algorithm is a limited (best-first search). The parameter for the beam width limits the choices considered at each step.\n\n\n\n\nBeam Search",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation"
    ]
  },
  {
    "objectID": "posts/c4w1/index.html#sec-minimum-bayes-risk",
    "href": "posts/c4w1/index.html#sec-minimum-bayes-risk",
    "title": "Week 1 - Neural Machine Translation",
    "section": "Minimum Bayes Risk",
    "text": "Minimum Bayes Risk\nMBR (Minimum Bayes Risk) Compares many samples against one another. To implement MBR:\n\nGenerate several random samples.\nCompare each sample against all the others and assign a similarity score using ROUGE.\nSelect the sample with the highest similarity: the golden one.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation"
    ]
  },
  {
    "objectID": "posts/c4w1/index.html#sec-summary",
    "href": "posts/c4w1/index.html#sec-summary",
    "title": "Week 1 - Neural Machine Translation",
    "section": "Summary",
    "text": "Summary\n\nMaximal Probability is a baseline - but not a particularly good one when the data is noisy.\nRandom sampling with temperature is better.\nBeam search uses conditional probabilities and the parameter.\nMBR takes several samples and compares them against each other to find the golden one.\n\nnote: although not mentioned in the next week‚Äôs notes Beam Search is useful for improving the summarization task. We can extract a golden summary from a number of samples using MBR. ROUGE-N is the preferred metric for evaluating summarization\n\nReferences\n\n-‚Äã (Peters et al. 2018)\n(Alammar 2024)\n\n\n\nAlammar, Jay. 2024. ‚ÄúThe Illustrated Transformer.‚Äù https://jalammar.github.io/illustrated-transformer.\n\n\nLin, Chin-Yew. 2004. ‚ÄúROUGE: A Package for Automatic Evaluation of Summaries.‚Äù In Text Summarization Branches Out, 74‚Äì81. Barcelona, Spain: Association for Computational Linguistics. https://aclanthology.org/W04-1013.\n\n\nPapineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. ‚ÄúBleu: A Method for Automatic Evaluation of Machine Translation.‚Äù In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, 311‚Äì18. https://www.aclweb.org/anthology/P02-1040.pdf.\n\n\nPeters, Matthew E., Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. ‚ÄúDeep Contextualized Word Representations.‚Äù CoRR abs/1802.05365. http://arxiv.org/abs/1802.05365.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation"
    ]
  },
  {
    "objectID": "posts/c2w4/lab04.html",
    "href": "posts/c2w4/lab04.html",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure¬†1\nIn this ungraded notebook, you‚Äôll try out all the individual techniques that you learned about in the lecture. Practicing on small examples will prepare you for the graded assignment, where you will combine the techniques in more advanced ways to create word embeddings from a real-life corpus.\nThis notebook is made of two main parts: data preparation, and the continuous bag-of-words (CBOW) model.\nTo get started, import and initialize all the libraries you will need.\nimport sys\n!{sys.executable} -m pip install emoji\n\nRequirement already satisfied: emoji in /home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages (1.4.1)\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize\nimport emoji\nimport numpy as np\n\nfrom utils2 import get_dict\n\nnltk.download('punkt')  # download pre-trained Punkt tokenizer for English\n\n[nltk_data] Downloading package punkt to /home/oren/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\nTrue"
  },
  {
    "objectID": "posts/c2w4/lab04.html#cleaning-and-tokenization",
    "href": "posts/c2w4/lab04.html#cleaning-and-tokenization",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Cleaning and tokenization",
    "text": "Cleaning and tokenization\nTo demonstrate the cleaning and tokenization process, consider a corpus that contains emojis and various punctuation signs.\n\ncorpus = 'Who ‚ù§Ô∏è \"word embeddings\" in 2020? I do!!!'\n\nFirst, replace all interrupting punctuation signs ‚Äî such as commas and exclamation marks ‚Äî with periods.\n\nprint(f'Corpus:  {corpus}')\ndata = re.sub(r'[,!?;-]+', '.', corpus)\nprint(f'After cleaning punctuation:  {data}')\n\nCorpus:  Who ‚ù§Ô∏è \"word embeddings\" in 2020? I do!!!\nAfter cleaning punctuation:  Who ‚ù§Ô∏è \"word embeddings\" in 2020. I do.\n\n\nNext, use NLTK‚Äôs tokenization engine to split the corpus into individual tokens.\n\nprint(f'Initial string:  {data}')\ndata = nltk.word_tokenize(data)\nprint(f'After tokenization:  {data}')\n\nInitial string:  Who ‚ù§Ô∏è \"word embeddings\" in 2020. I do.\nAfter tokenization:  ['Who', '‚ù§Ô∏è', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '.']\n\n\nFinally, as you saw in the lecture, get rid of numbers and punctuation other than periods, and convert all the remaining tokens to lowercase.\n\nprint(f'Initial list of tokens:  {data}')\ndata = [ ch.lower() for ch in data\n         if ch.isalpha()\n         or ch == '.'\n         or emoji.get_emoji_regexp().search(ch)\n       ]\nprint(f'After cleaning:  {data}')\n\nInitial list of tokens:  ['Who', '‚ù§Ô∏è', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '.']\nAfter cleaning:  ['who', '‚ù§Ô∏è', 'word', 'embeddings', 'in', '.', 'i', 'do', '.']\n\n\nNote that the heart emoji is considered as a token just like any normal word.\nNow let‚Äôs streamline the cleaning and tokenization process by wrapping the previous steps in a function.\n\ndef tokenize(corpus):\n    data = re.sub(r'[,!?;-]+', '.', corpus)\n    data = nltk.word_tokenize(data)  # tokenize string to words\n    data = [ ch.lower() for ch in data\n             if ch.isalpha()\n             or ch == '.'\n             or emoji.get_emoji_regexp().search(ch)\n           ]\n    return data\n\nApply this function to the corpus that you‚Äôll be working on in the rest of this notebook: ‚ÄúI am happy because I am learning‚Äù\n\ncorpus = 'I am happy because I¬†am learning'\nprint(f'Corpus:  {corpus}')\nwords = tokenize(corpus)\nprint(f'Words (tokens):  {words}')\n\nCorpus:  I am happy because I¬†am learning\nWords (tokens):  ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n\n\nNow try it out yourself with your own sentence.\n\ntokenize(\"Now it's your turn: try with your own sentence!\")\n\n['now', 'it', 'your', 'turn', 'try', 'with', 'your', 'own', 'sentence', '.']"
  },
  {
    "objectID": "posts/c2w4/lab04.html#sliding-window-of-words",
    "href": "posts/c2w4/lab04.html#sliding-window-of-words",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Sliding window of words",
    "text": "Sliding window of words\nNow that you have transformed the corpus into a list of clean tokens, you can slide a window of words across this list. For each window you can extract a center word and the context words.\nThe get_windows function in the next cell was introduced in the lecture.\n\ndef get_windows(words, C):\n    i = C\n    while i &lt; len(words) - C:\n        center_word = words[i]\n        context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n        yield context_words, center_word\n        i += 1\n\nThe first argument of this function is a list of words (or tokens). The second argument, C, is the context half-size. Recall that for a given center word, the context words are made of C words to the left and C words to the right of the center word.\nHere is how you can use this function to extract context words and center words from a list of tokens. These context and center words will make up the training set that you will use to train the CBOW model.\n\nfor x, y in get_windows(\n            ['i', 'am', 'happy', 'because', 'i', 'am', 'learning'],\n            2\n        ):\n    print(f'{x}\\t{y}')\n\n['i', 'am', 'because', 'i'] happy\n['am', 'happy', 'i', 'am']  because\n['happy', 'because', 'am', 'learning']  i\n\n\nThe first example of the training set is made of:\n\nthe context words ‚Äúi‚Äù, ‚Äúam‚Äù, ‚Äúbecause‚Äù, ‚Äúi‚Äù,\nand the center word to be predicted: ‚Äúhappy‚Äù.\n\nNow try it out yourself. In the next cell, you can change both the sentence and the context half-size.\n\nfor x, y in get_windows(tokenize(\"Now it's your turn: try with your own sentence!\"), 1):\n    print(f'{x}\\t{y}')\n\n['now', 'your'] it\n['it', 'turn']  your\n['your', 'try'] turn\n['turn', 'with']    try\n['try', 'your'] with\n['with', 'own'] your\n['your', 'sentence']    own\n['own', '.']    sentence"
  },
  {
    "objectID": "posts/c2w4/lab04.html#transforming-words-into-vectors-for-the-training-set",
    "href": "posts/c2w4/lab04.html#transforming-words-into-vectors-for-the-training-set",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Transforming words into vectors for the training set",
    "text": "Transforming words into vectors for the training set\nTo finish preparing the training set, you need to transform the context words and center words into vectors.\n\nMapping words to indices and indices to words\nThe center words will be represented as one-hot vectors, and the vectors that represent context words are also based on one-hot vectors.\nTo create one-hot word vectors, you can start by mapping each unique word to a unique integer (or index). We have provided a helper function, get_dict, that creates a Python dictionary that maps words to integers and back.\n\nword2Ind, Ind2word = get_dict(words)\n\nHere‚Äôs the dictionary that maps words to numeric indices.\n\nword2Ind\n\n{'am': 0, 'because': 1, 'happy': 2, 'i': 3, 'learning': 4}\n\n\nYou can use this dictionary to get the index of a word.\n\nprint(\"Index of the word 'i':  \",word2Ind['i'])\n\nIndex of the word 'i':   3\n\n\nAnd conversely, here‚Äôs the dictionary that maps indices to words.\n\nInd2word\n\n{0: 'am', 1: 'because', 2: 'happy', 3: 'i', 4: 'learning'}\n\n\n\nprint(\"Word which has index 2:  \",Ind2word[2] )\n\nWord which has index 2:   happy\n\n\nFinally, get the length of either of these dictionaries to get the size of the vocabulary of your corpus, in other words the number of different words making up the corpus.\n\nV = len(word2Ind)\nprint(\"Size of vocabulary: \", V)\n\nSize of vocabulary:  5\n\n\n\n\nGetting one-hot word vectors\nRecall from the lecture that you can easily convert an integer, n, into a one-hot vector.\nConsider the word ‚Äúhappy‚Äù. First, retrieve its numeric index.\n\nn = word2Ind['happy']\nn\n\n2\n\n\nNow create a vector with the size of the vocabulary, and fill it with zeros.\n\ncenter_word_vector = np.zeros(V)\ncenter_word_vector\n\narray([0., 0., 0., 0., 0.])\n\n\nYou can confirm that the vector has the right size.\n\nlen(center_word_vector) == V\n\nTrue\n\n\nNext, replace the 0 of the n-th element with a 1.\n\ncenter_word_vector[n] = 1\n\nAnd you have your one-hot word vector.\n\ncenter_word_vector\n\narray([0., 0., 1., 0., 0.])\n\n\nYou can now group all of these steps in a convenient function, which takes as parameters: a word to be encoded, a dictionary that maps words to indices, and the size of the vocabulary.\n\ndef word_to_one_hot_vector(word, word2Ind, V):\n    # BEGIN your code here\n    one_hot_vector = np.zeros(V)\n    one_hot_vector[word2Ind[word]] = 1\n    # END your code here\n    return one_hot_vector\n\nCheck that it works as intended.\n\nword_to_one_hot_vector('happy', word2Ind, V)\n\narray([0., 0., 1., 0., 0.])\n\n\nWhat is the word vector for ‚Äúlearning‚Äù?\n\n# BEGIN your code here\nword_to_one_hot_vector('learning', word2Ind, V)\n# END your code here\n\narray([0., 0., 0., 0., 1.])\n\n\nExpected output:\narray([0., 0., 0., 0., 1.])\n\n\nGetting context word vectors\nTo create the vectors that represent context words, you will calculate the average of the one-hot vectors representing the individual words.\nLet‚Äôs start with a list of context words.\n\ncontext_words = ['i', 'am', 'because', 'i']\n\nUsing Python‚Äôs list comprehension construct and the word_to_one_hot_vector function that you created in the previous section, you can create a list of one-hot vectors representing each of the context words.\n\ncontext_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\ncontext_words_vectors\n\n[array([0., 0., 0., 1., 0.]),\n array([1., 0., 0., 0., 0.]),\n array([0., 1., 0., 0., 0.]),\n array([0., 0., 0., 1., 0.])]\n\n\nAnd you can now simply get the average of these vectors using numpy‚Äôs mean function, to get the vector representation of the context words.\n\nnp.mean(context_words_vectors, axis=0)\n\narray([0.25, 0.25, 0.  , 0.5 , 0.  ])\n\n\nNote the axis=0 parameter that tells mean to calculate the average of the rows (if you had wanted the average of the columns, you would have used axis=1).\nNow create the context_words_to_vector function that takes in a list of context words, a word-to-index dictionary, and a vocabulary size, and outputs the vector representation of the context words.\n\ndef context_words_to_vector(context_words, word2Ind, V):\n    # BEGIN your code here\n    context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n    context_words_vectors = np.mean(context_words_vectors, axis=0)\n    # END your code here\n    return context_words_vectors\n\nAnd check that you obtain the same output as the manual approach above.\n\ncontext_words_to_vector(['i', 'am', 'because', 'i'], word2Ind, V)\n\narray([0.25, 0.25, 0.  , 0.5 , 0.  ])\n\n\nWhat is the vector representation of the context words ‚Äúam happy i am‚Äù?\n\n# BEGIN your code here\ncontext_words_to_vector(['am', 'happy', 'i', 'am'], word2Ind, V)\n# END your code here\n\narray([0.5 , 0.  , 0.25, 0.25, 0.  ])\n\n\nExpected output:\narray([0.5 , 0.  , 0.25, 0.25, 0.  ])"
  },
  {
    "objectID": "posts/c2w4/lab04.html#building-the-training-set",
    "href": "posts/c2w4/lab04.html#building-the-training-set",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Building the training set",
    "text": "Building the training set\nYou can now combine the functions that you created in the previous sections, to build a training set for the CBOW model, starting from the following tokenized corpus.\n\nwords\n\n['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n\n\nTo do this you need to use the sliding window function (get_windows) to extract the context words and center words, and you then convert these sets of words into a basic vector representation using word_to_one_hot_vector and context_words_to_vector.\n\nfor context_words, center_word in get_windows(words, 2):  # reminder: 2 is the context half-size\n    print(f'Context words:  {context_words} -&gt; {context_words_to_vector(context_words, word2Ind, V)}')\n    print(f'Center word:  {center_word} -&gt; {word_to_one_hot_vector(center_word, word2Ind, V)}')\n    print()\n\nContext words:  ['i', 'am', 'because', 'i'] -&gt; [0.25 0.25 0.   0.5  0.  ]\nCenter word:  happy -&gt; [0. 0. 1. 0. 0.]\n\nContext words:  ['am', 'happy', 'i', 'am'] -&gt; [0.5  0.   0.25 0.25 0.  ]\nCenter word:  because -&gt; [0. 1. 0. 0. 0.]\n\nContext words:  ['happy', 'because', 'am', 'learning'] -&gt; [0.25 0.25 0.25 0.   0.25]\nCenter word:  i -&gt; [0. 0. 0. 1. 0.]\n\n\n\nIn this practice notebook you‚Äôll be performing a single iteration of training using a single example, but in this week‚Äôs assignment you‚Äôll train the CBOW model using several iterations and batches of example. Here is how you would use a Python generator function (remember the yield keyword from the lecture?) to make it easier to iterate over a set of examples.\n\ndef get_training_example(words, C, word2Ind, V):\n    for context_words, center_word in get_windows(words, C):\n        yield context_words_to_vector(context_words, word2Ind, V), word_to_one_hot_vector(center_word, word2Ind, V)\n\nThe output of this function can be iterated on to get successive context word vectors and center word vectors, as demonstrated in the next cell.\n\nfor context_words_vector, center_word_vector in get_training_example(words, 2, word2Ind, V):\n    print(f'Context words vector:  {context_words_vector}')\n    print(f'Center word vector:  {center_word_vector}')\n    print()\n\nContext words vector:  [0.25 0.25 0.   0.5  0.  ]\nCenter word vector:  [0. 0. 1. 0. 0.]\n\nContext words vector:  [0.5  0.   0.25 0.25 0.  ]\nCenter word vector:  [0. 1. 0. 0. 0.]\n\nContext words vector:  [0.25 0.25 0.25 0.   0.25]\nCenter word vector:  [0. 0. 0. 1. 0.]\n\n\n\nYour training set is ready, you can now move on to the CBOW model itself."
  },
  {
    "objectID": "posts/c2w4/lab04.html#activation-functions",
    "href": "posts/c2w4/lab04.html#activation-functions",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Activation functions",
    "text": "Activation functions\nLet‚Äôs start by implementing the activation functions, ReLU and softmax.\n\nReLU\nReLU is used to calculate the values of the hidden layer, in the following formulas:\n\\begin{align}\n\\mathbf{z_1} &= \\mathbf{W_1}\\mathbf{x} + \\mathbf{b_1}  \\tag{1} \\\\\n\\mathbf{h} &= \\mathrm{ReLU}(\\mathbf{z_1})  \\tag{2} \\\\\n\\end{align}\nLet‚Äôs fix a value for \\mathbf{z_1} as a working example.\n\nnp.random.seed(10)\nz_1 = 10*np.random.rand(5, 1)-5\nz_1\n\narray([[ 2.71320643],\n       [-4.79248051],\n       [ 1.33648235],\n       [ 2.48803883],\n       [-0.01492988]])\n\n\nTo get the ReLU of this vector, you want all the negative values to become zeros.\nFirst create a copy of this vector.\n\nh = z_1.copy()\n\nNow determine which of its values are negative.\n\nh &lt; 0\n\narray([[False],\n       [ True],\n       [False],\n       [False],\n       [ True]])\n\n\nYou can now simply set all of the values which are negative to 0.\n\nh[h &lt; 0] = 0\n\nAnd that‚Äôs it: you have the ReLU of \\mathbf{z_1}!\n\nh\n\narray([[2.71320643],\n       [0.        ],\n       [1.33648235],\n       [2.48803883],\n       [0.        ]])\n\n\nNow implement ReLU as a function.\n\ndef relu(z):\n    # BEGIN your code here\n    result = z.copy()\n    result[result &lt; 0] = 0\n    # END your code here\n    \n    return result\n\nAnd check that it‚Äôs working.\n\nz = np.array([[-1.25459881], [ 4.50714306], [ 2.31993942], [ 0.98658484], [-3.4398136 ]])\nrelu(z)\n\narray([[0.        ],\n       [4.50714306],\n       [2.31993942],\n       [0.98658484],\n       [0.        ]])\n\n\nExpected output:\narray([[0.        ],\n       [4.50714306],\n       [2.31993942],\n       [0.98658484],\n       [0.        ]])\n\n\nSoftmax\nThe second activation function that you need is softmax. This function is used to calculate the values of the output layer of the neural network, using the following formulas:\n\\begin{align}\n\\mathbf{z_2} &= \\mathbf{W_2}\\mathbf{h} + \\mathbf{b_2}   \\tag{3} \\\\\n\\mathbf{\\hat y} &= \\mathrm{softmax}(\\mathbf{z_2})   \\tag{4} \\\\\n\\end{align}\nTo calculate softmax of a vector \\mathbf{z}, the i-th component of the resulting vector is given by:\n \\textrm{softmax}(\\textbf{z})_i = \\frac{e^{z_i} }{\\sum\\limits_{j=1}^{V} e^{z_j} }  \\tag{5} \nLet‚Äôs work through an example.\n\nz = np.array([9, 8, 11, 10, 8.5])\nz\n\narray([ 9. ,  8. , 11. , 10. ,  8.5])\n\n\nYou‚Äôll need to calculate the exponentials of each element, both for the numerator and for the denominator.\n\ne_z = np.exp(z)\ne_z\n\narray([ 8103.08392758,  2980.95798704, 59874.1417152 , 22026.46579481,\n        4914.7688403 ])\n\n\nThe denominator is equal to the sum of these exponentials.\n\nsum_e_z = np.sum(e_z)\nsum_e_z\n\nnp.float64(97899.41826492078)\n\n\nAnd the value of the first element of \\textrm{softmax}(\\textbf{z}) is given by:\n\ne_z[0]/sum_e_z\n\nnp.float64(0.08276947985173956)\n\n\nThis is for one element. You can use numpy‚Äôs vectorized operations to calculate the values of all the elements of the \\textrm{softmax}(\\textbf{z}) vector in one go.\nImplement the softmax function.\n\ndef softmax(z):\n    # BEGIN your code here\n    e_z = np.exp(z)\n    sum_e_z = np.sum(e_z)\n    return e_z / sum_e_z\n    # END your code here\n\nNow check that it works.\n\nsoftmax([9, 8, 11, 10, 8.5])\n\narray([0.08276948, 0.03044919, 0.61158833, 0.22499077, 0.05020223])\n\n\nExpected output:\narray([0.08276948, 0.03044919, 0.61158833, 0.22499077, 0.05020223])"
  },
  {
    "objectID": "posts/c2w4/lab04.html#dimensions-1-d-arrays-vs-2-d-column-vectors",
    "href": "posts/c2w4/lab04.html#dimensions-1-d-arrays-vs-2-d-column-vectors",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Dimensions: 1-D arrays vs 2-D column vectors",
    "text": "Dimensions: 1-D arrays vs 2-D column vectors\nBefore moving on to implement forward propagation, backpropagation, and gradient descent, let‚Äôs have a look at the dimensions of the vectors you‚Äôve been handling until now.\nCreate a vector of length V filled with zeros.\n\nx_array = np.zeros(V)\nx_array\n\narray([0., 0., 0., 0., 0.])\n\n\nThis is a 1-dimensional array, as revealed by the .shape property of the array.\n\nx_array.shape\n\n(5,)\n\n\nTo perform matrix multiplication in the next steps, you actually need your column vectors to be represented as a matrix with one column. In numpy, this matrix is represented as a 2-dimensional array.\nThe easiest way to convert a 1D vector to a 2D column matrix is to set its .shape property to the number of rows and one column, as shown in the next cell.\n\nx_column_vector = x_array.copy()\nx_column_vector.shape = (V, 1)  # alternatively ... = (x_array.shape[0], 1)\nx_column_vector\n\narray([[0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.]])\n\n\nThe shape of the resulting ‚Äúvector‚Äù is:\n\nx_column_vector.shape\n\n(5, 1)\n\n\nSo you now have a 5x1 matrix that you can use to perform standard matrix multiplication."
  },
  {
    "objectID": "posts/c2w4/lab04.html#forward-propagation",
    "href": "posts/c2w4/lab04.html#forward-propagation",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Forward propagation",
    "text": "Forward propagation\nLet‚Äôs dive into the neural network itself, which is shown below with all the dimensions and formulas you‚Äôll need.\n\n Figure 2\n\nSet N equal to 3. Remember that N is a hyperparameter of the CBOW model that represents the size of the word embedding vectors, as well as the size of the hidden layer.\n\nN = 3\n\n\nInitialization of the weights and biases\nBefore you start training the neural network, you need to initialize the weight matrices and bias vectors with random values.\nIn the assignment you will implement a function to do this yourself using numpy.random.rand. In this notebook, we‚Äôve pre-populated these matrices and vectors for you.\n\nW1 = np.array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n               [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n               [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n\nW2 = np.array([[-0.22182064, -0.43008631,  0.13310965],\n               [ 0.08476603,  0.08123194,  0.1772054 ],\n               [ 0.1871551 , -0.06107263, -0.1790735 ],\n               [ 0.07055222, -0.02015138,  0.36107434],\n               [ 0.33480474, -0.39423389, -0.43959196]])\n\nb1 = np.array([[ 0.09688219],\n               [ 0.29239497],\n               [-0.27364426]])\n\nb2 = np.array([[ 0.0352008 ],\n               [-0.36393384],\n               [-0.12775555],\n               [-0.34802326],\n               [-0.07017815]])\n\nCheck that the dimensions of these matrices match those shown in the figure above.\n\n# BEGIN your code here\nprint(f'V (vocabulary size): {V}')\nprint(f'N (embedding size / size of the hidden layer): {N}')\nprint(f'size of W1: {W1.shape} (NxV)')\nprint(f'size of b1: {b1.shape} (Nx1)')\nprint(f'size of W2: {W1.shape} (VxN)')\nprint(f'size of b2: {b2.shape} (Vx1)')\n# END your code here\n\nV (vocabulary size): 5\nN (embedding size / size of the hidden layer): 3\nsize of W1: (3, 5) (NxV)\nsize of b1: (3, 1) (Nx1)\nsize of W2: (3, 5) (VxN)\nsize of b2: (5, 1) (Vx1)\n\n\n\n\nTraining example\nRun the next cells to get the first training example, made of the vector representing the context words ‚Äúi am because i‚Äù, and the target which is the one-hot vector representing the center word ‚Äúhappy‚Äù.\n\nYou don‚Äôt need to worry about the Python syntax, but there are some explanations below if you want to know what‚Äôs happening behind the scenes.\n\n\ntraining_examples = get_training_example(words, 2, word2Ind, V)\n\n\nget_training_examples, which uses the yield keyword, is known as a generator. When run, it builds an iterator, which is a special type of object that‚Ä¶ you can iterate on (using a for loop for instance), to retrieve the successive values that the function generates.\nIn this case get_training_examples yields training examples, and iterating on training_examples will return the successive training examples.\n\n\nx_array, y_array = next(training_examples)\n\n\nnext is another special keyword, which gets the next available value from an iterator. Here, you‚Äôll get the very first value, which is the first training example. If you run this cell again, you‚Äôll get the next value, and so on until the iterator runs out of values to return.\nIn this notebook next is used because you will only be performing one iteration of training. In this week‚Äôs assignment with the full training over several iterations you‚Äôll use regular for loops with the iterator that supplies the training examples.\n\nThe vector representing the context words, which will be fed into the neural network, is:\n\nx_array\n\narray([0.25, 0.25, 0.  , 0.5 , 0.  ])\n\n\nThe one-hot vector representing the center word to be predicted is:\n\ny_array\n\narray([0., 0., 1., 0., 0.])\n\n\nNow convert these vectors into matrices (or 2D arrays) to be able to perform matrix multiplication on the right types of objects, as explained above.\n\nx = x_array.copy()\nx.shape = (V, 1)\nprint('x')\nprint(x)\nprint()\n\ny = y_array.copy()\ny.shape = (V, 1)\nprint('y')\nprint(y)\n\nx\n[[0.25]\n [0.25]\n [0.  ]\n [0.5 ]\n [0.  ]]\n\ny\n[[0.]\n [0.]\n [1.]\n [0.]\n [0.]]\n\n\n\n\nValues of the hidden layer\nNow that you have initialized all the variables that you need for forward propagation, you can calculate the values of the hidden layer using the following formulas:\n\\begin{align}\n\\mathbf{z_1} = \\mathbf{W_1}\\mathbf{x} + \\mathbf{b_1}  \\tag{1} \\\\\n\\mathbf{h} = \\mathrm{ReLU}(\\mathbf{z_1})  \\tag{2} \\\\\n\\end{align}\nFirst, you can calculate the value of \\mathbf{z_1}.\n\nz1 = np.dot(W1, x) + b1\n\n\n¬†np.dot is numpy‚Äôs function for matrix multiplication.\n\nAs expected you get an N by 1 matrix, or column vector with N elements, where N is equal to the embedding size, which is 3 in this example.\n\nz1\n\narray([[ 0.36483875],\n       [ 0.63710329],\n       [-0.3236647 ]])\n\n\nYou can now take the ReLU of \\mathbf{z_1} to get \\mathbf{h}, the vector with the values of the hidden layer.\n\nh = relu(z1)\nh\n\narray([[0.36483875],\n       [0.63710329],\n       [0.        ]])\n\n\nApplying ReLU means that the negative element of \\mathbf{z_1} has been replaced with a zero.\n\n\nValues of the output layer\nHere are the formulas you need to calculate the values of the output layer, represented by the vector \\mathbf{\\hat y}:\n\\begin{align}\n\\mathbf{z_2} &= \\mathbf{W_2}\\mathbf{h} + \\mathbf{b_2}   \\tag{3} \\\\\n\\mathbf{\\hat y} &= \\mathrm{softmax}(\\mathbf{z_2})   \\tag{4} \\\\\n\\end{align}\nFirst, calculate \\mathbf{z_2}.\n\n# BEGIN your code here\nz2 = np.dot(W2, h) + b2\n# END your code here\n\nz2\n\narray([[-0.31973737],\n       [-0.28125477],\n       [-0.09838369],\n       [-0.33512159],\n       [-0.19919612]])\n\n\nExpected output:\narray([[-0.31973737],\n       [-0.28125477],\n       [-0.09838369],\n       [-0.33512159],\n       [-0.19919612]])\nThis is a V by 1 matrix, where V is the size of the vocabulary, which is 5 in this example.\nNow calculate the value of \\mathbf{\\hat y}.\n\n# BEGIN your code here\ny_hat = softmax(z2)\n# END your code here\n\ny_hat\n\narray([[0.18519074],\n       [0.19245626],\n       [0.23107446],\n       [0.18236353],\n       [0.20891502]])\n\n\nExpected output:\narray([[0.18519074],\n       [0.19245626],\n       [0.23107446],\n       [0.18236353],\n       [0.20891502]])\nAs you‚Äôve performed the calculations with random matrices and vectors (apart from the input vector), the output of the neural network is essentially random at this point. The learning process will adjust the weights and biases to match the actual targets better.\nThat being said, what word did the neural network predict?\n\n\nSolution\n\n\nThe neural network predicted the word ‚Äúhappy‚Äù: the largest element of \\mathbf{\\hat y} is the third one, and the third word of the vocabulary is ‚Äúhappy‚Äù.\n\n\nHere‚Äôs how you could implement this in Python:\n\n\nprint(Ind2word[np.argmax(y_hat)])\n\nWell done, you‚Äôve completed the forward propagation phase!"
  },
  {
    "objectID": "posts/c2w4/lab04.html#cross-entropy-loss",
    "href": "posts/c2w4/lab04.html#cross-entropy-loss",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Cross-entropy loss",
    "text": "Cross-entropy loss\nNow that you have the network‚Äôs prediction, you can calculate the cross-entropy loss to determine how accurate the prediction was compared to the actual target.\n\nRemember that you are working on a single training example, not on a batch of examples, which is why you are using loss and not cost, which is the generalized form of loss.\n\nFirst let‚Äôs recall what the prediction was.\n\ny_hat\n\narray([[0.18519074],\n       [0.19245626],\n       [0.23107446],\n       [0.18236353],\n       [0.20891502]])\n\n\nAnd the actual target value is:\n\ny\n\narray([[0.],\n       [0.],\n       [1.],\n       [0.],\n       [0.]])\n\n\nThe formula for cross-entropy loss is:\n J=-\\sum\\limits_{k=1}^{V}y_k\\log{\\hat{y}_k} \\tag{6}\nImplement the cross-entropy loss function.\nHere are a some hints if you‚Äôre stuck.\n\n\nHint 1\n\n&lt;p&gt;To multiply two numpy matrices (such as &lt;code&gt;y&lt;/code&gt; and &lt;code&gt;y_hat&lt;/code&gt;) element-wise, you can simply use the &lt;code&gt;*&lt;/code&gt; operator.&lt;/p&gt;\n\n\nHint 2\n\n\nOnce you have a vector equal to the element-wise multiplication of y and y_hat, you can use np.sum to calculate the sum of the elements of this vector.\n\n\ndef cross_entropy_loss(y_predicted, y_actual):\n    # BEGIN your code here\n    loss = np.sum(-np.log(y_hat)*y)\n    # END your code here\n    return loss\n\nNow use this function to calculate the loss with the actual values of \\mathbf{y} and \\mathbf{\\hat y}.\n\ncross_entropy_loss(y_hat, y)\n\nnp.float64(1.4650152923611108)\n\n\nExpected output:\n1.4650152923611106\nThis value is neither good nor bad, which is expected as the neural network hasn‚Äôt learned anything yet.\nThe actual learning will start during the next phase: backpropagation."
  },
  {
    "objectID": "posts/c2w4/lab04.html#backpropagation",
    "href": "posts/c2w4/lab04.html#backpropagation",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Backpropagation",
    "text": "Backpropagation\nThe formulas that you will implement for backpropagation are the following.\n\\begin{align}\n\\frac{\\partial J}{\\partial \\mathbf{W_1}} &= \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right )\\mathbf{x}^\\top \\tag{7}\\\\\n\\frac{\\partial J}{\\partial \\mathbf{W_2}} &= (\\mathbf{\\hat{y}} - \\mathbf{y})\\mathbf{h^\\top} \\tag{8}\\\\\n\\frac{\\partial J}{\\partial \\mathbf{b_1}} &= \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right ) \\tag{9}\\\\\n\\frac{\\partial J}{\\partial \\mathbf{b_2}} &= \\mathbf{\\hat{y}} - \\mathbf{y} \\tag{10}\n\\end{align}\n\nNote: these formulas are slightly simplified compared to the ones in the lecture as you‚Äôre working on a single training example, whereas the lecture provided the formulas for a batch of examples. In the assignment you‚Äôll be implementing the latter.\n\nLet‚Äôs start with an easy one.\nCalculate the partial derivative of the loss function with respect to \\mathbf{b_2}, and store the result in grad_b2.\n\\frac{\\partial J}{\\partial \\mathbf{b_2}} = \\mathbf{\\hat{y}} - \\mathbf{y} \\tag{10}\n\n# BEGIN your code here\ngrad_b2 = y_hat - y\n# END your code here\n\ngrad_b2\n\narray([[ 0.18519074],\n       [ 0.19245626],\n       [-0.76892554],\n       [ 0.18236353],\n       [ 0.20891502]])\n\n\nExpected output:\narray([[ 0.18519074],\n       [ 0.19245626],\n       [-0.76892554],\n       [ 0.18236353],\n       [ 0.20891502]])\nNext, calculate the partial derivative of the loss function with respect to \\mathbf{W_2}, and store the result in grad_W2.\n\\frac{\\partial J}{\\partial \\mathbf{W_2}} = (\\mathbf{\\hat{y}} - \\mathbf{y})\\mathbf{h^\\top} \\tag{8}\n\nHint: use .T to get a transposed matrix, e.g.¬†h.T returns \\mathbf{h^\\top}.\n\n\n# BEGIN your code here\ngrad_W2 = np.dot(y_hat - y, h.T)\n# END your code here\n\ngrad_W2\n\narray([[ 0.06756476,  0.11798563,  0.        ],\n       [ 0.0702155 ,  0.12261452,  0.        ],\n       [-0.28053384, -0.48988499,  0.        ],\n       [ 0.06653328,  0.1161844 ,  0.        ],\n       [ 0.07622029,  0.13310045,  0.        ]])\n\n\nExpected output:\narray([[ 0.06756476,  0.11798563,  0.        ],\n       [ 0.0702155 ,  0.12261452,  0.        ],\n       [-0.28053384, -0.48988499,  0.        ],\n       [ 0.06653328,  0.1161844 ,  0.        ],\n       [ 0.07622029,  0.13310045,  0.        ]])\nNow calculate the partial derivative with respect to \\mathbf{b_1} and store the result in grad_b1.\n\\frac{\\partial J}{\\partial \\mathbf{b_1}} = \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right ) \\tag{9}\n\n# BEGIN your code here\ngrad_b1 = relu(np.dot(W2.T, y_hat - y))\n# END your code here\n\ngrad_b1\n\narray([[0.        ],\n       [0.        ],\n       [0.17045858]])\n\n\nExpected output:\narray([[0.        ],\n       [0.        ],\n       [0.17045858]])\nFinally, calculate the partial derivative of the loss with respect to \\mathbf{W_1}, and store it in grad_W1.\n\\frac{\\partial J}{\\partial \\mathbf{W_1}} = \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right )\\mathbf{x}^\\top \\tag{7}\n\n# BEGIN your code here\ngrad_W1 = np.dot(relu(np.dot(W2.T, y_hat - y)), x.T)\n# END your code here\n\ngrad_W1\n\narray([[0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.04261464, 0.04261464, 0.        , 0.08522929, 0.        ]])\n\n\nExpected output:\narray([[0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.04261464, 0.04261464, 0.        , 0.08522929, 0.        ]])\nBefore moving on to gradient descent, double-check that all the matrices have the expected dimensions.\n\n# BEGIN your code here\nprint(f'V (vocabulary size): {V}')\nprint(f'N (embedding size / size of the hidden layer): {N}')\nprint(f'size of grad_W1: {grad_W1.shape} (NxV)')\nprint(f'size of grad_b1: {grad_b1.shape} (Nx1)')\nprint(f'size of grad_W2: {grad_W1.shape} (VxN)')\nprint(f'size of grad_b2: {grad_b2.shape} (Vx1)')\n# END your code here\n\nV (vocabulary size): 5\nN (embedding size / size of the hidden layer): 3\nsize of grad_W1: (3, 5) (NxV)\nsize of grad_b1: (3, 1) (Nx1)\nsize of grad_W2: (3, 5) (VxN)\nsize of grad_b2: (5, 1) (Vx1)"
  },
  {
    "objectID": "posts/c2w4/lab04.html#gradient-descent",
    "href": "posts/c2w4/lab04.html#gradient-descent",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Gradient descent",
    "text": "Gradient descent\nDuring the gradient descent phase, you will update the weights and biases by subtracting \\alpha times the gradient from the original matrices and vectors, using the following formulas.\n\\begin{align}\n\\mathbf{W_1} &:= \\mathbf{W_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_1}} \\tag{11}\\\\\n\\mathbf{W_2} &:= \\mathbf{W_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_2}} \\tag{12}\\\\\n\\mathbf{b_1} &:= \\mathbf{b_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_1}} \\tag{13}\\\\\n\\mathbf{b_2} &:= \\mathbf{b_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_2}} \\tag{14}\\\\\n\\end{align}\nFirst, let set a value for \\alpha.\n\nalpha = 0.03\n\nThe updated weight matrix \\mathbf{W_1} will be:\n\nW1_new = W1 - alpha * grad_W1\n\nLet‚Äôs compare the previous and new values of \\mathbf{W_1}:\n\nprint('old value of W1:')\nprint(W1)\nprint()\nprint('new value of W1:')\nprint(W1_new)\n\nold value of W1:\n[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]\n [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]\n [ 0.26637602 -0.23846886 -0.37770863 -0.11399446  0.34008124]]\n\nnew value of W1:\n[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]\n [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]\n [ 0.26509758 -0.2397473  -0.37770863 -0.11655134  0.34008124]]\n\n\nThe difference is very subtle (hint: take a closer look at the last row), which is why it takes a fair amount of iterations to train the neural network until it reaches optimal weights and biases starting from random values.\nNow calculate the new values of \\mathbf{W_2} (to be stored in W2_new), \\mathbf{b_1} (in b1_new), and \\mathbf{b_2} (in b2_new).\n\\begin{align}\n\\mathbf{W_2} &:= \\mathbf{W_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_2}} \\tag{12}\\\\\n\\mathbf{b_1} &:= \\mathbf{b_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_1}} \\tag{13}\\\\\n\\mathbf{b_2} &:= \\mathbf{b_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_2}} \\tag{14}\\\\\n\\end{align}\n\n# BEGIN your code here\nW2_new = W2 - alpha * grad_W2\nb1_new = b1 - alpha * grad_b1\nb2_new = b2 - alpha * grad_b2\n# END your code here\n\nprint('W2_new')\nprint(W2_new)\nprint()\nprint('b1_new')\nprint(b1_new)\nprint()\nprint('b2_new')\nprint(b2_new)\n\nW2_new\n[[-0.22384758 -0.43362588  0.13310965]\n [ 0.08265956  0.0775535   0.1772054 ]\n [ 0.19557112 -0.04637608 -0.1790735 ]\n [ 0.06855622 -0.02363691  0.36107434]\n [ 0.33251813 -0.3982269  -0.43959196]]\n\nb1_new\n[[ 0.09688219]\n [ 0.29239497]\n [-0.27875802]]\n\nb2_new\n[[ 0.02964508]\n [-0.36970753]\n [-0.10468778]\n [-0.35349417]\n [-0.0764456 ]]\n\n\nExpected output:\nW2_new\n[[-0.22384758 -0.43362588  0.13310965]\n [ 0.08265956  0.0775535   0.1772054 ]\n [ 0.19557112 -0.04637608 -0.1790735 ]\n [ 0.06855622 -0.02363691  0.36107434]\n [ 0.33251813 -0.3982269  -0.43959196]]\n\nb1_new\n[[ 0.09688219]\n [ 0.29239497]\n [-0.27875802]]\n\nb2_new\n[[ 0.02964508]\n [-0.36970753]\n [-0.10468778]\n [-0.35349417]\n [-0.0764456 ]]\nCongratulations, you have completed one iteration of training using one training example!\nYou‚Äôll need many more iterations to fully train the neural network, and you can optimize the learning process by training on batches of examples, as described in the lecture. You will get to do this during this week‚Äôs assignment."
  },
  {
    "objectID": "posts/c2w4/lab04.html#extracting-word-embedding-vectors",
    "href": "posts/c2w4/lab04.html#extracting-word-embedding-vectors",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Extracting word embedding vectors",
    "text": "Extracting word embedding vectors\nOnce you have finished training the neural network, you have three options to get word embedding vectors for the words of your vocabulary, based on the weight matrices \\mathbf{W_1} and/or \\mathbf{W_2}.\n\nOption 1: extract embedding vectors from \\mathbf{W_1}\nThe first option is to take the columns of \\mathbf{W_1} as the embedding vectors of the words of the vocabulary, using the same order of the words as for the input and output vectors.\n\nNote: in this practice notebook the values of the word embedding vectors are meaningless after a single iteration with just one training example, but here‚Äôs how you would proceed after the training process is complete.\n\nFor example \\mathbf{W_1} is this matrix:\n\nW1\n\narray([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n       [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n       [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n\n\nThe first column, which is a 3-element vector, is the embedding vector of the first word of your vocabulary. The second column is the word embedding vector for the second word, and so on.\nThe first, second, etc. words are ordered as follows.\n\nfor i in range(V):\n    print(Ind2word[i])\n\nam\nbecause\nhappy\ni\nlearning\n\n\nSo the word embedding vectors corresponding to each word are:\n\n# loop through each word of the vocabulary\nfor word in word2Ind:\n    # extract the column corresponding to the index of the word in the vocabulary\n    word_embedding_vector = W1[:, word2Ind[word]]\n    \n    print(f'{word}: {word_embedding_vector}')\n\nam: [0.41687358 0.32735501 0.26637602]\nbecause: [ 0.08854191  0.22795148 -0.23846886]\nhappy: [-0.23495225 -0.23951958 -0.37770863]\ni: [ 0.28320538  0.4117634  -0.11399446]\nlearning: [ 0.41800106 -0.23924344  0.34008124]\n\n\n\n\nOption 2: extract embedding vectors from \\mathbf{W_2}\nThe second option is to take \\mathbf{W_2} transposed, and take its columns as the word embedding vectors just like you did for \\mathbf{W_1}.\n\nW2.T\n\narray([[-0.22182064,  0.08476603,  0.1871551 ,  0.07055222,  0.33480474],\n       [-0.43008631,  0.08123194, -0.06107263, -0.02015138, -0.39423389],\n       [ 0.13310965,  0.1772054 , -0.1790735 ,  0.36107434, -0.43959196]])\n\n\n\n# loop through each word of the vocabulary\nfor word in word2Ind:\n    # extract the column corresponding to the index of the word in the vocabulary\n    word_embedding_vector = W2.T[:, word2Ind[word]]\n    \n    print(f'{word}: {word_embedding_vector}')\n\nam: [-0.22182064 -0.43008631  0.13310965]\nbecause: [0.08476603 0.08123194 0.1772054 ]\nhappy: [ 0.1871551  -0.06107263 -0.1790735 ]\ni: [ 0.07055222 -0.02015138  0.36107434]\nlearning: [ 0.33480474 -0.39423389 -0.43959196]\n\n\n\n\nOption 3: extract embedding vectors from \\mathbf{W_1} and \\mathbf{W_2}\nThe third option, which is the one you will use in this week‚Äôs assignment, uses the average of \\mathbf{W_1} and \\mathbf{W_2^\\top}.\nCalculate the average of \\mathbf{W_1} and \\mathbf{W_2^\\top}, and store the result in W3.\n\n# BEGIN your code here\nW3 = (W1+W2.T)/2\n# END your code here\n\nW3\n\narray([[ 0.09752647,  0.08665397, -0.02389858,  0.1768788 ,  0.3764029 ],\n       [-0.05136565,  0.15459171, -0.15029611,  0.19580601, -0.31673866],\n       [ 0.19974284, -0.03063173, -0.27839106,  0.12353994, -0.04975536]])\n\n\nExpected output:\narray([[ 0.09752647,  0.08665397, -0.02389858,  0.1768788 ,  0.3764029 ],\n       [-0.05136565,  0.15459171, -0.15029611,  0.19580601, -0.31673866],\n       [ 0.19974284, -0.03063173, -0.27839106,  0.12353994, -0.04975536]])\nExtracting the word embedding vectors works just like the two previous options, by taking the columns of the matrix you‚Äôve just created.\n\n# loop through each word of the vocabulary\nfor word in word2Ind:\n    # extract the column corresponding to the index of the word in the vocabulary\n    word_embedding_vector = W3[:, word2Ind[word]]\n    \n    print(f'{word}: {word_embedding_vector}')\n\nam: [ 0.09752647 -0.05136565  0.19974284]\nbecause: [ 0.08665397  0.15459171 -0.03063173]\nhappy: [-0.02389858 -0.15029611 -0.27839106]\ni: [0.1768788  0.19580601 0.12353994]\nlearning: [ 0.3764029  -0.31673866 -0.04975536]\n\n\nYou‚Äôre now ready to take on this week‚Äôs assignment!\n\n\nHow this practice relates to and differs from the upcoming graded assignment\n\nIn the assignment, for each iteration of training you will use batches of examples instead of a single example. The formulas for forward propagation and backpropagation will be modified accordingly, and you will use cross-entropy cost instead of cross-entropy loss.\nYou will also complete several iterations of training, until you reach an acceptably low cross-entropy cost, at which point you can extract good word embeddings from the weight matrices.\nAfter extracting the word embedding vectors, you will use principal component analysis (PCA) to visualize the vectors, which will enable you to perform an intrinsic evaluation of the quality of the vectors, as explained in the lecture."
  },
  {
    "objectID": "posts/c2w4/lab01.html",
    "href": "posts/c2w4/lab01.html",
    "title": "Word Embeddings First Steps: Data Preparation",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure¬†1\nIn this series of ungraded notebooks, you‚Äôll try out all the individual techniques that you learned about in the lectures. Practicing on small examples will prepare you for the graded assignment, where you will combine the techniques in more advanced ways to create word embeddings from a real-life corpus.\nThis notebook focuses on data preparation, which is the first step of any machine learning algorithm. It is a very important step because models are only as good as the data they are trained on and the models used require the data to have a particular structure to process it properly.\nTo get started, import and initialize all the libraries you will need.\nimport re\nimport nltk\nimport emoji\nimport numpy as np\nfrom nltk.tokenize import word_tokenize\nfrom utils2 import get_dict",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "L1 - Data Preparation"
    ]
  },
  {
    "objectID": "posts/c2w4/lab01.html#cleaning-and-tokenization",
    "href": "posts/c2w4/lab01.html#cleaning-and-tokenization",
    "title": "Word Embeddings First Steps: Data Preparation",
    "section": "Cleaning and tokenization",
    "text": "Cleaning and tokenization\nTo demonstrate the cleaning and tokenization process, consider a corpus that contains emojis and various punctuation signs.\n\n# Define a corpus\ncorpus = 'Who ‚ù§Ô∏è \"word embeddings\" in 2020? I do!!!'\n\nFirst, replace all interrupting punctuation signs ‚Äî such as commas and exclamation marks ‚Äî with periods.\n\n# Print original corpus\nprint(f'Corpus:  {corpus}')\n\n# Do the substitution\ndata = re.sub(r'[,!?;-]+', '.', corpus)\n\n# Print cleaned corpus\nprint(f'After cleaning punctuation:  {data}')\n\nCorpus:  Who ‚ù§Ô∏è \"word embeddings\" in 2020? I do!!!\nAfter cleaning punctuation:  Who ‚ù§Ô∏è \"word embeddings\" in 2020. I do.\n\n\nNext, use NLTK‚Äôs tokenization engine to split the corpus into individual tokens.\n\n# Print cleaned corpus\nprint(f'Initial string:  {data}')\n\n# Tokenize the cleaned corpus\ndata = nltk.word_tokenize(data)\n\n# Print the tokenized version of the corpus\nprint(f'After tokenization:  {data}')\n\nInitial string:  Who ‚ù§Ô∏è \"word embeddings\" in 2020. I do.\nAfter tokenization:  ['Who', '‚ù§Ô∏è', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '.']\n\n\nFinally, as you saw in the lecture, get rid of numbers and punctuation other than periods, and convert all the remaining tokens to lowercase.\n\n# Print the tokenized version of the corpus\nprint(f'Initial list of tokens:  {data}')\n\n# Filter tokenized corpus using list comprehension\ndata = [ ch.lower() for ch in data\n         if ch.isalpha()\n         or ch == '.'\n         or emoji.get_emoji_regexp().search(ch)\n       ]\n\n# Print the tokenized and filtered version of the corpus\nprint(f'After cleaning:  {data}')\n\nInitial list of tokens:  ['Who', '‚ù§Ô∏è', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '.']\nAfter cleaning:  ['who', '‚ù§Ô∏è', 'word', 'embeddings', 'in', '.', 'i', 'do', '.']\n\n\nNote that the heart emoji is considered as a token just like any normal word.\nNow let‚Äôs streamline the cleaning and tokenization process by wrapping the previous steps in a function.\n\n# Define the 'tokenize' function that will include the steps previously seen\ndef tokenize(corpus):\n    data = re.sub(r'[,!?;-]+', '.', corpus)\n    data = nltk.word_tokenize(data)  # tokenize string to words\n    data = [ ch.lower() for ch in data\n             if ch.isalpha()\n             or ch == '.'\n             or emoji.get_emoji_regexp().search(ch)\n           ]\n    return data\n\nApply this function to the corpus that you‚Äôll be working on in the rest of this notebook: ‚ÄúI am happy because I am learning‚Äù\n\n# Define new corpus\ncorpus = 'I am happy because I¬†am learning'\n\n# Print new corpus\nprint(f'Corpus:  {corpus}')\n\n# Save tokenized version of corpus into 'words' variable\nwords = tokenize(corpus)\n\n# Print the tokenized version of the corpus\nprint(f'Words (tokens):  {words}')\n\nCorpus:  I am happy because I¬†am learning\nWords (tokens):  ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n\n\nNow try it out yourself with your own sentence.\n\n# Run this with any sentence\ntokenize(\"Now it's your turn: try with your own sentence!\")\n\n['now', 'it', 'your', 'turn', 'try', 'with', 'your', 'own', 'sentence', '.']",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "L1 - Data Preparation"
    ]
  },
  {
    "objectID": "posts/c2w4/lab01.html#sliding-window-of-words",
    "href": "posts/c2w4/lab01.html#sliding-window-of-words",
    "title": "Word Embeddings First Steps: Data Preparation",
    "section": "Sliding window of words",
    "text": "Sliding window of words\nNow that you have transformed the corpus into a list of clean tokens, you can slide a window of words across this list. For each window you can extract a center word and the context words.\nThe get_windows function in the next cell was introduced in the lecture.\n\n# Define the 'get_windows' function\ndef get_windows(words, C):\n    i = C\n    while i &lt; len(words) - C:\n        center_word = words[i]\n        context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n        yield context_words, center_word\n        i += 1\n\nThe first argument of this function is a list of words (or tokens). The second argument, C, is the context half-size. Recall that for a given center word, the context words are made of C words to the left and C words to the right of the center word.\nHere is how you can use this function to extract context words and center words from a list of tokens. These context and center words will make up the training set that you will use to train the CBOW model.\n\n# Print 'context_words' and 'center_word' for the new corpus with a 'context half-size' of 2\nfor x, y in get_windows(['i', 'am', 'happy', 'because', 'i', 'am', 'learning'], 2):\n    print(f'{x}\\t{y}')\n\n['i', 'am', 'because', 'i'] happy\n['am', 'happy', 'i', 'am']  because\n['happy', 'because', 'am', 'learning']  i\n\n\nThe first example of the training set is made of:\n\nthe context words ‚Äúi‚Äù, ‚Äúam‚Äù, ‚Äúbecause‚Äù, ‚Äúi‚Äù,\nand the center word to be predicted: ‚Äúhappy‚Äù.\n\nNow try it out yourself. In the next cell, you can change both the sentence and the context half-size.\n\n# Print 'context_words' and 'center_word' for any sentence with a 'context half-size' of 1\nfor x, y in get_windows(tokenize(\"Now it's your turn: try with your own sentence!\"), 1):\n    print(f'{x}\\t{y}')\n\n['now', 'your'] it\n['it', 'turn']  your\n['your', 'try'] turn\n['turn', 'with']    try\n['try', 'your'] with\n['with', 'own'] your\n['your', 'sentence']    own\n['own', '.']    sentence",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "L1 - Data Preparation"
    ]
  },
  {
    "objectID": "posts/c2w4/lab01.html#transforming-words-into-vectors-for-the-training-set",
    "href": "posts/c2w4/lab01.html#transforming-words-into-vectors-for-the-training-set",
    "title": "Word Embeddings First Steps: Data Preparation",
    "section": "Transforming words into vectors for the training set",
    "text": "Transforming words into vectors for the training set\nTo finish preparing the training set, you need to transform the context words and center words into vectors.\n\nMapping words to indices and indices to words\nThe center words will be represented as one-hot vectors, and the vectors that represent context words are also based on one-hot vectors.\nTo create one-hot word vectors, you can start by mapping each unique word to a unique integer (or index). We have provided a helper function, get_dict, that creates a Python dictionary that maps words to integers and back.\n\n# Get 'word2Ind' and 'Ind2word' dictionaries for the tokenized corpus\nword2Ind, Ind2word = get_dict(words)\n\nHere‚Äôs the dictionary that maps words to numeric indices.\n\n# Print 'word2Ind' dictionary\nword2Ind\n\n{'am': 0, 'because': 1, 'happy': 2, 'i': 3, 'learning': 4}\n\n\nYou can use this dictionary to get the index of a word.\n\n# Print value for the key 'i' within word2Ind dictionary\nprint(\"Index of the word 'i':  \",word2Ind['i'])\n\nIndex of the word 'i':   3\n\n\nAnd conversely, here‚Äôs the dictionary that maps indices to words.\n\n# Print 'Ind2word' dictionary\nInd2word\n\n{0: 'am', 1: 'because', 2: 'happy', 3: 'i', 4: 'learning'}\n\n\n\n# Print value for the key '2' within Ind2word dictionary\nprint(\"Word which has index 2:  \",Ind2word[2] )\n\nWord which has index 2:   happy\n\n\nFinally, get the length of either of these dictionaries to get the size of the vocabulary of your corpus, in other words the number of different words making up the corpus.\n\n# Save length of word2Ind dictionary into the 'V' variable\nV = len(word2Ind)\n\n# Print length of word2Ind dictionary\nprint(\"Size of vocabulary: \", V)\n\nSize of vocabulary:  5\n\n\n\n\nGetting one-hot word vectors\nRecall from the lecture that you can easily convert an integer, n, into a one-hot vector.\nConsider the word ‚Äúhappy‚Äù. First, retrieve its numeric index.\n\n# Save index of word 'happy' into the 'n' variable\nn = word2Ind['happy']\n\n# Print index of word 'happy'\nn\n\n2\n\n\nNow create a vector with the size of the vocabulary, and fill it with zeros.\n\n# Create vector with the same length as the vocabulary, filled with zeros\ncenter_word_vector = np.zeros(V)\n\n# Print vector\ncenter_word_vector\n\narray([0., 0., 0., 0., 0.])\n\n\nYou can confirm that the vector has the right size.\n\n# Assert that the length of the vector is the same as the size of the vocabulary\nlen(center_word_vector) == V\n\nTrue\n\n\nNext, replace the 0 of the n-th element with a 1.\n\n# Replace element number 'n' with a 1\ncenter_word_vector[n] = 1\n\nAnd you have your one-hot word vector.\n\n# Print vector\ncenter_word_vector\n\narray([0., 0., 1., 0., 0.])\n\n\nYou can now group all of these steps in a convenient function, which takes as parameters: a word to be encoded, a dictionary that maps words to indices, and the size of the vocabulary.\n\n# Define the 'word_to_one_hot_vector' function that will include the steps previously seen\ndef word_to_one_hot_vector(word, word2Ind, V):\n    one_hot_vector = np.zeros(V)\n    one_hot_vector[word2Ind[word]] = 1\n    return one_hot_vector\n\nCheck that it works as intended.\n\n# Print output of 'word_to_one_hot_vector' function for word 'happy'\nword_to_one_hot_vector('happy', word2Ind, V)\n\narray([0., 0., 1., 0., 0.])\n\n\nWhat is the word vector for ‚Äúlearning‚Äù?\n\n# Print output of 'word_to_one_hot_vector' function for word 'learning'\nword_to_one_hot_vector('learning', word2Ind, V)\n\narray([0., 0., 0., 0., 1.])\n\n\nExpected output:\narray([0., 0., 0., 0., 1.])\n\n\nGetting context word vectors\nTo create the vectors that represent context words, you will calculate the average of the one-hot vectors representing the individual words.\nLet‚Äôs start with a list of context words.\n\n# Define list containing context words\ncontext_words = ['i', 'am', 'because', 'i']\n\nUsing Python‚Äôs list comprehension construct and the word_to_one_hot_vector function that you created in the previous section, you can create a list of one-hot vectors representing each of the context words.\n\n# Create one-hot vectors for each context word using list comprehension\ncontext_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n\n# Print one-hot vectors for each context word\ncontext_words_vectors\n\n[array([0., 0., 0., 1., 0.]),\n array([1., 0., 0., 0., 0.]),\n array([0., 1., 0., 0., 0.]),\n array([0., 0., 0., 1., 0.])]\n\n\nAnd you can now simply get the average of these vectors using numpy‚Äôs mean function, to get the vector representation of the context words.\n\n# Compute mean of the vectors using numpy\nnp.mean(context_words_vectors, axis=0)\n\narray([0.25, 0.25, 0.  , 0.5 , 0.  ])\n\n\nNote the axis=0 parameter that tells mean to calculate the average of the rows (if you had wanted the average of the columns, you would have used axis=1).\nNow create the context_words_to_vector function that takes in a list of context words, a word-to-index dictionary, and a vocabulary size, and outputs the vector representation of the context words.\n\n# Define the 'context_words_to_vector' function that will include the steps previously seen\ndef context_words_to_vector(context_words, word2Ind, V):\n    context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n    context_words_vectors = np.mean(context_words_vectors, axis=0)\n    return context_words_vectors\n\nAnd check that you obtain the same output as the manual approach above.\n\n# Print output of 'context_words_to_vector' function for context words: 'i', 'am', 'because', 'i'\ncontext_words_to_vector(['i', 'am', 'because', 'i'], word2Ind, V)\n\narray([0.25, 0.25, 0.  , 0.5 , 0.  ])\n\n\nWhat is the vector representation of the context words ‚Äúam happy i am‚Äù?\n\n# Print output of 'context_words_to_vector' function for context words: 'am', 'happy', 'i', 'am'\ncontext_words_to_vector(['am', 'happy', 'i', 'am'], word2Ind, V)\n\narray([0.5 , 0.  , 0.25, 0.25, 0.  ])\n\n\nExpected output:\narray([0.5 , 0.  , 0.25, 0.25, 0.  ])",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "L1 - Data Preparation"
    ]
  },
  {
    "objectID": "posts/c2w4/lab01.html#building-the-training-set",
    "href": "posts/c2w4/lab01.html#building-the-training-set",
    "title": "Word Embeddings First Steps: Data Preparation",
    "section": "Building the training set",
    "text": "Building the training set\nYou can now combine the functions that you created in the previous sections, to build a training set for the CBOW model, starting from the following tokenized corpus.\n\n# Print corpus\nwords\n\n['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n\n\nTo do this you need to use the sliding window function (get_windows) to extract the context words and center words, and you then convert these sets of words into a basic vector representation using word_to_one_hot_vector and context_words_to_vector.\n\n# Print vectors associated to center and context words for corpus\nfor context_words, center_word in get_windows(words, 2):  # reminder: 2 is the context half-size\n    print(f'Context words:  {context_words} -&gt; {context_words_to_vector(context_words, word2Ind, V)}')\n    print(f'Center word:  {center_word} -&gt; {word_to_one_hot_vector(center_word, word2Ind, V)}')\n    print()\n\nContext words:  ['i', 'am', 'because', 'i'] -&gt; [0.25 0.25 0.   0.5  0.  ]\nCenter word:  happy -&gt; [0. 0. 1. 0. 0.]\n\nContext words:  ['am', 'happy', 'i', 'am'] -&gt; [0.5  0.   0.25 0.25 0.  ]\nCenter word:  because -&gt; [0. 1. 0. 0. 0.]\n\nContext words:  ['happy', 'because', 'am', 'learning'] -&gt; [0.25 0.25 0.25 0.   0.25]\nCenter word:  i -&gt; [0. 0. 0. 1. 0.]\n\n\n\nIn this practice notebook you‚Äôll be performing a single iteration of training using a single example, but in this week‚Äôs assignment you‚Äôll train the CBOW model using several iterations and batches of example. Here is how you would use a Python generator function (remember the yield keyword from the lecture?) to make it easier to iterate over a set of examples.\n\n# Define the generator function 'get_training_example'\ndef get_training_example(words, C, word2Ind, V):\n    for context_words, center_word in get_windows(words, C):\n        yield context_words_to_vector(context_words, word2Ind, V), word_to_one_hot_vector(center_word, word2Ind, V)\n\nThe output of this function can be iterated on to get successive context word vectors and center word vectors, as demonstrated in the next cell.\n\n# Print vectors associated to center and context words for corpus using the generator function\nfor context_words_vector, center_word_vector in get_training_example(words, 2, word2Ind, V):\n    print(f'Context words vector:  {context_words_vector}')\n    print(f'Center word vector:  {center_word_vector}')\n    print()\n\nContext words vector:  [0.25 0.25 0.   0.5  0.  ]\nCenter word vector:  [0. 0. 1. 0. 0.]\n\nContext words vector:  [0.5  0.   0.25 0.25 0.  ]\nCenter word vector:  [0. 1. 0. 0. 0.]\n\nContext words vector:  [0.25 0.25 0.25 0.   0.25]\nCenter word vector:  [0. 0. 0. 1. 0.]\n\n\n\nYour training set is ready, you can now move on to the CBOW model itself which will be covered in the next lecture notebook.\nCongratulations on finishing this lecture notebook! Hopefully you now have a better understanding of how to prepare your data before feeding it to a continuous bag-of-words model.\nKeep it up!",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "L1 - Data Preparation"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html",
    "href": "posts/c2w4/index.html",
    "title": "Word embeddings with neural networks",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure¬†1\n\n\n\n\n\n\n\n\nFigure¬†2\nThese are my notes for Week 4 notes for the Natural Language Processing with Probabilistic Models Course in the Natural Language Processing Specialization Offered by DeepLearning.AI on Coursera.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#overview",
    "href": "posts/c2w4/index.html#overview",
    "title": "Word embeddings with neural networks",
    "section": "Overview",
    "text": "Overview\nWord embeddings are used in most NLP applications. Whenever we are dealing with text, we first have to find a way to encode the words as numbers. Word embedding are a very common technique that allows we to do so. Here are a few applications of word embeddings that we should be able to implement by the time we complete the specialization.\n\n\n\n\n\n\n\napplications\n\n\n\n\nFigure¬†3: Basic Applications of word embeddings\n\n\n\n\n\n\n\n\napplications\n\n\n\n\nFigure¬†4: Advanced applications of word embeddings\n\n\n\nBy the end of this week we will be able to:\n\nIdentify the key concepts of word representations\nGenerate word embeddings\nPrepare text for machine learning\nImplement the continuous bag-of-words model",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#basic-word-representations",
    "href": "posts/c2w4/index.html#basic-word-representations",
    "title": "Word embeddings with neural networks",
    "section": "Basic Word Representations",
    "text": "Basic Word Representations\n\n\n\n\n\n\n\nFigure¬†5: One-hot vectors\n\n\n\n\n\n\n\n\nFigure¬†6: One-hot vectors\n\n\n\nBasic word representations could be classified into the following:\n\nIntegers\nOne-hot vectors\nWord embeddings\n\nTo the left, we have an example where we use integers to represent a word. The issue there is that there is no reason why one word corresponds to a bigger number than another. To fix this problem we introduce one hot vectors (diagram on the right). To implement one hot vectors, we have to initialize a vector of zeros of dimension V and then put a 1 in the index corresponding to the word we are representing.\nThe pros of one-hot vectors: simple and require no implied ordering. The cons of one-hot vectors: huge and encode no meaning.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#word-embeddings",
    "href": "posts/c2w4/index.html#word-embeddings",
    "title": "Word embeddings with neural networks",
    "section": "Word Embeddings",
    "text": "Word Embeddings\n\n\n\n\n\n\n\nFigure¬†7: Meaning as vectors in 1D\n\n\n\n\n\n\n\n\nFigure¬†8: Meaning as vectors in 2D\n\n\n\nFrom the plot above, we can see that when encoding a word in 2D, similar words tend to be found next to each other. Perhaps the first coordinate represents whether a word is positive or negative. The second coordinate tell we whether the word is abstract or concrete. This is just an example, in the real world we will find embeddings with hundreds of dimensions. We can think of each coordinate as a number telling we something about the word.\nThe pros:\n\nLow dimensions (less than V)\nAllow we to encode meaning",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#how-to-create-word-embeddings",
    "href": "posts/c2w4/index.html#how-to-create-word-embeddings",
    "title": "Word embeddings with neural networks",
    "section": "How to Create Word Embeddings?",
    "text": "How to Create Word Embeddings?\n\n\n\n\n\n\n\nFigure¬†9: Meaning as vectors in 2D\n\n\nTo create word embeddings we always need a corpus of text, and an embedding method.\nThe context of a word tells we what type of words tend to occur near that specific word. The context is important as this is what will give meaning to each word embedding.\nEmbeddings There are many types of possible methods that allow we to learn the word embeddings. The machine learning model performs a learning task, and the main by-products of this task are the word embeddings. The task could be to learn to predict a word based on the surrounding words in a sentence of the corpus, as in the case of the continuous bag-of-words.\nThe task is self-supervised: it is both unsupervised in the sense that the input data ‚Äî the corpus ‚Äî is unlabelled, and supervised in the sense that the data itself provides the necessary context which would ordinarily make up the labels.\nWhen training word vectors, there are some parameters we need to tune. (i.e.¬†the dimension of the word vector)",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#word-embedding-methods",
    "href": "posts/c2w4/index.html#word-embedding-methods",
    "title": "Word embeddings with neural networks",
    "section": "Word Embedding Methods",
    "text": "Word Embedding Methods\nClassical Methods\n\nword2vec (Google, 2013)\nContinuous bag-of-words (CBOW): the model learns to predict the center word given some context words.\nContinuous skip-gram / Skip-gram with negative sampling (SGNS): the model learns to predict the words surrounding a given input word.\nGlobal Vectors (GloVe) (Stanford, 2014): factorizes the logarithm of the corpus‚Äôs word co-occurrence matrix, similar to the count matrix you‚Äôve used before.\nfastText (Facebook, 2016): based on the skip-gram model and takes into account the structure of words by representing words as an n-gram of characters. It supports out-of-vocabulary (OOV) words.\n\nDeep learning, contextual embeddings\nIn these more advanced models, words have different embeddings depending on their context. We can download pre-trained embeddings for the following models.\n\nBERT (Google, 2018):\nELMo (Allen Institute for AI, 2018)\nGPT-2 (OpenAI, 2018)",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#continuous-bag-of-words-model",
    "href": "posts/c2w4/index.html#continuous-bag-of-words-model",
    "title": "Word embeddings with neural networks",
    "section": "Continuous Bag-of-Words Model",
    "text": "Continuous Bag-of-Words Model\n\n\n\n\n\n\n\nFigure¬†10: Meaning as vectors in 2D\n\n\n\n\n\n\n\n\nFigure¬†11: Meaning as vectors in 2D\n\n\n\n\n\n\n\n\nFigure¬†12: Meaning as vectors in 2D\n\n\n\n\nContinuous Bag of Words Model To create word embeddings, we need a corpus and a learning algorithm. The by-product of this task would be a set of word embeddings. In the case of the continuous bag-of-words model, the objective of the task is to predict a missing word based on the surrounding words.\nHere is a visualization that shows we how the models works.\nAs we can see, the window size in the image above is 5. The context size, C, is 2. C usually tells we how many words before or after the center word the model will use to make the prediction. Here is another visualization that shows an overview of the model.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#cleaning-and-tokenization",
    "href": "posts/c2w4/index.html#cleaning-and-tokenization",
    "title": "Word embeddings with neural networks",
    "section": "Cleaning and Tokenization",
    "text": "Cleaning and Tokenization\n\n\n\n\n\n\n\nFigure¬†13: Meaning as vectors in 2D\n\n\n\n\n\n\n\n\nFigure¬†14: Meaning as vectors in 2D\n\n\n\nBefore implementing any natural language processing algorithm, we might want to clean the data and tokenize it. Here are a few things to keep track of when handling your data.\nWe can clean data using python as follows:\nWe can add as many conditions as we want in the lines corresponding to the green rectangle above.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#sliding-window-of-words-in-python",
    "href": "posts/c2w4/index.html#sliding-window-of-words-in-python",
    "title": "Word embeddings with neural networks",
    "section": "Sliding Window of words in Python",
    "text": "Sliding Window of words in Python\n\n\n\n\n\n\n\nFigure¬†15: Sliding Window of words in Python\n\n\nThe code above shows we a function which takes in two parameters.\nWords: a list of words.\nC: the context size.\nWe first start by setting i to C. Then we single out the center_word, and the context_words. We then yield those and increment i.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#transforming-words-into-vectors",
    "href": "posts/c2w4/index.html#transforming-words-into-vectors",
    "title": "Word embeddings with neural networks",
    "section": "Transforming Words into Vectors",
    "text": "Transforming Words into Vectors\n\n\n\n\n\n\n\nFigure¬†16: Transforming Words into Vectors\n\n\nAs we can see, we started with one-hot vectors for the context words and and we transform them into a single vector by taking an average. As a result we end up having the following vectors that we can use for your training.\n\n\n\n\n\n\n\nFigure¬†17: Sliding Window of words in Python",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#lecture-notebook---data-preparation",
    "href": "posts/c2w4/index.html#lecture-notebook---data-preparation",
    "title": "Word embeddings with neural networks",
    "section": "Lecture Notebook - Data Preparation",
    "text": "Lecture Notebook - Data Preparation",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#architecture-of-the-cbow-model",
    "href": "posts/c2w4/index.html#architecture-of-the-cbow-model",
    "title": "Word embeddings with neural networks",
    "section": "Architecture of the CBOW Model",
    "text": "Architecture of the CBOW Model\nThe architecture for the CBOW model could be described as follows\n\n\n\n\n\n\n\nFigure¬†18: Architecture for the CBOW Model\n\n\nWe have an input, X, which is the average of all context vectors. We then multiply it by W_1 and add b1. The result goes through a ReLU function to give we your hidden layer. That layer is then multiplied by W_2 and we add b_2. The result goes through a softmax which gives we a distribution over V, vocabulary words. We pick the vocabulary word that corresponds to the arg-max of the output.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#architecture-of-the-cbow-model-dimensions",
    "href": "posts/c2w4/index.html#architecture-of-the-cbow-model-dimensions",
    "title": "Word embeddings with neural networks",
    "section": "Architecture of the CBOW Model: Dimensions",
    "text": "Architecture of the CBOW Model: Dimensions\nThe equations for the previous model are:\n\nz_1 = W_1 x + b_1\n\n\nh = ReLU(z_1)\n\n\nz_2 = W_2 h + b_2\n\n\n\\hat{y} = softmax(z_2)\n\nHere, we can see the dimensions:\n\n\n\n\n\n\n\nFigure¬†19: Architecture for the CBOW Model\n\n\nMake sure we go through the matrix multiplications and understand why the dimensions make sense.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#architecture-of-the-cbow-model-dimensions-2",
    "href": "posts/c2w4/index.html#architecture-of-the-cbow-model-dimensions-2",
    "title": "Word embeddings with neural networks",
    "section": "Architecture of the CBOW Model: Dimensions 2",
    "text": "Architecture of the CBOW Model: Dimensions 2\nWhen dealing with batch input, we can stack the examples as columns. We can then proceed to multiply the matrices as follows:\n\n\n\n\n\n\n\nFigure¬†20: Dimensions Batch Input\n\n\nIn the diagram above, we can see the dimensions of each matrix. Note that your \\hat{Y} is of dimension V by m. Each column is the prediction of the column corresponding to the context words. So the first column in \\hat{Y} is the prediction corresponding to the first column of X.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#architecture-of-the-cbow-model-activation-functions",
    "href": "posts/c2w4/index.html#architecture-of-the-cbow-model-activation-functions",
    "title": "Word embeddings with neural networks",
    "section": "Architecture of the CBOW Model: Activation Functions",
    "text": "Architecture of the CBOW Model: Activation Functions\n\nReLU funciton\nThe rectified linear unit (ReLU), is one of the most popular activation functions. When we feed a vector, namely x, into a ReLU function. We end up taking x=max(0,x). This is a drawing that shows ReLU.\n\n\n\n\n\n\n\nFigure¬†21: Dimensions Batch Input\n\n\n\n\nSoftmax function\nThe softmax function takes a vector and transforms it into a probability distribution. For example, given the following vector z, we can transform it into a probability distribution as follows.\n\n\n\n\n\n\n\nFigure¬†22: Dimensions Batch Input\n\n\nAs we can see, we can compute:\n\n\\hat{y} = \\frac{e^{z_i}}{\\sum_{j=1}^V e^{z_j}}",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#lecture-notebook---intro-to-cbow-model",
    "href": "posts/c2w4/index.html#lecture-notebook---intro-to-cbow-model",
    "title": "Word embeddings with neural networks",
    "section": "Lecture Notebook - Intro to CBOW model",
    "text": "Lecture Notebook - Intro to CBOW model\nlab 2 the CBOW model",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#training-a-cbow-model-cost-function",
    "href": "posts/c2w4/index.html#training-a-cbow-model-cost-function",
    "title": "Word embeddings with neural networks",
    "section": "Training a CBOW Model: Cost Function",
    "text": "Training a CBOW Model: Cost Function\nThe cost function for the CBOW model is a cross-entropy loss defined as:\n\nJ = -\\sum_{k=1}^V y_k log(\\hat{y}_k)\n\\tag{1}\nHere is an example where we use the equation above.\n\n\n\n\n\n\n\nFigure¬†23: Dimensions Batch Input\n\n\nWhy is the cost 4.61 in the example above?",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#training-a-cbow-model-forward-propagation",
    "href": "posts/c2w4/index.html#training-a-cbow-model-forward-propagation",
    "title": "Word embeddings with neural networks",
    "section": "Training a CBOW Model: Forward Propagation",
    "text": "Training a CBOW Model: Forward Propagation\nTraining a CBOW Model: Forward Propagation Forward propagation is defined as:\n\nZ_1 = W_1 X + B_1\n\n\nH = ReLU(Z_1)\n\n\nZ_2 = W_2 H + B_2\n\n\n\\hat{Y} = softmax(Z_2)\n\nIn the image below we start from the left and we forward propagate all the way to the right.\n\n\n\n\n\n\n\nFigure¬†24: Dimensions Batch Input\n\n\nTo calculate the loss of a batch, we have to compute the following:\n\nJ_{batch} = -\\frac{1}{m} \\sum_{i=1}^m \\sum_{j=1}^V y_j(i) log(\\hat{y}^j(i))\n\nGiven, your predicted center word matrix, and actual center word matrix, we can compute the loss.\n\n\n\n\n\n\n\nFigure¬†25: Dimensions Batch Input",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#training-a-cbow-model-backpropagation-and-gradient-descent",
    "href": "posts/c2w4/index.html#training-a-cbow-model-backpropagation-and-gradient-descent",
    "title": "Word embeddings with neural networks",
    "section": "Training a CBOW Model: Backpropagation and Gradient Descent",
    "text": "Training a CBOW Model: Backpropagation and Gradient Descent\n\n\n\n\n\n\n\nFigure¬†26: Dimensions Batch Input\n\n\n\n\n\n\n\n\nFigure¬†27: Dimensions Batch Input\n\n\n\nTraining a CBOW Model: Backpropagation and Gradient Descent Backpropagation: calculate partial derivatives of cost with respect to weights and biases.\nWhen computing the back-prop in this model, we need to compute the following:\n\n\\frac{\\partial J_{batch}}{\\partial W_1}, \\frac{\\partial J_{batch}}{\\partial W_2}, \\frac{\\partial J_{batch}}{\\partial B_1}, \\frac{\\partial J_{batch}}{\\partial B_2}\n\nGradient descent: update weights and biases\nNow to update the weights we can iterate as follows:\n\nW_1 := W_1 - \\alpha \\frac{\\partial J_{batch}}{\\partial W_1}\n\n\nW_2 := W_2 - \\alpha \\frac{\\partial J_{batch}}{\\partial W_2}\n\n\nB_1 := B_1 - \\alpha \\frac{\\partial J_{batch}}{\\partial B_1}\n\n\nB_2 := B_2 - \\alpha \\frac{\\partial J_{batch}}{\\partial B_2}\n\nA smaller alpha allows for more gradual updates to the weights and biases, whereas a larger number allows for a faster update of the weights. If Œ± is too large, we might not learn anything, if it is too small, your model will take forever to train.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#lecture-notebook---training-the-cbow-model",
    "href": "posts/c2w4/index.html#lecture-notebook---training-the-cbow-model",
    "title": "Word embeddings with neural networks",
    "section": "Lecture Notebook - Training the CBOW model",
    "text": "Lecture Notebook - Training the CBOW model",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#extracting-word-embedding-vectors",
    "href": "posts/c2w4/index.html#extracting-word-embedding-vectors",
    "title": "Word embeddings with neural networks",
    "section": "Extracting Word Embedding Vectors",
    "text": "Extracting Word Embedding Vectors\nThere are two options to extract word embeddings after training the continuous bag of words model. We can use W_1 as follows:\n\n\n\n\n\n\n\nFigure¬†28: Dimensions Batch Input\n\n\nIf we were to use W_1, each column will correspond to the embeddings of a specific word. We can also use W_2 as follows:\n\n\n\n\n\n\n\nFigure¬†29: Dimensions Batch Input\n\n\nThe final option is to take an average of both matrices as follows:\n\n\n\n\n\n\n\nFigure¬†30: Dimensions Batch Input",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#lecture-notebook---word-embeddings",
    "href": "posts/c2w4/index.html#lecture-notebook---word-embeddings",
    "title": "Word embeddings with neural networks",
    "section": "Lecture Notebook - Word Embeddings",
    "text": "Lecture Notebook - Word Embeddings\nlab 4 - Word Embeddings",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#lecture-notebook-word-embeddings-step-by-step",
    "href": "posts/c2w4/index.html#lecture-notebook-word-embeddings-step-by-step",
    "title": "Word embeddings with neural networks",
    "section": "Lecture notebook: Word embeddings step by step",
    "text": "Lecture notebook: Word embeddings step by step\nLab 5 - Word embeddings step by step",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#evaluating-word-embeddings-intrinsic-evaluation",
    "href": "posts/c2w4/index.html#evaluating-word-embeddings-intrinsic-evaluation",
    "title": "Word embeddings with neural networks",
    "section": "Evaluating Word Embeddings: Intrinsic Evaluation",
    "text": "Evaluating Word Embeddings: Intrinsic Evaluation\nIntrinsic evaluation allows we to test relationships between words. It allows we to capture semantic analogies as, ‚ÄúFrance‚Äù is to ‚ÄúParis‚Äù as ‚ÄúItaly‚Äù is to &lt;?&gt; and also syntactic analogies as ‚Äúseen‚Äù is to ‚Äúsaw‚Äù as ‚Äúbeen‚Äù is to &lt;?&gt;.\nAmbiguous cases could be much harder to track:\n\n\n\n\n\n\n\nFigure¬†31: Dimensions Batch Input\n\n\nHere are a few ways that allow to use intrinsic evaluation.\n\n\n\n\n\n\n\nFigure¬†32: Dimensions Batch Input\n\n\n\n\n\n\n\n\nFigure¬†33: Dimensions Batch Input",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#evaluating-word-embeddings-extrinsic-evaluation",
    "href": "posts/c2w4/index.html#evaluating-word-embeddings-extrinsic-evaluation",
    "title": "Word embeddings with neural networks",
    "section": "Evaluating Word Embeddings: Extrinsic Evaluation",
    "text": "Evaluating Word Embeddings: Extrinsic Evaluation\nExtrinsic evaluation tests word embeddings on external tasks like named entity recognition, parts-of-speech tagging, etc.\n\nEvaluates actual usefulness of embeddings\nTime Consuming\nMore difficult to trouble shoot\n\nSo now we know both intrinsic and extrinsic evaluation.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c2w4/index.html#conclusion",
    "href": "posts/c2w4/index.html#conclusion",
    "title": "Word embeddings with neural networks",
    "section": "Conclusion",
    "text": "Conclusion\nThis week we learned the following concepts.\n\nData preparation\nWord representations\nContinuous bag-of-words model\nEvaluation\n\nWe have all the foundations now. From now on, we will start using some advanced AI libraries in the next courses. Congratulations and good luck with the assignment",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks"
    ]
  },
  {
    "objectID": "posts/c1w2/assignment.html",
    "href": "posts/c1w2/assignment.html",
    "title": "Assignment 2: Naive Bayes",
    "section": "",
    "text": "Figure¬†1: course banner\nWelcome to week two of this specialization. You will learn about Naive Bayes. Concretely, you will be using Naive Bayes for sentiment analysis on tweets. Given a tweet, you will decide if it has a positive sentiment or a negative one. Specifically you will:\nYou may already be familiar with Naive Bayes and its justification in terms of conditional probabilities and independence. * In this week‚Äôs lectures and assignments we used the ratio of probabilities between positive and negative sentiments. * This approach gives us simpler formulas for these 2-way classification tasks.\nLoad the cell below to import some packages. You may want to browse the documentation of unfamiliar libraries and functions.\nfrom utils import process_tweet, lookup\nimport pdb\nfrom nltk.corpus import stopwords, twitter_samples\nimport numpy as np\nimport pandas as pd\nimport nltk\nimport string\nfrom nltk.tokenize import TweetTokenizer\nfrom os import getcwd\nIf you are running this notebook in your local computer, don‚Äôt forget to download the twitter samples and stopwords from nltk.\n# add folder, tmp2, from our local workspace containing pre-downloaded corpora files to nltk's data path\nfilePath = f\"{getcwd()}/../tmp2/\"\nnltk.data.path.append(filePath)\n# get the sets of positive and negative tweets\nall_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')\n\n# split the data into two pieces, one for training and one for testing (validation set)\ntest_pos = all_positive_tweets[4000:]\ntrain_pos = all_positive_tweets[:4000]\ntest_neg = all_negative_tweets[4000:]\ntrain_neg = all_negative_tweets[:4000]\n\ntrain_x = train_pos + train_neg\ntest_x = test_pos + test_neg\n\n# avoid assumptions about the length of all_positive_tweets\ntrain_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\ntest_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "A2 Logistic Regression"
    ]
  },
  {
    "objectID": "posts/c1w2/assignment.html#part-1.1-implementing-your-helper-functions",
    "href": "posts/c1w2/assignment.html#part-1.1-implementing-your-helper-functions",
    "title": "Assignment 2: Naive Bayes",
    "section": "Part 1.1 Implementing your helper functions",
    "text": "Part 1.1 Implementing your helper functions\nTo help train your naive bayes model, you will need to build a dictionary where the keys are a (word, label) tuple and the values are the corresponding frequency. Note that the labels we‚Äôll use here are 1 for positive and 0 for negative.\nYou will also implement a lookup() helper function that takes in the freqs dictionary, a word, and a label (1 or 0) and returns the number of times that word and label tuple appears in the collection of tweets.\nFor example: given a list of tweets [\"i am rather excited\", \"you are rather happy\"] and the label 1, the function will return a dictionary that contains the following key-value pairs:\n{ (‚Äúrather‚Äù, 1): 2 (‚Äúhappi‚Äù, 1) : 1 (‚Äúexcit‚Äù, 1) : 1 }\n\nNotice how for each word in the given string, the same label 1 is assigned to each word.\nNotice how the words ‚Äúi‚Äù and ‚Äúam‚Äù are not saved, since it was removed by process_tweet because it is a stopword.\nNotice how the word ‚Äúrather‚Äù appears twice in the list of tweets, and so its count value is 2.\n\n\nInstructions\nCreate a function count_tweets() that takes a list of tweets as input, cleans all of them, and returns a dictionary. - The key in the dictionary is a tuple containing the stemmed word and its class label, e.g.¬†(‚Äúhappi‚Äù,1). - The value the number of times this word appears in the given collection of tweets (an integer).\n\n\nHints\n\n\n\n\nPlease use the process_tweet function that was imported above, and then store the words in their respective dictionaries and sets.\n\n\nYou may find it useful to use the zip function to match each element in tweets with each element in ys.\n\n\nRemember to check if the key in the dictionary exists before adding that key to the dictionary, or incrementing its value.\n\n\nAssume that the result dictionary that is input will contain clean key-value pairs (you can assume that the values will be integers that can be incremented). It is good practice to check the datatype before incrementing the value, but it‚Äôs not required here.\n\n\n\n\n# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef count_tweets(result, tweets, ys):\n    '''\n    Input:\n        result: a dictionary that will be used to map each pair to its frequency\n        tweets: a list of tweets\n        ys: a list corresponding to the sentiment of each tweet (either 0 or 1)\n    Output:\n        result: a dictionary mapping each pair to its frequency\n    '''\n\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    for y, tweet in zip(ys, tweets):\n        for word in process_tweet(tweet):\n            # define the key, which is the word and label tuple\n            pair = None\n\n            # if the key exists in the dictionary, increment the count\n            if pair in result:\n                result[pair] += None\n\n            # else, if the key is new, add it to the dictionary and set the count to 1\n            else:\n                result[pair] = None\n    ### END CODE HERE ###\n\n    return result\n\n\n# Testing your function\n\n\nresult = {}\ntweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\nys = [1, 0, 0, 0, 0]\ncount_tweets(result, tweets, ys)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[6], line 7\n      5 tweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n      6 ys = [1, 0, 0, 0, 0]\n----&gt; 7 count_tweets(result, tweets, ys)\n\nCell In[5], line 20, in count_tweets(result, tweets, ys)\n     18 # if the key exists in the dictionary, increment the count\n     19 if pair in result:\n---&gt; 20     result[pair] += None\n     22 # else, if the key is new, add it to the dictionary and set the count to 1\n     23 else:\n     24     result[pair] = None\n\nTypeError: unsupported operand type(s) for +=: 'NoneType' and 'NoneType'\n\n\n\nExpected Output: {(‚Äòhappi‚Äô, 1): 1, (‚Äòtrick‚Äô, 0): 1, (‚Äòsad‚Äô, 0): 1, (‚Äòtire‚Äô, 0): 2}",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "A2 Logistic Regression"
    ]
  },
  {
    "objectID": "posts/c1w2/code.html",
    "href": "posts/c1w2/code.html",
    "title": "Probability and Bayes Rule",
    "section": "",
    "text": "Figure¬†1: course banner\n\n\n\nimport pandas as pd\nimport string \nraw_tweets=[\n  \"I am happy because I am learning NLP\",\n  \"I am sad, I am not learning NLP\",\n  \"I am happy, not sad\",\n  \"I am sad, not happy\",\n]\ndef clean(tweet:str):\n  return  tweet.translate(str.maketrans('', '', string.punctuation)).lower()\ntweets = [clean(tweet) for tweet in raw_tweets]\nlabels=['+','-','+','-']\ndf = pd.DataFrame({'tweets': tweets, 'labels': labels})\ndf\n\n\n\n\n\n\n\n\ntweets\nlabels\n\n\n\n\n0\ni am happy because i am learning nlp\n+\n\n\n1\ni am sad i am not learning nlp\n-\n\n\n2\ni am happy not sad\n+\n\n\n3\ni am sad not happy\n-\n\n\n\n\n\n\n\n\nimport numpy as np\nfrom collections import Counter\np_freq,n_freq = Counter(), Counter()\n#print( df[df.labels == '+']['tweets'].to_list())\n[p_freq.update(tweet.split()) for tweet in df[df.labels == '+']['tweets'].to_list()]\n[n_freq.update(tweet.split()) for tweet in df[df.labels == '-']['tweets'].to_list()]\nprint(p_freq)\nprint(n_freq)\nvocab = list(set(p_freq.keys()).union(set(n_freq.keys())))\npos_freq = [p_freq[word] for word in vocab ]\nneg_freq = [n_freq[word] for word in vocab ]\nvocab_df=pd.DataFrame({'vocab':vocab,'pos_freq':pos_freq,'neg_freq':neg_freq})\nvocab_df['p_pos']=vocab_df.pos_freq/vocab_df.pos_freq.sum()\nvocab_df['p_neg']=vocab_df.neg_freq/vocab_df.neg_freq.sum()\nvocab_df['p_pos_sm']=(vocab_df.pos_freq+1)/(vocab_df.pos_freq.sum()+vocab_df.shape[1])\nvocab_df['p_neg_sm']=(vocab_df.neg_freq+1)/(vocab_df.neg_freq.sum()+vocab_df.shape[1])\nvocab_df['ratio']= vocab_df.p_pos_sm/vocab_df.p_neg_sm\nvocab_df['lambda']= np.log(vocab_df.p_pos_sm/vocab_df.p_neg_sm)\npd.set_option('display.float_format', '{:.2f}'.format)\nvocab_df\nprint(vocab_df.shape)\n\n[None, None]\n\n\n[None, None]\n\n\nCounter({'i': 3, 'am': 3, 'happy': 2, 'because': 1, 'learning': 1, 'nlp': 1, 'not': 1, 'sad': 1})\nCounter({'i': 3, 'am': 3, 'sad': 2, 'not': 2, 'learning': 1, 'nlp': 1, 'happy': 1})\n\n\n\n\n\n\n\n\n\nvocab\npos_freq\nneg_freq\np_pos\np_neg\np_pos_sm\np_neg_sm\nratio\nlambda\n\n\n\n\n0\nnot\n1\n2\n0.08\n0.15\n0.11\n0.16\n0.70\n-0.35\n\n\n1\ni\n3\n3\n0.23\n0.23\n0.22\n0.21\n1.06\n0.05\n\n\n2\nam\n3\n3\n0.23\n0.23\n0.22\n0.21\n1.06\n0.05\n\n\n3\nhappy\n2\n1\n0.15\n0.08\n0.17\n0.11\n1.58\n0.46\n\n\n4\nlearning\n1\n1\n0.08\n0.08\n0.11\n0.11\n1.06\n0.05\n\n\n5\nbecause\n1\n0\n0.08\n0.00\n0.11\n0.05\n2.11\n0.75\n\n\n6\nsad\n1\n2\n0.08\n0.15\n0.11\n0.16\n0.70\n-0.35\n\n\n7\nnlp\n1\n1\n0.08\n0.08\n0.11\n0.11\n1.06\n0.05\n\n\n\n\n\n\n\n(8, 9)\n\n\n\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\ntable = [[\"Sun\",696000,1989100000],\n         [\"Earth\",6371,5973.6],\n         [\"Moon\",1737,73.5],\n         [\"Mars\",3390,641.85]]\nMarkdown(tabulate(\n  table, \n  headers=[\"Planet\",\"R (km)\", \"mass (x 10^29 kg)\"]\n))\n\n\n\nTable¬†1: Planets\n\n\n\n\n\n\nPlanet\nR (km)\nmass (x 10^29 kg)\n\n\n\n\nSun\n696000\n1.9891e+09\n\n\nEarth\n6371\n5973.6\n\n\nMoon\n1737\n73.5\n\n\nMars\n3390\n641.85\n\n\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{bochman2020,\n  author = {Bochman, Oren},\n  title = {Probability and {Bayes} {Rule}},\n  date = {2020-10-23},\n  url = {https://orenbochman.github.io/notes-nlp/posts/c1w2/code.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2020. ‚ÄúProbability and Bayes Rule.‚Äù October\n23, 2020. https://orenbochman.github.io/notes-nlp/posts/c1w2/code.html."
  },
  {
    "objectID": "posts/pytorch/c1w1.html",
    "href": "posts/pytorch/c1w1.html",
    "title": "Course Overview",
    "section": "",
    "text": "Establish the course framework by understanding the key topics and learning objectives.\nSet up the development environment to ensure all necessary tools and dependencies are installed.\nNavigate the course effectively, accessing and utilizing all provided materials."
  },
  {
    "objectID": "posts/pytorch/c1w1.html#learning-objectives",
    "href": "posts/pytorch/c1w1.html#learning-objectives",
    "title": "Course Overview",
    "section": "",
    "text": "Establish the course framework by understanding the key topics and learning objectives.\nSet up the development environment to ensure all necessary tools and dependencies are installed.\nNavigate the course effectively, accessing and utilizing all provided materials."
  },
  {
    "objectID": "posts/pytorch/c1w1.html#introduction-to-the-specialization",
    "href": "posts/pytorch/c1w1.html#introduction-to-the-specialization",
    "title": "Course Overview",
    "section": "Introduction to the Specialization",
    "text": "Introduction to the Specialization\nWelcome to the course Foundations and Core Concepts of PyTorch. The first of the 3 course Specialization in PyTorch Ultimate 2024 - From Basics to Cutting-Edge\nThe course is structured into seven comprehensive modules:\nCourse Overview and System Setup: In this module, we will introduce we to the course structure, covering the main topics and learning objectives. You‚Äôll learn how to set up your system, including installing necessary software and creating a conda environment. We‚Äôll also guide we on accessing course materials and provide tips for navigating the course efficiently.\nMachine Learning: In this module, we will delve into the basics of machine learning. We will start with an introduction to artificial intelligence and its core concepts. The module will then explore the essentials of machine learning and provide an overview of different machine learning models, laying the groundwork for more advanced topics.\nDeep Learning Introduction: In this module, we will explore the foundational concepts of deep learning. We will gain insights into deep learning models, their performance evaluation, and the evolution from perceptrons to neural networks. The module also covers various types of neural network layers, activation functions, loss functions, and optimization techniques, providing a robust understanding of deep learning frameworks.\nModel Evaluation: In this module, we will focus on evaluating machine learning models. We will learn about underfitting and overfitting, and how to mitigate these issues. The module will also cover the train-test split method and its importance in model evaluation, along with various resampling techniques to manage imbalanced datasets effectively.\nNeural Network from Scratch: In this module, we will guide we through the process of constructing a neural network from scratch. We will start with data preparation and model initialization, and proceed to implement essential functions such as forward and backward propagation. The module also covers training and evaluation techniques to help we build and assess your neural network model effectively.\nTensors: In this module, we will explore the concept of tensors and their significance in PyTorch. We will learn about the relationship between tensors and computational graphs, and gain hands-on experience with tensor operations through coding exercises. This module aims to equip we with the skills to apply tensors in real-world machine learning scenarios.\nPyTorch Modeling Introduction: In this module, we will introduce we to PyTorch modeling. We will learn to build and train models from scratch, including linear regression. The module covers batch processing, datasets, and dataloaders to manage data effectively. We will also explore techniques for saving, loading, and optimizing models, including hyperparameter tuning, to enhance your machine learning workflow.\nTarget Learner: This course is designed for data scientists, machine learning engineers, and AI enthusiasts with a basic understanding of programming (preferably Python) and fundamental machine learning concepts. Prior experience with deep learning is beneficial but not required.\nLearning Objectives:\n\nUnderstand and set up the PyTorch environment for development.\nGrasp the fundamental concepts of machine learning and artificial intelligence.\nDevelop a basic understanding of deep learning principles and frameworks.\nLearn to evaluate machine learning models and avoid issues like underfitting and overfitting.\nGain hands-on experience with tensors and computational graphs in PyTorch.\nBuild and understand the structure of neural networks from scratch."
  },
  {
    "objectID": "posts/pytorch/c1w1.html#introduction-to-the-course-foundations-and-core-concepts-of-pytorch",
    "href": "posts/pytorch/c1w1.html#introduction-to-the-course-foundations-and-core-concepts-of-pytorch",
    "title": "Course Overview",
    "section": "Introduction to the Course ‚ÄòFoundations and Core Concepts of PyTorch‚Äô",
    "text": "Introduction to the Course ‚ÄòFoundations and Core Concepts of PyTorch‚Äô"
  },
  {
    "objectID": "posts/pytorch/c1w1.html#full-specialization-resources",
    "href": "posts/pytorch/c1w1.html#full-specialization-resources",
    "title": "Course Overview",
    "section": "Full Specialization Resources",
    "text": "Full Specialization Resources\nThe resources in this link provide the supporting files for the entire course which will help we follow along with the instructor: https://github.com/PacktPublishing/PyTorch-Ultimate-2023‚ÄîFrom-Basics-to-Cutting-Edge\nPlease download the complete zipped file from GitHub, to view all the source code and all the supplementary materials available at the provided link."
  },
  {
    "objectID": "posts/pytorch/c1w1.html#pytorch-introduction",
    "href": "posts/pytorch/c1w1.html#pytorch-introduction",
    "title": "Course Overview",
    "section": "PyTorch Introduction",
    "text": "PyTorch Introduction"
  },
  {
    "objectID": "posts/pytorch/c1w1.html#system-setup",
    "href": "posts/pytorch/c1w1.html#system-setup",
    "title": "Course Overview",
    "section": "System Setup",
    "text": "System Setup"
  },
  {
    "objectID": "posts/pytorch/c1w1.html#how-to-get-the-course-material",
    "href": "posts/pytorch/c1w1.html#how-to-get-the-course-material",
    "title": "Course Overview",
    "section": "How to Get the Course Material",
    "text": "How to Get the Course Material"
  },
  {
    "objectID": "posts/pytorch/c1w1.html#setting-up-the-conda-environment",
    "href": "posts/pytorch/c1w1.html#setting-up-the-conda-environment",
    "title": "Course Overview",
    "section": "Setting Up the conda Environment",
    "text": "Setting Up the conda Environment"
  },
  {
    "objectID": "posts/pytorch/c1w1.html#how-to-work-with-the-course",
    "href": "posts/pytorch/c1w1.html#how-to-work-with-the-course",
    "title": "Course Overview",
    "section": "How to Work with the Course",
    "text": "How to Work with the Course"
  },
  {
    "objectID": "posts/c1w3/lab01.html",
    "href": "posts/c1w3/lab01.html",
    "title": "Linear algebra in Python with NumPy",
    "section": "",
    "text": "Figure¬†1: course banner\nIn this lab, you will have the opportunity to remember some basic concepts about linear algebra and how to use them in Python.\nNumpy is one of the most used libraries in Python for arrays manipulation. It adds to Python a set of functions that allows us to operate on large multidimensional arrays with just a few lines. So forget about writing nested loops for adding matrices! With NumPy, this is as simple as adding numbers.\nLet us import the numpy library and assign the alias np for it. We will follow this convention in almost every notebook in this course, and you‚Äôll see this in many resources outside this course as well.\nimport numpy as np  # The swiss knife of the data scientist.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "L1 Linear algebra with NumPy"
    ]
  },
  {
    "objectID": "posts/c1w3/lab01.html#defining-lists-and-numpy-arrays",
    "href": "posts/c1w3/lab01.html#defining-lists-and-numpy-arrays",
    "title": "Linear algebra in Python with NumPy",
    "section": "Defining lists and numpy arrays",
    "text": "Defining lists and numpy arrays\n\nalist = [1, 2, 3, 4, 5]   # Define a python list. It looks like an np array\nnarray = np.array([1, 2, 3, 4]) # Define a numpy array\n\nNote the difference between a Python list and a NumPy array.\n\nprint(alist)\nprint(narray)\n\nprint(type(alist))\nprint(type(narray))\n\n[1, 2, 3, 4, 5]\n[1 2 3 4]\n&lt;class 'list'&gt;\n&lt;class 'numpy.ndarray'&gt;",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "L1 Linear algebra with NumPy"
    ]
  },
  {
    "objectID": "posts/c1w3/lab01.html#algebraic-operators-on-numpy-arrays-vs.-python-lists",
    "href": "posts/c1w3/lab01.html#algebraic-operators-on-numpy-arrays-vs.-python-lists",
    "title": "Linear algebra in Python with NumPy",
    "section": "Algebraic operators on NumPy arrays vs.¬†Python lists",
    "text": "Algebraic operators on NumPy arrays vs.¬†Python lists\nOne of the common beginner mistakes is to mix up the concepts of NumPy arrays and Python lists. Just observe the next example, where we add two objects of the two mentioned types. Note that the ‚Äò+‚Äô operator on NumPy arrays perform an element-wise addition, while the same operation on Python lists results in a list concatenation. Be careful while coding. Knowing this can save many headaches.\n\nprint(narray + narray)\nprint(alist + alist)\n\n[2 4 6 8]\n[1, 2, 3, 4, 5, 1, 2, 3, 4, 5]\n\n\nIt is the same as with the product operator, *. In the first case, we scale the vector, while in the second case, we concatenate three times the same list.\n\nprint(narray * 3)\nprint(alist * 3)\n\n[ 3  6  9 12]\n[1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5]\n\n\nBe aware of the difference because, within the same function, both types of arrays can appear. Numpy arrays are designed for numerical and matrix operations, while lists are for more general purposes.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "L1 Linear algebra with NumPy"
    ]
  },
  {
    "objectID": "posts/c1w3/lab01.html#matrix-or-array-of-arrays",
    "href": "posts/c1w3/lab01.html#matrix-or-array-of-arrays",
    "title": "Linear algebra in Python with NumPy",
    "section": "Matrix or Array of Arrays",
    "text": "Matrix or Array of Arrays\nIn linear algebra, a matrix is a structure composed of n rows by m columns. That means each row must have the same number of columns. With NumPy, we have two ways to create a matrix: * Creating an array of arrays using np.array (recommended). * Creating a matrix using np.matrix (still available but might be removed soon).\nNumPy arrays or lists can be used to initialize a matrix, but the resulting matrix will be composed of NumPy arrays only.\n\nnpmatrix1 = np.array([narray, narray, narray]) # Matrix initialized with NumPy arrays\nnpmatrix2 = np.array([alist, alist, alist]) # Matrix initialized with lists\nnpmatrix3 = np.array([narray, [1, 1, 1, 1], narray]) # Matrix initialized with both types\n\nprint(npmatrix1)\nprint(npmatrix2)\nprint(npmatrix3)\n\n[[1 2 3 4]\n [1 2 3 4]\n [1 2 3 4]]\n[[1 2 3 4 5]\n [1 2 3 4 5]\n [1 2 3 4 5]]\n[[1 2 3 4]\n [1 1 1 1]\n [1 2 3 4]]\n\n\nHowever, when defining a matrix, be sure that all the rows contain the same number of elements. Otherwise, the linear algebra operations could lead to unexpected results.\nAnalyze the following two examples:\n\n# Example 1:\n\nokmatrix = np.array([[1, 2], [3, 4]]) # Define a 2x2 matrix\nprint(okmatrix) # Print okmatrix\nprint(okmatrix * 2) # Print a scaled version of okmatrix\n\n[[1 2]\n [3 4]]\n[[2 4]\n [6 8]]\n\n\n\n# Example 2:\n\nbadmatrix = np.array([[1, 2], [3, 4], [5, 6, 7]]) # Define a matrix. Note the third row contains 3 elements\nprint(badmatrix) # Print the malformed matrix\nprint(badmatrix * 2) # It is supposed to scale the whole matrix\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[47], line 3\n      1 # Example 2:\n----&gt; 3 badmatrix = np.array([[1, 2], [3, 4], [5, 6, 7]]) # Define a matrix. Note the third row contains 3 elements\n      4 print(badmatrix) # Print the malformed matrix\n      5 print(badmatrix * 2) # It is supposed to scale the whole matrix\n\nValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "L1 Linear algebra with NumPy"
    ]
  },
  {
    "objectID": "posts/c1w3/lab01.html#scaling-and-translating-matrices",
    "href": "posts/c1w3/lab01.html#scaling-and-translating-matrices",
    "title": "Linear algebra in Python with NumPy",
    "section": "Scaling and translating matrices",
    "text": "Scaling and translating matrices\nNow that you know how to build correct NumPy arrays and matrices, let us see how easy it is to operate with them in Python using the regular algebraic operators like + and -.\nOperations can be performed between arrays and arrays or between arrays and scalars.\n\n# Scale by 2 and translate 1 unit the matrix\nresult = okmatrix * 2 + 1 # For each element in the matrix, multiply by 2 and add 1\nprint(result)\n\n[[3 5]\n [7 9]]\n\n\n\n# Add two compatible matrices\nresult1 = okmatrix + okmatrix\nprint(result1)\n\n# Subtract two compatible matrices. This is called the difference vector\nresult2 = okmatrix - okmatrix\nprint(result2)\n\n[[2 4]\n [6 8]]\n[[0 0]\n [0 0]]\n\n\nThe product operator * when used on arrays or matrices indicates element-wise multiplications. Do not confuse it with the dot product.\n\nresult = okmatrix * okmatrix # Multiply each element by itself\nprint(result)\n\n[[ 1  4]\n [ 9 16]]",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "L1 Linear algebra with NumPy"
    ]
  },
  {
    "objectID": "posts/c1w3/lab01.html#transpose-a-matrix",
    "href": "posts/c1w3/lab01.html#transpose-a-matrix",
    "title": "Linear algebra in Python with NumPy",
    "section": "Transpose a matrix",
    "text": "Transpose a matrix\nIn linear algebra, the transpose of a matrix is an operator that flips a matrix over its diagonal, i.e., the transpose operator switches the row and column indices of the matrix producing another matrix. If the original matrix dimension is n by m, the resulting transposed matrix will be m by n.\nT denotes the transpose operations with NumPy matrices.\n\nmatrix3x2 = np.array([[1, 2], [3, 4], [5, 6]]) # Define a 3x2 matrix\nprint('Original matrix 3 x 2')\nprint(matrix3x2)\nprint('Transposed matrix 2 x 3')\nprint(matrix3x2.T)\n\nOriginal matrix 3 x 2\n[[1 2]\n [3 4]\n [5 6]]\nTransposed matrix 2 x 3\n[[1 3 5]\n [2 4 6]]\n\n\nHowever, note that the transpose operation does not affect 1D arrays.\n\nnparray = np.array([1, 2, 3, 4]) # Define an array\nprint('Original array')\nprint(nparray)\nprint('Transposed array')\nprint(nparray.T)\n\nOriginal array\n[1 2 3 4]\nTransposed array\n[1 2 3 4]\n\n\nperhaps in this case you wanted to do:\n\nnparray = np.array([[1, 2, 3, 4]]) # Define a 1 x 4 matrix. Note the 2 level of square brackets\nprint('Original array')\nprint(nparray)\nprint('Transposed array')\nprint(nparray.T)\n\nOriginal array\n[[1 2 3 4]]\nTransposed array\n[[1]\n [2]\n [3]\n [4]]",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "L1 Linear algebra with NumPy"
    ]
  },
  {
    "objectID": "posts/c1w3/lab01.html#get-the-norm-of-a-nparray-or-matrix",
    "href": "posts/c1w3/lab01.html#get-the-norm-of-a-nparray-or-matrix",
    "title": "Linear algebra in Python with NumPy",
    "section": "Get the norm of a nparray or matrix",
    "text": "Get the norm of a nparray or matrix\nIn linear algebra, the norm of an n-dimensional vector \\vec a is defined as:\n norm(\\vec a) = ||\\vec a|| = \\sqrt {\\sum_{i=1}^{n} a_i ^ 2}\nCalculating the norm of vector or even of a matrix is a general operation when dealing with data. Numpy has a set of functions for linear algebra in the subpackage linalg, including the norm function. Let us see how to get the norm a given array or matrix:\n\nnparray1 = np.array([1, 2, 3, 4]) # Define an array\nnorm1 = np.linalg.norm(nparray1)\n\nnparray2 = np.array([[1, 2], [3, 4]]) # Define a 2 x 2 matrix. Note the 2 level of square brackets\nnorm2 = np.linalg.norm(nparray2) \n\nprint(norm1)\nprint(norm2)\n\n5.477225575051661\n5.477225575051661\n\n\nNote that without any other parameter, the norm function treats the matrix as being just an array of numbers. However, it is possible to get the norm by rows or by columns. The axis parameter controls the form of the operation: * axis=0 means get the norm of each column * axis=1 means get the norm of each row.\n\nnparray2 = np.array([[1, 1], [2, 2], [3, 3]]) # Define a 3 x 2 matrix. \n\nnormByCols = np.linalg.norm(nparray2, axis=0) # Get the norm for each column. Returns 2 elements\nnormByRows = np.linalg.norm(nparray2, axis=1) # get the norm for each row. Returns 3 elements\n\nprint(normByCols)\nprint(normByRows)\n\n[3.74165739 3.74165739]\n[1.41421356 2.82842712 4.24264069]\n\n\nHowever, there are more ways to get the norm of a matrix in Python. For that, let us see all the different ways of defining the dot product between 2 arrays.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "L1 Linear algebra with NumPy"
    ]
  },
  {
    "objectID": "posts/c1w3/lab01.html#the-dot-product-between-arrays-all-the-flavors",
    "href": "posts/c1w3/lab01.html#the-dot-product-between-arrays-all-the-flavors",
    "title": "Linear algebra in Python with NumPy",
    "section": "The dot product between arrays: All the flavors",
    "text": "The dot product between arrays: All the flavors\nThe dot product or scalar product or inner product between two vectors \\vec a and \\vec b of the same size is defined as: \\vec a \\cdot \\vec b = \\sum_{i=1}^{n} a_i b_i\nThe dot product takes two vectors and returns a single number.\n\nnparray1 = np.array([0, 1, 2, 3]) # Define an array\nnparray2 = np.array([4, 5, 6, 7]) # Define an array\n\nflavor1 = np.dot(nparray1, nparray2) # Recommended way\nprint(flavor1)\n\nflavor2 = np.sum(nparray1 * nparray2) # Ok way\nprint(flavor2)\n\nflavor3 = nparray1 @ nparray2         # Geeks way\nprint(flavor3)\n\n# As you never should do:             # Noobs way\nflavor4 = 0\nfor a, b in zip(nparray1, nparray2):\n    flavor4 += a * b\n    \nprint(flavor4)\n\n38\n38\n38\n38\n\n\nWe strongly recommend using np.dot, since it is the only method that accepts arrays and lists without problems\n\nnorm1 = np.dot(np.array([1, 2]), np.array([3, 4])) # Dot product on nparrays\nnorm2 = np.dot([1, 2], [3, 4]) # Dot product on python lists\n\nprint(norm1, '=', norm2 )\n\n11 = 11\n\n\nFinally, note that the norm is the square root of the dot product of the vector with itself. That gives many options to write that function:\n norm(\\vec a) = ||\\vec a|| = \\sqrt {\\sum_{i=1}^{n} a_i ^ 2} = \\sqrt {a \\cdot a}",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "L1 Linear algebra with NumPy"
    ]
  },
  {
    "objectID": "posts/c1w3/lab01.html#sums-by-rows-or-columns",
    "href": "posts/c1w3/lab01.html#sums-by-rows-or-columns",
    "title": "Linear algebra in Python with NumPy",
    "section": "Sums by rows or columns",
    "text": "Sums by rows or columns\nAnother general operation performed on matrices is the sum by rows or columns. Just as we did for the function norm, the axis parameter controls the form of the operation: * axis=0 means to sum the elements of each column together. * axis=1 means to sum the elements of each row together.\n\nnparray2 = np.array([[1, -1], [2, -2], [3, -3]]) # Define a 3 x 2 matrix. \n\nsumByCols = np.sum(nparray2, axis=0) # Get the sum for each column. Returns 2 elements\nsumByRows = np.sum(nparray2, axis=1) # get the sum for each row. Returns 3 elements\n\nprint('Sum by columns: ')\nprint(sumByCols)\nprint('Sum by rows:')\nprint(sumByRows)\n\nSum by columns: \n[ 6 -6]\nSum by rows:\n[0 0 0]",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "L1 Linear algebra with NumPy"
    ]
  },
  {
    "objectID": "posts/c1w3/lab01.html#get-the-mean-by-rows-or-columns",
    "href": "posts/c1w3/lab01.html#get-the-mean-by-rows-or-columns",
    "title": "Linear algebra in Python with NumPy",
    "section": "Get the mean by rows or columns",
    "text": "Get the mean by rows or columns\nAs with the sums, one can get the mean by rows or columns using the axis parameter. Just remember that the mean is the sum of the elements divided by the length of the vector  mean(\\vec a) = \\frac {{\\sum_{i=1}^{n} a_i }}{n}\n\nnparray2 = np.array([[1, -1], [2, -2], [3, -3]]) # Define a 3 x 2 matrix. Chosen to be a matrix with 0 mean\n\nmean = np.mean(nparray2) # Get the mean for the whole matrix\nmeanByCols = np.mean(nparray2, axis=0) # Get the mean for each column. Returns 2 elements\nmeanByRows = np.mean(nparray2, axis=1) # get the mean for each row. Returns 3 elements\n\nprint('Matrix mean: ')\nprint(mean)\nprint('Mean by columns: ')\nprint(meanByCols)\nprint('Mean by rows:')\nprint(meanByRows)\n\nMatrix mean: \n0.0\nMean by columns: \n[ 2. -2.]\nMean by rows:\n[0. 0. 0.]",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "L1 Linear algebra with NumPy"
    ]
  },
  {
    "objectID": "posts/c1w3/lab01.html#center-the-columns-of-a-matrix",
    "href": "posts/c1w3/lab01.html#center-the-columns-of-a-matrix",
    "title": "Linear algebra in Python with NumPy",
    "section": "Center the columns of a matrix",
    "text": "Center the columns of a matrix\nCentering the attributes of a data matrix is another essential preprocessing step. Centering a matrix means to remove the column mean to each element inside the column. The mean by columns of a centered matrix is always 0.\nWith NumPy, this process is as simple as this:\n\nnparray2 = np.array([[1, 1], [2, 2], [3, 3]]) # Define a 3 x 2 matrix. \n\nnparrayCentered = nparray2 - np.mean(nparray2, axis=0) # Remove the mean for each column\n\nprint('Original matrix')\nprint(nparray2)\nprint('Centered by columns matrix')\nprint(nparrayCentered)\n\nprint('New mean by column')\nprint(nparrayCentered.mean(axis=0))\n\nOriginal matrix\n[[1 1]\n [2 2]\n [3 3]]\nCentered by columns matrix\n[[-1. -1.]\n [ 0.  0.]\n [ 1.  1.]]\nNew mean by column\n[0. 0.]\n\n\nWarning: This process does not apply for row centering. In such cases, consider transposing the matrix, centering by columns, and then transpose back the result.\nSee the example below:\n\nnparray2 = np.array([[1, 3], [2, 4], [3, 5]]) # Define a 3 x 2 matrix. \n\nnparrayCentered = nparray2.T - np.mean(nparray2, axis=1) # Remove the mean for each row\nnparrayCentered = nparrayCentered.T # Transpose back the result\n\nprint('Original matrix')\nprint(nparray2)\nprint('Centered by rows matrix')\nprint(nparrayCentered)\n\nprint('New mean by rows')\nprint(nparrayCentered.mean(axis=1))\n\nOriginal matrix\n[[1 3]\n [2 4]\n [3 5]]\nCentered by rows matrix\n[[-1.  1.]\n [-1.  1.]\n [-1.  1.]]\nNew mean by rows\n[0. 0. 0.]\n\n\nNote that some operations can be performed using static functions like np.sum() or np.mean(), or by using the inner functions of the array\n\nnparray2 = np.array([[1, 3], [2, 4], [3, 5]]) # Define a 3 x 2 matrix. \n\nmean1 = np.mean(nparray2) # Static way\nmean2 = nparray2.mean()   # Dinamic way\n\nprint(mean1, ' == ', mean2)\n\n3.0  ==  3.0\n\n\nEven if they are equivalent, we recommend the use of the static way always.\nCongratulations! You have successfully reviewed vector and matrix operations with Numpy!",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "L1 Linear algebra with NumPy"
    ]
  },
  {
    "objectID": "posts/c1w3/index.html",
    "href": "posts/c1w3/index.html",
    "title": "Vector Space Models",
    "section": "",
    "text": "Figure¬†1: course banner\n\n\n\n\n\n\n\n\nFigure¬†2",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models and PCA"
    ]
  },
  {
    "objectID": "posts/c1w3/index.html#lab-linear-algebra-in-python-with-numpy",
    "href": "posts/c1w3/index.html#lab-linear-algebra-in-python-with-numpy",
    "title": "Vector Space Models",
    "section": "LAB: Linear algebra in Python with numpy",
    "text": "LAB: Linear algebra in Python with numpy\nThe Numpy lab\n\nCosine Similarity Intuition\nOne of the issues with euclidean distance is that it is not always accurate and sometimes we are not looking for that type of similarity metric. For example, when comparing large documents to smaller ones with euclidean distance one could get an inaccurate result.\nLook at the diagram:\n\n\n\n\n\n\n\nCosine Similarity: Intuition\n\n\n\n\nFigure¬†11: The cosine similarity is the cosine of the angle between two vectors.\n\n\nNormally the food corpus and the agriculture corpus are more similar because they have the same proportion of words. However the food corpus is much smaller than the agriculture corpus. To further clarify, although the history corpus and the agriculture corpus are different, they have a smaller euclidean distance. Hence d_2&lt;d_1.\nTo solve this problem, we look at the cosine between the vectors. This allows us to compare B and Œ±.\n\n\nBackground\nBefore getting into the cosine similarity function remember that the norm of a vector is defined as:\n\n\nNorm of a Vector\n\n||\\vec{A}|| = \\sqrt{\\sum_{i=1}^{n} a_i^2}\n\\tag{2}\n\n\nDot-product of Two Vectors\nThe dot product is then defined as:\n\n\\vec{A} \\cdot \\vec{B} = \\sum_{i=1}^{n} a_i \\cdot b_i\n\\tag{3}\n\n\n\n\n\n\n\nCosine Similarity\n\n\n\n\nFigure¬†12: The cosine similarity is the cosine of the angle between two vectors.\n\n\nThe following cosine similarity equation makes sense:\n\n\\cos(\\theta) = \\frac{\\vec{v} \\cdot \\vec{w}}{||\\vec{v}|| \\cdot ||\\vec{w}||}\n\\tag{4}",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models and PCA"
    ]
  },
  {
    "objectID": "posts/c1w3/index.html#implementation",
    "href": "posts/c1w3/index.html#implementation",
    "title": "Vector Space Models",
    "section": "Implementation",
    "text": "Implementation\nWhen \\vec{v} and \\vec{u} are parallel the numerator is equal to the denominator so cos(\\beta)=1 thus \\angle \\beta=0.\nOn the other hand, the dot product of two orthogonal (perpendicular) vectors is 0. That takes place when \\angle \\beta=90.\n\n\n\n\n\n\n\nCosine Similarity Examples\n\n\n\n\nFigure¬†13: Examples of cosine similarity between similar and dissimilar vectors.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models and PCA"
    ]
  },
  {
    "objectID": "posts/c1w3/index.html#lab-on-manipulating-word-vectors",
    "href": "posts/c1w3/index.html#lab-on-manipulating-word-vectors",
    "title": "Vector Space Models",
    "section": "Lab on Manipulating Word Vectors",
    "text": "Lab on Manipulating Word Vectors\nThe lab",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models and PCA"
    ]
  },
  {
    "objectID": "posts/c1w3/index.html#visualization-of-word-vectors",
    "href": "posts/c1w3/index.html#visualization-of-word-vectors",
    "title": "Vector Space Models",
    "section": "Visualization of Word Vectors",
    "text": "Visualization of Word Vectors\nPrincipal component analysis is an unsupervised learning algorithm which can be used to reduce the dimension of your data. As a result, it allows you to visualize your data. It tries to combine variances across features. Here is a concrete example of PCA:\n\n\n\n\n\n\n\nWord analogy\n\n\n\n\nFigure¬†16: Some word analogies\n\n\n\n\n\n\n\n\nWord analogy\n\n\n\n\nFigure¬†17: Some word analogies\n\n\n\nThose are the results of plotting a couple of vectors in two dimensions. Note that words with similar part of speech (POS) tags are next to one another. This is because many of the training algorithms learn words by identifying the neighboring words. Thus, words with similar POS tags tend to be found in similar locations. An interesting insight is that synonyms and antonyms tend to be found next to each other in the plot. Why is that the case?",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models and PCA"
    ]
  },
  {
    "objectID": "posts/c1w3/index.html#implementation-1",
    "href": "posts/c1w3/index.html#implementation-1",
    "title": "Vector Space Models",
    "section": "Implementation",
    "text": "Implementation",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models and PCA"
    ]
  },
  {
    "objectID": "posts/c1w3/index.html#pca-algorithm",
    "href": "posts/c1w3/index.html#pca-algorithm",
    "title": "Vector Space Models",
    "section": "PCA algorithm",
    "text": "PCA algorithm\nPCA is commonly used to reduce the dimension of your data. Intuitively the model collapses the data across principal components. You can think of the first principal component (in a 2D dataset) as the line where there is the most amount of variance. You can then collapse the data points on that line. Hence you went from 2D to 1D. You can generalize this intuition to several dimensions.\n\n\n\n\n\n\n\nPCA\n\n\n\n\nFigure¬†18: Some word analogies\n\n\n\nEigenvector\n\nthe resulting vectors, also known as the uncorrelated features of your data\n\nEigenvalue\n\nthe amount of information retained by each new feature. You can think of it as the variance in the eigenvector.\n\n\nAlso each eigenvalue has a corresponding eigenvector. The eigenvalue tells you how much variance there is in the eigenvector. Here are the steps required to compute PCA:\n\n\n\n\n\n\n\nPCA\n\n\n\n\nFigure¬†19: Some word analogies\n\n\n\nSteps to Compute PCA:\n\nMean normalize your data\nCompute the covariance matrix\nCompute SVD on your covariance matrix. This returns [USV]=svd(Œ£) . The three matrices U, S, V are drawn above. U is labelled with eigenvectors, and S is labelled with eigenvalues.\nYou can then use the first n columns of vector U, to get your new data by multiplying XU[:,0:n].",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models and PCA"
    ]
  },
  {
    "objectID": "posts/c1w3/index.html#putting-it-together-with-code",
    "href": "posts/c1w3/index.html#putting-it-together-with-code",
    "title": "Vector Space Models",
    "section": "Putting It Together with Code",
    "text": "Putting It Together with Code\n\nimport numpy as np \n\ndef PCA(X , num_components):\n  # center data around the mean\n  X_meaned = X - np.mean(X , axis = 0) \n  # calculate the covariance matrix   \n  cov_mat = np.cov(X_meaned , rowvar = False) \n  # compute an uncorrelated feature basis (eigen vectors) \n  eigen_values , eigen_vectors = np.linalg.eigh(cov_mat)\n  # sort the new basis by decreasing eigen values (variance) \n  sorted_index = np.argsort(eigen_values)[::-1] \n  sorted_eigenvalue = eigen_values[sorted_index] \n  sorted_eigenvectors = eigen_vectors[:,sorted_index] \n  # by subseting the most leading features  \n  eigenvector_subset = sorted_eigenvectors[:,0:num_components] \n  #Step-6 \n  X_reduced = np.dot(eigenvector_subset.transpose() ,     \n                   X_meaned.transpose() ).transpose() \n  return X_reduced",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models and PCA"
    ]
  },
  {
    "objectID": "posts/c1w3/index.html#lab-on-pca",
    "href": "posts/c1w3/index.html#lab-on-pca",
    "title": "Vector Space Models",
    "section": "Lab on PCA",
    "text": "Lab on PCA\nPCA lab",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models and PCA"
    ]
  },
  {
    "objectID": "posts/c1w3/index.html#resources",
    "href": "posts/c1w3/index.html#resources",
    "title": "Vector Space Models",
    "section": "Resources",
    "text": "Resources\n\nAlex Williams - Everything you did and didn‚Äôt know about PCA\nUdell et al.¬†(2015).‚ÄØGeneralized Low-Rank Models ‚ÄØarxiv preprint\nTipping & Bishop (1999).‚ÄØProbabilistic principal component analysis Journal of the Royal Statistical Society: Series B\nIlin & Raiko (2010) Practical Approaches to Principal Component Analysis in the Presence of Missing Values Journal of Machine Learning Research\nGordon (2002).‚ÄØGeneralized2 Linear2‚ÄØModels‚ÄØNIPS\nCunningham & Ghahramani (2015)‚ÄØ Linear dimensionality reduction: survey, insights, and generalizations‚ÄØJournal of Machine Learning Research\nBurges (2009).‚ÄØDimension Reduction: A Guided Tour‚ÄØFoundations varia Trends in Machine Learning\nM. Gavish and D. L. Donoho, The Optimal Hard Threshold for Singular Values is \\frac{4}{\\sqrt{3}} in IEEE Transactions on Information Theory, vol. 60, no. 8, pp.¬†5040-5053, Aug.¬†2014, doi: 10.1109/TIT.2014.2323359.\nThomas P. Minka Automatic choice of dimensionality for PCA Dec.¬†2000",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models and PCA"
    ]
  },
  {
    "objectID": "posts/c2w2/lab01.html",
    "href": "posts/c2w2/lab01.html",
    "title": "Parts-of-Speech Tagging - First Steps: Working with text files, Creating a Vocabulary and Handling Unknown Words",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure¬†1\nIn this lecture notebook you will create a vocabulary from a tagged dataset and learn how to deal with words that are not present in this vocabulary when working with other text sources.\nAside from this you will also learn how to:\nimport string\nfrom collections import defaultdict",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "L1 - Vocabulary with Unknown Words"
    ]
  },
  {
    "objectID": "posts/c2w2/lab01.html#processing-new-text-sources",
    "href": "posts/c2w2/lab01.html#processing-new-text-sources",
    "title": "Parts-of-Speech Tagging - First Steps: Working with text files, Creating a Vocabulary and Handling Unknown Words",
    "section": "Processing new text sources",
    "text": "Processing new text sources\n\nDealing with unknown words\nNow that you have a vocabulary, you will use it when processing new text sources. A new text will have words that do not appear in the current vocabulary. To tackle this, you can simply classify each new word as an unknown one, but you can do better by creating a function that tries to classify the type of each unknown word and assign it a corresponding unknown token.\nThis function will do the following checks and return an appropriate token:\n\nCheck if the unknown word contains any character that is a digit\n\nreturn --unk_digit--\n\nCheck if the unknown word contains any punctuation character\n\nreturn --unk_punct--\n\nCheck if the unknown word contains any upper-case character\n\nreturn --unk_upper--\n\nCheck if the unknown word ends with a suffix that could indicate it is a noun, verb, adjective or adverb\n\nreturn --unk_noun--, --unk_verb--, --unk_adj--, --unk_adv-- respectively\n\n\nIf a word fails to fall under any condition then its token will be a plain --unk--. The conditions will be evaluated in the same order as listed here. So if a word contains a punctuation character but does not contain digits, it will fall under the second condition. To achieve this behaviour some if/elif statements can be used along with early returns.\nThis function is implemented next. Notice that the any() function is being heavily used. It returns True if at least one of the cases it evaluates is True.\n\ndef assign_unk(word):\n    \"\"\"\n    Assign tokens to unknown words\n    \"\"\"\n    \n    # Punctuation characters\n    # Try printing them out in a new cell!\n    punct = set(string.punctuation)\n    \n    # Suffixes\n    noun_suffix = [\"action\", \"age\", \"ance\", \"cy\", \"dom\", \"ee\", \"ence\", \"er\", \"hood\", \"ion\", \"ism\", \"ist\", \"ity\", \"ling\", \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ship\", \"ty\"]\n    verb_suffix = [\"ate\", \"ify\", \"ise\", \"ize\"]\n    adj_suffix = [\"able\", \"ese\", \"ful\", \"i\", \"ian\", \"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ly\", \"ous\"]\n    adv_suffix = [\"ward\", \"wards\", \"wise\"]\n\n    # Loop the characters in the word, check if any is a digit\n    if any(char.isdigit() for char in word):\n        return \"--unk_digit--\"\n\n    # Loop the characters in the word, check if any is a punctuation character\n    elif any(char in punct for char in word):\n        return \"--unk_punct--\"\n\n    # Loop the characters in the word, check if any is an upper case character\n    elif any(char.isupper() for char in word):\n        return \"--unk_upper--\"\n\n    # Check if word ends with any noun suffix\n    elif any(word.endswith(suffix) for suffix in noun_suffix):\n        return \"--unk_noun--\"\n\n    # Check if word ends with any verb suffix\n    elif any(word.endswith(suffix) for suffix in verb_suffix):\n        return \"--unk_verb--\"\n\n    # Check if word ends with any adjective suffix\n    elif any(word.endswith(suffix) for suffix in adj_suffix):\n        return \"--unk_adj--\"\n\n    # Check if word ends with any adverb suffix\n    elif any(word.endswith(suffix) for suffix in adv_suffix):\n        return \"--unk_adv--\"\n    \n    # If none of the previous criteria is met, return plain unknown\n    return \"--unk--\"\n\nA POS tagger will always encounter words that are not within the vocabulary that is being used. By augmenting the dataset to include these unknown word tokens you are helping the tagger to have a better idea of the appropriate tag for these words.\n\n\nGetting the correct tag for a word\nAll that is left is to implement a function that will get the correct tag for a particular word taking special considerations for unknown words. Since the dataset provides each word and tag within the same line and a word being known depends on the vocabulary used, these two elements should be arguments to this function.\nThis function should check if a line is empty and if so, it should return a placeholder word and tag, --n-- and --s-- respectively.\nIf not, it should process the line to return the correct word and tag pair, considering if a word is unknown in which scenario the function assign_unk() should be used.\nThe function is implemented next. Notice That the split() method can be used without specifying the delimiter, in which case it will default to any whitespace.\n\ndef get_word_tag(line, vocab):\n    # If line is empty return placeholders for word and tag\n    if not line.split():\n        word = \"--n--\"\n        tag = \"--s--\"\n    else:\n        # Split line to separate word and tag\n        word, tag = line.split()\n        # Check if word is not in vocabulary\n        if word not in vocab: \n            # Handle unknown word\n            word = assign_unk(word)\n    return word, tag\n\nNow you can try this function with some examples to test that it is working as intended:\n\nget_word_tag('\\n', vocab)\n\n('--n--', '--s--')\n\n\nSince this line only includes a newline character it returns a placeholder word and tag.\n\nget_word_tag('In\\tIN\\n', vocab)\n\n('In', 'IN')\n\n\nThis one is a valid line and the function does a fair job at returning the correct (word, tag) pair.\n\nget_word_tag('tardigrade\\tNN\\n', vocab)\n\n('--unk--', 'NN')\n\n\nThis line includes a noun that is not present in the vocabulary.\nThe assign_unk function fails to detect that it is a noun so it returns an unknown token.\n\nget_word_tag('scrutinize\\tVB\\n', vocab)\n\n('--unk_verb--', 'VB')\n\n\nThis line includes a verb that is not present in the vocabulary.\nIn this case the assign_unk is able to detect that it is a verb so it returns an unknown verb token.\nCongratulations on finishing this lecture notebook! Now you should be more familiar with working with text data and have a better understanding of how a basic POS tagger works.\nKeep it up!",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "L1 - Vocabulary with Unknown Words"
    ]
  },
  {
    "objectID": "posts/c2w2/index.html",
    "href": "posts/c2w2/index.html",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure¬†1\n\n\n\n\n\n\n\n\nFigure¬†2",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c2w2/index.html#part-of-speech-tagging",
    "href": "posts/c2w2/index.html#part-of-speech-tagging",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Part of Speech Tagging",
    "text": "Part of Speech Tagging\nPart of Speech Tagging (POS) is the process of assigning a part of speech to a word. By doing so, we will learn the following:\n\nMarkov Chains\nHidden Markov Models\nViterbi algorithm\n\nHere is a concrete example:\n\n\n\n\n\n\n\nautocorrect\n\n\n\n\nFigure¬†3: Learning Objectives\n\n\nWe can use part of speech tagging for:\n\nIdentifying named entities\nSpeech recognition\nCoreference Resolution\n\nWe can use the probabilities of POS tags happening near one another to come up with the most reasonable output",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c2w2/index.html#lab1-working-with-text-data",
    "href": "posts/c2w2/index.html#lab1-working-with-text-data",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Lab1: Working with text data",
    "text": "Lab1: Working with text data\nWorking with text data",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c2w2/index.html#markov-chains",
    "href": "posts/c2w2/index.html#markov-chains",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Markov Chains",
    "text": "Markov Chains\n\n\n\n\n\n\n\nPOS Tagging\n\n\n\n\nFigure¬†4: POS Tagging\n\n\n\n\n\n\n\n\nFSM\n\n\n\n\nFigure¬†5: FSM representation for POS tagging\n\n\n\nThe circles of the graph represent the states of your model. A state refers to a certain condition of the present moment. We can think of these as the POS tags of the current word.\n\nQ={q_1, q_2, q_3} \\qquad \\text{ is the set of all states in your model. }",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c2w2/index.html#markov-chains-and-pos-tags",
    "href": "posts/c2w2/index.html#markov-chains-and-pos-tags",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Markov Chains and POS Tags",
    "text": "Markov Chains and POS Tags\nTo help identify the parts of speech for every word, we need to build a transition matrix that gives we the probabilities from one state to another.\n\n\n\n\n\n\n\nFSM\n\n\n\n\nFigure¬†6: Learning Objectives\n\n\nIn the diagram above, the blue circles correspond to the part of speech tags, and the arrows correspond to the transition probabilities from one part of speech to another. We can populate the table on the right from the diagram on the left. The first row in your A matrix corresponds to the initial distribution among all the states. According to the table, the sentence has a 40% chance to start as a noun, 10% chance to start with a verb, and a 50% chance to start with another part of speech tag.\nIn more general notation, we can write the transition matrix A, given some states Q, as follows:\n\n\n\n\n\n\n\nFSM\n\n\n\n\nFigure¬†7: Learning Objectives",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c2w2/index.html#hidden-markov-models",
    "href": "posts/c2w2/index.html#hidden-markov-models",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Hidden Markov Models",
    "text": "Hidden Markov Models\nIn the previous video, I showed we an example with a simple markov model. The transition probabilities allowed we to identify the transition probability from one POS to another. We will now explore hidden markov models. In hidden markov models we make use of emission probabilities that give we the probability to go from one state (POS tag) to a specific word.\n\n\n\n\n\n\n\nState Transition Graph\n\n\n\n\nFigure¬†8: Emission probabilities\n\n\nFor example, given that we are in a verb state, we can go to other words with certain probabilities. This emission matrix B, will be used with your transition matrix A, to help we identify the part of speech of a word in a sentence. To populate your matrix B, we can just have a labelled dataset and compute the probabilities of going from a POS to each word in your vocabulary. Here is a recap of what we have seen so far:\n\n\n\n\n\n\n\nHMM\n\n\n\n\nFigure¬†9: HMM Summary\n\n\nNote that the sum of each row in your A and B matrix has to be 1. Next, I will show we how we can calculate the probabilities inside these matrices.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c2w2/index.html#calculating-probabilities",
    "href": "posts/c2w2/index.html#calculating-probabilities",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Calculating Probabilities",
    "text": "Calculating Probabilities\nHere is a visual representation on how to calculate the probabilities:\n\n\n\n\n\n\n\nTransition Probabilities\n\n\n\n\nFigure¬†10: Transition Probabilities\n\n\nThe number of times that blue is followed by purple is 2 out of 3. We will use the same logic to populate our transition and emission matrices. In the transition matrix we will count the number of times tag t_{(i‚àí1)},t{(i)} show up near each other and divide by the total number of times t_{(i‚àí1)} shows up. (which is the same as the number of times it shows up followed by anything else).\n\n\n\n\n\n\n\nTransition Probabilities\n\n\n\n\nFigure¬†11: Transition Probabilities\n\n\n\ncalculate co-occurrence of tag pairs \nC(t_{(i-1)},t_{(i)})\n\\tag{1}\ncalculate the probabilities using the counts \nP(t_{(i)}|t_{(i-1)}) = \\frac{C(t_{(i)}),t_{(i-1)},}{\\sum_{i=1}^{N} C(t_{(i-1)})}\n\\tag{2}\n\nWhere\nC(t_{(i‚àí1)} ,t_{(i)}) is the count of times tag t_{(i-1)} shows up before tag i.\nFrom this we can compute the probability that a tag shows up after another tag.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c2w2/index.html#populating-the-transition-matrix",
    "href": "posts/c2w2/index.html#populating-the-transition-matrix",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Populating the Transition Matrix",
    "text": "Populating the Transition Matrix\nTo populate the transition matrix we have to keep track of the number of times each tag shows up before another tag.\n\n\n\n\n\n\n\nTransition Probabilities\n\n\n\n\nFigure¬†12: Transition Probabilities\n\n\nIn the table above, we can see that green corresponds to nouns (NN), purple corresponds to verbs (VB), and blue corresponds to other (O). Orange (œÄ) corresponds to the initial state. The numbers inside the matrix correspond to the number of times a part of speech tag shows up right after another one.\nTo go from O to NN or in other words to calculate P(O‚à£NN) we have to compute the following:\n\n\n\n\n\n\n\nTransition Probabilities\n\n\n\n\nFigure¬†13: Transition Probabilities\n\n\nTo generalize:\n\nP(t_{(i)} \\mid t_{(i-1)}) = \\frac{C(t_{(i)},t_{(i-1)})}{\\sum_{j=1}^{N} C(t_{(i-1)},t_{(j)})}\n\\tag{3}\nWhere:\n\nC(t_{(i)},t_{(i-1)}) is the count of times tag t_{(i-1)} shows up before tag i.\n\nUnfortunately, sometimes we might not see two POS tags in front each other. This will give we a probability of 0. To solve this issue, we will ‚Äúsmooth‚Äù it as follows:\n\n\n\n\n\n\n\nTransition Probabilities\n\n\n\n\nFigure¬†14: Transition Probabilities\n\n\nThe \\epsilon allows we to not have any two sequences showing up with 0 probability. Why is this important?",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c2w2/index.html#populating-the-emission-matrix",
    "href": "posts/c2w2/index.html#populating-the-emission-matrix",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Populating the Emission Matrix",
    "text": "Populating the Emission Matrix\nTo populate the emission matrix, we have to keep track of the words associated with their parts of speech tags.\n\n\n\n\n\n\n\nThe Emission Matrix\n\n\n\n\nFigure¬†15: The Emission Matrix\n\n\nTo populate the matrix, we will also use smoothing as we have previously used:\n\nP(w_i \\mid t_i) = \\frac{C(t_i,w_i)+\\epsilon}{\\sum_{j=1}^{N} C(t_i,w_j)+ N \\times \\epsilon} = \\frac{C(t_i,w_i)+\\epsilon}{\\sum_{j=1}^{N} C(t_i)+N\\times \\epsilon}\n\nWhere C(t_i,w_i) is the count associated with how many times the tag t_i is associated with the word w_i. The epsilon above is the smoothing parameter. In the next video, we will talk about the Viterbi algorithm and discuss how we can use the transition and emission matrix to come up with probabilities.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c2w2/index.html#lab2-working-with-text-data",
    "href": "posts/c2w2/index.html#lab2-working-with-text-data",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Lab2: Working with text data",
    "text": "Lab2: Working with text data\nWorking with text data",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c2w2/index.html#the-viterbi-algorithm",
    "href": "posts/c2w2/index.html#the-viterbi-algorithm",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "The Viterbi Algorithm",
    "text": "The Viterbi Algorithm\nThe Viterbi algorithm makes use of the transition probabilities and the emission probabilities as follows.\n\n\n\n\n\n\n\nThe Emission Matrix\n\n\n\n\nFigure¬†16: The Viterbi Algorithm\n\n\nTo go from œÄ to O we need to multiply the corresponding transition probability (0.3) and the corresponding emission probability (0.5), which gives we 0.15. We keep doing that for all the words, until we get the probability of an entire sequence.\n\n\n\n\n\n\n\nThe Emission Matrix\n\n\n\n\nFigure¬†17: The Viterbi Algorithm\n\n\nWe can then see how we will just pick the sequence with the highest probability. We will show we a systematic way to accomplish this (Viterbi!).",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c2w2/index.html#viterbi-initialization",
    "href": "posts/c2w2/index.html#viterbi-initialization",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Viterbi Initialization",
    "text": "Viterbi Initialization\nWe will now populate a matrix C of dimension (num_tags, num_words). This matrix will have the probabilities that will tell we what part of speech each word belongs to.\n\n\n\n\n\n\n\nThe Emission Matrix\n\n\n\n\nFigure¬†18: The Viterbi Initialization\n\n\nNow to populate the first column, we just multiply the initial œÄ distribution, for each tag, times b_{i,cindex(w_1)}, which is the emission probability of the word 1 given the tag i. Where the i, corresponds to the tag of the initial distribution and the cindex(w_1), is the index of word 1 in the emission matrix. And that‚Äôs it, we are done with populating the first column of your new C matrix. We will now need to keep track what part of speech we are coming from. Hence we introduce a matrix D, which allows we to store the labels that represent the different states we are going through when finding the most likely sequence of POS tags for the given sequence of words w_2 ,‚Ä¶,w_k. At first we set the first column to 0, because we are not coming from any POS tag.\n\n\n\n\n\n\n\nThe Emission Matrix\n\n\n\n\nFigure¬†19: The Viterbi Initialization\n\n\nThese two matrices will make more sense in the next videos.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c2w2/index.html#viterbi-forward-pass",
    "href": "posts/c2w2/index.html#viterbi-forward-pass",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Viterbi: Forward Pass",
    "text": "Viterbi: Forward Pass\nThis will be best illustrated with an example:\n\n\n\n\n\n\n\nViterbi: Forward Pass\n\n\n\n\nFigure¬†20: Viterbi: Forward Pass\n\n\nSo to populate a cell (i.e.¬†1,2) in the image above, we have to take the max of [kth cells in the previous column, times the corresponding transition probability of the kth POS to the first POS times the emission probability of the first POS and the current word we are looking at]. We do that for all the cells. Take a paper and a pencil, and make sure we understand how it is done.\nThe general rule is c_{ij}= max_k c_{k,j-1} \\times a_{k,i} \\times b_{i,cindex(w_j)}\nNow to populate the D matrix, we will keep track of the argmax of where we came from as follows:\nNote that the only difference between c_{ij} and d_{ij}, is that in the former we compute the probability and in the latter we keep track of the index of the row where that probability came from. So we keep track of which k was used to get that max probability.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c2w2/index.html#viterbi-backward-pass",
    "href": "posts/c2w2/index.html#viterbi-backward-pass",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Viterbi: Backward Pass",
    "text": "Viterbi: Backward Pass\nGreat, now that we know how to compute A, B, C, and D, we will put it all together and show we how to construct the path that will give we the part of speech tags for your sentence.\n\n\n\n\n\n\n\nViterbi: Forward Pass\n\n\n\n\nFigure¬†21: Viterbi: Forward Pass\n\n\nThe equation above just gives we the index of the highest row in the last column of C. Once we have that, we can go ahead and start using your D matrix as follows:\n\n\n\n\n\n\n\nViterbi: Forward Pass\n\n\n\n\nFigure¬†22: Viterbi: Forward Pass\n\n\nNote that since we started at index one, hence the last word (w5) is t_1. Then we go to the first row of D and what ever that number is, it indicated the row of the next part of speech tag. Then next part of speech tag indicates the row of the next and so forth. This allows we to reconstruct the POS tags for your sentence.\nWe will be implementing this in this week‚Äôs programming assignment.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c2w2/index.html#assignment",
    "href": "posts/c2w2/index.html#assignment",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Assignment",
    "text": "Assignment\nPart-of-speech (POS) tagging",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c1w1/lab03.html",
    "href": "posts/c1w1/lab03.html",
    "title": "Visualizing tweets and the Logistic Regression model",
    "section": "",
    "text": "Figure¬†1: course banner\nObjectives: Visualize and interpret the logistic regression model\nSteps:",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "L3 Visualizing tweets"
    ]
  },
  {
    "objectID": "posts/c1w1/lab03.html#import-the-required-libraries",
    "href": "posts/c1w1/lab03.html#import-the-required-libraries",
    "title": "Visualizing tweets and the Logistic Regression model",
    "section": "Import the required libraries",
    "text": "Import the required libraries\nWe will be using NLTK, an opensource NLP library, for collecting, handling, and processing Twitter data. In this lab, we will use the example dataset that comes alongside with NLTK. This dataset has been manually annotated and serves to establish baselines for models quickly.\nSo, to start, let‚Äôs import the required libraries.\n\nimport nltk                         # NLP toolbox\nfrom os import getcwd\nimport pandas as pd                 # Library for Dataframes \nfrom nltk.corpus import twitter_samples \nimport matplotlib.pyplot as plt     # Library for visualization\nimport numpy as np                  # Library for math functions\n\nfrom utils import process_tweet, build_freqs # Our functions for NLP",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "L3 Visualizing tweets"
    ]
  },
  {
    "objectID": "posts/c1w1/lab03.html#load-the-nltk-sample-dataset",
    "href": "posts/c1w1/lab03.html#load-the-nltk-sample-dataset",
    "title": "Visualizing tweets and the Logistic Regression model",
    "section": "Load the NLTK sample dataset",
    "text": "Load the NLTK sample dataset\nTo complete this lab, you need the sample dataset of the previous lab. Here, we assume the files are already available, and we only need to load into Python lists.\n\n# select the set of positive and negative tweets\nall_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')\n\ntweets = all_positive_tweets + all_negative_tweets ## Concatenate the lists. \nlabels = np.append(np.ones((len(all_positive_tweets),1)), np.zeros((len(all_negative_tweets),1)), axis = 0)\n\n# split the data into two pieces, one for training and one for testing (validation set) \ntrain_pos  = all_positive_tweets[:4000]\ntrain_neg  = all_negative_tweets[:4000]\n\ntrain_x = train_pos + train_neg \n\nprint(\"Number of tweets: \", len(train_x))\n\nNumber of tweets:  8000",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "L3 Visualizing tweets"
    ]
  },
  {
    "objectID": "posts/c1w1/lab03.html#load-a-pretrained-logistic-regression-model",
    "href": "posts/c1w1/lab03.html#load-a-pretrained-logistic-regression-model",
    "title": "Visualizing tweets and the Logistic Regression model",
    "section": "Load a pretrained Logistic Regression model",
    "text": "Load a pretrained Logistic Regression model\nIn the same way, as part of this week‚Äôs assignment, a Logistic regression model must be trained. The next cell contains the resulting model from such training. Notice that a list of 3 numeric values represents the whole model, that we have called theta \\theta.\n\ntheta = [7e-08, 0.0005239, -0.00055517]",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "L3 Visualizing tweets"
    ]
  },
  {
    "objectID": "posts/c1w1/lab03.html#plot-the-samples-in-a-scatter-plot",
    "href": "posts/c1w1/lab03.html#plot-the-samples-in-a-scatter-plot",
    "title": "Visualizing tweets and the Logistic Regression model",
    "section": "Plot the samples in a scatter plot",
    "text": "Plot the samples in a scatter plot\nThe vector theta represents a plane that split our feature space into two parts. Samples located over that plane are considered positive, and samples located under that plane are considered negative. Remember that we have a 3D feature space, i.e., each tweet is represented as a vector comprised of three values: [bias, positive_sum, negative_sum], always having bias = 1.\nIf we ignore the bias term, we can plot each tweet in a cartesian plane, using positive_sum and negative_sum. In the cell below, we do precisely this. Additionally, we color each tweet, depending on its class. Positive tweets will be green and negative tweets will be red.\n\n# Plot the samples using columns 1 and 2 of the matrix\nfig, ax = plt.subplots(figsize = (8, 8))\n\ncolors = ['red', 'green']\n\n# Color based on the sentiment Y\nax.scatter(X[:,1], X[:,2], c=[colors[int(k)] for k in Y], s = 0.1)  # Plot a dot for each pair of words\nplt.xlabel(\"Positive\")\nplt.ylabel(\"Negative\")\n\nText(0.5, 0, 'Positive')\n\n\nText(0, 0.5, 'Negative')\n\n\n\n\n\n\n\n\n\nFrom the plot, it is evident that the features that we have chosen to represent tweets as numerical vectors allow an almost perfect separation between positive and negative tweets. So you can expect a very high accuracy for this model!",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "L3 Visualizing tweets"
    ]
  },
  {
    "objectID": "posts/c1w1/lab03.html#plot-the-model-alongside-the-data",
    "href": "posts/c1w1/lab03.html#plot-the-model-alongside-the-data",
    "title": "Visualizing tweets and the Logistic Regression model",
    "section": "Plot the model alongside the data",
    "text": "Plot the model alongside the data\nWe will draw a gray line to show the cutoff between the positive and negative regions. In other words, the gray line marks the line where  z = \\theta * x = 0. To draw this line, we have to solve the above equation in terms of one of the independent variables.\n z = \\theta * x = 0  x = [1, pos, neg]   z(\\theta, x) = \\theta_0+ \\theta_1 * pos + \\theta_2 * neg = 0   neg = (-\\theta_0 - \\theta_1 * pos) / \\theta_2 \nThe red and green lines that point in the direction of the corresponding sentiment are calculated using a perpendicular line to the separation line calculated in the previous equations(neg function). It must point in the same direction as the derivative of the Logit function, but the magnitude may differ. It is only for a visual representation of the model.\ndirection = pos * \\theta_2 / \\theta_1\n\n# Equation for the separation plane\n# It give a value in the negative axe as a function of a positive value\n# f(pos, neg, W) = w0 + w1 * pos + w2 * neg = 0\n# s(pos, W) = (w0 - w1 * pos) / w2\ndef neg(theta, pos):\n    return (-theta[0] - pos * theta[1]) / theta[2]\n\n# Equation for the direction of the sentiments change\n# We don't care about the magnitude of the change. We are only interested \n# in the direction. So this direction is just a perpendicular function to the \n# separation plane\n# df(pos, W) = pos * w2 / w1\ndef direction(theta, pos):\n    return    pos * theta[2] / theta[1]\n\nThe green line in the chart points in the direction where z &gt; 0 and the red line points in the direction where z &lt; 0. The direction of these lines are given by the weights \\theta_1 and \\theta_2\n\n# Plot the samples using columns 1 and 2 of the matrix\nfig, ax = plt.subplots(figsize = (8, 8))\n\ncolors = ['red', 'green']\n\n# Color base on the sentiment Y\nax.scatter(X[:,1], X[:,2], c=[colors[int(k)] for k in Y], s = 0.1)  # Plot a dot for each pair of words\nplt.xlabel(\"Positive\")\nplt.ylabel(\"Negative\")\n\n# Now lets represent the logistic regression model in this chart. \nmaxpos = np.max(X[:,1])\n\noffset = 5000 # The pos value for the direction vectors origin\n\n# Plot a gray line that divides the 2 areas.\nax.plot([0,  maxpos], [neg(theta, 0),   neg(theta, maxpos)], color = 'gray') \n\n# Plot a green line pointing to the positive direction\nax.arrow(offset, neg(theta, offset), offset, direction(theta, offset), head_width=500, head_length=500, fc='g', ec='g')\n# Plot a red line pointing to the negative direction\nax.arrow(offset, neg(theta, offset), -offset, -direction(theta, offset), head_width=500, head_length=500, fc='r', ec='r')\n\nplt.show()\n\nText(0.5, 0, 'Positive')\n\n\nText(0, 0.5, 'Negative')\n\n\n\n\n\n\n\n\n\nNote that more critical than the Logistic regression itself, are the features extracted from tweets that allow getting the right results in this exercise.\nThat is all, folks. Hopefully, now you understand better what the Logistic regression model represents, and why it works that well for this specific problem.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "L3 Visualizing tweets"
    ]
  },
  {
    "objectID": "posts/c1w1/lab02.html",
    "href": "posts/c1w1/lab02.html",
    "title": "Building and Visualizing word frequencies",
    "section": "",
    "text": "course banner\nIn this lab, we will focus on the build_freqs() helper function and visualizing a dataset fed into it. In our goal of tweet sentiment analysis, this function will build a dictionary where we can lookup how many times a word appears in the lists of positive or negative tweets. This will be very helpful when extracting the features of the dataset in the week‚Äôs programming assignment. Let‚Äôs see how this function is implemented under the hood in this notebook.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "L2 Frequencies"
    ]
  },
  {
    "objectID": "posts/c1w1/lab02.html#setup",
    "href": "posts/c1w1/lab02.html#setup",
    "title": "Building and Visualizing word frequencies",
    "section": "Setup",
    "text": "Setup\nLet‚Äôs import the required libraries for this lab:\n\nimport nltk                                  # Python library for NLP\nfrom nltk.corpus import twitter_samples      # sample Twitter dataset from NLTK\nimport matplotlib.pyplot as plt              # visualization library\nimport numpy as np                           # library for scientific computing and matrix operations\n\n\nImport some helper functions that we provided in the utils.py file:\n\nprocess_tweet(): Cleans the text, tokenizes it into separate words, removes stopwords, and converts words to stems.\nbuild_freqs(): This counts how often a word in the ‚Äòcorpus‚Äô (the entire set of tweets) was associated with a positive label 1 or a negative label 0. It then builds the freqs dictionary, where each key is a (word,label) tuple, and the value is the count of its frequency within the corpus of tweets.\n\n\n# download the stopwords for the process_tweet function\nnltk.download('stopwords')\n\n# import our convenience functions\nfrom utils import process_tweet, build_freqs\n\n[nltk_data] Downloading package stopwords to /home/oren/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "L2 Frequencies"
    ]
  },
  {
    "objectID": "posts/c1w1/lab02.html#load-the-nltk-sample-dataset",
    "href": "posts/c1w1/lab02.html#load-the-nltk-sample-dataset",
    "title": "Building and Visualizing word frequencies",
    "section": "Load the NLTK sample dataset",
    "text": "Load the NLTK sample dataset\nAs in the previous lab, we will be using the Twitter dataset from NLTK.\n\n# select the lists of positive and negative tweets\nall_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')\n\n# concatenate the lists, 1st part is the positive tweets followed by the negative\ntweets = all_positive_tweets + all_negative_tweets\n\n# let's see how many tweets we have\nprint(\"Number of tweets: \", len(tweets))\n\nNumber of tweets:  10000\n\n\nNext, we will build a labels array that matches the sentiments of our tweets. This data type works pretty much like a regular list but is optimized for computations and manipulation. The labels array will be composed of 10000 elements. The first 5000 will be filled with 1 labels denoting positive sentiments, and the next 5000 will be 0 labels denoting the opposite. We can do this easily with a series of operations provided by the numpy library:\n\nnp.ones() - create an array of 1‚Äôs\nnp.zeros() - create an array of 0‚Äôs\nnp.append() - concatenate arrays\n\n\n# make a numpy array representing labels of the tweets\nlabels = np.append(np.ones((len(all_positive_tweets))), np.zeros((len(all_negative_tweets))))",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "L2 Frequencies"
    ]
  },
  {
    "objectID": "posts/c1w1/lab02.html#dictionaries",
    "href": "posts/c1w1/lab02.html#dictionaries",
    "title": "Building and Visualizing word frequencies",
    "section": "Dictionaries",
    "text": "Dictionaries\nIn Python, a dictionary is a mutable and indexed collection. It stores items as key-value pairs and uses hash tables underneath to allow practically constant time lookups. In NLP, dictionaries are essential because it enables fast retrieval of items or containment checks even with thousands of entries in the collection.\n\nDefinition\nA dictionary in Python is declared using curly brackets. Look at the next example:\n\ndictionary = {'key1': 1, 'key2': 2}\n\nThe former line defines a dictionary with two entries. Keys and values can be almost any type (with a few restriction on keys), and in this case, we used strings. We can also use floats, integers, tuples, etc.\n\n\nAdding or editing entries\nNew entries can be inserted into dictionaries using square brackets. If the dictionary already contains the specified key, its value is overwritten.\n\n# Add a new entry\ndictionary['key3'] = -5\n\n# Overwrite the value of key1\ndictionary['key1'] = 0\n\nprint(dictionary)\n\n{'key1': 0, 'key2': 2, 'key3': -5}\n\n\n\n\nAccessing values and lookup keys\nPerforming dictionary lookups and retrieval are common tasks in NLP. There are two ways to do this:\n\nUsing square bracket notation: This form is allowed if the lookup key is in the dictionary. It produces an error otherwise.\nUsing the get() method: This allows us to set a default value if the dictionary key does not exist.\n\nLet us see these in action:\n\n# Square bracket lookup when the key exist\nprint(dictionary['key2'])\n\n2\n\n\nHowever, if the key is missing, the operation produce an error\n\n# The output of this line is intended to produce a KeyError\nprint(dictionary['key8'])\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nCell In[8], line 2\n      1 # The output of this line is intended to produce a KeyError\n----&gt; 2 print(dictionary['key8'])\n\nKeyError: 'key8'\n\n\n\nWhen using a square bracket lookup, it is common to use an if-else block to check for containment first (with the keyword in) before getting the item. On the other hand, you can use the .get() method if you want to set a default value when the key is not found. Let‚Äôs compare these in the cells below:\n\n# This prints a value\nif 'key1' in dictionary:\n    print(\"item found: \", dictionary['key1'])\nelse:\n    print('key1 is not defined')\n\n# Same as what you get with get\nprint(\"item found: \", dictionary.get('key1', -1))\n\nitem found:  0\nitem found:  0\n\n\n\n# This prints a message because the key is not found\nif 'key7' in dictionary:\n    print(dictionary['key7'])\nelse:\n    print('key does not exist!')\n\n# This prints -1 because the key is not found and we set the default to -1\nprint(dictionary.get('key7', -1))\n\nkey does not exist!\n-1",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "L2 Frequencies"
    ]
  },
  {
    "objectID": "posts/c1w1/lab02.html#word-frequency-dictionary",
    "href": "posts/c1w1/lab02.html#word-frequency-dictionary",
    "title": "Building and Visualizing word frequencies",
    "section": "Word frequency dictionary",
    "text": "Word frequency dictionary\nNow that we know the building blocks, let‚Äôs finally take a look at the build_freqs() function in utils.py. This is the function that creates the dictionary containing the word counts from each corpus.\ndef build_freqs(tweets, ys):\n    \"\"\"Build frequencies.\n    Input:\n        tweets: a list of tweets\n        ys: an m x 1 array with the sentiment label of each tweet\n            (either 0 or 1)\n    Output:\n        freqs: a dictionary mapping each (word, sentiment) pair to its\n        frequency\n    \"\"\"\n    # Convert np array to list since zip needs an iterable.\n    # The squeeze is necessary or the list ends up with one element.\n    # Also note that this is just a NOP if ys is already a list.\n    yslist = np.squeeze(ys).tolist()\n\n    # Start with an empty dictionary and populate it by looping over all tweets\n    # and over all processed words in each tweet.\n    freqs = {}\n    for y, tweet in zip(yslist, tweets):\n        for word in process_tweet(tweet):\n            pair = (word, y)\n            if pair in freqs:\n                freqs[pair] += 1\n            else:\n                freqs[pair] = 1    \n    return freqs\nYou can also do the for loop like this to make it a bit more compact:\n    for y, tweet in zip(yslist, tweets):\n        for word in process_tweet(tweet):\n            pair = (word, y)\n            freqs[pair] = freqs.get(pair, 0) + 1\nAs shown above, each key is a 2-element tuple containing a (word, y) pair. The word is an element in a processed tweet while y is an integer representing the corpus: 1 for the positive tweets and 0 for the negative tweets. The value associated with this key is the number of times that word appears in the specified corpus. For example:\n# \"folowfriday\" appears 25 times in the positive tweets\n('followfriday', 1.0): 25\n\n# \"shame\" appears 19 times in the negative tweets\n('shame', 0.0): 19 \nNow, it is time to use the dictionary returned by the build_freqs() function. First, let us feed our tweets and labels lists then print a basic report:\n\n# create frequency dictionary\nfreqs = build_freqs(tweets, labels)\n\n# check data type\nprint(f'type(freqs) = {type(freqs)}')\n\n# check length of the dictionary\nprint(f'len(freqs) = {len(freqs)}')\n\ntype(freqs) = &lt;class 'dict'&gt;\nlen(freqs) = 13065\n\n\nNow print the frequency of each word depending on its class.\n\nprint(freqs)\n\n{('followfriday', 1.0): 25, ('top', 1.0): 32, ('engag', 1.0): 7, ('member', 1.0): 16, ('commun', 1.0): 33, ('week', 1.0): 83, (':)', 1.0): 3568, ('hey', 1.0): 76, ('jame', 1.0): 7, ('odd', 1.0): 2, (':/', 1.0): 5, ('pleas', 1.0): 97, ('call', 1.0): 37, ('contact', 1.0): 7, ('centr', 1.0): 2, ('02392441234', 1.0): 1, ('abl', 1.0): 8, ('assist', 1.0): 1, ('mani', 1.0): 33, ('thank', 1.0): 620, ('listen', 1.0): 16, ('last', 1.0): 47, ('night', 1.0): 68, ('bleed', 1.0): 2, ('amaz', 1.0): 51, ('track', 1.0): 5, ('scotland', 1.0): 2, ('congrat', 1.0): 21, ('yeaaah', 1.0): 1, ('yipppi', 1.0): 1, ('accnt', 1.0): 2, ('verifi', 1.0): 2, ('rqst', 1.0): 1, ('succeed', 1.0): 1, ('got', 1.0): 69, ('blue', 1.0): 9, ('tick', 1.0): 1, ('mark', 1.0): 1, ('fb', 1.0): 6, ('profil', 1.0): 2, ('15', 1.0): 5, ('day', 1.0): 246, ('one', 1.0): 129, ('irresist', 1.0): 2, ('flipkartfashionfriday', 1.0): 17, ('like', 1.0): 233, ('keep', 1.0): 68, ('love', 1.0): 400, ('custom', 1.0): 4, ('wait', 1.0): 70, ('long', 1.0): 36, ('hope', 1.0): 141, ('enjoy', 1.0): 75, ('happi', 1.0): 211, ('friday', 1.0): 116, ('lwwf', 1.0): 1, ('second', 1.0): 10, ('thought', 1.0): 29, ('‚Äô', 1.0): 21, ('enough', 1.0): 18, ('time', 1.0): 127, ('dd', 1.0): 1, ('new', 1.0): 143, ('short', 1.0): 7, ('enter', 1.0): 9, ('system', 1.0): 2, ('sheep', 1.0): 1, ('must', 1.0): 18, ('buy', 1.0): 11, ('jgh', 1.0): 4, ('go', 1.0): 148, ('bayan', 1.0): 1, (':d', 1.0): 629, ('bye', 1.0): 7, ('act', 1.0): 8, ('mischiev', 1.0): 1, ('etl', 1.0): 1, ('layer', 1.0): 1, ('in-hous', 1.0): 1, ('wareh', 1.0): 1, ('app', 1.0): 16, ('katamari', 1.0): 1, ('well', 1.0): 81, ('‚Ä¶', 1.0): 38, ('name', 1.0): 18, ('impli', 1.0): 1, (':p', 1.0): 138, ('influenc', 1.0): 18, ('big', 1.0): 33, ('...', 1.0): 289, ('juici', 1.0): 3, ('selfi', 1.0): 12, ('follow', 1.0): 381, ('perfect', 1.0): 24, ('alreadi', 1.0): 28, ('know', 1.0): 145, (\"what'\", 1.0): 17, ('great', 1.0): 171, ('opportun', 1.0): 23, ('junior', 1.0): 2, ('triathlet', 1.0): 1, ('age', 1.0): 2, ('12', 1.0): 5, ('13', 1.0): 6, ('gatorad', 1.0): 1, ('seri', 1.0): 5, ('get', 1.0): 206, ('entri', 1.0): 4, ('lay', 1.0): 4, ('greet', 1.0): 5, ('card', 1.0): 8, ('rang', 1.0): 3, ('print', 1.0): 3, ('today', 1.0): 108, ('job', 1.0): 41, (':-)', 1.0): 692, (\"friend'\", 1.0): 3, ('lunch', 1.0): 5, ('yummm', 1.0): 1, ('nostalgia', 1.0): 1, ('tb', 1.0): 2, ('ku', 1.0): 1, ('id', 1.0): 8, ('conflict', 1.0): 1, ('help', 1.0): 41, (\"here'\", 1.0): 25, ('screenshot', 1.0): 3, ('work', 1.0): 110, ('hi', 1.0): 173, ('liv', 1.0): 2, ('hello', 1.0): 59, ('need', 1.0): 78, ('someth', 1.0): 28, ('u', 1.0): 175, ('fm', 1.0): 2, ('twitter', 1.0): 29, ('‚Äî', 1.0): 27, ('sure', 1.0): 58, ('thing', 1.0): 69, ('dm', 1.0): 39, ('x', 1.0): 72, (\"i'v\", 1.0): 35, ('heard', 1.0): 9, ('four', 1.0): 5, ('season', 1.0): 9, ('pretti', 1.0): 20, ('dope', 1.0): 2, ('penthous', 1.0): 1, ('obv', 1.0): 1, ('gobigorgohom', 1.0): 1, ('fun', 1.0): 58, (\"y'all\", 1.0): 3, ('yeah', 1.0): 47, ('suppos', 1.0): 7, ('lol', 1.0): 64, ('chat', 1.0): 13, ('bit', 1.0): 20, ('youth', 1.0): 19, ('üíÖüèΩ', 1.0): 1, ('üíã', 1.0): 2, ('seen', 1.0): 10, ('year', 1.0): 43, ('rest', 1.0): 12, ('goe', 1.0): 7, ('quickli', 1.0): 3, ('bed', 1.0): 16, ('music', 1.0): 21, ('fix', 1.0): 10, ('dream', 1.0): 20, ('spiritu', 1.0): 1, ('ritual', 1.0): 1, ('festiv', 1.0): 8, ('n√©pal', 1.0): 1, ('begin', 1.0): 4, ('line-up', 1.0): 4, ('left', 1.0): 13, ('see', 1.0): 184, ('sarah', 1.0): 4, ('send', 1.0): 22, ('us', 1.0): 109, ('email', 1.0): 26, ('bitsy@bitdefender.com', 1.0): 1, (\"we'll\", 1.0): 20, ('asap', 1.0): 5, ('kik', 1.0): 22, ('hatessuc', 1.0): 1, ('32429', 1.0): 1, ('kikm', 1.0): 1, ('lgbt', 1.0): 2, ('tinder', 1.0): 1, ('nsfw', 1.0): 1, ('akua', 1.0): 1, ('cumshot', 1.0): 1, ('come', 1.0): 70, ('hous', 1.0): 7, ('nsn_supplement', 1.0): 1, ('effect', 1.0): 4, ('press', 1.0): 1, ('releas', 1.0): 11, ('distribut', 1.0): 1, ('result', 1.0): 2, ('link', 1.0): 18, ('remov', 1.0): 3, ('pressreleas', 1.0): 1, ('newsdistribut', 1.0): 1, ('bam', 1.0): 44, ('bestfriend', 1.0): 50, ('lot', 1.0): 87, ('warsaw', 1.0): 44, ('&lt;3', 1.0): 134, ('x46', 1.0): 1, ('everyon', 1.0): 58, ('watch', 1.0): 46, ('documentari', 1.0): 1, ('earthl', 1.0): 2, ('youtub', 1.0): 13, ('support', 1.0): 27, ('buuut', 1.0): 1, ('oh', 1.0): 53, ('look', 1.0): 137, ('forward', 1.0): 29, ('visit', 1.0): 30, ('next', 1.0): 48, ('letsgetmessi', 1.0): 1, ('jo', 1.0): 1, ('make', 1.0): 99, ('feel', 1.0): 46, ('better', 1.0): 52, ('never', 1.0): 36, ('anyon', 1.0): 11, ('kpop', 1.0): 1, ('flesh', 1.0): 1, ('good', 1.0): 238, ('girl', 1.0): 44, ('best', 1.0): 65, ('wish', 1.0): 37, ('reason', 1.0): 13, ('epic', 1.0): 2, ('soundtrack', 1.0): 1, ('shout', 1.0): 12, ('ad', 1.0): 14, ('video', 1.0): 34, ('playlist', 1.0): 5, ('would', 1.0): 84, ('dear', 1.0): 17, ('jordan', 1.0): 1, ('okay', 1.0): 39, ('fake', 1.0): 2, ('gameplay', 1.0): 2, (';)', 1.0): 27, ('haha', 1.0): 53, ('im', 1.0): 51, ('kid', 1.0): 18, ('stuff', 1.0): 13, ('exactli', 1.0): 6, ('product', 1.0): 12, ('line', 1.0): 6, ('etsi', 1.0): 1, ('shop', 1.0): 16, ('check', 1.0): 52, ('vacat', 1.0): 6, ('recharg', 1.0): 1, ('normal', 1.0): 6, ('charger', 1.0): 2, ('asleep', 1.0): 9, ('talk', 1.0): 45, ('sooo', 1.0): 6, ('someon', 1.0): 34, ('text', 1.0): 18, ('ye', 1.0): 77, ('bet', 1.0): 6, (\"he'll\", 1.0): 4, ('fit', 1.0): 3, ('hear', 1.0): 33, ('speech', 1.0): 1, ('piti', 1.0): 3, ('green', 1.0): 3, ('garden', 1.0): 7, ('midnight', 1.0): 1, ('sun', 1.0): 6, ('beauti', 1.0): 50, ('canal', 1.0): 1, ('dasvidaniya', 1.0): 1, ('till', 1.0): 18, ('scout', 1.0): 1, ('sg', 1.0): 1, ('futur', 1.0): 13, ('wlan', 1.0): 1, ('pro', 1.0): 5, ('confer', 1.0): 1, ('asia', 1.0): 1, ('chang', 1.0): 24, ('lollipop', 1.0): 1, ('üç≠', 1.0): 1, ('nez', 1.0): 1, ('agnezmo', 1.0): 1, ('oley', 1.0): 1, ('mama', 1.0): 1, ('stand', 1.0): 8, ('stronger', 1.0): 1, ('god', 1.0): 20, ('misti', 1.0): 1, ('babi', 1.0): 20, ('cute', 1.0): 26, ('woohoo', 1.0): 3, (\"can't\", 1.0): 43, ('sign', 1.0): 11, ('yet', 1.0): 13, ('still', 1.0): 48, ('think', 1.0): 63, ('mka', 1.0): 5, ('liam', 1.0): 8, ('access', 1.0): 3, ('welcom', 1.0): 73, ('stat', 1.0): 60, ('arriv', 1.0): 67, ('1', 1.0): 75, ('unfollow', 1.0): 63, ('via', 1.0): 69, ('surpris', 1.0): 10, ('figur', 1.0): 5, ('happybirthdayemilybett', 1.0): 1, ('sweet', 1.0): 19, ('talent', 1.0): 5, ('2', 1.0): 58, ('plan', 1.0): 27, ('drain', 1.0): 1, ('gotta', 1.0): 5, ('timezon', 1.0): 1, ('parent', 1.0): 5, ('proud', 1.0): 12, ('least', 1.0): 16, ('mayb', 1.0): 18, ('sometim', 1.0): 13, ('grade', 1.0): 4, ('al', 1.0): 4, ('grand', 1.0): 4, ('manila_bro', 1.0): 2, ('chosen', 1.0): 1, ('let', 1.0): 68, ('around', 1.0): 17, ('..', 1.0): 128, ('side', 1.0): 15, ('world', 1.0): 27, ('eh', 1.0): 2, ('take', 1.0): 43, ('care', 1.0): 18, ('final', 1.0): 30, ('fuck', 1.0): 26, ('weekend', 1.0): 75, ('real', 1.0): 21, ('x45', 1.0): 1, ('join', 1.0): 23, ('hushedcallwithfraydo', 1.0): 1, ('gift', 1.0): 8, ('yeahhh', 1.0): 1, ('hushedpinwithsammi', 1.0): 2, ('event', 1.0): 8, ('might', 1.0): 27, ('luv', 1.0): 6, ('realli', 1.0): 79, ('appreci', 1.0): 31, ('share', 1.0): 46, ('wow', 1.0): 22, ('tom', 1.0): 5, ('gym', 1.0): 4, ('monday', 1.0): 9, ('invit', 1.0): 17, ('scope', 1.0): 5, ('friend', 1.0): 61, ('nude', 1.0): 2, ('sleep', 1.0): 45, ('birthday', 1.0): 74, ('want', 1.0): 96, ('t-shirt', 1.0): 3, ('cool', 1.0): 38, ('haw', 1.0): 1, ('phela', 1.0): 1, ('mom', 1.0): 10, ('obvious', 1.0): 2, ('princ', 1.0): 1, ('charm', 1.0): 1, ('stage', 1.0): 2, ('luck', 1.0): 30, ('tyler', 1.0): 2, ('hipster', 1.0): 1, ('glass', 1.0): 5, ('marti', 1.0): 2, ('glad', 1.0): 43, ('done', 1.0): 54, ('afternoon', 1.0): 10, ('read', 1.0): 34, ('kahfi', 1.0): 1, ('finish', 1.0): 17, ('ohmyg', 1.0): 1, ('yaya', 1.0): 3, ('dub', 1.0): 2, ('stalk', 1.0): 2, ('ig', 1.0): 3, ('gondooo', 1.0): 1, ('moo', 1.0): 2, ('tologooo', 1.0): 1, ('becom', 1.0): 10, ('detail', 1.0): 10, ('zzz', 1.0): 1, ('xx', 1.0): 42, ('physiotherapi', 1.0): 1, ('hashtag', 1.0): 5, ('üí™', 1.0): 1, ('monica', 1.0): 1, ('miss', 1.0): 27, ('sound', 1.0): 23, ('morn', 1.0): 101, (\"that'\", 1.0): 67, ('x43', 1.0): 1, ('definit', 1.0): 23, ('tri', 1.0): 44, ('tonight', 1.0): 20, ('took', 1.0): 8, ('advic', 1.0): 6, ('treviso', 1.0): 1, ('concert', 1.0): 24, ('citi', 1.0): 27, ('countri', 1.0): 23, (\"i'll\", 1.0): 90, ('start', 1.0): 61, ('fine', 1.0): 10, ('gorgeou', 1.0): 12, ('xo', 1.0): 2, ('oven', 1.0): 3, ('roast', 1.0): 2, ('garlic', 1.0): 1, ('oliv', 1.0): 1, ('oil', 1.0): 4, ('dri', 1.0): 5, ('tomato', 1.0): 1, ('basil', 1.0): 1, ('centuri', 1.0): 1, ('tuna', 1.0): 1, ('right', 1.0): 47, ('back', 1.0): 98, ('atchya', 1.0): 1, ('even', 1.0): 35, ('almost', 1.0): 10, ('chanc', 1.0): 6, ('cheer', 1.0): 20, ('po', 1.0): 4, ('ice', 1.0): 6, ('cream', 1.0): 6, ('agre', 1.0): 16, ('100', 1.0): 8, ('heheheh', 1.0): 2, ('that', 1.0): 13, ('point', 1.0): 13, ('stay', 1.0): 25, ('home', 1.0): 31, ('soon', 1.0): 47, ('promis', 1.0): 6, ('web', 1.0): 4, ('whatsapp', 1.0): 5, ('volta', 1.0): 1, ('funcionar', 1.0): 1, ('com', 1.0): 2, ('iphon', 1.0): 7, ('jailbroken', 1.0): 1, ('later', 1.0): 16, ('34', 1.0): 3, ('min', 1.0): 9, ('leia', 1.0): 1, ('appear', 1.0): 3, ('hologram', 1.0): 1, ('r2d2', 1.0): 1, ('w', 1.0): 18, ('messag', 1.0): 10, ('obi', 1.0): 1, ('wan', 1.0): 3, ('sit', 1.0): 8, ('luke', 1.0): 6, ('inter', 1.0): 1, ('3', 1.0): 31, ('ucl', 1.0): 1, ('arsen', 1.0): 2, ('small', 1.0): 4, ('team', 1.0): 29, ('pass', 1.0): 12, ('üöÇ', 1.0): 1, ('dewsburi', 1.0): 2, ('railway', 1.0): 1, ('station', 1.0): 4, ('dew', 1.0): 1, ('west', 1.0): 3, ('yorkshir', 1.0): 2, ('430', 1.0): 1, ('smh', 1.0): 2, ('9:25', 1.0): 1, ('live', 1.0): 26, ('strang', 1.0): 4, ('imagin', 1.0): 5, ('megan', 1.0): 1, ('masaantoday', 1.0): 6, ('a4', 1.0): 3, ('shweta', 1.0): 1, ('tripathi', 1.0): 1, ('5', 1.0): 17, ('20', 1.0): 6, ('kurta', 1.0): 3, ('half', 1.0): 7, ('number', 1.0): 13, ('wsalelov', 1.0): 16, ('ah', 1.0): 13, ('larri', 1.0): 3, ('anyway', 1.0): 16, ('kinda', 1.0): 13, ('goood', 1.0): 4, ('life', 1.0): 49, ('enn', 1.0): 1, ('could', 1.0): 32, ('warmup', 1.0): 1, ('15th', 1.0): 2, ('bath', 1.0): 7, ('dum', 1.0): 2, ('andar', 1.0): 1, ('ram', 1.0): 1, ('sampath', 1.0): 1, ('sona', 1.0): 1, ('mohapatra', 1.0): 1, ('samantha', 1.0): 1, ('edward', 1.0): 1, ('mein', 1.0): 1, ('tulan', 1.0): 1, ('razi', 1.0): 2, ('wah', 1.0): 2, ('josh', 1.0): 1, ('alway', 1.0): 67, ('smile', 1.0): 62, ('pictur', 1.0): 12, ('16.20', 1.0): 1, ('giveitup', 1.0): 1, ('given', 1.0): 3, ('ga', 1.0): 3, ('subsidi', 1.0): 1, ('initi', 1.0): 4, ('propos', 1.0): 3, ('delight', 1.0): 7, ('yesterday', 1.0): 7, ('x42', 1.0): 1, ('lmaoo', 1.0): 2, ('song', 1.0): 22, ('ever', 1.0): 23, ('shall', 1.0): 6, ('littl', 1.0): 31, ('throwback', 1.0): 3, ('outli', 1.0): 1, ('island', 1.0): 5, ('cheung', 1.0): 1, ('chau', 1.0): 1, ('mui', 1.0): 1, ('wo', 1.0): 1, ('total', 1.0): 9, ('differ', 1.0): 11, ('kfckitchentour', 1.0): 2, ('kitchen', 1.0): 4, ('clean', 1.0): 1, (\"i'm\", 1.0): 183, ('cusp', 1.0): 1, ('test', 1.0): 7, ('water', 1.0): 8, ('reward', 1.0): 1, ('arummzz', 1.0): 2, (\"let'\", 1.0): 23, ('drive', 1.0): 11, ('travel', 1.0): 20, ('yogyakarta', 1.0): 3, ('jeep', 1.0): 3, ('indonesia', 1.0): 4, ('instamood', 1.0): 3, ('wanna', 1.0): 30, ('skype', 1.0): 3, ('may', 1.0): 22, ('nice', 1.0): 98, ('friendli', 1.0): 2, ('pretend', 1.0): 2, ('film', 1.0): 9, ('congratul', 1.0): 15, ('winner', 1.0): 4, ('cheesydelight', 1.0): 1, ('contest', 1.0): 6, ('address', 1.0): 10, ('guy', 1.0): 60, ('market', 1.0): 5, ('24/7', 1.0): 1, ('14', 1.0): 1, ('hour', 1.0): 27, ('leav', 1.0): 12, ('without', 1.0): 12, ('delay', 1.0): 2, ('actual', 1.0): 19, ('easi', 1.0): 9, ('guess', 1.0): 14, ('train', 1.0): 10, ('wd', 1.0): 1, ('shift', 1.0): 5, ('engin', 1.0): 2, ('etc', 1.0): 2, ('sunburn', 1.0): 1, ('peel', 1.0): 2, ('blog', 1.0): 31, ('huge', 1.0): 11, ('warm', 1.0): 6, ('‚òÜ', 1.0): 3, ('complet', 1.0): 11, ('triangl', 1.0): 2, ('northern', 1.0): 1, ('ireland', 1.0): 2, ('sight', 1.0): 1, ('smthng', 1.0): 2, ('fr', 1.0): 3, ('hug', 1.0): 13, ('xoxo', 1.0): 3, ('uu', 1.0): 1, ('jaann', 1.0): 1, ('topnewfollow', 1.0): 2, ('connect', 1.0): 14, ('wonder', 1.0): 35, ('made', 1.0): 53, ('fluffi', 1.0): 1, ('insid', 1.0): 8, ('pirouett', 1.0): 1, ('moos', 1.0): 1, ('trip', 1.0): 14, ('philli', 1.0): 1, ('decemb', 1.0): 3, (\"i'd\", 1.0): 20, ('dude', 1.0): 6, ('x41', 1.0): 1, ('question', 1.0): 17, ('flaw', 1.0): 1, ('pain', 1.0): 9, ('negat', 1.0): 1, ('strength', 1.0): 3, ('went', 1.0): 12, ('solo', 1.0): 4, ('move', 1.0): 12, ('fav', 1.0): 13, ('nirvana', 1.0): 1, ('smell', 1.0): 2, ('teen', 1.0): 3, ('spirit', 1.0): 3, ('rip', 1.0): 3, ('ami', 1.0): 4, ('winehous', 1.0): 1, ('coupl', 1.0): 9, ('tomhiddleston', 1.0): 1, ('elizabetholsen', 1.0): 1, ('yaytheylookgreat', 1.0): 1, ('goodnight', 1.0): 24, ('vid', 1.0): 11, ('wake', 1.0): 12, ('gonna', 1.0): 21, ('shoot', 1.0): 6, ('itti', 1.0): 2, ('bitti', 1.0): 2, ('teeni', 1.0): 2, ('bikini', 1.0): 3, ('much', 1.0): 89, ('4th', 1.0): 4, ('togeth', 1.0): 7, ('end', 1.0): 20, ('xfile', 1.0): 1, ('content', 1.0): 4, ('rain', 1.0): 21, ('fabul', 1.0): 5, ('fantast', 1.0): 13, ('‚ô°', 1.0): 20, ('jb', 1.0): 1, ('forev', 1.0): 5, ('belieb', 1.0): 3, ('nighti', 1.0): 1, ('bug', 1.0): 3, ('bite', 1.0): 1, ('bracelet', 1.0): 2, ('idea', 1.0): 26, ('foundri', 1.0): 1, ('game', 1.0): 27, ('sens', 1.0): 7, ('pic', 1.0): 27, ('ef', 1.0): 1, ('phone', 1.0): 19, ('woot', 1.0): 2, ('derek', 1.0): 1, ('use', 1.0): 44, ('parkshar', 1.0): 1, ('gloucestershir', 1.0): 1, ('aaaahhh', 1.0): 1, ('man', 1.0): 23, ('traffic', 1.0): 2, ('stress', 1.0): 8, ('reliev', 1.0): 1, (\"how'r\", 1.0): 1, ('arbeloa', 1.0): 1, ('turn', 1.0): 15, ('17', 1.0): 4, ('omg', 1.0): 15, ('say', 1.0): 61, ('europ', 1.0): 1, ('rise', 1.0): 2, ('find', 1.0): 23, ('hard', 1.0): 12, ('believ', 1.0): 9, ('uncount', 1.0): 1, ('coz', 1.0): 3, ('unlimit', 1.0): 1, ('cours', 1.0): 18, ('teamposit', 1.0): 1, ('aldub', 1.0): 2, ('‚òï', 1.0): 3, ('rita', 1.0): 2, ('info', 1.0): 13, (\"we'd\", 1.0): 4, ('way', 1.0): 46, ('boy', 1.0): 21, ('x40', 1.0): 1, ('true', 1.0): 22, ('sethi', 1.0): 2, ('high', 1.0): 7, ('exe', 1.0): 1, ('skeem', 1.0): 1, ('saam', 1.0): 1, ('peopl', 1.0): 48, ('polit', 1.0): 2, ('izzat', 1.0): 1, ('wese', 1.0): 1, ('trust', 1.0): 9, ('khawateen', 1.0): 1, ('k', 1.0): 9, ('sath', 1.0): 2, ('mana', 1.0): 1, ('kar', 1.0): 1, ('deya', 1.0): 1, ('sort', 1.0): 9, ('smart', 1.0): 5, ('hair', 1.0): 12, ('tbh', 1.0): 5, ('jacob', 1.0): 2, ('g', 1.0): 10, ('upgrad', 1.0): 6, ('tee', 1.0): 2, ('famili', 1.0): 19, ('person', 1.0): 19, ('two', 1.0): 22, ('convers', 1.0): 6, ('onlin', 1.0): 7, ('mclaren', 1.0): 1, ('fridayfeel', 1.0): 5, ('tgif', 1.0): 10, ('squar', 1.0): 1, ('enix', 1.0): 1, ('bissmillah', 1.0): 1, ('ya', 1.0): 23, ('allah', 1.0): 3, (\"we'r\", 1.0): 29, ('socent', 1.0): 1, ('startup', 1.0): 2, ('drop', 1.0): 9, ('your', 1.0): 3, ('arnd', 1.0): 1, ('town', 1.0): 5, ('basic', 1.0): 4, ('piss', 1.0): 3, ('cup', 1.0): 4, ('also', 1.0): 35, ('terribl', 1.0): 2, ('complic', 1.0): 1, ('discuss', 1.0): 3, ('snapchat', 1.0): 36, ('lynettelow', 1.0): 1, ('kikmenow', 1.0): 3, ('snapm', 1.0): 2, ('hot', 1.0): 24, ('amazon', 1.0): 1, ('kikmeguy', 1.0): 3, ('defin', 1.0): 2, ('grow', 1.0): 7, ('sport', 1.0): 4, ('rt', 1.0): 12, ('rakyat', 1.0): 1, ('write', 1.0): 13, ('sinc', 1.0): 15, ('mention', 1.0): 24, ('fli', 1.0): 5, ('fish', 1.0): 3, ('promot', 1.0): 5, ('post', 1.0): 21, ('cyber', 1.0): 1, ('ourdaughtersourprid', 1.0): 5, ('mypapamyprid', 1.0): 2, ('papa', 1.0): 2, ('coach', 1.0): 2, ('posit', 1.0): 8, ('kha', 1.0): 1, ('atleast', 1.0): 2, ('x39', 1.0): 1, ('mango', 1.0): 1, (\"lassi'\", 1.0): 1, (\"monty'\", 1.0): 1, ('marvel', 1.0): 2, ('though', 1.0): 19, ('suspect', 1.0): 3, ('meant', 1.0): 3, ('24', 1.0): 4, ('hr', 1.0): 2, ('touch', 1.0): 15, ('kepler', 1.0): 4, ('452b', 1.0): 5, ('chalna', 1.0): 1, ('hai', 1.0): 11, ('thankyou', 1.0): 14, ('hazel', 1.0): 1, ('food', 1.0): 6, ('brooklyn', 1.0): 1, ('pta', 1.0): 2, ('awak', 1.0): 10, ('okayi', 1.0): 2, ('awww', 1.0): 15, ('ha', 1.0): 23, ('doc', 1.0): 1, ('splendid', 1.0): 1, ('spam', 1.0): 1, ('folder', 1.0): 1, ('amount', 1.0): 1, ('nigeria', 1.0): 1, ('claim', 1.0): 1, ('rted', 1.0): 1, ('leg', 1.0): 5, ('hurt', 1.0): 8, ('bad', 1.0): 18, ('mine', 1.0): 14, ('saturday', 1.0): 8, ('thaaank', 1.0): 1, ('puhon', 1.0): 1, ('happinesss', 1.0): 1, ('tnc', 1.0): 1, ('prior', 1.0): 1, ('notif', 1.0): 2, ('fat', 1.0): 1, ('co', 1.0): 1, ('probabl', 1.0): 9, ('ate', 1.0): 4, ('yuna', 1.0): 2, ('tamesid', 1.0): 1, ('¬¥', 1.0): 3, ('googl', 1.0): 6, ('account', 1.0): 19, ('scouser', 1.0): 1, ('everyth', 1.0): 13, ('zoe', 1.0): 2, ('mate', 1.0): 7, ('liter', 1.0): 6, (\"they'r\", 1.0): 12, ('samee', 1.0): 1, ('edgar', 1.0): 1, ('updat', 1.0): 13, ('log', 1.0): 4, ('bring', 1.0): 17, ('abe', 1.0): 1, ('meet', 1.0): 34, ('x38', 1.0): 1, ('sigh', 1.0): 3, ('dreamili', 1.0): 1, ('pout', 1.0): 1, ('eye', 1.0): 14, ('quacketyquack', 1.0): 7, ('funni', 1.0): 19, ('happen', 1.0): 16, ('phil', 1.0): 1, ('em', 1.0): 3, ('del', 1.0): 1, ('rodder', 1.0): 1, ('els', 1.0): 10, ('play', 1.0): 46, ('newest', 1.0): 1, ('gamejam', 1.0): 1, ('irish', 1.0): 2, ('literatur', 1.0): 2, ('inaccess', 1.0): 2, (\"kareena'\", 1.0): 2, ('fan', 1.0): 30, ('brain', 1.0): 13, ('dot', 1.0): 11, ('braindot', 1.0): 11, ('fair', 1.0): 5, ('rush', 1.0): 1, ('either', 1.0): 11, ('brandi', 1.0): 1, ('18', 1.0): 5, ('carniv', 1.0): 1, ('men', 1.0): 10, ('put', 1.0): 17, ('mask', 1.0): 3, ('xavier', 1.0): 1, ('forneret', 1.0): 1, ('jennif', 1.0): 1, ('site', 1.0): 9, ('free', 1.0): 37, ('50.000', 1.0): 3, ('8', 1.0): 10, ('ball', 1.0): 7, ('pool', 1.0): 5, ('coin', 1.0): 5, ('edit', 1.0): 7, ('trish', 1.0): 1, ('‚ô•', 1.0): 19, ('grate', 1.0): 5, ('three', 1.0): 10, ('comment', 1.0): 8, ('wakeup', 1.0): 1, ('besid', 1.0): 2, ('dirti', 1.0): 2, ('sex', 1.0): 6, ('lmaooo', 1.0): 1, ('üò§', 1.0): 2, ('loui', 1.0): 4, (\"he'\", 1.0): 11, ('throw', 1.0): 3, ('caus', 1.0): 15, ('inspir', 1.0): 7, ('ff', 1.0): 48, ('twoof', 1.0): 3, ('gr8', 1.0): 1, ('wkend', 1.0): 3, ('kind', 1.0): 24, ('exhaust', 1.0): 2, ('word', 1.0): 20, ('cheltenham', 1.0): 1, ('area', 1.0): 4, ('kale', 1.0): 1, ('crisp', 1.0): 1, ('ruin', 1.0): 5, ('x37', 1.0): 1, ('open', 1.0): 12, ('worldwid', 1.0): 2, ('outta', 1.0): 1, ('sfvbeta', 1.0): 1, ('vantast', 1.0): 1, ('xcylin', 1.0): 1, ('bundl', 1.0): 1, ('show', 1.0): 28, ('internet', 1.0): 2, ('price', 1.0): 4, ('realisticli', 1.0): 1, ('pay', 1.0): 8, ('net', 1.0): 1, ('educ', 1.0): 1, ('power', 1.0): 7, ('weapon', 1.0): 1, ('nelson', 1.0): 1, ('mandela', 1.0): 1, ('recent', 1.0): 9, ('j', 1.0): 3, ('chenab', 1.0): 1, ('flow', 1.0): 5, ('pakistan', 1.0): 2, ('incredibleindia', 1.0): 1, ('teenchoic', 1.0): 10, ('choiceinternationalartist', 1.0): 9, ('superjunior', 1.0): 9, ('caught', 1.0): 4, ('first', 1.0): 50, ('salmon', 1.0): 3, ('super-blend', 1.0): 1, ('project', 1.0): 6, ('youth@bipolaruk.org.uk', 1.0): 1, ('awesom', 1.0): 42, ('stream', 1.0): 14, ('alma', 1.0): 1, ('mater', 1.0): 1, ('highschoolday', 1.0): 1, ('clientvisit', 1.0): 1, ('faith', 1.0): 3, ('christian', 1.0): 1, ('school', 1.0): 9, ('lizaminnelli', 1.0): 1, ('upcom', 1.0): 2, ('uk', 1.0): 4, ('üòÑ', 1.0): 5, ('singl', 1.0): 6, ('hill', 1.0): 4, ('everi', 1.0): 26, ('beat', 1.0): 10, ('wrong', 1.0): 10, ('readi', 1.0): 25, ('natur', 1.0): 1, ('pefumeri', 1.0): 1, ('workshop', 1.0): 3, ('neal', 1.0): 1, ('yard', 1.0): 1, ('covent', 1.0): 1, ('tomorrow', 1.0): 40, ('fback', 1.0): 27, ('indo', 1.0): 1, ('harmo', 1.0): 1, ('americano', 1.0): 1, ('rememb', 1.0): 16, ('aww', 1.0): 10, ('head', 1.0): 14, ('saw', 1.0): 19, ('dark', 1.0): 6, ('handshom', 1.0): 1, ('juga', 1.0): 1, ('hurray', 1.0): 1, ('hate', 1.0): 13, ('cant', 1.0): 15, ('decid', 1.0): 4, ('save', 1.0): 12, ('list', 1.0): 15, ('hiya', 1.0): 4, ('exec', 1.0): 1, ('loryn.good@lincs-chamber.co.uk', 1.0): 1, ('photo', 1.0): 19, ('thx', 1.0): 15, ('4', 1.0): 24, ('china', 1.0): 2, ('homosexu', 1.0): 1, ('hyungbot', 1.0): 1, ('give', 1.0): 48, ('fam', 1.0): 5, ('mind', 1.0): 23, ('timetunnel', 1.0): 1, ('1982', 1.0): 1, ('quit', 1.0): 13, ('radio', 1.0): 5, ('set', 1.0): 11, ('heart', 1.0): 11, ('hiii', 1.0): 2, ('jack', 1.0): 3, ('ili', 1.0): 5, ('‚ú®', 1.0): 4, ('domino', 1.0): 1, ('pub', 1.0): 1, ('heat', 1.0): 1, ('prob', 1.0): 5, ('sorri', 1.0): 22, ('hastili', 1.0): 1, ('type', 1.0): 6, ('came', 1.0): 7, ('pakistani', 1.0): 1, ('x36', 1.0): 1, ('3point', 1.0): 1, ('dreamteam', 1.0): 1, ('gooo', 1.0): 2, ('bailey', 1.0): 2, ('pbb', 1.0): 4, ('737gold', 1.0): 3, ('drank', 1.0): 2, ('old', 1.0): 13, ('gotten', 1.0): 2, ('1/2', 1.0): 1, ('welsh', 1.0): 1, ('wale', 1.0): 3, ('yippe', 1.0): 1, ('üíü', 1.0): 4, ('bro', 1.0): 24, ('lord', 1.0): 4, ('michael', 1.0): 4, (\"u'r\", 1.0): 1, ('ure', 1.0): 1, ('bigot', 1.0): 1, ('usual', 1.0): 6, ('front', 1.0): 4, ('squat', 1.0): 1, ('dobar', 1.0): 1, ('dan', 1.0): 5, ('brand', 1.0): 8, ('heavi', 1.0): 2, ('musicolog', 1.0): 1, ('2015', 1.0): 16, ('spend', 1.0): 2, ('marathon', 1.0): 1, ('iflix', 1.0): 2, ('offici', 1.0): 10, ('graduat', 1.0): 3, ('cri', 1.0): 9, ('__', 1.0): 1, ('yep', 1.0): 9, ('expert', 1.0): 4, ('bisexu', 1.0): 1, ('minal', 1.0): 1, ('aidzin', 1.0): 1, ('yo', 1.0): 7, ('pi', 1.0): 1, ('cook', 1.0): 2, ('book', 1.0): 21, ('dinner', 1.0): 7, ('tough', 1.0): 2, ('choic', 1.0): 8, ('other', 1.0): 12, ('chill', 1.0): 6, ('smu', 1.0): 1, ('oval', 1.0): 1, ('basketbal', 1.0): 1, ('player', 1.0): 4, ('whahahaha', 1.0): 1, ('soamaz', 1.0): 1, ('moment', 1.0): 12, ('onto', 1.0): 3, ('a5', 1.0): 1, ('wardrob', 1.0): 2, ('user', 1.0): 3, ('teamr', 1.0): 1, ('appar', 1.0): 6, ('depend', 1.0): 2, ('greatli', 1.0): 1, ('design', 1.0): 21, ('ahhh', 1.0): 1, ('7th', 1.0): 1, ('cinepambata', 1.0): 1, ('mechan', 1.0): 1, ('form', 1.0): 4, ('download', 1.0): 10, ('ur', 1.0): 38, ('swisher', 1.0): 1, ('cop', 1.0): 1, ('ducktail', 1.0): 1, ('surreal', 1.0): 3, ('exposur', 1.0): 1, ('sotw', 1.0): 1, ('halesowen', 1.0): 1, ('blackcountryfair', 1.0): 1, ('street', 1.0): 1, ('assess', 1.0): 1, ('mental', 1.0): 4, ('bodi', 1.0): 15, ('ooz', 1.0): 1, ('appeal', 1.0): 1, ('amassiveoverdoseofship', 1.0): 1, ('latest', 1.0): 5, ('isi', 1.0): 1, ('chan', 1.0): 1, ('c', 1.0): 9, ('note', 1.0): 6, ('pkwalasawa', 1.0): 1, ('gemma', 1.0): 1, ('orlean', 1.0): 1, ('fever', 1.0): 2, ('geskenya', 1.0): 1, ('obamainkenya', 1.0): 1, ('magicalkenya', 1.0): 1, ('greatkenya', 1.0): 1, ('allgoodthingsk', 1.0): 1, ('anim', 1.0): 6, ('umaru', 1.0): 1, ('singer', 1.0): 4, ('ship', 1.0): 8, ('order', 1.0): 17, ('room', 1.0): 5, ('car', 1.0): 6, ('gone', 1.0): 5, ('hahaha', 1.0): 14, ('stori', 1.0): 11, ('relat', 1.0): 4, ('label', 1.0): 1, ('worst', 1.0): 3, ('batch', 1.0): 1, ('princip', 1.0): 1, ('due', 1.0): 3, ('march', 1.0): 1, ('wooftast', 1.0): 2, ('receiv', 1.0): 8, ('necessari', 1.0): 1, ('regret', 1.0): 4, ('rn', 1.0): 4, ('whatev', 1.0): 5, ('hat', 1.0): 1, ('success', 1.0): 6, ('abstin', 1.0): 1, ('wtf', 1.0): 3, (\"there'\", 1.0): 11, ('thrown', 1.0): 1, ('middl', 1.0): 2, ('repeat', 1.0): 3, ('relentlessli', 1.0): 1, ('approxim', 1.0): 1, ('oldschool', 1.0): 1, ('runescap', 1.0): 1, ('daaay', 1.0): 1, ('jumma_mubarik', 1.0): 1, ('frnd', 1.0): 1, ('stay_bless', 1.0): 1, ('bless', 1.0): 12, ('pussycat', 1.0): 1, ('main', 1.0): 7, ('launch', 1.0): 4, ('pretoria', 1.0): 1, ('fahrinahmad', 1.0): 1, ('tengkuaaronshah', 1.0): 1, ('eksperimencinta', 1.0): 1, ('tykk√§sin', 1.0): 1, ('videosta', 1.0): 1, ('month', 1.0): 13, ('hoodi', 1.0): 2, ('eeep', 1.0): 1, ('yay', 1.0): 16, ('sohappyrightnow', 1.0): 1, ('mmm', 1.0): 1, ('azz-set', 1.0): 1, ('babe', 1.0): 9, ('feedback', 1.0): 11, ('gain', 1.0): 6, ('valu', 1.0): 2, ('peac', 1.0): 8, ('refresh', 1.0): 5, ('manthan', 1.0): 1, ('tune', 1.0): 5, ('fresh', 1.0): 6, ('mother', 1.0): 5, ('determin', 1.0): 2, ('maxfreshmov', 1.0): 2, ('loneliest', 1.0): 1, ('tattoo', 1.0): 3, ('friday.and', 1.0): 1, ('magnific', 1.0): 2, ('e', 1.0): 5, ('achiev', 1.0): 2, ('rashmi', 1.0): 1, ('dedic', 1.0): 2, ('happyfriday', 1.0): 6, ('nearli', 1.0): 4, ('retweet', 1.0): 35, ('alert', 1.0): 1, ('da', 1.0): 5, ('dang', 1.0): 2, ('rad', 1.0): 2, ('fanart', 1.0): 1, ('massiv', 1.0): 1, ('niamh', 1.0): 1, ('fennel', 1.0): 1, ('journal', 1.0): 1, ('land', 1.0): 2, ('copi', 1.0): 5, ('past', 1.0): 7, ('tweet', 1.0): 61, ('yesss', 1.0): 5, ('ariana', 1.0): 2, ('selena', 1.0): 2, ('gomez', 1.0): 1, ('tomlinson', 1.0): 1, ('payn', 1.0): 1, ('caradelevingn', 1.0): 1, ('üå∑', 1.0): 1, ('trade', 1.0): 3, ('tire', 1.0): 5, ('nope', 1.0): 7, ('appli', 1.0): 6, ('iamca', 1.0): 1, ('found', 1.0): 15, ('afti', 1.0): 1, ('goodmorn', 1.0): 8, ('prokabaddi', 1.0): 1, ('koel', 1.0): 1, ('mallick', 1.0): 1, ('recit', 1.0): 4, ('nation', 1.0): 3, ('anthem', 1.0): 1, ('6', 1.0): 23, ('yournaturallead', 1.0): 1, ('youngnaturallead', 1.0): 1, ('mon', 1.0): 3, ('27juli', 1.0): 1, ('cumbria', 1.0): 1, ('flockstar', 1.0): 1, ('thur', 1.0): 2, ('30juli', 1.0): 1, ('itv', 1.0): 1, ('sleeptight', 1.0): 1, ('haveagoodday', 1.0): 1, ('septemb', 1.0): 5, ('perhap', 1.0): 3, ('bb', 1.0): 4, ('full', 1.0): 19, ('album', 1.0): 6, ('fulli', 1.0): 2, ('intend', 1.0): 1, ('possibl', 1.0): 7, ('attack', 1.0): 3, ('&gt;:d', 1.0): 4, ('bird', 1.0): 4, ('teamadmicro', 1.0): 1, ('fridaydownpour', 1.0): 1, ('clear', 1.0): 4, ('rohit', 1.0): 1, ('queen', 1.0): 8, ('otwolgrandtrail', 1.0): 3, ('sheer', 1.0): 1, ('fact', 1.0): 8, ('obama', 1.0): 1, ('innumer', 1.0): 1, ('presid', 1.0): 2, ('ni', 1.0): 3, ('shauri', 1.0): 1, ('yako', 1.0): 1, ('memotohat', 1.0): 1, ('sunday', 1.0): 9, ('pamper', 1.0): 2, (\"t'wa\", 1.0): 1, ('cabincrew', 1.0): 1, ('interview', 1.0): 5, ('langkawi', 1.0): 1, ('1st', 1.0): 1, ('august', 1.0): 7, ('fulfil', 1.0): 5, ('fantasi', 1.0): 6, ('üëâ', 1.0): 6, ('ex-tweleb', 1.0): 1, ('apart', 1.0): 2, ('makeov', 1.0): 1, ('brilliantli', 1.0): 1, ('happyyi', 1.0): 1, ('birthdaaayyy', 1.0): 2, ('kill', 1.0): 3, ('interest', 1.0): 20, ('internship', 1.0): 3, ('program', 1.0): 5, ('sadli', 1.0): 1, ('career', 1.0): 3, ('page', 1.0): 9, ('issu', 1.0): 10, ('sad', 1.0): 5, ('overwhelmingli', 1.0): 1, ('aha', 1.0): 2, ('beaut', 1.0): 2, ('‚ô¨', 1.0): 2, ('win', 1.0): 16, ('deo', 1.0): 1, ('faaabul', 1.0): 1, ('freebiefriday', 1.0): 4, ('aluminiumfre', 1.0): 1, ('stayfresh', 1.0): 1, ('john', 1.0): 6, ('worri', 1.0): 18, ('navig', 1.0): 1, ('thnk', 1.0): 1, ('progrmr', 1.0): 1, ('9pm', 1.0): 1, ('9am', 1.0): 2, ('hardli', 1.0): 1, ('rose', 1.0): 4, ('emot', 1.0): 3, ('poetri', 1.0): 1, ('frequentfly', 1.0): 1, ('break', 1.0): 10, ('apolog', 1.0): 4, ('kb', 1.0): 1, ('londondairi', 1.0): 1, ('icecream', 1.0): 2, ('experi', 1.0): 7, ('cover', 1.0): 9, ('sin', 1.0): 1, ('excit', 1.0): 33, (\":')\", 1.0): 2, ('xxx', 1.0): 15, ('jim', 1.0): 1, ('chuckl', 1.0): 1, ('cake', 1.0): 10, ('doh', 1.0): 1, ('500', 1.0): 2, ('subscrib', 1.0): 2, ('reach', 1.0): 1, ('scorch', 1.0): 1, ('summer', 1.0): 17, ('younger', 1.0): 4, ('woman', 1.0): 4, ('stamina', 1.0): 1, ('expect', 1.0): 6, ('anyth', 1.0): 22, ('less', 1.0): 8, ('tweeti', 1.0): 1, ('fab', 1.0): 12, ('dont', 1.0): 13, ('--&gt;', 1.0): 2, ('10', 1.0): 16, ('loner', 1.0): 3, ('introduc', 1.0): 3, ('vs', 1.0): 4, ('alter', 1.0): 1, ('understand', 1.0): 6, ('spread', 1.0): 8, ('problem', 1.0): 19, ('supa', 1.0): 1, ('dupa', 1.0): 1, ('near', 1.0): 6, ('dartmoor', 1.0): 1, ('gold', 1.0): 7, ('colour', 1.0): 4, ('ok', 1.0): 38, ('someday', 1.0): 4, ('r', 1.0): 14, ('dii', 1.0): 1, ('n', 1.0): 17, ('forget', 1.0): 17, ('si', 1.0): 4, ('smf', 1.0): 1, ('ft', 1.0): 4, ('japanes', 1.0): 3, ('import', 1.0): 5, ('kitti', 1.0): 1, ('match', 1.0): 6, ('stationari', 1.0): 1, ('draw', 1.0): 6, ('close', 1.0): 14, ('broken', 1.0): 3, ('specialis', 1.0): 4, ('thermal', 1.0): 4, ('imag', 1.0): 6, ('survey', 1.0): 4, ('‚Äì', 1.0): 14, ('south', 1.0): 2, ('korea', 1.0): 3, ('scamper', 1.0): 1, ('slept', 1.0): 4, ('alarm', 1.0): 1, (\"ain't\", 1.0): 5, ('mad', 1.0): 4, ('chweina', 1.0): 1, ('xd', 1.0): 4, ('jotzh', 1.0): 1, ('wast', 1.0): 7, ('place', 1.0): 21, ('worth', 1.0): 11, ('coat', 1.0): 3, ('beforehand', 1.0): 1, ('tho', 1.0): 12, ('foh', 1.0): 2, ('outsid', 1.0): 5, ('holiday', 1.0): 11, ('menac', 1.0): 1, ('jojo', 1.0): 2, ('ta', 1.0): 2, ('accept', 1.0): 1, ('admin', 1.0): 2, ('lukri', 1.0): 1, ('üòò', 1.0): 10, ('momma', 1.0): 2, ('bear', 1.0): 2, ('‚ù§', 1.0): 29, ('Ô∏è', 1.0): 20, ('redid', 1.0): 1, ('8th', 1.0): 1, ('v.ball', 1.0): 1, ('atm', 1.0): 4, ('build', 1.0): 8, ('pack', 1.0): 8, ('suitcas', 1.0): 2, ('hang-copi', 1.0): 1, ('translat', 1.0): 1, (\"dostoevsky'\", 1.0): 1, ('voucher', 1.0): 2, ('bugatti', 1.0): 1, ('bra', 1.0): 3, ('ŸÖÿ∑ÿπŸÖ_Ÿáÿßÿ¥ŸÖ', 1.0): 1, ('yummi', 1.0): 3, ('a7la', 1.0): 1, ('bdayt', 1.0): 1, ('mnwreeen', 1.0): 1, ('jazz', 1.0): 2, ('truck', 1.0): 1, ('x34', 1.0): 1, ('speak', 1.0): 8, ('pbevent', 1.0): 1, ('hq', 1.0): 1, ('add', 1.0): 22, ('yoona', 1.0): 1, ('hairpin', 1.0): 1, ('otp', 1.0): 1, ('collect', 1.0): 7, ('mastership', 1.0): 1, ('honey', 1.0): 4, ('paindo', 1.0): 1, ('await', 1.0): 1, ('report', 1.0): 3, ('manni', 1.0): 1, ('asshol', 1.0): 3, ('brijresid', 1.0): 1, ('structur', 1.0): 1, ('156', 1.0): 1, ('unit', 1.0): 3, ('encompass', 1.0): 1, ('bhk', 1.0): 1, ('flat', 1.0): 2, ('91', 1.0): 2, ('975-580-', 1.0): 1, ('444', 1.0): 1, ('honor', 1.0): 2, ('curri', 1.0): 2, ('clash', 1.0): 1, ('milano', 1.0): 1, ('üëå', 1.0): 1, ('followback', 1.0): 6, (':-d', 1.0): 5, ('legit', 1.0): 1, ('loser', 1.0): 5, ('gass', 1.0): 1, ('dead', 1.0): 4, ('starsquad', 1.0): 4, ('‚≠ê', 1.0): 3, ('news', 1.0): 25, ('utc', 1.0): 1, ('flume', 1.0): 1, ('kaytranada', 1.0): 1, ('alunageorg', 1.0): 1, ('ticket', 1.0): 12, ('km', 1.0): 1, ('certainti', 1.0): 1, ('solv', 1.0): 2, ('faster', 1.0): 3, ('üëä', 1.0): 1, ('hurri', 1.0): 5, ('totem', 1.0): 1, ('somewher', 1.0): 5, ('alic', 1.0): 4, ('dog', 1.0): 6, ('cat', 1.0): 5, ('goodwynsgoodi', 1.0): 1, ('ugh', 1.0): 1, ('fade', 1.0): 2, ('moan', 1.0): 1, ('leed', 1.0): 1, ('jozi', 1.0): 1, ('wasnt', 1.0): 2, ('fifth', 1.0): 2, ('avail', 1.0): 10, ('tix', 1.0): 2, ('pa', 1.0): 2, ('ba', 1.0): 2, ('ng', 1.0): 2, ('atl', 1.0): 1, ('coldplay', 1.0): 1, ('favorit', 1.0): 14, ('scientist', 1.0): 1, ('yellow', 1.0): 2, ('atla', 1.0): 1, ('yein', 1.0): 1, ('selo', 1.0): 1, ('jabongatpumaurbanstamped', 1.0): 4, ('an', 1.0): 3, ('7', 1.0): 8, ('waiter', 1.0): 1, ('bill', 1.0): 5, ('sir', 1.0): 12, ('titl', 1.0): 2, ('pocket', 1.0): 1, ('wrip', 1.0): 1, ('jean', 1.0): 1, ('conni', 1.0): 2, ('crew', 1.0): 3, ('staff', 1.0): 2, ('sweetan', 1.0): 1, ('ask', 1.0): 37, ('mum', 1.0): 2, ('beg', 1.0): 2, ('soprano', 1.0): 1, ('ukrain', 1.0): 2, ('x33', 1.0): 1, ('olli', 1.0): 2, ('disney.art', 1.0): 1, ('elmoprinssi', 1.0): 1, ('salsa', 1.0): 1, ('danc', 1.0): 2, ('tell', 1.0): 25, ('truth', 1.0): 4, ('pl', 1.0): 8, ('4-6', 1.0): 1, ('2nd', 1.0): 5, ('blogiversari', 1.0): 1, ('review', 1.0): 9, ('cuti', 1.0): 6, ('bohol', 1.0): 1, ('briliant', 1.0): 1, ('v', 1.0): 9, ('key', 1.0): 3, ('annual', 1.0): 1, ('far', 1.0): 19, ('spin', 1.0): 2, ('voic', 1.0): 3, ('\\U000fe334', 1.0): 1, ('yeheyi', 1.0): 1, ('pinya', 1.0): 1, ('whoooah', 1.0): 1, ('tranc', 1.0): 1, ('lover', 1.0): 4, ('subject', 1.0): 7, ('physic', 1.0): 1, ('stop', 1.0): 15, ('‡§¨', 1.0): 1, ('matter', 1.0): 6, ('jungl', 1.0): 1, ('accommod', 1.0): 1, ('secret', 1.0): 9, ('behind', 1.0): 3, ('sandroforceo', 1.0): 2, ('ceo', 1.0): 11, ('1month', 1.0): 11, ('swag', 1.0): 1, ('mia', 1.0): 1, ('workinprogress', 1.0): 1, ('choos', 1.0): 2, ('finnigan', 1.0): 1, ('loyal', 1.0): 2, ('royal', 1.0): 2, ('fotoset', 1.0): 1, ('reus', 1.0): 1, ('seem', 1.0): 10, ('somebodi', 1.0): 1, ('sell', 1.0): 1, ('young', 1.0): 3, ('muntu', 1.0): 1, ('anoth', 1.0): 23, ('gem', 1.0): 2, ('falco', 1.0): 1, ('supersmash', 1.0): 1, ('hotnsexi', 1.0): 1, ('friskyfriday', 1.0): 1, ('beach', 1.0): 4, ('movi', 1.0): 24, ('crop', 1.0): 2, ('nash', 1.0): 1, ('tissu', 1.0): 1, ('chocol', 1.0): 7, ('tea', 1.0): 6, ('hannib', 1.0): 3, ('episod', 1.0): 5, ('hotb', 1.0): 1, ('bush', 1.0): 2, ('classicassur', 1.0): 1, ('thrill', 1.0): 2, ('intern', 1.0): 2, ('assign', 1.0): 1, ('aerial', 1.0): 1, ('camera', 1.0): 6, ('oper', 1.0): 1, ('boom', 1.0): 3, ('hong', 1.0): 1, ('kong', 1.0): 1, ('ferri', 1.0): 1, ('central', 1.0): 2, ('girlfriend', 1.0): 4, ('after-work', 1.0): 1, ('drink', 1.0): 8, ('dj', 1.0): 3, ('resto', 1.0): 1, ('drinkt', 1.0): 1, ('koffi', 1.0): 1, ('a6', 1.0): 1, ('stargat', 1.0): 1, ('atlanti', 1.0): 1, ('muaahhh', 1.0): 1, ('ohh', 1.0): 3, ('hii', 1.0): 2, ('üôà', 1.0): 1, ('di', 1.0): 5, ('nagsend', 1.0): 1, ('yung', 1.0): 1, ('ko', 1.0): 4, ('&lt;/3', 1.0): 1, ('ulit', 1.0): 3, ('üéâ', 1.0): 5, ('üéà', 1.0): 1, ('ugli', 1.0): 4, ('legget', 1.0): 1, ('qui', 1.0): 1, ('per', 1.0): 1, ('la', 1.0): 8, ('mar', 1.0): 1, ('encourag', 1.0): 3, ('employ', 1.0): 5, ('board', 1.0): 5, ('sticker', 1.0): 1, ('sponsor', 1.0): 4, ('prize', 1.0): 3, ('(:', 1.0): 1, ('milo', 1.0): 1, ('aurini', 1.0): 1, ('juicebro', 1.0): 1, ('pillar', 1.0): 2, ('respect', 1.0): 2, ('boii', 1.0): 1, ('smashingbook', 1.0): 1, ('bibl', 1.0): 2, ('ill', 1.0): 6, ('sick', 1.0): 4, ('lamo', 1.0): 1, ('fangirl', 1.0): 3, ('platon', 1.0): 1, ('scienc', 1.0): 5, ('resid', 1.0): 2, ('servicewithasmil', 1.0): 1, ('bloodlin', 1.0): 1, ('huski', 1.0): 1, ('obituari', 1.0): 1, ('advert', 1.0): 1, ('goofingaround', 1.0): 1, ('bollywood', 1.0): 1, ('giveaway', 1.0): 6, ('dah', 1.0): 2, ('noth', 1.0): 15, ('bitter', 1.0): 2, ('anger', 1.0): 1, ('hatr', 1.0): 2, ('toward', 1.0): 2, ('pure', 1.0): 2, ('indiffer', 1.0): 1, ('suit', 1.0): 5, ('zach', 1.0): 1, ('codi', 1.0): 2, ('deliv', 1.0): 3, ('ac', 1.0): 1, ('excel', 1.0): 6, ('produc', 1.0): 1, ('boggl', 1.0): 1, ('fatigu', 1.0): 1, ('baareeq', 1.0): 1, ('gamedev', 1.0): 2, ('hobbi', 1.0): 1, ('tweenie_fox', 1.0): 1, ('click', 1.0): 3, ('accessori', 1.0): 1, ('tamang', 1.0): 1, ('hinala', 1.0): 1, ('niam', 1.0): 1, ('selfiee', 1.0): 1, ('especi', 1.0): 4, ('lass', 1.0): 1, ('ale', 1.0): 1, ('swim', 1.0): 3, ('bout', 1.0): 3, ('goodby', 1.0): 5, ('feminist', 1.0): 1, ('fought', 1.0): 1, ('snobbi', 1.0): 1, ('bitch', 1.0): 3, ('carolin', 1.0): 2, ('mighti', 1.0): 1, ('üî•', 1.0): 1, ('threw', 1.0): 2, ('hbd', 1.0): 1, ('follback', 1.0): 19, ('jog', 1.0): 1, ('remot', 1.0): 2, ('newli', 1.0): 1, ('ebay', 1.0): 2, ('store', 1.0): 15, ('disneyinfin', 1.0): 1, ('starwar', 1.0): 1, ('charact', 1.0): 3, ('preorder', 1.0): 1, ('starter', 1.0): 1, ('hit', 1.0): 13, ('snap', 1.0): 4, ('homi', 1.0): 3, ('bought', 1.0): 4, ('skin', 1.0): 8, ('bday', 1.0): 11, ('chant', 1.0): 2, ('jai', 1.0): 1, ('itali', 1.0): 2, ('fast', 1.0): 4, ('heeeyyy', 1.0): 1, ('woah', 1.0): 3, ('‚òÖ', 1.0): 5, ('üòä', 1.0): 11, ('whenev', 1.0): 4, ('ang', 1.0): 2, ('kiss', 1.0): 4, ('philippin', 1.0): 2, ('packag', 1.0): 3, ('bruis', 1.0): 1, ('rib', 1.0): 2, ('üòÄ', 1.0): 2, ('üòÅ', 1.0): 6, ('üòÇ', 1.0): 17, ('üòÉ', 1.0): 1, ('üòÖ', 1.0): 1, ('üòâ', 1.0): 2, ('tombraid', 1.0): 1, ('hype', 1.0): 1, ('thejuiceinthemix', 1.0): 1, ('rela', 1.0): 1, ('low', 1.0): 6, ('prioriti', 1.0): 1, ('harri', 1.0): 5, ('bc', 1.0): 9, ('collaps', 1.0): 2, ('chaotic', 1.0): 1, ('cosa', 1.0): 1, ('&lt;---', 1.0): 2, ('alliter', 1.0): 1, ('oppayaa', 1.0): 1, (\"how'\", 1.0): 4, ('natgeo', 1.0): 1, ('lick', 1.0): 1, ('elbow', 1.0): 2, ('. .', 1.0): 2, ('‚Äú', 1.0): 7, ('emu', 1.0): 1, ('stoke', 1.0): 1, ('woke', 1.0): 5, (\"people'\", 1.0): 3, ('approv', 1.0): 6, (\"god'\", 1.0): 2, ('jisung', 1.0): 1, ('sunshin', 1.0): 7, ('mm', 1.0): 6, ('nicola', 1.0): 1, ('brighten', 1.0): 2, ('helen', 1.0): 3, ('brian', 1.0): 3, ('2-3', 1.0): 1, ('australia', 1.0): 5, ('ol', 1.0): 2, ('bone', 1.0): 1, ('creak', 1.0): 1, ('abuti', 1.0): 1, ('tweetland', 1.0): 1, ('android', 1.0): 3, ('xma', 1.0): 2, ('skyblock', 1.0): 1, ('bcaus', 1.0): 1, ('2009', 1.0): 1, ('die', 1.0): 10, ('twitch', 1.0): 5, ('sympathi', 1.0): 1, ('laugh', 1.0): 5, ('unniee', 1.0): 1, ('nuka', 1.0): 1, ('penacova', 1.0): 1, ('djset', 1.0): 1, ('edm', 1.0): 1, ('kizomba', 1.0): 1, ('latinhous', 1.0): 1, ('housemus', 1.0): 3, ('portug', 1.0): 1, ('wild', 1.0): 2, ('ride', 1.0): 6, ('anytim', 1.0): 6, ('tast', 1.0): 5, ('yer', 1.0): 2, ('mtn', 1.0): 2, ('maganda', 1.0): 1, ('mistress', 1.0): 2, ('saphir', 1.0): 1, ('busi', 1.0): 19, ('4000', 1.0): 1, ('instagram', 1.0): 7, ('among', 1.0): 5, ('coconut', 1.0): 1, ('sambal', 1.0): 1, ('mussel', 1.0): 1, ('recip', 1.0): 5, ('kalin', 1.0): 1, ('mixcloud', 1.0): 1, ('sarcasm', 1.0): 2, ('chelsea', 1.0): 3, ('he', 1.0): 2, ('useless', 1.0): 2, ('thursday', 1.0): 2, ('hang', 1.0): 3, ('hehe', 1.0): 10, ('said', 1.0): 16, ('benson', 1.0): 1, ('facebook', 1.0): 5, ('solid', 1.0): 1, ('16/17', 1.0): 1, ('30', 1.0): 3, ('¬∞', 1.0): 1, ('üòú', 1.0): 2, ('maryhick', 1.0): 1, ('kikmeboy', 1.0): 7, ('photooftheday', 1.0): 4, ('musicbiz', 1.0): 2, ('sheskindahot', 1.0): 1, ('fleekil', 1.0): 1, ('mbalula', 1.0): 1, ('africa', 1.0): 1, ('mexican', 1.0): 1, ('scar', 1.0): 1, ('offic', 1.0): 8, ('donut', 1.0): 2, ('foiegra', 1.0): 2, ('despit', 1.0): 2, ('weather', 1.0): 9, ('wed', 1.0): 5, ('toni', 1.0): 2, ('stark', 1.0): 1, ('incred', 1.0): 7, ('poem', 1.0): 2, ('bubbl', 1.0): 3, ('dale', 1.0): 1, ('billion', 1.0): 1, ('magic', 1.0): 5, ('op', 1.0): 3, ('cast', 1.0): 1, ('vote', 1.0): 9, ('elect', 1.0): 1, ('jcreport', 1.0): 1, ('piggin', 1.0): 1, ('botan', 1.0): 2, ('soap', 1.0): 4, ('late', 1.0): 13, ('upload', 1.0): 5, ('freshli', 1.0): 1, ('3week', 1.0): 1, ('heal', 1.0): 1, ('tobi-bro', 1.0): 1, ('isp', 1.0): 1, ('steel', 1.0): 1, ('wednesday', 1.0): 1, ('swear', 1.0): 3, ('met', 1.0): 4, ('earlier', 1.0): 4, ('cam', 1.0): 3, ('üò≠', 1.0): 2, ('except', 1.0): 2, (\"masha'allah\", 1.0): 1, ('french', 1.0): 5, ('wwat', 1.0): 2, ('franc', 1.0): 5, ('yaaay', 1.0): 3, ('beirut', 1.0): 2, ('coffe', 1.0): 11, ('panda', 1.0): 6, ('eonni', 1.0): 2, ('favourit', 1.0): 13, ('soda', 1.0): 1, ('fuller', 1.0): 1, ('shit', 1.0): 13, ('healthi', 1.0): 2, ('üíì', 1.0): 2, ('rettweet', 1.0): 3, ('mvg', 1.0): 1, ('valuabl', 1.0): 1, ('madrid', 1.0): 3, ('sore', 1.0): 6, ('bergerac', 1.0): 1, ('u21', 1.0): 1, ('individu', 1.0): 2, ('adam', 1.0): 1, (\"beach'\", 1.0): 1, ('suicid', 1.0): 1, ('squad', 1.0): 1, ('fond', 1.0): 1, ('christoph', 1.0): 2, ('cocki', 1.0): 1, ('prove', 1.0): 3, (\"attitude'\", 1.0): 1, ('improv', 1.0): 3, ('suggest', 1.0): 6, ('date', 1.0): 12, ('inde', 1.0): 10, ('intellig', 1.0): 3, ('strong', 1.0): 7, ('cs', 1.0): 2, ('certain', 1.0): 2, ('exam', 1.0): 5, ('forgot', 1.0): 3, ('home-bas', 1.0): 1, ('knee', 1.0): 4, ('sale', 1.0): 3, ('fleur', 1.0): 1, ('dress', 1.0): 10, ('readystock_hijabmart', 1.0): 1, ('idr', 1.0): 2, ('325.000', 1.0): 1, ('200.000', 1.0): 1, ('tompolo', 1.0): 1, ('aim', 1.0): 1, ('cannot', 1.0): 4, ('buyer', 1.0): 3, ('disappoint', 1.0): 1, ('paper', 1.0): 4, ('slack', 1.0): 1, ('crack', 1.0): 1, ('particularli', 1.0): 2, ('strike', 1.0): 1, ('31', 1.0): 1, ('mam', 1.0): 2, ('feytyaz', 1.0): 1, ('instant', 1.0): 1, ('stiffen', 1.0): 1, ('ricky_feb', 1.0): 1, ('grindea', 1.0): 1, ('courier', 1.0): 1, ('crypt', 1.0): 1, ('arma', 1.0): 1, ('record', 1.0): 5, ('gosh', 1.0): 2, ('limbo', 1.0): 1, ('orchard', 1.0): 1, ('art', 1.0): 10, ('super', 1.0): 15, ('karachi', 1.0): 2, ('ka', 1.0): 4, ('venic', 1.0): 1, ('sever', 1.0): 3, ('part', 1.0): 15, ('wit', 1.0): 2, ('accumul', 1.0): 1, ('maroon', 1.0): 1, ('cocktail', 1.0): 4, ('0-100', 1.0): 1, ('quick', 1.0): 7, ('1100d', 1.0): 1, ('auto-focu', 1.0): 1, ('manual', 1.0): 2, ('vein', 1.0): 1, ('crackl', 1.0): 1, ('glaze', 1.0): 1, ('layout', 1.0): 3, ('bomb', 1.0): 4, ('social', 1.0): 4, ('websit', 1.0): 8, ('pake', 1.0): 1, ('joim', 1.0): 1, ('feed', 1.0): 4, ('troop', 1.0): 1, ('mail', 1.0): 3, ('ladolcevitainluxembourg@hotmail.com', 1.0): 1, ('prrequest', 1.0): 1, ('journorequest', 1.0): 1, ('the_madstork', 1.0): 1, ('shaun', 1.0): 1, ('bot', 1.0): 4, ('chloe', 1.0): 2, ('actress', 1.0): 3, ('away', 1.0): 13, ('wick', 1.0): 9, ('hola', 1.0): 1, ('juan', 1.0): 1, ('houston', 1.0): 1, ('tx', 1.0): 2, ('jenni', 1.0): 1, (\"year'\", 1.0): 2, ('stumbl', 1.0): 1, ('upon', 1.0): 1, ('prob.nic', 1.0): 1, ('choker', 1.0): 1, ('btw', 1.0): 12, ('seouljin', 1.0): 1, ('photoset', 1.0): 3, ('sadomasochistsparadis', 1.0): 1, ('wynter', 1.0): 1, ('bottom', 1.0): 3, ('outtak', 1.0): 1, ('sadomasochist', 1.0): 1, ('paradis', 1.0): 1, ('ty', 1.0): 8, ('bbi', 1.0): 3, ('clip', 1.0): 1, ('lose', 1.0): 6, ('cypher', 1.0): 1, ('amen', 1.0): 2, ('x32', 1.0): 1, ('plant', 1.0): 4, ('allow', 1.0): 4, ('corner', 1.0): 3, ('addict', 1.0): 4, ('gurl', 1.0): 1, ('suck', 1.0): 9, ('special', 1.0): 8, ('owe', 1.0): 1, ('daniel', 1.0): 2, ('ape', 1.0): 1, ('saar', 1.0): 1, ('ahead', 1.0): 4, ('vers', 1.0): 1, ('butterfli', 1.0): 1, ('bonu', 1.0): 2, ('fill', 1.0): 5, ('tear', 1.0): 1, ('laughter', 1.0): 2, ('5so', 1.0): 6, ('yummmyyi', 1.0): 1, ('eat', 1.0): 6, ('dosa', 1.0): 1, ('easier', 1.0): 2, ('unless', 1.0): 3, ('achi', 1.0): 2, ('youuu', 1.0): 2, ('bawi', 1.0): 1, ('ako', 1.0): 1, ('queenesth', 1.0): 1, ('sharp', 1.0): 2, ('yess', 1.0): 1, ('poldi', 1.0): 1, ('cimbom', 1.0): 1, ('buddi', 1.0): 7, ('bruhhh', 1.0): 1, ('daddi', 1.0): 2, ('‚Äù', 1.0): 5, ('knowledg', 1.0): 2, ('attent', 1.0): 4, ('1tb', 1.0): 1, ('bank', 1.0): 1, ('credit', 1.0): 4, ('depart', 1.0): 2, ('anz', 1.0): 1, ('extrem', 1.0): 3, ('offshor', 1.0): 1, ('absolut', 1.0): 9, ('classic', 1.0): 3, ('gottolovebank', 1.0): 1, ('yup', 1.0): 6, ('in-shaa-allah', 1.0): 1, ('dua', 1.0): 1, ('thru', 1.0): 2, ('aameen', 1.0): 2, ('4/5', 1.0): 1, ('coca', 1.0): 1, ('cola', 1.0): 1, ('fanta', 1.0): 1, ('pepsi', 1.0): 1, ('sprite', 1.0): 1, ('all', 1.0): 1, ('sweeeti', 1.0): 1, (';-)', 1.0): 3, ('welcometweet', 1.0): 2, ('psygustokita', 1.0): 4, ('setup', 1.0): 1, ('wet', 1.0): 3, ('feet', 1.0): 3, ('carpet', 1.0): 1, ('judgment', 1.0): 1, ('hypocrit', 1.0): 1, ('narcissist', 1.0): 1, ('jumpsuit', 1.0): 1, ('bt', 1.0): 2, ('denim', 1.0): 1, ('verg', 1.0): 1, ('owl', 1.0): 1, ('constant', 1.0): 1, ('run', 1.0): 12, ('sia', 1.0): 1, ('count', 1.0): 7, ('brilliant', 1.0): 9, ('teacher', 1.0): 1, ('compar', 1.0): 2, ('religion', 1.0): 1, ('rant', 1.0): 1, ('student', 1.0): 6, ('bencher', 1.0): 1, ('1/5', 1.0): 1, ('porsch', 1.0): 1, ('paddock', 1.0): 1, ('budapestgp', 1.0): 1, ('johnyherbert', 1.0): 1, ('roll', 1.0): 5, ('porschesupercup', 1.0): 1, ('koyal', 1.0): 1, ('melodi', 1.0): 1, ('unexpect', 1.0): 4, ('creat', 1.0): 8, ('memori', 1.0): 3, ('35', 1.0): 1, ('ep', 1.0): 3, ('catch', 1.0): 10, ('wirh', 1.0): 1, ('arc', 1.0): 1, ('x31', 1.0): 1, ('wolv', 1.0): 2, ('desir', 1.0): 1, ('ameen', 1.0): 1, ('kca', 1.0): 1, ('votejkt', 1.0): 1, ('48id', 1.0): 1, ('helpinggroupdm', 1.0): 1, ('quot', 1.0): 6, ('weird', 1.0): 5, ('dp', 1.0): 1, ('wife', 1.0): 5, ('poor', 1.0): 4, ('chick', 1.0): 1, ('guid', 1.0): 3, ('zonzofox', 1.0): 3, ('bhaiya', 1.0): 1, ('brother', 1.0): 4, ('lucki', 1.0): 10, ('patti', 1.0): 1, ('elabor', 1.0): 1, ('kuch', 1.0): 1, ('rate', 1.0): 1, ('merdeka', 1.0): 1, ('palac', 1.0): 2, ('hotel', 1.0): 5, ('plusmil', 1.0): 1, ('servic', 1.0): 7, ('hahahaa', 1.0): 1, ('mean', 1.0): 25, ('nex', 1.0): 2, ('safe', 1.0): 5, ('gwd', 1.0): 1, ('she', 1.0): 2, ('okok', 1.0): 1, ('33', 1.0): 4, ('idiot', 1.0): 1, ('chaerin', 1.0): 1, ('unni', 1.0): 1, ('viabl', 1.0): 1, ('altern', 1.0): 3, ('nowaday', 1.0): 2, ('ip', 1.0): 1, ('tombow', 1.0): 1, ('abt', 1.0): 2, ('friyay', 1.0): 2, ('smug', 1.0): 1, ('marrickvil', 1.0): 1, ('public', 1.0): 3, ('ten', 1.0): 1, ('ago', 1.0): 8, ('eighteen', 1.0): 1, ('auvssscr', 1.0): 1, ('ncaaseason', 1.0): 1, ('slow', 1.0): 2, ('popsicl', 1.0): 1, ('soft', 1.0): 2, ('melt', 1.0): 1, ('mouth', 1.0): 2, ('thankyouuu', 1.0): 1, ('dianna', 1.0): 1, ('ngga', 1.0): 1, ('usah', 1.0): 1, ('dipikirin', 1.0): 1, ('elah', 1.0): 1, ('easili', 1.0): 1, (\"who'\", 1.0): 9, ('entp', 1.0): 1, ('killin', 1.0): 1, ('meme', 1.0): 1, ('worthi', 1.0): 1, ('shot', 1.0): 6, ('emon', 1.0): 1, ('decent', 1.0): 2, ('outdoor', 1.0): 1, ('rave', 1.0): 1, ('dv', 1.0): 1, ('aku', 1.0): 1, ('bakal', 1.0): 1, ('liat', 1.0): 1, ('kak', 1.0): 2, ('merri', 1.0): 1, ('tv', 1.0): 5, ('outfit', 1.0): 3, ('---&gt;', 1.0): 1, ('fashionfriday', 1.0): 1, ('angle.nelson', 1.0): 1, ('cheap', 1.0): 1, ('mymonsoonstori', 1.0): 2, ('tree', 1.0): 2, ('lotion', 1.0): 1, ('moistur', 1.0): 1, ('monsoon', 1.0): 1, ('whoop', 1.0): 6, ('romant', 1.0): 2, ('valencia', 1.0): 1, ('daaru', 1.0): 1, ('parti', 1.0): 12, ('chaddi', 1.0): 1, ('wonderful.great', 1.0): 1, ('trim', 1.0): 1, ('pube', 1.0): 1, ('es', 1.0): 2, ('mi', 1.0): 5, ('tio', 1.0): 1, ('sinaloa', 1.0): 1, ('arr', 1.0): 1, ('stylish', 1.0): 1, ('trendi', 1.0): 1, ('kim', 1.0): 5, ('fabfriday', 1.0): 2, ('facetim', 1.0): 4, ('calum', 1.0): 3, ('constantli', 1.0): 1, ('announc', 1.0): 1, ('filbarbarian', 1.0): 1, ('beer', 1.0): 3, ('arm', 1.0): 3, ('testicl', 1.0): 1, ('light', 1.0): 13, ('katerina', 1.0): 1, ('maniataki', 1.0): 1, ('ahh', 1.0): 5, ('alright', 1.0): 6, ('worthwhil', 1.0): 3, ('judg', 1.0): 2, ('tech', 1.0): 2, ('window', 1.0): 7, ('stupid', 1.0): 8, ('plugin', 1.0): 1, ('bass', 1.0): 1, ('slap', 1.0): 1, ('6pm', 1.0): 1, ('door', 1.0): 3, ('vip', 1.0): 1, ('gener', 1.0): 4, ('seat', 1.0): 2, ('earli', 1.0): 9, ('london', 1.0): 9, ('toptravelcentar', 1.0): 1, ('ttctop', 1.0): 1, ('lux', 1.0): 1, ('luxurytravel', 1.0): 1, ('beograd', 1.0): 1, ('srbija', 1.0): 1, ('putovanja', 1.0): 1, ('wendi', 1.0): 2, ('provid', 1.0): 4, ('drainag', 1.0): 1, ('homebound', 1.0): 1, ('hahahay', 1.0): 1, ('yeeeah', 1.0): 1, ('moar', 1.0): 2, ('kitteh', 1.0): 1, ('incom', 1.0): 1, ('tower', 1.0): 2, ('yippee', 1.0): 1, ('scrummi', 1.0): 1, ('bio', 1.0): 5, ('mcpe', 1.0): 1, ('-&gt;', 1.0): 1, ('vainglori', 1.0): 1, ('driver', 1.0): 1, ('6:01', 1.0): 1, ('lilydal', 1.0): 1, ('fss', 1.0): 1, ('rais', 1.0): 3, ('magicalmysterytour', 1.0): 1, ('chek', 1.0): 2, ('rule', 1.0): 2, ('weebli', 1.0): 1, ('donetsk', 1.0): 1, ('earth', 1.0): 7, ('personalis', 1.0): 1, ('wrap', 1.0): 2, ('stationeri', 1.0): 1, ('adrian', 1.0): 1, ('parcel', 1.0): 2, ('tuesday', 1.0): 7, ('pri', 1.0): 3, ('80', 1.0): 3, ('wz', 1.0): 1, ('pattern', 1.0): 1, ('cut', 1.0): 3, ('buttonhol', 1.0): 1, ('4mi', 1.0): 1, ('famou', 1.0): 1, ('client', 1.0): 1, ('p', 1.0): 3, ('aliv', 1.0): 2, ('trial', 1.0): 1, ('spm', 1.0): 1, ('dinooo', 1.0): 1, ('cardio', 1.0): 1, ('steak', 1.0): 1, ('cue', 1.0): 1, ('laptop', 1.0): 1, ('guinea', 1.0): 1, ('pig', 1.0): 1, ('salamat', 1.0): 1, ('sa', 1.0): 6, ('mga', 1.0): 1, ('nag.greet', 1.0): 1, ('guis', 1.0): 1, ('godbless', 1.0): 2, ('crush', 1.0): 3, ('appl', 1.0): 4, ('deserv', 1.0): 11, ('charl', 1.0): 1, ('workhard', 1.0): 1, ('model', 1.0): 7, ('forrit', 1.0): 1, ('bread', 1.0): 2, ('bacon', 1.0): 2, ('butter', 1.0): 2, ('afang', 1.0): 2, ('soup', 1.0): 2, ('semo', 1.0): 2, ('brb', 1.0): 1, ('forc', 1.0): 2, ('doesnt', 1.0): 5, ('tato', 1.0): 1, ('bulat', 1.0): 1, ('concern', 1.0): 1, ('snake', 1.0): 1, ('perform', 1.0): 3, ('con', 1.0): 1, ('todayyy', 1.0): 1, ('max', 1.0): 2, ('gaza', 1.0): 1, ('bbb', 1.0): 1, ('pc', 1.0): 3, ('22', 1.0): 2, ('legal', 1.0): 1, ('ditch', 1.0): 2, ('tori', 1.0): 1, ('bajrangibhaijaanhighestweek', 1.0): 6, (\"s'okay\", 1.0): 1, ('andi', 1.0): 2, ('you-and', 1.0): 1, ('return', 1.0): 3, ('tuitutil', 1.0): 1, ('bud', 1.0): 2, ('learn', 1.0): 8, ('takeaway', 1.0): 1, ('instead', 1.0): 7, ('1hr', 1.0): 1, ('genial', 1.0): 1, ('competit', 1.0): 1, ('yosh', 1.0): 1, ('procrastin', 1.0): 1, ('plu', 1.0): 4, ('kfc', 1.0): 2, ('itun', 1.0): 1, ('dedicatedfan', 1.0): 1, ('üíú', 1.0): 7, ('daft', 1.0): 1, ('teeth', 1.0): 1, ('troubl', 1.0): 1, ('huxley', 1.0): 1, ('basket', 1.0): 2, ('ben', 1.0): 2, ('sent', 1.0): 8, ('gamer', 1.0): 3, ('activ', 1.0): 5, ('120', 1.0): 2, ('distanc', 1.0): 2, ('suitabl', 1.0): 1, ('stockholm', 1.0): 1, ('zack', 1.0): 1, ('destroy', 1.0): 1, ('heel', 1.0): 2, ('claw', 1.0): 1, ('q', 1.0): 2, ('blond', 1.0): 2, ('box', 1.0): 3, ('cheerio', 1.0): 1, ('seed', 1.0): 4, ('cutest', 1.0): 2, ('ffback', 1.0): 2, ('spotifi', 1.0): 3, (\"we'v\", 1.0): 7, ('vc', 1.0): 1, ('tgp', 1.0): 1, ('race', 1.0): 5, ('averag', 1.0): 2, (\"joe'\", 1.0): 1, ('bluejay', 1.0): 1, ('vinylbear', 1.0): 1, ('pal', 1.0): 1, ('furbabi', 1.0): 1, ('luff', 1.0): 1, ('mega', 1.0): 4, ('retail', 1.0): 4, ('boot', 1.0): 2, ('whsmith', 1.0): 1, ('ps3', 1.0): 1, ('shannon', 1.0): 1, ('na', 1.0): 9, ('redecor', 1.0): 1, ('bob', 1.0): 3, ('elli', 1.0): 4, ('mairi', 1.0): 1, ('workout', 1.0): 6, ('impair', 1.0): 1, ('uggghhh', 1.0): 1, ('dam', 1.0): 2, ('dun', 1.0): 2, ('eczema', 1.0): 1, ('suffer', 1.0): 4, ('ndee', 1.0): 1, ('pleasur', 1.0): 14, ('publiliu', 1.0): 1, ('syru', 1.0): 1, ('fear', 1.0): 1, ('death', 1.0): 3, ('dread', 1.0): 1, ('fell', 1.0): 3, ('fuk', 1.0): 1, ('unblock', 1.0): 1, ('tweak', 1.0): 2, ('php', 1.0): 1, ('fall', 1.0): 10, ('oomf', 1.0): 1, ('pippa', 1.0): 1, ('hschool', 1.0): 1, ('bu', 1.0): 3, ('cardi', 1.0): 1, ('everyday', 1.0): 3, ('everytim', 1.0): 3, ('hk', 1.0): 1, (\"why'd\", 1.0): 1, ('acorn', 1.0): 1, ('origin', 1.0): 7, ('c64', 1.0): 1, ('cpu', 1.0): 1, ('consider', 1.0): 1, ('advanc', 1.0): 1, ('onair', 1.0): 1, ('bay', 1.0): 1, ('hold', 1.0): 6, ('river', 1.0): 3, ('0878 0388', 1.0): 1, ('1033', 1.0): 1, ('0272 3306', 1.0): 1, ('70', 1.0): 5, ('rescu', 1.0): 1, ('mutt', 1.0): 1, ('confirm', 1.0): 3, ('deliveri', 1.0): 3, ('switch', 1.0): 2, ('lap', 1.0): 1, ('optim', 1.0): 1, ('lu', 1.0): 1, (':|', 1.0): 1, ('tweetofthedecad', 1.0): 1, ('class', 1.0): 5, ('happiest', 1.0): 2, ('bbmme', 1.0): 3, ('pin', 1.0): 4, ('7df9e60a', 1.0): 1, ('bbm', 1.0): 2, ('bbmpin', 1.0): 2, ('addmeonbbm', 1.0): 1, ('addm', 1.0): 1, (\"today'\", 1.0): 3, ('menu', 1.0): 1, ('marri', 1.0): 3, ('glenn', 1.0): 1, ('what', 1.0): 4, ('height', 1.0): 1, (\"sculptor'\", 1.0): 1, ('ti5', 1.0): 1, ('dota', 1.0): 3, ('nudg', 1.0): 1, ('spot', 1.0): 5, ('tasti', 1.0): 1, ('hilli', 1.0): 1, ('cycl', 1.0): 6, ('england', 1.0): 4, ('scotlandismass', 1.0): 1, ('gen', 1.0): 2, ('vikk', 1.0): 1, ('fna', 1.0): 1, ('mombasa', 1.0): 1, ('tukutanemombasa', 1.0): 1, ('100reasonstovisitmombasa', 1.0): 1, ('karibumombasa', 1.0): 1, ('hanbin', 1.0): 1, ('certainli', 1.0): 4, ('goosnight', 1.0): 1, ('kindli', 1.0): 4, ('familiar', 1.0): 2, ('jealou', 1.0): 4, ('tent', 1.0): 2, ('yea', 1.0): 2, ('cozi', 1.0): 1, ('phenomen', 1.0): 2, ('collab', 1.0): 2, ('gave', 1.0): 4, ('birth', 1.0): 1, ('behav', 1.0): 2, ('monster', 1.0): 1, ('spree', 1.0): 4, ('000', 1.0): 1, ('tank', 1.0): 6, ('outstand', 1.0): 1, ('donat', 1.0): 3, ('h', 1.0): 4, ('contestkiduniya', 1.0): 2, ('mfundo', 1.0): 1, ('och', 1.0): 1, ('hun', 1.0): 4, ('inner', 1.0): 2, ('nerd', 1.0): 2, ('tame', 1.0): 2, ('insidi', 1.0): 1, ('logic', 1.0): 1, ('math', 1.0): 1, ('channel', 1.0): 5, ('continu', 1.0): 4, ('doubt', 1.0): 3, ('300', 1.0): 2, ('sub', 1.0): 2, ('200', 1.0): 3, ('forgiven', 1.0): 1, ('manner', 1.0): 1, ('yhooo', 1.0): 1, ('ngi', 1.0): 1, ('mood', 1.0): 7, ('push', 1.0): 1, ('limit', 1.0): 6, ('obakeng', 1.0): 1, ('goat', 1.0): 1, ('alhamdullilah', 1.0): 1, ('pebbl', 1.0): 1, ('engross', 1.0): 1, ('bing', 1.0): 2, ('scream', 1.0): 2, ('whole', 1.0): 7, ('wide', 1.0): 2, ('üåé', 1.0): 2, ('üòß', 1.0): 1, ('wat', 1.0): 2, ('muahhh', 1.0): 1, ('pausetim', 1.0): 1, ('drift', 1.0): 1, ('loos', 1.0): 3, ('campaign', 1.0): 4, ('kickstart', 1.0): 1, ('articl', 1.0): 9, ('jenna', 1.0): 1, ('bellybutton', 1.0): 5, ('inni', 1.0): 4, ('outi', 1.0): 4, ('havent', 1.0): 4, ('delish', 1.0): 1, ('joselito', 1.0): 1, ('freya', 1.0): 1, ('nth', 1.0): 1, ('latepost', 1.0): 1, ('lupet', 1.0): 1, ('mo', 1.0): 2, ('eric', 1.0): 3, ('askaman', 1.0): 1, ('150', 1.0): 1, ('0345', 1.0): 2, ('454', 1.0): 1, ('111', 1.0): 1, ('webz', 1.0): 1, ('oop', 1.0): 5, (\"they'll\", 1.0): 6, ('realis', 1.0): 2, ('anymor', 1.0): 3, ('carmel', 1.0): 1, ('decis', 1.0): 5, ('matt', 1.0): 6, ('@commoncultur', 1.0): 1, ('@connorfranta', 1.0): 1, ('honestli', 1.0): 3, ('explain', 1.0): 3, ('relationship', 1.0): 4, ('pick', 1.0): 15, ('tessnzach', 1.0): 1, ('paperboy', 1.0): 1, ('honest', 1.0): 3, ('reassur', 1.0): 1, ('guysss', 1.0): 3, ('mubank', 1.0): 2, (\"dongwoo'\", 1.0): 1, ('bright', 1.0): 2, ('tommorow', 1.0): 3, ('newyork', 1.0): 1, ('lolll', 1.0): 1, ('twinx', 1.0): 1, ('16', 1.0): 2, ('path', 1.0): 1, ('firmansyahbl', 1.0): 1, ('procedur', 1.0): 1, ('grim', 1.0): 1, ('fandango', 1.0): 1, ('ordinari', 1.0): 1, ('extraordinari', 1.0): 1, ('bo', 1.0): 2, ('birmingham', 1.0): 1, ('oracl', 1.0): 1, ('samosa', 1.0): 1, ('firebal', 1.0): 1, ('shoe', 1.0): 4, ('serv', 1.0): 1, ('sushi', 1.0): 2, ('shoeshi', 1.0): 1, ('ÔøΩ', 1.0): 2, ('lymond', 1.0): 1, ('philippa', 1.0): 2, ('novel', 1.0): 1, ('tara', 1.0): 3, ('. . .', 1.0): 2, ('aur', 1.0): 2, ('han', 1.0): 1, ('imran', 1.0): 3, ('khan', 1.0): 7, ('63', 1.0): 1, ('agaaain', 1.0): 1, ('doli', 1.0): 1, ('siregar', 1.0): 1, ('ninh', 1.0): 1, ('size', 1.0): 5, ('geekiest', 1.0): 1, ('geek', 1.0): 2, ('wallet', 1.0): 3, ('request', 1.0): 4, ('media', 1.0): 4, ('ralli', 1.0): 1, ('rotat', 1.0): 3, ('direct', 1.0): 3, ('eek', 1.0): 1, ('red', 1.0): 6, ('beij', 1.0): 1, ('meni', 1.0): 1, ('tebrik', 1.0): 1, ('etdi', 1.0): 1, ('700', 1.0): 1, ('üíó', 1.0): 2, ('rod', 1.0): 1, ('embrac', 1.0): 1, ('actor', 1.0): 1, ('aplomb', 1.0): 1, ('foreveralon', 1.0): 2, ('mysumm', 1.0): 1, ('01482', 1.0): 1, ('333505', 1.0): 1, ('hahahaha', 1.0): 2, ('wear', 1.0): 6, ('uniform', 1.0): 1, ('evil', 1.0): 1, ('owww', 1.0): 1, ('choo', 1.0): 1, ('chweet', 1.0): 1, ('shorthair', 1.0): 1, ('oscar', 1.0): 1, ('realiz', 1.0): 7, ('harmoni', 1.0): 1, ('deneriveri', 1.0): 1, ('506', 1.0): 1, ('kiksext', 1.0): 5, ('kikkomansabor', 1.0): 2, ('killer', 1.0): 1, ('henessydiari', 1.0): 1, ('journey', 1.0): 4, ('band', 1.0): 4, ('plz', 1.0): 5, ('convo', 1.0): 3, ('11', 1.0): 5, ('vault', 1.0): 1, ('expand', 1.0): 2, ('vinni', 1.0): 1, ('money', 1.0): 9, ('hahahahaha', 1.0): 2, ('50cent', 1.0): 1, ('repay', 1.0): 1, ('debt', 1.0): 2, ('evet', 1.0): 1, ('wifi', 1.0): 3, ('lifestyl', 1.0): 1, ('qatarday', 1.0): 1, ('. ..', 1.0): 3, ('üåû', 1.0): 3, ('girli', 1.0): 1, ('india', 1.0): 4, ('innov', 1.0): 1, ('volunt', 1.0): 2, ('saran', 1.0): 1, ('drama', 1.0): 3, ('genr', 1.0): 1, ('romanc', 1.0): 1, ('comedi', 1.0): 1, ('leannerin', 1.0): 1, ('19', 1.0): 7, ('porno', 1.0): 1, ('l4l', 1.0): 3, ('weloveyounamjoon', 1.0): 1, ('homey', 1.0): 1, ('kenya', 1.0): 1, ('roller', 1.0): 2, ('coaster', 1.0): 1, ('aspect', 1.0): 1, ('najam', 1.0): 1, ('confess', 1.0): 2, ('pricelessantiqu', 1.0): 1, ('takesonetoknowon', 1.0): 1, ('extra', 1.0): 5, ('ucount', 1.0): 1, ('ji', 1.0): 3, ('turkish', 1.0): 1, ('knew', 1.0): 8, ('crap', 1.0): 1, ('burn', 1.0): 3, ('80x', 1.0): 1, ('airlin', 1.0): 1, ('sexi', 1.0): 10, ('yello', 1.0): 1, ('gail', 1.0): 1, ('yael', 1.0): 1, ('lesson', 1.0): 4, ('en', 1.0): 1, ('mano', 1.0): 1, ('hand', 1.0): 4, ('manag', 1.0): 6, ('prettiest', 1.0): 1, ('reader', 1.0): 4, ('dnt', 1.0): 1, ('ideal', 1.0): 2, ('weekli', 1.0): 2, ('idol', 1.0): 3, ('pose', 1.0): 2, ('shortlist', 1.0): 1, ('dominion', 1.0): 2, ('picnic', 1.0): 2, ('tmrw', 1.0): 3, ('nobodi', 1.0): 2, ('jummamubarak', 1.0): 1, ('shower', 1.0): 3, ('shalwarkameez', 1.0): 1, ('itter', 1.0): 1, ('offer', 1.0): 8, ('jummapray', 1.0): 1, ('af', 1.0): 8, ('display', 1.0): 1, ('enabl', 1.0): 1, ('compani', 1.0): 4, ('peep', 1.0): 4, ('tweep', 1.0): 2, ('folow', 1.0): 1, ('2k', 1.0): 1, ('ohhh', 1.0): 4, ('teaser', 1.0): 2, ('airec', 1.0): 1, ('009', 1.0): 1, ('acid', 1.0): 1, ('mous', 1.0): 2, ('31st', 1.0): 2, ('includ', 1.0): 5, ('robin', 1.0): 1, ('rough', 1.0): 4, ('control', 1.0): 1, ('remix', 1.0): 5, ('fave', 1.0): 3, ('toss', 1.0): 1, ('ladi', 1.0): 8, ('üêë', 1.0): 1, ('librari', 1.0): 3, ('mr2', 1.0): 1, ('climb', 1.0): 1, ('cuddl', 1.0): 1, ('jilla', 1.0): 1, ('headlin', 1.0): 1, ('2017', 1.0): 1, ('jumma', 1.0): 5, ('mubarik', 1.0): 2, ('spent', 1.0): 2, ('congratz', 1.0): 1, ('contribut', 1.0): 3, ('2.0', 1.0): 2, ('yuppiiee', 1.0): 1, ('alienthought', 1.0): 1, ('happyalien', 1.0): 1, ('crowd', 1.0): 2, ('loudest', 1.0): 2, ('gari', 1.0): 1, ('particular', 1.0): 1, ('attract', 1.0): 1, ('supprt', 1.0): 1, ('savag', 1.0): 1, ('cleans', 1.0): 1, ('scam', 1.0): 1, ('ridden', 1.0): 1, ('vyapam', 1.0): 2, ('renam', 1.0): 1, ('wave', 1.0): 2, ('couch', 1.0): 1, ('dodg', 1.0): 1, ('explan', 1.0): 2, ('bag', 1.0): 4, ('sanza', 1.0): 1, ('yaa', 1.0): 3, ('slr', 1.0): 1, ('som', 1.0): 1, ('honour', 1.0): 1, ('heheh', 1.0): 1, ('view', 1.0): 16, ('explor', 1.0): 2, ('wayanadan', 1.0): 1, ('forest', 1.0): 1, ('wayanad', 1.0): 1, ('srijith', 1.0): 1, ('whisper', 1.0): 1, ('lie', 1.0): 4, ('pokemon', 1.0): 1, ('dazzl', 1.0): 1, ('urself', 1.0): 2, ('doubl', 1.0): 2, ('flare', 1.0): 1, ('black', 1.0): 4, ('9', 1.0): 3, ('51', 1.0): 1, ('brows', 1.0): 1, ('bore', 1.0): 9, ('femal', 1.0): 2, ('tour', 1.0): 8, ('delv', 1.0): 2, ('muchhh', 1.0): 1, ('tmr', 1.0): 1, ('breakfast', 1.0): 4, ('gl', 1.0): 1, (\"tonight'\", 1.0): 2, ('):', 1.0): 7, ('litey', 1.0): 1, ('manuella', 1.0): 1, ('abhi', 1.0): 2, ('tak', 1.0): 2, ('nhi', 1.0): 2, ('dekhi', 1.0): 1, ('promo', 1.0): 3, ('se', 1.0): 4, ('xpax', 1.0): 1, ('lisa', 1.0): 2, ('aboard', 1.0): 3, ('institut', 1.0): 1, ('nc', 1.0): 2, ('chees', 1.0): 4, ('overload', 1.0): 1, ('pizza', 1.0): 1, ('‚Ä¢', 1.0): 3, ('mcfloat', 1.0): 1, ('fudg', 1.0): 3, ('sanda', 1.0): 1, ('munchkin', 1.0): 1, (\"d'd\", 1.0): 1, ('granni', 1.0): 1, ('baller', 1.0): 1, ('lil', 1.0): 4, ('chain', 1.0): 1, ('everybodi', 1.0): 1, ('ought', 1.0): 1, ('jay', 1.0): 3, ('events@breastcancernow.org', 1.0): 1, ('79x', 1.0): 1, ('champion', 1.0): 1, ('letter', 1.0): 2, ('uniqu', 1.0): 2, ('affaraid', 1.0): 1, ('dearslim', 1.0): 2, ('role', 1.0): 2, ('billi', 1.0): 2, ('lab', 1.0): 1, ('ovh', 1.0): 2, ('maxi', 1.0): 2, ('bunch', 1.0): 1, ('acc', 1.0): 2, ('sprit', 1.0): 1, ('you', 1.0): 1, ('til', 1.0): 2, ('hammi', 1.0): 1, ('freedom', 1.0): 2, ('pistol', 1.0): 1, ('unlock', 1.0): 1, ('bemeapp', 1.0): 1, ('thumb', 1.0): 1, ('beme', 1.0): 1, ('bemecod', 1.0): 1, ('proudtobem', 1.0): 1, ('round', 1.0): 2, ('calm', 1.0): 5, ('kepo', 1.0): 1, ('luckili', 1.0): 1, ('clearli', 1.0): 2, ('ÿØÿπŸÖŸÖ', 1.0): 1, ('ŸÑŸÑÿπŸàÿØÿ©', 1.0): 1, ('ŸÑŸÑÿ≠Ÿäÿßÿ©', 1.0): 1, ('heiyo', 1.0): 2, ('dudafti', 1.0): 1, ('breaktym', 1.0): 1, ('fatal', 1.0): 1, ('danger', 1.0): 1, ('term', 1.0): 2, ('health', 1.0): 2, ('outrag', 1.0): 1, ('645k', 1.0): 1, ('muna', 1.0): 1, ('magstart', 1.0): 1, ('salut', 1.0): 3, ('‚Üí', 1.0): 1, ('thq', 1.0): 1, ('contin', 1.0): 1, ('thalaivar', 1.0): 1, ('¬£', 1.0): 7, ('heiya', 1.0): 2, ('grab', 1.0): 3, ('30.000', 1.0): 2, ('av', 1.0): 1, ('gd', 1.0): 3, ('wknd', 1.0): 1, ('ear', 1.0): 12, (\"y'day\", 1.0): 1, ('hxh', 1.0): 1, ('badass', 1.0): 2, ('killua', 1.0): 1, ('scene', 1.0): 2, ('78x', 1.0): 1, ('unappreci', 1.0): 1, ('graciou', 1.0): 1, ('nailedit', 1.0): 1, ('ourdisneyinfin', 1.0): 1, ('mari', 1.0): 3, ('jillmil', 1.0): 1, ('webcam', 1.0): 2, ('elfindelmundo', 1.0): 1, ('mainli', 1.0): 1, ('favour', 1.0): 1, ('dancetast', 1.0): 1, ('satyajit', 1.0): 1, (\"ray'\", 1.0): 1, ('porosh', 1.0): 1, ('pathor', 1.0): 1, ('situat', 1.0): 3, ('goldbug', 1.0): 1, ('wine', 1.0): 3, ('bottl', 1.0): 2, ('spill', 1.0): 2, ('jazmin', 1.0): 3, ('bonilla', 1.0): 3, ('15000', 1.0): 1, ('star', 1.0): 9, ('hollywood', 1.0): 3, ('rofl', 1.0): 3, ('shade', 1.0): 1, ('grey', 1.0): 1, ('netsec', 1.0): 1, ('kev', 1.0): 1, ('sister', 1.0): 6, ('told', 1.0): 6, ('unlist', 1.0): 1, ('hickey', 1.0): 1, ('dad', 1.0): 5, ('hock', 1.0): 1, ('mamma', 1.0): 1, ('human', 1.0): 5, ('be', 1.0): 1, ('mere', 1.0): 1, ('holist', 1.0): 1, ('cosmovis', 1.0): 1, ('narrow-mind', 1.0): 1, ('charg', 1.0): 3, ('cess', 1.0): 1, ('alix', 1.0): 1, ('quan', 1.0): 1, ('tip', 1.0): 5, ('naaahhh', 1.0): 1, ('duh', 1.0): 2, ('emesh', 1.0): 1, ('hilari', 1.0): 4, ('kath', 1.0): 3, ('kia', 1.0): 1, ('@vauk', 1.0): 1, ('tango', 1.0): 1, ('tracerequest', 1.0): 2, ('dassi', 1.0): 1, ('fwm', 1.0): 1, ('selamat', 1.0): 1, ('nichola', 1.0): 2, ('malta', 1.0): 1, ('gto', 1.0): 1, ('tomorrowland', 1.0): 1, ('incal', 1.0): 1, ('shob', 1.0): 1, ('incomplet', 1.0): 1, ('barkada', 1.0): 1, ('silverston', 1.0): 1, ('pull', 1.0): 1, ('bookstor', 1.0): 1, ('ganna', 1.0): 1, ('hillari', 1.0): 1, ('clinton', 1.0): 1, ('court', 1.0): 2, ('notic', 1.0): 11, ('slice', 1.0): 2, ('life-so', 1.0): 1, ('hidden', 1.0): 1, ('untap', 1.0): 1, ('mca', 1.0): 2, ('gettin', 1.0): 1, ('hella', 1.0): 1, ('wana', 1.0): 1, ('bandz', 1.0): 1, ('hell', 1.0): 4, ('donington', 1.0): 1, ('park', 1.0): 8, ('24/25', 1.0): 1, ('x30', 1.0): 1, ('merci', 1.0): 1, ('bien', 1.0): 1, ('pitbul', 1.0): 1, ('777x', 1.0): 1, ('fri', 1.0): 3, ('annyeong', 1.0): 1, ('oppa', 1.0): 7, ('indonesian', 1.0): 1, ('elf', 1.0): 3, ('flight', 1.0): 2, ('bf', 1.0): 2, ('jennyjean', 1.0): 1, ('kikchat', 1.0): 1, ('sabadodeganarseguidor', 1.0): 1, ('sexysasunday', 1.0): 2, ('marseil', 1.0): 1, ('ganda', 1.0): 1, ('fnaf', 1.0): 5, ('steam', 1.0): 1, ('assur', 1.0): 2, ('current', 1.0): 7, ('goin', 1.0): 1, ('sweeti', 1.0): 4, ('strongest', 1.0): 1, (\"spot'\", 1.0): 1, ('barnstapl', 1.0): 1, ('bideford', 1.0): 1, ('abit', 1.0): 1, ('road', 1.0): 5, ('rocro', 1.0): 1, ('13glodyysbro', 1.0): 1, ('hire', 1.0): 1, ('2ne1', 1.0): 1, ('aspetti', 1.0): 1, ('chicken', 1.0): 4, ('chip', 1.0): 3, ('cupboard', 1.0): 1, ('empti', 1.0): 2, ('jami', 1.0): 2, ('ian', 1.0): 2, ('latin', 1.0): 5, ('asian', 1.0): 5, ('version', 1.0): 8, ('va', 1.0): 1, ('642', 1.0): 1, ('kikgirl', 1.0): 5, ('orgasm', 1.0): 1, ('phonesex', 1.0): 1, ('spacer', 1.0): 1, ('felic', 1.0): 1, ('smoak', 1.0): 1, ('üëì', 1.0): 1, ('üíò', 1.0): 3, ('children', 1.0): 3, ('psychopath', 1.0): 1, ('spoil', 1.0): 1, ('dimpl', 1.0): 1, ('contempl', 1.0): 1, ('indi', 1.0): 2, ('rout', 1.0): 4, ('jsl', 1.0): 1, ('76x', 1.0): 1, ('gotcha', 1.0): 1, ('kina', 1.0): 1, ('donna', 1.0): 3, ('reachabl', 1.0): 1, ('jk', 1.0): 1, ('s02e04', 1.0): 1, ('air', 1.0): 7, ('naggi', 1.0): 1, ('anal', 1.0): 1, ('child', 1.0): 3, ('vidcon', 1.0): 2, ('anxiou', 1.0): 1, ('shake', 1.0): 2, ('10:30', 1.0): 1, ('smoke', 1.0): 3, ('white', 1.0): 4, ('grandpa', 1.0): 4, ('prolli', 1.0): 1, ('stash', 1.0): 2, ('closer-chas', 1.0): 1, ('spec', 1.0): 1, ('leagu', 1.0): 3, ('chase', 1.0): 1, ('wall', 1.0): 3, ('angel', 1.0): 4, ('mochamichel', 1.0): 1, ('iph', 1.0): 4, ('0ne', 1.0): 4, ('simpli', 1.0): 3, ('bi0', 1.0): 8, ('x29', 1.0): 1, ('there', 1.0): 2, ('background', 1.0): 2, ('maggi', 1.0): 1, ('afraid', 1.0): 3, ('mull', 1.0): 1, ('nil', 1.0): 1, ('glasgow', 1.0): 2, ('netbal', 1.0): 1, ('thistl', 1.0): 1, ('thistlelov', 1.0): 1, ('minecraft', 1.0): 7, ('drew', 1.0): 3, ('delici', 1.0): 3, ('muddl', 1.0): 1, ('racket', 1.0): 2, ('isol', 1.0): 1, ('fa', 1.0): 1, ('particip', 1.0): 2, ('icecreammast', 1.0): 1, ('group', 1.0): 10, ('huhu', 1.0): 3, ('shet', 1.0): 1, ('desk', 1.0): 1, ('o_o', 1.0): 1, ('orz', 1.0): 1, ('problemmm', 1.0): 1, ('75x', 1.0): 1, ('english', 1.0): 4, ('yeeaayi', 1.0): 1, ('alhamdulillah', 1.0): 1, ('amin', 1.0): 1, ('weed', 1.0): 1, ('crowdfund', 1.0): 1, ('goal', 1.0): 2, ('walk', 1.0): 12, ('hellooo', 1.0): 2, ('select', 1.0): 1, ('lynn', 1.0): 1, ('buffer', 1.0): 2, ('button', 1.0): 2, ('compos', 1.0): 1, ('fridayfun', 1.0): 1, ('non-filipina', 1.0): 1, ('ejayst', 1.0): 1, ('state', 1.0): 2, ('le', 1.0): 2, ('stan', 1.0): 1, ('lee', 1.0): 2, ('discoveri', 1.0): 1, ('cousin', 1.0): 5, ('1400', 1.0): 1, ('yr', 1.0): 2, ('teleport', 1.0): 1, ('shahid', 1.0): 1, ('afridi', 1.0): 1, ('tou', 1.0): 1, ('mahnor', 1.0): 1, ('baloch', 1.0): 1, ('nikki', 1.0): 2, ('flower', 1.0): 4, ('blackfli', 1.0): 1, ('courgett', 1.0): 1, ('wont', 1.0): 5, ('affect', 1.0): 2, ('fruit', 1.0): 5, ('italian', 1.0): 1, ('netfilx', 1.0): 1, ('unmarri', 1.0): 1, ('finger', 1.0): 6, ('rock', 1.0): 10, ('wielli', 1.0): 1, ('paul', 1.0): 2, ('barcod', 1.0): 1, ('charlott', 1.0): 1, ('thta', 1.0): 1, ('trailblazerhonor', 1.0): 1, ('labour', 1.0): 3, ('leader', 1.0): 3, ('alot', 1.0): 2, ('agayhippiehippi', 1.0): 1, ('exercis', 1.0): 2, ('ginger', 1.0): 1, ('x28', 1.0): 1, ('teach', 1.0): 2, ('awar', 1.0): 1, ('::', 1.0): 4, ('portsmouth', 1.0): 1, ('sonal', 1.0): 1, ('hungri', 1.0): 2, ('hmmm', 1.0): 4, ('pedant', 1.0): 1, ('98', 1.0): 1, ('kit', 1.0): 2, ('ack', 1.0): 1, ('hih', 1.0): 1, ('choir', 1.0): 1, ('rosidbinr', 1.0): 1, ('duke', 1.0): 2, ('earl', 1.0): 1, ('tau', 1.0): 1, ('orayt', 1.0): 1, ('knw', 1.0): 1, ('block', 1.0): 3, ('dikha', 1.0): 1, ('reh', 1.0): 1, ('adolf', 1.0): 1, ('hitler', 1.0): 1, ('obstacl', 1.0): 1, ('exist', 1.0): 2, ('surrend', 1.0): 2, ('terrif', 1.0): 1, ('advaddict', 1.0): 1, ('_15', 1.0): 1, ('jimin', 1.0): 1, ('notanapolog', 1.0): 3, ('map', 1.0): 2, ('inform', 1.0): 5, ('0.7', 1.0): 1, ('motherfuck', 1.0): 1, (\"david'\", 1.0): 1, ('damn', 1.0): 3, ('colleg', 1.0): 2, ('24th', 1.0): 3, ('steroid', 1.0): 1, ('alansmithpart', 1.0): 1, ('servu', 1.0): 1, ('bonasio', 1.0): 1, (\"doido'\", 1.0): 1, ('task', 1.0): 2, ('deleg', 1.0): 1, ('aaahhh', 1.0): 1, ('jen', 1.0): 2, ('virgin', 1.0): 5, ('non-mapbox', 1.0): 1, ('restrict', 1.0): 1, ('mapbox', 1.0): 1, ('basemap', 1.0): 1, ('contractu', 1.0): 1, ('research', 1.0): 1, ('seafood', 1.0): 1, ('weltum', 1.0): 1, ('teh', 1.0): 1, ('deti', 1.0): 1, ('huh', 1.0): 2, ('=d', 1.0): 2, ('annoy', 1.0): 2, ('katmtan', 1.0): 1, ('swan', 1.0): 1, ('fandom', 1.0): 3, ('blurri', 1.0): 1, ('besok', 1.0): 1, ('b', 1.0): 8, ('urgent', 1.0): 3, ('within', 1.0): 4, ('dorset', 1.0): 1, ('goddess', 1.0): 1, ('blast', 1.0): 1, ('shitfac', 1.0): 1, ('soul', 1.0): 4, ('sing', 1.0): 5, ('disney', 1.0): 1, ('doug', 1.0): 3, ('28', 1.0): 2, ('bnte', 1.0): 1, ('hain', 1.0): 2, (';p', 1.0): 1, ('shiiitt', 1.0): 1, ('case', 1.0): 9, ('rm35', 1.0): 1, ('negooo', 1.0): 1, ('male', 1.0): 1, ('madelin', 1.0): 1, ('nun', 1.0): 1, ('mornin', 1.0): 2, ('yapster', 1.0): 1, ('pli', 1.0): 1, ('icon', 1.0): 2, ('alchemist', 1.0): 1, ('x27', 1.0): 1, ('dayz', 1.0): 1, ('preview', 1.0): 1, ('thug', 1.0): 1, ('lmao', 1.0): 3, ('sharethelov', 1.0): 2, ('highvalu', 1.0): 2, ('halsey', 1.0): 1, ('30th', 1.0): 1, ('anniversari', 1.0): 5, ('folk', 1.0): 10, ('bae', 1.0): 6, ('repli', 1.0): 5, ('complain', 1.0): 3, ('rude', 1.0): 3, ('bond', 1.0): 4, ('nigg', 1.0): 1, ('readingr', 1.0): 1, ('wordoftheweek', 1.0): 1, ('wotw', 1.0): 1, ('4:18', 1.0): 1, ('est', 1.0): 1, ('earn', 1.0): 1, ('jess', 1.0): 2, ('surri', 1.0): 1, ('botani', 1.0): 1, ('gel', 1.0): 1, ('alison', 1.0): 1, ('lsa', 1.0): 1, ('respons', 1.0): 7, ('fron', 1.0): 1, ('debbi', 1.0): 1, ('carol', 1.0): 2, ('patient', 1.0): 4, ('discharg', 1.0): 1, ('loung', 1.0): 1, ('walmart', 1.0): 1, ('balanc', 1.0): 2, ('studi', 1.0): 6, ('hayley', 1.0): 2, ('shoulder', 1.0): 1, ('pad', 1.0): 2, ('mount', 1.0): 1, ('inquisitor', 1.0): 1, ('cosplay', 1.0): 4, ('cosplayprogress', 1.0): 1, ('mike', 1.0): 3, ('dunno', 1.0): 2, ('insecur', 1.0): 2, ('nh', 1.0): 1, ('devolut', 1.0): 1, ('patriot', 1.0): 1, ('halla', 1.0): 1, ('ark', 1.0): 1, (\"jiyeon'\", 1.0): 1, ('buzz', 1.0): 2, ('burnt', 1.0): 1, ('mist', 1.0): 4, ('opi', 1.0): 1, ('avoplex', 1.0): 1, ('nail', 1.0): 3, ('cuticl', 1.0): 1, ('replenish', 1.0): 1, ('15ml', 1.0): 1, ('seriou', 1.0): 2, ('submiss', 1.0): 1, ('lb', 1.0): 2, ('cherish', 1.0): 2, ('flip', 1.0): 1, ('learnt', 1.0): 2, ('backflip', 1.0): 2, ('jumpgiant', 1.0): 1, ('foampit', 1.0): 1, ('usa', 1.0): 3, ('pamer', 1.0): 1, ('thk', 1.0): 1, ('actuallythough', 1.0): 1, ('craft', 1.0): 2, ('session', 1.0): 3, ('mehtab', 1.0): 1, ('aunti', 1.0): 1, ('gc', 1.0): 1, ('yeeew', 1.0): 1, ('pre', 1.0): 3, ('lan', 1.0): 1, ('yeey', 1.0): 1, ('arrang', 1.0): 1, ('doodl', 1.0): 2, ('comic', 1.0): 1, ('summon', 1.0): 1, ('none', 1.0): 1, ('üôÖ', 1.0): 1, ('lycra', 1.0): 1, ('vincent', 1.0): 1, ('couldnt', 1.0): 1, ('roy', 1.0): 1, ('bg', 1.0): 1, ('img', 1.0): 1, ('circl', 1.0): 1, ('font', 1.0): 1, ('deathofgrass', 1.0): 1, ('loan', 1.0): 2, ('lawnmow', 1.0): 1, ('popular', 1.0): 2, ('charismat', 1.0): 1, ('man.h', 1.0): 1, ('thrive', 1.0): 1, ('economi', 1.0): 1, ('burst', 1.0): 2, ('georgi', 1.0): 1, ('x26', 1.0): 1, ('million', 1.0): 4, ('fl', 1.0): 1, ('kindest', 1.0): 2, ('iceland', 1.0): 1, ('crazi', 1.0): 4, ('landscap', 1.0): 2, ('yok', 1.0): 1, ('lah', 1.0): 1, ('concordia', 1.0): 1, ('reunit', 1.0): 1, ('xxxibmchll', 1.0): 1, ('sea', 1.0): 4, ('prettier', 1.0): 2, ('imitatia', 1.0): 1, ('oe', 1.0): 1, ('michel', 1.0): 1, ('comeback', 1.0): 1, ('gross', 1.0): 1, ('treat', 1.0): 5, ('equal', 1.0): 2, ('injustic', 1.0): 1, ('femin', 1.0): 1, ('ineedfeminismbecaus', 1.0): 1, ('forgotten', 1.0): 3, ('stuck', 1.0): 4, ('recommend', 1.0): 4, ('redhead', 1.0): 1, ('wacki', 1.0): 1, ('rather', 1.0): 5, ('waytoliveahappylif', 1.0): 1, ('hoxton', 1.0): 1, ('holborn', 1.0): 1, ('karen', 1.0): 2, ('wag', 1.0): 2, ('bum', 1.0): 1, ('wwooo', 1.0): 1, ('nite', 1.0): 3, ('laiten', 1.0): 1, ('arond', 1.0): 1, ('1:30', 1.0): 1, ('consid', 1.0): 3, ('matur', 1.0): 3, ('journeyp', 1.0): 2, ('foam', 1.0): 1, (\"lady'\", 1.0): 1, ('mob', 1.0): 1, ('fals', 1.0): 1, ('bulletin', 1.0): 1, ('spring', 1.0): 1, ('fiesta', 1.0): 1, ('nois', 1.0): 2, ('awuuu', 1.0): 1, ('aich', 1.0): 1, ('sept', 1.0): 2, ('rudramadevi', 1.0): 1, ('anushka', 1.0): 1, ('gunashekar', 1.0): 1, ('harryxhood', 1.0): 1, ('upset', 1.0): 1, ('ooh', 1.0): 1, ('humanist', 1.0): 1, ('magazin', 1.0): 2, ('usernam', 1.0): 1, ('rape', 1.0): 1, ('csrrace', 1.0): 1, ('lack', 1.0): 6, ('hygien', 1.0): 1, ('tose', 1.0): 1, ('cloth', 1.0): 1, ('temperatur', 1.0): 1, ('planet', 1.0): 2, ('brave', 1.0): 2, ('ge', 1.0): 1, ('2015kenya', 1.0): 1, ('ryan', 1.0): 4, ('tidi', 1.0): 2, ('hagergang', 1.0): 1, ('chanhun', 1.0): 1, ('photoshoot', 1.0): 1, ('afteral', 1.0): 1, ('sadkaay', 1.0): 1, ('thark', 1.0): 1, ('peak', 1.0): 1, ('heatwav', 1.0): 1, ('lower', 1.0): 1, ('standard', 1.0): 2, ('x25', 1.0): 1, ('recruit', 1.0): 2, ('doom', 1.0): 1, ('nasti', 1.0): 1, ('affili', 1.0): 1, ('&gt;:)', 1.0): 2, ('64', 1.0): 2, ('74', 1.0): 1, ('40', 1.0): 4, ('00', 1.0): 1, ('hall', 1.0): 2, ('ted', 1.0): 3, ('pixgram', 1.0): 2, ('creativ', 1.0): 2, ('slideshow', 1.0): 1, ('nibbl', 1.0): 2, ('ivi', 1.0): 1, ('sho', 1.0): 1, ('superpow', 1.0): 2, ('obsess', 1.0): 2, ('oth', 1.0): 1, ('third', 1.0): 2, ('ngarepfollbackdarinabilahjkt', 1.0): 1, ('48', 1.0): 1, ('sunglass', 1.0): 1, ('jacki', 1.0): 2, ('sunni', 1.0): 6, ('style', 1.0): 5, ('jlo', 1.0): 1, ('jlover', 1.0): 1, ('turkey', 1.0): 1, ('goodafternoon', 1.0): 2, ('collag', 1.0): 2, ('furri', 1.0): 2, ('bruce', 1.0): 2, ('kunoriforceo', 1.0): 8, ('aayegi', 1.0): 1, ('tim', 1.0): 2, ('wiw', 1.0): 1, ('bip', 1.0): 1, ('zareen', 1.0): 1, ('daisi', 1.0): 1, (\"b'coz\", 1.0): 1, ('kart', 1.0): 1, ('mak', 1.0): 1, ('‚àó', 1.0): 2, ('lega', 1.0): 1, ('spag', 1.0): 1, ('boat', 1.0): 2, ('outboard', 1.0): 1, ('spell', 1.0): 4, ('reboard', 1.0): 1, ('fire', 1.0): 2, ('offboard', 1.0): 1, ('sn16', 1.0): 1, ('9dg', 1.0): 1, ('bnf', 1.0): 1, ('50', 1.0): 1, ('jason', 1.0): 1, ('rob', 1.0): 2, ('feb', 1.0): 1, ('victoriasecret', 1.0): 1, ('finland', 1.0): 1, ('helsinki', 1.0): 1, ('airport', 1.0): 3, ('plane', 1.0): 2, ('beyond', 1.0): 4, ('ont', 1.0): 1, ('tii', 1.0): 1, ('lng', 1.0): 2, ('yan', 1.0): 2, (\"u'll\", 1.0): 2, ('steve', 1.0): 2, ('bell', 1.0): 1, ('prescott', 1.0): 1, ('leadership', 1.0): 2, ('cartoon', 1.0): 1, ('upsid', 1.0): 2, ('statement', 1.0): 1, ('selamathariraya', 1.0): 1, ('lovesummertim', 1.0): 1, ('dumont', 1.0): 1, ('jax', 1.0): 1, ('jone', 1.0): 1, ('awesomee', 1.0): 1, ('x24', 1.0): 1, ('geoff', 1.0): 1, ('amazingli', 1.0): 1, ('talant', 1.0): 1, ('vsco', 1.0): 2, ('thanki', 1.0): 2, ('hash', 1.0): 1, ('tag', 1.0): 5, ('ifimeetanalien', 1.0): 1, ('bff', 1.0): 4, ('section', 1.0): 3, ('follbaaack', 1.0): 1, ('az', 1.0): 1, ('cauliflow', 1.0): 1, ('attempt', 1.0): 1, ('prinsesa', 1.0): 1, ('yaaah', 1.0): 2, ('law', 1.0): 3, ('toy', 1.0): 2, ('sonaaa', 1.0): 1, ('beautiful', 1.0): 2, (\"josephine'\", 1.0): 1, ('mirror', 1.0): 3, ('cretaperfect', 1.0): 2, ('4me', 1.0): 2, ('cretaperfectsuv', 1.0): 2, ('creta', 1.0): 1, ('load', 1.0): 1, ('telecom', 1.0): 2, ('judi', 1.0): 1, ('superb', 1.0): 1, ('slightli', 1.0): 1, ('rakna', 1.0): 1, ('ew', 1.0): 1, ('whose', 1.0): 1, ('fifa', 1.0): 1, ('lineup', 1.0): 1, ('surviv', 1.0): 2, ('p90x', 1.0): 1, ('p90', 1.0): 1, ('dishoom', 1.0): 2, ('rajnigandha', 1.0): 1, ('minju', 1.0): 1, ('rapper', 1.0): 1, ('lead', 1.0): 2, ('vocal', 1.0): 1, ('yujin', 1.0): 1, ('visual', 1.0): 2, ('makna', 1.0): 1, ('jane', 1.0): 2, ('hah', 1.0): 4, ('hawk', 1.0): 2, ('greatest', 1.0): 2, ('histori', 1.0): 2, ('along', 1.0): 6, ('talkback', 1.0): 1, ('process', 1.0): 4, ('featur', 1.0): 4, ('mostli', 1.0): 1, (\"cinema'\", 1.0): 1, ('defend', 1.0): 2, ('fashion', 1.0): 2, ('atroc', 1.0): 1, ('pandimension', 1.0): 1, ('manifest', 1.0): 1, ('argo', 1.0): 1, ('ring', 1.0): 4, ('640', 1.0): 1, ('nad', 1.0): 1, ('plezzz', 1.0): 1, ('asthma', 1.0): 1, ('inhal', 1.0): 1, ('breath', 1.0): 3, ('goodluck', 1.0): 1, ('hunger', 1.0): 1, ('mockingjay', 1.0): 1, ('thehungergam', 1.0): 1, ('ador', 1.0): 4, ('x23', 1.0): 1, ('reina', 1.0): 1, ('felt', 1.0): 3, ('excus', 1.0): 2, ('attend', 1.0): 2, ('whn', 1.0): 1, ('andr', 1.0): 1, ('mamayang', 1.0): 1, ('11pm', 1.0): 1, ('1d', 1.0): 2, ('89.9', 1.0): 1, ('powi', 1.0): 1, ('shropshir', 1.0): 1, ('border', 1.0): 1, (\"school'\", 1.0): 1, ('san', 1.0): 2, ('diego', 1.0): 1, ('jump', 1.0): 2, ('sourc', 1.0): 3, ('appeas', 1.0): 1, ('¬¶', 1.0): 1, ('aj', 1.0): 1, ('action', 1.0): 1, ('grunt', 1.0): 1, ('sc', 1.0): 1, ('anti-christ', 1.0): 1, ('m8', 1.0): 1, ('ju', 1.0): 1, ('halfway', 1.0): 1, ('ex', 1.0): 2, ('postiv', 1.0): 2, ('opinion', 1.0): 3, ('avi', 1.0): 1, ('dare', 1.0): 4, ('corridor', 1.0): 1, ('üëØ', 1.0): 2, ('neither', 1.0): 2, ('rundown', 1.0): 1, ('yah', 1.0): 4, ('leviboard', 1.0): 1, ('kleper', 1.0): 1, (':(', 1.0): 1, ('impecc', 1.0): 2, ('setokido', 1.0): 1, ('shoulda', 1.0): 3, ('hippo', 1.0): 1, ('materialist', 1.0): 1, ('showpo', 1.0): 1, ('cough', 1.0): 6, ('@artofsleepingin', 1.0): 1, ('x22', 1.0): 1, ('‚ò∫', 1.0): 5, ('makesm', 1.0): 1, ('santorini', 1.0): 1, ('escap', 1.0): 2, ('beatport', 1.0): 1, ('üëäüèª', 1.0): 1, ('trmdhesit', 1.0): 2, ('manuel', 1.0): 1, ('vall', 1.0): 1, ('king', 1.0): 3, ('seven', 1.0): 2, ('kingdom', 1.0): 2, ('andal', 1.0): 1, ('taught', 1.0): 1, ('hide', 1.0): 3, ('privaci', 1.0): 1, ('wise', 1.0): 1, ('natsuki', 1.0): 1, ('often', 1.0): 2, ('catchi', 1.0): 1, ('neil', 1.0): 2, ('emir', 1.0): 2, ('brill', 1.0): 1, ('urquhart', 1.0): 1, ('castl', 1.0): 1, ('simpl', 1.0): 2, ('shatter', 1.0): 2, ('contrast', 1.0): 1, ('educampakl', 1.0): 1, ('rotorua', 1.0): 1, ('pehli', 1.0): 1, ('phir', 1.0): 1, ('somi', 1.0): 1, ('burfday', 1.0): 1, ('univers', 1.0): 3, ('santo', 1.0): 1, ('toma', 1.0): 1, ('norh', 1.0): 1, ('dialogu', 1.0): 2, ('chainsaw', 1.0): 2, ('amus', 1.0): 1, ('awe', 1.0): 1, ('protect', 1.0): 2, ('pop', 1.0): 5, ('2ish', 1.0): 1, ('fahad', 1.0): 1, ('bhai', 1.0): 3, ('iqrar', 1.0): 1, ('waseem', 1.0): 1, ('abroad', 1.0): 2, ('movie', 1.0): 1, ('chef', 1.0): 1, ('grogol', 1.0): 1, ('long-dist', 1.0): 1, ('rhi', 1.0): 1, ('pwrfl', 1.0): 1, ('benefit', 1.0): 2, ('b2b', 1.0): 1, ('b2c', 1.0): 1, (\"else'\", 1.0): 2, ('soo', 1.0): 2, ('enterprison', 1.0): 1, ('schoolsoutforsumm', 1.0): 1, ('fellow', 1.0): 4, ('juggl', 1.0): 1, ('purrtho', 1.0): 1, ('catho', 1.0): 1, ('catami', 1.0): 1, ('fourfivesecond', 1.0): 4, ('deaf', 1.0): 4, ('drug', 1.0): 1, ('alcohol', 1.0): 1, ('apexi', 1.0): 3, ('crystal', 1.0): 3, ('meth', 1.0): 1, ('champagn', 1.0): 1, ('fc', 1.0): 1, ('streamer', 1.0): 1, ('juic', 1.0): 1, ('correct', 1.0): 1, ('portrait', 1.0): 1, ('izumi', 1.0): 1, ('fugiwara', 1.0): 1, ('clonmel', 1.0): 1, ('vibrant', 1.0): 1, ('estim', 1.0): 1, ('server', 1.0): 2, ('quiet', 1.0): 1, ('yey', 1.0): 1, (\"insha'allah\", 1.0): 1, ('wil', 1.0): 1, ('x21', 1.0): 1, ('trend', 1.0): 3, ('akshaymostlovedsuperstarev', 1.0): 1, ('indirect', 1.0): 1, ('askurban', 1.0): 1, ('lyka', 1.0): 2, ('nap', 1.0): 4, ('aff', 1.0): 1, ('unam', 1.0): 1, ('jonginuh', 1.0): 1, ('forecast', 1.0): 2, ('10am', 1.0): 2, ('5am', 1.0): 1, ('sooth', 1.0): 1, ('vii', 1.0): 1, ('sweetheart', 1.0): 1, ('freak', 1.0): 3, ('zayn', 1.0): 3, ('fucker', 1.0): 1, ('pet', 1.0): 2, ('illustr', 1.0): 1, ('wohoo', 1.0): 1, ('gleam', 1.0): 1, ('paint', 1.0): 4, ('deal', 1.0): 2, ('prime', 1.0): 2, ('minist', 1.0): 2, ('sunjam', 1.0): 1, ('industri', 1.0): 1, ('present', 1.0): 7, ('practic', 1.0): 3, ('proactiv', 1.0): 1, ('environ', 1.0): 1, ('unreal', 1.0): 1, ('zain', 1.0): 1, ('zac', 1.0): 1, ('isaac', 1.0): 1, ('oss', 1.0): 1, ('frank', 1.0): 1, ('iero', 1.0): 1, ('phase', 1.0): 2, ('david', 1.0): 1, ('beginn', 1.0): 1, ('shine', 1.0): 3, ('sunflow', 1.0): 2, ('tommarow', 1.0): 1, ('yall', 1.0): 2, ('rank', 1.0): 2, ('birthdaymonth', 1.0): 1, ('vianey', 1.0): 1, ('juli', 1.0): 11, ('birthdaygirl', 1.0): 1, (\"town'\", 1.0): 1, ('andrew', 1.0): 2, ('checkout', 1.0): 2, ('otwol', 1.0): 1, ('awhil', 1.0): 1, ('x20', 1.0): 1, ('all-tim', 1.0): 1, ('julia', 1.0): 1, ('robert', 1.0): 1, ('awwhh', 1.0): 1, ('bulldog', 1.0): 1, ('unfortun', 1.0): 2, ('02079', 1.0): 1, ('490', 1.0): 1, ('132', 1.0): 1, ('born', 1.0): 2, ('fightstickfriday', 1.0): 1, ('extravag', 1.0): 2, ('tearout', 1.0): 1, ('selekt', 1.0): 1, ('yoot', 1.0): 1, ('cross', 1.0): 3, ('gudday', 1.0): 1, ('dave', 1.0): 5, ('haileyhelp', 1.0): 1, ('eid', 1.0): 2, ('mubarak', 1.0): 5, ('brotheeerrr', 1.0): 1, ('adventur', 1.0): 5, ('tokyo', 1.0): 2, ('kansai', 1.0): 1, ('l', 1.0): 4, ('upp', 1.0): 2, ('om', 1.0): 1, ('60', 1.0): 1, ('minut', 1.0): 7, ('data', 1.0): 1, ('jesu', 1.0): 5, ('amsterdam', 1.0): 2, ('3rd', 1.0): 3, ('nextweek', 1.0): 1, ('booti', 1.0): 2, ('bcuz', 1.0): 1, ('step', 1.0): 3, ('option', 1.0): 3, ('stabl', 1.0): 1, ('sturdi', 1.0): 1, ('lukkke', 1.0): 1, ('again.ensoi', 1.0): 1, ('tc', 1.0): 1, ('madam', 1.0): 1, ('siddi', 1.0): 1, ('unknown', 1.0): 2, ('roomi', 1.0): 1, ('gn', 1.0): 2, ('gf', 1.0): 2, ('consent', 1.0): 1, ('mister', 1.0): 2, ('vine', 1.0): 2, ('peyton', 1.0): 1, ('nagato', 1.0): 1, ('yuki-chan', 1.0): 1, ('shoushitsu', 1.0): 1, ('archdbanterburi', 1.0): 3, ('experttradesmen', 1.0): 1, ('banter', 1.0): 1, ('quiz', 1.0): 1, ('tradetalk', 1.0): 1, ('floof', 1.0): 1, ('face', 1.0): 13, ('muahah', 1.0): 1, ('x19', 1.0): 1, ('anticip', 1.0): 1, ('jd', 1.0): 1, ('laro', 1.0): 1, ('tayo', 1.0): 1, ('answer', 1.0): 8, ('ht', 1.0): 1, ('angelica', 1.0): 1, ('anghel', 1.0): 1, ('aa', 1.0): 3, ('kkk', 1.0): 1, ('macbook', 1.0): 1, ('rehears', 1.0): 1, ('youthcelebr', 1.0): 1, ('mute', 1.0): 1, ('29th', 1.0): 1, ('gohf', 1.0): 4, ('vegetarian', 1.0): 1, (\"she'll\", 1.0): 1, ('gooday', 1.0): 3, ('101', 1.0): 3, ('12000', 1.0): 1, ('oshieer', 1.0): 1, ('realreview', 1.0): 1, ('happycustom', 1.0): 1, ('realoshi', 1.0): 1, ('dealsuthaonotebachao', 1.0): 1, ('bigger', 1.0): 2, ('dime', 1.0): 1, ('uhuh', 1.0): 1, ('üéµ', 1.0): 3, ('code', 1.0): 4, ('pleasant', 1.0): 2, ('on-board', 1.0): 1, ('raheel', 1.0): 1, ('flyhigh', 1.0): 1, ('bother', 1.0): 2, ('everett', 1.0): 1, ('taylor', 1.0): 1, ('ha-ha', 1.0): 1, ('peachyloan', 1.0): 1, ('fridayfreebi', 1.0): 1, ('noe', 1.0): 1, ('yisss', 1.0): 1, ('bindingofissac', 1.0): 1, ('xboxon', 1.0): 1, ('consol', 1.0): 1, ('justin', 1.0): 2, ('gladli', 1.0): 1, ('son', 1.0): 4, ('morocco', 1.0): 1, ('peru', 1.0): 1, ('nxt', 1.0): 1, ('bp', 1.0): 1, ('resort', 1.0): 1, ('x18', 1.0): 1, ('havuuulovey', 1.0): 1, ('uuu', 1.0): 1, ('possitv', 1.0): 1, ('hopey', 1.0): 1, ('throwbackfriday', 1.0): 1, ('christen', 1.0): 1, ('ki', 1.0): 1, ('yaad', 1.0): 1, ('gayi', 1.0): 1, ('opossum', 1.0): 1, ('belat', 1.0): 5, ('yeahh', 1.0): 2, ('kuffar', 1.0): 1, ('comput', 1.0): 5, ('cell', 1.0): 1, ('diarrhea', 1.0): 1, ('immigr', 1.0): 1, ('lice', 1.0): 1, ('goictiv', 1.0): 1, ('70685', 1.0): 1, ('tagsforlik', 1.0): 4, ('trapmus', 1.0): 1, ('hotmusicdeloco', 1.0): 1, ('kinick', 1.0): 1, ('01282', 1.0): 2, ('452096', 1.0): 1, ('shadi', 1.0): 1, ('reserv', 1.0): 3, ('tkt', 1.0): 1, ('likewis', 1.0): 4, ('overgener', 1.0): 1, ('ikr', 1.0): 1, ('üòç', 1.0): 2, ('consumer', 1.0): 1, ('fic', 1.0): 2, ('ouch', 1.0): 2, ('slip', 1.0): 1, ('disc', 1.0): 1, ('thw', 1.0): 1, ('chute', 1.0): 1, ('chalut', 1.0): 1, ('replay', 1.0): 1, ('iplay', 1.0): 1, ('11am', 1.0): 3, ('unneed', 1.0): 1, ('megamoh', 1.0): 1, ('7/29', 1.0): 1, ('tool', 1.0): 2, ('zealand', 1.0): 1, ('pile', 1.0): 2, ('dump', 1.0): 1, ('couscou', 1.0): 3, (\"women'\", 1.0): 2, ('fiction', 1.0): 1, ('wahahaah', 1.0): 1, ('x17', 1.0): 1, ('orhan', 1.0): 1, ('pamuk', 1.0): 1, ('hero', 1.0): 3, ('canopi', 1.0): 1, ('mapl', 1.0): 2, ('syrup', 1.0): 1, ('farm', 1.0): 2, ('stephani', 1.0): 2, ('üíñ', 1.0): 2, ('congrtaualt', 1.0): 1, ('philea', 1.0): 1, ('club', 1.0): 4, ('inc', 1.0): 1, ('photograph', 1.0): 2, ('phonegraph', 1.0): 1, ('srsli', 1.0): 1, ('10:17', 1.0): 1, ('ripaaa', 1.0): 1, ('banat', 1.0): 1, ('ray', 1.0): 1, ('dept', 1.0): 1, ('hospit', 1.0): 3, ('grt', 1.0): 1, ('infograph', 1.0): 1, (\"o'clock\", 1.0): 2, ('habit', 1.0): 1, ('1dfor', 1.0): 1, ('roadtrip', 1.0): 1, ('19:30', 1.0): 1, ('ifc', 1.0): 1, ('whip', 1.0): 1, ('lilsisbro', 1.0): 1, ('pre-ord', 1.0): 2, (\"pixar'\", 1.0): 2, ('steelbook', 1.0): 1, ('hmm', 1.0): 2, ('pegel', 1.0): 1, ('lemess', 1.0): 1, ('kyle', 1.0): 2, ('paypal', 1.0): 1, ('oct', 1.0): 1, ('tud', 1.0): 1, ('jst', 1.0): 2, ('humphrey', 1.0): 1, ('yell', 1.0): 2, ('erm', 1.0): 1, ('breach', 1.0): 1, ('lemon', 1.0): 2, ('yogurt', 1.0): 2, ('pot', 1.0): 1, ('discov', 1.0): 2, ('liquoric', 1.0): 1, ('pud', 1.0): 1, ('cajun', 1.0): 1, ('spice', 1.0): 1, ('yum', 1.0): 2, ('cajunchicken', 1.0): 1, ('infinit', 1.0): 2, ('fight', 1.0): 4, ('gern', 1.0): 1, ('cikaaa', 1.0): 1, ('maaf', 1.0): 1, ('telat', 1.0): 1, ('ngucapinnya', 1.0): 1, ('maaay', 1.0): 1, ('x16', 1.0): 1, ('viparita', 1.0): 1, ('karani', 1.0): 1, ('legsupthewal', 1.0): 1, ('unwind', 1.0): 1, ('coco', 1.0): 3, ('comfi', 1.0): 1, ('jalulu', 1.0): 1, ('rosh', 1.0): 1, ('gla', 1.0): 1, ('pallavi', 1.0): 1, ('nairobi', 1.0): 1, ('hrdstellobama', 1.0): 1, ('region', 1.0): 2, ('civil', 1.0): 1, ('societi', 1.0): 2, ('globe', 1.0): 1, ('hajur', 1.0): 1, ('yayi', 1.0): 2, (\"must'v\", 1.0): 1, ('nerv', 1.0): 1, ('prelim', 1.0): 1, ('costacc', 1.0): 1, ('nwb', 1.0): 1, ('shud', 1.0): 1, ('cold', 1.0): 2, ('hmu', 1.0): 2, ('cala', 1.0): 1, ('brush', 1.0): 1, ('ego', 1.0): 1, ('wherev', 1.0): 1, ('interact', 1.0): 2, ('dongsaeng', 1.0): 1, ('chorong', 1.0): 1, ('friendship', 1.0): 1, ('impress', 1.0): 3, ('dragon', 1.0): 2, ('duck', 1.0): 5, ('mix', 1.0): 5, ('cheetah', 1.0): 1, ('wagga', 1.0): 2, ('coursework', 1.0): 1, ('lorna', 1.0): 1, ('scan', 1.0): 1, ('x12', 1.0): 2, ('canva', 1.0): 2, ('iqbal', 1.0): 1, ('ima', 1.0): 1, ('hon', 1.0): 1, ('aja', 1.0): 1, ('besi', 1.0): 1, ('chati', 1.0): 1, ('phulani', 1.0): 1, ('swasa', 1.0): 1, ('bahari', 1.0): 1, ('jiba', 1.0): 1, ('mumbai', 1.0): 1, ('gujarat', 1.0): 1, ('distrub', 1.0): 1, ('otherwis', 1.0): 5, ('190cr', 1.0): 1, ('inspit', 1.0): 1, ('highest', 1.0): 1, ('holder', 1.0): 1, ('threaten', 1.0): 1, ('daili', 1.0): 2, ('basi', 1.0): 1, ('vr', 1.0): 1, ('angelo', 1.0): 1, ('quezon', 1.0): 1, ('sweatpant', 1.0): 1, ('farbridg', 1.0): 1, ('segalakatakata', 1.0): 1, ('nixu', 1.0): 1, ('begun', 1.0): 1, ('flint', 1.0): 1, ('üç∞', 1.0): 5, ('separ', 1.0): 1, ('criticis', 1.0): 1, ('gestur', 1.0): 1, ('pedal', 1.0): 1, ('stroke', 1.0): 1, ('caro', 1.0): 1, ('deposit', 1.0): 1, ('secur', 1.0): 2, ('shock', 1.0): 1, ('coff', 1.0): 2, ('tenerina', 1.0): 1, ('auguri', 1.0): 1, ('iso', 1.0): 1, ('certif', 1.0): 1, ('paralyz', 1.0): 1, ('anxieti', 1.0): 1, (\"it'd\", 1.0): 1, ('develop', 1.0): 3, ('spain', 1.0): 2, ('def', 1.0): 1, ('bantim', 1.0): 1, ('fail', 1.0): 5, ('2ban', 1.0): 1, ('x15', 1.0): 1, ('awkward', 1.0): 2, ('ab', 1.0): 1, ('gale', 1.0): 1, ('founder', 1.0): 1, ('loveyaaah', 1.0): 1, ('‚Öõ', 1.0): 1, ('‚Öû', 1.0): 1, ('‚àû', 1.0): 1, ('specialist', 1.0): 1, ('aw', 1.0): 3, ('babyyi', 1.0): 1, ('djstruthmat', 1.0): 1, ('re-cap', 1.0): 1, ('flickr', 1.0): 1, ('tack', 1.0): 2, ('zephbot', 1.0): 1, ('hhahahahaha', 1.0): 1, ('blew', 1.0): 2, ('entir', 1.0): 2, ('vega', 1.0): 3, ('strip', 1.0): 1, ('hahahahahhaha', 1.0): 1, (\"callie'\", 1.0): 1, ('puppi', 1.0): 1, ('owner', 1.0): 2, ('callinganimalabusehotlineasap', 1.0): 1, ('gorefiend', 1.0): 1, ('mythic', 1.0): 1, ('remind', 1.0): 6, ('9:00', 1.0): 1, ('‚ñ™', 1.0): 2, ('Ô∏èbea', 1.0): 1, ('miller', 1.0): 2, ('lockscreen', 1.0): 1, ('mbf', 1.0): 1, ('keesh', 1.0): 1, (\"yesterday'\", 1.0): 1, ('groupi', 1.0): 1, ('bebe', 1.0): 1, ('sizam', 1.0): 1, ('color', 1.0): 5, ('invoic', 1.0): 1, ('kanina', 1.0): 1, ('pong', 1.0): 1, ('umaga', 1.0): 1, ('browser', 1.0): 1, ('typic', 1.0): 2, ('pleass', 1.0): 5, ('leeteuk', 1.0): 1, ('pearl', 1.0): 1, ('thusi', 1.0): 1, ('pour', 1.0): 1, ('milk', 1.0): 2, ('tgv', 1.0): 1, ('pari', 1.0): 5, ('austerlitz', 1.0): 1, ('bloi', 1.0): 1, ('mile', 1.0): 3, ('chateau', 1.0): 1, ('de', 1.0): 1, ('marai', 1.0): 1, ('taxi', 1.0): 1, ('x14', 1.0): 1, ('nom', 1.0): 1, ('enji', 1.0): 1, ('hater', 1.0): 3, ('purchas', 1.0): 2, ('specially-mark', 1.0): 1, ('custard', 1.0): 1, ('sm', 1.0): 1, ('on-pack', 1.0): 1, ('instruct', 1.0): 1, ('tile', 1.0): 1, ('downstair', 1.0): 1, ('kelli', 1.0): 1, ('greek', 1.0): 2, ('petra', 1.0): 1, ('shadowplayloui', 1.0): 1, ('mutual', 1.0): 2, ('cuz', 1.0): 4, ('liveonstream', 1.0): 1, ('lani', 1.0): 1, ('graze', 1.0): 1, ('pride', 1.0): 1, ('bristolart', 1.0): 1, ('in-app', 1.0): 1, ('ensur', 1.0): 1, ('item', 1.0): 2, ('screw', 1.0): 1, ('amber', 1.0): 2, ('43', 1.0): 1, ('hpc', 1.0): 1, ('wip', 1.0): 2, ('sw', 1.0): 1, ('newsround', 1.0): 1, ('hound', 1.0): 1, ('7:40', 1.0): 1, ('ada', 1.0): 1, ('racist', 1.0): 1, ('hulk', 1.0): 1, ('tight', 1.0): 2, ('prayer', 1.0): 3, ('pardon', 1.0): 1, ('phl', 1.0): 1, ('abu', 1.0): 2, ('dhabi', 1.0): 1, ('hihihi', 1.0): 1, ('teamjanuaryclaim', 1.0): 1, ('godonna', 1.0): 1, ('msg', 1.0): 2, ('bowwowchicawowwow', 1.0): 1, ('settl', 1.0): 1, ('dkt', 1.0): 1, ('porch', 1.0): 1, ('uber', 1.0): 2, ('mobil', 1.0): 4, ('applic', 1.0): 3, ('giggl', 1.0): 2, ('bare', 1.0): 3, ('wind', 1.0): 2, ('kahlil', 1.0): 1, ('gibran', 1.0): 1, ('flash', 1.0): 1, ('stiff', 1.0): 1, ('upper', 1.0): 1, ('lip', 1.0): 1, ('britain', 1.0): 1, ('latmon', 1.0): 1, ('endeavour', 1.0): 1, ('ann', 1.0): 2, ('joy', 1.0): 4, ('os', 1.0): 1, ('exploit', 1.0): 1, ('ign', 1.0): 2, ('au', 1.0): 1, ('pubcast', 1.0): 1, ('tengaman', 1.0): 1, ('21', 1.0): 2, ('celebratio', 1.0): 1, ('women', 1.0): 1, ('instal', 1.0): 2, ('glorifi', 1.0): 1, ('infirm', 1.0): 1, ('silli', 1.0): 1, ('suav', 1.0): 1, ('gentlemen', 1.0): 1, ('monthli', 1.0): 1, ('mileag', 1.0): 1, ('target', 1.0): 2, ('samsung', 1.0): 1, ('qualiti', 1.0): 3, ('ey', 1.0): 1, ('beth', 1.0): 2, ('gangster', 1.0): 1, (\"athena'\", 1.0): 1, ('fanci', 1.0): 1, ('wellington', 1.0): 1, ('rich', 1.0): 2, ('christina', 1.0): 1, ('newslett', 1.0): 1, ('zy', 1.0): 1, ('olur', 1.0): 1, ('x13', 1.0): 1, ('flawless', 1.0): 1, ('reaction', 1.0): 2, ('hayli', 1.0): 1, ('edwin', 1.0): 1, ('elvena', 1.0): 1, ('emc', 1.0): 1, ('rubber', 1.0): 3, ('swearword', 1.0): 1, ('infect', 1.0): 1, ('10:16', 1.0): 1, ('wrote', 1.0): 3, ('gan', 1.0): 1, ('brotherhood', 1.0): 1, ('wolf', 1.0): 5, ('pill', 1.0): 1, ('nocturn', 1.0): 1, ('rrp', 1.0): 1, ('18.99', 1.0): 1, ('13.99', 1.0): 1, ('jah', 1.0): 1, ('wobbl', 1.0): 1, ('retard', 1.0): 1, ('50notif', 1.0): 1, ('check-up', 1.0): 1, ('pun', 1.0): 1, ('elit', 1.0): 1, ('camillu', 1.0): 1, ('pleasee', 1.0): 1, ('spare', 1.0): 1, ('tyre', 1.0): 2, ('joke', 1.0): 3, ('ahahah', 1.0): 1, ('shame', 1.0): 1, ('abandon', 1.0): 1, ('disagre', 1.0): 2, ('nowher', 1.0): 2, ('contradict', 1.0): 1, ('chao', 1.0): 1, ('contain', 1.0): 1, ('cranium', 1.0): 1, ('sneaker', 1.0): 1, ('nike', 1.0): 1, ('nikeorigin', 1.0): 1, ('nikeindonesia', 1.0): 1, ('pierojogg', 1.0): 1, ('skoy', 1.0): 1, ('winter', 1.0): 2, ('falkland', 1.0): 1, ('jamie-le', 1.0): 1, ('congraaat', 1.0): 1, ('hooh', 1.0): 1, ('chrome', 1.0): 1, ('storm', 1.0): 1, ('thunderstorm', 1.0): 1, ('circuscircu', 1.0): 1, ('omgg', 1.0): 1, ('tdi', 1.0): 1, ('(-:', 1.0): 2, ('peter', 1.0): 1, ('expel', 1.0): 2, ('boughi', 1.0): 1, ('kernel', 1.0): 1, ('paralysi', 1.0): 1, ('liza', 1.0): 1, ('lol.hook', 1.0): 1, ('vampir', 1.0): 2, ('diari', 1.0): 3, ('twice', 1.0): 1, ('thanq', 1.0): 2, ('goodwil', 1.0): 1, ('vandr', 1.0): 1, ('ash', 1.0): 1, ('debat', 1.0): 3, ('solar', 1.0): 1, ('6-5', 1.0): 1, ('shown', 1.0): 1, ('ek', 1.0): 1, ('taco', 1.0): 2, ('mexico', 1.0): 2, ('viva', 1.0): 1, ('m√©xico', 1.0): 1, ('burger', 1.0): 3, ('thebestangkapuso', 1.0): 1, ('lighter', 1.0): 1, ('tooth', 1.0): 2, ('korean', 1.0): 2, ('netizen', 1.0): 1, ('crueler', 1.0): 1, ('eleph', 1.0): 1, ('marula', 1.0): 1, ('tdif', 1.0): 1, ('shoutout', 1.0): 1, ('shortli', 1.0): 1, ('itsamarvelth', 1.0): 1, (\"japan'\", 1.0): 1, ('artist', 1.0): 1, ('homework', 1.0): 1, ('marco', 1.0): 1, ('herb', 1.0): 1, ('pm', 1.0): 3, ('self', 1.0): 1, ('esteem', 1.0): 1, ('patienc', 1.0): 1, ('sobtian', 1.0): 1, ('cowork', 1.0): 1, ('deathli', 1.0): 1, ('hallow', 1.0): 1, ('supernatur', 1.0): 1, ('consult', 1.0): 1, ('himach', 1.0): 1, ('2.25', 1.0): 1, ('asham', 1.0): 1, ('where.do.i.start', 1.0): 1, ('moviemarathon', 1.0): 1, ('skill', 1.0): 4, ('shadow', 1.0): 1, ('own', 1.0): 1, ('pair', 1.0): 3, (\"it'll\", 1.0): 6, ('cortez', 1.0): 1, ('superstar', 1.0): 1, ('tthank', 1.0): 1, ('colin', 1.0): 1, ('luxuou', 1.0): 1, ('tarryn', 1.0): 1, ('hbdme', 1.0): 1, ('yeeeyyy', 1.0): 1, ('barsostay', 1.0): 1, ('males', 1.0): 1, ('independ', 1.0): 1, ('sum', 1.0): 1, ('debacl', 1.0): 1, ('perfectli', 1.0): 1, ('longer', 1.0): 2, ('amyjackson', 1.0): 1, ('omegl', 1.0): 2, ('countrymus', 1.0): 1, ('five', 1.0): 2, (\"night'\", 1.0): 2, (\"freddy'\", 1.0): 2, ('demo', 1.0): 2, ('pump', 1.0): 2, ('fanboy', 1.0): 1, ('thegrandad', 1.0): 1, ('sidni', 1.0): 1, ('remarriag', 1.0): 1, ('occas', 1.0): 1, ('languag', 1.0): 1, ('java', 1.0): 1, (\"php'\", 1.0): 1, ('notion', 1.0): 1, ('refer', 1.0): 1, ('confus', 1.0): 3, ('ohioan', 1.0): 1, ('stick', 1.0): 2, ('doctor', 1.0): 3, ('offlin', 1.0): 1, ('thesim', 1.0): 1, ('mb', 1.0): 1, ('meaningless', 1.0): 1, ('common', 1.0): 1, ('celebr', 1.0): 9, ('muertosatfring', 1.0): 1, ('emul', 1.0): 1, ('brought', 1.0): 1, ('enemi', 1.0): 2, ('relax', 1.0): 3, ('ou', 1.0): 1, ('pink', 1.0): 2, ('cc', 1.0): 2, ('meooowww', 1.0): 1, ('barkkkiiidee', 1.0): 1, ('bark', 1.0): 1, ('x11', 1.0): 1, ('routin', 1.0): 4, ('alek', 1.0): 1, ('awh', 1.0): 2, ('kumpul', 1.0): 1, ('cantik', 1.0): 1, ('ganteng', 1.0): 1, ('kresna', 1.0): 1, ('jelli', 1.0): 1, ('simon', 1.0): 1, ('lesley', 1.0): 3, ('blood', 1.0): 2, ('panti', 1.0): 1, ('lion', 1.0): 1, ('artworkbyli', 1.0): 1, ('judo', 1.0): 1, ('daredevil', 1.0): 2, ('despond', 1.0): 1, ('re-watch', 1.0): 1, ('welcoma.hav', 1.0): 1, ('favor', 1.0): 5, ('tridon', 1.0): 1, ('21pic', 1.0): 1, ('master', 1.0): 3, ('nim', 1.0): 1, (\"there'r\", 1.0): 1, ('22pic', 1.0): 1, ('kebun', 1.0): 1, ('ubud', 1.0): 1, ('ladyposs', 1.0): 1, ('xoxoxo', 1.0): 1, ('sneak', 1.0): 3, ('peek', 1.0): 2, ('inbox', 1.0): 1, ('happyweekend', 1.0): 1, ('therealgolden', 1.0): 1, ('47', 1.0): 1, ('girlfriendsmya', 1.0): 1, ('ppl', 1.0): 2, ('closest', 1.0): 1, ('njoy', 1.0): 1, ('followingg', 1.0): 1, ('privat', 1.0): 1, ('pusher', 1.0): 1, ('stun', 1.0): 4, ('wooohooo', 1.0): 1, ('cuss', 1.0): 1, ('teenag', 1.0): 1, ('ace', 1.0): 1, ('sauc', 1.0): 3, ('livi', 1.0): 1, ('fowl', 1.0): 1, ('oliviafowl', 1.0): 1, ('891', 1.0): 1, ('burnout', 1.0): 1, ('johnforceo', 1.0): 1, ('matthew', 1.0): 1, ('provok', 1.0): 1, ('indiankultur', 1.0): 1, ('oppos', 1.0): 1, ('biker', 1.0): 1, ('lyk', 1.0): 1, ('gud', 1.0): 4, ('weight', 1.0): 6, ('bcu', 1.0): 1, ('rubbish', 1.0): 1, ('veggi', 1.0): 2, ('steph', 1.0): 1, ('nj', 1.0): 1, ('x10', 1.0): 1, ('cohes', 1.0): 1, ('gossip', 1.0): 2, ('alex', 1.0): 3, ('heswifi', 1.0): 1, ('7am', 1.0): 1, ('wub', 1.0): 1, ('cerbchan', 1.0): 1, ('jarraaa', 1.0): 1, ('morrrn', 1.0): 1, ('snooz', 1.0): 1, ('clicksco', 1.0): 1, ('gay', 1.0): 4, ('lesbian', 1.0): 2, ('rigid', 1.0): 1, ('theocrat', 1.0): 1, ('wing', 1.0): 1, ('fundamentalist', 1.0): 1, ('islamist', 1.0): 1, ('brianaaa', 1.0): 1, ('brianazabrocki', 1.0): 1, ('sky', 1.0): 2, ('batb', 1.0): 1, ('clap', 1.0): 3, ('whilst', 1.0): 1, ('aki', 1.0): 1, ('thencerest', 1.0): 2, ('547', 1.0): 2, ('indiemus', 1.0): 5, ('sexyjudi', 1.0): 3, ('pussi', 1.0): 4, ('sexo', 1.0): 3, ('humid', 1.0): 1, ('87', 1.0): 1, ('sloppi', 1.0): 1, (\"second'\", 1.0): 1, ('stock', 1.0): 3, ('marmit', 1.0): 2, ('x9', 1.0): 1, ('nic', 1.0): 3, ('taft', 1.0): 1, ('finalist', 1.0): 1, ('lotteri', 1.0): 1, ('award', 1.0): 3, ('usagi', 1.0): 1, ('looov', 1.0): 1, ('wowww', 1.0): 2, ('üíô', 1.0): 8, ('üíö', 1.0): 8, ('üíï', 1.0): 12, ('lepa', 1.0): 1, ('sembuh', 1.0): 1, ('sibuk', 1.0): 1, ('balik', 1.0): 1, ('kin', 1.0): 1, ('gotham', 1.0): 1, ('sunnyday', 1.0): 1, ('dudett', 1.0): 1, ('cost', 1.0): 1, ('flippin', 1.0): 1, ('fortun', 1.0): 1, ('divinediscont', 1.0): 1, (';}', 1.0): 1, ('amnot', 1.0): 1, ('autofollow', 1.0): 3, ('teamfollowback', 1.0): 4, ('geer', 1.0): 1, ('bat', 1.0): 2, ('mz', 1.0): 1, ('yang', 1.0): 2, ('deennya', 1.0): 1, ('jehwan', 1.0): 1, ('11:00', 1.0): 1, ('ashton', 1.0): 1, ('‚úß', 1.0): 12, ('ÔΩ°', 1.0): 4, ('chelni', 1.0): 2, ('datz', 1.0): 1, ('jeremi', 1.0): 1, ('fmt', 1.0): 1, ('dat', 1.0): 3, ('heartbeat', 1.0): 1, ('clutch', 1.0): 1, ('üê¢', 1.0): 2, ('besteverdoctorwhoepisod', 1.0): 1, ('relev', 1.0): 1, ('puke', 1.0): 1, ('proper', 1.0): 1, ('x8', 1.0): 1, ('sublimin', 1.0): 1, ('eatmeat', 1.0): 1, ('brewproject', 1.0): 1, ('lovenafianna', 1.0): 1, ('mr', 1.0): 7, ('lewi', 1.0): 1, ('clock', 1.0): 1, ('3:02', 1.0): 2, ('muslim', 1.0): 1, ('prophet', 1.0): 1, ('ÿ∫ÿ±ÿØŸÑŸä', 1.0): 4, ('is.h', 1.0): 1, ('mistak', 1.0): 4, ('understood', 1.0): 1, ('politician', 1.0): 1, ('argu', 1.0): 1, ('intellect', 1.0): 1, ('shiva', 1.0): 1, ('mp3', 1.0): 1, ('standrew', 1.0): 1, ('sandcastl', 1.0): 1, ('ewok', 1.0): 1, ('nate', 1.0): 2, ('brawl', 1.0): 1, ('rear', 1.0): 1, ('nake', 1.0): 1, ('choke', 1.0): 1, ('heck', 1.0): 1, ('gun', 1.0): 2, ('associ', 1.0): 1, ('um', 1.0): 1, ('endow', 1.0): 1, ('ai', 1.0): 1, ('sikandar', 1.0): 1, ('pti', 1.0): 1, ('standwdik', 1.0): 1, ('westandwithik', 1.0): 1, ('starbuck', 1.0): 2, ('logo', 1.0): 2, ('renew', 1.0): 1, ('chariti', 1.0): 1, ('ÿ¨ŸÖÿπÿ©_ŸÖÿ®ÿßÿ±ŸÉÿ©', 1.0): 1, ('hoki', 1.0): 1, ('biz', 1.0): 1, ('non', 1.0): 1, ('america', 1.0): 1, ('california', 1.0): 1, ('01:16', 1.0): 1, ('45gameplay', 1.0): 2, ('ilovey', 1.0): 2, ('vex', 1.0): 1, ('iger', 1.0): 1, ('leicaq', 1.0): 1, ('leica', 1.0): 1, ('dudee', 1.0): 1, ('persona', 1.0): 1, ('yepp', 1.0): 1, ('5878e503', 1.0): 1, ('x7', 1.0): 1, ('greg', 1.0): 1, ('posey', 1.0): 1, ('miami', 1.0): 1, ('james_yammouni', 1.0): 1, ('breakdown', 1.0): 1, ('materi', 1.0): 2, ('thorin', 1.0): 1, ('hunt', 1.0): 1, ('choroo', 1.0): 1, ('nahi', 1.0): 2, ('aztec', 1.0): 1, ('princess', 1.0): 2, ('raini', 1.0): 1, ('kingfish', 1.0): 1, ('chinua', 1.0): 1, ('acheb', 1.0): 1, ('intellectu', 1.0): 2, ('liquid', 1.0): 1, ('melbournetrip', 1.0): 1, ('taxikitchen', 1.0): 1, ('nooow', 1.0): 2, ('mcdo', 1.0): 1, ('everywher', 1.0): 2, ('dreamer', 1.0): 1, ('tanisha', 1.0): 1, ('1nonli', 1.0): 1, ('attitud', 1.0): 1, ('kindl', 1.0): 2, ('flame', 1.0): 1, ('convict', 1.0): 1, ('bar', 1.0): 1, ('repath', 1.0): 2, ('adi', 1.0): 1, ('stefani', 1.0): 1, ('sg1', 1.0): 1, ('lightbox', 1.0): 1, ('ran', 1.0): 2, ('incorrect', 1.0): 1, ('apologist', 1.0): 1, ('x6', 1.0): 1, ('vuli', 1.0): 1, ('01:15', 1.0): 1, ('batman', 1.0): 1, ('pearson', 1.0): 1, ('reput', 1.0): 2, ('nikkei', 1.0): 1, ('woodford', 1.0): 1, ('vscocam', 1.0): 1, ('vscoph', 1.0): 1, ('vscogood', 1.0): 1, ('vscophil', 1.0): 1, ('vscocousin', 1.0): 1, ('yaap', 1.0): 1, ('urwelc', 1.0): 1, ('neon', 1.0): 1, ('pant', 1.0): 1, ('haaa', 1.0): 1, ('will', 1.0): 2, ('auspost', 1.0): 1, ('openfollow', 1.0): 1, ('rp', 1.0): 2, ('eng', 1.0): 1, ('y≈´j≈ç-cosplay', 1.0): 1, ('luxembourg', 1.0): 1, ('bunni', 1.0): 1, ('broadcast', 1.0): 1, ('needa', 1.0): 1, ('gal', 1.0): 3, ('bend', 1.0): 3, ('heaven', 1.0): 2, ('score', 1.0): 2, ('januari', 1.0): 1, ('hanabutl', 1.0): 1, ('kikhorni', 1.0): 1, ('interraci', 1.0): 1, ('makeup', 1.0): 1, ('chu', 1.0): 1, (\"weekend'\", 1.0): 1, ('punt', 1.0): 1, ('horserac', 1.0): 1, ('hors', 1.0): 2, ('horseracingtip', 1.0): 1, ('guitar', 1.0): 1, ('cocoar', 1.0): 1, ('brief', 1.0): 1, ('introduct', 1.0): 1, ('earliest', 1.0): 1, ('indian', 1.0): 1, ('subcontin', 1.0): 1, ('bfr', 1.0): 1, ('maurya', 1.0): 1, ('jordanian', 1.0): 1, ('00962778381', 1.0): 1, ('838', 1.0): 1, ('tenyai', 1.0): 1, ('hee', 1.0): 2, ('ss', 1.0): 1, ('semi', 1.0): 1, ('atp', 1.0): 2, ('wimbledon', 1.0): 2, ('feder', 1.0): 1, ('nadal', 1.0): 1, ('monfil', 1.0): 1, ('handsom', 1.0): 2, ('cilic', 1.0): 3, ('firm', 1.0): 1, ('potenti', 1.0): 3, ('nyc', 1.0): 1, ('chillin', 1.0): 2, ('tail', 1.0): 2, ('kitten', 1.0): 1, ('garret', 1.0): 1, ('baz', 1.0): 1, ('leo', 1.0): 2, ('xst', 1.0): 1, ('centrifug', 1.0): 1, ('etern', 1.0): 3, ('forgiv', 1.0): 2, ('kangin', 1.0): 1, ('ÿ®ŸÜÿØÿ±', 1.0): 1, ('ÿßŸÑÿπŸÜÿ≤Ÿä', 1.0): 1, ('kristin', 1.0): 1, ('cass', 1.0): 1, ('surajettan', 1.0): 1, ('kashi', 1.0): 1, ('ashwathi', 1.0): 1, ('mommi', 1.0): 2, ('tirth', 1.0): 1, ('brambhatt', 1.0): 1, ('snooker', 1.0): 1, ('compens', 1.0): 1, ('theoper', 1.0): 1, ('479', 1.0): 1, ('premiostumundo', 1.0): 2, ('philosoph', 1.0): 1, ('x5', 1.0): 1, ('graphic', 1.0): 2, ('level', 1.0): 1, ('aug', 1.0): 3, ('excl', 1.0): 1, ('raw', 1.0): 1, ('weeni', 1.0): 1, ('annoyingbabi', 1.0): 1, ('lazi', 1.0): 2, ('cosi', 1.0): 1, ('client_amends_edit', 1.0): 1, ('_5_final_final_fin', 1.0): 1, ('pdf', 1.0): 1, ('mauliat', 1.0): 1, ('ito', 1.0): 2, ('okkay', 1.0): 1, ('knock', 1.0): 3, (\"soloist'\", 1.0): 1, ('ryu', 1.0): 1, ('saera', 1.0): 1, ('pinkeu', 1.0): 1, ('angri', 1.0): 3, ('screencap', 1.0): 1, ('jonghyun', 1.0): 1, ('seungyeon', 1.0): 1, ('cnblue', 1.0): 1, ('mbc', 1.0): 1, ('wgm', 1.0): 1, ('masa', 1.0): 2, ('entrepreneurship', 1.0): 1, ('empow', 1.0): 1, ('limpopo', 1.0): 1, ('pict', 1.0): 1, ('norapowel', 1.0): 1, ('hornykik', 1.0): 2, ('livesex', 1.0): 1, ('pumpkin', 1.0): 1, ('thrice', 1.0): 1, ('patron', 1.0): 1, ('ventur', 1.0): 1, ('deathcur', 1.0): 1, ('boob', 1.0): 1, ('blame', 1.0): 1, ('dine', 1.0): 1, ('modern', 1.0): 1, ('grill', 1.0): 1, ('disk', 1.0): 1, ('nt4', 1.0): 1, ('iirc', 1.0): 1, ('ux', 1.0): 1, ('refin', 1.0): 1, ('zdp', 1.0): 1, ('didnt', 1.0): 2, ('justic', 1.0): 1, ('daw', 1.0): 1, ('tine', 1.0): 1, ('gensan', 1.0): 1, ('frightl', 1.0): 1, ('undead', 1.0): 1, ('plush', 1.0): 1, ('cushion', 1.0): 1, ('nba', 1.0): 3, ('2k15', 1.0): 3, ('mypark', 1.0): 3, ('chronicl', 1.0): 4, ('gryph', 1.0): 3, ('volum', 1.0): 3, ('ellen', 1.0): 1, ('degener', 1.0): 1, ('shirt', 1.0): 1, ('mint', 1.0): 1, ('superdri', 1.0): 1, ('berangkaat', 1.0): 1, ('lagiii', 1.0): 1, ('siguro', 1.0): 1, ('un', 1.0): 1, ('kesa', 1.0): 1, ('lotsa', 1.0): 2, ('organis', 1.0): 2, ('4am', 1.0): 1, ('fingers-cross', 1.0): 1, ('deep', 1.0): 1, ('htaccess', 1.0): 1, ('file', 1.0): 2, ('adf', 1.0): 1, ('womad', 1.0): 1, ('gran', 1.0): 1, ('canaria', 1.0): 1, ('gig', 1.0): 1, ('twist', 1.0): 1, ('youv', 1.0): 1, ('teamnatur', 1.0): 1, ('huni', 1.0): 1, ('yayayayay', 1.0): 1, ('yt', 1.0): 2, ('convent', 1.0): 1, ('brighton', 1.0): 1, ('slay', 1.0): 1, ('nicknam', 1.0): 1, ('babygirl', 1.0): 1, ('regard', 1.0): 2, ('himmat', 1.0): 1, ('karain', 1.0): 2, ('baat', 1.0): 1, ('meri', 1.0): 1, ('hotee-mi', 1.0): 1, ('uncl', 1.0): 1, ('tongu', 1.0): 1, ('pronounc', 1.0): 1, ('nativ', 1.0): 1, ('american', 1.0): 2, ('proverb', 1.0): 1, ('lovabl', 1.0): 1, ('yesha', 1.0): 1, ('montoya', 1.0): 1, ('eagerli', 1.0): 1, ('payment', 1.0): 1, ('suprem', 1.0): 1, ('leon', 1.0): 1, ('ks', 1.0): 2, ('randi', 1.0): 1, ('9bi', 1.0): 1, ('physiqu', 1.0): 1, ('shave', 1.0): 1, ('uncut', 1.0): 1, ('boi', 1.0): 1, ('cheapest', 1.0): 1, ('regular', 1.0): 3, ('printer', 1.0): 3, ('nz', 1.0): 1, ('larg', 1.0): 4, ('format', 1.0): 1, ('10/10', 1.0): 1, ('senior', 1.0): 1, ('raid', 1.0): 2, ('conserv', 1.0): 1, ('batteri', 1.0): 1, ('comfort', 1.0): 2, ('swt', 1.0): 1, ('reservations@sandsbeach.eu', 1.0): 1, ('localgaragederbi', 1.0): 1, ('campu', 1.0): 1, ('subgam', 1.0): 1, ('faceit', 1.0): 1, ('snpcaht', 1.0): 1, ('hakhakhak', 1.0): 1, ('t___t', 1.0): 1, (\"kyungsoo'\", 1.0): 1, ('3d', 1.0): 2, ('properti', 1.0): 2, ('agent', 1.0): 1, ('accur', 1.0): 1, ('descript', 1.0): 1, ('theori', 1.0): 1, ('x4', 1.0): 1, ('15.90', 1.0): 1, ('yvett', 1.0): 1, ('author', 1.0): 2, ('mwf', 1.0): 1, ('programm', 1.0): 1, ('taal', 1.0): 1, ('lake', 1.0): 1, ('2emt', 1.0): 1, ('¬´', 1.0): 2, ('scurri', 1.0): 1, ('agil', 1.0): 1, ('solut', 1.0): 1, ('sme', 1.0): 1, ('omar', 1.0): 1, ('biggest', 1.0): 5, ('kamaal', 1.0): 1, ('amm', 1.0): 1, ('3am', 1.0): 1, ('hopehousekid', 1.0): 1, ('pitmantrain', 1.0): 1, ('walkersmithway', 1.0): 1, ('keepitloc', 1.0): 2, ('sehun', 1.0): 1, ('se100lead', 1.0): 1, ('unev', 1.0): 1, ('sofa', 1.0): 1, ('surf', 1.0): 1, ('cunt', 1.0): 1, ('rescoop', 1.0): 1, ('multiraci', 1.0): 1, ('fk', 1.0): 1, ('narrow', 1.0): 1, ('warlock', 1.0): 1, ('balloon', 1.0): 3, ('mj', 1.0): 1, ('madison', 1.0): 1, ('beonknockknock', 1.0): 1, ('con-gradu', 1.0): 1, ('gent', 1.0): 1, ('bitchfac', 1.0): 1, ('üòí', 1.0): 1, ('organ', 1.0): 1, ('12pm', 1.0): 2, ('york', 1.0): 2, ('nearest', 1.0): 1, ('lendal', 1.0): 1, ('pikami', 1.0): 1, ('captur', 1.0): 1, ('fulton', 1.0): 1, ('sheen', 1.0): 1, ('baloney', 1.0): 1, ('unvarnish', 1.0): 1, ('laid', 1.0): 2, ('thick', 1.0): 1, ('blarney', 1.0): 1, ('flatteri', 1.0): 1, ('thin', 1.0): 1, ('sachin', 1.0): 1, ('unimport', 1.0): 1, ('context', 1.0): 1, ('dampen', 1.0): 1, ('yu', 1.0): 1, ('rocket', 1.0): 1, ('narendra', 1.0): 1, ('modi', 1.0): 1, ('aaaand', 1.0): 1, (\"team'\", 1.0): 1, ('macauley', 1.0): 1, ('howev', 1.0): 3, ('x3', 1.0): 1, ('wheeen', 1.0): 1, ('heechul', 1.0): 1, ('toast', 1.0): 2, ('coffee-weekday', 1.0): 1, ('9-11', 1.0): 1, ('sail', 1.0): 1, (\"friday'\", 1.0): 1, ('commerci', 1.0): 1, ('insur', 1.0): 1, ('requir', 1.0): 2, ('lookfortheo', 1.0): 1, ('cl', 1.0): 1, ('thou', 1.0): 1, ('april', 1.0): 2, ('airforc', 1.0): 1, ('clark', 1.0): 1, ('field', 1.0): 1, ('pampanga', 1.0): 1, ('troll', 1.0): 1, ('‚ö°', 1.0): 1, ('brow', 1.0): 1, ('oili', 1.0): 1, ('maricarljanah', 1.0): 1, ('6:15', 1.0): 1, ('degre', 1.0): 3, ('fahrenheit', 1.0): 1, ('üç∏', 1.0): 7, ('‚ï≤', 1.0): 4, ('‚îÄ', 1.0): 8, ('‚ï±', 1.0): 5, ('üç§', 1.0): 4, ('‚ï≠', 1.0): 4, ('‚ïÆ', 1.0): 4, ('‚îì', 1.0): 2, ('‚î≥', 1.0): 1, ('‚î£', 1.0): 1, ('‚ï∞', 1.0): 3, ('‚ïØ', 1.0): 3, ('‚îó', 1.0): 2, ('‚îª', 1.0): 1, ('stool', 1.0): 1, ('toppl', 1.0): 1, ('findyourfit', 1.0): 1, ('prefer', 1.0): 2, ('whomosexu', 1.0): 1, ('stack', 1.0): 1, ('pandora', 1.0): 3, ('digitalexet', 1.0): 1, ('digitalmarket', 1.0): 1, ('sociamedia', 1.0): 1, ('nb', 1.0): 1, ('bom', 1.0): 1, ('dia', 1.0): 1, ('todo', 1.0): 1, ('forklift', 1.0): 1, ('warehous', 1.0): 1, ('worker', 1.0): 1, ('lsceen', 1.0): 1, ('immatur', 1.0): 1, ('gandhi', 1.0): 1, ('grassi', 1.0): 1, ('feetblog', 1.0): 2, ('daughter', 1.0): 3, ('4yr', 1.0): 1, ('old-porridg', 1.0): 1, ('fiend', 1.0): 1, ('2nite', 1.0): 1, ('comp', 1.0): 1, ('vike', 1.0): 1, ('t20blast', 1.0): 1, ('np', 1.0): 1, ('tax', 1.0): 1, ('ooohh', 1.0): 1, ('petjam', 1.0): 1, ('virtual', 1.0): 2, ('pounc', 1.0): 1, ('bentek', 1.0): 1, ('agn', 1.0): 1, ('socialmedia@dpdgroup.co.uk', 1.0): 1, ('sam', 1.0): 3, ('fruiti', 1.0): 1, ('vodka', 1.0): 2, ('sellyourcarin', 1.0): 2, ('5word', 1.0): 2, ('chaloniklo', 1.0): 2, ('pic.twitter.com/jxz2lbv6o', 1.0): 1, (\"paperwhite'\", 1.0): 1, ('laser-lik', 1.0): 1, ('focu', 1.0): 1, ('ghost', 1.0): 3, ('tagsforlikesapp', 1.0): 2, ('instagood', 1.0): 2, ('tbt', 1.0): 1, ('socket', 1.0): 1, ('spanner', 1.0): 1, ('üò¥', 1.0): 1, ('pglcsgo', 1.0): 1, ('x2', 1.0): 1, ('tend', 1.0): 1, ('crave', 1.0): 1, ('slower', 1.0): 1, ('sjw', 1.0): 1, ('cakehamp', 1.0): 1, ('glow', 1.0): 2, ('yayyy', 1.0): 1, ('merced', 1.0): 1, ('hood', 1.0): 1, ('badg', 1.0): 1, ('host', 1.0): 1, ('drone', 1.0): 1, ('blow', 1.0): 1, ('ignor', 1.0): 1, ('retali', 1.0): 1, ('bolling', 1.0): 1, (\"where'\", 1.0): 1, ('denmark', 1.0): 1, ('whitey', 1.0): 1, ('cultur', 1.0): 2, ('course', 1.0): 1, ('intro', 1.0): 2, ('graphicdesign', 1.0): 1, ('videograph', 1.0): 1, ('space', 1.0): 2, (\"ted'\", 1.0): 1, ('bogu', 1.0): 1, ('1000', 1.0): 1, ('hahahaaah', 1.0): 1, ('owli', 1.0): 1, ('afternon', 1.0): 1, ('whangarei', 1.0): 1, ('kati', 1.0): 2, ('paulin', 1.0): 1, ('traffick', 1.0): 1, ('wors', 1.0): 3, ('henc', 1.0): 1, ('express', 1.0): 1, ('wot', 1.0): 1, ('hand-lett', 1.0): 1, ('roof', 1.0): 1, ('eas', 1.0): 1, ('2/2', 1.0): 1, ('sour', 1.0): 1, ('dough', 1.0): 1, ('egypt', 1.0): 1, ('hubbi', 1.0): 2, ('sakin', 1.0): 1, ('six', 1.0): 1, ('christma', 1.0): 2, ('avril', 1.0): 1, ('n04j', 1.0): 1, ('25', 1.0): 1, ('prosecco', 1.0): 1, ('pech', 1.0): 1, ('micro', 1.0): 1, ('catspj', 1.0): 1, ('4:15', 1.0): 1, ('lazyweekend', 1.0): 1, ('overdu', 1.0): 1, ('mice', 1.0): 1, ('üíÉ', 1.0): 3, ('jurass', 1.0): 1, ('ding', 1.0): 1, ('nila', 1.0): 1, ('8)', 1.0): 1, ('cooki', 1.0): 1, ('shir', 1.0): 1, ('0', 1.0): 3, ('hale', 1.0): 1, ('cheshir', 1.0): 1, ('decor', 1.0): 1, ('lemm', 1.0): 2, ('rec', 1.0): 1, ('ingat', 1.0): 1, ('din', 1.0): 2, ('mono', 1.0): 1, ('kathryn', 1.0): 1, ('jr', 1.0): 1, ('hsr', 1.0): 1, ('base', 1.0): 3, ('major', 1.0): 1, ('sugarrush', 1.0): 1, ('knit', 1.0): 1, ('partli', 1.0): 1, ('homegirl', 1.0): 1, ('nanci', 1.0): 1, ('fenja', 1.0): 1, ('aapk', 1.0): 1, ('benchmark', 1.0): 1, ('ke', 1.0): 1, ('hisaab', 1.0): 1, ('ho', 1.0): 1, ('gaya', 1.0): 1, ('ofc', 1.0): 1, ('rtss', 1.0): 1, ('hwait', 1.0): 1, ('titanfal', 1.0): 1, ('xbox', 1.0): 2, ('ultim', 1.0): 2, ('gastronomi', 1.0): 1, ('newblogpost', 1.0): 1, ('foodiefriday', 1.0): 1, ('foodi', 1.0): 1, ('yoghurt', 1.0): 1, ('pancak', 1.0): 2, ('sabah', 1.0): 3, ('kapima', 1.0): 1, ('gelen', 1.0): 1, ('guzel', 1.0): 1, ('bir', 1.0): 1, ('hediy', 1.0): 1, ('thanx', 1.0): 1, ('üíû', 1.0): 2, ('visa', 1.0): 1, ('parisa', 1.0): 1, ('epiphani', 1.0): 1, ('lit', 1.0): 1, ('em-con', 1.0): 1, ('swore', 1.0): 1, ('0330 333 7234', 1.0): 1, ('kianweareproud', 1.0): 1, ('distract', 1.0): 1, ('dayofarch', 1.0): 1, ('10-20', 1.0): 1, ('bapu', 1.0): 1, ('ivypowel', 1.0): 1, ('newmus', 1.0): 1, ('sexchat', 1.0): 1, ('üçÖ', 1.0): 1, ('pathway', 1.0): 1, ('balkan', 1.0): 1, ('gypsi', 1.0): 1, ('mayhem', 1.0): 1, ('burek', 1.0): 1, ('meat', 1.0): 1, ('gibanica', 1.0): 1, ('pie', 1.0): 1, ('surrey', 1.0): 1, ('afterward', 1.0): 1, ('10.30', 1.0): 1, ('tempor', 1.0): 1, ('void', 1.0): 1, ('stem', 1.0): 1, ('sf', 1.0): 1, ('ykr', 1.0): 1, ('sparki', 1.0): 1, ('40mm', 1.0): 1, ('3.5', 1.0): 1, ('gr', 1.0): 1, ('rockfish', 1.0): 1, ('topwat', 1.0): 1, ('twitlong', 1.0): 1, ('me.so', 1.0): 1, ('jummah', 1.0): 3, ('durood', 1.0): 1, ('pak', 1.0): 1, ('cjradacomateada', 1.0): 2, ('supris', 1.0): 1, ('debut', 1.0): 1, ('shipper', 1.0): 1, ('asid', 1.0): 1, ('housem', 1.0): 1, ('737bigatingconcert', 1.0): 1, ('jedzjab≈Çka', 1.0): 1, ('pijjab≈Çka', 1.0): 1, ('polish', 1.0): 1, ('cider', 1.0): 1, ('mustread', 1.0): 1, ('cricket', 1.0): 1, ('5pm', 1.0): 1, ('queri', 1.0): 2, ('abbi', 1.0): 1, ('sumedh', 1.0): 1, ('sunnah', 1.0): 2, ('ÿπŸÜ', 1.0): 2, ('quad', 1.0): 1, ('bike', 1.0): 1, ('carri', 1.0): 2, ('proprieti', 1.0): 1, ('chronic', 1.0): 1, ('superday', 1.0): 1, ('chocolatey', 1.0): 1, ('yasu', 1.0): 1, ('ooooh', 1.0): 1, ('hallo', 1.0): 2, ('dylan', 1.0): 2, ('laura', 1.0): 1, ('patric', 1.0): 2, ('keepin', 1.0): 1, ('mohr', 1.0): 1, ('guest', 1.0): 1, (\"o'neal\", 1.0): 1, ('tk', 1.0): 1, ('lua', 1.0): 1, ('stone', 1.0): 2, ('quicker', 1.0): 1, ('diet', 1.0): 1, ('sosweet', 1.0): 1, ('nominier', 1.0): 1, ('und', 1.0): 1, ('hardcor', 1.0): 1, ('üòå', 1.0): 1, ('ff__special', 1.0): 1, ('acha', 1.0): 2, ('banda', 1.0): 1, ('‚úå', 1.0): 1, ('bhi', 1.0): 2, ('krta', 1.0): 1, ('beautifully-craft', 1.0): 1, ('mockingbird', 1.0): 1, ('diploma', 1.0): 1, ('blend', 1.0): 3, ('numbero', 1.0): 1, ('lolz', 1.0): 1, ('ambros', 1.0): 1, ('gwinett', 1.0): 1, ('bierc', 1.0): 1, ('ravag', 1.0): 1, ('illadvis', 1.0): 1, ('marriag', 1.0): 1, ('stare', 1.0): 1, ('cynic', 1.0): 2, ('yahuda', 1.0): 1, ('nosmet', 1.0): 1, ('poni', 1.0): 1, ('cuuut', 1.0): 1, (\"f'ing\", 1.0): 1, ('vacant', 1.0): 1, ('hauc', 1.0): 1, ('lovesss', 1.0): 1, ('hiss', 1.0): 1, ('overnight', 1.0): 1, ('cornish', 1.0): 1, ('all-clear', 1.0): 1, ('raincoat', 1.0): 1, ('measur', 1.0): 1, ('wealth', 1.0): 1, ('invest', 1.0): 2, ('garbi', 1.0): 1, ('wash', 1.0): 2, ('refuel', 1.0): 1, ('dunedin', 1.0): 1, ('kall', 1.0): 1, ('rakhi', 1.0): 1, ('12th', 1.0): 2, ('repres', 1.0): 3, ('slovenia', 1.0): 1, ('fridg', 1.0): 2, ('ludlow', 1.0): 1, ('28th', 1.0): 1, ('selway', 1.0): 1, ('submit', 1.0): 1, ('spanish', 1.0): 2, ('90210', 1.0): 1, ('oitnb', 1.0): 1, ('prepar', 1.0): 3, ('condit', 1.0): 1, ('msged', 1.0): 1, ('chiquito', 1.0): 1, ('ohaha', 1.0): 1, ('delhi', 1.0): 1, ('95', 1.0): 1, ('webtogsaward', 1.0): 1, ('grace', 1.0): 2, ('sheffield', 1.0): 1, ('tramlin', 1.0): 1, ('tl', 1.0): 2, ('hack', 1.0): 1, ('lad', 1.0): 1, ('beeepin', 1.0): 1, ('duper', 1.0): 1, ('handl', 1.0): 1, ('critiqu', 1.0): 1, ('contectu', 1.0): 1, ('ultor', 1.0): 2, ('mamaya', 1.0): 1, ('loiyal', 1.0): 1, ('para', 1.0): 1, ('truthfulwordsof', 1.0): 1, ('beanatividad', 1.0): 1, ('nknkkpagpapakumbaba', 1.0): 1, ('birthdaypres', 1.0): 1, ('compliment', 1.0): 1, ('swerv', 1.0): 1, ('goodtim', 1.0): 1, ('sinist', 1.0): 1, ('scare', 1.0): 1, ('tryna', 1.0): 1, ('anonym', 1.0): 1, ('dipsatch', 1.0): 1, ('aunt', 1.0): 1, ('dagga', 1.0): 1, ('burket', 1.0): 1, ('2am', 1.0): 1, ('twine', 1.0): 1, (\"diane'\", 1.0): 1, ('happybirthday', 1.0): 1, ('thanksss', 1.0): 1, ('randomli', 1.0): 1, ('buckinghampalac', 1.0): 1, ('chibi', 1.0): 1, ('maker', 1.0): 1, ('timog', 1.0): 1, ('18th', 1.0): 1, ('otw', 1.0): 1, ('kami', 1.0): 1, ('feelinggood', 1.0): 1, ('demand', 1.0): 2, ('naman', 1.0): 1, ('barkin', 1.0): 1, ('yeap', 1.0): 2, ('onkey', 1.0): 1, ('umma', 1.0): 1, ('pervert', 1.0): 1, ('onyu', 1.0): 1, ('appa', 1.0): 1, ('luci', 1.0): 1, ('horribl', 1.0): 1, ('quantum', 1.0): 1, ('greater', 1.0): 1, ('blockchain', 1.0): 1, ('nowplay', 1.0): 1, ('loftey', 1.0): 1, ('routt', 1.0): 1, ('assia', 1.0): 1, ('.\\n.\\n.', 1.0): 1, ('joint', 1.0): 1, ('futurereleas', 1.0): 1, (\"look'\", 1.0): 1, ('scari', 1.0): 1, ('murder', 1.0): 1, ('mysteri', 1.0): 1, ('comma', 1.0): 1, (\"j'\", 1.0): 1, ('hunni', 1.0): 2, ('diva', 1.0): 1, ('emili', 1.0): 3, ('nathan', 1.0): 1, ('medit', 1.0): 1, ('alumni', 1.0): 1, ('mba', 1.0): 1, ('foto', 1.0): 1, ('what-is-your-fashion', 1.0): 1, ('lorenangel', 1.0): 1, ('kw', 1.0): 2, ('tellanoldjokeday', 1.0): 1, ('reqd', 1.0): 1, ('specul', 1.0): 1, ('consist', 1.0): 4, ('tropic', 1.0): 1, ('startupph', 1.0): 1, ('zodiac', 1.0): 1, ('rapunzel', 1.0): 1, ('therver', 1.0): 1, ('85552', 1.0): 1, ('bestoftheday', 1.0): 1, ('oralsex', 1.0): 1, ('carli', 1.0): 1, ('happili', 1.0): 1, ('contract', 1.0): 1, ('matsu_bouzu', 1.0): 1, ('sonic', 1.0): 2, ('videogam', 1.0): 1, ('harana', 1.0): 1, ('belfast', 1.0): 1, ('danni', 1.0): 1, ('rare', 1.0): 1, ('sponsorship', 1.0): 1, ('aswel', 1.0): 1, ('gigi', 1.0): 1, ('nick', 1.0): 1, ('austin', 1.0): 1, ('youll', 1.0): 1, ('weak', 1.0): 4, ('10,000', 1.0): 1, ('bravo', 1.0): 1, ('iamamonst', 1.0): 1, ('rxthedailysurveyvot', 1.0): 1, ('broke', 1.0): 1, ('ass', 1.0): 1, ('roux', 1.0): 1, ('walkin', 1.0): 1, ('audienc', 1.0): 2, ('pfb', 1.0): 1, ('jute', 1.0): 1, ('walangmakakapigilsakin', 1.0): 1, ('lori', 1.0): 1, ('ehm', 1.0): 1, ('trick', 1.0): 1, ('baekhyun', 1.0): 1, ('eyesmil', 1.0): 1, ('borrow', 1.0): 1, ('knive', 1.0): 1, ('thek', 1.0): 1, ('eventu', 1.0): 1, ('reaapear', 1.0): 1, ('kno', 1.0): 1, ('whet', 1.0): 1, ('gratti', 1.0): 1, ('shorter', 1.0): 1, ('tweetin', 1.0): 1, ('inshallah', 1.0): 1, ('banana', 1.0): 1, ('raspberri', 1.0): 2, ('healthylifestyl', 1.0): 1, ('aint', 1.0): 2, ('skate', 1.0): 1, ('analyz', 1.0): 1, ('varieti', 1.0): 1, ('4:13', 1.0): 1, ('insomnia', 1.0): 1, ('medic', 1.0): 1, ('opposit', 1.0): 1, ('everlast', 1.0): 1, ('yoga', 1.0): 1, ('massag', 1.0): 2, ('osteopath', 1.0): 1, ('trainer', 1.0): 1, ('sharm', 1.0): 1, ('al_master_band', 1.0): 1, ('tbc', 1.0): 1, ('unives', 1.0): 1, ('architectur', 1.0): 1, ('random', 1.0): 1, ('isnt', 1.0): 1, ('typo', 1.0): 1, ('snark', 1.0): 1, ('lession', 1.0): 1, ('drunk', 1.0): 1, ('bruuh', 1.0): 1, ('2week', 1.0): 1, ('50europ', 1.0): 1, ('üá´üá∑', 1.0): 4, ('iov', 1.0): 1, ('accord', 1.0): 1, ('mne', 1.0): 1, ('pchelok', 1.0): 1, ('ja', 1.0): 1, ('=:', 1.0): 2, ('sweetest', 1.0): 1, ('comet', 1.0): 1, ('ahah', 1.0): 1, ('candi', 1.0): 2, ('axio', 1.0): 1, ('rabbit', 1.0): 2, ('nutshel', 1.0): 1, ('taken', 1.0): 1, ('letshavecocktailsafternuclai', 1.0): 1, ('malik', 1.0): 1, ('umair', 1.0): 1, ('canon', 1.0): 1, ('gang', 1.0): 1, ('grind', 1.0): 1, ('thoracicbridg', 1.0): 1, ('5minut', 1.0): 1, ('nonscript', 1.0): 1, ('password', 1.0): 1, ('shoshannavassil', 1.0): 1, ('addmeonsnapchat', 1.0): 1, ('dmme', 1.0): 1, ('mpoint', 1.0): 2, ('soph', 1.0): 1, ('anot', 1.0): 1, ('liao', 1.0): 2, ('ord', 1.0): 1, ('lor', 1.0): 1, ('sibei', 1.0): 1, ('xialan', 1.0): 1, ('thnx', 1.0): 1, ('malfunct', 1.0): 1, ('clown', 1.0): 1, ('joker', 1.0): 1, ('\\U000fec00', 1.0): 1, ('nigth', 1.0): 1, ('estoy', 1.0): 1, ('escuchando', 1.0): 1, ('elsewher', 1.0): 1, ('bipolar', 1.0): 1, ('hahahahahahahahahahahahahaha', 1.0): 1, ('yoohoo', 1.0): 1, ('bajrangibhaijaanstorm', 1.0): 1, ('superhappi', 1.0): 1, ('doll', 1.0): 1, ('energi', 1.0): 1, ('f', 1.0): 3, (\"m'dear\", 1.0): 1, ('emma', 1.0): 2, ('alrd', 1.0): 1, ('dhan', 1.0): 2, ('satguru', 1.0): 1, ('tera', 1.0): 1, ('aasra', 1.0): 1, ('pita', 1.0): 1, ('keeo', 1.0): 1, ('darl', 1.0): 2, ('akarshan', 1.0): 1, ('sweetpea', 1.0): 1, ('gluten', 1.0): 1, ('pastri', 1.0): 2, ('highfiv', 1.0): 1, ('artsi', 1.0): 1, ('verbal', 1.0): 1, ('kaaa', 1.0): 1, ('oxford', 1.0): 2, ('wahoo', 1.0): 1, ('anchor', 1.0): 1, ('partnership', 1.0): 1, ('robbenisland', 1.0): 1, ('whale', 1.0): 1, ('aquat', 1.0): 1, ('safari', 1.0): 1, ('garru', 1.0): 1, ('liara', 1.0): 1, ('appoint', 1.0): 1, ('burnley', 1.0): 1, ('453', 1.0): 1, ('110', 1.0): 2, ('49', 1.0): 1, ('footbal', 1.0): 1, ('fm15', 1.0): 1, ('fmfamili', 1.0): 1, ('aamir', 1.0): 1, ('difficult', 1.0): 1, ('medium', 1.0): 1, ('nva', 1.0): 1, ('minuet', 1.0): 1, ('gamec', 1.0): 1, ('headrest', 1.0): 1, ('pit', 1.0): 1, ('spoken', 1.0): 1, ('advis', 1.0): 1, ('paypoint', 1.0): 1, ('deepthroat', 1.0): 1, ('truli', 1.0): 3, ('bee', 1.0): 2, ('upward', 1.0): 1, ('bound', 1.0): 1, ('movingonup', 1.0): 1, ('aitor', 1.0): 1, ('sn', 1.0): 1, ('ps4', 1.0): 2, ('jawad', 1.0): 1, ('presal', 1.0): 1, ('betcha', 1.0): 1, ('dumb', 1.0): 2, ('butt', 1.0): 1, ('qualki', 1.0): 1, ('808', 1.0): 1, ('milf', 1.0): 1, ('4like', 1.0): 1, ('sexysaturday', 1.0): 1, ('vw', 1.0): 1, ('umpfff', 1.0): 1, ('ca', 1.0): 1, ('domg', 1.0): 1, ('nanti', 1.0): 1, ('difollow', 1.0): 1, ('stubborn', 1.0): 1, ('nothavingit', 1.0): 1, ('klee', 1.0): 1, ('hem', 1.0): 1, ('congrad', 1.0): 1, ('accomplish', 1.0): 1, ('kfcroleplay', 1.0): 3, ('tregaron', 1.0): 1, ('boar', 1.0): 1, ('sweati', 1.0): 1, ('glyon', 1.0): 1, ('üöÆ', 1.0): 1, (\"tee'\", 1.0): 1, ('johnni', 1.0): 1, ('utub', 1.0): 1, (\"video'\", 1.0): 1, ('loss', 1.0): 1, ('combin', 1.0): 2, ('pigeon', 1.0): 1, ('fingerscross', 1.0): 1, ('photobomb', 1.0): 1, ('90', 1.0): 1, ('23', 1.0): 1, ('gimm', 1.0): 1, ('definetli', 1.0): 1, ('exit', 1.0): 1, ('bom-dia', 1.0): 1, ('apod', 1.0): 1, ('ultraviolet', 1.0): 1, ('m31', 1.0): 1, ('jul', 1.0): 1, ('oooh', 1.0): 1, ('yawn', 1.0): 1, ('ftw', 1.0): 1, ('maman', 1.0): 1, ('afterznoon', 1.0): 1, ('tweeep', 1.0): 1, ('abp', 1.0): 2, ('kiya', 1.0): 1, ('van', 1.0): 1, ('olymp', 1.0): 1, ('üò∑', 1.0): 1, ('classi', 1.0): 1, ('attach', 1.0): 1, ('equip', 1.0): 1, ('bobbl', 1.0): 1, ('anu', 1.0): 1, ('mh3', 1.0): 1, ('patch', 1.0): 1, ('psp', 1.0): 1, ('huffpost', 1.0): 1, ('tribut', 1.0): 1, ('h_eartshapedbox', 1.0): 1, ('magictrikband', 1.0): 1, ('magictrik', 1.0): 2, ('roommat', 1.0): 1, ('tami', 1.0): 1, ('b3dk', 1.0): 1, ('7an', 1.0): 1, ('ank', 1.0): 1, ('purpos', 1.0): 1, ('struggl', 1.0): 1, ('eagl', 1.0): 1, ('oceana', 1.0): 1, ('idk', 1.0): 3, ('med', 1.0): 1, ('fridayfauxpa', 1.0): 1, ('subtl', 1.0): 1, ('hint', 1.0): 1, ('prim', 1.0): 1, ('algorithm', 1.0): 1, ('iii', 1.0): 1, ('rosa', 1.0): 1, ('yvw', 1.0): 1, ('here', 1.0): 1, ('boost', 1.0): 1, ('unforgett', 1.0): 1, ('humor', 1.0): 1, (\"mum'\", 1.0): 1, ('hahahhaah', 1.0): 1, ('sombrero', 1.0): 1, ('lost', 1.0): 2, ('spammer', 1.0): 1, ('proceed', 1.0): 1, ('entertain', 1.0): 1, ('100k', 1.0): 1, ('mileston', 1.0): 1, ('judith', 1.0): 1, ('district', 1.0): 1, ('council', 1.0): 1, ('midar', 1.0): 1, ('gender', 1.0): 1, ('ilysm', 1.0): 1, ('zen', 1.0): 1, ('neat', 1.0): 1, ('rider', 1.0): 1, ('fyi', 1.0): 1, ('dig', 1.0): 2, ('üë±üèΩ', 1.0): 1, ('üëΩ', 1.0): 1, ('üå≥', 1.0): 1, ('suspici', 1.0): 1, ('calori', 1.0): 1, ('harder', 1.0): 1, ('jessica', 1.0): 1, ('carina', 1.0): 1, ('francisco', 1.0): 1, ('teret', 1.0): 1, ('potassium', 1.0): 1, ('rehydr', 1.0): 1, ('drinkitallup', 1.0): 1, ('thirstquench', 1.0): 1, ('tapir', 1.0): 1, ('calf', 1.0): 1, ('mealtim', 1.0): 1, ('uhc', 1.0): 1, ('scale', 1.0): 1, ('network', 1.0): 1, ('areal', 1.0): 1, ('extremesport', 1.0): 1, ('quadbik', 1.0): 1, ('bloggersrequir', 1.0): 1, ('bloggersw', 1.0): 1, ('brainer', 1.0): 1, ('mse', 1.0): 1, ('fund', 1.0): 1, ('nooowww', 1.0): 1, ('lile', 1.0): 1, ('tid', 1.0): 1, ('tmi', 1.0): 1, ('deploy', 1.0): 1, ('jule', 1.0): 1, ('betti', 1.0): 1, ('hddc', 1.0): 1, ('salman', 1.0): 1, ('pthht', 1.0): 1, ('lfc', 1.0): 3, ('tope', 1.0): 1, ('xxoo', 1.0): 2, ('russia', 1.0): 2, ('silver-wash', 1.0): 1, ('fritillari', 1.0): 1, ('moon', 1.0): 1, ('ap', 1.0): 2, ('trash', 1.0): 2, ('clever', 1.0): 1, (\"thank'\", 1.0): 1, ('keven', 1.0): 1, ('pastim', 1.0): 1, ('ashramcal', 1.0): 1, ('ontrack', 1.0): 1, ('german', 1.0): 1, ('subtitl', 1.0): 1, ('pinter', 1.0): 1, ('morninggg', 1.0): 1, ('üê∂', 1.0): 1, ('pete', 1.0): 1, ('awesome-o', 1.0): 1, ('multipl', 1.0): 1, ('cya', 1.0): 1, ('harrog', 1.0): 1, ('jet', 1.0): 1, ('supplier', 1.0): 1, ('req', 1.0): 1, ('fridayloug', 1.0): 1, ('4thstreetmus', 1.0): 1, ('hawaii', 1.0): 1, ('kick', 1.0): 1, ('deepli', 1.0): 1, ('john@timney.eclipse.co.uk', 1.0): 1, ('thousand', 1.0): 2, ('newspap', 1.0): 1, ('lew', 1.0): 1, ('nah', 1.0): 1, ('fallout', 1.0): 2, ('technic', 1.0): 1, ('gunderson', 1.0): 1, ('europa', 1.0): 1, ('thoroughli', 1.0): 1, ('script', 1.0): 1, ('overtak', 1.0): 1, ('motorway', 1.0): 1, ('thu', 1.0): 1, ('niteflirt', 1.0): 1, ('hbu', 1.0): 2, ('bowl', 1.0): 1, ('chri', 1.0): 2, ('niall', 1.0): 2, ('94', 1.0): 1, ('ik', 1.0): 1, ('stydia', 1.0): 1, ('nawazuddin', 1.0): 1, ('siddiqu', 1.0): 1, ('nomnomnom', 1.0): 1, ('dukefreebiefriday', 1.0): 1, ('z', 1.0): 1, ('insyaallah', 1.0): 1, ('ham', 1.0): 1, ('villa', 1.0): 1, ('brum', 1.0): 1, ('deni', 1.0): 1, ('vagina', 1.0): 1, ('rli', 1.0): 1, ('izzi', 1.0): 1, ('mitch', 1.0): 1, ('minn', 1.0): 1, ('recently.websit', 1.0): 1, ('coolingtow', 1.0): 1, ('soon.thank', 1.0): 1, ('showinginterest', 1.0): 1, ('multicolor', 1.0): 1, ('wid', 1.0): 1, ('wedg', 1.0): 1, ('motiv', 1.0): 1, ('nnnnot', 1.0): 1, (\"gf'\", 1.0): 1, ('bluesidemenxix', 1.0): 1, ('ardent', 1.0): 1, ('mooorn', 1.0): 1, ('wuppert', 1.0): 1, ('fridayfunday', 1.0): 1, ('re-sign', 1.0): 1, ('chalkhil', 1.0): 1, ('midday', 1.0): 1, ('carter', 1.0): 1, ('remedi', 1.0): 1, ('atrack', 1.0): 1, ('christ', 1.0): 1, ('badminton', 1.0): 1, (\"littl'un\", 1.0): 1, ('ikprideofpak', 1.0): 1, ('janjua', 1.0): 1, ('pimpl', 1.0): 1, ('forehead', 1.0): 1, ('volcano', 1.0): 1, ('mag', 1.0): 1, ('miryenda', 1.0): 1, (\"technology'\", 1.0): 1, ('touch√©today', 1.0): 1, ('idownload', 1.0): 1, ('25ish', 1.0): 1, ('snowbal', 1.0): 1, ('nd', 1.0): 1, ('expir', 1.0): 1, ('6gb', 1.0): 1, ('loveu', 1.0): 1, ('morefuninthephilippin', 1.0): 1, ('laho', 1.0): 1, ('caramoan', 1.0): 1, ('kareem', 1.0): 1, ('surah', 1.0): 1, ('kahaf', 1.0): 1, ('melani', 1.0): 1, ('bosch', 1.0): 1, ('machin', 1.0): 1, (\"week'\", 1.0): 1, ('refollow', 1.0): 1, ('üòé', 1.0): 1, ('üíÅüèª', 1.0): 1, ('relaps', 1.0): 1, ('prada', 1.0): 2, ('punjabiswillgetit', 1.0): 1, ('hitter', 1.0): 1, ('mass', 1.0): 2, ('shoud', 1.0): 1, ('1:12', 1.0): 1, ('ughtm', 1.0): 1, ('545', 1.0): 1, ('kissm', 1.0): 1, ('likeforfollow', 1.0): 1, ('overwhelm', 1.0): 1, ('groupmat', 1.0): 1, ('75', 1.0): 2, ('kyunk', 1.0): 1, ('aitchison', 1.0): 1, ('curvi', 1.0): 1, ('mont', 1.0): 1, ('doa', 1.0): 1, ('header', 1.0): 1, ('speaker', 1.0): 3, ('avoid', 1.0): 1, ('laboratori', 1.0): 1, ('idc', 1.0): 1, ('fuckin', 1.0): 2, ('wooo', 1.0): 2, ('neobyt', 1.0): 1, ('pirat', 1.0): 1, ('takedown', 1.0): 1, ('indirag', 1.0): 1, ('judiciari', 1.0): 1, ('commit', 1.0): 4, ('govt', 1.0): 1, ('polici', 1.0): 1, ('rbi', 1.0): 1, ('similar', 1.0): 1, (\"thought'\", 1.0): 1, ('progress', 1.0): 1, ('transfer', 1.0): 1, ('gg', 1.0): 1, ('defenit', 1.0): 1, ('nofx', 1.0): 1, ('friskyfiday', 1.0): 1, ('yipee', 1.0): 1, ('shed', 1.0): 1, ('incent', 1.0): 1, ('vege', 1.0): 1, ('marin', 1.0): 1, ('gz', 1.0): 1, ('rajeev', 1.0): 1, ('hvng', 1.0): 1, ('funfil', 1.0): 1, ('friday.it', 1.0): 1, ('ws', 1.0): 1, ('reali', 1.0): 1, ('diff', 1.0): 1, ('kabir.fel', 1.0): 1, ('dresden', 1.0): 1, ('germani', 1.0): 1, ('plot', 1.0): 1, ('tdf', 1.0): 1, ('üç∑', 1.0): 2, ('‚òÄ', 1.0): 2, ('üö≤', 1.0): 2, ('minion', 1.0): 2, ('slot', 1.0): 1, (\"b'day\", 1.0): 1, ('isabella', 1.0): 1, ('okeyyy', 1.0): 1, ('vddd', 1.0): 1, (');', 1.0): 1, ('selfee', 1.0): 1, ('insta', 1.0): 1, ('üôÜ', 1.0): 1, ('üôå', 1.0): 1, ('üòõ', 1.0): 1, ('üêí', 1.0): 1, ('üòù', 1.0): 1, ('hhahhaaa', 1.0): 1, ('jeez', 1.0): 1, ('teamcannib', 1.0): 1, ('teamspacewhalingisthebest', 1.0): 1, ('fitfa', 1.0): 1, ('identifi', 1.0): 1, ('pharmaci', 1.0): 1, ('verylaterealis', 1.0): 1, ('iwishiknewbett', 1.0): 1, ('satisfi', 1.0): 1, ('ess-aych-eye-te', 1.0): 1, ('supposedli', 1.0): 1, ('üëç', 1.0): 1, ('immedi', 1.0): 1, (\"foxy'\", 1.0): 1, ('instrument', 1.0): 1, ('alon', 1.0): 2, ('goldcoast', 1.0): 1, ('lelomustfal', 1.0): 1, ('meal', 1.0): 1, ('5g', 1.0): 1, ('liker', 1.0): 1, ('newdress', 1.0): 1, ('resist', 1.0): 1, ('fot', 1.0): 1, ('troy', 1.0): 1, ('twitterfollowerswhatsup', 1.0): 1, ('happyfriedday', 1.0): 1, ('keepsafealway', 1.0): 1, ('loveyeah', 1.0): 1, ('emojasp_her', 1.0): 1, ('vanilla', 1.0): 1, ('sidemen', 1.0): 1, ('yaaayyy', 1.0): 1, ('friendaaa', 1.0): 1, ('bulb', 1.0): 5, ('corn', 1.0): 6, ('1tbps4', 1.0): 1, ('divin', 1.0): 1, ('wheeli', 1.0): 1, ('bin', 1.0): 1, ('ubericecream', 1.0): 1, ('messengerforaday', 1.0): 1, ('kyli', 1.0): 1, ('toilet', 1.0): 1, ('ikaw', 1.0): 1, ('musta', 1.0): 1, ('cheatmat', 1.0): 1, ('kyuhyun', 1.0): 1, ('ghanton', 1.0): 1, ('easy.get', 1.0): 1, ('5:30', 1.0): 1, ('therein', 1.0): 1, ('majalah', 1.0): 1, ('dominiqu', 1.0): 1, ('lamp', 1.0): 1, ('a-foot', 1.0): 1, ('revamp', 1.0): 1, ('brainchild', 1.0): 1, ('confid', 1.0): 1, ('confin', 1.0): 1, ('colorado', 1.0): 1, ('goodyear', 1.0): 1, ('upto', 1.0): 1, ('cashback', 1.0): 1, ('yourewelcom', 1.0): 1, ('nightli', 1.0): 1, ('simpin', 1.0): 1, ('sketchbook', 1.0): 1, ('4wild', 1.0): 1, ('colorpencil', 1.0): 1, ('cray', 1.0): 1, ('6:30', 1.0): 1, ('imma', 1.0): 3, ('ob', 1.0): 1, ('11h', 1.0): 1, ('kino', 1.0): 1, ('adult', 1.0): 1, ('kardamena', 1.0): 1, ('samo', 1.0): 1, ('greec', 1.0): 1, ('caesar', 1.0): 1, ('salad', 1.0): 1, ('tad', 1.0): 1, ('bland', 1.0): 1, ('respond', 1.0): 1, ('okk', 1.0): 1, ('den', 1.0): 1, ('allov', 1.0): 1, ('hangout', 1.0): 1, ('whoever', 1.0): 1, ('tourist', 1.0): 1, ('‚ôå', 1.0): 1, ('kutiyapanti', 1.0): 1, ('profession', 1.0): 1, ('boomshot', 1.0): 1, ('fuh', 1.0): 1, ('yeeey', 1.0): 1, ('donot', 1.0): 1, ('expos', 1.0): 1, ('lipstick', 1.0): 1, ('cran', 1.0): 1, ('prayr', 1.0): 1, ('‡∑Ñ‡∑ô‡∂Ω', 1.0): 1, ('‡∑Ñ‡∑Ä‡∑î‡∂Ω', 1.0): 1, ('onemochaonelov', 1.0): 1, ('southpaw', 1.0): 1, ('geniu', 1.0): 1, ('stroma', 1.0): 1, ('üî¥', 1.0): 1, ('younow', 1.0): 1, ('jonah', 1.0): 1, ('jareddd', 1.0): 1, ('postcod', 1.0): 1, ('talkmobil', 1.0): 1, ('huha', 1.0): 1, ('transform', 1.0): 1, ('sword', 1.0): 3, ('misread', 1.0): 1, ('richard', 1.0): 1, ('ibiza', 1.0): 1, ('birthdaymoneyforjesusjuic', 1.0): 1, ('ytb', 1.0): 1, ('tutori', 1.0): 1, ('construct', 1.0): 2, ('critic', 1.0): 1, ('ganesha', 1.0): 1, ('textur', 1.0): 1, ('photographi', 1.0): 1, ('hinduism', 1.0): 1, ('hindugod', 1.0): 1, ('elephantgod', 1.0): 1, ('selfish', 1.0): 1, ('bboy', 1.0): 1, ('cardgam', 1.0): 1, ('pixelart', 1.0): 1, ('gamedesign', 1.0): 1, ('indiedev', 1.0): 1, ('pixel_daili', 1.0): 1, ('plateau', 1.0): 1, ('laguna', 1.0): 1, ('tha', 1.0): 4, ('bahot', 1.0): 1, ('baje', 1.0): 1, ('raat', 1.0): 1, ('liya', 1.0): 1, ('hath', 1.0): 1, ('ghant', 1.0): 1, ('itna', 1.0): 2, ('bana', 1.0): 1, ('paya', 1.0): 1, ('uta', 1.0): 1, ('manga', 1.0): 1, ('jamuna', 1.0): 1, ('\\\\:', 1.0): 1, ('swiftma', 1.0): 1, ('trion', 1.0): 1, ('forum', 1.0): 1, ('b-day', 1.0): 1, ('disgust', 1.0): 1, ('commodor', 1.0): 1, ('annabel', 1.0): 1, ('bridg', 1.0): 1, ('quest', 1.0): 1, ('borderland', 1.0): 1, ('wanderrook', 1.0): 1, ('gm', 1.0): 1, ('preciou', 1.0): 2, ('mizz', 1.0): 1, ('bleedgreen', 1.0): 1, ('‚úåüèª', 1.0): 1, ('sophia', 1.0): 1, ('chicago', 1.0): 1, ('honeymoon', 1.0): 1, (\"da'esh\", 1.0): 1, ('co-ord', 1.0): 1, ('fsa', 1.0): 1, ('estat', 1.0): 1, (\"when'\", 1.0): 1, ('dusti', 1.0): 1, ('tunisia', 1.0): 2, (\"class'\", 1.0): 1, ('irrit', 1.0): 1, ('fiverr', 1.0): 1, ('gina', 1.0): 1, ('soproud', 1.0): 1, ('enought', 1.0): 1, ('hole', 1.0): 1, ('melbourneburg', 1.0): 1, ('arianna', 1.0): 1, ('esai', 1.0): 1, ('rotterdam', 1.0): 1, ('jordi', 1.0): 1, ('clasi', 1.0): 1, ('horni', 1.0): 1, ('salon', 1.0): 1, ('bleach', 1.0): 1, ('olaplex', 1.0): 1, ('damag', 1.0): 1, ('teamwork', 1.0): 1, ('zitecofficestori', 1.0): 1, ('Îã§Ïáº', 1.0): 1, ('colleagu', 1.0): 1, ('eb', 1.0): 1, (\"t'would\", 1.0): 1, ('tweetup', 1.0): 1, ('detect', 1.0): 1, ('jonathancreek', 1.0): 1, ('dvr', 1.0): 1, ('kat', 1.0): 1, ('rarer', 1.0): 1, ('okkk', 1.0): 1, ('frend', 1.0): 1, ('milt', 1.0): 1, ('mario', 1.0): 1, ('rewatch', 1.0): 1, ('1600', 1.0): 1, ('sige', 1.0): 1, ('punta', 1.0): 1, ('kayo', 1.0): 1, ('nooo', 1.0): 1, ('prompt', 1.0): 1, ('t-mobil', 1.0): 1, ('orang', 1.0): 1, ('ee', 1.0): 1, ('teapot', 1.0): 1, ('hotter', 1.0): 1, ('¬ª', 1.0): 1, ('londoutrad', 1.0): 1, ('kal', 1.0): 1, ('wayward', 1.0): 1, ('pine', 1.0): 1, ('muscl', 1.0): 1, ('ilikeit', 1.0): 1, ('belong', 1.0): 1, ('watford', 1.0): 1, ('enterpris', 1.0): 1, ('cube', 1.0): 1, ('particp', 1.0): 1, ('saudi', 1.0): 1, ('arabia', 1.0): 1, ('recogn', 1.0): 1, ('fanbas', 1.0): 3, ('bailona', 1.0): 3, ('responsibilti', 1.0): 1, ('sunlight', 1.0): 1, ('tiger', 1.0): 1, ('elev', 1.0): 1, ('horror', 1.0): 1, ('bitchesss', 1.0): 1, ('shitti', 1.0): 1, ('squash', 1.0): 1, ('becca', 1.0): 1, ('delta', 1.0): 1, ('nut', 1.0): 1, ('yun', 1.0): 1, ('joe', 1.0): 1, ('dirt', 1.0): 1, ('sharon', 1.0): 1, ('medicin', 1.0): 1, ('ttyl', 1.0): 1, ('gav', 1.0): 1, ('linda', 1.0): 1, ('3hr', 1.0): 1, ('tym', 1.0): 2, ('dieback', 1.0): 1, ('endit', 1.0): 1, ('minecon', 1.0): 1, ('sere', 1.0): 1, ('joerin', 1.0): 1, ('joshan', 1.0): 1, ('tandem', 1.0): 1, ('ligao', 1.0): 1, ('albay', 1.0): 1, ('bcyc', 1.0): 1, ('lnh', 1.0): 1, ('sat', 1.0): 1, ('honorari', 1.0): 1, ('alac', 1.0): 1, ('skelo_ghost', 1.0): 1, ('madadagdagan', 1.0): 1, ('bmc', 1.0): 1, ('11:11', 1.0): 2, ('embarrass', 1.0): 1, ('entropi', 1.0): 1, ('evolut', 1.0): 2, ('loop', 1.0): 1, ('eva', 1.0): 1, ('camden', 1.0): 1, ('uhh', 1.0): 1, ('scoup', 1.0): 1, ('jren', 1.0): 1, ('nuest', 1.0): 1, ('lovelayyy', 1.0): 1, ('kidney', 1.0): 1, ('neuer', 1.0): 1, ('spray', 1.0): 1, ('donnae.strydom@westerncape.gov.za', 1.0): 1, ('uni', 1.0): 1, ('uff', 1.0): 1, ('karhi', 1.0): 1, ('thi', 1.0): 1, ('juaquin', 1.0): 1, ('v3nzor99', 1.0): 1, ('shell', 1.0): 1, ('heyi', 1.0): 1, ('flavor', 1.0): 1, ('thakyou', 1.0): 1, ('beatriz', 1.0): 1, ('cancel', 1.0): 1, ('puff', 1.0): 1, ('egg', 1.0): 2, ('tart', 1.0): 1, ('chai', 1.0): 1, ('mtr', 1.0): 1, ('alyssa', 1.0): 1, ('rub', 1.0): 1, ('tummi', 1.0): 1, ('zelda', 1.0): 1, ('ive', 1.0): 1, ('üéÇ', 1.0): 1, ('jiva', 1.0): 1, ('üçπ', 1.0): 1, ('üçª', 1.0): 1, ('mubbarak', 1.0): 1, ('deborah', 1.0): 1, ('coupon', 1.0): 1, ('colourdeb', 1.0): 1, ('purpl', 1.0): 1, (\"chippy'\", 1.0): 1, ('vessel', 1.0): 1, ('ps', 1.0): 2, ('vintag', 1.0): 1, ('‚ú´', 1.0): 4, ('Àö', 1.0): 4, ('¬∑', 1.0): 4, ('‚úµ', 1.0): 4, ('‚äπ', 1.0): 4, ('1710', 1.0): 1, ('gooffeanotter', 1.0): 1, ('kiksex', 1.0): 1, ('mugshot', 1.0): 1, ('token', 1.0): 1, ('maritimen', 1.0): 1, ('rh', 1.0): 1, ('tatton', 1.0): 1, ('jump_julia', 1.0): 1, ('malema', 1.0): 1, ('fren', 1.0): 1, ('nuf', 1.0): 1, ('teas', 1.0): 1, ('alien', 1.0): 2, ('closer', 1.0): 1, ('monitor', 1.0): 1, ('kimmi', 1.0): 1, (\"channel'\", 1.0): 1, ('planetbollywoodnew', 1.0): 1, ('epi', 1.0): 1, ('tricki', 1.0): 1, ('be-shak', 1.0): 1, ('chenoweth', 1.0): 1, ('oodl', 1.0): 1, ('hailey', 1.0): 1, ('cra≈∫i', 1.0): 1, ('sƒôxxx√ø', 1.0): 1, ('c√∏√¥l', 1.0): 1, ('runway', 1.0): 1, ('gooodnight', 1.0): 1, ('iv', 1.0): 1, ('ri', 1.0): 1, ('jayci', 1.0): 1, ('karaok', 1.0): 1, ('ltsw', 1.0): 1, ('giant', 1.0): 1, ('1709', 1.0): 1, ('refus', 1.0): 1, ('collagen', 1.0): 1, ('2win', 1.0): 1, ('hopetowin', 1.0): 1, ('inventori', 1.0): 1, ('loveforfood', 1.0): 1, ('foodforthought', 1.0): 1, ('thoughtfortheday', 1.0): 1, ('carp', 1.0): 1, ('diem', 1.0): 1, ('nath', 1.0): 1, ('ning', 1.0): 1, ('although', 1.0): 1, ('harm', 1.0): 1, ('stormi', 1.0): 1, ('sync', 1.0): 1, ('devic', 1.0): 1, ('mess', 1.0): 1, ('nylon', 1.0): 1, ('gvb', 1.0): 1, ('cd', 1.0): 1, ('mountain.titl', 1.0): 1, ('unto', 1.0): 1, ('theworldwouldchang', 1.0): 1, ('categori', 1.0): 1, ('mah', 1.0): 1, ('panel', 1.0): 1, (\"i'am\", 1.0): 1, ('80-1', 1.0): 1, ('1708', 1.0): 1, ('neenkin', 1.0): 1, ('masterpiec', 1.0): 1, ('debit', 1.0): 1, ('beagl', 1.0): 1, ('‚ô´', 1.0): 1, ('feat', 1.0): 1, ('charli', 1.0): 1, ('puth', 1.0): 1, ('wiz', 1.0): 1, ('khalifa', 1.0): 1, ('svu', 1.0): 1, ('darker', 1.0): 1, ('berni', 1.0): 1, ('henri', 1.0): 1, ('trap', 1.0): 1, ('tommi', 1.0): 1, (\"vivian'\", 1.0): 1, ('transpar', 1.0): 1, ('bitcoin', 1.0): 1, ('insight', 1.0): 1, ('ping', 1.0): 1, ('masquerad', 1.0): 1, ('zorroreturm', 1.0): 1, ('1707', 1.0): 1, ('pk', 1.0): 1, ('hay', 1.0): 1, ('jacquelin', 1.0): 1, ('passion', 1.0): 1, ('full-fledg', 1.0): 1, ('workplac', 1.0): 1, ('venu', 1.0): 1, ('lago', 1.0): 1, ('luxord', 1.0): 1, ('potato', 1.0): 1, ('hundr', 1.0): 1, ('cite', 1.0): 1, ('academ', 1.0): 1, ('pokiri', 1.0): 1, ('1nenokkadin', 1.0): 1, ('heritag', 1.0): 1, ('wood', 1.0): 1, ('beleaf', 1.0): 1, ('spnfamili', 1.0): 1, ('spn', 1.0): 1, ('alwayskeepfight', 1.0): 1, ('jaredpadalecki', 1.0): 1, ('jensenackl', 1.0): 1, ('peasant', 1.0): 2, ('ahahha', 1.0): 1, ('distant', 1.0): 1, ('shout-out', 1.0): 1, ('adulthood', 1.0): 1, ('hopeless', 0.0): 2, ('tmr', 0.0): 3, (':(', 0.0): 4571, ('everyth', 0.0): 17, ('kid', 0.0): 20, ('section', 0.0): 3, ('ikea', 0.0): 1, ('cute', 0.0): 43, ('shame', 0.0): 19, (\"i'm\", 0.0): 343, ('nearli', 0.0): 3, ('19', 0.0): 8, ('2', 0.0): 41, ('month', 0.0): 23, ('heart', 0.0): 27, ('slide', 0.0): 1, ('wast', 0.0): 5, ('basket', 0.0): 1, ('‚Äú', 0.0): 15, ('hate', 0.0): 57, ('japanes', 0.0): 4, ('call', 0.0): 29, ('bani', 0.0): 2, ('‚Äù', 0.0): 11, ('dang', 0.0): 2, ('start', 0.0): 44, ('next', 0.0): 40, ('week', 0.0): 56, ('work', 0.0): 133, ('oh', 0.0): 92, ('god', 0.0): 15, ('babi', 0.0): 47, ('face', 0.0): 20, ('make', 0.0): 102, ('smile', 0.0): 10, ('neighbour', 0.0): 1, ('motor', 0.0): 1, ('ask', 0.0): 29, ('said', 0.0): 33, ('updat', 0.0): 11, ('search', 0.0): 3, ('sialan', 0.0): 1, ('athabasca', 0.0): 2, ('glacier', 0.0): 2, ('1948', 0.0): 1, (':-(', 0.0): 493, ('jasper', 0.0): 1, ('jaspernationalpark', 0.0): 1, ('alberta', 0.0): 1, ('explorealberta', 0.0): 1, ('‚Ä¶', 0.0): 16, ('realli', 0.0): 131, ('good', 0.0): 101, ('g', 0.0): 8, ('idea', 0.0): 10, ('never', 0.0): 57, ('go', 0.0): 224, ('meet', 0.0): 31, ('mare', 0.0): 1, ('ivan', 0.0): 1, ('happi', 0.0): 25, ('trip', 0.0): 11, ('keep', 0.0): 34, ('safe', 0.0): 5, ('see', 0.0): 124, ('soon', 0.0): 45, ('tire', 0.0): 50, ('hahahah', 0.0): 3, ('knee', 0.0): 2, ('replac', 0.0): 4, ('get', 0.0): 232, ('day', 0.0): 149, ('ouch', 0.0): 3, ('relat', 0.0): 2, ('sweet', 0.0): 7, ('n', 0.0): 21, ('sour', 0.0): 2, ('kind', 0.0): 11, ('bi-polar', 0.0): 1, ('peopl', 0.0): 75, ('life', 0.0): 33, ('...', 0.0): 331, ('cuz', 0.0): 4, ('full', 0.0): 16, ('pleass', 0.0): 2, ('im', 0.0): 129, ('sure', 0.0): 31, ('tho', 0.0): 28, ('feel', 0.0): 158, ('stupid', 0.0): 8, (\"can't\", 0.0): 180, ('seem', 0.0): 15, ('grasp', 0.0): 1, ('basic', 0.0): 2, ('digit', 0.0): 8, ('paint', 0.0): 3, ('noth', 0.0): 26, (\"i'v\", 0.0): 77, ('research', 0.0): 1, ('help', 0.0): 54, ('lord', 0.0): 2, ('lone', 0.0): 9, ('someon', 0.0): 57, ('talk', 0.0): 45, ('guy', 0.0): 62, ('girl', 0.0): 28, ('assign', 0.0): 5, ('project', 0.0): 3, ('üò©', 0.0): 14, ('want', 0.0): 246, ('play', 0.0): 48, ('video', 0.0): 23, ('game', 0.0): 28, ('watch', 0.0): 77, ('movi', 0.0): 24, ('choreograph', 0.0): 1, ('hard', 0.0): 35, ('email', 0.0): 10, ('link', 0.0): 12, ('still', 0.0): 124, ('say', 0.0): 63, ('longer', 0.0): 12, ('avail', 0.0): 13, ('cri', 0.0): 46, ('bc', 0.0): 50, ('miss', 0.0): 301, ('mingm', 0.0): 1, ('much', 0.0): 139, ('sorri', 0.0): 148, ('mom', 0.0): 13, ('far', 0.0): 18, ('away', 0.0): 28, (\"we'r\", 0.0): 30, ('truli', 0.0): 5, ('flight', 0.0): 6, ('friend', 0.0): 39, ('happen', 0.0): 51, ('sad', 0.0): 123, ('dog', 0.0): 17, ('pee', 0.0): 2, ('‚Äô', 0.0): 27, ('bag', 0.0): 8, ('take', 0.0): 49, ('newwin', 0.0): 1, ('15', 0.0): 10, ('doushit', 0.0): 1, ('late', 0.0): 27, ('suck', 0.0): 23, ('sick', 0.0): 43, ('plan', 0.0): 17, ('first', 0.0): 27, ('gundam', 0.0): 1, ('night', 0.0): 46, ('nope', 0.0): 6, ('dollar', 0.0): 1, ('üò≠', 0.0): 29, ('listen', 0.0): 18, ('back', 0.0): 122, ('old', 0.0): 16, ('show', 0.0): 26, ('know', 0.0): 131, ('weird', 0.0): 10, ('got', 0.0): 104, ('u', 0.0): 193, ('leav', 0.0): 42, ('might', 0.0): 11, ('give', 0.0): 36, ('pale', 0.0): 2, ('imit', 0.0): 1, ('went', 0.0): 32, ('sea', 0.0): 1, ('massiv', 0.0): 4, ('fuck', 0.0): 58, ('rash', 0.0): 1, ('bodi', 0.0): 12, ('pain', 0.0): 21, ('thing', 0.0): 52, ('ever', 0.0): 30, ('home', 0.0): 63, ('hi', 0.0): 34, ('absent', 0.0): 1, ('gran', 0.0): 2, ('knew', 0.0): 6, ('care', 0.0): 20, ('tell', 0.0): 26, ('love', 0.0): 152, ('wish', 0.0): 91, ('would', 0.0): 70, ('sequel', 0.0): 1, ('busi', 0.0): 28, ('sa', 0.0): 15, ('school', 0.0): 32, ('time', 0.0): 166, ('yah', 0.0): 3, ('xx', 0.0): 18, ('ouucchhh', 0.0): 1, ('one', 0.0): 148, ('wisdom', 0.0): 2, ('teeth', 0.0): 6, ('come', 0.0): 91, ('frighten', 0.0): 1, ('case', 0.0): 6, ('pret', 0.0): 1, ('wkwkw', 0.0): 1, ('verfi', 0.0): 1, ('activ', 0.0): 6, ('forget', 0.0): 8, ('follow', 0.0): 262, ('member', 0.0): 6, ('thank', 0.0): 107, ('join', 0.0): 8, ('goodby', 0.0): 14, ('¬¥', 0.0): 4, ('chain', 0.0): 1, ('‚Äî', 0.0): 26, ('sentir-s', 0.0): 1, ('incompleta', 0.0): 1, ('okay', 0.0): 38, ('..', 0.0): 108, ('wednesday', 0.0): 5, ('marvel', 0.0): 1, ('thwart', 0.0): 1, ('awh', 0.0): 3, (\"what'\", 0.0): 15, ('chanc', 0.0): 16, ('zant', 0.0): 1, ('need', 0.0): 106, ('someth', 0.0): 28, ('x', 0.0): 39, (\"when'\", 0.0): 1, ('birthday', 0.0): 23, ('worst', 0.0): 14, ('part', 0.0): 11, ('bad', 0.0): 73, ('audraesar', 0.0): 1, ('sushi', 0.0): 3, ('pic', 0.0): 15, ('tl', 0.0): 8, ('drive', 0.0): 16, ('craaazzyy', 0.0): 2, ('pop', 0.0): 3, ('like', 0.0): 228, ('helium', 0.0): 1, ('balloon', 0.0): 1, ('climatechang', 0.0): 5, ('cc', 0.0): 6, (\"california'\", 0.0): 1, ('power', 0.0): 6, ('influenti', 0.0): 1, ('air', 0.0): 3, ('pollut', 0.0): 1, ('watchdog', 0.0): 1, ('califor', 0.0): 1, ('elhaida', 0.0): 1, ('rob', 0.0): 2, ('juri', 0.0): 1, ('came', 0.0): 16, ('10th', 0.0): 1, ('televot', 0.0): 1, ('idaho', 0.0): 2, ('restrict', 0.0): 2, ('fish', 0.0): 2, ('despit', 0.0): 2, ('region', 0.0): 2, ('drought-link', 0.0): 1, ('die-of', 0.0): 1, ('abrupt', 0.0): 1, ('climat', 0.0): 1, ('chang', 0.0): 27, ('may', 0.0): 16, ('doom', 0.0): 2, ('mammoth', 0.0): 1, ('megafauna', 0.0): 1, ('sc', 0.0): 3, (\"australia'\", 0.0): 1, ('dirtiest', 0.0): 2, ('station', 0.0): 3, ('consid', 0.0): 5, ('clean', 0.0): 6, ('energi', 0.0): 3, ('biomass', 0.0): 1, (\"ain't\", 0.0): 5, ('easi', 0.0): 6, ('green', 0.0): 7, ('golf', 0.0): 1, ('cours', 0.0): 7, ('california', 0.0): 1, ('ulti', 0.0): 1, ('well', 0.0): 56, ('mine', 0.0): 12, ('gonna', 0.0): 51, ('sexi', 0.0): 14, ('prexi', 0.0): 1, ('kindergarten', 0.0): 1, ('hungri', 0.0): 19, ('cant', 0.0): 47, ('find', 0.0): 53, ('book', 0.0): 20, ('sane', 0.0): 1, ('liter', 0.0): 15, ('three', 0.0): 7, ('loung', 0.0): 1, ('event', 0.0): 4, ('turn', 0.0): 17, ('boss', 0.0): 5, ('hozier', 0.0): 1, (\"that'\", 0.0): 61, ('true', 0.0): 22, ('soooner', 0.0): 1, ('ahh', 0.0): 7, ('fam', 0.0): 3, ('respectlost', 0.0): 1, ('hypercholesteloremia', 0.0): 1, ('ok', 0.0): 33, ('look', 0.0): 100, ('gift', 0.0): 11, ('calibraska', 0.0): 1, ('actual', 0.0): 24, ('genuin', 0.0): 2, ('contend', 0.0): 1, ('head', 0.0): 23, ('alway', 0.0): 56, ('hurt', 0.0): 41, ('stay', 0.0): 24, ('lmao', 0.0): 13, ('older', 0.0): 5, ('sound', 0.0): 19, ('upset', 0.0): 11, ('infinit', 0.0): 10, ('ao', 0.0): 1, ('stick', 0.0): 1, ('8th', 0.0): 1, ('either', 0.0): 13, ('seriou', 0.0): 8, ('yun', 0.0): 1, ('eh', 0.0): 4, ('room', 0.0): 11, ('way', 0.0): 42, ('hot', 0.0): 15, ('havent', 0.0): 11, ('found', 0.0): 11, ('handsom', 0.0): 2, ('jack', 0.0): 3, ('draw', 0.0): 2, ('shit', 0.0): 36, ('cut', 0.0): 14, ('encor', 0.0): 4, ('4thwin', 0.0): 4, ('baymax', 0.0): 1, ('french', 0.0): 4, ('mixer', 0.0): 1, ('üíú', 0.0): 6, ('wft', 0.0): 1, ('awesom', 0.0): 5, ('replay', 0.0): 1, ('parti', 0.0): 15, ('promot', 0.0): 3, ('music', 0.0): 16, ('bank', 0.0): 9, ('short', 0.0): 11, ('boy', 0.0): 18, ('order', 0.0): 16, ('receiv', 0.0): 7, ('hub', 0.0): 1, ('nearest', 0.0): 1, ('deliv', 0.0): 3, ('today', 0.0): 108, ('1/2', 0.0): 3, ('mum', 0.0): 14, ('loud', 0.0): 2, ('final', 0.0): 35, ('parasyt', 0.0): 1, ('alll', 0.0): 1, ('zayniscomingbackonjuli', 0.0): 23, ('26', 0.0): 24, ('bye', 0.0): 8, ('era', 0.0): 1, ('„ÄÇ', 0.0): 3, ('œâ', 0.0): 1, ('„Äç', 0.0): 2, ('‚à†', 0.0): 2, ('):', 0.0): 6, ('nathann', 0.0): 1, ('üíï', 0.0): 7, ('hug', 0.0): 29, ('üòä', 0.0): 9, ('beauti', 0.0): 11, ('dieididieieiei', 0.0): 1, ('stage', 0.0): 15, ('mean', 0.0): 43, ('hello', 0.0): 13, ('lion', 0.0): 3, ('think', 0.0): 75, ('screw', 0.0): 4, ('netflix', 0.0): 5, ('chill', 0.0): 7, ('di', 0.0): 7, ('ervin', 0.0): 1, ('ohh', 0.0): 8, ('yeah', 0.0): 41, ('hope', 0.0): 102, ('accept', 0.0): 2, ('offer', 0.0): 10, ('desper', 0.0): 2, ('year', 0.0): 46, ('snapchat', 0.0): 79, ('amargolonnard', 0.0): 2, ('kikhorni', 0.0): 13, ('snapm', 0.0): 4, ('tagsforlik', 0.0): 5, ('batalladelosgallo', 0.0): 2, ('webcamsex', 0.0): 4, ('ugh', 0.0): 26, ('stream', 0.0): 24, ('duti', 0.0): 3, (\"u'v\", 0.0): 1, ('gone', 0.0): 24, ('alien', 0.0): 1, ('aww', 0.0): 21, ('wanna', 0.0): 94, ('sorka', 0.0): 1, ('funer', 0.0): 4, ('text', 0.0): 15, ('phone', 0.0): 34, ('sunni', 0.0): 1, ('nonexist', 0.0): 1, ('wowza', 0.0): 1, ('fah', 0.0): 1, ('taylor', 0.0): 3, ('crop', 0.0): 1, ('boo', 0.0): 5, ('count', 0.0): 7, ('new', 0.0): 51, ('guitar', 0.0): 1, ('jonghyun', 0.0): 1, ('hyung', 0.0): 1, ('pleas', 0.0): 275, ('predict', 0.0): 2, ('sj', 0.0): 3, ('nomin', 0.0): 1, ('vs', 0.0): 4, ('pl', 0.0): 45, ('dude', 0.0): 12, ('calm', 0.0): 3, ('brace', 0.0): 5, ('sir', 0.0): 5, ('plu', 0.0): 4, ('4', 0.0): 18, ('shock', 0.0): 3, ('omggg', 0.0): 2, ('yall', 0.0): 4, ('deserv', 0.0): 8, ('whenev', 0.0): 3, ('spend', 0.0): 8, ('smoke', 0.0): 3, ('end', 0.0): 40, ('fall', 0.0): 16, ('asleep', 0.0): 25, ('1', 0.0): 26, ('point', 0.0): 14, ('close', 0.0): 20, ('grand', 0.0): 1, ('whyyi', 0.0): 7, ('long', 0.0): 38, ('must', 0.0): 15, ('annoy', 0.0): 11, ('evan', 0.0): 1, ('option', 0.0): 3, ('opt', 0.0): 1, (\"who'\", 0.0): 7, ('giveaway', 0.0): 3, ('muster', 0.0): 1, ('merch', 0.0): 4, ('ah', 0.0): 18, ('funni', 0.0): 6, ('drink', 0.0): 7, ('savanna', 0.0): 1, ('straw', 0.0): 1, ('ignor', 0.0): 16, ('yester', 0.0): 1, ('afternoon', 0.0): 3, ('sleep', 0.0): 90, ('ye', 0.0): 48, ('sadli', 0.0): 11, ('when', 0.0): 2, ('album', 0.0): 16, ('last', 0.0): 72, ('chocol', 0.0): 8, ('consum', 0.0): 1, ('werk', 0.0): 1, ('morn', 0.0): 31, ('foreal', 0.0): 1, ('wesen', 0.0): 1, ('uwes', 0.0): 1, ('mj', 0.0): 1, ('üòÇ', 0.0): 24, ('catch', 0.0): 9, ('onlin', 0.0): 20, ('enough', 0.0): 24, ('haha', 0.0): 30, (\"he'\", 0.0): 23, ('bosen', 0.0): 1, ('die', 0.0): 21, ('egg', 0.0): 4, ('benni', 0.0): 1, ('sometim', 0.0): 16, ('followback', 0.0): 6, ('huhu', 0.0): 17, ('understand', 0.0): 15, ('badli', 0.0): 12, ('scare', 0.0): 16, ('&gt;:(', 0.0): 47, ('al', 0.0): 4, ('kati', 0.0): 3, ('zaz', 0.0): 1, ('ami', 0.0): 2, ('lot', 0.0): 27, ('diari', 0.0): 1, ('read', 0.0): 20, ('rehash', 0.0): 1, ('websit', 0.0): 7, ('mushroom', 0.0): 1, ('piec', 0.0): 4, ('except', 0.0): 5, ('reach', 0.0): 3, ('anyway', 0.0): 12, ('vicki', 0.0): 1, ('omg', 0.0): 63, ('wtf', 0.0): 13, ('lip', 0.0): 3, ('virgin', 0.0): 2, ('your', 0.0): 8, ('45', 0.0): 1, ('hahah', 0.0): 6, ('ninasti', 0.0): 1, ('tsktsk', 0.0): 1, ('oppa', 0.0): 4, ('wont', 0.0): 9, ('dick', 0.0): 5, ('kawaii', 0.0): 1, ('manli', 0.0): 1, ('xbox', 0.0): 3, ('alreadi', 0.0): 52, ('comfi', 0.0): 1, ('bed', 0.0): 12, ('youu', 0.0): 2, ('sigh', 0.0): 13, ('lol', 0.0): 43, ('potato', 0.0): 1, ('fri', 0.0): 7, ('guess', 0.0): 14, (\"y'all\", 0.0): 2, ('ugli', 0.0): 9, ('asf', 0.0): 1, ('huh', 0.0): 7, ('eish', 0.0): 1, ('ive', 0.0): 11, ('quit', 0.0): 9, ('lost', 0.0): 25, ('twitter', 0.0): 30, ('mojo', 0.0): 1, ('dont', 0.0): 53, ('mara', 0.0): 1, ('neh', 0.0): 2, ('fever', 0.0): 7, ('&lt;3', 0.0): 25, ('poor', 0.0): 35, ('bb', 0.0): 7, ('abl', 0.0): 22, ('associ', 0.0): 1, ('councillor', 0.0): 1, ('confer', 0.0): 2, ('weekend', 0.0): 25, ('skype', 0.0): 6, ('account', 0.0): 20, ('hack', 0.0): 8, ('contact', 0.0): 7, ('creat', 0.0): 2, ('tweet', 0.0): 35, ('spree', 0.0): 4, ('na', 0.0): 29, ('sholong', 0.0): 1, ('reject', 0.0): 7, ('propos', 0.0): 2, ('gee', 0.0): 1, ('fli', 0.0): 10, ('gidi', 0.0): 1, ('pamper', 0.0): 1, ('lago', 0.0): 1, ('ehn', 0.0): 1, ('arrest', 0.0): 1, ('girlfriend', 0.0): 2, ('he', 0.0): 3, ('nice', 0.0): 19, ('person', 0.0): 15, ('idk', 0.0): 26, ('anybodi', 0.0): 7, ('song', 0.0): 27, ('disappear', 0.0): 1, ('itun', 0.0): 3, ('daze', 0.0): 1, ('confus', 0.0): 8, ('surviv', 0.0): 5, ('fragment', 0.0): 1, (\"would'v\", 0.0): 2, ('forc', 0.0): 2, ('horribl', 0.0): 9, ('weather', 0.0): 29, ('us', 0.0): 43, ('could', 0.0): 69, ('walao', 0.0): 1, ('kb', 0.0): 1, ('send', 0.0): 12, ('ill', 0.0): 16, ('djderek', 0.0): 1, ('mani', 0.0): 29, ('fun', 0.0): 32, ('gig', 0.0): 3, ('absolut', 0.0): 6, ('legend', 0.0): 3, ('wait', 0.0): 43, ('till', 0.0): 8, ('saturday', 0.0): 10, ('homework', 0.0): 2, ('pa', 0.0): 8, ('made', 0.0): 23, ('da', 0.0): 5, ('greek', 0.0): 2, ('tragedi', 0.0): 1, ('rain', 0.0): 43, ('gym', 0.0): 6, ('üí™üèª', 0.0): 1, ('üêí', 0.0): 1, ('what', 0.0): 8, ('wrong', 0.0): 33, ('struck', 0.0): 1, ('anymor', 0.0): 20, ('belgium', 0.0): 4, ('fabian', 0.0): 2, ('delph', 0.0): 6, ('fallen', 0.0): 3, ('hide', 0.0): 4, ('drake', 0.0): 1, ('silent', 0.0): 1, ('hear', 0.0): 33, ('rest', 0.0): 21, ('peac', 0.0): 5, ('mo', 0.0): 4, ('tonight', 0.0): 24, ('t20blast', 0.0): 1, ('ahhh', 0.0): 5, ('wake', 0.0): 21, ('mumma', 0.0): 2, ('7', 0.0): 16, ('dead', 0.0): 10, ('tomorrow', 0.0): 34, (\"i'll\", 0.0): 41, ('high', 0.0): 8, ('low', 0.0): 8, ('pray', 0.0): 13, ('appropri', 0.0): 1, ('. . .', 0.0): 2, ('awak', 0.0): 10, ('woke', 0.0): 14, ('upp', 0.0): 1, ('dm', 0.0): 23, ('luke', 0.0): 6, ('hey', 0.0): 26, ('babe', 0.0): 19, ('across', 0.0): 4, ('hindi', 0.0): 1, ('reaction', 0.0): 1, ('5s', 0.0): 1, ('run', 0.0): 15, ('space', 0.0): 5, ('tbh', 0.0): 14, ('disabl', 0.0): 2, ('pension', 0.0): 1, ('ptsd', 0.0): 1, ('imposs', 0.0): 4, ('physic', 0.0): 7, ('financi', 0.0): 2, ('nooo', 0.0): 16, ('broke', 0.0): 9, ('soo', 0.0): 3, ('amaz', 0.0): 16, ('toghet', 0.0): 1, ('around', 0.0): 20, ('p', 0.0): 5, ('hold', 0.0): 9, ('anoth', 0.0): 27, ('septemb', 0.0): 2, ('21st', 0.0): 2, ('snsd', 0.0): 2, ('interact', 0.0): 2, ('anna', 0.0): 5, ('akana', 0.0): 1, ('askip', 0.0): 1, (\"t'exist\", 0.0): 1, ('channel', 0.0): 6, ('owner', 0.0): 1, ('decid', 0.0): 10, ('broadcast', 0.0): 6, ('kei', 0.0): 2, ('rate', 0.0): 4, ('se', 0.0): 2, ('notic', 0.0): 26, ('exist', 0.0): 2, ('traffic', 0.0): 5, ('terribl', 0.0): 12, ('eye', 0.0): 12, ('small', 0.0): 9, ('kate', 0.0): 2, ('spade', 0.0): 1, ('pero', 0.0): 3, ('walang', 0.0): 1, ('maganda', 0.0): 1, ('aw', 0.0): 42, ('seen', 0.0): 23, ('agesss', 0.0): 1, ('add', 0.0): 26, ('corinehurleigh', 0.0): 1, ('snapchatm', 0.0): 6, ('instagram', 0.0): 4, ('addmeonsnapchat', 0.0): 2, ('sf', 0.0): 3, ('quot', 0.0): 6, ('kiksext', 0.0): 6, ('bum', 0.0): 2, ('zara', 0.0): 1, ('trouser', 0.0): 1, ('effect', 0.0): 4, ('spanish', 0.0): 1, (\"it'okay\", 0.0): 1, ('health', 0.0): 2, ('luck', 0.0): 6, ('freed', 0.0): 1, ('rock', 0.0): 3, ('orcalov', 0.0): 1, ('tri', 0.0): 65, ('big', 0.0): 21, ('cuddl', 0.0): 8, ('lew', 0.0): 1, ('kiss', 0.0): 4, ('em', 0.0): 1, ('crave', 0.0): 8, ('banana', 0.0): 4, ('crumbl', 0.0): 1, ('mcflurri', 0.0): 1, ('cabl', 0.0): 1, ('car', 0.0): 17, ('brother', 0.0): 10, (\"venus'\", 0.0): 1, ('concept', 0.0): 4, ('rli', 0.0): 5, ('tea', 0.0): 7, ('tagal', 0.0): 2, (\"we'v\", 0.0): 3, ('appoint', 0.0): 1, (\"i'd\", 0.0): 11, ('sinc', 0.0): 35, (\"there'\", 0.0): 18, ('milk', 0.0): 3, ('left', 0.0): 26, ('cereal', 0.0): 2, ('film', 0.0): 6, ('date', 0.0): 7, ('previou', 0.0): 2, ('73', 0.0): 2, ('user', 0.0): 1, ('everywher', 0.0): 6, ('fansign', 0.0): 1, ('photo', 0.0): 15, ('expens', 0.0): 7, ('zzzz', 0.0): 1, ('let', 0.0): 37, ('sun', 0.0): 10, ('yet', 0.0): 33, (\"bff'\", 0.0): 1, ('extrem', 0.0): 3, ('stress', 0.0): 10, ('anyth', 0.0): 19, ('win', 0.0): 27, (\"deosn't\", 0.0): 1, ('liverpool', 0.0): 2, ('pool', 0.0): 3, ('though', 0.0): 57, ('bro', 0.0): 3, ('great', 0.0): 22, ('news', 0.0): 21, ('self', 0.0): 1, ('esteem', 0.0): 1, ('lowest', 0.0): 1, ('better', 0.0): 36, ('tacki', 0.0): 1, ('taken', 0.0): 9, ('man', 0.0): 32, ('lucki', 0.0): 16, ('charm', 0.0): 1, ('haaretz', 0.0): 1, ('israel', 0.0): 1, ('syria', 0.0): 2, ('continu', 0.0): 1, ('develop', 0.0): 5, ('chemic', 0.0): 1, ('weapon', 0.0): 2, ('offici', 0.0): 3, ('wsj', 0.0): 2, ('rep', 0.0): 1, ('bt', 0.0): 4, ('mr', 0.0): 9, ('wong', 0.0): 1, ('confisc', 0.0): 1, ('art', 0.0): 4, ('thought', 0.0): 31, ('icepack', 0.0): 1, ('dose', 0.0): 2, ('killer', 0.0): 2, ('board', 0.0): 1, ('whimper', 0.0): 1, ('fan', 0.0): 17, ('senpai', 0.0): 1, ('buttsex', 0.0): 1, ('joke', 0.0): 8, ('headlin', 0.0): 1, (\"dn't\", 0.0): 1, ('brk', 0.0): 1, (\":'(\", 0.0): 13, ('hit', 0.0): 7, ('voic', 0.0): 9, ('falsetto', 0.0): 1, ('zone', 0.0): 2, ('leannerin', 0.0): 1, ('hornykik', 0.0): 17, ('loveofmylif', 0.0): 2, ('dmme', 0.0): 2, ('pussi', 0.0): 2, ('newmus', 0.0): 3, ('sexo', 0.0): 2, ('s2', 0.0): 1, ('spain', 0.0): 4, ('delay', 0.0): 5, ('kill', 0.0): 22, ('singl', 0.0): 10, ('untruth', 0.0): 1, ('cross', 0.0): 4, ('countri', 0.0): 6, ('ij', 0.0): 1, ('üí•', 0.0): 1, ('‚ú®', 0.0): 1, ('üí´', 0.0): 1, ('bear', 0.0): 2, ('littl', 0.0): 21, ('apart', 0.0): 7, ('live', 0.0): 37, ('soshi', 0.0): 1, ('didnt', 0.0): 24, ('buttt', 0.0): 2, ('congrat', 0.0): 2, ('sunday', 0.0): 8, ('friday', 0.0): 12, ('shoulda', 0.0): 1, ('move', 0.0): 12, ('w', 0.0): 22, ('caus', 0.0): 16, (\"they'r\", 0.0): 14, ('heyyy', 0.0): 1, ('yeol', 0.0): 2, ('solo', 0.0): 6, ('dancee', 0.0): 1, ('inter', 0.0): 1, ('nemanja', 0.0): 1, ('vidic', 0.0): 1, ('roma', 0.0): 1, (\"mom'\", 0.0): 2, ('linguist', 0.0): 1, (\"dad'\", 0.0): 1, ('comput', 0.0): 6, ('scientist', 0.0): 1, ('dumbest', 0.0): 1, ('famili', 0.0): 9, ('broken', 0.0): 11, ('ice', 0.0): 35, ('cream', 0.0): 32, ('pour', 0.0): 1, ('crash', 0.0): 6, ('scienc', 0.0): 1, ('resourc', 0.0): 1, ('vehicl', 0.0): 5, ('ate', 0.0): 10, ('ayex', 0.0): 1, ('eat', 0.0): 27, ('swear', 0.0): 6, ('lamon', 0.0): 1, ('scroll', 0.0): 1, ('curv', 0.0): 2, ('üòâ', 0.0): 1, ('cement', 0.0): 1, ('cast', 0.0): 5, ('10.3', 0.0): 1, ('k', 0.0): 9, ('sign', 0.0): 9, ('zayn', 0.0): 8, ('bot', 0.0): 1, ('plz', 0.0): 3, ('mention', 0.0): 9, ('jmu', 0.0): 1, ('camp', 0.0): 7, ('teas', 0.0): 3, ('sweetest', 0.0): 1, ('awuna', 0.0): 1, ('mbulelo', 0.0): 1, ('match', 0.0): 7, ('pig', 0.0): 2, ('although', 0.0): 5, ('crackl', 0.0): 1, ('nois', 0.0): 3, ('plug', 0.0): 2, ('fuse', 0.0): 1, ('dammit', 0.0): 3, ('tip', 0.0): 2, ('carlton', 0.0): 2, ('aflblueshawk', 0.0): 2, (\"alex'\", 0.0): 1, ('hous', 0.0): 16, ('motorsport', 0.0): 1, ('seri', 0.0): 3, ('disc', 0.0): 1, ('right', 0.0): 51, ('cheeki', 0.0): 1, ('j', 0.0): 1, ('instead', 0.0): 4, ('seo', 0.0): 1, ('nl', 0.0): 1, ('bud', 0.0): 1, ('christi', 0.0): 1, ('xo', 0.0): 1, ('niec', 0.0): 1, ('summer', 0.0): 19, ('bloodi', 0.0): 2, ('sandwhich', 0.0): 1, ('buset', 0.0): 1, ('discrimin', 0.0): 4, ('five', 0.0): 5, ('learn', 0.0): 5, ('pregnanc', 0.0): 2, ('foot', 0.0): 5, ('f', 0.0): 4, ('matern', 0.0): 1, ('kick', 0.0): 6, ('domesticviol', 0.0): 1, ('law', 0.0): 4, ('domest', 0.0): 1, ('violenc', 0.0): 2, ('victim', 0.0): 4, ('98fm', 0.0): 1, ('exactli', 0.0): 5, ('unfortun', 0.0): 21, ('yesterday', 0.0): 13, ('uk', 0.0): 9, ('govern', 0.0): 1, ('sapiosexu', 0.0): 1, ('damn', 0.0): 29, ('beta', 0.0): 4, ('12', 0.0): 8, ('hour', 0.0): 35, ('world', 0.0): 17, ('hulk', 0.0): 3, ('hogan', 0.0): 3, ('scrub', 0.0): 1, ('wwe', 0.0): 2, ('histori', 0.0): 2, ('iren', 0.0): 4, ('mistak', 0.0): 6, ('naa', 0.0): 1, ('sold', 0.0): 6, ('h_my_k', 0.0): 1, ('lose', 0.0): 7, ('valentin', 0.0): 2, ('et', 0.0): 3, (\"r'ship\", 0.0): 1, ('btwn', 0.0): 1, ('homo', 0.0): 2, ('biphob', 0.0): 2, ('comment', 0.0): 4, ('certain', 0.0): 6, ('disciplin', 0.0): 2, ('incl', 0.0): 2, ('european', 0.0): 3, ('lang', 0.0): 6, ('lit', 0.0): 2, ('educ', 0.0): 2, ('fresherstofin', 0.0): 1, ('üíî', 0.0): 3, ('dream', 0.0): 24, ('gettin', 0.0): 2, ('realist', 0.0): 4, ('thx', 0.0): 1, ('real', 0.0): 21, ('isnt', 0.0): 7, ('prefer', 0.0): 4, ('benzema', 0.0): 2, ('hahahahahaah', 0.0): 1, ('donno', 0.0): 1, ('korean', 0.0): 2, ('languag', 0.0): 5, ('russian', 0.0): 2, ('waaa', 0.0): 1, ('eidwithgrof', 0.0): 1, ('boreddd', 0.0): 1, ('mug', 0.0): 3, ('piss', 0.0): 3, ('tiddler', 0.0): 1, ('silli', 0.0): 2, ('least', 0.0): 15, ('card', 0.0): 7, ('chorong', 0.0): 1, ('leader', 0.0): 1, ('ÏóêÏù¥ÌïëÌÅ¨', 0.0): 3, ('ÎçîÏáº', 0.0): 4, ('clan', 0.0): 1, ('slot', 0.0): 2, ('open', 0.0): 16, ('pfff', 0.0): 1, ('privat', 0.0): 2, ('bugbounti', 0.0): 1, ('self-xss', 0.0): 1, ('host', 0.0): 2, ('header', 0.0): 3, ('poison', 0.0): 3, ('code', 0.0): 8, ('execut', 0.0): 1, ('ktksbye', 0.0): 1, ('connect', 0.0): 3, ('compani', 0.0): 3, ('alert', 0.0): 2, ('cancel', 0.0): 10, ('uber', 0.0): 3, ('everyon', 0.0): 26, ('els', 0.0): 4, ('offic', 0.0): 7, ('ahahah', 0.0): 1, ('petit', 0.0): 1, ('relationship', 0.0): 4, ('height', 0.0): 2, ('cost', 0.0): 1, ('600', 0.0): 2, ('¬£', 0.0): 6, ('secur', 0.0): 4, ('odoo', 0.0): 2, ('8', 0.0): 11, ('partner', 0.0): 2, ('commun', 0.0): 2, ('spirit', 0.0): 3, ('jgh', 0.0): 2, ('effin', 0.0): 1, ('facebook', 0.0): 4, ('anyon', 0.0): 17, (\"else'\", 0.0): 1, ('box', 0.0): 8, ('ap', 0.0): 3, ('stori', 0.0): 13, ('london', 0.0): 12, ('imagin', 0.0): 2, ('elsewher', 0.0): 1, ('someday', 0.0): 1, ('ben', 0.0): 3, ('provid', 0.0): 3, ('name', 0.0): 15, ('branch', 0.0): 1, ('visit', 0.0): 12, ('address', 0.0): 3, ('concern', 0.0): 3, ('welsh', 0.0): 1, ('pod', 0.0): 1, ('juli', 0.0): 12, ('laura', 0.0): 4, ('insid', 0.0): 10, ('train', 0.0): 12, ('d;', 0.0): 1, ('talk-kama', 0.0): 1, ('hawako', 0.0): 1, ('waa', 0.0): 1, ('kimaaani', 0.0): 1, ('prisss', 0.0): 1, ('baggag', 0.0): 2, ('claim', 0.0): 3, ('plane', 0.0): 2, ('niamh', 0.0): 1, ('forev', 0.0): 10, ('hmmm', 0.0): 2, ('sugar', 0.0): 3, ('rare', 0.0): 1, ('paper', 0.0): 16, ('town', 0.0): 14, ('score', 0.0): 3, ('stuck', 0.0): 8, ('agh', 0.0): 2, ('middl', 0.0): 7, ('undercoverboss', 0.0): 1, ('ÿ™ŸÉŸÅŸâ', 0.0): 1, ('10', 0.0): 8, ('job', 0.0): 13, ('cat', 0.0): 17, ('forgotten', 0.0): 3, ('yep', 0.0): 5, ('stop', 0.0): 43, ('ach', 0.0): 2, ('wrist', 0.0): 1, ('nake', 0.0): 3, ('forgot', 0.0): 14, ('bracelet', 0.0): 3, ('ligo', 0.0): 1, ('dozen', 0.0): 1, ('parent', 0.0): 8, ('children', 0.0): 2, ('shark', 0.0): 2, ('selfi', 0.0): 6, ('heartach', 0.0): 1, ('zayniscomingback', 0.0): 3, ('mix', 0.0): 2, ('sweden', 0.0): 1, ('breath', 0.0): 4, ('moment', 0.0): 14, ('word', 0.0): 16, ('elmhurst', 0.0): 1, ('fc', 0.0): 1, ('etid', 0.0): 1, (\"chillin'with\", 0.0): 1, ('father', 0.0): 2, ('istanya', 0.0): 1, ('2suppli', 0.0): 1, ('extra', 0.0): 3, ('infrastructur', 0.0): 2, ('teacher', 0.0): 2, ('doctor', 0.0): 4, ('nurs', 0.0): 2, ('paramed', 0.0): 1, ('countless', 0.0): 1, ('2cope', 0.0): 1, ('bore', 0.0): 23, ('plea', 0.0): 2, ('arian', 0.0): 1, ('hahahaha', 0.0): 6, ('slr', 0.0): 1, ('kendal', 0.0): 1, ('kyli', 0.0): 3, (\"kylie'\", 0.0): 1, ('manila', 0.0): 3, ('jeebu', 0.0): 1, ('reabsorbt', 0.0): 1, ('tooth', 0.0): 2, ('abscess', 0.0): 1, ('threaten', 0.0): 2, ('affect', 0.0): 1, ('front', 0.0): 6, ('crown', 0.0): 1, ('ooouch', 0.0): 1, ('barney', 0.0): 1, (\"be'\", 0.0): 1, ('yo', 0.0): 4, ('later', 0.0): 14, ('realis', 0.0): 6, ('problemat', 0.0): 1, ('expect', 0.0): 5, ('proud', 0.0): 8, ('mess', 0.0): 7, ('maa', 0.0): 2, ('without', 0.0): 25, ('bangalor', 0.0): 1, ('awww', 0.0): 23, ('lui', 0.0): 1, ('manzano', 0.0): 1, ('shaaa', 0.0): 1, ('super', 0.0): 11, ('7th', 0.0): 1, ('conven', 0.0): 1, ('2:30', 0.0): 2, ('pm', 0.0): 8, ('forward', 0.0): 6, ('delet', 0.0): 5, ('turkey', 0.0): 1, ('bomb', 0.0): 3, ('isi', 0.0): 1, ('allow', 0.0): 9, ('usa', 0.0): 2, ('use', 0.0): 43, ('airfield', 0.0): 1, ('jet', 0.0): 1, (\"jack'\", 0.0): 1, ('spam', 0.0): 6, ('sooo', 0.0): 16, ('‚ò∫', 0.0): 3, (\"mommy'\", 0.0): 1, ('reason', 0.0): 8, ('overweight', 0.0): 1, ('sigeg', 0.0): 1, ('habhab', 0.0): 1, ('masud', 0.0): 1, ('kaha', 0.0): 1, ('ko', 0.0): 10, ('akong', 0.0): 1, ('un', 0.0): 1, ('hella', 0.0): 4, ('matter', 0.0): 4, ('pala', 0.0): 1, ('hahaha', 0.0): 11, ('lesson', 0.0): 1, ('dolphin', 0.0): 1, ('xxx', 0.0): 12, ('holi', 0.0): 2, ('anythin', 0.0): 1, ('trend', 0.0): 6, ('radio', 0.0): 4, ('sing', 0.0): 5, ('bewar', 0.0): 1, ('agonis', 0.0): 1, ('experi', 0.0): 2, ('ahead', 0.0): 3, ('modimo', 0.0): 1, ('ho', 0.0): 3, ('tseba', 0.0): 1, ('wena', 0.0): 1, ('fela', 0.0): 1, ('emot', 0.0): 8, ('hubbi', 0.0): 1, ('delight', 0.0): 1, ('return', 0.0): 6, ('bill', 0.0): 6, ('nowt', 0.0): 1, ('wors', 0.0): 8, ('willi', 0.0): 1, ('gon', 0.0): 1, ('vomit', 0.0): 1, ('famou', 0.0): 5, ('bowl', 0.0): 1, ('devast', 0.0): 1, ('titan', 0.0): 1, ('ae', 0.0): 1, ('mark', 0.0): 2, ('hair', 0.0): 21, ('shini', 0.0): 1, ('wavi', 0.0): 1, ('emo', 0.0): 2, ('germani', 0.0): 4, ('load', 0.0): 9, ('shed', 0.0): 2, ('ha', 0.0): 7, ('bheyp', 0.0): 1, ('ayemso', 0.0): 1, ('ear', 0.0): 5, ('swell', 0.0): 2, ('sm', 0.0): 7, ('fb', 0.0): 7, ('remind', 0.0): 3, ('abt', 0.0): 3, ('womad', 0.0): 1, ('wut', 0.0): 1, ('hell', 0.0): 11, ('viciou', 0.0): 1, ('circl', 0.0): 1, ('surpris', 0.0): 5, ('ticket', 0.0): 12, ('codi', 0.0): 1, ('simpson', 0.0): 1, ('concert', 0.0): 11, ('singapor', 0.0): 4, ('august', 0.0): 5, ('pooo', 0.0): 2, ('bh3', 0.0): 1, ('enter', 0.0): 1, ('pitchwar', 0.0): 1, ('chap', 0.0): 1, (\"mine'\", 0.0): 1, ('transcript', 0.0): 1, (\"apma'\", 0.0): 1, ('shoulder', 0.0): 2, ('bitch', 0.0): 11, ('competit', 0.0): 1, (\"it'll\", 0.0): 3, ('fine', 0.0): 6, ('timw', 0.0): 1, ('acc', 0.0): 8, ('rude', 0.0): 11, ('vitamin', 0.0): 1, ('e', 0.0): 9, ('oil', 0.0): 1, ('massag', 0.0): 5, ('everyday', 0.0): 7, ('healthier', 0.0): 1, ('easier', 0.0): 3, ('stretch', 0.0): 1, ('choos', 0.0): 7, ('blockjam', 0.0): 1, (\"schedule'\", 0.0): 1, ('whack', 0.0): 1, ('kik', 0.0): 69, ('thelock', 0.0): 1, ('76', 0.0): 1, ('sex', 0.0): 6, ('omegl', 0.0): 4, ('coupl', 0.0): 2, ('travel', 0.0): 11, ('hotgirl', 0.0): 2, ('2009', 0.0): 1, ('3', 0.0): 32, ('ghantay', 0.0): 1, ('light', 0.0): 8, ('nai', 0.0): 1, ('hay', 0.0): 8, ('deni', 0.0): 1, ('ruin', 0.0): 11, ('laguna', 0.0): 1, ('exit', 0.0): 2, ('gomen', 0.0): 1, ('heck', 0.0): 5, ('fair', 0.0): 12, ('grew', 0.0): 2, ('half', 0.0): 10, ('inch', 0.0): 2, ('two', 0.0): 19, ('problem', 0.0): 7, ('suuuper', 0.0): 1, ('65', 0.0): 1, ('sale', 0.0): 8, ('inact', 0.0): 8, ('orphan', 0.0): 1, ('black', 0.0): 12, ('earlier', 0.0): 9, ('whaaat', 0.0): 5, ('kaya', 0.0): 2, ('naaan', 0.0): 1, ('paus', 0.0): 1, ('randomli', 0.0): 1, ('app', 0.0): 13, ('3:30', 0.0): 1, ('walk', 0.0): 7, ('inglewood', 0.0): 1, ('ummm', 0.0): 4, ('anxieti', 0.0): 3, ('readi', 0.0): 12, ('also', 0.0): 19, ('charcoal', 0.0): 1, ('til', 0.0): 5, ('mid-end', 0.0): 1, ('aug', 0.0): 1, ('noooo', 0.0): 1, ('heard', 0.0): 6, ('rip', 0.0): 12, ('rodfanta', 0.0): 1, ('wasp', 0.0): 2, ('sting', 0.0): 1, ('avert', 0.0): 1, ('bug', 0.0): 3, ('(:', 0.0): 7, ('exo', 0.0): 2, ('seekli', 0.0): 1, ('riptito', 0.0): 1, ('manbearpig', 0.0): 1, ('cannot', 0.0): 7, ('grow', 0.0): 3, ('shorter', 0.0): 1, ('academ', 0.0): 1, ('free', 0.0): 19, ('exclus', 0.0): 2, ('unfair', 0.0): 7, ('esp', 0.0): 4, ('regard', 0.0): 1, ('current', 0.0): 7, ('bleak', 0.0): 1, ('german', 0.0): 1, ('chart', 0.0): 2, ('situat', 0.0): 2, ('entri', 0.0): 4, ('even', 0.0): 70, ('top', 0.0): 6, ('100', 0.0): 8, ('pfft', 0.0): 1, ('place', 0.0): 18, ('white', 0.0): 7, ('wash', 0.0): 1, ('polaroid', 0.0): 1, ('newbethvideo', 0.0): 1, ('greec', 0.0): 2, ('xur', 0.0): 2, ('imi', 0.0): 3, ('fill', 0.0): 1, ('‚ô°', 0.0): 11, ('‚ô•', 0.0): 22, ('xoxoxo', 0.0): 1, ('pictur', 0.0): 17, ('stud', 0.0): 1, ('hund', 0.0): 1, ('6', 0.0): 14, ('kikchat', 0.0): 9, ('amazon', 0.0): 5, ('3.4', 0.0): 1, ('yach', 0.0): 1, ('telat', 0.0): 1, ('huvvft', 0.0): 1, ('zoo', 0.0): 2, ('fieldtrip', 0.0): 1, ('touch', 0.0): 5, ('yan', 0.0): 1, ('posit', 0.0): 2, ('king', 0.0): 1, ('futur', 0.0): 4, ('sizw', 0.0): 1, ('write', 0.0): 13, ('20', 0.0): 9, ('result', 0.0): 3, ('km', 0.0): 2, ('four', 0.0): 4, ('shift', 0.0): 5, ('aaahhh', 0.0): 2, ('boredom', 0.0): 1, ('en', 0.0): 1, ('aint', 0.0): 7, ('who', 0.0): 1, ('sins', 0.0): 1, ('that', 0.0): 13, ('somehow', 0.0): 2, ('tini', 0.0): 4, ('ball', 0.0): 2, ('barbel', 0.0): 1, ('owww', 0.0): 2, ('amsterdam', 0.0): 1, ('luv', 0.0): 2, ('üíñ', 0.0): 4, ('ps', 0.0): 3, ('looong', 0.0): 1, ('especi', 0.0): 4, (':/', 0.0): 11, ('lap', 0.0): 1, ('litro', 0.0): 1, ('shepherd', 0.0): 2, ('lami', 0.0): 1, ('mayb', 0.0): 27, ('relax', 0.0): 3, ('lungomar', 0.0): 1, ('pesaro', 0.0): 1, ('giachietittiwed', 0.0): 1, ('igersoftheday', 0.0): 1, ('summertim', 0.0): 1, ('nose', 0.0): 7, ('bruis', 0.0): 1, ('lil', 0.0): 8, ('snake', 0.0): 3, ('journey', 0.0): 2, ('scarf', 0.0): 1, ('au', 0.0): 3, ('afford', 0.0): 7, ('fridayfeel', 0.0): 1, ('earli', 0.0): 12, ('money', 0.0): 24, ('chicken', 0.0): 5, ('woe', 0.0): 4, ('nigga', 0.0): 3, ('motn', 0.0): 1, ('make-up', 0.0): 1, ('justic', 0.0): 1, ('import', 0.0): 4, ('sit', 0.0): 5, ('mind', 0.0): 7, ('buy', 0.0): 17, ('limit', 0.0): 4, ('ver', 0.0): 1, ('normal', 0.0): 5, ('edit', 0.0): 7, ('huhuhu', 0.0): 3, ('stack', 0.0): 1, (\"m'ladi\", 0.0): 1, ('j8', 0.0): 1, ('j11', 0.0): 1, ('m20', 0.0): 1, ('jk', 0.0): 5, ('acad', 0.0): 1, ('schedul', 0.0): 9, ('nowww', 0.0): 1, ('cop', 0.0): 1, ('jame', 0.0): 4, ('window', 0.0): 6, ('hugh', 0.0): 2, ('paw', 0.0): 1, ('muddi', 0.0): 1, ('distract', 0.0): 1, ('heyi', 0.0): 1, ('otherwis', 0.0): 3, ('picnic', 0.0): 1, ('24', 0.0): 11, ('cupcak', 0.0): 2, ('talaga', 0.0): 1, ('best', 0.0): 22, ('femal', 0.0): 3, ('poppin', 0.0): 1, ('joc', 0.0): 1, ('playin', 0.0): 1, ('saw', 0.0): 19, ('fix', 0.0): 10, ('coldplay', 0.0): 1, ('media', 0.0): 1, ('player', 0.0): 3, ('fail', 0.0): 10, ('subj', 0.0): 1, ('sobrang', 0.0): 1, ('bv', 0.0): 1, ('zamn', 0.0): 1, ('line', 0.0): 8, ('afropunk', 0.0): 1, ('fest', 0.0): 1, ('brooklyn', 0.0): 2, ('id', 0.0): 5, ('put', 0.0): 14, ('50', 0.0): 5, ('madrid', 0.0): 7, ('shithous', 0.0): 1, ('cutest', 0.0): 2, ('danc', 0.0): 6, ('ur', 0.0): 26, ('arm', 0.0): 3, ('rais', 0.0): 1, ('hand', 0.0): 12, ('ladder', 0.0): 2, ('told', 0.0): 11, ('climb', 0.0): 3, ('success', 0.0): 4, ('nerv', 0.0): 1, ('wrack', 0.0): 1, ('test', 0.0): 8, ('booset', 0.0): 1, ('restart', 0.0): 1, ('assassin', 0.0): 1, ('creed', 0.0): 1, ('ii', 0.0): 1, ('heap', 0.0): 1, ('fell', 0.0): 10, ('daughter', 0.0): 1, ('begin', 0.0): 4, ('ps3', 0.0): 1, ('ankl', 0.0): 4, ('step', 0.0): 5, ('puddl', 0.0): 2, ('wear', 0.0): 5, ('slipper', 0.0): 1, ('eve', 0.0): 1, ('bbi', 0.0): 6, ('sararoc', 0.0): 1, ('angri', 0.0): 5, ('pretti', 0.0): 15, ('fnaf', 0.0): 1, ('holiday', 0.0): 20, ('cheer', 0.0): 6, ('üòò', 0.0): 11, ('anywayhedidanicejob', 0.0): 1, ('üòû', 0.0): 3, ('3am', 0.0): 2, ('other', 0.0): 7, ('local', 0.0): 3, ('cruis', 0.0): 1, ('done', 0.0): 24, ('doubl', 0.0): 4, ('wail', 0.0): 1, ('manual', 0.0): 2, ('wheelchair', 0.0): 1, ('check', 0.0): 19, ('fit', 0.0): 3, ('nh', 0.0): 3, ('26week', 0.0): 1, ('sbenu', 0.0): 1, ('sasin', 0.0): 1, ('team', 0.0): 14, ('anarchi', 0.0): 1, ('af', 0.0): 14, ('candl', 0.0): 1, ('forehead', 0.0): 4, ('medicin', 0.0): 3, ('welcom', 0.0): 5, ('oop', 0.0): 4, ('hoya', 0.0): 3, ('mah', 0.0): 2, ('a', 0.0): 1, ('nobodi', 0.0): 10, ('awhil', 0.0): 2, ('ago', 0.0): 20, ('b', 0.0): 10, ('hush', 0.0): 2, ('gurli', 0.0): 1, ('bring', 0.0): 9, ('purti', 0.0): 1, ('mouth', 0.0): 5, ('closer', 0.0): 2, ('shiver', 0.0): 1, ('solut', 0.0): 1, ('paid', 0.0): 8, ('properli', 0.0): 2, ('gol', 0.0): 1, ('pea', 0.0): 1, ('english', 0.0): 9, ('mental', 0.0): 4, ('tierd', 0.0): 2, ('third', 0.0): 1, (\"eye'\", 0.0): 1, ('thnkyouuu', 0.0): 1, ('carolin', 0.0): 1, ('neither', 0.0): 6, ('figur', 0.0): 6, ('mirror', 0.0): 1, ('highlight', 0.0): 2, ('pure', 0.0): 3, ('courag', 0.0): 1, ('bit', 0.0): 15, ('fishi', 0.0): 1, ('idek', 0.0): 1, ('apink', 0.0): 5, ('perform', 0.0): 8, ('bulet', 0.0): 1, ('gendut', 0.0): 1, ('noo', 0.0): 5, ('race', 0.0): 3, ('hotwheel', 0.0): 1, ('ms', 0.0): 1, ('patch', 0.0): 1, ('typic', 0.0): 2, ('ahaha', 0.0): 1, ('lay', 0.0): 2, ('wine', 0.0): 1, ('glass', 0.0): 3, (\"where'\", 0.0): 4, ('akon', 0.0): 1, ('somewher', 0.0): 5, ('nightmar', 0.0): 7, ('ya', 0.0): 15, ('mino', 0.0): 2, ('crazyyi', 0.0): 1, ('thooo', 0.0): 1, ('zz', 0.0): 1, ('airport', 0.0): 7, ('straight', 0.0): 4, ('soundcheck', 0.0): 1, ('hmm', 0.0): 4, ('antagonist', 0.0): 1, ('ob', 0.0): 1, ('phantasi', 0.0): 1, ('star', 0.0): 4, ('ip', 0.0): 1, ('issu', 0.0): 11, ('bruce', 0.0): 1, ('sleepdepriv', 0.0): 1, ('tiredashel', 0.0): 1, ('4aspot', 0.0): 1, (\"kinara'\", 0.0): 1, ('awami', 0.0): 1, ('question', 0.0): 9, ('niqqa', 0.0): 1, ('answer', 0.0): 14, ('mockingjay', 0.0): 1, ('slow', 0.0): 9, ('pb.contest', 0.0): 1, ('cycl', 0.0): 2, ('aarww', 0.0): 1, ('lmbo', 0.0): 1, ('dangit', 0.0): 1, ('ohmygod', 0.0): 1, ('scenario', 0.0): 1, ('tooo', 0.0): 2, ('duck', 0.0): 1, ('baechyyi', 0.0): 1, ('okayyy', 0.0): 1, ('noon', 0.0): 3, ('drag', 0.0): 5, ('serious', 0.0): 11, ('misundersrand', 0.0): 1, ('chal', 0.0): 1, ('raha', 0.0): 1, ('hai', 0.0): 11, ('yhm', 0.0): 1, ('edsa', 0.0): 2, ('jasmingarrick', 0.0): 2, ('kikmeguy', 0.0): 5, ('webcam', 0.0): 2, ('milf', 0.0): 1, ('nakamaforev', 0.0): 3, ('kiksex', 0.0): 7, (\"unicef'\", 0.0): 1, ('fu', 0.0): 1, ('alon', 0.0): 16, ('manag', 0.0): 13, ('stephen', 0.0): 1, ('street', 0.0): 2, ('35', 0.0): 1, ('min', 0.0): 7, ('appear', 0.0): 2, ('record', 0.0): 6, ('coz', 0.0): 4, ('frustrat', 0.0): 6, ('sent', 0.0): 9, ('interest', 0.0): 9, ('woza', 0.0): 1, ('promis', 0.0): 4, ('senight', 0.0): 1, ('468', 0.0): 1, ('kikmeboy', 0.0): 9, ('gay', 0.0): 6, ('teen', 0.0): 7, ('amateur', 0.0): 5, ('hotscratch', 0.0): 1, ('sell', 0.0): 8, ('sock', 0.0): 6, ('150-160', 0.0): 1, ('peso', 0.0): 1, ('gotta', 0.0): 8, ('pay', 0.0): 8, ('degrassi', 0.0): 1, ('4-6', 0.0): 1, ('bcz', 0.0): 1, ('kat', 0.0): 3, ('chem', 0.0): 2, ('onscreen', 0.0): 1, ('ofscreen', 0.0): 1, ('kinda', 0.0): 10, ('pak', 0.0): 4, ('class', 0.0): 10, ('monthli', 0.0): 1, ('roll', 0.0): 4, ('band', 0.0): 2, ('throw', 0.0): 2, ('ironi', 0.0): 2, ('rhisfor', 0.0): 1, ('500', 0.0): 2, ('bestoftheday', 0.0): 3, ('chat', 0.0): 9, ('camsex', 0.0): 5, ('unfollow', 0.0): 11, ('particular', 0.0): 1, ('support', 0.0): 26, ('bae', 0.0): 11, ('poopi', 0.0): 1, ('pip', 0.0): 1, ('post', 0.0): 12, ('felt', 0.0): 6, ('uff', 0.0): 1, ('1.300', 0.0): 1, ('credit', 0.0): 3, ('glue', 0.0): 1, ('factori', 0.0): 1, ('kuchar', 0.0): 1, ('fast', 0.0): 7, ('graduat', 0.0): 3, ('up', 0.0): 2, ('definit', 0.0): 3, ('uni', 0.0): 2, ('ee', 0.0): 1, ('tommi', 0.0): 1, ('georgia', 0.0): 2, ('bout', 0.0): 2, ('instant', 0.0): 1, ('transmiss', 0.0): 1, ('malik', 0.0): 1, ('orang', 0.0): 2, ('suma', 0.0): 1, ('shouldeeerr', 0.0): 1, ('outfit', 0.0): 5, ('age', 0.0): 8, ('repack', 0.0): 3, ('group', 0.0): 4, ('charl', 0.0): 1, ('grown', 0.0): 2, ('rememb', 0.0): 17, ('dy', 0.0): 1, ('rihanna', 0.0): 1, ('red', 0.0): 4, ('ging', 0.0): 2, ('boot', 0.0): 4, ('closest', 0.0): 3, ('nike', 0.0): 1, ('adida', 0.0): 1, ('inform', 0.0): 4, ('pro@illamasqua.com', 0.0): 1, ('set', 0.0): 13, ('ifeely', 0.0): 1, ('harder', 0.0): 2, ('usual', 0.0): 7, ('ratbaglat', 0.0): 1, ('second', 0.0): 5, ('semest', 0.0): 2, ('gin', 0.0): 1, ('gut', 0.0): 12, ('reynold', 0.0): 1, ('dessert', 0.0): 2, ('season', 0.0): 9, ('villag', 0.0): 1, ('differ', 0.0): 10, ('citi', 0.0): 11, ('unit', 0.0): 3, ('oppress', 0.0): 1, ('mass', 0.0): 2, ('wat', 0.0): 5, ('afghanistn', 0.0): 1, ('war', 0.0): 2, ('tore', 0.0): 1, ('sunggyu', 0.0): 5, ('injur', 0.0): 7, ('plaster', 0.0): 2, ('rtd', 0.0): 1, ('loui', 0.0): 4, ('harri', 0.0): 10, ('5so', 0.0): 7, ('crowd', 0.0): 1, ('stadium', 0.0): 4, ('welder', 0.0): 1, ('ghost', 0.0): 1, ('hogo', 0.0): 1, ('vishaya', 0.0): 1, ('adu', 0.0): 1, ('bjp', 0.0): 1, ('madatt', 0.0): 1, ('anta', 0.0): 1, ('vishwa', 0.0): 1, ('ne', 0.0): 3, ('illa', 0.0): 1, ('wua', 0.0): 1, ('picki', 0.0): 1, ('finger', 0.0): 8, ('favourit', 0.0): 9, ('mutual', 0.0): 2, ('gn', 0.0): 1, ('along', 0.0): 3, ('ass', 0.0): 9, ('thent', 0.0): 1, ('423', 0.0): 1, ('sabadodeganarseguidor', 0.0): 2, ('sexual', 0.0): 4, ('sync', 0.0): 2, ('plug.dj', 0.0): 1, ('peel', 0.0): 1, ('suspems', 0.0): 1, ('cope', 0.0): 3, ('offroad', 0.0): 1, ('adventur', 0.0): 1, ('there', 0.0): 5, ('harvest', 0.0): 1, ('machineri', 0.0): 1, ('inapropri', 0.0): 1, ('weav', 0.0): 2, ('nowher', 0.0): 3, ('decent', 0.0): 2, ('invest', 0.0): 2, ('scottish', 0.0): 1, ('footbal', 0.0): 3, ('dire', 0.0): 2, ('nomoney', 0.0): 1, ('nawf', 0.0): 1, ('sum', 0.0): 2, ('becho', 0.0): 1, ('danni', 0.0): 3, ('eng', 0.0): 2, (\"let'\", 0.0): 5, ('overli', 0.0): 2, ('lab', 0.0): 1, ('ty', 0.0): 3, ('zap', 0.0): 1, ('distress', 0.0): 1, ('shot', 0.0): 6, ('cinema', 0.0): 4, ('louisianashoot', 0.0): 1, ('laugh', 0.0): 7, ('har', 0.0): 3, (\"how'\", 0.0): 5, ('chum', 0.0): 1, ('ncc', 0.0): 1, ('ph', 0.0): 2, ('balik', 0.0): 1, ('naman', 0.0): 1, ('kayo', 0.0): 1, ('itong', 0.0): 1, ('shirt', 0.0): 3, ('thaaat', 0.0): 1, ('ctto', 0.0): 1, ('expir', 0.0): 3, ('bi', 0.0): 2, ('tough', 0.0): 2, ('11', 0.0): 4, ('3:33', 0.0): 2, ('jfc', 0.0): 1, ('bio', 0.0): 3, ('bodo', 0.0): 1, ('amat', 0.0): 1, ('quick', 0.0): 5, ('yelaaa', 0.0): 1, ('dublin', 0.0): 2, ('potter', 0.0): 1, ('marathon', 0.0): 3, ('balanc', 0.0): 2, ('warm', 0.0): 5, ('comic', 0.0): 5, ('pine', 0.0): 1, ('keybind', 0.0): 1, ('featur', 0.0): 4, ('wild', 0.0): 2, ('warfar', 0.0): 1, ('control', 0.0): 2, ('diagnos', 0.0): 1, ('wiv', 0.0): 1, (\"scheuermann'\", 0.0): 1, ('diseas', 0.0): 3, ('bone', 0.0): 1, ('rlyhurt', 0.0): 1, ('howdo', 0.0): 1, ('georgesampson', 0.0): 1, ('stand', 0.0): 6, ('signal', 0.0): 3, ('reckon', 0.0): 1, ('t20', 0.0): 1, ('action', 0.0): 2, ('taunton', 0.0): 1, ('vacat', 0.0): 3, ('excit', 0.0): 6, ('justiceforsandrabland', 0.0): 2, ('sandrabland', 0.0): 6, ('disturb', 0.0): 1, ('women', 0.0): 5, ('happpi', 0.0): 1, ('justinbieb', 0.0): 4, ('daianerufato', 0.0): 3, ('ilysm', 0.0): 3, ('2015', 0.0): 12, ('07:34', 0.0): 1, ('delphi', 0.0): 2, ('weak', 0.0): 2, ('dom', 0.0): 2, ('techniqu', 0.0): 1, ('minc', 0.0): 2, ('complet', 0.0): 9, ('symphoni', 0.0): 1, ('joe', 0.0): 3, ('co', 0.0): 6, ('wth', 0.0): 2, ('aisyhhh', 0.0): 1, ('bald', 0.0): 1, ('14', 0.0): 3, ('seungchan', 0.0): 1, ('aigooo', 0.0): 1, ('riri', 0.0): 1, ('origin', 0.0): 6, ('depend', 0.0): 2, ('vet', 0.0): 1, ('major', 0.0): 2, ('va', 0.0): 1, ('kept', 0.0): 2, ('lumin', 0.0): 1, ('follback', 0.0): 2, ('treat', 0.0): 5, ('v', 0.0): 6, ('product', 0.0): 4, ('letter', 0.0): 1, ('z', 0.0): 5, ('uniqu', 0.0): 2, ('refresh', 0.0): 1, ('popular', 0.0): 1, ('bebee', 0.0): 2, ('lt', 0.0): 1, ('inaccuraci', 0.0): 1, ('inaccur', 0.0): 1, ('worri', 0.0): 8, ('burn', 0.0): 4, ('rn', 0.0): 17, ('tragic', 0.0): 1, ('joy', 0.0): 2, ('sam', 0.0): 4, ('rush', 0.0): 2, ('toronto', 0.0): 1, ('stuart', 0.0): 1, (\"party'\", 0.0): 2, ('iyalaya', 0.0): 1, ('shade', 0.0): 3, ('round', 0.0): 3, ('clock', 0.0): 2, (';(', 0.0): 6, ('happier', 0.0): 1, ('h', 0.0): 8, ('ubusi', 0.0): 1, ('le', 0.0): 3, ('fifa', 0.0): 1, ('gymnast', 0.0): 1, ('aahhh', 0.0): 1, ('noggin', 0.0): 1, ('bump', 0.0): 1, ('feelslikeanidiot', 0.0): 1, ('pregnant', 0.0): 2, ('woman', 0.0): 5, ('dearli', 0.0): 1, ('sunshin', 0.0): 4, ('suk', 0.0): 2, ('pumpkin', 0.0): 1, ('scone', 0.0): 1, ('outnumb', 0.0): 1, ('vidcon', 0.0): 10, ('eri', 0.0): 1, ('geez', 0.0): 1, ('preciou', 0.0): 4, ('hive', 0.0): 1, ('vote', 0.0): 7, ('vietnam', 0.0): 1, ('decemb', 0.0): 2, ('dunt', 0.0): 1, ('ikr', 0.0): 3, ('sob', 0.0): 3, ('buff', 0.0): 1, ('leg', 0.0): 4, ('toni', 0.0): 1, ('deactiv', 0.0): 6, ('bra', 0.0): 2, (\"shady'\", 0.0): 1, ('isibaya', 0.0): 1, ('special', 0.0): 3, ('‚ù§', 0.0): 21, ('Ô∏è', 0.0): 19, ('üòì', 0.0): 2, ('slept', 0.0): 5, ('colder', 0.0): 1, ('took', 0.0): 9, ('med', 0.0): 1, ('sausag', 0.0): 1, ('adio', 0.0): 1, ('cold', 0.0): 15, ('sore', 0.0): 9, ('ew', 0.0): 3, ('h8', 0.0): 1, ('messeng', 0.0): 2, ('shittier', 0.0): 1, ('leno', 0.0): 1, ('ident', 0.0): 1, ('crisi', 0.0): 2, ('roommat', 0.0): 1, ('knock', 0.0): 3, ('nighter', 0.0): 3, ('bird', 0.0): 2, ('flew', 0.0): 2, ('thru', 0.0): 2, ('derek', 0.0): 3, ('tour', 0.0): 7, ('wetherspoon', 0.0): 1, ('pub', 0.0): 1, ('polic', 0.0): 4, ('frank', 0.0): 2, ('ocean', 0.0): 4, ('releas', 0.0): 8, ('ff', 0.0): 4, ('lisah', 0.0): 2, ('kikm', 0.0): 8, ('eboni', 0.0): 2, ('weloveyounamjoon', 0.0): 1, ('gave', 0.0): 8, ('dress', 0.0): 6, ('polka', 0.0): 1, ('dot', 0.0): 2, ('ndi', 0.0): 1, ('yum', 0.0): 1, ('feed', 0.0): 3, ('leftov', 0.0): 2, ('side', 0.0): 6, ('cs', 0.0): 2, ('own', 0.0): 1, ('walnut', 0.0): 1, ('whip', 0.0): 1, ('wife', 0.0): 6, ('boah', 0.0): 1, ('madi', 0.0): 2, ('def', 0.0): 3, ('manga', 0.0): 1, ('giant', 0.0): 3, ('aminormalyet', 0.0): 1, ('cooki', 0.0): 2, ('breakfast', 0.0): 5, ('clutch', 0.0): 1, ('poorli', 0.0): 6, ('tummi', 0.0): 6, ('pj', 0.0): 1, ('groan', 0.0): 1, ('nou', 0.0): 1, ('adam', 0.0): 2, ('ken', 0.0): 1, ('sara', 0.0): 2, ('sister', 0.0): 4, ('accid', 0.0): 2, ('sort', 0.0): 7, ('mate', 0.0): 2, ('pick', 0.0): 12, ('rang', 0.0): 4, ('fk', 0.0): 2, ('freak', 0.0): 5, ('describ', 0.0): 1, ('eric', 0.0): 2, ('prydz', 0.0): 1, ('sister-in-law', 0.0): 1, ('instal', 0.0): 2, ('seat', 0.0): 4, ('bought', 0.0): 6, ('rear-end', 0.0): 1, (\"everyone'\", 0.0): 4, ('trash', 0.0): 2, ('boob', 0.0): 3, ('whilst', 0.0): 3, ('stair', 0.0): 1, ('childhood', 0.0): 1, ('toothsensit', 0.0): 4, ('size', 0.0): 9, ('ke', 0.0): 3, ('shem', 0.0): 2, ('trust', 0.0): 2, ('awel', 0.0): 1, ('drunk', 0.0): 2, ('weekendofmad', 0.0): 1, ('üçπ', 0.0): 3, ('üç∏', 0.0): 1, ('cb', 0.0): 1, ('dancer', 0.0): 1, ('choregraph', 0.0): 1, ('626-430-8715', 0.0): 1, ('messag', 0.0): 8, ('repli', 0.0): 14, ('hoe', 0.0): 1, ('xd', 0.0): 7, ('xiu', 0.0): 1, ('nk', 0.0): 1, ('gi', 0.0): 2, ('uss', 0.0): 1, ('eliss', 0.0): 1, ('ksoo', 0.0): 2, ('session', 0.0): 5, ('tat', 0.0): 1, ('bcoz', 0.0): 1, ('bet', 0.0): 10, ('rancho', 0.0): 1, ('imperi', 0.0): 1, ('de', 0.0): 1, ('silang', 0.0): 1, ('subdivis', 0.0): 1, ('center', 0.0): 1, ('39', 0.0): 1, ('cornwal', 0.0): 1, ('verit', 0.0): 1, ('prize', 0.0): 2, ('regular', 0.0): 3, ('workout', 0.0): 1, ('spin', 0.0): 1, ('base', 0.0): 1, ('upon', 0.0): 1, ('penni', 0.0): 1, ('ebook', 0.0): 1, ('—Ñ–æ—Ç–æ—Å–µ—Ç', 0.0): 1, ('addicted-to-analsex', 0.0): 1, ('sweetbj', 0.0): 2, ('blowjob', 0.0): 1, ('mhhh', 0.0): 1, ('sed', 0.0): 1, ('sg', 0.0): 1, ('dinner', 0.0): 4, ('bless', 0.0): 2, ('mee', 0.0): 2, ('enviou', 0.0): 1, ('eonni', 0.0): 1, ('lovey', 0.0): 1, ('dovey', 0.0): 1, ('dongsaeng', 0.0): 1, ('workin', 0.0): 1, ('tuesday', 0.0): 4, ('schade', 0.0): 3, ('belfast', 0.0): 1, ('jealou', 0.0): 9, ('jacob', 0.0): 5, ('isco', 0.0): 4, ('peni', 0.0): 1, ('everi', 0.0): 16, ('convers', 0.0): 6, ('wonder', 0.0): 11, ('soul', 0.0): 5, ('nation', 0.0): 2, ('louisiana', 0.0): 4, ('lafayett', 0.0): 2, ('matteroftheheart', 0.0): 1, ('waduh', 0.0): 1, ('pant', 0.0): 3, ('suspend', 0.0): 2, ('believ', 0.0): 14, ('teenag', 0.0): 2, ('clich', 0.0): 1, ('youuu', 0.0): 5, ('rma', 0.0): 1, ('jersey', 0.0): 2, ('fake', 0.0): 4, ('jaclintil', 0.0): 1, ('model', 0.0): 9, ('likeforlik', 0.0): 7, ('mpoint', 0.0): 4, ('hotfmnoaidilforariana', 0.0): 2, ('ran', 0.0): 5, ('fuckkk', 0.0): 1, ('jump', 0.0): 3, ('justin', 0.0): 3, ('finish', 0.0): 14, ('sanum', 0.0): 1, ('llaollao', 0.0): 1, ('foood', 0.0): 1, ('ubericecream', 0.0): 14, ('glare', 0.0): 1, ('vine', 0.0): 3, ('tweetin', 0.0): 1, ('mood', 0.0): 3, ('elbow', 0.0): 1, ('choreo', 0.0): 1, ('offens', 0.0): 2, ('yeyi', 0.0): 1, ('hd', 0.0): 2, ('brow', 0.0): 1, ('kit', 0.0): 6, ('slightli', 0.0): 2, ('monday', 0.0): 10, ('sux', 0.0): 1, ('enjoy', 0.0): 9, ('nothaveld', 0.0): 1, ('765', 0.0): 1, ('edm', 0.0): 1, ('likeforfollow', 0.0): 3, ('hannib', 0.0): 3, ('mosquito', 0.0): 2, ('bite', 0.0): 5, ('kinki', 0.0): 1, ('hsould', 0.0): 1, ('justget', 0.0): 1, ('marri', 0.0): 2, ('la', 0.0): 11, ('shuffl', 0.0): 4, ('int', 0.0): 1, ('buckl', 0.0): 1, ('spring', 0.0): 1, ('millz', 0.0): 1, ('aski', 0.0): 2, ('awusasho', 0.0): 1, ('unlucki', 0.0): 2, ('driver', 0.0): 7, ('briefli', 0.0): 1, ('spot', 0.0): 4, ('144p', 0.0): 1, ('brook', 0.0): 1, ('crack', 0.0): 2, ('Ôº†', 0.0): 5, ('maverickgam', 0.0): 4, ('07:32', 0.0): 1, ('07:25', 0.0): 1, ('max', 0.0): 3, ('file', 0.0): 2, ('extern', 0.0): 2, ('sd', 0.0): 1, ('via', 0.0): 1, ('airdroid', 0.0): 1, ('android', 0.0): 2, ('4.4+', 0.0): 1, ('googl', 0.0): 5, ('alright', 0.0): 3, ('cramp', 0.0): 2, ('&lt;/3', 0.0): 6, ('unstan', 0.0): 1, ('tay', 0.0): 2, ('ngeze', 0.0): 1, ('cocktaili', 0.0): 1, ('classi', 0.0): 1, ('07:24', 0.0): 1, ('‚úà', 0.0): 2, ('Ô∏è2', 0.0): 1, ('raini', 0.0): 2, ('‚òî', 0.0): 2, ('peter', 0.0): 1, ('pen', 0.0): 1, ('spare', 0.0): 1, ('guest', 0.0): 2, ('barcelona', 0.0): 2, ('bilbao', 0.0): 1, ('booti', 0.0): 2, ('sharyl', 0.0): 1, ('shane', 0.0): 2, ('ta', 0.0): 1, ('giddi', 0.0): 1, ('d1', 0.0): 1, ('zipper', 0.0): 1, ('beyond', 0.0): 1, ('repair', 0.0): 4, ('iphon', 0.0): 5, ('upgrad', 0.0): 1, ('april', 0.0): 1, ('2016', 0.0): 1, ('cont', 0.0): 2, ('england', 0.0): 4, ('wore', 0.0): 2, ('greet', 0.0): 5, ('tempt', 0.0): 2, ('whole', 0.0): 16, ('pack', 0.0): 6, ('oreo', 0.0): 2, ('strength', 0.0): 1, ('wifi', 0.0): 5, ('network', 0.0): 4, ('within', 0.0): 3, ('lolipop', 0.0): 1, ('kebab', 0.0): 1, ('klappertart', 0.0): 1, ('cake', 0.0): 10, ('moodbost', 0.0): 2, ('shoot', 0.0): 6, ('unprepar', 0.0): 1, ('sri', 0.0): 1, ('dresscod', 0.0): 1, ('door', 0.0): 6, ('iam', 0.0): 2, ('dnt', 0.0): 1, ('stab', 0.0): 3, ('meh', 0.0): 3, ('wrocilam', 0.0): 1, ('otp', 0.0): 3, ('5', 0.0): 14, ('looww', 0.0): 1, ('recov', 0.0): 2, ('wayn', 0.0): 2, ('insur', 0.0): 3, ('loss', 0.0): 3, ('stolen', 0.0): 2, ('accident', 0.0): 1, ('damag', 0.0): 5, ('devic', 0.0): 3, ('warranti', 0.0): 1, ('centr', 0.0): 2, ('üëå', 0.0): 1, ('lmfaoo', 0.0): 1, ('accur', 0.0): 2, ('fra', 0.0): 4, ('aliv', 0.0): 2, ('steel', 0.0): 2, ('otamendi', 0.0): 1, ('ny', 0.0): 2, ('üöñ', 0.0): 1, ('üóΩ', 0.0): 1, ('üåÉ', 0.0): 1, ('stealth', 0.0): 2, ('bastard', 0.0): 2, ('inc', 0.0): 3, ('steam', 0.0): 2, ('therapi', 0.0): 1, ('exhaust', 0.0): 3, ('lie', 0.0): 7, ('total', 0.0): 11, ('block', 0.0): 11, ('choic', 0.0): 5, ('switzerland', 0.0): 1, ('kfc', 0.0): 1, ('common', 0.0): 4, ('th', 0.0): 5, ('wolrd', 0.0): 1, ('fyn', 0.0): 1, ('drop', 0.0): 10, ('state', 0.0): 4, ('3g', 0.0): 2, ('christ', 0.0): 1, ('scale', 0.0): 1, ('deck', 0.0): 1, ('chair', 0.0): 4, ('yk', 0.0): 1, ('resi', 0.0): 1, ('memori', 0.0): 5, ('nude', 0.0): 4, ('bruh', 0.0): 3, ('prepar', 0.0): 3, ('lock', 0.0): 2, ('view', 0.0): 7, ('fbc', 0.0): 3, ('mork', 0.0): 1, ('873', 0.0): 1, ('kikgirl', 0.0): 13, ('premiostumundo', 0.0): 2, ('hotspotwithdanri', 0.0): 1, ('hospit', 0.0): 3, ('food', 0.0): 18, ('sone', 0.0): 1, ('produc', 0.0): 1, ('potag', 0.0): 1, ('tomato', 0.0): 1, ('blight', 0.0): 1, ('sheffield', 0.0): 1, ('mych', 0.0): 1, ('shiiit', 0.0): 2, ('screenshot', 0.0): 4, ('prompt', 0.0): 1, ('areadi', 0.0): 1, ('similar', 0.0): 4, ('soulmat', 0.0): 1, ('canon', 0.0): 1, ('zzz', 0.0): 2, ('britain', 0.0): 1, ('üòÅ', 0.0): 3, ('mana', 0.0): 2, ('hw', 0.0): 1, ('jouch', 0.0): 1, ('por', 0.0): 1, ('que', 0.0): 1, ('liceooo', 0.0): 1, ('30', 0.0): 3, ('minut', 0.0): 6, ('pass', 0.0): 13, ('ayala', 0.0): 1, ('tunnel', 0.0): 2, ('thatscold', 0.0): 1, ('80', 0.0): 1, ('snap', 0.0): 3, ('lourd', 0.0): 1, ('bang', 0.0): 3, ('anywher', 0.0): 4, ('water', 0.0): 8, ('road', 0.0): 1, ('showbox', 0.0): 1, ('naruto', 0.0): 1, ('cartoon', 0.0): 1, ('companion', 0.0): 2, ('skinni', 0.0): 3, ('fat', 0.0): 4, ('bare', 0.0): 6, ('dubai', 0.0): 3, ('calum', 0.0): 1, ('ashton', 0.0): 1, ('‚úß', 0.0): 8, ('ÔΩ°', 0.0): 8, ('chelni', 0.0): 4, ('disappoint', 0.0): 13, ('everybodi', 0.0): 5, ('due', 0.0): 14, ('laribuggi', 0.0): 1, ('medic', 0.0): 1, ('nutella', 0.0): 1, (\"could'v\", 0.0): 3, ('siriu', 0.0): 1, ('goat', 0.0): 4, ('frudg', 0.0): 1, ('mike', 0.0): 1, ('cloth', 0.0): 6, ('stuff', 0.0): 11, ('sat', 0.0): 3, ('number', 0.0): 6, ('ring', 0.0): 1, ('bbz', 0.0): 1, ('angek', 0.0): 1, ('sbali', 0.0): 1, ('euuuwww', 0.0): 2, ('lunch', 0.0): 10, ('construct', 0.0): 3, ('worker', 0.0): 3, ('1k', 0.0): 3, ('style', 0.0): 4, ('nell', 0.0): 1, ('ik', 0.0): 2, ('death', 0.0): 3, ('jaysu', 0.0): 1, ('toast', 0.0): 1, ('insecur', 0.0): 2, ('buti', 0.0): 1, ('ure', 0.0): 2, ('poop', 0.0): 1, ('gorgeou', 0.0): 2, ('angel', 0.0): 2, ('rome', 0.0): 1, ('throat', 0.0): 10, ('llama', 0.0): 1, ('urself', 0.0): 2, ('getwellsoonamb', 0.0): 1, ('heath', 0.0): 2, ('ledger', 0.0): 1, ('appl', 0.0): 3, ('permiss', 0.0): 2, ('2-0', 0.0): 1, ('lead', 0.0): 3, ('supersport', 0.0): 1, ('milkshak', 0.0): 1, ('witcher', 0.0): 1, ('papertown', 0.0): 1, ('bale', 0.0): 1, ('9', 0.0): 5, ('m√©xico', 0.0): 1, ('bahay', 0.0): 1, ('bahayan', 0.0): 1, ('magisa', 0.0): 1, ('sadlyf', 0.0): 1, ('bunso', 0.0): 1, ('sleeep', 0.0): 4, ('astonvilla', 0.0): 1, ('berigaud', 0.0): 1, ('bakar', 0.0): 1, ('club', 0.0): 4, ('dear', 0.0): 11, ('allerg', 0.0): 4, ('depress', 0.0): 5, (\"blaine'\", 0.0): 1, ('acoust', 0.0): 2, ('version', 0.0): 5, ('excus', 0.0): 3, ('hernia', 0.0): 3, ('toxin', 0.0): 1, ('freedom', 0.0): 1, ('organ', 0.0): 2, ('ariel', 0.0): 1, ('slap', 0.0): 1, ('slam', 0.0): 1, ('bee', 0.0): 1, ('unknown', 0.0): 2, ('finddjderek', 0.0): 1, ('smell', 0.0): 3, ('uuughhh', 0.0): 1, ('grabe', 0.0): 5, ('ka', 0.0): 5, ('where', 0.0): 1, ('gf', 0.0): 3, ('james_yammouni', 0.0): 1, ('smi', 0.0): 1, ('nemesi', 0.0): 1, ('rule', 0.0): 1, ('doesnt', 0.0): 2, ('appeal', 0.0): 1, ('neeein', 0.0): 1, ('saaad', 0.0): 3, ('less', 0.0): 3, ('hang', 0.0): 7, ('creas', 0.0): 1, ('tan', 0.0): 3, ('dalla', 0.0): 4, ('suppos', 0.0): 7, ('infront', 0.0): 2, ('beato', 0.0): 1, ('tim', 0.0): 2, ('prob', 0.0): 5, ('minha', 0.0): 1, ('deleici', 0.0): 1, ('hr', 0.0): 2, ('pcb', 0.0): 1, ('ep', 0.0): 5, ('peregrin', 0.0): 1, ('8.40', 0.0): 1, ('pigeon', 0.0): 1, ('feet', 0.0): 3, ('tram', 0.0): 1, ('hav', 0.0): 2, ('spent', 0.0): 5, ('outsid', 0.0): 9, ('apt', 0.0): 1, ('build', 0.0): 3, ('key', 0.0): 3, ('bldg', 0.0): 1, ('wrote', 0.0): 3, ('dark', 0.0): 5, ('swan', 0.0): 1, ('fifth', 0.0): 2, ('mmmm', 0.0): 1, ('avi', 0.0): 4, ('nicki', 0.0): 1, ('fucjikg', 0.0): 1, ('disgust', 0.0): 6, ('buynotanapologyonitun', 0.0): 1, ('aval', 0.0): 1, ('denmark', 0.0): 1, ('nw', 0.0): 2, ('sch', 0.0): 2, ('share', 0.0): 11, ('jeslyn', 0.0): 1, ('72', 0.0): 4, ('root', 0.0): 2, ('kuch', 0.0): 1, ('nahi', 0.0): 1, ('hua', 0.0): 2, ('newbi', 0.0): 1, ('crap', 0.0): 3, ('miracl', 0.0): 1, ('4th', 0.0): 1, ('linda', 0.0): 1, ('click', 0.0): 1, ('pin', 0.0): 2, ('wing', 0.0): 3, ('epic', 0.0): 2, ('page', 0.0): 6, ('ang', 0.0): 8, ('ganda', 0.0): 1, ('üíó', 0.0): 4, ('nux', 0.0): 1, ('hinanap', 0.0): 1, ('ako', 0.0): 1, ('uy', 0.0): 1, ('sched', 0.0): 1, ('anyar', 0.0): 1, ('entertain', 0.0): 2, ('typa', 0.0): 3, ('buddi', 0.0): 2, ('transpar', 0.0): 1, ('photoshop', 0.0): 2, ('planner', 0.0): 1, ('helppp', 0.0): 2, ('wearig', 0.0): 1, ('dri', 0.0): 2, ('alot', 0.0): 3, ('bu', 0.0): 5, ('prey', 0.0): 1, ('gross', 0.0): 5, ('drain', 0.0): 3, ('ausfailia', 0.0): 1, ('snow', 0.0): 3, ('footi', 0.0): 3, ('2nd', 0.0): 5, ('row', 0.0): 3, (\"m'\", 0.0): 2, ('kitkat', 0.0): 2, ('bday', 0.0): 7, ('üò¢', 0.0): 8, ('suger', 0.0): 1, ('olivia', 0.0): 2, ('audit', 0.0): 1, ('american', 0.0): 1, ('idol', 0.0): 2, ('injuri', 0.0): 2, ('appendix', 0.0): 1, ('burst', 0.0): 2, ('append', 0.0): 1, ('yeahh', 0.0): 2, ('fack', 0.0): 2, ('nhl', 0.0): 1, ('khami', 0.0): 2, ('favorit', 0.0): 4, ('rise', 0.0): 3, ('reaali', 0.0): 1, ('ja', 0.0): 2, ('naomi', 0.0): 1, ('modern', 0.0): 1, ('contemporari', 0.0): 1, ('slack', 0.0): 1, ('565', 0.0): 1, ('blond', 0.0): 2, ('jahat', 0.0): 3, ('discount', 0.0): 1, ('thorp', 0.0): 2, ('park', 0.0): 7, ('esnho', 0.0): 1, ('node', 0.0): 1, ('advanc', 0.0): 4, ('directx', 0.0): 1, ('workshop', 0.0): 1, ('p2', 0.0): 1, ('upload', 0.0): 2, ('remov', 0.0): 5, ('blackberri', 0.0): 1, ('shitti', 0.0): 1, ('mobil', 0.0): 2, ('povertyyouareevil', 0.0): 1, ('struggl', 0.0): 4, ('math', 0.0): 1, ('emm', 0.0): 1, ('data', 0.0): 6, ('elgin', 0.0): 1, ('vava', 0.0): 1, ('makati', 0.0): 1, ('üíõ', 0.0): 4, ('baon', 0.0): 1, ('soup', 0.0): 3, ('soak', 0.0): 1, ('bread', 0.0): 2, ('mush', 0.0): 1, (\"they'd\", 0.0): 2, ('matt', 0.0): 2, ('ouat', 0.0): 1, ('beach', 0.0): 5, ('blinkin', 0.0): 1, ('unblock', 0.0): 1, ('headack', 0.0): 1, ('tension', 0.0): 1, ('erit', 0.0): 1, ('perspect', 0.0): 1, ('wed', 0.0): 4, ('playlist', 0.0): 2, ('endlessli', 0.0): 1, ('blush', 0.0): 1, ('bat', 0.0): 1, ('kiddo', 0.0): 1, ('rumbel', 0.0): 1, ('overwhelm', 0.0): 1, ('thrown', 0.0): 2, ('irrespons', 0.0): 1, ('pakighinabi', 0.0): 1, ('pinkfinit', 0.0): 1, ('beb', 0.0): 2, ('migrain', 0.0): 2, ('almost', 0.0): 11, ('coyot', 0.0): 1, ('outta', 0.0): 1, ('mad', 0.0): 11, ('üòí', 0.0): 3, ('headach', 0.0): 9, ('Ïù∏ÌîºÎãàÌä∏', 0.0): 2, ('save', 0.0): 6, ('baechu', 0.0): 1, ('calibraskaep', 0.0): 3, ('r', 0.0): 19, ('fanci', 0.0): 2, ('yt', 0.0): 3, ('purchas', 0.0): 2, ('elgato', 0.0): 1, ('ant', 0.0): 2, ('unexpect', 0.0): 2, ('bestfriend', 0.0): 9, ('faint', 0.0): 1, ('bp', 0.0): 1, ('appar', 0.0): 5, ('shower', 0.0): 3, ('subway', 0.0): 1, ('cool', 0.0): 5, ('prayer', 0.0): 2, ('fragil', 0.0): 1, ('huge', 0.0): 3, ('gap', 0.0): 1, ('plot', 0.0): 2, ('bungi', 0.0): 1, ('folk', 0.0): 1, ('raspberri', 0.0): 1, ('pi', 0.0): 1, ('shoe', 0.0): 2, ('woohyun', 0.0): 2, ('guilti', 0.0): 1, ('monica', 0.0): 2, ('davao', 0.0): 1, ('luckyyi', 0.0): 1, ('confid', 0.0): 1, ('eunha', 0.0): 1, ('misplac', 0.0): 1, ('den', 0.0): 1, ('dae', 0.0): 1, ('bap', 0.0): 1, ('likewis', 0.0): 1, ('liam', 0.0): 1, ('dylan', 0.0): 3, ('huehu', 0.0): 1, ('rice', 0.0): 1, ('krispi', 0.0): 1, ('marshmallow', 0.0): 2, ('srsli', 0.0): 7, ('birmingham', 0.0): 1, ('m5m6junction', 0.0): 1, ('soulsurvivor', 0.0): 1, ('stafford', 0.0): 1, ('progress', 0.0): 1, ('mixtur', 0.0): 1, (\"they'v\", 0.0): 4, ('practic', 0.0): 1, ('lage', 0.0): 1, ('ramd', 0.0): 1, ('lesbian', 0.0): 3, ('oralsex', 0.0): 4, ('munchkin', 0.0): 1, ('juja', 0.0): 1, ('murugan', 0.0): 1, ('handl', 0.0): 3, ('dia', 0.0): 2, ('bgtau', 0.0): 1, ('harap', 0.0): 1, ('bagi', 0.0): 1, ('aminn', 0.0): 1, ('fraand', 0.0): 1, ('üò¨', 0.0): 2, ('bigbang', 0.0): 2, ('steak', 0.0): 1, ('younger', 0.0): 2, ('sian', 0.0): 2, ('pizza', 0.0): 7, ('5am', 0.0): 5, ('nicoleapag', 0.0): 1, ('makeup', 0.0): 4, ('hellish', 0.0): 1, ('thirstyyi', 0.0): 1, ('chesti', 0.0): 1, ('dad', 0.0): 9, (\"nando'\", 0.0): 1, ('22', 0.0): 3, ('bow', 0.0): 2, ('queen', 0.0): 3, ('brave', 0.0): 1, ('hen', 0.0): 1, ('leed', 0.0): 9, ('rdd', 0.0): 1, ('dissip', 0.0): 1, ('. .', 0.0): 1, ('pump', 0.0): 2, ('capee', 0.0): 1, ('japan', 0.0): 2, ('random', 0.0): 1, ('young', 0.0): 5, ('outliv', 0.0): 1, ('x-ray', 0.0): 1, ('dental', 0.0): 1, ('spine', 0.0): 1, ('relief', 0.0): 1, ('popol', 0.0): 1, ('stomach', 0.0): 8, ('frog', 0.0): 2, ('brad', 0.0): 1, ('gen.ad', 0.0): 1, ('price', 0.0): 5, ('negoti', 0.0): 3, ('huhuhuhuhu', 0.0): 1, ('bbmadeinmanila', 0.0): 1, ('findavip', 0.0): 1, ('boyirl', 0.0): 1, ('yasss', 0.0): 1, ('6th', 0.0): 1, ('june', 0.0): 3, ('lain', 0.0): 1, ('diffici', 0.0): 1, ('custom', 0.0): 1, ('internet', 0.0): 9, ('near', 0.0): 9, ('speed', 0.0): 2, ('escap', 0.0): 1, ('rapist', 0.0): 1, ('commit', 0.0): 2, ('crime', 0.0): 1, ('bachpan', 0.0): 1, ('ki', 0.0): 2, ('yaadein', 0.0): 1, ('finnair', 0.0): 1, ('heathrow', 0.0): 1, ('norwegian', 0.0): 1, (':\\\\', 0.0): 1, ('batteri', 0.0): 3, ('upvot', 0.0): 4, ('keeno', 0.0): 1, ('whatthefuck', 0.0): 1, ('grotti', 0.0): 1, ('attent', 0.0): 1, ('seeker', 0.0): 1, ('moral', 0.0): 1, ('fern', 0.0): 1, ('mimi', 0.0): 1, ('bali', 0.0): 1, ('she', 0.0): 4, ('pleasee', 0.0): 3, ('brb', 0.0): 1, ('lowbat', 0.0): 1, ('otwolgrandtrail', 0.0): 4, ('funk', 0.0): 1, ('wewanticecream', 0.0): 1, ('sweat', 0.0): 2, ('eugh', 0.0): 1, ('speak', 0.0): 4, ('occasion', 0.0): 1, (\"izzy'\", 0.0): 1, ('dorm', 0.0): 1, ('choppi', 0.0): 1, ('paul', 0.0): 1, ('switch', 0.0): 4, (\"infinite'\", 0.0): 2, ('5:30', 0.0): 2, ('cayton', 0.0): 1, ('bay', 0.0): 2, ('emma', 0.0): 2, ('jen', 0.0): 1, ('darcey', 0.0): 1, ('connor', 0.0): 1, ('spoke', 0.0): 1, ('nail', 0.0): 2, ('biggest', 0.0): 3, ('blue', 0.0): 5, ('bottl', 0.0): 3, ('roommateexperi', 0.0): 1, ('yup', 0.0): 4, ('avoid', 0.0): 2, ('ic', 0.0): 1, ('te', 0.0): 1, ('auto-followback', 0.0): 1, ('asian', 0.0): 2, ('puppi', 0.0): 3, ('ljp', 0.0): 1, ('1/5', 0.0): 1, ('nowday', 0.0): 1, ('attach', 0.0): 2, ('beat', 0.0): 2, ('numb', 0.0): 1, ('dentist', 0.0): 3, ('misss', 0.0): 2, ('muchhh', 0.0): 1, ('youtub', 0.0): 5, ('rid', 0.0): 3, ('tab', 0.0): 2, ('uca', 0.0): 1, ('onto', 0.0): 2, ('track', 0.0): 3, ('bigtim', 0.0): 1, ('rumor', 0.0): 3, ('warmest', 0.0): 1, ('chin', 0.0): 2, ('tickl', 0.0): 1, ('‚ô´', 0.0): 1, ('zikra', 0.0): 1, ('lusi', 0.0): 1, ('hasya', 0.0): 1, ('nugget', 0.0): 3, ('som', 0.0): 1, ('lu', 0.0): 1, ('olymp', 0.0): 1, (\"millie'\", 0.0): 1, ('guinea', 0.0): 1, ('lewi', 0.0): 1, ('748292', 0.0): 1, (\"we'll\", 0.0): 8, ('ano', 0.0): 2, ('22stan', 0.0): 1, ('24/7', 0.0): 2, ('thankyou', 0.0): 2, ('kanina', 0.0): 2, ('breakdown', 0.0): 2, ('mag', 0.0): 2, ('hatee', 0.0): 1, ('leas', 0.0): 1, ('written', 0.0): 2, ('hurri', 0.0): 4, ('attempt', 0.0): 1, ('6g', 0.0): 1, ('unsuccess', 0.0): 1, ('earlob', 0.0): 1, ('sue', 0.0): 1, ('dreari', 0.0): 1, ('denis', 0.0): 1, ('muriel', 0.0): 1, ('ahour√©', 0.0): 1, ('pr', 0.0): 1, ('brand', 0.0): 1, ('imag', 0.0): 4, ('opportun', 0.0): 1, ('po', 0.0): 1, ('beg', 0.0): 2, (\"kath'd\", 0.0): 1, ('respond', 0.0): 2, ('chop', 0.0): 1, ('wbu', 0.0): 1, ('yess', 0.0): 2, ('kme', 0.0): 1, ('tom', 0.0): 4, ('cram', 0.0): 1, ('‚Äì', 0.0): 1, ('curiou', 0.0): 1, ('on-board', 0.0): 1, ('announc', 0.0): 3, ('trespass', 0.0): 1, ('fr', 0.0): 3, ('clandestin', 0.0): 1, ('muller', 0.0): 1, ('obviou', 0.0): 1, ('mufc', 0.0): 1, ('colour', 0.0): 4, ('stu', 0.0): 2, ('movie', 0.0): 1, ('buddyyi', 0.0): 1, ('feelgoodfriday', 0.0): 1, ('forest', 0.0): 1, ('6:30', 0.0): 1, ('babysit', 0.0): 1, ('opix', 0.0): 1, ('805', 0.0): 1, ('pilllow', 0.0): 1, ('fool', 0.0): 1, ('brag', 0.0): 1, ('skrillah', 0.0): 1, ('drown', 0.0): 2, ('gue', 0.0): 1, ('report', 0.0): 4, ('eventu', 0.0): 1, ('north', 0.0): 1, ('west', 0.0): 2, ('kitti', 0.0): 1, ('sjkao', 0.0): 1, ('mm', 0.0): 2, ('srri', 0.0): 1, ('honma', 0.0): 1, ('yeh', 0.0): 1, ('walay', 0.0): 1, ('bhi', 0.0): 2, ('bohat', 0.0): 1, ('wailay', 0.0): 1, ('hain', 0.0): 2, ('pre-season', 0.0): 1, ('friendli', 0.0): 3, ('pe', 0.0): 3, ('itna', 0.0): 2, ('shor', 0.0): 1, ('machaya', 0.0): 1, ('mein', 0.0): 1, ('samjha', 0.0): 1, ('cup', 0.0): 3, ('note', 0.0): 2, ('üòÑ', 0.0): 1, ('üëç', 0.0): 1, ('üòî', 0.0): 7, ('sirkay', 0.0): 1, ('wali', 0.0): 1, ('pyaaz', 0.0): 1, ('daal', 0.0): 2, ('onion', 0.0): 1, ('vinegar', 0.0): 1, ('cook', 0.0): 3, ('tutori', 0.0): 1, ('soho', 0.0): 1, ('wobbl', 0.0): 1, ('server', 0.0): 4, ('ciao', 0.0): 1, ('masaan', 0.0): 1, ('muv', 0.0): 1, ('beast', 0.0): 2, ('hayst', 0.0): 1, ('cr', 0.0): 1, ('hnnn', 0.0): 1, ('fluffi', 0.0): 2, ('comeback', 0.0): 3, ('korea', 0.0): 1, ('wow', 0.0): 10, ('act', 0.0): 4, ('optimis', 0.0): 1, ('soniii', 0.0): 1, ('kahaaa', 0.0): 1, ('shave', 0.0): 3, ('tryna', 0.0): 3, ('healthi', 0.0): 2, ('freez', 0.0): 3, ('fml', 0.0): 4, ('jacket', 0.0): 1, ('sleepi', 0.0): 4, ('cyber', 0.0): 1, ('bulli', 0.0): 2, ('racial', 0.0): 2, ('scari', 0.0): 6, ('hall', 0.0): 1, ('stockholm', 0.0): 1, ('loool', 0.0): 3, ('bunch', 0.0): 3, ('among', 0.0): 1, ('__', 0.0): 2, ('busier', 0.0): 1, ('onward', 0.0): 1, ('ol', 0.0): 2, ('coincid', 0.0): 1, ('imac', 0.0): 1, ('launch', 0.0): 2, ('gram', 0.0): 1, ('nearer', 0.0): 1, ('blain', 0.0): 2, ('darren', 0.0): 2, ('layout', 0.0): 3, ('fuuuck', 0.0): 2, ('jesu', 0.0): 1, ('gishwh', 0.0): 1, ('exclud', 0.0): 1, ('unless', 0.0): 4, ('c', 0.0): 7, ('angelica', 0.0): 1, ('pull', 0.0): 5, ('colleg', 0.0): 5, ('movement', 0.0): 1, ('frou', 0.0): 1, ('vaccin', 0.0): 1, ('armor', 0.0): 2, ('legendari', 0.0): 1, ('cash', 0.0): 2, ('effort', 0.0): 2, ('nat', 0.0): 2, ('brake', 0.0): 1, ('grumpi', 0.0): 4, ('wreck', 0.0): 1, ('decis', 0.0): 2, ('gahhh', 0.0): 1, ('teribl', 0.0): 1, ('kilig', 0.0): 1, ('togeth', 0.0): 7, ('weaker', 0.0): 1, ('shravan', 0.0): 1, ('tv', 0.0): 4, ('stooop', 0.0): 1, ('gi-guilti', 0.0): 1, ('akooo', 0.0): 1, ('imveryverysorri', 0.0): 1, ('cd', 0.0): 1, ('grey', 0.0): 3, ('basenam', 0.0): 1, ('path', 0.0): 1, ('theme', 0.0): 2, ('cigar', 0.0): 1, ('speaker', 0.0): 1, ('volum', 0.0): 1, ('promethazin', 0.0): 1, ('zopiclon', 0.0): 1, ('addit', 0.0): 1, ('quetiapin', 0.0): 1, ('modifi', 0.0): 1, ('prescript', 0.0): 1, ('greska', 0.0): 1, ('macedonian', 0.0): 1, ('slovak', 0.0): 1, ('hike', 0.0): 1, ('certainli', 0.0): 2, ('browser', 0.0): 2, ('os', 0.0): 1, ('zokay', 0.0): 1, ('accent', 0.0): 1, ('b-but', 0.0): 1, ('gintama', 0.0): 1, ('shinsengumi', 0.0): 1, ('chapter', 0.0): 1, ('andi', 0.0): 1, ('crappl', 0.0): 1, ('agre', 0.0): 5, ('ftw', 0.0): 2, ('phandroid', 0.0): 1, ('tline', 0.0): 1, ('orchestra', 0.0): 1, ('ppl', 0.0): 5, ('rehears', 0.0): 1, ('bittersweet', 0.0): 1, ('eunji', 0.0): 1, ('bakit', 0.0): 4, ('121st', 0.0): 1, (\"yesterday'\", 0.0): 1, ('rt', 0.0): 8, ('ehdar', 0.0): 1, ('pegea', 0.0): 1, ('panga', 0.0): 1, ('dosto', 0.0): 1, ('nd', 0.0): 1, ('real_liam_payn', 0.0): 1, ('retweet', 0.0): 5, ('3/10', 0.0): 1, ('dmed', 0.0): 1, ('ad', 0.0): 1, ('yay', 0.0): 3, ('23', 0.0): 2, ('alreaddyyi', 0.0): 1, ('luceleva', 0.0): 1, ('21', 0.0): 1, ('porno', 0.0): 3, ('countrymus', 0.0): 4, ('sexysasunday', 0.0): 2, ('naeun', 0.0): 1, ('goal', 0.0): 5, (\"son'\", 0.0): 1, ('kidney', 0.0): 2, ('printer', 0.0): 1, ('ink', 0.0): 2, ('asham', 0.0): 3, ('ihatesomepeopl', 0.0): 1, ('tabl', 0.0): 2, ('0-2', 0.0): 1, ('brain', 0.0): 2, ('hard-wir', 0.0): 1, ('canadian', 0.0): 1, ('acn', 0.0): 2, ('gulo', 0.0): 1, ('kandekj', 0.0): 1, ('rize', 0.0): 1, ('meydan', 0.0): 1, ('experienc', 0.0): 2, ('fcking', 0.0): 1, ('crei', 0.0): 1, ('stabl', 0.0): 1, ('dormmat', 0.0): 1, ('pre', 0.0): 3, ('bo3', 0.0): 1, ('cod', 0.0): 2, ('redeem', 0.0): 1, ('invalid', 0.0): 1, ('wag', 0.0): 1, ('hopia', 0.0): 1, ('campaign', 0.0): 2, ('editor', 0.0): 1, ('reveal', 0.0): 2, ('booo', 0.0): 2, ('extens', 0.0): 1, ('rightnow', 0.0): 1, ('btu', 0.0): 1, ('karaok', 0.0): 1, ('licenc', 0.0): 1, ('apb', 0.0): 2, ('mbf', 0.0): 1, ('kpop', 0.0): 2, ('hahahaokay', 0.0): 1, ('basara', 0.0): 1, ('capcom', 0.0): 3, ('pc', 0.0): 2, ('url', 0.0): 2, ('web', 0.0): 2, ('site', 0.0): 6, ('design', 0.0): 3, ('grumbl', 0.0): 2, ('migrant', 0.0): 1, ('daddi', 0.0): 4, ('legit', 0.0): 1, ('australia', 0.0): 3, ('awsm', 0.0): 1, ('entir', 0.0): 5, ('tmw', 0.0): 1, ('uwu', 0.0): 1, ('jinki', 0.0): 1, ('taem', 0.0): 1, ('gif', 0.0): 2, ('cambridg', 0.0): 1, ('viath', 0.0): 1, ('brilliant', 0.0): 1, ('cypru', 0.0): 1, ('wet', 0.0): 10, ('30th', 0.0): 1, ('zayncomebackto', 0.0): 2, ('1d', 0.0): 6, ('senior', 0.0): 2, ('spazz', 0.0): 1, ('soobin', 0.0): 1, ('27', 0.0): 1, ('unmarri', 0.0): 1, ('float', 0.0): 3, ('pressur', 0.0): 3, ('winter', 0.0): 4, ('lifetim', 0.0): 2, ('hiondsh', 0.0): 1, ('58543', 0.0): 1, ('kikmenow', 0.0): 9, ('sexdat', 0.0): 2, (\"demi'\", 0.0): 1, ('junjou', 0.0): 2, ('romantica', 0.0): 1, ('cruel', 0.0): 1, ('privileg', 0.0): 2, ('mixtap', 0.0): 2, ('convinc', 0.0): 3, ('friex', 0.0): 1, ('taco', 0.0): 2, ('europ', 0.0): 2, ('shaylan', 0.0): 1, ('4:20', 0.0): 1, ('ylona', 0.0): 1, ('nah', 0.0): 4, ('notanapolog', 0.0): 3, ('ouh', 0.0): 1, ('tax', 0.0): 4, ('ohhh', 0.0): 2, ('nm', 0.0): 1, ('term', 0.0): 1, ('apolog', 0.0): 3, ('encanta', 0.0): 1, ('vale', 0.0): 1, ('osea', 0.0): 1, ('bea', 0.0): 1, ('‚ôõ', 0.0): 210, ('„Äã', 0.0): 210, ('beliÃáev', 0.0): 35, ('wiÃáll', 0.0): 35, ('justiÃán', 0.0): 35, ('x15', 0.0): 35, ('350', 0.0): 4, ('ÔΩìÔΩÖÔΩÖ', 0.0): 35, ('ÔΩçÔΩÖ', 0.0): 35, ('40', 0.0): 3, ('dj', 0.0): 2, ('net', 0.0): 2, ('349', 0.0): 1, ('baek', 0.0): 1, ('tight', 0.0): 1, ('dunwan', 0.0): 1, ('suan', 0.0): 1, ('ba', 0.0): 3, ('haiz', 0.0): 1, ('otw', 0.0): 1, ('trade', 0.0): 3, ('venic', 0.0): 1, ('348', 0.0): 1, ('strong', 0.0): 6, ('adult', 0.0): 3, ('347', 0.0): 1, ('tree', 0.0): 3, ('hill', 0.0): 1, ('üòï', 0.0): 1, ('com', 0.0): 1, ('insonia', 0.0): 1, ('346', 0.0): 1, ('rick', 0.0): 1, ('ross', 0.0): 1, ('wallet', 0.0): 4, ('empti', 0.0): 3, ('heartbreak', 0.0): 2, ('episod', 0.0): 11, ('345', 0.0): 1, ('milli', 0.0): 1, (':)', 0.0): 2, ('diff', 0.0): 1, ('persona', 0.0): 1, ('golden', 0.0): 1, ('scene', 0.0): 1, ('advert', 0.0): 1, ('determin', 0.0): 2, ('roseburi', 0.0): 1, ('familyhom', 0.0): 1, ('daw', 0.0): 2, ('344', 0.0): 1, ('monkey', 0.0): 1, ('yea', 0.0): 2, ('343', 0.0): 1, ('sweeti', 0.0): 2, ('erica', 0.0): 1, ('istg', 0.0): 1, ('lick', 0.0): 1, ('jackson', 0.0): 4, ('nsbzhdnxndamal', 0.0): 1, ('342', 0.0): 1, ('11:15', 0.0): 1, ('2hour', 0.0): 1, ('11:25', 0.0): 1, ('341', 0.0): 1, ('fandom', 0.0): 2, ('mahilig', 0.0): 1, ('mam-bulli', 0.0): 1, ('mtaani', 0.0): 1, ('tunaita', 0.0): 1, ('viazi', 0.0): 1, ('choma', 0.0): 1, ('laid', 0.0): 1, ('celebr', 0.0): 3, ('7am', 0.0): 1, ('jerk', 0.0): 1, ('lah', 0.0): 2, ('magic', 0.0): 1, ('menil', 0.0): 1, ('340', 0.0): 1, (\"kam'\", 0.0): 1, ('meee', 0.0): 1, ('diz', 0.0): 1, ('biooo', 0.0): 1, ('ay', 0.0): 1, ('taray', 0.0): 1, ('yumu-youtub', 0.0): 1, ('339', 0.0): 1, ('parijat', 0.0): 1, ('willmissyouparijat', 0.0): 1, ('abroad', 0.0): 2, ('jolli', 0.0): 1, ('scotland', 0.0): 2, ('338', 0.0): 1, ('mcnugget', 0.0): 1, ('sophi', 0.0): 5, ('feedback', 0.0): 4, ('met', 0.0): 7, ('caramello', 0.0): 2, ('koala', 0.0): 1, ('bar', 0.0): 1, ('suckmejimin', 0.0): 1, ('337', 0.0): 1, ('sucki', 0.0): 2, ('laughter', 0.0): 1, ('pou', 0.0): 1, ('goddamn', 0.0): 1, ('bark', 0.0): 1, ('nje', 0.0): 1, ('blast', 0.0): 1, ('hun', 0.0): 4, ('dbn', 0.0): 2, ('üéÄ', 0.0): 1, ('336', 0.0): 1, ('hardest', 0.0): 1, ('335', 0.0): 1, ('pledg', 0.0): 1, ('realiz', 0.0): 7, ('viber', 0.0): 1, ('mwah', 0.0): 1, ('estat', 0.0): 1, ('crush', 0.0): 1, ('lansi', 0.0): 1, ('334', 0.0): 1, ('hp', 0.0): 4, ('waah', 0.0): 1, ('miami', 0.0): 1, ('vandag', 0.0): 1, ('kgola', 0.0): 1, ('neng', 0.0): 1, ('eintlik', 0.0): 1, ('porn', 0.0): 2, ('4like', 0.0): 5, ('repost', 0.0): 2, ('333', 0.0): 3, ('magpi', 0.0): 1, ('22.05', 0.0): 1, ('15-24', 0.0): 1, ('05.15', 0.0): 1, ('coach', 0.0): 2, ('ador', 0.0): 1, ('chswiyfxcskcalum', 0.0): 1, ('nvm', 0.0): 2, ('lemm', 0.0): 1, ('quiet', 0.0): 3, ('foof', 0.0): 1, ('332', 0.0): 1, ('casilla', 0.0): 1, ('manchest', 0.0): 3, ('xi', 0.0): 1, ('rmtour', 0.0): 1, ('heavi', 0.0): 3, ('irl', 0.0): 2, ('blooper', 0.0): 2, ('huhuhuhu', 0.0): 1, ('na-tak', 0.0): 1, ('sorta', 0.0): 1, ('unfriend', 0.0): 1, ('greysonch', 0.0): 1, ('sandwich', 0.0): 4, ('bell', 0.0): 1, ('sebastian', 0.0): 1, ('rewatch', 0.0): 1, ('s4', 0.0): 1, ('ser', 0.0): 1, ('past', 0.0): 5, ('heart-break', 0.0): 1, ('outdat', 0.0): 1, ('m4', 0.0): 1, ('abandon', 0.0): 1, ('theater', 0.0): 1, ('smh', 0.0): 6, ('7-3', 0.0): 1, ('7.30-', 0.0): 1, ('ekk', 0.0): 1, ('giriboy', 0.0): 1, ('harriet', 0.0): 1, ('gegu', 0.0): 1, ('gray', 0.0): 1, ('truth', 0.0): 4, ('tbt', 0.0): 1, ('331', 0.0): 1, ('roof', 0.0): 2, ('indian', 0.0): 2, ('polit', 0.0): 3, ('blame', 0.0): 3, ('68', 0.0): 1, ('repres', 0.0): 1, ('corbyn', 0.0): 1, (\"labour'\", 0.0): 1, ('fortun', 0.0): 1, ('icecream', 0.0): 3, ('cuti', 0.0): 2, ('ry', 0.0): 1, ('lfccw', 0.0): 1, ('5ever', 0.0): 1, ('america', 0.0): 3, ('ontheroadagain', 0.0): 1, ('halaaang', 0.0): 1, ('reciev', 0.0): 1, ('flip', 0.0): 4, ('flop', 0.0): 1, ('caesarspalac', 0.0): 1, ('socialreward', 0.0): 1, ('requir', 0.0): 2, ('cali', 0.0): 1, ('fuckboy', 0.0): 1, ('330', 0.0): 1, ('deliveri', 0.0): 3, ('chrompet', 0.0): 1, ('easili', 0.0): 2, ('immun', 0.0): 1, ('system', 0.0): 3, ('lush', 0.0): 1, ('bathtub', 0.0): 1, ('php', 0.0): 1, ('mysql', 0.0): 1, ('libmysqlclient-dev', 0.0): 1, ('dev', 0.0): 2, ('pleasanton', 0.0): 1, ('wala', 0.0): 1, ('329', 0.0): 1, ('quickli', 0.0): 2, ('megan', 0.0): 1, ('heed', 0.0): 2, ('328', 0.0): 1, ('gwss', 0.0): 1, ('thankyouu', 0.0): 1, ('charad', 0.0): 1, ('becom', 0.0): 5, ('piano', 0.0): 2, ('327', 0.0): 1, ('complaint', 0.0): 2, ('yell', 0.0): 2, ('whatsoev', 0.0): 2, ('pete', 0.0): 1, ('wentz', 0.0): 1, ('shogi', 0.0): 1, ('blameshoghicp', 0.0): 1, ('classmat', 0.0): 1, ('troubl', 0.0): 1, ('fixedgearfrenzi', 0.0): 1, ('dispatch', 0.0): 1, ('theyr', 0.0): 2, ('hat', 0.0): 2, (\"shamuon'\", 0.0): 1, ('tokyo', 0.0): 1, ('toe', 0.0): 2, ('horrend', 0.0): 2, (\"someone'\", 0.0): 2, ('326', 0.0): 1, ('hasb', 0.0): 1, ('atti', 0.0): 1, ('muji', 0.0): 1, ('sirf', 0.0): 1, ('sensibl', 0.0): 1, ('etc', 0.0): 2, ('brum', 0.0): 1, ('cyclerevolut', 0.0): 1, ('caaannnttt', 0.0): 1, ('payment', 0.0): 3, ('overdrawn', 0.0): 1, ('tbf', 0.0): 1, ('complain', 0.0): 2, ('perfum', 0.0): 1, ('sampl', 0.0): 1, ('chanel', 0.0): 1, ('burberri', 0.0): 1, ('prada', 0.0): 1, ('325', 0.0): 1, ('noesss', 0.0): 1, ('topgear', 0.0): 1, ('worthi', 0.0): 1, ('bridesmaid', 0.0): 1, (\"tomorrow'\", 0.0): 2, ('gather', 0.0): 1, ('sudden', 0.0): 4, ('324', 0.0): 1, ('randomrestart', 0.0): 1, ('randomreboot', 0.0): 1, ('lumia', 0.0): 1, ('windowsphon', 0.0): 1, (\"microsoft'\", 0.0): 1, ('ma√±ana', 0.0): 1, ('male', 0.0): 1, ('rap', 0.0): 1, ('sponsor', 0.0): 3, ('striker', 0.0): 2, ('lvg', 0.0): 1, ('behind', 0.0): 3, ('refurbish', 0.0): 1, ('cintiq', 0.0): 1, (\"finnick'\", 0.0): 1, ('askfinnick', 0.0): 1, ('contain', 0.0): 1, ('hairi', 0.0): 1, ('323', 0.0): 1, ('buri', 0.0): 1, ('omaygad', 0.0): 1, ('vic', 0.0): 1, ('surgeri', 0.0): 4, ('amber', 0.0): 8, ('tt.tt', 0.0): 1, ('hyper', 0.0): 2, ('vega', 0.0): 2, ('322', 0.0): 1, ('imiss', 0.0): 1, ('321', 0.0): 1, ('320', 0.0): 1, ('know.for', 0.0): 1, ('prepaid', 0.0): 1, ('none', 0.0): 4, ('319', 0.0): 1, ('grandma', 0.0): 1, (\"grandpa'\", 0.0): 1, ('farm', 0.0): 1, ('cow', 0.0): 1, ('sheep', 0.0): 1, ('hors', 0.0): 3, ('fruit', 0.0): 2, ('veget', 0.0): 1, ('puke', 0.0): 2, ('deliri', 0.0): 1, ('motilium', 0.0): 1, ('shite', 0.0): 1, ('318', 0.0): 1, ('schoolwork', 0.0): 1, (\"phoebe'\", 0.0): 1, ('317', 0.0): 1, ('pothol', 0.0): 1, ('316', 0.0): 1, ('notif', 0.0): 3, ('1,300', 0.0): 1, ('robyn', 0.0): 1, ('necklac', 0.0): 1, ('rachel', 0.0): 1, ('bhai', 0.0): 1, ('ramzan', 0.0): 1, ('crosss', 0.0): 1, ('clapham', 0.0): 1, ('investig', 0.0): 2, ('sth', 0.0): 1, ('essenti', 0.0): 1, ('photoshooot', 0.0): 1, ('austin', 0.0): 1, ('mahon', 0.0): 1, ('shut', 0.0): 3, ('andam', 0.0): 1, ('memor', 0.0): 1, ('cotton', 0.0): 1, ('candi', 0.0): 3, ('stock', 0.0): 3, ('swallow', 0.0): 1, ('snot', 0.0): 1, ('choke', 0.0): 1, ('taknottem', 0.0): 1, ('477', 0.0): 1, ('btob', 0.0): 2, ('percentag', 0.0): 1, ('shoshannavassil', 0.0): 1, ('swift', 0.0): 1, ('flat', 0.0): 3, ('a9', 0.0): 2, ('wsalelov', 0.0): 5, ('sexyjan', 0.0): 1, ('horni', 0.0): 2, ('goodmus', 0.0): 4, ('debut', 0.0): 3, ('lart', 0.0): 1, ('sew', 0.0): 1, ('skyfal', 0.0): 1, ('premier', 0.0): 1, ('yummi', 0.0): 2, ('manteca', 0.0): 1, (\"she'd\", 0.0): 2, ('probabl', 0.0): 8, ('shiatsu', 0.0): 1, ('heat', 0.0): 1, ('risk', 0.0): 3, ('edward', 0.0): 1, ('hopper', 0.0): 1, ('eyyah', 0.0): 1, ('utd', 0.0): 2, ('born', 0.0): 1, ('1-0', 0.0): 1, ('cart', 0.0): 1, ('shop', 0.0): 10, ('log', 0.0): 2, ('aaa', 0.0): 2, ('waifu', 0.0): 1, ('break', 0.0): 8, ('breakup', 0.0): 3, ('bother', 0.0): 3, ('bia', 0.0): 1, ('syndrom', 0.0): 1, ('shi', 0.0): 1, ('bias', 0.0): 1, ('pixel', 0.0): 2, ('weh', 0.0): 2, ('area', 0.0): 4, ('maymay', 0.0): 1, ('magpaalam', 0.0): 1, ('tf', 0.0): 3, ('subtitl', 0.0): 1, ('oitnb', 0.0): 1, ('backstori', 0.0): 1, ('jeremi', 0.0): 1, ('kyle', 0.0): 1, ('gimm', 0.0): 2, ('meal', 0.0): 3, ('neat-o', 0.0): 1, ('wru', 0.0): 1, ('scissor', 0.0): 1, ('creation', 0.0): 1, ('public', 0.0): 1, ('amtir', 0.0): 1, ('imysm', 0.0): 2, ('tut', 0.0): 1, ('trop', 0.0): 2, ('tard', 0.0): 1, ('deadlin', 0.0): 1, ('31', 0.0): 2, ('st', 0.0): 3, ('child', 0.0): 4, ('oct', 0.0): 2, ('bush', 0.0): 2, ('premiun', 0.0): 1, ('notcool', 0.0): 1, ('2/3', 0.0): 2, ('lahat', 0.0): 2, ('ng', 0.0): 4, ('araw', 0.0): 1, ('nage', 0.0): 1, ('gyu', 0.0): 4, ('lmfaooo', 0.0): 2, ('download', 0.0): 3, ('leagu', 0.0): 1, ('mashup', 0.0): 1, ('eu', 0.0): 1, ('lc', 0.0): 1, ('typo', 0.0): 2, ('itali', 0.0): 1, ('yass', 0.0): 1, ('christma', 0.0): 2, ('rel', 0.0): 1, ('yr', 0.0): 3, ('sydney', 0.0): 1, ('mb', 0.0): 1, ('perf', 0.0): 2, ('programm', 0.0): 1, ('bff', 0.0): 2, ('hashtag', 0.0): 1, ('omfg', 0.0): 4, ('exercis', 0.0): 2, ('combat', 0.0): 1, ('dosent', 0.0): 1, (\"sod'\", 0.0): 1, ('20min', 0.0): 1, ('request', 0.0): 2, ('yahoo', 0.0): 2, ('yodel', 0.0): 2, ('jokingli', 0.0): 1, ('regret', 0.0): 5, ('starbuck', 0.0): 3, ('lynettelow', 0.0): 1, ('interraci', 0.0): 3, (\"today'\", 0.0): 3, ('tgif', 0.0): 1, ('gahd', 0.0): 1, ('26th', 0.0): 1, ('discov', 0.0): 1, ('12.00', 0.0): 1, ('obyun', 0.0): 1, ('unni', 0.0): 4, ('wayhh', 0.0): 1, ('preval', 0.0): 1, ('controversi', 0.0): 1, ('üçµ', 0.0): 2, ('‚òï', 0.0): 1, ('tube', 0.0): 1, ('strike', 0.0): 3, ('meck', 0.0): 1, ('mcfc', 0.0): 1, ('fresh', 0.0): 1, ('ucan', 0.0): 1, ('anxiou', 0.0): 1, ('poc', 0.0): 1, ('specif', 0.0): 2, ('sinhala', 0.0): 1, ('billionair', 0.0): 1, ('1645', 0.0): 1, ('island', 0.0): 3, ('1190', 0.0): 1, ('maldiv', 0.0): 1, ('dheena', 0.0): 1, ('fasgadah', 0.0): 1, ('alvadhaau', 0.0): 1, ('countdown', 0.0): 1, ('function', 0.0): 3, ('desktop', 0.0): 1, ('evelineconrad', 0.0): 1, ('facetim', 0.0): 4, ('kikmsn', 0.0): 2, ('selfshot', 0.0): 2, ('panda', 0.0): 1, ('backkk', 0.0): 1, ('transfer', 0.0): 3, ('dan', 0.0): 2, ('dull', 0.0): 1, ('overcast', 0.0): 1, ('folder', 0.0): 1, ('truck', 0.0): 2, ('missin', 0.0): 2, ('hangin', 0.0): 1, ('wiff', 0.0): 1, ('dept', 0.0): 1, ('cherri', 0.0): 1, ('bakewel', 0.0): 1, ('collect', 0.0): 3, ('teal', 0.0): 1, ('sect', 0.0): 1, ('tennunb', 0.0): 1, ('rather', 0.0): 4, ('skip', 0.0): 1, ('doomsday', 0.0): 1, ('neglect', 0.0): 1, ('posti', 0.0): 1, ('goodnight', 0.0): 1, ('donat', 0.0): 3, ('ship', 0.0): 6, ('bellami', 0.0): 1, ('raven', 0.0): 2, ('clark', 0.0): 1, ('helmi', 0.0): 1, ('uh', 0.0): 5, ('cnt', 0.0): 1, ('whereisthesun', 0.0): 1, ('summerismiss', 0.0): 1, ('longgg', 0.0): 1, ('ridicul', 0.0): 4, ('stocko', 0.0): 1, ('lucozad', 0.0): 1, ('explos', 0.0): 1, ('beh', 0.0): 2, ('half-rememb', 0.0): 1, (\"melody'\", 0.0): 1, ('recal', 0.0): 2, ('level', 0.0): 3, ('target', 0.0): 1, ('difficult', 0.0): 4, ('mile', 0.0): 1, ('pfb', 0.0): 1, ('nate', 0.0): 2, ('expo', 0.0): 2, ('jisoo', 0.0): 1, ('chloe', 0.0): 2, ('anon', 0.0): 2, ('mager', 0.0): 1, ('wi', 0.0): 1, ('knw', 0.0): 1, ('wht', 0.0): 1, ('distant', 0.0): 1, ('buffer', 0.0): 2, ('insan', 0.0): 1, ('charli', 0.0): 1, ('finland', 0.0): 3, ('gana', 0.0): 1, ('studio', 0.0): 3, ('arch', 0.0): 1, ('lyin', 0.0): 1, ('kian', 0.0): 3, ('supercar', 0.0): 1, ('gurgaon', 0.0): 1, ('locat', 0.0): 7, ('9:15', 0.0): 1, ('satir', 0.0): 1, ('gener', 0.0): 2, ('peanut', 0.0): 3, ('butter', 0.0): 1, ('garden', 0.0): 2, ('beer', 0.0): 1, ('viner', 0.0): 1, ('palembang', 0.0): 1, ('sorrryyi', 0.0): 1, ('fani', 0.0): 1, ('hahahahaha', 0.0): 2, ('boner', 0.0): 1, ('merci', 0.0): 1, ('yuki', 0.0): 1, ('2500k', 0.0): 1, ('mari', 0.0): 1, ('jake', 0.0): 1, ('gyllenha', 0.0): 1, ('impact', 0.0): 1, (\"ledger'\", 0.0): 1, ('btw', 0.0): 5, ('cough', 0.0): 4, ('hunni', 0.0): 1, ('b4', 0.0): 1, ('deplet', 0.0): 1, ('mbasa', 0.0): 1, ('client', 0.0): 3, ('ray', 0.0): 1, ('aah', 0.0): 1, ('type', 0.0): 2, ('suit', 0.0): 5, ('pa-copi', 0.0): 1, ('proper', 0.0): 2, ('biom', 0.0): 1, ('mosqu', 0.0): 1, ('smelli', 0.0): 1, ('taxi', 0.0): 4, ('emptier', 0.0): 1, (\"ciara'\", 0.0): 1, (\"everything'\", 0.0): 1, ('clip', 0.0): 2, ('tall', 0.0): 2, ('gladli', 0.0): 1, ('intent', 0.0): 1, ('amb', 0.0): 1, (\"harry'\", 0.0): 2, ('jean', 0.0): 2, ('mayday', 0.0): 1, ('parad', 0.0): 2, ('lyf', 0.0): 1, ('13th', 0.0): 1, ('anim', 0.0): 4, ('kingdom', 0.0): 1, ('chri', 0.0): 7, ('brown', 0.0): 4, ('riski', 0.0): 1, ('cologn', 0.0): 1, ('duo', 0.0): 3, ('ballad', 0.0): 2, ('bish', 0.0): 2, ('intern', 0.0): 2, ('brought', 0.0): 1, ('yumyum', 0.0): 1, (\"cathy'\", 0.0): 1, ('missyou', 0.0): 1, ('rubi', 0.0): 2, ('rose', 0.0): 2, ('tou', 0.0): 1, ('main', 0.0): 1, ('pora', 0.0): 1, ('stalk', 0.0): 3, ('karlia', 0.0): 1, ('khatam', 0.0): 2, ('bandi', 0.0): 1, ('üëë', 0.0): 1, ('pyaari', 0.0): 1, ('gawd', 0.0): 1, ('understood', 0.0): 1, ('review', 0.0): 3, ('massi', 0.0): 1, ('thatselfiethough', 0.0): 1, ('loop', 0.0): 1, ('ofc', 0.0): 1, ('pict', 0.0): 1, ('caught', 0.0): 1, ('aishhh', 0.0): 1, ('viewer', 0.0): 1, ('exam', 0.0): 5, ('sighsss', 0.0): 1, ('burnt', 0.0): 2, ('toffe', 0.0): 2, ('honesti', 0.0): 1, ('cheatday', 0.0): 1, ('protein', 0.0): 1, ('sissi', 0.0): 1, ('tote', 0.0): 1, ('slowli', 0.0): 1, ('church', 0.0): 2, ('pll', 0.0): 1, ('sel', 0.0): 1, ('beth', 0.0): 2, ('serbia', 0.0): 1, ('serbian', 0.0): 1, ('selen', 0.0): 1, ('motav', 0.0): 1, ('üíã', 0.0): 2, ('zayyyn', 0.0): 1, ('momma', 0.0): 1, ('happend', 0.0): 1, ('imper', 0.0): 1, ('trmdhesit', 0.0): 1, ('pana', 0.0): 1, ('quickest', 0.0): 2, ('blood', 0.0): 5, ('sake', 0.0): 1, ('hamstr', 0.0): 1, ('rodwel', 0.0): 1, ('trace', 0.0): 1, ('artist', 0.0): 4, ('tp', 0.0): 1, ('powder', 0.0): 1, ('wider', 0.0): 1, ('honestli', 0.0): 4, ('comfort', 0.0): 3, ('bruno', 0.0): 1, ('1.8', 0.0): 1, ('ed', 0.0): 7, ('croke', 0.0): 2, ('deal', 0.0): 6, ('toll', 0.0): 1, ('packag', 0.0): 1, ('shape', 0.0): 1, ('unluckiest', 0.0): 1, ('bettor', 0.0): 1, ('nstp', 0.0): 1, ('sem', 0.0): 2, ('chipotl', 0.0): 1, ('chick-fil-a', 0.0): 1, ('stole', 0.0): 3, ('evet', 0.0): 1, ('ramadhan', 0.0): 1, ('eid', 0.0): 4, ('stexpert', 0.0): 1, ('ripstegi', 0.0): 1, ('nickyyi', 0.0): 1, ('¬ø', 0.0): 1, ('centralis', 0.0): 1, ('discontinu', 0.0): 1, ('sniff', 0.0): 1, (\"i't\", 0.0): 1, ('glad', 0.0): 2, ('fab', 0.0): 2, ('theres', 0.0): 1, ('cred', 0.0): 1, ('t_t', 0.0): 1, ('elimin', 0.0): 1, ('teamzip', 0.0): 1, ('smtm', 0.0): 1, ('assingn', 0.0): 1, ('editi', 0.0): 1, ('nakaka', 0.0): 1, ('beastmod', 0.0): 1, ('gaaawd', 0.0): 1, ('jane', 0.0): 1, ('mango', 0.0): 1, ('colombia', 0.0): 1, ('yot', 0.0): 1, ('labyo', 0.0): 1, ('pano', 0.0): 1, ('nalamannn', 0.0): 1, ('hardhead', 0.0): 1, ('cell', 0.0): 1, (\"zach'\", 0.0): 1, ('burger', 0.0): 2, ('xpress', 0.0): 1, ('hopkin', 0.0): 1, ('melatonin', 0.0): 1, ('2-4', 0.0): 1, ('nap', 0.0): 2, ('wide', 0.0): 2, ('task', 0.0): 1, ('9pm', 0.0): 1, ('hahaah', 0.0): 1, ('frequent', 0.0): 1, ('jail', 0.0): 2, ('weirddd', 0.0): 1, ('donghyuk', 0.0): 1, ('stan', 0.0): 1, ('bek', 0.0): 1, ('13', 0.0): 4, ('reynoldsgrl', 0.0): 1, ('ole', 0.0): 1, ('beardi', 0.0): 1, ('kaussi', 0.0): 1, ('bummer', 0.0): 3, ('fightingmciren', 0.0): 1, (\"michael'\", 0.0): 1, ('ÔøΩ', 0.0): 21, ('miser', 0.0): 2, ('üí¶', 0.0): 1, ('yoga', 0.0): 2, ('üåû', 0.0): 1, ('üíÉüèΩ', 0.0): 1, ('shouldv', 0.0): 1, ('saffron', 0.0): 1, ('peasant', 0.0): 1, ('wouldv', 0.0): 1, ('nfinit', 0.0): 1, ('admin_myung', 0.0): 1, ('slp', 0.0): 1, ('saddest', 0.0): 2, ('laomma', 0.0): 2, ('kebaya', 0.0): 1, ('bandung', 0.0): 1, ('indonesia', 0.0): 1, ('7df89150', 0.0): 1, ('whatsapp', 0.0): 2, ('62', 0.0): 1, ('08962464174', 0.0): 1, ('laomma_coutur', 0.0): 1, ('haizzz', 0.0): 1, ('urghhh', 0.0): 1, ('working-on-a-tight-schedul', 0.0): 1, ('ganbarimasu', 0.0): 1, ('livid', 0.0): 1, ('whammi', 0.0): 1, ('quuuee', 0.0): 1, ('friooo', 0.0): 1, ('ladi', 0.0): 4, ('stereo', 0.0): 1, ('chwang', 0.0): 1, ('lorm', 0.0): 1, ('823', 0.0): 1, ('rp', 0.0): 1, ('indiemus', 0.0): 10, ('unhappi', 0.0): 2, ('jennyjean', 0.0): 1, ('elfindelmundo', 0.0): 2, ('lolzz', 0.0): 1, ('dat', 0.0): 4, ('corey', 0.0): 1, ('appreci', 0.0): 2, ('weekli', 0.0): 2, ('mahirap', 0.0): 1, ('nash', 0.0): 1, ('gosh', 0.0): 6, ('noodl', 0.0): 1, ('veeerri', 0.0): 1, ('rted', 0.0): 2, ('orig', 0.0): 1, ('starholicxx', 0.0): 1, ('07:17', 0.0): 2, ('@the', 0.0): 1, ('notr', 0.0): 1, ('hwi', 0.0): 1, ('niall', 0.0): 5, ('fraud', 0.0): 1, ('diplomaci', 0.0): 1, ('fittest', 0.0): 1, ('zero', 0.0): 1, ('toler', 0.0): 2, ('gurl', 0.0): 1, ('notion', 0.0): 1, ('pier', 0.0): 1, ('approach', 0.0): 1, ('rattl', 0.0): 1, ('robe', 0.0): 1, ('emphasi', 0.0): 1, ('vocal', 0.0): 1, ('chose', 0.0): 1, ('erm', 0.0): 1, ('abby.can', 0.0): 1, ('persuad', 0.0): 1, ('lyric', 0.0): 1, (\"emily'\", 0.0): 1, ('odd', 0.0): 3, ('possibl', 0.0): 8, ('elect', 0.0): 2, ('kamiss', 0.0): 1, ('mwa', 0.0): 1, ('mommi', 0.0): 3, ('scream', 0.0): 1, ('fight', 0.0): 2, ('cafe', 0.0): 2, ('melbourn', 0.0): 1, ('anyonnee', 0.0): 1, ('loner', 0.0): 1, ('fricken', 0.0): 2, ('rito', 0.0): 1, ('friendzon', 0.0): 1, ('panel', 0.0): 1, ('repeat', 0.0): 2, ('audienc', 0.0): 1, ('hsm', 0.0): 1, ('canario', 0.0): 1, ('hotel', 0.0): 8, ('ukiss', 0.0): 1, ('faith', 0.0): 2, ('kurt', 0.0): 1, (\"fatma'm\", 0.0): 1, ('alex', 0.0): 4, ('swag', 0.0): 1, ('lmfao', 0.0): 2, ('flapjack', 0.0): 1, ('countthecost', 0.0): 1, ('ihop', 0.0): 1, ('infra', 0.0): 1, ('lq', 0.0): 1, ('knive', 0.0): 1, ('sotir', 0.0): 1, ('mybrainneedstoshutoff', 0.0): 1, ('macci', 0.0): 1, ('chees', 0.0): 7, ('25', 0.0): 2, ('tend', 0.0): 1, ('510', 0.0): 1, ('silicon', 0.0): 1, ('cover', 0.0): 2, ('kbye', 0.0): 1, ('ini', 0.0): 1, ('anytim', 0.0): 1, ('citizen', 0.0): 1, ('compar', 0.0): 2, ('rank', 0.0): 1, ('mcountdown', 0.0): 2, ('5h', 0.0): 1, ('thapelo', 0.0): 1, ('op', 0.0): 1, ('civ', 0.0): 1, ('wooden', 0.0): 1, ('mic', 0.0): 1, ('embarrass', 0.0): 2, ('translat', 0.0): 3, ('daili', 0.0): 3, ('mecha-totem', 0.0): 1, ('nak', 0.0): 1, ('tgk', 0.0): 1, ('townsss', 0.0): 1, ('jokid', 0.0): 1, ('rent', 0.0): 2, ('degre', 0.0): 1, ('inconsider', 0.0): 2, ('softbal', 0.0): 1, ('appli', 0.0): 1, ('tomcat', 0.0): 1, ('chel', 0.0): 1, ('jemma', 0.0): 1, ('detail', 0.0): 4, ('list', 0.0): 4, ('matchi', 0.0): 2, ('elsa', 0.0): 1, ('postpon', 0.0): 1, ('karin', 0.0): 1, ('honey', 0.0): 2, ('vist', 0.0): 1, ('unhealthi', 0.0): 1, ('propa', 0.0): 1, ('knockin', 0.0): 1, ('bacon', 0.0): 1, ('market', 0.0): 2, ('pre-holiday', 0.0): 1, ('diet', 0.0): 1, ('meani', 0.0): 1, ('deathbybaconsmel', 0.0): 1, ('init', 0.0): 2, ('destin', 0.0): 1, ('victoria', 0.0): 2, ('luna', 0.0): 1, ('krystal', 0.0): 1, ('sarajevo', 0.0): 1, ('haix', 0.0): 2, ('sp', 0.0): 1, ('student', 0.0): 4, ('wii', 0.0): 2, ('bayonetta', 0.0): 1, ('101', 0.0): 1, ('doabl', 0.0): 1, ('drove', 0.0): 1, ('agenc', 0.0): 1, ('story.miss', 0.0): 1, ('everon', 0.0): 1, ('jp', 0.0): 1, ('mamabear', 0.0): 1, ('imintoh', 0.0): 1, ('underr', 0.0): 1, (\"slovakia'\", 0.0): 1, ('d:', 0.0): 6, ('saklap', 0.0): 1, ('grade', 0.0): 2, ('rizal', 0.0): 1, ('lib', 0.0): 1, ('discuss', 0.0): 1, ('advisori', 0.0): 1, ('period', 0.0): 2, ('dit', 0.0): 1, ('du', 0.0): 1, ('harsh', 0.0): 2, ('ohgod', 0.0): 1, ('abligaverin', 0.0): 2, ('photooftheday', 0.0): 2, ('sexygirlbypreciouslemmi', 0.0): 3, ('ripsandrabland', 0.0): 1, ('edel', 0.0): 1, ('salam', 0.0): 1, ('mubark', 0.0): 1, ('dong', 0.0): 3, ('tammirossm', 0.0): 4, ('speck', 0.0): 1, ('abbymil', 0.0): 2, ('18', 0.0): 8, ('ion', 0.0): 1, ('5min', 0.0): 1, ('hse', 0.0): 1, ('noob', 0.0): 1, ('nxt', 0.0): 1, ('2week', 0.0): 1, ('300', 0.0): 3, ('fck', 0.0): 2, ('nae', 0.0): 2, ('deep', 0.0): 3, ('human', 0.0): 3, ('whit', 0.0): 1, ('van', 0.0): 4, ('bristol', 0.0): 1, ('subserv', 0.0): 1, ('si', 0.0): 4, ('oo', 0.0): 1, ('tub', 0.0): 1, ('penyfan', 0.0): 1, ('forecast', 0.0): 2, ('breconbeacon', 0.0): 1, ('tittheir', 0.0): 1, ('42', 0.0): 1, ('hotti', 0.0): 3, ('uu', 0.0): 2, ('rough', 0.0): 1, ('fuzzi', 0.0): 1, ('san', 0.0): 3, ('antonio', 0.0): 1, ('kang', 0.0): 1, ('junhe', 0.0): 1, ('couldv', 0.0): 1, ('pz', 0.0): 1, ('somerset', 0.0): 1, ('given', 0.0): 2, ('sunburnt', 0.0): 1, ('safer', 0.0): 1, ('k3g', 0.0): 1, ('input', 0.0): 1, ('gamestomp', 0.0): 1, ('desc', 0.0): 1, (\"angelo'\", 0.0): 1, ('yna', 0.0): 1, ('psygustokita', 0.0): 2, ('fiver', 0.0): 1, ('toward', 0.0): 1, ('sakho', 0.0): 1, ('threat', 0.0): 1, ('goalscor', 0.0): 1, ('10:59', 0.0): 1, ('11.00', 0.0): 1, ('sham', 0.0): 1, ('tricki', 0.0): 1, ('baao', 0.0): 1, ('nisrina', 0.0): 1, ('crazi', 0.0): 8, ('ladygaga', 0.0): 1, (\"you'\", 0.0): 2, ('pari', 0.0): 2, ('marrish', 0.0): 1, (\"otp'\", 0.0): 1, ('6:15', 0.0): 1, ('edomnt', 0.0): 1, ('qih', 0.0): 1, ('shxb', 0.0): 1, ('1000', 0.0): 1, ('chilton', 0.0): 1, ('mother', 0.0): 2, ('obsess', 0.0): 1, ('creepi', 0.0): 2, ('josh', 0.0): 1, ('boohoo', 0.0): 1, ('fellow', 0.0): 2, ('tweep', 0.0): 1, ('roar', 0.0): 1, ('victori', 0.0): 1, ('tweepsmatchout', 0.0): 1, ('nein', 0.0): 3, ('404', 0.0): 1, ('midnight', 0.0): 2, ('willlow', 0.0): 1, ('hbd', 0.0): 1, ('sowwi', 0.0): 1, ('3000', 0.0): 1, ('grind', 0.0): 1, ('gear', 0.0): 1, ('0.001', 0.0): 1, ('meant', 0.0): 6, ('portrait', 0.0): 1, ('mode', 0.0): 2, ('fact', 0.0): 4, ('11:11', 0.0): 4, ('shanzay', 0.0): 1, ('salabrati', 0.0): 1, ('journo', 0.0): 1, ('lure', 0.0): 1, ('gang', 0.0): 1, ('twist', 0.0): 1, ('mashaket', 0.0): 1, ('pet', 0.0): 2, ('bapak', 0.0): 1, ('royal', 0.0): 2, ('prima', 0.0): 1, ('mune', 0.0): 1, ('874', 0.0): 1, ('plisss', 0.0): 1, ('elf', 0.0): 1, ('teenchoic', 0.0): 5, ('choiceinternationalartist', 0.0): 5, ('superjunior', 0.0): 5, (\"he'll\", 0.0): 1, ('sunway', 0.0): 1, ('petal', 0.0): 1, ('jaya', 0.0): 1, ('selangor', 0.0): 1, ('glow', 0.0): 1, ('huhuu', 0.0): 1, ('congratul', 0.0): 2, ('margo', 0.0): 1, ('konga', 0.0): 1, ('ni', 0.0): 4, ('wa', 0.0): 2, ('ode', 0.0): 1, ('disvirgin', 0.0): 1, ('bride', 0.0): 3, ('yulin', 0.0): 1, ('meat', 0.0): 1, ('festiv', 0.0): 2, ('imma', 0.0): 2, ('syawal', 0.0): 1, ('lapar', 0.0): 1, ('foundat', 0.0): 1, ('clash', 0.0): 2, ('facil', 0.0): 1, ('dh', 0.0): 2, ('chalet', 0.0): 1, ('suay', 0.0): 1, ('anot', 0.0): 1, ('bugger', 0.0): 1, ('‡§è‡§ï', 0.0): 1, ('‡§¨‡§æ‡§∞', 0.0): 1, ('‡§´‡§ø‡§∞', 0.0): 1, ('‡§∏‡•á‡§Å', 0.0): 1, ('‡§ß‡•ã‡§ñ‡§æ', 0.0): 1, ('chandauli', 0.0): 1, ('majhwar', 0.0): 1, ('railway', 0.0): 1, ('tito', 0.0): 2, ('tita', 0.0): 1, ('cousin', 0.0): 3, ('critic', 0.0): 1, ('condit', 0.0): 1, ('steal', 0.0): 1, ('narco', 0.0): 1, ('regen', 0.0): 1, ('unfav', 0.0): 2, ('benadryl', 0.0): 1, ('offlin', 0.0): 1, ('arent', 0.0): 1, ('msg', 0.0): 1, ('yg', 0.0): 1, ('gg', 0.0): 3, ('sxrew', 0.0): 1, ('dissappear', 0.0): 1, ('swap', 0.0): 1, ('bleed', 0.0): 1, ('ishal', 0.0): 1, ('mi', 0.0): 2, ('thaank', 0.0): 1, ('jhezz', 0.0): 1, ('sneak', 0.0): 3, ('soft', 0.0): 1, ('defenc', 0.0): 1, ('defens', 0.0): 1, ('nrltigersroost', 0.0): 1, ('indiana', 0.0): 2, ('hibb', 0.0): 1, ('biblethump', 0.0): 1, ('rlyyi', 0.0): 1, ('septum', 0.0): 1, ('pierc', 0.0): 2, ('goood', 0.0): 1, ('hiya', 0.0): 1, ('fire', 0.0): 1, ('venom', 0.0): 1, ('carriag', 0.0): 1, ('pink', 0.0): 1, ('fur-trim', 0.0): 1, ('stetson', 0.0): 1, ('error', 0.0): 4, ('59', 0.0): 1, ('xue', 0.0): 1, ('midori', 0.0): 1, ('sakit', 0.0): 2, ('mateo', 0.0): 1, ('hawk', 0.0): 2, ('bartend', 0.0): 1, ('surf', 0.0): 1, ('despair', 0.0): 1, ('insta', 0.0): 1, ('promo', 0.0): 1, ('iwantin', 0.0): 1, ('___', 0.0): 2, ('fault', 0.0): 3, ('goodluck', 0.0): 1, ('pocket', 0.0): 1, ('help@veryhq.co.uk', 0.0): 1, ('benedictervent', 0.0): 1, ('content', 0.0): 1, ('221b', 0.0): 1, ('popcorn', 0.0): 3, ('joyc', 0.0): 1, ('ooop', 0.0): 1, ('spotifi', 0.0): 1, ('paalam', 0.0): 1, ('sazbal', 0.0): 1, ('incid', 0.0): 1, ('aaahh', 0.0): 1, ('gooo', 0.0): 1, (\"stomach'\", 0.0): 1, ('growl', 0.0): 1, ('beard', 0.0): 1, ('nooop', 0.0): 1, ('üéâ', 0.0): 3, ('ding', 0.0): 3, ('hundr', 0.0): 1, ('meg', 0.0): 1, (\"verity'\", 0.0): 1, ('rupert', 0.0): 1, ('amin', 0.0): 1, ('studi', 0.0): 2, ('pleaaas', 0.0): 1, ('üëÜüèª', 0.0): 2, ('woaah', 0.0): 1, ('solvo', 0.0): 1, ('twin', 0.0): 2, (\"friday'\", 0.0): 1, ('lego', 0.0): 1, ('barefoot', 0.0): 1, ('twelvyy', 0.0): 1, ('boaz', 0.0): 1, ('myhil', 0.0): 1, ('takeov', 0.0): 1, ('wba', 0.0): 1, (\"taeyeon'\", 0.0): 1, ('derp', 0.0): 1, ('pd', 0.0): 1, ('zoom', 0.0): 2, (\"sunny'\", 0.0): 1, ('besst', 0.0): 1, ('plagu', 0.0): 1, ('pit', 0.0): 1, ('rich', 0.0): 1, ('sight', 0.0): 1, ('frail', 0.0): 1, ('lotteri', 0.0): 1, ('ride', 0.0): 2, ('twurkin', 0.0): 1, ('razzist', 0.0): 1, ('tumblr', 0.0): 1, ('shek', 0.0): 1, ('609', 0.0): 1, ('mugshot', 0.0): 1, ('attend', 0.0): 3, ('plsss', 0.0): 4, ('taissa', 0.0): 1, ('farmiga', 0.0): 1, ('robert', 0.0): 1, ('qualiti', 0.0): 1, ('daniel', 0.0): 1, ('latest', 0.0): 3, ('softwar', 0.0): 1, ('restor', 0.0): 2, ('momo', 0.0): 2, ('pharma', 0.0): 1, ('immov', 0.0): 1, ('messi', 0.0): 1, ('ansh', 0.0): 1, ('f1', 0.0): 1, ('billion', 0.0): 1, ('rand', 0.0): 1, ('bein', 0.0): 1, ('tla', 0.0): 1, ('tweng', 0.0): 1, ('gene', 0.0): 1, ('up.com', 0.0): 1, ('counti', 0.0): 2, ('cooler', 0.0): 1, ('minhyuk', 0.0): 1, ('gold', 0.0): 2, ('1900', 0.0): 1, ('üò™', 0.0): 3, ('yu', 0.0): 1, ('hz', 0.0): 2, ('selena', 0.0): 2, ('emta', 0.0): 1, ('hatigii', 0.0): 1, ('b2aa', 0.0): 1, ('yayyy', 0.0): 1, ('anesthesia', 0.0): 1, ('penrith', 0.0): 1, ('emu', 0.0): 1, ('plain', 0.0): 1, ('staff', 0.0): 3, ('untouch', 0.0): 1, ('brienn', 0.0): 1, ('lsh', 0.0): 1, ('gunna', 0.0): 1, ('former', 0.0): 1, ('darn', 0.0): 1, ('allah', 0.0): 4, ('pakistan', 0.0): 2, ('juudiciari', 0.0): 1, (\"horton'\", 0.0): 1, ('dunkin', 0.0): 1, ('socialis', 0.0): 1, ('cara', 0.0): 1, (\"delevingne'\", 0.0): 1, ('fear', 0.0): 1, ('drug', 0.0): 1, ('lace', 0.0): 1, ('fank', 0.0): 1, ('takfaham', 0.0): 1, ('ufff', 0.0): 1, ('sr', 0.0): 2, ('dard', 0.0): 1, ('katekyn', 0.0): 1, ('ehh', 0.0): 1, ('yeahhh', 0.0): 2, ('hacharatt', 0.0): 1, ('niwll', 0.0): 1, ('defin', 0.0): 1, ('wit', 0.0): 2, ('goa', 0.0): 1, ('lini', 0.0): 1, ('kasi', 0.0): 3, ('rhd', 0.0): 1, ('1st', 0.0): 3, ('wae', 0.0): 1, ('subsid', 0.0): 1, ('20th', 0.0): 1, ('anniversari', 0.0): 1, ('youngja', 0.0): 1, ('harumph', 0.0): 1, ('soggi', 0.0): 1, ('weed', 0.0): 1, ('ireland', 0.0): 3, ('sakura', 0.0): 1, ('flavour', 0.0): 1, ('chokki', 0.0): 1, ('üå∏', 0.0): 1, ('unavail', 0.0): 2, ('richard', 0.0): 2, ('laptop', 0.0): 2, ('satya', 0.0): 1, ('aditya', 0.0): 1, ('üçú', 0.0): 3, ('vibrat', 0.0): 1, ('an', 0.0): 2, ('cu', 0.0): 1, ('dhaka', 0.0): 1, ('jam', 0.0): 1, ('shall', 0.0): 2, ('cornetto', 0.0): 3, ('noseble', 0.0): 1, ('nintendo', 0.0): 3, ('wew', 0.0): 1, ('ramo', 0.0): 1, ('ground', 0.0): 2, ('shawn', 0.0): 1, ('mend', 0.0): 1, ('l', 0.0): 2, ('dinghi', 0.0): 1, ('skye', 0.0): 1, ('store', 0.0): 3, ('descript', 0.0): 2, ('colleagu', 0.0): 2, ('gagal', 0.0): 2, ('txt', 0.0): 1, ('sim', 0.0): 1, ('nooot', 0.0): 1, ('notch', 0.0): 1, ('tht', 0.0): 2, ('starv', 0.0): 4, ('\\U000fe196', 0.0): 1, ('pyjama', 0.0): 1, ('swifti', 0.0): 1, ('sorna', 0.0): 1, ('lurgi', 0.0): 1, ('jim', 0.0): 2, ('6gb', 0.0): 1, ('fenestoscop', 0.0): 1, ('etienn', 0.0): 1, ('bandana', 0.0): 3, ('bigger', 0.0): 2, ('vagina', 0.0): 1, ('suriya', 0.0): 1, ('dangl', 0.0): 1, ('mjhe', 0.0): 2, ('aaj', 0.0): 1, ('tak', 0.0): 3, ('kisi', 0.0): 1, ('kiya', 0.0): 1, ('eyesight', 0.0): 1, ('25x30', 0.0): 1, ('aftenoon', 0.0): 1, ('booor', 0.0): 1, ('uuu', 0.0): 1, ('boyfriend', 0.0): 8, ('freebiefriday', 0.0): 1, ('garag', 0.0): 1, ('michael', 0.0): 1, ('obvious', 0.0): 1, ('denim', 0.0): 1, ('somebodi', 0.0): 1, ('ce', 0.0): 1, ('gw', 0.0): 1, ('anatomi', 0.0): 1, ('no1', 0.0): 1, (\"morisette'\", 0.0): 1, ('flash', 0.0): 1, ('non-trial', 0.0): 1, ('sayhernam', 0.0): 1, ('lootcrat', 0.0): 1, ('item', 0.0): 1, ('inca', 0.0): 1, ('trail', 0.0): 1, ('sandboard', 0.0): 1, ('derbi', 0.0): 1, ('coffe', 0.0): 1, ('unabl', 0.0): 3, ('signatur', 0.0): 1, ('dish', 0.0): 1, ('unfamiliar', 0.0): 1, ('kitchen', 0.0): 3, ('coldest', 0.0): 1, (\"old'\", 0.0): 1, ('14518344', 0.0): 1, ('61', 0.0): 1, ('thirdwheel', 0.0): 1, ('lovebird', 0.0): 1, ('nth', 0.0): 1, ('imo', 0.0): 1, ('familiar', 0.0): 1, ('@juliettemaughan', 0.0): 1, ('copi', 0.0): 1, ('sensiesha', 0.0): 1, ('eldest', 0.0): 1, ('netbal', 0.0): 1, ('üòü', 0.0): 1, ('keedz', 0.0): 1, ('taybigail', 0.0): 1, ('jordan', 0.0): 1, ('tournament', 0.0): 1, ('goin', 0.0): 1, ('ps4', 0.0): 3, ('kink', 0.0): 1, ('charger', 0.0): 1, ('streak', 0.0): 1, ('scorch', 0.0): 1, ('srski', 0.0): 1, ('tdc', 0.0): 1, ('egypt', 0.0): 1, ('in-sensit', 0.0): 1, ('cooper', 0.0): 3, ('invit', 0.0): 1, ('donna', 0.0): 1, ('thurston', 0.0): 1, ('collin', 0.0): 1, ('quietli', 0.0): 2, ('kennel', 0.0): 1, ('911', 0.0): 1, ('pluckersss', 0.0): 1, ('gion', 0.0): 1, ('886', 0.0): 1, ('nsfw', 0.0): 1, ('kidschoiceaward', 0.0): 1, ('ming', 0.0): 1, ('pbr', 0.0): 1, ('shoutout', 0.0): 1, ('periscop', 0.0): 1, ('ut', 0.0): 1, ('shawti', 0.0): 1, ('naw', 0.0): 4, (\"sterling'\", 0.0): 1, ('9muse', 0.0): 1, ('hrryok', 0.0): 2, ('asap', 0.0): 2, ('wnt', 0.0): 1, ('9:30', 0.0): 1, ('9:48', 0.0): 1, ('9/11', 0.0): 1, ('bueno', 0.0): 1, ('receptionist', 0.0): 1, ('ella', 0.0): 2, ('goe', 0.0): 4, ('ketchup', 0.0): 1, ('tasteless', 0.0): 1, ('deantd', 0.0): 1, ('justgotkanekifi', 0.0): 1, ('notgonnabeactivefor', 0.0): 1, ('2weeksdontmissittoomuch', 0.0): 1, ('2013', 0.0): 1, ('disney', 0.0): 2, ('vlog', 0.0): 1, ('swim', 0.0): 1, ('turtl', 0.0): 2, ('cnn', 0.0): 2, ('straplin', 0.0): 1, ('theatr', 0.0): 1, ('guncontrol', 0.0): 1, ('stung', 0.0): 2, ('tweak', 0.0): 1, (\"th√°t'\", 0.0): 1, ('powerpoint', 0.0): 1, ('present', 0.0): 5, ('diner', 0.0): 1, ('no-no', 0.0): 1, ('hind', 0.0): 1, ('circuit', 0.0): 1, ('secondari', 0.0): 1, ('sodder', 0.0): 1, ('perhap', 0.0): 2, ('mobitel', 0.0): 1, ('colin', 0.0): 1, ('playstat', 0.0): 2, ('charg', 0.0): 4, ('exp', 0.0): 1, ('misspelt', 0.0): 1, ('wan', 0.0): 1, ('hyungwon', 0.0): 2, ('alarm', 0.0): 1, ('needicecreamnow', 0.0): 1, ('shake', 0.0): 1, ('repeatedli', 0.0): 1, ('nu-uh', 0.0): 1, ('jace', 0.0): 1, ('mostest', 0.0): 1, ('vip', 0.0): 1, ('urgh', 0.0): 1, ('consol', 0.0): 1, (\"grigson'\", 0.0): 1, ('carrot', 0.0): 1, ('&gt;:-(', 0.0): 4, ('sunburn', 0.0): 1, ('ughh', 0.0): 2, ('enabl', 0.0): 1, ('otter', 0.0): 1, ('protect', 0.0): 1, ('argh', 0.0): 1, ('pon', 0.0): 1, ('otl', 0.0): 2, ('sleepov', 0.0): 2, ('jess', 0.0): 2, ('bebe', 0.0): 1, ('fabina', 0.0): 1, (\"barrista'\", 0.0): 1, ('plant', 0.0): 3, ('pup', 0.0): 2, ('brolli', 0.0): 1, ('mere', 0.0): 2, ('nhi', 0.0): 1, ('dey', 0.0): 2, ('serv', 0.0): 1, ('kepo', 0.0): 1, ('bitin', 0.0): 1, ('pretzel', 0.0): 1, ('bb17', 0.0): 1, ('bblf', 0.0): 1, ('fuckin', 0.0): 1, ('vanilla', 0.0): 1, ('latt', 0.0): 1, ('skulker', 0.0): 1, ('thread', 0.0): 1, ('hungrrryyi', 0.0): 1, ('icloud', 0.0): 1, ('ipod', 0.0): 3, ('hallyu', 0.0): 1, ('buuut', 0.0): 1, ('√ºber', 0.0): 1, ('oki', 0.0): 2, ('8p', 0.0): 1, ('champagn', 0.0): 1, ('harlo', 0.0): 1, ('torrentialrain', 0.0): 1, ('lloyd', 0.0): 1, ('asshol', 0.0): 1, ('clearli', 0.0): 2, ('knowww', 0.0): 2, ('runni', 0.0): 1, ('sehun', 0.0): 1, ('sweater', 0.0): 1, ('intoler', 0.0): 2, ('xenophob', 0.0): 1, ('wtfff', 0.0): 1, ('tone', 0.0): 1, ('wasnt', 0.0): 1, ('1pm', 0.0): 2, ('fantasi', 0.0): 1, ('newer', 0.0): 1, ('pish', 0.0): 1, ('comparison', 0.0): 1, ('remast', 0.0): 1, ('fe14', 0.0): 1, ('icon', 0.0): 2, ('strawberri', 0.0): 1, ('loos', 0.0): 1, ('kapatidkongpogi', 0.0): 1, ('steph', 0.0): 1, ('mel', 0.0): 1, ('longest', 0.0): 1, ('carmen', 0.0): 1, ('login', 0.0): 1, ('respons', 0.0): 3, ('00128835', 0.0): 1, ('wingstop', 0.0): 1, ('budg', 0.0): 1, ('fuq', 0.0): 1, ('ilhoon', 0.0): 1, ('ganteng', 0.0): 1, ('simpl', 0.0): 1, ('getthescoop', 0.0): 1, ('hearess', 0.0): 1, ('677', 0.0): 1, ('txt_shot', 0.0): 1, ('standbi', 0.0): 1, ('inatal', 0.0): 1, ('zenmat', 0.0): 1, ('namecheck', 0.0): 1, ('whistl', 0.0): 1, ('junmyeon', 0.0): 1, ('ddi', 0.0): 1, ('arini', 0.0): 1, ('je', 0.0): 1, ('bright', 0.0): 2, ('igbo', 0.0): 1, ('blamehoney', 0.0): 1, ('whhr', 0.0): 1, ('juan', 0.0): 1, ('snuggl', 0.0): 1, ('internship', 0.0): 1, ('usag', 0.0): 1, ('warn', 0.0): 1, ('vertigo', 0.0): 1, ('panic', 0.0): 1, ('attack', 0.0): 4, ('dual', 0.0): 1, ('carriageway', 0.0): 1, ('aragalang', 0.0): 1, ('08', 0.0): 1, ('tam', 0.0): 1, ('bose', 0.0): 1, ('theo', 0.0): 1, ('anymoree', 0.0): 1, ('rubbish', 0.0): 1, ('cactu', 0.0): 1, ('sorrri', 0.0): 1, ('bowel', 0.0): 1, ('nasti', 0.0): 2, ('tumour', 0.0): 1, ('faster', 0.0): 1, ('puffi', 0.0): 1, ('eyelid', 0.0): 1, ('musica', 0.0): 1, ('dota', 0.0): 1, ('4am', 0.0): 1, ('campsit', 0.0): 1, ('miah', 0.0): 1, ('hahay', 0.0): 1, ('churro', 0.0): 1, ('montana', 0.0): 2, ('reign', 0.0): 1, ('exampl', 0.0): 1, ('inflat', 0.0): 1, ('sic', 0.0): 1, ('reset', 0.0): 1, ('entlerbountli', 0.0): 1, ('tinder', 0.0): 3, ('dirtykik', 0.0): 2, ('sexcam', 0.0): 3, ('spray', 0.0): 1, ('industri', 0.0): 1, ('swollen', 0.0): 1, ('distanc', 0.0): 2, ('jojo', 0.0): 1, ('postcod', 0.0): 1, ('kafi', 0.0): 1, ('din', 0.0): 1, ('mene', 0.0): 1, ('aj', 0.0): 1, ('koi', 0.0): 1, ('rewert', 0.0): 1, ('bunta', 0.0): 1, ('warnaaa', 0.0): 1, ('tortur', 0.0): 2, ('field', 0.0): 1, ('wall', 0.0): 2, ('iran', 0.0): 1, ('irand', 0.0): 1, ('us-iran', 0.0): 1, ('nuclear', 0.0): 1, (\"mit'\", 0.0): 1, ('expert', 0.0): 1, ('sever', 0.0): 3, ('li', 0.0): 1, ('s2e12', 0.0): 1, ('rumpi', 0.0): 1, ('gallon', 0.0): 1, ('ryan', 0.0): 1, ('secret', 0.0): 2, ('dandia', 0.0): 1, ('rbi', 0.0): 1, ('cage', 0.0): 2, ('parrot', 0.0): 1, ('1li', 0.0): 1, ('commiss', 0.0): 1, ('cag', 0.0): 1, ('stripe', 0.0): 2, ('gujarat', 0.0): 1, ('tear', 0.0): 3, ('ily.melani', 0.0): 1, ('unlik', 0.0): 2, ('talent', 0.0): 2, ('deepxcap', 0.0): 1, ('doin', 0.0): 3, ('5:08', 0.0): 1, ('thesi', 0.0): 11, ('belieb', 0.0): 2, ('gtg', 0.0): 1, ('compet', 0.0): 1, ('vv', 0.0): 1, ('respect', 0.0): 5, ('opt-out', 0.0): 1, ('vam', 0.0): 1, ('spece', 0.0): 1, ('ell', 0.0): 1, ('articl', 0.0): 1, ('sexyameli', 0.0): 1, ('fineandyu', 0.0): 1, ('gd', 0.0): 1, ('flesh', 0.0): 1, ('daft', 0.0): 1, ('imsorri', 0.0): 1, ('aku', 0.0): 1, ('chelsea', 0.0): 2, ('koe', 0.0): 1, ('emyu', 0.0): 1, ('confetti', 0.0): 1, ('bf', 0.0): 2, ('sini', 0.0): 1, ('dipoppo', 0.0): 1, ('hop', 0.0): 2, ('bestweekend', 0.0): 1, ('okay-ish', 0.0): 1, ('html', 0.0): 1, ('geneva', 0.0): 1, ('patml', 0.0): 1, ('482', 0.0): 1, ('orgasm', 0.0): 3, ('abouti', 0.0): 1, ('797', 0.0): 1, ('reaalli', 0.0): 1, ('aldub', 0.0): 1, ('nila', 0.0): 1, ('smart', 0.0): 1, ('meter', 0.0): 1, ('display', 0.0): 1, ('unansw', 0.0): 1, ('bri', 0.0): 1, ('magcon', 0.0): 1, ('sinuend', 0.0): 1, ('kak', 0.0): 1, ('laper', 0.0): 2, ('rage', 0.0): 1, ('loser', 0.0): 1, ('brendon', 0.0): 1, (\"urie'\", 0.0): 1, ('sumer', 0.0): 1, ('repackag', 0.0): 1, (\":'d\", 0.0): 1, ('matthew', 0.0): 1, ('yongb', 0.0): 1, ('sued', 0.0): 1, ('suprem', 0.0): 1, ('warm-up', 0.0): 1, ('arriv', 0.0): 4, ('brill', 0.0): 1, ('120', 0.0): 1, ('rub', 0.0): 1, ('belli', 0.0): 1, ('jannatul', 0.0): 1, ('ferdou', 0.0): 1, ('ekta', 0.0): 1, ('kharap', 0.0): 1, ('manush', 0.0): 1, ('mart', 0.0): 2, ('gua', 0.0): 1, ('can', 0.0): 1, (\"khloe'\", 0.0): 1, ('nhe', 0.0): 1, ('yar', 0.0): 1, ('minkyuk', 0.0): 1, ('hol', 0.0): 1, ('isol', 0.0): 1, ('hk', 0.0): 1, ('sensor', 0.0): 1, ('broker', 0.0): 1, ('wna', 0.0): 1, ('flaviana', 0.0): 1, ('chickmt', 0.0): 1, ('123', 0.0): 1, ('letsfootbal', 0.0): 2, ('atk', 0.0): 2, ('greymind', 0.0): 2, ('43', 0.0): 2, ('gayl', 0.0): 2, ('cricket', 0.0): 3, ('2-3', 0.0): 2, ('mood-dump', 0.0): 1, ('livestream', 0.0): 1, ('gotten', 0.0): 1, ('felton', 0.0): 1, ('veriti', 0.0): 1, (\"standen'\", 0.0): 1, ('shortli', 0.0): 1, ('üòÜ', 0.0): 2, ('takoyaki', 0.0): 1, ('piti', 0.0): 1, ('aisyah', 0.0): 1, ('ffvi', 0.0): 1, ('youtu.be/2_gpctsojkw', 0.0): 1, ('donutsss', 0.0): 1, ('50p', 0.0): 1, ('grate', 0.0): 1, ('spars', 0.0): 1, ('dd', 0.0): 1, ('lagi', 0.0): 1, ('rider', 0.0): 1, ('pride', 0.0): 1, ('hueee', 0.0): 1, ('password', 0.0): 1, ('thingi', 0.0): 1, ('georg', 0.0): 1, ('afraid', 0.0): 2, ('chew', 0.0): 2, ('toy', 0.0): 1, ('stella', 0.0): 1, ('threw', 0.0): 2, ('theaccidentalcoupl', 0.0): 1, ('smooth', 0.0): 1, ('handov', 0.0): 1, ('spick', 0.0): 1, ('bebii', 0.0): 1, ('happenend', 0.0): 1, ('dr', 0.0): 1, ('balm', 0.0): 1, ('hmph', 0.0): 1, ('bubba', 0.0): 2, ('floor', 0.0): 3, ('georgi', 0.0): 1, ('oi', 0.0): 1, ('bengali', 0.0): 1, ('masterchef', 0.0): 1, ('whatchya', 0.0): 1, ('petrol', 0.0): 1, ('diesel', 0.0): 1, ('wardrob', 0.0): 1, ('awe', 0.0): 1, ('cock', 0.0): 1, ('nyquil', 0.0): 1, ('poootek', 0.0): 1, ('1,500', 0.0): 1, ('bobbl', 0.0): 1, ('leak', 0.0): 1, ('thermo', 0.0): 1, ('classic', 0.0): 1, ('ti5', 0.0): 1, ('12th', 0.0): 1, ('skate', 0.0): 1, ('tae', 0.0): 1, ('kita', 0.0): 4, ('ia', 0.0): 1, ('pkwalasawa', 0.0): 1, ('india', 0.0): 1, ('corrupt', 0.0): 2, ('access', 0.0): 2, ('anything.sur', 0.0): 1, ('info', 0.0): 6, ('octob', 0.0): 1, ('mubank', 0.0): 2, ('ene', 0.0): 2, ('3k', 0.0): 1, ('zehr', 0.0): 1, ('khani', 0.0): 1, ('groceri', 0.0): 1, ('hubba', 0.0): 1, ('bubbl', 0.0): 1, ('gum', 0.0): 2, ('closet', 0.0): 1, ('jhalak', 0.0): 1, ('. ..', 0.0): 2, ('bakwa', 0.0): 1, ('. ...', 0.0): 1, ('seehiah', 0.0): 1, ('goy', 0.0): 1, ('nacho', 0.0): 1, ('braid', 0.0): 2, ('initi', 0.0): 1, ('ruth', 0.0): 1, ('boong', 0.0): 1, ('recommend', 0.0): 3, ('gta', 0.0): 1, ('cwnt', 0.0): 1, ('trivia', 0.0): 1, ('belat', 0.0): 1, ('rohingya', 0.0): 1, ('muslim', 0.0): 2, ('indict', 0.0): 1, ('traffick', 0.0): 1, ('thailand', 0.0): 1, ('asia', 0.0): 1, ('rumbl', 0.0): 1, ('kumbl', 0.0): 1, ('scold', 0.0): 1, ('phrase', 0.0): 1, ('includ', 0.0): 1, ('tag', 0.0): 2, ('melt', 0.0): 1, ('tfw', 0.0): 1, ('jest', 0.0): 1, ('offend', 0.0): 2, ('sleepingwithsiren', 0.0): 1, ('17th', 0.0): 1, ('bringmethehorizon', 0.0): 1, ('18th', 0.0): 2, ('carva', 0.0): 1, ('regularli', 0.0): 2, ('sympathi', 0.0): 1, ('revamp', 0.0): 1, ('headphon', 0.0): 1, ('cunt', 0.0): 1, ('wacha', 0.0): 1, ('niend', 0.0): 1, ('bravo', 0.0): 1, ('2hr', 0.0): 1, ('13m', 0.0): 1, ('kk', 0.0): 2, ('calibraksaep', 0.0): 2, ('darlin', 0.0): 1, ('stun', 0.0): 1, (\"doedn't\", 0.0): 1, ('meaning', 0.0): 1, ('horrif', 0.0): 2, ('scoup', 0.0): 2, ('paypal', 0.0): 3, ('sweedi', 0.0): 1, ('nam', 0.0): 1, (\"sacconejoly'\", 0.0): 1, ('bethesda', 0.0): 1, ('fallout', 0.0): 1, ('minecon', 0.0): 1, ('perfect', 0.0): 2, ('katee', 0.0): 1, ('iloveyouu', 0.0): 1, ('linux', 0.0): 1, ('nawww', 0.0): 1, ('chikka', 0.0): 1, ('ug', 0.0): 1, ('rata', 0.0): 1, ('soonest', 0.0): 1, ('mwamwa', 0.0): 1, ('faggot', 0.0): 1, ('doubt', 0.0): 2, ('fyi', 0.0): 1, ('profil', 0.0): 1, ('nicest', 0.0): 1, ('mehendi', 0.0): 1, ('dash', 0.0): 1, ('bookmark', 0.0): 1, ('whay', 0.0): 1, ('shaa', 0.0): 1, ('prami', 0.0): 1, ('üòö', 0.0): 4, ('ngee', 0.0): 1, ('ann', 0.0): 1, ('crikey', 0.0): 2, ('snit', 0.0): 1, ('nathanielhinanakit', 0.0): 1, ('naya', 0.0): 1, ('spinni', 0.0): 1, ('wheel', 0.0): 2, ('albeit', 0.0): 1, ('athlet', 0.0): 1, ('gfriend', 0.0): 2, ('yung', 0.0): 2, ('fugli', 0.0): 1, ('üíû', 0.0): 4, ('jongda', 0.0): 1, ('hardli', 0.0): 2, ('tlist', 0.0): 1, ('budget', 0.0): 1, ('pabebegirl', 0.0): 1, ('pabeb', 0.0): 2, ('alter', 0.0): 1, ('sandra', 0.0): 2, ('bland', 0.0): 2, ('storifi', 0.0): 1, ('abbi', 0.0): 2, ('mtvhottest', 0.0): 1, ('gaga', 0.0): 1, ('rib', 0.0): 1, ('üòµ', 0.0): 1, ('hulkamania', 0.0): 1, ('unlov', 0.0): 1, ('lazi', 0.0): 3, ('ihhh', 0.0): 1, ('stackar', 0.0): 1, ('basil', 0.0): 1, ('remedi', 0.0): 1, ('ov', 0.0): 2, ('raiz', 0.0): 1, ('nvr', 0.0): 1, ('gv', 0.0): 1, ('up.wt', 0.0): 1, ('wt', 0.0): 1, ('imran', 0.0): 2, ('achiev', 0.0): 1, ('thr', 0.0): 1, ('soln', 0.0): 1, (\"sister'\", 0.0): 1, ('hong', 0.0): 1, ('kong', 0.0): 1, ('31st', 0.0): 1, ('pipe', 0.0): 1, ('sept', 0.0): 2, ('lawn', 0.0): 1, (\"cupid'\", 0.0): 1, ('torn', 0.0): 1, ('retain', 0.0): 1, ('clown', 0.0): 2, ('lipstick', 0.0): 1, ('haiss', 0.0): 1, ('todayi', 0.0): 1, ('thoo', 0.0): 1, ('everday', 0.0): 1, ('hangout', 0.0): 2, ('steven', 0.0): 2, ('william', 0.0): 1, ('umboh', 0.0): 1, ('goodafternoon', 0.0): 1, ('jadin', 0.0): 1, ('thiz', 0.0): 1, ('iz', 0.0): 1, ('emeg', 0.0): 1, ('kennat', 0.0): 1, ('reunit', 0.0): 1, ('abi', 0.0): 1, ('arctic', 0.0): 1, ('chicsirif', 0.0): 1, ('structur', 0.0): 1, ('cumbia', 0.0): 1, ('correct', 0.0): 1, ('badlif', 0.0): 1, ('4-5', 0.0): 2, ('kaslkdja', 0.0): 1, ('3wk', 0.0): 1, ('flower', 0.0): 1, ('feverfew', 0.0): 1, ('weddingflow', 0.0): 1, ('diyflow', 0.0): 1, ('fitn', 0.0): 1, ('worth', 0.0): 4, ('wolverin', 0.0): 1, ('khan', 0.0): 1, ('innoc', 0.0): 1, ('üôèüèª', 0.0): 1, ('üéÇ', 0.0): 2, ('memem', 0.0): 2, ('krystoria', 0.0): 1, ('snob', 0.0): 1, ('zumba', 0.0): 1, ('greekcrisi', 0.0): 1, ('remain', 0.0): 1, ('dutch', 0.0): 1, ('legibl', 0.0): 2, ('isra', 0.0): 1, ('passport', 0.0): 1, ('froze', 0.0): 1, ('theori', 0.0): 1, ('23rd', 0.0): 1, ('24th', 0.0): 1, ('stomachach', 0.0): 1, ('slice', 0.0): 1, ('‡ΩÄ', 0.0): 1, ('again', 0.0): 1, ('otani', 0.0): 1, ('3-0', 0.0): 1, ('3rd', 0.0): 3, ('bottom', 0.0): 2, ('niaaa', 0.0): 1, ('2/4', 0.0): 1, ('scheme', 0.0): 2, ('fckin', 0.0): 1, ('hii', 0.0): 1, ('vin', 0.0): 1, ('plss', 0.0): 1, ('rpli', 0.0): 1, ('rat', 0.0): 3, ('bollywood', 0.0): 1, ('mac', 0.0): 1, ('backup', 0.0): 2, ('lune', 0.0): 1, ('robinhood', 0.0): 1, ('robinhoodi', 0.0): 1, ('üöô', 0.0): 1, ('üíö', 0.0): 1, ('docopenhagen', 0.0): 1, ('setter', 0.0): 1, ('swipe', 0.0): 1, ('bbygurl', 0.0): 1, ('neil', 0.0): 1, ('caribbean', 0.0): 1, ('6yr', 0.0): 1, ('jabongatpumaurbanstamped', 0.0): 2, ('takraw', 0.0): 1, ('fersure', 0.0): 1, ('angi', 0.0): 1, ('sheriff', 0.0): 1, ('aaag', 0.0): 1, (\"i'mo\", 0.0): 1, ('sulk', 0.0): 1, ('selfish', 0.0): 1, ('trick', 0.0): 2, ('nonc', 0.0): 1, ('pad', 0.0): 1, ('bison', 0.0): 1, ('motiv', 0.0): 2, (\"q'don\", 0.0): 1, ('cheat', 0.0): 2, ('stomp', 0.0): 1, ('aaaaaaaaah', 0.0): 1, ('kany', 0.0): 1, ('mama', 0.0): 1, ('jdjdjdjd', 0.0): 1, (\"jimin'\", 0.0): 1, ('fancaf', 0.0): 1, ('waffl', 0.0): 1, ('87.7', 0.0): 1, ('2fm', 0.0): 1, ('himseek', 0.0): 1, ('kissm', 0.0): 1, ('akua', 0.0): 1, ('glo', 0.0): 1, ('cori', 0.0): 1, ('monteith', 0.0): 1, ('often', 0.0): 1, ('hashbrown', 0.0): 1, ('üíò', 0.0): 2, ('pg', 0.0): 1, ('msc', 0.0): 1, ('hierro', 0.0): 1, ('shirleycam', 0.0): 1, ('phonesex', 0.0): 2, ('pal', 0.0): 1, ('111', 0.0): 1, ('gilet', 0.0): 1, ('cheek', 0.0): 1, ('squishi', 0.0): 1, ('lahhh', 0.0): 1, ('eon', 0.0): 1, ('sunris', 0.0): 1, ('beeti', 0.0): 1, ('697', 0.0): 1, ('kikkomansabor', 0.0): 1, ('getaway', 0.0): 1, ('crimin', 0.0): 1, ('amiibo', 0.0): 1, ('batman', 0.0): 1, ('habe', 0.0): 1, ('siannn', 0.0): 1, ('march', 0.0): 1, ('2017', 0.0): 1, ('chuckin', 0.0): 1, ('ampsha', 0.0): 1, ('nia', 0.0): 1, ('strap', 0.0): 1, ('dz9055', 0.0): 1, ('entlead', 0.0): 1, ('590', 0.0): 1, ('twice', 0.0): 5, ('07:02', 0.0): 1, ('ifsc', 0.0): 1, ('mayor', 0.0): 1, ('biodivers', 0.0): 1, ('taxonom', 0.0): 1, ('collabor', 0.0): 1, ('speci', 0.0): 1, ('discoveri', 0.0): 1, ('collar', 0.0): 1, ('3:03', 0.0): 1, ('belt', 0.0): 1, ('smith', 0.0): 2, ('eyelin', 0.0): 1, ('therefor', 0.0): 1, ('netherland', 0.0): 1, ('el', 0.0): 1, ('jeb', 0.0): 1, ('blacklivesmatt', 0.0): 1, ('slogan', 0.0): 1, ('msnbc', 0.0): 1, ('jebbush', 0.0): 1, ('famish', 0.0): 1, ('marino', 0.0): 1, ('qualifi', 0.0): 2, ('suzi', 0.0): 1, ('skirt', 0.0): 1, ('tama', 0.0): 1, ('warrior', 0.0): 2, ('wound', 0.0): 1, ('iraq', 0.0): 1, ('be', 0.0): 2, ('camara', 0.0): 1, ('coveral', 0.0): 1, ('happili', 0.0): 1, ('sneezi', 0.0): 1, ('rogerwatch', 0.0): 1, ('stalker', 0.0): 1, ('velvet', 0.0): 1, ('tradit', 0.0): 1, (\"people'\", 0.0): 1, ('beheaviour', 0.0): 1, (\"robert'\", 0.0): 1, ('.\\n.', 0.0): 2, ('aaron', 0.0): 1, ('jelous', 0.0): 1, ('mtg', 0.0): 1, ('thoughtseiz', 0.0): 1, ('playabl', 0.0): 1, ('oldi', 0.0): 1, ('goodi', 0.0): 1, ('mcg', 0.0): 1, ('inspirit', 0.0): 1, ('shine', 0.0): 1, ('ise', 0.0): 1, ('assum', 0.0): 2, ('waist', 0.0): 2, ('guin', 0.0): 1, ('venu', 0.0): 1, ('evil', 0.0): 1, ('pepper', 0.0): 1, ('thessidew', 0.0): 1, ('877', 0.0): 1, ('genesi', 0.0): 1, ('mexico', 0.0): 2, ('novemb', 0.0): 1, ('mash', 0.0): 1, ('whattsap', 0.0): 1, ('inuyasha', 0.0): 2, ('outfwith', 0.0): 1, ('myungsoo', 0.0): 1, ('organis', 0.0): 1, ('satisfi', 0.0): 1, ('wah', 0.0): 1, ('challo', 0.0): 1, ('pliss', 0.0): 1, ('juliana', 0.0): 1, ('enrol', 0.0): 1, ('darlen', 0.0): 1, ('emoji', 0.0): 2, ('brisban', 0.0): 1, ('merlin', 0.0): 1, ('nawwwe', 0.0): 1, ('hyperbulli', 0.0): 1, ('tong', 0.0): 1, ('nga', 0.0): 1, ('seatmat', 0.0): 1, ('rajud', 0.0): 1, ('barkada', 0.0): 1, ('ore', 0.0): 1, ('kayla', 0.0): 1, ('ericavan', 0.0): 1, ('jong', 0.0): 1, ('dongwoo', 0.0): 1, ('photocard', 0.0): 1, ('wh', 0.0): 1, ('dw', 0.0): 1, ('tumor', 0.0): 1, ('vivian', 0.0): 1, ('mmsmalubhangsakit', 0.0): 1, ('jillcruz', 0.0): 2, ('lgbt', 0.0): 3, ('qt', 0.0): 1, ('19th', 0.0): 1, ('toss', 0.0): 1, ('co-work', 0.0): 1, ('mia', 0.0): 1, ('push', 0.0): 4, ('dare', 0.0): 2, ('unsettl', 0.0): 1, ('gh', 0.0): 1, ('18c', 0.0): 1, ('rlli', 0.0): 2, ('hamster', 0.0): 2, ('sheeran', 0.0): 2, ('preform', 0.0): 2, ('monash', 0.0): 1, ('hitmark', 0.0): 1, ('glitch', 0.0): 1, ('safaa', 0.0): 1, (\"selena'\", 0.0): 1, ('galat', 0.0): 1, ('tum', 0.0): 1, ('ab', 0.0): 5, ('non', 0.0): 1, ('lrka', 0.0): 1, ('bna', 0.0): 1, ('kia', 0.0): 1, ('bhook', 0.0): 1, ('jai', 0.0): 1, ('social', 0.0): 2, ('afterschool', 0.0): 1, ('bilal', 0.0): 1, ('ashraf', 0.0): 1, ('icu', 0.0): 1, ('thanksss', 0.0): 1, ('annnd', 0.0): 1, ('winchest', 0.0): 1, ('{:', 0.0): 1, ('grepe', 0.0): 1, ('grepein', 0.0): 1, ('panem', 0.0): 1, ('lover', 0.0): 1, ('sulli', 0.0): 1, ('cpm', 0.0): 1, ('condemn', 0.0): 1, ('‚úî', 0.0): 1, ('occur', 0.0): 1, ('unagi', 0.0): 1, ('7elw', 0.0): 1, ('mesh', 0.0): 1, ('beyt', 0.0): 1, ('3a2ad', 0.0): 1, ('fluent', 0.0): 1, ('varsiti', 0.0): 1, ('sengenza', 0.0): 1, ('context', 0.0): 1, ('movnat', 0.0): 1, ('yield', 0.0): 1, ('nbhero', 0.0): 1, (\"it'd\", 0.0): 1, ('background', 0.0): 1, ('agov', 0.0): 1, ('brasileirao', 0.0): 2, ('abus', 0.0): 1, ('unpar', 0.0): 1, ('bianca', 0.0): 1, ('bun', 0.0): 1, ('dislik', 0.0): 1, ('burdensom', 0.0): 1, ('clear', 0.0): 2, ('amelia', 0.0): 1, ('melon', 0.0): 2, ('useless', 0.0): 1, ('soccer', 0.0): 2, ('interview', 0.0): 2, ('thursday', 0.0): 1, ('nevermind', 0.0): 1, ('jeon', 0.0): 1, ('claw', 0.0): 1, ('thigh', 0.0): 2, ('traction', 0.0): 1, ('damnit', 0.0): 1, ('pri', 0.0): 1, ('pv', 0.0): 2, ('reliv', 0.0): 1, ('nyc', 0.0): 2, ('klm', 0.0): 1, ('11am', 0.0): 1, (\"mcd'\", 0.0): 1, ('hung', 0.0): 1, ('bam', 0.0): 1, ('seventh', 0.0): 1, ('splendour', 0.0): 1, ('swedish', 0.0): 1, ('metal', 0.0): 1, ('h√§irf√∏rc', 0.0): 1, ('givecodpieceach', 0.0): 1, ('alic', 0.0): 3, ('stile', 0.0): 1, ('explain', 0.0): 3, ('ili', 0.0): 1, ('pragu', 0.0): 1, ('sadi', 0.0): 1, ('charact', 0.0): 1, ('915', 0.0): 1, ('hayee', 0.0): 2, ('patwari', 0.0): 1, ('mam', 0.0): 1, (\"ik'\", 0.0): 1, ('vision', 0.0): 2, ('ga', 0.0): 1, ('awhhh', 0.0): 1, ('nalang', 0.0): 1, ('hehe', 0.0): 1, ('albanian', 0.0): 1, ('curs', 0.0): 2, ('tava', 0.0): 1, ('chara', 0.0): 1, ('teteh', 0.0): 1, ('verri', 0.0): 1, ('shatter', 0.0): 2, ('sb', 0.0): 1, ('nawe', 0.0): 1, ('bulldog', 0.0): 1, ('macho', 0.0): 1, ('puriti', 0.0): 1, ('kwento', 0.0): 1, ('nakakapikon', 0.0): 1, ('nagbabasa', 0.0): 1, ('blog', 0.0): 2, ('cancer', 0.0): 1, (':-\\\\', 0.0): 1, ('jonatha', 0.0): 4, ('beti', 0.0): 4, ('sogok', 0.0): 1, ('premium', 0.0): 2, ('instrument', 0.0): 1, ('howev', 0.0): 1, ('dastardli', 0.0): 1, ('swine', 0.0): 1, ('envelop', 0.0): 1, ('pipol', 0.0): 1, ('tad', 0.0): 1, ('wiper', 0.0): 2, ('supposedli', 0.0): 1, ('kernel', 0.0): 1, ('intel', 0.0): 1, ('mega', 0.0): 1, ('bent', 0.0): 1, ('socket', 0.0): 1, ('pcgame', 0.0): 1, ('pcupgrad', 0.0): 1, ('brainwash', 0.0): 2, ('smosh', 0.0): 1, ('plawnew', 0.0): 1, ('837', 0.0): 1, ('aswel', 0.0): 1, ('litter', 0.0): 1, ('mensch', 0.0): 1, ('sepanx', 0.0): 1, ('pci', 0.0): 1, ('caerphilli', 0.0): 1, ('omw', 0.0): 1, ('üòç', 0.0): 1, ('hahdhdhshh', 0.0): 1, ('growinguppoor', 0.0): 1, ('üá∫üá∏', 0.0): 2, (\"bangtan'\", 0.0): 1, ('taimoor', 0.0): 1, ('meray', 0.0): 1, ('dost', 0.0): 1, ('tya', 0.0): 1, ('refollow', 0.0): 1, ('dumb', 0.0): 2, ('butt', 0.0): 1, ('pissbabi', 0.0): 1, ('plank', 0.0): 1, ('inconsist', 0.0): 1, ('moor', 0.0): 1, ('bin', 0.0): 1, ('osx', 0.0): 1, ('chrome', 0.0): 1, ('voiceov', 0.0): 1, ('devo', 0.0): 1, ('hulkhogan', 0.0): 1, ('unpleas', 0.0): 1, ('daaamn', 0.0): 1, ('dada', 0.0): 1, ('fulli', 0.0): 1, ('spike', 0.0): 1, (\"panic'\", 0.0): 1, ('22nd', 0.0): 1, ('south', 0.0): 2, ('africa', 0.0): 2, ('190', 0.0): 2, ('lizardz', 0.0): 1, ('deepli', 0.0): 1, ('emerg', 0.0): 1, ('engin', 0.0): 1, ('dormtel', 0.0): 1, ('scho', 0.0): 1, ('siya', 0.0): 1, ('onee', 0.0): 1, ('carri', 0.0): 1, ('7pm', 0.0): 1, ('feta', 0.0): 1, ('blaaaz', 0.0): 1, ('nausea', 0.0): 1, ('awar', 0.0): 1, ('top-up', 0.0): 1, ('sharknado', 0.0): 1, ('erni', 0.0): 1, ('ezoo', 0.0): 1, ('lilybutl', 0.0): 1, ('seduc', 0.0): 2, ('powai', 0.0): 1, ('neighbor', 0.0): 1, ('delhi', 0.0): 1, ('unsaf', 0.0): 1, ('halo', 0.0): 1, ('fred', 0.0): 1, ('gaon', 0.0): 1, ('infnt', 0.0): 1, ('elig', 0.0): 1, ('acub', 0.0): 1, (\"why'd\", 0.0): 1, ('bullshit', 0.0): 2, ('hanaaa', 0.0): 1, ('jn', 0.0): 1, ('tau', 0.0): 1, ('basta', 0.0): 1, ('sext', 0.0): 1, ('addm', 0.0): 1, ('hotmusicdeloco', 0.0): 2, ('dhi', 0.0): 1, ('üëâ', 0.0): 1, ('8ball', 0.0): 1, ('fakmarey', 0.0): 1, ('doo', 0.0): 2, ('six', 0.0): 3, ('flag', 0.0): 1, ('fulltim', 0.0): 1, ('awkward', 0.0): 1, ('beet', 0.0): 1, ('juic', 0.0): 1, ('dci', 0.0): 1, ('granddad', 0.0): 1, ('minion', 0.0): 3, ('bucket', 0.0): 1, ('kapan', 0.0): 1, ('udah', 0.0): 1, ('dihapu', 0.0): 1, ('hilang', 0.0): 1, ('dari', 0.0): 1, ('muka', 0.0): 1, ('bumi', 0.0): 1, ('narrow', 0.0): 1, ('gona', 0.0): 2, ('chello', 0.0): 1, ('gate', 0.0): 1, ('guard', 0.0): 1, ('crepe', 0.0): 1, ('forsaken', 0.0): 1, ('kanin', 0.0): 1, ('hypixel', 0.0): 1, ('grrr', 0.0): 1, ('thestruggleisr', 0.0): 1, ('geek', 0.0): 1, ('gamer', 0.0): 2, ('afterbirth', 0.0): 1, (\"apink'\", 0.0): 1, ('overperhatian', 0.0): 1, ('son', 0.0): 1, ('pox', 0.0): 1, ('ahm', 0.0): 1, ('karli', 0.0): 1, ('kloss', 0.0): 1, ('goofi', 0.0): 1, ('pcd', 0.0): 1, ('antagonis', 0.0): 1, ('writer', 0.0): 1, ('nudg', 0.0): 1, ('delv', 0.0): 1, ('grandad', 0.0): 1, (\"gray'\", 0.0): 1, ('followk', 0.0): 1, ('suggest', 0.0): 2, ('pace', 0.0): 1, ('maker', 0.0): 1, ('molli', 0.0): 1, ('higher', 0.0): 1, ('ceremoni', 0.0): 1, ('christin', 0.0): 1, ('moodi', 0.0): 1, ('throwback', 0.0): 1, ('fav', 0.0): 3, ('barb', 0.0): 1, ('creasi', 0.0): 1, ('deputi', 0.0): 1, ('tast', 0.0): 1, (\"banana'\", 0.0): 1, ('saludo', 0.0): 1, ('dissapoint', 0.0): 1, ('üò´', 0.0): 1, ('&lt;--', 0.0): 1, (\"bae'\", 0.0): 1, ('pimpl', 0.0): 2, ('amount', 0.0): 2, ('tdi', 0.0): 1, ('pamela', 0.0): 1, ('mini', 0.0): 1, ('mast', 0.0): 1, ('intermitt', 0.0): 1, ('servic', 0.0): 3, ('janniecam', 0.0): 1, ('musicbiz', 0.0): 1, ('braxton', 0.0): 1, ('pro', 0.0): 2, ('urban', 0.0): 1, ('unpreced', 0.0): 1, ('tebow', 0.0): 1, ('okaaay', 0.0): 1, ('sayanggg', 0.0): 1, ('housework', 0.0): 1, ('bust', 0.0): 2, ('disneyland', 0.0): 1, ('thoma', 0.0): 1, ('tommyy', 0.0): 1, ('billi', 0.0): 1, ('kevin', 0.0): 1, ('clifton', 0.0): 1, ('strictli', 0.0): 1, ('nsc', 0.0): 1, ('mat', 0.0): 1, ('0', 0.0): 1, ('awhh', 0.0): 1, ('ram', 0.0): 2, ('voucher', 0.0): 1, ('smadvow', 0.0): 1, ('544', 0.0): 1, ('acdc', 0.0): 1, ('aker', 0.0): 1, ('gmail', 0.0): 1, ('sprevelink', 0.0): 1, ('633', 0.0): 1, ('lana', 0.0): 2, ('loveyoutilltheendcart', 0.0): 1, ('sfv', 0.0): 1, ('6/7', 0.0): 1, ('winner', 0.0): 1, ('20/1', 0.0): 1, ('david', 0.0): 1, ('rosi', 0.0): 1, ('hayoung', 0.0): 1, ('nlb', 0.0): 1, ('@_', 0.0): 1, ('tayo', 0.0): 1, ('forth', 0.0): 1, ('suspect', 0.0): 1, ('mening', 0.0): 1, ('viral', 0.0): 1, ('tonsil', 0.0): 1, ('üò∑', 0.0): 1, ('üòù', 0.0): 1, ('babyy', 0.0): 2, ('cushion', 0.0): 1, ('üòø', 0.0): 1, ('üíì', 0.0): 2, ('weigh', 0.0): 1, ('keen', 0.0): 1, ('petrofac', 0.0): 1, (';-)', 0.0): 1, ('wig', 0.0): 1, (\"mark'\", 0.0): 1, ('pathet', 0.0): 1, ('burden.say', 0.0): 1, ('itchi', 0.0): 1, ('cheaper', 0.0): 1, ('malaysia', 0.0): 1, ('130', 0.0): 1, ('snapchattimg', 0.0): 1, ('üòè', 0.0): 4, ('sin', 0.0): 1, ('lor', 0.0): 1, ('dedic', 0.0): 1, ('worriedli', 0.0): 1, ('stare', 0.0): 1, ('toneadi', 0.0): 1, ('46532', 0.0): 1, ('snapdirti', 0.0): 1, ('sheskindahot', 0.0): 1, ('corps', 0.0): 1, ('taeni', 0.0): 1, ('fyeah', 0.0): 1, ('andromeda', 0.0): 1, ('yunni', 0.0): 1, ('whdjwksja', 0.0): 1, ('ziam', 0.0): 1, ('100k', 0.0): 1, ('spoil', 0.0): 1, ('curtain', 0.0): 1, ('watchabl', 0.0): 1, ('migrin', 0.0): 1, ('gdce', 0.0): 1, ('gamescom', 0.0): 1, (\"do't\", 0.0): 1, ('parcel', 0.0): 1, ('num', 0.0): 1, ('oooouch', 0.0): 1, ('pinki', 0.0): 1, ('üë£', 0.0): 1, ('podiatrist', 0.0): 1, ('gusto', 0.0): 1, (\"rodic'\", 0.0): 1, (\"one'\", 0.0): 1, ('adoohh', 0.0): 1, ('b-butt', 0.0): 1, ('tigermilk', 0.0): 1, ('east', 0.0): 1, ('dulwich', 0.0): 1, ('intens', 0.0): 1, ('kagami', 0.0): 1, ('kuroko', 0.0): 1, ('sana', 0.0): 2, ('makita', 0.0): 1, ('spooki', 0.0): 1, ('smol', 0.0): 1, ('bean', 0.0): 1, ('fagan', 0.0): 1, ('meadowhal', 0.0): 1, ('lola', 0.0): 1, ('nadalaw', 0.0): 1, ('labyu', 0.0): 1, ('jot', 0.0): 1, ('ivypowel', 0.0): 1, ('homeslic', 0.0): 1, ('33', 0.0): 2, ('emoticon', 0.0): 2, ('eyebrow', 0.0): 1, ('prettylook', 0.0): 1, ('whitney', 0.0): 1, ('houston', 0.0): 1, ('aur', 0.0): 1, ('shamil', 0.0): 1, ('tonn', 0.0): 1, ('statu', 0.0): 1, ('‚Üí', 0.0): 1, ('suddenli', 0.0): 2, ('alli', 0.0): 2, ('wrap', 0.0): 1, ('neck', 0.0): 1, ('heartbroken', 0.0): 1, ('chover', 0.0): 1, ('cebu', 0.0): 1, ('lechon', 0.0): 1, ('kitten', 0.0): 2, ('jannygreen', 0.0): 2, ('suicid', 0.0): 2, ('forgiv', 0.0): 1, ('conno', 0.0): 1, ('brooo', 0.0): 1, ('rout', 0.0): 1, ('lovebox', 0.0): 1, ('prod', 0.0): 1, ('osad', 0.0): 1, ('scam', 0.0): 1, ('itb', 0.0): 1, ('omigod', 0.0): 1, ('ehem', 0.0): 1, ('ala', 0.0): 1, ('yeke', 0.0): 1, ('jumpa', 0.0): 1, ('üòã', 0.0): 1, ('ape', 0.0): 1, ('1.2', 0.0): 1, ('map', 0.0): 1, ('namin', 0.0): 1, ('govt', 0.0): 1, ('e-petit', 0.0): 1, ('pretend', 0.0): 1, ('irk', 0.0): 1, ('ruess', 0.0): 1, ('program', 0.0): 1, ('aigoo', 0.0): 1, ('doujin', 0.0): 1, ('killua', 0.0): 1, ('ginggon', 0.0): 1, ('guys.al', 0.0): 1, ('ytd', 0.0): 1, ('pdapaghimok', 0.0): 1, ('flexibl', 0.0): 1, ('sheet', 0.0): 1, ('nanaman', 0.0): 1, ('pinay', 0.0): 1, ('pie', 0.0): 1, ('jadi', 0.0): 1, ('langsung', 0.0): 1, ('flasback', 0.0): 1, ('franc', 0.0): 1, (':|', 0.0): 1, ('lo', 0.0): 1, ('nicknam', 0.0): 1, ('involv', 0.0): 1, ('scrape', 0.0): 1, ('pile', 0.0): 1, ('sare', 0.0): 1, ('bandar', 0.0): 1, ('varg', 0.0): 1, ('hammer', 0.0): 1, ('lolo', 0.0): 1, ('xbsbabnb', 0.0): 1, ('stilll', 0.0): 1, ('apma', 0.0): 2, ('leadership', 0.0): 1, ('wakeupgop', 0.0): 1, ('mv', 0.0): 1, ('bull', 0.0): 1, ('trafficcc', 0.0): 1, ('oscar', 0.0): 1, ('pornographi', 0.0): 1, ('slutsham', 0.0): 1, ('ect', 0.0): 1, ('poland', 0.0): 1, ('faraway', 0.0): 1, ('700', 0.0): 1, ('800', 0.0): 1, ('cgi', 0.0): 1, ('pun', 0.0): 1, (\"x'\", 0.0): 1, ('osaka', 0.0): 1, ('junior', 0.0): 1, ('aytona', 0.0): 1, ('hala', 0.0): 1, ('mathird', 0.0): 1, ('jkjk', 0.0): 1, ('backtrack', 0.0): 1, ('util', 0.0): 1, ('pat', 0.0): 1, ('jay', 0.0): 2, ('broh', 0.0): 1, ('calll', 0.0): 1, ('icaru', 0.0): 1, ('awn', 0.0): 1, ('bach', 0.0): 1, ('court', 0.0): 1, ('landlord', 0.0): 1, (\"mp'\", 0.0): 1, ('dame', 0.0): 1, ('gossip', 0.0): 1, ('purpl', 0.0): 2, ('tie', 0.0): 1, ('ishii', 0.0): 1, ('clara', 0.0): 1, ('yile', 0.0): 1, ('whatev', 0.0): 1, ('stil', 0.0): 1, ('sidharth', 0.0): 1, ('ndabenhl', 0.0): 1, ('doggi', 0.0): 1, ('antag', 0.0): 1, ('41', 0.0): 1, ('thu', 0.0): 1, ('jenner', 0.0): 1, ('troubleshoot', 0.0): 1, (\"convo'\", 0.0): 1, ('dem', 0.0): 1, ('tix', 0.0): 2, ('automat', 0.0): 1, ('redirect', 0.0): 1, ('gigi', 0.0): 1, ('carter', 0.0): 1, ('corn', 0.0): 2, ('chip', 0.0): 2, ('nnnooo', 0.0): 1, ('cz', 0.0): 1, ('gorilla', 0.0): 1, ('hbm', 0.0): 1, ('humid', 0.0): 1, ('admir', 0.0): 1, ('consist', 0.0): 1, ('jason', 0.0): 1, (\"shackell'\", 0.0): 1, ('podcast', 0.0): 1, ('envi', 0.0): 1, ('twer', 0.0): 1, ('782', 0.0): 1, ('hahaahahahaha', 0.0): 1, ('sm1', 0.0): 1, ('mutil', 0.0): 1, ('robot', 0.0): 1, ('destroy', 0.0): 1, ('freakin', 0.0): 1, ('haestarr', 0.0): 1, ('üòÄ', 0.0): 3, ('audio', 0.0): 1, ('snippet', 0.0): 1, ('brotherhood', 0.0): 1, ('mefd', 0.0): 1, ('diana', 0.0): 1, ('master', 0.0): 1, ('led', 0.0): 1, ('award', 0.0): 1, ('meowkd', 0.0): 1, ('complic', 0.0): 1, (\"c'mon\", 0.0): 1, (\"swimmer'\", 0.0): 1, ('leh', 0.0): 1, ('corner', 0.0): 1, ('didnot', 0.0): 1, ('usanel', 0.0): 2, ('nathan', 0.0): 1, ('micha', 0.0): 1, ('fave', 0.0): 2, ('creep', 0.0): 1, ('throughout', 0.0): 1, ('whose', 0.0): 1, ('ave', 0.0): 1, ('tripl', 0.0): 1, ('lectur', 0.0): 1, ('2-5', 0.0): 1, ('jaw', 0.0): 1, ('quarter', 0.0): 1, ('soni', 0.0): 1, ('followmeaaron', 0.0): 1, ('tzelumxoxo', 0.0): 1, ('drank', 0.0): 1, ('mew', 0.0): 1, ('indic', 0.0): 1, ('ouliv', 0.0): 1, ('70748', 0.0): 1, ('viernesderolenahot', 0.0): 1, ('longmorn', 0.0): 1, ('tobermori', 0.0): 1, ('32', 0.0): 1, ('tail', 0.0): 1, ('recuerda', 0.0): 1, ('tanto', 0.0): 1, ('bath', 0.0): 1, ('muna', 0.0): 1, ('await', 0.0): 1, ('urslef', 0.0): 1, ('lime', 0.0): 1, ('truckload', 0.0): 1, ('favour', 0.0): 2, ('spectat', 0.0): 1, ('sail', 0.0): 1, (\"w'end\", 0.0): 1, ('bbc', 0.0): 1, ('‚Äò', 0.0): 1, ('foil', 0.0): 1, ('ac45', 0.0): 1, ('catamaran', 0.0): 1, ('peli', 0.0): 1, ('829', 0.0): 1, ('sextaatequemfimseguesdvcomvalentino', 0.0): 1, ('befor', 0.0): 1, ('valu', 0.0): 1, ('cinnamon', 0.0): 1, ('mtap', 0.0): 1, ('peng', 0.0): 1, ('frozen', 0.0): 1, ('bagu', 0.0): 1, ('emang', 0.0): 1, ('engg', 0.0): 1, ('cmc', 0.0): 1, ('mage', 0.0): 1, ('statement', 0.0): 1, ('moodsw', 0.0): 1, ('termin', 0.0): 1, ('men', 0.0): 1, ('peep', 0.0): 1, ('multipl', 0.0): 1, ('mef', 0.0): 1, ('rebound', 0.0): 1, ('pooor', 0.0): 1, ('2am', 0.0): 1, ('perpetu', 0.0): 1, ('bitchfac', 0.0): 1, ('clever', 0.0): 1, ('iceland', 0.0): 1, ('zayn_come_back_we_miss_y', 0.0): 1, ('pmsl', 0.0): 1, ('mianh', 0.0): 1, ('milkeu', 0.0): 1, ('lrt', 0.0): 1, ('bambam', 0.0): 1, ('soda', 0.0): 1, ('payback', 0.0): 1, ('87000', 0.0): 1, ('jobe', 0.0): 1, ('muchi', 0.0): 1, ('üéà', 0.0): 1, ('bathroom', 0.0): 1, ('lagg', 0.0): 1, ('banget', 0.0): 1, ('novel', 0.0): 1, (\"there'd\", 0.0): 1, ('invis', 0.0): 1, ('scuttl', 0.0): 1, ('worm', 0.0): 1, ('bauuukkk', 0.0): 1, ('jessica', 0.0): 1, ('5:15', 0.0): 1, ('argument', 0.0): 1, ('couldnt', 0.0): 2, ('yepp', 0.0): 1, ('üò∫', 0.0): 1, ('üíí', 0.0): 1, ('üíé', 0.0): 1, ('feelin', 0.0): 1, ('biscuit', 0.0): 1, ('slather', 0.0): 1, ('jsut', 0.0): 1, ('belov', 0.0): 1, ('grandmoth', 0.0): 1, ('princess', 0.0): 2, ('babee', 0.0): 1, ('demn', 0.0): 1, ('hotaisndonwyvauwjoqhsjsnaihsuswtf', 0.0): 1, ('sia', 0.0): 1, ('niram', 0.0): 1, ('geng', 0.0): 1, ('fikri', 0.0): 1, ('tirtagangga', 0.0): 1, ('char', 0.0): 1, ('font', 0.0): 2, ('riprishikeshwari', 0.0): 1, ('creamist', 0.0): 1, ('challeng', 0.0): 1, ('substitut', 0.0): 1, ('skin', 0.0): 1, ('cplt', 0.0): 1, ('cp', 0.0): 1, ('hannah', 0.0): 1, ('üíô', 0.0): 1, ('üí™', 0.0): 1, ('opu', 0.0): 1, ('inner', 0.0): 1, ('pleasur', 0.0): 1, ('bbq', 0.0): 1, ('lolliv', 0.0): 1, ('split', 0.0): 3, ('collat', 0.0): 2, ('spilt', 0.0): 2, ('quitkarwaoyaaro', 0.0): 1, ('deactiÃáv', 0.0): 1, ('2.5', 0.0): 1, ('g2a', 0.0): 1, ('sherep', 0.0): 1, ('nemen', 0.0): 1, ('behey', 0.0): 1, ('motherfuck', 0.0): 1, ('tattoo', 0.0): 1, ('reec', 0.0): 1, ('vm', 0.0): 1, ('deth', 0.0): 2, ('lest', 0.0): 1, ('gp', 0.0): 1, ('departur', 0.0): 1, ('wipe', 0.0): 1, ('yuck', 0.0): 1, ('ystrday', 0.0): 1, ('seolhyun', 0.0): 1, ('drama', 0.0): 1, ('spici', 0.0): 1, ('owl', 0.0): 1, ('mumbai', 0.0): 1, (\"pj'\", 0.0): 1, ('wallpap', 0.0): 1, ('cba', 0.0): 1, ('hotter', 0.0): 1, ('rec', 0.0): 1, ('gotdamn', 0.0): 1, ('baaack', 0.0): 1, ('honest', 0.0): 1, ('srw', 0.0): 1, ('mobag', 0.0): 1, ('dunno', 0.0): 1, ('stroke', 0.0): 1, ('gnr', 0.0): 1, ('backstag', 0.0): 1, ('slash', 0.0): 1, ('prolli', 0.0): 1, ('bunni', 0.0): 1, ('sooner', 0.0): 1, ('analyst', 0.0): 1, ('expedia', 0.0): 1, ('bellevu', 0.0): 1, ('prison', 0.0): 1, ('alcohol', 0.0): 1, ('huhuh', 0.0): 1, ('heartburn', 0.0): 1, ('awalmu', 0.0): 1, ('njareeem', 0.0): 1, ('maggi', 0.0): 1, ('psycho', 0.0): 1, ('wahhh', 0.0): 1, ('abudhabi', 0.0): 1, ('hiby', 0.0): 1, ('shareyoursumm', 0.0): 1, ('b8', 0.0): 1, ('must.b', 0.0): 1, ('dairi', 0.0): 1, ('produxt', 0.0): 1, ('lactos', 0.0): 2, ('midland', 0.0): 1, ('knacker', 0.0): 1, ('footag', 0.0): 1, ('lifeless', 0.0): 1, ('shell', 0.0): 1, ('44', 0.0): 1, ('7782', 0.0): 1, ('pengen', 0.0): 1, ('girlll', 0.0): 1, ('tsunami', 0.0): 1, ('indi', 0.0): 1, ('nick', 0.0): 1, ('tirad', 0.0): 1, ('stoop', 0.0): 1, ('lower', 0.0): 1, ('role', 0.0): 1, ('thunder', 0.0): 1, ('paradis', 0.0): 1, ('habit', 0.0): 1, ('facad', 0.0): 1, ('democraci', 0.0): 1, ('brat', 0.0): 1, ('tb', 0.0): 1, (\"o'\", 0.0): 1, ('bade', 0.0): 1, ('fursat', 0.0): 1, ('usey', 0.0): 2, ('banaya', 0.0): 1, ('uppar', 0.0): 1, ('waal', 0.0): 1, ('ney', 0.0): 1, ('afso', 0.0): 1, ('hums', 0.0): 1, ('dur', 0.0): 1, ('wo', 0.0): 1, (\"who'd\", 0.0): 1, ('naruhina', 0.0): 1, ('namee', 0.0): 1, ('haiqal', 0.0): 1, ('360hr', 0.0): 1, ('picc', 0.0): 1, ('instor', 0.0): 1, ('pre-vot', 0.0): 1, ('5th', 0.0): 1, ('usernam', 0.0): 1, ('minho', 0.0): 1, ('durian', 0.0): 1, ('strudel', 0.0): 1, ('tsk', 0.0): 1, ('marin', 0.0): 1, ('kailan', 0.0): 1, ('separ', 0.0): 1, ('payday', 0.0): 1, ('payhour', 0.0): 1, ('immedi', 0.0): 1, ('natur', 0.0): 1, ('pre-ord', 0.0): 1, ('fwm', 0.0): 1, ('guppi', 0.0): 1, ('poorkid', 0.0): 1, ('lack', 0.0): 1, ('misunderstood', 0.0): 1, ('cuddli', 0.0): 1, ('scratch', 0.0): 1, ('thumb', 0.0): 1, ('compens', 0.0): 1, ('kirkiri', 0.0): 1, ('phase', 0.0): 1, ('wonho', 0.0): 1, ('visual', 0.0): 1, (\"='(\", 0.0): 1, ('mission', 0.0): 1, ('pap', 0.0): 1, ('danzel', 0.0): 1, ('craft', 0.0): 1, ('devil', 0.0): 1, ('phil', 0.0): 1, ('sheff', 0.0): 1, ('york', 0.0): 1, ('visa', 0.0): 1, ('gim', 0.0): 1, ('bench', 0.0): 1, ('harm', 0.0): 1, ('yolo', 0.0): 1, ('bloat', 0.0): 1, ('olli', 0.0): 1, ('alterni', 0.0): 1, ('earth', 0.0): 1, ('influenc', 0.0): 1, ('overal', 0.0): 1, ('continent', 0.0): 1, ('üî´', 0.0): 1, ('tank', 0.0): 1, ('thirsti', 0.0): 1, ('konami', 0.0): 1, ('polici', 0.0): 1, ('ranti', 0.0): 1, ('atm', 0.0): 1, ('pervers', 0.0): 1, ('bylfnnz', 0.0): 1, ('ban', 0.0): 1, ('failsatlif', 0.0): 1, ('press', 0.0): 1, ('duper', 0.0): 1, ('waaah', 0.0): 1, ('jaebum', 0.0): 1, ('ahmad', 0.0): 1, ('maslan', 0.0): 1, ('hull', 0.0): 1, ('misser', 0.0): 1}\n\n\nUnfortunately, this does not help much to understand the data. It would be better to visualize this output to gain better insights.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "L2 Frequencies"
    ]
  },
  {
    "objectID": "posts/c1w1/lab02.html#table-of-word-counts",
    "href": "posts/c1w1/lab02.html#table-of-word-counts",
    "title": "Building and Visualizing word frequencies",
    "section": "Table of word counts",
    "text": "Table of word counts\nWe will select a set of words that we would like to visualize. It is better to store this temporary information in a table that is very easy to use later.\n\n# select some words to appear in the report. we will assume that each word is unique (i.e. no duplicates)\nkeys = ['happi', 'merri', 'nice', 'good', 'bad', 'sad', 'mad', 'best', 'pretti',\n        '‚ù§', ':)', ':(', 'üòí', 'üò¨', 'üòÑ', 'üòç', '‚ôõ',\n        'song', 'idea', 'power', 'play', 'magnific']\n\n# list representing our table of word counts.\n# each element consist of a sublist with this pattern: [&lt;word&gt;, &lt;positive_count&gt;, &lt;negative_count&gt;]\ndata = []\n\n# loop through our selected words\nfor word in keys:\n    \n    # initialize positive and negative counts\n    pos = 0\n    neg = 0\n    \n    # retrieve number of positive counts\n    if (word, 1) in freqs:\n        pos = freqs[(word, 1)]\n        \n    # retrieve number of negative counts\n    if (word, 0) in freqs:\n        neg = freqs[(word, 0)]\n        \n    # append the word counts to the table\n    data.append([word, pos, neg])\n    \ndata\n\n[['happi', 211, 25],\n ['merri', 1, 0],\n ['nice', 98, 19],\n ['good', 238, 101],\n ['bad', 18, 73],\n ['sad', 5, 123],\n ['mad', 4, 11],\n ['best', 65, 22],\n ['pretti', 20, 15],\n ['‚ù§', 29, 21],\n [':)', 3568, 2],\n [':(', 1, 4571],\n ['üòí', 1, 3],\n ['üò¨', 0, 2],\n ['üòÑ', 5, 1],\n ['üòç', 2, 1],\n ['‚ôõ', 0, 210],\n ['song', 22, 27],\n ['idea', 26, 10],\n ['power', 7, 6],\n ['play', 46, 48],\n ['magnific', 2, 0]]\n\n\nWe can then use a scatter plot to inspect this table visually. Instead of plotting the raw counts, we will plot it in the logarithmic scale to take into account the wide discrepancies between the raw counts (e.g.¬†:) has 3568 counts in the positive while only 2 in the negative). The red line marks the boundary between positive and negative areas. Words close to the red line can be classified as neutral.\n\nfig, ax = plt.subplots(figsize = (8, 8))\n\n# convert positive raw counts to logarithmic scale. we add 1 to avoid log(0)\nx = np.log([x[1] + 1 for x in data])  \n\n# do the same for the negative counts\ny = np.log([x[2] + 1 for x in data]) \n\n# Plot a dot for each pair of words\nax.scatter(x, y)  \n\n# assign axis labels\nplt.xlabel(\"Log Positive count\")\nplt.ylabel(\"Log Negative count\")\n\n# Add the word as the label at the same position as you added the points just before\nfor i in range(0, len(data)):\n    ax.annotate(data[i][0], (x[i], y[i]), fontsize=12)\n\nax.plot([0, 9], [0, 9], color = 'red') # Plot the red line that divides the 2 areas.\nplt.show()\n\nText(0.5, 0, 'Log Positive count')\n\n\nText(0, 0.5, 'Log Negative count')\n\n\nText(5.356586274672012, 3.258096538021482, 'happi')\n\n\nText(0.6931471805599453, 0.0, 'merri')\n\n\nText(4.59511985013459, 2.995732273553991, 'nice')\n\n\nText(5.476463551931511, 4.624972813284271, 'good')\n\n\nText(2.9444389791664403, 4.30406509320417, 'bad')\n\n\nText(1.791759469228055, 4.820281565605037, 'sad')\n\n\nText(1.6094379124341003, 2.4849066497880004, 'mad')\n\n\nText(4.189654742026425, 3.1354942159291497, 'best')\n\n\nText(3.044522437723423, 2.772588722239781, 'pretti')\n\n\nText(3.4011973816621555, 3.091042453358316, '‚ù§')\n\n\nText(8.18004072349016, 1.0986122886681098, ':)')\n\n\nText(0.6931471805599453, 8.427706024914702, ':(')\n\n\nText(0.6931471805599453, 1.3862943611198906, 'üòí')\n\n\nText(0.0, 1.0986122886681098, 'üò¨')\n\n\nText(1.791759469228055, 0.6931471805599453, 'üòÑ')\n\n\nText(1.0986122886681098, 0.6931471805599453, 'üòç')\n\n\nText(0.0, 5.351858133476067, '‚ôõ')\n\n\nText(3.1354942159291497, 3.332204510175204, 'song')\n\n\nText(3.295836866004329, 2.3978952727983707, 'idea')\n\n\nText(2.0794415416798357, 1.9459101490553132, 'power')\n\n\nText(3.8501476017100584, 3.8918202981106265, 'play')\n\n\nText(1.0986122886681098, 0.0, 'magnific')\n\n\n/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 128556 (\\N{GRIMACING FACE}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n\n\n\n\n\n\n\n\n\nThis chart is straightforward to interpret. It shows that emoticons :) and :( are very important for sentiment analysis. Thus, we should not let preprocessing steps get rid of these symbols!\nFurthermore, what is the meaning of the crown symbol? It seems to be very negative!\n\nConclusion\nThat‚Äôs all for this lab!\nWe‚Äôve seen how to build a word frequency dictionary and this will come in handy when extracting the features of a list of tweets. Next up, we will be reviewing Logistic Regression. Keep it up!",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "L2 Frequencies"
    ]
  },
  {
    "objectID": "posts/c3w4/assignment.html",
    "href": "posts/c3w4/assignment.html",
    "title": "Assignment 4: Question duplicates",
    "section": "",
    "text": "course banner\nWelcome to the fourth assignment of course 3. In this assignment you will explore Siamese networks applied to natural language processing. You will further explore the fundamentals of Trax and you will be able to implement a more complicated structure using it. By completing this assignment, you will learn how to implement models with different architectures."
  },
  {
    "objectID": "posts/c3w4/assignment.html#outline",
    "href": "posts/c3w4/assignment.html#outline",
    "title": "Assignment 4: Question duplicates",
    "section": "Outline",
    "text": "Outline\n\nOverview\nPart 1: Importing the Data\n\n1.1 Loading in the data\n1.2 Converting a question to a tensor\n1.3 Understanding the iterator\n\nExercise 01\n\n\nPart 2: Defining the Siamese model\n\n2.1 Understanding Siamese Network\n\nExercise 02\n\n2.2 Hard Negative Mining\n\nExercise 03\n\n\nPart 3: Training\n\n3.1 Training the model\n\nExercise 04\n\n\nPart 4: Evaluation\n\n4.1 Evaluating your siamese network\n4.2 Classify\n\nExercise 05\n\n\nPart 5: Testing with your own questions\n\nExercise 06\n\nOn Siamese networks\n\n\nOverview\nIn this assignment, concretely you will:\n\nLearn about Siamese networks\nUnderstand how the triplet loss works\nUnderstand how to evaluate accuracy\nUse cosine similarity between the model‚Äôs outputted vectors\nUse the data generator to get batches of questions\nPredict using your own model\n\nBy now, you are familiar with trax and know how to make use of classes to define your model. We will start this homework by asking you to preprocess the data the same way you did in the previous assignments. After processing the data you will build a classifier that will allow you to identify whether to questions are the same or not.\n\n\n\nmeme\n\n\nYou will process the data first and then pad in a similar way you have done in the previous assignment. Your model will take in the two question embeddings, run them through an LSTM, and then compare the outputs of the two sub networks using cosine similarity. Before taking a deep dive into the model, start by importing the data set.\n # Part 1: Importing the Data  ### 1.1 Loading in the data\nYou will be using the Quora question answer dataset to build a model that could identify similar questions. This is a useful task because you don‚Äôt want to have several versions of the same question posted. Several times when teaching I end up responding to similar questions on piazza, or on other community forums. This data set has been labeled for you. Run the cell below to import some of the packages you will be using.\n\nimport os\nimport nltk\nimport trax\nfrom trax import layers as tl\nfrom trax.supervised import training\nfrom trax.fastmath import numpy as fastnp\nimport numpy as np\nimport pandas as pd\nimport random as rnd\n\n# set random seeds\ntrax.supervised.trainer_lib.init_random_number_generators(34)\nrnd.seed(34)\n\n2025-02-05 21:49:40.336562: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1738784980.354192  609749 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1738784980.360264  609749 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[1], line 12\n      9 import random as rnd\n     11 # set random seeds\n---&gt; 12 trax.supervised.trainer_lib.init_random_number_generators(34)\n     13 rnd.seed(34)\n\nAttributeError: module 'trax.supervised.trainer_lib' has no attribute 'init_random_number_generators'\n\n\n\nNotice that for this assignment Trax‚Äôs numpy is referred to as fastnp, while regular numpy is referred to as np.\nYou will now load in the data set. We have done some preprocessing for you. If you have taken the deeplearning specialization, this is a slightly different training method than the one you have seen there. If you have not, then don‚Äôt worry about it, we will explain everything.\n\ndata = pd.read_csv(\"questions.csv\")\nN=len(data)\nprint('Number of question pairs: ', N)\ndata.head()\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nCell In[2], line 1\n----&gt; 1 data = pd.read_csv(\"questions.csv\")\n      2 N=len(data)\n      3 print('Number of question pairs: ', N)\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n   1013 kwds_defaults = _refine_defaults_read(\n   1014     dialect,\n   1015     delimiter,\n   (...)\n   1022     dtype_backend=dtype_backend,\n   1023 )\n   1024 kwds.update(kwds_defaults)\n-&gt; 1026 return _read(filepath_or_buffer, kwds)\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620, in _read(filepath_or_buffer, kwds)\n    617 _validate_names(kwds.get(\"names\", None))\n    619 # Create the parser.\n--&gt; 620 parser = TextFileReader(filepath_or_buffer, **kwds)\n    622 if chunksize or iterator:\n    623     return parser\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620, in TextFileReader.__init__(self, f, engine, **kwds)\n   1617     self.options[\"has_index_names\"] = kwds[\"has_index_names\"]\n   1619 self.handles: IOHandles | None = None\n-&gt; 1620 self._engine = self._make_engine(f, self.engine)\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880, in TextFileReader._make_engine(self, f, engine)\n   1878     if \"b\" not in mode:\n   1879         mode += \"b\"\n-&gt; 1880 self.handles = get_handle(\n   1881     f,\n   1882     mode,\n   1883     encoding=self.options.get(\"encoding\", None),\n   1884     compression=self.options.get(\"compression\", None),\n   1885     memory_map=self.options.get(\"memory_map\", False),\n   1886     is_text=is_text,\n   1887     errors=self.options.get(\"encoding_errors\", \"strict\"),\n   1888     storage_options=self.options.get(\"storage_options\", None),\n   1889 )\n   1890 assert self.handles is not None\n   1891 f = self.handles.handle\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/pandas/io/common.py:873, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\n    868 elif isinstance(handle, str):\n    869     # Check whether the filename is to be opened in binary mode.\n    870     # Binary mode does not support 'encoding' and 'newline'.\n    871     if ioargs.encoding and \"b\" not in ioargs.mode:\n    872         # Encoding\n--&gt; 873         handle = open(\n    874             handle,\n    875             ioargs.mode,\n    876             encoding=ioargs.encoding,\n    877             errors=errors,\n    878             newline=\"\",\n    879         )\n    880     else:\n    881         # Binary mode\n    882         handle = open(handle, ioargs.mode)\n\nFileNotFoundError: [Errno 2] No such file or directory: 'questions.csv'\n\n\n\nWe first split the data into a train and test set. The test set will be used later to evaluate our model.\n\nN_train = 300000\nN_test  = 10*1024\ndata_train = data[:N_train]\ndata_test  = data[N_train:N_train+N_test]\nprint(\"Train set:\", len(data_train), \"Test set:\", len(data_test))\ndel(data) # remove to free memory\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[3], line 3\n      1 N_train = 300000\n      2 N_test  = 10*1024\n----&gt; 3 data_train = data[:N_train]\n      4 data_test  = data[N_train:N_train+N_test]\n      5 print(\"Train set:\", len(data_train), \"Test set:\", len(data_test))\n\nNameError: name 'data' is not defined\n\n\n\nAs explained in the lectures, we select only the question pairs that are duplicate to train the model.  We build two batches as input for the Siamese network and we assume that question q1_i (question i in the first batch) is a duplicate of q2_i (question i in the second batch), but all other questions in the second batch are not duplicates of q1_i.\nThe test set uses the original pairs of questions and the status describing if the questions are duplicates.\n\ntd_index = (data_train['is_duplicate'] == 1).to_numpy()\ntd_index = [i for i, x in enumerate(td_index) if x] \nprint('number of duplicate questions: ', len(td_index))\nprint('indexes of first ten duplicate questions:', td_index[:10])\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[4], line 1\n----&gt; 1 td_index = (data_train['is_duplicate'] == 1).to_numpy()\n      2 td_index = [i for i, x in enumerate(td_index) if x] \n      3 print('number of duplicate questions: ', len(td_index))\n\nNameError: name 'data_train' is not defined\n\n\n\n\nprint(data_train['question1'][5])  #  Example of question duplicates (first one in data)\nprint(data_train['question2'][5])\nprint('is_duplicate: ', data_train['is_duplicate'][5])\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[5], line 1\n----&gt; 1 print(data_train['question1'][5])  #  Example of question duplicates (first one in data)\n      2 print(data_train['question2'][5])\n      3 print('is_duplicate: ', data_train['is_duplicate'][5])\n\nNameError: name 'data_train' is not defined\n\n\n\n\nQ1_train_words = np.array(data_train['question1'][td_index])\nQ2_train_words = np.array(data_train['question2'][td_index])\n\nQ1_test_words = np.array(data_test['question1'])\nQ2_test_words = np.array(data_test['question2'])\ny_test  = np.array(data_test['is_duplicate'])\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[6], line 1\n----&gt; 1 Q1_train_words = np.array(data_train['question1'][td_index])\n      2 Q2_train_words = np.array(data_train['question2'][td_index])\n      4 Q1_test_words = np.array(data_test['question1'])\n\nNameError: name 'data_train' is not defined\n\n\n\nAbove, you have seen that you only took the duplicated questions for training our model. You did so on purpose, because the data generator will produce batches ([q1_1, q1_2, q1_3, ...], [q2_1, q2_2,q2_3, ...]) where q1_i and q2_k are duplicate if and only if i = k.\nLet‚Äôs print to see what your data looks like.\n\nprint('TRAINING QUESTIONS:\\n')\nprint('Question 1: ', Q1_train_words[0])\nprint('Question 2: ', Q2_train_words[0], '\\n')\nprint('Question 1: ', Q1_train_words[5])\nprint('Question 2: ', Q2_train_words[5], '\\n')\n\nprint('TESTING QUESTIONS:\\n')\nprint('Question 1: ', Q1_test_words[0])\nprint('Question 2: ', Q2_test_words[0], '\\n')\nprint('is_duplicate =', y_test[0], '\\n')\n\nTRAINING QUESTIONS:\n\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[7], line 2\n      1 print('TRAINING QUESTIONS:\\n')\n----&gt; 2 print('Question 1: ', Q1_train_words[0])\n      3 print('Question 2: ', Q2_train_words[0], '\\n')\n      4 print('Question 1: ', Q1_train_words[5])\n\nNameError: name 'Q1_train_words' is not defined\n\n\n\nYou will now encode each word of the selected duplicate pairs with an index.  Given a question, you can then just encode it as a list of numbers.\nFirst you tokenize the questions using nltk.word_tokenize.  You need a python default dictionary which later, during inference, assigns the values 0 to all Out Of Vocabulary (OOV) words. Then you encode each word of the selected duplicate pairs with an index. Given a question, you can then just encode it as a list of numbers.\n\n#create arrays\nQ1_train = np.empty_like(Q1_train_words)\nQ2_train = np.empty_like(Q2_train_words)\n\nQ1_test = np.empty_like(Q1_test_words)\nQ2_test = np.empty_like(Q2_test_words)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[8], line 2\n      1 #create arrays\n----&gt; 2 Q1_train = np.empty_like(Q1_train_words)\n      3 Q2_train = np.empty_like(Q2_train_words)\n      5 Q1_test = np.empty_like(Q1_test_words)\n\nNameError: name 'Q1_train_words' is not defined\n\n\n\n\n# Building the vocabulary with the train set         (this might take a minute)\nfrom collections import defaultdict\n\nvocab = defaultdict(lambda: 0)\nvocab['&lt;PAD&gt;'] = 1\n\nfor idx in range(len(Q1_train_words)):\n    Q1_train[idx] = nltk.word_tokenize(Q1_train_words[idx])\n    Q2_train[idx] = nltk.word_tokenize(Q2_train_words[idx])\n    q = Q1_train[idx] + Q2_train[idx]\n    for word in q:\n        if word not in vocab:\n            vocab[word] = len(vocab) + 1\nprint('The length of the vocabulary is: ', len(vocab))\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[9], line 7\n      4 vocab = defaultdict(lambda: 0)\n      5 vocab['&lt;PAD&gt;'] = 1\n----&gt; 7 for idx in range(len(Q1_train_words)):\n      8     Q1_train[idx] = nltk.word_tokenize(Q1_train_words[idx])\n      9     Q2_train[idx] = nltk.word_tokenize(Q2_train_words[idx])\n\nNameError: name 'Q1_train_words' is not defined\n\n\n\n\nprint(vocab['&lt;PAD&gt;'])\nprint(vocab['Astrology'])\nprint(vocab['Astronomy'])  #not in vocabulary, returns 0\n\n1\n0\n0\n\n\n\nfor idx in range(len(Q1_test_words)): \n    Q1_test[idx] = nltk.word_tokenize(Q1_test_words[idx])\n    Q2_test[idx] = nltk.word_tokenize(Q2_test_words[idx])\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[11], line 1\n----&gt; 1 for idx in range(len(Q1_test_words)): \n      2     Q1_test[idx] = nltk.word_tokenize(Q1_test_words[idx])\n      3     Q2_test[idx] = nltk.word_tokenize(Q2_test_words[idx])\n\nNameError: name 'Q1_test_words' is not defined\n\n\n\n\nprint('Train set has reduced to: ', len(Q1_train) ) \nprint('Test set length: ', len(Q1_test) ) \n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[12], line 1\n----&gt; 1 print('Train set has reduced to: ', len(Q1_train) ) \n      2 print('Test set length: ', len(Q1_test) ) \n\nNameError: name 'Q1_train' is not defined\n\n\n\n ### 1.2 Converting a question to a tensor\nYou will now convert every question to a tensor, or an array of numbers, using your vocabulary built above.\n\n# Converting questions to array of integers\nfor i in range(len(Q1_train)):\n    Q1_train[i] = [vocab[word] for word in Q1_train[i]]\n    Q2_train[i] = [vocab[word] for word in Q2_train[i]]\n\n        \nfor i in range(len(Q1_test)):\n    Q1_test[i] = [vocab[word] for word in Q1_test[i]]\n    Q2_test[i] = [vocab[word] for word in Q2_test[i]]\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[13], line 2\n      1 # Converting questions to array of integers\n----&gt; 2 for i in range(len(Q1_train)):\n      3     Q1_train[i] = [vocab[word] for word in Q1_train[i]]\n      4     Q2_train[i] = [vocab[word] for word in Q2_train[i]]\n\nNameError: name 'Q1_train' is not defined\n\n\n\n\nprint('first question in the train set:\\n')\nprint(Q1_train_words[0], '\\n') \nprint('encoded version:')\nprint(Q1_train[0],'\\n')\n\nprint('first question in the test set:\\n')\nprint(Q1_test_words[0], '\\n')\nprint('encoded version:')\nprint(Q1_test[0]) \n\nfirst question in the train set:\n\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[14], line 2\n      1 print('first question in the train set:\\n')\n----&gt; 2 print(Q1_train_words[0], '\\n') \n      3 print('encoded version:')\n      4 print(Q1_train[0],'\\n')\n\nNameError: name 'Q1_train_words' is not defined\n\n\n\nYou will now split your train set into a training/validation set so that you can use it to train and evaluate your Siamese model.\n\n# Splitting the data\ncut_off = int(len(Q1_train)*.8)\ntrain_Q1, train_Q2 = Q1_train[:cut_off], Q2_train[:cut_off]\nval_Q1, val_Q2 = Q1_train[cut_off: ], Q2_train[cut_off:]\nprint('Number of duplicate questions: ', len(Q1_train))\nprint(\"The length of the training set is:  \", len(train_Q1))\nprint(\"The length of the validation set is: \", len(val_Q1))\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[15], line 2\n      1 # Splitting the data\n----&gt; 2 cut_off = int(len(Q1_train)*.8)\n      3 train_Q1, train_Q2 = Q1_train[:cut_off], Q2_train[:cut_off]\n      4 val_Q1, val_Q2 = Q1_train[cut_off: ], Q2_train[cut_off:]\n\nNameError: name 'Q1_train' is not defined\n\n\n\n ### 1.3 Understanding the iterator\nMost of the time in Natural Language Processing, and AI in general we use batches when training our data sets. If you were to use stochastic gradient descent with one example at a time, it will take you forever to build a model. In this example, we show you how you can build a data generator that takes in Q1 and Q2 and returns a batch of size batch_size in the following format ([q1_1, q1_2, q1_3, ...], [q2_1, q2_2,q2_3, ...]). The tuple consists of two arrays and each array has batch_size questions. Again, q1_i and q2_i are duplicates, but they are not duplicates with any other elements in the batch.\n\nThe command next(data_generator)returns the next batch. This iterator returns the data in a format that you could directly use in your model when computing the feed-forward of your algorithm. This iterator returns a pair of arrays of questions.\n ### Exercise 01\nInstructions:\nImplement the data generator below. Here are some things you will need.\n\nWhile true loop.\nif index &gt;= len_Q1, set the idx to 0.\nThe generator should return shuffled batches of data. To achieve this without modifying the actual question lists, a list containing the indexes of the questions is created. This list can be shuffled and used to get random batches everytime the index is reset.\nAppend elements of Q1 and Q2 to input1 and input2 respectively.\nif len(input1) == batch_size, determine max_len as the longest question in input1 and input2. Ceil max_len to a power of 2 (for computation purposes) using the following command: max_len = 2**int(np.ceil(np.log2(max_len))).\nPad every question by vocab['&lt;PAD&gt;'] until you get the length max_len.\nUse yield to return input1, input2.\nDon‚Äôt forget to reset input1, input2 to empty arrays at the end (data generator resumes from where it last left).\n\n\n# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: data_generator\ndef data_generator(Q1, Q2, batch_size, pad=1, shuffle=True):\n    \"\"\"Generator function that yields batches of data\n\n    Args:\n        Q1 (list): List of transformed (to tensor) questions.\n        Q2 (list): List of transformed (to tensor) questions.\n        batch_size (int): Number of elements per batch.\n        pad (int, optional): Pad character from the vocab. Defaults to 1.\n        shuffle (bool, optional): If the batches should be randomnized or not. Defaults to True.\n    Yields:\n        tuple: Of the form (input1, input2) with types (numpy.ndarray, numpy.ndarray)\n        NOTE: input1: inputs to your model [q1a, q2a, q3a, ...] i.e. (q1a,q1b) are duplicates\n              input2: targets to your model [q1b, q2b,q3b, ...] i.e. (q1a,q2i) i!=a are not duplicates\n    \"\"\"\n\n    input1 = []\n    input2 = []\n    idx = 0\n    len_q = len(Q1)\n    question_indexes = [*range(len_q)]\n    \n    if shuffle:\n        rnd.shuffle(question_indexes)\n    \n    ### START CODE HERE (Replace instances of 'None' with your code) ###\n    while True:\n        if idx &gt;= len_q:\n            # if idx is greater than or equal to len_q, set idx accordingly \n            # (Hint: look at the instructions above)\n            idx = None\n            # shuffle to get random batches if shuffle is set to True\n            if shuffle:\n                rnd.shuffle(question_indexes)\n        \n        # get questions at the `question_indexes[idx]` position in Q1 and Q2\n        q1 = None\n        q2 = None\n        \n        # increment idx by 1\n        idx += None\n        # append q1\n        input1.append(None)\n        # append q2\n        input2.append(None)\n        if len(input1) == batch_size:\n            # determine max_len as the longest question in input1 & input 2\n            # Hint: use the `max` function. \n            # take max of input1 & input2 and then max out of the two of them.\n            max_len = None\n            # pad to power-of-2 (Hint: look at the instructions above)\n            max_len = None\n            b1 = []\n            b2 = []\n            for q1, q2 in zip(input1, input2):\n                # add [pad] to q1 until it reaches max_len\n                q1 = None\n                # add [pad] to q2 until it reaches max_len\n                q2 = None\n                # append q1\n                b1.append(None)\n                # append q2\n                b2.append(None)\n            # use b1 and b2\n            yield np.array(None), np.array(None)\n    ### END CODE HERE ###\n            # reset the batches\n            input1, input2 = [], []  # reset the batches\n\n\nbatch_size = 2\nres1, res2 = next(data_generator(train_Q1, train_Q2, batch_size))\nprint(\"First questions  : \",'\\n', res1, '\\n')\nprint(\"Second questions : \",'\\n', res2)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[17], line 2\n      1 batch_size = 2\n----&gt; 2 res1, res2 = next(data_generator(train_Q1, train_Q2, batch_size))\n      3 print(\"First questions  : \",'\\n', res1, '\\n')\n      4 print(\"Second questions : \",'\\n', res2)\n\nNameError: name 'train_Q1' is not defined\n\n\n\nNote: The following expected output is valid only if you run the above test cell once (first time). The output will change on each execution.\nIf you think your implementation is correct and it is not matching the output, make sure to restart the kernel and run all the cells from the top again.\nExpected Output:\nFirst questions  :  \n [[  30   87   78  134 2132 1981   28   78  594   21    1    1    1    1\n     1    1]\n [  30   55   78 3541 1460   28   56  253   21    1    1    1    1    1\n     1    1]] \n\nSecond questions :  \n [[  30  156   78  134 2132 9508   21    1    1    1    1    1    1    1\n     1    1]\n [  30  156   78 3541 1460  131   56  253   21    1    1    1    1    1\n     1    1]]\nNow that you have your generator, you can just call it and it will return tensors which correspond to your questions in the Quora data set.Now you can go ahead and start building your neural network.\n # Part 2: Defining the Siamese model\n\n\n\n2.1 Understanding Siamese Network\nA Siamese network is a neural network which uses the same weights while working in tandem on two different input vectors to compute comparable output vectors.The Siamese network you are about to implement looks like this:\n\nYou get the question embedding, run it through an LSTM layer, normalize v_1 and v_2, and finally use a triplet loss (explained below) to get the corresponding cosine similarity for each pair of questions. As usual, you will start by importing the data set. The triplet loss makes use of a baseline (anchor) input that is compared to a positive (truthy) input and a negative (falsy) input. The distance from the baseline (anchor) input to the positive (truthy) input is minimized, and the distance from the baseline (anchor) input to the negative (falsy) input is maximized. In math equations, you are trying to maximize the following.\n\\mathcal{L}(A, P, N)=\\max \\left(\\|\\mathrm{f}(A)-\\mathrm{f}(P)\\|^{2}-\\|\\mathrm{f}(A)-\\mathrm{f}(N)\\|^{2}+\\alpha, 0\\right)\nA is the anchor input, for example q1_1, P the duplicate input, for example, q2_1, and N the negative input (the non duplicate question), for example q2_2. \\alpha is a margin; you can think about it as a safety net, or by how much you want to push the duplicates from the non duplicates. \n ### Exercise 02\nInstructions: Implement the Siamese function below. You should be using all the objects explained below.\nTo implement this model, you will be using trax. Concretely, you will be using the following functions.\n\ntl.Serial: Combinator that applies layers serially (by function composition) allows you set up the overall structure of the feedforward. docs / source code\n\nYou can pass in the layers as arguments to Serial, separated by commas.\nFor example: tl.Serial(tl.Embeddings(...), tl.Mean(...), tl.Dense(...), tl.LogSoftmax(...))\n\ntl.Embedding: Maps discrete tokens to vectors. It will have shape (vocabulary length X dimension of output vectors). The dimension of output vectors (also called d_feature) is the number of elements in the word embedding. docs / source code\n\ntl.Embedding(vocab_size, d_feature).\nvocab_size is the number of unique words in the given vocabulary.\nd_feature is the number of elements in the word embedding (some choices for a word embedding size range from 150 to 300, for example).\n\ntl.LSTM The LSTM layer. It leverages another Trax layer called LSTMCell. The number of units should be specified and should match the number of elements in the word embedding. docs / source code\n\ntl.LSTM(n_units) Builds an LSTM layer of n_units.\n\ntl.Mean: Computes the mean across a desired axis. Mean uses one tensor axis to form groups of values and replaces each group with the mean value of that group. docs / source code\n\ntl.Mean(axis=1) mean over columns.\n\ntl.Fn Layer with no weights that applies the function f, which should be specified using a lambda syntax. docs / source doce\n\nx -&gt; This is used for cosine similarity.\ntl.Fn('Normalize', lambda x: normalize(x)) Returns a layer with no weights that applies the function f\n\ntl.parallel: It is a combinator layer (like Serial) that applies a list of layers in parallel to its inputs. docs / source code\n\n\n# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: Siamese\ndef Siamese(vocab_size=len(vocab), d_model=128, mode='train'):\n    \"\"\"Returns a Siamese model.\n\n    Args:\n        vocab_size (int, optional): Length of the vocabulary. Defaults to len(vocab).\n        d_model (int, optional): Depth of the model. Defaults to 128.\n        mode (str, optional): 'train', 'eval' or 'predict', predict mode is for fast inference. Defaults to 'train'.\n\n    Returns:\n        trax.layers.combinators.Parallel: A Siamese model. \n    \"\"\"\n\n    def normalize(x):  # normalizes the vectors to have L2 norm 1\n        return x / fastnp.sqrt(fastnp.sum(x * x, axis=-1, keepdims=True))\n    \n    ### START CODE HERE (Replace instances of 'None' with your code) ###\n    q_processor = tl.Serial(  # Processor will run on Q1 and Q2.\n        None, # Embedding layer\n        None, # LSTM layer\n        None, # Mean over columns\n        None  # Apply normalize function\n    )  # Returns one vector of shape [batch_size, d_model].\n    \n    ### END CODE HERE ###\n    \n    # Run on Q1 and Q2 in parallel.\n    model = tl.Parallel(q_processor, q_processor)\n    return model\n\nSetup the Siamese network model\n\n# check your model\nmodel = Siamese()\nprint(model)\n\n/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/combinators.py:437: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n  if self._mode == 'predict' and self._state[1] is not ():  # pylint: disable=literal-comparison\n/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/combinators.py:910: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n  if state[0] is ():  # pylint: disable=literal-comparison\n/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/combinators.py:437: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n  if self._mode == 'predict' and self._state[1] is not ():  # pylint: disable=literal-comparison\n/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/combinators.py:910: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n  if state[0] is ():  # pylint: disable=literal-comparison\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[19], line 2\n      1 # check your model\n----&gt; 2 model = Siamese()\n      3 print(model)\n\nCell In[18], line 19, in Siamese(vocab_size, d_model, mode)\n     16     return x / fastnp.sqrt(fastnp.sum(x * x, axis=-1, keepdims=True))\n     18 ### START CODE HERE (Replace instances of 'None' with your code) ###\n---&gt; 19 q_processor = tl.Serial(  # Processor will run on Q1 and Q2.\n     20     None, # Embedding layer\n     21     None, # LSTM layer\n     22     None, # Mean over columns\n     23     None  # Apply normalize function\n     24 )  # Returns one vector of shape [batch_size, d_model].\n     26 ### END CODE HERE ###\n     27 \n     28 # Run on Q1 and Q2 in parallel.\n     29 model = tl.Parallel(q_processor, q_processor)\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/combinators.py:59, in Serial.__init__(self, name, sublayers_to_print, *sublayers)\n     55 def __init__(self, *sublayers, name=None, sublayers_to_print=None):\n     56   super().__init__(\n     57       name=name, sublayers_to_print=sublayers_to_print)\n---&gt; 59   sublayers = _ensure_flat(sublayers)\n     60   self._sublayers = sublayers\n     61   self._n_layers = len(sublayers)\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/combinators.py:1110, in _ensure_flat(layers)\n   1108 for obj in layers:\n   1109   if not isinstance(obj, base.Layer):\n-&gt; 1110     raise ValueError(\n   1111         f'Found nonlayer object ({obj}) in layers: {layers}')\n   1112 return layers\n\nValueError: Found nonlayer object (None) in layers: [None, None, None, None]\n\n\n\nExpected output:\nParallel_in2_out2[\n  Serial[\n    Embedding_41699_128\n    LSTM_128\n    Mean\n    Normalize\n  ]\n  Serial[\n    Embedding_41699_128\n    LSTM_128\n    Mean\n    Normalize\n  ]\n]\n\n\n\n2.2 Hard Negative Mining\nYou will now implement the TripletLoss. As explained in the lecture, loss is composed of two terms. One term utilizes the mean of all the non duplicates, the second utilizes the closest negative. Our loss expression is then:\n\\begin{align}\n\\mathcal{Loss_1(A,P,N)} &=\\max \\left( -cos(A,P)  + mean_{neg} +\\alpha, 0\\right) \\\\\n\\mathcal{Loss_2(A,P,N)} &=\\max \\left( -cos(A,P)  + closest_{neg} +\\alpha, 0\\right) \\\\\n\\mathcal{Loss(A,P,N)} &= mean(Loss_1 + Loss_2) \\\\\n\\end{align}\nFurther, two sets of instructions are provided. The first set provides a brief description of the task. If that set proves insufficient, a more detailed set can be displayed.\n ### Exercise 03\nInstructions (Brief): Here is a list of things you should do: \n\nAs this will be run inside trax, use fastnp.xyz when using any xyz numpy function\nUse fastnp.dot to calculate the similarity matrix v_1v_2^T of dimension batch_size x batch_size\nTake the score of the duplicates on the diagonal fastnp.diagonal\nUse the trax functions fastnp.eye and fastnp.maximum for the identity matrix and the maximum.\n\n\n\nMore Detailed Instructions \n\nWe‚Äôll describe the algorithm using a detailed example. Below, V1, V2 are the output of the normalization blocks in our model. Here we will use a batch_size of 4 and a d_model of 3. As explained in lecture, the inputs, Q1, Q2 are arranged so that corresponding inputs are duplicates while non-corresponding entries are not. The outputs will have the same pattern.  This testcase arranges the outputs, v1,v2, to highlight different scenarios. Here, the first two outputs V1[0], V2[0] match exactly - so the model is generating the same vector for Q1[0] and Q2[0] inputs. The second outputs differ, circled in orange, we set, V2[1] is set to match V2[2], simulating a model which is generating very poor results. V1[3] and V2[3] match exactly again while V1[4] and V2[4] are set to be exactly wrong - 180 degrees from each other, circled in blue.\nThe first step is to compute the cosine similarity matrix or score in the code. As explained in lecture, this is V_1 V_2^T This is generated with fastnp.dot.  The clever arrangement of inputs creates the data needed for positive and negative examples without having to run all pair-wise combinations. Because Q1[n] is a duplicate of only Q2[n], other combinations are explicitly created negative examples or Hard Negative examples. The matrix multiplication efficiently produces the cosine similarity of all positive/negative combinations as shown above on the left side of the diagram. ‚ÄòPositive‚Äô are the results of duplicate examples and ‚Äònegative‚Äô are the results of explicitly created negative examples. The results for our test case are as expected, V1[0]V2[0] match producing ‚Äò1‚Äô while our other ‚Äòpositive‚Äô cases (in green) don‚Äôt match well, as was arranged. The V2[2] was set to match V1[3] producing a poor match at score[2,2] and an undesired ‚Äònegative‚Äô case of a ‚Äò1‚Äô shown in grey.\nWith the similarity matrix (score) we can begin to implement the loss equations. First, we can extract cos(A,P) by utilizing fastnp.diagonal. The goal is to grab all the green entries in the diagram above. This is positive in the code.\nNext, we will create the closest_negative. This is the nonduplicate entry in V2 that is closest (has largest cosine similarity) to an entry in V1. Each row, n, of score represents all comparisons of the results of Q1[n] vs Q2[x] within a batch. A specific example in our testcase is row score[2,:]. It has the cosine similarity of V1[2] and V2[x]. The closest_negative, as was arranged, is V2[2] which has a score of 1. This is the maximum value of the ‚Äònegative‚Äô entries (blue entries in the diagram).\nTo implement this, we need to pick the maximum entry on a row of score, ignoring the ‚Äòpositive‚Äô/green entries. To avoid selecting the ‚Äòpositive‚Äô/green entries, we can make them larger negative numbers. Multiply fastnp.eye(batch_size) with 2.0 and subtract it out of scores. The result is negative_without_positive. Now we can use fastnp.max, row by row (axis=1), to select the maximum which is closest_negative.\nNext, we‚Äôll create mean_negative. As the name suggests, this is the mean of all the ‚Äònegative‚Äô/blue values in score on a row by row basis. We can use fastnp.eye(batch_size) and a constant, this time to create a mask with zeros on the diagonal. Element-wise multiply this with score to get just the ‚Äônegative values. This is negative_zero_on_duplicate in the code. Compute the mean by using fastnp.sum on negative_zero_on_duplicate for axis=1 and divide it by (batch_size - 1) . This is mean_negative.\nNow, we can compute loss using the two equations above and fastnp.maximum. This will form triplet_loss1 and triplet_loss2.\ntriple_loss is the fastnp.mean of the sum of the two individual losses.\nOnce you have this code matching the expected results, you can clip out the section between ### START CODE HERE and ### END CODE HERE it out and insert it into TripletLoss below.\n&lt;&gt;\n\n# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: TripletLossFn\ndef TripletLossFn(v1, v2, margin=0.25):\n    \"\"\"Custom Loss function.\n\n    Args:\n        v1 (numpy.ndarray): Array with dimension (batch_size, model_dimension) associated to Q1.\n        v2 (numpy.ndarray): Array with dimension (batch_size, model_dimension) associated to Q2.\n        margin (float, optional): Desired margin. Defaults to 0.25.\n\n    Returns:\n        jax.interpreters.xla.DeviceArray: Triplet Loss.\n    \"\"\"\n    ### START CODE HERE (Replace instances of 'None' with your code) ###\n    \n    # use fastnp to take the dot product of the two batches (don't forget to transpose the second argument)\n    scores = None  # pairwise cosine sim\n    # calculate new batch size\n    batch_size = len(scores)\n    # use fastnp to grab all postive `diagonal` entries in `scores`\n    positive = None  # the positive ones (duplicates)\n    # multiply `fastnp.eye(batch_size)` with 2.0 and subtract it out of `scores`\n    negative_without_positive = None\n    # take the row by row `max` of `negative_without_positive`. \n    # Hint: negative_without_positive.max(axis = [?])  \n    closest_negative = None\n    # subtract `fastnp.eye(batch_size)` out of 1.0 and do element-wise multiplication with `scores`\n    negative_zero_on_duplicate = None\n    # use `fastnp.sum` on `negative_zero_on_duplicate` for `axis=1` and divide it by `(batch_size - 1)` \n    mean_negative = None\n    # compute `fastnp.maximum` among 0.0 and `A`\n    # A = subtract `positive` from `margin` and add `closest_negative` \n    triplet_loss1 = None\n    # compute `fastnp.maximum` among 0.0 and `B`\n    # B = subtract `positive` from `margin` and add `mean_negative`\n    triplet_loss2 = None\n    # add the two losses together and take the `fastnp.mean` of it\n    triplet_loss = None\n    \n    ### END CODE HERE ###\n    \n    return triplet_loss\n\n\nv1 = np.array([[0.26726124, 0.53452248, 0.80178373],[0.5178918 , 0.57543534, 0.63297887]])\nv2 = np.array([[ 0.26726124,  0.53452248,  0.80178373],[-0.5178918 , -0.57543534, -0.63297887]])\nTripletLossFn(v2,v1)\nprint(\"Triplet Loss:\", TripletLossFn(v2,v1))\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[21], line 3\n      1 v1 = np.array([[0.26726124, 0.53452248, 0.80178373],[0.5178918 , 0.57543534, 0.63297887]])\n      2 v2 = np.array([[ 0.26726124,  0.53452248,  0.80178373],[-0.5178918 , -0.57543534, -0.63297887]])\n----&gt; 3 TripletLossFn(v2,v1)\n      4 print(\"Triplet Loss:\", TripletLossFn(v2,v1))\n\nCell In[20], line 19, in TripletLossFn(v1, v2, margin)\n     17 scores = None  # pairwise cosine sim\n     18 # calculate new batch size\n---&gt; 19 batch_size = len(scores)\n     20 # use fastnp to grab all postive `diagonal` entries in `scores`\n     21 positive = None  # the positive ones (duplicates)\n\nTypeError: object of type 'NoneType' has no len()\n\n\n\nExpected Output:\nTriplet Loss: 0.5\nTo make a layer out of a function with no trainable variables, use tl.Fn.\n\nfrom functools import partial\ndef TripletLoss(margin=0.25):\n    triplet_loss_fn = partial(TripletLossFn, margin=margin)\n    return tl.Fn('TripletLoss', triplet_loss_fn)"
  },
  {
    "objectID": "posts/c3w4/lab01.html",
    "href": "posts/c3w4/lab01.html",
    "title": "Creating a Siamese model using Trax: Ungraded Lecture Notebook",
    "section": "",
    "text": "course banner\nimport trax\nfrom trax import layers as tl\nimport trax.fastmath.numpy as np\nimport numpy\n\n# Setting random seeds\n# set random seeds to make this notebook easier to replicate\nfrom trax import fastmath\nseed=10\nrng = fastmath.random.get_prng(seed)\n#trax.supervised.trainer_lib.init_random_number_generators(10)\nnumpy.random.seed(seed)\n\n2025-02-05 21:57:27.336143: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1738785447.353122  615059 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1738785447.357758  615059 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered"
  },
  {
    "objectID": "posts/c3w4/lab01.html#l2-normalization",
    "href": "posts/c3w4/lab01.html#l2-normalization",
    "title": "Creating a Siamese model using Trax: Ungraded Lecture Notebook",
    "section": "L2 Normalization",
    "text": "L2 Normalization\nBefore building the model you will need to define a function that applies L2 normalization to a tensor. This is very important because in this week‚Äôs assignment you will create a custom loss function which expects the tensors it receives to be normalized. Luckily this is pretty straightforward:\n\ndef normalize(x):\n    return x / np.sqrt(np.sum(x * x, axis=-1, keepdims=True))\n\nNotice that the denominator can be replaced by np.linalg.norm(x, axis=-1, keepdims=True) to achieve the same results and that Trax‚Äôs numpy is being used within the function.\n\ntensor = numpy.random.random((2,5))\nprint(f'The tensor is of type: {type(tensor)}\\n\\nAnd looks like this:\\n\\n {tensor}')\n\nThe tensor is of type: &lt;class 'numpy.ndarray'&gt;\n\nAnd looks like this:\n\n [[0.77132064 0.02075195 0.63364823 0.74880388 0.49850701]\n [0.22479665 0.19806286 0.76053071 0.16911084 0.08833981]]\n\n\n\nnorm_tensor = normalize(tensor)\nprint(f'The normalized tensor is of type: {type(norm_tensor)}\\n\\nAnd looks like this:\\n\\n {norm_tensor}')\n\nThe normalized tensor is of type: &lt;class 'jaxlib.xla_extension.ArrayImpl'&gt;\n\nAnd looks like this:\n\n [[0.5739379  0.01544148 0.4714962  0.5571832  0.37093794]\n [0.26781026 0.23596111 0.9060541  0.20146926 0.10524315]]\n\n\nNotice that the initial tensor was converted from a numpy array to a jax array in the process."
  },
  {
    "objectID": "posts/c3w4/lab01.html#siamese-model",
    "href": "posts/c3w4/lab01.html#siamese-model",
    "title": "Creating a Siamese model using Trax: Ungraded Lecture Notebook",
    "section": "Siamese Model",
    "text": "Siamese Model\nTo create a Siamese model you will first need to create a LSTM model using the Serial combinator layer and then use another combinator layer called Parallel to create the Siamese model. You should be familiar with the following layers (notice each layer can be clicked to go to the docs): - Serial A combinator layer that allows to stack layers serially using function composition. - Embedding Maps discrete tokens to vectors. It will have shape (vocabulary length X dimension of output vectors). The dimension of output vectors (also called d_feature) is the number of elements in the word embedding. - LSTM The LSTM layer. It leverages another Trax layer called LSTMCell. The number of units should be specified and should match the number of elements in the word embedding. - Mean Computes the mean across a desired axis. Mean uses one tensor axis to form groups of values and replaces each group with the mean value of that group. - Fn Layer with no weights that applies the function f, which should be specified using a lambda syntax. - Parallel It is a combinator layer (like Serial) that applies a list of layers in parallel to its inputs.\nPutting everything together the Siamese model will look like this:\n\nvocab_size = 500\nmodel_dimension = 128\n\n# Define the LSTM model\nLSTM = tl.Serial(\n        tl.Embedding(vocab_size=vocab_size, d_feature=model_dimension),\n        tl.LSTM(model_dimension),\n        tl.Mean(axis=1),\n        tl.Fn('Normalize', lambda x: normalize(x))\n    )\n\n# Use the Parallel combinator to create a Siamese model out of the LSTM \nSiamese = tl.Parallel(LSTM, LSTM)\n\nNext is a helper function that prints information for every layer (sublayer within Serial):\n\ndef show_layers(model, layer_prefix):\n    print(f\"Total layers: {len(model.sublayers)}\\n\")\n    for i in range(len(model.sublayers)):\n        print('========')\n        print(f'{layer_prefix}_{i}: {model.sublayers[i]}\\n')\n\nprint('Siamese model:\\n')\nshow_layers(Siamese, 'Parallel.sublayers')\n\nprint('Detail of LSTM models:\\n')\nshow_layers(LSTM, 'Serial.sublayers')\n\nSiamese model:\n\nTotal layers: 2\n\n========\nParallel.sublayers_0: Serial[\n  Embedding_500_128\n  LSTM_128\n  Mean\n  Normalize\n]\n\n========\nParallel.sublayers_1: Serial[\n  Embedding_500_128\n  LSTM_128\n  Mean\n  Normalize\n]\n\nDetail of LSTM models:\n\nTotal layers: 4\n\n========\nSerial.sublayers_0: Embedding_500_128\n\n========\nSerial.sublayers_1: LSTM_128\n\n========\nSerial.sublayers_2: Mean\n\n========\nSerial.sublayers_3: Normalize\n\n\n\nTry changing the parameters defined before the Siamese model and see how it changes!\nYou will actually train this model in this week‚Äôs assignment. For now you should be more familiarized with creating Siamese models using Trax.\nKeep it up!"
  },
  {
    "objectID": "posts/c3w4/lab02.html",
    "href": "posts/c3w4/lab02.html",
    "title": "Modified Triplet Loss : Ungraded Lecture Notebook",
    "section": "",
    "text": "In this notebook you‚Äôll see how to calculate the full triplet loss, step by step, including the mean negative and the closest negative. You‚Äôll also calculate the matrix of similarity scores."
  },
  {
    "objectID": "posts/c3w4/lab02.html#background",
    "href": "posts/c3w4/lab02.html#background",
    "title": "Modified Triplet Loss : Ungraded Lecture Notebook",
    "section": "Background",
    "text": "Background\nThis is the original triplet loss function:\n\\mathcal{L_\\mathrm{Original}} = \\max{(\\mathrm{s}(A,N) -\\mathrm{s}(A,P) +\\alpha, 0)}\nIt can be improved by including the mean negative and the closest negative, to create a new full loss function. The inputs are the Anchor \\mathrm{A}, Positive \\mathrm{P} and Negative \\mathrm{N}.\n\\mathcal{L_\\mathrm{1}} = \\max{(mean\\_neg -\\mathrm{s}(A,P)  +\\alpha, 0)}\n\\mathcal{L_\\mathrm{2}} = \\max{(closest\\_neg -\\mathrm{s}(A,P)  +\\alpha, 0)}\n\\mathcal{L_\\mathrm{Full}} = \\mathcal{L_\\mathrm{1}} + \\mathcal{L_\\mathrm{2}}\nLet me show you what that means exactly, and how to calculate each step."
  },
  {
    "objectID": "posts/c3w4/lab02.html#imports",
    "href": "posts/c3w4/lab02.html#imports",
    "title": "Modified Triplet Loss : Ungraded Lecture Notebook",
    "section": "Imports",
    "text": "Imports\n\nimport numpy as np"
  },
  {
    "objectID": "posts/c3w4/lab02.html#similarity-scores",
    "href": "posts/c3w4/lab02.html#similarity-scores",
    "title": "Modified Triplet Loss : Ungraded Lecture Notebook",
    "section": "Similarity Scores",
    "text": "Similarity Scores\nThe first step is to calculate the matrix of similarity scores using cosine similarity so that you can look up \\mathrm{s}(A,P), \\mathrm{s}(A,N) as needed for the loss formulas.\n\nTwo Vectors\nFirst I‚Äôll show you how to calculate the similarity score, using cosine similarity, for 2 vectors.\n\\mathrm{s}(v_1,v_2) = \\mathrm{cosine \\ similarity}(v_1,v_2) = \\frac{v_1 \\cdot v_2}{||v_1||~||v_2||} * Try changing the values in the second vector to see how it changes the cosine similarity.\n\n# Two vector example\n# Input data\nprint(\"-- Inputs --\")\nv1 = np.array([1, 2, 3], dtype=float)\nv2 = np.array([1, 2, 3.5])  # notice the 3rd element is offset by 0.5\n### START CODE HERE ###\n# Try modifying the vector v2 to see how it impacts the cosine similarity\n# v2 = v1                   # identical vector\n# v2 = v1 * -1              # opposite vector\n# v2 = np.array([0,-42,1])  # random example\n### END CODE HERE ###\nprint(\"v1 :\", v1)\nprint(\"v2 :\", v2, \"\\n\")\n\n# Similarity score\ndef cosine_similarity(v1, v2):\n    numerator = np.dot(v1, v2)\n    denominator = np.sqrt(np.dot(v1, v1)) * np.sqrt(np.dot(v2, v2))\n    return numerator / denominator\n\nprint(\"-- Outputs --\")\nprint(\"cosine similarity :\", cosine_similarity(v1, v2))\n\n-- Inputs --\nv1 : [1. 2. 3.]\nv2 : [1.  2.  3.5] \n\n-- Outputs --\ncosine similarity : 0.9974086507360697\n\n\n\n\nTwo Batches of Vectors\nNow i‚Äôll show you how to calculate the similarity scores, using cosine similarity, for 2 batches of vectors. These are rows of individual vectors, just like in the example above, but stacked vertically into a matrix. They would look like the image below for a batch size (row count) of 4 and embedding size (column count) of 5.\nThe data is setup so that v_{1\\_1} and v_{2\\_1} represent duplicate inputs, but they are not duplicates with any other rows in the batch. This means v_{1\\_1} and v_{2\\_1} (green and green) have more similar vectors than say v_{1\\_1} and v_{2\\_2} (green and magenta).\nI‚Äôll show you two different methods for calculating the matrix of similarities from 2 batches of vectors.\n\n\n# Two batches of vectors example\n# Input data\nprint(\"-- Inputs --\")\nv1_1 = np.array([1, 2, 3])\nv1_2 = np.array([9, 8, 7])\nv1_3 = np.array([-1, -4, -2])\nv1_4 = np.array([1, -7, 2])\nv1 = np.vstack([v1_1, v1_2, v1_3, v1_4])\nprint(\"v1 :\")\nprint(v1, \"\\n\")\nv2_1 = v1_1 + np.random.normal(0, 2, 3)  # add some noise to create approximate duplicate\nv2_2 = v1_2 + np.random.normal(0, 2, 3)\nv2_3 = v1_3 + np.random.normal(0, 2, 3)\nv2_4 = v1_4 + np.random.normal(0, 2, 3)\nv2 = np.vstack([v2_1, v2_2, v2_3, v2_4])\nprint(\"v2 :\")\nprint(v2, \"\\n\")\n\n# Batch sizes must match\nb = len(v1)\nprint(\"batch sizes match :\", b == len(v2), \"\\n\")\n\n# Similarity scores\nprint(\"-- Outputs --\")\n# Option 1 : nested loops and the cosine similarity function\nsim_1 = np.zeros([b, b])  # empty array to take similarity scores\n# Loop\nfor row in range(0, sim_1.shape[0]):\n    for col in range(0, sim_1.shape[1]):\n        sim_1[row, col] = cosine_similarity(v1[row], v2[col])\n\nprint(\"option 1 : loop\")\nprint(sim_1, \"\\n\")\n\n# Option 2 : vector normalization and dot product\ndef norm(x):\n    return x / np.sqrt(np.sum(x * x, axis=1, keepdims=True))\n\nsim_2 = np.dot(norm(v1), norm(v2).T)\n\nprint(\"option 2 : vec norm & dot product\")\nprint(sim_2, \"\\n\")\n\n# Check\nprint(\"outputs are the same :\", np.allclose(sim_1, sim_2))\n\n-- Inputs --\nv1 :\n[[ 1  2  3]\n [ 9  8  7]\n [-1 -4 -2]\n [ 1 -7  2]] \n\nv2 :\n[[ 0.85231869 -1.91539032  2.92461762]\n [ 8.23912099  9.65332634  9.59079958]\n [ 0.70405796 -1.13598034  0.33190387]\n [-1.40312107 -4.36375527  2.99906014]] \n\nbatch sizes match : True \n\n-- Outputs --\noption 1 : loop\n[[ 0.43043548  0.94619046 -0.11105101 -0.05530171]\n [ 0.25578722  0.98621822 -0.02231425 -0.34791801]\n [ 0.05821759 -0.90584138  0.50329648  0.51230883]\n [ 0.76048231 -0.3434871   0.92098338  0.87301268]] \n\noption 2 : vec norm & dot product\n[[ 0.43043548  0.94619046 -0.11105101 -0.05530171]\n [ 0.25578722  0.98621822 -0.02231425 -0.34791801]\n [ 0.05821759 -0.90584138  0.50329648  0.51230883]\n [ 0.76048231 -0.3434871   0.92098338  0.87301268]] \n\noutputs are the same : True"
  },
  {
    "objectID": "posts/c3w4/lab02.html#hard-negative-mining",
    "href": "posts/c3w4/lab02.html#hard-negative-mining",
    "title": "Modified Triplet Loss : Ungraded Lecture Notebook",
    "section": "Hard Negative Mining",
    "text": "Hard Negative Mining\nI‚Äôll now show you how to calculate the mean negative mean\\_neg and the closest negative close\\_neg used in calculating \\mathcal{L_\\mathrm{1}} and \\mathcal{L_\\mathrm{2}}.\n\\mathcal{L_\\mathrm{1}} = \\max{(mean\\_neg -\\mathrm{s}(A,P)  +\\alpha, 0)}\n\\mathcal{L_\\mathrm{2}} = \\max{(closest\\_neg -\\mathrm{s}(A,P)  +\\alpha, 0)}\nYou‚Äôll do this using the matrix of similarity scores you already know how to make, like the example below for a batch size of 4. The diagonal of the matrix contains all the \\mathrm{s}(A,P) values, similarities from duplicate question pairs (aka Positives). This is an important attribute for the calculations to follow.\n\n\nMean Negative\nmean\\_neg is the average of the off diagonals, the \\mathrm{s}(A,N) values, for each row.\n\n\nClosest Negative\nclosest\\_neg is the largest off diagonal value, \\mathrm{s}(A,N), that is smaller than the diagonal \\mathrm{s}(A,P) for each row. * Try using a different matrix of similarity scores.\n\n# Hardcoded matrix of similarity scores\nsim_hardcoded = np.array(\n    [\n        [0.9, -0.8, 0.3, -0.5],\n        [-0.4, 0.5, 0.1, -0.1],\n        [0.3, 0.1, -0.4, -0.8],\n        [-0.5, -0.2, -0.7, 0.5],\n    ]\n)\n\nsim = sim_hardcoded\n### START CODE HERE ###\n# Try using different values for the matrix of similarity scores\n# sim = 2 * np.random.random_sample((b,b)) -1   # random similarity scores between -1 and 1\n# sim = sim_2                                   # the matrix calculated previously\n### END CODE HERE ###\n\n# Batch size\nb = sim.shape[0]\n\nprint(\"-- Inputs --\")\nprint(\"sim :\")\nprint(sim)\nprint(\"shape :\", sim.shape, \"\\n\")\n\n# Positives\n# All the s(A,P) values : similarities from duplicate question pairs (aka Positives)\n# These are along the diagonal\nsim_ap = np.diag(sim)\nprint(\"sim_ap :\")\nprint(np.diag(sim_ap), \"\\n\")\n\n# Negatives\n# all the s(A,N) values : similarities the non duplicate question pairs (aka Negatives)\n# These are in the off diagonals\nsim_an = sim - np.diag(sim_ap)\nprint(\"sim_an :\")\nprint(sim_an, \"\\n\")\n\nprint(\"-- Outputs --\")\n# Mean negative\n# Average of the s(A,N) values for each row\nmean_neg = np.sum(sim_an, axis=1, keepdims=True) / (b - 1)\nprint(\"mean_neg :\")\nprint(mean_neg, \"\\n\")\n\n# Closest negative\n# Max s(A,N) that is &lt;= s(A,P) for each row\nmask_1 = np.identity(b) == 1            # mask to exclude the diagonal\nmask_2 = sim_an &gt; sim_ap.reshape(b, 1)  # mask to exclude sim_an &gt; sim_ap\nmask = mask_1 | mask_2\nsim_an_masked = np.copy(sim_an)         # create a copy to preserve sim_an\nsim_an_masked[mask] = -2\n\nclosest_neg = np.max(sim_an_masked, axis=1, keepdims=True)\nprint(\"closest_neg :\")\nprint(closest_neg, \"\\n\")\n\n-- Inputs --\nsim :\n[[ 0.9 -0.8  0.3 -0.5]\n [-0.4  0.5  0.1 -0.1]\n [ 0.3  0.1 -0.4 -0.8]\n [-0.5 -0.2 -0.7  0.5]]\nshape : (4, 4) \n\nsim_ap :\n[[ 0.9  0.   0.   0. ]\n [ 0.   0.5  0.   0. ]\n [ 0.   0.  -0.4  0. ]\n [ 0.   0.   0.   0.5]] \n\nsim_an :\n[[ 0.  -0.8  0.3 -0.5]\n [-0.4  0.   0.1 -0.1]\n [ 0.3  0.1  0.  -0.8]\n [-0.5 -0.2 -0.7  0. ]] \n\n-- Outputs --\nmean_neg :\n[[-0.33333333]\n [-0.13333333]\n [-0.13333333]\n [-0.46666667]] \n\nclosest_neg :\n[[ 0.3]\n [ 0.1]\n [-0.8]\n [-0.2]]"
  },
  {
    "objectID": "posts/c3w4/lab02.html#the-loss-functions",
    "href": "posts/c3w4/lab02.html#the-loss-functions",
    "title": "Modified Triplet Loss : Ungraded Lecture Notebook",
    "section": "The Loss Functions",
    "text": "The Loss Functions\nThe last step is to calculate the loss functions.\n\\mathcal{L_\\mathrm{1}} = \\max{(mean\\_neg -\\mathrm{s}(A,P)  +\\alpha, 0)}\n\\mathcal{L_\\mathrm{2}} = \\max{(closest\\_neg -\\mathrm{s}(A,P)  +\\alpha, 0)}\n\\mathcal{L_\\mathrm{Full}} = \\mathcal{L_\\mathrm{1}} + \\mathcal{L_\\mathrm{2}}\n\n# Alpha margin\nalpha = 0.25\n\n# Modified triplet loss\n# Loss 1\nl_1 = np.maximum(mean_neg - sim_ap.reshape(b, 1) + alpha, 0)\n# Loss 2\nl_2 = np.maximum(closest_neg - sim_ap.reshape(b, 1) + alpha, 0)\n# Loss full\nl_full = l_1 + l_2\n# Cost\ncost = np.sum(l_full)\n\nprint(\"-- Outputs --\")\nprint(\"loss full :\")\nprint(l_full, \"\\n\")\nprint(\"cost :\", \"{:.3f}\".format(cost))\n\n-- Outputs --\nloss full :\n[[0.        ]\n [0.        ]\n [0.51666667]\n [0.        ]] \n\ncost : 0.517"
  },
  {
    "objectID": "posts/c3w4/lab02.html#summary",
    "href": "posts/c3w4/lab02.html#summary",
    "title": "Modified Triplet Loss : Ungraded Lecture Notebook",
    "section": "Summary",
    "text": "Summary\nThere were a lot of steps in there, so well done. You now know how to calculate a modified triplet loss, incorporating the mean negative and the closest negative. You also learned how to create a matrix of similarity scores based on cosine similarity."
  },
  {
    "objectID": "posts/c3w4/lab03.html",
    "href": "posts/c3w4/lab03.html",
    "title": "Evaluate a Siamese model: Ungraded Lecture Notebook",
    "section": "",
    "text": "course banner\nimport trax.fastmath.numpy as np\n\n2025-02-05 22:01:13.248560: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1738785673.263043  617145 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1738785673.268182  617145 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered"
  },
  {
    "objectID": "posts/c3w4/lab03.html#inspecting-the-necessary-elements",
    "href": "posts/c3w4/lab03.html#inspecting-the-necessary-elements",
    "title": "Evaluate a Siamese model: Ungraded Lecture Notebook",
    "section": "Inspecting the necessary elements",
    "text": "Inspecting the necessary elements\nIn this lecture notebook you will learn how to evaluate a Siamese model using the accuracy metric. Because there are many steps before evaluating a Siamese network (as you will see in this week‚Äôs assignment) the necessary elements and variables are replicated here using real data from the assignment:\n\nq1: vector with dimension (batch_size X max_length) containing first questions to compare in the test set.\nq2: vector with dimension (batch_size X max_length) containing second questions to compare in the test set.\n\nNotice that for each pair of vectors within a batch ([q1_1, q1_2, q1_3, ...], [q2_1, q2_2,q2_3, ...]) q1_i is associated to q2_k.\n\ny_test: 1 if q1_i and q2_k are duplicates, 0 otherwise.\nv1: output vector from the model‚Äôs prediction associated with the first questions.\nv2: output vector from the model‚Äôs prediction associated with the second questions.\n\nYou can inspect each one of these variables by running the following cells:\n\nq1 = np.load('q1.npy')\nprint(f'q1 has shape: {q1.shape} \\n\\nAnd it looks like this: \\n\\n {q1}\\n\\n')\n\nq1 has shape: (512, 64) \n\nAnd it looks like this: \n\n [[ 32  38   4 ...   1   1   1]\n [ 30 156  78 ...   1   1   1]\n [ 32  38   4 ...   1   1   1]\n ...\n [ 32  33   4 ...   1   1   1]\n [ 30 156 317 ...   1   1   1]\n [ 30 156   6 ...   1   1   1]]\n\n\n\n\nNotice those 1s on the right-hand side?\nHope you remember that the value of 1 was used for padding.\n\nq2 = np.load('q2.npy')\nprint(f'q2 has shape: {q2.shape} \\n\\nAnd looks like this: \\n\\n {q2}\\n\\n')\n\nq2 has shape: (512, 64) \n\nAnd looks like this: \n\n [[   30   156    78 ...     1     1     1]\n [  283   156    78 ...     1     1     1]\n [   32    38     4 ...     1     1     1]\n ...\n [   32    33     4 ...     1     1     1]\n [   30   156    78 ...     1     1     1]\n [   30   156 10596 ...     1     1     1]]\n\n\n\n\n\ny_test = np.load('y_test.npy')\nprint(f'y_test has shape: {y_test.shape} \\n\\nAnd looks like this: \\n\\n {y_test}\\n\\n')\n\ny_test has shape: (512,) \n\nAnd looks like this: \n\n [0 1 1 0 0 0 0 1 0 1 1 0 0 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0\n 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0\n 0 0 0 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0\n 0 1 0 0 1 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0\n 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 1 1\n 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1\n 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1\n 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1 1 0 0\n 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0\n 0 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1 0\n 1 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1\n 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1\n 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n\n\n\n\n\nv1 = np.load('v1.npy')\nprint(f'v1 has shape: {v1.shape} \\n\\nAnd looks like this: \\n\\n {v1}\\n\\n')\nv2 = np.load('v2.npy')\nprint(f'v2 has shape: {v2.shape} \\n\\nAnd looks like this: \\n\\n {v2}\\n\\n')\n\nv1 has shape: (512, 128) \n\nAnd looks like this: \n\n [[ 0.01273625 -0.1496373  -0.01982759 ...  0.02205012 -0.00169148\n  -0.01598107]\n [-0.05592084  0.05792497 -0.02226785 ...  0.08156938 -0.02570007\n  -0.00503111]\n [ 0.05686752  0.0294889   0.04522024 ...  0.03141788 -0.08459651\n  -0.00968536]\n ...\n [ 0.15115018  0.17791134  0.02200656 ... -0.00851707  0.00571415\n  -0.00431194]\n [ 0.06995274  0.13110274  0.0202337  ... -0.00902792 -0.01221745\n   0.00505962]\n [-0.16043712 -0.11899089 -0.15950686 ...  0.06544471 -0.01208312\n  -0.01183368]]\n\n\nv2 has shape: (512, 128) \n\nAnd looks like this: \n\n [[ 0.07437647  0.02804951 -0.02974014 ...  0.02378932 -0.01696189\n  -0.01897198]\n [ 0.03270066  0.15122835 -0.02175895 ...  0.00517202 -0.14617395\n   0.00204823]\n [ 0.05635608  0.05454165  0.042222   ...  0.03831453 -0.05387777\n  -0.01447786]\n ...\n [ 0.04727105 -0.06748016  0.04194937 ...  0.07600753 -0.03072828\n   0.00400715]\n [ 0.00269269  0.15222628  0.01714724 ...  0.01482705 -0.0197884\n   0.01389528]\n [-0.15475044 -0.15718803 -0.14732707 ...  0.04299919 -0.01070975\n  -0.01318042]]"
  },
  {
    "objectID": "posts/c3w4/lab03.html#calculating-the-accuracy",
    "href": "posts/c3w4/lab03.html#calculating-the-accuracy",
    "title": "Evaluate a Siamese model: Ungraded Lecture Notebook",
    "section": "Calculating the accuracy",
    "text": "Calculating the accuracy\nYou will calculate the accuracy by iterating over the test set and checking if the model predicts right or wrong.\nThe first step is to set the accuracy to zero:\n\naccuracy = 0\n\nYou will also need the batch size and the threshold that determines if two questions are the same or not.\nNote :A higher threshold means that only very similar questions will be considered as the same question.\n\nbatch_size = 512 # Note: The max it can be is y_test.shape[0] i.e all the samples in test data\nthreshold = 0.7 # You can play around with threshold and then see the change in accuracy.\n\nIn the assignment you will iterate over multiple batches of data but since this is a simplified version only one batch is provided.\nNote: Be careful with the indices when slicing the test data in the assignment!\nThe process is pretty straightforward: - Iterate over each one of the elements in the batch - Compute the cosine similarity between the predictions - For computing the cosine similarity, the two output vectors should have been normalized using L2 normalization meaning their magnitude will be 1. This has been taken care off by the Siamese network you will build in the assignment. Hence the cosine similarity here is just dot product between two vectors. You can check by implementing the usual cosine similarity formula and check if this holds or not. - Determine if this value is greater than the threshold (If it is, consider the two questions as the same and return 1 else 0) - Compare against the actual target and if the prediction matches, add 1 to the accuracy (increment the correct prediction counter) - Divide the accuracy by the number of processed elements\n\nfor j in range(batch_size):        # Iterate over each one of the elements in the batch\n    \n    d = np.dot(v1[j],v2[j])        # Compute the cosine similarity between the predictions as l2 normalized, ||v1[j]||==||v2[j]||==1 so only dot product is needed\n    res = d &gt; threshold            # Determine if this value is greater than the threshold (if it is consider the two questions as the same)\n    accuracy += (y_test[j] == res) # Compare against the actual target and if the prediction matches, add 1 to the accuracy\n\naccuracy = accuracy / batch_size   # Divide the accuracy by the number of processed elements\n\n\nprint(f'The accuracy of the model is: {accuracy}')\n\nThe accuracy of the model is: 0.7421875\n\n\nCongratulations on finishing this lecture notebook!\nNow you should have a clearer understanding of how to evaluate your Siamese language models using the accuracy metric.\nKeep it up!"
  },
  {
    "objectID": "posts/c2w1/assignment.html#0",
    "href": "posts/c2w1/assignment.html#0",
    "title": "Assignment 1: Auto Correct",
    "section": "0. Overview",
    "text": "0. Overview\nWe use autocorrect every day on your cell phone and computer. In this assignment, we will explore what really goes on behind the scenes. Of course, the model we are about to implement is not identical to the one used in your phone, but it is still quite good.\nBy completing this assignment we will learn how to:\n\nGet a word count given a corpus\nGet a word probability in the corpus\nManipulate strings\nFilter strings\nImplement Minimum edit distance to compare strings and to help find the optimal path for the edits.\nUnderstand how dynamic programming works\n\nSimilar systems are used everywhere. - For example, if we type in the word ‚ÄúI am lerningg‚Äù, chances are very high that we meant to write ‚Äúlearning‚Äù, as shown in Figure 1.\n\n Figure 1\n\n #### 0.1 Edit Distance\nIn this assignment, we will implement models that correct words that are 1 and 2 edit distances away. - We say two words are n edit distance away from each other when we need n edits to change one word into another.\nAn edit could consist of one of the following options:\n\nDelete (remove a letter): ‚Äòhat‚Äô =&gt; ‚Äòat, ha, ht‚Äô\nSwitch (swap 2 adjacent letters): ‚Äòeta‚Äô =&gt; ‚Äòeat, tea,‚Ä¶‚Äô\nReplace (change 1 letter to another): ‚Äòjat‚Äô =&gt; ‚Äòhat, rat, cat, mat, ‚Ä¶‚Äô\nInsert (add a letter): ‚Äòte‚Äô =&gt; ‚Äòthe, ten, ate, ‚Ä¶‚Äô\n\nWe will be using the four methods above to implement an Auto-correct. - To do so, we will need to compute probabilities that a certain word is correct given an input.\nThis auto-correct we are about to implement was first created by Peter Norvig in 2007. - His original article may be a useful reference for this assignment.\nThe goal of our spell check model is to compute the following probability:\nP(c|w) = \\frac{P(w|c)\\times P(c)}{P(w)} \\tag{Eqn-1}\nThe equation above is Bayes Rule. - Equation 1 says that the probability of a word being correct $P(c|w) $is equal to the probability of having a certain word w, given that it is correct P(w|c), multiplied by the probability of being correct in general P(C) divided by the probability of that word w appearing P(w) in general. - To compute equation 1, we will first import a data set and then create all the probabilities that we need using that data set.\n # Part 1: Data Preprocessing\n\nimport re\nfrom collections import Counter\nimport numpy as np\nimport pandas as pd\n\nAs in any other machine learning task, the first thing we have to do is process your data set. - Many courses load in pre-processed data for you. - However, in the real world, when we build these NLP systems, we load the datasets and process them. - So let‚Äôs get some real world practice in pre-processing the data!\nYour first task is to read in a file called ‚Äòshakespeare.txt‚Äô which is found in your file directory. To look at this file we can go to File ==&gt; Open.\n ### Exercise 1 Implement the function process_data which\n\nReads in a corpus (text file)\nChanges everything to lowercase\nReturns a list of words.\n\n\nOptions and Hints\n\nIf we would like more of a real-life practice, don‚Äôt open the ‚ÄòHints‚Äô below (yet) and try searching the web to derive your answer.\nIf we want a little help, click on the green ‚ÄúGeneral Hints‚Äù section by clicking on it with your mouse.\nIf we get stuck or are not getting the expected results, click on the green ‚ÄòDetailed Hints‚Äô section to get hints for each step that you‚Äôll take to complete this function.\n\n\n\nGeneral Hints\n\n\nGeneral Hints to get started\n\n\nPython input and output\n\n\nPython ‚Äòre‚Äô documentation \n\n\n\n\n\nDetailed Hints\n\n\nDetailed hints if you‚Äôre stuck\n\n\nUse ‚Äòwith‚Äô syntax to read a file\n\n\nDecide whether to use ‚Äòread()‚Äô or ‚Äôreadline(). What‚Äôs the difference?\n\n\nChoose whether to use either str.lower() or str.lowercase(). What is the difference?\n\n\nUse re.findall(pattern, string)\n\n\nLook for the ‚ÄúRaw String Notation‚Äù section in the Python ‚Äòre‚Äô documentation to understand the difference between r‚Äô‚Äò, r‚Äô‚Äô and ‚Äò\\W‚Äô.\n\n\nFor the pattern, decide between using ‚Äò‚Äô, ‚Äò‚Äô, ‚Äò+‚Äô or ‚Äò+‚Äô. What do we think are the differences?\n\n\n\n\n# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: process_data\ndef process_data(file_name):\n    \"\"\"\n    Input: \n        A file_name which is found in your current directory. We just have to read it in. \n    Output: \n        words: a list containing all the words in the corpus (text file we read) in lower case. \n    \"\"\"\n    words = [] # return this variable correctly\n\n    ### START CODE HERE ### \n    \n    ### END CODE HERE ###\n    \n    return words\n\nNote, in the following cell, ‚Äòwords‚Äô is converted to a python set. This eliminates any duplicate entries.\n\n#DO NOT MODIFY THIS CELL\nword_l = process_data('shakespeare.txt')\nvocab = set(word_l)  # this will be your new vocabulary\nprint(f\"The first ten words in the text are: \\n{word_l[0:10]}\")\nprint(f\"There are {len(vocab)} unique words in the vocabulary.\")\n\nThe first ten words in the text are: \n[]\nThere are 0 unique words in the vocabulary.\n\n\n\n\nExpected Output\nThe first ten words in the text are: \n['o', 'for', 'a', 'muse', 'of', 'fire', 'that', 'would', 'ascend', 'the']\nThere are 6116 unique words in the vocabulary.\n ### Exercise 2\nImplement a get_count function that returns a dictionary - The dictionary‚Äôs keys are words - The value for each word is the number of times that word appears in the corpus.\nFor example, given the following sentence: ‚ÄúI am happy because I am learning‚Äù, your dictionary should return the following:\n\n\n\nKey \n\n\nValue \n\n\n\n\nI\n\n\n2\n\n\n\n\nam\n\n\n2\n\n\n\n\nhappy\n\n\n1\n\n\n\n\nbecause\n\n\n1\n\n\n\n\nlearning\n\n\n1\n\n\n\nInstructions: Implement a get_count which returns a dictionary where the key is a word and the value is the number of times the word appears in the list.\n\n\nHints\n\n\n\n\nTry implementing this using a for loop and a regular dictionary. This may be good practice for similar coding interview questions\n\n\nWe can also use defaultdict instead of a regualr dictionary, along with the for loop\n\n\nOtherwise, to skip using a for loop, we can use Python‚Äôs  Counter class\n\n\n\n\n# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# UNIT TEST COMMENT: Candidate for Table Driven Tests\n# GRADED FUNCTION: get_count\ndef get_count(word_l):\n    '''\n    Input:\n        word_l: a set of words representing the corpus. \n    Output:\n        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.\n    '''\n    \n    word_count_dict = {}  # fill this with word counts\n    ### START CODE HERE \n            \n    ### END CODE HERE ### \n    return word_count_dict\n\n\n#DO NOT MODIFY THIS CELL\nword_count_dict = get_count(word_l)\nprint(f\"There are {len(word_count_dict)} key values pairs\")\nprint(f\"The count for the word 'thee' is {word_count_dict.get('thee',0)}\")\n\nThere are 0 key values pairs\nThe count for the word 'thee' is 0\n\n\n\n\nExpected Output\nThere are 6116 key values pairs\nThe count for the word 'thee' is 240\n ### Exercise 3 Given the dictionary of word counts, compute the probability that each word will appear if randomly selected from the corpus of words.\nP(w_i) = \\frac{C(w_i)}{M} \\tag{Eqn-2} where\nC(w_i) is the total number of times w_i appears in the corpus.\nM is the total number of words in the corpus.\nFor example, the probability of the word ‚Äòam‚Äô in the sentence ‚ÄòI am happy because I am learning‚Äô is:\nP(am) = \\frac{C(w_i)}{M} = \\frac {2}{7} \\tag{Eqn-3}.\nInstructions: Implement get_probs function which gives we the probability that a word occurs in a sample. This returns a dictionary where the keys are words, and the value for each word is its probability in the corpus of words.\n\n\nHints\n\n\nGeneral advice\n\n\nUse dictionary.values()\n\n\nUse sum()\n\n\nThe cardinality (number of words in the corpus should be equal to len(word_l). We will calculate this same number, but using the word count dictionary.\n\n\nIf you‚Äôre using a for loop:\n\n\nUse dictionary.keys()\n\n\nIf you‚Äôre using a dictionary comprehension:\n\n\nUse dictionary.items()\n\n\n\n\n# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: get_probs\ndef get_probs(word_count_dict):\n    '''\n    Input:\n        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.\n    Output:\n        probs: A dictionary where keys are the words and the values are the probability that a word will occur. \n    '''\n    probs = {}  # return this variable correctly\n    \n    ### START CODE HERE ###\n    \n    ### END CODE HERE ###\n    return probs\n\n\n#DO NOT MODIFY THIS CELL\nprobs = get_probs(word_count_dict)\nprint(f\"Length of probs is {len(probs)}\")\nprint(f\"P('thee') is {probs['thee']:.4f}\")\n\nLength of probs is 0\n\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nCell In[7], line 4\n      2 probs = get_probs(word_count_dict)\n      3 print(f\"Length of probs is {len(probs)}\")\n----&gt; 4 print(f\"P('thee') is {probs['thee']:.4f}\")\n\nKeyError: 'thee'\n\n\n\n\n\nExpected Output\nLength of probs is 6116\nP('thee') is 0.0045\n # Part 2: String Manipulations\nNow, that we have computed P(w_i) for all the words in the corpus, we will write a few functions to manipulate strings so that we can edit the erroneous strings and return the right spellings of the words. In this section, we will implement four functions:\n\ndelete_letter: given a word, it returns all the possible strings that have one character removed.\nswitch_letter: given a word, it returns all the possible strings that have two adjacent letters switched.\nreplace_letter: given a word, it returns all the possible strings that have one character replaced by another different letter.\ninsert_letter: given a word, it returns all the possible strings that have an additional character inserted.\n\n\n\nList comprehensions\nString and list manipulation in python will often make use of a python feature called list comprehensions. The routines below will be described as using list comprehensions, but if we would rather implement them in another way, we are free to do so as long as the result is the same. Further, the following section will provide detailed instructions on how to use list comprehensions and how to implement the desired functions. If we are a python expert, feel free to skip the python hints and move to implementing the routines directly.\nPython List Comprehensions embed a looping structure inside of a list declaration, collapsing many lines of code into a single line. If we are not familiar with them, they seem slightly out of order relative to for loops.\n\n Figure 2\n\nThe diagram above shows that the components of a list comprehension are the same components we would find in a typical for loop that appends to a list, but in a different order. With that in mind, we‚Äôll continue the specifics of this assignment. We will be very descriptive for the first function, deletes(), and less so in later functions as we become familiar with list comprehensions.\n ### Exercise 4\nInstructions for delete_letter(): Implement a delete_letter() function that, given a word, returns a list of strings with one character deleted.\nFor example, given the word nice, it would return the set: {‚Äòice‚Äô, ‚Äònce‚Äô, ‚Äònic‚Äô, ‚Äònie‚Äô}.\nStep 1: Create a list of ‚Äòsplits‚Äô. This is all the ways we can split a word into Left and Right: For example,\n‚Äônice is split into : [('', 'nice'), ('n', 'ice'), ('ni', 'ce'), ('nic', 'e'), ('nice', '')] This is common to all four functions (delete, replace, switch, insert).\n\n Figure 3\n\nStep 2: This is specific to delete_letter. Here, we are generating all words that result from deleting one character.\nThis can be done in a single line with a list comprehension. We can make use of this type of syntax:\n[f(a,b) for a, b in splits if condition]\nFor our ‚Äònice‚Äô example we get: [‚Äòice‚Äô, ‚Äònce‚Äô, ‚Äònie‚Äô, ‚Äònic‚Äô]\n\n Figure 4\n\n\n\nLevels of assistance\nTry this exercise with these levels of assistance.\n- We hope that this will make it both a meaningful experience but also not a frustrating experience. - Start with level 1, then move onto level 2, and 3 as needed.\n- Level 1. Try to think this through and implement this yourself.\n- Level 2. Click on the \"Level 2 Hints\" section for some hints to get started.\n- Level 3. If we would prefer more guidance, please click on the \"Level 3 Hints\" cell for step by step instructions.\n\nIf we are still stuck, look at the images in the ‚Äúlist comprehensions‚Äù section above.\n\n\n\nLevel 2 Hints\n\n\n\n\n Use array slicing like my_string[0:2] \n\n\n Use list comprehensions or for loops \n\n\n\n\n\nLevel 3 Hints\n\n\n\n\nsplits: Use array slicing, like my_str[0:2], to separate a string into two pieces.\n\n\nDo this in a loop or list comprehension, so that we have a list of tuples.\n\nFor example, ‚Äúcake‚Äù can get split into ‚Äúca‚Äù and ‚Äúke‚Äù. They‚Äôre stored in a tuple (‚Äúca‚Äù,‚Äúke‚Äù), and the tuple is appended to a list. We‚Äôll refer to these as L and R, so the tuple is (L,R)\n\n&lt;li&gt;When choosing the range for your loop, if we input the word \"cans\" and generate the tuple  ('cans',''), make sure to include an if statement to check the length of that right-side string (R) in the tuple (L,R) &lt;/li&gt;\n&lt;li&gt;deletes: Go through the list of tuples and combine the two strings together. We can use the + operator to combine two strings&lt;/li&gt;\n&lt;li&gt;When combining the tuples, make sure that we leave out a middle character.&lt;/li&gt;\n&lt;li&gt;Use array slicing to leave out the first character of the right substring.&lt;/li&gt;\n\n\n\n# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# UNIT TEST COMMENT: Candidate for Table Driven Tests\n# GRADED FUNCTION: deletes\ndef delete_letter(word, verbose=False):\n    '''\n    Input:\n        word: the string/word for which we will generate all possible words \n                in the vocabulary which have 1 missing character\n    Output:\n        delete_l: a list of all possible strings obtained by deleting 1 character from word\n    '''\n    \n    delete_l = []\n    split_l = []\n    \n    ### START CODE HERE ###\n    \n    ### END CODE HERE ###\n\n    if verbose: print(f\"input word {word}, \\nsplit_l = {split_l}, \\ndelete_l = {delete_l}\")\n\n    return delete_l\n\n\ndelete_word_l = delete_letter(word=\"cans\",\n                        verbose=True)\n\ninput word cans, \nsplit_l = [], \ndelete_l = []\n\n\n\n\nExpected Output\nNote: We might get a slightly different result with split_l\n\ninput word cans, \nsplit_l = [('', 'cans'), ('c', 'ans'), ('ca', 'ns'), ('can', 's')], \ndelete_l = ['ans', 'cns', 'cas', 'can']\n\n\nNote 1\n\nNotice how it has the extra tuple ('cans', '').\nThis will be fine as long as we have checked the size of the right-side substring in tuple (L,R).\nCan we explain why this will give we the same result for the list of deletion strings (delete_l)?\n\ninput word cans, \nsplit_l = [('', 'cans'), ('c', 'ans'), ('ca', 'ns'), ('can', 's'), ('cans', '')], \ndelete_l = ['ans', 'cns', 'cas', 'can']\n\n\nNote 2\nIf we end up getting the same word as your input word, like this:\ninput word cans, \nsplit_l = [('', 'cans'), ('c', 'ans'), ('ca', 'ns'), ('can', 's'), ('cans', '')], \ndelete_l = ['ans', 'cns', 'cas', 'can', 'cans']\n\nCheck how we set the range.\nSee if we check the length of the string on the right-side of the split.\n\n\n# test # 2\nprint(f\"Number of outputs of delete_letter('at') is {len(delete_letter('at'))}\")\n\nNumber of outputs of delete_letter('at') is 0\n\n\n\n\nExpected output\nNumber of outputs of delete_letter('at') is 2\n ### Exercise 5\nInstructions for switch_letter(): Now implement a function that switches two letters in a word. It takes in a word and returns a list of all the possible switches of two letters that are adjacent to each other. - For example, given the word ‚Äòeta‚Äô, it returns {‚Äòeat‚Äô, ‚Äòtea‚Äô}, but does not return ‚Äòate‚Äô.\nStep 1: is the same as in delete_letter()\nStep 2: A list comprehension or for loop which forms strings by swapping adjacent letters. This is of the form:\n[f(L,R) for L, R in splits if condition] where ‚Äòcondition‚Äô will test the length of R in a given iteration. See below.\n\n Figure 5\n\n\n\nLevels of difficulty\nTry this exercise with these levels of difficulty.\n- Level 1. Try to think this through and implement this yourself. - Level 2. Click on the ‚ÄúLevel 2 Hints‚Äù section for some hints to get started. - Level 3. If we would prefer more guidance, please click on the ‚ÄúLevel 3 Hints‚Äù cell for step by step instructions.\n\n\nLevel 2 Hints\n\n\n\n\n Use array slicing like my_string[0:2] \n\n\n Use list comprehensions or for loops \n\n\nTo do a switch, think of the whole word as divided into 4 distinct parts. Write out ‚Äòcupcakes‚Äô on a piece of paper and see how we can split it into (‚Äòcupc‚Äô, ‚Äòk‚Äô, ‚Äòa‚Äô, ‚Äòes‚Äô)\n\n\n\n\n\nLevel 3 Hints\n\n\n\n\nsplits: Use array slicing, like my_str[0:2], to separate a string into two pieces.\n\n\nSplitting is the same as for delete_letter\n\n\nTo perform the switch, go through the list of tuples and combine four strings together. We can use the + operator to combine strings\n\n\nThe four strings will be the left substring from the split tuple, followed by the first (index 1) character of the right substring, then the zero-th character (index 0) of the right substring, and then the remaining part of the right substring.\n\n\nUnlike delete_letter, we will want to check that your right substring is at least a minimum length. To see why, review the previous hint bullet point (directly before this one).\n\n\n\n\n# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# UNIT TEST COMMENT: Candidate for Table Driven Tests\n# GRADED FUNCTION: switches\ndef switch_letter(word, verbose=False):\n    '''\n    Input:\n        word: input string\n     Output:\n        switches: a list of all possible strings with one adjacent charater switched\n    ''' \n    \n    switch_l = []\n    split_l = []\n    \n    ### START CODE HERE ###\n    \n    ### END CODE HERE ###\n    \n    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nswitch_l = {switch_l}\") \n\n    return switch_l\n\n\nswitch_word_l = switch_letter(word=\"eta\",\n                         verbose=True)\n\nInput word = eta \nsplit_l = [] \nswitch_l = []\n\n\n\n\nExpected output\nInput word = eta \nsplit_l = [('', 'eta'), ('e', 'ta'), ('et', 'a')] \nswitch_l = ['tea', 'eat']\n\n\nNote 1\nWe may get this:\nInput word = eta \nsplit_l = [('', 'eta'), ('e', 'ta'), ('et', 'a'), ('eta', '')] \nswitch_l = ['tea', 'eat']\n\nNotice how it has the extra tuple ('eta', '').\nThis is also correct.\nCan we think of why this is the case?\n\n\n\nNote 2\nIf we get an error\nIndexError: string index out of range\n\nPlease see if we have checked the length of the strings when switching characters.\n\n\n# test # 2\nprint(f\"Number of outputs of switch_letter('at') is {len(switch_letter('at'))}\")\n\nNumber of outputs of switch_letter('at') is 0\n\n\n\n\nExpected output\nNumber of outputs of switch_letter('at') is 1\n ### Exercise 6 Instructions for replace_letter(): Now implement a function that takes in a word and returns a list of strings with one replaced letter from the original word.\nStep 1: is the same as in delete_letter()\nStep 2: A list comprehension or for loop which form strings by replacing letters. This can be of the form:\n[f(a,b,c) for a, b in splits if condition for c in string] Note the use of the second for loop.\nIt is expected in this routine that one or more of the replacements will include the original word. For example, replacing the first letter of ‚Äòear‚Äô with ‚Äòe‚Äô will return ‚Äòear‚Äô.\nStep 3: Remove the original input letter from the output.\n\n\nHints\n\n\n\n\nTo remove a word from a list, first store its contents inside a set()\n\n\nUse set.discard(‚Äòthe_word‚Äô) to remove a word in a set (if the word does not exist in the set, then it will not throw a KeyError. Using set.remove(‚Äòthe_word‚Äô) throws a KeyError if the word does not exist in the set.\n\n\n\n\n# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# UNIT TEST COMMENT: Candidate for Table Driven Tests\n# GRADED FUNCTION: replaces\ndef replace_letter(word, verbose=False):\n    '''\n    Input:\n        word: the input string/word \n    Output:\n        replaces: a list of all possible strings where we replaced one letter from the original word. \n    ''' \n    \n    letters = 'abcdefghijklmnopqrstuvwxyz'\n    replace_l = []\n    split_l = []\n    \n    ### START CODE HERE ###\n\n    ### END CODE HERE ###\n    \n    # turn the set back into a list and sort it, for easier viewing\n    replace_l = sorted(list(replace_set))\n    \n    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nreplace_l {replace_l}\")   \n    \n    return replace_l\n\n\nreplace_l = replace_letter(word='can',\n                              verbose=True)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[15], line 1\n----&gt; 1 replace_l = replace_letter(word='can',\n      2                               verbose=True)\n\nCell In[14], line 21, in replace_letter(word, verbose)\n     14 split_l = []\n     16 ### START CODE HERE ###\n     17 \n     18 ### END CODE HERE ###\n     19 \n     20 # turn the set back into a list and sort it, for easier viewing\n---&gt; 21 replace_l = sorted(list(replace_set))\n     23 if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nreplace_l {replace_l}\")   \n     25 return replace_l\n\nNameError: name 'replace_set' is not defined\n\n\n\n\n\nExpected Output**:\nInput word = can \nsplit_l = [('', 'can'), ('c', 'an'), ('ca', 'n')] \nreplace_l ['aan', 'ban', 'caa', 'cab', 'cac', 'cad', 'cae', 'caf', 'cag', 'cah', 'cai', 'caj', 'cak', 'cal', 'cam', 'cao', 'cap', 'caq', 'car', 'cas', 'cat', 'cau', 'cav', 'caw', 'cax', 'cay', 'caz', 'cbn', 'ccn', 'cdn', 'cen', 'cfn', 'cgn', 'chn', 'cin', 'cjn', 'ckn', 'cln', 'cmn', 'cnn', 'con', 'cpn', 'cqn', 'crn', 'csn', 'ctn', 'cun', 'cvn', 'cwn', 'cxn', 'cyn', 'czn', 'dan', 'ean', 'fan', 'gan', 'han', 'ian', 'jan', 'kan', 'lan', 'man', 'nan', 'oan', 'pan', 'qan', 'ran', 'san', 'tan', 'uan', 'van', 'wan', 'xan', 'yan', 'zan']\n\nNote how the input word ‚Äòcan‚Äô should not be one of the output words.\n\n\n\nNote 1\nIf we get something like this:\nInput word = can \nsplit_l = [('', 'can'), ('c', 'an'), ('ca', 'n'), ('can', '')] \nreplace_l ['aan', 'ban', 'caa', 'cab', 'cac', 'cad', 'cae', 'caf', 'cag', 'cah', 'cai', 'caj', 'cak', 'cal', 'cam', 'cao', 'cap', 'caq', 'car', 'cas', 'cat', 'cau', 'cav', 'caw', 'cax', 'cay', 'caz', 'cbn', 'ccn', 'cdn', 'cen', 'cfn', 'cgn', 'chn', 'cin', 'cjn', 'ckn', 'cln', 'cmn', 'cnn', 'con', 'cpn', 'cqn', 'crn', 'csn', 'ctn', 'cun', 'cvn', 'cwn', 'cxn', 'cyn', 'czn', 'dan', 'ean', 'fan', 'gan', 'han', 'ian', 'jan', 'kan', 'lan', 'man', 'nan', 'oan', 'pan', 'qan', 'ran', 'san', 'tan', 'uan', 'van', 'wan', 'xan', 'yan', 'zan']\n\nNotice how split_l has an extra tuple ('can', ''), but the output is still the same, so this is okay.\n\n\n\nNote 2\nIf we get something like this:\nInput word = can \nsplit_l = [('', 'can'), ('c', 'an'), ('ca', 'n'), ('can', '')] \nreplace_l ['aan', 'ban', 'caa', 'cab', 'cac', 'cad', 'cae', 'caf', 'cag', 'cah', 'cai', 'caj', 'cak', 'cal', 'cam', 'cana', 'canb', 'canc', 'cand', 'cane', 'canf', 'cang', 'canh', 'cani', 'canj', 'cank', 'canl', 'canm', 'cann', 'cano', 'canp', 'canq', 'canr', 'cans', 'cant', 'canu', 'canv', 'canw', 'canx', 'cany', 'canz', 'cao', 'cap', 'caq', 'car', 'cas', 'cat', 'cau', 'cav', 'caw', 'cax', 'cay', 'caz', 'cbn', 'ccn', 'cdn', 'cen', 'cfn', 'cgn', 'chn', 'cin', 'cjn', 'ckn', 'cln', 'cmn', 'cnn', 'con', 'cpn', 'cqn', 'crn', 'csn', 'ctn', 'cun', 'cvn', 'cwn', 'cxn', 'cyn', 'czn', 'dan', 'ean', 'fan', 'gan', 'han', 'ian', 'jan', 'kan', 'lan', 'man', 'nan', 'oan', 'pan', 'qan', 'ran', 'san', 'tan', 'uan', 'van', 'wan', 'xan', 'yan', 'zan']\n\nNotice how there are strings that are 1 letter longer than the original word, such as cana.\nPlease check for the case when there is an empty string '', and if so, do not use that empty string when setting replace_l.\n\n\n# test # 2\nprint(f\"Number of outputs of switch_letter('at') is {len(switch_letter('at'))}\")\n\nNumber of outputs of switch_letter('at') is 0\n\n\n\n\nExpected output\nNumber of outputs of switch_letter('at') is 1\n ### Exercise 7\nInstructions for insert_letter(): Now implement a function that takes in a word and returns a list with a letter inserted at every offset.\nStep 1: is the same as in delete_letter()\nStep 2: This can be a list comprehension of the form:\n[f(a,b,c) for a, b in splits if condition for c in string]\n\n# UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# UNIT TEST COMMENT: Candidate for Table Driven Tests\n# GRADED FUNCTION: inserts\ndef insert_letter(word, verbose=False):\n    '''\n    Input:\n        word: the input string/word \n    Output:\n        inserts: a set of all possible strings with one new letter inserted at every offset\n    ''' \n    letters = 'abcdefghijklmnopqrstuvwxyz'\n    insert_l = []\n    split_l = []\n    \n    ### START CODE HERE ###\n\n    ### END CODE HERE ###\n\n    if verbose: print(f\"Input word {word} \\nsplit_l = {split_l} \\ninsert_l = {insert_l}\")\n    \n    return insert_l\n\n\ninsert_l = insert_letter('at', True)\nprint(f\"Number of strings output by insert_letter('at') is {len(insert_l)}\")\n\nInput word at \nsplit_l = [] \ninsert_l = []\nNumber of strings output by insert_letter('at') is 0\n\n\n\n\nExpected output\nInput word at \nsplit_l = [('', 'at'), ('a', 't'), ('at', '')] \ninsert_l = ['aat', 'bat', 'cat', 'dat', 'eat', 'fat', 'gat', 'hat', 'iat', 'jat', 'kat', 'lat', 'mat', 'nat', 'oat', 'pat', 'qat', 'rat', 'sat', 'tat', 'uat', 'vat', 'wat', 'xat', 'yat', 'zat', 'aat', 'abt', 'act', 'adt', 'aet', 'aft', 'agt', 'aht', 'ait', 'ajt', 'akt', 'alt', 'amt', 'ant', 'aot', 'apt', 'aqt', 'art', 'ast', 'att', 'aut', 'avt', 'awt', 'axt', 'ayt', 'azt', 'ata', 'atb', 'atc', 'atd', 'ate', 'atf', 'atg', 'ath', 'ati', 'atj', 'atk', 'atl', 'atm', 'atn', 'ato', 'atp', 'atq', 'atr', 'ats', 'att', 'atu', 'atv', 'atw', 'atx', 'aty', 'atz']\nNumber of strings output by insert_letter('at') is 78\n\n\nNote 1\nIf we get a split_l like this:\nInput word at \nsplit_l = [('', 'at'), ('a', 't')] \ninsert_l = ['aat', 'bat', 'cat', 'dat', 'eat', 'fat', 'gat', 'hat', 'iat', 'jat', 'kat', 'lat', 'mat', 'nat', 'oat', 'pat', 'qat', 'rat', 'sat', 'tat', 'uat', 'vat', 'wat', 'xat', 'yat', 'zat', 'aat', 'abt', 'act', 'adt', 'aet', 'aft', 'agt', 'aht', 'ait', 'ajt', 'akt', 'alt', 'amt', 'ant', 'aot', 'apt', 'aqt', 'art', 'ast', 'att', 'aut', 'avt', 'awt', 'axt', 'ayt', 'azt']\nNumber of strings output by insert_letter('at') is 52\n\nNotice that split_l is missing the extra tuple (‚Äòat‚Äô, ‚Äô‚Äô). For insertion, we actually WANT this tuple.\nThe function is not creating all the desired output strings.\nCheck the range that we use for the for loop.\n\n\n\nNote 2\nIf we see this:\nInput word at \nsplit_l = [('', 'at'), ('a', 't'), ('at', '')] \ninsert_l = ['aat', 'bat', 'cat', 'dat', 'eat', 'fat', 'gat', 'hat', 'iat', 'jat', 'kat', 'lat', 'mat', 'nat', 'oat', 'pat', 'qat', 'rat', 'sat', 'tat', 'uat', 'vat', 'wat', 'xat', 'yat', 'zat', 'aat', 'abt', 'act', 'adt', 'aet', 'aft', 'agt', 'aht', 'ait', 'ajt', 'akt', 'alt', 'amt', 'ant', 'aot', 'apt', 'aqt', 'art', 'ast', 'att', 'aut', 'avt', 'awt', 'axt', 'ayt', 'azt']\nNumber of strings output by insert_letter('at') is 52\n\nEven though we may have fixed the split_l so that it contains the tuple ('at', ''), notice that you‚Äôre still missing some output strings.\n\nNotice that it‚Äôs missing strings such as ‚Äòata‚Äô, ‚Äòatb‚Äô, ‚Äòatc‚Äô all the way to ‚Äòatz‚Äô.\n\nTo fix this, make sure that when we set insert_l, we allow the use of the empty string ''.\n\n\n# test # 2\nprint(f\"Number of outputs of insert_letter('at') is {len(insert_letter('at'))}\")\n\nNumber of outputs of insert_letter('at') is 0\n\n\n\n\nExpected output\nNumber of outputs of insert_letter('at') is 78",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "A1 - Auto Correct"
    ]
  },
  {
    "objectID": "posts/c2w3/lab02.html#language-model-evaluation",
    "href": "posts/c2w3/lab02.html#language-model-evaluation",
    "title": "Building the language model",
    "section": "Language model evaluation",
    "text": "Language model evaluation\n\nTrain/validation/test split\nIn the videos, we saw that to evaluate language models, we need to keep some of the corpus data for validation and testing.\nThe choice of the test and validation data should correspond as much as possible to the distribution of the data coming from the actual application. If nothing but the input corpus is known, then random sampling from the corpus is used to define the test and validation subset.\nHere is a code similar to what you‚Äôll see in the code assignment. The following function allows we to randomly sample the input data and return train/validation/test subsets in a split given by the method parameters.\n\n# we only need train and validation %, test is the remainder\nimport random\ndef train_validation_test_split(data, train_percent, validation_percent):\n    \"\"\"\n    Splits the input data to  train/validation/test according to the percentage provided\n    \n    Args:\n        data: Pre-processed and tokenized corpus, i.e. list of sentences.\n        train_percent: integer 0-100, defines the portion of input corpus allocated for training\n        validation_percent: integer 0-100, defines the portion of input corpus allocated for validation\n        \n        Note: train_percent + validation_percent need to be &lt;=100\n              the reminder to 100 is allocated for the test set\n    \n    Returns:\n        train_data: list of sentences, the training part of the corpus\n        validation_data: list of sentences, the validation part of the corpus\n        test_data: list of sentences, the test part of the corpus\n    \"\"\"\n    # fixed seed here for reproducibility\n    random.seed(87)\n    \n    # reshuffle all input sentences\n    random.shuffle(data)\n\n    train_size = int(len(data) * train_percent / 100)\n    train_data = data[0:train_size]\n    \n    validation_size = int(len(data) * validation_percent / 100)\n    validation_data = data[train_size:train_size + validation_size]\n    \n    test_data = data[train_size + validation_size:]\n    \n    return train_data, validation_data, test_data\n\ndata = [x for x in range (0, 100)]\n\ntrain_data, validation_data, test_data = train_validation_test_split(data, 80, 10)\nprint(\"split 80/10/10:\\n\",f\"train data:{train_data}\\n\", f\"validation data:{validation_data}\\n\", \n      f\"test data:{test_data}\\n\")\n\ntrain_data, validation_data, test_data = train_validation_test_split(data, 98, 1)\nprint(\"split 98/1/1:\\n\",f\"train data:{train_data}\\n\", f\"validation data:{validation_data}\\n\", \n      f\"test data:{test_data}\\n\")\n\nsplit 80/10/10:\n train data:[28, 76, 5, 0, 62, 29, 54, 95, 88, 58, 4, 22, 92, 14, 50, 77, 47, 33, 75, 68, 56, 74, 43, 80, 83, 84, 73, 93, 66, 87, 9, 91, 64, 79, 20, 51, 17, 27, 12, 31, 67, 81, 7, 34, 45, 72, 38, 30, 16, 60, 40, 86, 48, 21, 70, 59, 6, 19, 2, 99, 37, 36, 52, 61, 97, 44, 26, 57, 89, 55, 53, 85, 3, 39, 10, 71, 23, 32, 25, 8]\n validation data:[78, 65, 63, 11, 49, 98, 1, 46, 15, 41]\n test data:[90, 96, 82, 42, 35, 13, 69, 24, 94, 18]\n\nsplit 98/1/1:\n train data:[66, 23, 29, 28, 52, 87, 70, 13, 15, 2, 62, 43, 82, 50, 40, 32, 30, 79, 71, 89, 6, 10, 34, 78, 11, 49, 39, 42, 26, 46, 58, 96, 97, 8, 56, 86, 33, 93, 92, 91, 57, 65, 95, 20, 72, 3, 12, 9, 47, 37, 67, 1, 16, 74, 53, 99, 54, 68, 5, 18, 27, 17, 48, 36, 24, 45, 73, 19, 41, 59, 21, 98, 0, 31, 4, 85, 80, 64, 84, 88, 25, 44, 61, 22, 60, 94, 76, 38, 77, 81, 90, 69, 63, 7, 51, 14, 55, 83]\n validation data:[35]\n test data:[75]\n\n\n\n\n\nPerplexity\nIn order to implement the perplexity formula, you‚Äôll need to know how to implement m-th order root of a variable.\n\\begin{equation*}\nPP(W)=\\sqrt[M]{\\prod_{i=1}^{m}{\\frac{1}{P(w_i|w_{i-1})}}}\n\\end{equation*}\nRemember from calculus:\n\\begin{equation*}\n\\sqrt[M]{\\frac{1}{x}} = x^{-\\frac{1}{M}}\n\\end{equation*}\nHere is a code that will help we with the formula.\n\n# to calculate the exponent, use the following syntax\np = 10 ** (-250)\nM = 100\nperplexity = p ** (-1 / M)\nprint(perplexity)\n\n316.22776601683796\n\n\nThat‚Äôs all for the lab for ‚ÄúN-gram language model‚Äù lesson of week 3.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "L2 - Building the language model"
    ]
  },
  {
    "objectID": "posts/c2w1/index.html#lab-candidates-from-edits",
    "href": "posts/c2w1/index.html#lab-candidates-from-edits",
    "title": "Autocorrect and minimum edit distance",
    "section": "Lab: Candidates from Edits",
    "text": "Lab: Candidates from Edits\nCandidates from Edits",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "Notes"
    ]
  },
  {
    "objectID": "posts/c3w2/assignment.html",
    "href": "posts/c3w2/assignment.html",
    "title": "Assignment 2: Deep N-grams",
    "section": "",
    "text": "course banner\nWelcome to the second assignment of course 3. In this assignment you will explore Recurrent Neural Networks RNN. - You will be using the fundamentals of google‚Äôs trax package to implement any kind of deeplearning model.\nBy completing this assignment, you will learn how to implement models from scratch: - How to convert a line of text into a tensor - Create an iterator to feed data to the model - Define a GRU model using trax - Train the model using trax - Compute the accuracy of your model using the perplexity - Predict using your own model",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "A2 - Deep N-grams"
    ]
  },
  {
    "objectID": "posts/c3w2/assignment.html#outline",
    "href": "posts/c3w2/assignment.html#outline",
    "title": "Assignment 2: Deep N-grams",
    "section": "Outline",
    "text": "Outline\n\nOverview\nPart 1: Importing the Data\n\n1.1 Loading in the data\n1.2 Convert a line to tensor\n\nExercise 01\n\n1.3 Batch generator\n\nExercise 02\n\n1.4 Repeating Batch generator\n\n\nPart 2: Defining the GRU model\n\nExercise 03\n\nPart 3: Training\n\n3.1 Training the Model\n\nExercise 04\n\n\nPart 4: Evaluation\n\n4.1 Evaluating using the deep nets\n\nExercise 05\n\n\nPart 5: Generating the language with your own model\n\nSummary\n\n\n\nOverview\nYour task will be to predict the next set of characters using the previous characters. - Although this task sounds simple, it is pretty useful. - You will start by converting a line of text into a tensor - Then you will create a generator to feed data into the model - You will train a neural network in order to predict the new set of characters of defined length. - You will use embeddings for each character and feed them as inputs to your model. - Many natural language tasks rely on using embeddings for predictions. - Your model will convert each character to its embedding, run the embeddings through a Gated Recurrent Unit GRU, and run it through a linear layer to predict the next set of characters.\n\nThe figure above gives you a summary of what you are about to implement. - You will get the embeddings; - Stack the embeddings on top of each other; - Run them through two layers with a relu activation in the middle; - Finally, you will compute the softmax.\nTo predict the next character: - Use the softmax output and identify the word with the highest probability. - The word with the highest probability is the prediction for the next word.\n\nimport os\nimport trax\nimport trax.fastmath.numpy as np\nimport pickle\nimport numpy\nimport random as rnd\nfrom trax import fastmath\nfrom trax import layers as tl\n\n# set random seed\ntrax.supervised.trainer_lib.init_random_number_generators(32)\nrnd.seed(32)\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[89], line 11\n      8 from trax import layers as tl\n     10 # set random seed\n---&gt; 11 trax.supervised.trainer_lib.init_random_number_generators(32)\n     12 rnd.seed(32)\n\nAttributeError: module 'trax.supervised.trainer_lib' has no attribute 'init_random_number_generators'",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "A2 - Deep N-grams"
    ]
  }
]