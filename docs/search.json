[
  {
    "objectID": "posts/2025-02-10-summerizer/index.html",
    "href": "posts/2025-02-10-summerizer/index.html",
    "title": "Summarization Task",
    "section": "",
    "text": "In the Deeplearning.ai NLP specialization, we implemented both Q&A and Summarization tasks. However, we only looked at these tasks in terms of the attention models. It is now time to dive deeper into the summarization task. In this article I will review a talk covering a body of research on summarization, which has many ideas about features. Consider some of these and see what aspects are relevant to a modern task implementation.\nSome of the tasks here can be done with a simple LSTM or Transformer. However it seems that RL may be useful if we want to plan the summary in a non-linear fashion or to make use of a dynamic programming approach. For instance to identify the narrative structure might require many more steps then we typically use in a summerization task. For a speeches we may want to capture a gist of the rhetoric."
  },
  {
    "objectID": "posts/2025-02-10-summerizer/index.html#motivation",
    "href": "posts/2025-02-10-summerizer/index.html#motivation",
    "title": "Summarization Task",
    "section": "",
    "text": "In the Deeplearning.ai NLP specialization, we implemented both Q&A and Summarization tasks. However, we only looked at these tasks in terms of the attention models. It is now time to dive deeper into the summarization task. In this article I will review a talk covering a body of research on summarization, which has many ideas about features. Consider some of these and see what aspects are relevant to a modern task implementation.\nSome of the tasks here can be done with a simple LSTM or Transformer. However it seems that RL may be useful if we want to plan the summary in a non-linear fashion or to make use of a dynamic programming approach. For instance to identify the narrative structure might require many more steps then we typically use in a summerization task. For a speeches we may want to capture a gist of the rhetoric."
  },
  {
    "objectID": "posts/2025-02-10-summerizer/index.html#ideas",
    "href": "posts/2025-02-10-summerizer/index.html#ideas",
    "title": "Summarization Task",
    "section": "Ideas",
    "text": "Ideas\nSeveral ideas surfaced while working on the assignment for building a hybrid generative/abstractive summarizer based on GPT21. Many ideas came from prior work developing search engines. I had noticed that some issues were anathema to summarization from its inception.\n1 a Generative Pre-training Transformer or a decoder transformer\nCoverage ranking content by importance\nThe first issue is coverage, which is how much of the original document to include. Coverage is a much more difficult problem than it appears. For example, technical documents, like patents, headings, academic writing cues, and even IR statistics, may serve as features that we may feed into a regression that can guide us in discarding the chuff from the grain. On the other hand, for movie scripts or novels, we can’t use all that, and we need to decide what is essential based on the narrative structure, which requires sophisticated processing and extensive domain knowledge to extract. So, When we want to create a synopsis of a book like War and Peace, specific details are part of the narrative structure that stands out as essential to us, and I can say that even in 2025, LLM is not good at picking these out of a large document. For attentive readers with a suitable background, if we double the length of the text, they might pick additional plot points and then less significant subplots. If again we double we would, they would include more details concerning characters, their motivations, etc.\nTo conclude, different documents can have radically different structures and cues. These suggest different strategies for selecting and ranking the material to be included in a summary of a given length. More so, when we consider summaries of different lengths, one can see that we are talking about a hierarchal structure."
  },
  {
    "objectID": "posts/2025-02-10-summerizer/index.html#avoiding-repetition",
    "href": "posts/2025-02-10-summerizer/index.html#avoiding-repetition",
    "title": "Summarization Task",
    "section": "Avoiding repetition",
    "text": "Avoiding repetition\nA second challenge that is pervasive is avoiding repetition. In the generative settings we have a tougher challenge since sentences that are generated may well be equivalent to sentences we have already generated, and we want the model to redirect it attention from material already covered. Luckily we have learned to detect similar sentences in the Q&A task2\n2 using Siamese networks for oneshot similarity detection.check if it is similar to any sentence in the summary\nAlg1: similarity detection\nsummary = []\ntokenized_doc = tokenize(doc)\nembeddings_doc= embed(tokenized_doc)\nwhile len(summary) &lt; doc_tokens * summary_ratio: \n    a = gen_a_sentence(embeddings_doc,summary)\n    for s in summary:\n        if sim(a,s) &gt; threshold:\n            continue\n    else:\n        summary.append(a)\ns -&gt; gen a sentence,"
  },
  {
    "objectID": "posts/2025-02-10-summerizer/index.html#context-window-constraints",
    "href": "posts/2025-02-10-summerizer/index.html#context-window-constraints",
    "title": "Summarization Task",
    "section": "Context window constraints",
    "text": "Context window constraints\nThe third challenge is technical and is related to the length of the context window. as transformer models have a limit on the number of tokens that they can process in a context window. A second issue here is that the summary itself needs to also fit in the context window. If the structure is linear and incremental we can chunk it into many smaller pieced say using sections. However in reality we are often dealing with trees of graphs structures and chunking can erode the model’s ability to inspect said hierarchy of the structure it is tasked with summarizing. In Q&A or IR tasks since we can process each chunk separately and then combine the results.\nIf we are not using a generative model one imagines a tree data structure that can efficiently track what what part of the document has been summarized. If we can use that to work with the attention mechanism we may be able to reduce the effective window size as we progress with the summary. A more nuanced idea would come from bayesian search where we may want to keep moving the probability mass for the attention mechanism to regions that are significant but have not been covered.\nAnother challenge is that we may want to grow our summary in a non-linear fashion. We may consider the main narrative structure then add in the subplots and then the character motivations. This suggests two approaches - a top down planning based approach3 and a bottom up approach where we start with the most important details, then add in the less important ones. In the second case we may want to revise the initial sentences to efficiently incorporate more details as we progress.\n3 using rl or dynamic programming4 reformer layersI also interviewed with a company that told me they often worked with patents and that these documents were frequently in excess of a hundred pages and they were having lots of problems with the summarization task. This suggest that an efficient transformer4 would be needed if we are to support context windows that can span hundreds of pages. But that besides the context window we need some good way to ensure that the summary is balanced and that it is not too repetitive."
  },
  {
    "objectID": "posts/2025-02-10-summerizer/index.html#compassion",
    "href": "posts/2025-02-10-summerizer/index.html#compassion",
    "title": "Summarization Task",
    "section": "Compassion",
    "text": "Compassion\nMy experience with summarization by hand is that most texts can be compressed by 5-10% without losing any information simply by rephrasing with more concise language. So another idea that should be obvious is that the generated summary should be compressive, i.e., once we generate a good sentence we should consider if we can make it shorter. This should be fairly easy as we are reducing the length of a single sentence. We may however be able to do even better by considering a section and trying to see if we can merge two sentences or if two sentences have partial overlap that can be merged or referenced via an anaphora. This is another area where we diverge from Q&A, where we do not have a length constraint and may often want to include all relevant information. Implementation of this idea cam be easily embodied in a loss function that penalizes summaries for each token or character. This same idea can also be used in a RL summarizer."
  },
  {
    "objectID": "posts/2025-02-10-summerizer/index.html#grounding",
    "href": "posts/2025-02-10-summerizer/index.html#grounding",
    "title": "Summarization Task",
    "section": "Grounding",
    "text": "Grounding\nGenerative summarization is a case where we can require that all generated text be grounded in the original document. In reality generative models have many glitches, due to tokenization issues, or failures of the attention mechanism, or out of distribution generation, due to bias learned from the corpus, due to negative recency bias and and due to emergent contradictions (where a contradiction is introduced into the context window and cannot be removed.) And even if we can avoid all these there is still nothing in the model that makes it prefer truthy generations. So it seems essential that the generated text is grounded in the original document. This seems to be verifiable by visually inspecting the using the attention mechanism. In reality there may be multiple heads and multiple layers. This means we need to access the attention mechanism programmatically and store annotations from the source document for each generated token. We may also want to make abstracting summarization explicit. c.f. [Extrinsic Hallucinations in LLMs in ](https://lilianweng.github.io/posts/2024-07-07-hallucination/) by Lilian Weng"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NLP course notes and research notebooks",
    "section": "",
    "text": "Floating Constraints in Lexical Choice\n\n\nreview\n\n\n\n\n\n\n\n\nMonday, February 10, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nSummarization Task\n\n\n\n\n\n\nNLP\n\n\nNotes\n\n\nLiterature review\n\n\nSummarization task\n\n\nRelevance\n\n\nConference talk\n\n\n\nConcepts, slide commentaries and Lecture notes on Automatic text Summarization by Masa Nekic\n\n\n\n\n\nMonday, February 10, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nMorphological Word Embeddings\n\n\nReview\n\n\n\nPaper\n\n\nReview\n\n\nPodcast\n\n\nNLP\n\n\nMorphology\n\n\n\n\n\n\n\n\n\nFriday, February 7, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nPractical and Optimal LSH for Angular Distance\n\n\nReview\n\n\n\nPaper\n\n\nReview\n\n\n\n\n\n\n\n\n\nWednesday, February 5, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nFinetuning Pretrained Transformers into RNNs\n\n\nReview\n\n\n\nPaper\n\n\nReview\n\n\nLSTM\n\n\nRNN\n\n\nTransformer\n\n\n\n\n\n\n\n\n\nMonday, February 3, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nxLSTM: Extended Long Short-Term Memory\n\n\nReview\n\n\n\nPaper\n\n\nReview\n\n\nNLP\n\n\nLSTM\n\n\nSeq2Seqs\n\n\nRNN\n\n\nPodcast\n\n\n\n\n\n\n\n\n\nMonday, February 3, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nThe Secret Life of Pronouns What Our Words Say About Us\n\n\nReview\n\n\n\nReview\n\n\nBook\n\n\nNLP\n\n\nSentiment Analysis\n\n\nSentiment Analysis\n\n\nPodcast\n\n\n\n\n\n\n\n\n\nThursday, January 30, 2025\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nIt’s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk\n\n\nReview\n\n\n\nNLP\n\n\nPaper\n\n\nReview\n\n\nMBR\n\n\nMinimum Bayes Risk\n\n\nASR task\n\n\nAutomatic speech recognition\n\n\nMT task\n\n\nSyntactic Parsing task\n\n\nAMR parsing task\n\n\nQuestion answering task\n\n\nSummarization task\n\n\n\n\n\n\n\n\n\nMonday, May 10, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nGenerating Diverse and High-Quality Texts by Minimum Bayes Risk Decoding\n\n\nReview\n\n\n\nNLP\n\n\nPaper\n\n\nReview\n\n\nMBR\n\n\nMinimum Bayes Risk\n\n\nText Generation\n\n\nDecoding Algorithms\n\n\nEncoder-Decoder Models\n\n\nLanguage Models\n\n\nMachine Translation\n\n\nImage Captioning\n\n\nQuestion Generation\n\n\nCommon Sense Reasoning\n\n\nText Summarization\n\n\n\n\n\n\n\n\n\nMonday, May 10, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nELMo - Deep contextualized word representations\n\n\nReview\n\n\n\nAttention\n\n\nBidirectional LSTM\n\n\nDeep learning\n\n\nEmbeddings\n\n\nLLM\n\n\nNLP\n\n\nPaper\n\n\nReview\n\n\nStub\n\n\nPodcast\n\n\n\nreview of the paper “Deep contextualized word representations” on dot product attention\n\n\n\n\n\nSunday, May 9, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nExposing Attention Glitches with Flip-Flop Language Modeling\n\n\nReview\n\n\n\nAttention\n\n\nLSTM\n\n\nDeep learning\n\n\nLLM\n\n\nNLP\n\n\nPaper\n\n\nPodcast\n\n\nReview\n\n\n\nreview\n\n\n\n\n\nSunday, May 9, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nAttention Is All You Need\n\n\nreview\n\n\n\nNLP\n\n\nPaper\n\n\nAttention\n\n\nDeep learning\n\n\nReview\n\n\nStub\n\n\n\nReview of the 2017 paper “Attention Is All You Need” on the transformer architecture.\n\n\n\n\n\nSaturday, May 8, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nEffective Approaches to Attention-based Neural Machine Translation\n\n\nReview\n\n\n\nNLP\n\n\nPaper\n\n\nAttention\n\n\nDeep learning\n\n\nReview\n\n\nPodcast\n\n\n\nReview of the 2015 paper on Attention-based maechanisms for Neural Machine Translation\n\n\n\n\n\nSaturday, May 8, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nPutting the “Re” in Reformer: Ungraded Lab\n\n\n\n\n\n\n\n\n\n\n\nThursday, April 29, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nReformer Efficient Attention: Ungraded Lab\n\n\n\n\n\n\n\n\n\n\n\nWednesday, April 28, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nChat Bots\n\n\nNLP with Attention Models\n\n\n\nAttention\n\n\nChat bot development\n\n\nCoursera\n\n\nIntelligent agents\n\n\nLocality sensitive hashing\n\n\nNLP with Attention Models\n\n\nNeural Machine Translation\n\n\nNLP\n\n\nNotes\n\n\nPositional encoding\n\n\nQuestion answering task\n\n\nReversible layers\n\n\nTeacher forcing\n\n\nTransformer\n\n\n\nThis week of the NLP Specialization, we explore Chatbot. We will be building Reformer model, an efficient transformer to create intelligent conversational agents.We will learn how to train this model on dialogue datasets and generate responses that mimic human-like conversations. Through hands-on exercises using JAX, we will gain practical experience in building chatbot that can understand and respond to user queries. We will master the skills to develop sophisticated chatbot applications using state-of-the-art NLP techniques\n\n\n\n\n\nTuesday, April 27, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nAutomatic Summarization Task\n\n\n\n\n\n\nNLP\n\n\nNotes\n\n\nLiterature review\n\n\nSummarization task\n\n\nRelevance\n\n\nConference talk\n\n\n\nConcepts, slide commentaries and Lecture notes on Automatic text Summarization by Masa Nekic\n\n\n\n\n\nSaturday, April 24, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 3 Ungraded Sections - Part 2: T5 SQuAD Model\n\n\nNLP with Attention Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nNLP with Attention Models\n\n\n\n\n\n\n\n\n\nTuesday, April 13, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 3 Ungraded Sections - Part 1: BERT Loss Model\n\n\nNLP with Attention Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nNLP with Attention Models\n\n\n\n\n\n\n\n\n\nMonday, April 12, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nSentencePiece and Byte Pair Encoding\n\n\nNLP with Attention Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nNLP with Attention Models\n\n\n\n\n\n\n\n\n\nSunday, April 11, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion Answering\n\n\nNLP with Attention Models\n\n\n\nAttention\n\n\nCoursera\n\n\nDeep Learning Algorithms\n\n\nNLP\n\n\nNotes\n\n\nNLP with Attention Models\n\n\nNeural Machine Translation\n\n\nTransformer\n\n\nTeacher forcing\n\n\nPositional encoding\n\n\nQuestion answering task\n\n\n\nThis week we will dive into Neural Question Answering. We will build advanced models like T5 and BERT to accurately answer questions based on given contexts. We will fine-tune these models to optimize their performance. We will gain practical experience in building question-answering systems.\n\n\n\n\n\nSaturday, April 10, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nThe Transformer Decoder: Ungraded Lab Notebook\n\n\n\n\n\n\n\n\n\n\n\nSaturday, April 3, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nThe Three Ways of Attention and Dot Product Attention: Ungraded Lab Notebook\n\n\n\n\n\n\n\n\n\n\n\nFriday, April 2, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 2 - Text Summarization\n\n\nNLP with Attention Models\n\n\n\nNLP with Attention Models\n\n\nNeural Machine Translation\n\n\nCoursera\n\n\nNotes\n\n\nDeep Learning Algorithms\n\n\nTransformer\n\n\nTeacher forcing\n\n\nPositional encoding\n\n\nGPT2\n\n\nTransformer decoder\n\n\nAttention\n\n\nDot product attention\n\n\nSelf attention\n\n\nCausal attention\n\n\nMulti-head attention\n\n\nSummarization task\n\n\n\nThis week we unlock the secrets of Neural Text Summarization. We will building a powerful Transformer model to extract crucial information and create concise summaries. Through hands-on exercises using JAX, we will learn techniques like beam search and length normalization to enhance the quality of our summaries. Through hands-on exercises we will train our model on a dataset of articles.\n\n\n\n\n\nThursday, April 1, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nCalculating the Bilingual Evaluation Understudy (BLEU) score: Ungraded Lab\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nTuesday, March 23, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nStack Semantics in Trax: Ungraded Lab\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nMonday, March 22, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Machine Translation\n\n\nNLP with Attention Models\n\n\n\nAttention\n\n\nBeam search\n\n\nBLEU\n\n\nROUGE\n\n\nCoursera\n\n\nNLP with Attention Models\n\n\nNotes\n\n\nMachine translation task\n\n\nMBR\n\n\nNLP\n\n\nPositional encoding\n\n\nSeq2Seq\n\n\nTransformer\n\n\nTeacher forcing\n\n\nTranslation task\n\n\nWord alignment\n\n\n\nThis week we dive deep into the Neural Machine Translation. We’ll learn about the encoder-decoder architecture, explore attention mechanisms that enable the model to focus on different parts of the input sequence during translation. In the hands-on exercises, we’ll implement an attention model for English to German translation, train it on a dataset of sentence pairs, and evaluate its performance.\n\n\n\n\n\nSaturday, March 20, 2021\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluate a Siamese model: Ungraded Lecture Notebook\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nSaturday, November 21, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nModified Triplet Loss\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nFriday, November 20, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nCreating a Siamese model using Trax: Ungraded Lecture Notebook\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nThursday, November 19, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nSiamese Networks\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nnotes\n\n\nSequence Models\n\n\nLSTM\n\n\nSiamese networks\n\n\nOne shot learning\n\n\nTriplet loss\n\n\n\nwe cover Neural networks for deep learning, then build a tweet classifier that places tweets into positive or negative sentiment categories, using a DNN.\n\n\n\n\n\nWednesday, November 18, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nVanishing Gradients\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nTuesday, November 17, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nLSTMs and Named Entity Recognition\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nNotes\n\n\nSequence Models\n\n\n\nwe cover Neural networks for deep learning, then build a tweet classifier that places tweets into positive or negative sentiment categories, using a DNN.\n\n\n\n\n\nMonday, November 16, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nCreating a GRU model using Trax: Ungraded Lecture Notebook\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nSaturday, November 14, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nVanilla RNNs, GRUs and the scan function\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nFriday, November 13, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with JAX numpy and calculating perplexity: Ungraded Lecture Notebook\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nThursday, November 12, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nHidden State Activation : Ungraded Lecture Notebook\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nWednesday, November 11, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nRecurrent Neural Networks for Language Modeling\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nNotes\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nTuesday, November 10, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nData generators\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nMonday, November 9, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nClasses and subclasses\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nSunday, November 8, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nTrax : Ungraded Lecture Notebook\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nSequence Models\n\n\n\n\n\n\n\n\n\nSaturday, November 7, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Networks for Sentiment Analysis\n\n\nSequence Models\n\n\n\nNLP\n\n\nCoursera\n\n\nNotes\n\n\nSequence Models\n\n\nSentiment analysis task\n\n\n\nwe cover Neural networks for deep learning, then build a tweet classifier that places tweets into positive or negative sentiment categories, using a DNN.\n\n\n\n\n\nFriday, November 6, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nWord Embeddings: Hands On\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nProbabilistic Models\n\n\n\n\n\n\n\n\n\nWednesday, November 4, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nWord Embeddings: Ungraded Practice Notebook\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nProbabilistic Models\n\n\n\n\n\n\n\n\n\nTuesday, November 3, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nWord Embeddings: Training the CBOW model\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nProbabilistic Models\n\n\n\n\n\n\n\n\n\nMonday, November 2, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nWord Embeddings: Intro to CBOW model, activation functions and working with Numpy\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nProbabilistic Models\n\n\n\n\n\n\n\n\n\nSunday, November 1, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nWord Embeddings First Steps: Data Preparation\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nProbabilistic Models\n\n\n\n\n\n\n\n\n\nFriday, October 30, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nWord embeddings with neural networks\n\n\nProbabilistic Models\n\n\n\nNLP\n\n\nCoursera\n\n\nnotes\n\n\nProbabilistic Models\n\n\n\nExtract features from text into numerical vectors, then build a binary classifier for tweets using logistic regression!\n\n\n\n\n\nThursday, October 29, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nOut of vocabulary words (OOV)\n\n\nProbabilistic Models\n\n\n\nNLP\n\n\nCoursera\n\n\nNotes\n\n\nProbabilistic Models\n\n\n\n\n\n\n\n\n\nTuesday, October 27, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding the language model\n\n\nProbabilistic Models\n\n\n\nNLP\n\n\nCoursera\n\n\nNotes\n\n\nProbabilistic Models\n\n\n\n\n\n\n\n\n\nMonday, October 26, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nN-grams Corpus preprocessing\n\n\nProbabilistic Models\n\n\n\nNLP\n\n\nCoursera\n\n\nNotes\n\n\nProbabilistic Models\n\n\n\n\n\n\n\n\n\nSunday, October 25, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nAutocomplete and Language Models\n\n\nProbabilistic Models\n\n\n\nNLP\n\n\nCoursera\n\n\nNotes\n\n\nProbabilistic Models\n\n\n\nExtract features from text into numerical vectors, then build a binary classifier for tweets using logistic regression!\n\n\n\n\n\nSaturday, October 24, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nParts-of-Speech Tagging - Working with tags and Numpy\n\n\nProbabilistic Models\n\n\n\nNLP\n\n\nCoursera\n\n\nlab\n\n\nProbabilistic Models\n\n\n\nIn this lab we will create a matrix using some tag information and then modify it using different approaches. This will serve as hands-on experience working with Numpy\n\n\n\n\n\nThursday, October 22, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nParts-of-Speech Tagging - First Steps: Working with text files, Creating a Vocabulary and Handling Unknown Words\n\n\nProbabilistic Models\n\n\n\nNLP\n\n\nCoursera\n\n\nlab\n\n\nProbabilistic Models\n\n\n\nExtract features from text into numerical vectors, then build a binary classifier for tweets using logistic regression!\n\n\n\n\n\nWednesday, October 21, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nPart of Speech Tagging and Hidden Markov Models\n\n\nProbabilistic Models\n\n\n\nNLP\n\n\nCoursera\n\n\nNotes\n\n\nProbabilistic Models\n\n\n\nExtract features from text into numerical vectors, then build a binary classifier for tweets using logistic regression!\n\n\n\n\n\nTuesday, October 20, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 1: Auto Correct\n\n\n\n\n\n\n\n\n\n\n\nSunday, October 18, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nCandidates from String Edits\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nLogistic regression\n\n\nSentiment analysis task\n\n\nClassification & Vector Spaces\n\n\n\n\n\n\n\n\n\nSaturday, October 17, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding the vocabulary\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nLogistic regression\n\n\nSentiment analysis task\n\n\nClassification & Vector Spaces\n\n\n\n\n\n\n\n\n\nFriday, October 16, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nAutocorrect and minimum edit distance\n\n\nProbabilistic Models\n\n\n\nNLP\n\n\nCoursera\n\n\nNotes\n\n\nProbabilistic Models\n\n\n\nAuto-correct text with minimum edit distances\n\n\n\n\n\nThursday, October 15, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nHash functions and multiplanes\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nWord Embeddings\n\n\nTranslation task\n\n\nSearch Task\n\n\n\n\n\n\n\n\n\nTuesday, October 13, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nVector manipulation in Python\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nWord Embeddings\n\n\nTranslation task\n\n\nSearch Task\n\n\n\n\n\n\n\n\n\nMonday, October 12, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Translation and Document Search\n\n\nClassification & Vector Spaces\n\n\n\nNLP\n\n\nCoursera\n\n\nnotes\n\n\nWord Embeddings\n\n\nTranslation task\n\n\nSearch Task\n\n\n\nConcepts, code snippets, and slide commentaries for this week’s lesson of the Course notes from the deeplearning.ai natural language programming specialization.\n\n\n\n\n\nSunday, October 11, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nManipulating word embeddings\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nLogistic regression\n\n\nSentiment analysis task\n\n\nClassification & Vector Spaces\n\n\n\n\n\n\n\n\n\nFriday, October 9, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nLinear algebra in Python with NumPy\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nLogistic regression\n\n\nSentiment analysis task\n\n\nClassification & Vector Spaces\n\n\n\n\n\n\n\n\n\nThursday, October 8, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nVector Space Models\n\n\nClassification & Vector Spaces\n\n\n\nNLP\n\n\nCoursera\n\n\nNotes\n\n\nClassification & Vector Spaces\n\n\n\nVector space models capture semantic meaning and relationships between words. You’ll learn how to create word vectors that capture dependencies between words, then visualize their relationships in two dimensions using PCA.\n\n\n\n\n\nWednesday, October 7, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing Naive Bayes\n\n\n\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nLogistic regression\n\n\nSentiment analysis task\n\n\nClassification & Vector Spaces\n\n\n\n\n\n\n\n\n\nTuesday, October 6, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nProbability and Bayes Rule\n\n\nClassification & Vector Spaces\n\n\n\nNLP\n\n\nCoursera\n\n\nCode\n\n\nConditional Probability\n\n\nBayes rule\n\n\nNaïve Bayes\n\n\nLaplace smoothing\n\n\nLog-likelihood\n\n\nClassification\n\n\nSentiment analysis task\n\n\n\nConcepts, code snippets, and slide commentaries for this week’s lesson of the Course notes from the deeplearning.ai natural language programming specialization.\n\n\n\n\n\nTuesday, October 6, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nProbability and Bayes Rule\n\n\nClassification & Vector Spaces\n\n\n\nNLP\n\n\nCoursera\n\n\nClassification & Vector Spaces\n\n\nNotes\n\n\nNaïve Bayes\n\n\nSentiment analysis task\n\n\n\nThe theory behind Bayes’ rule for conditional probabilities, and its application toward building a Naive Bayes tweet classifier\n\n\n\n\n\nMonday, October 5, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nLab: Visualizing tweets & the Logistic Regression model\n\n\nNLP with Classification & Vector Spaces\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nLogistic regression\n\n\nSentiment analysis task\n\n\nClassification & Vector Spaces\n\n\nNLTK\n\n\n\n\n\n\n\n\n\nSunday, October 4, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nLab: Building and Visualizing word frequencies\n\n\nNLP with Classification & Vector Spaces\n\n\n\nClassification & Vector Spaces\n\n\nCoursera\n\n\nLab\n\n\nNLP\n\n\nNLTK\n\n\nSentiment analysis task\n\n\nWord frequencies\n\n\n\n\n\n\n\n\n\nSaturday, October 3, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nLab: Preprocessing\n\n\nNLP with Classification & Vector Spaces\n\n\n\nNLP\n\n\nCoursera\n\n\nLab\n\n\nLogistic regression\n\n\nSentiment analysis task\n\n\nClassification & Vector Spaces\n\n\nNLTK\n\n\n\n\n\n\n\n\n\nFriday, October 2, 2020\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nSentiment Analysis with Logistic Regression\n\n\nNLP with Classification & Vector Spaces\n\n\n\nClassification & Vector Spaces\n\n\nCoursera\n\n\nLogistic regression\n\n\nNLP\n\n\nNotes\n\n\nSentiment analysis task\n\n\n\nExtract features from text into numerical vectors, then build a binary classifier for tweets using logistic regression!\n\n\n\n\n\nThursday, October 1, 2020\n\n\nOren Bochman\n\n\n\n\n\n\nNo matching items\n\n\n  \n\nCitationBibTeX citation:@online{bochman2025,\n  author = {Bochman, Oren},\n  title = {NLP Course Notes and Research Notebooks},\n  date = {2025-02-11},\n  url = {https://orenbochman.github.io/notes-nlp/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2025. “NLP Course Notes and Research Notebooks\n.” February 11, 2025. https://orenbochman.github.io/notes-nlp/."
  },
  {
    "objectID": "notes/c1w1/lab02.html",
    "href": "notes/c1w1/lab02.html",
    "title": "Lab: Building and Visualizing word frequencies",
    "section": "",
    "text": "Figure 1: course banner\nIn this lab, we will focus on the build_freqs() helper function and visualizing a dataset fed into it. In our goal of tweet sentiment analysis, this function will build a dictionary where we can lookup how many times a word appears in the lists of positive or negative tweets. This will be very helpful when extracting the features of the dataset in the week’s programming assignment. Let’s see how this function is implemented under the hood in this notebook.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "L2 - Frequencies"
    ]
  },
  {
    "objectID": "notes/c1w1/lab02.html#setup",
    "href": "notes/c1w1/lab02.html#setup",
    "title": "Lab: Building and Visualizing word frequencies",
    "section": "Setup",
    "text": "Setup\nLet’s import the required libraries for this lab:\n\nimport nltk                                  # Python library for NLP\nfrom nltk.corpus import twitter_samples      # sample Twitter dataset from NLTK\nimport matplotlib.pyplot as plt              # visualization library\nimport numpy as np                           # library for scientific computing and matrix operations\n\n\nImport some helper functions that we provided in the utils.py file:\n\nprocess_tweet(): Cleans the text, tokenizes it into separate words, removes stopwords, and converts words to stems.\nbuild_freqs(): This counts how often a word in the ‘corpus’ (the entire set of tweets) was associated with a positive label 1 or a negative label 0. It then builds the freqs dictionary, where each key is a (word,label) tuple, and the value is the count of its frequency within the corpus of tweets.\n\n\n# download the stopwords for the process_tweet function\nnltk.download('stopwords')\n\n# import our convenience functions\nfrom utils import process_tweet, build_freqs\n\n[nltk_data] Downloading package stopwords to /home/oren/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "L2 - Frequencies"
    ]
  },
  {
    "objectID": "notes/c1w1/lab02.html#load-the-nltk-sample-dataset",
    "href": "notes/c1w1/lab02.html#load-the-nltk-sample-dataset",
    "title": "Lab: Building and Visualizing word frequencies",
    "section": "Load the NLTK sample dataset",
    "text": "Load the NLTK sample dataset\nAs in the previous lab, we will be using the Twitter dataset from NLTK.\n\n# select the lists of positive and negative tweets\nall_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')\n\n# concatenate the lists, 1st part is the positive tweets followed by the negative\ntweets = all_positive_tweets + all_negative_tweets\n\n# let's see how many tweets we have\nprint(\"Number of tweets: \", len(tweets))\n\nNumber of tweets:  10000\n\n\nNext, we will build a labels array that matches the sentiments of our tweets. This data type works pretty much like a regular list but is optimized for computations and manipulation. The labels array will be composed of 10000 elements. The first 5000 will be filled with 1 labels denoting positive sentiments, and the next 5000 will be 0 labels denoting the opposite. We can do this easily with a series of operations provided by the numpy library:\n\nnp.ones() - create an array of 1’s\nnp.zeros() - create an array of 0’s\nnp.append() - concatenate arrays\n\n\n# make a numpy array representing labels of the tweets\nlabels = np.append(np.ones((len(all_positive_tweets))), np.zeros((len(all_negative_tweets))))",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "L2 - Frequencies"
    ]
  },
  {
    "objectID": "notes/c1w1/lab02.html#dictionaries",
    "href": "notes/c1w1/lab02.html#dictionaries",
    "title": "Lab: Building and Visualizing word frequencies",
    "section": "Dictionaries",
    "text": "Dictionaries\nIn Python, a dictionary is a mutable and indexed collection. It stores items as key-value pairs and uses hash tables underneath to allow practically constant time lookups. In NLP, dictionaries are essential because it enables fast retrieval of items or containment checks even with thousands of entries in the collection.\n\nDefinition\nA dictionary in Python is declared using curly brackets. Look at the next example:\n\ndictionary = {'key1': 1, 'key2': 2}\n\nThe former line defines a dictionary with two entries. Keys and values can be almost any type (with a few restriction on keys), and in this case, we used strings. We can also use floats, integers, tuples, etc.\n\n\nAdding or editing entries\nNew entries can be inserted into dictionaries using square brackets. If the dictionary already contains the specified key, its value is overwritten.\n\n# Add a new entry\ndictionary['key3'] = -5\n\n# Overwrite the value of key1\ndictionary['key1'] = 0\n\nprint(dictionary)\n\n{'key1': 0, 'key2': 2, 'key3': -5}\n\n\n\n\nAccessing values and lookup keys\nPerforming dictionary lookups and retrieval are common tasks in NLP. There are two ways to do this:\n\nUsing square bracket notation: This form is allowed if the lookup key is in the dictionary. It produces an error otherwise.\nUsing the get() method: This allows us to set a default value if the dictionary key does not exist.\n\nLet us see these in action:\n\n# Square bracket lookup when the key exist\nprint(dictionary['key2'])\n\n2\n\n\nHowever, if the key is missing, the operation produce an error\n\n# The output of this line is intended to produce a KeyError\nprint(dictionary['key8'])\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nCell In[8], line 2\n      1 # The output of this line is intended to produce a KeyError\n----&gt; 2 print(dictionary['key8'])\n\nKeyError: 'key8'\n\n\n\nWhen using a square bracket lookup, it is common to use an if-else block to check for containment first (with the keyword in) before getting the item. On the other hand, we can use the .get() method if we want to set a default value when the key is not found. Let’s compare these in the cells below:\n\n# This prints a value\nif 'key1' in dictionary:\n    print(\"item found: \", dictionary['key1'])\nelse:\n    print('key1 is not defined')\n\n# Same as what we get with get\nprint(\"item found: \", dictionary.get('key1', -1))\n\nitem found:  0\nitem found:  0\n\n\n\n# This prints a message because the key is not found\nif 'key7' in dictionary:\n    print(dictionary['key7'])\nelse:\n    print('key does not exist!')\n\n# This prints -1 because the key is not found and we set the default to -1\nprint(dictionary.get('key7', -1))\n\nkey does not exist!\n-1",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "L2 - Frequencies"
    ]
  },
  {
    "objectID": "notes/c1w1/lab02.html#word-frequency-dictionary",
    "href": "notes/c1w1/lab02.html#word-frequency-dictionary",
    "title": "Lab: Building and Visualizing word frequencies",
    "section": "Word frequency dictionary",
    "text": "Word frequency dictionary\nNow that we know the building blocks, let’s finally take a look at the build_freqs() function in utils.py. This is the function that creates the dictionary containing the word counts from each corpus.\ndef build_freqs(tweets, ys):\n    \"\"\"Build frequencies.\n    Input:\n        tweets: a list of tweets\n        ys: an m x 1 array with the sentiment label of each tweet\n            (either 0 or 1)\n    Output:\n        freqs: a dictionary mapping each (word, sentiment) pair to its\n        frequency\n    \"\"\"\n    # Convert np array to list since zip needs an iterable.\n    # The squeeze is necessary or the list ends up with one element.\n    # Also note that this is just a NOP if ys is already a list.\n    yslist = np.squeeze(ys).tolist()\n\n    # Start with an empty dictionary and populate it by looping over all tweets\n    # and over all processed words in each tweet.\n    freqs = {}\n    for y, tweet in zip(yslist, tweets):\n        for word in process_tweet(tweet):\n            pair = (word, y)\n            if pair in freqs:\n                freqs[pair] += 1\n            else:\n                freqs[pair] = 1    \n    return freqs\nWe can also do the for loop like this to make it a bit more compact:\n    for y, tweet in zip(yslist, tweets):\n        for word in process_tweet(tweet):\n            pair = (word, y)\n            freqs[pair] = freqs.get(pair, 0) + 1\nAs shown above, each key is a 2-element tuple containing a (word, y) pair. The word is an element in a processed tweet while y is an integer representing the corpus: 1 for the positive tweets and 0 for the negative tweets. The value associated with this key is the number of times that word appears in the specified corpus. For example:\n# \"folowfriday\" appears 25 times in the positive tweets\n('followfriday', 1.0): 25\n\n# \"shame\" appears 19 times in the negative tweets\n('shame', 0.0): 19 \nNow, it is time to use the dictionary returned by the build_freqs() function. First, let us feed our tweets and labels lists then print a basic report:\n\n# create frequency dictionary\nfreqs = build_freqs(tweets, labels)\n\n# check data type\nprint(f'type(freqs) = {type(freqs)}')\n\n# check length of the dictionary\nprint(f'len(freqs) = {len(freqs)}')\n\ntype(freqs) = &lt;class 'dict'&gt;\nlen(freqs) = 13065\n\n\nNow print the frequency of each word depending on its class.\n\nprint(freqs)\n\n{('followfriday', 1.0): 25, ('top', 1.0): 32, ('engag', 1.0): 7, ('member', 1.0): 16, ('commun', 1.0): 33, ('week', 1.0): 83, (':)', 1.0): 3568, ('hey', 1.0): 76, ('jame', 1.0): 7, ('odd', 1.0): 2, (':/', 1.0): 5, ('pleas', 1.0): 97, ('call', 1.0): 37, ('contact', 1.0): 7, ('centr', 1.0): 2, ('02392441234', 1.0): 1, ('abl', 1.0): 8, ('assist', 1.0): 1, ('mani', 1.0): 33, ('thank', 1.0): 620, ('listen', 1.0): 16, ('last', 1.0): 47, ('night', 1.0): 68, ('bleed', 1.0): 2, ('amaz', 1.0): 51, ('track', 1.0): 5, ('scotland', 1.0): 2, ('congrat', 1.0): 21, ('yeaaah', 1.0): 1, ('yipppi', 1.0): 1, ('accnt', 1.0): 2, ('verifi', 1.0): 2, ('rqst', 1.0): 1, ('succeed', 1.0): 1, ('got', 1.0): 69, ('blue', 1.0): 9, ('tick', 1.0): 1, ('mark', 1.0): 1, ('fb', 1.0): 6, ('profil', 1.0): 2, ('15', 1.0): 5, ('day', 1.0): 246, ('one', 1.0): 129, ('irresist', 1.0): 2, ('flipkartfashionfriday', 1.0): 17, ('like', 1.0): 233, ('keep', 1.0): 68, ('love', 1.0): 400, ('custom', 1.0): 4, ('wait', 1.0): 70, ('long', 1.0): 36, ('hope', 1.0): 141, ('enjoy', 1.0): 75, ('happi', 1.0): 211, ('friday', 1.0): 116, ('lwwf', 1.0): 1, ('second', 1.0): 10, ('thought', 1.0): 29, ('’', 1.0): 21, ('enough', 1.0): 18, ('time', 1.0): 127, ('dd', 1.0): 1, ('new', 1.0): 143, ('short', 1.0): 7, ('enter', 1.0): 9, ('system', 1.0): 2, ('sheep', 1.0): 1, ('must', 1.0): 18, ('buy', 1.0): 11, ('jgh', 1.0): 4, ('go', 1.0): 148, ('bayan', 1.0): 1, (':d', 1.0): 629, ('bye', 1.0): 7, ('act', 1.0): 8, ('mischiev', 1.0): 1, ('etl', 1.0): 1, ('layer', 1.0): 1, ('in-hous', 1.0): 1, ('wareh', 1.0): 1, ('app', 1.0): 16, ('katamari', 1.0): 1, ('well', 1.0): 81, ('…', 1.0): 38, ('name', 1.0): 18, ('impli', 1.0): 1, (':p', 1.0): 138, ('influenc', 1.0): 18, ('big', 1.0): 33, ('...', 1.0): 289, ('juici', 1.0): 3, ('selfi', 1.0): 12, ('follow', 1.0): 381, ('perfect', 1.0): 24, ('alreadi', 1.0): 28, ('know', 1.0): 145, (\"what'\", 1.0): 17, ('great', 1.0): 171, ('opportun', 1.0): 23, ('junior', 1.0): 2, ('triathlet', 1.0): 1, ('age', 1.0): 2, ('12', 1.0): 5, ('13', 1.0): 6, ('gatorad', 1.0): 1, ('seri', 1.0): 5, ('get', 1.0): 206, ('entri', 1.0): 4, ('lay', 1.0): 4, ('greet', 1.0): 5, ('card', 1.0): 8, ('rang', 1.0): 3, ('print', 1.0): 3, ('today', 1.0): 108, ('job', 1.0): 41, (':-)', 1.0): 692, (\"friend'\", 1.0): 3, ('lunch', 1.0): 5, ('yummm', 1.0): 1, ('nostalgia', 1.0): 1, ('tb', 1.0): 2, ('ku', 1.0): 1, ('id', 1.0): 8, ('conflict', 1.0): 1, ('help', 1.0): 41, (\"here'\", 1.0): 25, ('screenshot', 1.0): 3, ('work', 1.0): 110, ('hi', 1.0): 173, ('liv', 1.0): 2, ('hello', 1.0): 59, ('need', 1.0): 78, ('someth', 1.0): 28, ('u', 1.0): 175, ('fm', 1.0): 2, ('twitter', 1.0): 29, ('—', 1.0): 27, ('sure', 1.0): 58, ('thing', 1.0): 69, ('dm', 1.0): 39, ('x', 1.0): 72, (\"i'v\", 1.0): 35, ('heard', 1.0): 9, ('four', 1.0): 5, ('season', 1.0): 9, ('pretti', 1.0): 20, ('dope', 1.0): 2, ('penthous', 1.0): 1, ('obv', 1.0): 1, ('gobigorgohom', 1.0): 1, ('fun', 1.0): 58, (\"y'all\", 1.0): 3, ('yeah', 1.0): 47, ('suppos', 1.0): 7, ('lol', 1.0): 64, ('chat', 1.0): 13, ('bit', 1.0): 20, ('youth', 1.0): 19, ('💅🏽', 1.0): 1, ('💋', 1.0): 2, ('seen', 1.0): 10, ('year', 1.0): 43, ('rest', 1.0): 12, ('goe', 1.0): 7, ('quickli', 1.0): 3, ('bed', 1.0): 16, ('music', 1.0): 21, ('fix', 1.0): 10, ('dream', 1.0): 20, ('spiritu', 1.0): 1, ('ritual', 1.0): 1, ('festiv', 1.0): 8, ('népal', 1.0): 1, ('begin', 1.0): 4, ('line-up', 1.0): 4, ('left', 1.0): 13, ('see', 1.0): 184, ('sarah', 1.0): 4, ('send', 1.0): 22, ('us', 1.0): 109, ('email', 1.0): 26, ('bitsy@bitdefender.com', 1.0): 1, (\"we'll\", 1.0): 20, ('asap', 1.0): 5, ('kik', 1.0): 22, ('hatessuc', 1.0): 1, ('32429', 1.0): 1, ('kikm', 1.0): 1, ('lgbt', 1.0): 2, ('tinder', 1.0): 1, ('nsfw', 1.0): 1, ('akua', 1.0): 1, ('cumshot', 1.0): 1, ('come', 1.0): 70, ('hous', 1.0): 7, ('nsn_supplement', 1.0): 1, ('effect', 1.0): 4, ('press', 1.0): 1, ('releas', 1.0): 11, ('distribut', 1.0): 1, ('result', 1.0): 2, ('link', 1.0): 18, ('remov', 1.0): 3, ('pressreleas', 1.0): 1, ('newsdistribut', 1.0): 1, ('bam', 1.0): 44, ('bestfriend', 1.0): 50, ('lot', 1.0): 87, ('warsaw', 1.0): 44, ('&lt;3', 1.0): 134, ('x46', 1.0): 1, ('everyon', 1.0): 58, ('watch', 1.0): 46, ('documentari', 1.0): 1, ('earthl', 1.0): 2, ('youtub', 1.0): 13, ('support', 1.0): 27, ('buuut', 1.0): 1, ('oh', 1.0): 53, ('look', 1.0): 137, ('forward', 1.0): 29, ('visit', 1.0): 30, ('next', 1.0): 48, ('letsgetmessi', 1.0): 1, ('jo', 1.0): 1, ('make', 1.0): 99, ('feel', 1.0): 46, ('better', 1.0): 52, ('never', 1.0): 36, ('anyon', 1.0): 11, ('kpop', 1.0): 1, ('flesh', 1.0): 1, ('good', 1.0): 238, ('girl', 1.0): 44, ('best', 1.0): 65, ('wish', 1.0): 37, ('reason', 1.0): 13, ('epic', 1.0): 2, ('soundtrack', 1.0): 1, ('shout', 1.0): 12, ('ad', 1.0): 14, ('video', 1.0): 34, ('playlist', 1.0): 5, ('would', 1.0): 84, ('dear', 1.0): 17, ('jordan', 1.0): 1, ('okay', 1.0): 39, ('fake', 1.0): 2, ('gameplay', 1.0): 2, (';)', 1.0): 27, ('haha', 1.0): 53, ('im', 1.0): 51, ('kid', 1.0): 18, ('stuff', 1.0): 13, ('exactli', 1.0): 6, ('product', 1.0): 12, ('line', 1.0): 6, ('etsi', 1.0): 1, ('shop', 1.0): 16, ('check', 1.0): 52, ('vacat', 1.0): 6, ('recharg', 1.0): 1, ('normal', 1.0): 6, ('charger', 1.0): 2, ('asleep', 1.0): 9, ('talk', 1.0): 45, ('sooo', 1.0): 6, ('someon', 1.0): 34, ('text', 1.0): 18, ('ye', 1.0): 77, ('bet', 1.0): 6, (\"he'll\", 1.0): 4, ('fit', 1.0): 3, ('hear', 1.0): 33, ('speech', 1.0): 1, ('piti', 1.0): 3, ('green', 1.0): 3, ('garden', 1.0): 7, ('midnight', 1.0): 1, ('sun', 1.0): 6, ('beauti', 1.0): 50, ('canal', 1.0): 1, ('dasvidaniya', 1.0): 1, ('till', 1.0): 18, ('scout', 1.0): 1, ('sg', 1.0): 1, ('futur', 1.0): 13, ('wlan', 1.0): 1, ('pro', 1.0): 5, ('confer', 1.0): 1, ('asia', 1.0): 1, ('chang', 1.0): 24, ('lollipop', 1.0): 1, ('🍭', 1.0): 1, ('nez', 1.0): 1, ('agnezmo', 1.0): 1, ('oley', 1.0): 1, ('mama', 1.0): 1, ('stand', 1.0): 8, ('stronger', 1.0): 1, ('god', 1.0): 20, ('misti', 1.0): 1, ('babi', 1.0): 20, ('cute', 1.0): 26, ('woohoo', 1.0): 3, (\"can't\", 1.0): 43, ('sign', 1.0): 11, ('yet', 1.0): 13, ('still', 1.0): 48, ('think', 1.0): 63, ('mka', 1.0): 5, ('liam', 1.0): 8, ('access', 1.0): 3, ('welcom', 1.0): 73, ('stat', 1.0): 60, ('arriv', 1.0): 67, ('1', 1.0): 75, ('unfollow', 1.0): 63, ('via', 1.0): 69, ('surpris', 1.0): 10, ('figur', 1.0): 5, ('happybirthdayemilybett', 1.0): 1, ('sweet', 1.0): 19, ('talent', 1.0): 5, ('2', 1.0): 58, ('plan', 1.0): 27, ('drain', 1.0): 1, ('gotta', 1.0): 5, ('timezon', 1.0): 1, ('parent', 1.0): 5, ('proud', 1.0): 12, ('least', 1.0): 16, ('mayb', 1.0): 18, ('sometim', 1.0): 13, ('grade', 1.0): 4, ('al', 1.0): 4, ('grand', 1.0): 4, ('manila_bro', 1.0): 2, ('chosen', 1.0): 1, ('let', 1.0): 68, ('around', 1.0): 17, ('..', 1.0): 128, ('side', 1.0): 15, ('world', 1.0): 27, ('eh', 1.0): 2, ('take', 1.0): 43, ('care', 1.0): 18, ('final', 1.0): 30, ('fuck', 1.0): 26, ('weekend', 1.0): 75, ('real', 1.0): 21, ('x45', 1.0): 1, ('join', 1.0): 23, ('hushedcallwithfraydo', 1.0): 1, ('gift', 1.0): 8, ('yeahhh', 1.0): 1, ('hushedpinwithsammi', 1.0): 2, ('event', 1.0): 8, ('might', 1.0): 27, ('luv', 1.0): 6, ('realli', 1.0): 79, ('appreci', 1.0): 31, ('share', 1.0): 46, ('wow', 1.0): 22, ('tom', 1.0): 5, ('gym', 1.0): 4, ('monday', 1.0): 9, ('invit', 1.0): 17, ('scope', 1.0): 5, ('friend', 1.0): 61, ('nude', 1.0): 2, ('sleep', 1.0): 45, ('birthday', 1.0): 74, ('want', 1.0): 96, ('t-shirt', 1.0): 3, ('cool', 1.0): 38, ('haw', 1.0): 1, ('phela', 1.0): 1, ('mom', 1.0): 10, ('obvious', 1.0): 2, ('princ', 1.0): 1, ('charm', 1.0): 1, ('stage', 1.0): 2, ('luck', 1.0): 30, ('tyler', 1.0): 2, ('hipster', 1.0): 1, ('glass', 1.0): 5, ('marti', 1.0): 2, ('glad', 1.0): 43, ('done', 1.0): 54, ('afternoon', 1.0): 10, ('read', 1.0): 34, ('kahfi', 1.0): 1, ('finish', 1.0): 17, ('ohmyg', 1.0): 1, ('yaya', 1.0): 3, ('dub', 1.0): 2, ('stalk', 1.0): 2, ('ig', 1.0): 3, ('gondooo', 1.0): 1, ('moo', 1.0): 2, ('tologooo', 1.0): 1, ('becom', 1.0): 10, ('detail', 1.0): 10, ('zzz', 1.0): 1, ('xx', 1.0): 42, ('physiotherapi', 1.0): 1, ('hashtag', 1.0): 5, ('💪', 1.0): 1, ('monica', 1.0): 1, ('miss', 1.0): 27, ('sound', 1.0): 23, ('morn', 1.0): 101, (\"that'\", 1.0): 67, ('x43', 1.0): 1, ('definit', 1.0): 23, ('tri', 1.0): 44, ('tonight', 1.0): 20, ('took', 1.0): 8, ('advic', 1.0): 6, ('treviso', 1.0): 1, ('concert', 1.0): 24, ('citi', 1.0): 27, ('countri', 1.0): 23, (\"i'll\", 1.0): 90, ('start', 1.0): 61, ('fine', 1.0): 10, ('gorgeou', 1.0): 12, ('xo', 1.0): 2, ('oven', 1.0): 3, ('roast', 1.0): 2, ('garlic', 1.0): 1, ('oliv', 1.0): 1, ('oil', 1.0): 4, ('dri', 1.0): 5, ('tomato', 1.0): 1, ('basil', 1.0): 1, ('centuri', 1.0): 1, ('tuna', 1.0): 1, ('right', 1.0): 47, ('back', 1.0): 98, ('atchya', 1.0): 1, ('even', 1.0): 35, ('almost', 1.0): 10, ('chanc', 1.0): 6, ('cheer', 1.0): 20, ('po', 1.0): 4, ('ice', 1.0): 6, ('cream', 1.0): 6, ('agre', 1.0): 16, ('100', 1.0): 8, ('heheheh', 1.0): 2, ('that', 1.0): 13, ('point', 1.0): 13, ('stay', 1.0): 25, ('home', 1.0): 31, ('soon', 1.0): 47, ('promis', 1.0): 6, ('web', 1.0): 4, ('whatsapp', 1.0): 5, ('volta', 1.0): 1, ('funcionar', 1.0): 1, ('com', 1.0): 2, ('iphon', 1.0): 7, ('jailbroken', 1.0): 1, ('later', 1.0): 16, ('34', 1.0): 3, ('min', 1.0): 9, ('leia', 1.0): 1, ('appear', 1.0): 3, ('hologram', 1.0): 1, ('r2d2', 1.0): 1, ('w', 1.0): 18, ('messag', 1.0): 10, ('obi', 1.0): 1, ('wan', 1.0): 3, ('sit', 1.0): 8, ('luke', 1.0): 6, ('inter', 1.0): 1, ('3', 1.0): 31, ('ucl', 1.0): 1, ('arsen', 1.0): 2, ('small', 1.0): 4, ('team', 1.0): 29, ('pass', 1.0): 12, ('🚂', 1.0): 1, ('dewsburi', 1.0): 2, ('railway', 1.0): 1, ('station', 1.0): 4, ('dew', 1.0): 1, ('west', 1.0): 3, ('yorkshir', 1.0): 2, ('430', 1.0): 1, ('smh', 1.0): 2, ('9:25', 1.0): 1, ('live', 1.0): 26, ('strang', 1.0): 4, ('imagin', 1.0): 5, ('megan', 1.0): 1, ('masaantoday', 1.0): 6, ('a4', 1.0): 3, ('shweta', 1.0): 1, ('tripathi', 1.0): 1, ('5', 1.0): 17, ('20', 1.0): 6, ('kurta', 1.0): 3, ('half', 1.0): 7, ('number', 1.0): 13, ('wsalelov', 1.0): 16, ('ah', 1.0): 13, ('larri', 1.0): 3, ('anyway', 1.0): 16, ('kinda', 1.0): 13, ('goood', 1.0): 4, ('life', 1.0): 49, ('enn', 1.0): 1, ('could', 1.0): 32, ('warmup', 1.0): 1, ('15th', 1.0): 2, ('bath', 1.0): 7, ('dum', 1.0): 2, ('andar', 1.0): 1, ('ram', 1.0): 1, ('sampath', 1.0): 1, ('sona', 1.0): 1, ('mohapatra', 1.0): 1, ('samantha', 1.0): 1, ('edward', 1.0): 1, ('mein', 1.0): 1, ('tulan', 1.0): 1, ('razi', 1.0): 2, ('wah', 1.0): 2, ('josh', 1.0): 1, ('alway', 1.0): 67, ('smile', 1.0): 62, ('pictur', 1.0): 12, ('16.20', 1.0): 1, ('giveitup', 1.0): 1, ('given', 1.0): 3, ('ga', 1.0): 3, ('subsidi', 1.0): 1, ('initi', 1.0): 4, ('propos', 1.0): 3, ('delight', 1.0): 7, ('yesterday', 1.0): 7, ('x42', 1.0): 1, ('lmaoo', 1.0): 2, ('song', 1.0): 22, ('ever', 1.0): 23, ('shall', 1.0): 6, ('littl', 1.0): 31, ('throwback', 1.0): 3, ('outli', 1.0): 1, ('island', 1.0): 5, ('cheung', 1.0): 1, ('chau', 1.0): 1, ('mui', 1.0): 1, ('wo', 1.0): 1, ('total', 1.0): 9, ('differ', 1.0): 11, ('kfckitchentour', 1.0): 2, ('kitchen', 1.0): 4, ('clean', 1.0): 1, (\"i'm\", 1.0): 183, ('cusp', 1.0): 1, ('test', 1.0): 7, ('water', 1.0): 8, ('reward', 1.0): 1, ('arummzz', 1.0): 2, (\"let'\", 1.0): 23, ('drive', 1.0): 11, ('travel', 1.0): 20, ('yogyakarta', 1.0): 3, ('jeep', 1.0): 3, ('indonesia', 1.0): 4, ('instamood', 1.0): 3, ('wanna', 1.0): 30, ('skype', 1.0): 3, ('may', 1.0): 22, ('nice', 1.0): 98, ('friendli', 1.0): 2, ('pretend', 1.0): 2, ('film', 1.0): 9, ('congratul', 1.0): 15, ('winner', 1.0): 4, ('cheesydelight', 1.0): 1, ('contest', 1.0): 6, ('address', 1.0): 10, ('guy', 1.0): 60, ('market', 1.0): 5, ('24/7', 1.0): 1, ('14', 1.0): 1, ('hour', 1.0): 27, ('leav', 1.0): 12, ('without', 1.0): 12, ('delay', 1.0): 2, ('actual', 1.0): 19, ('easi', 1.0): 9, ('guess', 1.0): 14, ('train', 1.0): 10, ('wd', 1.0): 1, ('shift', 1.0): 5, ('engin', 1.0): 2, ('etc', 1.0): 2, ('sunburn', 1.0): 1, ('peel', 1.0): 2, ('blog', 1.0): 31, ('huge', 1.0): 11, ('warm', 1.0): 6, ('☆', 1.0): 3, ('complet', 1.0): 11, ('triangl', 1.0): 2, ('northern', 1.0): 1, ('ireland', 1.0): 2, ('sight', 1.0): 1, ('smthng', 1.0): 2, ('fr', 1.0): 3, ('hug', 1.0): 13, ('xoxo', 1.0): 3, ('uu', 1.0): 1, ('jaann', 1.0): 1, ('topnewfollow', 1.0): 2, ('connect', 1.0): 14, ('wonder', 1.0): 35, ('made', 1.0): 53, ('fluffi', 1.0): 1, ('insid', 1.0): 8, ('pirouett', 1.0): 1, ('moos', 1.0): 1, ('trip', 1.0): 14, ('philli', 1.0): 1, ('decemb', 1.0): 3, (\"i'd\", 1.0): 20, ('dude', 1.0): 6, ('x41', 1.0): 1, ('question', 1.0): 17, ('flaw', 1.0): 1, ('pain', 1.0): 9, ('negat', 1.0): 1, ('strength', 1.0): 3, ('went', 1.0): 12, ('solo', 1.0): 4, ('move', 1.0): 12, ('fav', 1.0): 13, ('nirvana', 1.0): 1, ('smell', 1.0): 2, ('teen', 1.0): 3, ('spirit', 1.0): 3, ('rip', 1.0): 3, ('ami', 1.0): 4, ('winehous', 1.0): 1, ('coupl', 1.0): 9, ('tomhiddleston', 1.0): 1, ('elizabetholsen', 1.0): 1, ('yaytheylookgreat', 1.0): 1, ('goodnight', 1.0): 24, ('vid', 1.0): 11, ('wake', 1.0): 12, ('gonna', 1.0): 21, ('shoot', 1.0): 6, ('itti', 1.0): 2, ('bitti', 1.0): 2, ('teeni', 1.0): 2, ('bikini', 1.0): 3, ('much', 1.0): 89, ('4th', 1.0): 4, ('togeth', 1.0): 7, ('end', 1.0): 20, ('xfile', 1.0): 1, ('content', 1.0): 4, ('rain', 1.0): 21, ('fabul', 1.0): 5, ('fantast', 1.0): 13, ('♡', 1.0): 20, ('jb', 1.0): 1, ('forev', 1.0): 5, ('belieb', 1.0): 3, ('nighti', 1.0): 1, ('bug', 1.0): 3, ('bite', 1.0): 1, ('bracelet', 1.0): 2, ('idea', 1.0): 26, ('foundri', 1.0): 1, ('game', 1.0): 27, ('sens', 1.0): 7, ('pic', 1.0): 27, ('ef', 1.0): 1, ('phone', 1.0): 19, ('woot', 1.0): 2, ('derek', 1.0): 1, ('use', 1.0): 44, ('parkshar', 1.0): 1, ('gloucestershir', 1.0): 1, ('aaaahhh', 1.0): 1, ('man', 1.0): 23, ('traffic', 1.0): 2, ('stress', 1.0): 8, ('reliev', 1.0): 1, (\"how'r\", 1.0): 1, ('arbeloa', 1.0): 1, ('turn', 1.0): 15, ('17', 1.0): 4, ('omg', 1.0): 15, ('say', 1.0): 61, ('europ', 1.0): 1, ('rise', 1.0): 2, ('find', 1.0): 23, ('hard', 1.0): 12, ('believ', 1.0): 9, ('uncount', 1.0): 1, ('coz', 1.0): 3, ('unlimit', 1.0): 1, ('cours', 1.0): 18, ('teamposit', 1.0): 1, ('aldub', 1.0): 2, ('☕', 1.0): 3, ('rita', 1.0): 2, ('info', 1.0): 13, (\"we'd\", 1.0): 4, ('way', 1.0): 46, ('boy', 1.0): 21, ('x40', 1.0): 1, ('true', 1.0): 22, ('sethi', 1.0): 2, ('high', 1.0): 7, ('exe', 1.0): 1, ('skeem', 1.0): 1, ('saam', 1.0): 1, ('peopl', 1.0): 48, ('polit', 1.0): 2, ('izzat', 1.0): 1, ('wese', 1.0): 1, ('trust', 1.0): 9, ('khawateen', 1.0): 1, ('k', 1.0): 9, ('sath', 1.0): 2, ('mana', 1.0): 1, ('kar', 1.0): 1, ('deya', 1.0): 1, ('sort', 1.0): 9, ('smart', 1.0): 5, ('hair', 1.0): 12, ('tbh', 1.0): 5, ('jacob', 1.0): 2, ('g', 1.0): 10, ('upgrad', 1.0): 6, ('tee', 1.0): 2, ('famili', 1.0): 19, ('person', 1.0): 19, ('two', 1.0): 22, ('convers', 1.0): 6, ('onlin', 1.0): 7, ('mclaren', 1.0): 1, ('fridayfeel', 1.0): 5, ('tgif', 1.0): 10, ('squar', 1.0): 1, ('enix', 1.0): 1, ('bissmillah', 1.0): 1, ('ya', 1.0): 23, ('allah', 1.0): 3, (\"we'r\", 1.0): 29, ('socent', 1.0): 1, ('startup', 1.0): 2, ('drop', 1.0): 9, ('your', 1.0): 3, ('arnd', 1.0): 1, ('town', 1.0): 5, ('basic', 1.0): 4, ('piss', 1.0): 3, ('cup', 1.0): 4, ('also', 1.0): 35, ('terribl', 1.0): 2, ('complic', 1.0): 1, ('discuss', 1.0): 3, ('snapchat', 1.0): 36, ('lynettelow', 1.0): 1, ('kikmenow', 1.0): 3, ('snapm', 1.0): 2, ('hot', 1.0): 24, ('amazon', 1.0): 1, ('kikmeguy', 1.0): 3, ('defin', 1.0): 2, ('grow', 1.0): 7, ('sport', 1.0): 4, ('rt', 1.0): 12, ('rakyat', 1.0): 1, ('write', 1.0): 13, ('sinc', 1.0): 15, ('mention', 1.0): 24, ('fli', 1.0): 5, ('fish', 1.0): 3, ('promot', 1.0): 5, ('post', 1.0): 21, ('cyber', 1.0): 1, ('ourdaughtersourprid', 1.0): 5, ('mypapamyprid', 1.0): 2, ('papa', 1.0): 2, ('coach', 1.0): 2, ('posit', 1.0): 8, ('kha', 1.0): 1, ('atleast', 1.0): 2, ('x39', 1.0): 1, ('mango', 1.0): 1, (\"lassi'\", 1.0): 1, (\"monty'\", 1.0): 1, ('marvel', 1.0): 2, ('though', 1.0): 19, ('suspect', 1.0): 3, ('meant', 1.0): 3, ('24', 1.0): 4, ('hr', 1.0): 2, ('touch', 1.0): 15, ('kepler', 1.0): 4, ('452b', 1.0): 5, ('chalna', 1.0): 1, ('hai', 1.0): 11, ('thankyou', 1.0): 14, ('hazel', 1.0): 1, ('food', 1.0): 6, ('brooklyn', 1.0): 1, ('pta', 1.0): 2, ('awak', 1.0): 10, ('okayi', 1.0): 2, ('awww', 1.0): 15, ('ha', 1.0): 23, ('doc', 1.0): 1, ('splendid', 1.0): 1, ('spam', 1.0): 1, ('folder', 1.0): 1, ('amount', 1.0): 1, ('nigeria', 1.0): 1, ('claim', 1.0): 1, ('rted', 1.0): 1, ('leg', 1.0): 5, ('hurt', 1.0): 8, ('bad', 1.0): 18, ('mine', 1.0): 14, ('saturday', 1.0): 8, ('thaaank', 1.0): 1, ('puhon', 1.0): 1, ('happinesss', 1.0): 1, ('tnc', 1.0): 1, ('prior', 1.0): 1, ('notif', 1.0): 2, ('fat', 1.0): 1, ('co', 1.0): 1, ('probabl', 1.0): 9, ('ate', 1.0): 4, ('yuna', 1.0): 2, ('tamesid', 1.0): 1, ('´', 1.0): 3, ('googl', 1.0): 6, ('account', 1.0): 19, ('scouser', 1.0): 1, ('everyth', 1.0): 13, ('zoe', 1.0): 2, ('mate', 1.0): 7, ('liter', 1.0): 6, (\"they'r\", 1.0): 12, ('samee', 1.0): 1, ('edgar', 1.0): 1, ('updat', 1.0): 13, ('log', 1.0): 4, ('bring', 1.0): 17, ('abe', 1.0): 1, ('meet', 1.0): 34, ('x38', 1.0): 1, ('sigh', 1.0): 3, ('dreamili', 1.0): 1, ('pout', 1.0): 1, ('eye', 1.0): 14, ('quacketyquack', 1.0): 7, ('funni', 1.0): 19, ('happen', 1.0): 16, ('phil', 1.0): 1, ('em', 1.0): 3, ('del', 1.0): 1, ('rodder', 1.0): 1, ('els', 1.0): 10, ('play', 1.0): 46, ('newest', 1.0): 1, ('gamejam', 1.0): 1, ('irish', 1.0): 2, ('literatur', 1.0): 2, ('inaccess', 1.0): 2, (\"kareena'\", 1.0): 2, ('fan', 1.0): 30, ('brain', 1.0): 13, ('dot', 1.0): 11, ('braindot', 1.0): 11, ('fair', 1.0): 5, ('rush', 1.0): 1, ('either', 1.0): 11, ('brandi', 1.0): 1, ('18', 1.0): 5, ('carniv', 1.0): 1, ('men', 1.0): 10, ('put', 1.0): 17, ('mask', 1.0): 3, ('xavier', 1.0): 1, ('forneret', 1.0): 1, ('jennif', 1.0): 1, ('site', 1.0): 9, ('free', 1.0): 37, ('50.000', 1.0): 3, ('8', 1.0): 10, ('ball', 1.0): 7, ('pool', 1.0): 5, ('coin', 1.0): 5, ('edit', 1.0): 7, ('trish', 1.0): 1, ('♥', 1.0): 19, ('grate', 1.0): 5, ('three', 1.0): 10, ('comment', 1.0): 8, ('wakeup', 1.0): 1, ('besid', 1.0): 2, ('dirti', 1.0): 2, ('sex', 1.0): 6, ('lmaooo', 1.0): 1, ('😤', 1.0): 2, ('loui', 1.0): 4, (\"he'\", 1.0): 11, ('throw', 1.0): 3, ('caus', 1.0): 15, ('inspir', 1.0): 7, ('ff', 1.0): 48, ('twoof', 1.0): 3, ('gr8', 1.0): 1, ('wkend', 1.0): 3, ('kind', 1.0): 24, ('exhaust', 1.0): 2, ('word', 1.0): 20, ('cheltenham', 1.0): 1, ('area', 1.0): 4, ('kale', 1.0): 1, ('crisp', 1.0): 1, ('ruin', 1.0): 5, ('x37', 1.0): 1, ('open', 1.0): 12, ('worldwid', 1.0): 2, ('outta', 1.0): 1, ('sfvbeta', 1.0): 1, ('vantast', 1.0): 1, ('xcylin', 1.0): 1, ('bundl', 1.0): 1, ('show', 1.0): 28, ('internet', 1.0): 2, ('price', 1.0): 4, ('realisticli', 1.0): 1, ('pay', 1.0): 8, ('net', 1.0): 1, ('educ', 1.0): 1, ('power', 1.0): 7, ('weapon', 1.0): 1, ('nelson', 1.0): 1, ('mandela', 1.0): 1, ('recent', 1.0): 9, ('j', 1.0): 3, ('chenab', 1.0): 1, ('flow', 1.0): 5, ('pakistan', 1.0): 2, ('incredibleindia', 1.0): 1, ('teenchoic', 1.0): 10, ('choiceinternationalartist', 1.0): 9, ('superjunior', 1.0): 9, ('caught', 1.0): 4, ('first', 1.0): 50, ('salmon', 1.0): 3, ('super-blend', 1.0): 1, ('project', 1.0): 6, ('youth@bipolaruk.org.uk', 1.0): 1, ('awesom', 1.0): 42, ('stream', 1.0): 14, ('alma', 1.0): 1, ('mater', 1.0): 1, ('highschoolday', 1.0): 1, ('clientvisit', 1.0): 1, ('faith', 1.0): 3, ('christian', 1.0): 1, ('school', 1.0): 9, ('lizaminnelli', 1.0): 1, ('upcom', 1.0): 2, ('uk', 1.0): 4, ('😄', 1.0): 5, ('singl', 1.0): 6, ('hill', 1.0): 4, ('everi', 1.0): 26, ('beat', 1.0): 10, ('wrong', 1.0): 10, ('readi', 1.0): 25, ('natur', 1.0): 1, ('pefumeri', 1.0): 1, ('workshop', 1.0): 3, ('neal', 1.0): 1, ('yard', 1.0): 1, ('covent', 1.0): 1, ('tomorrow', 1.0): 40, ('fback', 1.0): 27, ('indo', 1.0): 1, ('harmo', 1.0): 1, ('americano', 1.0): 1, ('rememb', 1.0): 16, ('aww', 1.0): 10, ('head', 1.0): 14, ('saw', 1.0): 19, ('dark', 1.0): 6, ('handshom', 1.0): 1, ('juga', 1.0): 1, ('hurray', 1.0): 1, ('hate', 1.0): 13, ('cant', 1.0): 15, ('decid', 1.0): 4, ('save', 1.0): 12, ('list', 1.0): 15, ('hiya', 1.0): 4, ('exec', 1.0): 1, ('loryn.good@lincs-chamber.co.uk', 1.0): 1, ('photo', 1.0): 19, ('thx', 1.0): 15, ('4', 1.0): 24, ('china', 1.0): 2, ('homosexu', 1.0): 1, ('hyungbot', 1.0): 1, ('give', 1.0): 48, ('fam', 1.0): 5, ('mind', 1.0): 23, ('timetunnel', 1.0): 1, ('1982', 1.0): 1, ('quit', 1.0): 13, ('radio', 1.0): 5, ('set', 1.0): 11, ('heart', 1.0): 11, ('hiii', 1.0): 2, ('jack', 1.0): 3, ('ili', 1.0): 5, ('✨', 1.0): 4, ('domino', 1.0): 1, ('pub', 1.0): 1, ('heat', 1.0): 1, ('prob', 1.0): 5, ('sorri', 1.0): 22, ('hastili', 1.0): 1, ('type', 1.0): 6, ('came', 1.0): 7, ('pakistani', 1.0): 1, ('x36', 1.0): 1, ('3point', 1.0): 1, ('dreamteam', 1.0): 1, ('gooo', 1.0): 2, ('bailey', 1.0): 2, ('pbb', 1.0): 4, ('737gold', 1.0): 3, ('drank', 1.0): 2, ('old', 1.0): 13, ('gotten', 1.0): 2, ('1/2', 1.0): 1, ('welsh', 1.0): 1, ('wale', 1.0): 3, ('yippe', 1.0): 1, ('💟', 1.0): 4, ('bro', 1.0): 24, ('lord', 1.0): 4, ('michael', 1.0): 4, (\"u'r\", 1.0): 1, ('ure', 1.0): 1, ('bigot', 1.0): 1, ('usual', 1.0): 6, ('front', 1.0): 4, ('squat', 1.0): 1, ('dobar', 1.0): 1, ('dan', 1.0): 5, ('brand', 1.0): 8, ('heavi', 1.0): 2, ('musicolog', 1.0): 1, ('2015', 1.0): 16, ('spend', 1.0): 2, ('marathon', 1.0): 1, ('iflix', 1.0): 2, ('offici', 1.0): 10, ('graduat', 1.0): 3, ('cri', 1.0): 9, ('__', 1.0): 1, ('yep', 1.0): 9, ('expert', 1.0): 4, ('bisexu', 1.0): 1, ('minal', 1.0): 1, ('aidzin', 1.0): 1, ('yo', 1.0): 7, ('pi', 1.0): 1, ('cook', 1.0): 2, ('book', 1.0): 21, ('dinner', 1.0): 7, ('tough', 1.0): 2, ('choic', 1.0): 8, ('other', 1.0): 12, ('chill', 1.0): 6, ('smu', 1.0): 1, ('oval', 1.0): 1, ('basketbal', 1.0): 1, ('player', 1.0): 4, ('whahahaha', 1.0): 1, ('soamaz', 1.0): 1, ('moment', 1.0): 12, ('onto', 1.0): 3, ('a5', 1.0): 1, ('wardrob', 1.0): 2, ('user', 1.0): 3, ('teamr', 1.0): 1, ('appar', 1.0): 6, ('depend', 1.0): 2, ('greatli', 1.0): 1, ('design', 1.0): 21, ('ahhh', 1.0): 1, ('7th', 1.0): 1, ('cinepambata', 1.0): 1, ('mechan', 1.0): 1, ('form', 1.0): 4, ('download', 1.0): 10, ('ur', 1.0): 38, ('swisher', 1.0): 1, ('cop', 1.0): 1, ('ducktail', 1.0): 1, ('surreal', 1.0): 3, ('exposur', 1.0): 1, ('sotw', 1.0): 1, ('halesowen', 1.0): 1, ('blackcountryfair', 1.0): 1, ('street', 1.0): 1, ('assess', 1.0): 1, ('mental', 1.0): 4, ('bodi', 1.0): 15, ('ooz', 1.0): 1, ('appeal', 1.0): 1, ('amassiveoverdoseofship', 1.0): 1, ('latest', 1.0): 5, ('isi', 1.0): 1, ('chan', 1.0): 1, ('c', 1.0): 9, ('note', 1.0): 6, ('pkwalasawa', 1.0): 1, ('gemma', 1.0): 1, ('orlean', 1.0): 1, ('fever', 1.0): 2, ('geskenya', 1.0): 1, ('obamainkenya', 1.0): 1, ('magicalkenya', 1.0): 1, ('greatkenya', 1.0): 1, ('allgoodthingsk', 1.0): 1, ('anim', 1.0): 6, ('umaru', 1.0): 1, ('singer', 1.0): 4, ('ship', 1.0): 8, ('order', 1.0): 17, ('room', 1.0): 5, ('car', 1.0): 6, ('gone', 1.0): 5, ('hahaha', 1.0): 14, ('stori', 1.0): 11, ('relat', 1.0): 4, ('label', 1.0): 1, ('worst', 1.0): 3, ('batch', 1.0): 1, ('princip', 1.0): 1, ('due', 1.0): 3, ('march', 1.0): 1, ('wooftast', 1.0): 2, ('receiv', 1.0): 8, ('necessari', 1.0): 1, ('regret', 1.0): 4, ('rn', 1.0): 4, ('whatev', 1.0): 5, ('hat', 1.0): 1, ('success', 1.0): 6, ('abstin', 1.0): 1, ('wtf', 1.0): 3, (\"there'\", 1.0): 11, ('thrown', 1.0): 1, ('middl', 1.0): 2, ('repeat', 1.0): 3, ('relentlessli', 1.0): 1, ('approxim', 1.0): 1, ('oldschool', 1.0): 1, ('runescap', 1.0): 1, ('daaay', 1.0): 1, ('jumma_mubarik', 1.0): 1, ('frnd', 1.0): 1, ('stay_bless', 1.0): 1, ('bless', 1.0): 12, ('pussycat', 1.0): 1, ('main', 1.0): 7, ('launch', 1.0): 4, ('pretoria', 1.0): 1, ('fahrinahmad', 1.0): 1, ('tengkuaaronshah', 1.0): 1, ('eksperimencinta', 1.0): 1, ('tykkäsin', 1.0): 1, ('videosta', 1.0): 1, ('month', 1.0): 13, ('hoodi', 1.0): 2, ('eeep', 1.0): 1, ('yay', 1.0): 16, ('sohappyrightnow', 1.0): 1, ('mmm', 1.0): 1, ('azz-set', 1.0): 1, ('babe', 1.0): 9, ('feedback', 1.0): 11, ('gain', 1.0): 6, ('valu', 1.0): 2, ('peac', 1.0): 8, ('refresh', 1.0): 5, ('manthan', 1.0): 1, ('tune', 1.0): 5, ('fresh', 1.0): 6, ('mother', 1.0): 5, ('determin', 1.0): 2, ('maxfreshmov', 1.0): 2, ('loneliest', 1.0): 1, ('tattoo', 1.0): 3, ('friday.and', 1.0): 1, ('magnific', 1.0): 2, ('e', 1.0): 5, ('achiev', 1.0): 2, ('rashmi', 1.0): 1, ('dedic', 1.0): 2, ('happyfriday', 1.0): 6, ('nearli', 1.0): 4, ('retweet', 1.0): 35, ('alert', 1.0): 1, ('da', 1.0): 5, ('dang', 1.0): 2, ('rad', 1.0): 2, ('fanart', 1.0): 1, ('massiv', 1.0): 1, ('niamh', 1.0): 1, ('fennel', 1.0): 1, ('journal', 1.0): 1, ('land', 1.0): 2, ('copi', 1.0): 5, ('past', 1.0): 7, ('tweet', 1.0): 61, ('yesss', 1.0): 5, ('ariana', 1.0): 2, ('selena', 1.0): 2, ('gomez', 1.0): 1, ('tomlinson', 1.0): 1, ('payn', 1.0): 1, ('caradelevingn', 1.0): 1, ('🌷', 1.0): 1, ('trade', 1.0): 3, ('tire', 1.0): 5, ('nope', 1.0): 7, ('appli', 1.0): 6, ('iamca', 1.0): 1, ('found', 1.0): 15, ('afti', 1.0): 1, ('goodmorn', 1.0): 8, ('prokabaddi', 1.0): 1, ('koel', 1.0): 1, ('mallick', 1.0): 1, ('recit', 1.0): 4, ('nation', 1.0): 3, ('anthem', 1.0): 1, ('6', 1.0): 23, ('yournaturallead', 1.0): 1, ('youngnaturallead', 1.0): 1, ('mon', 1.0): 3, ('27juli', 1.0): 1, ('cumbria', 1.0): 1, ('flockstar', 1.0): 1, ('thur', 1.0): 2, ('30juli', 1.0): 1, ('itv', 1.0): 1, ('sleeptight', 1.0): 1, ('haveagoodday', 1.0): 1, ('septemb', 1.0): 5, ('perhap', 1.0): 3, ('bb', 1.0): 4, ('full', 1.0): 19, ('album', 1.0): 6, ('fulli', 1.0): 2, ('intend', 1.0): 1, ('possibl', 1.0): 7, ('attack', 1.0): 3, ('&gt;:d', 1.0): 4, ('bird', 1.0): 4, ('teamadmicro', 1.0): 1, ('fridaydownpour', 1.0): 1, ('clear', 1.0): 4, ('rohit', 1.0): 1, ('queen', 1.0): 8, ('otwolgrandtrail', 1.0): 3, ('sheer', 1.0): 1, ('fact', 1.0): 8, ('obama', 1.0): 1, ('innumer', 1.0): 1, ('presid', 1.0): 2, ('ni', 1.0): 3, ('shauri', 1.0): 1, ('yako', 1.0): 1, ('memotohat', 1.0): 1, ('sunday', 1.0): 9, ('pamper', 1.0): 2, (\"t'wa\", 1.0): 1, ('cabincrew', 1.0): 1, ('interview', 1.0): 5, ('langkawi', 1.0): 1, ('1st', 1.0): 1, ('august', 1.0): 7, ('fulfil', 1.0): 5, ('fantasi', 1.0): 6, ('👉', 1.0): 6, ('ex-tweleb', 1.0): 1, ('apart', 1.0): 2, ('makeov', 1.0): 1, ('brilliantli', 1.0): 1, ('happyyi', 1.0): 1, ('birthdaaayyy', 1.0): 2, ('kill', 1.0): 3, ('interest', 1.0): 20, ('internship', 1.0): 3, ('program', 1.0): 5, ('sadli', 1.0): 1, ('career', 1.0): 3, ('page', 1.0): 9, ('issu', 1.0): 10, ('sad', 1.0): 5, ('overwhelmingli', 1.0): 1, ('aha', 1.0): 2, ('beaut', 1.0): 2, ('♬', 1.0): 2, ('win', 1.0): 16, ('deo', 1.0): 1, ('faaabul', 1.0): 1, ('freebiefriday', 1.0): 4, ('aluminiumfre', 1.0): 1, ('stayfresh', 1.0): 1, ('john', 1.0): 6, ('worri', 1.0): 18, ('navig', 1.0): 1, ('thnk', 1.0): 1, ('progrmr', 1.0): 1, ('9pm', 1.0): 1, ('9am', 1.0): 2, ('hardli', 1.0): 1, ('rose', 1.0): 4, ('emot', 1.0): 3, ('poetri', 1.0): 1, ('frequentfly', 1.0): 1, ('break', 1.0): 10, ('apolog', 1.0): 4, ('kb', 1.0): 1, ('londondairi', 1.0): 1, ('icecream', 1.0): 2, ('experi', 1.0): 7, ('cover', 1.0): 9, ('sin', 1.0): 1, ('excit', 1.0): 33, (\":')\", 1.0): 2, ('xxx', 1.0): 15, ('jim', 1.0): 1, ('chuckl', 1.0): 1, ('cake', 1.0): 10, ('doh', 1.0): 1, ('500', 1.0): 2, ('subscrib', 1.0): 2, ('reach', 1.0): 1, ('scorch', 1.0): 1, ('summer', 1.0): 17, ('younger', 1.0): 4, ('woman', 1.0): 4, ('stamina', 1.0): 1, ('expect', 1.0): 6, ('anyth', 1.0): 22, ('less', 1.0): 8, ('tweeti', 1.0): 1, ('fab', 1.0): 12, ('dont', 1.0): 13, ('--&gt;', 1.0): 2, ('10', 1.0): 16, ('loner', 1.0): 3, ('introduc', 1.0): 3, ('vs', 1.0): 4, ('alter', 1.0): 1, ('understand', 1.0): 6, ('spread', 1.0): 8, ('problem', 1.0): 19, ('supa', 1.0): 1, ('dupa', 1.0): 1, ('near', 1.0): 6, ('dartmoor', 1.0): 1, ('gold', 1.0): 7, ('colour', 1.0): 4, ('ok', 1.0): 38, ('someday', 1.0): 4, ('r', 1.0): 14, ('dii', 1.0): 1, ('n', 1.0): 17, ('forget', 1.0): 17, ('si', 1.0): 4, ('smf', 1.0): 1, ('ft', 1.0): 4, ('japanes', 1.0): 3, ('import', 1.0): 5, ('kitti', 1.0): 1, ('match', 1.0): 6, ('stationari', 1.0): 1, ('draw', 1.0): 6, ('close', 1.0): 14, ('broken', 1.0): 3, ('specialis', 1.0): 4, ('thermal', 1.0): 4, ('imag', 1.0): 6, ('survey', 1.0): 4, ('–', 1.0): 14, ('south', 1.0): 2, ('korea', 1.0): 3, ('scamper', 1.0): 1, ('slept', 1.0): 4, ('alarm', 1.0): 1, (\"ain't\", 1.0): 5, ('mad', 1.0): 4, ('chweina', 1.0): 1, ('xd', 1.0): 4, ('jotzh', 1.0): 1, ('wast', 1.0): 7, ('place', 1.0): 21, ('worth', 1.0): 11, ('coat', 1.0): 3, ('beforehand', 1.0): 1, ('tho', 1.0): 12, ('foh', 1.0): 2, ('outsid', 1.0): 5, ('holiday', 1.0): 11, ('menac', 1.0): 1, ('jojo', 1.0): 2, ('ta', 1.0): 2, ('accept', 1.0): 1, ('admin', 1.0): 2, ('lukri', 1.0): 1, ('😘', 1.0): 10, ('momma', 1.0): 2, ('bear', 1.0): 2, ('❤', 1.0): 29, ('️', 1.0): 20, ('redid', 1.0): 1, ('8th', 1.0): 1, ('v.ball', 1.0): 1, ('atm', 1.0): 4, ('build', 1.0): 8, ('pack', 1.0): 8, ('suitcas', 1.0): 2, ('hang-copi', 1.0): 1, ('translat', 1.0): 1, (\"dostoevsky'\", 1.0): 1, ('voucher', 1.0): 2, ('bugatti', 1.0): 1, ('bra', 1.0): 3, ('مطعم_هاشم', 1.0): 1, ('yummi', 1.0): 3, ('a7la', 1.0): 1, ('bdayt', 1.0): 1, ('mnwreeen', 1.0): 1, ('jazz', 1.0): 2, ('truck', 1.0): 1, ('x34', 1.0): 1, ('speak', 1.0): 8, ('pbevent', 1.0): 1, ('hq', 1.0): 1, ('add', 1.0): 22, ('yoona', 1.0): 1, ('hairpin', 1.0): 1, ('otp', 1.0): 1, ('collect', 1.0): 7, ('mastership', 1.0): 1, ('honey', 1.0): 4, ('paindo', 1.0): 1, ('await', 1.0): 1, ('report', 1.0): 3, ('manni', 1.0): 1, ('asshol', 1.0): 3, ('brijresid', 1.0): 1, ('structur', 1.0): 1, ('156', 1.0): 1, ('unit', 1.0): 3, ('encompass', 1.0): 1, ('bhk', 1.0): 1, ('flat', 1.0): 2, ('91', 1.0): 2, ('975-580-', 1.0): 1, ('444', 1.0): 1, ('honor', 1.0): 2, ('curri', 1.0): 2, ('clash', 1.0): 1, ('milano', 1.0): 1, ('👌', 1.0): 1, ('followback', 1.0): 6, (':-d', 1.0): 5, ('legit', 1.0): 1, ('loser', 1.0): 5, ('gass', 1.0): 1, ('dead', 1.0): 4, ('starsquad', 1.0): 4, ('⭐', 1.0): 3, ('news', 1.0): 25, ('utc', 1.0): 1, ('flume', 1.0): 1, ('kaytranada', 1.0): 1, ('alunageorg', 1.0): 1, ('ticket', 1.0): 12, ('km', 1.0): 1, ('certainti', 1.0): 1, ('solv', 1.0): 2, ('faster', 1.0): 3, ('👊', 1.0): 1, ('hurri', 1.0): 5, ('totem', 1.0): 1, ('somewher', 1.0): 5, ('alic', 1.0): 4, ('dog', 1.0): 6, ('cat', 1.0): 5, ('goodwynsgoodi', 1.0): 1, ('ugh', 1.0): 1, ('fade', 1.0): 2, ('moan', 1.0): 1, ('leed', 1.0): 1, ('jozi', 1.0): 1, ('wasnt', 1.0): 2, ('fifth', 1.0): 2, ('avail', 1.0): 10, ('tix', 1.0): 2, ('pa', 1.0): 2, ('ba', 1.0): 2, ('ng', 1.0): 2, ('atl', 1.0): 1, ('coldplay', 1.0): 1, ('favorit', 1.0): 14, ('scientist', 1.0): 1, ('yellow', 1.0): 2, ('atla', 1.0): 1, ('yein', 1.0): 1, ('selo', 1.0): 1, ('jabongatpumaurbanstamped', 1.0): 4, ('an', 1.0): 3, ('7', 1.0): 8, ('waiter', 1.0): 1, ('bill', 1.0): 5, ('sir', 1.0): 12, ('titl', 1.0): 2, ('pocket', 1.0): 1, ('wrip', 1.0): 1, ('jean', 1.0): 1, ('conni', 1.0): 2, ('crew', 1.0): 3, ('staff', 1.0): 2, ('sweetan', 1.0): 1, ('ask', 1.0): 37, ('mum', 1.0): 2, ('beg', 1.0): 2, ('soprano', 1.0): 1, ('ukrain', 1.0): 2, ('x33', 1.0): 1, ('olli', 1.0): 2, ('disney.art', 1.0): 1, ('elmoprinssi', 1.0): 1, ('salsa', 1.0): 1, ('danc', 1.0): 2, ('tell', 1.0): 25, ('truth', 1.0): 4, ('pl', 1.0): 8, ('4-6', 1.0): 1, ('2nd', 1.0): 5, ('blogiversari', 1.0): 1, ('review', 1.0): 9, ('cuti', 1.0): 6, ('bohol', 1.0): 1, ('briliant', 1.0): 1, ('v', 1.0): 9, ('key', 1.0): 3, ('annual', 1.0): 1, ('far', 1.0): 19, ('spin', 1.0): 2, ('voic', 1.0): 3, ('\\U000fe334', 1.0): 1, ('yeheyi', 1.0): 1, ('pinya', 1.0): 1, ('whoooah', 1.0): 1, ('tranc', 1.0): 1, ('lover', 1.0): 4, ('subject', 1.0): 7, ('physic', 1.0): 1, ('stop', 1.0): 15, ('ब', 1.0): 1, ('matter', 1.0): 6, ('jungl', 1.0): 1, ('accommod', 1.0): 1, ('secret', 1.0): 9, ('behind', 1.0): 3, ('sandroforceo', 1.0): 2, ('ceo', 1.0): 11, ('1month', 1.0): 11, ('swag', 1.0): 1, ('mia', 1.0): 1, ('workinprogress', 1.0): 1, ('choos', 1.0): 2, ('finnigan', 1.0): 1, ('loyal', 1.0): 2, ('royal', 1.0): 2, ('fotoset', 1.0): 1, ('reus', 1.0): 1, ('seem', 1.0): 10, ('somebodi', 1.0): 1, ('sell', 1.0): 1, ('young', 1.0): 3, ('muntu', 1.0): 1, ('anoth', 1.0): 23, ('gem', 1.0): 2, ('falco', 1.0): 1, ('supersmash', 1.0): 1, ('hotnsexi', 1.0): 1, ('friskyfriday', 1.0): 1, ('beach', 1.0): 4, ('movi', 1.0): 24, ('crop', 1.0): 2, ('nash', 1.0): 1, ('tissu', 1.0): 1, ('chocol', 1.0): 7, ('tea', 1.0): 6, ('hannib', 1.0): 3, ('episod', 1.0): 5, ('hotb', 1.0): 1, ('bush', 1.0): 2, ('classicassur', 1.0): 1, ('thrill', 1.0): 2, ('intern', 1.0): 2, ('assign', 1.0): 1, ('aerial', 1.0): 1, ('camera', 1.0): 6, ('oper', 1.0): 1, ('boom', 1.0): 3, ('hong', 1.0): 1, ('kong', 1.0): 1, ('ferri', 1.0): 1, ('central', 1.0): 2, ('girlfriend', 1.0): 4, ('after-work', 1.0): 1, ('drink', 1.0): 8, ('dj', 1.0): 3, ('resto', 1.0): 1, ('drinkt', 1.0): 1, ('koffi', 1.0): 1, ('a6', 1.0): 1, ('stargat', 1.0): 1, ('atlanti', 1.0): 1, ('muaahhh', 1.0): 1, ('ohh', 1.0): 3, ('hii', 1.0): 2, ('🙈', 1.0): 1, ('di', 1.0): 5, ('nagsend', 1.0): 1, ('yung', 1.0): 1, ('ko', 1.0): 4, ('&lt;/3', 1.0): 1, ('ulit', 1.0): 3, ('🎉', 1.0): 5, ('🎈', 1.0): 1, ('ugli', 1.0): 4, ('legget', 1.0): 1, ('qui', 1.0): 1, ('per', 1.0): 1, ('la', 1.0): 8, ('mar', 1.0): 1, ('encourag', 1.0): 3, ('employ', 1.0): 5, ('board', 1.0): 5, ('sticker', 1.0): 1, ('sponsor', 1.0): 4, ('prize', 1.0): 3, ('(:', 1.0): 1, ('milo', 1.0): 1, ('aurini', 1.0): 1, ('juicebro', 1.0): 1, ('pillar', 1.0): 2, ('respect', 1.0): 2, ('boii', 1.0): 1, ('smashingbook', 1.0): 1, ('bibl', 1.0): 2, ('ill', 1.0): 6, ('sick', 1.0): 4, ('lamo', 1.0): 1, ('fangirl', 1.0): 3, ('platon', 1.0): 1, ('scienc', 1.0): 5, ('resid', 1.0): 2, ('servicewithasmil', 1.0): 1, ('bloodlin', 1.0): 1, ('huski', 1.0): 1, ('obituari', 1.0): 1, ('advert', 1.0): 1, ('goofingaround', 1.0): 1, ('bollywood', 1.0): 1, ('giveaway', 1.0): 6, ('dah', 1.0): 2, ('noth', 1.0): 15, ('bitter', 1.0): 2, ('anger', 1.0): 1, ('hatr', 1.0): 2, ('toward', 1.0): 2, ('pure', 1.0): 2, ('indiffer', 1.0): 1, ('suit', 1.0): 5, ('zach', 1.0): 1, ('codi', 1.0): 2, ('deliv', 1.0): 3, ('ac', 1.0): 1, ('excel', 1.0): 6, ('produc', 1.0): 1, ('boggl', 1.0): 1, ('fatigu', 1.0): 1, ('baareeq', 1.0): 1, ('gamedev', 1.0): 2, ('hobbi', 1.0): 1, ('tweenie_fox', 1.0): 1, ('click', 1.0): 3, ('accessori', 1.0): 1, ('tamang', 1.0): 1, ('hinala', 1.0): 1, ('niam', 1.0): 1, ('selfiee', 1.0): 1, ('especi', 1.0): 4, ('lass', 1.0): 1, ('ale', 1.0): 1, ('swim', 1.0): 3, ('bout', 1.0): 3, ('goodby', 1.0): 5, ('feminist', 1.0): 1, ('fought', 1.0): 1, ('snobbi', 1.0): 1, ('bitch', 1.0): 3, ('carolin', 1.0): 2, ('mighti', 1.0): 1, ('🔥', 1.0): 1, ('threw', 1.0): 2, ('hbd', 1.0): 1, ('follback', 1.0): 19, ('jog', 1.0): 1, ('remot', 1.0): 2, ('newli', 1.0): 1, ('ebay', 1.0): 2, ('store', 1.0): 15, ('disneyinfin', 1.0): 1, ('starwar', 1.0): 1, ('charact', 1.0): 3, ('preorder', 1.0): 1, ('starter', 1.0): 1, ('hit', 1.0): 13, ('snap', 1.0): 4, ('homi', 1.0): 3, ('bought', 1.0): 4, ('skin', 1.0): 8, ('bday', 1.0): 11, ('chant', 1.0): 2, ('jai', 1.0): 1, ('itali', 1.0): 2, ('fast', 1.0): 4, ('heeeyyy', 1.0): 1, ('woah', 1.0): 3, ('★', 1.0): 5, ('😊', 1.0): 11, ('whenev', 1.0): 4, ('ang', 1.0): 2, ('kiss', 1.0): 4, ('philippin', 1.0): 2, ('packag', 1.0): 3, ('bruis', 1.0): 1, ('rib', 1.0): 2, ('😀', 1.0): 2, ('😁', 1.0): 6, ('😂', 1.0): 17, ('😃', 1.0): 1, ('😅', 1.0): 1, ('😉', 1.0): 2, ('tombraid', 1.0): 1, ('hype', 1.0): 1, ('thejuiceinthemix', 1.0): 1, ('rela', 1.0): 1, ('low', 1.0): 6, ('prioriti', 1.0): 1, ('harri', 1.0): 5, ('bc', 1.0): 9, ('collaps', 1.0): 2, ('chaotic', 1.0): 1, ('cosa', 1.0): 1, ('&lt;---', 1.0): 2, ('alliter', 1.0): 1, ('oppayaa', 1.0): 1, (\"how'\", 1.0): 4, ('natgeo', 1.0): 1, ('lick', 1.0): 1, ('elbow', 1.0): 2, ('. .', 1.0): 2, ('“', 1.0): 7, ('emu', 1.0): 1, ('stoke', 1.0): 1, ('woke', 1.0): 5, (\"people'\", 1.0): 3, ('approv', 1.0): 6, (\"god'\", 1.0): 2, ('jisung', 1.0): 1, ('sunshin', 1.0): 7, ('mm', 1.0): 6, ('nicola', 1.0): 1, ('brighten', 1.0): 2, ('helen', 1.0): 3, ('brian', 1.0): 3, ('2-3', 1.0): 1, ('australia', 1.0): 5, ('ol', 1.0): 2, ('bone', 1.0): 1, ('creak', 1.0): 1, ('abuti', 1.0): 1, ('tweetland', 1.0): 1, ('android', 1.0): 3, ('xma', 1.0): 2, ('skyblock', 1.0): 1, ('bcaus', 1.0): 1, ('2009', 1.0): 1, ('die', 1.0): 10, ('twitch', 1.0): 5, ('sympathi', 1.0): 1, ('laugh', 1.0): 5, ('unniee', 1.0): 1, ('nuka', 1.0): 1, ('penacova', 1.0): 1, ('djset', 1.0): 1, ('edm', 1.0): 1, ('kizomba', 1.0): 1, ('latinhous', 1.0): 1, ('housemus', 1.0): 3, ('portug', 1.0): 1, ('wild', 1.0): 2, ('ride', 1.0): 6, ('anytim', 1.0): 6, ('tast', 1.0): 5, ('yer', 1.0): 2, ('mtn', 1.0): 2, ('maganda', 1.0): 1, ('mistress', 1.0): 2, ('saphir', 1.0): 1, ('busi', 1.0): 19, ('4000', 1.0): 1, ('instagram', 1.0): 7, ('among', 1.0): 5, ('coconut', 1.0): 1, ('sambal', 1.0): 1, ('mussel', 1.0): 1, ('recip', 1.0): 5, ('kalin', 1.0): 1, ('mixcloud', 1.0): 1, ('sarcasm', 1.0): 2, ('chelsea', 1.0): 3, ('he', 1.0): 2, ('useless', 1.0): 2, ('thursday', 1.0): 2, ('hang', 1.0): 3, ('hehe', 1.0): 10, ('said', 1.0): 16, ('benson', 1.0): 1, ('facebook', 1.0): 5, ('solid', 1.0): 1, ('16/17', 1.0): 1, ('30', 1.0): 3, ('°', 1.0): 1, ('😜', 1.0): 2, ('maryhick', 1.0): 1, ('kikmeboy', 1.0): 7, ('photooftheday', 1.0): 4, ('musicbiz', 1.0): 2, ('sheskindahot', 1.0): 1, ('fleekil', 1.0): 1, ('mbalula', 1.0): 1, ('africa', 1.0): 1, ('mexican', 1.0): 1, ('scar', 1.0): 1, ('offic', 1.0): 8, ('donut', 1.0): 2, ('foiegra', 1.0): 2, ('despit', 1.0): 2, ('weather', 1.0): 9, ('wed', 1.0): 5, ('toni', 1.0): 2, ('stark', 1.0): 1, ('incred', 1.0): 7, ('poem', 1.0): 2, ('bubbl', 1.0): 3, ('dale', 1.0): 1, ('billion', 1.0): 1, ('magic', 1.0): 5, ('op', 1.0): 3, ('cast', 1.0): 1, ('vote', 1.0): 9, ('elect', 1.0): 1, ('jcreport', 1.0): 1, ('piggin', 1.0): 1, ('botan', 1.0): 2, ('soap', 1.0): 4, ('late', 1.0): 13, ('upload', 1.0): 5, ('freshli', 1.0): 1, ('3week', 1.0): 1, ('heal', 1.0): 1, ('tobi-bro', 1.0): 1, ('isp', 1.0): 1, ('steel', 1.0): 1, ('wednesday', 1.0): 1, ('swear', 1.0): 3, ('met', 1.0): 4, ('earlier', 1.0): 4, ('cam', 1.0): 3, ('😭', 1.0): 2, ('except', 1.0): 2, (\"masha'allah\", 1.0): 1, ('french', 1.0): 5, ('wwat', 1.0): 2, ('franc', 1.0): 5, ('yaaay', 1.0): 3, ('beirut', 1.0): 2, ('coffe', 1.0): 11, ('panda', 1.0): 6, ('eonni', 1.0): 2, ('favourit', 1.0): 13, ('soda', 1.0): 1, ('fuller', 1.0): 1, ('shit', 1.0): 13, ('healthi', 1.0): 2, ('💓', 1.0): 2, ('rettweet', 1.0): 3, ('mvg', 1.0): 1, ('valuabl', 1.0): 1, ('madrid', 1.0): 3, ('sore', 1.0): 6, ('bergerac', 1.0): 1, ('u21', 1.0): 1, ('individu', 1.0): 2, ('adam', 1.0): 1, (\"beach'\", 1.0): 1, ('suicid', 1.0): 1, ('squad', 1.0): 1, ('fond', 1.0): 1, ('christoph', 1.0): 2, ('cocki', 1.0): 1, ('prove', 1.0): 3, (\"attitude'\", 1.0): 1, ('improv', 1.0): 3, ('suggest', 1.0): 6, ('date', 1.0): 12, ('inde', 1.0): 10, ('intellig', 1.0): 3, ('strong', 1.0): 7, ('cs', 1.0): 2, ('certain', 1.0): 2, ('exam', 1.0): 5, ('forgot', 1.0): 3, ('home-bas', 1.0): 1, ('knee', 1.0): 4, ('sale', 1.0): 3, ('fleur', 1.0): 1, ('dress', 1.0): 10, ('readystock_hijabmart', 1.0): 1, ('idr', 1.0): 2, ('325.000', 1.0): 1, ('200.000', 1.0): 1, ('tompolo', 1.0): 1, ('aim', 1.0): 1, ('cannot', 1.0): 4, ('buyer', 1.0): 3, ('disappoint', 1.0): 1, ('paper', 1.0): 4, ('slack', 1.0): 1, ('crack', 1.0): 1, ('particularli', 1.0): 2, ('strike', 1.0): 1, ('31', 1.0): 1, ('mam', 1.0): 2, ('feytyaz', 1.0): 1, ('instant', 1.0): 1, ('stiffen', 1.0): 1, ('ricky_feb', 1.0): 1, ('grindea', 1.0): 1, ('courier', 1.0): 1, ('crypt', 1.0): 1, ('arma', 1.0): 1, ('record', 1.0): 5, ('gosh', 1.0): 2, ('limbo', 1.0): 1, ('orchard', 1.0): 1, ('art', 1.0): 10, ('super', 1.0): 15, ('karachi', 1.0): 2, ('ka', 1.0): 4, ('venic', 1.0): 1, ('sever', 1.0): 3, ('part', 1.0): 15, ('wit', 1.0): 2, ('accumul', 1.0): 1, ('maroon', 1.0): 1, ('cocktail', 1.0): 4, ('0-100', 1.0): 1, ('quick', 1.0): 7, ('1100d', 1.0): 1, ('auto-focu', 1.0): 1, ('manual', 1.0): 2, ('vein', 1.0): 1, ('crackl', 1.0): 1, ('glaze', 1.0): 1, ('layout', 1.0): 3, ('bomb', 1.0): 4, ('social', 1.0): 4, ('websit', 1.0): 8, ('pake', 1.0): 1, ('joim', 1.0): 1, ('feed', 1.0): 4, ('troop', 1.0): 1, ('mail', 1.0): 3, ('ladolcevitainluxembourg@hotmail.com', 1.0): 1, ('prrequest', 1.0): 1, ('journorequest', 1.0): 1, ('the_madstork', 1.0): 1, ('shaun', 1.0): 1, ('bot', 1.0): 4, ('chloe', 1.0): 2, ('actress', 1.0): 3, ('away', 1.0): 13, ('wick', 1.0): 9, ('hola', 1.0): 1, ('juan', 1.0): 1, ('houston', 1.0): 1, ('tx', 1.0): 2, ('jenni', 1.0): 1, (\"year'\", 1.0): 2, ('stumbl', 1.0): 1, ('upon', 1.0): 1, ('prob.nic', 1.0): 1, ('choker', 1.0): 1, ('btw', 1.0): 12, ('seouljin', 1.0): 1, ('photoset', 1.0): 3, ('sadomasochistsparadis', 1.0): 1, ('wynter', 1.0): 1, ('bottom', 1.0): 3, ('outtak', 1.0): 1, ('sadomasochist', 1.0): 1, ('paradis', 1.0): 1, ('ty', 1.0): 8, ('bbi', 1.0): 3, ('clip', 1.0): 1, ('lose', 1.0): 6, ('cypher', 1.0): 1, ('amen', 1.0): 2, ('x32', 1.0): 1, ('plant', 1.0): 4, ('allow', 1.0): 4, ('corner', 1.0): 3, ('addict', 1.0): 4, ('gurl', 1.0): 1, ('suck', 1.0): 9, ('special', 1.0): 8, ('owe', 1.0): 1, ('daniel', 1.0): 2, ('ape', 1.0): 1, ('saar', 1.0): 1, ('ahead', 1.0): 4, ('vers', 1.0): 1, ('butterfli', 1.0): 1, ('bonu', 1.0): 2, ('fill', 1.0): 5, ('tear', 1.0): 1, ('laughter', 1.0): 2, ('5so', 1.0): 6, ('yummmyyi', 1.0): 1, ('eat', 1.0): 6, ('dosa', 1.0): 1, ('easier', 1.0): 2, ('unless', 1.0): 3, ('achi', 1.0): 2, ('youuu', 1.0): 2, ('bawi', 1.0): 1, ('ako', 1.0): 1, ('queenesth', 1.0): 1, ('sharp', 1.0): 2, ('yess', 1.0): 1, ('poldi', 1.0): 1, ('cimbom', 1.0): 1, ('buddi', 1.0): 7, ('bruhhh', 1.0): 1, ('daddi', 1.0): 2, ('”', 1.0): 5, ('knowledg', 1.0): 2, ('attent', 1.0): 4, ('1tb', 1.0): 1, ('bank', 1.0): 1, ('credit', 1.0): 4, ('depart', 1.0): 2, ('anz', 1.0): 1, ('extrem', 1.0): 3, ('offshor', 1.0): 1, ('absolut', 1.0): 9, ('classic', 1.0): 3, ('gottolovebank', 1.0): 1, ('yup', 1.0): 6, ('in-shaa-allah', 1.0): 1, ('dua', 1.0): 1, ('thru', 1.0): 2, ('aameen', 1.0): 2, ('4/5', 1.0): 1, ('coca', 1.0): 1, ('cola', 1.0): 1, ('fanta', 1.0): 1, ('pepsi', 1.0): 1, ('sprite', 1.0): 1, ('all', 1.0): 1, ('sweeeti', 1.0): 1, (';-)', 1.0): 3, ('welcometweet', 1.0): 2, ('psygustokita', 1.0): 4, ('setup', 1.0): 1, ('wet', 1.0): 3, ('feet', 1.0): 3, ('carpet', 1.0): 1, ('judgment', 1.0): 1, ('hypocrit', 1.0): 1, ('narcissist', 1.0): 1, ('jumpsuit', 1.0): 1, ('bt', 1.0): 2, ('denim', 1.0): 1, ('verg', 1.0): 1, ('owl', 1.0): 1, ('constant', 1.0): 1, ('run', 1.0): 12, ('sia', 1.0): 1, ('count', 1.0): 7, ('brilliant', 1.0): 9, ('teacher', 1.0): 1, ('compar', 1.0): 2, ('religion', 1.0): 1, ('rant', 1.0): 1, ('student', 1.0): 6, ('bencher', 1.0): 1, ('1/5', 1.0): 1, ('porsch', 1.0): 1, ('paddock', 1.0): 1, ('budapestgp', 1.0): 1, ('johnyherbert', 1.0): 1, ('roll', 1.0): 5, ('porschesupercup', 1.0): 1, ('koyal', 1.0): 1, ('melodi', 1.0): 1, ('unexpect', 1.0): 4, ('creat', 1.0): 8, ('memori', 1.0): 3, ('35', 1.0): 1, ('ep', 1.0): 3, ('catch', 1.0): 10, ('wirh', 1.0): 1, ('arc', 1.0): 1, ('x31', 1.0): 1, ('wolv', 1.0): 2, ('desir', 1.0): 1, ('ameen', 1.0): 1, ('kca', 1.0): 1, ('votejkt', 1.0): 1, ('48id', 1.0): 1, ('helpinggroupdm', 1.0): 1, ('quot', 1.0): 6, ('weird', 1.0): 5, ('dp', 1.0): 1, ('wife', 1.0): 5, ('poor', 1.0): 4, ('chick', 1.0): 1, ('guid', 1.0): 3, ('zonzofox', 1.0): 3, ('bhaiya', 1.0): 1, ('brother', 1.0): 4, ('lucki', 1.0): 10, ('patti', 1.0): 1, ('elabor', 1.0): 1, ('kuch', 1.0): 1, ('rate', 1.0): 1, ('merdeka', 1.0): 1, ('palac', 1.0): 2, ('hotel', 1.0): 5, ('plusmil', 1.0): 1, ('servic', 1.0): 7, ('hahahaa', 1.0): 1, ('mean', 1.0): 25, ('nex', 1.0): 2, ('safe', 1.0): 5, ('gwd', 1.0): 1, ('she', 1.0): 2, ('okok', 1.0): 1, ('33', 1.0): 4, ('idiot', 1.0): 1, ('chaerin', 1.0): 1, ('unni', 1.0): 1, ('viabl', 1.0): 1, ('altern', 1.0): 3, ('nowaday', 1.0): 2, ('ip', 1.0): 1, ('tombow', 1.0): 1, ('abt', 1.0): 2, ('friyay', 1.0): 2, ('smug', 1.0): 1, ('marrickvil', 1.0): 1, ('public', 1.0): 3, ('ten', 1.0): 1, ('ago', 1.0): 8, ('eighteen', 1.0): 1, ('auvssscr', 1.0): 1, ('ncaaseason', 1.0): 1, ('slow', 1.0): 2, ('popsicl', 1.0): 1, ('soft', 1.0): 2, ('melt', 1.0): 1, ('mouth', 1.0): 2, ('thankyouuu', 1.0): 1, ('dianna', 1.0): 1, ('ngga', 1.0): 1, ('usah', 1.0): 1, ('dipikirin', 1.0): 1, ('elah', 1.0): 1, ('easili', 1.0): 1, (\"who'\", 1.0): 9, ('entp', 1.0): 1, ('killin', 1.0): 1, ('meme', 1.0): 1, ('worthi', 1.0): 1, ('shot', 1.0): 6, ('emon', 1.0): 1, ('decent', 1.0): 2, ('outdoor', 1.0): 1, ('rave', 1.0): 1, ('dv', 1.0): 1, ('aku', 1.0): 1, ('bakal', 1.0): 1, ('liat', 1.0): 1, ('kak', 1.0): 2, ('merri', 1.0): 1, ('tv', 1.0): 5, ('outfit', 1.0): 3, ('---&gt;', 1.0): 1, ('fashionfriday', 1.0): 1, ('angle.nelson', 1.0): 1, ('cheap', 1.0): 1, ('mymonsoonstori', 1.0): 2, ('tree', 1.0): 2, ('lotion', 1.0): 1, ('moistur', 1.0): 1, ('monsoon', 1.0): 1, ('whoop', 1.0): 6, ('romant', 1.0): 2, ('valencia', 1.0): 1, ('daaru', 1.0): 1, ('parti', 1.0): 12, ('chaddi', 1.0): 1, ('wonderful.great', 1.0): 1, ('trim', 1.0): 1, ('pube', 1.0): 1, ('es', 1.0): 2, ('mi', 1.0): 5, ('tio', 1.0): 1, ('sinaloa', 1.0): 1, ('arr', 1.0): 1, ('stylish', 1.0): 1, ('trendi', 1.0): 1, ('kim', 1.0): 5, ('fabfriday', 1.0): 2, ('facetim', 1.0): 4, ('calum', 1.0): 3, ('constantli', 1.0): 1, ('announc', 1.0): 1, ('filbarbarian', 1.0): 1, ('beer', 1.0): 3, ('arm', 1.0): 3, ('testicl', 1.0): 1, ('light', 1.0): 13, ('katerina', 1.0): 1, ('maniataki', 1.0): 1, ('ahh', 1.0): 5, ('alright', 1.0): 6, ('worthwhil', 1.0): 3, ('judg', 1.0): 2, ('tech', 1.0): 2, ('window', 1.0): 7, ('stupid', 1.0): 8, ('plugin', 1.0): 1, ('bass', 1.0): 1, ('slap', 1.0): 1, ('6pm', 1.0): 1, ('door', 1.0): 3, ('vip', 1.0): 1, ('gener', 1.0): 4, ('seat', 1.0): 2, ('earli', 1.0): 9, ('london', 1.0): 9, ('toptravelcentar', 1.0): 1, ('ttctop', 1.0): 1, ('lux', 1.0): 1, ('luxurytravel', 1.0): 1, ('beograd', 1.0): 1, ('srbija', 1.0): 1, ('putovanja', 1.0): 1, ('wendi', 1.0): 2, ('provid', 1.0): 4, ('drainag', 1.0): 1, ('homebound', 1.0): 1, ('hahahay', 1.0): 1, ('yeeeah', 1.0): 1, ('moar', 1.0): 2, ('kitteh', 1.0): 1, ('incom', 1.0): 1, ('tower', 1.0): 2, ('yippee', 1.0): 1, ('scrummi', 1.0): 1, ('bio', 1.0): 5, ('mcpe', 1.0): 1, ('-&gt;', 1.0): 1, ('vainglori', 1.0): 1, ('driver', 1.0): 1, ('6:01', 1.0): 1, ('lilydal', 1.0): 1, ('fss', 1.0): 1, ('rais', 1.0): 3, ('magicalmysterytour', 1.0): 1, ('chek', 1.0): 2, ('rule', 1.0): 2, ('weebli', 1.0): 1, ('donetsk', 1.0): 1, ('earth', 1.0): 7, ('personalis', 1.0): 1, ('wrap', 1.0): 2, ('stationeri', 1.0): 1, ('adrian', 1.0): 1, ('parcel', 1.0): 2, ('tuesday', 1.0): 7, ('pri', 1.0): 3, ('80', 1.0): 3, ('wz', 1.0): 1, ('pattern', 1.0): 1, ('cut', 1.0): 3, ('buttonhol', 1.0): 1, ('4mi', 1.0): 1, ('famou', 1.0): 1, ('client', 1.0): 1, ('p', 1.0): 3, ('aliv', 1.0): 2, ('trial', 1.0): 1, ('spm', 1.0): 1, ('dinooo', 1.0): 1, ('cardio', 1.0): 1, ('steak', 1.0): 1, ('cue', 1.0): 1, ('laptop', 1.0): 1, ('guinea', 1.0): 1, ('pig', 1.0): 1, ('salamat', 1.0): 1, ('sa', 1.0): 6, ('mga', 1.0): 1, ('nag.greet', 1.0): 1, ('guis', 1.0): 1, ('godbless', 1.0): 2, ('crush', 1.0): 3, ('appl', 1.0): 4, ('deserv', 1.0): 11, ('charl', 1.0): 1, ('workhard', 1.0): 1, ('model', 1.0): 7, ('forrit', 1.0): 1, ('bread', 1.0): 2, ('bacon', 1.0): 2, ('butter', 1.0): 2, ('afang', 1.0): 2, ('soup', 1.0): 2, ('semo', 1.0): 2, ('brb', 1.0): 1, ('forc', 1.0): 2, ('doesnt', 1.0): 5, ('tato', 1.0): 1, ('bulat', 1.0): 1, ('concern', 1.0): 1, ('snake', 1.0): 1, ('perform', 1.0): 3, ('con', 1.0): 1, ('todayyy', 1.0): 1, ('max', 1.0): 2, ('gaza', 1.0): 1, ('bbb', 1.0): 1, ('pc', 1.0): 3, ('22', 1.0): 2, ('legal', 1.0): 1, ('ditch', 1.0): 2, ('tori', 1.0): 1, ('bajrangibhaijaanhighestweek', 1.0): 6, (\"s'okay\", 1.0): 1, ('andi', 1.0): 2, ('you-and', 1.0): 1, ('return', 1.0): 3, ('tuitutil', 1.0): 1, ('bud', 1.0): 2, ('learn', 1.0): 8, ('takeaway', 1.0): 1, ('instead', 1.0): 7, ('1hr', 1.0): 1, ('genial', 1.0): 1, ('competit', 1.0): 1, ('yosh', 1.0): 1, ('procrastin', 1.0): 1, ('plu', 1.0): 4, ('kfc', 1.0): 2, ('itun', 1.0): 1, ('dedicatedfan', 1.0): 1, ('💜', 1.0): 7, ('daft', 1.0): 1, ('teeth', 1.0): 1, ('troubl', 1.0): 1, ('huxley', 1.0): 1, ('basket', 1.0): 2, ('ben', 1.0): 2, ('sent', 1.0): 8, ('gamer', 1.0): 3, ('activ', 1.0): 5, ('120', 1.0): 2, ('distanc', 1.0): 2, ('suitabl', 1.0): 1, ('stockholm', 1.0): 1, ('zack', 1.0): 1, ('destroy', 1.0): 1, ('heel', 1.0): 2, ('claw', 1.0): 1, ('q', 1.0): 2, ('blond', 1.0): 2, ('box', 1.0): 3, ('cheerio', 1.0): 1, ('seed', 1.0): 4, ('cutest', 1.0): 2, ('ffback', 1.0): 2, ('spotifi', 1.0): 3, (\"we'v\", 1.0): 7, ('vc', 1.0): 1, ('tgp', 1.0): 1, ('race', 1.0): 5, ('averag', 1.0): 2, (\"joe'\", 1.0): 1, ('bluejay', 1.0): 1, ('vinylbear', 1.0): 1, ('pal', 1.0): 1, ('furbabi', 1.0): 1, ('luff', 1.0): 1, ('mega', 1.0): 4, ('retail', 1.0): 4, ('boot', 1.0): 2, ('whsmith', 1.0): 1, ('ps3', 1.0): 1, ('shannon', 1.0): 1, ('na', 1.0): 9, ('redecor', 1.0): 1, ('bob', 1.0): 3, ('elli', 1.0): 4, ('mairi', 1.0): 1, ('workout', 1.0): 6, ('impair', 1.0): 1, ('uggghhh', 1.0): 1, ('dam', 1.0): 2, ('dun', 1.0): 2, ('eczema', 1.0): 1, ('suffer', 1.0): 4, ('ndee', 1.0): 1, ('pleasur', 1.0): 14, ('publiliu', 1.0): 1, ('syru', 1.0): 1, ('fear', 1.0): 1, ('death', 1.0): 3, ('dread', 1.0): 1, ('fell', 1.0): 3, ('fuk', 1.0): 1, ('unblock', 1.0): 1, ('tweak', 1.0): 2, ('php', 1.0): 1, ('fall', 1.0): 10, ('oomf', 1.0): 1, ('pippa', 1.0): 1, ('hschool', 1.0): 1, ('bu', 1.0): 3, ('cardi', 1.0): 1, ('everyday', 1.0): 3, ('everytim', 1.0): 3, ('hk', 1.0): 1, (\"why'd\", 1.0): 1, ('acorn', 1.0): 1, ('origin', 1.0): 7, ('c64', 1.0): 1, ('cpu', 1.0): 1, ('consider', 1.0): 1, ('advanc', 1.0): 1, ('onair', 1.0): 1, ('bay', 1.0): 1, ('hold', 1.0): 6, ('river', 1.0): 3, ('0878 0388', 1.0): 1, ('1033', 1.0): 1, ('0272 3306', 1.0): 1, ('70', 1.0): 5, ('rescu', 1.0): 1, ('mutt', 1.0): 1, ('confirm', 1.0): 3, ('deliveri', 1.0): 3, ('switch', 1.0): 2, ('lap', 1.0): 1, ('optim', 1.0): 1, ('lu', 1.0): 1, (':|', 1.0): 1, ('tweetofthedecad', 1.0): 1, ('class', 1.0): 5, ('happiest', 1.0): 2, ('bbmme', 1.0): 3, ('pin', 1.0): 4, ('7df9e60a', 1.0): 1, ('bbm', 1.0): 2, ('bbmpin', 1.0): 2, ('addmeonbbm', 1.0): 1, ('addm', 1.0): 1, (\"today'\", 1.0): 3, ('menu', 1.0): 1, ('marri', 1.0): 3, ('glenn', 1.0): 1, ('what', 1.0): 4, ('height', 1.0): 1, (\"sculptor'\", 1.0): 1, ('ti5', 1.0): 1, ('dota', 1.0): 3, ('nudg', 1.0): 1, ('spot', 1.0): 5, ('tasti', 1.0): 1, ('hilli', 1.0): 1, ('cycl', 1.0): 6, ('england', 1.0): 4, ('scotlandismass', 1.0): 1, ('gen', 1.0): 2, ('vikk', 1.0): 1, ('fna', 1.0): 1, ('mombasa', 1.0): 1, ('tukutanemombasa', 1.0): 1, ('100reasonstovisitmombasa', 1.0): 1, ('karibumombasa', 1.0): 1, ('hanbin', 1.0): 1, ('certainli', 1.0): 4, ('goosnight', 1.0): 1, ('kindli', 1.0): 4, ('familiar', 1.0): 2, ('jealou', 1.0): 4, ('tent', 1.0): 2, ('yea', 1.0): 2, ('cozi', 1.0): 1, ('phenomen', 1.0): 2, ('collab', 1.0): 2, ('gave', 1.0): 4, ('birth', 1.0): 1, ('behav', 1.0): 2, ('monster', 1.0): 1, ('spree', 1.0): 4, ('000', 1.0): 1, ('tank', 1.0): 6, ('outstand', 1.0): 1, ('donat', 1.0): 3, ('h', 1.0): 4, ('contestkiduniya', 1.0): 2, ('mfundo', 1.0): 1, ('och', 1.0): 1, ('hun', 1.0): 4, ('inner', 1.0): 2, ('nerd', 1.0): 2, ('tame', 1.0): 2, ('insidi', 1.0): 1, ('logic', 1.0): 1, ('math', 1.0): 1, ('channel', 1.0): 5, ('continu', 1.0): 4, ('doubt', 1.0): 3, ('300', 1.0): 2, ('sub', 1.0): 2, ('200', 1.0): 3, ('forgiven', 1.0): 1, ('manner', 1.0): 1, ('yhooo', 1.0): 1, ('ngi', 1.0): 1, ('mood', 1.0): 7, ('push', 1.0): 1, ('limit', 1.0): 6, ('obakeng', 1.0): 1, ('goat', 1.0): 1, ('alhamdullilah', 1.0): 1, ('pebbl', 1.0): 1, ('engross', 1.0): 1, ('bing', 1.0): 2, ('scream', 1.0): 2, ('whole', 1.0): 7, ('wide', 1.0): 2, ('🌎', 1.0): 2, ('😧', 1.0): 1, ('wat', 1.0): 2, ('muahhh', 1.0): 1, ('pausetim', 1.0): 1, ('drift', 1.0): 1, ('loos', 1.0): 3, ('campaign', 1.0): 4, ('kickstart', 1.0): 1, ('articl', 1.0): 9, ('jenna', 1.0): 1, ('bellybutton', 1.0): 5, ('inni', 1.0): 4, ('outi', 1.0): 4, ('havent', 1.0): 4, ('delish', 1.0): 1, ('joselito', 1.0): 1, ('freya', 1.0): 1, ('nth', 1.0): 1, ('latepost', 1.0): 1, ('lupet', 1.0): 1, ('mo', 1.0): 2, ('eric', 1.0): 3, ('askaman', 1.0): 1, ('150', 1.0): 1, ('0345', 1.0): 2, ('454', 1.0): 1, ('111', 1.0): 1, ('webz', 1.0): 1, ('oop', 1.0): 5, (\"they'll\", 1.0): 6, ('realis', 1.0): 2, ('anymor', 1.0): 3, ('carmel', 1.0): 1, ('decis', 1.0): 5, ('matt', 1.0): 6, ('@commoncultur', 1.0): 1, ('@connorfranta', 1.0): 1, ('honestli', 1.0): 3, ('explain', 1.0): 3, ('relationship', 1.0): 4, ('pick', 1.0): 15, ('tessnzach', 1.0): 1, ('paperboy', 1.0): 1, ('honest', 1.0): 3, ('reassur', 1.0): 1, ('guysss', 1.0): 3, ('mubank', 1.0): 2, (\"dongwoo'\", 1.0): 1, ('bright', 1.0): 2, ('tommorow', 1.0): 3, ('newyork', 1.0): 1, ('lolll', 1.0): 1, ('twinx', 1.0): 1, ('16', 1.0): 2, ('path', 1.0): 1, ('firmansyahbl', 1.0): 1, ('procedur', 1.0): 1, ('grim', 1.0): 1, ('fandango', 1.0): 1, ('ordinari', 1.0): 1, ('extraordinari', 1.0): 1, ('bo', 1.0): 2, ('birmingham', 1.0): 1, ('oracl', 1.0): 1, ('samosa', 1.0): 1, ('firebal', 1.0): 1, ('shoe', 1.0): 4, ('serv', 1.0): 1, ('sushi', 1.0): 2, ('shoeshi', 1.0): 1, ('�', 1.0): 2, ('lymond', 1.0): 1, ('philippa', 1.0): 2, ('novel', 1.0): 1, ('tara', 1.0): 3, ('. . .', 1.0): 2, ('aur', 1.0): 2, ('han', 1.0): 1, ('imran', 1.0): 3, ('khan', 1.0): 7, ('63', 1.0): 1, ('agaaain', 1.0): 1, ('doli', 1.0): 1, ('siregar', 1.0): 1, ('ninh', 1.0): 1, ('size', 1.0): 5, ('geekiest', 1.0): 1, ('geek', 1.0): 2, ('wallet', 1.0): 3, ('request', 1.0): 4, ('media', 1.0): 4, ('ralli', 1.0): 1, ('rotat', 1.0): 3, ('direct', 1.0): 3, ('eek', 1.0): 1, ('red', 1.0): 6, ('beij', 1.0): 1, ('meni', 1.0): 1, ('tebrik', 1.0): 1, ('etdi', 1.0): 1, ('700', 1.0): 1, ('💗', 1.0): 2, ('rod', 1.0): 1, ('embrac', 1.0): 1, ('actor', 1.0): 1, ('aplomb', 1.0): 1, ('foreveralon', 1.0): 2, ('mysumm', 1.0): 1, ('01482', 1.0): 1, ('333505', 1.0): 1, ('hahahaha', 1.0): 2, ('wear', 1.0): 6, ('uniform', 1.0): 1, ('evil', 1.0): 1, ('owww', 1.0): 1, ('choo', 1.0): 1, ('chweet', 1.0): 1, ('shorthair', 1.0): 1, ('oscar', 1.0): 1, ('realiz', 1.0): 7, ('harmoni', 1.0): 1, ('deneriveri', 1.0): 1, ('506', 1.0): 1, ('kiksext', 1.0): 5, ('kikkomansabor', 1.0): 2, ('killer', 1.0): 1, ('henessydiari', 1.0): 1, ('journey', 1.0): 4, ('band', 1.0): 4, ('plz', 1.0): 5, ('convo', 1.0): 3, ('11', 1.0): 5, ('vault', 1.0): 1, ('expand', 1.0): 2, ('vinni', 1.0): 1, ('money', 1.0): 9, ('hahahahaha', 1.0): 2, ('50cent', 1.0): 1, ('repay', 1.0): 1, ('debt', 1.0): 2, ('evet', 1.0): 1, ('wifi', 1.0): 3, ('lifestyl', 1.0): 1, ('qatarday', 1.0): 1, ('. ..', 1.0): 3, ('🌞', 1.0): 3, ('girli', 1.0): 1, ('india', 1.0): 4, ('innov', 1.0): 1, ('volunt', 1.0): 2, ('saran', 1.0): 1, ('drama', 1.0): 3, ('genr', 1.0): 1, ('romanc', 1.0): 1, ('comedi', 1.0): 1, ('leannerin', 1.0): 1, ('19', 1.0): 7, ('porno', 1.0): 1, ('l4l', 1.0): 3, ('weloveyounamjoon', 1.0): 1, ('homey', 1.0): 1, ('kenya', 1.0): 1, ('roller', 1.0): 2, ('coaster', 1.0): 1, ('aspect', 1.0): 1, ('najam', 1.0): 1, ('confess', 1.0): 2, ('pricelessantiqu', 1.0): 1, ('takesonetoknowon', 1.0): 1, ('extra', 1.0): 5, ('ucount', 1.0): 1, ('ji', 1.0): 3, ('turkish', 1.0): 1, ('knew', 1.0): 8, ('crap', 1.0): 1, ('burn', 1.0): 3, ('80x', 1.0): 1, ('airlin', 1.0): 1, ('sexi', 1.0): 10, ('yello', 1.0): 1, ('gail', 1.0): 1, ('yael', 1.0): 1, ('lesson', 1.0): 4, ('en', 1.0): 1, ('mano', 1.0): 1, ('hand', 1.0): 4, ('manag', 1.0): 6, ('prettiest', 1.0): 1, ('reader', 1.0): 4, ('dnt', 1.0): 1, ('ideal', 1.0): 2, ('weekli', 1.0): 2, ('idol', 1.0): 3, ('pose', 1.0): 2, ('shortlist', 1.0): 1, ('dominion', 1.0): 2, ('picnic', 1.0): 2, ('tmrw', 1.0): 3, ('nobodi', 1.0): 2, ('jummamubarak', 1.0): 1, ('shower', 1.0): 3, ('shalwarkameez', 1.0): 1, ('itter', 1.0): 1, ('offer', 1.0): 8, ('jummapray', 1.0): 1, ('af', 1.0): 8, ('display', 1.0): 1, ('enabl', 1.0): 1, ('compani', 1.0): 4, ('peep', 1.0): 4, ('tweep', 1.0): 2, ('folow', 1.0): 1, ('2k', 1.0): 1, ('ohhh', 1.0): 4, ('teaser', 1.0): 2, ('airec', 1.0): 1, ('009', 1.0): 1, ('acid', 1.0): 1, ('mous', 1.0): 2, ('31st', 1.0): 2, ('includ', 1.0): 5, ('robin', 1.0): 1, ('rough', 1.0): 4, ('control', 1.0): 1, ('remix', 1.0): 5, ('fave', 1.0): 3, ('toss', 1.0): 1, ('ladi', 1.0): 8, ('🐑', 1.0): 1, ('librari', 1.0): 3, ('mr2', 1.0): 1, ('climb', 1.0): 1, ('cuddl', 1.0): 1, ('jilla', 1.0): 1, ('headlin', 1.0): 1, ('2017', 1.0): 1, ('jumma', 1.0): 5, ('mubarik', 1.0): 2, ('spent', 1.0): 2, ('congratz', 1.0): 1, ('contribut', 1.0): 3, ('2.0', 1.0): 2, ('yuppiiee', 1.0): 1, ('alienthought', 1.0): 1, ('happyalien', 1.0): 1, ('crowd', 1.0): 2, ('loudest', 1.0): 2, ('gari', 1.0): 1, ('particular', 1.0): 1, ('attract', 1.0): 1, ('supprt', 1.0): 1, ('savag', 1.0): 1, ('cleans', 1.0): 1, ('scam', 1.0): 1, ('ridden', 1.0): 1, ('vyapam', 1.0): 2, ('renam', 1.0): 1, ('wave', 1.0): 2, ('couch', 1.0): 1, ('dodg', 1.0): 1, ('explan', 1.0): 2, ('bag', 1.0): 4, ('sanza', 1.0): 1, ('yaa', 1.0): 3, ('slr', 1.0): 1, ('som', 1.0): 1, ('honour', 1.0): 1, ('heheh', 1.0): 1, ('view', 1.0): 16, ('explor', 1.0): 2, ('wayanadan', 1.0): 1, ('forest', 1.0): 1, ('wayanad', 1.0): 1, ('srijith', 1.0): 1, ('whisper', 1.0): 1, ('lie', 1.0): 4, ('pokemon', 1.0): 1, ('dazzl', 1.0): 1, ('urself', 1.0): 2, ('doubl', 1.0): 2, ('flare', 1.0): 1, ('black', 1.0): 4, ('9', 1.0): 3, ('51', 1.0): 1, ('brows', 1.0): 1, ('bore', 1.0): 9, ('femal', 1.0): 2, ('tour', 1.0): 8, ('delv', 1.0): 2, ('muchhh', 1.0): 1, ('tmr', 1.0): 1, ('breakfast', 1.0): 4, ('gl', 1.0): 1, (\"tonight'\", 1.0): 2, ('):', 1.0): 7, ('litey', 1.0): 1, ('manuella', 1.0): 1, ('abhi', 1.0): 2, ('tak', 1.0): 2, ('nhi', 1.0): 2, ('dekhi', 1.0): 1, ('promo', 1.0): 3, ('se', 1.0): 4, ('xpax', 1.0): 1, ('lisa', 1.0): 2, ('aboard', 1.0): 3, ('institut', 1.0): 1, ('nc', 1.0): 2, ('chees', 1.0): 4, ('overload', 1.0): 1, ('pizza', 1.0): 1, ('•', 1.0): 3, ('mcfloat', 1.0): 1, ('fudg', 1.0): 3, ('sanda', 1.0): 1, ('munchkin', 1.0): 1, (\"d'd\", 1.0): 1, ('granni', 1.0): 1, ('baller', 1.0): 1, ('lil', 1.0): 4, ('chain', 1.0): 1, ('everybodi', 1.0): 1, ('ought', 1.0): 1, ('jay', 1.0): 3, ('events@breastcancernow.org', 1.0): 1, ('79x', 1.0): 1, ('champion', 1.0): 1, ('letter', 1.0): 2, ('uniqu', 1.0): 2, ('affaraid', 1.0): 1, ('dearslim', 1.0): 2, ('role', 1.0): 2, ('billi', 1.0): 2, ('lab', 1.0): 1, ('ovh', 1.0): 2, ('maxi', 1.0): 2, ('bunch', 1.0): 1, ('acc', 1.0): 2, ('sprit', 1.0): 1, ('you', 1.0): 1, ('til', 1.0): 2, ('hammi', 1.0): 1, ('freedom', 1.0): 2, ('pistol', 1.0): 1, ('unlock', 1.0): 1, ('bemeapp', 1.0): 1, ('thumb', 1.0): 1, ('beme', 1.0): 1, ('bemecod', 1.0): 1, ('proudtobem', 1.0): 1, ('round', 1.0): 2, ('calm', 1.0): 5, ('kepo', 1.0): 1, ('luckili', 1.0): 1, ('clearli', 1.0): 2, ('دعمم', 1.0): 1, ('للعودة', 1.0): 1, ('للحياة', 1.0): 1, ('heiyo', 1.0): 2, ('dudafti', 1.0): 1, ('breaktym', 1.0): 1, ('fatal', 1.0): 1, ('danger', 1.0): 1, ('term', 1.0): 2, ('health', 1.0): 2, ('outrag', 1.0): 1, ('645k', 1.0): 1, ('muna', 1.0): 1, ('magstart', 1.0): 1, ('salut', 1.0): 3, ('→', 1.0): 1, ('thq', 1.0): 1, ('contin', 1.0): 1, ('thalaivar', 1.0): 1, ('£', 1.0): 7, ('heiya', 1.0): 2, ('grab', 1.0): 3, ('30.000', 1.0): 2, ('av', 1.0): 1, ('gd', 1.0): 3, ('wknd', 1.0): 1, ('ear', 1.0): 12, (\"y'day\", 1.0): 1, ('hxh', 1.0): 1, ('badass', 1.0): 2, ('killua', 1.0): 1, ('scene', 1.0): 2, ('78x', 1.0): 1, ('unappreci', 1.0): 1, ('graciou', 1.0): 1, ('nailedit', 1.0): 1, ('ourdisneyinfin', 1.0): 1, ('mari', 1.0): 3, ('jillmil', 1.0): 1, ('webcam', 1.0): 2, ('elfindelmundo', 1.0): 1, ('mainli', 1.0): 1, ('favour', 1.0): 1, ('dancetast', 1.0): 1, ('satyajit', 1.0): 1, (\"ray'\", 1.0): 1, ('porosh', 1.0): 1, ('pathor', 1.0): 1, ('situat', 1.0): 3, ('goldbug', 1.0): 1, ('wine', 1.0): 3, ('bottl', 1.0): 2, ('spill', 1.0): 2, ('jazmin', 1.0): 3, ('bonilla', 1.0): 3, ('15000', 1.0): 1, ('star', 1.0): 9, ('hollywood', 1.0): 3, ('rofl', 1.0): 3, ('shade', 1.0): 1, ('grey', 1.0): 1, ('netsec', 1.0): 1, ('kev', 1.0): 1, ('sister', 1.0): 6, ('told', 1.0): 6, ('unlist', 1.0): 1, ('hickey', 1.0): 1, ('dad', 1.0): 5, ('hock', 1.0): 1, ('mamma', 1.0): 1, ('human', 1.0): 5, ('be', 1.0): 1, ('mere', 1.0): 1, ('holist', 1.0): 1, ('cosmovis', 1.0): 1, ('narrow-mind', 1.0): 1, ('charg', 1.0): 3, ('cess', 1.0): 1, ('alix', 1.0): 1, ('quan', 1.0): 1, ('tip', 1.0): 5, ('naaahhh', 1.0): 1, ('duh', 1.0): 2, ('emesh', 1.0): 1, ('hilari', 1.0): 4, ('kath', 1.0): 3, ('kia', 1.0): 1, ('@vauk', 1.0): 1, ('tango', 1.0): 1, ('tracerequest', 1.0): 2, ('dassi', 1.0): 1, ('fwm', 1.0): 1, ('selamat', 1.0): 1, ('nichola', 1.0): 2, ('malta', 1.0): 1, ('gto', 1.0): 1, ('tomorrowland', 1.0): 1, ('incal', 1.0): 1, ('shob', 1.0): 1, ('incomplet', 1.0): 1, ('barkada', 1.0): 1, ('silverston', 1.0): 1, ('pull', 1.0): 1, ('bookstor', 1.0): 1, ('ganna', 1.0): 1, ('hillari', 1.0): 1, ('clinton', 1.0): 1, ('court', 1.0): 2, ('notic', 1.0): 11, ('slice', 1.0): 2, ('life-so', 1.0): 1, ('hidden', 1.0): 1, ('untap', 1.0): 1, ('mca', 1.0): 2, ('gettin', 1.0): 1, ('hella', 1.0): 1, ('wana', 1.0): 1, ('bandz', 1.0): 1, ('hell', 1.0): 4, ('donington', 1.0): 1, ('park', 1.0): 8, ('24/25', 1.0): 1, ('x30', 1.0): 1, ('merci', 1.0): 1, ('bien', 1.0): 1, ('pitbul', 1.0): 1, ('777x', 1.0): 1, ('fri', 1.0): 3, ('annyeong', 1.0): 1, ('oppa', 1.0): 7, ('indonesian', 1.0): 1, ('elf', 1.0): 3, ('flight', 1.0): 2, ('bf', 1.0): 2, ('jennyjean', 1.0): 1, ('kikchat', 1.0): 1, ('sabadodeganarseguidor', 1.0): 1, ('sexysasunday', 1.0): 2, ('marseil', 1.0): 1, ('ganda', 1.0): 1, ('fnaf', 1.0): 5, ('steam', 1.0): 1, ('assur', 1.0): 2, ('current', 1.0): 7, ('goin', 1.0): 1, ('sweeti', 1.0): 4, ('strongest', 1.0): 1, (\"spot'\", 1.0): 1, ('barnstapl', 1.0): 1, ('bideford', 1.0): 1, ('abit', 1.0): 1, ('road', 1.0): 5, ('rocro', 1.0): 1, ('13glodyysbro', 1.0): 1, ('hire', 1.0): 1, ('2ne1', 1.0): 1, ('aspetti', 1.0): 1, ('chicken', 1.0): 4, ('chip', 1.0): 3, ('cupboard', 1.0): 1, ('empti', 1.0): 2, ('jami', 1.0): 2, ('ian', 1.0): 2, ('latin', 1.0): 5, ('asian', 1.0): 5, ('version', 1.0): 8, ('va', 1.0): 1, ('642', 1.0): 1, ('kikgirl', 1.0): 5, ('orgasm', 1.0): 1, ('phonesex', 1.0): 1, ('spacer', 1.0): 1, ('felic', 1.0): 1, ('smoak', 1.0): 1, ('👓', 1.0): 1, ('💘', 1.0): 3, ('children', 1.0): 3, ('psychopath', 1.0): 1, ('spoil', 1.0): 1, ('dimpl', 1.0): 1, ('contempl', 1.0): 1, ('indi', 1.0): 2, ('rout', 1.0): 4, ('jsl', 1.0): 1, ('76x', 1.0): 1, ('gotcha', 1.0): 1, ('kina', 1.0): 1, ('donna', 1.0): 3, ('reachabl', 1.0): 1, ('jk', 1.0): 1, ('s02e04', 1.0): 1, ('air', 1.0): 7, ('naggi', 1.0): 1, ('anal', 1.0): 1, ('child', 1.0): 3, ('vidcon', 1.0): 2, ('anxiou', 1.0): 1, ('shake', 1.0): 2, ('10:30', 1.0): 1, ('smoke', 1.0): 3, ('white', 1.0): 4, ('grandpa', 1.0): 4, ('prolli', 1.0): 1, ('stash', 1.0): 2, ('closer-chas', 1.0): 1, ('spec', 1.0): 1, ('leagu', 1.0): 3, ('chase', 1.0): 1, ('wall', 1.0): 3, ('angel', 1.0): 4, ('mochamichel', 1.0): 1, ('iph', 1.0): 4, ('0ne', 1.0): 4, ('simpli', 1.0): 3, ('bi0', 1.0): 8, ('x29', 1.0): 1, ('there', 1.0): 2, ('background', 1.0): 2, ('maggi', 1.0): 1, ('afraid', 1.0): 3, ('mull', 1.0): 1, ('nil', 1.0): 1, ('glasgow', 1.0): 2, ('netbal', 1.0): 1, ('thistl', 1.0): 1, ('thistlelov', 1.0): 1, ('minecraft', 1.0): 7, ('drew', 1.0): 3, ('delici', 1.0): 3, ('muddl', 1.0): 1, ('racket', 1.0): 2, ('isol', 1.0): 1, ('fa', 1.0): 1, ('particip', 1.0): 2, ('icecreammast', 1.0): 1, ('group', 1.0): 10, ('huhu', 1.0): 3, ('shet', 1.0): 1, ('desk', 1.0): 1, ('o_o', 1.0): 1, ('orz', 1.0): 1, ('problemmm', 1.0): 1, ('75x', 1.0): 1, ('english', 1.0): 4, ('yeeaayi', 1.0): 1, ('alhamdulillah', 1.0): 1, ('amin', 1.0): 1, ('weed', 1.0): 1, ('crowdfund', 1.0): 1, ('goal', 1.0): 2, ('walk', 1.0): 12, ('hellooo', 1.0): 2, ('select', 1.0): 1, ('lynn', 1.0): 1, ('buffer', 1.0): 2, ('button', 1.0): 2, ('compos', 1.0): 1, ('fridayfun', 1.0): 1, ('non-filipina', 1.0): 1, ('ejayst', 1.0): 1, ('state', 1.0): 2, ('le', 1.0): 2, ('stan', 1.0): 1, ('lee', 1.0): 2, ('discoveri', 1.0): 1, ('cousin', 1.0): 5, ('1400', 1.0): 1, ('yr', 1.0): 2, ('teleport', 1.0): 1, ('shahid', 1.0): 1, ('afridi', 1.0): 1, ('tou', 1.0): 1, ('mahnor', 1.0): 1, ('baloch', 1.0): 1, ('nikki', 1.0): 2, ('flower', 1.0): 4, ('blackfli', 1.0): 1, ('courgett', 1.0): 1, ('wont', 1.0): 5, ('affect', 1.0): 2, ('fruit', 1.0): 5, ('italian', 1.0): 1, ('netfilx', 1.0): 1, ('unmarri', 1.0): 1, ('finger', 1.0): 6, ('rock', 1.0): 10, ('wielli', 1.0): 1, ('paul', 1.0): 2, ('barcod', 1.0): 1, ('charlott', 1.0): 1, ('thta', 1.0): 1, ('trailblazerhonor', 1.0): 1, ('labour', 1.0): 3, ('leader', 1.0): 3, ('alot', 1.0): 2, ('agayhippiehippi', 1.0): 1, ('exercis', 1.0): 2, ('ginger', 1.0): 1, ('x28', 1.0): 1, ('teach', 1.0): 2, ('awar', 1.0): 1, ('::', 1.0): 4, ('portsmouth', 1.0): 1, ('sonal', 1.0): 1, ('hungri', 1.0): 2, ('hmmm', 1.0): 4, ('pedant', 1.0): 1, ('98', 1.0): 1, ('kit', 1.0): 2, ('ack', 1.0): 1, ('hih', 1.0): 1, ('choir', 1.0): 1, ('rosidbinr', 1.0): 1, ('duke', 1.0): 2, ('earl', 1.0): 1, ('tau', 1.0): 1, ('orayt', 1.0): 1, ('knw', 1.0): 1, ('block', 1.0): 3, ('dikha', 1.0): 1, ('reh', 1.0): 1, ('adolf', 1.0): 1, ('hitler', 1.0): 1, ('obstacl', 1.0): 1, ('exist', 1.0): 2, ('surrend', 1.0): 2, ('terrif', 1.0): 1, ('advaddict', 1.0): 1, ('_15', 1.0): 1, ('jimin', 1.0): 1, ('notanapolog', 1.0): 3, ('map', 1.0): 2, ('inform', 1.0): 5, ('0.7', 1.0): 1, ('motherfuck', 1.0): 1, (\"david'\", 1.0): 1, ('damn', 1.0): 3, ('colleg', 1.0): 2, ('24th', 1.0): 3, ('steroid', 1.0): 1, ('alansmithpart', 1.0): 1, ('servu', 1.0): 1, ('bonasio', 1.0): 1, (\"doido'\", 1.0): 1, ('task', 1.0): 2, ('deleg', 1.0): 1, ('aaahhh', 1.0): 1, ('jen', 1.0): 2, ('virgin', 1.0): 5, ('non-mapbox', 1.0): 1, ('restrict', 1.0): 1, ('mapbox', 1.0): 1, ('basemap', 1.0): 1, ('contractu', 1.0): 1, ('research', 1.0): 1, ('seafood', 1.0): 1, ('weltum', 1.0): 1, ('teh', 1.0): 1, ('deti', 1.0): 1, ('huh', 1.0): 2, ('=d', 1.0): 2, ('annoy', 1.0): 2, ('katmtan', 1.0): 1, ('swan', 1.0): 1, ('fandom', 1.0): 3, ('blurri', 1.0): 1, ('besok', 1.0): 1, ('b', 1.0): 8, ('urgent', 1.0): 3, ('within', 1.0): 4, ('dorset', 1.0): 1, ('goddess', 1.0): 1, ('blast', 1.0): 1, ('shitfac', 1.0): 1, ('soul', 1.0): 4, ('sing', 1.0): 5, ('disney', 1.0): 1, ('doug', 1.0): 3, ('28', 1.0): 2, ('bnte', 1.0): 1, ('hain', 1.0): 2, (';p', 1.0): 1, ('shiiitt', 1.0): 1, ('case', 1.0): 9, ('rm35', 1.0): 1, ('negooo', 1.0): 1, ('male', 1.0): 1, ('madelin', 1.0): 1, ('nun', 1.0): 1, ('mornin', 1.0): 2, ('yapster', 1.0): 1, ('pli', 1.0): 1, ('icon', 1.0): 2, ('alchemist', 1.0): 1, ('x27', 1.0): 1, ('dayz', 1.0): 1, ('preview', 1.0): 1, ('thug', 1.0): 1, ('lmao', 1.0): 3, ('sharethelov', 1.0): 2, ('highvalu', 1.0): 2, ('halsey', 1.0): 1, ('30th', 1.0): 1, ('anniversari', 1.0): 5, ('folk', 1.0): 10, ('bae', 1.0): 6, ('repli', 1.0): 5, ('complain', 1.0): 3, ('rude', 1.0): 3, ('bond', 1.0): 4, ('nigg', 1.0): 1, ('readingr', 1.0): 1, ('wordoftheweek', 1.0): 1, ('wotw', 1.0): 1, ('4:18', 1.0): 1, ('est', 1.0): 1, ('earn', 1.0): 1, ('jess', 1.0): 2, ('surri', 1.0): 1, ('botani', 1.0): 1, ('gel', 1.0): 1, ('alison', 1.0): 1, ('lsa', 1.0): 1, ('respons', 1.0): 7, ('fron', 1.0): 1, ('debbi', 1.0): 1, ('carol', 1.0): 2, ('patient', 1.0): 4, ('discharg', 1.0): 1, ('loung', 1.0): 1, ('walmart', 1.0): 1, ('balanc', 1.0): 2, ('studi', 1.0): 6, ('hayley', 1.0): 2, ('shoulder', 1.0): 1, ('pad', 1.0): 2, ('mount', 1.0): 1, ('inquisitor', 1.0): 1, ('cosplay', 1.0): 4, ('cosplayprogress', 1.0): 1, ('mike', 1.0): 3, ('dunno', 1.0): 2, ('insecur', 1.0): 2, ('nh', 1.0): 1, ('devolut', 1.0): 1, ('patriot', 1.0): 1, ('halla', 1.0): 1, ('ark', 1.0): 1, (\"jiyeon'\", 1.0): 1, ('buzz', 1.0): 2, ('burnt', 1.0): 1, ('mist', 1.0): 4, ('opi', 1.0): 1, ('avoplex', 1.0): 1, ('nail', 1.0): 3, ('cuticl', 1.0): 1, ('replenish', 1.0): 1, ('15ml', 1.0): 1, ('seriou', 1.0): 2, ('submiss', 1.0): 1, ('lb', 1.0): 2, ('cherish', 1.0): 2, ('flip', 1.0): 1, ('learnt', 1.0): 2, ('backflip', 1.0): 2, ('jumpgiant', 1.0): 1, ('foampit', 1.0): 1, ('usa', 1.0): 3, ('pamer', 1.0): 1, ('thk', 1.0): 1, ('actuallythough', 1.0): 1, ('craft', 1.0): 2, ('session', 1.0): 3, ('mehtab', 1.0): 1, ('aunti', 1.0): 1, ('gc', 1.0): 1, ('yeeew', 1.0): 1, ('pre', 1.0): 3, ('lan', 1.0): 1, ('yeey', 1.0): 1, ('arrang', 1.0): 1, ('doodl', 1.0): 2, ('comic', 1.0): 1, ('summon', 1.0): 1, ('none', 1.0): 1, ('🙅', 1.0): 1, ('lycra', 1.0): 1, ('vincent', 1.0): 1, ('couldnt', 1.0): 1, ('roy', 1.0): 1, ('bg', 1.0): 1, ('img', 1.0): 1, ('circl', 1.0): 1, ('font', 1.0): 1, ('deathofgrass', 1.0): 1, ('loan', 1.0): 2, ('lawnmow', 1.0): 1, ('popular', 1.0): 2, ('charismat', 1.0): 1, ('man.h', 1.0): 1, ('thrive', 1.0): 1, ('economi', 1.0): 1, ('burst', 1.0): 2, ('georgi', 1.0): 1, ('x26', 1.0): 1, ('million', 1.0): 4, ('fl', 1.0): 1, ('kindest', 1.0): 2, ('iceland', 1.0): 1, ('crazi', 1.0): 4, ('landscap', 1.0): 2, ('yok', 1.0): 1, ('lah', 1.0): 1, ('concordia', 1.0): 1, ('reunit', 1.0): 1, ('xxxibmchll', 1.0): 1, ('sea', 1.0): 4, ('prettier', 1.0): 2, ('imitatia', 1.0): 1, ('oe', 1.0): 1, ('michel', 1.0): 1, ('comeback', 1.0): 1, ('gross', 1.0): 1, ('treat', 1.0): 5, ('equal', 1.0): 2, ('injustic', 1.0): 1, ('femin', 1.0): 1, ('ineedfeminismbecaus', 1.0): 1, ('forgotten', 1.0): 3, ('stuck', 1.0): 4, ('recommend', 1.0): 4, ('redhead', 1.0): 1, ('wacki', 1.0): 1, ('rather', 1.0): 5, ('waytoliveahappylif', 1.0): 1, ('hoxton', 1.0): 1, ('holborn', 1.0): 1, ('karen', 1.0): 2, ('wag', 1.0): 2, ('bum', 1.0): 1, ('wwooo', 1.0): 1, ('nite', 1.0): 3, ('laiten', 1.0): 1, ('arond', 1.0): 1, ('1:30', 1.0): 1, ('consid', 1.0): 3, ('matur', 1.0): 3, ('journeyp', 1.0): 2, ('foam', 1.0): 1, (\"lady'\", 1.0): 1, ('mob', 1.0): 1, ('fals', 1.0): 1, ('bulletin', 1.0): 1, ('spring', 1.0): 1, ('fiesta', 1.0): 1, ('nois', 1.0): 2, ('awuuu', 1.0): 1, ('aich', 1.0): 1, ('sept', 1.0): 2, ('rudramadevi', 1.0): 1, ('anushka', 1.0): 1, ('gunashekar', 1.0): 1, ('harryxhood', 1.0): 1, ('upset', 1.0): 1, ('ooh', 1.0): 1, ('humanist', 1.0): 1, ('magazin', 1.0): 2, ('usernam', 1.0): 1, ('rape', 1.0): 1, ('csrrace', 1.0): 1, ('lack', 1.0): 6, ('hygien', 1.0): 1, ('tose', 1.0): 1, ('cloth', 1.0): 1, ('temperatur', 1.0): 1, ('planet', 1.0): 2, ('brave', 1.0): 2, ('ge', 1.0): 1, ('2015kenya', 1.0): 1, ('ryan', 1.0): 4, ('tidi', 1.0): 2, ('hagergang', 1.0): 1, ('chanhun', 1.0): 1, ('photoshoot', 1.0): 1, ('afteral', 1.0): 1, ('sadkaay', 1.0): 1, ('thark', 1.0): 1, ('peak', 1.0): 1, ('heatwav', 1.0): 1, ('lower', 1.0): 1, ('standard', 1.0): 2, ('x25', 1.0): 1, ('recruit', 1.0): 2, ('doom', 1.0): 1, ('nasti', 1.0): 1, ('affili', 1.0): 1, ('&gt;:)', 1.0): 2, ('64', 1.0): 2, ('74', 1.0): 1, ('40', 1.0): 4, ('00', 1.0): 1, ('hall', 1.0): 2, ('ted', 1.0): 3, ('pixgram', 1.0): 2, ('creativ', 1.0): 2, ('slideshow', 1.0): 1, ('nibbl', 1.0): 2, ('ivi', 1.0): 1, ('sho', 1.0): 1, ('superpow', 1.0): 2, ('obsess', 1.0): 2, ('oth', 1.0): 1, ('third', 1.0): 2, ('ngarepfollbackdarinabilahjkt', 1.0): 1, ('48', 1.0): 1, ('sunglass', 1.0): 1, ('jacki', 1.0): 2, ('sunni', 1.0): 6, ('style', 1.0): 5, ('jlo', 1.0): 1, ('jlover', 1.0): 1, ('turkey', 1.0): 1, ('goodafternoon', 1.0): 2, ('collag', 1.0): 2, ('furri', 1.0): 2, ('bruce', 1.0): 2, ('kunoriforceo', 1.0): 8, ('aayegi', 1.0): 1, ('tim', 1.0): 2, ('wiw', 1.0): 1, ('bip', 1.0): 1, ('zareen', 1.0): 1, ('daisi', 1.0): 1, (\"b'coz\", 1.0): 1, ('kart', 1.0): 1, ('mak', 1.0): 1, ('∗', 1.0): 2, ('lega', 1.0): 1, ('spag', 1.0): 1, ('boat', 1.0): 2, ('outboard', 1.0): 1, ('spell', 1.0): 4, ('reboard', 1.0): 1, ('fire', 1.0): 2, ('offboard', 1.0): 1, ('sn16', 1.0): 1, ('9dg', 1.0): 1, ('bnf', 1.0): 1, ('50', 1.0): 1, ('jason', 1.0): 1, ('rob', 1.0): 2, ('feb', 1.0): 1, ('victoriasecret', 1.0): 1, ('finland', 1.0): 1, ('helsinki', 1.0): 1, ('airport', 1.0): 3, ('plane', 1.0): 2, ('beyond', 1.0): 4, ('ont', 1.0): 1, ('tii', 1.0): 1, ('lng', 1.0): 2, ('yan', 1.0): 2, (\"u'll\", 1.0): 2, ('steve', 1.0): 2, ('bell', 1.0): 1, ('prescott', 1.0): 1, ('leadership', 1.0): 2, ('cartoon', 1.0): 1, ('upsid', 1.0): 2, ('statement', 1.0): 1, ('selamathariraya', 1.0): 1, ('lovesummertim', 1.0): 1, ('dumont', 1.0): 1, ('jax', 1.0): 1, ('jone', 1.0): 1, ('awesomee', 1.0): 1, ('x24', 1.0): 1, ('geoff', 1.0): 1, ('amazingli', 1.0): 1, ('talant', 1.0): 1, ('vsco', 1.0): 2, ('thanki', 1.0): 2, ('hash', 1.0): 1, ('tag', 1.0): 5, ('ifimeetanalien', 1.0): 1, ('bff', 1.0): 4, ('section', 1.0): 3, ('follbaaack', 1.0): 1, ('az', 1.0): 1, ('cauliflow', 1.0): 1, ('attempt', 1.0): 1, ('prinsesa', 1.0): 1, ('yaaah', 1.0): 2, ('law', 1.0): 3, ('toy', 1.0): 2, ('sonaaa', 1.0): 1, ('beautiful', 1.0): 2, (\"josephine'\", 1.0): 1, ('mirror', 1.0): 3, ('cretaperfect', 1.0): 2, ('4me', 1.0): 2, ('cretaperfectsuv', 1.0): 2, ('creta', 1.0): 1, ('load', 1.0): 1, ('telecom', 1.0): 2, ('judi', 1.0): 1, ('superb', 1.0): 1, ('slightli', 1.0): 1, ('rakna', 1.0): 1, ('ew', 1.0): 1, ('whose', 1.0): 1, ('fifa', 1.0): 1, ('lineup', 1.0): 1, ('surviv', 1.0): 2, ('p90x', 1.0): 1, ('p90', 1.0): 1, ('dishoom', 1.0): 2, ('rajnigandha', 1.0): 1, ('minju', 1.0): 1, ('rapper', 1.0): 1, ('lead', 1.0): 2, ('vocal', 1.0): 1, ('yujin', 1.0): 1, ('visual', 1.0): 2, ('makna', 1.0): 1, ('jane', 1.0): 2, ('hah', 1.0): 4, ('hawk', 1.0): 2, ('greatest', 1.0): 2, ('histori', 1.0): 2, ('along', 1.0): 6, ('talkback', 1.0): 1, ('process', 1.0): 4, ('featur', 1.0): 4, ('mostli', 1.0): 1, (\"cinema'\", 1.0): 1, ('defend', 1.0): 2, ('fashion', 1.0): 2, ('atroc', 1.0): 1, ('pandimension', 1.0): 1, ('manifest', 1.0): 1, ('argo', 1.0): 1, ('ring', 1.0): 4, ('640', 1.0): 1, ('nad', 1.0): 1, ('plezzz', 1.0): 1, ('asthma', 1.0): 1, ('inhal', 1.0): 1, ('breath', 1.0): 3, ('goodluck', 1.0): 1, ('hunger', 1.0): 1, ('mockingjay', 1.0): 1, ('thehungergam', 1.0): 1, ('ador', 1.0): 4, ('x23', 1.0): 1, ('reina', 1.0): 1, ('felt', 1.0): 3, ('excus', 1.0): 2, ('attend', 1.0): 2, ('whn', 1.0): 1, ('andr', 1.0): 1, ('mamayang', 1.0): 1, ('11pm', 1.0): 1, ('1d', 1.0): 2, ('89.9', 1.0): 1, ('powi', 1.0): 1, ('shropshir', 1.0): 1, ('border', 1.0): 1, (\"school'\", 1.0): 1, ('san', 1.0): 2, ('diego', 1.0): 1, ('jump', 1.0): 2, ('sourc', 1.0): 3, ('appeas', 1.0): 1, ('¦', 1.0): 1, ('aj', 1.0): 1, ('action', 1.0): 1, ('grunt', 1.0): 1, ('sc', 1.0): 1, ('anti-christ', 1.0): 1, ('m8', 1.0): 1, ('ju', 1.0): 1, ('halfway', 1.0): 1, ('ex', 1.0): 2, ('postiv', 1.0): 2, ('opinion', 1.0): 3, ('avi', 1.0): 1, ('dare', 1.0): 4, ('corridor', 1.0): 1, ('👯', 1.0): 2, ('neither', 1.0): 2, ('rundown', 1.0): 1, ('yah', 1.0): 4, ('leviboard', 1.0): 1, ('kleper', 1.0): 1, (':(', 1.0): 1, ('impecc', 1.0): 2, ('setokido', 1.0): 1, ('shoulda', 1.0): 3, ('hippo', 1.0): 1, ('materialist', 1.0): 1, ('showpo', 1.0): 1, ('cough', 1.0): 6, ('@artofsleepingin', 1.0): 1, ('x22', 1.0): 1, ('☺', 1.0): 5, ('makesm', 1.0): 1, ('santorini', 1.0): 1, ('escap', 1.0): 2, ('beatport', 1.0): 1, ('👊🏻', 1.0): 1, ('trmdhesit', 1.0): 2, ('manuel', 1.0): 1, ('vall', 1.0): 1, ('king', 1.0): 3, ('seven', 1.0): 2, ('kingdom', 1.0): 2, ('andal', 1.0): 1, ('taught', 1.0): 1, ('hide', 1.0): 3, ('privaci', 1.0): 1, ('wise', 1.0): 1, ('natsuki', 1.0): 1, ('often', 1.0): 2, ('catchi', 1.0): 1, ('neil', 1.0): 2, ('emir', 1.0): 2, ('brill', 1.0): 1, ('urquhart', 1.0): 1, ('castl', 1.0): 1, ('simpl', 1.0): 2, ('shatter', 1.0): 2, ('contrast', 1.0): 1, ('educampakl', 1.0): 1, ('rotorua', 1.0): 1, ('pehli', 1.0): 1, ('phir', 1.0): 1, ('somi', 1.0): 1, ('burfday', 1.0): 1, ('univers', 1.0): 3, ('santo', 1.0): 1, ('toma', 1.0): 1, ('norh', 1.0): 1, ('dialogu', 1.0): 2, ('chainsaw', 1.0): 2, ('amus', 1.0): 1, ('awe', 1.0): 1, ('protect', 1.0): 2, ('pop', 1.0): 5, ('2ish', 1.0): 1, ('fahad', 1.0): 1, ('bhai', 1.0): 3, ('iqrar', 1.0): 1, ('waseem', 1.0): 1, ('abroad', 1.0): 2, ('movie', 1.0): 1, ('chef', 1.0): 1, ('grogol', 1.0): 1, ('long-dist', 1.0): 1, ('rhi', 1.0): 1, ('pwrfl', 1.0): 1, ('benefit', 1.0): 2, ('b2b', 1.0): 1, ('b2c', 1.0): 1, (\"else'\", 1.0): 2, ('soo', 1.0): 2, ('enterprison', 1.0): 1, ('schoolsoutforsumm', 1.0): 1, ('fellow', 1.0): 4, ('juggl', 1.0): 1, ('purrtho', 1.0): 1, ('catho', 1.0): 1, ('catami', 1.0): 1, ('fourfivesecond', 1.0): 4, ('deaf', 1.0): 4, ('drug', 1.0): 1, ('alcohol', 1.0): 1, ('apexi', 1.0): 3, ('crystal', 1.0): 3, ('meth', 1.0): 1, ('champagn', 1.0): 1, ('fc', 1.0): 1, ('streamer', 1.0): 1, ('juic', 1.0): 1, ('correct', 1.0): 1, ('portrait', 1.0): 1, ('izumi', 1.0): 1, ('fugiwara', 1.0): 1, ('clonmel', 1.0): 1, ('vibrant', 1.0): 1, ('estim', 1.0): 1, ('server', 1.0): 2, ('quiet', 1.0): 1, ('yey', 1.0): 1, (\"insha'allah\", 1.0): 1, ('wil', 1.0): 1, ('x21', 1.0): 1, ('trend', 1.0): 3, ('akshaymostlovedsuperstarev', 1.0): 1, ('indirect', 1.0): 1, ('askurban', 1.0): 1, ('lyka', 1.0): 2, ('nap', 1.0): 4, ('aff', 1.0): 1, ('unam', 1.0): 1, ('jonginuh', 1.0): 1, ('forecast', 1.0): 2, ('10am', 1.0): 2, ('5am', 1.0): 1, ('sooth', 1.0): 1, ('vii', 1.0): 1, ('sweetheart', 1.0): 1, ('freak', 1.0): 3, ('zayn', 1.0): 3, ('fucker', 1.0): 1, ('pet', 1.0): 2, ('illustr', 1.0): 1, ('wohoo', 1.0): 1, ('gleam', 1.0): 1, ('paint', 1.0): 4, ('deal', 1.0): 2, ('prime', 1.0): 2, ('minist', 1.0): 2, ('sunjam', 1.0): 1, ('industri', 1.0): 1, ('present', 1.0): 7, ('practic', 1.0): 3, ('proactiv', 1.0): 1, ('environ', 1.0): 1, ('unreal', 1.0): 1, ('zain', 1.0): 1, ('zac', 1.0): 1, ('isaac', 1.0): 1, ('oss', 1.0): 1, ('frank', 1.0): 1, ('iero', 1.0): 1, ('phase', 1.0): 2, ('david', 1.0): 1, ('beginn', 1.0): 1, ('shine', 1.0): 3, ('sunflow', 1.0): 2, ('tommarow', 1.0): 1, ('yall', 1.0): 2, ('rank', 1.0): 2, ('birthdaymonth', 1.0): 1, ('vianey', 1.0): 1, ('juli', 1.0): 11, ('birthdaygirl', 1.0): 1, (\"town'\", 1.0): 1, ('andrew', 1.0): 2, ('checkout', 1.0): 2, ('otwol', 1.0): 1, ('awhil', 1.0): 1, ('x20', 1.0): 1, ('all-tim', 1.0): 1, ('julia', 1.0): 1, ('robert', 1.0): 1, ('awwhh', 1.0): 1, ('bulldog', 1.0): 1, ('unfortun', 1.0): 2, ('02079', 1.0): 1, ('490', 1.0): 1, ('132', 1.0): 1, ('born', 1.0): 2, ('fightstickfriday', 1.0): 1, ('extravag', 1.0): 2, ('tearout', 1.0): 1, ('selekt', 1.0): 1, ('yoot', 1.0): 1, ('cross', 1.0): 3, ('gudday', 1.0): 1, ('dave', 1.0): 5, ('haileyhelp', 1.0): 1, ('eid', 1.0): 2, ('mubarak', 1.0): 5, ('brotheeerrr', 1.0): 1, ('adventur', 1.0): 5, ('tokyo', 1.0): 2, ('kansai', 1.0): 1, ('l', 1.0): 4, ('upp', 1.0): 2, ('om', 1.0): 1, ('60', 1.0): 1, ('minut', 1.0): 7, ('data', 1.0): 1, ('jesu', 1.0): 5, ('amsterdam', 1.0): 2, ('3rd', 1.0): 3, ('nextweek', 1.0): 1, ('booti', 1.0): 2, ('bcuz', 1.0): 1, ('step', 1.0): 3, ('option', 1.0): 3, ('stabl', 1.0): 1, ('sturdi', 1.0): 1, ('lukkke', 1.0): 1, ('again.ensoi', 1.0): 1, ('tc', 1.0): 1, ('madam', 1.0): 1, ('siddi', 1.0): 1, ('unknown', 1.0): 2, ('roomi', 1.0): 1, ('gn', 1.0): 2, ('gf', 1.0): 2, ('consent', 1.0): 1, ('mister', 1.0): 2, ('vine', 1.0): 2, ('peyton', 1.0): 1, ('nagato', 1.0): 1, ('yuki-chan', 1.0): 1, ('shoushitsu', 1.0): 1, ('archdbanterburi', 1.0): 3, ('experttradesmen', 1.0): 1, ('banter', 1.0): 1, ('quiz', 1.0): 1, ('tradetalk', 1.0): 1, ('floof', 1.0): 1, ('face', 1.0): 13, ('muahah', 1.0): 1, ('x19', 1.0): 1, ('anticip', 1.0): 1, ('jd', 1.0): 1, ('laro', 1.0): 1, ('tayo', 1.0): 1, ('answer', 1.0): 8, ('ht', 1.0): 1, ('angelica', 1.0): 1, ('anghel', 1.0): 1, ('aa', 1.0): 3, ('kkk', 1.0): 1, ('macbook', 1.0): 1, ('rehears', 1.0): 1, ('youthcelebr', 1.0): 1, ('mute', 1.0): 1, ('29th', 1.0): 1, ('gohf', 1.0): 4, ('vegetarian', 1.0): 1, (\"she'll\", 1.0): 1, ('gooday', 1.0): 3, ('101', 1.0): 3, ('12000', 1.0): 1, ('oshieer', 1.0): 1, ('realreview', 1.0): 1, ('happycustom', 1.0): 1, ('realoshi', 1.0): 1, ('dealsuthaonotebachao', 1.0): 1, ('bigger', 1.0): 2, ('dime', 1.0): 1, ('uhuh', 1.0): 1, ('🎵', 1.0): 3, ('code', 1.0): 4, ('pleasant', 1.0): 2, ('on-board', 1.0): 1, ('raheel', 1.0): 1, ('flyhigh', 1.0): 1, ('bother', 1.0): 2, ('everett', 1.0): 1, ('taylor', 1.0): 1, ('ha-ha', 1.0): 1, ('peachyloan', 1.0): 1, ('fridayfreebi', 1.0): 1, ('noe', 1.0): 1, ('yisss', 1.0): 1, ('bindingofissac', 1.0): 1, ('xboxon', 1.0): 1, ('consol', 1.0): 1, ('justin', 1.0): 2, ('gladli', 1.0): 1, ('son', 1.0): 4, ('morocco', 1.0): 1, ('peru', 1.0): 1, ('nxt', 1.0): 1, ('bp', 1.0): 1, ('resort', 1.0): 1, ('x18', 1.0): 1, ('havuuulovey', 1.0): 1, ('uuu', 1.0): 1, ('possitv', 1.0): 1, ('hopey', 1.0): 1, ('throwbackfriday', 1.0): 1, ('christen', 1.0): 1, ('ki', 1.0): 1, ('yaad', 1.0): 1, ('gayi', 1.0): 1, ('opossum', 1.0): 1, ('belat', 1.0): 5, ('yeahh', 1.0): 2, ('kuffar', 1.0): 1, ('comput', 1.0): 5, ('cell', 1.0): 1, ('diarrhea', 1.0): 1, ('immigr', 1.0): 1, ('lice', 1.0): 1, ('goictiv', 1.0): 1, ('70685', 1.0): 1, ('tagsforlik', 1.0): 4, ('trapmus', 1.0): 1, ('hotmusicdeloco', 1.0): 1, ('kinick', 1.0): 1, ('01282', 1.0): 2, ('452096', 1.0): 1, ('shadi', 1.0): 1, ('reserv', 1.0): 3, ('tkt', 1.0): 1, ('likewis', 1.0): 4, ('overgener', 1.0): 1, ('ikr', 1.0): 1, ('😍', 1.0): 2, ('consumer', 1.0): 1, ('fic', 1.0): 2, ('ouch', 1.0): 2, ('slip', 1.0): 1, ('disc', 1.0): 1, ('thw', 1.0): 1, ('chute', 1.0): 1, ('chalut', 1.0): 1, ('replay', 1.0): 1, ('iplay', 1.0): 1, ('11am', 1.0): 3, ('unneed', 1.0): 1, ('megamoh', 1.0): 1, ('7/29', 1.0): 1, ('tool', 1.0): 2, ('zealand', 1.0): 1, ('pile', 1.0): 2, ('dump', 1.0): 1, ('couscou', 1.0): 3, (\"women'\", 1.0): 2, ('fiction', 1.0): 1, ('wahahaah', 1.0): 1, ('x17', 1.0): 1, ('orhan', 1.0): 1, ('pamuk', 1.0): 1, ('hero', 1.0): 3, ('canopi', 1.0): 1, ('mapl', 1.0): 2, ('syrup', 1.0): 1, ('farm', 1.0): 2, ('stephani', 1.0): 2, ('💖', 1.0): 2, ('congrtaualt', 1.0): 1, ('philea', 1.0): 1, ('club', 1.0): 4, ('inc', 1.0): 1, ('photograph', 1.0): 2, ('phonegraph', 1.0): 1, ('srsli', 1.0): 1, ('10:17', 1.0): 1, ('ripaaa', 1.0): 1, ('banat', 1.0): 1, ('ray', 1.0): 1, ('dept', 1.0): 1, ('hospit', 1.0): 3, ('grt', 1.0): 1, ('infograph', 1.0): 1, (\"o'clock\", 1.0): 2, ('habit', 1.0): 1, ('1dfor', 1.0): 1, ('roadtrip', 1.0): 1, ('19:30', 1.0): 1, ('ifc', 1.0): 1, ('whip', 1.0): 1, ('lilsisbro', 1.0): 1, ('pre-ord', 1.0): 2, (\"pixar'\", 1.0): 2, ('steelbook', 1.0): 1, ('hmm', 1.0): 2, ('pegel', 1.0): 1, ('lemess', 1.0): 1, ('kyle', 1.0): 2, ('paypal', 1.0): 1, ('oct', 1.0): 1, ('tud', 1.0): 1, ('jst', 1.0): 2, ('humphrey', 1.0): 1, ('yell', 1.0): 2, ('erm', 1.0): 1, ('breach', 1.0): 1, ('lemon', 1.0): 2, ('yogurt', 1.0): 2, ('pot', 1.0): 1, ('discov', 1.0): 2, ('liquoric', 1.0): 1, ('pud', 1.0): 1, ('cajun', 1.0): 1, ('spice', 1.0): 1, ('yum', 1.0): 2, ('cajunchicken', 1.0): 1, ('infinit', 1.0): 2, ('fight', 1.0): 4, ('gern', 1.0): 1, ('cikaaa', 1.0): 1, ('maaf', 1.0): 1, ('telat', 1.0): 1, ('ngucapinnya', 1.0): 1, ('maaay', 1.0): 1, ('x16', 1.0): 1, ('viparita', 1.0): 1, ('karani', 1.0): 1, ('legsupthewal', 1.0): 1, ('unwind', 1.0): 1, ('coco', 1.0): 3, ('comfi', 1.0): 1, ('jalulu', 1.0): 1, ('rosh', 1.0): 1, ('gla', 1.0): 1, ('pallavi', 1.0): 1, ('nairobi', 1.0): 1, ('hrdstellobama', 1.0): 1, ('region', 1.0): 2, ('civil', 1.0): 1, ('societi', 1.0): 2, ('globe', 1.0): 1, ('hajur', 1.0): 1, ('yayi', 1.0): 2, (\"must'v\", 1.0): 1, ('nerv', 1.0): 1, ('prelim', 1.0): 1, ('costacc', 1.0): 1, ('nwb', 1.0): 1, ('shud', 1.0): 1, ('cold', 1.0): 2, ('hmu', 1.0): 2, ('cala', 1.0): 1, ('brush', 1.0): 1, ('ego', 1.0): 1, ('wherev', 1.0): 1, ('interact', 1.0): 2, ('dongsaeng', 1.0): 1, ('chorong', 1.0): 1, ('friendship', 1.0): 1, ('impress', 1.0): 3, ('dragon', 1.0): 2, ('duck', 1.0): 5, ('mix', 1.0): 5, ('cheetah', 1.0): 1, ('wagga', 1.0): 2, ('coursework', 1.0): 1, ('lorna', 1.0): 1, ('scan', 1.0): 1, ('x12', 1.0): 2, ('canva', 1.0): 2, ('iqbal', 1.0): 1, ('ima', 1.0): 1, ('hon', 1.0): 1, ('aja', 1.0): 1, ('besi', 1.0): 1, ('chati', 1.0): 1, ('phulani', 1.0): 1, ('swasa', 1.0): 1, ('bahari', 1.0): 1, ('jiba', 1.0): 1, ('mumbai', 1.0): 1, ('gujarat', 1.0): 1, ('distrub', 1.0): 1, ('otherwis', 1.0): 5, ('190cr', 1.0): 1, ('inspit', 1.0): 1, ('highest', 1.0): 1, ('holder', 1.0): 1, ('threaten', 1.0): 1, ('daili', 1.0): 2, ('basi', 1.0): 1, ('vr', 1.0): 1, ('angelo', 1.0): 1, ('quezon', 1.0): 1, ('sweatpant', 1.0): 1, ('farbridg', 1.0): 1, ('segalakatakata', 1.0): 1, ('nixu', 1.0): 1, ('begun', 1.0): 1, ('flint', 1.0): 1, ('🍰', 1.0): 5, ('separ', 1.0): 1, ('criticis', 1.0): 1, ('gestur', 1.0): 1, ('pedal', 1.0): 1, ('stroke', 1.0): 1, ('caro', 1.0): 1, ('deposit', 1.0): 1, ('secur', 1.0): 2, ('shock', 1.0): 1, ('coff', 1.0): 2, ('tenerina', 1.0): 1, ('auguri', 1.0): 1, ('iso', 1.0): 1, ('certif', 1.0): 1, ('paralyz', 1.0): 1, ('anxieti', 1.0): 1, (\"it'd\", 1.0): 1, ('develop', 1.0): 3, ('spain', 1.0): 2, ('def', 1.0): 1, ('bantim', 1.0): 1, ('fail', 1.0): 5, ('2ban', 1.0): 1, ('x15', 1.0): 1, ('awkward', 1.0): 2, ('ab', 1.0): 1, ('gale', 1.0): 1, ('founder', 1.0): 1, ('loveyaaah', 1.0): 1, ('⅛', 1.0): 1, ('⅞', 1.0): 1, ('∞', 1.0): 1, ('specialist', 1.0): 1, ('aw', 1.0): 3, ('babyyi', 1.0): 1, ('djstruthmat', 1.0): 1, ('re-cap', 1.0): 1, ('flickr', 1.0): 1, ('tack', 1.0): 2, ('zephbot', 1.0): 1, ('hhahahahaha', 1.0): 1, ('blew', 1.0): 2, ('entir', 1.0): 2, ('vega', 1.0): 3, ('strip', 1.0): 1, ('hahahahahhaha', 1.0): 1, (\"callie'\", 1.0): 1, ('puppi', 1.0): 1, ('owner', 1.0): 2, ('callinganimalabusehotlineasap', 1.0): 1, ('gorefiend', 1.0): 1, ('mythic', 1.0): 1, ('remind', 1.0): 6, ('9:00', 1.0): 1, ('▪', 1.0): 2, ('️bea', 1.0): 1, ('miller', 1.0): 2, ('lockscreen', 1.0): 1, ('mbf', 1.0): 1, ('keesh', 1.0): 1, (\"yesterday'\", 1.0): 1, ('groupi', 1.0): 1, ('bebe', 1.0): 1, ('sizam', 1.0): 1, ('color', 1.0): 5, ('invoic', 1.0): 1, ('kanina', 1.0): 1, ('pong', 1.0): 1, ('umaga', 1.0): 1, ('browser', 1.0): 1, ('typic', 1.0): 2, ('pleass', 1.0): 5, ('leeteuk', 1.0): 1, ('pearl', 1.0): 1, ('thusi', 1.0): 1, ('pour', 1.0): 1, ('milk', 1.0): 2, ('tgv', 1.0): 1, ('pari', 1.0): 5, ('austerlitz', 1.0): 1, ('bloi', 1.0): 1, ('mile', 1.0): 3, ('chateau', 1.0): 1, ('de', 1.0): 1, ('marai', 1.0): 1, ('taxi', 1.0): 1, ('x14', 1.0): 1, ('nom', 1.0): 1, ('enji', 1.0): 1, ('hater', 1.0): 3, ('purchas', 1.0): 2, ('specially-mark', 1.0): 1, ('custard', 1.0): 1, ('sm', 1.0): 1, ('on-pack', 1.0): 1, ('instruct', 1.0): 1, ('tile', 1.0): 1, ('downstair', 1.0): 1, ('kelli', 1.0): 1, ('greek', 1.0): 2, ('petra', 1.0): 1, ('shadowplayloui', 1.0): 1, ('mutual', 1.0): 2, ('cuz', 1.0): 4, ('liveonstream', 1.0): 1, ('lani', 1.0): 1, ('graze', 1.0): 1, ('pride', 1.0): 1, ('bristolart', 1.0): 1, ('in-app', 1.0): 1, ('ensur', 1.0): 1, ('item', 1.0): 2, ('screw', 1.0): 1, ('amber', 1.0): 2, ('43', 1.0): 1, ('hpc', 1.0): 1, ('wip', 1.0): 2, ('sw', 1.0): 1, ('newsround', 1.0): 1, ('hound', 1.0): 1, ('7:40', 1.0): 1, ('ada', 1.0): 1, ('racist', 1.0): 1, ('hulk', 1.0): 1, ('tight', 1.0): 2, ('prayer', 1.0): 3, ('pardon', 1.0): 1, ('phl', 1.0): 1, ('abu', 1.0): 2, ('dhabi', 1.0): 1, ('hihihi', 1.0): 1, ('teamjanuaryclaim', 1.0): 1, ('godonna', 1.0): 1, ('msg', 1.0): 2, ('bowwowchicawowwow', 1.0): 1, ('settl', 1.0): 1, ('dkt', 1.0): 1, ('porch', 1.0): 1, ('uber', 1.0): 2, ('mobil', 1.0): 4, ('applic', 1.0): 3, ('giggl', 1.0): 2, ('bare', 1.0): 3, ('wind', 1.0): 2, ('kahlil', 1.0): 1, ('gibran', 1.0): 1, ('flash', 1.0): 1, ('stiff', 1.0): 1, ('upper', 1.0): 1, ('lip', 1.0): 1, ('britain', 1.0): 1, ('latmon', 1.0): 1, ('endeavour', 1.0): 1, ('ann', 1.0): 2, ('joy', 1.0): 4, ('os', 1.0): 1, ('exploit', 1.0): 1, ('ign', 1.0): 2, ('au', 1.0): 1, ('pubcast', 1.0): 1, ('tengaman', 1.0): 1, ('21', 1.0): 2, ('celebratio', 1.0): 1, ('women', 1.0): 1, ('instal', 1.0): 2, ('glorifi', 1.0): 1, ('infirm', 1.0): 1, ('silli', 1.0): 1, ('suav', 1.0): 1, ('gentlemen', 1.0): 1, ('monthli', 1.0): 1, ('mileag', 1.0): 1, ('target', 1.0): 2, ('samsung', 1.0): 1, ('qualiti', 1.0): 3, ('ey', 1.0): 1, ('beth', 1.0): 2, ('gangster', 1.0): 1, (\"athena'\", 1.0): 1, ('fanci', 1.0): 1, ('wellington', 1.0): 1, ('rich', 1.0): 2, ('christina', 1.0): 1, ('newslett', 1.0): 1, ('zy', 1.0): 1, ('olur', 1.0): 1, ('x13', 1.0): 1, ('flawless', 1.0): 1, ('reaction', 1.0): 2, ('hayli', 1.0): 1, ('edwin', 1.0): 1, ('elvena', 1.0): 1, ('emc', 1.0): 1, ('rubber', 1.0): 3, ('swearword', 1.0): 1, ('infect', 1.0): 1, ('10:16', 1.0): 1, ('wrote', 1.0): 3, ('gan', 1.0): 1, ('brotherhood', 1.0): 1, ('wolf', 1.0): 5, ('pill', 1.0): 1, ('nocturn', 1.0): 1, ('rrp', 1.0): 1, ('18.99', 1.0): 1, ('13.99', 1.0): 1, ('jah', 1.0): 1, ('wobbl', 1.0): 1, ('retard', 1.0): 1, ('50notif', 1.0): 1, ('check-up', 1.0): 1, ('pun', 1.0): 1, ('elit', 1.0): 1, ('camillu', 1.0): 1, ('pleasee', 1.0): 1, ('spare', 1.0): 1, ('tyre', 1.0): 2, ('joke', 1.0): 3, ('ahahah', 1.0): 1, ('shame', 1.0): 1, ('abandon', 1.0): 1, ('disagre', 1.0): 2, ('nowher', 1.0): 2, ('contradict', 1.0): 1, ('chao', 1.0): 1, ('contain', 1.0): 1, ('cranium', 1.0): 1, ('sneaker', 1.0): 1, ('nike', 1.0): 1, ('nikeorigin', 1.0): 1, ('nikeindonesia', 1.0): 1, ('pierojogg', 1.0): 1, ('skoy', 1.0): 1, ('winter', 1.0): 2, ('falkland', 1.0): 1, ('jamie-le', 1.0): 1, ('congraaat', 1.0): 1, ('hooh', 1.0): 1, ('chrome', 1.0): 1, ('storm', 1.0): 1, ('thunderstorm', 1.0): 1, ('circuscircu', 1.0): 1, ('omgg', 1.0): 1, ('tdi', 1.0): 1, ('(-:', 1.0): 2, ('peter', 1.0): 1, ('expel', 1.0): 2, ('boughi', 1.0): 1, ('kernel', 1.0): 1, ('paralysi', 1.0): 1, ('liza', 1.0): 1, ('lol.hook', 1.0): 1, ('vampir', 1.0): 2, ('diari', 1.0): 3, ('twice', 1.0): 1, ('thanq', 1.0): 2, ('goodwil', 1.0): 1, ('vandr', 1.0): 1, ('ash', 1.0): 1, ('debat', 1.0): 3, ('solar', 1.0): 1, ('6-5', 1.0): 1, ('shown', 1.0): 1, ('ek', 1.0): 1, ('taco', 1.0): 2, ('mexico', 1.0): 2, ('viva', 1.0): 1, ('méxico', 1.0): 1, ('burger', 1.0): 3, ('thebestangkapuso', 1.0): 1, ('lighter', 1.0): 1, ('tooth', 1.0): 2, ('korean', 1.0): 2, ('netizen', 1.0): 1, ('crueler', 1.0): 1, ('eleph', 1.0): 1, ('marula', 1.0): 1, ('tdif', 1.0): 1, ('shoutout', 1.0): 1, ('shortli', 1.0): 1, ('itsamarvelth', 1.0): 1, (\"japan'\", 1.0): 1, ('artist', 1.0): 1, ('homework', 1.0): 1, ('marco', 1.0): 1, ('herb', 1.0): 1, ('pm', 1.0): 3, ('self', 1.0): 1, ('esteem', 1.0): 1, ('patienc', 1.0): 1, ('sobtian', 1.0): 1, ('cowork', 1.0): 1, ('deathli', 1.0): 1, ('hallow', 1.0): 1, ('supernatur', 1.0): 1, ('consult', 1.0): 1, ('himach', 1.0): 1, ('2.25', 1.0): 1, ('asham', 1.0): 1, ('where.do.i.start', 1.0): 1, ('moviemarathon', 1.0): 1, ('skill', 1.0): 4, ('shadow', 1.0): 1, ('own', 1.0): 1, ('pair', 1.0): 3, (\"it'll\", 1.0): 6, ('cortez', 1.0): 1, ('superstar', 1.0): 1, ('tthank', 1.0): 1, ('colin', 1.0): 1, ('luxuou', 1.0): 1, ('tarryn', 1.0): 1, ('hbdme', 1.0): 1, ('yeeeyyy', 1.0): 1, ('barsostay', 1.0): 1, ('males', 1.0): 1, ('independ', 1.0): 1, ('sum', 1.0): 1, ('debacl', 1.0): 1, ('perfectli', 1.0): 1, ('longer', 1.0): 2, ('amyjackson', 1.0): 1, ('omegl', 1.0): 2, ('countrymus', 1.0): 1, ('five', 1.0): 2, (\"night'\", 1.0): 2, (\"freddy'\", 1.0): 2, ('demo', 1.0): 2, ('pump', 1.0): 2, ('fanboy', 1.0): 1, ('thegrandad', 1.0): 1, ('sidni', 1.0): 1, ('remarriag', 1.0): 1, ('occas', 1.0): 1, ('languag', 1.0): 1, ('java', 1.0): 1, (\"php'\", 1.0): 1, ('notion', 1.0): 1, ('refer', 1.0): 1, ('confus', 1.0): 3, ('ohioan', 1.0): 1, ('stick', 1.0): 2, ('doctor', 1.0): 3, ('offlin', 1.0): 1, ('thesim', 1.0): 1, ('mb', 1.0): 1, ('meaningless', 1.0): 1, ('common', 1.0): 1, ('celebr', 1.0): 9, ('muertosatfring', 1.0): 1, ('emul', 1.0): 1, ('brought', 1.0): 1, ('enemi', 1.0): 2, ('relax', 1.0): 3, ('ou', 1.0): 1, ('pink', 1.0): 2, ('cc', 1.0): 2, ('meooowww', 1.0): 1, ('barkkkiiidee', 1.0): 1, ('bark', 1.0): 1, ('x11', 1.0): 1, ('routin', 1.0): 4, ('alek', 1.0): 1, ('awh', 1.0): 2, ('kumpul', 1.0): 1, ('cantik', 1.0): 1, ('ganteng', 1.0): 1, ('kresna', 1.0): 1, ('jelli', 1.0): 1, ('simon', 1.0): 1, ('lesley', 1.0): 3, ('blood', 1.0): 2, ('panti', 1.0): 1, ('lion', 1.0): 1, ('artworkbyli', 1.0): 1, ('judo', 1.0): 1, ('daredevil', 1.0): 2, ('despond', 1.0): 1, ('re-watch', 1.0): 1, ('welcoma.hav', 1.0): 1, ('favor', 1.0): 5, ('tridon', 1.0): 1, ('21pic', 1.0): 1, ('master', 1.0): 3, ('nim', 1.0): 1, (\"there'r\", 1.0): 1, ('22pic', 1.0): 1, ('kebun', 1.0): 1, ('ubud', 1.0): 1, ('ladyposs', 1.0): 1, ('xoxoxo', 1.0): 1, ('sneak', 1.0): 3, ('peek', 1.0): 2, ('inbox', 1.0): 1, ('happyweekend', 1.0): 1, ('therealgolden', 1.0): 1, ('47', 1.0): 1, ('girlfriendsmya', 1.0): 1, ('ppl', 1.0): 2, ('closest', 1.0): 1, ('njoy', 1.0): 1, ('followingg', 1.0): 1, ('privat', 1.0): 1, ('pusher', 1.0): 1, ('stun', 1.0): 4, ('wooohooo', 1.0): 1, ('cuss', 1.0): 1, ('teenag', 1.0): 1, ('ace', 1.0): 1, ('sauc', 1.0): 3, ('livi', 1.0): 1, ('fowl', 1.0): 1, ('oliviafowl', 1.0): 1, ('891', 1.0): 1, ('burnout', 1.0): 1, ('johnforceo', 1.0): 1, ('matthew', 1.0): 1, ('provok', 1.0): 1, ('indiankultur', 1.0): 1, ('oppos', 1.0): 1, ('biker', 1.0): 1, ('lyk', 1.0): 1, ('gud', 1.0): 4, ('weight', 1.0): 6, ('bcu', 1.0): 1, ('rubbish', 1.0): 1, ('veggi', 1.0): 2, ('steph', 1.0): 1, ('nj', 1.0): 1, ('x10', 1.0): 1, ('cohes', 1.0): 1, ('gossip', 1.0): 2, ('alex', 1.0): 3, ('heswifi', 1.0): 1, ('7am', 1.0): 1, ('wub', 1.0): 1, ('cerbchan', 1.0): 1, ('jarraaa', 1.0): 1, ('morrrn', 1.0): 1, ('snooz', 1.0): 1, ('clicksco', 1.0): 1, ('gay', 1.0): 4, ('lesbian', 1.0): 2, ('rigid', 1.0): 1, ('theocrat', 1.0): 1, ('wing', 1.0): 1, ('fundamentalist', 1.0): 1, ('islamist', 1.0): 1, ('brianaaa', 1.0): 1, ('brianazabrocki', 1.0): 1, ('sky', 1.0): 2, ('batb', 1.0): 1, ('clap', 1.0): 3, ('whilst', 1.0): 1, ('aki', 1.0): 1, ('thencerest', 1.0): 2, ('547', 1.0): 2, ('indiemus', 1.0): 5, ('sexyjudi', 1.0): 3, ('pussi', 1.0): 4, ('sexo', 1.0): 3, ('humid', 1.0): 1, ('87', 1.0): 1, ('sloppi', 1.0): 1, (\"second'\", 1.0): 1, ('stock', 1.0): 3, ('marmit', 1.0): 2, ('x9', 1.0): 1, ('nic', 1.0): 3, ('taft', 1.0): 1, ('finalist', 1.0): 1, ('lotteri', 1.0): 1, ('award', 1.0): 3, ('usagi', 1.0): 1, ('looov', 1.0): 1, ('wowww', 1.0): 2, ('💙', 1.0): 8, ('💚', 1.0): 8, ('💕', 1.0): 12, ('lepa', 1.0): 1, ('sembuh', 1.0): 1, ('sibuk', 1.0): 1, ('balik', 1.0): 1, ('kin', 1.0): 1, ('gotham', 1.0): 1, ('sunnyday', 1.0): 1, ('dudett', 1.0): 1, ('cost', 1.0): 1, ('flippin', 1.0): 1, ('fortun', 1.0): 1, ('divinediscont', 1.0): 1, (';}', 1.0): 1, ('amnot', 1.0): 1, ('autofollow', 1.0): 3, ('teamfollowback', 1.0): 4, ('geer', 1.0): 1, ('bat', 1.0): 2, ('mz', 1.0): 1, ('yang', 1.0): 2, ('deennya', 1.0): 1, ('jehwan', 1.0): 1, ('11:00', 1.0): 1, ('ashton', 1.0): 1, ('✧', 1.0): 12, ('｡', 1.0): 4, ('chelni', 1.0): 2, ('datz', 1.0): 1, ('jeremi', 1.0): 1, ('fmt', 1.0): 1, ('dat', 1.0): 3, ('heartbeat', 1.0): 1, ('clutch', 1.0): 1, ('🐢', 1.0): 2, ('besteverdoctorwhoepisod', 1.0): 1, ('relev', 1.0): 1, ('puke', 1.0): 1, ('proper', 1.0): 1, ('x8', 1.0): 1, ('sublimin', 1.0): 1, ('eatmeat', 1.0): 1, ('brewproject', 1.0): 1, ('lovenafianna', 1.0): 1, ('mr', 1.0): 7, ('lewi', 1.0): 1, ('clock', 1.0): 1, ('3:02', 1.0): 2, ('muslim', 1.0): 1, ('prophet', 1.0): 1, ('غردلي', 1.0): 4, ('is.h', 1.0): 1, ('mistak', 1.0): 4, ('understood', 1.0): 1, ('politician', 1.0): 1, ('argu', 1.0): 1, ('intellect', 1.0): 1, ('shiva', 1.0): 1, ('mp3', 1.0): 1, ('standrew', 1.0): 1, ('sandcastl', 1.0): 1, ('ewok', 1.0): 1, ('nate', 1.0): 2, ('brawl', 1.0): 1, ('rear', 1.0): 1, ('nake', 1.0): 1, ('choke', 1.0): 1, ('heck', 1.0): 1, ('gun', 1.0): 2, ('associ', 1.0): 1, ('um', 1.0): 1, ('endow', 1.0): 1, ('ai', 1.0): 1, ('sikandar', 1.0): 1, ('pti', 1.0): 1, ('standwdik', 1.0): 1, ('westandwithik', 1.0): 1, ('starbuck', 1.0): 2, ('logo', 1.0): 2, ('renew', 1.0): 1, ('chariti', 1.0): 1, ('جمعة_مباركة', 1.0): 1, ('hoki', 1.0): 1, ('biz', 1.0): 1, ('non', 1.0): 1, ('america', 1.0): 1, ('california', 1.0): 1, ('01:16', 1.0): 1, ('45gameplay', 1.0): 2, ('ilovey', 1.0): 2, ('vex', 1.0): 1, ('iger', 1.0): 1, ('leicaq', 1.0): 1, ('leica', 1.0): 1, ('dudee', 1.0): 1, ('persona', 1.0): 1, ('yepp', 1.0): 1, ('5878e503', 1.0): 1, ('x7', 1.0): 1, ('greg', 1.0): 1, ('posey', 1.0): 1, ('miami', 1.0): 1, ('james_yammouni', 1.0): 1, ('breakdown', 1.0): 1, ('materi', 1.0): 2, ('thorin', 1.0): 1, ('hunt', 1.0): 1, ('choroo', 1.0): 1, ('nahi', 1.0): 2, ('aztec', 1.0): 1, ('princess', 1.0): 2, ('raini', 1.0): 1, ('kingfish', 1.0): 1, ('chinua', 1.0): 1, ('acheb', 1.0): 1, ('intellectu', 1.0): 2, ('liquid', 1.0): 1, ('melbournetrip', 1.0): 1, ('taxikitchen', 1.0): 1, ('nooow', 1.0): 2, ('mcdo', 1.0): 1, ('everywher', 1.0): 2, ('dreamer', 1.0): 1, ('tanisha', 1.0): 1, ('1nonli', 1.0): 1, ('attitud', 1.0): 1, ('kindl', 1.0): 2, ('flame', 1.0): 1, ('convict', 1.0): 1, ('bar', 1.0): 1, ('repath', 1.0): 2, ('adi', 1.0): 1, ('stefani', 1.0): 1, ('sg1', 1.0): 1, ('lightbox', 1.0): 1, ('ran', 1.0): 2, ('incorrect', 1.0): 1, ('apologist', 1.0): 1, ('x6', 1.0): 1, ('vuli', 1.0): 1, ('01:15', 1.0): 1, ('batman', 1.0): 1, ('pearson', 1.0): 1, ('reput', 1.0): 2, ('nikkei', 1.0): 1, ('woodford', 1.0): 1, ('vscocam', 1.0): 1, ('vscoph', 1.0): 1, ('vscogood', 1.0): 1, ('vscophil', 1.0): 1, ('vscocousin', 1.0): 1, ('yaap', 1.0): 1, ('urwelc', 1.0): 1, ('neon', 1.0): 1, ('pant', 1.0): 1, ('haaa', 1.0): 1, ('will', 1.0): 2, ('auspost', 1.0): 1, ('openfollow', 1.0): 1, ('rp', 1.0): 2, ('eng', 1.0): 1, ('yūjō-cosplay', 1.0): 1, ('luxembourg', 1.0): 1, ('bunni', 1.0): 1, ('broadcast', 1.0): 1, ('needa', 1.0): 1, ('gal', 1.0): 3, ('bend', 1.0): 3, ('heaven', 1.0): 2, ('score', 1.0): 2, ('januari', 1.0): 1, ('hanabutl', 1.0): 1, ('kikhorni', 1.0): 1, ('interraci', 1.0): 1, ('makeup', 1.0): 1, ('chu', 1.0): 1, (\"weekend'\", 1.0): 1, ('punt', 1.0): 1, ('horserac', 1.0): 1, ('hors', 1.0): 2, ('horseracingtip', 1.0): 1, ('guitar', 1.0): 1, ('cocoar', 1.0): 1, ('brief', 1.0): 1, ('introduct', 1.0): 1, ('earliest', 1.0): 1, ('indian', 1.0): 1, ('subcontin', 1.0): 1, ('bfr', 1.0): 1, ('maurya', 1.0): 1, ('jordanian', 1.0): 1, ('00962778381', 1.0): 1, ('838', 1.0): 1, ('tenyai', 1.0): 1, ('hee', 1.0): 2, ('ss', 1.0): 1, ('semi', 1.0): 1, ('atp', 1.0): 2, ('wimbledon', 1.0): 2, ('feder', 1.0): 1, ('nadal', 1.0): 1, ('monfil', 1.0): 1, ('handsom', 1.0): 2, ('cilic', 1.0): 3, ('firm', 1.0): 1, ('potenti', 1.0): 3, ('nyc', 1.0): 1, ('chillin', 1.0): 2, ('tail', 1.0): 2, ('kitten', 1.0): 1, ('garret', 1.0): 1, ('baz', 1.0): 1, ('leo', 1.0): 2, ('xst', 1.0): 1, ('centrifug', 1.0): 1, ('etern', 1.0): 3, ('forgiv', 1.0): 2, ('kangin', 1.0): 1, ('بندر', 1.0): 1, ('العنزي', 1.0): 1, ('kristin', 1.0): 1, ('cass', 1.0): 1, ('surajettan', 1.0): 1, ('kashi', 1.0): 1, ('ashwathi', 1.0): 1, ('mommi', 1.0): 2, ('tirth', 1.0): 1, ('brambhatt', 1.0): 1, ('snooker', 1.0): 1, ('compens', 1.0): 1, ('theoper', 1.0): 1, ('479', 1.0): 1, ('premiostumundo', 1.0): 2, ('philosoph', 1.0): 1, ('x5', 1.0): 1, ('graphic', 1.0): 2, ('level', 1.0): 1, ('aug', 1.0): 3, ('excl', 1.0): 1, ('raw', 1.0): 1, ('weeni', 1.0): 1, ('annoyingbabi', 1.0): 1, ('lazi', 1.0): 2, ('cosi', 1.0): 1, ('client_amends_edit', 1.0): 1, ('_5_final_final_fin', 1.0): 1, ('pdf', 1.0): 1, ('mauliat', 1.0): 1, ('ito', 1.0): 2, ('okkay', 1.0): 1, ('knock', 1.0): 3, (\"soloist'\", 1.0): 1, ('ryu', 1.0): 1, ('saera', 1.0): 1, ('pinkeu', 1.0): 1, ('angri', 1.0): 3, ('screencap', 1.0): 1, ('jonghyun', 1.0): 1, ('seungyeon', 1.0): 1, ('cnblue', 1.0): 1, ('mbc', 1.0): 1, ('wgm', 1.0): 1, ('masa', 1.0): 2, ('entrepreneurship', 1.0): 1, ('empow', 1.0): 1, ('limpopo', 1.0): 1, ('pict', 1.0): 1, ('norapowel', 1.0): 1, ('hornykik', 1.0): 2, ('livesex', 1.0): 1, ('pumpkin', 1.0): 1, ('thrice', 1.0): 1, ('patron', 1.0): 1, ('ventur', 1.0): 1, ('deathcur', 1.0): 1, ('boob', 1.0): 1, ('blame', 1.0): 1, ('dine', 1.0): 1, ('modern', 1.0): 1, ('grill', 1.0): 1, ('disk', 1.0): 1, ('nt4', 1.0): 1, ('iirc', 1.0): 1, ('ux', 1.0): 1, ('refin', 1.0): 1, ('zdp', 1.0): 1, ('didnt', 1.0): 2, ('justic', 1.0): 1, ('daw', 1.0): 1, ('tine', 1.0): 1, ('gensan', 1.0): 1, ('frightl', 1.0): 1, ('undead', 1.0): 1, ('plush', 1.0): 1, ('cushion', 1.0): 1, ('nba', 1.0): 3, ('2k15', 1.0): 3, ('mypark', 1.0): 3, ('chronicl', 1.0): 4, ('gryph', 1.0): 3, ('volum', 1.0): 3, ('ellen', 1.0): 1, ('degener', 1.0): 1, ('shirt', 1.0): 1, ('mint', 1.0): 1, ('superdri', 1.0): 1, ('berangkaat', 1.0): 1, ('lagiii', 1.0): 1, ('siguro', 1.0): 1, ('un', 1.0): 1, ('kesa', 1.0): 1, ('lotsa', 1.0): 2, ('organis', 1.0): 2, ('4am', 1.0): 1, ('fingers-cross', 1.0): 1, ('deep', 1.0): 1, ('htaccess', 1.0): 1, ('file', 1.0): 2, ('adf', 1.0): 1, ('womad', 1.0): 1, ('gran', 1.0): 1, ('canaria', 1.0): 1, ('gig', 1.0): 1, ('twist', 1.0): 1, ('youv', 1.0): 1, ('teamnatur', 1.0): 1, ('huni', 1.0): 1, ('yayayayay', 1.0): 1, ('yt', 1.0): 2, ('convent', 1.0): 1, ('brighton', 1.0): 1, ('slay', 1.0): 1, ('nicknam', 1.0): 1, ('babygirl', 1.0): 1, ('regard', 1.0): 2, ('himmat', 1.0): 1, ('karain', 1.0): 2, ('baat', 1.0): 1, ('meri', 1.0): 1, ('hotee-mi', 1.0): 1, ('uncl', 1.0): 1, ('tongu', 1.0): 1, ('pronounc', 1.0): 1, ('nativ', 1.0): 1, ('american', 1.0): 2, ('proverb', 1.0): 1, ('lovabl', 1.0): 1, ('yesha', 1.0): 1, ('montoya', 1.0): 1, ('eagerli', 1.0): 1, ('payment', 1.0): 1, ('suprem', 1.0): 1, ('leon', 1.0): 1, ('ks', 1.0): 2, ('randi', 1.0): 1, ('9bi', 1.0): 1, ('physiqu', 1.0): 1, ('shave', 1.0): 1, ('uncut', 1.0): 1, ('boi', 1.0): 1, ('cheapest', 1.0): 1, ('regular', 1.0): 3, ('printer', 1.0): 3, ('nz', 1.0): 1, ('larg', 1.0): 4, ('format', 1.0): 1, ('10/10', 1.0): 1, ('senior', 1.0): 1, ('raid', 1.0): 2, ('conserv', 1.0): 1, ('batteri', 1.0): 1, ('comfort', 1.0): 2, ('swt', 1.0): 1, ('reservations@sandsbeach.eu', 1.0): 1, ('localgaragederbi', 1.0): 1, ('campu', 1.0): 1, ('subgam', 1.0): 1, ('faceit', 1.0): 1, ('snpcaht', 1.0): 1, ('hakhakhak', 1.0): 1, ('t___t', 1.0): 1, (\"kyungsoo'\", 1.0): 1, ('3d', 1.0): 2, ('properti', 1.0): 2, ('agent', 1.0): 1, ('accur', 1.0): 1, ('descript', 1.0): 1, ('theori', 1.0): 1, ('x4', 1.0): 1, ('15.90', 1.0): 1, ('yvett', 1.0): 1, ('author', 1.0): 2, ('mwf', 1.0): 1, ('programm', 1.0): 1, ('taal', 1.0): 1, ('lake', 1.0): 1, ('2emt', 1.0): 1, ('«', 1.0): 2, ('scurri', 1.0): 1, ('agil', 1.0): 1, ('solut', 1.0): 1, ('sme', 1.0): 1, ('omar', 1.0): 1, ('biggest', 1.0): 5, ('kamaal', 1.0): 1, ('amm', 1.0): 1, ('3am', 1.0): 1, ('hopehousekid', 1.0): 1, ('pitmantrain', 1.0): 1, ('walkersmithway', 1.0): 1, ('keepitloc', 1.0): 2, ('sehun', 1.0): 1, ('se100lead', 1.0): 1, ('unev', 1.0): 1, ('sofa', 1.0): 1, ('surf', 1.0): 1, ('cunt', 1.0): 1, ('rescoop', 1.0): 1, ('multiraci', 1.0): 1, ('fk', 1.0): 1, ('narrow', 1.0): 1, ('warlock', 1.0): 1, ('balloon', 1.0): 3, ('mj', 1.0): 1, ('madison', 1.0): 1, ('beonknockknock', 1.0): 1, ('con-gradu', 1.0): 1, ('gent', 1.0): 1, ('bitchfac', 1.0): 1, ('😒', 1.0): 1, ('organ', 1.0): 1, ('12pm', 1.0): 2, ('york', 1.0): 2, ('nearest', 1.0): 1, ('lendal', 1.0): 1, ('pikami', 1.0): 1, ('captur', 1.0): 1, ('fulton', 1.0): 1, ('sheen', 1.0): 1, ('baloney', 1.0): 1, ('unvarnish', 1.0): 1, ('laid', 1.0): 2, ('thick', 1.0): 1, ('blarney', 1.0): 1, ('flatteri', 1.0): 1, ('thin', 1.0): 1, ('sachin', 1.0): 1, ('unimport', 1.0): 1, ('context', 1.0): 1, ('dampen', 1.0): 1, ('yu', 1.0): 1, ('rocket', 1.0): 1, ('narendra', 1.0): 1, ('modi', 1.0): 1, ('aaaand', 1.0): 1, (\"team'\", 1.0): 1, ('macauley', 1.0): 1, ('howev', 1.0): 3, ('x3', 1.0): 1, ('wheeen', 1.0): 1, ('heechul', 1.0): 1, ('toast', 1.0): 2, ('coffee-weekday', 1.0): 1, ('9-11', 1.0): 1, ('sail', 1.0): 1, (\"friday'\", 1.0): 1, ('commerci', 1.0): 1, ('insur', 1.0): 1, ('requir', 1.0): 2, ('lookfortheo', 1.0): 1, ('cl', 1.0): 1, ('thou', 1.0): 1, ('april', 1.0): 2, ('airforc', 1.0): 1, ('clark', 1.0): 1, ('field', 1.0): 1, ('pampanga', 1.0): 1, ('troll', 1.0): 1, ('⚡', 1.0): 1, ('brow', 1.0): 1, ('oili', 1.0): 1, ('maricarljanah', 1.0): 1, ('6:15', 1.0): 1, ('degre', 1.0): 3, ('fahrenheit', 1.0): 1, ('🍸', 1.0): 7, ('╲', 1.0): 4, ('─', 1.0): 8, ('╱', 1.0): 5, ('🍤', 1.0): 4, ('╭', 1.0): 4, ('╮', 1.0): 4, ('┓', 1.0): 2, ('┳', 1.0): 1, ('┣', 1.0): 1, ('╰', 1.0): 3, ('╯', 1.0): 3, ('┗', 1.0): 2, ('┻', 1.0): 1, ('stool', 1.0): 1, ('toppl', 1.0): 1, ('findyourfit', 1.0): 1, ('prefer', 1.0): 2, ('whomosexu', 1.0): 1, ('stack', 1.0): 1, ('pandora', 1.0): 3, ('digitalexet', 1.0): 1, ('digitalmarket', 1.0): 1, ('sociamedia', 1.0): 1, ('nb', 1.0): 1, ('bom', 1.0): 1, ('dia', 1.0): 1, ('todo', 1.0): 1, ('forklift', 1.0): 1, ('warehous', 1.0): 1, ('worker', 1.0): 1, ('lsceen', 1.0): 1, ('immatur', 1.0): 1, ('gandhi', 1.0): 1, ('grassi', 1.0): 1, ('feetblog', 1.0): 2, ('daughter', 1.0): 3, ('4yr', 1.0): 1, ('old-porridg', 1.0): 1, ('fiend', 1.0): 1, ('2nite', 1.0): 1, ('comp', 1.0): 1, ('vike', 1.0): 1, ('t20blast', 1.0): 1, ('np', 1.0): 1, ('tax', 1.0): 1, ('ooohh', 1.0): 1, ('petjam', 1.0): 1, ('virtual', 1.0): 2, ('pounc', 1.0): 1, ('bentek', 1.0): 1, ('agn', 1.0): 1, ('socialmedia@dpdgroup.co.uk', 1.0): 1, ('sam', 1.0): 3, ('fruiti', 1.0): 1, ('vodka', 1.0): 2, ('sellyourcarin', 1.0): 2, ('5word', 1.0): 2, ('chaloniklo', 1.0): 2, ('pic.twitter.com/jxz2lbv6o', 1.0): 1, (\"paperwhite'\", 1.0): 1, ('laser-lik', 1.0): 1, ('focu', 1.0): 1, ('ghost', 1.0): 3, ('tagsforlikesapp', 1.0): 2, ('instagood', 1.0): 2, ('tbt', 1.0): 1, ('socket', 1.0): 1, ('spanner', 1.0): 1, ('😴', 1.0): 1, ('pglcsgo', 1.0): 1, ('x2', 1.0): 1, ('tend', 1.0): 1, ('crave', 1.0): 1, ('slower', 1.0): 1, ('sjw', 1.0): 1, ('cakehamp', 1.0): 1, ('glow', 1.0): 2, ('yayyy', 1.0): 1, ('merced', 1.0): 1, ('hood', 1.0): 1, ('badg', 1.0): 1, ('host', 1.0): 1, ('drone', 1.0): 1, ('blow', 1.0): 1, ('ignor', 1.0): 1, ('retali', 1.0): 1, ('bolling', 1.0): 1, (\"where'\", 1.0): 1, ('denmark', 1.0): 1, ('whitey', 1.0): 1, ('cultur', 1.0): 2, ('course', 1.0): 1, ('intro', 1.0): 2, ('graphicdesign', 1.0): 1, ('videograph', 1.0): 1, ('space', 1.0): 2, (\"ted'\", 1.0): 1, ('bogu', 1.0): 1, ('1000', 1.0): 1, ('hahahaaah', 1.0): 1, ('owli', 1.0): 1, ('afternon', 1.0): 1, ('whangarei', 1.0): 1, ('kati', 1.0): 2, ('paulin', 1.0): 1, ('traffick', 1.0): 1, ('wors', 1.0): 3, ('henc', 1.0): 1, ('express', 1.0): 1, ('wot', 1.0): 1, ('hand-lett', 1.0): 1, ('roof', 1.0): 1, ('eas', 1.0): 1, ('2/2', 1.0): 1, ('sour', 1.0): 1, ('dough', 1.0): 1, ('egypt', 1.0): 1, ('hubbi', 1.0): 2, ('sakin', 1.0): 1, ('six', 1.0): 1, ('christma', 1.0): 2, ('avril', 1.0): 1, ('n04j', 1.0): 1, ('25', 1.0): 1, ('prosecco', 1.0): 1, ('pech', 1.0): 1, ('micro', 1.0): 1, ('catspj', 1.0): 1, ('4:15', 1.0): 1, ('lazyweekend', 1.0): 1, ('overdu', 1.0): 1, ('mice', 1.0): 1, ('💃', 1.0): 3, ('jurass', 1.0): 1, ('ding', 1.0): 1, ('nila', 1.0): 1, ('8)', 1.0): 1, ('cooki', 1.0): 1, ('shir', 1.0): 1, ('0', 1.0): 3, ('hale', 1.0): 1, ('cheshir', 1.0): 1, ('decor', 1.0): 1, ('lemm', 1.0): 2, ('rec', 1.0): 1, ('ingat', 1.0): 1, ('din', 1.0): 2, ('mono', 1.0): 1, ('kathryn', 1.0): 1, ('jr', 1.0): 1, ('hsr', 1.0): 1, ('base', 1.0): 3, ('major', 1.0): 1, ('sugarrush', 1.0): 1, ('knit', 1.0): 1, ('partli', 1.0): 1, ('homegirl', 1.0): 1, ('nanci', 1.0): 1, ('fenja', 1.0): 1, ('aapk', 1.0): 1, ('benchmark', 1.0): 1, ('ke', 1.0): 1, ('hisaab', 1.0): 1, ('ho', 1.0): 1, ('gaya', 1.0): 1, ('ofc', 1.0): 1, ('rtss', 1.0): 1, ('hwait', 1.0): 1, ('titanfal', 1.0): 1, ('xbox', 1.0): 2, ('ultim', 1.0): 2, ('gastronomi', 1.0): 1, ('newblogpost', 1.0): 1, ('foodiefriday', 1.0): 1, ('foodi', 1.0): 1, ('yoghurt', 1.0): 1, ('pancak', 1.0): 2, ('sabah', 1.0): 3, ('kapima', 1.0): 1, ('gelen', 1.0): 1, ('guzel', 1.0): 1, ('bir', 1.0): 1, ('hediy', 1.0): 1, ('thanx', 1.0): 1, ('💞', 1.0): 2, ('visa', 1.0): 1, ('parisa', 1.0): 1, ('epiphani', 1.0): 1, ('lit', 1.0): 1, ('em-con', 1.0): 1, ('swore', 1.0): 1, ('0330 333 7234', 1.0): 1, ('kianweareproud', 1.0): 1, ('distract', 1.0): 1, ('dayofarch', 1.0): 1, ('10-20', 1.0): 1, ('bapu', 1.0): 1, ('ivypowel', 1.0): 1, ('newmus', 1.0): 1, ('sexchat', 1.0): 1, ('🍅', 1.0): 1, ('pathway', 1.0): 1, ('balkan', 1.0): 1, ('gypsi', 1.0): 1, ('mayhem', 1.0): 1, ('burek', 1.0): 1, ('meat', 1.0): 1, ('gibanica', 1.0): 1, ('pie', 1.0): 1, ('surrey', 1.0): 1, ('afterward', 1.0): 1, ('10.30', 1.0): 1, ('tempor', 1.0): 1, ('void', 1.0): 1, ('stem', 1.0): 1, ('sf', 1.0): 1, ('ykr', 1.0): 1, ('sparki', 1.0): 1, ('40mm', 1.0): 1, ('3.5', 1.0): 1, ('gr', 1.0): 1, ('rockfish', 1.0): 1, ('topwat', 1.0): 1, ('twitlong', 1.0): 1, ('me.so', 1.0): 1, ('jummah', 1.0): 3, ('durood', 1.0): 1, ('pak', 1.0): 1, ('cjradacomateada', 1.0): 2, ('supris', 1.0): 1, ('debut', 1.0): 1, ('shipper', 1.0): 1, ('asid', 1.0): 1, ('housem', 1.0): 1, ('737bigatingconcert', 1.0): 1, ('jedzjabłka', 1.0): 1, ('pijjabłka', 1.0): 1, ('polish', 1.0): 1, ('cider', 1.0): 1, ('mustread', 1.0): 1, ('cricket', 1.0): 1, ('5pm', 1.0): 1, ('queri', 1.0): 2, ('abbi', 1.0): 1, ('sumedh', 1.0): 1, ('sunnah', 1.0): 2, ('عن', 1.0): 2, ('quad', 1.0): 1, ('bike', 1.0): 1, ('carri', 1.0): 2, ('proprieti', 1.0): 1, ('chronic', 1.0): 1, ('superday', 1.0): 1, ('chocolatey', 1.0): 1, ('yasu', 1.0): 1, ('ooooh', 1.0): 1, ('hallo', 1.0): 2, ('dylan', 1.0): 2, ('laura', 1.0): 1, ('patric', 1.0): 2, ('keepin', 1.0): 1, ('mohr', 1.0): 1, ('guest', 1.0): 1, (\"o'neal\", 1.0): 1, ('tk', 1.0): 1, ('lua', 1.0): 1, ('stone', 1.0): 2, ('quicker', 1.0): 1, ('diet', 1.0): 1, ('sosweet', 1.0): 1, ('nominier', 1.0): 1, ('und', 1.0): 1, ('hardcor', 1.0): 1, ('😌', 1.0): 1, ('ff__special', 1.0): 1, ('acha', 1.0): 2, ('banda', 1.0): 1, ('✌', 1.0): 1, ('bhi', 1.0): 2, ('krta', 1.0): 1, ('beautifully-craft', 1.0): 1, ('mockingbird', 1.0): 1, ('diploma', 1.0): 1, ('blend', 1.0): 3, ('numbero', 1.0): 1, ('lolz', 1.0): 1, ('ambros', 1.0): 1, ('gwinett', 1.0): 1, ('bierc', 1.0): 1, ('ravag', 1.0): 1, ('illadvis', 1.0): 1, ('marriag', 1.0): 1, ('stare', 1.0): 1, ('cynic', 1.0): 2, ('yahuda', 1.0): 1, ('nosmet', 1.0): 1, ('poni', 1.0): 1, ('cuuut', 1.0): 1, (\"f'ing\", 1.0): 1, ('vacant', 1.0): 1, ('hauc', 1.0): 1, ('lovesss', 1.0): 1, ('hiss', 1.0): 1, ('overnight', 1.0): 1, ('cornish', 1.0): 1, ('all-clear', 1.0): 1, ('raincoat', 1.0): 1, ('measur', 1.0): 1, ('wealth', 1.0): 1, ('invest', 1.0): 2, ('garbi', 1.0): 1, ('wash', 1.0): 2, ('refuel', 1.0): 1, ('dunedin', 1.0): 1, ('kall', 1.0): 1, ('rakhi', 1.0): 1, ('12th', 1.0): 2, ('repres', 1.0): 3, ('slovenia', 1.0): 1, ('fridg', 1.0): 2, ('ludlow', 1.0): 1, ('28th', 1.0): 1, ('selway', 1.0): 1, ('submit', 1.0): 1, ('spanish', 1.0): 2, ('90210', 1.0): 1, ('oitnb', 1.0): 1, ('prepar', 1.0): 3, ('condit', 1.0): 1, ('msged', 1.0): 1, ('chiquito', 1.0): 1, ('ohaha', 1.0): 1, ('delhi', 1.0): 1, ('95', 1.0): 1, ('webtogsaward', 1.0): 1, ('grace', 1.0): 2, ('sheffield', 1.0): 1, ('tramlin', 1.0): 1, ('tl', 1.0): 2, ('hack', 1.0): 1, ('lad', 1.0): 1, ('beeepin', 1.0): 1, ('duper', 1.0): 1, ('handl', 1.0): 1, ('critiqu', 1.0): 1, ('contectu', 1.0): 1, ('ultor', 1.0): 2, ('mamaya', 1.0): 1, ('loiyal', 1.0): 1, ('para', 1.0): 1, ('truthfulwordsof', 1.0): 1, ('beanatividad', 1.0): 1, ('nknkkpagpapakumbaba', 1.0): 1, ('birthdaypres', 1.0): 1, ('compliment', 1.0): 1, ('swerv', 1.0): 1, ('goodtim', 1.0): 1, ('sinist', 1.0): 1, ('scare', 1.0): 1, ('tryna', 1.0): 1, ('anonym', 1.0): 1, ('dipsatch', 1.0): 1, ('aunt', 1.0): 1, ('dagga', 1.0): 1, ('burket', 1.0): 1, ('2am', 1.0): 1, ('twine', 1.0): 1, (\"diane'\", 1.0): 1, ('happybirthday', 1.0): 1, ('thanksss', 1.0): 1, ('randomli', 1.0): 1, ('buckinghampalac', 1.0): 1, ('chibi', 1.0): 1, ('maker', 1.0): 1, ('timog', 1.0): 1, ('18th', 1.0): 1, ('otw', 1.0): 1, ('kami', 1.0): 1, ('feelinggood', 1.0): 1, ('demand', 1.0): 2, ('naman', 1.0): 1, ('barkin', 1.0): 1, ('yeap', 1.0): 2, ('onkey', 1.0): 1, ('umma', 1.0): 1, ('pervert', 1.0): 1, ('onyu', 1.0): 1, ('appa', 1.0): 1, ('luci', 1.0): 1, ('horribl', 1.0): 1, ('quantum', 1.0): 1, ('greater', 1.0): 1, ('blockchain', 1.0): 1, ('nowplay', 1.0): 1, ('loftey', 1.0): 1, ('routt', 1.0): 1, ('assia', 1.0): 1, ('.\\n.\\n.', 1.0): 1, ('joint', 1.0): 1, ('futurereleas', 1.0): 1, (\"look'\", 1.0): 1, ('scari', 1.0): 1, ('murder', 1.0): 1, ('mysteri', 1.0): 1, ('comma', 1.0): 1, (\"j'\", 1.0): 1, ('hunni', 1.0): 2, ('diva', 1.0): 1, ('emili', 1.0): 3, ('nathan', 1.0): 1, ('medit', 1.0): 1, ('alumni', 1.0): 1, ('mba', 1.0): 1, ('foto', 1.0): 1, ('what-is-your-fashion', 1.0): 1, ('lorenangel', 1.0): 1, ('kw', 1.0): 2, ('tellanoldjokeday', 1.0): 1, ('reqd', 1.0): 1, ('specul', 1.0): 1, ('consist', 1.0): 4, ('tropic', 1.0): 1, ('startupph', 1.0): 1, ('zodiac', 1.0): 1, ('rapunzel', 1.0): 1, ('therver', 1.0): 1, ('85552', 1.0): 1, ('bestoftheday', 1.0): 1, ('oralsex', 1.0): 1, ('carli', 1.0): 1, ('happili', 1.0): 1, ('contract', 1.0): 1, ('matsu_bouzu', 1.0): 1, ('sonic', 1.0): 2, ('videogam', 1.0): 1, ('harana', 1.0): 1, ('belfast', 1.0): 1, ('danni', 1.0): 1, ('rare', 1.0): 1, ('sponsorship', 1.0): 1, ('aswel', 1.0): 1, ('gigi', 1.0): 1, ('nick', 1.0): 1, ('austin', 1.0): 1, ('youll', 1.0): 1, ('weak', 1.0): 4, ('10,000', 1.0): 1, ('bravo', 1.0): 1, ('iamamonst', 1.0): 1, ('rxthedailysurveyvot', 1.0): 1, ('broke', 1.0): 1, ('ass', 1.0): 1, ('roux', 1.0): 1, ('walkin', 1.0): 1, ('audienc', 1.0): 2, ('pfb', 1.0): 1, ('jute', 1.0): 1, ('walangmakakapigilsakin', 1.0): 1, ('lori', 1.0): 1, ('ehm', 1.0): 1, ('trick', 1.0): 1, ('baekhyun', 1.0): 1, ('eyesmil', 1.0): 1, ('borrow', 1.0): 1, ('knive', 1.0): 1, ('thek', 1.0): 1, ('eventu', 1.0): 1, ('reaapear', 1.0): 1, ('kno', 1.0): 1, ('whet', 1.0): 1, ('gratti', 1.0): 1, ('shorter', 1.0): 1, ('tweetin', 1.0): 1, ('inshallah', 1.0): 1, ('banana', 1.0): 1, ('raspberri', 1.0): 2, ('healthylifestyl', 1.0): 1, ('aint', 1.0): 2, ('skate', 1.0): 1, ('analyz', 1.0): 1, ('varieti', 1.0): 1, ('4:13', 1.0): 1, ('insomnia', 1.0): 1, ('medic', 1.0): 1, ('opposit', 1.0): 1, ('everlast', 1.0): 1, ('yoga', 1.0): 1, ('massag', 1.0): 2, ('osteopath', 1.0): 1, ('trainer', 1.0): 1, ('sharm', 1.0): 1, ('al_master_band', 1.0): 1, ('tbc', 1.0): 1, ('unives', 1.0): 1, ('architectur', 1.0): 1, ('random', 1.0): 1, ('isnt', 1.0): 1, ('typo', 1.0): 1, ('snark', 1.0): 1, ('lession', 1.0): 1, ('drunk', 1.0): 1, ('bruuh', 1.0): 1, ('2week', 1.0): 1, ('50europ', 1.0): 1, ('🇫🇷', 1.0): 4, ('iov', 1.0): 1, ('accord', 1.0): 1, ('mne', 1.0): 1, ('pchelok', 1.0): 1, ('ja', 1.0): 1, ('=:', 1.0): 2, ('sweetest', 1.0): 1, ('comet', 1.0): 1, ('ahah', 1.0): 1, ('candi', 1.0): 2, ('axio', 1.0): 1, ('rabbit', 1.0): 2, ('nutshel', 1.0): 1, ('taken', 1.0): 1, ('letshavecocktailsafternuclai', 1.0): 1, ('malik', 1.0): 1, ('umair', 1.0): 1, ('canon', 1.0): 1, ('gang', 1.0): 1, ('grind', 1.0): 1, ('thoracicbridg', 1.0): 1, ('5minut', 1.0): 1, ('nonscript', 1.0): 1, ('password', 1.0): 1, ('shoshannavassil', 1.0): 1, ('addmeonsnapchat', 1.0): 1, ('dmme', 1.0): 1, ('mpoint', 1.0): 2, ('soph', 1.0): 1, ('anot', 1.0): 1, ('liao', 1.0): 2, ('ord', 1.0): 1, ('lor', 1.0): 1, ('sibei', 1.0): 1, ('xialan', 1.0): 1, ('thnx', 1.0): 1, ('malfunct', 1.0): 1, ('clown', 1.0): 1, ('joker', 1.0): 1, ('\\U000fec00', 1.0): 1, ('nigth', 1.0): 1, ('estoy', 1.0): 1, ('escuchando', 1.0): 1, ('elsewher', 1.0): 1, ('bipolar', 1.0): 1, ('hahahahahahahahahahahahahaha', 1.0): 1, ('yoohoo', 1.0): 1, ('bajrangibhaijaanstorm', 1.0): 1, ('superhappi', 1.0): 1, ('doll', 1.0): 1, ('energi', 1.0): 1, ('f', 1.0): 3, (\"m'dear\", 1.0): 1, ('emma', 1.0): 2, ('alrd', 1.0): 1, ('dhan', 1.0): 2, ('satguru', 1.0): 1, ('tera', 1.0): 1, ('aasra', 1.0): 1, ('pita', 1.0): 1, ('keeo', 1.0): 1, ('darl', 1.0): 2, ('akarshan', 1.0): 1, ('sweetpea', 1.0): 1, ('gluten', 1.0): 1, ('pastri', 1.0): 2, ('highfiv', 1.0): 1, ('artsi', 1.0): 1, ('verbal', 1.0): 1, ('kaaa', 1.0): 1, ('oxford', 1.0): 2, ('wahoo', 1.0): 1, ('anchor', 1.0): 1, ('partnership', 1.0): 1, ('robbenisland', 1.0): 1, ('whale', 1.0): 1, ('aquat', 1.0): 1, ('safari', 1.0): 1, ('garru', 1.0): 1, ('liara', 1.0): 1, ('appoint', 1.0): 1, ('burnley', 1.0): 1, ('453', 1.0): 1, ('110', 1.0): 2, ('49', 1.0): 1, ('footbal', 1.0): 1, ('fm15', 1.0): 1, ('fmfamili', 1.0): 1, ('aamir', 1.0): 1, ('difficult', 1.0): 1, ('medium', 1.0): 1, ('nva', 1.0): 1, ('minuet', 1.0): 1, ('gamec', 1.0): 1, ('headrest', 1.0): 1, ('pit', 1.0): 1, ('spoken', 1.0): 1, ('advis', 1.0): 1, ('paypoint', 1.0): 1, ('deepthroat', 1.0): 1, ('truli', 1.0): 3, ('bee', 1.0): 2, ('upward', 1.0): 1, ('bound', 1.0): 1, ('movingonup', 1.0): 1, ('aitor', 1.0): 1, ('sn', 1.0): 1, ('ps4', 1.0): 2, ('jawad', 1.0): 1, ('presal', 1.0): 1, ('betcha', 1.0): 1, ('dumb', 1.0): 2, ('butt', 1.0): 1, ('qualki', 1.0): 1, ('808', 1.0): 1, ('milf', 1.0): 1, ('4like', 1.0): 1, ('sexysaturday', 1.0): 1, ('vw', 1.0): 1, ('umpfff', 1.0): 1, ('ca', 1.0): 1, ('domg', 1.0): 1, ('nanti', 1.0): 1, ('difollow', 1.0): 1, ('stubborn', 1.0): 1, ('nothavingit', 1.0): 1, ('klee', 1.0): 1, ('hem', 1.0): 1, ('congrad', 1.0): 1, ('accomplish', 1.0): 1, ('kfcroleplay', 1.0): 3, ('tregaron', 1.0): 1, ('boar', 1.0): 1, ('sweati', 1.0): 1, ('glyon', 1.0): 1, ('🚮', 1.0): 1, (\"tee'\", 1.0): 1, ('johnni', 1.0): 1, ('utub', 1.0): 1, (\"video'\", 1.0): 1, ('loss', 1.0): 1, ('combin', 1.0): 2, ('pigeon', 1.0): 1, ('fingerscross', 1.0): 1, ('photobomb', 1.0): 1, ('90', 1.0): 1, ('23', 1.0): 1, ('gimm', 1.0): 1, ('definetli', 1.0): 1, ('exit', 1.0): 1, ('bom-dia', 1.0): 1, ('apod', 1.0): 1, ('ultraviolet', 1.0): 1, ('m31', 1.0): 1, ('jul', 1.0): 1, ('oooh', 1.0): 1, ('yawn', 1.0): 1, ('ftw', 1.0): 1, ('maman', 1.0): 1, ('afterznoon', 1.0): 1, ('tweeep', 1.0): 1, ('abp', 1.0): 2, ('kiya', 1.0): 1, ('van', 1.0): 1, ('olymp', 1.0): 1, ('😷', 1.0): 1, ('classi', 1.0): 1, ('attach', 1.0): 1, ('equip', 1.0): 1, ('bobbl', 1.0): 1, ('anu', 1.0): 1, ('mh3', 1.0): 1, ('patch', 1.0): 1, ('psp', 1.0): 1, ('huffpost', 1.0): 1, ('tribut', 1.0): 1, ('h_eartshapedbox', 1.0): 1, ('magictrikband', 1.0): 1, ('magictrik', 1.0): 2, ('roommat', 1.0): 1, ('tami', 1.0): 1, ('b3dk', 1.0): 1, ('7an', 1.0): 1, ('ank', 1.0): 1, ('purpos', 1.0): 1, ('struggl', 1.0): 1, ('eagl', 1.0): 1, ('oceana', 1.0): 1, ('idk', 1.0): 3, ('med', 1.0): 1, ('fridayfauxpa', 1.0): 1, ('subtl', 1.0): 1, ('hint', 1.0): 1, ('prim', 1.0): 1, ('algorithm', 1.0): 1, ('iii', 1.0): 1, ('rosa', 1.0): 1, ('yvw', 1.0): 1, ('here', 1.0): 1, ('boost', 1.0): 1, ('unforgett', 1.0): 1, ('humor', 1.0): 1, (\"mum'\", 1.0): 1, ('hahahhaah', 1.0): 1, ('sombrero', 1.0): 1, ('lost', 1.0): 2, ('spammer', 1.0): 1, ('proceed', 1.0): 1, ('entertain', 1.0): 1, ('100k', 1.0): 1, ('mileston', 1.0): 1, ('judith', 1.0): 1, ('district', 1.0): 1, ('council', 1.0): 1, ('midar', 1.0): 1, ('gender', 1.0): 1, ('ilysm', 1.0): 1, ('zen', 1.0): 1, ('neat', 1.0): 1, ('rider', 1.0): 1, ('fyi', 1.0): 1, ('dig', 1.0): 2, ('👱🏽', 1.0): 1, ('👽', 1.0): 1, ('🌳', 1.0): 1, ('suspici', 1.0): 1, ('calori', 1.0): 1, ('harder', 1.0): 1, ('jessica', 1.0): 1, ('carina', 1.0): 1, ('francisco', 1.0): 1, ('teret', 1.0): 1, ('potassium', 1.0): 1, ('rehydr', 1.0): 1, ('drinkitallup', 1.0): 1, ('thirstquench', 1.0): 1, ('tapir', 1.0): 1, ('calf', 1.0): 1, ('mealtim', 1.0): 1, ('uhc', 1.0): 1, ('scale', 1.0): 1, ('network', 1.0): 1, ('areal', 1.0): 1, ('extremesport', 1.0): 1, ('quadbik', 1.0): 1, ('bloggersrequir', 1.0): 1, ('bloggersw', 1.0): 1, ('brainer', 1.0): 1, ('mse', 1.0): 1, ('fund', 1.0): 1, ('nooowww', 1.0): 1, ('lile', 1.0): 1, ('tid', 1.0): 1, ('tmi', 1.0): 1, ('deploy', 1.0): 1, ('jule', 1.0): 1, ('betti', 1.0): 1, ('hddc', 1.0): 1, ('salman', 1.0): 1, ('pthht', 1.0): 1, ('lfc', 1.0): 3, ('tope', 1.0): 1, ('xxoo', 1.0): 2, ('russia', 1.0): 2, ('silver-wash', 1.0): 1, ('fritillari', 1.0): 1, ('moon', 1.0): 1, ('ap', 1.0): 2, ('trash', 1.0): 2, ('clever', 1.0): 1, (\"thank'\", 1.0): 1, ('keven', 1.0): 1, ('pastim', 1.0): 1, ('ashramcal', 1.0): 1, ('ontrack', 1.0): 1, ('german', 1.0): 1, ('subtitl', 1.0): 1, ('pinter', 1.0): 1, ('morninggg', 1.0): 1, ('🐶', 1.0): 1, ('pete', 1.0): 1, ('awesome-o', 1.0): 1, ('multipl', 1.0): 1, ('cya', 1.0): 1, ('harrog', 1.0): 1, ('jet', 1.0): 1, ('supplier', 1.0): 1, ('req', 1.0): 1, ('fridayloug', 1.0): 1, ('4thstreetmus', 1.0): 1, ('hawaii', 1.0): 1, ('kick', 1.0): 1, ('deepli', 1.0): 1, ('john@timney.eclipse.co.uk', 1.0): 1, ('thousand', 1.0): 2, ('newspap', 1.0): 1, ('lew', 1.0): 1, ('nah', 1.0): 1, ('fallout', 1.0): 2, ('technic', 1.0): 1, ('gunderson', 1.0): 1, ('europa', 1.0): 1, ('thoroughli', 1.0): 1, ('script', 1.0): 1, ('overtak', 1.0): 1, ('motorway', 1.0): 1, ('thu', 1.0): 1, ('niteflirt', 1.0): 1, ('hbu', 1.0): 2, ('bowl', 1.0): 1, ('chri', 1.0): 2, ('niall', 1.0): 2, ('94', 1.0): 1, ('ik', 1.0): 1, ('stydia', 1.0): 1, ('nawazuddin', 1.0): 1, ('siddiqu', 1.0): 1, ('nomnomnom', 1.0): 1, ('dukefreebiefriday', 1.0): 1, ('z', 1.0): 1, ('insyaallah', 1.0): 1, ('ham', 1.0): 1, ('villa', 1.0): 1, ('brum', 1.0): 1, ('deni', 1.0): 1, ('vagina', 1.0): 1, ('rli', 1.0): 1, ('izzi', 1.0): 1, ('mitch', 1.0): 1, ('minn', 1.0): 1, ('recently.websit', 1.0): 1, ('coolingtow', 1.0): 1, ('soon.thank', 1.0): 1, ('showinginterest', 1.0): 1, ('multicolor', 1.0): 1, ('wid', 1.0): 1, ('wedg', 1.0): 1, ('motiv', 1.0): 1, ('nnnnot', 1.0): 1, (\"gf'\", 1.0): 1, ('bluesidemenxix', 1.0): 1, ('ardent', 1.0): 1, ('mooorn', 1.0): 1, ('wuppert', 1.0): 1, ('fridayfunday', 1.0): 1, ('re-sign', 1.0): 1, ('chalkhil', 1.0): 1, ('midday', 1.0): 1, ('carter', 1.0): 1, ('remedi', 1.0): 1, ('atrack', 1.0): 1, ('christ', 1.0): 1, ('badminton', 1.0): 1, (\"littl'un\", 1.0): 1, ('ikprideofpak', 1.0): 1, ('janjua', 1.0): 1, ('pimpl', 1.0): 1, ('forehead', 1.0): 1, ('volcano', 1.0): 1, ('mag', 1.0): 1, ('miryenda', 1.0): 1, (\"technology'\", 1.0): 1, ('touchétoday', 1.0): 1, ('idownload', 1.0): 1, ('25ish', 1.0): 1, ('snowbal', 1.0): 1, ('nd', 1.0): 1, ('expir', 1.0): 1, ('6gb', 1.0): 1, ('loveu', 1.0): 1, ('morefuninthephilippin', 1.0): 1, ('laho', 1.0): 1, ('caramoan', 1.0): 1, ('kareem', 1.0): 1, ('surah', 1.0): 1, ('kahaf', 1.0): 1, ('melani', 1.0): 1, ('bosch', 1.0): 1, ('machin', 1.0): 1, (\"week'\", 1.0): 1, ('refollow', 1.0): 1, ('😎', 1.0): 1, ('💁🏻', 1.0): 1, ('relaps', 1.0): 1, ('prada', 1.0): 2, ('punjabiswillgetit', 1.0): 1, ('hitter', 1.0): 1, ('mass', 1.0): 2, ('shoud', 1.0): 1, ('1:12', 1.0): 1, ('ughtm', 1.0): 1, ('545', 1.0): 1, ('kissm', 1.0): 1, ('likeforfollow', 1.0): 1, ('overwhelm', 1.0): 1, ('groupmat', 1.0): 1, ('75', 1.0): 2, ('kyunk', 1.0): 1, ('aitchison', 1.0): 1, ('curvi', 1.0): 1, ('mont', 1.0): 1, ('doa', 1.0): 1, ('header', 1.0): 1, ('speaker', 1.0): 3, ('avoid', 1.0): 1, ('laboratori', 1.0): 1, ('idc', 1.0): 1, ('fuckin', 1.0): 2, ('wooo', 1.0): 2, ('neobyt', 1.0): 1, ('pirat', 1.0): 1, ('takedown', 1.0): 1, ('indirag', 1.0): 1, ('judiciari', 1.0): 1, ('commit', 1.0): 4, ('govt', 1.0): 1, ('polici', 1.0): 1, ('rbi', 1.0): 1, ('similar', 1.0): 1, (\"thought'\", 1.0): 1, ('progress', 1.0): 1, ('transfer', 1.0): 1, ('gg', 1.0): 1, ('defenit', 1.0): 1, ('nofx', 1.0): 1, ('friskyfiday', 1.0): 1, ('yipee', 1.0): 1, ('shed', 1.0): 1, ('incent', 1.0): 1, ('vege', 1.0): 1, ('marin', 1.0): 1, ('gz', 1.0): 1, ('rajeev', 1.0): 1, ('hvng', 1.0): 1, ('funfil', 1.0): 1, ('friday.it', 1.0): 1, ('ws', 1.0): 1, ('reali', 1.0): 1, ('diff', 1.0): 1, ('kabir.fel', 1.0): 1, ('dresden', 1.0): 1, ('germani', 1.0): 1, ('plot', 1.0): 1, ('tdf', 1.0): 1, ('🍷', 1.0): 2, ('☀', 1.0): 2, ('🚲', 1.0): 2, ('minion', 1.0): 2, ('slot', 1.0): 1, (\"b'day\", 1.0): 1, ('isabella', 1.0): 1, ('okeyyy', 1.0): 1, ('vddd', 1.0): 1, (');', 1.0): 1, ('selfee', 1.0): 1, ('insta', 1.0): 1, ('🙆', 1.0): 1, ('🙌', 1.0): 1, ('😛', 1.0): 1, ('🐒', 1.0): 1, ('😝', 1.0): 1, ('hhahhaaa', 1.0): 1, ('jeez', 1.0): 1, ('teamcannib', 1.0): 1, ('teamspacewhalingisthebest', 1.0): 1, ('fitfa', 1.0): 1, ('identifi', 1.0): 1, ('pharmaci', 1.0): 1, ('verylaterealis', 1.0): 1, ('iwishiknewbett', 1.0): 1, ('satisfi', 1.0): 1, ('ess-aych-eye-te', 1.0): 1, ('supposedli', 1.0): 1, ('👍', 1.0): 1, ('immedi', 1.0): 1, (\"foxy'\", 1.0): 1, ('instrument', 1.0): 1, ('alon', 1.0): 2, ('goldcoast', 1.0): 1, ('lelomustfal', 1.0): 1, ('meal', 1.0): 1, ('5g', 1.0): 1, ('liker', 1.0): 1, ('newdress', 1.0): 1, ('resist', 1.0): 1, ('fot', 1.0): 1, ('troy', 1.0): 1, ('twitterfollowerswhatsup', 1.0): 1, ('happyfriedday', 1.0): 1, ('keepsafealway', 1.0): 1, ('loveyeah', 1.0): 1, ('emojasp_her', 1.0): 1, ('vanilla', 1.0): 1, ('sidemen', 1.0): 1, ('yaaayyy', 1.0): 1, ('friendaaa', 1.0): 1, ('bulb', 1.0): 5, ('corn', 1.0): 6, ('1tbps4', 1.0): 1, ('divin', 1.0): 1, ('wheeli', 1.0): 1, ('bin', 1.0): 1, ('ubericecream', 1.0): 1, ('messengerforaday', 1.0): 1, ('kyli', 1.0): 1, ('toilet', 1.0): 1, ('ikaw', 1.0): 1, ('musta', 1.0): 1, ('cheatmat', 1.0): 1, ('kyuhyun', 1.0): 1, ('ghanton', 1.0): 1, ('easy.get', 1.0): 1, ('5:30', 1.0): 1, ('therein', 1.0): 1, ('majalah', 1.0): 1, ('dominiqu', 1.0): 1, ('lamp', 1.0): 1, ('a-foot', 1.0): 1, ('revamp', 1.0): 1, ('brainchild', 1.0): 1, ('confid', 1.0): 1, ('confin', 1.0): 1, ('colorado', 1.0): 1, ('goodyear', 1.0): 1, ('upto', 1.0): 1, ('cashback', 1.0): 1, ('yourewelcom', 1.0): 1, ('nightli', 1.0): 1, ('simpin', 1.0): 1, ('sketchbook', 1.0): 1, ('4wild', 1.0): 1, ('colorpencil', 1.0): 1, ('cray', 1.0): 1, ('6:30', 1.0): 1, ('imma', 1.0): 3, ('ob', 1.0): 1, ('11h', 1.0): 1, ('kino', 1.0): 1, ('adult', 1.0): 1, ('kardamena', 1.0): 1, ('samo', 1.0): 1, ('greec', 1.0): 1, ('caesar', 1.0): 1, ('salad', 1.0): 1, ('tad', 1.0): 1, ('bland', 1.0): 1, ('respond', 1.0): 1, ('okk', 1.0): 1, ('den', 1.0): 1, ('allov', 1.0): 1, ('hangout', 1.0): 1, ('whoever', 1.0): 1, ('tourist', 1.0): 1, ('♌', 1.0): 1, ('kutiyapanti', 1.0): 1, ('profession', 1.0): 1, ('boomshot', 1.0): 1, ('fuh', 1.0): 1, ('yeeey', 1.0): 1, ('donot', 1.0): 1, ('expos', 1.0): 1, ('lipstick', 1.0): 1, ('cran', 1.0): 1, ('prayr', 1.0): 1, ('හෙල', 1.0): 1, ('හවුල', 1.0): 1, ('onemochaonelov', 1.0): 1, ('southpaw', 1.0): 1, ('geniu', 1.0): 1, ('stroma', 1.0): 1, ('🔴', 1.0): 1, ('younow', 1.0): 1, ('jonah', 1.0): 1, ('jareddd', 1.0): 1, ('postcod', 1.0): 1, ('talkmobil', 1.0): 1, ('huha', 1.0): 1, ('transform', 1.0): 1, ('sword', 1.0): 3, ('misread', 1.0): 1, ('richard', 1.0): 1, ('ibiza', 1.0): 1, ('birthdaymoneyforjesusjuic', 1.0): 1, ('ytb', 1.0): 1, ('tutori', 1.0): 1, ('construct', 1.0): 2, ('critic', 1.0): 1, ('ganesha', 1.0): 1, ('textur', 1.0): 1, ('photographi', 1.0): 1, ('hinduism', 1.0): 1, ('hindugod', 1.0): 1, ('elephantgod', 1.0): 1, ('selfish', 1.0): 1, ('bboy', 1.0): 1, ('cardgam', 1.0): 1, ('pixelart', 1.0): 1, ('gamedesign', 1.0): 1, ('indiedev', 1.0): 1, ('pixel_daili', 1.0): 1, ('plateau', 1.0): 1, ('laguna', 1.0): 1, ('tha', 1.0): 4, ('bahot', 1.0): 1, ('baje', 1.0): 1, ('raat', 1.0): 1, ('liya', 1.0): 1, ('hath', 1.0): 1, ('ghant', 1.0): 1, ('itna', 1.0): 2, ('bana', 1.0): 1, ('paya', 1.0): 1, ('uta', 1.0): 1, ('manga', 1.0): 1, ('jamuna', 1.0): 1, ('\\\\:', 1.0): 1, ('swiftma', 1.0): 1, ('trion', 1.0): 1, ('forum', 1.0): 1, ('b-day', 1.0): 1, ('disgust', 1.0): 1, ('commodor', 1.0): 1, ('annabel', 1.0): 1, ('bridg', 1.0): 1, ('quest', 1.0): 1, ('borderland', 1.0): 1, ('wanderrook', 1.0): 1, ('gm', 1.0): 1, ('preciou', 1.0): 2, ('mizz', 1.0): 1, ('bleedgreen', 1.0): 1, ('✌🏻', 1.0): 1, ('sophia', 1.0): 1, ('chicago', 1.0): 1, ('honeymoon', 1.0): 1, (\"da'esh\", 1.0): 1, ('co-ord', 1.0): 1, ('fsa', 1.0): 1, ('estat', 1.0): 1, (\"when'\", 1.0): 1, ('dusti', 1.0): 1, ('tunisia', 1.0): 2, (\"class'\", 1.0): 1, ('irrit', 1.0): 1, ('fiverr', 1.0): 1, ('gina', 1.0): 1, ('soproud', 1.0): 1, ('enought', 1.0): 1, ('hole', 1.0): 1, ('melbourneburg', 1.0): 1, ('arianna', 1.0): 1, ('esai', 1.0): 1, ('rotterdam', 1.0): 1, ('jordi', 1.0): 1, ('clasi', 1.0): 1, ('horni', 1.0): 1, ('salon', 1.0): 1, ('bleach', 1.0): 1, ('olaplex', 1.0): 1, ('damag', 1.0): 1, ('teamwork', 1.0): 1, ('zitecofficestori', 1.0): 1, ('다쇼', 1.0): 1, ('colleagu', 1.0): 1, ('eb', 1.0): 1, (\"t'would\", 1.0): 1, ('tweetup', 1.0): 1, ('detect', 1.0): 1, ('jonathancreek', 1.0): 1, ('dvr', 1.0): 1, ('kat', 1.0): 1, ('rarer', 1.0): 1, ('okkk', 1.0): 1, ('frend', 1.0): 1, ('milt', 1.0): 1, ('mario', 1.0): 1, ('rewatch', 1.0): 1, ('1600', 1.0): 1, ('sige', 1.0): 1, ('punta', 1.0): 1, ('kayo', 1.0): 1, ('nooo', 1.0): 1, ('prompt', 1.0): 1, ('t-mobil', 1.0): 1, ('orang', 1.0): 1, ('ee', 1.0): 1, ('teapot', 1.0): 1, ('hotter', 1.0): 1, ('»', 1.0): 1, ('londoutrad', 1.0): 1, ('kal', 1.0): 1, ('wayward', 1.0): 1, ('pine', 1.0): 1, ('muscl', 1.0): 1, ('ilikeit', 1.0): 1, ('belong', 1.0): 1, ('watford', 1.0): 1, ('enterpris', 1.0): 1, ('cube', 1.0): 1, ('particp', 1.0): 1, ('saudi', 1.0): 1, ('arabia', 1.0): 1, ('recogn', 1.0): 1, ('fanbas', 1.0): 3, ('bailona', 1.0): 3, ('responsibilti', 1.0): 1, ('sunlight', 1.0): 1, ('tiger', 1.0): 1, ('elev', 1.0): 1, ('horror', 1.0): 1, ('bitchesss', 1.0): 1, ('shitti', 1.0): 1, ('squash', 1.0): 1, ('becca', 1.0): 1, ('delta', 1.0): 1, ('nut', 1.0): 1, ('yun', 1.0): 1, ('joe', 1.0): 1, ('dirt', 1.0): 1, ('sharon', 1.0): 1, ('medicin', 1.0): 1, ('ttyl', 1.0): 1, ('gav', 1.0): 1, ('linda', 1.0): 1, ('3hr', 1.0): 1, ('tym', 1.0): 2, ('dieback', 1.0): 1, ('endit', 1.0): 1, ('minecon', 1.0): 1, ('sere', 1.0): 1, ('joerin', 1.0): 1, ('joshan', 1.0): 1, ('tandem', 1.0): 1, ('ligao', 1.0): 1, ('albay', 1.0): 1, ('bcyc', 1.0): 1, ('lnh', 1.0): 1, ('sat', 1.0): 1, ('honorari', 1.0): 1, ('alac', 1.0): 1, ('skelo_ghost', 1.0): 1, ('madadagdagan', 1.0): 1, ('bmc', 1.0): 1, ('11:11', 1.0): 2, ('embarrass', 1.0): 1, ('entropi', 1.0): 1, ('evolut', 1.0): 2, ('loop', 1.0): 1, ('eva', 1.0): 1, ('camden', 1.0): 1, ('uhh', 1.0): 1, ('scoup', 1.0): 1, ('jren', 1.0): 1, ('nuest', 1.0): 1, ('lovelayyy', 1.0): 1, ('kidney', 1.0): 1, ('neuer', 1.0): 1, ('spray', 1.0): 1, ('donnae.strydom@westerncape.gov.za', 1.0): 1, ('uni', 1.0): 1, ('uff', 1.0): 1, ('karhi', 1.0): 1, ('thi', 1.0): 1, ('juaquin', 1.0): 1, ('v3nzor99', 1.0): 1, ('shell', 1.0): 1, ('heyi', 1.0): 1, ('flavor', 1.0): 1, ('thakyou', 1.0): 1, ('beatriz', 1.0): 1, ('cancel', 1.0): 1, ('puff', 1.0): 1, ('egg', 1.0): 2, ('tart', 1.0): 1, ('chai', 1.0): 1, ('mtr', 1.0): 1, ('alyssa', 1.0): 1, ('rub', 1.0): 1, ('tummi', 1.0): 1, ('zelda', 1.0): 1, ('ive', 1.0): 1, ('🎂', 1.0): 1, ('jiva', 1.0): 1, ('🍹', 1.0): 1, ('🍻', 1.0): 1, ('mubbarak', 1.0): 1, ('deborah', 1.0): 1, ('coupon', 1.0): 1, ('colourdeb', 1.0): 1, ('purpl', 1.0): 1, (\"chippy'\", 1.0): 1, ('vessel', 1.0): 1, ('ps', 1.0): 2, ('vintag', 1.0): 1, ('✫', 1.0): 4, ('˚', 1.0): 4, ('·', 1.0): 4, ('✵', 1.0): 4, ('⊹', 1.0): 4, ('1710', 1.0): 1, ('gooffeanotter', 1.0): 1, ('kiksex', 1.0): 1, ('mugshot', 1.0): 1, ('token', 1.0): 1, ('maritimen', 1.0): 1, ('rh', 1.0): 1, ('tatton', 1.0): 1, ('jump_julia', 1.0): 1, ('malema', 1.0): 1, ('fren', 1.0): 1, ('nuf', 1.0): 1, ('teas', 1.0): 1, ('alien', 1.0): 2, ('closer', 1.0): 1, ('monitor', 1.0): 1, ('kimmi', 1.0): 1, (\"channel'\", 1.0): 1, ('planetbollywoodnew', 1.0): 1, ('epi', 1.0): 1, ('tricki', 1.0): 1, ('be-shak', 1.0): 1, ('chenoweth', 1.0): 1, ('oodl', 1.0): 1, ('hailey', 1.0): 1, ('craźi', 1.0): 1, ('sęxxxÿ', 1.0): 1, ('cøôl', 1.0): 1, ('runway', 1.0): 1, ('gooodnight', 1.0): 1, ('iv', 1.0): 1, ('ri', 1.0): 1, ('jayci', 1.0): 1, ('karaok', 1.0): 1, ('ltsw', 1.0): 1, ('giant', 1.0): 1, ('1709', 1.0): 1, ('refus', 1.0): 1, ('collagen', 1.0): 1, ('2win', 1.0): 1, ('hopetowin', 1.0): 1, ('inventori', 1.0): 1, ('loveforfood', 1.0): 1, ('foodforthought', 1.0): 1, ('thoughtfortheday', 1.0): 1, ('carp', 1.0): 1, ('diem', 1.0): 1, ('nath', 1.0): 1, ('ning', 1.0): 1, ('although', 1.0): 1, ('harm', 1.0): 1, ('stormi', 1.0): 1, ('sync', 1.0): 1, ('devic', 1.0): 1, ('mess', 1.0): 1, ('nylon', 1.0): 1, ('gvb', 1.0): 1, ('cd', 1.0): 1, ('mountain.titl', 1.0): 1, ('unto', 1.0): 1, ('theworldwouldchang', 1.0): 1, ('categori', 1.0): 1, ('mah', 1.0): 1, ('panel', 1.0): 1, (\"i'am\", 1.0): 1, ('80-1', 1.0): 1, ('1708', 1.0): 1, ('neenkin', 1.0): 1, ('masterpiec', 1.0): 1, ('debit', 1.0): 1, ('beagl', 1.0): 1, ('♫', 1.0): 1, ('feat', 1.0): 1, ('charli', 1.0): 1, ('puth', 1.0): 1, ('wiz', 1.0): 1, ('khalifa', 1.0): 1, ('svu', 1.0): 1, ('darker', 1.0): 1, ('berni', 1.0): 1, ('henri', 1.0): 1, ('trap', 1.0): 1, ('tommi', 1.0): 1, (\"vivian'\", 1.0): 1, ('transpar', 1.0): 1, ('bitcoin', 1.0): 1, ('insight', 1.0): 1, ('ping', 1.0): 1, ('masquerad', 1.0): 1, ('zorroreturm', 1.0): 1, ('1707', 1.0): 1, ('pk', 1.0): 1, ('hay', 1.0): 1, ('jacquelin', 1.0): 1, ('passion', 1.0): 1, ('full-fledg', 1.0): 1, ('workplac', 1.0): 1, ('venu', 1.0): 1, ('lago', 1.0): 1, ('luxord', 1.0): 1, ('potato', 1.0): 1, ('hundr', 1.0): 1, ('cite', 1.0): 1, ('academ', 1.0): 1, ('pokiri', 1.0): 1, ('1nenokkadin', 1.0): 1, ('heritag', 1.0): 1, ('wood', 1.0): 1, ('beleaf', 1.0): 1, ('spnfamili', 1.0): 1, ('spn', 1.0): 1, ('alwayskeepfight', 1.0): 1, ('jaredpadalecki', 1.0): 1, ('jensenackl', 1.0): 1, ('peasant', 1.0): 2, ('ahahha', 1.0): 1, ('distant', 1.0): 1, ('shout-out', 1.0): 1, ('adulthood', 1.0): 1, ('hopeless', 0.0): 2, ('tmr', 0.0): 3, (':(', 0.0): 4571, ('everyth', 0.0): 17, ('kid', 0.0): 20, ('section', 0.0): 3, ('ikea', 0.0): 1, ('cute', 0.0): 43, ('shame', 0.0): 19, (\"i'm\", 0.0): 343, ('nearli', 0.0): 3, ('19', 0.0): 8, ('2', 0.0): 41, ('month', 0.0): 23, ('heart', 0.0): 27, ('slide', 0.0): 1, ('wast', 0.0): 5, ('basket', 0.0): 1, ('“', 0.0): 15, ('hate', 0.0): 57, ('japanes', 0.0): 4, ('call', 0.0): 29, ('bani', 0.0): 2, ('”', 0.0): 11, ('dang', 0.0): 2, ('start', 0.0): 44, ('next', 0.0): 40, ('week', 0.0): 56, ('work', 0.0): 133, ('oh', 0.0): 92, ('god', 0.0): 15, ('babi', 0.0): 47, ('face', 0.0): 20, ('make', 0.0): 102, ('smile', 0.0): 10, ('neighbour', 0.0): 1, ('motor', 0.0): 1, ('ask', 0.0): 29, ('said', 0.0): 33, ('updat', 0.0): 11, ('search', 0.0): 3, ('sialan', 0.0): 1, ('athabasca', 0.0): 2, ('glacier', 0.0): 2, ('1948', 0.0): 1, (':-(', 0.0): 493, ('jasper', 0.0): 1, ('jaspernationalpark', 0.0): 1, ('alberta', 0.0): 1, ('explorealberta', 0.0): 1, ('…', 0.0): 16, ('realli', 0.0): 131, ('good', 0.0): 101, ('g', 0.0): 8, ('idea', 0.0): 10, ('never', 0.0): 57, ('go', 0.0): 224, ('meet', 0.0): 31, ('mare', 0.0): 1, ('ivan', 0.0): 1, ('happi', 0.0): 25, ('trip', 0.0): 11, ('keep', 0.0): 34, ('safe', 0.0): 5, ('see', 0.0): 124, ('soon', 0.0): 45, ('tire', 0.0): 50, ('hahahah', 0.0): 3, ('knee', 0.0): 2, ('replac', 0.0): 4, ('get', 0.0): 232, ('day', 0.0): 149, ('ouch', 0.0): 3, ('relat', 0.0): 2, ('sweet', 0.0): 7, ('n', 0.0): 21, ('sour', 0.0): 2, ('kind', 0.0): 11, ('bi-polar', 0.0): 1, ('peopl', 0.0): 75, ('life', 0.0): 33, ('...', 0.0): 331, ('cuz', 0.0): 4, ('full', 0.0): 16, ('pleass', 0.0): 2, ('im', 0.0): 129, ('sure', 0.0): 31, ('tho', 0.0): 28, ('feel', 0.0): 158, ('stupid', 0.0): 8, (\"can't\", 0.0): 180, ('seem', 0.0): 15, ('grasp', 0.0): 1, ('basic', 0.0): 2, ('digit', 0.0): 8, ('paint', 0.0): 3, ('noth', 0.0): 26, (\"i'v\", 0.0): 77, ('research', 0.0): 1, ('help', 0.0): 54, ('lord', 0.0): 2, ('lone', 0.0): 9, ('someon', 0.0): 57, ('talk', 0.0): 45, ('guy', 0.0): 62, ('girl', 0.0): 28, ('assign', 0.0): 5, ('project', 0.0): 3, ('😩', 0.0): 14, ('want', 0.0): 246, ('play', 0.0): 48, ('video', 0.0): 23, ('game', 0.0): 28, ('watch', 0.0): 77, ('movi', 0.0): 24, ('choreograph', 0.0): 1, ('hard', 0.0): 35, ('email', 0.0): 10, ('link', 0.0): 12, ('still', 0.0): 124, ('say', 0.0): 63, ('longer', 0.0): 12, ('avail', 0.0): 13, ('cri', 0.0): 46, ('bc', 0.0): 50, ('miss', 0.0): 301, ('mingm', 0.0): 1, ('much', 0.0): 139, ('sorri', 0.0): 148, ('mom', 0.0): 13, ('far', 0.0): 18, ('away', 0.0): 28, (\"we'r\", 0.0): 30, ('truli', 0.0): 5, ('flight', 0.0): 6, ('friend', 0.0): 39, ('happen', 0.0): 51, ('sad', 0.0): 123, ('dog', 0.0): 17, ('pee', 0.0): 2, ('’', 0.0): 27, ('bag', 0.0): 8, ('take', 0.0): 49, ('newwin', 0.0): 1, ('15', 0.0): 10, ('doushit', 0.0): 1, ('late', 0.0): 27, ('suck', 0.0): 23, ('sick', 0.0): 43, ('plan', 0.0): 17, ('first', 0.0): 27, ('gundam', 0.0): 1, ('night', 0.0): 46, ('nope', 0.0): 6, ('dollar', 0.0): 1, ('😭', 0.0): 29, ('listen', 0.0): 18, ('back', 0.0): 122, ('old', 0.0): 16, ('show', 0.0): 26, ('know', 0.0): 131, ('weird', 0.0): 10, ('got', 0.0): 104, ('u', 0.0): 193, ('leav', 0.0): 42, ('might', 0.0): 11, ('give', 0.0): 36, ('pale', 0.0): 2, ('imit', 0.0): 1, ('went', 0.0): 32, ('sea', 0.0): 1, ('massiv', 0.0): 4, ('fuck', 0.0): 58, ('rash', 0.0): 1, ('bodi', 0.0): 12, ('pain', 0.0): 21, ('thing', 0.0): 52, ('ever', 0.0): 30, ('home', 0.0): 63, ('hi', 0.0): 34, ('absent', 0.0): 1, ('gran', 0.0): 2, ('knew', 0.0): 6, ('care', 0.0): 20, ('tell', 0.0): 26, ('love', 0.0): 152, ('wish', 0.0): 91, ('would', 0.0): 70, ('sequel', 0.0): 1, ('busi', 0.0): 28, ('sa', 0.0): 15, ('school', 0.0): 32, ('time', 0.0): 166, ('yah', 0.0): 3, ('xx', 0.0): 18, ('ouucchhh', 0.0): 1, ('one', 0.0): 148, ('wisdom', 0.0): 2, ('teeth', 0.0): 6, ('come', 0.0): 91, ('frighten', 0.0): 1, ('case', 0.0): 6, ('pret', 0.0): 1, ('wkwkw', 0.0): 1, ('verfi', 0.0): 1, ('activ', 0.0): 6, ('forget', 0.0): 8, ('follow', 0.0): 262, ('member', 0.0): 6, ('thank', 0.0): 107, ('join', 0.0): 8, ('goodby', 0.0): 14, ('´', 0.0): 4, ('chain', 0.0): 1, ('—', 0.0): 26, ('sentir-s', 0.0): 1, ('incompleta', 0.0): 1, ('okay', 0.0): 38, ('..', 0.0): 108, ('wednesday', 0.0): 5, ('marvel', 0.0): 1, ('thwart', 0.0): 1, ('awh', 0.0): 3, (\"what'\", 0.0): 15, ('chanc', 0.0): 16, ('zant', 0.0): 1, ('need', 0.0): 106, ('someth', 0.0): 28, ('x', 0.0): 39, (\"when'\", 0.0): 1, ('birthday', 0.0): 23, ('worst', 0.0): 14, ('part', 0.0): 11, ('bad', 0.0): 73, ('audraesar', 0.0): 1, ('sushi', 0.0): 3, ('pic', 0.0): 15, ('tl', 0.0): 8, ('drive', 0.0): 16, ('craaazzyy', 0.0): 2, ('pop', 0.0): 3, ('like', 0.0): 228, ('helium', 0.0): 1, ('balloon', 0.0): 1, ('climatechang', 0.0): 5, ('cc', 0.0): 6, (\"california'\", 0.0): 1, ('power', 0.0): 6, ('influenti', 0.0): 1, ('air', 0.0): 3, ('pollut', 0.0): 1, ('watchdog', 0.0): 1, ('califor', 0.0): 1, ('elhaida', 0.0): 1, ('rob', 0.0): 2, ('juri', 0.0): 1, ('came', 0.0): 16, ('10th', 0.0): 1, ('televot', 0.0): 1, ('idaho', 0.0): 2, ('restrict', 0.0): 2, ('fish', 0.0): 2, ('despit', 0.0): 2, ('region', 0.0): 2, ('drought-link', 0.0): 1, ('die-of', 0.0): 1, ('abrupt', 0.0): 1, ('climat', 0.0): 1, ('chang', 0.0): 27, ('may', 0.0): 16, ('doom', 0.0): 2, ('mammoth', 0.0): 1, ('megafauna', 0.0): 1, ('sc', 0.0): 3, (\"australia'\", 0.0): 1, ('dirtiest', 0.0): 2, ('station', 0.0): 3, ('consid', 0.0): 5, ('clean', 0.0): 6, ('energi', 0.0): 3, ('biomass', 0.0): 1, (\"ain't\", 0.0): 5, ('easi', 0.0): 6, ('green', 0.0): 7, ('golf', 0.0): 1, ('cours', 0.0): 7, ('california', 0.0): 1, ('ulti', 0.0): 1, ('well', 0.0): 56, ('mine', 0.0): 12, ('gonna', 0.0): 51, ('sexi', 0.0): 14, ('prexi', 0.0): 1, ('kindergarten', 0.0): 1, ('hungri', 0.0): 19, ('cant', 0.0): 47, ('find', 0.0): 53, ('book', 0.0): 20, ('sane', 0.0): 1, ('liter', 0.0): 15, ('three', 0.0): 7, ('loung', 0.0): 1, ('event', 0.0): 4, ('turn', 0.0): 17, ('boss', 0.0): 5, ('hozier', 0.0): 1, (\"that'\", 0.0): 61, ('true', 0.0): 22, ('soooner', 0.0): 1, ('ahh', 0.0): 7, ('fam', 0.0): 3, ('respectlost', 0.0): 1, ('hypercholesteloremia', 0.0): 1, ('ok', 0.0): 33, ('look', 0.0): 100, ('gift', 0.0): 11, ('calibraska', 0.0): 1, ('actual', 0.0): 24, ('genuin', 0.0): 2, ('contend', 0.0): 1, ('head', 0.0): 23, ('alway', 0.0): 56, ('hurt', 0.0): 41, ('stay', 0.0): 24, ('lmao', 0.0): 13, ('older', 0.0): 5, ('sound', 0.0): 19, ('upset', 0.0): 11, ('infinit', 0.0): 10, ('ao', 0.0): 1, ('stick', 0.0): 1, ('8th', 0.0): 1, ('either', 0.0): 13, ('seriou', 0.0): 8, ('yun', 0.0): 1, ('eh', 0.0): 4, ('room', 0.0): 11, ('way', 0.0): 42, ('hot', 0.0): 15, ('havent', 0.0): 11, ('found', 0.0): 11, ('handsom', 0.0): 2, ('jack', 0.0): 3, ('draw', 0.0): 2, ('shit', 0.0): 36, ('cut', 0.0): 14, ('encor', 0.0): 4, ('4thwin', 0.0): 4, ('baymax', 0.0): 1, ('french', 0.0): 4, ('mixer', 0.0): 1, ('💜', 0.0): 6, ('wft', 0.0): 1, ('awesom', 0.0): 5, ('replay', 0.0): 1, ('parti', 0.0): 15, ('promot', 0.0): 3, ('music', 0.0): 16, ('bank', 0.0): 9, ('short', 0.0): 11, ('boy', 0.0): 18, ('order', 0.0): 16, ('receiv', 0.0): 7, ('hub', 0.0): 1, ('nearest', 0.0): 1, ('deliv', 0.0): 3, ('today', 0.0): 108, ('1/2', 0.0): 3, ('mum', 0.0): 14, ('loud', 0.0): 2, ('final', 0.0): 35, ('parasyt', 0.0): 1, ('alll', 0.0): 1, ('zayniscomingbackonjuli', 0.0): 23, ('26', 0.0): 24, ('bye', 0.0): 8, ('era', 0.0): 1, ('。', 0.0): 3, ('ω', 0.0): 1, ('」', 0.0): 2, ('∠', 0.0): 2, ('):', 0.0): 6, ('nathann', 0.0): 1, ('💕', 0.0): 7, ('hug', 0.0): 29, ('😊', 0.0): 9, ('beauti', 0.0): 11, ('dieididieieiei', 0.0): 1, ('stage', 0.0): 15, ('mean', 0.0): 43, ('hello', 0.0): 13, ('lion', 0.0): 3, ('think', 0.0): 75, ('screw', 0.0): 4, ('netflix', 0.0): 5, ('chill', 0.0): 7, ('di', 0.0): 7, ('ervin', 0.0): 1, ('ohh', 0.0): 8, ('yeah', 0.0): 41, ('hope', 0.0): 102, ('accept', 0.0): 2, ('offer', 0.0): 10, ('desper', 0.0): 2, ('year', 0.0): 46, ('snapchat', 0.0): 79, ('amargolonnard', 0.0): 2, ('kikhorni', 0.0): 13, ('snapm', 0.0): 4, ('tagsforlik', 0.0): 5, ('batalladelosgallo', 0.0): 2, ('webcamsex', 0.0): 4, ('ugh', 0.0): 26, ('stream', 0.0): 24, ('duti', 0.0): 3, (\"u'v\", 0.0): 1, ('gone', 0.0): 24, ('alien', 0.0): 1, ('aww', 0.0): 21, ('wanna', 0.0): 94, ('sorka', 0.0): 1, ('funer', 0.0): 4, ('text', 0.0): 15, ('phone', 0.0): 34, ('sunni', 0.0): 1, ('nonexist', 0.0): 1, ('wowza', 0.0): 1, ('fah', 0.0): 1, ('taylor', 0.0): 3, ('crop', 0.0): 1, ('boo', 0.0): 5, ('count', 0.0): 7, ('new', 0.0): 51, ('guitar', 0.0): 1, ('jonghyun', 0.0): 1, ('hyung', 0.0): 1, ('pleas', 0.0): 275, ('predict', 0.0): 2, ('sj', 0.0): 3, ('nomin', 0.0): 1, ('vs', 0.0): 4, ('pl', 0.0): 45, ('dude', 0.0): 12, ('calm', 0.0): 3, ('brace', 0.0): 5, ('sir', 0.0): 5, ('plu', 0.0): 4, ('4', 0.0): 18, ('shock', 0.0): 3, ('omggg', 0.0): 2, ('yall', 0.0): 4, ('deserv', 0.0): 8, ('whenev', 0.0): 3, ('spend', 0.0): 8, ('smoke', 0.0): 3, ('end', 0.0): 40, ('fall', 0.0): 16, ('asleep', 0.0): 25, ('1', 0.0): 26, ('point', 0.0): 14, ('close', 0.0): 20, ('grand', 0.0): 1, ('whyyi', 0.0): 7, ('long', 0.0): 38, ('must', 0.0): 15, ('annoy', 0.0): 11, ('evan', 0.0): 1, ('option', 0.0): 3, ('opt', 0.0): 1, (\"who'\", 0.0): 7, ('giveaway', 0.0): 3, ('muster', 0.0): 1, ('merch', 0.0): 4, ('ah', 0.0): 18, ('funni', 0.0): 6, ('drink', 0.0): 7, ('savanna', 0.0): 1, ('straw', 0.0): 1, ('ignor', 0.0): 16, ('yester', 0.0): 1, ('afternoon', 0.0): 3, ('sleep', 0.0): 90, ('ye', 0.0): 48, ('sadli', 0.0): 11, ('when', 0.0): 2, ('album', 0.0): 16, ('last', 0.0): 72, ('chocol', 0.0): 8, ('consum', 0.0): 1, ('werk', 0.0): 1, ('morn', 0.0): 31, ('foreal', 0.0): 1, ('wesen', 0.0): 1, ('uwes', 0.0): 1, ('mj', 0.0): 1, ('😂', 0.0): 24, ('catch', 0.0): 9, ('onlin', 0.0): 20, ('enough', 0.0): 24, ('haha', 0.0): 30, (\"he'\", 0.0): 23, ('bosen', 0.0): 1, ('die', 0.0): 21, ('egg', 0.0): 4, ('benni', 0.0): 1, ('sometim', 0.0): 16, ('followback', 0.0): 6, ('huhu', 0.0): 17, ('understand', 0.0): 15, ('badli', 0.0): 12, ('scare', 0.0): 16, ('&gt;:(', 0.0): 47, ('al', 0.0): 4, ('kati', 0.0): 3, ('zaz', 0.0): 1, ('ami', 0.0): 2, ('lot', 0.0): 27, ('diari', 0.0): 1, ('read', 0.0): 20, ('rehash', 0.0): 1, ('websit', 0.0): 7, ('mushroom', 0.0): 1, ('piec', 0.0): 4, ('except', 0.0): 5, ('reach', 0.0): 3, ('anyway', 0.0): 12, ('vicki', 0.0): 1, ('omg', 0.0): 63, ('wtf', 0.0): 13, ('lip', 0.0): 3, ('virgin', 0.0): 2, ('your', 0.0): 8, ('45', 0.0): 1, ('hahah', 0.0): 6, ('ninasti', 0.0): 1, ('tsktsk', 0.0): 1, ('oppa', 0.0): 4, ('wont', 0.0): 9, ('dick', 0.0): 5, ('kawaii', 0.0): 1, ('manli', 0.0): 1, ('xbox', 0.0): 3, ('alreadi', 0.0): 52, ('comfi', 0.0): 1, ('bed', 0.0): 12, ('youu', 0.0): 2, ('sigh', 0.0): 13, ('lol', 0.0): 43, ('potato', 0.0): 1, ('fri', 0.0): 7, ('guess', 0.0): 14, (\"y'all\", 0.0): 2, ('ugli', 0.0): 9, ('asf', 0.0): 1, ('huh', 0.0): 7, ('eish', 0.0): 1, ('ive', 0.0): 11, ('quit', 0.0): 9, ('lost', 0.0): 25, ('twitter', 0.0): 30, ('mojo', 0.0): 1, ('dont', 0.0): 53, ('mara', 0.0): 1, ('neh', 0.0): 2, ('fever', 0.0): 7, ('&lt;3', 0.0): 25, ('poor', 0.0): 35, ('bb', 0.0): 7, ('abl', 0.0): 22, ('associ', 0.0): 1, ('councillor', 0.0): 1, ('confer', 0.0): 2, ('weekend', 0.0): 25, ('skype', 0.0): 6, ('account', 0.0): 20, ('hack', 0.0): 8, ('contact', 0.0): 7, ('creat', 0.0): 2, ('tweet', 0.0): 35, ('spree', 0.0): 4, ('na', 0.0): 29, ('sholong', 0.0): 1, ('reject', 0.0): 7, ('propos', 0.0): 2, ('gee', 0.0): 1, ('fli', 0.0): 10, ('gidi', 0.0): 1, ('pamper', 0.0): 1, ('lago', 0.0): 1, ('ehn', 0.0): 1, ('arrest', 0.0): 1, ('girlfriend', 0.0): 2, ('he', 0.0): 3, ('nice', 0.0): 19, ('person', 0.0): 15, ('idk', 0.0): 26, ('anybodi', 0.0): 7, ('song', 0.0): 27, ('disappear', 0.0): 1, ('itun', 0.0): 3, ('daze', 0.0): 1, ('confus', 0.0): 8, ('surviv', 0.0): 5, ('fragment', 0.0): 1, (\"would'v\", 0.0): 2, ('forc', 0.0): 2, ('horribl', 0.0): 9, ('weather', 0.0): 29, ('us', 0.0): 43, ('could', 0.0): 69, ('walao', 0.0): 1, ('kb', 0.0): 1, ('send', 0.0): 12, ('ill', 0.0): 16, ('djderek', 0.0): 1, ('mani', 0.0): 29, ('fun', 0.0): 32, ('gig', 0.0): 3, ('absolut', 0.0): 6, ('legend', 0.0): 3, ('wait', 0.0): 43, ('till', 0.0): 8, ('saturday', 0.0): 10, ('homework', 0.0): 2, ('pa', 0.0): 8, ('made', 0.0): 23, ('da', 0.0): 5, ('greek', 0.0): 2, ('tragedi', 0.0): 1, ('rain', 0.0): 43, ('gym', 0.0): 6, ('💪🏻', 0.0): 1, ('🐒', 0.0): 1, ('what', 0.0): 8, ('wrong', 0.0): 33, ('struck', 0.0): 1, ('anymor', 0.0): 20, ('belgium', 0.0): 4, ('fabian', 0.0): 2, ('delph', 0.0): 6, ('fallen', 0.0): 3, ('hide', 0.0): 4, ('drake', 0.0): 1, ('silent', 0.0): 1, ('hear', 0.0): 33, ('rest', 0.0): 21, ('peac', 0.0): 5, ('mo', 0.0): 4, ('tonight', 0.0): 24, ('t20blast', 0.0): 1, ('ahhh', 0.0): 5, ('wake', 0.0): 21, ('mumma', 0.0): 2, ('7', 0.0): 16, ('dead', 0.0): 10, ('tomorrow', 0.0): 34, (\"i'll\", 0.0): 41, ('high', 0.0): 8, ('low', 0.0): 8, ('pray', 0.0): 13, ('appropri', 0.0): 1, ('. . .', 0.0): 2, ('awak', 0.0): 10, ('woke', 0.0): 14, ('upp', 0.0): 1, ('dm', 0.0): 23, ('luke', 0.0): 6, ('hey', 0.0): 26, ('babe', 0.0): 19, ('across', 0.0): 4, ('hindi', 0.0): 1, ('reaction', 0.0): 1, ('5s', 0.0): 1, ('run', 0.0): 15, ('space', 0.0): 5, ('tbh', 0.0): 14, ('disabl', 0.0): 2, ('pension', 0.0): 1, ('ptsd', 0.0): 1, ('imposs', 0.0): 4, ('physic', 0.0): 7, ('financi', 0.0): 2, ('nooo', 0.0): 16, ('broke', 0.0): 9, ('soo', 0.0): 3, ('amaz', 0.0): 16, ('toghet', 0.0): 1, ('around', 0.0): 20, ('p', 0.0): 5, ('hold', 0.0): 9, ('anoth', 0.0): 27, ('septemb', 0.0): 2, ('21st', 0.0): 2, ('snsd', 0.0): 2, ('interact', 0.0): 2, ('anna', 0.0): 5, ('akana', 0.0): 1, ('askip', 0.0): 1, (\"t'exist\", 0.0): 1, ('channel', 0.0): 6, ('owner', 0.0): 1, ('decid', 0.0): 10, ('broadcast', 0.0): 6, ('kei', 0.0): 2, ('rate', 0.0): 4, ('se', 0.0): 2, ('notic', 0.0): 26, ('exist', 0.0): 2, ('traffic', 0.0): 5, ('terribl', 0.0): 12, ('eye', 0.0): 12, ('small', 0.0): 9, ('kate', 0.0): 2, ('spade', 0.0): 1, ('pero', 0.0): 3, ('walang', 0.0): 1, ('maganda', 0.0): 1, ('aw', 0.0): 42, ('seen', 0.0): 23, ('agesss', 0.0): 1, ('add', 0.0): 26, ('corinehurleigh', 0.0): 1, ('snapchatm', 0.0): 6, ('instagram', 0.0): 4, ('addmeonsnapchat', 0.0): 2, ('sf', 0.0): 3, ('quot', 0.0): 6, ('kiksext', 0.0): 6, ('bum', 0.0): 2, ('zara', 0.0): 1, ('trouser', 0.0): 1, ('effect', 0.0): 4, ('spanish', 0.0): 1, (\"it'okay\", 0.0): 1, ('health', 0.0): 2, ('luck', 0.0): 6, ('freed', 0.0): 1, ('rock', 0.0): 3, ('orcalov', 0.0): 1, ('tri', 0.0): 65, ('big', 0.0): 21, ('cuddl', 0.0): 8, ('lew', 0.0): 1, ('kiss', 0.0): 4, ('em', 0.0): 1, ('crave', 0.0): 8, ('banana', 0.0): 4, ('crumbl', 0.0): 1, ('mcflurri', 0.0): 1, ('cabl', 0.0): 1, ('car', 0.0): 17, ('brother', 0.0): 10, (\"venus'\", 0.0): 1, ('concept', 0.0): 4, ('rli', 0.0): 5, ('tea', 0.0): 7, ('tagal', 0.0): 2, (\"we'v\", 0.0): 3, ('appoint', 0.0): 1, (\"i'd\", 0.0): 11, ('sinc', 0.0): 35, (\"there'\", 0.0): 18, ('milk', 0.0): 3, ('left', 0.0): 26, ('cereal', 0.0): 2, ('film', 0.0): 6, ('date', 0.0): 7, ('previou', 0.0): 2, ('73', 0.0): 2, ('user', 0.0): 1, ('everywher', 0.0): 6, ('fansign', 0.0): 1, ('photo', 0.0): 15, ('expens', 0.0): 7, ('zzzz', 0.0): 1, ('let', 0.0): 37, ('sun', 0.0): 10, ('yet', 0.0): 33, (\"bff'\", 0.0): 1, ('extrem', 0.0): 3, ('stress', 0.0): 10, ('anyth', 0.0): 19, ('win', 0.0): 27, (\"deosn't\", 0.0): 1, ('liverpool', 0.0): 2, ('pool', 0.0): 3, ('though', 0.0): 57, ('bro', 0.0): 3, ('great', 0.0): 22, ('news', 0.0): 21, ('self', 0.0): 1, ('esteem', 0.0): 1, ('lowest', 0.0): 1, ('better', 0.0): 36, ('tacki', 0.0): 1, ('taken', 0.0): 9, ('man', 0.0): 32, ('lucki', 0.0): 16, ('charm', 0.0): 1, ('haaretz', 0.0): 1, ('israel', 0.0): 1, ('syria', 0.0): 2, ('continu', 0.0): 1, ('develop', 0.0): 5, ('chemic', 0.0): 1, ('weapon', 0.0): 2, ('offici', 0.0): 3, ('wsj', 0.0): 2, ('rep', 0.0): 1, ('bt', 0.0): 4, ('mr', 0.0): 9, ('wong', 0.0): 1, ('confisc', 0.0): 1, ('art', 0.0): 4, ('thought', 0.0): 31, ('icepack', 0.0): 1, ('dose', 0.0): 2, ('killer', 0.0): 2, ('board', 0.0): 1, ('whimper', 0.0): 1, ('fan', 0.0): 17, ('senpai', 0.0): 1, ('buttsex', 0.0): 1, ('joke', 0.0): 8, ('headlin', 0.0): 1, (\"dn't\", 0.0): 1, ('brk', 0.0): 1, (\":'(\", 0.0): 13, ('hit', 0.0): 7, ('voic', 0.0): 9, ('falsetto', 0.0): 1, ('zone', 0.0): 2, ('leannerin', 0.0): 1, ('hornykik', 0.0): 17, ('loveofmylif', 0.0): 2, ('dmme', 0.0): 2, ('pussi', 0.0): 2, ('newmus', 0.0): 3, ('sexo', 0.0): 2, ('s2', 0.0): 1, ('spain', 0.0): 4, ('delay', 0.0): 5, ('kill', 0.0): 22, ('singl', 0.0): 10, ('untruth', 0.0): 1, ('cross', 0.0): 4, ('countri', 0.0): 6, ('ij', 0.0): 1, ('💥', 0.0): 1, ('✨', 0.0): 1, ('💫', 0.0): 1, ('bear', 0.0): 2, ('littl', 0.0): 21, ('apart', 0.0): 7, ('live', 0.0): 37, ('soshi', 0.0): 1, ('didnt', 0.0): 24, ('buttt', 0.0): 2, ('congrat', 0.0): 2, ('sunday', 0.0): 8, ('friday', 0.0): 12, ('shoulda', 0.0): 1, ('move', 0.0): 12, ('w', 0.0): 22, ('caus', 0.0): 16, (\"they'r\", 0.0): 14, ('heyyy', 0.0): 1, ('yeol', 0.0): 2, ('solo', 0.0): 6, ('dancee', 0.0): 1, ('inter', 0.0): 1, ('nemanja', 0.0): 1, ('vidic', 0.0): 1, ('roma', 0.0): 1, (\"mom'\", 0.0): 2, ('linguist', 0.0): 1, (\"dad'\", 0.0): 1, ('comput', 0.0): 6, ('scientist', 0.0): 1, ('dumbest', 0.0): 1, ('famili', 0.0): 9, ('broken', 0.0): 11, ('ice', 0.0): 35, ('cream', 0.0): 32, ('pour', 0.0): 1, ('crash', 0.0): 6, ('scienc', 0.0): 1, ('resourc', 0.0): 1, ('vehicl', 0.0): 5, ('ate', 0.0): 10, ('ayex', 0.0): 1, ('eat', 0.0): 27, ('swear', 0.0): 6, ('lamon', 0.0): 1, ('scroll', 0.0): 1, ('curv', 0.0): 2, ('😉', 0.0): 1, ('cement', 0.0): 1, ('cast', 0.0): 5, ('10.3', 0.0): 1, ('k', 0.0): 9, ('sign', 0.0): 9, ('zayn', 0.0): 8, ('bot', 0.0): 1, ('plz', 0.0): 3, ('mention', 0.0): 9, ('jmu', 0.0): 1, ('camp', 0.0): 7, ('teas', 0.0): 3, ('sweetest', 0.0): 1, ('awuna', 0.0): 1, ('mbulelo', 0.0): 1, ('match', 0.0): 7, ('pig', 0.0): 2, ('although', 0.0): 5, ('crackl', 0.0): 1, ('nois', 0.0): 3, ('plug', 0.0): 2, ('fuse', 0.0): 1, ('dammit', 0.0): 3, ('tip', 0.0): 2, ('carlton', 0.0): 2, ('aflblueshawk', 0.0): 2, (\"alex'\", 0.0): 1, ('hous', 0.0): 16, ('motorsport', 0.0): 1, ('seri', 0.0): 3, ('disc', 0.0): 1, ('right', 0.0): 51, ('cheeki', 0.0): 1, ('j', 0.0): 1, ('instead', 0.0): 4, ('seo', 0.0): 1, ('nl', 0.0): 1, ('bud', 0.0): 1, ('christi', 0.0): 1, ('xo', 0.0): 1, ('niec', 0.0): 1, ('summer', 0.0): 19, ('bloodi', 0.0): 2, ('sandwhich', 0.0): 1, ('buset', 0.0): 1, ('discrimin', 0.0): 4, ('five', 0.0): 5, ('learn', 0.0): 5, ('pregnanc', 0.0): 2, ('foot', 0.0): 5, ('f', 0.0): 4, ('matern', 0.0): 1, ('kick', 0.0): 6, ('domesticviol', 0.0): 1, ('law', 0.0): 4, ('domest', 0.0): 1, ('violenc', 0.0): 2, ('victim', 0.0): 4, ('98fm', 0.0): 1, ('exactli', 0.0): 5, ('unfortun', 0.0): 21, ('yesterday', 0.0): 13, ('uk', 0.0): 9, ('govern', 0.0): 1, ('sapiosexu', 0.0): 1, ('damn', 0.0): 29, ('beta', 0.0): 4, ('12', 0.0): 8, ('hour', 0.0): 35, ('world', 0.0): 17, ('hulk', 0.0): 3, ('hogan', 0.0): 3, ('scrub', 0.0): 1, ('wwe', 0.0): 2, ('histori', 0.0): 2, ('iren', 0.0): 4, ('mistak', 0.0): 6, ('naa', 0.0): 1, ('sold', 0.0): 6, ('h_my_k', 0.0): 1, ('lose', 0.0): 7, ('valentin', 0.0): 2, ('et', 0.0): 3, (\"r'ship\", 0.0): 1, ('btwn', 0.0): 1, ('homo', 0.0): 2, ('biphob', 0.0): 2, ('comment', 0.0): 4, ('certain', 0.0): 6, ('disciplin', 0.0): 2, ('incl', 0.0): 2, ('european', 0.0): 3, ('lang', 0.0): 6, ('lit', 0.0): 2, ('educ', 0.0): 2, ('fresherstofin', 0.0): 1, ('💔', 0.0): 3, ('dream', 0.0): 24, ('gettin', 0.0): 2, ('realist', 0.0): 4, ('thx', 0.0): 1, ('real', 0.0): 21, ('isnt', 0.0): 7, ('prefer', 0.0): 4, ('benzema', 0.0): 2, ('hahahahahaah', 0.0): 1, ('donno', 0.0): 1, ('korean', 0.0): 2, ('languag', 0.0): 5, ('russian', 0.0): 2, ('waaa', 0.0): 1, ('eidwithgrof', 0.0): 1, ('boreddd', 0.0): 1, ('mug', 0.0): 3, ('piss', 0.0): 3, ('tiddler', 0.0): 1, ('silli', 0.0): 2, ('least', 0.0): 15, ('card', 0.0): 7, ('chorong', 0.0): 1, ('leader', 0.0): 1, ('에이핑크', 0.0): 3, ('더쇼', 0.0): 4, ('clan', 0.0): 1, ('slot', 0.0): 2, ('open', 0.0): 16, ('pfff', 0.0): 1, ('privat', 0.0): 2, ('bugbounti', 0.0): 1, ('self-xss', 0.0): 1, ('host', 0.0): 2, ('header', 0.0): 3, ('poison', 0.0): 3, ('code', 0.0): 8, ('execut', 0.0): 1, ('ktksbye', 0.0): 1, ('connect', 0.0): 3, ('compani', 0.0): 3, ('alert', 0.0): 2, ('cancel', 0.0): 10, ('uber', 0.0): 3, ('everyon', 0.0): 26, ('els', 0.0): 4, ('offic', 0.0): 7, ('ahahah', 0.0): 1, ('petit', 0.0): 1, ('relationship', 0.0): 4, ('height', 0.0): 2, ('cost', 0.0): 1, ('600', 0.0): 2, ('£', 0.0): 6, ('secur', 0.0): 4, ('odoo', 0.0): 2, ('8', 0.0): 11, ('partner', 0.0): 2, ('commun', 0.0): 2, ('spirit', 0.0): 3, ('jgh', 0.0): 2, ('effin', 0.0): 1, ('facebook', 0.0): 4, ('anyon', 0.0): 17, (\"else'\", 0.0): 1, ('box', 0.0): 8, ('ap', 0.0): 3, ('stori', 0.0): 13, ('london', 0.0): 12, ('imagin', 0.0): 2, ('elsewher', 0.0): 1, ('someday', 0.0): 1, ('ben', 0.0): 3, ('provid', 0.0): 3, ('name', 0.0): 15, ('branch', 0.0): 1, ('visit', 0.0): 12, ('address', 0.0): 3, ('concern', 0.0): 3, ('welsh', 0.0): 1, ('pod', 0.0): 1, ('juli', 0.0): 12, ('laura', 0.0): 4, ('insid', 0.0): 10, ('train', 0.0): 12, ('d;', 0.0): 1, ('talk-kama', 0.0): 1, ('hawako', 0.0): 1, ('waa', 0.0): 1, ('kimaaani', 0.0): 1, ('prisss', 0.0): 1, ('baggag', 0.0): 2, ('claim', 0.0): 3, ('plane', 0.0): 2, ('niamh', 0.0): 1, ('forev', 0.0): 10, ('hmmm', 0.0): 2, ('sugar', 0.0): 3, ('rare', 0.0): 1, ('paper', 0.0): 16, ('town', 0.0): 14, ('score', 0.0): 3, ('stuck', 0.0): 8, ('agh', 0.0): 2, ('middl', 0.0): 7, ('undercoverboss', 0.0): 1, ('تكفى', 0.0): 1, ('10', 0.0): 8, ('job', 0.0): 13, ('cat', 0.0): 17, ('forgotten', 0.0): 3, ('yep', 0.0): 5, ('stop', 0.0): 43, ('ach', 0.0): 2, ('wrist', 0.0): 1, ('nake', 0.0): 3, ('forgot', 0.0): 14, ('bracelet', 0.0): 3, ('ligo', 0.0): 1, ('dozen', 0.0): 1, ('parent', 0.0): 8, ('children', 0.0): 2, ('shark', 0.0): 2, ('selfi', 0.0): 6, ('heartach', 0.0): 1, ('zayniscomingback', 0.0): 3, ('mix', 0.0): 2, ('sweden', 0.0): 1, ('breath', 0.0): 4, ('moment', 0.0): 14, ('word', 0.0): 16, ('elmhurst', 0.0): 1, ('fc', 0.0): 1, ('etid', 0.0): 1, (\"chillin'with\", 0.0): 1, ('father', 0.0): 2, ('istanya', 0.0): 1, ('2suppli', 0.0): 1, ('extra', 0.0): 3, ('infrastructur', 0.0): 2, ('teacher', 0.0): 2, ('doctor', 0.0): 4, ('nurs', 0.0): 2, ('paramed', 0.0): 1, ('countless', 0.0): 1, ('2cope', 0.0): 1, ('bore', 0.0): 23, ('plea', 0.0): 2, ('arian', 0.0): 1, ('hahahaha', 0.0): 6, ('slr', 0.0): 1, ('kendal', 0.0): 1, ('kyli', 0.0): 3, (\"kylie'\", 0.0): 1, ('manila', 0.0): 3, ('jeebu', 0.0): 1, ('reabsorbt', 0.0): 1, ('tooth', 0.0): 2, ('abscess', 0.0): 1, ('threaten', 0.0): 2, ('affect', 0.0): 1, ('front', 0.0): 6, ('crown', 0.0): 1, ('ooouch', 0.0): 1, ('barney', 0.0): 1, (\"be'\", 0.0): 1, ('yo', 0.0): 4, ('later', 0.0): 14, ('realis', 0.0): 6, ('problemat', 0.0): 1, ('expect', 0.0): 5, ('proud', 0.0): 8, ('mess', 0.0): 7, ('maa', 0.0): 2, ('without', 0.0): 25, ('bangalor', 0.0): 1, ('awww', 0.0): 23, ('lui', 0.0): 1, ('manzano', 0.0): 1, ('shaaa', 0.0): 1, ('super', 0.0): 11, ('7th', 0.0): 1, ('conven', 0.0): 1, ('2:30', 0.0): 2, ('pm', 0.0): 8, ('forward', 0.0): 6, ('delet', 0.0): 5, ('turkey', 0.0): 1, ('bomb', 0.0): 3, ('isi', 0.0): 1, ('allow', 0.0): 9, ('usa', 0.0): 2, ('use', 0.0): 43, ('airfield', 0.0): 1, ('jet', 0.0): 1, (\"jack'\", 0.0): 1, ('spam', 0.0): 6, ('sooo', 0.0): 16, ('☺', 0.0): 3, (\"mommy'\", 0.0): 1, ('reason', 0.0): 8, ('overweight', 0.0): 1, ('sigeg', 0.0): 1, ('habhab', 0.0): 1, ('masud', 0.0): 1, ('kaha', 0.0): 1, ('ko', 0.0): 10, ('akong', 0.0): 1, ('un', 0.0): 1, ('hella', 0.0): 4, ('matter', 0.0): 4, ('pala', 0.0): 1, ('hahaha', 0.0): 11, ('lesson', 0.0): 1, ('dolphin', 0.0): 1, ('xxx', 0.0): 12, ('holi', 0.0): 2, ('anythin', 0.0): 1, ('trend', 0.0): 6, ('radio', 0.0): 4, ('sing', 0.0): 5, ('bewar', 0.0): 1, ('agonis', 0.0): 1, ('experi', 0.0): 2, ('ahead', 0.0): 3, ('modimo', 0.0): 1, ('ho', 0.0): 3, ('tseba', 0.0): 1, ('wena', 0.0): 1, ('fela', 0.0): 1, ('emot', 0.0): 8, ('hubbi', 0.0): 1, ('delight', 0.0): 1, ('return', 0.0): 6, ('bill', 0.0): 6, ('nowt', 0.0): 1, ('wors', 0.0): 8, ('willi', 0.0): 1, ('gon', 0.0): 1, ('vomit', 0.0): 1, ('famou', 0.0): 5, ('bowl', 0.0): 1, ('devast', 0.0): 1, ('titan', 0.0): 1, ('ae', 0.0): 1, ('mark', 0.0): 2, ('hair', 0.0): 21, ('shini', 0.0): 1, ('wavi', 0.0): 1, ('emo', 0.0): 2, ('germani', 0.0): 4, ('load', 0.0): 9, ('shed', 0.0): 2, ('ha', 0.0): 7, ('bheyp', 0.0): 1, ('ayemso', 0.0): 1, ('ear', 0.0): 5, ('swell', 0.0): 2, ('sm', 0.0): 7, ('fb', 0.0): 7, ('remind', 0.0): 3, ('abt', 0.0): 3, ('womad', 0.0): 1, ('wut', 0.0): 1, ('hell', 0.0): 11, ('viciou', 0.0): 1, ('circl', 0.0): 1, ('surpris', 0.0): 5, ('ticket', 0.0): 12, ('codi', 0.0): 1, ('simpson', 0.0): 1, ('concert', 0.0): 11, ('singapor', 0.0): 4, ('august', 0.0): 5, ('pooo', 0.0): 2, ('bh3', 0.0): 1, ('enter', 0.0): 1, ('pitchwar', 0.0): 1, ('chap', 0.0): 1, (\"mine'\", 0.0): 1, ('transcript', 0.0): 1, (\"apma'\", 0.0): 1, ('shoulder', 0.0): 2, ('bitch', 0.0): 11, ('competit', 0.0): 1, (\"it'll\", 0.0): 3, ('fine', 0.0): 6, ('timw', 0.0): 1, ('acc', 0.0): 8, ('rude', 0.0): 11, ('vitamin', 0.0): 1, ('e', 0.0): 9, ('oil', 0.0): 1, ('massag', 0.0): 5, ('everyday', 0.0): 7, ('healthier', 0.0): 1, ('easier', 0.0): 3, ('stretch', 0.0): 1, ('choos', 0.0): 7, ('blockjam', 0.0): 1, (\"schedule'\", 0.0): 1, ('whack', 0.0): 1, ('kik', 0.0): 69, ('thelock', 0.0): 1, ('76', 0.0): 1, ('sex', 0.0): 6, ('omegl', 0.0): 4, ('coupl', 0.0): 2, ('travel', 0.0): 11, ('hotgirl', 0.0): 2, ('2009', 0.0): 1, ('3', 0.0): 32, ('ghantay', 0.0): 1, ('light', 0.0): 8, ('nai', 0.0): 1, ('hay', 0.0): 8, ('deni', 0.0): 1, ('ruin', 0.0): 11, ('laguna', 0.0): 1, ('exit', 0.0): 2, ('gomen', 0.0): 1, ('heck', 0.0): 5, ('fair', 0.0): 12, ('grew', 0.0): 2, ('half', 0.0): 10, ('inch', 0.0): 2, ('two', 0.0): 19, ('problem', 0.0): 7, ('suuuper', 0.0): 1, ('65', 0.0): 1, ('sale', 0.0): 8, ('inact', 0.0): 8, ('orphan', 0.0): 1, ('black', 0.0): 12, ('earlier', 0.0): 9, ('whaaat', 0.0): 5, ('kaya', 0.0): 2, ('naaan', 0.0): 1, ('paus', 0.0): 1, ('randomli', 0.0): 1, ('app', 0.0): 13, ('3:30', 0.0): 1, ('walk', 0.0): 7, ('inglewood', 0.0): 1, ('ummm', 0.0): 4, ('anxieti', 0.0): 3, ('readi', 0.0): 12, ('also', 0.0): 19, ('charcoal', 0.0): 1, ('til', 0.0): 5, ('mid-end', 0.0): 1, ('aug', 0.0): 1, ('noooo', 0.0): 1, ('heard', 0.0): 6, ('rip', 0.0): 12, ('rodfanta', 0.0): 1, ('wasp', 0.0): 2, ('sting', 0.0): 1, ('avert', 0.0): 1, ('bug', 0.0): 3, ('(:', 0.0): 7, ('exo', 0.0): 2, ('seekli', 0.0): 1, ('riptito', 0.0): 1, ('manbearpig', 0.0): 1, ('cannot', 0.0): 7, ('grow', 0.0): 3, ('shorter', 0.0): 1, ('academ', 0.0): 1, ('free', 0.0): 19, ('exclus', 0.0): 2, ('unfair', 0.0): 7, ('esp', 0.0): 4, ('regard', 0.0): 1, ('current', 0.0): 7, ('bleak', 0.0): 1, ('german', 0.0): 1, ('chart', 0.0): 2, ('situat', 0.0): 2, ('entri', 0.0): 4, ('even', 0.0): 70, ('top', 0.0): 6, ('100', 0.0): 8, ('pfft', 0.0): 1, ('place', 0.0): 18, ('white', 0.0): 7, ('wash', 0.0): 1, ('polaroid', 0.0): 1, ('newbethvideo', 0.0): 1, ('greec', 0.0): 2, ('xur', 0.0): 2, ('imi', 0.0): 3, ('fill', 0.0): 1, ('♡', 0.0): 11, ('♥', 0.0): 22, ('xoxoxo', 0.0): 1, ('pictur', 0.0): 17, ('stud', 0.0): 1, ('hund', 0.0): 1, ('6', 0.0): 14, ('kikchat', 0.0): 9, ('amazon', 0.0): 5, ('3.4', 0.0): 1, ('yach', 0.0): 1, ('telat', 0.0): 1, ('huvvft', 0.0): 1, ('zoo', 0.0): 2, ('fieldtrip', 0.0): 1, ('touch', 0.0): 5, ('yan', 0.0): 1, ('posit', 0.0): 2, ('king', 0.0): 1, ('futur', 0.0): 4, ('sizw', 0.0): 1, ('write', 0.0): 13, ('20', 0.0): 9, ('result', 0.0): 3, ('km', 0.0): 2, ('four', 0.0): 4, ('shift', 0.0): 5, ('aaahhh', 0.0): 2, ('boredom', 0.0): 1, ('en', 0.0): 1, ('aint', 0.0): 7, ('who', 0.0): 1, ('sins', 0.0): 1, ('that', 0.0): 13, ('somehow', 0.0): 2, ('tini', 0.0): 4, ('ball', 0.0): 2, ('barbel', 0.0): 1, ('owww', 0.0): 2, ('amsterdam', 0.0): 1, ('luv', 0.0): 2, ('💖', 0.0): 4, ('ps', 0.0): 3, ('looong', 0.0): 1, ('especi', 0.0): 4, (':/', 0.0): 11, ('lap', 0.0): 1, ('litro', 0.0): 1, ('shepherd', 0.0): 2, ('lami', 0.0): 1, ('mayb', 0.0): 27, ('relax', 0.0): 3, ('lungomar', 0.0): 1, ('pesaro', 0.0): 1, ('giachietittiwed', 0.0): 1, ('igersoftheday', 0.0): 1, ('summertim', 0.0): 1, ('nose', 0.0): 7, ('bruis', 0.0): 1, ('lil', 0.0): 8, ('snake', 0.0): 3, ('journey', 0.0): 2, ('scarf', 0.0): 1, ('au', 0.0): 3, ('afford', 0.0): 7, ('fridayfeel', 0.0): 1, ('earli', 0.0): 12, ('money', 0.0): 24, ('chicken', 0.0): 5, ('woe', 0.0): 4, ('nigga', 0.0): 3, ('motn', 0.0): 1, ('make-up', 0.0): 1, ('justic', 0.0): 1, ('import', 0.0): 4, ('sit', 0.0): 5, ('mind', 0.0): 7, ('buy', 0.0): 17, ('limit', 0.0): 4, ('ver', 0.0): 1, ('normal', 0.0): 5, ('edit', 0.0): 7, ('huhuhu', 0.0): 3, ('stack', 0.0): 1, (\"m'ladi\", 0.0): 1, ('j8', 0.0): 1, ('j11', 0.0): 1, ('m20', 0.0): 1, ('jk', 0.0): 5, ('acad', 0.0): 1, ('schedul', 0.0): 9, ('nowww', 0.0): 1, ('cop', 0.0): 1, ('jame', 0.0): 4, ('window', 0.0): 6, ('hugh', 0.0): 2, ('paw', 0.0): 1, ('muddi', 0.0): 1, ('distract', 0.0): 1, ('heyi', 0.0): 1, ('otherwis', 0.0): 3, ('picnic', 0.0): 1, ('24', 0.0): 11, ('cupcak', 0.0): 2, ('talaga', 0.0): 1, ('best', 0.0): 22, ('femal', 0.0): 3, ('poppin', 0.0): 1, ('joc', 0.0): 1, ('playin', 0.0): 1, ('saw', 0.0): 19, ('fix', 0.0): 10, ('coldplay', 0.0): 1, ('media', 0.0): 1, ('player', 0.0): 3, ('fail', 0.0): 10, ('subj', 0.0): 1, ('sobrang', 0.0): 1, ('bv', 0.0): 1, ('zamn', 0.0): 1, ('line', 0.0): 8, ('afropunk', 0.0): 1, ('fest', 0.0): 1, ('brooklyn', 0.0): 2, ('id', 0.0): 5, ('put', 0.0): 14, ('50', 0.0): 5, ('madrid', 0.0): 7, ('shithous', 0.0): 1, ('cutest', 0.0): 2, ('danc', 0.0): 6, ('ur', 0.0): 26, ('arm', 0.0): 3, ('rais', 0.0): 1, ('hand', 0.0): 12, ('ladder', 0.0): 2, ('told', 0.0): 11, ('climb', 0.0): 3, ('success', 0.0): 4, ('nerv', 0.0): 1, ('wrack', 0.0): 1, ('test', 0.0): 8, ('booset', 0.0): 1, ('restart', 0.0): 1, ('assassin', 0.0): 1, ('creed', 0.0): 1, ('ii', 0.0): 1, ('heap', 0.0): 1, ('fell', 0.0): 10, ('daughter', 0.0): 1, ('begin', 0.0): 4, ('ps3', 0.0): 1, ('ankl', 0.0): 4, ('step', 0.0): 5, ('puddl', 0.0): 2, ('wear', 0.0): 5, ('slipper', 0.0): 1, ('eve', 0.0): 1, ('bbi', 0.0): 6, ('sararoc', 0.0): 1, ('angri', 0.0): 5, ('pretti', 0.0): 15, ('fnaf', 0.0): 1, ('holiday', 0.0): 20, ('cheer', 0.0): 6, ('😘', 0.0): 11, ('anywayhedidanicejob', 0.0): 1, ('😞', 0.0): 3, ('3am', 0.0): 2, ('other', 0.0): 7, ('local', 0.0): 3, ('cruis', 0.0): 1, ('done', 0.0): 24, ('doubl', 0.0): 4, ('wail', 0.0): 1, ('manual', 0.0): 2, ('wheelchair', 0.0): 1, ('check', 0.0): 19, ('fit', 0.0): 3, ('nh', 0.0): 3, ('26week', 0.0): 1, ('sbenu', 0.0): 1, ('sasin', 0.0): 1, ('team', 0.0): 14, ('anarchi', 0.0): 1, ('af', 0.0): 14, ('candl', 0.0): 1, ('forehead', 0.0): 4, ('medicin', 0.0): 3, ('welcom', 0.0): 5, ('oop', 0.0): 4, ('hoya', 0.0): 3, ('mah', 0.0): 2, ('a', 0.0): 1, ('nobodi', 0.0): 10, ('awhil', 0.0): 2, ('ago', 0.0): 20, ('b', 0.0): 10, ('hush', 0.0): 2, ('gurli', 0.0): 1, ('bring', 0.0): 9, ('purti', 0.0): 1, ('mouth', 0.0): 5, ('closer', 0.0): 2, ('shiver', 0.0): 1, ('solut', 0.0): 1, ('paid', 0.0): 8, ('properli', 0.0): 2, ('gol', 0.0): 1, ('pea', 0.0): 1, ('english', 0.0): 9, ('mental', 0.0): 4, ('tierd', 0.0): 2, ('third', 0.0): 1, (\"eye'\", 0.0): 1, ('thnkyouuu', 0.0): 1, ('carolin', 0.0): 1, ('neither', 0.0): 6, ('figur', 0.0): 6, ('mirror', 0.0): 1, ('highlight', 0.0): 2, ('pure', 0.0): 3, ('courag', 0.0): 1, ('bit', 0.0): 15, ('fishi', 0.0): 1, ('idek', 0.0): 1, ('apink', 0.0): 5, ('perform', 0.0): 8, ('bulet', 0.0): 1, ('gendut', 0.0): 1, ('noo', 0.0): 5, ('race', 0.0): 3, ('hotwheel', 0.0): 1, ('ms', 0.0): 1, ('patch', 0.0): 1, ('typic', 0.0): 2, ('ahaha', 0.0): 1, ('lay', 0.0): 2, ('wine', 0.0): 1, ('glass', 0.0): 3, (\"where'\", 0.0): 4, ('akon', 0.0): 1, ('somewher', 0.0): 5, ('nightmar', 0.0): 7, ('ya', 0.0): 15, ('mino', 0.0): 2, ('crazyyi', 0.0): 1, ('thooo', 0.0): 1, ('zz', 0.0): 1, ('airport', 0.0): 7, ('straight', 0.0): 4, ('soundcheck', 0.0): 1, ('hmm', 0.0): 4, ('antagonist', 0.0): 1, ('ob', 0.0): 1, ('phantasi', 0.0): 1, ('star', 0.0): 4, ('ip', 0.0): 1, ('issu', 0.0): 11, ('bruce', 0.0): 1, ('sleepdepriv', 0.0): 1, ('tiredashel', 0.0): 1, ('4aspot', 0.0): 1, (\"kinara'\", 0.0): 1, ('awami', 0.0): 1, ('question', 0.0): 9, ('niqqa', 0.0): 1, ('answer', 0.0): 14, ('mockingjay', 0.0): 1, ('slow', 0.0): 9, ('pb.contest', 0.0): 1, ('cycl', 0.0): 2, ('aarww', 0.0): 1, ('lmbo', 0.0): 1, ('dangit', 0.0): 1, ('ohmygod', 0.0): 1, ('scenario', 0.0): 1, ('tooo', 0.0): 2, ('duck', 0.0): 1, ('baechyyi', 0.0): 1, ('okayyy', 0.0): 1, ('noon', 0.0): 3, ('drag', 0.0): 5, ('serious', 0.0): 11, ('misundersrand', 0.0): 1, ('chal', 0.0): 1, ('raha', 0.0): 1, ('hai', 0.0): 11, ('yhm', 0.0): 1, ('edsa', 0.0): 2, ('jasmingarrick', 0.0): 2, ('kikmeguy', 0.0): 5, ('webcam', 0.0): 2, ('milf', 0.0): 1, ('nakamaforev', 0.0): 3, ('kiksex', 0.0): 7, (\"unicef'\", 0.0): 1, ('fu', 0.0): 1, ('alon', 0.0): 16, ('manag', 0.0): 13, ('stephen', 0.0): 1, ('street', 0.0): 2, ('35', 0.0): 1, ('min', 0.0): 7, ('appear', 0.0): 2, ('record', 0.0): 6, ('coz', 0.0): 4, ('frustrat', 0.0): 6, ('sent', 0.0): 9, ('interest', 0.0): 9, ('woza', 0.0): 1, ('promis', 0.0): 4, ('senight', 0.0): 1, ('468', 0.0): 1, ('kikmeboy', 0.0): 9, ('gay', 0.0): 6, ('teen', 0.0): 7, ('amateur', 0.0): 5, ('hotscratch', 0.0): 1, ('sell', 0.0): 8, ('sock', 0.0): 6, ('150-160', 0.0): 1, ('peso', 0.0): 1, ('gotta', 0.0): 8, ('pay', 0.0): 8, ('degrassi', 0.0): 1, ('4-6', 0.0): 1, ('bcz', 0.0): 1, ('kat', 0.0): 3, ('chem', 0.0): 2, ('onscreen', 0.0): 1, ('ofscreen', 0.0): 1, ('kinda', 0.0): 10, ('pak', 0.0): 4, ('class', 0.0): 10, ('monthli', 0.0): 1, ('roll', 0.0): 4, ('band', 0.0): 2, ('throw', 0.0): 2, ('ironi', 0.0): 2, ('rhisfor', 0.0): 1, ('500', 0.0): 2, ('bestoftheday', 0.0): 3, ('chat', 0.0): 9, ('camsex', 0.0): 5, ('unfollow', 0.0): 11, ('particular', 0.0): 1, ('support', 0.0): 26, ('bae', 0.0): 11, ('poopi', 0.0): 1, ('pip', 0.0): 1, ('post', 0.0): 12, ('felt', 0.0): 6, ('uff', 0.0): 1, ('1.300', 0.0): 1, ('credit', 0.0): 3, ('glue', 0.0): 1, ('factori', 0.0): 1, ('kuchar', 0.0): 1, ('fast', 0.0): 7, ('graduat', 0.0): 3, ('up', 0.0): 2, ('definit', 0.0): 3, ('uni', 0.0): 2, ('ee', 0.0): 1, ('tommi', 0.0): 1, ('georgia', 0.0): 2, ('bout', 0.0): 2, ('instant', 0.0): 1, ('transmiss', 0.0): 1, ('malik', 0.0): 1, ('orang', 0.0): 2, ('suma', 0.0): 1, ('shouldeeerr', 0.0): 1, ('outfit', 0.0): 5, ('age', 0.0): 8, ('repack', 0.0): 3, ('group', 0.0): 4, ('charl', 0.0): 1, ('grown', 0.0): 2, ('rememb', 0.0): 17, ('dy', 0.0): 1, ('rihanna', 0.0): 1, ('red', 0.0): 4, ('ging', 0.0): 2, ('boot', 0.0): 4, ('closest', 0.0): 3, ('nike', 0.0): 1, ('adida', 0.0): 1, ('inform', 0.0): 4, ('pro@illamasqua.com', 0.0): 1, ('set', 0.0): 13, ('ifeely', 0.0): 1, ('harder', 0.0): 2, ('usual', 0.0): 7, ('ratbaglat', 0.0): 1, ('second', 0.0): 5, ('semest', 0.0): 2, ('gin', 0.0): 1, ('gut', 0.0): 12, ('reynold', 0.0): 1, ('dessert', 0.0): 2, ('season', 0.0): 9, ('villag', 0.0): 1, ('differ', 0.0): 10, ('citi', 0.0): 11, ('unit', 0.0): 3, ('oppress', 0.0): 1, ('mass', 0.0): 2, ('wat', 0.0): 5, ('afghanistn', 0.0): 1, ('war', 0.0): 2, ('tore', 0.0): 1, ('sunggyu', 0.0): 5, ('injur', 0.0): 7, ('plaster', 0.0): 2, ('rtd', 0.0): 1, ('loui', 0.0): 4, ('harri', 0.0): 10, ('5so', 0.0): 7, ('crowd', 0.0): 1, ('stadium', 0.0): 4, ('welder', 0.0): 1, ('ghost', 0.0): 1, ('hogo', 0.0): 1, ('vishaya', 0.0): 1, ('adu', 0.0): 1, ('bjp', 0.0): 1, ('madatt', 0.0): 1, ('anta', 0.0): 1, ('vishwa', 0.0): 1, ('ne', 0.0): 3, ('illa', 0.0): 1, ('wua', 0.0): 1, ('picki', 0.0): 1, ('finger', 0.0): 8, ('favourit', 0.0): 9, ('mutual', 0.0): 2, ('gn', 0.0): 1, ('along', 0.0): 3, ('ass', 0.0): 9, ('thent', 0.0): 1, ('423', 0.0): 1, ('sabadodeganarseguidor', 0.0): 2, ('sexual', 0.0): 4, ('sync', 0.0): 2, ('plug.dj', 0.0): 1, ('peel', 0.0): 1, ('suspems', 0.0): 1, ('cope', 0.0): 3, ('offroad', 0.0): 1, ('adventur', 0.0): 1, ('there', 0.0): 5, ('harvest', 0.0): 1, ('machineri', 0.0): 1, ('inapropri', 0.0): 1, ('weav', 0.0): 2, ('nowher', 0.0): 3, ('decent', 0.0): 2, ('invest', 0.0): 2, ('scottish', 0.0): 1, ('footbal', 0.0): 3, ('dire', 0.0): 2, ('nomoney', 0.0): 1, ('nawf', 0.0): 1, ('sum', 0.0): 2, ('becho', 0.0): 1, ('danni', 0.0): 3, ('eng', 0.0): 2, (\"let'\", 0.0): 5, ('overli', 0.0): 2, ('lab', 0.0): 1, ('ty', 0.0): 3, ('zap', 0.0): 1, ('distress', 0.0): 1, ('shot', 0.0): 6, ('cinema', 0.0): 4, ('louisianashoot', 0.0): 1, ('laugh', 0.0): 7, ('har', 0.0): 3, (\"how'\", 0.0): 5, ('chum', 0.0): 1, ('ncc', 0.0): 1, ('ph', 0.0): 2, ('balik', 0.0): 1, ('naman', 0.0): 1, ('kayo', 0.0): 1, ('itong', 0.0): 1, ('shirt', 0.0): 3, ('thaaat', 0.0): 1, ('ctto', 0.0): 1, ('expir', 0.0): 3, ('bi', 0.0): 2, ('tough', 0.0): 2, ('11', 0.0): 4, ('3:33', 0.0): 2, ('jfc', 0.0): 1, ('bio', 0.0): 3, ('bodo', 0.0): 1, ('amat', 0.0): 1, ('quick', 0.0): 5, ('yelaaa', 0.0): 1, ('dublin', 0.0): 2, ('potter', 0.0): 1, ('marathon', 0.0): 3, ('balanc', 0.0): 2, ('warm', 0.0): 5, ('comic', 0.0): 5, ('pine', 0.0): 1, ('keybind', 0.0): 1, ('featur', 0.0): 4, ('wild', 0.0): 2, ('warfar', 0.0): 1, ('control', 0.0): 2, ('diagnos', 0.0): 1, ('wiv', 0.0): 1, (\"scheuermann'\", 0.0): 1, ('diseas', 0.0): 3, ('bone', 0.0): 1, ('rlyhurt', 0.0): 1, ('howdo', 0.0): 1, ('georgesampson', 0.0): 1, ('stand', 0.0): 6, ('signal', 0.0): 3, ('reckon', 0.0): 1, ('t20', 0.0): 1, ('action', 0.0): 2, ('taunton', 0.0): 1, ('vacat', 0.0): 3, ('excit', 0.0): 6, ('justiceforsandrabland', 0.0): 2, ('sandrabland', 0.0): 6, ('disturb', 0.0): 1, ('women', 0.0): 5, ('happpi', 0.0): 1, ('justinbieb', 0.0): 4, ('daianerufato', 0.0): 3, ('ilysm', 0.0): 3, ('2015', 0.0): 12, ('07:34', 0.0): 1, ('delphi', 0.0): 2, ('weak', 0.0): 2, ('dom', 0.0): 2, ('techniqu', 0.0): 1, ('minc', 0.0): 2, ('complet', 0.0): 9, ('symphoni', 0.0): 1, ('joe', 0.0): 3, ('co', 0.0): 6, ('wth', 0.0): 2, ('aisyhhh', 0.0): 1, ('bald', 0.0): 1, ('14', 0.0): 3, ('seungchan', 0.0): 1, ('aigooo', 0.0): 1, ('riri', 0.0): 1, ('origin', 0.0): 6, ('depend', 0.0): 2, ('vet', 0.0): 1, ('major', 0.0): 2, ('va', 0.0): 1, ('kept', 0.0): 2, ('lumin', 0.0): 1, ('follback', 0.0): 2, ('treat', 0.0): 5, ('v', 0.0): 6, ('product', 0.0): 4, ('letter', 0.0): 1, ('z', 0.0): 5, ('uniqu', 0.0): 2, ('refresh', 0.0): 1, ('popular', 0.0): 1, ('bebee', 0.0): 2, ('lt', 0.0): 1, ('inaccuraci', 0.0): 1, ('inaccur', 0.0): 1, ('worri', 0.0): 8, ('burn', 0.0): 4, ('rn', 0.0): 17, ('tragic', 0.0): 1, ('joy', 0.0): 2, ('sam', 0.0): 4, ('rush', 0.0): 2, ('toronto', 0.0): 1, ('stuart', 0.0): 1, (\"party'\", 0.0): 2, ('iyalaya', 0.0): 1, ('shade', 0.0): 3, ('round', 0.0): 3, ('clock', 0.0): 2, (';(', 0.0): 6, ('happier', 0.0): 1, ('h', 0.0): 8, ('ubusi', 0.0): 1, ('le', 0.0): 3, ('fifa', 0.0): 1, ('gymnast', 0.0): 1, ('aahhh', 0.0): 1, ('noggin', 0.0): 1, ('bump', 0.0): 1, ('feelslikeanidiot', 0.0): 1, ('pregnant', 0.0): 2, ('woman', 0.0): 5, ('dearli', 0.0): 1, ('sunshin', 0.0): 4, ('suk', 0.0): 2, ('pumpkin', 0.0): 1, ('scone', 0.0): 1, ('outnumb', 0.0): 1, ('vidcon', 0.0): 10, ('eri', 0.0): 1, ('geez', 0.0): 1, ('preciou', 0.0): 4, ('hive', 0.0): 1, ('vote', 0.0): 7, ('vietnam', 0.0): 1, ('decemb', 0.0): 2, ('dunt', 0.0): 1, ('ikr', 0.0): 3, ('sob', 0.0): 3, ('buff', 0.0): 1, ('leg', 0.0): 4, ('toni', 0.0): 1, ('deactiv', 0.0): 6, ('bra', 0.0): 2, (\"shady'\", 0.0): 1, ('isibaya', 0.0): 1, ('special', 0.0): 3, ('❤', 0.0): 21, ('️', 0.0): 19, ('😓', 0.0): 2, ('slept', 0.0): 5, ('colder', 0.0): 1, ('took', 0.0): 9, ('med', 0.0): 1, ('sausag', 0.0): 1, ('adio', 0.0): 1, ('cold', 0.0): 15, ('sore', 0.0): 9, ('ew', 0.0): 3, ('h8', 0.0): 1, ('messeng', 0.0): 2, ('shittier', 0.0): 1, ('leno', 0.0): 1, ('ident', 0.0): 1, ('crisi', 0.0): 2, ('roommat', 0.0): 1, ('knock', 0.0): 3, ('nighter', 0.0): 3, ('bird', 0.0): 2, ('flew', 0.0): 2, ('thru', 0.0): 2, ('derek', 0.0): 3, ('tour', 0.0): 7, ('wetherspoon', 0.0): 1, ('pub', 0.0): 1, ('polic', 0.0): 4, ('frank', 0.0): 2, ('ocean', 0.0): 4, ('releas', 0.0): 8, ('ff', 0.0): 4, ('lisah', 0.0): 2, ('kikm', 0.0): 8, ('eboni', 0.0): 2, ('weloveyounamjoon', 0.0): 1, ('gave', 0.0): 8, ('dress', 0.0): 6, ('polka', 0.0): 1, ('dot', 0.0): 2, ('ndi', 0.0): 1, ('yum', 0.0): 1, ('feed', 0.0): 3, ('leftov', 0.0): 2, ('side', 0.0): 6, ('cs', 0.0): 2, ('own', 0.0): 1, ('walnut', 0.0): 1, ('whip', 0.0): 1, ('wife', 0.0): 6, ('boah', 0.0): 1, ('madi', 0.0): 2, ('def', 0.0): 3, ('manga', 0.0): 1, ('giant', 0.0): 3, ('aminormalyet', 0.0): 1, ('cooki', 0.0): 2, ('breakfast', 0.0): 5, ('clutch', 0.0): 1, ('poorli', 0.0): 6, ('tummi', 0.0): 6, ('pj', 0.0): 1, ('groan', 0.0): 1, ('nou', 0.0): 1, ('adam', 0.0): 2, ('ken', 0.0): 1, ('sara', 0.0): 2, ('sister', 0.0): 4, ('accid', 0.0): 2, ('sort', 0.0): 7, ('mate', 0.0): 2, ('pick', 0.0): 12, ('rang', 0.0): 4, ('fk', 0.0): 2, ('freak', 0.0): 5, ('describ', 0.0): 1, ('eric', 0.0): 2, ('prydz', 0.0): 1, ('sister-in-law', 0.0): 1, ('instal', 0.0): 2, ('seat', 0.0): 4, ('bought', 0.0): 6, ('rear-end', 0.0): 1, (\"everyone'\", 0.0): 4, ('trash', 0.0): 2, ('boob', 0.0): 3, ('whilst', 0.0): 3, ('stair', 0.0): 1, ('childhood', 0.0): 1, ('toothsensit', 0.0): 4, ('size', 0.0): 9, ('ke', 0.0): 3, ('shem', 0.0): 2, ('trust', 0.0): 2, ('awel', 0.0): 1, ('drunk', 0.0): 2, ('weekendofmad', 0.0): 1, ('🍹', 0.0): 3, ('🍸', 0.0): 1, ('cb', 0.0): 1, ('dancer', 0.0): 1, ('choregraph', 0.0): 1, ('626-430-8715', 0.0): 1, ('messag', 0.0): 8, ('repli', 0.0): 14, ('hoe', 0.0): 1, ('xd', 0.0): 7, ('xiu', 0.0): 1, ('nk', 0.0): 1, ('gi', 0.0): 2, ('uss', 0.0): 1, ('eliss', 0.0): 1, ('ksoo', 0.0): 2, ('session', 0.0): 5, ('tat', 0.0): 1, ('bcoz', 0.0): 1, ('bet', 0.0): 10, ('rancho', 0.0): 1, ('imperi', 0.0): 1, ('de', 0.0): 1, ('silang', 0.0): 1, ('subdivis', 0.0): 1, ('center', 0.0): 1, ('39', 0.0): 1, ('cornwal', 0.0): 1, ('verit', 0.0): 1, ('prize', 0.0): 2, ('regular', 0.0): 3, ('workout', 0.0): 1, ('spin', 0.0): 1, ('base', 0.0): 1, ('upon', 0.0): 1, ('penni', 0.0): 1, ('ebook', 0.0): 1, ('фотосет', 0.0): 1, ('addicted-to-analsex', 0.0): 1, ('sweetbj', 0.0): 2, ('blowjob', 0.0): 1, ('mhhh', 0.0): 1, ('sed', 0.0): 1, ('sg', 0.0): 1, ('dinner', 0.0): 4, ('bless', 0.0): 2, ('mee', 0.0): 2, ('enviou', 0.0): 1, ('eonni', 0.0): 1, ('lovey', 0.0): 1, ('dovey', 0.0): 1, ('dongsaeng', 0.0): 1, ('workin', 0.0): 1, ('tuesday', 0.0): 4, ('schade', 0.0): 3, ('belfast', 0.0): 1, ('jealou', 0.0): 9, ('jacob', 0.0): 5, ('isco', 0.0): 4, ('peni', 0.0): 1, ('everi', 0.0): 16, ('convers', 0.0): 6, ('wonder', 0.0): 11, ('soul', 0.0): 5, ('nation', 0.0): 2, ('louisiana', 0.0): 4, ('lafayett', 0.0): 2, ('matteroftheheart', 0.0): 1, ('waduh', 0.0): 1, ('pant', 0.0): 3, ('suspend', 0.0): 2, ('believ', 0.0): 14, ('teenag', 0.0): 2, ('clich', 0.0): 1, ('youuu', 0.0): 5, ('rma', 0.0): 1, ('jersey', 0.0): 2, ('fake', 0.0): 4, ('jaclintil', 0.0): 1, ('model', 0.0): 9, ('likeforlik', 0.0): 7, ('mpoint', 0.0): 4, ('hotfmnoaidilforariana', 0.0): 2, ('ran', 0.0): 5, ('fuckkk', 0.0): 1, ('jump', 0.0): 3, ('justin', 0.0): 3, ('finish', 0.0): 14, ('sanum', 0.0): 1, ('llaollao', 0.0): 1, ('foood', 0.0): 1, ('ubericecream', 0.0): 14, ('glare', 0.0): 1, ('vine', 0.0): 3, ('tweetin', 0.0): 1, ('mood', 0.0): 3, ('elbow', 0.0): 1, ('choreo', 0.0): 1, ('offens', 0.0): 2, ('yeyi', 0.0): 1, ('hd', 0.0): 2, ('brow', 0.0): 1, ('kit', 0.0): 6, ('slightli', 0.0): 2, ('monday', 0.0): 10, ('sux', 0.0): 1, ('enjoy', 0.0): 9, ('nothaveld', 0.0): 1, ('765', 0.0): 1, ('edm', 0.0): 1, ('likeforfollow', 0.0): 3, ('hannib', 0.0): 3, ('mosquito', 0.0): 2, ('bite', 0.0): 5, ('kinki', 0.0): 1, ('hsould', 0.0): 1, ('justget', 0.0): 1, ('marri', 0.0): 2, ('la', 0.0): 11, ('shuffl', 0.0): 4, ('int', 0.0): 1, ('buckl', 0.0): 1, ('spring', 0.0): 1, ('millz', 0.0): 1, ('aski', 0.0): 2, ('awusasho', 0.0): 1, ('unlucki', 0.0): 2, ('driver', 0.0): 7, ('briefli', 0.0): 1, ('spot', 0.0): 4, ('144p', 0.0): 1, ('brook', 0.0): 1, ('crack', 0.0): 2, ('＠', 0.0): 5, ('maverickgam', 0.0): 4, ('07:32', 0.0): 1, ('07:25', 0.0): 1, ('max', 0.0): 3, ('file', 0.0): 2, ('extern', 0.0): 2, ('sd', 0.0): 1, ('via', 0.0): 1, ('airdroid', 0.0): 1, ('android', 0.0): 2, ('4.4+', 0.0): 1, ('googl', 0.0): 5, ('alright', 0.0): 3, ('cramp', 0.0): 2, ('&lt;/3', 0.0): 6, ('unstan', 0.0): 1, ('tay', 0.0): 2, ('ngeze', 0.0): 1, ('cocktaili', 0.0): 1, ('classi', 0.0): 1, ('07:24', 0.0): 1, ('✈', 0.0): 2, ('️2', 0.0): 1, ('raini', 0.0): 2, ('☔', 0.0): 2, ('peter', 0.0): 1, ('pen', 0.0): 1, ('spare', 0.0): 1, ('guest', 0.0): 2, ('barcelona', 0.0): 2, ('bilbao', 0.0): 1, ('booti', 0.0): 2, ('sharyl', 0.0): 1, ('shane', 0.0): 2, ('ta', 0.0): 1, ('giddi', 0.0): 1, ('d1', 0.0): 1, ('zipper', 0.0): 1, ('beyond', 0.0): 1, ('repair', 0.0): 4, ('iphon', 0.0): 5, ('upgrad', 0.0): 1, ('april', 0.0): 1, ('2016', 0.0): 1, ('cont', 0.0): 2, ('england', 0.0): 4, ('wore', 0.0): 2, ('greet', 0.0): 5, ('tempt', 0.0): 2, ('whole', 0.0): 16, ('pack', 0.0): 6, ('oreo', 0.0): 2, ('strength', 0.0): 1, ('wifi', 0.0): 5, ('network', 0.0): 4, ('within', 0.0): 3, ('lolipop', 0.0): 1, ('kebab', 0.0): 1, ('klappertart', 0.0): 1, ('cake', 0.0): 10, ('moodbost', 0.0): 2, ('shoot', 0.0): 6, ('unprepar', 0.0): 1, ('sri', 0.0): 1, ('dresscod', 0.0): 1, ('door', 0.0): 6, ('iam', 0.0): 2, ('dnt', 0.0): 1, ('stab', 0.0): 3, ('meh', 0.0): 3, ('wrocilam', 0.0): 1, ('otp', 0.0): 3, ('5', 0.0): 14, ('looww', 0.0): 1, ('recov', 0.0): 2, ('wayn', 0.0): 2, ('insur', 0.0): 3, ('loss', 0.0): 3, ('stolen', 0.0): 2, ('accident', 0.0): 1, ('damag', 0.0): 5, ('devic', 0.0): 3, ('warranti', 0.0): 1, ('centr', 0.0): 2, ('👌', 0.0): 1, ('lmfaoo', 0.0): 1, ('accur', 0.0): 2, ('fra', 0.0): 4, ('aliv', 0.0): 2, ('steel', 0.0): 2, ('otamendi', 0.0): 1, ('ny', 0.0): 2, ('🚖', 0.0): 1, ('🗽', 0.0): 1, ('🌃', 0.0): 1, ('stealth', 0.0): 2, ('bastard', 0.0): 2, ('inc', 0.0): 3, ('steam', 0.0): 2, ('therapi', 0.0): 1, ('exhaust', 0.0): 3, ('lie', 0.0): 7, ('total', 0.0): 11, ('block', 0.0): 11, ('choic', 0.0): 5, ('switzerland', 0.0): 1, ('kfc', 0.0): 1, ('common', 0.0): 4, ('th', 0.0): 5, ('wolrd', 0.0): 1, ('fyn', 0.0): 1, ('drop', 0.0): 10, ('state', 0.0): 4, ('3g', 0.0): 2, ('christ', 0.0): 1, ('scale', 0.0): 1, ('deck', 0.0): 1, ('chair', 0.0): 4, ('yk', 0.0): 1, ('resi', 0.0): 1, ('memori', 0.0): 5, ('nude', 0.0): 4, ('bruh', 0.0): 3, ('prepar', 0.0): 3, ('lock', 0.0): 2, ('view', 0.0): 7, ('fbc', 0.0): 3, ('mork', 0.0): 1, ('873', 0.0): 1, ('kikgirl', 0.0): 13, ('premiostumundo', 0.0): 2, ('hotspotwithdanri', 0.0): 1, ('hospit', 0.0): 3, ('food', 0.0): 18, ('sone', 0.0): 1, ('produc', 0.0): 1, ('potag', 0.0): 1, ('tomato', 0.0): 1, ('blight', 0.0): 1, ('sheffield', 0.0): 1, ('mych', 0.0): 1, ('shiiit', 0.0): 2, ('screenshot', 0.0): 4, ('prompt', 0.0): 1, ('areadi', 0.0): 1, ('similar', 0.0): 4, ('soulmat', 0.0): 1, ('canon', 0.0): 1, ('zzz', 0.0): 2, ('britain', 0.0): 1, ('😁', 0.0): 3, ('mana', 0.0): 2, ('hw', 0.0): 1, ('jouch', 0.0): 1, ('por', 0.0): 1, ('que', 0.0): 1, ('liceooo', 0.0): 1, ('30', 0.0): 3, ('minut', 0.0): 6, ('pass', 0.0): 13, ('ayala', 0.0): 1, ('tunnel', 0.0): 2, ('thatscold', 0.0): 1, ('80', 0.0): 1, ('snap', 0.0): 3, ('lourd', 0.0): 1, ('bang', 0.0): 3, ('anywher', 0.0): 4, ('water', 0.0): 8, ('road', 0.0): 1, ('showbox', 0.0): 1, ('naruto', 0.0): 1, ('cartoon', 0.0): 1, ('companion', 0.0): 2, ('skinni', 0.0): 3, ('fat', 0.0): 4, ('bare', 0.0): 6, ('dubai', 0.0): 3, ('calum', 0.0): 1, ('ashton', 0.0): 1, ('✧', 0.0): 8, ('｡', 0.0): 8, ('chelni', 0.0): 4, ('disappoint', 0.0): 13, ('everybodi', 0.0): 5, ('due', 0.0): 14, ('laribuggi', 0.0): 1, ('medic', 0.0): 1, ('nutella', 0.0): 1, (\"could'v\", 0.0): 3, ('siriu', 0.0): 1, ('goat', 0.0): 4, ('frudg', 0.0): 1, ('mike', 0.0): 1, ('cloth', 0.0): 6, ('stuff', 0.0): 11, ('sat', 0.0): 3, ('number', 0.0): 6, ('ring', 0.0): 1, ('bbz', 0.0): 1, ('angek', 0.0): 1, ('sbali', 0.0): 1, ('euuuwww', 0.0): 2, ('lunch', 0.0): 10, ('construct', 0.0): 3, ('worker', 0.0): 3, ('1k', 0.0): 3, ('style', 0.0): 4, ('nell', 0.0): 1, ('ik', 0.0): 2, ('death', 0.0): 3, ('jaysu', 0.0): 1, ('toast', 0.0): 1, ('insecur', 0.0): 2, ('buti', 0.0): 1, ('ure', 0.0): 2, ('poop', 0.0): 1, ('gorgeou', 0.0): 2, ('angel', 0.0): 2, ('rome', 0.0): 1, ('throat', 0.0): 10, ('llama', 0.0): 1, ('urself', 0.0): 2, ('getwellsoonamb', 0.0): 1, ('heath', 0.0): 2, ('ledger', 0.0): 1, ('appl', 0.0): 3, ('permiss', 0.0): 2, ('2-0', 0.0): 1, ('lead', 0.0): 3, ('supersport', 0.0): 1, ('milkshak', 0.0): 1, ('witcher', 0.0): 1, ('papertown', 0.0): 1, ('bale', 0.0): 1, ('9', 0.0): 5, ('méxico', 0.0): 1, ('bahay', 0.0): 1, ('bahayan', 0.0): 1, ('magisa', 0.0): 1, ('sadlyf', 0.0): 1, ('bunso', 0.0): 1, ('sleeep', 0.0): 4, ('astonvilla', 0.0): 1, ('berigaud', 0.0): 1, ('bakar', 0.0): 1, ('club', 0.0): 4, ('dear', 0.0): 11, ('allerg', 0.0): 4, ('depress', 0.0): 5, (\"blaine'\", 0.0): 1, ('acoust', 0.0): 2, ('version', 0.0): 5, ('excus', 0.0): 3, ('hernia', 0.0): 3, ('toxin', 0.0): 1, ('freedom', 0.0): 1, ('organ', 0.0): 2, ('ariel', 0.0): 1, ('slap', 0.0): 1, ('slam', 0.0): 1, ('bee', 0.0): 1, ('unknown', 0.0): 2, ('finddjderek', 0.0): 1, ('smell', 0.0): 3, ('uuughhh', 0.0): 1, ('grabe', 0.0): 5, ('ka', 0.0): 5, ('where', 0.0): 1, ('gf', 0.0): 3, ('james_yammouni', 0.0): 1, ('smi', 0.0): 1, ('nemesi', 0.0): 1, ('rule', 0.0): 1, ('doesnt', 0.0): 2, ('appeal', 0.0): 1, ('neeein', 0.0): 1, ('saaad', 0.0): 3, ('less', 0.0): 3, ('hang', 0.0): 7, ('creas', 0.0): 1, ('tan', 0.0): 3, ('dalla', 0.0): 4, ('suppos', 0.0): 7, ('infront', 0.0): 2, ('beato', 0.0): 1, ('tim', 0.0): 2, ('prob', 0.0): 5, ('minha', 0.0): 1, ('deleici', 0.0): 1, ('hr', 0.0): 2, ('pcb', 0.0): 1, ('ep', 0.0): 5, ('peregrin', 0.0): 1, ('8.40', 0.0): 1, ('pigeon', 0.0): 1, ('feet', 0.0): 3, ('tram', 0.0): 1, ('hav', 0.0): 2, ('spent', 0.0): 5, ('outsid', 0.0): 9, ('apt', 0.0): 1, ('build', 0.0): 3, ('key', 0.0): 3, ('bldg', 0.0): 1, ('wrote', 0.0): 3, ('dark', 0.0): 5, ('swan', 0.0): 1, ('fifth', 0.0): 2, ('mmmm', 0.0): 1, ('avi', 0.0): 4, ('nicki', 0.0): 1, ('fucjikg', 0.0): 1, ('disgust', 0.0): 6, ('buynotanapologyonitun', 0.0): 1, ('aval', 0.0): 1, ('denmark', 0.0): 1, ('nw', 0.0): 2, ('sch', 0.0): 2, ('share', 0.0): 11, ('jeslyn', 0.0): 1, ('72', 0.0): 4, ('root', 0.0): 2, ('kuch', 0.0): 1, ('nahi', 0.0): 1, ('hua', 0.0): 2, ('newbi', 0.0): 1, ('crap', 0.0): 3, ('miracl', 0.0): 1, ('4th', 0.0): 1, ('linda', 0.0): 1, ('click', 0.0): 1, ('pin', 0.0): 2, ('wing', 0.0): 3, ('epic', 0.0): 2, ('page', 0.0): 6, ('ang', 0.0): 8, ('ganda', 0.0): 1, ('💗', 0.0): 4, ('nux', 0.0): 1, ('hinanap', 0.0): 1, ('ako', 0.0): 1, ('uy', 0.0): 1, ('sched', 0.0): 1, ('anyar', 0.0): 1, ('entertain', 0.0): 2, ('typa', 0.0): 3, ('buddi', 0.0): 2, ('transpar', 0.0): 1, ('photoshop', 0.0): 2, ('planner', 0.0): 1, ('helppp', 0.0): 2, ('wearig', 0.0): 1, ('dri', 0.0): 2, ('alot', 0.0): 3, ('bu', 0.0): 5, ('prey', 0.0): 1, ('gross', 0.0): 5, ('drain', 0.0): 3, ('ausfailia', 0.0): 1, ('snow', 0.0): 3, ('footi', 0.0): 3, ('2nd', 0.0): 5, ('row', 0.0): 3, (\"m'\", 0.0): 2, ('kitkat', 0.0): 2, ('bday', 0.0): 7, ('😢', 0.0): 8, ('suger', 0.0): 1, ('olivia', 0.0): 2, ('audit', 0.0): 1, ('american', 0.0): 1, ('idol', 0.0): 2, ('injuri', 0.0): 2, ('appendix', 0.0): 1, ('burst', 0.0): 2, ('append', 0.0): 1, ('yeahh', 0.0): 2, ('fack', 0.0): 2, ('nhl', 0.0): 1, ('khami', 0.0): 2, ('favorit', 0.0): 4, ('rise', 0.0): 3, ('reaali', 0.0): 1, ('ja', 0.0): 2, ('naomi', 0.0): 1, ('modern', 0.0): 1, ('contemporari', 0.0): 1, ('slack', 0.0): 1, ('565', 0.0): 1, ('blond', 0.0): 2, ('jahat', 0.0): 3, ('discount', 0.0): 1, ('thorp', 0.0): 2, ('park', 0.0): 7, ('esnho', 0.0): 1, ('node', 0.0): 1, ('advanc', 0.0): 4, ('directx', 0.0): 1, ('workshop', 0.0): 1, ('p2', 0.0): 1, ('upload', 0.0): 2, ('remov', 0.0): 5, ('blackberri', 0.0): 1, ('shitti', 0.0): 1, ('mobil', 0.0): 2, ('povertyyouareevil', 0.0): 1, ('struggl', 0.0): 4, ('math', 0.0): 1, ('emm', 0.0): 1, ('data', 0.0): 6, ('elgin', 0.0): 1, ('vava', 0.0): 1, ('makati', 0.0): 1, ('💛', 0.0): 4, ('baon', 0.0): 1, ('soup', 0.0): 3, ('soak', 0.0): 1, ('bread', 0.0): 2, ('mush', 0.0): 1, (\"they'd\", 0.0): 2, ('matt', 0.0): 2, ('ouat', 0.0): 1, ('beach', 0.0): 5, ('blinkin', 0.0): 1, ('unblock', 0.0): 1, ('headack', 0.0): 1, ('tension', 0.0): 1, ('erit', 0.0): 1, ('perspect', 0.0): 1, ('wed', 0.0): 4, ('playlist', 0.0): 2, ('endlessli', 0.0): 1, ('blush', 0.0): 1, ('bat', 0.0): 1, ('kiddo', 0.0): 1, ('rumbel', 0.0): 1, ('overwhelm', 0.0): 1, ('thrown', 0.0): 2, ('irrespons', 0.0): 1, ('pakighinabi', 0.0): 1, ('pinkfinit', 0.0): 1, ('beb', 0.0): 2, ('migrain', 0.0): 2, ('almost', 0.0): 11, ('coyot', 0.0): 1, ('outta', 0.0): 1, ('mad', 0.0): 11, ('😒', 0.0): 3, ('headach', 0.0): 9, ('인피니트', 0.0): 2, ('save', 0.0): 6, ('baechu', 0.0): 1, ('calibraskaep', 0.0): 3, ('r', 0.0): 19, ('fanci', 0.0): 2, ('yt', 0.0): 3, ('purchas', 0.0): 2, ('elgato', 0.0): 1, ('ant', 0.0): 2, ('unexpect', 0.0): 2, ('bestfriend', 0.0): 9, ('faint', 0.0): 1, ('bp', 0.0): 1, ('appar', 0.0): 5, ('shower', 0.0): 3, ('subway', 0.0): 1, ('cool', 0.0): 5, ('prayer', 0.0): 2, ('fragil', 0.0): 1, ('huge', 0.0): 3, ('gap', 0.0): 1, ('plot', 0.0): 2, ('bungi', 0.0): 1, ('folk', 0.0): 1, ('raspberri', 0.0): 1, ('pi', 0.0): 1, ('shoe', 0.0): 2, ('woohyun', 0.0): 2, ('guilti', 0.0): 1, ('monica', 0.0): 2, ('davao', 0.0): 1, ('luckyyi', 0.0): 1, ('confid', 0.0): 1, ('eunha', 0.0): 1, ('misplac', 0.0): 1, ('den', 0.0): 1, ('dae', 0.0): 1, ('bap', 0.0): 1, ('likewis', 0.0): 1, ('liam', 0.0): 1, ('dylan', 0.0): 3, ('huehu', 0.0): 1, ('rice', 0.0): 1, ('krispi', 0.0): 1, ('marshmallow', 0.0): 2, ('srsli', 0.0): 7, ('birmingham', 0.0): 1, ('m5m6junction', 0.0): 1, ('soulsurvivor', 0.0): 1, ('stafford', 0.0): 1, ('progress', 0.0): 1, ('mixtur', 0.0): 1, (\"they'v\", 0.0): 4, ('practic', 0.0): 1, ('lage', 0.0): 1, ('ramd', 0.0): 1, ('lesbian', 0.0): 3, ('oralsex', 0.0): 4, ('munchkin', 0.0): 1, ('juja', 0.0): 1, ('murugan', 0.0): 1, ('handl', 0.0): 3, ('dia', 0.0): 2, ('bgtau', 0.0): 1, ('harap', 0.0): 1, ('bagi', 0.0): 1, ('aminn', 0.0): 1, ('fraand', 0.0): 1, ('😬', 0.0): 2, ('bigbang', 0.0): 2, ('steak', 0.0): 1, ('younger', 0.0): 2, ('sian', 0.0): 2, ('pizza', 0.0): 7, ('5am', 0.0): 5, ('nicoleapag', 0.0): 1, ('makeup', 0.0): 4, ('hellish', 0.0): 1, ('thirstyyi', 0.0): 1, ('chesti', 0.0): 1, ('dad', 0.0): 9, (\"nando'\", 0.0): 1, ('22', 0.0): 3, ('bow', 0.0): 2, ('queen', 0.0): 3, ('brave', 0.0): 1, ('hen', 0.0): 1, ('leed', 0.0): 9, ('rdd', 0.0): 1, ('dissip', 0.0): 1, ('. .', 0.0): 1, ('pump', 0.0): 2, ('capee', 0.0): 1, ('japan', 0.0): 2, ('random', 0.0): 1, ('young', 0.0): 5, ('outliv', 0.0): 1, ('x-ray', 0.0): 1, ('dental', 0.0): 1, ('spine', 0.0): 1, ('relief', 0.0): 1, ('popol', 0.0): 1, ('stomach', 0.0): 8, ('frog', 0.0): 2, ('brad', 0.0): 1, ('gen.ad', 0.0): 1, ('price', 0.0): 5, ('negoti', 0.0): 3, ('huhuhuhuhu', 0.0): 1, ('bbmadeinmanila', 0.0): 1, ('findavip', 0.0): 1, ('boyirl', 0.0): 1, ('yasss', 0.0): 1, ('6th', 0.0): 1, ('june', 0.0): 3, ('lain', 0.0): 1, ('diffici', 0.0): 1, ('custom', 0.0): 1, ('internet', 0.0): 9, ('near', 0.0): 9, ('speed', 0.0): 2, ('escap', 0.0): 1, ('rapist', 0.0): 1, ('commit', 0.0): 2, ('crime', 0.0): 1, ('bachpan', 0.0): 1, ('ki', 0.0): 2, ('yaadein', 0.0): 1, ('finnair', 0.0): 1, ('heathrow', 0.0): 1, ('norwegian', 0.0): 1, (':\\\\', 0.0): 1, ('batteri', 0.0): 3, ('upvot', 0.0): 4, ('keeno', 0.0): 1, ('whatthefuck', 0.0): 1, ('grotti', 0.0): 1, ('attent', 0.0): 1, ('seeker', 0.0): 1, ('moral', 0.0): 1, ('fern', 0.0): 1, ('mimi', 0.0): 1, ('bali', 0.0): 1, ('she', 0.0): 4, ('pleasee', 0.0): 3, ('brb', 0.0): 1, ('lowbat', 0.0): 1, ('otwolgrandtrail', 0.0): 4, ('funk', 0.0): 1, ('wewanticecream', 0.0): 1, ('sweat', 0.0): 2, ('eugh', 0.0): 1, ('speak', 0.0): 4, ('occasion', 0.0): 1, (\"izzy'\", 0.0): 1, ('dorm', 0.0): 1, ('choppi', 0.0): 1, ('paul', 0.0): 1, ('switch', 0.0): 4, (\"infinite'\", 0.0): 2, ('5:30', 0.0): 2, ('cayton', 0.0): 1, ('bay', 0.0): 2, ('emma', 0.0): 2, ('jen', 0.0): 1, ('darcey', 0.0): 1, ('connor', 0.0): 1, ('spoke', 0.0): 1, ('nail', 0.0): 2, ('biggest', 0.0): 3, ('blue', 0.0): 5, ('bottl', 0.0): 3, ('roommateexperi', 0.0): 1, ('yup', 0.0): 4, ('avoid', 0.0): 2, ('ic', 0.0): 1, ('te', 0.0): 1, ('auto-followback', 0.0): 1, ('asian', 0.0): 2, ('puppi', 0.0): 3, ('ljp', 0.0): 1, ('1/5', 0.0): 1, ('nowday', 0.0): 1, ('attach', 0.0): 2, ('beat', 0.0): 2, ('numb', 0.0): 1, ('dentist', 0.0): 3, ('misss', 0.0): 2, ('muchhh', 0.0): 1, ('youtub', 0.0): 5, ('rid', 0.0): 3, ('tab', 0.0): 2, ('uca', 0.0): 1, ('onto', 0.0): 2, ('track', 0.0): 3, ('bigtim', 0.0): 1, ('rumor', 0.0): 3, ('warmest', 0.0): 1, ('chin', 0.0): 2, ('tickl', 0.0): 1, ('♫', 0.0): 1, ('zikra', 0.0): 1, ('lusi', 0.0): 1, ('hasya', 0.0): 1, ('nugget', 0.0): 3, ('som', 0.0): 1, ('lu', 0.0): 1, ('olymp', 0.0): 1, (\"millie'\", 0.0): 1, ('guinea', 0.0): 1, ('lewi', 0.0): 1, ('748292', 0.0): 1, (\"we'll\", 0.0): 8, ('ano', 0.0): 2, ('22stan', 0.0): 1, ('24/7', 0.0): 2, ('thankyou', 0.0): 2, ('kanina', 0.0): 2, ('breakdown', 0.0): 2, ('mag', 0.0): 2, ('hatee', 0.0): 1, ('leas', 0.0): 1, ('written', 0.0): 2, ('hurri', 0.0): 4, ('attempt', 0.0): 1, ('6g', 0.0): 1, ('unsuccess', 0.0): 1, ('earlob', 0.0): 1, ('sue', 0.0): 1, ('dreari', 0.0): 1, ('denis', 0.0): 1, ('muriel', 0.0): 1, ('ahouré', 0.0): 1, ('pr', 0.0): 1, ('brand', 0.0): 1, ('imag', 0.0): 4, ('opportun', 0.0): 1, ('po', 0.0): 1, ('beg', 0.0): 2, (\"kath'd\", 0.0): 1, ('respond', 0.0): 2, ('chop', 0.0): 1, ('wbu', 0.0): 1, ('yess', 0.0): 2, ('kme', 0.0): 1, ('tom', 0.0): 4, ('cram', 0.0): 1, ('–', 0.0): 1, ('curiou', 0.0): 1, ('on-board', 0.0): 1, ('announc', 0.0): 3, ('trespass', 0.0): 1, ('fr', 0.0): 3, ('clandestin', 0.0): 1, ('muller', 0.0): 1, ('obviou', 0.0): 1, ('mufc', 0.0): 1, ('colour', 0.0): 4, ('stu', 0.0): 2, ('movie', 0.0): 1, ('buddyyi', 0.0): 1, ('feelgoodfriday', 0.0): 1, ('forest', 0.0): 1, ('6:30', 0.0): 1, ('babysit', 0.0): 1, ('opix', 0.0): 1, ('805', 0.0): 1, ('pilllow', 0.0): 1, ('fool', 0.0): 1, ('brag', 0.0): 1, ('skrillah', 0.0): 1, ('drown', 0.0): 2, ('gue', 0.0): 1, ('report', 0.0): 4, ('eventu', 0.0): 1, ('north', 0.0): 1, ('west', 0.0): 2, ('kitti', 0.0): 1, ('sjkao', 0.0): 1, ('mm', 0.0): 2, ('srri', 0.0): 1, ('honma', 0.0): 1, ('yeh', 0.0): 1, ('walay', 0.0): 1, ('bhi', 0.0): 2, ('bohat', 0.0): 1, ('wailay', 0.0): 1, ('hain', 0.0): 2, ('pre-season', 0.0): 1, ('friendli', 0.0): 3, ('pe', 0.0): 3, ('itna', 0.0): 2, ('shor', 0.0): 1, ('machaya', 0.0): 1, ('mein', 0.0): 1, ('samjha', 0.0): 1, ('cup', 0.0): 3, ('note', 0.0): 2, ('😄', 0.0): 1, ('👍', 0.0): 1, ('😔', 0.0): 7, ('sirkay', 0.0): 1, ('wali', 0.0): 1, ('pyaaz', 0.0): 1, ('daal', 0.0): 2, ('onion', 0.0): 1, ('vinegar', 0.0): 1, ('cook', 0.0): 3, ('tutori', 0.0): 1, ('soho', 0.0): 1, ('wobbl', 0.0): 1, ('server', 0.0): 4, ('ciao', 0.0): 1, ('masaan', 0.0): 1, ('muv', 0.0): 1, ('beast', 0.0): 2, ('hayst', 0.0): 1, ('cr', 0.0): 1, ('hnnn', 0.0): 1, ('fluffi', 0.0): 2, ('comeback', 0.0): 3, ('korea', 0.0): 1, ('wow', 0.0): 10, ('act', 0.0): 4, ('optimis', 0.0): 1, ('soniii', 0.0): 1, ('kahaaa', 0.0): 1, ('shave', 0.0): 3, ('tryna', 0.0): 3, ('healthi', 0.0): 2, ('freez', 0.0): 3, ('fml', 0.0): 4, ('jacket', 0.0): 1, ('sleepi', 0.0): 4, ('cyber', 0.0): 1, ('bulli', 0.0): 2, ('racial', 0.0): 2, ('scari', 0.0): 6, ('hall', 0.0): 1, ('stockholm', 0.0): 1, ('loool', 0.0): 3, ('bunch', 0.0): 3, ('among', 0.0): 1, ('__', 0.0): 2, ('busier', 0.0): 1, ('onward', 0.0): 1, ('ol', 0.0): 2, ('coincid', 0.0): 1, ('imac', 0.0): 1, ('launch', 0.0): 2, ('gram', 0.0): 1, ('nearer', 0.0): 1, ('blain', 0.0): 2, ('darren', 0.0): 2, ('layout', 0.0): 3, ('fuuuck', 0.0): 2, ('jesu', 0.0): 1, ('gishwh', 0.0): 1, ('exclud', 0.0): 1, ('unless', 0.0): 4, ('c', 0.0): 7, ('angelica', 0.0): 1, ('pull', 0.0): 5, ('colleg', 0.0): 5, ('movement', 0.0): 1, ('frou', 0.0): 1, ('vaccin', 0.0): 1, ('armor', 0.0): 2, ('legendari', 0.0): 1, ('cash', 0.0): 2, ('effort', 0.0): 2, ('nat', 0.0): 2, ('brake', 0.0): 1, ('grumpi', 0.0): 4, ('wreck', 0.0): 1, ('decis', 0.0): 2, ('gahhh', 0.0): 1, ('teribl', 0.0): 1, ('kilig', 0.0): 1, ('togeth', 0.0): 7, ('weaker', 0.0): 1, ('shravan', 0.0): 1, ('tv', 0.0): 4, ('stooop', 0.0): 1, ('gi-guilti', 0.0): 1, ('akooo', 0.0): 1, ('imveryverysorri', 0.0): 1, ('cd', 0.0): 1, ('grey', 0.0): 3, ('basenam', 0.0): 1, ('path', 0.0): 1, ('theme', 0.0): 2, ('cigar', 0.0): 1, ('speaker', 0.0): 1, ('volum', 0.0): 1, ('promethazin', 0.0): 1, ('zopiclon', 0.0): 1, ('addit', 0.0): 1, ('quetiapin', 0.0): 1, ('modifi', 0.0): 1, ('prescript', 0.0): 1, ('greska', 0.0): 1, ('macedonian', 0.0): 1, ('slovak', 0.0): 1, ('hike', 0.0): 1, ('certainli', 0.0): 2, ('browser', 0.0): 2, ('os', 0.0): 1, ('zokay', 0.0): 1, ('accent', 0.0): 1, ('b-but', 0.0): 1, ('gintama', 0.0): 1, ('shinsengumi', 0.0): 1, ('chapter', 0.0): 1, ('andi', 0.0): 1, ('crappl', 0.0): 1, ('agre', 0.0): 5, ('ftw', 0.0): 2, ('phandroid', 0.0): 1, ('tline', 0.0): 1, ('orchestra', 0.0): 1, ('ppl', 0.0): 5, ('rehears', 0.0): 1, ('bittersweet', 0.0): 1, ('eunji', 0.0): 1, ('bakit', 0.0): 4, ('121st', 0.0): 1, (\"yesterday'\", 0.0): 1, ('rt', 0.0): 8, ('ehdar', 0.0): 1, ('pegea', 0.0): 1, ('panga', 0.0): 1, ('dosto', 0.0): 1, ('nd', 0.0): 1, ('real_liam_payn', 0.0): 1, ('retweet', 0.0): 5, ('3/10', 0.0): 1, ('dmed', 0.0): 1, ('ad', 0.0): 1, ('yay', 0.0): 3, ('23', 0.0): 2, ('alreaddyyi', 0.0): 1, ('luceleva', 0.0): 1, ('21', 0.0): 1, ('porno', 0.0): 3, ('countrymus', 0.0): 4, ('sexysasunday', 0.0): 2, ('naeun', 0.0): 1, ('goal', 0.0): 5, (\"son'\", 0.0): 1, ('kidney', 0.0): 2, ('printer', 0.0): 1, ('ink', 0.0): 2, ('asham', 0.0): 3, ('ihatesomepeopl', 0.0): 1, ('tabl', 0.0): 2, ('0-2', 0.0): 1, ('brain', 0.0): 2, ('hard-wir', 0.0): 1, ('canadian', 0.0): 1, ('acn', 0.0): 2, ('gulo', 0.0): 1, ('kandekj', 0.0): 1, ('rize', 0.0): 1, ('meydan', 0.0): 1, ('experienc', 0.0): 2, ('fcking', 0.0): 1, ('crei', 0.0): 1, ('stabl', 0.0): 1, ('dormmat', 0.0): 1, ('pre', 0.0): 3, ('bo3', 0.0): 1, ('cod', 0.0): 2, ('redeem', 0.0): 1, ('invalid', 0.0): 1, ('wag', 0.0): 1, ('hopia', 0.0): 1, ('campaign', 0.0): 2, ('editor', 0.0): 1, ('reveal', 0.0): 2, ('booo', 0.0): 2, ('extens', 0.0): 1, ('rightnow', 0.0): 1, ('btu', 0.0): 1, ('karaok', 0.0): 1, ('licenc', 0.0): 1, ('apb', 0.0): 2, ('mbf', 0.0): 1, ('kpop', 0.0): 2, ('hahahaokay', 0.0): 1, ('basara', 0.0): 1, ('capcom', 0.0): 3, ('pc', 0.0): 2, ('url', 0.0): 2, ('web', 0.0): 2, ('site', 0.0): 6, ('design', 0.0): 3, ('grumbl', 0.0): 2, ('migrant', 0.0): 1, ('daddi', 0.0): 4, ('legit', 0.0): 1, ('australia', 0.0): 3, ('awsm', 0.0): 1, ('entir', 0.0): 5, ('tmw', 0.0): 1, ('uwu', 0.0): 1, ('jinki', 0.0): 1, ('taem', 0.0): 1, ('gif', 0.0): 2, ('cambridg', 0.0): 1, ('viath', 0.0): 1, ('brilliant', 0.0): 1, ('cypru', 0.0): 1, ('wet', 0.0): 10, ('30th', 0.0): 1, ('zayncomebackto', 0.0): 2, ('1d', 0.0): 6, ('senior', 0.0): 2, ('spazz', 0.0): 1, ('soobin', 0.0): 1, ('27', 0.0): 1, ('unmarri', 0.0): 1, ('float', 0.0): 3, ('pressur', 0.0): 3, ('winter', 0.0): 4, ('lifetim', 0.0): 2, ('hiondsh', 0.0): 1, ('58543', 0.0): 1, ('kikmenow', 0.0): 9, ('sexdat', 0.0): 2, (\"demi'\", 0.0): 1, ('junjou', 0.0): 2, ('romantica', 0.0): 1, ('cruel', 0.0): 1, ('privileg', 0.0): 2, ('mixtap', 0.0): 2, ('convinc', 0.0): 3, ('friex', 0.0): 1, ('taco', 0.0): 2, ('europ', 0.0): 2, ('shaylan', 0.0): 1, ('4:20', 0.0): 1, ('ylona', 0.0): 1, ('nah', 0.0): 4, ('notanapolog', 0.0): 3, ('ouh', 0.0): 1, ('tax', 0.0): 4, ('ohhh', 0.0): 2, ('nm', 0.0): 1, ('term', 0.0): 1, ('apolog', 0.0): 3, ('encanta', 0.0): 1, ('vale', 0.0): 1, ('osea', 0.0): 1, ('bea', 0.0): 1, ('♛', 0.0): 210, ('》', 0.0): 210, ('beli̇ev', 0.0): 35, ('wi̇ll', 0.0): 35, ('justi̇n', 0.0): 35, ('x15', 0.0): 35, ('350', 0.0): 4, ('ｓｅｅ', 0.0): 35, ('ｍｅ', 0.0): 35, ('40', 0.0): 3, ('dj', 0.0): 2, ('net', 0.0): 2, ('349', 0.0): 1, ('baek', 0.0): 1, ('tight', 0.0): 1, ('dunwan', 0.0): 1, ('suan', 0.0): 1, ('ba', 0.0): 3, ('haiz', 0.0): 1, ('otw', 0.0): 1, ('trade', 0.0): 3, ('venic', 0.0): 1, ('348', 0.0): 1, ('strong', 0.0): 6, ('adult', 0.0): 3, ('347', 0.0): 1, ('tree', 0.0): 3, ('hill', 0.0): 1, ('😕', 0.0): 1, ('com', 0.0): 1, ('insonia', 0.0): 1, ('346', 0.0): 1, ('rick', 0.0): 1, ('ross', 0.0): 1, ('wallet', 0.0): 4, ('empti', 0.0): 3, ('heartbreak', 0.0): 2, ('episod', 0.0): 11, ('345', 0.0): 1, ('milli', 0.0): 1, (':)', 0.0): 2, ('diff', 0.0): 1, ('persona', 0.0): 1, ('golden', 0.0): 1, ('scene', 0.0): 1, ('advert', 0.0): 1, ('determin', 0.0): 2, ('roseburi', 0.0): 1, ('familyhom', 0.0): 1, ('daw', 0.0): 2, ('344', 0.0): 1, ('monkey', 0.0): 1, ('yea', 0.0): 2, ('343', 0.0): 1, ('sweeti', 0.0): 2, ('erica', 0.0): 1, ('istg', 0.0): 1, ('lick', 0.0): 1, ('jackson', 0.0): 4, ('nsbzhdnxndamal', 0.0): 1, ('342', 0.0): 1, ('11:15', 0.0): 1, ('2hour', 0.0): 1, ('11:25', 0.0): 1, ('341', 0.0): 1, ('fandom', 0.0): 2, ('mahilig', 0.0): 1, ('mam-bulli', 0.0): 1, ('mtaani', 0.0): 1, ('tunaita', 0.0): 1, ('viazi', 0.0): 1, ('choma', 0.0): 1, ('laid', 0.0): 1, ('celebr', 0.0): 3, ('7am', 0.0): 1, ('jerk', 0.0): 1, ('lah', 0.0): 2, ('magic', 0.0): 1, ('menil', 0.0): 1, ('340', 0.0): 1, (\"kam'\", 0.0): 1, ('meee', 0.0): 1, ('diz', 0.0): 1, ('biooo', 0.0): 1, ('ay', 0.0): 1, ('taray', 0.0): 1, ('yumu-youtub', 0.0): 1, ('339', 0.0): 1, ('parijat', 0.0): 1, ('willmissyouparijat', 0.0): 1, ('abroad', 0.0): 2, ('jolli', 0.0): 1, ('scotland', 0.0): 2, ('338', 0.0): 1, ('mcnugget', 0.0): 1, ('sophi', 0.0): 5, ('feedback', 0.0): 4, ('met', 0.0): 7, ('caramello', 0.0): 2, ('koala', 0.0): 1, ('bar', 0.0): 1, ('suckmejimin', 0.0): 1, ('337', 0.0): 1, ('sucki', 0.0): 2, ('laughter', 0.0): 1, ('pou', 0.0): 1, ('goddamn', 0.0): 1, ('bark', 0.0): 1, ('nje', 0.0): 1, ('blast', 0.0): 1, ('hun', 0.0): 4, ('dbn', 0.0): 2, ('🎀', 0.0): 1, ('336', 0.0): 1, ('hardest', 0.0): 1, ('335', 0.0): 1, ('pledg', 0.0): 1, ('realiz', 0.0): 7, ('viber', 0.0): 1, ('mwah', 0.0): 1, ('estat', 0.0): 1, ('crush', 0.0): 1, ('lansi', 0.0): 1, ('334', 0.0): 1, ('hp', 0.0): 4, ('waah', 0.0): 1, ('miami', 0.0): 1, ('vandag', 0.0): 1, ('kgola', 0.0): 1, ('neng', 0.0): 1, ('eintlik', 0.0): 1, ('porn', 0.0): 2, ('4like', 0.0): 5, ('repost', 0.0): 2, ('333', 0.0): 3, ('magpi', 0.0): 1, ('22.05', 0.0): 1, ('15-24', 0.0): 1, ('05.15', 0.0): 1, ('coach', 0.0): 2, ('ador', 0.0): 1, ('chswiyfxcskcalum', 0.0): 1, ('nvm', 0.0): 2, ('lemm', 0.0): 1, ('quiet', 0.0): 3, ('foof', 0.0): 1, ('332', 0.0): 1, ('casilla', 0.0): 1, ('manchest', 0.0): 3, ('xi', 0.0): 1, ('rmtour', 0.0): 1, ('heavi', 0.0): 3, ('irl', 0.0): 2, ('blooper', 0.0): 2, ('huhuhuhu', 0.0): 1, ('na-tak', 0.0): 1, ('sorta', 0.0): 1, ('unfriend', 0.0): 1, ('greysonch', 0.0): 1, ('sandwich', 0.0): 4, ('bell', 0.0): 1, ('sebastian', 0.0): 1, ('rewatch', 0.0): 1, ('s4', 0.0): 1, ('ser', 0.0): 1, ('past', 0.0): 5, ('heart-break', 0.0): 1, ('outdat', 0.0): 1, ('m4', 0.0): 1, ('abandon', 0.0): 1, ('theater', 0.0): 1, ('smh', 0.0): 6, ('7-3', 0.0): 1, ('7.30-', 0.0): 1, ('ekk', 0.0): 1, ('giriboy', 0.0): 1, ('harriet', 0.0): 1, ('gegu', 0.0): 1, ('gray', 0.0): 1, ('truth', 0.0): 4, ('tbt', 0.0): 1, ('331', 0.0): 1, ('roof', 0.0): 2, ('indian', 0.0): 2, ('polit', 0.0): 3, ('blame', 0.0): 3, ('68', 0.0): 1, ('repres', 0.0): 1, ('corbyn', 0.0): 1, (\"labour'\", 0.0): 1, ('fortun', 0.0): 1, ('icecream', 0.0): 3, ('cuti', 0.0): 2, ('ry', 0.0): 1, ('lfccw', 0.0): 1, ('5ever', 0.0): 1, ('america', 0.0): 3, ('ontheroadagain', 0.0): 1, ('halaaang', 0.0): 1, ('reciev', 0.0): 1, ('flip', 0.0): 4, ('flop', 0.0): 1, ('caesarspalac', 0.0): 1, ('socialreward', 0.0): 1, ('requir', 0.0): 2, ('cali', 0.0): 1, ('fuckboy', 0.0): 1, ('330', 0.0): 1, ('deliveri', 0.0): 3, ('chrompet', 0.0): 1, ('easili', 0.0): 2, ('immun', 0.0): 1, ('system', 0.0): 3, ('lush', 0.0): 1, ('bathtub', 0.0): 1, ('php', 0.0): 1, ('mysql', 0.0): 1, ('libmysqlclient-dev', 0.0): 1, ('dev', 0.0): 2, ('pleasanton', 0.0): 1, ('wala', 0.0): 1, ('329', 0.0): 1, ('quickli', 0.0): 2, ('megan', 0.0): 1, ('heed', 0.0): 2, ('328', 0.0): 1, ('gwss', 0.0): 1, ('thankyouu', 0.0): 1, ('charad', 0.0): 1, ('becom', 0.0): 5, ('piano', 0.0): 2, ('327', 0.0): 1, ('complaint', 0.0): 2, ('yell', 0.0): 2, ('whatsoev', 0.0): 2, ('pete', 0.0): 1, ('wentz', 0.0): 1, ('shogi', 0.0): 1, ('blameshoghicp', 0.0): 1, ('classmat', 0.0): 1, ('troubl', 0.0): 1, ('fixedgearfrenzi', 0.0): 1, ('dispatch', 0.0): 1, ('theyr', 0.0): 2, ('hat', 0.0): 2, (\"shamuon'\", 0.0): 1, ('tokyo', 0.0): 1, ('toe', 0.0): 2, ('horrend', 0.0): 2, (\"someone'\", 0.0): 2, ('326', 0.0): 1, ('hasb', 0.0): 1, ('atti', 0.0): 1, ('muji', 0.0): 1, ('sirf', 0.0): 1, ('sensibl', 0.0): 1, ('etc', 0.0): 2, ('brum', 0.0): 1, ('cyclerevolut', 0.0): 1, ('caaannnttt', 0.0): 1, ('payment', 0.0): 3, ('overdrawn', 0.0): 1, ('tbf', 0.0): 1, ('complain', 0.0): 2, ('perfum', 0.0): 1, ('sampl', 0.0): 1, ('chanel', 0.0): 1, ('burberri', 0.0): 1, ('prada', 0.0): 1, ('325', 0.0): 1, ('noesss', 0.0): 1, ('topgear', 0.0): 1, ('worthi', 0.0): 1, ('bridesmaid', 0.0): 1, (\"tomorrow'\", 0.0): 2, ('gather', 0.0): 1, ('sudden', 0.0): 4, ('324', 0.0): 1, ('randomrestart', 0.0): 1, ('randomreboot', 0.0): 1, ('lumia', 0.0): 1, ('windowsphon', 0.0): 1, (\"microsoft'\", 0.0): 1, ('mañana', 0.0): 1, ('male', 0.0): 1, ('rap', 0.0): 1, ('sponsor', 0.0): 3, ('striker', 0.0): 2, ('lvg', 0.0): 1, ('behind', 0.0): 3, ('refurbish', 0.0): 1, ('cintiq', 0.0): 1, (\"finnick'\", 0.0): 1, ('askfinnick', 0.0): 1, ('contain', 0.0): 1, ('hairi', 0.0): 1, ('323', 0.0): 1, ('buri', 0.0): 1, ('omaygad', 0.0): 1, ('vic', 0.0): 1, ('surgeri', 0.0): 4, ('amber', 0.0): 8, ('tt.tt', 0.0): 1, ('hyper', 0.0): 2, ('vega', 0.0): 2, ('322', 0.0): 1, ('imiss', 0.0): 1, ('321', 0.0): 1, ('320', 0.0): 1, ('know.for', 0.0): 1, ('prepaid', 0.0): 1, ('none', 0.0): 4, ('319', 0.0): 1, ('grandma', 0.0): 1, (\"grandpa'\", 0.0): 1, ('farm', 0.0): 1, ('cow', 0.0): 1, ('sheep', 0.0): 1, ('hors', 0.0): 3, ('fruit', 0.0): 2, ('veget', 0.0): 1, ('puke', 0.0): 2, ('deliri', 0.0): 1, ('motilium', 0.0): 1, ('shite', 0.0): 1, ('318', 0.0): 1, ('schoolwork', 0.0): 1, (\"phoebe'\", 0.0): 1, ('317', 0.0): 1, ('pothol', 0.0): 1, ('316', 0.0): 1, ('notif', 0.0): 3, ('1,300', 0.0): 1, ('robyn', 0.0): 1, ('necklac', 0.0): 1, ('rachel', 0.0): 1, ('bhai', 0.0): 1, ('ramzan', 0.0): 1, ('crosss', 0.0): 1, ('clapham', 0.0): 1, ('investig', 0.0): 2, ('sth', 0.0): 1, ('essenti', 0.0): 1, ('photoshooot', 0.0): 1, ('austin', 0.0): 1, ('mahon', 0.0): 1, ('shut', 0.0): 3, ('andam', 0.0): 1, ('memor', 0.0): 1, ('cotton', 0.0): 1, ('candi', 0.0): 3, ('stock', 0.0): 3, ('swallow', 0.0): 1, ('snot', 0.0): 1, ('choke', 0.0): 1, ('taknottem', 0.0): 1, ('477', 0.0): 1, ('btob', 0.0): 2, ('percentag', 0.0): 1, ('shoshannavassil', 0.0): 1, ('swift', 0.0): 1, ('flat', 0.0): 3, ('a9', 0.0): 2, ('wsalelov', 0.0): 5, ('sexyjan', 0.0): 1, ('horni', 0.0): 2, ('goodmus', 0.0): 4, ('debut', 0.0): 3, ('lart', 0.0): 1, ('sew', 0.0): 1, ('skyfal', 0.0): 1, ('premier', 0.0): 1, ('yummi', 0.0): 2, ('manteca', 0.0): 1, (\"she'd\", 0.0): 2, ('probabl', 0.0): 8, ('shiatsu', 0.0): 1, ('heat', 0.0): 1, ('risk', 0.0): 3, ('edward', 0.0): 1, ('hopper', 0.0): 1, ('eyyah', 0.0): 1, ('utd', 0.0): 2, ('born', 0.0): 1, ('1-0', 0.0): 1, ('cart', 0.0): 1, ('shop', 0.0): 10, ('log', 0.0): 2, ('aaa', 0.0): 2, ('waifu', 0.0): 1, ('break', 0.0): 8, ('breakup', 0.0): 3, ('bother', 0.0): 3, ('bia', 0.0): 1, ('syndrom', 0.0): 1, ('shi', 0.0): 1, ('bias', 0.0): 1, ('pixel', 0.0): 2, ('weh', 0.0): 2, ('area', 0.0): 4, ('maymay', 0.0): 1, ('magpaalam', 0.0): 1, ('tf', 0.0): 3, ('subtitl', 0.0): 1, ('oitnb', 0.0): 1, ('backstori', 0.0): 1, ('jeremi', 0.0): 1, ('kyle', 0.0): 1, ('gimm', 0.0): 2, ('meal', 0.0): 3, ('neat-o', 0.0): 1, ('wru', 0.0): 1, ('scissor', 0.0): 1, ('creation', 0.0): 1, ('public', 0.0): 1, ('amtir', 0.0): 1, ('imysm', 0.0): 2, ('tut', 0.0): 1, ('trop', 0.0): 2, ('tard', 0.0): 1, ('deadlin', 0.0): 1, ('31', 0.0): 2, ('st', 0.0): 3, ('child', 0.0): 4, ('oct', 0.0): 2, ('bush', 0.0): 2, ('premiun', 0.0): 1, ('notcool', 0.0): 1, ('2/3', 0.0): 2, ('lahat', 0.0): 2, ('ng', 0.0): 4, ('araw', 0.0): 1, ('nage', 0.0): 1, ('gyu', 0.0): 4, ('lmfaooo', 0.0): 2, ('download', 0.0): 3, ('leagu', 0.0): 1, ('mashup', 0.0): 1, ('eu', 0.0): 1, ('lc', 0.0): 1, ('typo', 0.0): 2, ('itali', 0.0): 1, ('yass', 0.0): 1, ('christma', 0.0): 2, ('rel', 0.0): 1, ('yr', 0.0): 3, ('sydney', 0.0): 1, ('mb', 0.0): 1, ('perf', 0.0): 2, ('programm', 0.0): 1, ('bff', 0.0): 2, ('hashtag', 0.0): 1, ('omfg', 0.0): 4, ('exercis', 0.0): 2, ('combat', 0.0): 1, ('dosent', 0.0): 1, (\"sod'\", 0.0): 1, ('20min', 0.0): 1, ('request', 0.0): 2, ('yahoo', 0.0): 2, ('yodel', 0.0): 2, ('jokingli', 0.0): 1, ('regret', 0.0): 5, ('starbuck', 0.0): 3, ('lynettelow', 0.0): 1, ('interraci', 0.0): 3, (\"today'\", 0.0): 3, ('tgif', 0.0): 1, ('gahd', 0.0): 1, ('26th', 0.0): 1, ('discov', 0.0): 1, ('12.00', 0.0): 1, ('obyun', 0.0): 1, ('unni', 0.0): 4, ('wayhh', 0.0): 1, ('preval', 0.0): 1, ('controversi', 0.0): 1, ('🍵', 0.0): 2, ('☕', 0.0): 1, ('tube', 0.0): 1, ('strike', 0.0): 3, ('meck', 0.0): 1, ('mcfc', 0.0): 1, ('fresh', 0.0): 1, ('ucan', 0.0): 1, ('anxiou', 0.0): 1, ('poc', 0.0): 1, ('specif', 0.0): 2, ('sinhala', 0.0): 1, ('billionair', 0.0): 1, ('1645', 0.0): 1, ('island', 0.0): 3, ('1190', 0.0): 1, ('maldiv', 0.0): 1, ('dheena', 0.0): 1, ('fasgadah', 0.0): 1, ('alvadhaau', 0.0): 1, ('countdown', 0.0): 1, ('function', 0.0): 3, ('desktop', 0.0): 1, ('evelineconrad', 0.0): 1, ('facetim', 0.0): 4, ('kikmsn', 0.0): 2, ('selfshot', 0.0): 2, ('panda', 0.0): 1, ('backkk', 0.0): 1, ('transfer', 0.0): 3, ('dan', 0.0): 2, ('dull', 0.0): 1, ('overcast', 0.0): 1, ('folder', 0.0): 1, ('truck', 0.0): 2, ('missin', 0.0): 2, ('hangin', 0.0): 1, ('wiff', 0.0): 1, ('dept', 0.0): 1, ('cherri', 0.0): 1, ('bakewel', 0.0): 1, ('collect', 0.0): 3, ('teal', 0.0): 1, ('sect', 0.0): 1, ('tennunb', 0.0): 1, ('rather', 0.0): 4, ('skip', 0.0): 1, ('doomsday', 0.0): 1, ('neglect', 0.0): 1, ('posti', 0.0): 1, ('goodnight', 0.0): 1, ('donat', 0.0): 3, ('ship', 0.0): 6, ('bellami', 0.0): 1, ('raven', 0.0): 2, ('clark', 0.0): 1, ('helmi', 0.0): 1, ('uh', 0.0): 5, ('cnt', 0.0): 1, ('whereisthesun', 0.0): 1, ('summerismiss', 0.0): 1, ('longgg', 0.0): 1, ('ridicul', 0.0): 4, ('stocko', 0.0): 1, ('lucozad', 0.0): 1, ('explos', 0.0): 1, ('beh', 0.0): 2, ('half-rememb', 0.0): 1, (\"melody'\", 0.0): 1, ('recal', 0.0): 2, ('level', 0.0): 3, ('target', 0.0): 1, ('difficult', 0.0): 4, ('mile', 0.0): 1, ('pfb', 0.0): 1, ('nate', 0.0): 2, ('expo', 0.0): 2, ('jisoo', 0.0): 1, ('chloe', 0.0): 2, ('anon', 0.0): 2, ('mager', 0.0): 1, ('wi', 0.0): 1, ('knw', 0.0): 1, ('wht', 0.0): 1, ('distant', 0.0): 1, ('buffer', 0.0): 2, ('insan', 0.0): 1, ('charli', 0.0): 1, ('finland', 0.0): 3, ('gana', 0.0): 1, ('studio', 0.0): 3, ('arch', 0.0): 1, ('lyin', 0.0): 1, ('kian', 0.0): 3, ('supercar', 0.0): 1, ('gurgaon', 0.0): 1, ('locat', 0.0): 7, ('9:15', 0.0): 1, ('satir', 0.0): 1, ('gener', 0.0): 2, ('peanut', 0.0): 3, ('butter', 0.0): 1, ('garden', 0.0): 2, ('beer', 0.0): 1, ('viner', 0.0): 1, ('palembang', 0.0): 1, ('sorrryyi', 0.0): 1, ('fani', 0.0): 1, ('hahahahaha', 0.0): 2, ('boner', 0.0): 1, ('merci', 0.0): 1, ('yuki', 0.0): 1, ('2500k', 0.0): 1, ('mari', 0.0): 1, ('jake', 0.0): 1, ('gyllenha', 0.0): 1, ('impact', 0.0): 1, (\"ledger'\", 0.0): 1, ('btw', 0.0): 5, ('cough', 0.0): 4, ('hunni', 0.0): 1, ('b4', 0.0): 1, ('deplet', 0.0): 1, ('mbasa', 0.0): 1, ('client', 0.0): 3, ('ray', 0.0): 1, ('aah', 0.0): 1, ('type', 0.0): 2, ('suit', 0.0): 5, ('pa-copi', 0.0): 1, ('proper', 0.0): 2, ('biom', 0.0): 1, ('mosqu', 0.0): 1, ('smelli', 0.0): 1, ('taxi', 0.0): 4, ('emptier', 0.0): 1, (\"ciara'\", 0.0): 1, (\"everything'\", 0.0): 1, ('clip', 0.0): 2, ('tall', 0.0): 2, ('gladli', 0.0): 1, ('intent', 0.0): 1, ('amb', 0.0): 1, (\"harry'\", 0.0): 2, ('jean', 0.0): 2, ('mayday', 0.0): 1, ('parad', 0.0): 2, ('lyf', 0.0): 1, ('13th', 0.0): 1, ('anim', 0.0): 4, ('kingdom', 0.0): 1, ('chri', 0.0): 7, ('brown', 0.0): 4, ('riski', 0.0): 1, ('cologn', 0.0): 1, ('duo', 0.0): 3, ('ballad', 0.0): 2, ('bish', 0.0): 2, ('intern', 0.0): 2, ('brought', 0.0): 1, ('yumyum', 0.0): 1, (\"cathy'\", 0.0): 1, ('missyou', 0.0): 1, ('rubi', 0.0): 2, ('rose', 0.0): 2, ('tou', 0.0): 1, ('main', 0.0): 1, ('pora', 0.0): 1, ('stalk', 0.0): 3, ('karlia', 0.0): 1, ('khatam', 0.0): 2, ('bandi', 0.0): 1, ('👑', 0.0): 1, ('pyaari', 0.0): 1, ('gawd', 0.0): 1, ('understood', 0.0): 1, ('review', 0.0): 3, ('massi', 0.0): 1, ('thatselfiethough', 0.0): 1, ('loop', 0.0): 1, ('ofc', 0.0): 1, ('pict', 0.0): 1, ('caught', 0.0): 1, ('aishhh', 0.0): 1, ('viewer', 0.0): 1, ('exam', 0.0): 5, ('sighsss', 0.0): 1, ('burnt', 0.0): 2, ('toffe', 0.0): 2, ('honesti', 0.0): 1, ('cheatday', 0.0): 1, ('protein', 0.0): 1, ('sissi', 0.0): 1, ('tote', 0.0): 1, ('slowli', 0.0): 1, ('church', 0.0): 2, ('pll', 0.0): 1, ('sel', 0.0): 1, ('beth', 0.0): 2, ('serbia', 0.0): 1, ('serbian', 0.0): 1, ('selen', 0.0): 1, ('motav', 0.0): 1, ('💋', 0.0): 2, ('zayyyn', 0.0): 1, ('momma', 0.0): 1, ('happend', 0.0): 1, ('imper', 0.0): 1, ('trmdhesit', 0.0): 1, ('pana', 0.0): 1, ('quickest', 0.0): 2, ('blood', 0.0): 5, ('sake', 0.0): 1, ('hamstr', 0.0): 1, ('rodwel', 0.0): 1, ('trace', 0.0): 1, ('artist', 0.0): 4, ('tp', 0.0): 1, ('powder', 0.0): 1, ('wider', 0.0): 1, ('honestli', 0.0): 4, ('comfort', 0.0): 3, ('bruno', 0.0): 1, ('1.8', 0.0): 1, ('ed', 0.0): 7, ('croke', 0.0): 2, ('deal', 0.0): 6, ('toll', 0.0): 1, ('packag', 0.0): 1, ('shape', 0.0): 1, ('unluckiest', 0.0): 1, ('bettor', 0.0): 1, ('nstp', 0.0): 1, ('sem', 0.0): 2, ('chipotl', 0.0): 1, ('chick-fil-a', 0.0): 1, ('stole', 0.0): 3, ('evet', 0.0): 1, ('ramadhan', 0.0): 1, ('eid', 0.0): 4, ('stexpert', 0.0): 1, ('ripstegi', 0.0): 1, ('nickyyi', 0.0): 1, ('¿', 0.0): 1, ('centralis', 0.0): 1, ('discontinu', 0.0): 1, ('sniff', 0.0): 1, (\"i't\", 0.0): 1, ('glad', 0.0): 2, ('fab', 0.0): 2, ('theres', 0.0): 1, ('cred', 0.0): 1, ('t_t', 0.0): 1, ('elimin', 0.0): 1, ('teamzip', 0.0): 1, ('smtm', 0.0): 1, ('assingn', 0.0): 1, ('editi', 0.0): 1, ('nakaka', 0.0): 1, ('beastmod', 0.0): 1, ('gaaawd', 0.0): 1, ('jane', 0.0): 1, ('mango', 0.0): 1, ('colombia', 0.0): 1, ('yot', 0.0): 1, ('labyo', 0.0): 1, ('pano', 0.0): 1, ('nalamannn', 0.0): 1, ('hardhead', 0.0): 1, ('cell', 0.0): 1, (\"zach'\", 0.0): 1, ('burger', 0.0): 2, ('xpress', 0.0): 1, ('hopkin', 0.0): 1, ('melatonin', 0.0): 1, ('2-4', 0.0): 1, ('nap', 0.0): 2, ('wide', 0.0): 2, ('task', 0.0): 1, ('9pm', 0.0): 1, ('hahaah', 0.0): 1, ('frequent', 0.0): 1, ('jail', 0.0): 2, ('weirddd', 0.0): 1, ('donghyuk', 0.0): 1, ('stan', 0.0): 1, ('bek', 0.0): 1, ('13', 0.0): 4, ('reynoldsgrl', 0.0): 1, ('ole', 0.0): 1, ('beardi', 0.0): 1, ('kaussi', 0.0): 1, ('bummer', 0.0): 3, ('fightingmciren', 0.0): 1, (\"michael'\", 0.0): 1, ('�', 0.0): 21, ('miser', 0.0): 2, ('💦', 0.0): 1, ('yoga', 0.0): 2, ('🌞', 0.0): 1, ('💃🏽', 0.0): 1, ('shouldv', 0.0): 1, ('saffron', 0.0): 1, ('peasant', 0.0): 1, ('wouldv', 0.0): 1, ('nfinit', 0.0): 1, ('admin_myung', 0.0): 1, ('slp', 0.0): 1, ('saddest', 0.0): 2, ('laomma', 0.0): 2, ('kebaya', 0.0): 1, ('bandung', 0.0): 1, ('indonesia', 0.0): 1, ('7df89150', 0.0): 1, ('whatsapp', 0.0): 2, ('62', 0.0): 1, ('08962464174', 0.0): 1, ('laomma_coutur', 0.0): 1, ('haizzz', 0.0): 1, ('urghhh', 0.0): 1, ('working-on-a-tight-schedul', 0.0): 1, ('ganbarimasu', 0.0): 1, ('livid', 0.0): 1, ('whammi', 0.0): 1, ('quuuee', 0.0): 1, ('friooo', 0.0): 1, ('ladi', 0.0): 4, ('stereo', 0.0): 1, ('chwang', 0.0): 1, ('lorm', 0.0): 1, ('823', 0.0): 1, ('rp', 0.0): 1, ('indiemus', 0.0): 10, ('unhappi', 0.0): 2, ('jennyjean', 0.0): 1, ('elfindelmundo', 0.0): 2, ('lolzz', 0.0): 1, ('dat', 0.0): 4, ('corey', 0.0): 1, ('appreci', 0.0): 2, ('weekli', 0.0): 2, ('mahirap', 0.0): 1, ('nash', 0.0): 1, ('gosh', 0.0): 6, ('noodl', 0.0): 1, ('veeerri', 0.0): 1, ('rted', 0.0): 2, ('orig', 0.0): 1, ('starholicxx', 0.0): 1, ('07:17', 0.0): 2, ('@the', 0.0): 1, ('notr', 0.0): 1, ('hwi', 0.0): 1, ('niall', 0.0): 5, ('fraud', 0.0): 1, ('diplomaci', 0.0): 1, ('fittest', 0.0): 1, ('zero', 0.0): 1, ('toler', 0.0): 2, ('gurl', 0.0): 1, ('notion', 0.0): 1, ('pier', 0.0): 1, ('approach', 0.0): 1, ('rattl', 0.0): 1, ('robe', 0.0): 1, ('emphasi', 0.0): 1, ('vocal', 0.0): 1, ('chose', 0.0): 1, ('erm', 0.0): 1, ('abby.can', 0.0): 1, ('persuad', 0.0): 1, ('lyric', 0.0): 1, (\"emily'\", 0.0): 1, ('odd', 0.0): 3, ('possibl', 0.0): 8, ('elect', 0.0): 2, ('kamiss', 0.0): 1, ('mwa', 0.0): 1, ('mommi', 0.0): 3, ('scream', 0.0): 1, ('fight', 0.0): 2, ('cafe', 0.0): 2, ('melbourn', 0.0): 1, ('anyonnee', 0.0): 1, ('loner', 0.0): 1, ('fricken', 0.0): 2, ('rito', 0.0): 1, ('friendzon', 0.0): 1, ('panel', 0.0): 1, ('repeat', 0.0): 2, ('audienc', 0.0): 1, ('hsm', 0.0): 1, ('canario', 0.0): 1, ('hotel', 0.0): 8, ('ukiss', 0.0): 1, ('faith', 0.0): 2, ('kurt', 0.0): 1, (\"fatma'm\", 0.0): 1, ('alex', 0.0): 4, ('swag', 0.0): 1, ('lmfao', 0.0): 2, ('flapjack', 0.0): 1, ('countthecost', 0.0): 1, ('ihop', 0.0): 1, ('infra', 0.0): 1, ('lq', 0.0): 1, ('knive', 0.0): 1, ('sotir', 0.0): 1, ('mybrainneedstoshutoff', 0.0): 1, ('macci', 0.0): 1, ('chees', 0.0): 7, ('25', 0.0): 2, ('tend', 0.0): 1, ('510', 0.0): 1, ('silicon', 0.0): 1, ('cover', 0.0): 2, ('kbye', 0.0): 1, ('ini', 0.0): 1, ('anytim', 0.0): 1, ('citizen', 0.0): 1, ('compar', 0.0): 2, ('rank', 0.0): 1, ('mcountdown', 0.0): 2, ('5h', 0.0): 1, ('thapelo', 0.0): 1, ('op', 0.0): 1, ('civ', 0.0): 1, ('wooden', 0.0): 1, ('mic', 0.0): 1, ('embarrass', 0.0): 2, ('translat', 0.0): 3, ('daili', 0.0): 3, ('mecha-totem', 0.0): 1, ('nak', 0.0): 1, ('tgk', 0.0): 1, ('townsss', 0.0): 1, ('jokid', 0.0): 1, ('rent', 0.0): 2, ('degre', 0.0): 1, ('inconsider', 0.0): 2, ('softbal', 0.0): 1, ('appli', 0.0): 1, ('tomcat', 0.0): 1, ('chel', 0.0): 1, ('jemma', 0.0): 1, ('detail', 0.0): 4, ('list', 0.0): 4, ('matchi', 0.0): 2, ('elsa', 0.0): 1, ('postpon', 0.0): 1, ('karin', 0.0): 1, ('honey', 0.0): 2, ('vist', 0.0): 1, ('unhealthi', 0.0): 1, ('propa', 0.0): 1, ('knockin', 0.0): 1, ('bacon', 0.0): 1, ('market', 0.0): 2, ('pre-holiday', 0.0): 1, ('diet', 0.0): 1, ('meani', 0.0): 1, ('deathbybaconsmel', 0.0): 1, ('init', 0.0): 2, ('destin', 0.0): 1, ('victoria', 0.0): 2, ('luna', 0.0): 1, ('krystal', 0.0): 1, ('sarajevo', 0.0): 1, ('haix', 0.0): 2, ('sp', 0.0): 1, ('student', 0.0): 4, ('wii', 0.0): 2, ('bayonetta', 0.0): 1, ('101', 0.0): 1, ('doabl', 0.0): 1, ('drove', 0.0): 1, ('agenc', 0.0): 1, ('story.miss', 0.0): 1, ('everon', 0.0): 1, ('jp', 0.0): 1, ('mamabear', 0.0): 1, ('imintoh', 0.0): 1, ('underr', 0.0): 1, (\"slovakia'\", 0.0): 1, ('d:', 0.0): 6, ('saklap', 0.0): 1, ('grade', 0.0): 2, ('rizal', 0.0): 1, ('lib', 0.0): 1, ('discuss', 0.0): 1, ('advisori', 0.0): 1, ('period', 0.0): 2, ('dit', 0.0): 1, ('du', 0.0): 1, ('harsh', 0.0): 2, ('ohgod', 0.0): 1, ('abligaverin', 0.0): 2, ('photooftheday', 0.0): 2, ('sexygirlbypreciouslemmi', 0.0): 3, ('ripsandrabland', 0.0): 1, ('edel', 0.0): 1, ('salam', 0.0): 1, ('mubark', 0.0): 1, ('dong', 0.0): 3, ('tammirossm', 0.0): 4, ('speck', 0.0): 1, ('abbymil', 0.0): 2, ('18', 0.0): 8, ('ion', 0.0): 1, ('5min', 0.0): 1, ('hse', 0.0): 1, ('noob', 0.0): 1, ('nxt', 0.0): 1, ('2week', 0.0): 1, ('300', 0.0): 3, ('fck', 0.0): 2, ('nae', 0.0): 2, ('deep', 0.0): 3, ('human', 0.0): 3, ('whit', 0.0): 1, ('van', 0.0): 4, ('bristol', 0.0): 1, ('subserv', 0.0): 1, ('si', 0.0): 4, ('oo', 0.0): 1, ('tub', 0.0): 1, ('penyfan', 0.0): 1, ('forecast', 0.0): 2, ('breconbeacon', 0.0): 1, ('tittheir', 0.0): 1, ('42', 0.0): 1, ('hotti', 0.0): 3, ('uu', 0.0): 2, ('rough', 0.0): 1, ('fuzzi', 0.0): 1, ('san', 0.0): 3, ('antonio', 0.0): 1, ('kang', 0.0): 1, ('junhe', 0.0): 1, ('couldv', 0.0): 1, ('pz', 0.0): 1, ('somerset', 0.0): 1, ('given', 0.0): 2, ('sunburnt', 0.0): 1, ('safer', 0.0): 1, ('k3g', 0.0): 1, ('input', 0.0): 1, ('gamestomp', 0.0): 1, ('desc', 0.0): 1, (\"angelo'\", 0.0): 1, ('yna', 0.0): 1, ('psygustokita', 0.0): 2, ('fiver', 0.0): 1, ('toward', 0.0): 1, ('sakho', 0.0): 1, ('threat', 0.0): 1, ('goalscor', 0.0): 1, ('10:59', 0.0): 1, ('11.00', 0.0): 1, ('sham', 0.0): 1, ('tricki', 0.0): 1, ('baao', 0.0): 1, ('nisrina', 0.0): 1, ('crazi', 0.0): 8, ('ladygaga', 0.0): 1, (\"you'\", 0.0): 2, ('pari', 0.0): 2, ('marrish', 0.0): 1, (\"otp'\", 0.0): 1, ('6:15', 0.0): 1, ('edomnt', 0.0): 1, ('qih', 0.0): 1, ('shxb', 0.0): 1, ('1000', 0.0): 1, ('chilton', 0.0): 1, ('mother', 0.0): 2, ('obsess', 0.0): 1, ('creepi', 0.0): 2, ('josh', 0.0): 1, ('boohoo', 0.0): 1, ('fellow', 0.0): 2, ('tweep', 0.0): 1, ('roar', 0.0): 1, ('victori', 0.0): 1, ('tweepsmatchout', 0.0): 1, ('nein', 0.0): 3, ('404', 0.0): 1, ('midnight', 0.0): 2, ('willlow', 0.0): 1, ('hbd', 0.0): 1, ('sowwi', 0.0): 1, ('3000', 0.0): 1, ('grind', 0.0): 1, ('gear', 0.0): 1, ('0.001', 0.0): 1, ('meant', 0.0): 6, ('portrait', 0.0): 1, ('mode', 0.0): 2, ('fact', 0.0): 4, ('11:11', 0.0): 4, ('shanzay', 0.0): 1, ('salabrati', 0.0): 1, ('journo', 0.0): 1, ('lure', 0.0): 1, ('gang', 0.0): 1, ('twist', 0.0): 1, ('mashaket', 0.0): 1, ('pet', 0.0): 2, ('bapak', 0.0): 1, ('royal', 0.0): 2, ('prima', 0.0): 1, ('mune', 0.0): 1, ('874', 0.0): 1, ('plisss', 0.0): 1, ('elf', 0.0): 1, ('teenchoic', 0.0): 5, ('choiceinternationalartist', 0.0): 5, ('superjunior', 0.0): 5, (\"he'll\", 0.0): 1, ('sunway', 0.0): 1, ('petal', 0.0): 1, ('jaya', 0.0): 1, ('selangor', 0.0): 1, ('glow', 0.0): 1, ('huhuu', 0.0): 1, ('congratul', 0.0): 2, ('margo', 0.0): 1, ('konga', 0.0): 1, ('ni', 0.0): 4, ('wa', 0.0): 2, ('ode', 0.0): 1, ('disvirgin', 0.0): 1, ('bride', 0.0): 3, ('yulin', 0.0): 1, ('meat', 0.0): 1, ('festiv', 0.0): 2, ('imma', 0.0): 2, ('syawal', 0.0): 1, ('lapar', 0.0): 1, ('foundat', 0.0): 1, ('clash', 0.0): 2, ('facil', 0.0): 1, ('dh', 0.0): 2, ('chalet', 0.0): 1, ('suay', 0.0): 1, ('anot', 0.0): 1, ('bugger', 0.0): 1, ('एक', 0.0): 1, ('बार', 0.0): 1, ('फिर', 0.0): 1, ('सेँ', 0.0): 1, ('धोखा', 0.0): 1, ('chandauli', 0.0): 1, ('majhwar', 0.0): 1, ('railway', 0.0): 1, ('tito', 0.0): 2, ('tita', 0.0): 1, ('cousin', 0.0): 3, ('critic', 0.0): 1, ('condit', 0.0): 1, ('steal', 0.0): 1, ('narco', 0.0): 1, ('regen', 0.0): 1, ('unfav', 0.0): 2, ('benadryl', 0.0): 1, ('offlin', 0.0): 1, ('arent', 0.0): 1, ('msg', 0.0): 1, ('yg', 0.0): 1, ('gg', 0.0): 3, ('sxrew', 0.0): 1, ('dissappear', 0.0): 1, ('swap', 0.0): 1, ('bleed', 0.0): 1, ('ishal', 0.0): 1, ('mi', 0.0): 2, ('thaank', 0.0): 1, ('jhezz', 0.0): 1, ('sneak', 0.0): 3, ('soft', 0.0): 1, ('defenc', 0.0): 1, ('defens', 0.0): 1, ('nrltigersroost', 0.0): 1, ('indiana', 0.0): 2, ('hibb', 0.0): 1, ('biblethump', 0.0): 1, ('rlyyi', 0.0): 1, ('septum', 0.0): 1, ('pierc', 0.0): 2, ('goood', 0.0): 1, ('hiya', 0.0): 1, ('fire', 0.0): 1, ('venom', 0.0): 1, ('carriag', 0.0): 1, ('pink', 0.0): 1, ('fur-trim', 0.0): 1, ('stetson', 0.0): 1, ('error', 0.0): 4, ('59', 0.0): 1, ('xue', 0.0): 1, ('midori', 0.0): 1, ('sakit', 0.0): 2, ('mateo', 0.0): 1, ('hawk', 0.0): 2, ('bartend', 0.0): 1, ('surf', 0.0): 1, ('despair', 0.0): 1, ('insta', 0.0): 1, ('promo', 0.0): 1, ('iwantin', 0.0): 1, ('___', 0.0): 2, ('fault', 0.0): 3, ('goodluck', 0.0): 1, ('pocket', 0.0): 1, ('help@veryhq.co.uk', 0.0): 1, ('benedictervent', 0.0): 1, ('content', 0.0): 1, ('221b', 0.0): 1, ('popcorn', 0.0): 3, ('joyc', 0.0): 1, ('ooop', 0.0): 1, ('spotifi', 0.0): 1, ('paalam', 0.0): 1, ('sazbal', 0.0): 1, ('incid', 0.0): 1, ('aaahh', 0.0): 1, ('gooo', 0.0): 1, (\"stomach'\", 0.0): 1, ('growl', 0.0): 1, ('beard', 0.0): 1, ('nooop', 0.0): 1, ('🎉', 0.0): 3, ('ding', 0.0): 3, ('hundr', 0.0): 1, ('meg', 0.0): 1, (\"verity'\", 0.0): 1, ('rupert', 0.0): 1, ('amin', 0.0): 1, ('studi', 0.0): 2, ('pleaaas', 0.0): 1, ('👆🏻', 0.0): 2, ('woaah', 0.0): 1, ('solvo', 0.0): 1, ('twin', 0.0): 2, (\"friday'\", 0.0): 1, ('lego', 0.0): 1, ('barefoot', 0.0): 1, ('twelvyy', 0.0): 1, ('boaz', 0.0): 1, ('myhil', 0.0): 1, ('takeov', 0.0): 1, ('wba', 0.0): 1, (\"taeyeon'\", 0.0): 1, ('derp', 0.0): 1, ('pd', 0.0): 1, ('zoom', 0.0): 2, (\"sunny'\", 0.0): 1, ('besst', 0.0): 1, ('plagu', 0.0): 1, ('pit', 0.0): 1, ('rich', 0.0): 1, ('sight', 0.0): 1, ('frail', 0.0): 1, ('lotteri', 0.0): 1, ('ride', 0.0): 2, ('twurkin', 0.0): 1, ('razzist', 0.0): 1, ('tumblr', 0.0): 1, ('shek', 0.0): 1, ('609', 0.0): 1, ('mugshot', 0.0): 1, ('attend', 0.0): 3, ('plsss', 0.0): 4, ('taissa', 0.0): 1, ('farmiga', 0.0): 1, ('robert', 0.0): 1, ('qualiti', 0.0): 1, ('daniel', 0.0): 1, ('latest', 0.0): 3, ('softwar', 0.0): 1, ('restor', 0.0): 2, ('momo', 0.0): 2, ('pharma', 0.0): 1, ('immov', 0.0): 1, ('messi', 0.0): 1, ('ansh', 0.0): 1, ('f1', 0.0): 1, ('billion', 0.0): 1, ('rand', 0.0): 1, ('bein', 0.0): 1, ('tla', 0.0): 1, ('tweng', 0.0): 1, ('gene', 0.0): 1, ('up.com', 0.0): 1, ('counti', 0.0): 2, ('cooler', 0.0): 1, ('minhyuk', 0.0): 1, ('gold', 0.0): 2, ('1900', 0.0): 1, ('😪', 0.0): 3, ('yu', 0.0): 1, ('hz', 0.0): 2, ('selena', 0.0): 2, ('emta', 0.0): 1, ('hatigii', 0.0): 1, ('b2aa', 0.0): 1, ('yayyy', 0.0): 1, ('anesthesia', 0.0): 1, ('penrith', 0.0): 1, ('emu', 0.0): 1, ('plain', 0.0): 1, ('staff', 0.0): 3, ('untouch', 0.0): 1, ('brienn', 0.0): 1, ('lsh', 0.0): 1, ('gunna', 0.0): 1, ('former', 0.0): 1, ('darn', 0.0): 1, ('allah', 0.0): 4, ('pakistan', 0.0): 2, ('juudiciari', 0.0): 1, (\"horton'\", 0.0): 1, ('dunkin', 0.0): 1, ('socialis', 0.0): 1, ('cara', 0.0): 1, (\"delevingne'\", 0.0): 1, ('fear', 0.0): 1, ('drug', 0.0): 1, ('lace', 0.0): 1, ('fank', 0.0): 1, ('takfaham', 0.0): 1, ('ufff', 0.0): 1, ('sr', 0.0): 2, ('dard', 0.0): 1, ('katekyn', 0.0): 1, ('ehh', 0.0): 1, ('yeahhh', 0.0): 2, ('hacharatt', 0.0): 1, ('niwll', 0.0): 1, ('defin', 0.0): 1, ('wit', 0.0): 2, ('goa', 0.0): 1, ('lini', 0.0): 1, ('kasi', 0.0): 3, ('rhd', 0.0): 1, ('1st', 0.0): 3, ('wae', 0.0): 1, ('subsid', 0.0): 1, ('20th', 0.0): 1, ('anniversari', 0.0): 1, ('youngja', 0.0): 1, ('harumph', 0.0): 1, ('soggi', 0.0): 1, ('weed', 0.0): 1, ('ireland', 0.0): 3, ('sakura', 0.0): 1, ('flavour', 0.0): 1, ('chokki', 0.0): 1, ('🌸', 0.0): 1, ('unavail', 0.0): 2, ('richard', 0.0): 2, ('laptop', 0.0): 2, ('satya', 0.0): 1, ('aditya', 0.0): 1, ('🍜', 0.0): 3, ('vibrat', 0.0): 1, ('an', 0.0): 2, ('cu', 0.0): 1, ('dhaka', 0.0): 1, ('jam', 0.0): 1, ('shall', 0.0): 2, ('cornetto', 0.0): 3, ('noseble', 0.0): 1, ('nintendo', 0.0): 3, ('wew', 0.0): 1, ('ramo', 0.0): 1, ('ground', 0.0): 2, ('shawn', 0.0): 1, ('mend', 0.0): 1, ('l', 0.0): 2, ('dinghi', 0.0): 1, ('skye', 0.0): 1, ('store', 0.0): 3, ('descript', 0.0): 2, ('colleagu', 0.0): 2, ('gagal', 0.0): 2, ('txt', 0.0): 1, ('sim', 0.0): 1, ('nooot', 0.0): 1, ('notch', 0.0): 1, ('tht', 0.0): 2, ('starv', 0.0): 4, ('\\U000fe196', 0.0): 1, ('pyjama', 0.0): 1, ('swifti', 0.0): 1, ('sorna', 0.0): 1, ('lurgi', 0.0): 1, ('jim', 0.0): 2, ('6gb', 0.0): 1, ('fenestoscop', 0.0): 1, ('etienn', 0.0): 1, ('bandana', 0.0): 3, ('bigger', 0.0): 2, ('vagina', 0.0): 1, ('suriya', 0.0): 1, ('dangl', 0.0): 1, ('mjhe', 0.0): 2, ('aaj', 0.0): 1, ('tak', 0.0): 3, ('kisi', 0.0): 1, ('kiya', 0.0): 1, ('eyesight', 0.0): 1, ('25x30', 0.0): 1, ('aftenoon', 0.0): 1, ('booor', 0.0): 1, ('uuu', 0.0): 1, ('boyfriend', 0.0): 8, ('freebiefriday', 0.0): 1, ('garag', 0.0): 1, ('michael', 0.0): 1, ('obvious', 0.0): 1, ('denim', 0.0): 1, ('somebodi', 0.0): 1, ('ce', 0.0): 1, ('gw', 0.0): 1, ('anatomi', 0.0): 1, ('no1', 0.0): 1, (\"morisette'\", 0.0): 1, ('flash', 0.0): 1, ('non-trial', 0.0): 1, ('sayhernam', 0.0): 1, ('lootcrat', 0.0): 1, ('item', 0.0): 1, ('inca', 0.0): 1, ('trail', 0.0): 1, ('sandboard', 0.0): 1, ('derbi', 0.0): 1, ('coffe', 0.0): 1, ('unabl', 0.0): 3, ('signatur', 0.0): 1, ('dish', 0.0): 1, ('unfamiliar', 0.0): 1, ('kitchen', 0.0): 3, ('coldest', 0.0): 1, (\"old'\", 0.0): 1, ('14518344', 0.0): 1, ('61', 0.0): 1, ('thirdwheel', 0.0): 1, ('lovebird', 0.0): 1, ('nth', 0.0): 1, ('imo', 0.0): 1, ('familiar', 0.0): 1, ('@juliettemaughan', 0.0): 1, ('copi', 0.0): 1, ('sensiesha', 0.0): 1, ('eldest', 0.0): 1, ('netbal', 0.0): 1, ('😟', 0.0): 1, ('keedz', 0.0): 1, ('taybigail', 0.0): 1, ('jordan', 0.0): 1, ('tournament', 0.0): 1, ('goin', 0.0): 1, ('ps4', 0.0): 3, ('kink', 0.0): 1, ('charger', 0.0): 1, ('streak', 0.0): 1, ('scorch', 0.0): 1, ('srski', 0.0): 1, ('tdc', 0.0): 1, ('egypt', 0.0): 1, ('in-sensit', 0.0): 1, ('cooper', 0.0): 3, ('invit', 0.0): 1, ('donna', 0.0): 1, ('thurston', 0.0): 1, ('collin', 0.0): 1, ('quietli', 0.0): 2, ('kennel', 0.0): 1, ('911', 0.0): 1, ('pluckersss', 0.0): 1, ('gion', 0.0): 1, ('886', 0.0): 1, ('nsfw', 0.0): 1, ('kidschoiceaward', 0.0): 1, ('ming', 0.0): 1, ('pbr', 0.0): 1, ('shoutout', 0.0): 1, ('periscop', 0.0): 1, ('ut', 0.0): 1, ('shawti', 0.0): 1, ('naw', 0.0): 4, (\"sterling'\", 0.0): 1, ('9muse', 0.0): 1, ('hrryok', 0.0): 2, ('asap', 0.0): 2, ('wnt', 0.0): 1, ('9:30', 0.0): 1, ('9:48', 0.0): 1, ('9/11', 0.0): 1, ('bueno', 0.0): 1, ('receptionist', 0.0): 1, ('ella', 0.0): 2, ('goe', 0.0): 4, ('ketchup', 0.0): 1, ('tasteless', 0.0): 1, ('deantd', 0.0): 1, ('justgotkanekifi', 0.0): 1, ('notgonnabeactivefor', 0.0): 1, ('2weeksdontmissittoomuch', 0.0): 1, ('2013', 0.0): 1, ('disney', 0.0): 2, ('vlog', 0.0): 1, ('swim', 0.0): 1, ('turtl', 0.0): 2, ('cnn', 0.0): 2, ('straplin', 0.0): 1, ('theatr', 0.0): 1, ('guncontrol', 0.0): 1, ('stung', 0.0): 2, ('tweak', 0.0): 1, (\"thát'\", 0.0): 1, ('powerpoint', 0.0): 1, ('present', 0.0): 5, ('diner', 0.0): 1, ('no-no', 0.0): 1, ('hind', 0.0): 1, ('circuit', 0.0): 1, ('secondari', 0.0): 1, ('sodder', 0.0): 1, ('perhap', 0.0): 2, ('mobitel', 0.0): 1, ('colin', 0.0): 1, ('playstat', 0.0): 2, ('charg', 0.0): 4, ('exp', 0.0): 1, ('misspelt', 0.0): 1, ('wan', 0.0): 1, ('hyungwon', 0.0): 2, ('alarm', 0.0): 1, ('needicecreamnow', 0.0): 1, ('shake', 0.0): 1, ('repeatedli', 0.0): 1, ('nu-uh', 0.0): 1, ('jace', 0.0): 1, ('mostest', 0.0): 1, ('vip', 0.0): 1, ('urgh', 0.0): 1, ('consol', 0.0): 1, (\"grigson'\", 0.0): 1, ('carrot', 0.0): 1, ('&gt;:-(', 0.0): 4, ('sunburn', 0.0): 1, ('ughh', 0.0): 2, ('enabl', 0.0): 1, ('otter', 0.0): 1, ('protect', 0.0): 1, ('argh', 0.0): 1, ('pon', 0.0): 1, ('otl', 0.0): 2, ('sleepov', 0.0): 2, ('jess', 0.0): 2, ('bebe', 0.0): 1, ('fabina', 0.0): 1, (\"barrista'\", 0.0): 1, ('plant', 0.0): 3, ('pup', 0.0): 2, ('brolli', 0.0): 1, ('mere', 0.0): 2, ('nhi', 0.0): 1, ('dey', 0.0): 2, ('serv', 0.0): 1, ('kepo', 0.0): 1, ('bitin', 0.0): 1, ('pretzel', 0.0): 1, ('bb17', 0.0): 1, ('bblf', 0.0): 1, ('fuckin', 0.0): 1, ('vanilla', 0.0): 1, ('latt', 0.0): 1, ('skulker', 0.0): 1, ('thread', 0.0): 1, ('hungrrryyi', 0.0): 1, ('icloud', 0.0): 1, ('ipod', 0.0): 3, ('hallyu', 0.0): 1, ('buuut', 0.0): 1, ('über', 0.0): 1, ('oki', 0.0): 2, ('8p', 0.0): 1, ('champagn', 0.0): 1, ('harlo', 0.0): 1, ('torrentialrain', 0.0): 1, ('lloyd', 0.0): 1, ('asshol', 0.0): 1, ('clearli', 0.0): 2, ('knowww', 0.0): 2, ('runni', 0.0): 1, ('sehun', 0.0): 1, ('sweater', 0.0): 1, ('intoler', 0.0): 2, ('xenophob', 0.0): 1, ('wtfff', 0.0): 1, ('tone', 0.0): 1, ('wasnt', 0.0): 1, ('1pm', 0.0): 2, ('fantasi', 0.0): 1, ('newer', 0.0): 1, ('pish', 0.0): 1, ('comparison', 0.0): 1, ('remast', 0.0): 1, ('fe14', 0.0): 1, ('icon', 0.0): 2, ('strawberri', 0.0): 1, ('loos', 0.0): 1, ('kapatidkongpogi', 0.0): 1, ('steph', 0.0): 1, ('mel', 0.0): 1, ('longest', 0.0): 1, ('carmen', 0.0): 1, ('login', 0.0): 1, ('respons', 0.0): 3, ('00128835', 0.0): 1, ('wingstop', 0.0): 1, ('budg', 0.0): 1, ('fuq', 0.0): 1, ('ilhoon', 0.0): 1, ('ganteng', 0.0): 1, ('simpl', 0.0): 1, ('getthescoop', 0.0): 1, ('hearess', 0.0): 1, ('677', 0.0): 1, ('txt_shot', 0.0): 1, ('standbi', 0.0): 1, ('inatal', 0.0): 1, ('zenmat', 0.0): 1, ('namecheck', 0.0): 1, ('whistl', 0.0): 1, ('junmyeon', 0.0): 1, ('ddi', 0.0): 1, ('arini', 0.0): 1, ('je', 0.0): 1, ('bright', 0.0): 2, ('igbo', 0.0): 1, ('blamehoney', 0.0): 1, ('whhr', 0.0): 1, ('juan', 0.0): 1, ('snuggl', 0.0): 1, ('internship', 0.0): 1, ('usag', 0.0): 1, ('warn', 0.0): 1, ('vertigo', 0.0): 1, ('panic', 0.0): 1, ('attack', 0.0): 4, ('dual', 0.0): 1, ('carriageway', 0.0): 1, ('aragalang', 0.0): 1, ('08', 0.0): 1, ('tam', 0.0): 1, ('bose', 0.0): 1, ('theo', 0.0): 1, ('anymoree', 0.0): 1, ('rubbish', 0.0): 1, ('cactu', 0.0): 1, ('sorrri', 0.0): 1, ('bowel', 0.0): 1, ('nasti', 0.0): 2, ('tumour', 0.0): 1, ('faster', 0.0): 1, ('puffi', 0.0): 1, ('eyelid', 0.0): 1, ('musica', 0.0): 1, ('dota', 0.0): 1, ('4am', 0.0): 1, ('campsit', 0.0): 1, ('miah', 0.0): 1, ('hahay', 0.0): 1, ('churro', 0.0): 1, ('montana', 0.0): 2, ('reign', 0.0): 1, ('exampl', 0.0): 1, ('inflat', 0.0): 1, ('sic', 0.0): 1, ('reset', 0.0): 1, ('entlerbountli', 0.0): 1, ('tinder', 0.0): 3, ('dirtykik', 0.0): 2, ('sexcam', 0.0): 3, ('spray', 0.0): 1, ('industri', 0.0): 1, ('swollen', 0.0): 1, ('distanc', 0.0): 2, ('jojo', 0.0): 1, ('postcod', 0.0): 1, ('kafi', 0.0): 1, ('din', 0.0): 1, ('mene', 0.0): 1, ('aj', 0.0): 1, ('koi', 0.0): 1, ('rewert', 0.0): 1, ('bunta', 0.0): 1, ('warnaaa', 0.0): 1, ('tortur', 0.0): 2, ('field', 0.0): 1, ('wall', 0.0): 2, ('iran', 0.0): 1, ('irand', 0.0): 1, ('us-iran', 0.0): 1, ('nuclear', 0.0): 1, (\"mit'\", 0.0): 1, ('expert', 0.0): 1, ('sever', 0.0): 3, ('li', 0.0): 1, ('s2e12', 0.0): 1, ('rumpi', 0.0): 1, ('gallon', 0.0): 1, ('ryan', 0.0): 1, ('secret', 0.0): 2, ('dandia', 0.0): 1, ('rbi', 0.0): 1, ('cage', 0.0): 2, ('parrot', 0.0): 1, ('1li', 0.0): 1, ('commiss', 0.0): 1, ('cag', 0.0): 1, ('stripe', 0.0): 2, ('gujarat', 0.0): 1, ('tear', 0.0): 3, ('ily.melani', 0.0): 1, ('unlik', 0.0): 2, ('talent', 0.0): 2, ('deepxcap', 0.0): 1, ('doin', 0.0): 3, ('5:08', 0.0): 1, ('thesi', 0.0): 11, ('belieb', 0.0): 2, ('gtg', 0.0): 1, ('compet', 0.0): 1, ('vv', 0.0): 1, ('respect', 0.0): 5, ('opt-out', 0.0): 1, ('vam', 0.0): 1, ('spece', 0.0): 1, ('ell', 0.0): 1, ('articl', 0.0): 1, ('sexyameli', 0.0): 1, ('fineandyu', 0.0): 1, ('gd', 0.0): 1, ('flesh', 0.0): 1, ('daft', 0.0): 1, ('imsorri', 0.0): 1, ('aku', 0.0): 1, ('chelsea', 0.0): 2, ('koe', 0.0): 1, ('emyu', 0.0): 1, ('confetti', 0.0): 1, ('bf', 0.0): 2, ('sini', 0.0): 1, ('dipoppo', 0.0): 1, ('hop', 0.0): 2, ('bestweekend', 0.0): 1, ('okay-ish', 0.0): 1, ('html', 0.0): 1, ('geneva', 0.0): 1, ('patml', 0.0): 1, ('482', 0.0): 1, ('orgasm', 0.0): 3, ('abouti', 0.0): 1, ('797', 0.0): 1, ('reaalli', 0.0): 1, ('aldub', 0.0): 1, ('nila', 0.0): 1, ('smart', 0.0): 1, ('meter', 0.0): 1, ('display', 0.0): 1, ('unansw', 0.0): 1, ('bri', 0.0): 1, ('magcon', 0.0): 1, ('sinuend', 0.0): 1, ('kak', 0.0): 1, ('laper', 0.0): 2, ('rage', 0.0): 1, ('loser', 0.0): 1, ('brendon', 0.0): 1, (\"urie'\", 0.0): 1, ('sumer', 0.0): 1, ('repackag', 0.0): 1, (\":'d\", 0.0): 1, ('matthew', 0.0): 1, ('yongb', 0.0): 1, ('sued', 0.0): 1, ('suprem', 0.0): 1, ('warm-up', 0.0): 1, ('arriv', 0.0): 4, ('brill', 0.0): 1, ('120', 0.0): 1, ('rub', 0.0): 1, ('belli', 0.0): 1, ('jannatul', 0.0): 1, ('ferdou', 0.0): 1, ('ekta', 0.0): 1, ('kharap', 0.0): 1, ('manush', 0.0): 1, ('mart', 0.0): 2, ('gua', 0.0): 1, ('can', 0.0): 1, (\"khloe'\", 0.0): 1, ('nhe', 0.0): 1, ('yar', 0.0): 1, ('minkyuk', 0.0): 1, ('hol', 0.0): 1, ('isol', 0.0): 1, ('hk', 0.0): 1, ('sensor', 0.0): 1, ('broker', 0.0): 1, ('wna', 0.0): 1, ('flaviana', 0.0): 1, ('chickmt', 0.0): 1, ('123', 0.0): 1, ('letsfootbal', 0.0): 2, ('atk', 0.0): 2, ('greymind', 0.0): 2, ('43', 0.0): 2, ('gayl', 0.0): 2, ('cricket', 0.0): 3, ('2-3', 0.0): 2, ('mood-dump', 0.0): 1, ('livestream', 0.0): 1, ('gotten', 0.0): 1, ('felton', 0.0): 1, ('veriti', 0.0): 1, (\"standen'\", 0.0): 1, ('shortli', 0.0): 1, ('😆', 0.0): 2, ('takoyaki', 0.0): 1, ('piti', 0.0): 1, ('aisyah', 0.0): 1, ('ffvi', 0.0): 1, ('youtu.be/2_gpctsojkw', 0.0): 1, ('donutsss', 0.0): 1, ('50p', 0.0): 1, ('grate', 0.0): 1, ('spars', 0.0): 1, ('dd', 0.0): 1, ('lagi', 0.0): 1, ('rider', 0.0): 1, ('pride', 0.0): 1, ('hueee', 0.0): 1, ('password', 0.0): 1, ('thingi', 0.0): 1, ('georg', 0.0): 1, ('afraid', 0.0): 2, ('chew', 0.0): 2, ('toy', 0.0): 1, ('stella', 0.0): 1, ('threw', 0.0): 2, ('theaccidentalcoupl', 0.0): 1, ('smooth', 0.0): 1, ('handov', 0.0): 1, ('spick', 0.0): 1, ('bebii', 0.0): 1, ('happenend', 0.0): 1, ('dr', 0.0): 1, ('balm', 0.0): 1, ('hmph', 0.0): 1, ('bubba', 0.0): 2, ('floor', 0.0): 3, ('georgi', 0.0): 1, ('oi', 0.0): 1, ('bengali', 0.0): 1, ('masterchef', 0.0): 1, ('whatchya', 0.0): 1, ('petrol', 0.0): 1, ('diesel', 0.0): 1, ('wardrob', 0.0): 1, ('awe', 0.0): 1, ('cock', 0.0): 1, ('nyquil', 0.0): 1, ('poootek', 0.0): 1, ('1,500', 0.0): 1, ('bobbl', 0.0): 1, ('leak', 0.0): 1, ('thermo', 0.0): 1, ('classic', 0.0): 1, ('ti5', 0.0): 1, ('12th', 0.0): 1, ('skate', 0.0): 1, ('tae', 0.0): 1, ('kita', 0.0): 4, ('ia', 0.0): 1, ('pkwalasawa', 0.0): 1, ('india', 0.0): 1, ('corrupt', 0.0): 2, ('access', 0.0): 2, ('anything.sur', 0.0): 1, ('info', 0.0): 6, ('octob', 0.0): 1, ('mubank', 0.0): 2, ('ene', 0.0): 2, ('3k', 0.0): 1, ('zehr', 0.0): 1, ('khani', 0.0): 1, ('groceri', 0.0): 1, ('hubba', 0.0): 1, ('bubbl', 0.0): 1, ('gum', 0.0): 2, ('closet', 0.0): 1, ('jhalak', 0.0): 1, ('. ..', 0.0): 2, ('bakwa', 0.0): 1, ('. ...', 0.0): 1, ('seehiah', 0.0): 1, ('goy', 0.0): 1, ('nacho', 0.0): 1, ('braid', 0.0): 2, ('initi', 0.0): 1, ('ruth', 0.0): 1, ('boong', 0.0): 1, ('recommend', 0.0): 3, ('gta', 0.0): 1, ('cwnt', 0.0): 1, ('trivia', 0.0): 1, ('belat', 0.0): 1, ('rohingya', 0.0): 1, ('muslim', 0.0): 2, ('indict', 0.0): 1, ('traffick', 0.0): 1, ('thailand', 0.0): 1, ('asia', 0.0): 1, ('rumbl', 0.0): 1, ('kumbl', 0.0): 1, ('scold', 0.0): 1, ('phrase', 0.0): 1, ('includ', 0.0): 1, ('tag', 0.0): 2, ('melt', 0.0): 1, ('tfw', 0.0): 1, ('jest', 0.0): 1, ('offend', 0.0): 2, ('sleepingwithsiren', 0.0): 1, ('17th', 0.0): 1, ('bringmethehorizon', 0.0): 1, ('18th', 0.0): 2, ('carva', 0.0): 1, ('regularli', 0.0): 2, ('sympathi', 0.0): 1, ('revamp', 0.0): 1, ('headphon', 0.0): 1, ('cunt', 0.0): 1, ('wacha', 0.0): 1, ('niend', 0.0): 1, ('bravo', 0.0): 1, ('2hr', 0.0): 1, ('13m', 0.0): 1, ('kk', 0.0): 2, ('calibraksaep', 0.0): 2, ('darlin', 0.0): 1, ('stun', 0.0): 1, (\"doedn't\", 0.0): 1, ('meaning', 0.0): 1, ('horrif', 0.0): 2, ('scoup', 0.0): 2, ('paypal', 0.0): 3, ('sweedi', 0.0): 1, ('nam', 0.0): 1, (\"sacconejoly'\", 0.0): 1, ('bethesda', 0.0): 1, ('fallout', 0.0): 1, ('minecon', 0.0): 1, ('perfect', 0.0): 2, ('katee', 0.0): 1, ('iloveyouu', 0.0): 1, ('linux', 0.0): 1, ('nawww', 0.0): 1, ('chikka', 0.0): 1, ('ug', 0.0): 1, ('rata', 0.0): 1, ('soonest', 0.0): 1, ('mwamwa', 0.0): 1, ('faggot', 0.0): 1, ('doubt', 0.0): 2, ('fyi', 0.0): 1, ('profil', 0.0): 1, ('nicest', 0.0): 1, ('mehendi', 0.0): 1, ('dash', 0.0): 1, ('bookmark', 0.0): 1, ('whay', 0.0): 1, ('shaa', 0.0): 1, ('prami', 0.0): 1, ('😚', 0.0): 4, ('ngee', 0.0): 1, ('ann', 0.0): 1, ('crikey', 0.0): 2, ('snit', 0.0): 1, ('nathanielhinanakit', 0.0): 1, ('naya', 0.0): 1, ('spinni', 0.0): 1, ('wheel', 0.0): 2, ('albeit', 0.0): 1, ('athlet', 0.0): 1, ('gfriend', 0.0): 2, ('yung', 0.0): 2, ('fugli', 0.0): 1, ('💞', 0.0): 4, ('jongda', 0.0): 1, ('hardli', 0.0): 2, ('tlist', 0.0): 1, ('budget', 0.0): 1, ('pabebegirl', 0.0): 1, ('pabeb', 0.0): 2, ('alter', 0.0): 1, ('sandra', 0.0): 2, ('bland', 0.0): 2, ('storifi', 0.0): 1, ('abbi', 0.0): 2, ('mtvhottest', 0.0): 1, ('gaga', 0.0): 1, ('rib', 0.0): 1, ('😵', 0.0): 1, ('hulkamania', 0.0): 1, ('unlov', 0.0): 1, ('lazi', 0.0): 3, ('ihhh', 0.0): 1, ('stackar', 0.0): 1, ('basil', 0.0): 1, ('remedi', 0.0): 1, ('ov', 0.0): 2, ('raiz', 0.0): 1, ('nvr', 0.0): 1, ('gv', 0.0): 1, ('up.wt', 0.0): 1, ('wt', 0.0): 1, ('imran', 0.0): 2, ('achiev', 0.0): 1, ('thr', 0.0): 1, ('soln', 0.0): 1, (\"sister'\", 0.0): 1, ('hong', 0.0): 1, ('kong', 0.0): 1, ('31st', 0.0): 1, ('pipe', 0.0): 1, ('sept', 0.0): 2, ('lawn', 0.0): 1, (\"cupid'\", 0.0): 1, ('torn', 0.0): 1, ('retain', 0.0): 1, ('clown', 0.0): 2, ('lipstick', 0.0): 1, ('haiss', 0.0): 1, ('todayi', 0.0): 1, ('thoo', 0.0): 1, ('everday', 0.0): 1, ('hangout', 0.0): 2, ('steven', 0.0): 2, ('william', 0.0): 1, ('umboh', 0.0): 1, ('goodafternoon', 0.0): 1, ('jadin', 0.0): 1, ('thiz', 0.0): 1, ('iz', 0.0): 1, ('emeg', 0.0): 1, ('kennat', 0.0): 1, ('reunit', 0.0): 1, ('abi', 0.0): 1, ('arctic', 0.0): 1, ('chicsirif', 0.0): 1, ('structur', 0.0): 1, ('cumbia', 0.0): 1, ('correct', 0.0): 1, ('badlif', 0.0): 1, ('4-5', 0.0): 2, ('kaslkdja', 0.0): 1, ('3wk', 0.0): 1, ('flower', 0.0): 1, ('feverfew', 0.0): 1, ('weddingflow', 0.0): 1, ('diyflow', 0.0): 1, ('fitn', 0.0): 1, ('worth', 0.0): 4, ('wolverin', 0.0): 1, ('khan', 0.0): 1, ('innoc', 0.0): 1, ('🙏🏻', 0.0): 1, ('🎂', 0.0): 2, ('memem', 0.0): 2, ('krystoria', 0.0): 1, ('snob', 0.0): 1, ('zumba', 0.0): 1, ('greekcrisi', 0.0): 1, ('remain', 0.0): 1, ('dutch', 0.0): 1, ('legibl', 0.0): 2, ('isra', 0.0): 1, ('passport', 0.0): 1, ('froze', 0.0): 1, ('theori', 0.0): 1, ('23rd', 0.0): 1, ('24th', 0.0): 1, ('stomachach', 0.0): 1, ('slice', 0.0): 1, ('ཀ', 0.0): 1, ('again', 0.0): 1, ('otani', 0.0): 1, ('3-0', 0.0): 1, ('3rd', 0.0): 3, ('bottom', 0.0): 2, ('niaaa', 0.0): 1, ('2/4', 0.0): 1, ('scheme', 0.0): 2, ('fckin', 0.0): 1, ('hii', 0.0): 1, ('vin', 0.0): 1, ('plss', 0.0): 1, ('rpli', 0.0): 1, ('rat', 0.0): 3, ('bollywood', 0.0): 1, ('mac', 0.0): 1, ('backup', 0.0): 2, ('lune', 0.0): 1, ('robinhood', 0.0): 1, ('robinhoodi', 0.0): 1, ('🚙', 0.0): 1, ('💚', 0.0): 1, ('docopenhagen', 0.0): 1, ('setter', 0.0): 1, ('swipe', 0.0): 1, ('bbygurl', 0.0): 1, ('neil', 0.0): 1, ('caribbean', 0.0): 1, ('6yr', 0.0): 1, ('jabongatpumaurbanstamped', 0.0): 2, ('takraw', 0.0): 1, ('fersure', 0.0): 1, ('angi', 0.0): 1, ('sheriff', 0.0): 1, ('aaag', 0.0): 1, (\"i'mo\", 0.0): 1, ('sulk', 0.0): 1, ('selfish', 0.0): 1, ('trick', 0.0): 2, ('nonc', 0.0): 1, ('pad', 0.0): 1, ('bison', 0.0): 1, ('motiv', 0.0): 2, (\"q'don\", 0.0): 1, ('cheat', 0.0): 2, ('stomp', 0.0): 1, ('aaaaaaaaah', 0.0): 1, ('kany', 0.0): 1, ('mama', 0.0): 1, ('jdjdjdjd', 0.0): 1, (\"jimin'\", 0.0): 1, ('fancaf', 0.0): 1, ('waffl', 0.0): 1, ('87.7', 0.0): 1, ('2fm', 0.0): 1, ('himseek', 0.0): 1, ('kissm', 0.0): 1, ('akua', 0.0): 1, ('glo', 0.0): 1, ('cori', 0.0): 1, ('monteith', 0.0): 1, ('often', 0.0): 1, ('hashbrown', 0.0): 1, ('💘', 0.0): 2, ('pg', 0.0): 1, ('msc', 0.0): 1, ('hierro', 0.0): 1, ('shirleycam', 0.0): 1, ('phonesex', 0.0): 2, ('pal', 0.0): 1, ('111', 0.0): 1, ('gilet', 0.0): 1, ('cheek', 0.0): 1, ('squishi', 0.0): 1, ('lahhh', 0.0): 1, ('eon', 0.0): 1, ('sunris', 0.0): 1, ('beeti', 0.0): 1, ('697', 0.0): 1, ('kikkomansabor', 0.0): 1, ('getaway', 0.0): 1, ('crimin', 0.0): 1, ('amiibo', 0.0): 1, ('batman', 0.0): 1, ('habe', 0.0): 1, ('siannn', 0.0): 1, ('march', 0.0): 1, ('2017', 0.0): 1, ('chuckin', 0.0): 1, ('ampsha', 0.0): 1, ('nia', 0.0): 1, ('strap', 0.0): 1, ('dz9055', 0.0): 1, ('entlead', 0.0): 1, ('590', 0.0): 1, ('twice', 0.0): 5, ('07:02', 0.0): 1, ('ifsc', 0.0): 1, ('mayor', 0.0): 1, ('biodivers', 0.0): 1, ('taxonom', 0.0): 1, ('collabor', 0.0): 1, ('speci', 0.0): 1, ('discoveri', 0.0): 1, ('collar', 0.0): 1, ('3:03', 0.0): 1, ('belt', 0.0): 1, ('smith', 0.0): 2, ('eyelin', 0.0): 1, ('therefor', 0.0): 1, ('netherland', 0.0): 1, ('el', 0.0): 1, ('jeb', 0.0): 1, ('blacklivesmatt', 0.0): 1, ('slogan', 0.0): 1, ('msnbc', 0.0): 1, ('jebbush', 0.0): 1, ('famish', 0.0): 1, ('marino', 0.0): 1, ('qualifi', 0.0): 2, ('suzi', 0.0): 1, ('skirt', 0.0): 1, ('tama', 0.0): 1, ('warrior', 0.0): 2, ('wound', 0.0): 1, ('iraq', 0.0): 1, ('be', 0.0): 2, ('camara', 0.0): 1, ('coveral', 0.0): 1, ('happili', 0.0): 1, ('sneezi', 0.0): 1, ('rogerwatch', 0.0): 1, ('stalker', 0.0): 1, ('velvet', 0.0): 1, ('tradit', 0.0): 1, (\"people'\", 0.0): 1, ('beheaviour', 0.0): 1, (\"robert'\", 0.0): 1, ('.\\n.', 0.0): 2, ('aaron', 0.0): 1, ('jelous', 0.0): 1, ('mtg', 0.0): 1, ('thoughtseiz', 0.0): 1, ('playabl', 0.0): 1, ('oldi', 0.0): 1, ('goodi', 0.0): 1, ('mcg', 0.0): 1, ('inspirit', 0.0): 1, ('shine', 0.0): 1, ('ise', 0.0): 1, ('assum', 0.0): 2, ('waist', 0.0): 2, ('guin', 0.0): 1, ('venu', 0.0): 1, ('evil', 0.0): 1, ('pepper', 0.0): 1, ('thessidew', 0.0): 1, ('877', 0.0): 1, ('genesi', 0.0): 1, ('mexico', 0.0): 2, ('novemb', 0.0): 1, ('mash', 0.0): 1, ('whattsap', 0.0): 1, ('inuyasha', 0.0): 2, ('outfwith', 0.0): 1, ('myungsoo', 0.0): 1, ('organis', 0.0): 1, ('satisfi', 0.0): 1, ('wah', 0.0): 1, ('challo', 0.0): 1, ('pliss', 0.0): 1, ('juliana', 0.0): 1, ('enrol', 0.0): 1, ('darlen', 0.0): 1, ('emoji', 0.0): 2, ('brisban', 0.0): 1, ('merlin', 0.0): 1, ('nawwwe', 0.0): 1, ('hyperbulli', 0.0): 1, ('tong', 0.0): 1, ('nga', 0.0): 1, ('seatmat', 0.0): 1, ('rajud', 0.0): 1, ('barkada', 0.0): 1, ('ore', 0.0): 1, ('kayla', 0.0): 1, ('ericavan', 0.0): 1, ('jong', 0.0): 1, ('dongwoo', 0.0): 1, ('photocard', 0.0): 1, ('wh', 0.0): 1, ('dw', 0.0): 1, ('tumor', 0.0): 1, ('vivian', 0.0): 1, ('mmsmalubhangsakit', 0.0): 1, ('jillcruz', 0.0): 2, ('lgbt', 0.0): 3, ('qt', 0.0): 1, ('19th', 0.0): 1, ('toss', 0.0): 1, ('co-work', 0.0): 1, ('mia', 0.0): 1, ('push', 0.0): 4, ('dare', 0.0): 2, ('unsettl', 0.0): 1, ('gh', 0.0): 1, ('18c', 0.0): 1, ('rlli', 0.0): 2, ('hamster', 0.0): 2, ('sheeran', 0.0): 2, ('preform', 0.0): 2, ('monash', 0.0): 1, ('hitmark', 0.0): 1, ('glitch', 0.0): 1, ('safaa', 0.0): 1, (\"selena'\", 0.0): 1, ('galat', 0.0): 1, ('tum', 0.0): 1, ('ab', 0.0): 5, ('non', 0.0): 1, ('lrka', 0.0): 1, ('bna', 0.0): 1, ('kia', 0.0): 1, ('bhook', 0.0): 1, ('jai', 0.0): 1, ('social', 0.0): 2, ('afterschool', 0.0): 1, ('bilal', 0.0): 1, ('ashraf', 0.0): 1, ('icu', 0.0): 1, ('thanksss', 0.0): 1, ('annnd', 0.0): 1, ('winchest', 0.0): 1, ('{:', 0.0): 1, ('grepe', 0.0): 1, ('grepein', 0.0): 1, ('panem', 0.0): 1, ('lover', 0.0): 1, ('sulli', 0.0): 1, ('cpm', 0.0): 1, ('condemn', 0.0): 1, ('✔', 0.0): 1, ('occur', 0.0): 1, ('unagi', 0.0): 1, ('7elw', 0.0): 1, ('mesh', 0.0): 1, ('beyt', 0.0): 1, ('3a2ad', 0.0): 1, ('fluent', 0.0): 1, ('varsiti', 0.0): 1, ('sengenza', 0.0): 1, ('context', 0.0): 1, ('movnat', 0.0): 1, ('yield', 0.0): 1, ('nbhero', 0.0): 1, (\"it'd\", 0.0): 1, ('background', 0.0): 1, ('agov', 0.0): 1, ('brasileirao', 0.0): 2, ('abus', 0.0): 1, ('unpar', 0.0): 1, ('bianca', 0.0): 1, ('bun', 0.0): 1, ('dislik', 0.0): 1, ('burdensom', 0.0): 1, ('clear', 0.0): 2, ('amelia', 0.0): 1, ('melon', 0.0): 2, ('useless', 0.0): 1, ('soccer', 0.0): 2, ('interview', 0.0): 2, ('thursday', 0.0): 1, ('nevermind', 0.0): 1, ('jeon', 0.0): 1, ('claw', 0.0): 1, ('thigh', 0.0): 2, ('traction', 0.0): 1, ('damnit', 0.0): 1, ('pri', 0.0): 1, ('pv', 0.0): 2, ('reliv', 0.0): 1, ('nyc', 0.0): 2, ('klm', 0.0): 1, ('11am', 0.0): 1, (\"mcd'\", 0.0): 1, ('hung', 0.0): 1, ('bam', 0.0): 1, ('seventh', 0.0): 1, ('splendour', 0.0): 1, ('swedish', 0.0): 1, ('metal', 0.0): 1, ('häirførc', 0.0): 1, ('givecodpieceach', 0.0): 1, ('alic', 0.0): 3, ('stile', 0.0): 1, ('explain', 0.0): 3, ('ili', 0.0): 1, ('pragu', 0.0): 1, ('sadi', 0.0): 1, ('charact', 0.0): 1, ('915', 0.0): 1, ('hayee', 0.0): 2, ('patwari', 0.0): 1, ('mam', 0.0): 1, (\"ik'\", 0.0): 1, ('vision', 0.0): 2, ('ga', 0.0): 1, ('awhhh', 0.0): 1, ('nalang', 0.0): 1, ('hehe', 0.0): 1, ('albanian', 0.0): 1, ('curs', 0.0): 2, ('tava', 0.0): 1, ('chara', 0.0): 1, ('teteh', 0.0): 1, ('verri', 0.0): 1, ('shatter', 0.0): 2, ('sb', 0.0): 1, ('nawe', 0.0): 1, ('bulldog', 0.0): 1, ('macho', 0.0): 1, ('puriti', 0.0): 1, ('kwento', 0.0): 1, ('nakakapikon', 0.0): 1, ('nagbabasa', 0.0): 1, ('blog', 0.0): 2, ('cancer', 0.0): 1, (':-\\\\', 0.0): 1, ('jonatha', 0.0): 4, ('beti', 0.0): 4, ('sogok', 0.0): 1, ('premium', 0.0): 2, ('instrument', 0.0): 1, ('howev', 0.0): 1, ('dastardli', 0.0): 1, ('swine', 0.0): 1, ('envelop', 0.0): 1, ('pipol', 0.0): 1, ('tad', 0.0): 1, ('wiper', 0.0): 2, ('supposedli', 0.0): 1, ('kernel', 0.0): 1, ('intel', 0.0): 1, ('mega', 0.0): 1, ('bent', 0.0): 1, ('socket', 0.0): 1, ('pcgame', 0.0): 1, ('pcupgrad', 0.0): 1, ('brainwash', 0.0): 2, ('smosh', 0.0): 1, ('plawnew', 0.0): 1, ('837', 0.0): 1, ('aswel', 0.0): 1, ('litter', 0.0): 1, ('mensch', 0.0): 1, ('sepanx', 0.0): 1, ('pci', 0.0): 1, ('caerphilli', 0.0): 1, ('omw', 0.0): 1, ('😍', 0.0): 1, ('hahdhdhshh', 0.0): 1, ('growinguppoor', 0.0): 1, ('🇺🇸', 0.0): 2, (\"bangtan'\", 0.0): 1, ('taimoor', 0.0): 1, ('meray', 0.0): 1, ('dost', 0.0): 1, ('tya', 0.0): 1, ('refollow', 0.0): 1, ('dumb', 0.0): 2, ('butt', 0.0): 1, ('pissbabi', 0.0): 1, ('plank', 0.0): 1, ('inconsist', 0.0): 1, ('moor', 0.0): 1, ('bin', 0.0): 1, ('osx', 0.0): 1, ('chrome', 0.0): 1, ('voiceov', 0.0): 1, ('devo', 0.0): 1, ('hulkhogan', 0.0): 1, ('unpleas', 0.0): 1, ('daaamn', 0.0): 1, ('dada', 0.0): 1, ('fulli', 0.0): 1, ('spike', 0.0): 1, (\"panic'\", 0.0): 1, ('22nd', 0.0): 1, ('south', 0.0): 2, ('africa', 0.0): 2, ('190', 0.0): 2, ('lizardz', 0.0): 1, ('deepli', 0.0): 1, ('emerg', 0.0): 1, ('engin', 0.0): 1, ('dormtel', 0.0): 1, ('scho', 0.0): 1, ('siya', 0.0): 1, ('onee', 0.0): 1, ('carri', 0.0): 1, ('7pm', 0.0): 1, ('feta', 0.0): 1, ('blaaaz', 0.0): 1, ('nausea', 0.0): 1, ('awar', 0.0): 1, ('top-up', 0.0): 1, ('sharknado', 0.0): 1, ('erni', 0.0): 1, ('ezoo', 0.0): 1, ('lilybutl', 0.0): 1, ('seduc', 0.0): 2, ('powai', 0.0): 1, ('neighbor', 0.0): 1, ('delhi', 0.0): 1, ('unsaf', 0.0): 1, ('halo', 0.0): 1, ('fred', 0.0): 1, ('gaon', 0.0): 1, ('infnt', 0.0): 1, ('elig', 0.0): 1, ('acub', 0.0): 1, (\"why'd\", 0.0): 1, ('bullshit', 0.0): 2, ('hanaaa', 0.0): 1, ('jn', 0.0): 1, ('tau', 0.0): 1, ('basta', 0.0): 1, ('sext', 0.0): 1, ('addm', 0.0): 1, ('hotmusicdeloco', 0.0): 2, ('dhi', 0.0): 1, ('👉', 0.0): 1, ('8ball', 0.0): 1, ('fakmarey', 0.0): 1, ('doo', 0.0): 2, ('six', 0.0): 3, ('flag', 0.0): 1, ('fulltim', 0.0): 1, ('awkward', 0.0): 1, ('beet', 0.0): 1, ('juic', 0.0): 1, ('dci', 0.0): 1, ('granddad', 0.0): 1, ('minion', 0.0): 3, ('bucket', 0.0): 1, ('kapan', 0.0): 1, ('udah', 0.0): 1, ('dihapu', 0.0): 1, ('hilang', 0.0): 1, ('dari', 0.0): 1, ('muka', 0.0): 1, ('bumi', 0.0): 1, ('narrow', 0.0): 1, ('gona', 0.0): 2, ('chello', 0.0): 1, ('gate', 0.0): 1, ('guard', 0.0): 1, ('crepe', 0.0): 1, ('forsaken', 0.0): 1, ('kanin', 0.0): 1, ('hypixel', 0.0): 1, ('grrr', 0.0): 1, ('thestruggleisr', 0.0): 1, ('geek', 0.0): 1, ('gamer', 0.0): 2, ('afterbirth', 0.0): 1, (\"apink'\", 0.0): 1, ('overperhatian', 0.0): 1, ('son', 0.0): 1, ('pox', 0.0): 1, ('ahm', 0.0): 1, ('karli', 0.0): 1, ('kloss', 0.0): 1, ('goofi', 0.0): 1, ('pcd', 0.0): 1, ('antagonis', 0.0): 1, ('writer', 0.0): 1, ('nudg', 0.0): 1, ('delv', 0.0): 1, ('grandad', 0.0): 1, (\"gray'\", 0.0): 1, ('followk', 0.0): 1, ('suggest', 0.0): 2, ('pace', 0.0): 1, ('maker', 0.0): 1, ('molli', 0.0): 1, ('higher', 0.0): 1, ('ceremoni', 0.0): 1, ('christin', 0.0): 1, ('moodi', 0.0): 1, ('throwback', 0.0): 1, ('fav', 0.0): 3, ('barb', 0.0): 1, ('creasi', 0.0): 1, ('deputi', 0.0): 1, ('tast', 0.0): 1, (\"banana'\", 0.0): 1, ('saludo', 0.0): 1, ('dissapoint', 0.0): 1, ('😫', 0.0): 1, ('&lt;--', 0.0): 1, (\"bae'\", 0.0): 1, ('pimpl', 0.0): 2, ('amount', 0.0): 2, ('tdi', 0.0): 1, ('pamela', 0.0): 1, ('mini', 0.0): 1, ('mast', 0.0): 1, ('intermitt', 0.0): 1, ('servic', 0.0): 3, ('janniecam', 0.0): 1, ('musicbiz', 0.0): 1, ('braxton', 0.0): 1, ('pro', 0.0): 2, ('urban', 0.0): 1, ('unpreced', 0.0): 1, ('tebow', 0.0): 1, ('okaaay', 0.0): 1, ('sayanggg', 0.0): 1, ('housework', 0.0): 1, ('bust', 0.0): 2, ('disneyland', 0.0): 1, ('thoma', 0.0): 1, ('tommyy', 0.0): 1, ('billi', 0.0): 1, ('kevin', 0.0): 1, ('clifton', 0.0): 1, ('strictli', 0.0): 1, ('nsc', 0.0): 1, ('mat', 0.0): 1, ('0', 0.0): 1, ('awhh', 0.0): 1, ('ram', 0.0): 2, ('voucher', 0.0): 1, ('smadvow', 0.0): 1, ('544', 0.0): 1, ('acdc', 0.0): 1, ('aker', 0.0): 1, ('gmail', 0.0): 1, ('sprevelink', 0.0): 1, ('633', 0.0): 1, ('lana', 0.0): 2, ('loveyoutilltheendcart', 0.0): 1, ('sfv', 0.0): 1, ('6/7', 0.0): 1, ('winner', 0.0): 1, ('20/1', 0.0): 1, ('david', 0.0): 1, ('rosi', 0.0): 1, ('hayoung', 0.0): 1, ('nlb', 0.0): 1, ('@_', 0.0): 1, ('tayo', 0.0): 1, ('forth', 0.0): 1, ('suspect', 0.0): 1, ('mening', 0.0): 1, ('viral', 0.0): 1, ('tonsil', 0.0): 1, ('😷', 0.0): 1, ('😝', 0.0): 1, ('babyy', 0.0): 2, ('cushion', 0.0): 1, ('😿', 0.0): 1, ('💓', 0.0): 2, ('weigh', 0.0): 1, ('keen', 0.0): 1, ('petrofac', 0.0): 1, (';-)', 0.0): 1, ('wig', 0.0): 1, (\"mark'\", 0.0): 1, ('pathet', 0.0): 1, ('burden.say', 0.0): 1, ('itchi', 0.0): 1, ('cheaper', 0.0): 1, ('malaysia', 0.0): 1, ('130', 0.0): 1, ('snapchattimg', 0.0): 1, ('😏', 0.0): 4, ('sin', 0.0): 1, ('lor', 0.0): 1, ('dedic', 0.0): 1, ('worriedli', 0.0): 1, ('stare', 0.0): 1, ('toneadi', 0.0): 1, ('46532', 0.0): 1, ('snapdirti', 0.0): 1, ('sheskindahot', 0.0): 1, ('corps', 0.0): 1, ('taeni', 0.0): 1, ('fyeah', 0.0): 1, ('andromeda', 0.0): 1, ('yunni', 0.0): 1, ('whdjwksja', 0.0): 1, ('ziam', 0.0): 1, ('100k', 0.0): 1, ('spoil', 0.0): 1, ('curtain', 0.0): 1, ('watchabl', 0.0): 1, ('migrin', 0.0): 1, ('gdce', 0.0): 1, ('gamescom', 0.0): 1, (\"do't\", 0.0): 1, ('parcel', 0.0): 1, ('num', 0.0): 1, ('oooouch', 0.0): 1, ('pinki', 0.0): 1, ('👣', 0.0): 1, ('podiatrist', 0.0): 1, ('gusto', 0.0): 1, (\"rodic'\", 0.0): 1, (\"one'\", 0.0): 1, ('adoohh', 0.0): 1, ('b-butt', 0.0): 1, ('tigermilk', 0.0): 1, ('east', 0.0): 1, ('dulwich', 0.0): 1, ('intens', 0.0): 1, ('kagami', 0.0): 1, ('kuroko', 0.0): 1, ('sana', 0.0): 2, ('makita', 0.0): 1, ('spooki', 0.0): 1, ('smol', 0.0): 1, ('bean', 0.0): 1, ('fagan', 0.0): 1, ('meadowhal', 0.0): 1, ('lola', 0.0): 1, ('nadalaw', 0.0): 1, ('labyu', 0.0): 1, ('jot', 0.0): 1, ('ivypowel', 0.0): 1, ('homeslic', 0.0): 1, ('33', 0.0): 2, ('emoticon', 0.0): 2, ('eyebrow', 0.0): 1, ('prettylook', 0.0): 1, ('whitney', 0.0): 1, ('houston', 0.0): 1, ('aur', 0.0): 1, ('shamil', 0.0): 1, ('tonn', 0.0): 1, ('statu', 0.0): 1, ('→', 0.0): 1, ('suddenli', 0.0): 2, ('alli', 0.0): 2, ('wrap', 0.0): 1, ('neck', 0.0): 1, ('heartbroken', 0.0): 1, ('chover', 0.0): 1, ('cebu', 0.0): 1, ('lechon', 0.0): 1, ('kitten', 0.0): 2, ('jannygreen', 0.0): 2, ('suicid', 0.0): 2, ('forgiv', 0.0): 1, ('conno', 0.0): 1, ('brooo', 0.0): 1, ('rout', 0.0): 1, ('lovebox', 0.0): 1, ('prod', 0.0): 1, ('osad', 0.0): 1, ('scam', 0.0): 1, ('itb', 0.0): 1, ('omigod', 0.0): 1, ('ehem', 0.0): 1, ('ala', 0.0): 1, ('yeke', 0.0): 1, ('jumpa', 0.0): 1, ('😋', 0.0): 1, ('ape', 0.0): 1, ('1.2', 0.0): 1, ('map', 0.0): 1, ('namin', 0.0): 1, ('govt', 0.0): 1, ('e-petit', 0.0): 1, ('pretend', 0.0): 1, ('irk', 0.0): 1, ('ruess', 0.0): 1, ('program', 0.0): 1, ('aigoo', 0.0): 1, ('doujin', 0.0): 1, ('killua', 0.0): 1, ('ginggon', 0.0): 1, ('guys.al', 0.0): 1, ('ytd', 0.0): 1, ('pdapaghimok', 0.0): 1, ('flexibl', 0.0): 1, ('sheet', 0.0): 1, ('nanaman', 0.0): 1, ('pinay', 0.0): 1, ('pie', 0.0): 1, ('jadi', 0.0): 1, ('langsung', 0.0): 1, ('flasback', 0.0): 1, ('franc', 0.0): 1, (':|', 0.0): 1, ('lo', 0.0): 1, ('nicknam', 0.0): 1, ('involv', 0.0): 1, ('scrape', 0.0): 1, ('pile', 0.0): 1, ('sare', 0.0): 1, ('bandar', 0.0): 1, ('varg', 0.0): 1, ('hammer', 0.0): 1, ('lolo', 0.0): 1, ('xbsbabnb', 0.0): 1, ('stilll', 0.0): 1, ('apma', 0.0): 2, ('leadership', 0.0): 1, ('wakeupgop', 0.0): 1, ('mv', 0.0): 1, ('bull', 0.0): 1, ('trafficcc', 0.0): 1, ('oscar', 0.0): 1, ('pornographi', 0.0): 1, ('slutsham', 0.0): 1, ('ect', 0.0): 1, ('poland', 0.0): 1, ('faraway', 0.0): 1, ('700', 0.0): 1, ('800', 0.0): 1, ('cgi', 0.0): 1, ('pun', 0.0): 1, (\"x'\", 0.0): 1, ('osaka', 0.0): 1, ('junior', 0.0): 1, ('aytona', 0.0): 1, ('hala', 0.0): 1, ('mathird', 0.0): 1, ('jkjk', 0.0): 1, ('backtrack', 0.0): 1, ('util', 0.0): 1, ('pat', 0.0): 1, ('jay', 0.0): 2, ('broh', 0.0): 1, ('calll', 0.0): 1, ('icaru', 0.0): 1, ('awn', 0.0): 1, ('bach', 0.0): 1, ('court', 0.0): 1, ('landlord', 0.0): 1, (\"mp'\", 0.0): 1, ('dame', 0.0): 1, ('gossip', 0.0): 1, ('purpl', 0.0): 2, ('tie', 0.0): 1, ('ishii', 0.0): 1, ('clara', 0.0): 1, ('yile', 0.0): 1, ('whatev', 0.0): 1, ('stil', 0.0): 1, ('sidharth', 0.0): 1, ('ndabenhl', 0.0): 1, ('doggi', 0.0): 1, ('antag', 0.0): 1, ('41', 0.0): 1, ('thu', 0.0): 1, ('jenner', 0.0): 1, ('troubleshoot', 0.0): 1, (\"convo'\", 0.0): 1, ('dem', 0.0): 1, ('tix', 0.0): 2, ('automat', 0.0): 1, ('redirect', 0.0): 1, ('gigi', 0.0): 1, ('carter', 0.0): 1, ('corn', 0.0): 2, ('chip', 0.0): 2, ('nnnooo', 0.0): 1, ('cz', 0.0): 1, ('gorilla', 0.0): 1, ('hbm', 0.0): 1, ('humid', 0.0): 1, ('admir', 0.0): 1, ('consist', 0.0): 1, ('jason', 0.0): 1, (\"shackell'\", 0.0): 1, ('podcast', 0.0): 1, ('envi', 0.0): 1, ('twer', 0.0): 1, ('782', 0.0): 1, ('hahaahahahaha', 0.0): 1, ('sm1', 0.0): 1, ('mutil', 0.0): 1, ('robot', 0.0): 1, ('destroy', 0.0): 1, ('freakin', 0.0): 1, ('haestarr', 0.0): 1, ('😀', 0.0): 3, ('audio', 0.0): 1, ('snippet', 0.0): 1, ('brotherhood', 0.0): 1, ('mefd', 0.0): 1, ('diana', 0.0): 1, ('master', 0.0): 1, ('led', 0.0): 1, ('award', 0.0): 1, ('meowkd', 0.0): 1, ('complic', 0.0): 1, (\"c'mon\", 0.0): 1, (\"swimmer'\", 0.0): 1, ('leh', 0.0): 1, ('corner', 0.0): 1, ('didnot', 0.0): 1, ('usanel', 0.0): 2, ('nathan', 0.0): 1, ('micha', 0.0): 1, ('fave', 0.0): 2, ('creep', 0.0): 1, ('throughout', 0.0): 1, ('whose', 0.0): 1, ('ave', 0.0): 1, ('tripl', 0.0): 1, ('lectur', 0.0): 1, ('2-5', 0.0): 1, ('jaw', 0.0): 1, ('quarter', 0.0): 1, ('soni', 0.0): 1, ('followmeaaron', 0.0): 1, ('tzelumxoxo', 0.0): 1, ('drank', 0.0): 1, ('mew', 0.0): 1, ('indic', 0.0): 1, ('ouliv', 0.0): 1, ('70748', 0.0): 1, ('viernesderolenahot', 0.0): 1, ('longmorn', 0.0): 1, ('tobermori', 0.0): 1, ('32', 0.0): 1, ('tail', 0.0): 1, ('recuerda', 0.0): 1, ('tanto', 0.0): 1, ('bath', 0.0): 1, ('muna', 0.0): 1, ('await', 0.0): 1, ('urslef', 0.0): 1, ('lime', 0.0): 1, ('truckload', 0.0): 1, ('favour', 0.0): 2, ('spectat', 0.0): 1, ('sail', 0.0): 1, (\"w'end\", 0.0): 1, ('bbc', 0.0): 1, ('‘', 0.0): 1, ('foil', 0.0): 1, ('ac45', 0.0): 1, ('catamaran', 0.0): 1, ('peli', 0.0): 1, ('829', 0.0): 1, ('sextaatequemfimseguesdvcomvalentino', 0.0): 1, ('befor', 0.0): 1, ('valu', 0.0): 1, ('cinnamon', 0.0): 1, ('mtap', 0.0): 1, ('peng', 0.0): 1, ('frozen', 0.0): 1, ('bagu', 0.0): 1, ('emang', 0.0): 1, ('engg', 0.0): 1, ('cmc', 0.0): 1, ('mage', 0.0): 1, ('statement', 0.0): 1, ('moodsw', 0.0): 1, ('termin', 0.0): 1, ('men', 0.0): 1, ('peep', 0.0): 1, ('multipl', 0.0): 1, ('mef', 0.0): 1, ('rebound', 0.0): 1, ('pooor', 0.0): 1, ('2am', 0.0): 1, ('perpetu', 0.0): 1, ('bitchfac', 0.0): 1, ('clever', 0.0): 1, ('iceland', 0.0): 1, ('zayn_come_back_we_miss_y', 0.0): 1, ('pmsl', 0.0): 1, ('mianh', 0.0): 1, ('milkeu', 0.0): 1, ('lrt', 0.0): 1, ('bambam', 0.0): 1, ('soda', 0.0): 1, ('payback', 0.0): 1, ('87000', 0.0): 1, ('jobe', 0.0): 1, ('muchi', 0.0): 1, ('🎈', 0.0): 1, ('bathroom', 0.0): 1, ('lagg', 0.0): 1, ('banget', 0.0): 1, ('novel', 0.0): 1, (\"there'd\", 0.0): 1, ('invis', 0.0): 1, ('scuttl', 0.0): 1, ('worm', 0.0): 1, ('bauuukkk', 0.0): 1, ('jessica', 0.0): 1, ('5:15', 0.0): 1, ('argument', 0.0): 1, ('couldnt', 0.0): 2, ('yepp', 0.0): 1, ('😺', 0.0): 1, ('💒', 0.0): 1, ('💎', 0.0): 1, ('feelin', 0.0): 1, ('biscuit', 0.0): 1, ('slather', 0.0): 1, ('jsut', 0.0): 1, ('belov', 0.0): 1, ('grandmoth', 0.0): 1, ('princess', 0.0): 2, ('babee', 0.0): 1, ('demn', 0.0): 1, ('hotaisndonwyvauwjoqhsjsnaihsuswtf', 0.0): 1, ('sia', 0.0): 1, ('niram', 0.0): 1, ('geng', 0.0): 1, ('fikri', 0.0): 1, ('tirtagangga', 0.0): 1, ('char', 0.0): 1, ('font', 0.0): 2, ('riprishikeshwari', 0.0): 1, ('creamist', 0.0): 1, ('challeng', 0.0): 1, ('substitut', 0.0): 1, ('skin', 0.0): 1, ('cplt', 0.0): 1, ('cp', 0.0): 1, ('hannah', 0.0): 1, ('💙', 0.0): 1, ('💪', 0.0): 1, ('opu', 0.0): 1, ('inner', 0.0): 1, ('pleasur', 0.0): 1, ('bbq', 0.0): 1, ('lolliv', 0.0): 1, ('split', 0.0): 3, ('collat', 0.0): 2, ('spilt', 0.0): 2, ('quitkarwaoyaaro', 0.0): 1, ('deacti̇v', 0.0): 1, ('2.5', 0.0): 1, ('g2a', 0.0): 1, ('sherep', 0.0): 1, ('nemen', 0.0): 1, ('behey', 0.0): 1, ('motherfuck', 0.0): 1, ('tattoo', 0.0): 1, ('reec', 0.0): 1, ('vm', 0.0): 1, ('deth', 0.0): 2, ('lest', 0.0): 1, ('gp', 0.0): 1, ('departur', 0.0): 1, ('wipe', 0.0): 1, ('yuck', 0.0): 1, ('ystrday', 0.0): 1, ('seolhyun', 0.0): 1, ('drama', 0.0): 1, ('spici', 0.0): 1, ('owl', 0.0): 1, ('mumbai', 0.0): 1, (\"pj'\", 0.0): 1, ('wallpap', 0.0): 1, ('cba', 0.0): 1, ('hotter', 0.0): 1, ('rec', 0.0): 1, ('gotdamn', 0.0): 1, ('baaack', 0.0): 1, ('honest', 0.0): 1, ('srw', 0.0): 1, ('mobag', 0.0): 1, ('dunno', 0.0): 1, ('stroke', 0.0): 1, ('gnr', 0.0): 1, ('backstag', 0.0): 1, ('slash', 0.0): 1, ('prolli', 0.0): 1, ('bunni', 0.0): 1, ('sooner', 0.0): 1, ('analyst', 0.0): 1, ('expedia', 0.0): 1, ('bellevu', 0.0): 1, ('prison', 0.0): 1, ('alcohol', 0.0): 1, ('huhuh', 0.0): 1, ('heartburn', 0.0): 1, ('awalmu', 0.0): 1, ('njareeem', 0.0): 1, ('maggi', 0.0): 1, ('psycho', 0.0): 1, ('wahhh', 0.0): 1, ('abudhabi', 0.0): 1, ('hiby', 0.0): 1, ('shareyoursumm', 0.0): 1, ('b8', 0.0): 1, ('must.b', 0.0): 1, ('dairi', 0.0): 1, ('produxt', 0.0): 1, ('lactos', 0.0): 2, ('midland', 0.0): 1, ('knacker', 0.0): 1, ('footag', 0.0): 1, ('lifeless', 0.0): 1, ('shell', 0.0): 1, ('44', 0.0): 1, ('7782', 0.0): 1, ('pengen', 0.0): 1, ('girlll', 0.0): 1, ('tsunami', 0.0): 1, ('indi', 0.0): 1, ('nick', 0.0): 1, ('tirad', 0.0): 1, ('stoop', 0.0): 1, ('lower', 0.0): 1, ('role', 0.0): 1, ('thunder', 0.0): 1, ('paradis', 0.0): 1, ('habit', 0.0): 1, ('facad', 0.0): 1, ('democraci', 0.0): 1, ('brat', 0.0): 1, ('tb', 0.0): 1, (\"o'\", 0.0): 1, ('bade', 0.0): 1, ('fursat', 0.0): 1, ('usey', 0.0): 2, ('banaya', 0.0): 1, ('uppar', 0.0): 1, ('waal', 0.0): 1, ('ney', 0.0): 1, ('afso', 0.0): 1, ('hums', 0.0): 1, ('dur', 0.0): 1, ('wo', 0.0): 1, (\"who'd\", 0.0): 1, ('naruhina', 0.0): 1, ('namee', 0.0): 1, ('haiqal', 0.0): 1, ('360hr', 0.0): 1, ('picc', 0.0): 1, ('instor', 0.0): 1, ('pre-vot', 0.0): 1, ('5th', 0.0): 1, ('usernam', 0.0): 1, ('minho', 0.0): 1, ('durian', 0.0): 1, ('strudel', 0.0): 1, ('tsk', 0.0): 1, ('marin', 0.0): 1, ('kailan', 0.0): 1, ('separ', 0.0): 1, ('payday', 0.0): 1, ('payhour', 0.0): 1, ('immedi', 0.0): 1, ('natur', 0.0): 1, ('pre-ord', 0.0): 1, ('fwm', 0.0): 1, ('guppi', 0.0): 1, ('poorkid', 0.0): 1, ('lack', 0.0): 1, ('misunderstood', 0.0): 1, ('cuddli', 0.0): 1, ('scratch', 0.0): 1, ('thumb', 0.0): 1, ('compens', 0.0): 1, ('kirkiri', 0.0): 1, ('phase', 0.0): 1, ('wonho', 0.0): 1, ('visual', 0.0): 1, (\"='(\", 0.0): 1, ('mission', 0.0): 1, ('pap', 0.0): 1, ('danzel', 0.0): 1, ('craft', 0.0): 1, ('devil', 0.0): 1, ('phil', 0.0): 1, ('sheff', 0.0): 1, ('york', 0.0): 1, ('visa', 0.0): 1, ('gim', 0.0): 1, ('bench', 0.0): 1, ('harm', 0.0): 1, ('yolo', 0.0): 1, ('bloat', 0.0): 1, ('olli', 0.0): 1, ('alterni', 0.0): 1, ('earth', 0.0): 1, ('influenc', 0.0): 1, ('overal', 0.0): 1, ('continent', 0.0): 1, ('🔫', 0.0): 1, ('tank', 0.0): 1, ('thirsti', 0.0): 1, ('konami', 0.0): 1, ('polici', 0.0): 1, ('ranti', 0.0): 1, ('atm', 0.0): 1, ('pervers', 0.0): 1, ('bylfnnz', 0.0): 1, ('ban', 0.0): 1, ('failsatlif', 0.0): 1, ('press', 0.0): 1, ('duper', 0.0): 1, ('waaah', 0.0): 1, ('jaebum', 0.0): 1, ('ahmad', 0.0): 1, ('maslan', 0.0): 1, ('hull', 0.0): 1, ('misser', 0.0): 1}\n\n\nUnfortunately, this does not help much to understand the data. It would be better to visualize this output to gain better insights.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "L2 - Frequencies"
    ]
  },
  {
    "objectID": "notes/c1w1/lab02.html#table-of-word-counts",
    "href": "notes/c1w1/lab02.html#table-of-word-counts",
    "title": "Lab: Building and Visualizing word frequencies",
    "section": "Table of word counts",
    "text": "Table of word counts\nWe will select a set of words that we would like to visualize. It is better to store this temporary information in a table that is very easy to use later.\n\n# select some words to appear in the report. we will assume that each word is unique (i.e. no duplicates)\nkeys = ['happi', 'merri', 'nice', 'good', 'bad', 'sad', 'mad', 'best', 'pretti',\n        '❤', ':)', ':(', '😒', '😬', '😄', '😍', '♛',\n        'song', 'idea', 'power', 'play', 'magnific']\n\n# list representing our table of word counts.\n# each element consist of a sublist with this pattern: [&lt;word&gt;, &lt;positive_count&gt;, &lt;negative_count&gt;]\ndata = []\n\n# loop through our selected words\nfor word in keys:\n    \n    # initialize positive and negative counts\n    pos = 0\n    neg = 0\n    \n    # retrieve number of positive counts\n    if (word, 1) in freqs:\n        pos = freqs[(word, 1)]\n        \n    # retrieve number of negative counts\n    if (word, 0) in freqs:\n        neg = freqs[(word, 0)]\n        \n    # append the word counts to the table\n    data.append([word, pos, neg])\n    \ndata\n\n[['happi', 211, 25],\n ['merri', 1, 0],\n ['nice', 98, 19],\n ['good', 238, 101],\n ['bad', 18, 73],\n ['sad', 5, 123],\n ['mad', 4, 11],\n ['best', 65, 22],\n ['pretti', 20, 15],\n ['❤', 29, 21],\n [':)', 3568, 2],\n [':(', 1, 4571],\n ['😒', 1, 3],\n ['😬', 0, 2],\n ['😄', 5, 1],\n ['😍', 2, 1],\n ['♛', 0, 210],\n ['song', 22, 27],\n ['idea', 26, 10],\n ['power', 7, 6],\n ['play', 46, 48],\n ['magnific', 2, 0]]\n\n\nWe can then use a scatter plot to inspect this table visually. Instead of plotting the raw counts, we will plot it in the logarithmic scale to take into account the wide discrepancies between the raw counts (e.g. :) has 3568 counts in the positive while only 2 in the negative). The red line marks the boundary between positive and negative areas. Words close to the red line can be classified as neutral.\n\nfig, ax = plt.subplots(figsize = (8, 8))\n\n# convert positive raw counts to logarithmic scale. we add 1 to avoid log(0)\nx = np.log([x[1] + 1 for x in data])  \n\n# do the same for the negative counts\ny = np.log([x[2] + 1 for x in data]) \n\n# Plot a dot for each pair of words\nax.scatter(x, y)  \n\n# assign axis labels\nplt.xlabel(\"Log Positive count\")\nplt.ylabel(\"Log Negative count\")\n\n# Add the word as the label at the same position as we added the points just before\nfor i in range(0, len(data)):\n    ax.annotate(data[i][0], (x[i], y[i]), fontsize=12)\n\nax.plot([0, 9], [0, 9], color = 'red') # Plot the red line that divides the 2 areas.\nplt.show()\n\nText(0.5, 0, 'Log Positive count')\n\n\nText(0, 0.5, 'Log Negative count')\n\n\nText(5.356586274672012, 3.258096538021482, 'happi')\n\n\nText(0.6931471805599453, 0.0, 'merri')\n\n\nText(4.59511985013459, 2.995732273553991, 'nice')\n\n\nText(5.476463551931511, 4.624972813284271, 'good')\n\n\nText(2.9444389791664403, 4.30406509320417, 'bad')\n\n\nText(1.791759469228055, 4.820281565605037, 'sad')\n\n\nText(1.6094379124341003, 2.4849066497880004, 'mad')\n\n\nText(4.189654742026425, 3.1354942159291497, 'best')\n\n\nText(3.044522437723423, 2.772588722239781, 'pretti')\n\n\nText(3.4011973816621555, 3.091042453358316, '❤')\n\n\nText(8.18004072349016, 1.0986122886681098, ':)')\n\n\nText(0.6931471805599453, 8.427706024914702, ':(')\n\n\nText(0.6931471805599453, 1.3862943611198906, '😒')\n\n\nText(0.0, 1.0986122886681098, '😬')\n\n\nText(1.791759469228055, 0.6931471805599453, '😄')\n\n\nText(1.0986122886681098, 0.6931471805599453, '😍')\n\n\nText(0.0, 5.351858133476067, '♛')\n\n\nText(3.1354942159291497, 3.332204510175204, 'song')\n\n\nText(3.295836866004329, 2.3978952727983707, 'idea')\n\n\nText(2.0794415416798357, 1.9459101490553132, 'power')\n\n\nText(3.8501476017100584, 3.8918202981106265, 'play')\n\n\nText(1.0986122886681098, 0.0, 'magnific')\n\n\n/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 128556 (\\N{GRIMACING FACE}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n\n\n\n\n\n\n\n\n\nThis chart is straightforward to interpret. It shows that emoticons :) and :( are very important for sentiment analysis. Thus, we should not let preprocessing steps get rid of these symbols!\nFurthermore, what is the meaning of the crown symbol? It seems to be very negative!\n\nConclusion\nThat’s all for this lab!\nWe’ve seen how to build a word frequency dictionary and this will come in handy when extracting the features of a list of tweets. Next up, we will be reviewing Logistic Regression. Keep it up!",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "L2 - Frequencies"
    ]
  },
  {
    "objectID": "notes/c1w1/lab03.html",
    "href": "notes/c1w1/lab03.html",
    "title": "Lab: Visualizing tweets & the Logistic Regression model",
    "section": "",
    "text": "Figure 1: course banner\nObjectives: Visualize and interpret the logistic regression model\nSteps:",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "L3 - Visualizing tweets"
    ]
  },
  {
    "objectID": "notes/c1w1/lab03.html#import-the-required-libraries",
    "href": "notes/c1w1/lab03.html#import-the-required-libraries",
    "title": "Lab: Visualizing tweets & the Logistic Regression model",
    "section": "Import the required libraries",
    "text": "Import the required libraries\nWe will be using NLTK, an opensource NLP library, for collecting, handling, and processing Twitter data. In this lab, we will use the example dataset that comes alongside with NLTK. This dataset has been manually annotated and serves to establish baselines for models quickly.\nSo, to start, let’s import the required libraries.\n\nimport nltk                         # NLP toolbox\nfrom os import getcwd\nimport pandas as pd                 # Library for Dataframes \nfrom nltk.corpus import twitter_samples \nimport matplotlib.pyplot as plt     # Library for visualization\nimport numpy as np                  # Library for math functions\n\nfrom utils import process_tweet, build_freqs # Our functions for NLP",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "L3 - Visualizing tweets"
    ]
  },
  {
    "objectID": "notes/c1w1/lab03.html#load-the-nltk-sample-dataset",
    "href": "notes/c1w1/lab03.html#load-the-nltk-sample-dataset",
    "title": "Lab: Visualizing tweets & the Logistic Regression model",
    "section": "Load the NLTK sample dataset",
    "text": "Load the NLTK sample dataset\nTo complete this lab, we need the sample dataset of the previous lab. Here, we assume the files are already available, and we only need to load into Python lists.\n\n# select the set of positive and negative tweets\nall_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')\n\ntweets = all_positive_tweets + all_negative_tweets ## Concatenate the lists. \nlabels = np.append(np.ones((len(all_positive_tweets),1)), np.zeros((len(all_negative_tweets),1)), axis = 0)\n\n# split the data into two pieces, one for training and one for testing (validation set) \ntrain_pos  = all_positive_tweets[:4000]\ntrain_neg  = all_negative_tweets[:4000]\n\ntrain_x = train_pos + train_neg \n\nprint(\"Number of tweets: \", len(train_x))\n\nNumber of tweets:  8000",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "L3 - Visualizing tweets"
    ]
  },
  {
    "objectID": "notes/c1w1/lab03.html#load-the-extracted-features",
    "href": "notes/c1w1/lab03.html#load-the-extracted-features",
    "title": "Lab: Visualizing tweets & the Logistic Regression model",
    "section": "Load the extracted features",
    "text": "Load the extracted features\nPart of this week’s assignment is the creation of the numerical features needed for the Logistic regression model. In order not to interfere with it, we have previously calculated and stored these features in a CSV file for the entire training set.\nSo, please load these features created for the tweets sample.\n\ndata = pd.read_csv('logistic_features.csv'); # Load a 3 columns csv file using pandas function\ndata.head(10) # Print the first 10 data entries\n\n\n\n\n\n\n\n\nbias\npositive\nnegative\nsentiment\n\n\n\n\n0\n1.0\n3020.0\n61.0\n1.0\n\n\n1\n1.0\n3573.0\n444.0\n1.0\n\n\n2\n1.0\n3005.0\n115.0\n1.0\n\n\n3\n1.0\n2862.0\n4.0\n1.0\n\n\n4\n1.0\n3119.0\n225.0\n1.0\n\n\n5\n1.0\n2955.0\n119.0\n1.0\n\n\n6\n1.0\n3934.0\n538.0\n1.0\n\n\n7\n1.0\n3162.0\n276.0\n1.0\n\n\n8\n1.0\n628.0\n189.0\n1.0\n\n\n9\n1.0\n264.0\n112.0\n1.0\n\n\n\n\n\n\n\nNow let us get rid of the data frame to keep only Numpy arrays.\n\n# Each feature is labeled as bias, positive and negative\nX = data[['bias', 'positive', 'negative']].values # Get only the numerical values of the dataframe\nY = data['sentiment'].values; # Put in Y the corresponding labels or sentiments\n\nprint(X.shape) # Print the shape of the X part\nprint(X) # Print some rows of X\n\n(8000, 3)\n[[1.000e+00 3.020e+03 6.100e+01]\n [1.000e+00 3.573e+03 4.440e+02]\n [1.000e+00 3.005e+03 1.150e+02]\n ...\n [1.000e+00 1.440e+02 7.830e+02]\n [1.000e+00 2.050e+02 3.890e+03]\n [1.000e+00 1.890e+02 3.974e+03]]",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "L3 - Visualizing tweets"
    ]
  },
  {
    "objectID": "notes/c1w1/lab03.html#load-a-pretrained-logistic-regression-model",
    "href": "notes/c1w1/lab03.html#load-a-pretrained-logistic-regression-model",
    "title": "Lab: Visualizing tweets & the Logistic Regression model",
    "section": "Load a pretrained Logistic Regression model",
    "text": "Load a pretrained Logistic Regression model\nIn the same way, as part of this week’s assignment, a Logistic regression model must be trained. The next cell contains the resulting model from such training. Notice that a list of 3 numeric values represents the whole model, that we have called theta \\theta.\n\ntheta = [7e-08, 0.0005239, -0.00055517]",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "L3 - Visualizing tweets"
    ]
  },
  {
    "objectID": "notes/c1w1/lab03.html#plot-the-samples-in-a-scatter-plot",
    "href": "notes/c1w1/lab03.html#plot-the-samples-in-a-scatter-plot",
    "title": "Lab: Visualizing tweets & the Logistic Regression model",
    "section": "Plot the samples in a scatter plot",
    "text": "Plot the samples in a scatter plot\nThe vector theta represents a plane that split our feature space into two parts. Samples located over that plane are considered positive, and samples located under that plane are considered negative. Remember that we have a 3D feature space, i.e., each tweet is represented as a vector comprised of three values: [bias, positive_sum, negative_sum], always having bias = 1.\nIf we ignore the bias term, we can plot each tweet in a cartesian plane, using positive_sum and negative_sum. In the cell below, we do precisely this. Additionally, we color each tweet, depending on its class. Positive tweets will be green and negative tweets will be red.\n\n# Plot the samples using columns 1 and 2 of the matrix\nfig, ax = plt.subplots(figsize = (8, 8))\n\ncolors = ['red', 'green']\n\n# Color based on the sentiment Y\nax.scatter(X[:,1], X[:,2], c=[colors[int(k)] for k in Y], s = 0.1)  # Plot a dot for each pair of words\nplt.xlabel(\"Positive\")\nplt.ylabel(\"Negative\")\n\nText(0.5, 0, 'Positive')\n\n\nText(0, 0.5, 'Negative')\n\n\n\n\n\n\n\n\n\nFrom the plot, it is evident that the features that we have chosen to represent tweets as numerical vectors allow an almost perfect separation between positive and negative tweets. So we can expect a very high accuracy for this model!",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "L3 - Visualizing tweets"
    ]
  },
  {
    "objectID": "notes/c1w1/lab03.html#plot-the-model-alongside-the-data",
    "href": "notes/c1w1/lab03.html#plot-the-model-alongside-the-data",
    "title": "Lab: Visualizing tweets & the Logistic Regression model",
    "section": "Plot the model alongside the data",
    "text": "Plot the model alongside the data\nWe will draw a gray line to show the cutoff between the positive and negative regions. In other words, the gray line marks the line where \nz = \\theta * x = 0.\n\nTo draw this line, we have to solve the above equation in terms of one of the independent variables.\n\nz = \\theta * x = 0\n\n\nx = [1, pos, neg]\n\n\nz(\\theta, x) = \\theta_0+ \\theta_1 * pos + \\theta_2 * neg = 0\n\n\nneg = (-\\theta_0 - \\theta_1 * pos) / \\theta_2\n\nThe red and green lines that point in the direction of the corresponding sentiment are calculated using a perpendicular line to the separation line calculated in the previous equations(neg function). It must point in the same direction as the derivative of the Logit function, but the magnitude may differ. It is only for a visual representation of the model.\n\ndirection = pos * \\theta_2 / \\theta_1\n\n\n# Equation for the separation plane\n# It give a value in the negative axe as a function of a positive value\n# f(pos, neg, W) = w0 + w1 * pos + w2 * neg = 0\n# s(pos, W) = (w0 - w1 * pos) / w2\ndef neg(theta, pos):\n    return (-theta[0] - pos * theta[1]) / theta[2]\n\n# Equation for the direction of the sentiments change\n# We don't care about the magnitude of the change. We are only interested \n# in the direction. So this direction is just a perpendicular function to the \n# separation plane\n# df(pos, W) = pos * w2 / w1\ndef direction(theta, pos):\n    return    pos * theta[2] / theta[1]\n\nThe green line in the chart points in the direction where z &gt; 0 and the red line points in the direction where z &lt; 0. The direction of these lines are given by the weights \\theta_1 and \\theta_2\n\n# Plot the samples using columns 1 and 2 of the matrix\nfig, ax = plt.subplots(figsize = (8, 8))\n\ncolors = ['red', 'green']\n\n# Color base on the sentiment Y\nax.scatter(X[:,1], X[:,2], c=[colors[int(k)] for k in Y], s = 0.1)  # Plot a dot for each pair of words\nplt.xlabel(\"Positive\")\nplt.ylabel(\"Negative\")\n\n# Now lets represent the logistic regression model in this chart. \nmaxpos = np.max(X[:,1])\n\noffset = 5000 # The pos value for the direction vectors origin\n\n# Plot a gray line that divides the 2 areas.\nax.plot([0,  maxpos], [neg(theta, 0),   neg(theta, maxpos)], color = 'gray') \n\n# Plot a green line pointing to the positive direction\nax.arrow(offset, neg(theta, offset), offset, direction(theta, offset), head_width=500, head_length=500, fc='g', ec='g')\n# Plot a red line pointing to the negative direction\nax.arrow(offset, neg(theta, offset), -offset, -direction(theta, offset), head_width=500, head_length=500, fc='r', ec='r')\n\nplt.show()\n\nText(0.5, 0, 'Positive')\n\n\nText(0, 0.5, 'Negative')\n\n\n\n\n\n\n\n\n\nNote that more critical than the Logistic regression itself, are the features extracted from tweets that allow getting the right results in this exercise.\nThat is all, folks.\nHopefully, now we understand better what the Logistic regression model represents, and why it works that well for this specific problem.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "L3 - Visualizing tweets"
    ]
  },
  {
    "objectID": "notes/c2w2/index.html",
    "href": "notes/c2w2/index.html",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\n\nFigure 2",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w2/index.html#part-of-speech-tagging",
    "href": "notes/c2w2/index.html#part-of-speech-tagging",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Part of Speech Tagging",
    "text": "Part of Speech Tagging\nPart of Speech Tagging (POS) is the process of assigning a part of speech to a word. By doing so, we will learn the following:\n\nMarkov Chains\nHidden Markov Models\nViterbi algorithm\n\nHere is a concrete example:\n\n\n\n\n\n\n\nautocorrect\n\n\n\n\nFigure 3: Learning Objectives\n\n\nWe can use part of speech tagging for:\n\nIdentifying named entities\nSpeech recognition\nCoreference Resolution\n\nWe can use the probabilities of POS tags happening near one another to come up with the most reasonable output",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w2/index.html#lab1-working-with-text-data",
    "href": "notes/c2w2/index.html#lab1-working-with-text-data",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Lab1: Working with text data",
    "text": "Lab1: Working with text data\nWorking with text data",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w2/index.html#markov-chains",
    "href": "notes/c2w2/index.html#markov-chains",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Markov Chains",
    "text": "Markov Chains\n\n\n\n\n\n\n\nPOS Tagging\n\n\n\n\nFigure 4: POS Tagging\n\n\n\n\n\n\n\n\nFSM\n\n\n\n\nFigure 5: FSM representation for POS tagging\n\n\n\nThe circles of the graph represent the states of your model. A state refers to a certain condition of the present moment. We can think of these as the POS tags of the current word.\n\nQ={q_1, q_2, q_3} \\qquad \\text{ is the set of all states in your model. }",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w2/index.html#markov-chains-and-pos-tags",
    "href": "notes/c2w2/index.html#markov-chains-and-pos-tags",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Markov Chains and POS Tags",
    "text": "Markov Chains and POS Tags\nTo help identify the parts of speech for every word, we need to build a transition matrix that gives we the probabilities from one state to another.\n\n\n\n\n\n\n\nFSM\n\n\n\n\nFigure 6: Learning Objectives\n\n\nIn the diagram above, the blue circles correspond to the part of speech tags, and the arrows correspond to the transition probabilities from one part of speech to another. We can populate the table on the right from the diagram on the left. The first row in your A matrix corresponds to the initial distribution among all the states. According to the table, the sentence has a 40% chance to start as a noun, 10% chance to start with a verb, and a 50% chance to start with another part of speech tag.\nIn more general notation, we can write the transition matrix A, given some states Q, as follows:\n\n\n\n\n\n\n\nFSM\n\n\n\n\nFigure 7: Learning Objectives",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w2/index.html#hidden-markov-models",
    "href": "notes/c2w2/index.html#hidden-markov-models",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Hidden Markov Models",
    "text": "Hidden Markov Models\nIn the previous video, I showed we an example with a simple markov model. The transition probabilities allowed we to identify the transition probability from one POS to another. We will now explore hidden markov models. In hidden markov models we make use of emission probabilities that give we the probability to go from one state (POS tag) to a specific word.\n\n\n\n\n\n\n\nState Transition Graph\n\n\n\n\nFigure 8: Emission probabilities\n\n\nFor example, given that we are in a verb state, we can go to other words with certain probabilities. This emission matrix B, will be used with your transition matrix A, to help we identify the part of speech of a word in a sentence. To populate your matrix B, we can just have a labelled dataset and compute the probabilities of going from a POS to each word in your vocabulary. Here is a recap of what we have seen so far:\n\n\n\n\n\n\n\nHMM\n\n\n\n\nFigure 9: HMM Summary\n\n\nNote that the sum of each row in your A and B matrix has to be 1. Next, I will show we how we can calculate the probabilities inside these matrices.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w2/index.html#calculating-probabilities",
    "href": "notes/c2w2/index.html#calculating-probabilities",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Calculating Probabilities",
    "text": "Calculating Probabilities\nHere is a visual representation on how to calculate the probabilities:\n\n\n\n\n\n\n\nTransition Probabilities\n\n\n\n\nFigure 10: Transition Probabilities\n\n\nThe number of times that blue is followed by purple is 2 out of 3. We will use the same logic to populate our transition and emission matrices. In the transition matrix we will count the number of times tag t_{(i−1)},t{(i)} show up near each other and divide by the total number of times t_{(i−1)} shows up. (which is the same as the number of times it shows up followed by anything else).\n\n\n\n\n\n\n\nTransition Probabilities\n\n\n\n\nFigure 11: Transition Probabilities\n\n\n\ncalculate co-occurrence of tag pairs \nC(t_{(i-1)},t_{(i)})\n\\tag{1}\ncalculate the probabilities using the counts \nP(t_{(i)}|t_{(i-1)}) = \\frac{C(t_{(i)}),t_{(i-1)},}{\\sum_{i=1}^{N} C(t_{(i-1)})}\n\\tag{2}\n\nWhere\nC(t_{(i−1)} ,t_{(i)}) is the count of times tag t_{(i-1)} shows up before tag i.\nFrom this we can compute the probability that a tag shows up after another tag.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w2/index.html#populating-the-transition-matrix",
    "href": "notes/c2w2/index.html#populating-the-transition-matrix",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Populating the Transition Matrix",
    "text": "Populating the Transition Matrix\nTo populate the transition matrix we have to keep track of the number of times each tag shows up before another tag.\n\n\n\n\n\n\n\nTransition Probabilities\n\n\n\n\nFigure 12: Transition Probabilities\n\n\nIn the table above, we can see that green corresponds to nouns (NN), purple corresponds to verbs (VB), and blue corresponds to other (O). Orange (π) corresponds to the initial state. The numbers inside the matrix correspond to the number of times a part of speech tag shows up right after another one.\nTo go from O to NN or in other words to calculate P(O∣NN) we have to compute the following:\n\n\n\n\n\n\n\nTransition Probabilities\n\n\n\n\nFigure 13: Transition Probabilities\n\n\nTo generalize:\n\nP(t_{(i)} \\mid t_{(i-1)}) = \\frac{C(t_{(i)},t_{(i-1)})}{\\sum_{j=1}^{N} C(t_{(i-1)},t_{(j)})}\n\\tag{3}\nWhere:\n\nC(t_{(i)},t_{(i-1)}) is the count of times tag t_{(i-1)} shows up before tag i.\n\nUnfortunately, sometimes we might not see two POS tags in front each other. This will give we a probability of 0. To solve this issue, we will “smooth” it as follows:\n\n\n\n\n\n\n\nTransition Probabilities\n\n\n\n\nFigure 14: Transition Probabilities\n\n\nThe \\epsilon allows we to not have any two sequences showing up with 0 probability. Why is this important?",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w2/index.html#populating-the-emission-matrix",
    "href": "notes/c2w2/index.html#populating-the-emission-matrix",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Populating the Emission Matrix",
    "text": "Populating the Emission Matrix\nTo populate the emission matrix, we have to keep track of the words associated with their parts of speech tags.\n\n\n\n\n\n\n\nThe Emission Matrix\n\n\n\n\nFigure 15: The Emission Matrix\n\n\nTo populate the matrix, we will also use smoothing as we have previously used:\n\nP(w_i \\mid t_i) = \\frac{C(t_i,w_i)+\\epsilon}{\\sum_{j=1}^{N} C(t_i,w_j)+ N \\times \\epsilon} = \\frac{C(t_i,w_i)+\\epsilon}{\\sum_{j=1}^{N} C(t_i)+N\\times \\epsilon}\n\nWhere C(t_i,w_i) is the count associated with how many times the tag t_i is associated with the word w_i. The epsilon above is the smoothing parameter. In the next video, we will talk about the Viterbi algorithm and discuss how we can use the transition and emission matrix to come up with probabilities.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w2/index.html#lab2-working-with-text-data",
    "href": "notes/c2w2/index.html#lab2-working-with-text-data",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Lab2: Working with text data",
    "text": "Lab2: Working with text data\nWorking with text data",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w2/index.html#the-viterbi-algorithm",
    "href": "notes/c2w2/index.html#the-viterbi-algorithm",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "The Viterbi Algorithm",
    "text": "The Viterbi Algorithm\nThe Viterbi algorithm makes use of the transition probabilities and the emission probabilities as follows.\n\n\n\n\n\n\n\nThe Emission Matrix\n\n\n\n\nFigure 16: The Viterbi Algorithm\n\n\nTo go from π to O we need to multiply the corresponding transition probability (0.3) and the corresponding emission probability (0.5), which gives we 0.15. We keep doing that for all the words, until we get the probability of an entire sequence.\n\n\n\n\n\n\n\nThe Emission Matrix\n\n\n\n\nFigure 17: The Viterbi Algorithm\n\n\nWe can then see how we will just pick the sequence with the highest probability. We will show we a systematic way to accomplish this (Viterbi!).",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w2/index.html#viterbi-initialization",
    "href": "notes/c2w2/index.html#viterbi-initialization",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Viterbi Initialization",
    "text": "Viterbi Initialization\nWe will now populate a matrix C of dimension (num_tags, num_words). This matrix will have the probabilities that will tell we what part of speech each word belongs to.\n\n\n\n\n\n\n\nThe Emission Matrix\n\n\n\n\nFigure 18: The Viterbi Initialization\n\n\nNow to populate the first column, we just multiply the initial π distribution, for each tag, times b_{i,cindex(w_1)}, which is the emission probability of the word 1 given the tag i. Where the i, corresponds to the tag of the initial distribution and the cindex(w_1), is the index of word 1 in the emission matrix. And that’s it, we are done with populating the first column of your new C matrix. We will now need to keep track what part of speech we are coming from. Hence we introduce a matrix D, which allows we to store the labels that represent the different states we are going through when finding the most likely sequence of POS tags for the given sequence of words w_2 ,…,w_k. At first we set the first column to 0, because we are not coming from any POS tag.\n\n\n\n\n\n\n\nThe Emission Matrix\n\n\n\n\nFigure 19: The Viterbi Initialization\n\n\nThese two matrices will make more sense in the next videos.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w2/index.html#viterbi-forward-pass",
    "href": "notes/c2w2/index.html#viterbi-forward-pass",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Viterbi: Forward Pass",
    "text": "Viterbi: Forward Pass\nThis will be best illustrated with an example:\n\n\n\n\n\n\n\nViterbi: Forward Pass\n\n\n\n\nFigure 20: Viterbi: Forward Pass\n\n\nSo to populate a cell (i.e. 1,2) in the image above, we have to take the max of [kth cells in the previous column, times the corresponding transition probability of the kth POS to the first POS times the emission probability of the first POS and the current word we are looking at]. We do that for all the cells. Take a paper and a pencil, and make sure we understand how it is done.\nThe general rule is c_{ij}= max_k c_{k,j-1} \\times a_{k,i} \\times b_{i,cindex(w_j)}\nNow to populate the D matrix, we will keep track of the argmax of where we came from as follows:\nNote that the only difference between c_{ij} and d_{ij}, is that in the former we compute the probability and in the latter we keep track of the index of the row where that probability came from. So we keep track of which k was used to get that max probability.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w2/index.html#viterbi-backward-pass",
    "href": "notes/c2w2/index.html#viterbi-backward-pass",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Viterbi: Backward Pass",
    "text": "Viterbi: Backward Pass\nGreat, now that we know how to compute A, B, C, and D, we will put it all together and show we how to construct the path that will give we the part of speech tags for your sentence.\n\n\n\n\n\n\n\nViterbi: Forward Pass\n\n\n\n\nFigure 21: Viterbi: Forward Pass\n\n\nThe equation above just gives we the index of the highest row in the last column of C. Once we have that, we can go ahead and start using your D matrix as follows:\n\n\n\n\n\n\n\nViterbi: Forward Pass\n\n\n\n\nFigure 22: Viterbi: Forward Pass\n\n\nNote that since we started at index one, hence the last word (w5) is t_1. Then we go to the first row of D and what ever that number is, it indicated the row of the next part of speech tag. Then next part of speech tag indicates the row of the next and so forth. This allows we to reconstruct the POS tags for your sentence.\nWe will be implementing this in this week’s programming assignment.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w2/index.html#assignment",
    "href": "notes/c2w2/index.html#assignment",
    "title": "Part of Speech Tagging and Hidden Markov Models",
    "section": "Assignment",
    "text": "Assignment\nPart-of-speech (POS) tagging",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w2/lab01.html",
    "href": "notes/c2w2/lab01.html",
    "title": "Parts-of-Speech Tagging - First Steps: Working with text files, Creating a Vocabulary and Handling Unknown Words",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure 1\nIn this lecture notebook we will create a vocabulary from a tagged dataset and learn how to deal with words that are not present in this vocabulary when working with other text sources.\nAside from this we will also learn how to:\nimport string\nfrom collections import defaultdict",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "L1 - Vocabulary with unknowns"
    ]
  },
  {
    "objectID": "notes/c2w2/lab01.html#processing-new-text-sources",
    "href": "notes/c2w2/lab01.html#processing-new-text-sources",
    "title": "Parts-of-Speech Tagging - First Steps: Working with text files, Creating a Vocabulary and Handling Unknown Words",
    "section": "Processing new text sources",
    "text": "Processing new text sources\n\nDealing with unknown words\nNow that we have a vocabulary, we will use it when processing new text sources. A new text will have words that do not appear in the current vocabulary. To tackle this, we can simply classify each new word as an unknown one, but we can do better by creating a function that tries to classify the type of each unknown word and assign it a corresponding unknown token.\nThis function will do the following checks and return an appropriate token:\n\nCheck if the unknown word contains any character that is a digit\n\nreturn --unk_digit--\n\nCheck if the unknown word contains any punctuation character\n\nreturn --unk_punct--\n\nCheck if the unknown word contains any upper-case character\n\nreturn --unk_upper--\n\nCheck if the unknown word ends with a suffix that could indicate it is a noun, verb, adjective or adverb\n\nreturn --unk_noun--, --unk_verb--, --unk_adj--, --unk_adv-- respectively\n\n\nIf a word fails to fall under any condition then its token will be a plain --unk--. The conditions will be evaluated in the same order as listed here. So if a word contains a punctuation character but does not contain digits, it will fall under the second condition. To achieve this behaviour some if/elif statements can be used along with early returns.\nThis function is implemented next. Notice that the any() function is being heavily used. It returns True if at least one of the cases it evaluates is True.\n\ndef assign_unk(word):\n    \"\"\"\n    Assign tokens to unknown words\n    \"\"\"\n    \n    # Punctuation characters\n    # Try printing them out in a new cell!\n    punct = set(string.punctuation)\n    \n    # Suffixes\n    noun_suffix = [\"action\", \"age\", \"ance\", \"cy\", \"dom\", \"ee\", \"ence\", \"er\", \"hood\", \"ion\", \"ism\", \"ist\", \"ity\", \"ling\", \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ship\", \"ty\"]\n    verb_suffix = [\"ate\", \"ify\", \"ise\", \"ize\"]\n    adj_suffix = [\"able\", \"ese\", \"ful\", \"i\", \"ian\", \"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ly\", \"ous\"]\n    adv_suffix = [\"ward\", \"wards\", \"wise\"]\n\n    # Loop the characters in the word, check if any is a digit\n    if any(char.isdigit() for char in word):\n        return \"--unk_digit--\"\n\n    # Loop the characters in the word, check if any is a punctuation character\n    elif any(char in punct for char in word):\n        return \"--unk_punct--\"\n\n    # Loop the characters in the word, check if any is an upper case character\n    elif any(char.isupper() for char in word):\n        return \"--unk_upper--\"\n\n    # Check if word ends with any noun suffix\n    elif any(word.endswith(suffix) for suffix in noun_suffix):\n        return \"--unk_noun--\"\n\n    # Check if word ends with any verb suffix\n    elif any(word.endswith(suffix) for suffix in verb_suffix):\n        return \"--unk_verb--\"\n\n    # Check if word ends with any adjective suffix\n    elif any(word.endswith(suffix) for suffix in adj_suffix):\n        return \"--unk_adj--\"\n\n    # Check if word ends with any adverb suffix\n    elif any(word.endswith(suffix) for suffix in adv_suffix):\n        return \"--unk_adv--\"\n    \n    # If none of the previous criteria is met, return plain unknown\n    return \"--unk--\"\n\nA POS tagger will always encounter words that are not within the vocabulary that is being used. By augmenting the dataset to include these unknown word tokens we are helping the tagger to have a better idea of the appropriate tag for these words.\n\n\nGetting the correct tag for a word\nAll that is left is to implement a function that will get the correct tag for a particular word taking special considerations for unknown words. Since the dataset provides each word and tag within the same line and a word being known depends on the vocabulary used, these two elements should be arguments to this function.\nThis function should check if a line is empty and if so, it should return a placeholder word and tag, --n-- and --s-- respectively.\nIf not, it should process the line to return the correct word and tag pair, considering if a word is unknown in which scenario the function assign_unk() should be used.\nThe function is implemented next. Notice That the split() method can be used without specifying the delimiter, in which case it will default to any whitespace.\n\ndef get_word_tag(line, vocab):\n    # If line is empty return placeholders for word and tag\n    if not line.split():\n        word = \"--n--\"\n        tag = \"--s--\"\n    else:\n        # Split line to separate word and tag\n        word, tag = line.split()\n        # Check if word is not in vocabulary\n        if word not in vocab: \n            # Handle unknown word\n            word = assign_unk(word)\n    return word, tag\n\nNow we can try this function with some examples to test that it is working as intended:\n\nget_word_tag('\\n', vocab)\n\n('--n--', '--s--')\n\n\nSince this line only includes a newline character it returns a placeholder word and tag.\n\nget_word_tag('In\\tIN\\n', vocab)\n\n('In', 'IN')\n\n\nThis one is a valid line and the function does a fair job at returning the correct (word, tag) pair.\n\nget_word_tag('tardigrade\\tNN\\n', vocab)\n\n('--unk--', 'NN')\n\n\nThis line includes a noun that is not present in the vocabulary.\nThe assign_unk function fails to detect that it is a noun so it returns an unknown token.\n\nget_word_tag('scrutinize\\tVB\\n', vocab)\n\n('--unk_verb--', 'VB')\n\n\nThis line includes a verb that is not present in the vocabulary.\nIn this case the assign_unk is able to detect that it is a verb so it returns an unknown verb token.\nCongratulations on finishing this lecture notebook! Now we should be more familiar with working with text data and have a better understanding of how a basic POS tagger works.\nKeep it up!",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "L1 - Vocabulary with unknowns"
    ]
  },
  {
    "objectID": "notes/c1w3/index.html",
    "href": "notes/c1w3/index.html",
    "title": "Vector Space Models",
    "section": "",
    "text": "Figure 1: course banner\n\n\n\n\n\n\n\n\nFigure 2",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w3/index.html#lab-linear-algebra-in-python-with-numpy",
    "href": "notes/c1w3/index.html#lab-linear-algebra-in-python-with-numpy",
    "title": "Vector Space Models",
    "section": "LAB: Linear algebra in Python with numpy",
    "text": "LAB: Linear algebra in Python with numpy\nThe Numpy lab\n\nCosine Similarity Intuition\nOne of the issues with euclidean distance is that it is not always accurate and sometimes we are not looking for that type of similarity metric. For example, when comparing large documents to smaller ones with euclidean distance one could get an inaccurate result.\nLook at the diagram:\n\n\n\n\n\n\n\nCosine Similarity: Intuition\n\n\n\n\nFigure 11: The cosine similarity is the cosine of the angle between two vectors.\n\n\nNormally the food corpus and the agriculture corpus are more similar because they have the same proportion of words. However the food corpus is much smaller than the agriculture corpus. To further clarify, although the history corpus and the agriculture corpus are different, they have a smaller euclidean distance. Hence d_2&lt;d_1.\nTo solve this problem, we look at the cosine between the vectors. This allows us to compare B and α.\n\n\nBackground\nBefore getting into the cosine similarity function remember that the norm of a vector is defined as:\n\n\nNorm of a Vector\n\n||\\vec{A}|| = \\sqrt{\\sum_{i=1}^{n} a_i^2}\n\\tag{2}\n\n\nDot-product of Two Vectors\nThe dot product is then defined as:\n\n\\vec{A} \\cdot \\vec{B} = \\sum_{i=1}^{n} a_i \\cdot b_i\n\\tag{3}\n\n\n\n\n\n\n\nCosine Similarity\n\n\n\n\nFigure 12: The cosine similarity is the cosine of the angle between two vectors.\n\n\nThe following cosine similarity equation makes sense:\n\n\\cos(\\theta) = \\frac{\\vec{v} \\cdot \\vec{w}}{||\\vec{v}|| \\cdot ||\\vec{w}||}\n\\tag{4}",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w3/index.html#implementation",
    "href": "notes/c1w3/index.html#implementation",
    "title": "Vector Space Models",
    "section": "Implementation",
    "text": "Implementation\nWhen \\vec{v} and \\vec{u} are parallel the numerator is equal to the denominator so cos(\\beta)=1 thus \\angle \\beta=0.\nOn the other hand, the dot product of two orthogonal (perpendicular) vectors is 0. That takes place when \\angle \\beta=90.\n\n\n\n\n\n\n\nCosine Similarity Examples\n\n\n\n\nFigure 13: Examples of cosine similarity between similar and dissimilar vectors.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w3/index.html#lab-on-manipulating-word-vectors",
    "href": "notes/c1w3/index.html#lab-on-manipulating-word-vectors",
    "title": "Vector Space Models",
    "section": "Lab on Manipulating Word Vectors",
    "text": "Lab on Manipulating Word Vectors\nThe lab",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w3/index.html#visualization-of-word-vectors",
    "href": "notes/c1w3/index.html#visualization-of-word-vectors",
    "title": "Vector Space Models",
    "section": "Visualization of Word Vectors",
    "text": "Visualization of Word Vectors\nPrincipal component analysis is an unsupervised learning algorithm which can be used to reduce the dimension of your data. As a result, it allows we to visualize your data. It tries to combine variances across features. Here is a concrete example of PCA:\n\n\n\n\n\n\n\nWord analogy\n\n\n\n\nFigure 16: Some word analogies\n\n\n\n\n\n\n\n\nWord analogy\n\n\n\n\nFigure 17: Some word analogies\n\n\n\nThose are the results of plotting a couple of vectors in two dimensions. Note that words with similar part of speech (POS) tags are next to one another. This is because many of the training algorithms learn words by identifying the neighboring words. Thus, words with similar POS tags tend to be found in similar locations. An interesting insight is that synonyms and antonyms tend to be found next to each other in the plot. Why is that the case?",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w3/index.html#implementation-1",
    "href": "notes/c1w3/index.html#implementation-1",
    "title": "Vector Space Models",
    "section": "Implementation",
    "text": "Implementation",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w3/index.html#pca-algorithm",
    "href": "notes/c1w3/index.html#pca-algorithm",
    "title": "Vector Space Models",
    "section": "PCA algorithm",
    "text": "PCA algorithm\nPCA is commonly used to reduce the dimension of your data. Intuitively the model collapses the data across principal components. We can think of the first principal component (in a 2D dataset) as the line where there is the most amount of variance. We can then collapse the data points on that line. Hence we went from 2D to 1D. We can generalize this intuition to several dimensions.\n\n\n\n\n\n\n\nPCA\n\n\n\n\nFigure 18: Some word analogies\n\n\n\nEigenvector\n\nthe resulting vectors, also known as the uncorrelated features of your data\n\nEigenvalue\n\nthe amount of information retained by each new feature. We can think of it as the variance in the eigenvector.\n\n\nAlso each eigenvalue has a corresponding eigenvector. The eigenvalue tells we how much variance there is in the eigenvector. Here are the steps required to compute PCA:\n\n\n\n\n\n\n\nPCA\n\n\n\n\nFigure 19: Some word analogies\n\n\n\nSteps to Compute PCA:\n\nMean normalize your data\nCompute the covariance matrix\nCompute SVD on your covariance matrix. This returns [USV]=svd(Σ) . The three matrices U, S, V are drawn above. U is labelled with eigenvectors, and S is labelled with eigenvalues.\nWe can then use the first n columns of vector U, to get your new data by multiplying XU[:,0:n].",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w3/index.html#putting-it-together-with-code",
    "href": "notes/c1w3/index.html#putting-it-together-with-code",
    "title": "Vector Space Models",
    "section": "Putting It Together with Code",
    "text": "Putting It Together with Code\n\nimport numpy as np \n\ndef PCA(X , num_components):\n  # center data around the mean\n  X_meaned = X - np.mean(X , axis = 0) \n  # calculate the covariance matrix   \n  cov_mat = np.cov(X_meaned , rowvar = False) \n  # compute an uncorrelated feature basis (eigen vectors) \n  eigen_values , eigen_vectors = np.linalg.eigh(cov_mat)\n  # sort the new basis by decreasing eigen values (variance) \n  sorted_index = np.argsort(eigen_values)[::-1] \n  sorted_eigenvalue = eigen_values[sorted_index] \n  sorted_eigenvectors = eigen_vectors[:,sorted_index] \n  # by subseting the most leading features  \n  eigenvector_subset = sorted_eigenvectors[:,0:num_components] \n  #Step-6 \n  X_reduced = np.dot(eigenvector_subset.transpose() ,     \n                   X_meaned.transpose() ).transpose() \n  return X_reduced",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w3/index.html#lab-on-pca",
    "href": "notes/c1w3/index.html#lab-on-pca",
    "title": "Vector Space Models",
    "section": "Lab on PCA",
    "text": "Lab on PCA\nPCA lab",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w3/index.html#resources",
    "href": "notes/c1w3/index.html#resources",
    "title": "Vector Space Models",
    "section": "Resources",
    "text": "Resources\n\nAlex Williams - Everything we did and didn’t know about PCA\nUdell et al. (2015). Generalized Low-Rank Models  arxiv preprint\nTipping & Bishop (1999). Probabilistic principal component analysis Journal of the Royal Statistical Society: Series B\nIlin & Raiko (2010) Practical Approaches to Principal Component Analysis in the Presence of Missing Values Journal of Machine Learning Research\nGordon (2002). Generalized2 Linear2 Models NIPS\nCunningham & Ghahramani (2015)  Linear dimensionality reduction: survey, insights, and generalizations Journal of Machine Learning Research\nBurges (2009). Dimension Reduction: A Guided Tour Foundations varia Trends in Machine Learning\nM. Gavish and D. L. Donoho, The Optimal Hard Threshold for Singular Values is \\frac{4}{\\sqrt{3}} in IEEE Transactions on Information Theory, vol. 60, no. 8, pp. 5040-5053, Aug. 2014, doi: 10.1109/TIT.2014.2323359.\nThomas P. Minka Automatic choice of dimensionality for PCA Dec. 2000",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w3/lab01.html",
    "href": "notes/c1w3/lab01.html",
    "title": "Linear algebra in Python with NumPy",
    "section": "",
    "text": "Figure 1: course banner\nIn this lab, we will have the opportunity to remember some basic concepts about linear algebra and how to use them in Python.\nNumpy is one of the most used libraries in Python for arrays manipulation. It adds to Python a set of functions that allows us to operate on large multidimensional arrays with just a few lines. So forget about writing nested loops for adding matrices! With NumPy, this is as simple as adding numbers.\nLet us import the numpy library and assign the alias np for it. We will follow this convention in almost every notebook in this course, and you’ll see this in many resources outside this course as well.\nimport numpy as np  # The swiss knife of the data scientist.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "L1 - Linear algebra with NumPy"
    ]
  },
  {
    "objectID": "notes/c1w3/lab01.html#defining-lists-and-numpy-arrays",
    "href": "notes/c1w3/lab01.html#defining-lists-and-numpy-arrays",
    "title": "Linear algebra in Python with NumPy",
    "section": "Defining lists and numpy arrays",
    "text": "Defining lists and numpy arrays\n\nalist = [1, 2, 3, 4, 5]   # Define a python list. It looks like an np array\nnarray = np.array([1, 2, 3, 4]) # Define a numpy array\n\nNote the difference between a Python list and a NumPy array.\n\nprint(alist)\nprint(narray)\n\nprint(type(alist))\nprint(type(narray))\n\n[1, 2, 3, 4, 5]\n[1 2 3 4]\n&lt;class 'list'&gt;\n&lt;class 'numpy.ndarray'&gt;",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "L1 - Linear algebra with NumPy"
    ]
  },
  {
    "objectID": "notes/c1w3/lab01.html#algebraic-operators-on-numpy-arrays-vs.-python-lists",
    "href": "notes/c1w3/lab01.html#algebraic-operators-on-numpy-arrays-vs.-python-lists",
    "title": "Linear algebra in Python with NumPy",
    "section": "Algebraic operators on NumPy arrays vs. Python lists",
    "text": "Algebraic operators on NumPy arrays vs. Python lists\nOne of the common beginner mistakes is to mix up the concepts of NumPy arrays and Python lists. Just observe the next example, where we add two objects of the two mentioned types. Note that the ‘+’ operator on NumPy arrays perform an element-wise addition, while the same operation on Python lists results in a list concatenation. Be careful while coding. Knowing this can save many headaches.\n\nprint(narray + narray)\nprint(alist + alist)\n\n[2 4 6 8]\n[1, 2, 3, 4, 5, 1, 2, 3, 4, 5]\n\n\nIt is the same as with the product operator, *. In the first case, we scale the vector, while in the second case, we concatenate three times the same list.\n\nprint(narray * 3)\nprint(alist * 3)\n\n[ 3  6  9 12]\n[1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5]\n\n\nBe aware of the difference because, within the same function, both types of arrays can appear. Numpy arrays are designed for numerical and matrix operations, while lists are for more general purposes.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "L1 - Linear algebra with NumPy"
    ]
  },
  {
    "objectID": "notes/c1w3/lab01.html#matrix-or-array-of-arrays",
    "href": "notes/c1w3/lab01.html#matrix-or-array-of-arrays",
    "title": "Linear algebra in Python with NumPy",
    "section": "Matrix or Array of Arrays",
    "text": "Matrix or Array of Arrays\nIn linear algebra, a matrix is a structure composed of n rows by m columns. That means each row must have the same number of columns. With NumPy, we have two ways to create a matrix: * Creating an array of arrays using np.array (recommended). * Creating a matrix using np.matrix (still available but might be removed soon).\nNumPy arrays or lists can be used to initialize a matrix, but the resulting matrix will be composed of NumPy arrays only.\n\nnpmatrix1 = np.array([narray, narray, narray]) # Matrix initialized with NumPy arrays\nnpmatrix2 = np.array([alist, alist, alist]) # Matrix initialized with lists\nnpmatrix3 = np.array([narray, [1, 1, 1, 1], narray]) # Matrix initialized with both types\n\nprint(npmatrix1)\nprint(npmatrix2)\nprint(npmatrix3)\n\n[[1 2 3 4]\n [1 2 3 4]\n [1 2 3 4]]\n[[1 2 3 4 5]\n [1 2 3 4 5]\n [1 2 3 4 5]]\n[[1 2 3 4]\n [1 1 1 1]\n [1 2 3 4]]\n\n\nHowever, when defining a matrix, be sure that all the rows contain the same number of elements. Otherwise, the linear algebra operations could lead to unexpected results.\nAnalyze the following two examples:\n\n# Example 1:\n\nokmatrix = np.array([[1, 2], [3, 4]]) # Define a 2x2 matrix\nprint(okmatrix) # Print okmatrix\nprint(okmatrix * 2) # Print a scaled version of okmatrix\n\n[[1 2]\n [3 4]]\n[[2 4]\n [6 8]]\n\n\n\n# Example 2:\n\nbadmatrix = np.array([[1, 2], [3, 4], [5, 6, 7]]) # Define a matrix. Note the third row contains 3 elements\nprint(badmatrix) # Print the malformed matrix\nprint(badmatrix * 2) # It is supposed to scale the whole matrix\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[8], line 3\n      1 # Example 2:\n----&gt; 3 badmatrix = np.array([[1, 2], [3, 4], [5, 6, 7]]) # Define a matrix. Note the third row contains 3 elements\n      4 print(badmatrix) # Print the malformed matrix\n      5 print(badmatrix * 2) # It is supposed to scale the whole matrix\n\nValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "L1 - Linear algebra with NumPy"
    ]
  },
  {
    "objectID": "notes/c1w3/lab01.html#scaling-and-translating-matrices",
    "href": "notes/c1w3/lab01.html#scaling-and-translating-matrices",
    "title": "Linear algebra in Python with NumPy",
    "section": "Scaling and translating matrices",
    "text": "Scaling and translating matrices\nNow that we know how to build correct NumPy arrays and matrices, let us see how easy it is to operate with them in Python using the regular algebraic operators like + and -.\nOperations can be performed between arrays and arrays or between arrays and scalars.\n\n# Scale by 2 and translate 1 unit the matrix\nresult = okmatrix * 2 + 1 # For each element in the matrix, multiply by 2 and add 1\nprint(result)\n\n[[3 5]\n [7 9]]\n\n\n\n# Add two compatible matrices\nresult1 = okmatrix + okmatrix\nprint(result1)\n\n# Subtract two compatible matrices. This is called the difference vector\nresult2 = okmatrix - okmatrix\nprint(result2)\n\n[[2 4]\n [6 8]]\n[[0 0]\n [0 0]]\n\n\nThe product operator * when used on arrays or matrices indicates element-wise multiplications. Do not confuse it with the dot product.\n\nresult = okmatrix * okmatrix # Multiply each element by itself\nprint(result)\n\n[[ 1  4]\n [ 9 16]]",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "L1 - Linear algebra with NumPy"
    ]
  },
  {
    "objectID": "notes/c1w3/lab01.html#transpose-a-matrix",
    "href": "notes/c1w3/lab01.html#transpose-a-matrix",
    "title": "Linear algebra in Python with NumPy",
    "section": "Transpose a matrix",
    "text": "Transpose a matrix\nIn linear algebra, the transpose of a matrix is an operator that flips a matrix over its diagonal, i.e., the transpose operator switches the row and column indices of the matrix producing another matrix. If the original matrix dimension is n by m, the resulting transposed matrix will be m by n.\nT denotes the transpose operations with NumPy matrices.\n\nmatrix3x2 = np.array([[1, 2], [3, 4], [5, 6]]) # Define a 3x2 matrix\nprint('Original matrix 3 x 2')\nprint(matrix3x2)\nprint('Transposed matrix 2 x 3')\nprint(matrix3x2.T)\n\nOriginal matrix 3 x 2\n[[1 2]\n [3 4]\n [5 6]]\nTransposed matrix 2 x 3\n[[1 3 5]\n [2 4 6]]\n\n\nHowever, note that the transpose operation does not affect 1D arrays.\n\nnparray = np.array([1, 2, 3, 4]) # Define an array\nprint('Original array')\nprint(nparray)\nprint('Transposed array')\nprint(nparray.T)\n\nOriginal array\n[1 2 3 4]\nTransposed array\n[1 2 3 4]\n\n\nperhaps in this case we wanted to do:\n\nnparray = np.array([[1, 2, 3, 4]]) # Define a 1 x 4 matrix. Note the 2 level of square brackets\nprint('Original array')\nprint(nparray)\nprint('Transposed array')\nprint(nparray.T)\n\nOriginal array\n[[1 2 3 4]]\nTransposed array\n[[1]\n [2]\n [3]\n [4]]",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "L1 - Linear algebra with NumPy"
    ]
  },
  {
    "objectID": "notes/c1w3/lab01.html#get-the-norm-of-a-nparray-or-matrix",
    "href": "notes/c1w3/lab01.html#get-the-norm-of-a-nparray-or-matrix",
    "title": "Linear algebra in Python with NumPy",
    "section": "Get the norm of a nparray or matrix",
    "text": "Get the norm of a nparray or matrix\nIn linear algebra, the norm of an n-dimensional vector \\vec a is defined as:\n norm(\\vec a) = ||\\vec a|| = \\sqrt {\\sum_{i=1}^{n} a_i ^ 2}\nCalculating the norm of vector or even of a matrix is a general operation when dealing with data. Numpy has a set of functions for linear algebra in the subpackage linalg, including the norm function. Let us see how to get the norm a given array or matrix:\n\nnparray1 = np.array([1, 2, 3, 4]) # Define an array\nnorm1 = np.linalg.norm(nparray1)\n\nnparray2 = np.array([[1, 2], [3, 4]]) # Define a 2 x 2 matrix. Note the 2 level of square brackets\nnorm2 = np.linalg.norm(nparray2) \n\nprint(norm1)\nprint(norm2)\n\n5.477225575051661\n5.477225575051661\n\n\nNote that without any other parameter, the norm function treats the matrix as being just an array of numbers. However, it is possible to get the norm by rows or by columns. The axis parameter controls the form of the operation: * axis=0 means get the norm of each column * axis=1 means get the norm of each row.\n\nnparray2 = np.array([[1, 1], [2, 2], [3, 3]]) # Define a 3 x 2 matrix. \n\nnormByCols = np.linalg.norm(nparray2, axis=0) # Get the norm for each column. Returns 2 elements\nnormByRows = np.linalg.norm(nparray2, axis=1) # get the norm for each row. Returns 3 elements\n\nprint(normByCols)\nprint(normByRows)\n\n[3.74165739 3.74165739]\n[1.41421356 2.82842712 4.24264069]\n\n\nHowever, there are more ways to get the norm of a matrix in Python. For that, let us see all the different ways of defining the dot product between 2 arrays.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "L1 - Linear algebra with NumPy"
    ]
  },
  {
    "objectID": "notes/c1w3/lab01.html#the-dot-product-between-arrays-all-the-flavors",
    "href": "notes/c1w3/lab01.html#the-dot-product-between-arrays-all-the-flavors",
    "title": "Linear algebra in Python with NumPy",
    "section": "The dot product between arrays: All the flavors",
    "text": "The dot product between arrays: All the flavors\nThe dot product or scalar product or inner product between two vectors \\vec a and \\vec b of the same size is defined as: \\vec a \\cdot \\vec b = \\sum_{i=1}^{n} a_i b_i\nThe dot product takes two vectors and returns a single number.\n\nnparray1 = np.array([0, 1, 2, 3]) # Define an array\nnparray2 = np.array([4, 5, 6, 7]) # Define an array\n\nflavor1 = np.dot(nparray1, nparray2) # Recommended way\nprint(flavor1)\n\nflavor2 = np.sum(nparray1 * nparray2) # Ok way\nprint(flavor2)\n\nflavor3 = nparray1 @ nparray2         # Geeks way\nprint(flavor3)\n\n# As we never should do:             # Noobs way\nflavor4 = 0\nfor a, b in zip(nparray1, nparray2):\n    flavor4 += a * b\n    \nprint(flavor4)\n\n38\n38\n38\n38\n\n\nWe strongly recommend using np.dot, since it is the only method that accepts arrays and lists without problems\n\nnorm1 = np.dot(np.array([1, 2]), np.array([3, 4])) # Dot product on nparrays\nnorm2 = np.dot([1, 2], [3, 4]) # Dot product on python lists\n\nprint(norm1, '=', norm2 )\n\n11 = 11\n\n\nFinally, note that the norm is the square root of the dot product of the vector with itself. That gives many options to write that function:\n norm(\\vec a) = ||\\vec a|| = \\sqrt {\\sum_{i=1}^{n} a_i ^ 2} = \\sqrt {a \\cdot a}",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "L1 - Linear algebra with NumPy"
    ]
  },
  {
    "objectID": "notes/c1w3/lab01.html#sums-by-rows-or-columns",
    "href": "notes/c1w3/lab01.html#sums-by-rows-or-columns",
    "title": "Linear algebra in Python with NumPy",
    "section": "Sums by rows or columns",
    "text": "Sums by rows or columns\nAnother general operation performed on matrices is the sum by rows or columns. Just as we did for the function norm, the axis parameter controls the form of the operation: * axis=0 means to sum the elements of each column together. * axis=1 means to sum the elements of each row together.\n\nnparray2 = np.array([[1, -1], [2, -2], [3, -3]]) # Define a 3 x 2 matrix. \n\nsumByCols = np.sum(nparray2, axis=0) # Get the sum for each column. Returns 2 elements\nsumByRows = np.sum(nparray2, axis=1) # get the sum for each row. Returns 3 elements\n\nprint('Sum by columns: ')\nprint(sumByCols)\nprint('Sum by rows:')\nprint(sumByRows)\n\nSum by columns: \n[ 6 -6]\nSum by rows:\n[0 0 0]",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "L1 - Linear algebra with NumPy"
    ]
  },
  {
    "objectID": "notes/c1w3/lab01.html#get-the-mean-by-rows-or-columns",
    "href": "notes/c1w3/lab01.html#get-the-mean-by-rows-or-columns",
    "title": "Linear algebra in Python with NumPy",
    "section": "Get the mean by rows or columns",
    "text": "Get the mean by rows or columns\nAs with the sums, one can get the mean by rows or columns using the axis parameter. Just remember that the mean is the sum of the elements divided by the length of the vector  mean(\\vec a) = \\frac {{\\sum_{i=1}^{n} a_i }}{n}\n\nnparray2 = np.array([[1, -1], [2, -2], [3, -3]]) # Define a 3 x 2 matrix. Chosen to be a matrix with 0 mean\n\nmean = np.mean(nparray2) # Get the mean for the whole matrix\nmeanByCols = np.mean(nparray2, axis=0) # Get the mean for each column. Returns 2 elements\nmeanByRows = np.mean(nparray2, axis=1) # get the mean for each row. Returns 3 elements\n\nprint('Matrix mean: ')\nprint(mean)\nprint('Mean by columns: ')\nprint(meanByCols)\nprint('Mean by rows:')\nprint(meanByRows)\n\nMatrix mean: \n0.0\nMean by columns: \n[ 2. -2.]\nMean by rows:\n[0. 0. 0.]",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "L1 - Linear algebra with NumPy"
    ]
  },
  {
    "objectID": "notes/c1w3/lab01.html#center-the-columns-of-a-matrix",
    "href": "notes/c1w3/lab01.html#center-the-columns-of-a-matrix",
    "title": "Linear algebra in Python with NumPy",
    "section": "Center the columns of a matrix",
    "text": "Center the columns of a matrix\nCentering the attributes of a data matrix is another essential preprocessing step. Centering a matrix means to remove the column mean to each element inside the column. The mean by columns of a centered matrix is always 0.\nWith NumPy, this process is as simple as this:\n\nnparray2 = np.array([[1, 1], [2, 2], [3, 3]]) # Define a 3 x 2 matrix. \n\nnparrayCentered = nparray2 - np.mean(nparray2, axis=0) # Remove the mean for each column\n\nprint('Original matrix')\nprint(nparray2)\nprint('Centered by columns matrix')\nprint(nparrayCentered)\n\nprint('New mean by column')\nprint(nparrayCentered.mean(axis=0))\n\nOriginal matrix\n[[1 1]\n [2 2]\n [3 3]]\nCentered by columns matrix\n[[-1. -1.]\n [ 0.  0.]\n [ 1.  1.]]\nNew mean by column\n[0. 0.]\n\n\nWarning: This process does not apply for row centering. In such cases, consider transposing the matrix, centering by columns, and then transpose back the result.\nSee the example below:\n\nnparray2 = np.array([[1, 3], [2, 4], [3, 5]]) # Define a 3 x 2 matrix. \n\nnparrayCentered = nparray2.T - np.mean(nparray2, axis=1) # Remove the mean for each row\nnparrayCentered = nparrayCentered.T # Transpose back the result\n\nprint('Original matrix')\nprint(nparray2)\nprint('Centered by rows matrix')\nprint(nparrayCentered)\n\nprint('New mean by rows')\nprint(nparrayCentered.mean(axis=1))\n\nOriginal matrix\n[[1 3]\n [2 4]\n [3 5]]\nCentered by rows matrix\n[[-1.  1.]\n [-1.  1.]\n [-1.  1.]]\nNew mean by rows\n[0. 0. 0.]\n\n\nNote that some operations can be performed using static functions like np.sum() or np.mean(), or by using the inner functions of the array\n\nnparray2 = np.array([[1, 3], [2, 4], [3, 5]]) # Define a 3 x 2 matrix. \n\nmean1 = np.mean(nparray2) # Static way\nmean2 = nparray2.mean()   # Dinamic way\n\nprint(mean1, ' == ', mean2)\n\n3.0  ==  3.0\n\n\nEven if they are equivalent, we recommend the use of the static way always.\nCongratulations! We have successfully reviewed vector and matrix operations with Numpy!",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "L1 - Linear algebra with NumPy"
    ]
  },
  {
    "objectID": "notes/c1w2/code.html",
    "href": "notes/c1w2/code.html",
    "title": "Probability and Bayes Rule",
    "section": "",
    "text": "course banner\n\n\nimport pandas as pd\nimport string \nraw_tweets=[\n  \"I am happy because I am learning NLP\",\n  \"I am sad, I am not learning NLP\",\n  \"I am happy, not sad\",\n  \"I am sad, not happy\",\n]\ndef clean(tweet:str):\n  return  tweet.translate(str.maketrans('', '', string.punctuation)).lower()\ntweets = [clean(tweet) for tweet in raw_tweets]\nlabels=['+','-','+','-']\ndf = pd.DataFrame({'tweets': tweets, 'labels': labels})\ndf\n\n\n\n\n\n\n\n\ntweets\nlabels\n\n\n\n\n0\ni am happy because i am learning nlp\n+\n\n\n1\ni am sad i am not learning nlp\n-\n\n\n2\ni am happy not sad\n+\n\n\n3\ni am sad not happy\n-\n\n\n\n\n\n\n\n\nimport numpy as np\nfrom collections import Counter\np_freq,n_freq = Counter(), Counter()\n#print( df[df.labels == '+']['tweets'].to_list())\n[p_freq.update(tweet.split()) for tweet in df[df.labels == '+']['tweets'].to_list()]\n[n_freq.update(tweet.split()) for tweet in df[df.labels == '-']['tweets'].to_list()]\nprint(p_freq)\nprint(n_freq)\nvocab = list(set(p_freq.keys()).union(set(n_freq.keys())))\npos_freq = [p_freq[word] for word in vocab ]\nneg_freq = [n_freq[word] for word in vocab ]\nvocab_df=pd.DataFrame({'vocab':vocab,'pos_freq':pos_freq,'neg_freq':neg_freq})\nvocab_df['p_pos']=vocab_df.pos_freq/vocab_df.pos_freq.sum()\nvocab_df['p_neg']=vocab_df.neg_freq/vocab_df.neg_freq.sum()\nvocab_df['p_pos_sm']=(vocab_df.pos_freq+1)/(vocab_df.pos_freq.sum()+vocab_df.shape[1])\nvocab_df['p_neg_sm']=(vocab_df.neg_freq+1)/(vocab_df.neg_freq.sum()+vocab_df.shape[1])\nvocab_df['ratio']= vocab_df.p_pos_sm/vocab_df.p_neg_sm\nvocab_df['lambda']= np.log(vocab_df.p_pos_sm/vocab_df.p_neg_sm)\npd.set_option('display.float_format', '{:.2f}'.format)\nvocab_df\nprint(vocab_df.shape)\n\n[None, None]\n\n\n[None, None]\n\n\nCounter({'i': 3, 'am': 3, 'happy': 2, 'because': 1, 'learning': 1, 'nlp': 1, 'not': 1, 'sad': 1})\nCounter({'i': 3, 'am': 3, 'sad': 2, 'not': 2, 'learning': 1, 'nlp': 1, 'happy': 1})\n\n\n\n\n\n\n\n\n\nvocab\npos_freq\nneg_freq\np_pos\np_neg\np_pos_sm\np_neg_sm\nratio\nlambda\n\n\n\n\n0\nam\n3\n3\n0.23\n0.23\n0.22\n0.21\n1.06\n0.05\n\n\n1\nnlp\n1\n1\n0.08\n0.08\n0.11\n0.11\n1.06\n0.05\n\n\n2\nbecause\n1\n0\n0.08\n0.00\n0.11\n0.05\n2.11\n0.75\n\n\n3\nlearning\n1\n1\n0.08\n0.08\n0.11\n0.11\n1.06\n0.05\n\n\n4\nhappy\n2\n1\n0.15\n0.08\n0.17\n0.11\n1.58\n0.46\n\n\n5\nsad\n1\n2\n0.08\n0.15\n0.11\n0.16\n0.70\n-0.35\n\n\n6\ni\n3\n3\n0.23\n0.23\n0.22\n0.21\n1.06\n0.05\n\n\n7\nnot\n1\n2\n0.08\n0.15\n0.11\n0.16\n0.70\n-0.35\n\n\n\n\n\n\n\n(8, 9)\n\n\n\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\ntable = [[\"Sun\",696000,1989100000],\n         [\"Earth\",6371,5973.6],\n         [\"Moon\",1737,73.5],\n         [\"Mars\",3390,641.85]]\nMarkdown(tabulate(\n  table, \n  headers=[\"Planet\",\"R (km)\", \"mass (x 10^29 kg)\"]\n))\n\n\n\nTable 1: Planets\n\n\n\n\n\n\nPlanet\nR (km)\nmass (x 10^29 kg)\n\n\n\n\nSun\n696000\n1.9891e+09\n\n\nEarth\n6371\n5973.6\n\n\nMoon\n1737\n73.5\n\n\nMars\n3390\n641.85\n\n\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{bochman2020,\n  author = {Bochman, Oren},\n  title = {Probability and {Bayes} {Rule}},\n  date = {2020-10-06},\n  url = {https://orenbochman.github.io/notes-nlp/notes/c1w2/code.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2020. “Probability and Bayes Rule.” October\n6, 2020. https://orenbochman.github.io/notes-nlp/notes/c1w2/code.html."
  },
  {
    "objectID": "notes/c2w4/index.html",
    "href": "notes/c2w4/index.html",
    "title": "Word embeddings with neural networks",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\n\nFigure 2\nThese are my notes for Week 4 notes for the Natural Language Processing with Probabilistic Models Course in the Natural Language Processing Specialization Offered by DeepLearning.AI on Coursera.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#overview",
    "href": "notes/c2w4/index.html#overview",
    "title": "Word embeddings with neural networks",
    "section": "Overview",
    "text": "Overview\nWord embeddings are used in most NLP applications. Whenever we are dealing with text, we first have to find a way to encode the words as numbers. Word embedding are a very common technique that allows we to do so. Here are a few applications of word embeddings that we should be able to implement by the time we complete the specialization.\n\n\n\n\n\n\n\napplications\n\n\n\n\nFigure 3: Basic Applications of word embeddings\n\n\n\n\n\n\n\n\napplications\n\n\n\n\nFigure 4: Advanced applications of word embeddings\n\n\n\nBy the end of this week we will be able to:\n\nIdentify the key concepts of word representations\nGenerate word embeddings\nPrepare text for machine learning\nImplement the continuous bag-of-words model",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#basic-word-representations",
    "href": "notes/c2w4/index.html#basic-word-representations",
    "title": "Word embeddings with neural networks",
    "section": "Basic Word Representations",
    "text": "Basic Word Representations\n\n\n\n\n\n\n\nFigure 5: One-hot vectors\n\n\n\n\n\n\n\n\nFigure 6: One-hot vectors\n\n\n\nBasic word representations could be classified into the following:\n\nIntegers\nOne-hot vectors\nWord embeddings\n\nTo the left, we have an example where we use integers to represent a word. The issue there is that there is no reason why one word corresponds to a bigger number than another. To fix this problem we introduce one hot vectors (diagram on the right). To implement one hot vectors, we have to initialize a vector of zeros of dimension V and then put a 1 in the index corresponding to the word we are representing.\nThe pros of one-hot vectors: simple and require no implied ordering. The cons of one-hot vectors: huge and encode no meaning.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#word-embeddings",
    "href": "notes/c2w4/index.html#word-embeddings",
    "title": "Word embeddings with neural networks",
    "section": "Word Embeddings",
    "text": "Word Embeddings\n\n\n\n\n\n\n\nFigure 7: Meaning as vectors in 1D\n\n\n\n\n\n\n\n\nFigure 8: Meaning as vectors in 2D\n\n\n\nFrom the plot above, we can see that when encoding a word in 2D, similar words tend to be found next to each other. Perhaps the first coordinate represents whether a word is positive or negative. The second coordinate tell we whether the word is abstract or concrete. This is just an example, in the real world we will find embeddings with hundreds of dimensions. We can think of each coordinate as a number telling we something about the word.\nThe pros:\n\nLow dimensions (less than V)\nAllow we to encode meaning",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#how-to-create-word-embeddings",
    "href": "notes/c2w4/index.html#how-to-create-word-embeddings",
    "title": "Word embeddings with neural networks",
    "section": "How to Create Word Embeddings?",
    "text": "How to Create Word Embeddings?\n\n\n\n\n\n\n\nFigure 9: Meaning as vectors in 2D\n\n\nTo create word embeddings we always need a corpus of text, and an embedding method.\nThe context of a word tells we what type of words tend to occur near that specific word. The context is important as this is what will give meaning to each word embedding.\nEmbeddings There are many types of possible methods that allow we to learn the word embeddings. The machine learning model performs a learning task, and the main by-products of this task are the word embeddings. The task could be to learn to predict a word based on the surrounding words in a sentence of the corpus, as in the case of the continuous bag-of-words.\nThe task is self-supervised: it is both unsupervised in the sense that the input data — the corpus — is unlabelled, and supervised in the sense that the data itself provides the necessary context which would ordinarily make up the labels.\nWhen training word vectors, there are some parameters we need to tune. (i.e. the dimension of the word vector)",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#word-embedding-methods",
    "href": "notes/c2w4/index.html#word-embedding-methods",
    "title": "Word embeddings with neural networks",
    "section": "Word Embedding Methods",
    "text": "Word Embedding Methods\nClassical Methods\n\nword2vec (Google, 2013)\nContinuous bag-of-words (CBOW): the model learns to predict the center word given some context words.\nContinuous skip-gram / Skip-gram with negative sampling (SGNS): the model learns to predict the words surrounding a given input word.\nGlobal Vectors (GloVe) (Stanford, 2014): factorizes the logarithm of the corpus’s word co-occurrence matrix, similar to the count matrix you’ve used before.\nfastText (Facebook, 2016): based on the skip-gram model and takes into account the structure of words by representing words as an n-gram of characters. It supports out-of-vocabulary (OOV) words.\n\nDeep learning, contextual embeddings\nIn these more advanced models, words have different embeddings depending on their context. We can download pre-trained embeddings for the following models.\n\nBERT (Google, 2018):\nELMo (Allen Institute for AI, 2018)\nGPT-2 (OpenAI, 2018)",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#continuous-bag-of-words-model",
    "href": "notes/c2w4/index.html#continuous-bag-of-words-model",
    "title": "Word embeddings with neural networks",
    "section": "Continuous Bag-of-Words Model",
    "text": "Continuous Bag-of-Words Model\n\n\n\n\n\n\n\nFigure 10: Meaning as vectors in 2D\n\n\n\n\n\n\n\n\nFigure 11: Meaning as vectors in 2D\n\n\n\n\n\n\n\n\nFigure 12: Meaning as vectors in 2D\n\n\n\n\nContinuous Bag of Words Model To create word embeddings, we need a corpus and a learning algorithm. The by-product of this task would be a set of word embeddings. In the case of the continuous bag-of-words model, the objective of the task is to predict a missing word based on the surrounding words.\nHere is a visualization that shows we how the models works.\nAs we can see, the window size in the image above is 5. The context size, C, is 2. C usually tells we how many words before or after the center word the model will use to make the prediction. Here is another visualization that shows an overview of the model.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#cleaning-and-tokenization",
    "href": "notes/c2w4/index.html#cleaning-and-tokenization",
    "title": "Word embeddings with neural networks",
    "section": "Cleaning and Tokenization",
    "text": "Cleaning and Tokenization\n\n\n\n\n\n\n\nFigure 13: Meaning as vectors in 2D\n\n\n\n\n\n\n\n\nFigure 14: Meaning as vectors in 2D\n\n\n\nBefore implementing any natural language processing algorithm, we might want to clean the data and tokenize it. Here are a few things to keep track of when handling your data.\nWe can clean data using python as follows:\nWe can add as many conditions as we want in the lines corresponding to the green rectangle above.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#sliding-window-of-words-in-python",
    "href": "notes/c2w4/index.html#sliding-window-of-words-in-python",
    "title": "Word embeddings with neural networks",
    "section": "Sliding Window of words in Python",
    "text": "Sliding Window of words in Python\n\n\n\n\n\n\n\nFigure 15: Sliding Window of words in Python\n\n\nThe code above shows we a function which takes in two parameters.\nWords: a list of words.\nC: the context size.\nWe first start by setting i to C. Then we single out the center_word, and the context_words. We then yield those and increment i.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#transforming-words-into-vectors",
    "href": "notes/c2w4/index.html#transforming-words-into-vectors",
    "title": "Word embeddings with neural networks",
    "section": "Transforming Words into Vectors",
    "text": "Transforming Words into Vectors\n\n\n\n\n\n\n\nFigure 16: Transforming Words into Vectors\n\n\nAs we can see, we started with one-hot vectors for the context words and and we transform them into a single vector by taking an average. As a result we end up having the following vectors that we can use for your training.\n\n\n\n\n\n\n\nFigure 17: Sliding Window of words in Python",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#lecture-notebook---data-preparation",
    "href": "notes/c2w4/index.html#lecture-notebook---data-preparation",
    "title": "Word embeddings with neural networks",
    "section": "Lecture Notebook - Data Preparation",
    "text": "Lecture Notebook - Data Preparation",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#architecture-of-the-cbow-model",
    "href": "notes/c2w4/index.html#architecture-of-the-cbow-model",
    "title": "Word embeddings with neural networks",
    "section": "Architecture of the CBOW Model",
    "text": "Architecture of the CBOW Model\nThe architecture for the CBOW model could be described as follows\n\n\n\n\n\n\n\nFigure 18: Architecture for the CBOW Model\n\n\nWe have an input, X, which is the average of all context vectors. We then multiply it by W_1 and add b1. The result goes through a ReLU function to give we your hidden layer. That layer is then multiplied by W_2 and we add b_2. The result goes through a softmax which gives we a distribution over V, vocabulary words. We pick the vocabulary word that corresponds to the arg-max of the output.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#architecture-of-the-cbow-model-dimensions",
    "href": "notes/c2w4/index.html#architecture-of-the-cbow-model-dimensions",
    "title": "Word embeddings with neural networks",
    "section": "Architecture of the CBOW Model: Dimensions",
    "text": "Architecture of the CBOW Model: Dimensions\nThe equations for the previous model are:\n\nz_1 = W_1 x + b_1\n\n\nh = ReLU(z_1)\n\n\nz_2 = W_2 h + b_2\n\n\n\\hat{y} = softmax(z_2)\n\nHere, we can see the dimensions:\n\n\n\n\n\n\n\nFigure 19: Architecture for the CBOW Model\n\n\nMake sure we go through the matrix multiplications and understand why the dimensions make sense.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#architecture-of-the-cbow-model-dimensions-2",
    "href": "notes/c2w4/index.html#architecture-of-the-cbow-model-dimensions-2",
    "title": "Word embeddings with neural networks",
    "section": "Architecture of the CBOW Model: Dimensions 2",
    "text": "Architecture of the CBOW Model: Dimensions 2\nWhen dealing with batch input, we can stack the examples as columns. We can then proceed to multiply the matrices as follows:\n\n\n\n\n\n\n\nFigure 20: Dimensions Batch Input\n\n\nIn the diagram above, we can see the dimensions of each matrix. Note that your \\hat{Y} is of dimension V by m. Each column is the prediction of the column corresponding to the context words. So the first column in \\hat{Y} is the prediction corresponding to the first column of X.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#architecture-of-the-cbow-model-activation-functions",
    "href": "notes/c2w4/index.html#architecture-of-the-cbow-model-activation-functions",
    "title": "Word embeddings with neural networks",
    "section": "Architecture of the CBOW Model: Activation Functions",
    "text": "Architecture of the CBOW Model: Activation Functions\n\nReLU funciton\nThe rectified linear unit (ReLU), is one of the most popular activation functions. When we feed a vector, namely x, into a ReLU function. We end up taking x=max(0,x). This is a drawing that shows ReLU.\n\n\n\n\n\n\n\nFigure 21: Dimensions Batch Input\n\n\n\n\nSoftmax function\nThe softmax function takes a vector and transforms it into a probability distribution. For example, given the following vector z, we can transform it into a probability distribution as follows.\n\n\n\n\n\n\n\nFigure 22: Dimensions Batch Input\n\n\nAs we can see, we can compute:\n\n\\hat{y} = \\frac{e^{z_i}}{\\sum_{j=1}^V e^{z_j}}",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#lecture-notebook---intro-to-cbow-model",
    "href": "notes/c2w4/index.html#lecture-notebook---intro-to-cbow-model",
    "title": "Word embeddings with neural networks",
    "section": "Lecture Notebook - Intro to CBOW model",
    "text": "Lecture Notebook - Intro to CBOW model\nlab 2 the CBOW model",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#training-a-cbow-model-cost-function",
    "href": "notes/c2w4/index.html#training-a-cbow-model-cost-function",
    "title": "Word embeddings with neural networks",
    "section": "Training a CBOW Model: Cost Function",
    "text": "Training a CBOW Model: Cost Function\nThe cost function for the CBOW model is a cross-entropy loss defined as:\n\nJ = -\\sum_{k=1}^V y_k log(\\hat{y}_k)\n\\tag{1}\nHere is an example where we use the equation above.\n\n\n\n\n\n\n\nFigure 23: Dimensions Batch Input\n\n\nWhy is the cost 4.61 in the example above?",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#training-a-cbow-model-forward-propagation",
    "href": "notes/c2w4/index.html#training-a-cbow-model-forward-propagation",
    "title": "Word embeddings with neural networks",
    "section": "Training a CBOW Model: Forward Propagation",
    "text": "Training a CBOW Model: Forward Propagation\nTraining a CBOW Model: Forward Propagation Forward propagation is defined as:\n\nZ_1 = W_1 X + B_1\n\n\nH = ReLU(Z_1)\n\n\nZ_2 = W_2 H + B_2\n\n\n\\hat{Y} = softmax(Z_2)\n\nIn the image below we start from the left and we forward propagate all the way to the right.\n\n\n\n\n\n\n\nFigure 24: Dimensions Batch Input\n\n\nTo calculate the loss of a batch, we have to compute the following:\n\nJ_{batch} = -\\frac{1}{m} \\sum_{i=1}^m \\sum_{j=1}^V y_j(i) log(\\hat{y}^j(i))\n\nGiven, your predicted center word matrix, and actual center word matrix, we can compute the loss.\n\n\n\n\n\n\n\nFigure 25: Dimensions Batch Input",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#training-a-cbow-model-backpropagation-and-gradient-descent",
    "href": "notes/c2w4/index.html#training-a-cbow-model-backpropagation-and-gradient-descent",
    "title": "Word embeddings with neural networks",
    "section": "Training a CBOW Model: Backpropagation and Gradient Descent",
    "text": "Training a CBOW Model: Backpropagation and Gradient Descent\n\n\n\n\n\n\n\nFigure 26: Dimensions Batch Input\n\n\n\n\n\n\n\n\nFigure 27: Dimensions Batch Input\n\n\n\nTraining a CBOW Model: Backpropagation and Gradient Descent Backpropagation: calculate partial derivatives of cost with respect to weights and biases.\nWhen computing the back-prop in this model, we need to compute the following:\n\n\\frac{\\partial J_{batch}}{\\partial W_1}, \\frac{\\partial J_{batch}}{\\partial W_2}, \\frac{\\partial J_{batch}}{\\partial B_1}, \\frac{\\partial J_{batch}}{\\partial B_2}\n\nGradient descent: update weights and biases\nNow to update the weights we can iterate as follows:\n\nW_1 := W_1 - \\alpha \\frac{\\partial J_{batch}}{\\partial W_1}\n\n\nW_2 := W_2 - \\alpha \\frac{\\partial J_{batch}}{\\partial W_2}\n\n\nB_1 := B_1 - \\alpha \\frac{\\partial J_{batch}}{\\partial B_1}\n\n\nB_2 := B_2 - \\alpha \\frac{\\partial J_{batch}}{\\partial B_2}\n\nA smaller alpha allows for more gradual updates to the weights and biases, whereas a larger number allows for a faster update of the weights. If α is too large, we might not learn anything, if it is too small, your model will take forever to train.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#lecture-notebook---training-the-cbow-model",
    "href": "notes/c2w4/index.html#lecture-notebook---training-the-cbow-model",
    "title": "Word embeddings with neural networks",
    "section": "Lecture Notebook - Training the CBOW model",
    "text": "Lecture Notebook - Training the CBOW model",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#extracting-word-embedding-vectors",
    "href": "notes/c2w4/index.html#extracting-word-embedding-vectors",
    "title": "Word embeddings with neural networks",
    "section": "Extracting Word Embedding Vectors",
    "text": "Extracting Word Embedding Vectors\nThere are two options to extract word embeddings after training the continuous bag of words model. We can use W_1 as follows:\n\n\n\n\n\n\n\nFigure 28: Dimensions Batch Input\n\n\nIf we were to use W_1, each column will correspond to the embeddings of a specific word. We can also use W_2 as follows:\n\n\n\n\n\n\n\nFigure 29: Dimensions Batch Input\n\n\nThe final option is to take an average of both matrices as follows:\n\n\n\n\n\n\n\nFigure 30: Dimensions Batch Input",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#lecture-notebook---word-embeddings",
    "href": "notes/c2w4/index.html#lecture-notebook---word-embeddings",
    "title": "Word embeddings with neural networks",
    "section": "Lecture Notebook - Word Embeddings",
    "text": "Lecture Notebook - Word Embeddings\nlab 4 - Word Embeddings",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#lecture-notebook-word-embeddings-step-by-step",
    "href": "notes/c2w4/index.html#lecture-notebook-word-embeddings-step-by-step",
    "title": "Word embeddings with neural networks",
    "section": "Lecture notebook: Word embeddings step by step",
    "text": "Lecture notebook: Word embeddings step by step\nLab 5 - Word embeddings step by step",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#evaluating-word-embeddings-intrinsic-evaluation",
    "href": "notes/c2w4/index.html#evaluating-word-embeddings-intrinsic-evaluation",
    "title": "Word embeddings with neural networks",
    "section": "Evaluating Word Embeddings: Intrinsic Evaluation",
    "text": "Evaluating Word Embeddings: Intrinsic Evaluation\nIntrinsic evaluation allows we to test relationships between words. It allows we to capture semantic analogies as, “France” is to “Paris” as “Italy” is to &lt;?&gt; and also syntactic analogies as “seen” is to “saw” as “been” is to &lt;?&gt;.\nAmbiguous cases could be much harder to track:\n\n\n\n\n\n\n\nFigure 31: Dimensions Batch Input\n\n\nHere are a few ways that allow to use intrinsic evaluation.\n\n\n\n\n\n\n\nFigure 32: Dimensions Batch Input\n\n\n\n\n\n\n\n\nFigure 33: Dimensions Batch Input",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#evaluating-word-embeddings-extrinsic-evaluation",
    "href": "notes/c2w4/index.html#evaluating-word-embeddings-extrinsic-evaluation",
    "title": "Word embeddings with neural networks",
    "section": "Evaluating Word Embeddings: Extrinsic Evaluation",
    "text": "Evaluating Word Embeddings: Extrinsic Evaluation\nExtrinsic evaluation tests word embeddings on external tasks like named entity recognition, parts-of-speech tagging, etc.\n\nEvaluates actual usefulness of embeddings\nTime Consuming\nMore difficult to trouble shoot\n\nSo now we know both intrinsic and extrinsic evaluation.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/index.html#conclusion",
    "href": "notes/c2w4/index.html#conclusion",
    "title": "Word embeddings with neural networks",
    "section": "Conclusion",
    "text": "Conclusion\nThis week we learned the following concepts.\n\nData preparation\nWord representations\nContinuous bag-of-words model\nEvaluation\n\nWe have all the foundations now. From now on, we will start using some advanced AI libraries in the next courses. Congratulations and good luck with the assignment",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w4/lab01.html",
    "href": "notes/c2w4/lab01.html",
    "title": "Word Embeddings First Steps: Data Preparation",
    "section": "",
    "text": "course banner\nIn this series of ungraded notebooks, you’ll try out all the individual techniques that we learned about in the lectures. Practicing on small examples will prepare we for the graded assignment, where we will combine the techniques in more advanced ways to create word embeddings from a real-life corpus.\nThis notebook focuses on data preparation, which is the first step of any machine learning algorithm. It is a very important step because models are only as good as the data they are trained on and the models used require the data to have a particular structure to process it properly.\nTo get started, import and initialize all the libraries we will need.\nimport re\nimport nltk\nimport emoji\nimport numpy as np\nfrom nltk.tokenize import word_tokenize\nfrom utils2 import get_dict",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "L1 - Data preparation"
    ]
  },
  {
    "objectID": "notes/c2w4/lab01.html#cleaning-and-tokenization",
    "href": "notes/c2w4/lab01.html#cleaning-and-tokenization",
    "title": "Word Embeddings First Steps: Data Preparation",
    "section": "Cleaning and tokenization",
    "text": "Cleaning and tokenization\nTo demonstrate the cleaning and tokenization process, consider a corpus that contains emojis and various punctuation signs.\n\n# Define a corpus\ncorpus = 'Who ❤️ \"word embeddings\" in 2020? I do!!!'\n\nFirst, replace all interrupting punctuation signs — such as commas and exclamation marks — with periods.\n\n# Print original corpus\nprint(f'Corpus:  {corpus}')\n\n# Do the substitution\ndata = re.sub(r'[,!?;-]+', '.', corpus)\n\n# Print cleaned corpus\nprint(f'After cleaning punctuation:  {data}')\n\nCorpus:  Who ❤️ \"word embeddings\" in 2020? I do!!!\nAfter cleaning punctuation:  Who ❤️ \"word embeddings\" in 2020. I do.\n\n\nNext, use NLTK’s tokenization engine to split the corpus into individual tokens.\n\n# Print cleaned corpus\nprint(f'Initial string:  {data}')\n\n# Tokenize the cleaned corpus\ndata = nltk.word_tokenize(data)\n\n# Print the tokenized version of the corpus\nprint(f'After tokenization:  {data}')\n\nInitial string:  Who ❤️ \"word embeddings\" in 2020. I do.\nAfter tokenization:  ['Who', '❤️', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '.']\n\n\nFinally, as we saw in the lecture, get rid of numbers and punctuation other than periods, and convert all the remaining tokens to lowercase.\n\n# Print the tokenized version of the corpus\nprint(f'Initial list of tokens:  {data}')\n\n# Filter tokenized corpus using list comprehension\ndata = [ ch.lower() for ch in data\n         if ch.isalpha()\n         or ch == '.'\n         or emoji.get_emoji_regexp().search(ch)\n       ]\n\n# Print the tokenized and filtered version of the corpus\nprint(f'After cleaning:  {data}')\n\nInitial list of tokens:  ['Who', '❤️', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '.']\nAfter cleaning:  ['who', '❤️', 'word', 'embeddings', 'in', '.', 'i', 'do', '.']\n\n\nNote that the heart emoji is considered as a token just like any normal word.\nNow let’s streamline the cleaning and tokenization process by wrapping the previous steps in a function.\n\n# Define the 'tokenize' function that will include the steps previously seen\ndef tokenize(corpus):\n    data = re.sub(r'[,!?;-]+', '.', corpus)\n    data = nltk.word_tokenize(data)  # tokenize string to words\n    data = [ ch.lower() for ch in data\n             if ch.isalpha()\n             or ch == '.'\n             or emoji.get_emoji_regexp().search(ch)\n           ]\n    return data\n\nApply this function to the corpus that you’ll be working on in the rest of this notebook: “I am happy because I am learning”\n\n# Define new corpus\ncorpus = 'I am happy because I am learning'\n\n# Print new corpus\nprint(f'Corpus:  {corpus}')\n\n# Save tokenized version of corpus into 'words' variable\nwords = tokenize(corpus)\n\n# Print the tokenized version of the corpus\nprint(f'Words (tokens):  {words}')\n\nCorpus:  I am happy because I am learning\nWords (tokens):  ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n\n\nNow try it out yourself with your own sentence.\n\n# Run this with any sentence\ntokenize(\"Now it's your turn: try with your own sentence!\")\n\n['now', 'it', 'your', 'turn', 'try', 'with', 'your', 'own', 'sentence', '.']",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "L1 - Data preparation"
    ]
  },
  {
    "objectID": "notes/c2w4/lab01.html#sliding-window-of-words",
    "href": "notes/c2w4/lab01.html#sliding-window-of-words",
    "title": "Word Embeddings First Steps: Data Preparation",
    "section": "Sliding window of words",
    "text": "Sliding window of words\nNow that we have transformed the corpus into a list of clean tokens, we can slide a window of words across this list. For each window we can extract a center word and the context words.\nThe get_windows function in the next cell was introduced in the lecture.\n\n# Define the 'get_windows' function\ndef get_windows(words, C):\n    i = C\n    while i &lt; len(words) - C:\n        center_word = words[i]\n        context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n        yield context_words, center_word\n        i += 1\n\nThe first argument of this function is a list of words (or tokens). The second argument, C, is the context half-size. Recall that for a given center word, the context words are made of C words to the left and C words to the right of the center word.\nHere is how we can use this function to extract context words and center words from a list of tokens. These context and center words will make up the training set that we will use to train the CBOW model.\n\n# Print 'context_words' and 'center_word' for the new corpus with a 'context half-size' of 2\nfor x, y in get_windows(['i', 'am', 'happy', 'because', 'i', 'am', 'learning'], 2):\n    print(f'{x}\\t{y}')\n\n['i', 'am', 'because', 'i'] happy\n['am', 'happy', 'i', 'am']  because\n['happy', 'because', 'am', 'learning']  i\n\n\nThe first example of the training set is made of:\n\nthe context words “i”, “am”, “because”, “i”,\nand the center word to be predicted: “happy”.\n\nNow try it out yourself. In the next cell, we can change both the sentence and the context half-size.\n\n# Print 'context_words' and 'center_word' for any sentence with a 'context half-size' of 1\nfor x, y in get_windows(tokenize(\"Now it's your turn: try with your own sentence!\"), 1):\n    print(f'{x}\\t{y}')\n\n['now', 'your'] it\n['it', 'turn']  your\n['your', 'try'] turn\n['turn', 'with']    try\n['try', 'your'] with\n['with', 'own'] your\n['your', 'sentence']    own\n['own', '.']    sentence",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "L1 - Data preparation"
    ]
  },
  {
    "objectID": "notes/c2w4/lab01.html#transforming-words-into-vectors-for-the-training-set",
    "href": "notes/c2w4/lab01.html#transforming-words-into-vectors-for-the-training-set",
    "title": "Word Embeddings First Steps: Data Preparation",
    "section": "Transforming words into vectors for the training set",
    "text": "Transforming words into vectors for the training set\nTo finish preparing the training set, we need to transform the context words and center words into vectors.\n\nMapping words to indices and indices to words\nThe center words will be represented as one-hot vectors, and the vectors that represent context words are also based on one-hot vectors.\nTo create one-hot word vectors, we can start by mapping each unique word to a unique integer (or index). We have provided a helper function, get_dict, that creates a Python dictionary that maps words to integers and back.\n\n# Get 'word2Ind' and 'Ind2word' dictionaries for the tokenized corpus\nword2Ind, Ind2word = get_dict(words)\n\nHere’s the dictionary that maps words to numeric indices.\n\n# Print 'word2Ind' dictionary\nword2Ind\n\n{'am': 0, 'because': 1, 'happy': 2, 'i': 3, 'learning': 4}\n\n\nWe can use this dictionary to get the index of a word.\n\n# Print value for the key 'i' within word2Ind dictionary\nprint(\"Index of the word 'i':  \",word2Ind['i'])\n\nIndex of the word 'i':   3\n\n\nAnd conversely, here’s the dictionary that maps indices to words.\n\n# Print 'Ind2word' dictionary\nInd2word\n\n{0: 'am', 1: 'because', 2: 'happy', 3: 'i', 4: 'learning'}\n\n\n\n# Print value for the key '2' within Ind2word dictionary\nprint(\"Word which has index 2:  \",Ind2word[2] )\n\nWord which has index 2:   happy\n\n\nFinally, get the length of either of these dictionaries to get the size of the vocabulary of your corpus, in other words the number of different words making up the corpus.\n\n# Save length of word2Ind dictionary into the 'V' variable\nV = len(word2Ind)\n\n# Print length of word2Ind dictionary\nprint(\"Size of vocabulary: \", V)\n\nSize of vocabulary:  5\n\n\n\n\nGetting one-hot word vectors\nRecall from the lecture that we can easily convert an integer, n, into a one-hot vector.\nConsider the word “happy”. First, retrieve its numeric index.\n\n# Save index of word 'happy' into the 'n' variable\nn = word2Ind['happy']\n\n# Print index of word 'happy'\nn\n\n2\n\n\nNow create a vector with the size of the vocabulary, and fill it with zeros.\n\n# Create vector with the same length as the vocabulary, filled with zeros\ncenter_word_vector = np.zeros(V)\n\n# Print vector\ncenter_word_vector\n\narray([0., 0., 0., 0., 0.])\n\n\nWe can confirm that the vector has the right size.\n\n# Assert that the length of the vector is the same as the size of the vocabulary\nlen(center_word_vector) == V\n\nTrue\n\n\nNext, replace the 0 of the n-th element with a 1.\n\n# Replace element number 'n' with a 1\ncenter_word_vector[n] = 1\n\nAnd we have your one-hot word vector.\n\n# Print vector\ncenter_word_vector\n\narray([0., 0., 1., 0., 0.])\n\n\nWe can now group all of these steps in a convenient function, which takes as parameters: a word to be encoded, a dictionary that maps words to indices, and the size of the vocabulary.\n\n# Define the 'word_to_one_hot_vector' function that will include the steps previously seen\ndef word_to_one_hot_vector(word, word2Ind, V):\n    one_hot_vector = np.zeros(V)\n    one_hot_vector[word2Ind[word]] = 1\n    return one_hot_vector\n\nCheck that it works as intended.\n\n# Print output of 'word_to_one_hot_vector' function for word 'happy'\nword_to_one_hot_vector('happy', word2Ind, V)\n\narray([0., 0., 1., 0., 0.])\n\n\nWhat is the word vector for “learning”?\n\n# Print output of 'word_to_one_hot_vector' function for word 'learning'\nword_to_one_hot_vector('learning', word2Ind, V)\n\narray([0., 0., 0., 0., 1.])\n\n\nExpected output:\narray([0., 0., 0., 0., 1.])\n\n\nGetting context word vectors\nTo create the vectors that represent context words, we will calculate the average of the one-hot vectors representing the individual words.\nLet’s start with a list of context words.\n\n# Define list containing context words\ncontext_words = ['i', 'am', 'because', 'i']\n\nUsing Python’s list comprehension construct and the word_to_one_hot_vector function that we created in the previous section, we can create a list of one-hot vectors representing each of the context words.\n\n# Create one-hot vectors for each context word using list comprehension\ncontext_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n\n# Print one-hot vectors for each context word\ncontext_words_vectors\n\n[array([0., 0., 0., 1., 0.]),\n array([1., 0., 0., 0., 0.]),\n array([0., 1., 0., 0., 0.]),\n array([0., 0., 0., 1., 0.])]\n\n\nAnd we can now simply get the average of these vectors using numpy’s mean function, to get the vector representation of the context words.\n\n# Compute mean of the vectors using numpy\nnp.mean(context_words_vectors, axis=0)\n\narray([0.25, 0.25, 0.  , 0.5 , 0.  ])\n\n\nNote the axis=0 parameter that tells mean to calculate the average of the rows (if we had wanted the average of the columns, we would have used axis=1).\nNow create the context_words_to_vector function that takes in a list of context words, a word-to-index dictionary, and a vocabulary size, and outputs the vector representation of the context words.\n\n# Define the 'context_words_to_vector' function that will include the steps previously seen\ndef context_words_to_vector(context_words, word2Ind, V):\n    context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n    context_words_vectors = np.mean(context_words_vectors, axis=0)\n    return context_words_vectors\n\nAnd check that we obtain the same output as the manual approach above.\n\n# Print output of 'context_words_to_vector' function for context words: 'i', 'am', 'because', 'i'\ncontext_words_to_vector(['i', 'am', 'because', 'i'], word2Ind, V)\n\narray([0.25, 0.25, 0.  , 0.5 , 0.  ])\n\n\nWhat is the vector representation of the context words “am happy i am”?\n\n# Print output of 'context_words_to_vector' function for context words: 'am', 'happy', 'i', 'am'\ncontext_words_to_vector(['am', 'happy', 'i', 'am'], word2Ind, V)\n\narray([0.5 , 0.  , 0.25, 0.25, 0.  ])\n\n\nExpected output:\narray([0.5 , 0.  , 0.25, 0.25, 0.  ])",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "L1 - Data preparation"
    ]
  },
  {
    "objectID": "notes/c2w4/lab01.html#building-the-training-set",
    "href": "notes/c2w4/lab01.html#building-the-training-set",
    "title": "Word Embeddings First Steps: Data Preparation",
    "section": "Building the training set",
    "text": "Building the training set\nWe can now combine the functions that we created in the previous sections, to build a training set for the CBOW model, starting from the following tokenized corpus.\n\n# Print corpus\nwords\n\n['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n\n\nTo do this we need to use the sliding window function (get_windows) to extract the context words and center words, and we then convert these sets of words into a basic vector representation using word_to_one_hot_vector and context_words_to_vector.\n\n# Print vectors associated to center and context words for corpus\nfor context_words, center_word in get_windows(words, 2):  # reminder: 2 is the context half-size\n    print(f'Context words:  {context_words} -&gt; {context_words_to_vector(context_words, word2Ind, V)}')\n    print(f'Center word:  {center_word} -&gt; {word_to_one_hot_vector(center_word, word2Ind, V)}')\n    print()\n\nContext words:  ['i', 'am', 'because', 'i'] -&gt; [0.25 0.25 0.   0.5  0.  ]\nCenter word:  happy -&gt; [0. 0. 1. 0. 0.]\n\nContext words:  ['am', 'happy', 'i', 'am'] -&gt; [0.5  0.   0.25 0.25 0.  ]\nCenter word:  because -&gt; [0. 1. 0. 0. 0.]\n\nContext words:  ['happy', 'because', 'am', 'learning'] -&gt; [0.25 0.25 0.25 0.   0.25]\nCenter word:  i -&gt; [0. 0. 0. 1. 0.]\n\n\n\nIn this practice notebook you’ll be performing a single iteration of training using a single example, but in this week’s assignment you’ll train the CBOW model using several iterations and batches of example. Here is how we would use a Python generator function (remember the yield keyword from the lecture?) to make it easier to iterate over a set of examples.\n\n# Define the generator function 'get_training_example'\ndef get_training_example(words, C, word2Ind, V):\n    for context_words, center_word in get_windows(words, C):\n        yield context_words_to_vector(context_words, word2Ind, V), word_to_one_hot_vector(center_word, word2Ind, V)\n\nThe output of this function can be iterated on to get successive context word vectors and center word vectors, as demonstrated in the next cell.\n\n# Print vectors associated to center and context words for corpus using the generator function\nfor context_words_vector, center_word_vector in get_training_example(words, 2, word2Ind, V):\n    print(f'Context words vector:  {context_words_vector}')\n    print(f'Center word vector:  {center_word_vector}')\n    print()\n\nContext words vector:  [0.25 0.25 0.   0.5  0.  ]\nCenter word vector:  [0. 0. 1. 0. 0.]\n\nContext words vector:  [0.5  0.   0.25 0.25 0.  ]\nCenter word vector:  [0. 1. 0. 0. 0.]\n\nContext words vector:  [0.25 0.25 0.25 0.   0.25]\nCenter word vector:  [0. 0. 0. 1. 0.]\n\n\n\nYour training set is ready, we can now move on to the CBOW model itself which will be covered in the next lecture notebook.\nCongratulations on finishing this lecture notebook! Hopefully we now have a better understanding of how to prepare your data before feeding it to a continuous bag-of-words model.\nKeep it up!",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "L1 - Data preparation"
    ]
  },
  {
    "objectID": "notes/c2w4/lab04.html",
    "href": "notes/c2w4/lab04.html",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "",
    "text": "course banner\nIn this ungraded notebook, you’ll try out all the individual techniques that we learned about in the lecture. Practicing on small examples will prepare we for the graded assignment, where we will combine the techniques in more advanced ways to create word embeddings from a real-life corpus.\nThis notebook is made of two main parts:\nTo get started, import and initialize all the libraries we will need.\nimport sys\n!{sys.executable} -m pip install emoji\n\nRequirement already satisfied: emoji in /home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages (1.4.1)\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize\nimport emoji\nimport numpy as np\n\nfrom utils2 import get_dict\n\nnltk.download('punkt')  # download pre-trained Punkt tokenizer for English\n\n[nltk_data] Downloading package punkt to /home/oren/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\nTrue"
  },
  {
    "objectID": "notes/c2w4/lab04.html#cleaning-and-tokenization",
    "href": "notes/c2w4/lab04.html#cleaning-and-tokenization",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Cleaning and tokenization",
    "text": "Cleaning and tokenization\nTo demonstrate the cleaning and tokenization process, consider a corpus that contains emojis and various punctuation signs.\n\ncorpus = 'Who ❤️ \"word embeddings\" in 2020? I do!!!'\n\nFirst, replace all interrupting punctuation signs — such as commas and exclamation marks — with periods.\n\nprint(f'Corpus:  {corpus}')\ndata = re.sub(r'[,!?;-]+', '.', corpus)\nprint(f'After cleaning punctuation:  {data}')\n\nCorpus:  Who ❤️ \"word embeddings\" in 2020? I do!!!\nAfter cleaning punctuation:  Who ❤️ \"word embeddings\" in 2020. I do.\n\n\nNext, use NLTK’s tokenization engine to split the corpus into individual tokens.\n\nprint(f'Initial string:  {data}')\ndata = nltk.word_tokenize(data)\nprint(f'After tokenization:  {data}')\n\nInitial string:  Who ❤️ \"word embeddings\" in 2020. I do.\nAfter tokenization:  ['Who', '❤️', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '.']\n\n\nFinally, as we saw in the lecture, get rid of numbers and punctuation other than periods, and convert all the remaining tokens to lowercase.\n\nprint(f'Initial list of tokens:  {data}')\ndata = [ ch.lower() for ch in data\n         if ch.isalpha()\n         or ch == '.'\n         or emoji.get_emoji_regexp().search(ch)\n       ]\nprint(f'After cleaning:  {data}')\n\nInitial list of tokens:  ['Who', '❤️', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '.']\nAfter cleaning:  ['who', '❤️', 'word', 'embeddings', 'in', '.', 'i', 'do', '.']\n\n\nNote that the heart emoji is considered as a token just like any normal word.\nNow let’s streamline the cleaning and tokenization process by wrapping the previous steps in a function.\n\ndef tokenize(corpus):\n    data = re.sub(r'[,!?;-]+', '.', corpus)\n    data = nltk.word_tokenize(data)  # tokenize string to words\n    data = [ ch.lower() for ch in data\n             if ch.isalpha()\n             or ch == '.'\n             or emoji.get_emoji_regexp().search(ch)\n           ]\n    return data\n\nApply this function to the corpus that you’ll be working on in the rest of this notebook: “I am happy because I am learning”\n\ncorpus = 'I am happy because I am learning'\nprint(f'Corpus:  {corpus}')\nwords = tokenize(corpus)\nprint(f'Words (tokens):  {words}')\n\nCorpus:  I am happy because I am learning\nWords (tokens):  ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n\n\nNow try it out yourself with your own sentence.\n\ntokenize(\"Now it's your turn: try with your own sentence!\")\n\n['now', 'it', 'your', 'turn', 'try', 'with', 'your', 'own', 'sentence', '.']"
  },
  {
    "objectID": "notes/c2w4/lab04.html#sliding-window-of-words",
    "href": "notes/c2w4/lab04.html#sliding-window-of-words",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Sliding window of words",
    "text": "Sliding window of words\nNow that we have transformed the corpus into a list of clean tokens, we can slide a window of words across this list. For each window we can extract a center word and the context words.\nThe get_windows function in the next cell was introduced in the lecture.\n\ndef get_windows(words, C):\n    i = C\n    while i &lt; len(words) - C:\n        center_word = words[i]\n        context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n        yield context_words, center_word\n        i += 1\n\nThe first argument of this function is a list of words (or tokens). The second argument, C, is the context half-size. Recall that for a given center word, the context words are made of C words to the left and C words to the right of the center word.\nHere is how we can use this function to extract context words and center words from a list of tokens. These context and center words will make up the training set that we will use to train the CBOW model.\n\nfor x, y in get_windows(\n            ['i', 'am', 'happy', 'because', 'i', 'am', 'learning'],\n            2\n        ):\n    print(f'{x}\\t{y}')\n\n['i', 'am', 'because', 'i'] happy\n['am', 'happy', 'i', 'am']  because\n['happy', 'because', 'am', 'learning']  i\n\n\nThe first example of the training set is made of:\n\nthe context words “i”, “am”, “because”, “i”,\nand the center word to be predicted: “happy”.\n\nNow try it out yourself. In the next cell, we can change both the sentence and the context half-size.\n\nfor x, y in get_windows(tokenize(\"Now it's your turn: try with your own sentence!\"), 1):\n    print(f'{x}\\t{y}')\n\n['now', 'your'] it\n['it', 'turn']  your\n['your', 'try'] turn\n['turn', 'with']    try\n['try', 'your'] with\n['with', 'own'] your\n['your', 'sentence']    own\n['own', '.']    sentence"
  },
  {
    "objectID": "notes/c2w4/lab04.html#transforming-words-into-vectors-for-the-training-set",
    "href": "notes/c2w4/lab04.html#transforming-words-into-vectors-for-the-training-set",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Transforming words into vectors for the training set",
    "text": "Transforming words into vectors for the training set\nTo finish preparing the training set, we need to transform the context words and center words into vectors.\n\nMapping words to indices and indices to words\nThe center words will be represented as one-hot vectors, and the vectors that represent context words are also based on one-hot vectors.\nTo create one-hot word vectors, we can start by mapping each unique word to a unique integer (or index). We have provided a helper function, get_dict, that creates a Python dictionary that maps words to integers and back.\n\nword2Ind, Ind2word = get_dict(words)\n\nHere’s the dictionary that maps words to numeric indices.\n\nword2Ind\n\n{'am': 0, 'because': 1, 'happy': 2, 'i': 3, 'learning': 4}\n\n\nWe can use this dictionary to get the index of a word.\n\nprint(\"Index of the word 'i':  \",word2Ind['i'])\n\nIndex of the word 'i':   3\n\n\nAnd conversely, here’s the dictionary that maps indices to words.\n\nInd2word\n\n{0: 'am', 1: 'because', 2: 'happy', 3: 'i', 4: 'learning'}\n\n\n\nprint(\"Word which has index 2:  \",Ind2word[2] )\n\nWord which has index 2:   happy\n\n\nFinally, get the length of either of these dictionaries to get the size of the vocabulary of your corpus, in other words the number of different words making up the corpus.\n\nV = len(word2Ind)\nprint(\"Size of vocabulary: \", V)\n\nSize of vocabulary:  5\n\n\n\n\nGetting one-hot word vectors\nRecall from the lecture that we can easily convert an integer, n, into a one-hot vector.\nConsider the word “happy”. First, retrieve its numeric index.\n\nn = word2Ind['happy']\nn\n\n2\n\n\nNow create a vector with the size of the vocabulary, and fill it with zeros.\n\ncenter_word_vector = np.zeros(V)\ncenter_word_vector\n\narray([0., 0., 0., 0., 0.])\n\n\nWe can confirm that the vector has the right size.\n\nlen(center_word_vector) == V\n\nTrue\n\n\nNext, replace the 0 of the n-th element with a 1.\n\ncenter_word_vector[n] = 1\n\nAnd we have your one-hot word vector.\n\ncenter_word_vector\n\narray([0., 0., 1., 0., 0.])\n\n\nWe can now group all of these steps in a convenient function, which takes as parameters: a word to be encoded, a dictionary that maps words to indices, and the size of the vocabulary.\n\ndef word_to_one_hot_vector(word, word2Ind, V):\n    # BEGIN your code here\n    one_hot_vector = np.zeros(V)\n    one_hot_vector[word2Ind[word]] = 1\n    # END your code here\n    return one_hot_vector\n\nCheck that it works as intended.\n\nword_to_one_hot_vector('happy', word2Ind, V)\n\narray([0., 0., 1., 0., 0.])\n\n\nWhat is the word vector for “learning”?\n\n# BEGIN your code here\nword_to_one_hot_vector('learning', word2Ind, V)\n# END your code here\n\narray([0., 0., 0., 0., 1.])\n\n\nExpected output:\narray([0., 0., 0., 0., 1.])\n\n\nGetting context word vectors\nTo create the vectors that represent context words, we will calculate the average of the one-hot vectors representing the individual words.\nLet’s start with a list of context words.\n\ncontext_words = ['i', 'am', 'because', 'i']\n\nUsing Python’s list comprehension construct and the word_to_one_hot_vector function that we created in the previous section, we can create a list of one-hot vectors representing each of the context words.\n\ncontext_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\ncontext_words_vectors\n\n[array([0., 0., 0., 1., 0.]),\n array([1., 0., 0., 0., 0.]),\n array([0., 1., 0., 0., 0.]),\n array([0., 0., 0., 1., 0.])]\n\n\nAnd we can now simply get the average of these vectors using numpy’s mean function, to get the vector representation of the context words.\n\nnp.mean(context_words_vectors, axis=0)\n\narray([0.25, 0.25, 0.  , 0.5 , 0.  ])\n\n\nNote the axis=0 parameter that tells mean to calculate the average of the rows (if we had wanted the average of the columns, we would have used axis=1).\nNow create the context_words_to_vector function that takes in a list of context words, a word-to-index dictionary, and a vocabulary size, and outputs the vector representation of the context words.\n\ndef context_words_to_vector(context_words, word2Ind, V):\n    # BEGIN your code here\n    context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n    context_words_vectors = np.mean(context_words_vectors, axis=0)\n    # END your code here\n    return context_words_vectors\n\nAnd check that we obtain the same output as the manual approach above.\n\ncontext_words_to_vector(['i', 'am', 'because', 'i'], word2Ind, V)\n\narray([0.25, 0.25, 0.  , 0.5 , 0.  ])\n\n\nWhat is the vector representation of the context words “am happy i am”?\n\n# BEGIN your code here\ncontext_words_to_vector(['am', 'happy', 'i', 'am'], word2Ind, V)\n# END your code here\n\narray([0.5 , 0.  , 0.25, 0.25, 0.  ])\n\n\nExpected output:\narray([0.5 , 0.  , 0.25, 0.25, 0.  ])"
  },
  {
    "objectID": "notes/c2w4/lab04.html#building-the-training-set",
    "href": "notes/c2w4/lab04.html#building-the-training-set",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Building the training set",
    "text": "Building the training set\nWe can now combine the functions that we created in the previous sections, to build a training set for the CBOW model, starting from the following tokenized corpus.\n\nwords\n\n['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n\n\nTo do this we need to use the sliding window function (get_windows) to extract the context words and center words, and we then convert these sets of words into a basic vector representation using word_to_one_hot_vector and context_words_to_vector.\n\nfor context_words, center_word in get_windows(words, 2):  # reminder: 2 is the context half-size\n    print(f'Context words:  {context_words} -&gt; {context_words_to_vector(context_words, word2Ind, V)}')\n    print(f'Center word:  {center_word} -&gt; {word_to_one_hot_vector(center_word, word2Ind, V)}')\n    print()\n\nContext words:  ['i', 'am', 'because', 'i'] -&gt; [0.25 0.25 0.   0.5  0.  ]\nCenter word:  happy -&gt; [0. 0. 1. 0. 0.]\n\nContext words:  ['am', 'happy', 'i', 'am'] -&gt; [0.5  0.   0.25 0.25 0.  ]\nCenter word:  because -&gt; [0. 1. 0. 0. 0.]\n\nContext words:  ['happy', 'because', 'am', 'learning'] -&gt; [0.25 0.25 0.25 0.   0.25]\nCenter word:  i -&gt; [0. 0. 0. 1. 0.]\n\n\n\nIn this practice notebook you’ll be performing a single iteration of training using a single example, but in this week’s assignment you’ll train the CBOW model using several iterations and batches of example. Here is how we would use a Python generator function (remember the yield keyword from the lecture?) to make it easier to iterate over a set of examples.\n\ndef get_training_example(words, C, word2Ind, V):\n    for context_words, center_word in get_windows(words, C):\n        yield context_words_to_vector(context_words, word2Ind, V), word_to_one_hot_vector(center_word, word2Ind, V)\n\nThe output of this function can be iterated on to get successive context word vectors and center word vectors, as demonstrated in the next cell.\n\nfor context_words_vector, center_word_vector in get_training_example(words, 2, word2Ind, V):\n    print(f'Context words vector:  {context_words_vector}')\n    print(f'Center word vector:  {center_word_vector}')\n    print()\n\nContext words vector:  [0.25 0.25 0.   0.5  0.  ]\nCenter word vector:  [0. 0. 1. 0. 0.]\n\nContext words vector:  [0.5  0.   0.25 0.25 0.  ]\nCenter word vector:  [0. 1. 0. 0. 0.]\n\nContext words vector:  [0.25 0.25 0.25 0.   0.25]\nCenter word vector:  [0. 0. 0. 1. 0.]\n\n\n\nYour training set is ready, we can now move on to the CBOW model itself."
  },
  {
    "objectID": "notes/c2w4/lab04.html#activation-functions",
    "href": "notes/c2w4/lab04.html#activation-functions",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Activation functions",
    "text": "Activation functions\nLet’s start by implementing the activation functions, ReLU and softmax.\n\nReLU\nReLU is used to calculate the values of the hidden layer, in the following formulas:\n\\begin{align}\n\\mathbf{z_1} &= \\mathbf{W_1}\\mathbf{x} + \\mathbf{b_1}  \\tag{1} \\\\\n\\mathbf{h} &= \\mathrm{ReLU}(\\mathbf{z_1})  \\tag{2} \\\\\n\\end{align}\nLet’s fix a value for \\mathbf{z_1} as a working example.\n\nnp.random.seed(10)\nz_1 = 10*np.random.rand(5, 1)-5\nz_1\n\narray([[ 2.71320643],\n       [-4.79248051],\n       [ 1.33648235],\n       [ 2.48803883],\n       [-0.01492988]])\n\n\nTo get the ReLU of this vector, we want all the negative values to become zeros.\nFirst create a copy of this vector.\n\nh = z_1.copy()\n\nNow determine which of its values are negative.\n\nh &lt; 0\n\narray([[False],\n       [ True],\n       [False],\n       [False],\n       [ True]])\n\n\nWe can now simply set all of the values which are negative to 0.\n\nh[h &lt; 0] = 0\n\nAnd that’s it: we have the ReLU of \\mathbf{z_1}!\n\nh\n\narray([[2.71320643],\n       [0.        ],\n       [1.33648235],\n       [2.48803883],\n       [0.        ]])\n\n\nNow implement ReLU as a function.\n\ndef relu(z):\n    # BEGIN your code here\n    result = z.copy()\n    result[result &lt; 0] = 0\n    # END your code here\n    \n    return result\n\nAnd check that it’s working.\n\nz = np.array([[-1.25459881], [ 4.50714306], [ 2.31993942], [ 0.98658484], [-3.4398136 ]])\nrelu(z)\n\narray([[0.        ],\n       [4.50714306],\n       [2.31993942],\n       [0.98658484],\n       [0.        ]])\n\n\nExpected output:\narray([[0.        ],\n       [4.50714306],\n       [2.31993942],\n       [0.98658484],\n       [0.        ]])\n\n\nSoftmax\nThe second activation function that we need is softmax. This function is used to calculate the values of the output layer of the neural network, using the following formulas:\n\\begin{align}\n\\mathbf{z_2} &= \\mathbf{W_2}\\mathbf{h} + \\mathbf{b_2}   \\tag{3} \\\\\n\\mathbf{\\hat y} &= \\mathrm{softmax}(\\mathbf{z_2})   \\tag{4} \\\\\n\\end{align}\nTo calculate softmax of a vector \\mathbf{z}, the i-th component of the resulting vector is given by:\n \\textrm{softmax}(\\textbf{z})_i = \\frac{e^{z_i} }{\\sum\\limits_{j=1}^{V} e^{z_j} }  \\tag{5} \nLet’s work through an example.\n\nz = np.array([9, 8, 11, 10, 8.5])\nz\n\narray([ 9. ,  8. , 11. , 10. ,  8.5])\n\n\nYou’ll need to calculate the exponentials of each element, both for the numerator and for the denominator.\n\ne_z = np.exp(z)\ne_z\n\narray([ 8103.08392758,  2980.95798704, 59874.1417152 , 22026.46579481,\n        4914.7688403 ])\n\n\nThe denominator is equal to the sum of these exponentials.\n\nsum_e_z = np.sum(e_z)\nsum_e_z\n\nnp.float64(97899.41826492078)\n\n\nAnd the value of the first element of \\textrm{softmax}(\\textbf{z}) is given by:\n\ne_z[0]/sum_e_z\n\nnp.float64(0.08276947985173956)\n\n\nThis is for one element. We can use numpy’s vectorized operations to calculate the values of all the elements of the \\textrm{softmax}(\\textbf{z}) vector in one go.\nImplement the softmax function.\n\ndef softmax(z):\n    # BEGIN your code here\n    e_z = np.exp(z)\n    sum_e_z = np.sum(e_z)\n    return e_z / sum_e_z\n    # END your code here\n\nNow check that it works.\n\nsoftmax([9, 8, 11, 10, 8.5])\n\narray([0.08276948, 0.03044919, 0.61158833, 0.22499077, 0.05020223])\n\n\nExpected output:\narray([0.08276948, 0.03044919, 0.61158833, 0.22499077, 0.05020223])"
  },
  {
    "objectID": "notes/c2w4/lab04.html#dimensions-1-d-arrays-vs-2-d-column-vectors",
    "href": "notes/c2w4/lab04.html#dimensions-1-d-arrays-vs-2-d-column-vectors",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Dimensions: 1-D arrays vs 2-D column vectors",
    "text": "Dimensions: 1-D arrays vs 2-D column vectors\nBefore moving on to implement forward propagation, backpropagation, and gradient descent, let’s have a look at the dimensions of the vectors you’ve been handling until now.\nCreate a vector of length V filled with zeros.\n\nx_array = np.zeros(V)\nx_array\n\narray([0., 0., 0., 0., 0.])\n\n\nThis is a 1-dimensional array, as revealed by the .shape property of the array.\n\nx_array.shape\n\n(5,)\n\n\nTo perform matrix multiplication in the next steps, we actually need your column vectors to be represented as a matrix with one column. In numpy, this matrix is represented as a 2-dimensional array.\nThe easiest way to convert a 1D vector to a 2D column matrix is to set its .shape property to the number of rows and one column, as shown in the next cell.\n\nx_column_vector = x_array.copy()\nx_column_vector.shape = (V, 1)  # alternatively ... = (x_array.shape[0], 1)\nx_column_vector\n\narray([[0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.]])\n\n\nThe shape of the resulting “vector” is:\n\nx_column_vector.shape\n\n(5, 1)\n\n\nSo we now have a 5x1 matrix that we can use to perform standard matrix multiplication."
  },
  {
    "objectID": "notes/c2w4/lab04.html#forward-propagation",
    "href": "notes/c2w4/lab04.html#forward-propagation",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Forward propagation",
    "text": "Forward propagation\nLet’s dive into the neural network itself, which is shown below with all the dimensions and formulas you’ll need.\n\n Figure 2\n\nSet N equal to 3. Remember that N is a hyperparameter of the CBOW model that represents the size of the word embedding vectors, as well as the size of the hidden layer.\n\nN = 3\n\n\nInitialization of the weights and biases\nBefore we start training the neural network, we need to initialize the weight matrices and bias vectors with random values.\nIn the assignment we will implement a function to do this yourself using numpy.random.rand. In this notebook, we’ve pre-populated these matrices and vectors for you.\n\nW1 = np.array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n               [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n               [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n\nW2 = np.array([[-0.22182064, -0.43008631,  0.13310965],\n               [ 0.08476603,  0.08123194,  0.1772054 ],\n               [ 0.1871551 , -0.06107263, -0.1790735 ],\n               [ 0.07055222, -0.02015138,  0.36107434],\n               [ 0.33480474, -0.39423389, -0.43959196]])\n\nb1 = np.array([[ 0.09688219],\n               [ 0.29239497],\n               [-0.27364426]])\n\nb2 = np.array([[ 0.0352008 ],\n               [-0.36393384],\n               [-0.12775555],\n               [-0.34802326],\n               [-0.07017815]])\n\nCheck that the dimensions of these matrices match those shown in the figure above.\n\n# BEGIN your code here\nprint(f'V (vocabulary size): {V}')\nprint(f'N (embedding size / size of the hidden layer): {N}')\nprint(f'size of W1: {W1.shape} (NxV)')\nprint(f'size of b1: {b1.shape} (Nx1)')\nprint(f'size of W2: {W1.shape} (VxN)')\nprint(f'size of b2: {b2.shape} (Vx1)')\n# END your code here\n\nV (vocabulary size): 5\nN (embedding size / size of the hidden layer): 3\nsize of W1: (3, 5) (NxV)\nsize of b1: (3, 1) (Nx1)\nsize of W2: (3, 5) (VxN)\nsize of b2: (5, 1) (Vx1)\n\n\n\n\nTraining example\nRun the next cells to get the first training example, made of the vector representing the context words “i am because i”, and the target which is the one-hot vector representing the center word “happy”.\n\nWe don’t need to worry about the Python syntax, but there are some explanations below if we want to know what’s happening behind the scenes.\n\n\ntraining_examples = get_training_example(words, 2, word2Ind, V)\n\n\nget_training_examples, which uses the yield keyword, is known as a generator. When run, it builds an iterator, which is a special type of object that… we can iterate on (using a for loop for instance), to retrieve the successive values that the function generates.\nIn this case get_training_examples yields training examples, and iterating on training_examples will return the successive training examples.\n\n\nx_array, y_array = next(training_examples)\n\n\nnext is another special keyword, which gets the next available value from an iterator. Here, you’ll get the very first value, which is the first training example. If we run this cell again, you’ll get the next value, and so on until the iterator runs out of values to return.\nIn this notebook next is used because we will only be performing one iteration of training. In this week’s assignment with the full training over several iterations you’ll use regular for loops with the iterator that supplies the training examples.\n\nThe vector representing the context words, which will be fed into the neural network, is:\n\nx_array\n\narray([0.25, 0.25, 0.  , 0.5 , 0.  ])\n\n\nThe one-hot vector representing the center word to be predicted is:\n\ny_array\n\narray([0., 0., 1., 0., 0.])\n\n\nNow convert these vectors into matrices (or 2D arrays) to be able to perform matrix multiplication on the right types of objects, as explained above.\n\nx = x_array.copy()\nx.shape = (V, 1)\nprint('x')\nprint(x)\nprint()\n\ny = y_array.copy()\ny.shape = (V, 1)\nprint('y')\nprint(y)\n\nx\n[[0.25]\n [0.25]\n [0.  ]\n [0.5 ]\n [0.  ]]\n\ny\n[[0.]\n [0.]\n [1.]\n [0.]\n [0.]]\n\n\n\n\nValues of the hidden layer\nNow that we have initialized all the variables that we need for forward propagation, we can calculate the values of the hidden layer using the following formulas:\n\\begin{align}\n\\mathbf{z_1} = \\mathbf{W_1}\\mathbf{x} + \\mathbf{b_1}  \\tag{1} \\\\\n\\mathbf{h} = \\mathrm{ReLU}(\\mathbf{z_1})  \\tag{2} \\\\\n\\end{align}\nFirst, we can calculate the value of \\mathbf{z_1}.\n\nz1 = np.dot(W1, x) + b1\n\n\n np.dot is numpy’s function for matrix multiplication.\n\nAs expected we get an N by 1 matrix, or column vector with N elements, where N is equal to the embedding size, which is 3 in this example.\n\nz1\n\narray([[ 0.36483875],\n       [ 0.63710329],\n       [-0.3236647 ]])\n\n\nWe can now take the ReLU of \\mathbf{z_1} to get \\mathbf{h}, the vector with the values of the hidden layer.\n\nh = relu(z1)\nh\n\narray([[0.36483875],\n       [0.63710329],\n       [0.        ]])\n\n\nApplying ReLU means that the negative element of \\mathbf{z_1} has been replaced with a zero.\n\n\nValues of the output layer\nHere are the formulas we need to calculate the values of the output layer, represented by the vector \\mathbf{\\hat y}:\n\\begin{align}\n\\mathbf{z_2} &= \\mathbf{W_2}\\mathbf{h} + \\mathbf{b_2}   \\tag{3} \\\\\n\\mathbf{\\hat y} &= \\mathrm{softmax}(\\mathbf{z_2})   \\tag{4} \\\\\n\\end{align}\nFirst, calculate \\mathbf{z_2}.\n\n# BEGIN your code here\nz2 = np.dot(W2, h) + b2\n# END your code here\n\nz2\n\narray([[-0.31973737],\n       [-0.28125477],\n       [-0.09838369],\n       [-0.33512159],\n       [-0.19919612]])\n\n\nExpected output:\narray([[-0.31973737],\n       [-0.28125477],\n       [-0.09838369],\n       [-0.33512159],\n       [-0.19919612]])\nThis is a V by 1 matrix, where V is the size of the vocabulary, which is 5 in this example.\nNow calculate the value of \\mathbf{\\hat y}.\n\n# BEGIN your code here\ny_hat = softmax(z2)\n# END your code here\n\ny_hat\n\narray([[0.18519074],\n       [0.19245626],\n       [0.23107446],\n       [0.18236353],\n       [0.20891502]])\n\n\nExpected output:\narray([[0.18519074],\n       [0.19245626],\n       [0.23107446],\n       [0.18236353],\n       [0.20891502]])\nAs you’ve performed the calculations with random matrices and vectors (apart from the input vector), the output of the neural network is essentially random at this point. The learning process will adjust the weights and biases to match the actual targets better.\nThat being said, what word did the neural network predict?\n\n\nSolution\n\n\nThe neural network predicted the word “happy”: the largest element of \\mathbf{\\hat y} is the third one, and the third word of the vocabulary is “happy”.\n\n\nHere’s how we could implement this in Python:\n\n\nprint(Ind2word[np.argmax(y_hat)])\n\n\nWell done, you’ve completed the forward propagation phase!"
  },
  {
    "objectID": "notes/c2w4/lab04.html#cross-entropy-loss",
    "href": "notes/c2w4/lab04.html#cross-entropy-loss",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Cross-entropy loss",
    "text": "Cross-entropy loss\nNow that we have the network’s prediction, we can calculate the cross-entropy loss to determine how accurate the prediction was compared to the actual target.\n\nRemember that we are working on a single training example, not on a batch of examples, which is why we are using loss and not cost, which is the generalized form of loss.\n\nFirst let’s recall what the prediction was.\n\ny_hat\n\narray([[0.18519074],\n       [0.19245626],\n       [0.23107446],\n       [0.18236353],\n       [0.20891502]])\n\n\nAnd the actual target value is:\n\ny\n\narray([[0.],\n       [0.],\n       [1.],\n       [0.],\n       [0.]])\n\n\nThe formula for cross-entropy loss is:\n J=-\\sum\\limits_{k=1}^{V}y_k\\log{\\hat{y}_k} \\tag{6}\nImplement the cross-entropy loss function.\nHere are a some hints if you’re stuck.\n\n\nHint 1\n\n&lt;p&gt;To multiply two numpy matrices (such as &lt;code&gt;y&lt;/code&gt; and &lt;code&gt;y_hat&lt;/code&gt;) element-wise, we can simply use the &lt;code&gt;*&lt;/code&gt; operator.&lt;/p&gt;\n\n\n\nHint 2\n\n\nOnce we have a vector equal to the element-wise multiplication of y and y_hat, we can use np.sum to calculate the sum of the elements of this vector.\n\n\n\ndef cross_entropy_loss(y_predicted, y_actual):\n    # BEGIN your code here\n    loss = np.sum(-np.log(y_hat)*y)\n    # END your code here\n    return loss\n\nNow use this function to calculate the loss with the actual values of \\mathbf{y} and \\mathbf{\\hat y}.\n\ncross_entropy_loss(y_hat, y)\n\nnp.float64(1.4650152923611108)\n\n\nExpected output:\n1.4650152923611106\nThis value is neither good nor bad, which is expected as the neural network hasn’t learned anything yet.\nThe actual learning will start during the next phase: backpropagation."
  },
  {
    "objectID": "notes/c2w4/lab04.html#backpropagation",
    "href": "notes/c2w4/lab04.html#backpropagation",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Backpropagation",
    "text": "Backpropagation\nThe formulas that we will implement for backpropagation are the following.\n\\begin{align}\n\\frac{\\partial J}{\\partial \\mathbf{W_1}} &= \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right )\\mathbf{x}^\\top \\tag{7}\\\\\n\\frac{\\partial J}{\\partial \\mathbf{W_2}} &= (\\mathbf{\\hat{y}} - \\mathbf{y})\\mathbf{h^\\top} \\tag{8}\\\\\n\\frac{\\partial J}{\\partial \\mathbf{b_1}} &= \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right ) \\tag{9}\\\\\n\\frac{\\partial J}{\\partial \\mathbf{b_2}} &= \\mathbf{\\hat{y}} - \\mathbf{y} \\tag{10}\n\\end{align}\n\nNote: these formulas are slightly simplified compared to the ones in the lecture as you’re working on a single training example, whereas the lecture provided the formulas for a batch of examples. In the assignment you’ll be implementing the latter.\n\nLet’s start with an easy one.\nCalculate the partial derivative of the loss function with respect to \\mathbf{b_2}, and store the result in grad_b2.\n\\frac{\\partial J}{\\partial \\mathbf{b_2}} = \\mathbf{\\hat{y}} - \\mathbf{y} \\tag{10}\n\n# BEGIN your code here\ngrad_b2 = y_hat - y\n# END your code here\n\ngrad_b2\n\narray([[ 0.18519074],\n       [ 0.19245626],\n       [-0.76892554],\n       [ 0.18236353],\n       [ 0.20891502]])\n\n\nExpected output:\narray([[ 0.18519074],\n       [ 0.19245626],\n       [-0.76892554],\n       [ 0.18236353],\n       [ 0.20891502]])\nNext, calculate the partial derivative of the loss function with respect to \\mathbf{W_2}, and store the result in grad_W2.\n\\frac{\\partial J}{\\partial \\mathbf{W_2}} = (\\mathbf{\\hat{y}} - \\mathbf{y})\\mathbf{h^\\top} \\tag{8}\n\nHint: use .T to get a transposed matrix, e.g. h.T returns \\mathbf{h^\\top}.\n\n\n# BEGIN your code here\ngrad_W2 = np.dot(y_hat - y, h.T)\n# END your code here\n\ngrad_W2\n\narray([[ 0.06756476,  0.11798563,  0.        ],\n       [ 0.0702155 ,  0.12261452,  0.        ],\n       [-0.28053384, -0.48988499,  0.        ],\n       [ 0.06653328,  0.1161844 ,  0.        ],\n       [ 0.07622029,  0.13310045,  0.        ]])\n\n\nExpected output:\narray([[ 0.06756476,  0.11798563,  0.        ],\n       [ 0.0702155 ,  0.12261452,  0.        ],\n       [-0.28053384, -0.48988499,  0.        ],\n       [ 0.06653328,  0.1161844 ,  0.        ],\n       [ 0.07622029,  0.13310045,  0.        ]])\nNow calculate the partial derivative with respect to \\mathbf{b_1} and store the result in grad_b1.\n\\frac{\\partial J}{\\partial \\mathbf{b_1}} = \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right ) \\tag{9}\n\n# BEGIN your code here\ngrad_b1 = relu(np.dot(W2.T, y_hat - y))\n# END your code here\n\ngrad_b1\n\narray([[0.        ],\n       [0.        ],\n       [0.17045858]])\n\n\nExpected output:\narray([[0.        ],\n       [0.        ],\n       [0.17045858]])\nFinally, calculate the partial derivative of the loss with respect to \\mathbf{W_1}, and store it in grad_W1.\n\\frac{\\partial J}{\\partial \\mathbf{W_1}} = \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right )\\mathbf{x}^\\top \\tag{7}\n\n# BEGIN your code here\ngrad_W1 = np.dot(relu(np.dot(W2.T, y_hat - y)), x.T)\n# END your code here\n\ngrad_W1\n\narray([[0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.04261464, 0.04261464, 0.        , 0.08522929, 0.        ]])\n\n\nExpected output:\narray([[0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.04261464, 0.04261464, 0.        , 0.08522929, 0.        ]])\nBefore moving on to gradient descent, double-check that all the matrices have the expected dimensions.\n\n# BEGIN your code here\nprint(f'V (vocabulary size): {V}')\nprint(f'N (embedding size / size of the hidden layer): {N}')\nprint(f'size of grad_W1: {grad_W1.shape} (NxV)')\nprint(f'size of grad_b1: {grad_b1.shape} (Nx1)')\nprint(f'size of grad_W2: {grad_W1.shape} (VxN)')\nprint(f'size of grad_b2: {grad_b2.shape} (Vx1)')\n# END your code here\n\nV (vocabulary size): 5\nN (embedding size / size of the hidden layer): 3\nsize of grad_W1: (3, 5) (NxV)\nsize of grad_b1: (3, 1) (Nx1)\nsize of grad_W2: (3, 5) (VxN)\nsize of grad_b2: (5, 1) (Vx1)"
  },
  {
    "objectID": "notes/c2w4/lab04.html#gradient-descent",
    "href": "notes/c2w4/lab04.html#gradient-descent",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Gradient descent",
    "text": "Gradient descent\nDuring the gradient descent phase, we will update the weights and biases by subtracting \\alpha times the gradient from the original matrices and vectors, using the following formulas.\n\\begin{align}\n\\mathbf{W_1} &:= \\mathbf{W_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_1}} \\tag{11}\\\\\n\\mathbf{W_2} &:= \\mathbf{W_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_2}} \\tag{12}\\\\\n\\mathbf{b_1} &:= \\mathbf{b_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_1}} \\tag{13}\\\\\n\\mathbf{b_2} &:= \\mathbf{b_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_2}} \\tag{14}\\\\\n\\end{align}\nFirst, let set a value for \\alpha.\n\nalpha = 0.03\n\nThe updated weight matrix \\mathbf{W_1} will be:\n\nW1_new = W1 - alpha * grad_W1\n\nLet’s compare the previous and new values of \\mathbf{W_1}:\n\nprint('old value of W1:')\nprint(W1)\nprint()\nprint('new value of W1:')\nprint(W1_new)\n\nold value of W1:\n[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]\n [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]\n [ 0.26637602 -0.23846886 -0.37770863 -0.11399446  0.34008124]]\n\nnew value of W1:\n[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]\n [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]\n [ 0.26509758 -0.2397473  -0.37770863 -0.11655134  0.34008124]]\n\n\nThe difference is very subtle (hint: take a closer look at the last row), which is why it takes a fair amount of iterations to train the neural network until it reaches optimal weights and biases starting from random values.\nNow calculate the new values of \\mathbf{W_2} (to be stored in W2_new), \\mathbf{b_1} (in b1_new), and \\mathbf{b_2} (in b2_new).\n\\begin{align}\n\\mathbf{W_2} &:= \\mathbf{W_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_2}} \\tag{12}\\\\\n\\mathbf{b_1} &:= \\mathbf{b_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_1}} \\tag{13}\\\\\n\\mathbf{b_2} &:= \\mathbf{b_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_2}} \\tag{14}\\\\\n\\end{align}\n\n# BEGIN your code here\nW2_new = W2 - alpha * grad_W2\nb1_new = b1 - alpha * grad_b1\nb2_new = b2 - alpha * grad_b2\n# END your code here\n\nprint('W2_new')\nprint(W2_new)\nprint()\nprint('b1_new')\nprint(b1_new)\nprint()\nprint('b2_new')\nprint(b2_new)\n\nW2_new\n[[-0.22384758 -0.43362588  0.13310965]\n [ 0.08265956  0.0775535   0.1772054 ]\n [ 0.19557112 -0.04637608 -0.1790735 ]\n [ 0.06855622 -0.02363691  0.36107434]\n [ 0.33251813 -0.3982269  -0.43959196]]\n\nb1_new\n[[ 0.09688219]\n [ 0.29239497]\n [-0.27875802]]\n\nb2_new\n[[ 0.02964508]\n [-0.36970753]\n [-0.10468778]\n [-0.35349417]\n [-0.0764456 ]]\n\n\nExpected output:\nW2_new\n[[-0.22384758 -0.43362588  0.13310965]\n [ 0.08265956  0.0775535   0.1772054 ]\n [ 0.19557112 -0.04637608 -0.1790735 ]\n [ 0.06855622 -0.02363691  0.36107434]\n [ 0.33251813 -0.3982269  -0.43959196]]\n\nb1_new\n[[ 0.09688219]\n [ 0.29239497]\n [-0.27875802]]\n\nb2_new\n[[ 0.02964508]\n [-0.36970753]\n [-0.10468778]\n [-0.35349417]\n [-0.0764456 ]]\nCongratulations, we have completed one iteration of training using one training example!\nYou’ll need many more iterations to fully train the neural network, and we can optimize the learning process by training on batches of examples, as described in the lecture. We will get to do this during this week’s assignment."
  },
  {
    "objectID": "notes/c2w4/lab04.html#extracting-word-embedding-vectors",
    "href": "notes/c2w4/lab04.html#extracting-word-embedding-vectors",
    "title": "Word Embeddings: Ungraded Practice Notebook",
    "section": "Extracting word embedding vectors",
    "text": "Extracting word embedding vectors\nOnce we have finished training the neural network, we have three options to get word embedding vectors for the words of your vocabulary, based on the weight matrices \\mathbf{W_1} and/or \\mathbf{W_2}.\n\nOption 1: extract embedding vectors from \\mathbf{W_1}\nThe first option is to take the columns of \\mathbf{W_1} as the embedding vectors of the words of the vocabulary, using the same order of the words as for the input and output vectors.\n\nNote: in this practice notebook the values of the word embedding vectors are meaningless after a single iteration with just one training example, but here’s how we would proceed after the training process is complete.\n\nFor example \\mathbf{W_1} is this matrix:\n\nW1\n\narray([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n       [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n       [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n\n\nThe first column, which is a 3-element vector, is the embedding vector of the first word of your vocabulary. The second column is the word embedding vector for the second word, and so on.\nThe first, second, etc. words are ordered as follows.\n\nfor i in range(V):\n    print(Ind2word[i])\n\nam\nbecause\nhappy\ni\nlearning\n\n\nSo the word embedding vectors corresponding to each word are:\n\n# loop through each word of the vocabulary\nfor word in word2Ind:\n    # extract the column corresponding to the index of the word in the vocabulary\n    word_embedding_vector = W1[:, word2Ind[word]]\n    \n    print(f'{word}: {word_embedding_vector}')\n\nam: [0.41687358 0.32735501 0.26637602]\nbecause: [ 0.08854191  0.22795148 -0.23846886]\nhappy: [-0.23495225 -0.23951958 -0.37770863]\ni: [ 0.28320538  0.4117634  -0.11399446]\nlearning: [ 0.41800106 -0.23924344  0.34008124]\n\n\n\n\nOption 2: extract embedding vectors from \\mathbf{W_2}\nThe second option is to take \\mathbf{W_2} transposed, and take its columns as the word embedding vectors just like we did for \\mathbf{W_1}.\n\nW2.T\n\narray([[-0.22182064,  0.08476603,  0.1871551 ,  0.07055222,  0.33480474],\n       [-0.43008631,  0.08123194, -0.06107263, -0.02015138, -0.39423389],\n       [ 0.13310965,  0.1772054 , -0.1790735 ,  0.36107434, -0.43959196]])\n\n\n\n# loop through each word of the vocabulary\nfor word in word2Ind:\n    # extract the column corresponding to the index of the word in the vocabulary\n    word_embedding_vector = W2.T[:, word2Ind[word]]\n    \n    print(f'{word}: {word_embedding_vector}')\n\nam: [-0.22182064 -0.43008631  0.13310965]\nbecause: [0.08476603 0.08123194 0.1772054 ]\nhappy: [ 0.1871551  -0.06107263 -0.1790735 ]\ni: [ 0.07055222 -0.02015138  0.36107434]\nlearning: [ 0.33480474 -0.39423389 -0.43959196]\n\n\n\n\nOption 3: extract embedding vectors from \\mathbf{W_1} and \\mathbf{W_2}\nThe third option, which is the one we will use in this week’s assignment, uses the average of \\mathbf{W_1} and \\mathbf{W_2^\\top}.\nCalculate the average of \\mathbf{W_1} and \\mathbf{W_2^\\top}, and store the result in W3.\n\n# BEGIN your code here\nW3 = (W1+W2.T)/2\n# END your code here\n\nW3\n\narray([[ 0.09752647,  0.08665397, -0.02389858,  0.1768788 ,  0.3764029 ],\n       [-0.05136565,  0.15459171, -0.15029611,  0.19580601, -0.31673866],\n       [ 0.19974284, -0.03063173, -0.27839106,  0.12353994, -0.04975536]])\n\n\nExpected output:\narray([[ 0.09752647,  0.08665397, -0.02389858,  0.1768788 ,  0.3764029 ],\n       [-0.05136565,  0.15459171, -0.15029611,  0.19580601, -0.31673866],\n       [ 0.19974284, -0.03063173, -0.27839106,  0.12353994, -0.04975536]])\nExtracting the word embedding vectors works just like the two previous options, by taking the columns of the matrix you’ve just created.\n\n# loop through each word of the vocabulary\nfor word in word2Ind:\n    # extract the column corresponding to the index of the word in the vocabulary\n    word_embedding_vector = W3[:, word2Ind[word]]\n    \n    print(f'{word}: {word_embedding_vector}')\n\nam: [ 0.09752647 -0.05136565  0.19974284]\nbecause: [ 0.08665397  0.15459171 -0.03063173]\nhappy: [-0.02389858 -0.15029611 -0.27839106]\ni: [0.1768788  0.19580601 0.12353994]\nlearning: [ 0.3764029  -0.31673866 -0.04975536]\n\n\nYou’re now ready to take on this week’s assignment!\n\n\nHow this practice relates to and differs from the upcoming graded assignment\n\nIn the assignment, for each iteration of training we will use batches of examples instead of a single example. The formulas for forward propagation and backpropagation will be modified accordingly, and we will use cross-entropy cost instead of cross-entropy loss.\nWe will also complete several iterations of training, until we reach an acceptably low cross-entropy cost, at which point we can extract good word embeddings from the weight matrices.\nAfter extracting the word embedding vectors, we will use principal component analysis (PCA) to visualize the vectors, which will enable we to perform an intrinsic evaluation of the quality of the vectors, as explained in the lecture."
  },
  {
    "objectID": "notes/c4w1/index.html",
    "href": "notes/c4w1/index.html",
    "title": "Neural Machine Translation",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 1\nMy notes for Week 1 of the Natural Language Processing with Attention Labels Course in the Natural Language Processing Specialization Offered by DeepLearning.AI on Coursera",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w1/index.html#sec-sec-video1-intro",
    "href": "notes/c4w1/index.html#sec-sec-video1-intro",
    "title": "Neural Machine Translation",
    "section": "Intro",
    "text": "Intro\nThis course covers most modern practical NLP methods. We’ll use a powerful technique called attention to build several different models. Some of the things we build using the attention mechanism, include a powerful language translation model, an algorithm capable of summarizing texts, a model that can actually answer questions about the piece of text, and a chat bot that we can actually have a conversation with.\nWe also take another look at sentiment analysis.\nWhen it comes to modern deep learning, there’s a sort of new normal, which is to say, most people aren’t actually building and training models from scratch. Instead, it’s more common to download a pre-trained model and then tweak it and find units for your specific use case. In this course, we show we how to build the models from scratch, but we also provide we custom pre-trained models that we created just for you. By training them continuously for weeks on the most powerful TPU clusters that are currently only available to researchers as Google.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w1/index.html#sec-sec-video2-seq2seq",
    "href": "notes/c4w1/index.html#sec-sec-video2-seq2seq",
    "title": "Neural Machine Translation",
    "section": "Seq2Seq",
    "text": "Seq2Seq\n\nOutline:\n\nIntroduction to Neural Machine Translation\nSeq2Seq model and its shortcomings\nSolution for the information bottleneck\n\nThe sequential nature of models we learned in the previous course (RNNs, LSTMs, GRUs) does not allow for speed ups within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. (because we can run different batches or examples in parallel or even different directions)\n\n\n\n\nscreenshot_of_outline_slide\n\nIn other words, if we rely on sequences and we need to know the beginning of a text before being able to compute something about the ending of it, then we can not use parallel computing. We would have to wait until the initial computations are complete. This isn’t good, because if your text is too long, then\n\nIt’ll take a long time for we to process it and\nThere is the information loss mentioned earlier in the text as we approach the end.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w1/index.html#sec-sec-seq2seq-model",
    "href": "notes/c4w1/index.html#sec-sec-seq2seq-model",
    "title": "Neural Machine Translation",
    "section": "Seq2Seq model",
    "text": "Seq2Seq model\n\nIntroduced by Google in 2014\nMaps variable-length sequences to fixed-length memory\nLSTMs and GRUs are typically used to overcome the vanishing gradient problem\n\n\n\n\n\nencoder decoder architecture\n\nTherefore, attention mechanisms have become critical for sequence modeling in various tasks, allowing modeling of dependencies without caring too much about their distance in the input or output sequences.\nin this encoder decoder architecture the yellow block in the middle is the final hidden state produced by the encoder. It’s essentials a compressed representation of the sequence in this case the English sentence. The problem with RNN is they tend to have a bias for representing more recent data.\nOne approach to overcome this issue is to provide the decoder with the attention layer.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w1/index.html#sec-sec-video3-alignment",
    "href": "notes/c4w1/index.html#sec-sec-video3-alignment",
    "title": "Neural Machine Translation",
    "section": "3: Alignment",
    "text": "3: Alignment\nAlignment is an old problem and there are a number of papers on learning to align and translate which helped put attention mechanism into focus.\n\nNEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE 2016\nJointly Learning to Align and Translate with Transformer Models 2019\n\nberliner = citizen of berlin\nberliner = jelly doughnut\nNot all words translate precisely to another word. Adding an attention layers allows the model to give different words more importance when translating another word. This is a good task for an attention layer\nDeveloping intuition about alignment:\nalso check out this page Deep Learning: The Good, the Bad and the Ugly in a 2017 talk, Lukasz Kaiser referred to [K,V] as a memory. We want to manage information better in our model. We keep the information in a memory consisting of keys and values. (It needs to be differentiable so we can use it with back propagation)\nThen we put in the query a sequence and in the keys another sequence (depending on the task they may be the same say for summarization or different for alignment or translation) By combining Q K using a Softmax we get a vector of probabilities each position in the memory is relevant. weight matrix to apply to the values in the memory.\n\nget all of the available hidden states ready for the encoder and do the same for the first hidden states of the decoder. (In the example, there are two encoder hidden states, shown by blue dots, and one decoder hidden states.)\nNext, score each of the encoder hidden states by getting its dot product between each encoder state and decoder hidden states.\n\nA higher score means that the hidden state has greater influence on the output.\nThen we run the scores through a Softmax, squashing them between 0 and 1, and giving the attention distribution.\n\nTake each encoder hidden state, and multiply it by its Softmax score, which is a number between 0 and 1, this results in the alignments vector.\nAdd up everything in the alignments vector to arrive at what’s called the context vector.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w1/index.html#sec-w1v4-attention",
    "href": "notes/c4w1/index.html#sec-w1v4-attention",
    "title": "Neural Machine Translation",
    "section": "Attention",
    "text": "Attention\nThe attention mechanism uses encoded representations of both the input or the encoder hidden states and the outputs or the decoder hidden states. The keys and values are pairs. Both of dimension NN, where NN is the input sequence length and comes from the encoder hidden states. Keys and values have their own respective matrices, but the matrices have the same shape and are often the same. While the queries come from the decoder hidden states. One way we can think of it is as follows. Imagine that we are translating English into German. We can represent the word embeddings in the English language as keys and values. The queries will then be the German equivalent. We can then calculate the dot product between the query and the key. Note that similar vectors have higher dot products and non-similar vectors will have lower dot products. The intuition here is that we want to identify the corresponding words in the queries that are similar to the keys. This would allow your model to “look” or focus on the right place when translating each word.\nWe then run a Softmax:\n\nsoftmax(QK^T )  \n\\tag{1}\nThat gives a distribution of numbers between 0 and 1.\nWe then would multiply the output by V. Remember V in this example was the same as our keys, corresponding to the English word embeddings. Hence the equation becomes\n\nsoftmax(QK^T )V  \n {#sec-softmax-formula-2}\nIn the matrix, the lighter square shows where the model is actually looking when making the translation of that word. This mapping isn’t necessarily be one to one. The lighting just tells we to what extent is each word contributing to the input that’s fed into the decoder. As we can see several words can contribute to translating another word, depending on the weights (output) of the softmax that we use to create the new input. a picture of attention in translation with English to German An important thing to keep in mind is that the model should be flexible enough to connect each English word with its relevant German word, even if they don’t appear in the same position in their respective sentences. In other words, it should be flexible enough to handle differences in grammar and word ordering in different languages.\nIn a situation like the one just mentioned, where the grammar of foreign language requires a difference word order than the other, the attention is so flexible enough to find the connection. The first four tokens, the agreements on the, are pretty straightforward, but then the grammatical structure between French and English changes. Now instead of looking at the corresponding fifth token to translate the French word zone, the attention knows to look further down at the eighth token, which corresponds to the English word area, glorious and necessary. It’s pretty amazing, was a little matrix multiplication can do.\nSo attention is a layer of calculations that let your model focus on the most important parts of the sequence for each step. Queries, values, and keys are representations of the encoder and decoder hidden states. And they’re used to retrieve information inside the attention layer by calculating the similarity between the decoder queries and the encoder key- value pairs.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w1/index.html#sec-evaluation-metrics",
    "href": "notes/c4w1/index.html#sec-evaluation-metrics",
    "title": "Neural Machine Translation",
    "section": "Evaluation metrics for Machine Translation",
    "text": "Evaluation metrics for Machine Translation\n\nBLEU\n\nThe authors of (Papineni et al. 2002) introduced the BLEU score.\nThe closer the BLEU score is to 1, the better a model preforms.\nThe closer to 0, the worse it does.\n\nTo get the BLEU score, the candidates and the references are usually based on an average of unigrams, bigrams, trigrams or even four-gram precision. For example using uni-grams:\n\n\n\n\nscreenshot_of_outline_slide\n\nWe would sum over the unique n-gram counts in the candidate and divide by the total number of words in the candidate. The same concept could apply to unigrams, bigrams, etc. One issue with the BLEU score is that it doesn’t take into account semantics, so it doesn’t take into account the order of the n-grams in the sentence.\n\nBLEU = BP\\Bigl(\\prod_{i=1}^{4}\\text{precision}_i\\Bigr)^{(1/4)}\n\\tag{2}\nwith the Brevity Penalty and precision defined as:\n\nBP = min\\Bigl(1, e^{(1-({ref}/{cand}))}\\Bigr)\n\\tag{3}\n\n\\text{Precision}_i = \\frac {\\sum_{snt \\in{cand}}\\sum_{i\\in{snt}}min\\Bigl(m^{i}_{cand}, m^{i}_{ref}\\Bigr)}{w^{i}_{t}}\n\\tag{4}\nwhere:\n\nm^{i}_{cand}, is the count of i-gram in candidate matching the reference translation.\nm^{i}_{ref}, is the count of i-gram in the reference translation.\nw^{i}_{t}, is the total number of i-grams in candidate translation.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w1/index.html#sec-rouge",
    "href": "notes/c4w1/index.html#sec-rouge",
    "title": "Neural Machine Translation",
    "section": "ROUGE",
    "text": "ROUGE\n(Lin 2004) introduced a similar method for evaluation called the ROUGE score which calculates precision and recall for machine texts by counting the n-gram overlap between the machine texts and a reference text. Here is an example that calculates recall: \n\nRouge_{recall} = \\sum  \\frac{(\\{\\text{prediction n-grams}\\} \\cap \\{ \\text{test n-grams}\\})}{\\vert{ \\text{test n-grams}}\\vert }\n\\tag{5}\nRouge also allows we to compute precision as follows:\n\n\n\n\nprecision in ROUGE\n\n \\text{ROUGE}_{\\text{precision}} = \\sum \\frac{(\\{\\text{prediction n-grams}\\} \\cap \\{ \\text{test ngrams}\\})}{\\vert\\{ \\text{vocab}\\}\\vert}\n\\tag{6}\nThe ROUGE-N refers to the overlap of N-grams between the actual system and the reference summaries. The F-score metric combines Recall and precision into one metric.\n\nF_{score}= 2 \\times \\frac{(\\text{precision} \\times \\text{recall})}{(\\text{precision} + \\text{recall})}\n\\tag{7}",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w1/index.html#sec-random-sampling",
    "href": "notes/c4w1/index.html#sec-random-sampling",
    "title": "Neural Machine Translation",
    "section": "Random sampling",
    "text": "Random sampling\nRandom sampling for decoding involves drawing a word from the softmax distribution. To explore the latent space it is possible to introduce a temperature variable which controls the randomness of the sample.\ndef logsoftmax_sample(log_probs, temperature=1.0):  \n  \"\"\"Returns a sample from a log-softmax output, with temperature.\n  Args:\n    log_probs: Logarithms of probabilities (often coming from LogSofmax)\n    temperature: For scaling before sampling (1.0 = default, 0.0 = pick argmax)\n  \"\"\"\n  # This is equivalent to sampling from a softmax with temperature.\n  u = np.random.uniform(low=1e-6, high=1.0 - 1e-6, size=log_probs.shape)\n  g = -np.log(-np.log(u))\n  return np.argmax(log_probs + g * temperature, axis=-1)",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w1/index.html#sec-beam-search",
    "href": "notes/c4w1/index.html#sec-beam-search",
    "title": "Neural Machine Translation",
    "section": "Beam Search",
    "text": "Beam Search\nThe beam search algorithm is a limited (best-first search). The parameter for the beam width limits the choices considered at each step.\n\n\n\n\nBeam Search",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w1/index.html#sec-minimum-bayes-risk",
    "href": "notes/c4w1/index.html#sec-minimum-bayes-risk",
    "title": "Neural Machine Translation",
    "section": "Minimum Bayes Risk",
    "text": "Minimum Bayes Risk\nMBR (Minimum Bayes Risk) Compares many samples against one another. To implement MBR:\n\nGenerate several random samples.\nCompare each sample against all the others and assign a similarity score using ROUGE.\nSelect the sample with the highest similarity: the golden one.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w1/index.html#sec-summary",
    "href": "notes/c4w1/index.html#sec-summary",
    "title": "Neural Machine Translation",
    "section": "Summary",
    "text": "Summary\n\nMaximal Probability is a baseline - but not a particularly good one when the data is noisy.\nRandom sampling with temperature is better.\nBeam search uses conditional probabilities and the parameter.\nMBR takes several samples and compares them against each other to find the golden one.\n\nnote: although not mentioned in the next week’s notes Beam Search is useful for improving the summarization task. We can extract a golden summary from a number of samples using MBR. ROUGE-N is the preferred metric for evaluating summarization\n\nReferences\n\n-​ (Peters et al. 2018)\n(Alammar 2024)\n\n\n\nAlammar, Jay. 2024. “The Illustrated Transformer.” https://jalammar.github.io/illustrated-transformer.\n\n\nLin, Chin-Yew. 2004. “ROUGE: A Package for Automatic Evaluation of Summaries.” In Text Summarization Branches Out, 74–81. Barcelona, Spain: Association for Computational Linguistics. https://aclanthology.org/W04-1013.\n\n\nPapineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. “Bleu: A Method for Automatic Evaluation of Machine Translation.” In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, 311–18. https://www.aclweb.org/anthology/P02-1040.pdf.\n\n\nPeters, Matthew E., Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. “Deep Contextualized Word Representations.” CoRR abs/1802.05365. http://arxiv.org/abs/1802.05365.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w1/lab01.html",
    "href": "notes/c4w1/lab01.html",
    "title": "Stack Semantics in Trax: Ungraded Lab",
    "section": "",
    "text": "In this ungraded lab, we will explain the stack semantics in Trax. This will help in understanding how to use layers like Select and Residual which gets . If you’ve taken a computer science class before, you will recall that a stack is a data structure that follows the Last In, First Out (LIFO) principle. That is, whatever is the latest element that is pushed into the stack will also be the first one to be popped out. If you’re not yet familiar with stacks, then you may find this short tutorial useful. In a nutshell, all you really need to remember is it puts elements one on top of the other. You should be aware of what is on top of the stack to know which element you will be popping. You will see this in the discussions below. Let’s get started!",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "L1 - Stack Semantics"
    ]
  },
  {
    "objectID": "notes/c4w1/lab01.html#imports",
    "href": "notes/c4w1/lab01.html#imports",
    "title": "Stack Semantics in Trax: Ungraded Lab",
    "section": "Imports",
    "text": "Imports\n\nimport numpy as np              # regular ol' numpy\nfrom trax import layers as tl   # core building block\nfrom trax import shapes         # data signatures: dimensionality and type\nfrom trax import fastmath       # uses jax, offers numpy on steroids\n\n2025-02-10 16:59:40.963815: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1739199580.979731  126645 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1739199580.984598  126645 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "L1 - Stack Semantics"
    ]
  },
  {
    "objectID": "notes/c4w1/lab01.html#the-tl.serial-combinator-is-stack-oriented.",
    "href": "notes/c4w1/lab01.html#the-tl.serial-combinator-is-stack-oriented.",
    "title": "Stack Semantics in Trax: Ungraded Lab",
    "section": "1. The tl.Serial Combinator is Stack Oriented.",
    "text": "1. The tl.Serial Combinator is Stack Oriented.\nTo understand how stack-orientation works in Trax, most times one will be using the Serial layer. We will define two simple Function layers: 1) Addition and 2) Multiplication.\nSuppose we want to make the simple calculation (3 + 4) * 15 + 3. Serial will perform the calculations in the following manner 3 4 add 15 mul 3 add. The steps of the calculation are shown in the table below. The first column shows the operations made on the stack and the second column the output of those operations. Moreover, the rightmost element in the second column represents the top of the stack (e.g. in the second row, Push(3) pushes 3 on top of the stack and 4 is now under it).\n\n\n\nAfter processing all the stack contains 108 which is the answer to our simple computation.\nFrom this, the following can be concluded: a stack-based layer has only one way to handle data, by taking one piece of data from atop the stack, termed popping, and putting data back atop the stack, termed pushing. Any expression that can be written conventionally, can be written in this form and thus be amenable to being interpreted by a stack-oriented layer like Serial.\n\nCoding the example in the table:\nDefining addition\n\ndef Addition():\n    layer_name = \"Addition\"  # don't forget to give your custom layer a name to identify\n\n    # Custom function for the custom layer\n    def func(x, y):\n        return x + y\n\n    return tl.Fn(layer_name, func)\n\n\n# Test it\nadd = Addition()\n\n# Inspect properties\nprint(\"-- Properties --\")\nprint(\"name :\", add.name)\nprint(\"expected inputs :\", add.n_in)\nprint(\"promised outputs :\", add.n_out, \"\\n\")\n\n# Inputs\nx = np.array([3])\ny = np.array([4])\nprint(\"-- Inputs --\")\nprint(\"x :\", x, \"\\n\")\nprint(\"y :\", y, \"\\n\")\n\n# Outputs\nz = add((x, y))\nprint(\"-- Outputs --\")\nprint(\"z :\", z)\n\n-- Properties --\nname : Addition\nexpected inputs : 2\npromised outputs : 1 \n\n-- Inputs --\nx : [3] \n\ny : [4] \n\n-- Outputs --\nz : [7]\n\n\nDefining multiplication\n\ndef Multiplication():\n    layer_name = (\n        \"Multiplication\"  # don't forget to give your custom layer a name to identify\n    )\n\n    # Custom function for the custom layer\n    def func(x, y):\n        return x * y\n\n    return tl.Fn(layer_name, func)\n\n\n# Test it\nmul = Multiplication()\n\n# Inspect properties\nprint(\"-- Properties --\")\nprint(\"name :\", mul.name)\nprint(\"expected inputs :\", mul.n_in)\nprint(\"promised outputs :\", mul.n_out, \"\\n\")\n\n# Inputs\nx = np.array([7])\ny = np.array([15])\nprint(\"-- Inputs --\")\nprint(\"x :\", x, \"\\n\")\nprint(\"y :\", y, \"\\n\")\n\n# Outputs\nz = mul((x, y))\nprint(\"-- Outputs --\")\nprint(\"z :\", z)\n\n-- Properties --\nname : Multiplication\nexpected inputs : 2\npromised outputs : 1 \n\n-- Inputs --\nx : [7] \n\ny : [15] \n\n-- Outputs --\nz : [105]\n\n\nImplementing the computations using Serial combinator.\n\n# Serial combinator\nserial = tl.Serial(\n    Addition(), Multiplication(), Addition()  # add 3 + 4  # multiply result by 15\n)\n\n# Initialization\nx = (np.array([3]), np.array([4]), np.array([15]), np.array([3]))  # input\n\nserial.init(shapes.signature(x))  # initializing serial instance\n\n\nprint(\"-- Serial Model --\")\nprint(serial, \"\\n\")\nprint(\"-- Properties --\")\nprint(\"name :\", serial.name)\nprint(\"sublayers :\", serial.sublayers)\nprint(\"expected inputs :\", serial.n_in)\nprint(\"promised outputs :\", serial.n_out, \"\\n\")\n\n# Inputs\nprint(\"-- Inputs --\")\nprint(\"x :\", x, \"\\n\")\n\n# Outputs\ny = serial(x)\nprint(\"-- Outputs --\")\nprint(\"y :\", y)\n\n(((), (), ()), ((), (), ()))\n\n\n-- Serial Model --\nSerial_in4[\n  Addition_in2\n  Multiplication_in2\n  Addition_in2\n] \n\n-- Properties --\nname : Serial\nsublayers : [Addition_in2, Multiplication_in2, Addition_in2]\nexpected inputs : 4\npromised outputs : 1 \n\n-- Inputs --\nx : (array([3]), array([4]), array([15]), array([3])) \n\n-- Outputs --\ny : [108]\n\n\nThe example with the two simple adition and multiplication functions that where coded together with the serial combinator show how stack semantics work in Trax.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "L1 - Stack Semantics"
    ]
  },
  {
    "objectID": "notes/c4w1/lab01.html#the-tl.select-combinator-in-the-context-of-the-serial-combinator",
    "href": "notes/c4w1/lab01.html#the-tl.select-combinator-in-the-context-of-the-serial-combinator",
    "title": "Stack Semantics in Trax: Ungraded Lab",
    "section": "2. The tl.Select combinator in the context of the Serial combinator",
    "text": "2. The tl.Select combinator in the context of the Serial combinator\nHaving understood how stack semantics work in Trax, we will demonstrate how the tl.Select combinator works.\n\nFirst example of tl.Select\nSuppose we want to make the simple calculation (3 + 4) * 3 + 4. We can use Select to perform the calculations in the following manner:\n\n4\n3\ntl.Select([0,1,0,1])\nadd\nmul\nadd.\n\nThe tl.Select requires a list or tuple of 0-based indices to select elements relative to the top of the stack. For our example, the top of the stack is 3 (which is at index 0) then 4 (index 1) and we Select to add in an ordered manner to the top of the stack which after the command is 3 4 3 4. The steps of the calculation for our example are shown in the table below. As in the previous table each column shows the contents of the stack and the outputs after the operations are carried out.\n\n\n\nAfter processing all the inputs the stack contains 25 which is the answer we get above.\n\nserial = tl.Serial(tl.Select([0, 1, 0, 1]), Addition(), Multiplication(), Addition())\n\n# Initialization\nx = (np.array([3]), np.array([4]))  # input\n\nserial.init(shapes.signature(x))  # initializing serial instance\n\n\nprint(\"-- Serial Model --\")\nprint(serial, \"\\n\")\nprint(\"-- Properties --\")\nprint(\"name :\", serial.name)\nprint(\"sublayers :\", serial.sublayers)\nprint(\"expected inputs :\", serial.n_in)\nprint(\"promised outputs :\", serial.n_out, \"\\n\")\n\n# Inputs\nprint(\"-- Inputs --\")\nprint(\"x :\", x, \"\\n\")\n\n# Outputs\ny = serial(x)\nprint(\"-- Outputs --\")\nprint(\"y :\", y)\n\n(((), (), (), ()), ((), (), (), ()))\n\n\n-- Serial Model --\nSerial_in2[\n  Select[0,1,0,1]_in2_out4\n  Addition_in2\n  Multiplication_in2\n  Addition_in2\n] \n\n-- Properties --\nname : Serial\nsublayers : [Select[0,1,0,1]_in2_out4, Addition_in2, Multiplication_in2, Addition_in2]\nexpected inputs : 2\npromised outputs : 1 \n\n-- Inputs --\nx : (array([3]), array([4])) \n\n-- Outputs --\ny : [25]\n\n\n\n\nSecond example of tl.Select\nSuppose we want to make the simple calculation (3 + 4) * 4. We can use Select to perform the calculations in the following manner:\n\n4\n3\ntl.Select([0,1,0,1])\nadd\ntl.Select([0], n_in=2)\nmul\n\nThe example is a bit contrived but it demonstrates the flexibility of the command. The second tl.Select pops two elements (specified in n_in) from the stack starting from index 0 (i.e. top of the stack). This means that 7 and 3 will be popped out because n_in = 2) but only 7 is placed back on top because it only selects [0]. As in the previous table each column shows the contents of the stack and the outputs after the operations are carried out.\n\n\n\nAfter processing all the inputs the stack contains 28 which is the answer we get above.\n\nserial = tl.Serial(\n    tl.Select([0, 1, 0, 1]), Addition(), tl.Select([0], n_in=2), Multiplication()\n)\n\n# Initialization\nx = (np.array([3]), np.array([4]))  # input\n\nserial.init(shapes.signature(x))  # initializing serial instance\n\n\nprint(\"-- Serial Model --\")\nprint(serial, \"\\n\")\nprint(\"-- Properties --\")\nprint(\"name :\", serial.name)\nprint(\"sublayers :\", serial.sublayers)\nprint(\"expected inputs :\", serial.n_in)\nprint(\"promised outputs :\", serial.n_out, \"\\n\")\n\n# Inputs\nprint(\"-- Inputs --\")\nprint(\"x :\", x, \"\\n\")\n\n# Outputs\ny = serial(x)\nprint(\"-- Outputs --\")\nprint(\"y :\", y)\n\n(((), (), (), ()), ((), (), (), ()))\n\n\n-- Serial Model --\nSerial_in2[\n  Select[0,1,0,1]_in2_out4\n  Addition_in2\n  Select[0]_in2\n  Multiplication_in2\n] \n\n-- Properties --\nname : Serial\nsublayers : [Select[0,1,0,1]_in2_out4, Addition_in2, Select[0]_in2, Multiplication_in2]\nexpected inputs : 2\npromised outputs : 1 \n\n-- Inputs --\nx : (array([3]), array([4])) \n\n-- Outputs --\ny : [28]\n\n\nIn summary, what Select does in this example is a copy of the inputs in order to be used further along in the stack of operations.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "L1 - Stack Semantics"
    ]
  },
  {
    "objectID": "notes/c4w1/lab01.html#the-tl.residual-combinator-in-the-context-of-the-serial-combinator",
    "href": "notes/c4w1/lab01.html#the-tl.residual-combinator-in-the-context-of-the-serial-combinator",
    "title": "Stack Semantics in Trax: Ungraded Lab",
    "section": "3. The tl.Residual combinator in the context of the Serial combinator",
    "text": "3. The tl.Residual combinator in the context of the Serial combinator\n\ntl.Residual\nResidual networks are frequently used to make deep models easier to train and you will be using it in the assignment as well. Trax already has a built in layer for this. The Residual layer computes the element-wise sum of the stack-top input with the output of the layer series. For example, if we wanted the cumulative sum of the folowing series of computations (3 + 4) * 3 + 4. The result can be obtained with the use of the Residual combinator in the following manner\n\n4\n3\ntl.Select([0,1,0,1])\nadd\nmul\ntl.Residual.\n\nFor our example the top of the stack is 3 4 and we select to add the same to numbers in an ordered manner to the top of the stack which after the command is 3 4 3 4. The steps of the calculation for our example are shown in the table below together with the cumulative sum which is the result of tl.Residual.\n\n\n\nAfter processing all the inputs the stack contains 50 which is the cumulative sum of all the operations.\n\nserial = tl.Serial(\n    tl.Select([0, 1, 0, 1]), Addition(), Multiplication(), Addition(), tl.Residual()\n)\n\n# Initialization\nx = (np.array([3]), np.array([4]))  # input\n\nserial.init(shapes.signature(x))  # initializing serial instance\n\n\nprint(\"-- Serial Model --\")\nprint(serial, \"\\n\")\nprint(\"-- Properties --\")\nprint(\"name :\", serial.name)\nprint(\"sublayers :\", serial.sublayers)\nprint(\"expected inputs :\", serial.n_in)\nprint(\"promised outputs :\", serial.n_out, \"\\n\")\n\n# Inputs\nprint(\"-- Inputs --\")\nprint(\"x :\", x, \"\\n\")\n\n# Outputs\ny = serial(x)\nprint(\"-- Outputs --\")\nprint(\"y :\", y)\n\n(((), (), (), (), (((), ((), ())), ())),\n ((), (), (), (), (((), ((), ())), ())))\n\n\n-- Serial Model --\nSerial_in2[\n  Select[0,1,0,1]_in2_out4\n  Addition_in2\n  Multiplication_in2\n  Addition_in2\n  Serial[\n    Branch_out2[\n      None\n      Serial\n    ]\n    Add_in2\n  ]\n] \n\n-- Properties --\nname : Serial\nsublayers : [Select[0,1,0,1]_in2_out4, Addition_in2, Multiplication_in2, Addition_in2, Serial[\n  Branch_out2[\n    None\n    Serial\n  ]\n  Add_in2\n]]\nexpected inputs : 2\npromised outputs : 1 \n\n-- Inputs --\nx : (array([3]), array([4])) \n\n-- Outputs --\ny : [50]\n\n\n\n\nA slightly trickier example:\nNormally, the Residual layer will accept a layer as an argument and it will add the output of that layer to the current stack top input. In the example below, you’ll notice that in the last step, we specify tl.Residual(Addition()). If you refer to the same figure above, you’ll notice that the stack at that point has 21 4 where 21 is the top of the stack. The Residual layer remembers this value (i.e. 21) so the result of the Addition() layer nested into it (i.e. 25) is added to this stack top input to arrive at the result: 46.\n\nserial = tl.Serial(\n    tl.Select([0, 1, 0, 1]), Addition(), Multiplication(), tl.Residual(Addition()) \n)\n\n# Initialization\nx = (np.array([3]), np.array([4]))  # input\n\nserial.init(shapes.signature(x))  # initializing serial instance\n\n\nprint(\"-- Serial Model --\")\nprint(serial, \"\\n\")\nprint(\"-- Properties --\")\nprint(\"name :\", serial.name)\nprint(\"sublayers :\", serial.sublayers)\nprint(\"expected inputs :\", serial.n_in)\nprint(\"promised outputs :\", serial.n_out, \"\\n\")\n\n# Inputs\nprint(\"-- Inputs --\")\nprint(\"x :\", x, \"\\n\")\n\n# Outputs\ny = serial(x)\nprint(\"-- Outputs --\")\nprint(\"y :\", y)\n\n(((), (), (), (((), ((), ())), ())), ((), (), (), (((), ((), ())), ())))\n\n\n-- Serial Model --\nSerial_in2[\n  Select[0,1,0,1]_in2_out4\n  Addition_in2\n  Multiplication_in2\n  Serial_in2[\n    Branch_in2_out2[\n      None\n      Addition_in2\n    ]\n    Add_in2\n  ]\n] \n\n-- Properties --\nname : Serial\nsublayers : [Select[0,1,0,1]_in2_out4, Addition_in2, Multiplication_in2, Serial_in2[\n  Branch_in2_out2[\n    None\n    Addition_in2\n  ]\n  Add_in2\n]]\nexpected inputs : 2\npromised outputs : 1 \n\n-- Inputs --\nx : (array([3]), array([4])) \n\n-- Outputs --\ny : [46]",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "L1 - Stack Semantics"
    ]
  },
  {
    "objectID": "notes/c3w2/index.html",
    "href": "notes/c3w2/index.html",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nFigure 1\nMy notes for Week 2 of the Natural Language Processing with Sequence Models Course in the Natural Language Processing Specialization Offered by DeepLearning.AI on Coursera",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w2/index.html#traditional-language-models",
    "href": "notes/c3w2/index.html#traditional-language-models",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "Traditional Language models",
    "text": "Traditional Language models\n\nIn the example above, the second sentence is the one that is most likely to take place as it has the highest probability of happening. To compute the probabilities, we can do the following:\n\nLarge N-grams capture dependencies between distant words and need a lot of space and RAM. Hence, we resort to using different types of alternatives.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w2/index.html#recurrent-neural-networks",
    "href": "notes/c3w2/index.html#recurrent-neural-networks",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "Recurrent Neural Networks",
    "text": "Recurrent Neural Networks\nPreviously, we tried using traditional language models, but it turns out they took a lot of space and RAM. For example, in the sentence below:\n\nAn N-gram (trigram) would only look at “did not” and would try to complete the sentence from there. As a result, the model will not be able to see the beginning of the sentence “I called her but she”. Probably the most likely word is have after “did not”. RNNs help us solve this problem by being able to track dependencies that are much further apart from each other. As the RNN makes its way through a text corpus, it picks up some information as follows:\n\nNote that as we feed in more information into the model, the previous word’s retention gets weaker, but it is still there. Look at the orange rectangle above and see how it becomes smaller as we make your way through the text. This shows that your model is capable of capturing dependencies and remembers a previous word although it is at the beginning of a sentence or paragraph. Another advantage of RNNs is that a lot of the computation shares parameters.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w2/index.html#application-of-rnns",
    "href": "notes/c3w2/index.html#application-of-rnns",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "Application of RNNs",
    "text": "Application of RNNs\nRNNs could be used in a variety of tasks ranging from machine translation to caption generation. There are many ways to implement an RNN model:\n\nOne to One: given some scores of a championship, we can predict the winner.\nOne to Many: given an image, we can predict what the caption is going to be.\nMany to One: given a tweet, we can predict the sentiment of that tweet.\nMany to Many: given an english sentence, we can translate it to its German equivalent.\n\nIn the next video, we will see the math in simple RNNs.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w2/index.html#math-in-simple-rnns",
    "href": "notes/c3w2/index.html#math-in-simple-rnns",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "Math in Simple RNNs",
    "text": "Math in Simple RNNs\nIt is best to explain the math behind a simple RNN with a diagram:\n\nNote that:\n\nh^{&lt;t&gt;} = g(W_{h}[h^{&lt;t-1&gt;}, x^{&lt;t&gt;}] + b_h)\n\nIs the same as multiplying W_{hh} by h and W_{hx} by x. In other words, we can concatenate it as follows:\n\nh^{&lt;t&gt;} = g(W_{hh}[h^{&lt;t-1&gt;} \\oplus W_{hx}x^{&lt;t&gt;}] + b_h)\n\nFor the prediction at each time step, we can use the following:\n\n\\hat{y}^{&lt;t&gt;} = g(W_{yh}h^{&lt;t&gt;} + b_y)\n\nNote that we end up training W_{hh}, W_{hx}, W_{yh}, b_h, and b_y. Here is a visualization of the model.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w2/index.html#lab-lecture-notebook-hidden-state-activation",
    "href": "notes/c3w2/index.html#lab-lecture-notebook-hidden-state-activation",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "Lab: Lecture Notebook: Hidden State Activation",
    "text": "Lab: Lecture Notebook: Hidden State Activation\nHidden State Activation",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w2/index.html#cost-function-for-rnns",
    "href": "notes/c3w2/index.html#cost-function-for-rnns",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "Cost Function for RNNs",
    "text": "Cost Function for RNNs\nThe cost function used in an RNN is the cross entropy loss. If we were to visualize it\n\nwe are basically summing over the all the classes and then multiplying y_j times log(\\hat{y}_j). If we were to compute the loss over several time steps, use the following formula:\n\nJ = -\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{j=1}^{K} y_j^{&lt;t&gt;} log \\hat{y}^{&lt;t&gt;}\n\nwhere T is the number of time steps and K is the number of classes.\nNote that we are simply summing over all the time steps and dividing by T, to get the average cost in each time step. Hence, we are just taking an average through time.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w2/index.html#implementation-note",
    "href": "notes/c3w2/index.html#implementation-note",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "Implementation Note",
    "text": "Implementation Note\nThe scan function is built as follows:\n\nNote, that is basically what an RNN is doing. It takes the initializer, and returns a list of outputs (ys), and uses the current value, to get the next y and the next current value. These type of abstractions allow for much faster computation.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w2/index.html#lab-working-with-jax-numpy-and-calculating-perplexity",
    "href": "notes/c3w2/index.html#lab-working-with-jax-numpy-and-calculating-perplexity",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "Lab: Working with JAX NumPy and Calculating Perplexity",
    "text": "Lab: Working with JAX NumPy and Calculating Perplexity\nWorking with JAX NumPy and Calculating Perplexity",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w2/index.html#gated-recurrent-units",
    "href": "notes/c3w2/index.html#gated-recurrent-units",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "Gated Recurrent Units",
    "text": "Gated Recurrent Units\nGated recurrent units are very similar to vanilla RNNs, except that they have a “relevance” and “update” gate that allow the model to update and get relevant information. I personally find it easier to understand by looking at the formulas:\n\nTo the left, we have the diagram and equations for a simple RNN. To the right, we explain the GRU. Note that we add 3 layers before computing h and y.\n\n\\begin{align*}\n\\Gamma_u &= \\sigma(W_u[h^{&lt;t-1&gt;}, x^{&lt;t&gt;}]+b_u) \\\\\n\\Gamma_r &= \\sigma(W_r[h^{&lt;t-1&gt;}, x^{&lt;t&gt;}]+b_r) \\\\\nh'^{&lt;t&gt;} &= \\tanh(W_h[\\Gamma_r*h^{&lt;t-1&gt;}, x^{&lt;t&gt;}]+b_h) \\\\\nh^{&lt;t&gt;} &= \\Gamma_u*h^{&lt;t-1&gt;} + (1-\\Gamma_u)*h'^{&lt;t&gt;}\n\\end{align*}\n\nThe first gate Γ_u allows we to decide how much we want to update the weights by. The second gate Γ_r, helps we find a relevance score. We can compute the new h by using the relevance gate. Finally we can compute h, using the update gate. GRUs “decide” how to update the hidden state. GRUs help preserve important information.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w2/index.html#lab-vanilla-rnns-grus-and-the-scan-function",
    "href": "notes/c3w2/index.html#lab-vanilla-rnns-grus-and-the-scan-function",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "Lab: Vanilla RNNs, GRUs and the scan function",
    "text": "Lab: Vanilla RNNs, GRUs and the scan function\nVanilla RNNs, GRUs and the scan function",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w2/index.html#lab-creating-a-gru-model-using-trax",
    "href": "notes/c3w2/index.html#lab-creating-a-gru-model-using-trax",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "Lab: Creating a GRU model using Trax",
    "text": "Lab: Creating a GRU model using Trax\nCreating a GRU model using Trax",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w2/index.html#deep-and-bi-directional-rnns",
    "href": "notes/c3w2/index.html#deep-and-bi-directional-rnns",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "Deep and Bi-directional RNNs",
    "text": "Deep and Bi-directional RNNs\nBi-directional RNNs are important, because knowing what is next in the sentence could give we more context about the sentence itself.\n\nSo we can see, in order to make a prediction \\hat{y}, we will use the hidden states from both directions and combine them to make one hidden state, we can then proceed as we would with a simple vanilla RNN. When implementing Deep RNNs, we would compute the following.\n\nNote that at layer l, we are using the input from the bottom a^{[;-1]} and the hidden state h^{&lt;l&gt;} That allows we to get your new h, and then to get your new a, we will train another weight matrix W_{a}, which we will multiply by the corresponding h add the bias and then run it through an activation layer.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w2/index.html#resources",
    "href": "notes/c3w2/index.html#resources",
    "title": "Recurrent Neural Networks for Language Modeling",
    "section": "Resources:",
    "text": "Resources:\n\n(Chadha 2020) Aman Chadha’s Notes\nIbrahim Jelliti’s Notes",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w2/lab01.html",
    "href": "notes/c3w2/lab01.html",
    "title": "Hidden State Activation : Ungraded Lecture Notebook",
    "section": "",
    "text": "course banner\nIn this notebook you’ll take another look at the hidden state activation function. It can be written in two different ways.\nI’ll show you, step by step, how to implement each of them and then how to verify whether the results produced by each of them are same or not.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "L1 - Hidden State Activation"
    ]
  },
  {
    "objectID": "notes/c3w2/lab01.html#background",
    "href": "notes/c3w2/lab01.html#background",
    "title": "Hidden State Activation : Ungraded Lecture Notebook",
    "section": "Background",
    "text": "Background\n\n\n\nvanilla rnn\n\n\nThis is the hidden state activation function for a vanilla RNN.\nh^{&lt;t&gt;}=g(W_{h}[h^{&lt;t-1&gt;},x^{&lt;t&gt;}] + b_h)\nWhich is another way of writing this:\nh^{&lt;t&gt;}=g(W_{hh}h^{&lt;t-1&gt;} \\oplus W_{hx}x^{&lt;t&gt;} + b_h)\nWhere\n\nW_{h} in the first formula is denotes the horizontal concatenation of W_{hh} and W_{hx} from the second formula.\nW_{h} in the first formula is then multiplied by [h^{&lt;t-1&gt;},x^{&lt;t&gt;}], another concatenation of parameters from the second formula but this time in a different direction, i.e vertical!\n\nLet us see what this means computationally.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "L1 - Hidden State Activation"
    ]
  },
  {
    "objectID": "notes/c3w2/lab01.html#imports",
    "href": "notes/c3w2/lab01.html#imports",
    "title": "Hidden State Activation : Ungraded Lecture Notebook",
    "section": "Imports",
    "text": "Imports\n\nimport numpy as np",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "L1 - Hidden State Activation"
    ]
  },
  {
    "objectID": "notes/c3w2/lab01.html#joining-concatenation",
    "href": "notes/c3w2/lab01.html#joining-concatenation",
    "title": "Hidden State Activation : Ungraded Lecture Notebook",
    "section": "Joining (Concatenation)",
    "text": "Joining (Concatenation)\n\nWeights\nA join along the vertical boundary is called a horizontal concatenation or horizontal stack.\nVisually, it looks like this:- W_h = \\left [ W_{hh} \\ | \\ W_{hx} \\right ]\nI’ll show you two different ways to achieve this using numpy.\nNote: The values used to populate the arrays, below, have been chosen to aid in visual illustration only. They are NOT what you’d expect to use building a model, which would typically be random variables instead.\n\nTry using random initializations for the weight arrays.\n\n\n# Create some dummy data\n\nw_hh = np.full((3, 2), 1)  # illustration purposes only, returns an array of size 3x2 filled with all 1s\nw_hx = np.full((3, 3), 9)  # illustration purposes only, returns an array of size 3x3 filled with all 9s\n\n\n### START CODE HERE ###\n# Try using some random initializations, though it will obfuscate the join. eg: uncomment these lines\n# w_hh = np.random.standard_normal((3,2))\n# w_hx = np.random.standard_normal((3,3))\n### END CODE HERE ###\n\nprint(\"-- Data --\\n\")\nprint(\"w_hh :\")\nprint(w_hh)\nprint(\"w_hh shape :\", w_hh.shape, \"\\n\")\nprint(\"w_hx :\")\nprint(w_hx)\nprint(\"w_hx shape :\", w_hx.shape, \"\\n\")\n\n# Joining the arrays\nprint(\"-- Joining --\\n\")\n# Option 1: concatenate - horizontal\nw_h1 = np.concatenate((w_hh, w_hx), axis=1)\nprint(\"option 1 : concatenate\\n\")\nprint(\"w_h :\")\nprint(w_h1)\nprint(\"w_h shape :\", w_h1.shape, \"\\n\")\n\n# Option 2: hstack\nw_h2 = np.hstack((w_hh, w_hx))\nprint(\"option 2 : hstack\\n\")\nprint(\"w_h :\")\nprint(w_h2)\nprint(\"w_h shape :\", w_h2.shape)\n\n-- Data --\n\nw_hh :\n[[1 1]\n [1 1]\n [1 1]]\nw_hh shape : (3, 2) \n\nw_hx :\n[[9 9 9]\n [9 9 9]\n [9 9 9]]\nw_hx shape : (3, 3) \n\n-- Joining --\n\noption 1 : concatenate\n\nw_h :\n[[1 1 9 9 9]\n [1 1 9 9 9]\n [1 1 9 9 9]]\nw_h shape : (3, 5) \n\noption 2 : hstack\n\nw_h :\n[[1 1 9 9 9]\n [1 1 9 9 9]\n [1 1 9 9 9]]\nw_h shape : (3, 5)\n\n\n\n\nHidden State & Inputs\nJoining along a horizontal boundary is called a vertical concatenation or vertical stack. Visually it looks like this:\n[h^{&lt;t-1&gt;},x^{&lt;t&gt;}] = \\left[ \\frac{h^{&lt;t-1&gt;}}{x^{&lt;t&gt;}} \\right]\nI’ll show you two different ways to achieve this using numpy.\nTry using random initializations for the hiddent state and input matrices.\n\n# Create some more dummy data\nh_t_prev = np.full((2, 1), 1)  # illustration purposes only, returns an array of size 2x1 filled with all 1s\nx_t = np.full((3, 1), 9)       # illustration purposes only, returns an array of size 3x1 filled with all 9s\n\n# Try using some random initializations, though it will obfuscate the join. eg: uncomment these lines\n\n### START CODE HERE ###\n# h_t_prev = np.random.standard_normal((2,1))\n# x_t = np.random.standard_normal((3,1))\n### END CODE HERE ###\n\nprint(\"-- Data --\\n\")\nprint(\"h_t_prev :\")\nprint(h_t_prev)\nprint(\"h_t_prev shape :\", h_t_prev.shape, \"\\n\")\nprint(\"x_t :\")\nprint(x_t)\nprint(\"x_t shape :\", x_t.shape, \"\\n\")\n\n# Joining the arrays\nprint(\"-- Joining --\\n\")\n\n# Option 1: concatenate - vertical\nax_1 = np.concatenate(\n    (h_t_prev, x_t), axis=0\n)  # note the difference in axis parameter vs earlier\nprint(\"option 1 : concatenate\\n\")\nprint(\"ax_1 :\")\nprint(ax_1)\nprint(\"ax_1 shape :\", ax_1.shape, \"\\n\")\n\n# Option 2: vstack\nax_2 = np.vstack((h_t_prev, x_t))\nprint(\"option 2 : vstack\\n\")\nprint(\"ax_2 :\")\nprint(ax_2)\nprint(\"ax_2 shape :\", ax_2.shape)\n\n-- Data --\n\nh_t_prev :\n[[1]\n [1]]\nh_t_prev shape : (2, 1) \n\nx_t :\n[[9]\n [9]\n [9]]\nx_t shape : (3, 1) \n\n-- Joining --\n\noption 1 : concatenate\n\nax_1 :\n[[1]\n [1]\n [9]\n [9]\n [9]]\nax_1 shape : (5, 1) \n\noption 2 : vstack\n\nax_2 :\n[[1]\n [1]\n [9]\n [9]\n [9]]\nax_2 shape : (5, 1)",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "L1 - Hidden State Activation"
    ]
  },
  {
    "objectID": "notes/c3w2/lab01.html#verify-formulas",
    "href": "notes/c3w2/lab01.html#verify-formulas",
    "title": "Hidden State Activation : Ungraded Lecture Notebook",
    "section": "Verify Formulas",
    "text": "Verify Formulas\nNow you know how to do the concatenations, horizontal and vertical, lets verify if the two formulas produce the same result.\nFormula 1: h^{&lt;t&gt;}=g(W_{h}[h^{&lt;t-1&gt;},x^{&lt;t&gt;}] + b_h)\nFormula 2: h^{&lt;t&gt;}=g(W_{hh}h^{&lt;t-1&gt;} \\oplus W_{hx}x^{&lt;t&gt;} + b_h)\nTo prove:- Formula 1 \\Leftrightarrow Formula 2\nWe will ignore the bias term b_h and the activation function g(\\ ) because the transformation will be identical for each formula. So what we really want to compare is the result of the following parameters inside each formula:\n$W_{h}[h{},x{}] W_{hh}h^{} W_{hx}x^{} $\nWe’ll see how to do this using matrix multiplication combined with the data and techniques (stacking/concatenating) from above.\n\nTry adding a sigmoid activation function and bias term to the checks for completeness.\n\n\n# Data\n\nw_hh = np.full((3, 2), 1)  # returns an array of size 3x2 filled with all 1s\nw_hx = np.full((3, 3), 9)  # returns an array of size 3x3 filled with all 9s\nh_t_prev = np.full((2, 1), 1)  # returns an array of size 2x1 filled with all 1s\nx_t = np.full((3, 1), 9)       # returns an array of size 3x1 filled with all 9s\n\n\n# If you want to randomize the values, uncomment the next 4 lines\n\n# w_hh = np.random.standard_normal((3,2))\n# w_hx = np.random.standard_normal((3,3))\n# h_t_prev = np.random.standard_normal((2,1))\n# x_t = np.random.standard_normal((3,1))\n\n# Results\nprint(\"-- Results --\")\n# Formula 1\nstack_1 = np.hstack((w_hh, w_hx))\nstack_2 = np.vstack((h_t_prev, x_t))\n\nprint(\"\\nFormula 1\")\nprint(\"Term1:\\n\",stack_1)\nprint(\"Term2:\\n\",stack_2)\nformula_1 = np.matmul(np.hstack((w_hh, w_hx)), np.vstack((h_t_prev, x_t)))\nprint(\"Output:\")\nprint(formula_1)\n\n# Formula 2\nmul_1 = np.matmul(w_hh, h_t_prev)\nmul_2 = np.matmul(w_hx, x_t)\nprint(\"\\nFormula 2\")\nprint(\"Term1:\\n\",mul_1)\nprint(\"Term2:\\n\",mul_2)\n\nformula_2 = np.matmul(w_hh, h_t_prev) + np.matmul(w_hx, x_t)\nprint(\"\\nOutput:\")\nprint(formula_2, \"\\n\")\n\n# Verification \n# np.allclose - to check if two arrays are elementwise equal upto certain tolerance, here  \n# https://numpy.org/doc/stable/reference/generated/numpy.allclose.html\n\nprint(\"-- Verify --\")\nprint(\"Results are the same :\", np.allclose(formula_1, formula_2))\n\n### START CODE HERE ###\n# # Try adding a sigmoid activation function and bias term as a final check\n# # Activation\n# def sigmoid(x):\n#     return 1 / (1 + np.exp(-x))\n\n# # Bias and check\n# b = np.random.standard_normal((formula_1.shape[0],1))\n# print(\"Formula 1 Output:\\n\",sigmoid(formula_1+b))\n# print(\"Formula 2 Output:\\n\",sigmoid(formula_2+b))\n\n# all_close = np.allclose(sigmoid(formula_1+b), sigmoid(formula_2+b))\n# print(\"Results after activation are the same :\",all_close)\n### END CODE HERE ###\n\n-- Results --\n\nFormula 1\nTerm1:\n [[1 1 9 9 9]\n [1 1 9 9 9]\n [1 1 9 9 9]]\nTerm2:\n [[1]\n [1]\n [9]\n [9]\n [9]]\nOutput:\n[[245]\n [245]\n [245]]\n\nFormula 2\nTerm1:\n [[2]\n [2]\n [2]]\nTerm2:\n [[243]\n [243]\n [243]]\n\nOutput:\n[[245]\n [245]\n [245]] \n\n-- Verify --\nResults are the same : True",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "L1 - Hidden State Activation"
    ]
  },
  {
    "objectID": "notes/c3w2/lab01.html#summary",
    "href": "notes/c3w2/lab01.html#summary",
    "title": "Hidden State Activation : Ungraded Lecture Notebook",
    "section": "Summary",
    "text": "Summary\nThat’s it! We’ve verified that the two formulas produce the same results, and seen how to combine matrices vertically and horizontally to make that happen. We now have all the intuition needed to understand the math notation of RNNs.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "L1 - Hidden State Activation"
    ]
  },
  {
    "objectID": "notes/c3w2/lab04.html",
    "href": "notes/c3w2/lab04.html",
    "title": "Creating a GRU model using Trax: Ungraded Lecture Notebook",
    "section": "",
    "text": "course banner\nFor this lecture notebook you will be using Trax’s layers. These are the building blocks for creating neural networks with Trax.\nimport trax\nfrom trax import layers as tl\n\n2025-02-10 16:57:42.066058: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1739199462.079538  125276 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1739199462.084360  125276 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nTrax allows to define neural network architectures by stacking layers (similarly to other libraries such as Keras). For this the Serial() is often used as it is a combinator that allows to stack layers serially using function composition.\nNext you can see a simple vanilla NN architecture containing 1 hidden(dense) layer with 128 cells and output (dense) layer with 10 cells on which we apply the final layer of logsoftmax.\nmlp = tl.Serial(\n  tl.Dense(128),\n  tl.Relu(),\n  tl.Dense(10),\n  tl.LogSoftmax()\n)\nEach of the layers within the Serial combinator layer is considered a sublayer. Notice that unlike similar libraries, in Trax the activation functions are considered layers. To know more about the Serial layer check the docs here.\nYou can try printing this object:\nprint(mlp)\n\nSerial[\n  Dense_128\n  Serial[\n    Relu\n  ]\n  Dense_10\n  LogSoftmax\n]\nPrinting the model gives you the exact same information as the model’s definition itself.\nBy just looking at the definition you can clearly see what is going on inside the neural network. Trax is very straightforward in the way a network is defined, that is one of the things that makes it awesome!",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "L4 - Creating a GRU model using Trax"
    ]
  },
  {
    "objectID": "notes/c3w2/lab04.html#gru-model",
    "href": "notes/c3w2/lab04.html#gru-model",
    "title": "Creating a GRU model using Trax: Ungraded Lecture Notebook",
    "section": "GRU MODEL",
    "text": "GRU MODEL\nTo create a GRU model you will need to be familiar with the following layers (Documentation link attached with each layer name): - ShiftRight Shifts the tensor to the right by padding on axis 1. The mode should be specified and it refers to the context in which the model is being used. Possible values are: ‘train’, ‘eval’ or ‘predict’, predict mode is for fast inference. Defaults to “train”.\n\nEmbedding Maps discrete tokens to vectors. It will have shape (vocabulary length X dimension of output vectors). The dimension of output vectors (also called d_feature) is the number of elements in the word embedding.\nGRU The GRU layer. It leverages another Trax layer called GRUCell. The number of GRU units should be specified and should match the number of elements in the word embedding. If you want to stack two consecutive GRU layers, it can be done by using python’s list comprehension.\nDense Vanilla Dense layer.\nLogSoftMax Log Softmax function.\n\nPutting everything together the GRU model will look like this:\n\nmode = 'train'\nvocab_size = 256\nmodel_dimension = 512\nn_layers = 2\n\nGRU = tl.Serial(\n      tl.ShiftRight(mode=mode), # Do remember to pass the mode parameter if you are using it for interence/test as default is train \n      tl.Embedding(vocab_size=vocab_size, d_feature=model_dimension),\n      [tl.GRU(n_units=model_dimension) for _ in range(n_layers)], # You can play around n_layers if you want to stack more GRU layers together\n      tl.Dense(n_units=vocab_size),\n      tl.LogSoftmax()\n    )\n\nNext is a helper function that prints information for every layer (sublayer within Serial):\nTry changing the parameters defined before the GRU model and see how it changes!\n\ndef show_layers(model, layer_prefix=\"Serial.sublayers\"):\n    print(f\"Total layers: {len(model.sublayers)}\\n\")\n    for i in range(len(model.sublayers)):\n        print('========')\n        print(f'{layer_prefix}_{i}: {model.sublayers[i]}\\n')\n        \nshow_layers(GRU)\n\nTotal layers: 6\n\n========\nSerial.sublayers_0: Serial[\n  ShiftRight(1)\n]\n\n========\nSerial.sublayers_1: Embedding_256_512\n\n========\nSerial.sublayers_2: GRU_512\n\n========\nSerial.sublayers_3: GRU_512\n\n========\nSerial.sublayers_4: Dense_256\n\n========\nSerial.sublayers_5: LogSoftmax\n\n\n\nHope you are now more familiarized with creating GRU models using Trax.\nYou will train this model in this week’s assignment and see it in action.\nGRU and the trax minions will return, in this week’s endgame.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "L4 - Creating a GRU model using Trax"
    ]
  },
  {
    "objectID": "notes/c3w4/index.html",
    "href": "notes/c3w4/index.html",
    "title": "Siamese Networks",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 1\nMy notes for Week 4 of the Natural Language Processing with Sequence Models Course in the Natural Language Processing Specialization Offered by DeepLearning.AI on Coursera",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w4/index.html#sec-siamese-network",
    "href": "notes/c3w4/index.html#sec-siamese-network",
    "title": "Siamese Networks",
    "section": "Siamese Network",
    "text": "Siamese Network\nIt is best to describe what a Siamese network is through an example.\n\n\n\n\n\n\n\nFigure 1: Comparisons questions pairs\n\n\nNote that in the first example above, the two sentences mean the same thing but have completely different words. While in the second case, the two sentences mean completely different things but they have very similar words.\n\nClassification: learns what makes an input what it is.\nSiamese Networks: learns what makes two inputs the same\n\nHere are a few applications of siamese networks:\n\n\n\n\n\n\n\nFigure 2: NLP applications of Siamese Networks include, comparing two signatures, comparing questions or search engine queries,\n\n\n\n\n\n\n\n\nVideo Transcript\n\n\n\n\n\n\nIn this video you’re going to learn about a special type of neural network known as the Siamese network. It is a neural network made up of two identical neural networks which are merged at the end. This type of architecture has many applications and NLP. And in this video, you’ll see the different examples where you can use it.\n\n\nConsider the following question. How old are you? And what is your age? You can see that these questions don’t have any words in common. However, they both mean the same thing. On the other hand, if you were to look at the following questions, where are you from? And where are you going? You can see that the first three words are the same. However, the last word completely changes the meaning of each question. This example shows that comparing meaning is not as simple as just comparing words. Coming up, you’re going to see how you can use Siamese networks to compare the meaning of word sequences, and identify question duplicates, which is a very important NLP application at the core of platforms like Stack Overflow or Quora.\n\n\nBefore these platforms allow you to post a new question, they want to be sure that your question hasn’t already been posted by somebody else. Now take this sentence, I’m happy because I’m learning, and consider it in the context of sentiment analysis and binary classification. Now in training a classification algorithm, you discover what features give the statement a positive or negative sentiment.\n\n\nWith Siamese networks you’ll be aiming to identify what’s makes two input similar, and what makes them different. Take a look at these two questions. What is your age? And how old are you? When you build a Siamese model, you’re trying to identify the difference or the similarity between these two questions. You do this by computing a single similarity score, representing the relationship between the two questions. And based on that score when compared against a threshold, you can predict whether these two are the same or different.\n\n\nSiamese networks have many applications in NLP, you can use them to authenticate handwritten checks by determining whether two signatures are the same or not. You can use them to identify question duplicates on platforms like Quora or Stack Overflow.\n\n\nAnd you can use them in search engine queries to predict whether a new query is similar to the one that was already executed. These are just a few examples, but there are many more applications of Siamese networks in NLP.\n\nYou can use Siamese networks in many types of NLP applications. In the next video, I’ll walk you through the architecture that is used in this type of model. And I’ll show you how you can use it in a text. I’ll see you in the next video.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w4/index.html#sec-architecture",
    "href": "notes/c3w4/index.html#sec-architecture",
    "title": "Siamese Networks",
    "section": "Architecture",
    "text": "Architecture\nThe model architecture of a typical siamese network could look as follows:\n\n\n\n\n\n\n\nFigure 3: The architecture of a typical siamese network has two sub-networks consisting of embedding LSTMs and a cosine similarity function that evaluates their outputs.\n\n\nThese two sub-networks are sister-networks which come together to produce a similarity score. Not all Siamese networks will be designed to contain LSTMs. One thing to remember is that sub-networks share identical parameters. This means that we only need to train one set of weights and not two.\nThe output of each sub-network is a vector. We can then run the output through a cosine similarity function to get the similarity score. In the next video, we will talk about the cost function for such a network.\n\n\n\n\n\n\nVideo Transcript\n\n\n\n\n\nSiamese networks have a special type of architecture. They have two identical sub-networks which are merged together through a dense layer to produce a final output or its similarity score. I like to think of these two sub-networks as sister-networks which come together to produce a similarity score.\nIn Figure 3 we can see the model architecture for a Siamese network. Note that the architecture presented here is just an example. Not all Siamese networks will be designed to contain LSTMs.\nOn the left, you have two inputs which represents Question 1 and Question 2. You will take each question, transform it into an embedding and then you’ll run the embedding through an LSTM layer to model the questions meaning. Each LSTM outputs a vector.\nIn this architecture, you have two identical sub-networks.\nOne for Question 1 and the second for Question 2. An important note here is that the sub-networks share identical parameters. That is the learned parameters of each sub-network are exactly the same. So you actually only need to train one sets of weights, not two.\nThen given the two outputs vectors, one corresponding to each question, find their cosine similarity.\n\nWhat is cosine similarity?\nRecall that the cosine similarity is a measure of similarity between two vectors. When two vectors point generally in the same direction, the cosine of the angle between them is near one. For vectors that point in opposite directions, the cosine of the angle between them is minus one. If that sounds unfamiliar don’t worry.\nRight now you just need to know that the cosine similarity tells you how similar two vectors are. In this case, it tells you how similar the two questions are.\nThe cosine similarity gives the Siamese networks prediction, denoted here by the variable y-hat, which will be a value between minus one and positive one.\n\n\nHow do we interpret y-hat and tau?\nIf y-hat is less than or equal to some threshold, tau, then you will say that the input questions are different. If y-hat is greater than tau, then you will say that they are the same.\nThe threshold tau is a parameter that you will choose based on how often you want to interpret cosine similarity to indicate that two questions are similar or not. A higher threshold means that only very similar sentences will be considered similar.\n\n\nHow would we walking through the architecture\nIf you think of this process as a series of steps you take to get from your inputs to your outputs, it would go something like this; you start with a model architecture for a Siamese network made up of two identical sub-networks. In this case, your inputs are questions that you feed into each sub-network and each question gets transformed into an embedding and pass through an LSTM layer. Then you take the outputs of each of the sub-networks and compare them using cosine similarity to get your y-hat.\nAfter seeing the model architecture, I’ll start talking about different cost functions you can use for this type of architecture.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w4/index.html#sec-lab-siamese",
    "href": "notes/c3w4/index.html#sec-lab-siamese",
    "title": "Siamese Networks",
    "section": "Lab: Creating a Siamese Model using Trax",
    "text": "Lab: Creating a Siamese Model using Trax\nCreating a Siamese Model using Trax",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w4/index.html#sec-cost-function",
    "href": "notes/c3w4/index.html#sec-cost-function",
    "title": "Siamese Networks",
    "section": "Cost Function",
    "text": "Cost Function\nLet us take a close look at the following slide:\n\n\n\n\n\n\n\nFigure 4: Understanding the triplet loss cost function\n\n\nNote that when trying to compute the cost for a siamese network we use the triplet loss. The triplet loss usually consists of an Anchor and a Positive example. Note that the anchor and the positive example have a cosine similarity score that is very close to one. On the other hand, the anchor and the negative example have a cosine similarity score close to -1. Now we are ideally trying to optimize the following equation: −cos(A,P)+cos(A,N)≤0\nNote that if cos(A,P)=1 is 1 and cos(A,N)=−1, then the equation is definitely less than 0. However, as cos(A,P) deviates from 1 and cos(A,N) deviates from -1, then we can end up getting a cost that is &gt; 0. Here is a visualization that would help we understand what is going on. Feel free to play with different numbers.\n\n\n\n\n\n\n\nFigure 5: A worked example of triplet loss\n\n\n\n\n\n\n\n\nFigure 6: Chart for the loss function\n\n\n\n\n\n\n\n\n\nVideo Transcript\n\n\n\n\n\n\nI’ll now show you a simple loss function you can use in your Siamese network.\nJust as a recap, this is the overall structure of the Siamese network, which enables you to predict whether two questions are similar or different, or the outputs of the network, you are able to calculate y-hat, which is the similarity between the two questions.\nNow, I’ll show you a loss function for a Siamese network.\n\n\nWhat are positive and negative questions?\n\nI’ll starts by looking at this first question, which is, “How old are you?” I’ll call this first question the anchor, which I’m going to use to compare against two other questions relative to the anchor.\n\n\nOther questions that have the same meaning as the anchor are called positive questions. Whereas questions that do not have the same meaning as the anchor are called negative questions.\n\n\nNote that the meaning of positive and negative in the context of finding question duplicates is referring to whether a question is similar to the anchor or not, and not whether it has a positive or negative sentiment.\n\n\n\nWhat is a positive question?\n\nThe question, “What is your age?” is considered a positive question relative to the anchor, because “How old are you?” and “What is your age?” mean the same thing.\n\n\n\nWhat is a negative question?\n\nThis other question, “Where are you from?” is considered a negative question because it does not have the same meaning as the anchor question.\n\n\n\nWhat is cosine similarity?\n\nHere’s a definition of cosine similarity between two vectors. Figure 4 That will be the similarity of function s. To train your model, you’ll be comparing the vectors that are outputs by each sub-network using similarity.\nSo for this example, you’re going to take the similarity between A and P, where A refers to the anchor question, and P refers to the positive question.\nSimilarity is bounded between negative one and one. So for vectors that are completely different, the similarity is near negative one, and for vectors that are nearly identical, there similarity is close to positive one.\nFor a well-trained model, you would like to see a similarity close to one when comparing the anchor and the positive example. Similarly, when comparing the anchor to the negative example, a successful model should yield a similarity close to negative one.\n\n\n\nHow do you compute the loss?\n\nTo begin building a loss function, you start with the similarity of A and N and subtract the similarity of A and P to calculate the difference.\nWhat you have here Figure 6 is a loss function that allows you to determine whether your model is roughly doing what you hope it will do.\nNamely, finding that the anchor and the positive example are similar, and that the anchor and the negative example are different.\nAs the difference gets bigger or smaller along the x-axis, the loss gets bigger or smaller along the y-axis.\nWhen minimizing the loss in training, you are in effect minimizing this difference. You’ve started seeing a difference approach which will allow you to build a different cost function.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w4/index.html#sec-triplets",
    "href": "notes/c3w4/index.html#sec-triplets",
    "title": "Siamese Networks",
    "section": "Triplets",
    "text": "Triplets\nWe will now build on top of our previous cost function. To get the full cost function we will add a margin.\n\n\n\n\n\n\n\nFigure 7: Adding a margin to the triplet loss\n\n\nNote that we added an α in the equation above. This allows we to have a margin of “safety”.\nWhen computing the full cost, we take the max of that the outcome of −cos(A,P)+cos(A,N)+α and 0. Note, we do not want to take a negative number as a cost.\nHere is a quick summary:\n\n𝜶: controls how far cos(A,P) is from cos(A,N)\nEasy negative triplet: cos(A,N) &lt; cos(A,P)\nSemi-hard negative triplet: $cos(A,N) &lt; cos(A,P) &lt; cos(A,N) + 𝜶 $\nHard negative triplet: cos(A,P) &lt; cos(A,N)\n\n\n\n\n\n\n\nVideo Transcript\n\n\n\n\n\n\nYou will now explore triplets. You’ll see how you can build pairs of inputs. Rather than just classifying what’s an input is, you’re going to build something that will allow you to identify the difference between two inputs. Let’s see how this works.\nHere are three questions where the first one, how old are you, is the anchor. The second one is a positive example, what is your age? The third one, where are you from? is a negative example.\nHaving the three components here is what gives rise to the name triplets, which is to say, an anchor being used in conjunction with a positive and negative pairing. Accordingly, triplet loss is the name for a loss function that uses three components. The intuition behind this simple function is to minimize the difference between the similarity of A and N, and the similarity of A and P. You already know that, as the difference gets bigger or smaller along the x axis, the loss gets bigger or smaller along the y axis.\n\n\nDo we want the loss to be less than zero?\n\nBut notice, when the difference is less than zero, do you also want the loss to be less than zero? Let’s think about this for a moment.\nIf you gave the model a positive loss value, the model uses this to update its weight to improve.\nIf you gave the model a negative loss value, this is like telling the model, “Good job. Please update your weight to the worst next time.”\nSo you don’t actually want to give the model a loss value that’s less than zero. In other words, when the model is doing a good job, you don’t want it to undo a its update. To make sure that the model doesn’t update itself to do worse, you can modify the loss so that whenever the diff is less than zero, the loss should just be zero. When the loss is zero, we’re effectively not asking the model to update it’s weights, because it is performing as expected for that training example. The loss-function now cannot take on negative values. If the difference is less than or equal to 0, the loss is 0. If the difference is greater than 0, then the loss is equal to the difference.\nNotice the non-linearity happens at the origin of this line chart.\nBut you might also wonder what’s happens when the model is correct but only by a tiny bits? The model is still correct if the difference is a tiny number, that is less than zero.\nWhat if you want the model to still learn from this example, and ask it to predict a wider difference for this training example?\nYou can think of shifting this loss function a little to the left, by a margin that we’ll refer to as Alpha. Let’s say we chose Alpha to be 0.2, if the difference between similarities is very small, like negative 0.1, then if you add it to the Alpha of 0.2, the result is still greater than 0. The sum of the diff plus Alpha can be considered a positive loss that tells the model to learn from this example.\nYou can see this visually in the line chart.  The loss function is shifted to the left by the amount Alpha. The diff is along the horizontal axis. When the difference is less than zero but small in magnitude, the loss is greater than zero. So if the difference is smaller in magnitude than Alpha, then there is still a loss. This loss tells the model that it can still improve and learn from this training example. Triplet loss, as the difference with a margin Alpha, is what you will implement in the assignments which you will code like this, which is the triplet loss function for A, P and N. A small detail worth noting.\nIn these explanations, I’ve been using similarity because that’s what will be used in the programming assignments, so similarity of v_1, v_2.\nBut if you were to read the literature, you might find d of v_1, v_2 used also, where this d could be any function that calculates the distance between two vectors. A distance metric is the mirror image of a similarity metric, and a similarity metric can be derived from a distance metric.\nOne example of a distance metric is Euclidean distance.\n\n\n\nHow do we pick good triplets?\n\nSelecting triplets A, P, and N for training involves two steps; first, select a pair of questions that are known to be duplicates to serve as the anchor and positive, and you’ll do this from the training set; second, select a question that is known to be difference in meaning from the anchor, to form the anchor and the negative pair.\n\n\n\nWhy not use random triplets?\n\nIf you were to select triplets at random, you’d be likely to select non-duplicative pairs A and N, where the loss is 0.\nThe loss is zero whenever the model correctly predicts that A and P are more similar relative to A and N.\nWhen the loss is 0, the network has nothing more to learn from the triplets example. So we can train more efficiently if we choose triplets that show the model when it’s incorrect, so that’s just going to adjust it’s weight and improve.\n\n\n\nWhat are hard triplets?\n\nInstead of selecting random triplets, you’ll specifically select so-called hard triplets. That is, triplets that are more difficult to train on. Hard triplets are those where the similarity between anchor and negative is very close to, but still smaller than the similarity between anchor and positive. When the model encounters a hard triplet, the learning algorithm needs to adjust its weight, so that’s it’s going to yield similarities that line up with the real-world labels. So by selecting hard triplets, focusing the training on doing better, on the difficult cases, that it’s predicting incorrectly.\n\n\nI spoke about easy and hard triplets. I also spoke about a margin. In the next video, you’ll see how all these concepts come together to help us create a cost function.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w4/index.html#sec-computing-cost-1",
    "href": "notes/c3w4/index.html#sec-computing-cost-1",
    "title": "Siamese Networks",
    "section": "Computing the Cost I",
    "text": "Computing the Cost I\nTo compute the cost, we will prepare the batches as follows:\n\n\n\n\n\n\n\nFigure 8: An example batch of question pairs\n\n\nNote that each example, has a similar example to its right, but no other example means the same thing. We will now introduce hard negative mining.\n\n\n\n\n\n\n\nFigure 9: Hard negative mining\n\n\nEach horizontal vector corresponds to the encoding of the corresponding question. Now when we multiply the two matrices and compute the cosine, we get the following:\n\n\n\n\n\n\n\nFigure 10: Understanding Cost matrix for a batch of question pairs\n\n\nThe diagonal line corresponds to scores of similar sentences, (normally they should be positive). The off-diagonals correspond to cosine scores between the anchor and the negative examples.\n\n\n\n\n\n\nVideo Transcript\n\n\n\n\n\nWelcome back. As promised, you’ll see how everything fits together now. You will start by building a cost function, and then you will use gradient descent to optimize this cost function. Let’s take a look at how this works. To compute the cost, begin by preparing the data in batches. Here you have the questions, what is your age and how old are you? You can see these are duplicates, because they mean the same thing. Can you see me and are you seeing me? Are also duplicates. Where are thou and where are you? Are duplicates too. As are, when is the game and what time is the game? So with four pairs, you have batch size of four. Here we will use the letter b to stand for batch size. Something that’s very important to note is that each question has its corresponding duplicate to the left or right of it. That is, in each row all of the sentences in the columns are duplicates. But you will notice that each question has no duplicates above or below it. That is, for any column, none of the rows in those column contain a sentence that is a duplicate of another sentence in those column. So this is how you prepare the batches. Now, let me show you how you will want to organize the data in this way. Given the first batch, you’re going to run it through this model to get a vector v_1 with dimensions one row by five columns. The number of columns shown in this matrix is equal to the dimension of your embedding layer, which in this case is five. I’ll refer to this dimension of the embedding layer as d model for each question in the batch. I haven’t talked about the dimension of the embedding layer yet, but don’t worry, it will become more clear once you’re working with the code. The important takeaway is that the dimension of the embedding, the model, is a parameter that determines the dimensions of the weights through each layer in the model, and thus determines the size of the outputs vector. The model is running a batch size that is greater than one. So the v_1 outputs is actually a matrix of stacked vectors like this. In this visual example, there are four rows in this matrix to indicate that there are four observations in this batch. The batch size is four. Our subscript to observations in the batch as v_1_1, v_1_2, and so on corresponding to the vector outputs for each question in the batch. You’ll do the same thing for the batch of v_2 vectors. Each question in the batch 1 is a duplicate of its corresponding question in batch 2. But none of the questions in batch 1 are duplicates of each other. The same applies to batch 2. Here, for example, v_1_1 is a duplicate of v_2_1, as are the rest of the respective row pairs. But v_1_1 is not a duplicates of any other rows in v_1. The last step is to combine the two branches of the Siamese network by calculating the similarity between all vector pair combinations of v_1 with v_2. For this example with a batch size of four, you might get a matrix of similarities that looks like this. The diagonal is a key feature here. These values are the similarities for all your positive examples, the question duplicates. Notice that all the values are generally greater than the numbers in the off diagonals. So the model is performing as you would expect for duplicates questions, because you would expect that the question duplicates to have higher similarity compared to the non-duplicates. In the upper right and lower left, you have the similarities for all the negative examples. These are the results for the non-duplicates pairs. Notice that most of these numbers are lower than the similarities that’s are along the diagonal. Also notice that you can have negative example question pairs that still have a similarity greater than zero. The range of similarity ranges from negative 1 to positive 1, but there isn’t any special requirements that a similarity greater than zero indicates duplicates or that’s a similarity less than zero indicates non-duplicates. What’s matters for a properly functioning model is that it generally finds that duplicates have a higher similarity relative to non-duplicates. Creating non-duplicates pairs like this removes the need for additional non-duplicate examples and the input data, which turns out to be a big deal. Instead of needing to sets up specific batches with negative examples, your model can learn from them in the existing question duplicates batches. Now, you can just stop here and use these similarities with the triplet loss function you already know shown here. Then the overall costs for your Siamese network will be the sum of these individual losses over the training sets. Here you can see that superscripts i refers to a specific training example and there are m observations, but there are more techniques available that’s can vastly improve upon model performance. I’ll show you those next.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w4/index.html#sec-computing-cost-2",
    "href": "notes/c3w4/index.html#sec-computing-cost-2",
    "title": "Siamese Networks",
    "section": "Computing the Cost II",
    "text": "Computing the Cost II\nNow that we have the matrix with cosine similarity scores, which is the product of two matrices, we go ahead and compute the cost.\n\n\n\n\n\n\n\nFigure 11\n\n\nWe now introduce two concepts, the mean_neg, which is the mean negative of all the other off diagonals in the row, and the closest_neg, which corresponds to the highest number in the off diagonals.\n\nCost = \\max(−\\cos(A,P)+\\cos(A,N)+α,0)\n\nSo we will have two costs now:\n\nCost_1 = \\max(−\\cos(A,P)+ mean_n eg + α,0)\n\n\nCost_2 = \\max(−\\cos(A,P)+ closest_n eg + α,0)\n ⁡\nThe full cost is defined as: Cost1 + Cost2.\n\n\n\n\n\n\nVideo Transcript\n\n\n\n\n\n\nPreviously, you set up the training data into two specific batches, each batch containing no duplicate questions within it. You ran those batches through one sub network each. And that’s produced a vector of supports per question. Which has dimension 1 by d_model, where d_model is the embedding dimension. And is equal to the number of columns in the matrix, which is five, at least in this example. The v_1 vectors for a single batch are stuck together. And in this case, the batch size is the number of rows shown in this matrix, which is four. You can see a similar batch of v_2 vectors as well. The last step was to combine the two branches of the Siamese network. By calculating the similarity between all vector pair combinations of the v_1 vectors and v_2 vectors. For this example with a batch size of four, that last step would produce a matrix of similarities that looks something like this.\n\n\nWhat are the attributes of this matrix?\n\nThis matrix has some important attributes. The similarities along the green diagonal contain similarities for the duplicate questions. For a well trained model, these values should be greater than similarities for the off-diagonals. Reflecting the fact that the network produces similar vector outputs for duplicate questions. The orange values in the upper right and lower left are similarities for the non duplicate questions.\nNow this is where things get really interesting. You can use this off diagonal information to make some modifications to the loss function and really improve your models performance. To do so, I’m going to make use of two concepts.\n\n\n\nWhat is the mean negative?\n\nThe first concept is the mean negative, which is just the mean or average of all the off-diagonal values in each row. Notice that off-diagonal elements can still be positive numbers. So when I say mean negative, I’m referring to the mean of the similarity for negative examples, not the mean of negative numbers in a row.\nFor example, the mean negative of the first row is just the mean of all the off-diagonal values in that row.\nIn this case, -0.8, 0.3 and -0.5, excluding the value 0.9, which is on the diagonal. You can use the mean negative to help speed up training by modifying the loss function, which I’ll show you soon.\n\n\n\nWhat is the closest negative?\n\nThe next concept is what’s called the closest negative. As mentioned earlier, because of the way you define the triplet loss function, you’ll need to choose so called hard triplets to train on. What this means is that for training, you want to choose triplets where the cosine similarity of the negative example is close to the similarity of the positive example.\nThis forces your model to learn what differentiates these examples and ultimately drive those similarity values further apart through training. To do this, you’ll search each row in your output matrix for the closest negative. Which is to say the off diagonal value which is closest to, but still less than the value on the on diagonal for that row. So in this first row, the value on the diagonal is 0.9. So the closest off-diagonal elements in this case is 0.3. What this means is that this negative example with a similarity of 0.3 has the most to offer your model in terms of learning opportunity.\n\n\n\nHow do we use these new concepts ?\n\nTo make use of these new concepts, recall that the triplet loss was defined as the max of the similarity of A and N minus the similarity of A and B plus the margin alpha and 0. Also recall that we refer to the difference between the two similarities with the variable named diff.\nHere, we’re just writing out the definition of diff. So in order to minimize the loss you want this diff plus the margin alpha to be less than or equal to 0. I’ll introduce loss 1 to be the max of the mean negative minus the similarity of A and P plus alpha and 0. The change between the formulas for triplet loss and loss 1 is the replacement of similarity of A and N. With the mean negative, this helps the model converge faster during training by reducing noise. It reduces noise by training on just the average of several observations, rather than training the model on each of these off-diagonal examples.\nSo why does taking the average of several observations usually reduce noise? Well, we define noise to be a small value that comes from a distribution that is centered around 0. So in other words, the average of several noise values is usually 0. So if we took the average of several examples, this has the effect of cancelling out the individual noise from those observations. Then loss 2 will be the max of the closest negative minus the similarity of A and B plus alpha and 0.\nThe difference between the formulas this time is the replacement of the cosine of A and N. With the closest negative, this helps create a slightly larger penalty by diminishing the effects of the otherwise more negative similarity of A and N that it replaces.\nYou can think of the closest negative as finding the negative example that results in the smallest difference between the two cosine similarities. If you had that small difference to alpha, then you’re able to generate the largest loss among all of the other examples in that row.\nBy focusing the training on the examples that produce higher loss values, you make the model update its weights more.\nTo learn from these more difficult examples, then you can define the full loss as loss 1 + loss 2. And you will use this new full loss as an improved triplet loss in the assignments. The overall costs for your Siamese network will be the sum of these individual losses over the training sets.\n\n\nIn the next video, you will use this cost function in one shot learning. One shot learning is a very effective technique that can save you a lot of time when comparing the authenticity of checks or of any other type of inputs",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w4/index.html#sec-lab-modified",
    "href": "notes/c3w4/index.html#sec-lab-modified",
    "title": "Siamese Networks",
    "section": "Lab: Lecture Notebook: Modified Triplet Loss",
    "text": "Lab: Lecture Notebook: Modified Triplet Loss\nModified Triplet Loss",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w4/index.html#sec-one-shot",
    "href": "notes/c3w4/index.html#sec-one-shot",
    "title": "Siamese Networks",
    "section": "One Shot Learning",
    "text": "One Shot Learning\nImagine we are working in a bank and we need to verify the signature of a check. We can either build a classifier with K possible signatures as an output or we can build a classifier that tells we whether two signatures are the same.\n\n\n\n\n\n\n\nFigure 12: Classification vs one shot learning\n\n\nHence, we resort to one shot learning. Instead of retraining your model for every signature, we can just learn a similarity score as follows:\n\n\n\n\n\n\n\nFigure 13\n\n\n\n\n\n\n\n\nVideo Transcript\n\n\n\n\n\n\nLet’s say that you’re trying to identify whether the author for a certain poem is Lucas or not. You can either take all of Lucas’ poems and put them into datasets, and instead of predicting K classes, you will now predict K plus 1 classes. All the previous poems of other authors plus Lucas’, so that’s why it’s k plus 1. Or you can compare one of Lucas’ poems to another poem, and that is where one-shot learning comes in. In this video, I’ll show you how you can do that. To understand the difference between classification and one-shot learning, first consider identifying or classifying signatures based on one through K possible classes. You might use some classification model trained on the K classes, probably with a softmax function at the end to find the maximum probability. Then at recognition time, classify the input signature to one of those corresponding classes. That’s great if you have a signature list that’s rarely changes. But what if you get a new signature to classify? It would be expensive to retrain the model every time this happens, and besides, unless you have a great many examples of that new signature, model training won’t work very well. In one-shot learning, you need to be able to recognize a signature repeatedly from just one example. You can do this with a learned similarity function. Then you can test a similarity score against some threshold to see if two signatures are the same. So the problem changes to determining which class to instead measuring similarity between two classes. This is very useful, especially in banks, for example. Every time there’s a new signature, you can’t retrain your entire system to classify the signatures into K possible outputs. So instead, you just learn a similarity function that can be used to calculate a similarity score. That can in turn be used to identify whether two signatures are the same. You already did this using cosine similarity as the similarity function. If the result was greater than some threshold Tau, you determine the inputs to be the same. In the case of comparing signatures, if the similarity is less than or equal to Tau, then the signatures are different. In this video, I spoke about one-shot learning and I told you why it is a very effective technique. One-shot learning makes use of Siamese networks. In the next video, I’ll show you how you can train and test your Siamese network.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w4/index.html#sec-training-testing",
    "href": "notes/c3w4/index.html#sec-training-testing",
    "title": "Siamese Networks",
    "section": "Training and Testing",
    "text": "Training and Testing\nAfter preparing the batches of vectors, we can proceed to multiplying the two matrices.\nHere is a quick recap of the first step:\n\n\n\n\n\n\n\nFigure 14: Preparing batches of questions\n\n\nThe next step is to implement the siamese model as follows:\n\n\n\n\n\n\n\nFigure 15: Reviewing the architecture of the siamese networks\n\n\nFinally when testing:\n\nConvert two inputs into an array of numbers\nFeed it into your model\nCompare 𝒗_1,𝒗_2 using cosine similarity\nTest against a threshold \\tau\n\n\n\n\n\n\n\nVideo Transcript\n\n\n\n\n\n\nIn this video, you’re goingto see what the dataset would look like for a Siamese network. I’ll show you how you can train your model and then you can use that model to test your Siamese network. Let’s take a look at how you can do this. You’ll be using the Quora question duplicates datasets for this week’s programming assignment. It looks like this. It consists of a collection of question pairs within its duplicates Boolean for each question. For example, for Question 1 and Question 2, “What is your age?” and “How old are you?” Its duplicate equals “true” because these two questions are duplicates. “Where are you from?” and “Where are you going?” are not duplicates, so it’s false and so on. This dataset gives your model plenty of examples to learn from. First, you will process the dataset so that it looks like this. You will pre-process the data into batches of size b. The corresponding questions from each batch are duplicates. For example, the first question in Batch 1, “What is your age?” is a duplicate of the first question in Batch 2, “How old are you?” The second question in Batch 1 is a duplicate of the second question in Batch 2 and so on.\nNote however, that there are no duplicates within an individual batch. If I call this q1_a, this q2_a, then q1_a and q2_a are duplicates. If this was q1_b and this was q2_b, then q1_b and q2_b are duplicates. However, q1_a and q1_b are not duplicates. Similarly, q2_a and q2_b are not duplicates. I’ll show you how to prepare the batches in such a way that no question within the same batch is duplicated. Finally, you’ll use these inputs to get outputs vectors for each batch. Then, you can calculate the cosine similarity between each pair of output vectors. This is the Siamese model that you’ll be implementing in the assignment. You’ll create a subnetwork, which is then duplicated and drawn in parallel. In each subnetwork, you got the embedding, run it through the LSTM, take your vector output, and then use them to find the cosine similarity.\n\n\nAn important note here, is that the learned parameters of the subnetworks are exactly the same between the two subnetworks. So you are actually only training one sets of weights, not two. When testing the model, you will perform one-shot learning. The goal is to find a similarity score between two inputs questions. First, convert each input into an array of numbers. Feed these into your model. Compare the subnetwork outputs v_1 and v_2 using cosine similarity for a similarity score. Then, test the score against some threshold Tau, and if the cosine similarity is greater than Tau, then the questions are classified as duplicates.\n\n\nNote that both $tau$ and the margin \\alpha from the last function are tunable hyperparameters.\n\n\nCongratulations, you now know how to train your Siamese network and you know how to test it. In this week’s programming exercise, you’ll be using a Siamese network to identify whether a question is a duplicate or not. Specifically, you’ll be using the Quora question duplicate data sets, and using that, you’ll be able to get a very good accuracy.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w4/index.html#sec-lab-evaluate",
    "href": "notes/c3w4/index.html#sec-lab-evaluate",
    "title": "Siamese Networks",
    "section": "Lab: Evaluate a Siamese Model",
    "text": "Lab: Evaluate a Siamese Model\nEvaluate a Siamese Model",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w4/index.html#sec-reflections",
    "href": "notes/c3w4/index.html#sec-reflections",
    "title": "Siamese Networks",
    "section": "Reflections",
    "text": "Reflections\n\nCould we improve the model by Can we give each LSTM it own loss function?\nWhat are some typical use cases for using Siamese networks?\n\nFace recognition. The faces are very different from each other but we don’t want to retrain the model for every new face - we typically want to check if a face is the same as one of the faces on file.\nSignature verification is a good example of a use case for Siamese networks. We want to check if a signatures we get is sufficiently similar to the few samples we have on file.\nOne shot learning is another area where Siamese networks could be useful.\n\nIn which NLP tasks are Siamese networks utilized?\n\nSearch engine queries are often challenging since they are very brief compared to the documents they are searching for, and so they tend to miss the best variation for some query. Also the distribution has many similar queries and a long tail of unique queries.\nQuestion-Answering sites like Stack overflow or Quora is another place where Siamese networks could be useful. The crowd sourcing works better if the questions are not repeated so that all the answers are in one place.\nParaphrase detection is another area where Siamese networks could be useful. The paraphrase could be a completely different sentence but the meaning remains the same.\nSpam detection is another area where Siamese networks could be useful. The spammer could change the words in the spam message but the meaning remains the same.\n\nHow can we improve the model by using a different similarity function?\nAre there any benefits to use Location sensitive hashing with the cosine similarity function ?",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w4/index.html#sec-acknowledgments",
    "href": "notes/c3w4/index.html#sec-acknowledgments",
    "title": "Siamese Networks",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\n(Chadha 2020) Aman Chadha’s Notes\nIbrahim Jelliti’s Notes",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w4/lab01.html",
    "href": "notes/c3w4/lab01.html",
    "title": "Creating a Siamese model using Trax: Ungraded Lecture Notebook",
    "section": "",
    "text": "course banner\nimport trax\nfrom trax import layers as tl\nimport trax.fastmath.numpy as np\nimport numpy\n\n# Setting random seeds\n# set random seeds to make this notebook easier to replicate\nfrom trax import fastmath\nseed=10\nrng = fastmath.random.get_prng(seed)\n#trax.supervised.trainer_lib.init_random_number_generators(10)\nnumpy.random.seed(seed)\n\n2025-02-10 16:56:47.210910: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1739199407.224698  124526 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1739199407.228897  124526 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "L1 - Creating a Siamese Model using Trax"
    ]
  },
  {
    "objectID": "notes/c3w4/lab01.html#l2-normalization",
    "href": "notes/c3w4/lab01.html#l2-normalization",
    "title": "Creating a Siamese model using Trax: Ungraded Lecture Notebook",
    "section": "L2 Normalization",
    "text": "L2 Normalization\nBefore building the model you will need to define a function that applies L2 normalization to a tensor. This is very important because in this week’s assignment you will create a custom loss function which expects the tensors it receives to be normalized. Luckily this is pretty straightforward:\n\ndef normalize(x):\n    return x / np.sqrt(np.sum(x * x, axis=-1, keepdims=True))\n\nNotice that the denominator can be replaced by np.linalg.norm(x, axis=-1, keepdims=True) to achieve the same results and that Trax’s numpy is being used within the function.\n\ntensor = numpy.random.random((2,5))\nprint(f'The tensor is of type: {type(tensor)}\\n\\nAnd looks like this:\\n\\n {tensor}')\n\nThe tensor is of type: &lt;class 'numpy.ndarray'&gt;\n\nAnd looks like this:\n\n [[0.77132064 0.02075195 0.63364823 0.74880388 0.49850701]\n [0.22479665 0.19806286 0.76053071 0.16911084 0.08833981]]\n\n\n\nnorm_tensor = normalize(tensor)\nprint(f'The normalized tensor is of type: {type(norm_tensor)}\\n\\nAnd looks like this:\\n\\n {norm_tensor}')\n\nThe normalized tensor is of type: &lt;class 'jaxlib.xla_extension.ArrayImpl'&gt;\n\nAnd looks like this:\n\n [[0.5739379  0.01544148 0.4714962  0.5571832  0.37093794]\n [0.26781026 0.23596111 0.9060541  0.20146926 0.10524315]]\n\n\nNotice that the initial tensor was converted from a numpy array to a jax array in the process.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "L1 - Creating a Siamese Model using Trax"
    ]
  },
  {
    "objectID": "notes/c3w4/lab01.html#siamese-model",
    "href": "notes/c3w4/lab01.html#siamese-model",
    "title": "Creating a Siamese model using Trax: Ungraded Lecture Notebook",
    "section": "Siamese Model",
    "text": "Siamese Model\nTo create a Siamese model you will first need to create a LSTM model using the Serial combinator layer and then use another combinator layer called Parallel to create the Siamese model. You should be familiar with the following layers (notice each layer can be clicked to go to the docs): - Serial A combinator layer that allows to stack layers serially using function composition. - Embedding Maps discrete tokens to vectors. It will have shape (vocabulary length X dimension of output vectors). The dimension of output vectors (also called d_feature) is the number of elements in the word embedding. - LSTM The LSTM layer. It leverages another Trax layer called LSTMCell. The number of units should be specified and should match the number of elements in the word embedding. - Mean Computes the mean across a desired axis. Mean uses one tensor axis to form groups of values and replaces each group with the mean value of that group. - Fn Layer with no weights that applies the function f, which should be specified using a lambda syntax. - Parallel It is a combinator layer (like Serial) that applies a list of layers in parallel to its inputs.\nPutting everything together the Siamese model will look like this:\n\nvocab_size = 500\nmodel_dimension = 128\n\n# Define the LSTM model\nLSTM = tl.Serial(\n        tl.Embedding(vocab_size=vocab_size, d_feature=model_dimension),\n        tl.LSTM(model_dimension),\n        tl.Mean(axis=1),\n        tl.Fn('Normalize', lambda x: normalize(x))\n    )\n\n# Use the Parallel combinator to create a Siamese model out of the LSTM \nSiamese = tl.Parallel(LSTM, LSTM)\n\nNext is a helper function that prints information for every layer (sublayer within Serial):\n\ndef show_layers(model, layer_prefix):\n    print(f\"Total layers: {len(model.sublayers)}\\n\")\n    for i in range(len(model.sublayers)):\n        print('========')\n        print(f'{layer_prefix}_{i}: {model.sublayers[i]}\\n')\n\nprint('Siamese model:\\n')\nshow_layers(Siamese, 'Parallel.sublayers')\n\nprint('Detail of LSTM models:\\n')\nshow_layers(LSTM, 'Serial.sublayers')\n\nSiamese model:\n\nTotal layers: 2\n\n========\nParallel.sublayers_0: Serial[\n  Embedding_500_128\n  LSTM_128\n  Mean\n  Normalize\n]\n\n========\nParallel.sublayers_1: Serial[\n  Embedding_500_128\n  LSTM_128\n  Mean\n  Normalize\n]\n\nDetail of LSTM models:\n\nTotal layers: 4\n\n========\nSerial.sublayers_0: Embedding_500_128\n\n========\nSerial.sublayers_1: LSTM_128\n\n========\nSerial.sublayers_2: Mean\n\n========\nSerial.sublayers_3: Normalize\n\n\n\nTry changing the parameters defined before the Siamese model and see how it changes!\nYou will actually train this model in this week’s assignment. For now you should be more familiarized with creating Siamese models using Trax.\nKeep it up!",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "L1 - Creating a Siamese Model using Trax"
    ]
  },
  {
    "objectID": "notes/c4w2/lab02.html",
    "href": "notes/c4w2/lab02.html",
    "title": "The Transformer Decoder: Ungraded Lab Notebook",
    "section": "",
    "text": "In this notebook, you’ll explore the transformer decoder and how to implement it with Trax.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "L2 - The Transformer Decoder"
    ]
  },
  {
    "objectID": "notes/c4w2/lab02.html#background",
    "href": "notes/c4w2/lab02.html#background",
    "title": "The Transformer Decoder: Ungraded Lab Notebook",
    "section": "Background",
    "text": "Background\nIn the last lecture notebook, you saw how to translate the mathematics of attention into NumPy code. Here, you’ll see how multi-head causal attention fits into a GPT-2 transformer decoder, and how to build one with Trax layers. In the assignment notebook, you’ll implement causal attention from scratch, but here, you’ll exploit the handy-dandy tl.CausalAttention() layer.\nThe schematic below illustrates the components and flow of a transformer decoder. Note that while the algorithm diagram flows from the bottom to the top, the overview and subsequent Trax layer codes are top-down.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "L2 - The Transformer Decoder"
    ]
  },
  {
    "objectID": "notes/c4w2/lab02.html#imports",
    "href": "notes/c4w2/lab02.html#imports",
    "title": "The Transformer Decoder: Ungraded Lab Notebook",
    "section": "Imports",
    "text": "Imports\n\nimport sys\nimport os\n\nimport time\nimport numpy as np\nimport gin\n\nimport textwrap\nwrapper = textwrap.TextWrapper(width=70)\n\nimport trax\nfrom trax import layers as tl\nfrom trax.fastmath import numpy as jnp\n\n# to print the entire np array\nnp.set_printoptions(threshold=sys.maxsize)\n\n2025-02-10 16:55:22.489737: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1739199322.502312  123480 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1739199322.506307  123480 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "L2 - The Transformer Decoder"
    ]
  },
  {
    "objectID": "notes/c4w2/lab02.html#sentence-gets-embedded-add-positional-encoding",
    "href": "notes/c4w2/lab02.html#sentence-gets-embedded-add-positional-encoding",
    "title": "The Transformer Decoder: Ungraded Lab Notebook",
    "section": "Sentence gets embedded, add positional encoding",
    "text": "Sentence gets embedded, add positional encoding\nEmbed the words, then create vectors representing each word’s position in each sentence \\in \\{ 0, 1, 2, \\ldots , K\\} = range(max_len), where max_len = K+1)\n\ndef PositionalEncoder(vocab_size, d_model, dropout, max_len, mode):\n    \"\"\"Returns a list of layers that: \n    1. takes a block of text as input, \n    2. embeds the words in that text, and \n    3. adds positional encoding, \n       i.e. associates a number in range(max_len) with \n       each word in each sentence of embedded input text \n    \n    The input is a list of tokenized blocks of text\n    \n    Args:\n        vocab_size (int): vocab size.\n        d_model (int):  depth of embedding.\n        dropout (float): dropout rate (how much to drop out).\n        max_len (int): maximum symbol length for positional encoding.\n        mode (str): 'train' or 'eval'.\n    \"\"\"\n    # Embedding inputs and positional encoder\n    return [ \n        # Add embedding layer of dimension (vocab_size, d_model)\n        tl.Embedding(vocab_size, d_model),  \n        # Use dropout with rate and mode specified\n        tl.Dropout(rate=dropout, mode=mode), \n        # Add positional encoding layer with maximum input length and mode specified\n        tl.PositionalEncoding(max_len=max_len, mode=mode)]",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "L2 - The Transformer Decoder"
    ]
  },
  {
    "objectID": "notes/c4w2/lab02.html#multi-head-causal-attention",
    "href": "notes/c4w2/lab02.html#multi-head-causal-attention",
    "title": "The Transformer Decoder: Ungraded Lab Notebook",
    "section": "Multi-head causal attention",
    "text": "Multi-head causal attention\nThe layers and array dimensions involved in multi-head causal attention (which looks at previous words in the input text) are summarized in the figure below:\n\ntl.CausalAttention() does all of this for you! You might be wondering, though, whether you need to pass in your input text 3 times, since for causal attention, the queries Q, keys K, and values V all come from the same source. Fortunately, tl.CausalAttention() handles this as well by making use of the tl.Branch() combinator layer. In general, each branch within a tl.Branch() layer performs parallel operations on copies of the layer’s inputs. For causal attention, each branch (representing Q, K, and V) applies a linear transformation (i.e. a dense layer without a subsequent activation) to its copy of the input, then splits that result into heads. You can see the syntax for this in the screenshot from the trax.layers.attention.py source code below:",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "L2 - The Transformer Decoder"
    ]
  },
  {
    "objectID": "notes/c4w2/lab02.html#feed-forward-layer",
    "href": "notes/c4w2/lab02.html#feed-forward-layer",
    "title": "The Transformer Decoder: Ungraded Lab Notebook",
    "section": "Feed-forward layer",
    "text": "Feed-forward layer\n\nTypically ends with a ReLU activation, but we’ll leave open the possibility of a different activation\nMost of the parameters are here\n\n\ndef FeedForward(d_model, d_ff, dropout, mode, ff_activation):\n    \"\"\"Returns a list of layers that implements a feed-forward block.\n\n    The input is an activation tensor.\n\n    Args:\n        d_model (int):  depth of embedding.\n        d_ff (int): depth of feed-forward layer.\n        dropout (float): dropout rate (how much to drop out).\n        mode (str): 'train' or 'eval'.\n        ff_activation (function): the non-linearity in feed-forward layer.\n\n    Returns:\n        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\n    \"\"\"\n    \n    # Create feed-forward block (list) with two dense layers with dropout and input normalized\n    return [ \n        # Normalize layer inputs\n        tl.LayerNorm(), \n        # Add first feed forward (dense) layer (don't forget to set the correct value for n_units)\n        tl.Dense(d_ff), \n        # Add activation function passed in as a parameter (you need to call it!)\n        ff_activation(),  # Generally ReLU\n        # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\n        tl.Dropout(rate=dropout, mode=mode), \n        # Add second feed forward layer (don't forget to set the correct value for n_units)\n        tl.Dense(d_model), \n        # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\n        tl.Dropout(rate=dropout, mode=mode) \n    ]",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "L2 - The Transformer Decoder"
    ]
  },
  {
    "objectID": "notes/c4w2/lab02.html#decoder-block",
    "href": "notes/c4w2/lab02.html#decoder-block",
    "title": "The Transformer Decoder: Ungraded Lab Notebook",
    "section": "Decoder block",
    "text": "Decoder block\nHere, we return a list containing two residual blocks. The first wraps around the causal attention layer, whose inputs are normalized and to which we apply dropout regulation. The second wraps around the feed-forward layer. You may notice that the second call to tl.Residual() doesn’t call a normalization layer before calling the feed-forward layer. This is because the normalization layer is included in the feed-forward layer.\n\ndef DecoderBlock(d_model, d_ff, n_heads,\n                 dropout, mode, ff_activation):\n    \"\"\"Returns a list of layers that implements a Transformer decoder block.\n\n    The input is an activation tensor.\n\n    Args:\n        d_model (int):  depth of embedding.\n        d_ff (int): depth of feed-forward layer.\n        n_heads (int): number of attention heads.\n        dropout (float): dropout rate (how much to drop out).\n        mode (str): 'train' or 'eval'.\n        ff_activation (function): the non-linearity in feed-forward layer.\n\n    Returns:\n        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\n    \"\"\"\n        \n    # Add list of two Residual blocks: the attention with normalization and dropout and feed-forward blocks\n    return [\n      tl.Residual(\n          # Normalize layer input\n          tl.LayerNorm(), \n          # Add causal attention \n          tl.CausalAttention(d_feature, n_heads=n_heads, dropout=dropout, mode=mode) \n        ),\n      tl.Residual(\n          # Add feed-forward block\n          # We don't need to normalize the layer inputs here. The feed-forward block takes care of that for us.\n          FeedForward(d_model, d_ff, dropout, mode, ff_activation)\n        ),\n      ]",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "L2 - The Transformer Decoder"
    ]
  },
  {
    "objectID": "notes/c4w2/lab02.html#the-transformer-decoder-putting-it-all-together",
    "href": "notes/c4w2/lab02.html#the-transformer-decoder-putting-it-all-together",
    "title": "The Transformer Decoder: Ungraded Lab Notebook",
    "section": "The transformer decoder: putting it all together",
    "text": "The transformer decoder: putting it all together",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "L2 - The Transformer Decoder"
    ]
  },
  {
    "objectID": "notes/c4w2/lab02.html#a.k.a.-repeat-n-times-dense-layer-and-softmax-for-output",
    "href": "notes/c4w2/lab02.html#a.k.a.-repeat-n-times-dense-layer-and-softmax-for-output",
    "title": "The Transformer Decoder: Ungraded Lab Notebook",
    "section": "A.k.a. repeat N times, dense layer and softmax for output",
    "text": "A.k.a. repeat N times, dense layer and softmax for output\n\ndef TransformerLM(vocab_size=33300,\n                  d_model=512,\n                  d_ff=2048,\n                  n_layers=6,\n                  n_heads=8,\n                  dropout=0.1,\n                  max_len=4096,\n                  mode='train',\n                  ff_activation=tl.Relu):\n    \"\"\"Returns a Transformer language model.\n\n    The input to the model is a tensor of tokens. (This model uses only the\n    decoder part of the overall Transformer.)\n\n    Args:\n        vocab_size (int): vocab size.\n        d_model (int):  depth of embedding.\n        d_ff (int): depth of feed-forward layer.\n        n_layers (int): number of decoder layers.\n        n_heads (int): number of attention heads.\n        dropout (float): dropout rate (how much to drop out).\n        max_len (int): maximum symbol length for positional encoding.\n        mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.\n        ff_activation (function): the non-linearity in feed-forward layer.\n\n    Returns:\n        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens\n        to activations over a vocab set.\n    \"\"\"\n    \n    # Create stack (list) of decoder blocks with n_layers with necessary parameters\n    decoder_blocks = [ \n        DecoderBlock(d_model, d_ff, n_heads, dropout, mode, ff_activation) for _ in range(n_layers)] \n\n    # Create the complete model as written in the figure\n    return tl.Serial(\n        # Use teacher forcing (feed output of previous step to current step)\n        tl.ShiftRight(mode=mode), \n        # Add embedding inputs and positional encoder\n        PositionalEncoder(vocab_size, d_model, dropout, max_len, mode),\n        # Add decoder blocks\n        decoder_blocks, \n        # Normalize layer\n        tl.LayerNorm(), \n\n        # Add dense layer of vocab_size (since need to select a word to translate to)\n        # (a.k.a., logits layer. Note: activation already set by ff_activation)\n        tl.Dense(vocab_size), \n        # Get probabilities with Logsoftmax\n        tl.LogSoftmax() \n    )",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "L2 - The Transformer Decoder"
    ]
  },
  {
    "objectID": "notes/c4w2/lab02.html#concluding-remarks",
    "href": "notes/c4w2/lab02.html#concluding-remarks",
    "title": "The Transformer Decoder: Ungraded Lab Notebook",
    "section": "Concluding remarks",
    "text": "Concluding remarks\nIn this week’s assignment, you’ll see how to train a transformer decoder on the cnn_dailymail dataset, available from TensorFlow Datasets (part of TensorFlow Data Services). Because training such a model from scratch is time-intensive, you’ll use a pre-trained model to summarize documents later in the assignment. Due to time and storage concerns, we will also not train the decoder on a different summarization dataset in this lab. If you have the time and space, we encourage you to explore the other summarization datasets at TensorFlow Datasets. Which of them might suit your purposes better than the cnn_dailymail dataset? Where else can you find datasets for text summarization models?",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "L2 - The Transformer Decoder"
    ]
  },
  {
    "objectID": "notes/c2w3/lab02.html",
    "href": "notes/c2w3/lab02.html",
    "title": "Building the language model",
    "section": "",
    "text": "course banner",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "L2 - Building the language model"
    ]
  },
  {
    "objectID": "notes/c2w3/lab02.html#language-model-evaluation",
    "href": "notes/c2w3/lab02.html#language-model-evaluation",
    "title": "Building the language model",
    "section": "Language model evaluation",
    "text": "Language model evaluation\n\nTrain/validation/test split\nIn the videos, we saw that to evaluate language models, we need to keep some of the corpus data for validation and testing.\nThe choice of the test and validation data should correspond as much as possible to the distribution of the data coming from the actual application. If nothing but the input corpus is known, then random sampling from the corpus is used to define the test and validation subset.\nHere is a code similar to what you’ll see in the code assignment. The following function allows we to randomly sample the input data and return train/validation/test subsets in a split given by the method parameters.\n\n# we only need train and validation %, test is the remainder\nimport random\ndef train_validation_test_split(data, train_percent, validation_percent):\n    \"\"\"\n    Splits the input data to  train/validation/test according to the percentage provided\n    \n    Args:\n        data: Pre-processed and tokenized corpus, i.e. list of sentences.\n        train_percent: integer 0-100, defines the portion of input corpus allocated for training\n        validation_percent: integer 0-100, defines the portion of input corpus allocated for validation\n        \n        Note: train_percent + validation_percent need to be &lt;=100\n              the reminder to 100 is allocated for the test set\n    \n    Returns:\n        train_data: list of sentences, the training part of the corpus\n        validation_data: list of sentences, the validation part of the corpus\n        test_data: list of sentences, the test part of the corpus\n    \"\"\"\n    # fixed seed here for reproducibility\n    random.seed(87)\n    \n    # reshuffle all input sentences\n    random.shuffle(data)\n\n    train_size = int(len(data) * train_percent / 100)\n    train_data = data[0:train_size]\n    \n    validation_size = int(len(data) * validation_percent / 100)\n    validation_data = data[train_size:train_size + validation_size]\n    \n    test_data = data[train_size + validation_size:]\n    \n    return train_data, validation_data, test_data\n\ndata = [x for x in range (0, 100)]\n\ntrain_data, validation_data, test_data = train_validation_test_split(data, 80, 10)\nprint(\"split 80/10/10:\\n\",f\"train data:{train_data}\\n\", f\"validation data:{validation_data}\\n\", \n      f\"test data:{test_data}\\n\")\n\ntrain_data, validation_data, test_data = train_validation_test_split(data, 98, 1)\nprint(\"split 98/1/1:\\n\",f\"train data:{train_data}\\n\", f\"validation data:{validation_data}\\n\", \n      f\"test data:{test_data}\\n\")\n\nsplit 80/10/10:\n train data:[28, 76, 5, 0, 62, 29, 54, 95, 88, 58, 4, 22, 92, 14, 50, 77, 47, 33, 75, 68, 56, 74, 43, 80, 83, 84, 73, 93, 66, 87, 9, 91, 64, 79, 20, 51, 17, 27, 12, 31, 67, 81, 7, 34, 45, 72, 38, 30, 16, 60, 40, 86, 48, 21, 70, 59, 6, 19, 2, 99, 37, 36, 52, 61, 97, 44, 26, 57, 89, 55, 53, 85, 3, 39, 10, 71, 23, 32, 25, 8]\n validation data:[78, 65, 63, 11, 49, 98, 1, 46, 15, 41]\n test data:[90, 96, 82, 42, 35, 13, 69, 24, 94, 18]\n\nsplit 98/1/1:\n train data:[66, 23, 29, 28, 52, 87, 70, 13, 15, 2, 62, 43, 82, 50, 40, 32, 30, 79, 71, 89, 6, 10, 34, 78, 11, 49, 39, 42, 26, 46, 58, 96, 97, 8, 56, 86, 33, 93, 92, 91, 57, 65, 95, 20, 72, 3, 12, 9, 47, 37, 67, 1, 16, 74, 53, 99, 54, 68, 5, 18, 27, 17, 48, 36, 24, 45, 73, 19, 41, 59, 21, 98, 0, 31, 4, 85, 80, 64, 84, 88, 25, 44, 61, 22, 60, 94, 76, 38, 77, 81, 90, 69, 63, 7, 51, 14, 55, 83]\n validation data:[35]\n test data:[75]\n\n\n\n\n\nPerplexity\nIn order to implement the perplexity formula, you’ll need to know how to implement m-th order root of a variable.\n\\begin{equation*}\nPP(W)=\\sqrt[M]{\\prod_{i=1}^{m}{\\frac{1}{P(w_i|w_{i-1})}}}\n\\end{equation*}\nRemember from calculus:\n\\begin{equation*}\n\\sqrt[M]{\\frac{1}{x}} = x^{-\\frac{1}{M}}\n\\end{equation*}\nHere is a code that will help we with the formula.\n\n# to calculate the exponent, use the following syntax\np = 10 ** (-250)\nM = 100\nperplexity = p ** (-1 / M)\nprint(perplexity)\n\n316.22776601683796\n\n\nThat’s all for the lab for “N-gram language model” lesson of week 3.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "L2 - Building the language model"
    ]
  },
  {
    "objectID": "notes/c2w3/lab03.html",
    "href": "notes/c2w3/lab03.html",
    "title": "Out of vocabulary words (OOV)",
    "section": "",
    "text": "course banner\n\n\n\nVocabulary\nIn the video about the out of vocabulary words, we saw that the first step in dealing with the unknown words is to decide which words belong to the vocabulary.\nIn the code assignment, we will try the method based on minimum frequency - all words appearing in the training set with frequency &gt;= minimum frequency are added to the vocabulary.\nHere is a code for the other method, where the target size of the vocabulary is known in advance and the vocabulary is filled with words based on their frequency in the training set.\n\n# build the vocabulary from M most frequent words\n# use Counter object from the collections library to find M most common words\nfrom collections import Counter\n\n# the target size of the vocabulary\nM = 3\n\n# pre-calculated word counts\n# Counter could be used to build this dictionary from the source corpus\nword_counts = {'happy': 5, 'because': 3, 'i': 2, 'am': 2, 'learning': 3, '.': 1}\n\nvocabulary = Counter(word_counts).most_common(M)\n\n# remove the frequencies and leave just the words\nvocabulary = [w[0] for w in vocabulary]\n\nprint(f\"the new vocabulary containing {M} most frequent words: {vocabulary}\\n\") \n\nthe new vocabulary containing 3 most frequent words: ['happy', 'because', 'learning']\n\n\n\nNow that the vocabulary is ready, we can use it to replace the OOV words with &lt;UNK&gt; as we saw in the lecture.\n\n# test if words in the input sentences are in the vocabulary, if OOV, print &lt;UNK&gt;\nsentence = ['am', 'i', 'learning']\noutput_sentence = []\nprint(f\"input sentence: {sentence}\")\n\nfor w in sentence:\n    # test if word w is in vocabulary\n    if w in vocabulary:\n        output_sentence.append(w)\n    else:\n        output_sentence.append('&lt;UNK&gt;')\n        \nprint(f\"output sentence: {output_sentence}\")\n\ninput sentence: ['am', 'i', 'learning']\noutput sentence: ['&lt;UNK&gt;', '&lt;UNK&gt;', 'learning']\n\n\nWhen building the vocabulary in the code assignment, we will need to know how to iterate through the word counts dictionary.\nHere is an example of a similar task showing how to go through all the word counts and print out only the words with the frequency equal to f. \n\n# iterate through all word counts and print words with given frequency f\nf = 3\n\nword_counts = {'happy': 5, 'because': 3, 'i': 2, 'am': 2, 'learning':3, '.': 1}\n\nfor word, freq in word_counts.items():\n    if freq == f:\n        print(word)\n\nbecause\nlearning\n\n\nAs mentioned in the videos, if there are many &lt;UNK&gt; replacements in your train and test set, we may get a very low perplexity even though the model itself wouldn’t be very helpful.\nHere is a sample code showing this unwanted effect.\n\n# many &lt;unk&gt; low perplexity \ntraining_set = ['i', 'am', 'happy', 'because','i', 'am', 'learning', '.']\ntraining_set_unk = ['i', 'am', '&lt;UNK&gt;', '&lt;UNK&gt;','i', 'am', '&lt;UNK&gt;', '&lt;UNK&gt;']\n\ntest_set = ['i', 'am', 'learning']\ntest_set_unk = ['i', 'am', '&lt;UNK&gt;']\n\nM = len(test_set)\nprobability = 1\nprobability_unk = 1\n\n# pre-calculated probabilities\nbigram_probabilities = {('i', 'am'): 1.0, ('am', 'happy'): 0.5, ('happy', 'because'): 1.0, ('because', 'i'): 1.0, ('am', 'learning'): 0.5, ('learning', '.'): 1.0}\nbigram_probabilities_unk = {('i', 'am'): 1.0, ('am', '&lt;UNK&gt;'): 1.0, ('&lt;UNK&gt;', '&lt;UNK&gt;'): 0.5, ('&lt;UNK&gt;', 'i'): 0.25}\n\n# got through the test set and calculate its bigram probability\nfor i in range(len(test_set) - 2 + 1):\n    bigram = tuple(test_set[i: i + 2])\n    probability = probability * bigram_probabilities[bigram]\n        \n    bigram_unk = tuple(test_set_unk[i: i + 2])\n    probability_unk = probability_unk * bigram_probabilities_unk[bigram_unk]\n\n# calculate perplexity for both original test set and test set with &lt;UNK&gt;\nperplexity = probability ** (-1 / M)\nperplexity_unk = probability_unk ** (-1 / M)\n\nprint(f\"perplexity for the training set: {perplexity}\")\nprint(f\"perplexity for the training set with &lt;UNK&gt;: {perplexity_unk}\")\n\nperplexity for the training set: 1.2599210498948732\nperplexity for the training set with &lt;UNK&gt;: 1.0\n\n\n\n\nSmoothing\nAdd-k smoothing was described as a method for smoothing of the probabilities for previously unseen n-grams.\nHere is an example code that shows how to implement add-k smoothing but also highlights a disadvantage of this method. The downside is that n-grams not previously seen in the training dataset get too high probability.\nIn the code output bellow you’ll see that a phrase that is in the training set gets the same probability as an unknown phrase.\n\ndef add_k_smooting_probability(k, vocabulary_size, n_gram_count, n_gram_prefix_count):\n    numerator = n_gram_count + k\n    denominator = n_gram_prefix_count + k * vocabulary_size\n    return numerator / denominator\n\ntrigram_probabilities = {('i', 'am', 'happy') : 2}\nbigram_probabilities = {( 'am', 'happy') : 10}\nvocabulary_size = 5\nk = 1\n\nprobability_known_trigram = add_k_smooting_probability(k, vocabulary_size, trigram_probabilities[('i', 'am', 'happy')], \n                           bigram_probabilities[( 'am', 'happy')])\n\nprobability_unknown_trigram = add_k_smooting_probability(k, vocabulary_size, 0, 0)\n\nprint(f\"probability_known_trigram: {probability_known_trigram}\")\nprint(f\"probability_unknown_trigram: {probability_unknown_trigram}\")\n\nprobability_known_trigram: 0.2\nprobability_unknown_trigram: 0.2\n\n\n\n\nBack-off\nBack-off is a model generalization method that leverages information from lower order n-grams in case information about the high order n-grams is missing. For example, if the probability of an trigram is missing, use bigram information and so on.\nHere we can see an example of a simple back-off technique.\n\n# pre-calculated probabilities of all types of n-grams\ntrigram_probabilities = {('i', 'am', 'happy'): 0}\nbigram_probabilities = {( 'am', 'happy'): 0.3}\nunigram_probabilities = {'happy': 0.4}\n\n# this is the input trigram we need to estimate\ntrigram = ('are', 'you', 'happy')\n\n# find the last bigram and unigram of the input\nbigram = trigram[1: 3]\nunigram = trigram[2]\nprint(f\"besides the trigram {trigram} we also use bigram {bigram} and unigram ({unigram})\\n\")\n\n# 0.4 is used as an example, experimentally found for web-scale corpuses when using the \"stupid\" back-off\nlambda_factor = 0.4\nprobability_hat_trigram = 0\n\n# search for first non-zero probability starting with trigram\n# to generalize this for any order of n-gram hierarchy, \n# we could loop through the probability dictionaries instead of if/else cascade\nif trigram not in trigram_probabilities or trigram_probabilities[trigram] == 0:\n    print(f\"probability for trigram {trigram} not found\")\n    \n    if bigram not in bigram_probabilities or bigram_probabilities[bigram] == 0:\n        print(f\"probability for bigram {bigram} not found\")\n        \n        if unigram in unigram_probabilities:\n            print(f\"probability for unigram {unigram} found\\n\")\n            probability_hat_trigram = lambda_factor * lambda_factor * unigram_probabilities[unigram]\n        else:\n            probability_hat_trigram = 0\n    else:\n        probability_hat_trigram = lambda_factor * bigram_probabilities[bigram]\nelse:\n    probability_hat_trigram = trigram_probabilities[trigram]\n\nprint(f\"probability for trigram {trigram} estimated as {probability_hat_trigram}\")\n\nbesides the trigram ('are', 'you', 'happy') we also use bigram ('you', 'happy') and unigram (happy)\n\nprobability for trigram ('are', 'you', 'happy') not found\nprobability for bigram ('you', 'happy') not found\nprobability for unigram happy found\n\nprobability for trigram ('are', 'you', 'happy') estimated as 0.06400000000000002\n\n\n\n\nInterpolation\nThe other method for using probabilities of lower order n-grams is the interpolation. In this case, we use weighted probabilities of n-grams of all orders every time, not just when high order information is missing.\nFor example, we always combine trigram, bigram and unigram probability. We can see how this in the following code snippet.\n\n# pre-calculated probabilities of all types of n-grams\ntrigram_probabilities = {('i', 'am', 'happy'): 0.15}\nbigram_probabilities = {( 'am', 'happy'): 0.3}\nunigram_probabilities = {'happy': 0.4}\n\n# the weights come from optimization on a validation set\nlambda_1 = 0.8\nlambda_2 = 0.15\nlambda_3 = 0.05\n\n# this is the input trigram we need to estimate\ntrigram = ('i', 'am', 'happy')\n\n# find the last bigram and unigram of the input\nbigram = trigram[1: 3]\nunigram = trigram[2]\nprint(f\"besides the trigram {trigram} we also use bigram {bigram} and unigram ({unigram})\\n\")\n\n# in the production code, we would need to check if the probability n-gram dictionary contains the n-gram\nprobability_hat_trigram = lambda_1 * trigram_probabilities[trigram] \n+ lambda_2 * bigram_probabilities[bigram]\n+ lambda_3 * unigram_probabilities[unigram]\n\nprint(f\"estimated probability of the input trigram {trigram} is {probability_hat_trigram}\")\n\nbesides the trigram ('i', 'am', 'happy') we also use bigram ('am', 'happy') and unigram (happy)\n\n\n\n0.045\n\n\n0.020000000000000004\n\n\nestimated probability of the input trigram ('i', 'am', 'happy') is 0.12\n\n\nThat’s it for week 3, we should be ready now for the code assignment.\n\n\n\n\nCitationBibTeX citation:@online{bochman2020,\n  author = {Bochman, Oren},\n  title = {Out of Vocabulary Words {(OOV)}},\n  date = {2020-10-27},\n  url = {https://orenbochman.github.io/notes-nlp/notes/c2w3/lab03.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2020. “Out of Vocabulary Words (OOV).”\nOctober 27, 2020. https://orenbochman.github.io/notes-nlp/notes/c2w3/lab03.html.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "L3 - Out of vocabulary words"
    ]
  },
  {
    "objectID": "notes/c4w4/lab02.html",
    "href": "notes/c4w4/lab02.html",
    "title": "Putting the “Re” in Reformer: Ungraded Lab",
    "section": "",
    "text": "This ungraded lab will explore Reversible Residual Networks. You will use these networks in this week’s assignment that utilizes the Reformer model. It is based on on the Transformer model you already know, but with two unique features. * Locality Sensitive Hashing (LSH) Attention to reduce the compute cost of the dot product attention and * Reversible Residual Networks (RevNets) organization to reduce the storage requirements when doing backpropagation in training.\nIn this ungraded lab we’ll start with a quick review of Residual Networks and their implementation in Trax. Then we will discuss the Revnet architecture and its use in Reformer.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "L2 - Revnet"
    ]
  },
  {
    "objectID": "notes/c4w4/lab02.html#outline",
    "href": "notes/c4w4/lab02.html#outline",
    "title": "Putting the “Re” in Reformer: Ungraded Lab",
    "section": "Outline",
    "text": "Outline\n\nPart 1: Residual Networks\n\n1.1 Branch\n1.2 Residual Model\n\nPart 2: Reversible Residual Networks\n\n2.1 Trax Reversible Layers\n2.2 Residual Model\n\n\n\nimport trax\nfrom trax import layers as tl               # core building block\nimport numpy as np                          # regular ol' numpy\nfrom trax.models.reformer.reformer import (\n    ReversibleHalfResidualV2 as ReversibleHalfResidual,\n)                                           # unique spot\nfrom trax import fastmath                   # uses jax, offers numpy on steroids\nfrom trax import shapes                     # data signatures: dimensionality and type\nfrom trax.fastmath import numpy as jnp      # For use in defining new layer types.\nfrom trax.shapes import ShapeDtype\nfrom trax.shapes import signature\n\n2025-02-10 16:54:01.601593: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1739199241.613582  121997 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1739199241.617462  121997 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\n\n\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\nCell In[1], line 4\n      2 from trax import layers as tl               # core building block\n      3 import numpy as np                          # regular ol' numpy\n----&gt; 4 from trax.models.reformer.reformer import (\n      5     ReversibleHalfResidualV2 as ReversibleHalfResidual,\n      6 )                                           # unique spot\n      7 from trax import fastmath                   # uses jax, offers numpy on steroids\n      8 from trax import shapes                     # data signatures: dimensionality and type\n\nImportError: cannot import name 'ReversibleHalfResidualV2' from 'trax.models.reformer.reformer' (/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/models/reformer/reformer.py)",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "L2 - Revnet"
    ]
  },
  {
    "objectID": "notes/c4w4/lab02.html#1",
    "href": "notes/c4w4/lab02.html#1",
    "title": "Putting the “Re” in Reformer: Ungraded Lab",
    "section": "Part 1.0 Residual Networks",
    "text": "Part 1.0 Residual Networks\nDeep Residual Networks (Resnets) were introduced to improve convergence in deep networks. Residual Networks introduce a shortcut connection around one or more layers in a deep network as shown in the diagram below from the original paper.\n\n\n\n\n\n\nFigure 1: Residual Network diagram from original paper\n\n\n\nThe Trax documentation describes an implementation of Resnets using branch. We’ll explore that here by implementing a simple resnet built from simple function based layers. Specifically, we’ll build a 4 layer network based on two functions, ‘F’ and ‘G’.\n\n\n\n\n\n\nFigure 2: 4 stage Residual network\n\n\n\nDon’t worry about the lengthy equations. Those are simply there to be referenced later in the notebook.\n\nPart 1.1 Branch\nTrax branch figures prominently in the residual network layer so we will first examine it. You can see from the figure above that we will need a function that will copy an input and send it down multiple paths. This is accomplished with a branch layer, one of the Trax ‘combinators’. Branch is a combinator that applies a list of layers in parallel to copies of inputs. Lets try it out! First we will need some layers to play with. Let’s build some from functions.\n\n# simple function taking one input and one output\nbl_add1 = tl.Fn(\"add1\", lambda x0: (x0 + 1), n_out=1)\nbl_add2 = tl.Fn(\"add2\", lambda x0: (x0 + 2), n_out=1)\nbl_add3 = tl.Fn(\"add3\", lambda x0: (x0 + 3), n_out=1)\n# try them out\nx = np.array([1])\nprint(bl_add1(x), bl_add2(x), bl_add3(x))\n# some information about our new layers\nprint(\n    \"name:\",\n    bl_add1.name,\n    \"number of inputs:\",\n    bl_add1.n_in,\n    \"number of outputs:\",\n    bl_add1.n_out,\n)\n\n[2] [3] [4]\nname: add1 number of inputs: 1 number of outputs: 1\n\n\n\nbl_3add1s = tl.Branch(bl_add1, bl_add2, bl_add3)\nbl_3add1s\n\nBranch_out3[\n  add1\n  add2\n  add3\n]\n\n\nTrax uses the concept of a ‘stack’ to transfer data between layers. For Branch, for each of its layer arguments, it copies the n_in inputs from the stack and provides them to the layer, tracking the max_n_in, or the largest n_in required. It then pops the max_n_in elements from the stack. \n\nFigure 3: One in, one out Branch\n\nOn output, each layer, in succession pushes its results onto the stack. Note that the push/pull operations impact the top of the stack. Elements that are not part of the operation (n, and m in the diagram) remain intact.\n\n# n_in = 1, Each bl_addx pushes n_out = 1 elements onto the stack\nbl_3add1s(x)\n\n(array([2]), array([3]), array([4]))\n\n\n\n# n = np.array([10]); m = np.array([20])  # n, m will remain on the stack\nn = \"n\"\nm = \"m\"  # n, m will remain on the stack\nbl_3add1s([x, n, m]) \n\n(array([2]), array([3]), array([4]), 'n', 'm')\n\n\nEach layer in the input list copies as many inputs from the stack as it needs, and their outputs are successively combined on stack. Put another way, each element of the branch can have differing numbers of inputs and outputs. Let’s try a more complex example.\n\nbl_addab = tl.Fn(\n    \"addab\", lambda x0, x1: (x0 + x1), n_out=1\n)  # Trax figures out how many inputs there are\nbl_rep3x = tl.Fn(\n    \"add2x\", lambda x0: (x0, x0, x0), n_out=3\n)  # but you have to tell it how many outputs there are\nbl_3ops = tl.Branch(bl_add1, bl_addab, bl_rep3x)\n\nIn this case, the number if inputs being copied from the stack varies with the layer \n\nFigure 4: variable in, variable out Branch\n\nThe stack when the operation is finished is 5 entries reflecting the total from each layer.\n\n# Before Running this cell, what is the output you are expecting?\ny = np.array([3])\nbl_3ops([x, y, n, m])\n\n(array([2]), array([4]), array([1]), array([1]), array([1]), 'n', 'm')\n\n\nBranch has a special feature to support Residual Network. If an argument is ‘None’, it will pull the top of stack and push it (at its location in the sequence) onto the output stack \n\nFigure 5: Branch for Residual\n\n\nbl_2ops = tl.Branch(bl_add1, None)\nbl_2ops([x, n, m])\n\n(array([2]), array([1]), 'n', 'm')\n\n\n ### Part 1.2 Residual Model OK, your turn. Write a function ‘MyResidual’, that uses tl.Branch and tl.Add to build a residual layer. If you are curious about the Trax implementation, you can see the code here.\n\ndef MyResidual(layer):\n    return tl.Serial(\n        ### START CODE HERE ###\n        tl.Branch(layer, None),\n        tl.Add(),\n        ### END CODE HERE ###\n    )\n\n\n# Lets Try it\nmr = MyResidual(bl_add1)\nx = np.array([1])\nmr([x, n, m])\n\n(array([3]), 'n', 'm')\n\n\nExpected Result (array([3]), ‘n’, ‘m’)\nGreat! Now, let’s build the 4 layer residual Network in Figure 2. You can use MyResidual, or if you prefer, the tl.Residual in Trax, or a combination!\n\nFl = tl.Fn(\"F\", lambda x0: (2 * x0), n_out=1)\nGl = tl.Fn(\"G\", lambda x0: (10 * x0), n_out=1)\nx1 = np.array([1])\n\n\nresfg = tl.Serial(\n    ### START CODE HERE ###\n    # None,  #Fl    # x + F(x)\n    # None,  #Gl    # x + F(x) + G(x + F(x)) etc\n    # None,  #Fl\n    # None,  #Gl\n    ### END CODE HERE ###\n)\n\n\n# Lets try it\nresfg([x1, n, m])\n\n[array([1]), 'n', 'm']\n\n\nExpected Results (array([1089]), ‘n’, ‘m’)\n ## Part 2.0 Reversible Residual Networks The Reformer utilized RevNets to reduce the storage requirements for performing backpropagation. \n\nFigure 6: Reversible Residual Networks \n\nThe standard approach on the left above requires one to store the outputs of each stage for use during backprop. By using the organization to the right, one need only store the outputs of the last stage, y1, y2 in the diagram. Using those values and running the algorithm in reverse, one can reproduce the values required for backprop. This trades additional computation for memory space which is at a premium with the current generation of GPU’s/TPU’s.\nOne thing to note is that the forward functions produced by two networks are similar, but they are not equivalent. Note for example the asymmetry in the output equations after two stages of operation. \n\nFigure 7: ‘Normal’ Residual network (Top) vs REversible Residual Network \n\n\n\nPart 2.1 Trax Reversible Layers\nLet’s take a look at how this is used in the Reformer.\n\nrefm = trax.models.reformer.ReformerLM(\n    vocab_size=33000, n_layers=2, mode=\"train\"  # Add more options.\n)\nrefm\n\nSerial[\n  Serial[\n    ShiftRight(1)\n  ]\n  Embedding_33000_512\n  Dropout\n  Serial[\n    PositionalEncoding\n  ]\n  Dup_out2\n  ReversibleSerial_in2_out2[\n    ReversibleHalfResidualDecoderAttn_in2_out2[\n      Serial[\n        LayerNorm\n      ]\n      SelfAttention\n    ]\n    ReversibleSwap_in2_out2\n    ReversibleHalfResidualDecoderFF_in2_out2[\n      Serial[\n        LayerNorm\n        Dense_2048\n        Dropout\n        Serial[\n          FastGelu\n        ]\n        Dense_512\n        Dropout\n      ]\n    ]\n    ReversibleSwap_in2_out2\n    ReversibleHalfResidualDecoderAttn_in2_out2[\n      Serial[\n        LayerNorm\n      ]\n      SelfAttention\n    ]\n    ReversibleSwap_in2_out2\n    ReversibleHalfResidualDecoderFF_in2_out2[\n      Serial[\n        LayerNorm\n        Dense_2048\n        Dropout\n        Serial[\n          FastGelu\n        ]\n        Dense_512\n        Dropout\n      ]\n    ]\n    ReversibleSwap_in2_out2\n  ]\n  Concatenate_in2\n  LayerNorm\n  Dropout\n  Serial[\n    Dense_33000\n  ]\n]\n\n\nEliminating some of the detail, we can see the structure of the network. \n\nFigure 8: Key Structure of Reformer Reversible Network Layers in Trax \n\nWe’ll review the Trax layers used to implement the Reversible section of the Reformer. First we can note that not all of the reformer is reversible. Only the section in the ReversibleSerial layer is reversible. In a large Reformer model, that section is repeated many times making up the majority of the model. \n\nFigure 9: Functional Diagram of Trax elements in Reformer \n\nThe implementation starts by duplicating the input to allow the two paths that are part of the reversible residual organization with Dup. Note that this is accomplished by copying the top of stack and pushing two copies of it onto the stack. This then feeds into the ReversibleHalfResidual layer which we’ll review in more detail below. This is followed by ReversibleSwap. As the name implies, this performs a swap, in this case, the two topmost entries in the stack. This pattern is repeated until we reach the end of the ReversibleSerial section. At that point, the topmost 2 entries of the stack represent the two paths through the network. These are concatenated and pushed onto the stack. The result is an entry that is twice the size of the non-reversible version.\nLet’s look more closely at the ReversibleHalfResidual. This layer is responsible for executing the layer or layers provided as arguments and adding the output of those layers, the ‘residual’, to the top of the stack. Below is the ‘forward’ routine which implements this. \n\nFigure 10: ReversibleHalfResidual code and diagram \n\nUnlike the previous residual function, the value that is added is from the second path rather than the input to the set of sublayers in this layer. Note that the Layers called by the ReversibleHalfResidual forward function are not modified to support reverse functionality. This layer provides them a ‘normal’ view of the stack and takes care of reverse operation.\nLet’s try out some of these layers! We’ll start with the ones that just operate on the stack, Dup() and Swap().\n\nx1 = np.array([1])\nx2 = np.array([5])\n# Dup() duplicates the Top of Stack and returns the stack\ndl = tl.Dup()\ndl(x1)\n\n(array([1]), array([1]))\n\n\n\n# ReversibleSwap() duplicates the Top of Stack and returns the stack\nsl = tl.ReversibleSwap()\nsl([x1, x2])\n\n(array([5]), array([1]))\n\n\nYou are no doubt wondering “How is ReversibleSwap different from Swap?”. Good question! Lets look: \n\nFigure 11: Two versions of Swap() \n\nThe ReverseXYZ functions include a “reverse” compliment to their “forward” function that provides the functionality to run in reverse when doing backpropagation. It can also be run in reverse by simply calling ‘reverse’.\n\n# Demonstrate reverse swap\nprint(x1, x2, sl.reverse([x1, x2]))\n\n[1] [5] (array([5]), array([1]))\n\n\nLet’s try ReversibleHalfResidual, First we’ll need some layers..\n\nFl = tl.Fn(\"F\", lambda x0: (2 * x0), n_out=1)\nGl = tl.Fn(\"G\", lambda x0: (10 * x0), n_out=1)\n\nJust a note about ReversibleHalfResidual. As this is written, it resides in the Reformer model and is a layer. It is invoked a bit differently that other layers. Rather than tl.XYZ, it is just ReversibleHalfResidual(layers..) as shown below. This may change in the future.\n\nhalf_res_F = ReversibleHalfResidual(Fl)\nprint(type(half_res_F), \"\\n\", half_res_F)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[19], line 1\n----&gt; 1 half_res_F = ReversibleHalfResidual(Fl)\n      2 print(type(half_res_F), \"\\n\", half_res_F)\n\nNameError: name 'ReversibleHalfResidual' is not defined\n\n\n\n\nhalf_res_F([x1, x1])  # this is going to produce an error - why?\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[20], line 1\n----&gt; 1 half_res_F([x1, x1])  # this is going to produce an error - why?\n\nNameError: name 'half_res_F' is not defined\n\n\n\n\n# we have to initialize the ReversibleHalfResidual layer to let it know what the input is going to look like\nhalf_res_F.init(shapes.signature([x1, x1]))\nhalf_res_F([x1, x1])\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[21], line 2\n      1 # we have to initialize the ReversibleHalfResidual layer to let it know what the input is going to look like\n----&gt; 2 half_res_F.init(shapes.signature([x1, x1]))\n      3 half_res_F([x1, x1])\n\nNameError: name 'half_res_F' is not defined\n\n\n\nNotice the output: (DeviceArray([3], dtype=int32), array([1])). The first value, (DeviceArray([3], dtype=int32) is the output of the “Fl” layer and has been converted to a ‘Jax’ DeviceArray. The second array([1]) is just passed through (recall the diagram of ReversibleHalfResidual above).\nThe final layer we need is the ReversibleSerial Layer. This is the reversible equivalent of the Serial layer and is used in the same manner to build a sequence of layers.\n ### Part 2.2 Build a reversible model We now have all the layers we need to build the model shown below. Let’s build it in two parts. First we’ll build ‘blk’ and then a list of blk’s. And then ‘mod’.\n\n\n\n\nFigure 12: Reversible Model we will build using Trax components \n\n\nblk = [  # a list of the 4 layers shown above\n    ### START CODE HERE ###\n    None,\n    None,\n    None,\n    None,\n]\nblks = [None, None]\n### END CODE HERE ###\n\n\nmod = tl.Serial(\n    ### START CODE HERE ###\n    None,\n    None,\n    None,\n    ### END CODE HERE ###\n)\nmod\n\n/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/combinators.py:437: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n  if self._mode == 'predict' and self._state[1] is not ():  # pylint: disable=literal-comparison\n/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/combinators.py:910: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n  if state[0] is ():  # pylint: disable=literal-comparison\n/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/combinators.py:437: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n  if self._mode == 'predict' and self._state[1] is not ():  # pylint: disable=literal-comparison\n/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/combinators.py:910: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n  if state[0] is ():  # pylint: disable=literal-comparison\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[23], line 1\n----&gt; 1 mod = tl.Serial(\n      2     ### START CODE HERE ###\n      3     None,\n      4     None,\n      5     None,\n      6     ### END CODE HERE ###\n      7 )\n      8 mod\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/combinators.py:59, in Serial.__init__(self, name, sublayers_to_print, *sublayers)\n     55 def __init__(self, *sublayers, name=None, sublayers_to_print=None):\n     56   super().__init__(\n     57       name=name, sublayers_to_print=sublayers_to_print)\n---&gt; 59   sublayers = _ensure_flat(sublayers)\n     60   self._sublayers = sublayers\n     61   self._n_layers = len(sublayers)\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/combinators.py:1110, in _ensure_flat(layers)\n   1108 for obj in layers:\n   1109   if not isinstance(obj, base.Layer):\n-&gt; 1110     raise ValueError(\n   1111         f'Found nonlayer object ({obj}) in layers: {layers}')\n   1112 return layers\n\nValueError: Found nonlayer object (None) in layers: [None, None, None]\n\n\n\nExpected Output\nSerial[\n  Dup_out2\n  ReversibleSerial_in2_out2[\n    ReversibleHalfResidualV2_in2_out2[\n      Serial[\n        F\n      ]\n    ]\n    ReversibleSwap_in2_out2\n    ReversibleHalfResidualV2_in2_out2[\n      Serial[\n        G\n      ]\n    ]\n    ReversibleSwap_in2_out2\n    ReversibleHalfResidualV2_in2_out2[\n      Serial[\n        F\n      ]\n    ]\n    ReversibleSwap_in2_out2\n    ReversibleHalfResidualV2_in2_out2[\n      Serial[\n        G\n      ]\n    ]\n    ReversibleSwap_in2_out2\n  ]\n  Concatenate_in2\n]\n\nmod.init(shapes.signature(x1))\nout = mod(x1)\nout\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[24], line 1\n----&gt; 1 mod.init(shapes.signature(x1))\n      2 out = mod(x1)\n      3 out\n\nNameError: name 'mod' is not defined\n\n\n\nExpected Result DeviceArray([ 65, 681], dtype=int32)\nOK, now you have had a chance to try all the ‘Reversible’ functions in Trax. On to the Assignment!",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "L2 - Revnet"
    ]
  },
  {
    "objectID": "notes/c3w1/index.html",
    "href": "notes/c3w1/index.html",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nFigure 1\nMy notes for Week 1 of the Natural Language Processing with Sequence Models Course in the Natural Language Processing Specialization Offered by DeepLearning.AI on Coursera",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w1/index.html#sec-course-intro",
    "href": "notes/c3w1/index.html#sec-course-intro",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "Course Intro",
    "text": "Course Intro\n\n\n\n\n\n\nCourse Intro Video Transcript\n\n\n\n\n\n\nWelcome to the third course of this specialization. This course is called natural language processing with sequence models. In this course, here are some of the things you get to build. First, you take sentiment analysis to the next level with deep neural networks. You also build a language generator using RNNs or recurrent neural networks. You apply LSTM units. LSTM stands for long short term memory. You apply LCM units to the problem of named entity recognition. And, finally you use Siamese networks to identify duplicate questions like say if there’s an online discussion forum and different people are asking questions. Can you figure out if two different people ask essentially the same question but with different wording? With the skills you develop in this course, you’ll be able to build powerful NLP systems to solve problems across a wide range of industries. It is my pleasure to welcome Lukasz and Younes as your instructors for this course and they’re thrilled to dive into these topics with you. Younes, perhaps you could say a bit more about the applications that learners will build in this course.\n\n\n\nSure thing. Thanks, Andrew. Well, I’d like to start by saying that in the first two courses of the specialization, you build a powerful foundation that will provide you with both the context and the fundamental skills you need to tackle this course. For example, you’ve already done sentiment analysis in course one with a simple naive Bayes classifier. But now, you will leverage the power of deep neural networks to build a much more robust sentiment analysis classifier. You’ve also seen how to do things like predict the next word in a sequence, using relatively simple n-gram language models in course two. In this course, you’ll create an advanced model using recurrent neural networks to generate text. You can think of this course as taking the step from foundational skills into building real world NLP applications.\n\n\n\n\nVery cool, Younes. I think that’s a great way to think about it. Lukasz, maybe you could say a bit more about the applications learners will get to build.\n\n\n\n\nSure, thank you, Andrew. Well, as you saw in the first course, the problem of sentiment analysis is a really tricky one. But in many applications you want to determine the sentiment of a sentence. So it’s a really good problem to work on as well. With language modeling, which you’ll tackle in week two of this course, the problems you can solve are almost infinite. From translation to autocomplete to generating text from scratch. In week three of this course, you’ll work on named entity recognition, which is the problem of separating named entities in sentences, like people and places. This is a building block of many important NLP systems. Finally, in week four you will tackle the problem of identifying duplicates. The question whether two pieces of texts are duplicates of each other might not sound very interesting at first, but as you will see, it’s actually core building block of things like online forums and search engines and we’ll show you how to solve it. So we’re really excited to show you these applications and bring your skills to the next level.\n\n\n\n\nThank you Lukasz and thank you Younes. This is going to be an exciting course, so let’s get started.\n\n\n\n\nGood luck.\n\n\n\n\nHave fun.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w1/index.html#neural-networks-for-sentiment-analysis",
    "href": "notes/c3w1/index.html#neural-networks-for-sentiment-analysis",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "Neural Networks for Sentiment Analysis",
    "text": "Neural Networks for Sentiment Analysis\nPreviously in the course we did sentiment analysis with logistic regression and naive Bayes. Those models were in a sense more naive, and are not able to catch the sentiment off a tweet like: “I am not happy” or “If only it was a good day”. When using a neural network to predict the sentiment of a sentence, we can use the following. Note that the image below has three outputs, in this case we might want to predict, “positive”, “neutral”, or “negative”.\n\nNote that the network above has three layers. To go from one layer to another we can use a W matrix to propagate to the next layer. Hence, we call this concept of going from the input until the final layer, forward propagation. To represent a tweet, we can use the following:\nNote, that we add zeros for padding to match the size of the longest tweet.\n\n\n\n\n\n\n\nVideo Transcript\n\n\n\n\n\n\nThis week I’ll show you how to create neural networks using layers. This simplifies the task a lot as you will see. Let’s dive in. First, you’ll revisit the general structure of neural networks and how they make predictions. I’ll show you the structure you’ll be using to perform sentiment analysis during this week. Neural networks are computational structures that, in a very simplistic way, attempt to mimic the way the human brain recognizes patterns. They’re used in many applications of artificial intelligence and have proven very effective on a variety of tasks, including those in NLP. Have a look at this example of a simple neural network with n input parameters, two hidden layers, and three output units. As inputs, this neural network receives a data representation x with n features, then performs computations in its hidden layers. Finally, it delivers an output which in this case has size 3. Let’s take a look at how it works mathematically. All the nodes every activation layer as a_i, where i is the layer’s number. First, define a_0 to be the input vector x. To get the values for each layer’s activation, a, you have to compute the value for z_i, which depends on both the weights matrix for that layer and the activations, a, from the previous layer. Finally, you get the values for each layer by applying an activation function, g, to the value of z. As you can see, this computation moves forward through the left of the neural network towards the right. That’s why this process is called forward propagation. For this module’s assignments, you’re going to implement a neural network that looks like this. As inputs, it will receive a simple vector representation of your tweets. It will have an embedding layer that will transform your representation into an optimal one for this task. Finally, it will have a hidden layer with a ReLU activation function and then output layer with the softmax function that will give you the probabilities for whether a tweet has a positive or negative sentiment. This neural network will allow you to predict sentiments for complex tweets, such as a tweet like this one that says, “This movie was almost good.” That you wouldn’t have been able to classify correctly using simpler methods such as Naive Bayes because they missed important information. The initial representation, x, that you’ll use for this neural network will be a vector of integers. Similar to your previous work with sentiment analysis, you will first need to list all of your words from your vocabulary. Next for this application, you’ll assign an integer index to each of them. Then for each word in your tweets add the index from your vocabulary to construct a vector like this one for every tweet. After you have all the vector representations of your tweets, you will need to identify the maximum vector size and fill every vector with zeros to match that size. This process is called padding and ensures that all of your vectors have the same size even if your tweets don’t. Let’s do a quick recap. At this point, you’re familiar with the general structure of the neural network that you’ll be using to classify sentiments for a set of complex nuance tweets. You also reviewed the integer representation that’s going to be used in this module. Next, I’ll introduce the tracks library for neural networks and demonstrate how the embedding layer works. I’ll see you later.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w1/index.html#sec-trax-nn",
    "href": "notes/c3w1/index.html#sec-trax-nn",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "Trax: Neural Networks",
    "text": "Trax: Neural Networks\nTrax has several advantages:\n\nRuns fast on CPUs, GPUs and TPUs\nParallel computing\nRecord algebraic computations for gradient evaluation\n\nHere is an example of how we can code a neural network in Trax:\n\n\n\n\n\n\n\nVideo Transcript",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w1/index.html#sec-trax-jax",
    "href": "notes/c3w1/index.html#sec-trax-jax",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "Trax and JAX, docs and code",
    "text": "Trax and JAX, docs and code\nOfficial Trax documentation maintained by the Google Brain team:\n\nhttps://trax-ml.readthedocs.io/en/latest/\n\nTrax source code on GitHub:\n\nhttps://github.com/google/trax\n\nJAX library:\n\nhttps://jax.readthedocs.io/en/latest/index.html",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w1/index.html#sec-intro-trax",
    "href": "notes/c3w1/index.html#sec-intro-trax",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "Lab: Introduction to Trax",
    "text": "Lab: Introduction to Trax\nIntroduction to Trax",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w1/index.html#sec-trax-layers",
    "href": "notes/c3w1/index.html#sec-trax-layers",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "Trax: Layers",
    "text": "Trax: Layers\nTrax makes use of classes. If we are not familiar with classes in python, don’t worry about it, here is an example.\n\nIn the example above, we can see that a class takes in an __init__ and a __call__ method.\nThese methods allow we to initialize your internal variables and allow we to execute your function when called.\nTo the right we can see how we can initialize your class. When we call MyClass(7) , we are setting the y variable to 7. Now when we call f(3) we are adding 7 + 3.\nWe can change the my_method function to do whatever we want, and we can have as many methods as we want in a class.\n\n\n\n\n\n\nVideo Transcript\n\n\n\n\n\n\nTo create a neural network out of layers, you need to put them together. In a deep network, you run them one after the other in a sequential way. Let me show you how you can do this. First, you will observe how a basic neural network is defined in Trax. Then I’ll show you some of the benefits of using a framework like TensorFlow, which is the framework that Trax is built on. Let’s take this network architecture as an example. In this model, you have two hidden layers with sigmoid activation functions and an output layer with softmax activation. In Trax, you’ll need to specify the type of model architecture.\nFor simple architectures like this one, you’ll use a serial model. To start, list the layers from left to right, or from your input variables to the output layer. In this case, first you have a dense layer with four units, and then assign the sigmoid activation function to that layer. After that, repeat the process for the second hidden layer and the output layer. You can specify any architecture you like in the simple way. Note that, this way to specify your models architecture, follows the order in which the computations are made in your neural network. There are several advantages to using libraries like Trax, such as they’re designed to perform computations efficiently in hardware like CPUs, GPUs, and even TPUs. They allow you to easily perform parallel computing by running gear models on multiple machines or course simultaneously. They keep a record of all the algebraic operations on your neural net in the order of computation. So they are able to compute the gradients of your model automatically. There are many open source frameworks out there, and Trax is one of the latest.\nIt’s based on TensorFlow. You might be already familiar with TensorFlow, PyTorch, and JAX. If you’re not familiar with those, don’t worry. I’ll show you the basics of Trax and you’ll be able to implement amazing NLP models. So far, I showed you how to define a model in Trax with the simple sequential architecture, and I pointed out some of the advantages to be had, like computational efficiency and parallel computing.\n\n\nNext, I’ll get into more detail on how to use Trax.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w1/index.html#sec-why-trax",
    "href": "notes/c3w1/index.html#sec-why-trax",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "Why we recommend Trax",
    "text": "Why we recommend Trax\n\n\n\n\n\n\nVideo Transcript\n\n\n\n\n\n\nHi. My name is Lukasz, and I want to tell you in this video why we made the machine learning library Trax. This is a little bit of a personal story for me. I’ve been at Google for about seven years now. I’m a researcher in the Google Brain Team. But before I was a researcher, I was a software engineer. I worked on a lot of machine learning projects and frameworks. This journey for me ended in the Trax library. I believe Trax is currently the best library to learn and to productionize machine learning research and machine learning models, especially sequenced models, models like transformer and models that are used in natural language processing. The reasons I believe that come from a personal journey that I took that led me here. I will tell you a little bit about myself and how I came here, and then I’ll tell you why I think Trax is the best thing to use currently for machine learning, especially in natural language processing. My journey with machine learning and machine learning frameworks started around 2014-15 when we were making TensorFlow. TensorFlow is, you probably know, is a big machine learning systems, it has about 100 million downloads by now. It was released in November 2015. It was a very emotional moment for all of us when we were releasing it. At that point, we were not sure if deep learning will become as big as it did. We were not sure how many users there will be. What we wanted to do was a system that’s primarily very fast, that can run distributed machine learning systems, large-scale fast training. The main focus was speed. A secondary focus was to make it easy to program the systems that wasn’t a reader, but it was not the most important thing. After releasing TensorFlow, I worked on machine translation and especially on the Google’s Neural Machine Translation System. This was the first system using deep sequence models that was used by the Google Translate team that was actually released as a product. It’s handling all of Google translations these days. Every language that we have has a neural model. It started with LSTMs and RNN models, and now it’s a lot of transformers. We released that in 2016 based on the TensorFlow framework. These models, they’re amazing. They’re much better than the previous phrase-based translation models, but they took a long time to train. They were training for days on clusters of GPUs at that time. This was not practical for anyone else to do rather than Google. This was only because we had this TensorFlow system, a large group of engineers who would ferry very well, and we were training for days and days. That was great. But I felt like this is not satisfactory because no one else can do that. It’s not possible to be done at the university. You cannot launch a startup doing that, because it was impossible if you were not Google, or maybe from Microsoft, but no one else. I wanted to change that. To do that, we created the Tensor2Tensor Library. The Tensor2Tensor Library, which was released in 2017, started with the thought that we should make this deep learning research, especially for sequence models, widely accessible. This was not working with these large RNN models, but while writing the library, we created this transformer model. This transformer has taken NLP by storm because it allows you to train much faster. At that time within a few days, now, it’s less than a day in a matter of hours on an 8 GPU system. You can create translation models that surpass any RNN models. The Tensor2Tensor library has become already widely used. It’s used in production Google systems. It’s used by some very large companies in the world and it has led to a number of startups that they know about that basically exists thanks to this library. You can say, well, this is done and this is good, but, the problem is, it’s become complicated and it’s not nice to learn and it’s become very hard to do new researcher. Around 2018, we decided it’s time to improve. As time moves on, we need to do even better, and this is how we created Trax. Trax is a deep-learning library that’s focused on clear code and speed. Let me tell you why, so, if you think carefully what you want from a deep-learning library, there are really two things that matters. You want the the programmers to be efficient and you want the code to run fast, and this is because what costs you is the time of the programmer, and the money you need to pay for running your training code. Programmer’s time is very important. You need to use it efficiently, but in deep learning you’re training big models and these costs money too. For example, using eight GPUs on-demand from the Cloud, can cost $20 an hour almost. But using the preemptible eight could TPU costs only $1.40. In Trax, you can use one or the other without changing a single character in your code. How does Trax make programmers sufficient? Well, it was redesigned from the bottom-up to be easy to debug and understand. You can literally read Trax code and understand what’s going to come. This is not the case in some other libraries, this is unluckily of the case anymore in TensorFlow. But, you can say, well it used to be the case, but nowadays TensorFlow, even when we clean up the code, it needs to be backwards compatible. It carries the weight of these years of development, and this is crazy errors of Machine Learning. There is a lot of baggage that it just has to carry because it’s backward compatible. What we do in Trax is we break the backwards compatibility. This means you need to learn new things. This carries some price. But what you get for that price, is that it’s a newly cleanly designed library which has four models, not just primitives to build them, but also four models with dataset bindings, we regression test these models daily because we use these libraries, so we know every day these monster running. It’s like a new programming language. It costs a little bit to learn, this is a new thing, but it makes your life much more efficient. To make this point point clear, the Adam Optimizer, the most popular optimizer in machine learning timesteps. On the left, you see a screenshot from the paper that introduced data, and you see it has like about seven lines. Next is just a part of the Adam implementation and patronage, which is one of the cleanest ones actually and you need to know way more, you need to know what are parameter groups, you need to know secret keys into these groups that key parameters by some means, you need to do seven stick initialization and some conditional to introduce either and other things. On the right, you see the Adam optimizer in TensorFlow and Keras and as you’ll see it’s even longer. You need to apply it to resource variables and two non-research variables and you need to know what these are. The reason they exist is historical. Currently we only use resource variables, but we have to support people who used the old non-research variables too. There are a lot of things that in 2020 you actually don’t need anymore, but they have to be there and painted and in TensorFlow code. While if you go to Trax code, this is the full code of Adam and Trax. It’s very similar for the paper. That’s the whole point. Because if you’re implementing a new paper or if you’re learning and you want to find, in the code of the framework, where are the equations from the paper, you can really do with this here. So that is the benefit of Trax. The price of this benefit is that you’re using a new thing. But there is a huge gain that comes to you when you’re actually debugging your code. When you’re debugging your code, you will hit lines that are in the framework. So you will actually need to understand these lines, which means you need to understand all of these PyTorch and all of these TensorFlow if you use those. But in Trax, you only need to understand these Trax lines. It’s much easier to debug, which makes programmers more efficient. Now this efficiency would not be worth that much if the code is running slow. Hey, there’s a lot of beautiful things where you can program things in a few line, but the run so slowly that it’s actually useless. Not so in Trax because we use the just-in-time compiler technology that was built in the last six years of TensorFlow. It’s called XLA, and we use it on top of Trax. These teams have put tremendous effort to make this coat the fastest code on the planet. There is an industry competition called MLPerf. In 2020, JAX actually won this competition, being the fastest transformer to ever be benchmarked independently. So JAX transformer ran in 0.26 of a minute, so in about 16 seconds, I think, while the fastest TensorFlow transformer on the same hardware took 0.35 minutes. So you see, it’s almost 50 percent slower. The fastest PyTorch, but this was not on TPU, took 0.62. So being two times faster is significant game. It’s not clear you’ll get the same gain in any model on other hardware. There was a lot of work to tune it for this particular model hardware. But in general, Trax runs fast. This means, you’ll pay less for the TPUs and GPUs you’ll be running on Cloud. It’s also tested with TPUs on Colab. Colabs are the IPython notebooks that Google gives you for free. You can select a hardware accelerator, you can select TPU and run the same code with no changes. It’s GPU, TPU, or CPU, on this Colab, where you’re getting an eight-code TPU for free. So you can test your code there and then run it on Cloud for much cheaper than other frameworks, and it really runs fast. So these are the reasons to use Trax, and for me, Trax is also super fun. It’s super fun to learn, it’s super fun to use, because we had the liberty to do things from scratch using many years of experience now. You can write model using combinators. This is a whole transformer language model on the left. On the right, you can see it’s from a README. This is everything you need to run a pre-trained model and get your translations. So this gave us the opportunity to clean up the framework, clean up the code, make sure it runs really fast. It’s a lot of fun to use. So I encourage you come check it out. See how you can use Trax for your own machine learning endeavors, both for research. If you want to start a startup or if you want to run it for a big company, I think Trax will be there for you.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w1/index.html#lab-classes-and-subclasses",
    "href": "notes/c3w1/index.html#lab-classes-and-subclasses",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "Lab: Classes and Subclasses",
    "text": "Lab: Classes and Subclasses\nClasses and Subclasses",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w1/index.html#dense-and-relu-layer",
    "href": "notes/c3w1/index.html#dense-and-relu-layer",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "Dense and ReLU layer",
    "text": "Dense and ReLU layer\nThe Dense layer is the computation of the inner product between a set of trainable weights (weight matrix) and an input vector. The visualization of the dense layer could be seen in the image below.\n\nThe orange box shows the dense layer. An activation layer is the set of blue nodes. Concretely one of the most commonly used activation layers is the rectified linear unit (ReLU).\n\nReLU(x) is defined as max(0,x) for any input x.\n\n\n\n\n\n\nVideo Transcript\n\n\n\n\n\n\nNow, I’ll show you dense and ReLU layers. First, I’ll show you dense layers and then ReLU layers. Suppose that you have a simple serial model like this one. Let’s focus on the first parts of the model. Here, you have an input vector X fully connected to a layer of activations. To get the value of z, let’s go into the activations. You will have to compute the inner products between a set of trainable weights and the input vector. This single computation is called a dense layer. The ReLU layer is much simpler. Let’s take the same model you’ve been working with.\nThe ReLU layer is an activation layer that typically follows a dense fully connected layer, and transforms any negative values to zero before sending them onto the next layer. To do this, the ReLU layer computes the function g, which returns a value of zero for all negative values of z, and z for all positive ones. You’ve now seen the dense layer and the ReLU layer. Next, I’ll show you how to put a model together.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w1/index.html#serial-layer",
    "href": "notes/c3w1/index.html#serial-layer",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "Serial Layer",
    "text": "Serial Layer\nA serial layer allows we to compose layers in a serial arrangement:\n\nIt is a composition of sublayers. These layers are usually dense layers followed by activation layers.\n\n\n\n\n\n\nVideo Transcript\n\n\n\n\n\n\nTo create a neural network out of layers, you need to put them together. In a deep network, you run them one after the other in a sequential way. Let me show you how you can do this. Previously, you saw how to define layers, and I showed you how the dense and ReLU layers performed single steps of forward propagation. Now, I’ll show you how to define a serial neural network as a composition of layers thats operates in a sequence. Imagine, a basic neural network like this one. You have some dense layers and activation layers, and the sequential arrangements of those layers is done in tracts, when you define a serial layer. You could think of this new serial layer as your whole neural network model in one layer. Let’s summarize what you just learned. A serial layer is a composition of sublayers that operates sequentially to perform the forward computation of your entire model. Coming up, I’ll introduce some additional layers and the training procedure.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w1/index.html#other-layers",
    "href": "notes/c3w1/index.html#other-layers",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "Other Layers",
    "text": "Other Layers\nOther layers could include embedding layers and mean layers. For example, we can learn word embeddings for each word in your vocabulary as follows:\n\nThe mean layer allows we to take the average of the embeddings. We can visualize it as follows:\n\nThis layer does not have any trainable parameters.\n\n\n\n\n\n\nVideo Transcript\n\n\n\n\n\n\nEarlier, you worked with a few different ways to represent text data. When using neural networks for NLP tasks, embedding layers are often included in the model. Going forward, I will introduce embedding layers, and explain why you will need to include layers such as the mean layer in serial models directly after your embedding layer. Let’s now dive into the embedding layer. In NLP, you usually have a set of unique words called the vocabulary. An embedding layer takes an index assigned to each word from your vocabulary and maps it to a representation of that word with a determined dimension. In this example, embedding of size equal to two. For instance, the embedding layer in this example, we’ll return a vector equal to 0.020, and 0.006 for the word I, and negative 0.009 and 0.050 for the word NLP. Every value from this part of the table is trainable in tracks. When using an embedding layer in your model, you will learn the representation that gives the best performance for your NLP task. For the embedding layer in your model, you’d have to learn a matrix of weights, of size equal to your vocabulary times the dimension of the embedding. The size of the embedding could be treated as a hyperparameter in your model. With this layer, your model can learn or improve the word embeddings for your NLP task like it improves the weights matrices on each layer.\nThe embedding layer is able to map words to embeddings. If you had a series of words, like a tweet that says, “I am happy,” the embedding layer will map each of those words to their corresponding embedding, and return a matrix of word embeddings. If you had padded vectors representing your tweets, you could unroll this matrix and feed its values to the next layer on the neural network. But in doing this, you might end up with lots of parameters to train. As an alternative, you could just take the mean of each feature from the embedding, and that’s exactly what the mean layer does in tracks. After the mean layer, you will have the same number of features as you’re embedding size. Even for sequences of texts, those are very long. Note that this layer doesn’t have any trainable parameters because it’s only computing the mean of the word embeddings.\nAt this point, you are familiar with what’s embedding layers, and mean layers are, and how they work. An important takeaway here is that using an embedding layer in your model allows you to learn a good representation of your vocabulary for the specific task you’re working on, and that’s the mean layer takes a matrix of embeddings, and returns a vector representation for a set of words, like tweets.\n\n\n\n\n\nTraining\nIn Trax, the function grad allows we to compute the gradient. We can use it as follows:\n\nNow if we were to evaluate grad_f at a certain value, namely z, it would be the same as computing 6z+1. Now to do the training, it becomes very simple:\n\nWe simply compute the gradients by feeding in y.forward (the latest value of y), the weights, and the input x, and then it does the back-propagation for we in a single line. We can then have the loop that allows we to update the weights (i.e. gradient descent!).\n\n\n\n\n\n\nVideo Transcript\n\n\n\n\n\n\nTo train a neural network, you need to compute gradients. You’ve done it by hand earlier in this specialization, and you’ve seen that it can be quite complex. But you know what? You don’t have to necessarily do it by yourself. Deep learning frameworks can do it for you. Let’s dive in. I will show you how the trax grad function allows you to easily compute gradients, which will allow you to perform back-propagation and train your model. You will see how easy it is compared to back propagating by hand. Computing ingredients using Trax is pretty straightforward. Suppose that you have the following equation, f of x, whose gradient with respect to x is equal to this derivative. To get that derivative in Trax, define the function f that takes in x, and then just call the grad function with f as its single parameter. The function grad will return a function that computes the gradient of f. That’s nice.\nUsing the grad function to train a model is similarly painless. Suppose that you have a neural network model y. To get the gradient of your model, just apply the grad function with the forward method of your model as a single parameter. Then evaluate the gradient with your weights and inputs. Notice the double sets of parentheses.\nThe first one passes the arguments for the grad function, and the second one, the arguments for the function returned by grad. After you have the gradients for your model, just iterate until convergence is reached.\nThat’s it, forward and back-propagation performed in a single line. This final block is gradient descent. You can always change the optimization algorithm if necessary. So let’s summarize. Having a defined model in Trax, make training significantly easier than computing forward and back-propagation by hand because the built-in grad function automatically computes everything you need.\n\n\nIn the programming assignments from this module, you’ll be able to define and train a neural network using Trax.\nGood luck, and have fun. This was the last video of the week. You now know how to create neural networks with layers and how to train them.\nNext, you will learn even more complex layers and built networks that perform better.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w1/index.html#lab-data-generators",
    "href": "notes/c3w1/index.html#lab-data-generators",
    "title": "Neural Networks for Sentiment Analysis",
    "section": "Lab: Data Generators",
    "text": "Lab: Data Generators\nData Generators",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w1/lab01.html",
    "href": "notes/c3w1/lab01.html",
    "title": "Trax : Ungraded Lecture Notebook",
    "section": "",
    "text": "course banner\nIn this notebook you’ll get to know about the Trax framework and learn about some of its basic building blocks.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "L1 - Introduction to Trax"
    ]
  },
  {
    "objectID": "notes/c3w1/lab01.html#background",
    "href": "notes/c3w1/lab01.html#background",
    "title": "Trax : Ungraded Lecture Notebook",
    "section": "Background",
    "text": "Background\n\nWhy Trax and not TensorFlow or PyTorch?\nTensorFlow and PyTorch are both extensive frameworks that can do almost anything in deep learning. They offer a lot of flexibility, but that often means verbosity of syntax and extra time to code.\nTrax is much more concise. It runs on a TensorFlow backend but allows you to train models with 1 line commands. Trax also runs end to end, allowing you to get data, model and train all with a single terse statements. This means you can focus on learning, instead of spending hours on the idiosyncrasies of big framework implementation.\n\n\nWhy not Keras then?\nKeras is now part of Tensorflow itself from 2.0 onwards. Also, Trax is good for implementing new state of the art algorithms like Transformers, Reformers, BERT because it is actively maintained by Google Brain Team for advanced deep learning tasks. It runs smoothly on CPUs, GPUs and TPUs as well with comparatively lesser modifications in code.\n\n\nHow to Code in Trax\nBuilding models in Traxrelies on 2 key concepts:\n\nlayers and\ncombinators.\n\nTrax layers are simple objects that process data and perform computations. They can be chained together into composite layers using Trax combinators, allowing you to build layers and models of any complexity.\n\n\nTrax, JAX, TensorFlow and Tensor2Tensor\nYou already know that Trax uses Tensorflow as a backend, but it also uses the JAX library to speed up computation too. You can view JAX as an enhanced and optimized version of numpy.\nWatch out for assignments which import import trax.fastmath.numpy as np. If you see this line, remember that when calling np you are really calling Trax’s version of numpy that is compatible with JAX.\nAs a result of this, where you used to encounter the type numpy.ndarray now you will find the type jax.interpreters.xla.DeviceArray.\nTensor2Tensor is another name you might have heard. It started as an end to end solution much like how Trax is designed, but it grew unwieldy and complicated. So you can view Trax as the new improved version that operates much faster and simpler.\n\n\nResources\n\nTrax source code can be found on Github: Trax\nJAX library: JAX",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "L1 - Introduction to Trax"
    ]
  },
  {
    "objectID": "notes/c3w1/lab01.html#installing-trax",
    "href": "notes/c3w1/lab01.html#installing-trax",
    "title": "Trax : Ungraded Lecture Notebook",
    "section": "Installing Trax",
    "text": "Installing Trax\nTrax has dependencies on JAX and some libraries like JAX which are yet to be supported in Windows but work well in Ubuntu and MacOS. We would suggest that if you are working on Windows, try to install Trax on WSL2.\nOfficial maintained documentation - trax-ml not to be confused with this TraX\n\n#!pip install trax==1.3.1 Use this version for this notebook",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "L1 - Introduction to Trax"
    ]
  },
  {
    "objectID": "notes/c3w1/lab01.html#imports",
    "href": "notes/c3w1/lab01.html#imports",
    "title": "Trax : Ungraded Lecture Notebook",
    "section": "Imports",
    "text": "Imports\n\nimport numpy as np  # regular ol' numpy\n\nfrom trax import layers as tl  # core building block\nfrom trax import shapes  # data signatures: dimensionality and type\nfrom trax import fastmath  # uses jax, offers numpy on steroids\n\n2025-02-10 16:53:07.573380: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1739199187.587843  120958 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1739199187.592157  120958 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\n\n\n# Trax version 1.3.1 or better \n!pip list | grep trax\n\ntrax                         1.4.1",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "L1 - Introduction to Trax"
    ]
  },
  {
    "objectID": "notes/c3w1/lab01.html#layers",
    "href": "notes/c3w1/lab01.html#layers",
    "title": "Trax : Ungraded Lecture Notebook",
    "section": "Layers",
    "text": "Layers\nLayers are the core building blocks in Trax or as mentioned in the lectures, they are the base classes.\nThey take inputs, compute functions/custom calculations and return outputs.\nYou can also inspect layer properties. Let me show you some examples.\n\nRelu Layer\nFirst I’ll show you how to build a relu activation function as a layer. A layer like this is one of the simplest types. Notice there is no object initialization so it works just like a math function.\nNote: Activation functions are also layers in Trax, which might look odd if you have been using other frameworks for a longer time.\n\n# Layers\n# Create a relu trax layer\nrelu = tl.Relu()\n\n# Inspect properties\nprint(\"-- Properties --\")\nprint(\"name :\", relu.name)\nprint(\"expected inputs :\", relu.n_in)\nprint(\"promised outputs :\", relu.n_out, \"\\n\")\n\n# Inputs\nx = np.array([-2, -1, 0, 1, 2])\nprint(\"-- Inputs --\")\nprint(\"x :\", x, \"\\n\")\n\n# Outputs\ny = relu(x)\nprint(\"-- Outputs --\")\nprint(\"y :\", y)\n\n-- Properties --\nname : Serial\nexpected inputs : 1\npromised outputs : 1 \n\n-- Inputs --\nx : [-2 -1  0  1  2] \n\n-- Outputs --\ny : [0 0 0 1 2]\n\n\n\n\nConcatenate Layer\nNow I’ll show you how to build a layer that takes 2 inputs. Notice the change in the expected inputs property from 1 to 2.\n\n# Create a concatenate trax layer\nconcat = tl.Concatenate()\nprint(\"-- Properties --\")\nprint(\"name :\", concat.name)\nprint(\"expected inputs :\", concat.n_in)\nprint(\"promised outputs :\", concat.n_out, \"\\n\")\n\n# Inputs\nx1 = np.array([-10, -20, -30])\nx2 = x1 / -10\nprint(\"-- Inputs --\")\nprint(\"x1 :\", x1)\nprint(\"x2 :\", x2, \"\\n\")\n\n# Outputs\ny = concat([x1, x2])\nprint(\"-- Outputs --\")\nprint(\"y :\", y)\n\n-- Properties --\nname : Concatenate\nexpected inputs : 2\npromised outputs : 1 \n\n-- Inputs --\nx1 : [-10 -20 -30]\nx2 : [1. 2. 3.] \n\n-- Outputs --\ny : [-10. -20. -30.   1.   2.   3.]",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "L1 - Introduction to Trax"
    ]
  },
  {
    "objectID": "notes/c3w1/lab01.html#layers-are-configurable",
    "href": "notes/c3w1/lab01.html#layers-are-configurable",
    "title": "Trax : Ungraded Lecture Notebook",
    "section": "Layers are Configurable",
    "text": "Layers are Configurable\nYou can change the default settings of layers. For example, you can change the expected inputs for a concatenate layer from 2 to 3 using the optional parameter n_items.\n\n# Configure a concatenate layer\nconcat_3 = tl.Concatenate(n_items=3)  # configure the layer's expected inputs\nprint(\"-- Properties --\")\nprint(\"name :\", concat_3.name)\nprint(\"expected inputs :\", concat_3.n_in)\nprint(\"promised outputs :\", concat_3.n_out, \"\\n\")\n\n# Inputs\nx1 = np.array([-10, -20, -30])\nx2 = x1 / -10\nx3 = x2 * 0.99\nprint(\"-- Inputs --\")\nprint(\"x1 :\", x1)\nprint(\"x2 :\", x2)\nprint(\"x3 :\", x3, \"\\n\")\n\n# Outputs\ny = concat_3([x1, x2, x3])\nprint(\"-- Outputs --\")\nprint(\"y :\", y)\n\n-- Properties --\nname : Concatenate\nexpected inputs : 3\npromised outputs : 1 \n\n-- Inputs --\nx1 : [-10 -20 -30]\nx2 : [1. 2. 3.]\nx3 : [0.99 1.98 2.97] \n\n-- Outputs --\ny : [-10.   -20.   -30.     1.     2.     3.     0.99   1.98   2.97]\n\n\nNote: At any point,if you want to refer the function help/ look up the documentation or use help function.\n\n#help(tl.Concatenate) #Uncomment this to see the function docstring with explaination",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "L1 - Introduction to Trax"
    ]
  },
  {
    "objectID": "notes/c3w1/lab01.html#layers-can-have-weights",
    "href": "notes/c3w1/lab01.html#layers-can-have-weights",
    "title": "Trax : Ungraded Lecture Notebook",
    "section": "Layers can have Weights",
    "text": "Layers can have Weights\nSome layer types include mutable weights and biases that are used in computation and training. Layers of this type require initialization before use.\nFor example the LayerNorm layer calculates normalized data, that is also scaled by weights and biases. During initialization you pass the data shape and data type of the inputs, so the layer can initialize compatible arrays of weights and biases.\n\n# Uncomment any of them to see information regarding the function\n# help(tl.LayerNorm)\n# help(shapes.signature)\n\n\n# Layer initialization\nnorm = tl.LayerNorm()\n# You first must know what the input data will look like\nx = np.array([0, 1, 2, 3], dtype=\"float\")\n\n# Use the input data signature to get shape and type for initializing weights and biases\nnorm.init(shapes.signature(x)) # We need to convert the input datatype from usual tuple to trax ShapeDtype\n\nprint(\"Normal shape:\",x.shape, \"Data Type:\",type(x.shape))\nprint(\"Shapes Trax:\",shapes.signature(x),\"Data Type:\",type(shapes.signature(x)))\n\n# Inspect properties\nprint(\"-- Properties --\")\nprint(\"name :\", norm.name)\nprint(\"expected inputs :\", norm.n_in)\nprint(\"promised outputs :\", norm.n_out)\n# Weights and biases\nprint(\"weights :\", norm.weights[0])\nprint(\"biases :\", norm.weights[1], \"\\n\")\n\n# Inputs\nprint(\"-- Inputs --\")\nprint(\"x :\", x)\n\n# Outputs\ny = norm(x)\nprint(\"-- Outputs --\")\nprint(\"y :\", y)\n\n/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/normalization.py:141: UserWarning: Explicitly requested dtype float64 requested in ones is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n  scale = jnp.ones(features, dtype=input_signature.dtype)\n/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/normalization.py:142: UserWarning: Explicitly requested dtype float64 requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n  bias = jnp.zeros(features, dtype=input_signature.dtype)\n\n\n((Array([1., 1., 1., 1.], dtype=float32),\n  Array([0., 0., 0., 0.], dtype=float32)),\n ())\n\n\nNormal shape: (4,) Data Type: &lt;class 'tuple'&gt;\nShapes Trax: ShapeDtype{shape:(4,), dtype:float64} Data Type: &lt;class 'trax.shapes.ShapeDtype'&gt;\n-- Properties --\nname : LayerNorm\nexpected inputs : 1\npromised outputs : 1\nweights : [1. 1. 1. 1.]\nbiases : [0. 0. 0. 0.] \n\n-- Inputs --\nx : [0. 1. 2. 3.]\n-- Outputs --\ny : [-1.3416404  -0.44721344  0.44721344  1.3416404 ]",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "L1 - Introduction to Trax"
    ]
  },
  {
    "objectID": "notes/c3w1/lab01.html#custom-layers",
    "href": "notes/c3w1/lab01.html#custom-layers",
    "title": "Trax : Ungraded Lecture Notebook",
    "section": "Custom Layers",
    "text": "Custom Layers\nThis is where things start getting more interesting! You can create your own custom layers too and define custom functions for computations by using tl.Fn. Let me show you how.\n\nhelp(tl.Fn)\n\nHelp on function Fn in module trax.layers.base:\n\nFn(name, f, n_out=1)\n    Returns a layer with no weights that applies the function `f`.\n    \n    `f` can take and return any number of arguments, and takes only positional\n    arguments -- no default or keyword arguments. It often uses JAX-numpy (`jnp`).\n    The following, for example, would create a layer that takes two inputs and\n    returns two outputs -- element-wise sums and maxima:\n    \n        `Fn('SumAndMax', lambda x0, x1: (x0 + x1, jnp.maximum(x0, x1)), n_out=2)`\n    \n    The layer's number of inputs (`n_in`) is automatically set to number of\n    positional arguments in `f`, but you must explicitly set the number of\n    outputs (`n_out`) whenever it's not the default value 1.\n    \n    Args:\n      name: Class-like name for the resulting layer; for use in debugging.\n      f: Pure function from input tensors to output tensors, where each input\n          tensor is a separate positional arg, e.g., `f(x0, x1) --&gt; x0 + x1`.\n          Output tensors must be packaged as specified in the `Layer` class\n          docstring.\n      n_out: Number of outputs promised by the layer; default value 1.\n    \n    Returns:\n      Layer executing the function `f`.\n\n\n\n\n# Define a custom layer\n# In this example you will create a layer to calculate the input times 2\n\ndef TimesTwo():\n    layer_name = \"TimesTwo\" #don't forget to give your custom layer a name to identify\n\n    # Custom function for the custom layer\n    def func(x):\n        return x * 2\n\n    return tl.Fn(layer_name, func)\n\n\n# Test it\ntimes_two = TimesTwo()\n\n# Inspect properties\nprint(\"-- Properties --\")\nprint(\"name :\", times_two.name)\nprint(\"expected inputs :\", times_two.n_in)\nprint(\"promised outputs :\", times_two.n_out, \"\\n\")\n\n# Inputs\nx = np.array([1, 2, 3])\nprint(\"-- Inputs --\")\nprint(\"x :\", x, \"\\n\")\n\n# Outputs\ny = times_two(x)\nprint(\"-- Outputs --\")\nprint(\"y :\", y)\n\n-- Properties --\nname : TimesTwo\nexpected inputs : 1\npromised outputs : 1 \n\n-- Inputs --\nx : [1 2 3] \n\n-- Outputs --\ny : [2 4 6]",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "L1 - Introduction to Trax"
    ]
  },
  {
    "objectID": "notes/c3w1/lab01.html#combinators",
    "href": "notes/c3w1/lab01.html#combinators",
    "title": "Trax : Ungraded Lecture Notebook",
    "section": "Combinators",
    "text": "Combinators\nYou can combine layers to build more complex layers. Trax provides a set of objects named combinator layers to make this happen. Combinators are themselves layers, so behavior commutes.\n\nSerial Combinator\nThis is the most common and easiest to use. For example could build a simple neural network by combining layers into a single layer using the Serial combinator. This new layer then acts just like a single layer, so you can inspect intputs, outputs and weights. Or even combine it into another layer! Combinators can then be used as trainable models. Try adding more layers\nNote:As you must have guessed, if there is serial combinator, there must be a parallel combinator as well. Do try to explore about combinators and other layers from the trax documentation and look at the repo to understand how these layers are written.\n\n# help(tl.Serial)\n# help(tl.Parallel)\n\n\n# Serial combinator\nserial = tl.Serial(\n    tl.LayerNorm(),         # normalize input\n    tl.Relu(),              # convert negative values to zero\n    times_two,              # the custom layer you created above, multiplies the input recieved from above by 2\n    \n    ### START CODE HERE\n#     tl.Dense(n_units=2),  # try adding more layers. eg uncomment these lines\n#     tl.Dense(n_units=1),  # Binary classification, maybe? uncomment at your own peril\n#     tl.LogSoftmax()       # Yes, LogSoftmax is also a layer\n    ### END CODE HERE\n)\n\n# Initialization\nx = np.array([-2, -1, 0, 1, 2]) #input\nserial.init(shapes.signature(x)) #initialising serial instance\n\nprint(\"-- Serial Model --\")\nprint(serial,\"\\n\")\nprint(\"-- Properties --\")\nprint(\"name :\", serial.name)\nprint(\"sublayers :\", serial.sublayers)\nprint(\"expected inputs :\", serial.n_in)\nprint(\"promised outputs :\", serial.n_out)\nprint(\"weights & biases:\", serial.weights, \"\\n\")\n\n# Inputs\nprint(\"-- Inputs --\")\nprint(\"x :\", x, \"\\n\")\n\n# Outputs\ny = serial(x)\nprint(\"-- Outputs --\")\nprint(\"y :\", y)\n\n/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/normalization.py:141: UserWarning: Explicitly requested dtype int64 requested in ones is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n  scale = jnp.ones(features, dtype=input_signature.dtype)\n/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/normalization.py:142: UserWarning: Explicitly requested dtype int64 requested in zeros is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n  bias = jnp.zeros(features, dtype=input_signature.dtype)\n\n\n(((Array([1, 1, 1, 1, 1], dtype=int32), Array([0, 0, 0, 0, 0], dtype=int32)),\n  ((), (), ()),\n  ()),\n ((), ((), (), ()), ()))\n\n\n-- Serial Model --\nSerial[\n  LayerNorm\n  Serial[\n    Relu\n  ]\n  TimesTwo\n] \n\n-- Properties --\nname : Serial\nsublayers : [LayerNorm, Serial[\n  Relu\n], TimesTwo]\nexpected inputs : 1\npromised outputs : 1\nweights & biases: ((Array([1, 1, 1, 1, 1], dtype=int32), Array([0, 0, 0, 0, 0], dtype=int32)), ((), (), ()), ()) \n\n-- Inputs --\nx : [-2 -1  0  1  2] \n\n-- Outputs --\ny : [0.        0.        0.        1.4142132 2.8284264]",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "L1 - Introduction to Trax"
    ]
  },
  {
    "objectID": "notes/c3w1/lab01.html#jax",
    "href": "notes/c3w1/lab01.html#jax",
    "title": "Trax : Ungraded Lecture Notebook",
    "section": "JAX",
    "text": "JAX\nJust remember to lookout for which numpy you are using, the regular ol’ numpy or Trax’s JAX compatible numpy. Both tend to use the alias np so watch those import blocks.\nNote:There are certain things which are still not possible in fastmath.numpy which can be done in numpy so you will see in assignments we will switch between them to get our work done.\n\n# Numpy vs fastmath.numpy have different data types\n# Regular ol' numpy\nx_numpy = np.array([1, 2, 3])\nprint(\"good old numpy : \", type(x_numpy), \"\\n\")\n\n# Fastmath and jax numpy\nx_jax = fastmath.numpy.array([1, 2, 3])\nprint(\"jax trax numpy : \", type(x_jax))\n\ngood old numpy :  &lt;class 'numpy.ndarray'&gt; \n\njax trax numpy :  &lt;class 'jaxlib.xla_extension.ArrayImpl'&gt;",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "L1 - Introduction to Trax"
    ]
  },
  {
    "objectID": "notes/c3w1/lab01.html#summary",
    "href": "notes/c3w1/lab01.html#summary",
    "title": "Trax : Ungraded Lecture Notebook",
    "section": "Summary",
    "text": "Summary\nTrax is a concise framework, built on TensorFlow, for end to end machine learning. The key building blocks are layers and combinators. This notebook is just a taste, but sets you up with some key inuitions to take forward into the rest of the course and assignments where you will build end to end models.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "L1 - Introduction to Trax"
    ]
  },
  {
    "objectID": "notes/c1w4/lab02.html",
    "href": "notes/c1w4/lab02.html",
    "title": "Hash functions and multiplanes",
    "section": "",
    "text": "Figure 1: course banner\nIn this lab, we are going to practice the most important concepts related to the hash functions explained in the videos. We will be using these in this week’s assignment.\nA key point for the lookup using hash functions is the calculation of the hash key or bucket id that we assign for a given entry. In this notebook, we will cover:"
  },
  {
    "objectID": "notes/c1w4/lab02.html#basic-hash-tables",
    "href": "notes/c1w4/lab02.html#basic-hash-tables",
    "title": "Hash functions and multiplanes",
    "section": "Basic Hash tables",
    "text": "Basic Hash tables\nHash tables are data structures that allow indexing data to make lookup tasks more efficient. In this part, we will see the implementation of the simplest hash function.\n\nimport numpy as np                # library for array and matrix manipulation\nimport pprint                     # utilities for console printing \nfrom utils_nb import plot_vectors # helper function to plot vectors\nimport matplotlib.pyplot as plt   # visualization library\n\npp = pprint.PrettyPrinter(indent=4) # Instantiate a pretty printer\n\nIn the next cell, we will define a straightforward hash function for integer numbers. The function will receive a list of integer numbers and the desired amount of buckets. The function will produce a hash table stored as a dictionary, where keys contain the hash keys, and the values will provide the hashed elements of the input list.\nThe hash function is just the remainder of the integer division between each element and the desired number of buckets.\n\ndef basic_hash_table(value_l, n_buckets):\n    \n    def hash_function(value, n_buckets):\n        return int(value) % n_buckets\n    \n    hash_table = {i:[] for i in range(n_buckets)} # Initialize all the buckets in the hash table as empty lists\n\n    for value in value_l:\n        hash_value = hash_function(value,n_buckets) # Get the hash key for the given value\n        hash_table[hash_value].append(value) # Add the element to the corresponding bucket\n    \n    return hash_table\n\nNow let’s see the hash table function in action. The pretty print function (pprint()) will produce a visually appealing output.\n\nvalue_l = [100, 10, 14, 17, 97] # Set of values to hash\nhash_table_example = basic_hash_table(value_l, n_buckets=10)\npp.pprint(hash_table_example)\n\n{   0: [100, 10],\n    1: [],\n    2: [],\n    3: [],\n    4: [14],\n    5: [],\n    6: [],\n    7: [17, 97],\n    8: [],\n    9: []}\n\n\nIn this case, the bucket key must be the rightmost digit of each number."
  },
  {
    "objectID": "notes/c1w4/lab02.html#planes",
    "href": "notes/c1w4/lab02.html#planes",
    "title": "Hash functions and multiplanes",
    "section": "Planes",
    "text": "Planes\nMultiplanes hash functions are other types of hash functions. Multiplanes hash functions are based on the idea of numbering every single region that is formed by the intersection of n planes. In the following code, we show the most basic forms of the multiplanes principle. First, with a single plane:\n\nP = np.array([[1, 1]]) # Define a single plane. \nfig, ax1 = plt.subplots(figsize=(8, 8)) # Create a plot\n\nplot_vectors([P], axes=[2, 2], ax=ax1) # Plot the plane P as a vector\n\n# Plot  random points. \nfor i in range(0, 10):\n        v1 = np.array(np.random.uniform(-2, 2, 2)) # Get a pair of random numbers between -4 and 4 \n        side_of_plane = np.sign(np.dot(P, v1.T)) \n        \n        # Color the points depending on the sign of the result of np.dot(P, point.T)\n        if side_of_plane == 1:\n            ax1.plot([v1[0]], [v1[1]], 'bo') # Plot blue points\n        else:\n            ax1.plot([v1[0]], [v1[1]], 'ro') # Plot red points\n\nplt.show()\n\n\n\n\n\n\n\n\nThe first thing to note is that the vector that defines the plane does not mark the boundary between the two sides of the plane. It marks the direction in which we find the ‘positive’ side of the plane. Not intuitive at all!\nIf we want to plot the separation plane, we need to plot a line that is perpendicular to our vector P. We can get such a line using a 90^o rotation matrix.\nFeel free to change the direction of the plane P.\n\nP = np.array([[1, 2]])  # Define a single plane. We may change the direction\n\n# Get a new plane perpendicular to P. We use a rotation matrix\nPT = np.dot([[0, 1], [-1, 0]], P.T).T  \n\nfig, ax1 = plt.subplots(figsize=(8, 8)) # Create a plot with custom size\n\nplot_vectors([P], colors=['b'], axes=[2, 2], ax=ax1) # Plot the plane P as a vector\n\n# Plot the plane P as a 2 vectors. \n# We scale by 2 just to get the arrows outside the current box\nplot_vectors([PT * 4, PT * -4], colors=['k', 'k'], axes=[4, 4], ax=ax1)\n\n# Plot 20 random points. \nfor i in range(0, 20):\n        v1 = np.array(np.random.uniform(-4, 4, 2)) # Get a pair of random numbers between -4 and 4 \n        side_of_plane = np.sign(np.dot(P, v1.T)) # Get the sign of the dot product with P\n        # Color the points depending on the sign of the result of np.dot(P, point.T)\n        if side_of_plane == 1:\n            ax1.plot([v1[0]], [v1[1]], 'bo') # Plot a blue point\n        else:\n            ax1.plot([v1[0]], [v1[1]], 'ro') # Plot a red point\n\nplt.show()\n\n\n\n\n\n\n\n\nNow, let us see what is inside the code that color the points.\n\nP = np.array([[1, 1]])      # Single plane\nv1 = np.array([[1, 2]])     # Sample point 1\nv2 = np.array([[-1, 1]])    # Sample point 2\nv3 = np.array([[-2, -1]])   # Sample point 3\n\n\nnp.dot(P, v1.T)\n\narray([[3]])\n\n\n\nnp.dot(P, v2.T)\n\narray([[0]])\n\n\n\nnp.dot(P, v3.T)\n\narray([[-3]])\n\n\nThe function below checks in which side of the plane P is located the vector v\n\ndef side_of_plane(P, v):\n    dotproduct = np.dot(P, v.T) # Get the dot product P * v'\n    sign_of_dot_product = np.sign(dotproduct) # The sign of the elements of the dotproduct matrix \n    sign_of_dot_product_scalar = sign_of_dot_product.item() # The value of the first item\n    return sign_of_dot_product_scalar\n\n\nside_of_plane(P, v1) # In which side is [1, 2]\n\n1\n\n\n\nside_of_plane(P, v2) # In which side is [-1, 1]\n\n0\n\n\n\nside_of_plane(P, v3) # In which side is [-2, -1]\n\n-1"
  },
  {
    "objectID": "notes/c1w4/lab02.html#hash-function-with-multiple-planes",
    "href": "notes/c1w4/lab02.html#hash-function-with-multiple-planes",
    "title": "Hash functions and multiplanes",
    "section": "Hash Function with multiple planes",
    "text": "Hash Function with multiple planes\nIn the following section, we are going to define a hash function with a list of three custom planes in 2D.\n\nP1 = np.array([[1, 1]])   # First plane 2D\nP2 = np.array([[-1, 1]])  # Second plane 2D\nP3 = np.array([[-1, -1]]) # Third plane 2D\nP_l = [P1, P2, P3]  # List of arrays. It is the multi plane\n\n# Vector to search\nv = np.array([[2, 2]])\n\nThe next function creates a hash value based on a set of planes. The output value is a combination of the side of the plane where the vector is localized with respect to the collection of planes.\nWe can think of this list of planes as a set of basic hash functions, each of which can produce only 1 or 0 as output.\n\ndef hash_multi_plane(P_l, v):\n    hash_value = 0\n    for i, P in enumerate(P_l):\n        sign = side_of_plane(P,v)\n        hash_i = 1 if sign &gt;=0 else 0\n        hash_value += 2**i * hash_i\n    return hash_value\n\n\nhash_multi_plane(P_l, v) # Find the number of the plane that containes this value\n\n3"
  },
  {
    "objectID": "notes/c1w4/lab02.html#random-planes",
    "href": "notes/c1w4/lab02.html#random-planes",
    "title": "Hash functions and multiplanes",
    "section": "Random Planes",
    "text": "Random Planes\nIn the cell below, we create a set of three random planes\n\nnp.random.seed(0)\nnum_dimensions = 2 # is 300 in assignment\nnum_planes = 3 # is 10 in assignment\nrandom_planes_matrix = np.random.normal(\n                       size=(num_planes,\n                             num_dimensions))\nprint(random_planes_matrix)\n\n[[ 1.76405235  0.40015721]\n [ 0.97873798  2.2408932 ]\n [ 1.86755799 -0.97727788]]\n\n\n\nv = np.array([[2, 2]])\n\nThe next function is similar to the side_of_plane() function, but it evaluates more than a plane each time. The result is an array with the side of the plane of v, for the set of planes P\n\n# Side of the plane function. The result is a matrix\ndef side_of_plane_matrix(P, v):\n    dotproduct = np.dot(P, v.T)\n    sign_of_dot_product = np.sign(dotproduct) # Get a boolean value telling if the value in the cell is positive or negative\n    return sign_of_dot_product\n\nGet the side of the plane of the vector [2, 2] for the set of random planes.\n\nsides_l = side_of_plane_matrix(\n            random_planes_matrix, v)\nsides_l\n\narray([[1.],\n       [1.],\n       [1.]])\n\n\nNow, let us use the former function to define our multiplane hash function\n\ndef hash_multi_plane_matrix(P, v, num_planes):\n    sides_matrix = side_of_plane_matrix(P, v) # Get the side of planes for P and v\n    hash_value = 0\n    for i in range(num_planes):\n        sign = sides_matrix[i].item() # Get the value inside the matrix cell\n        hash_i = 1 if sign &gt;=0 else 0\n        hash_value += 2**i * hash_i # sum 2^i * hash_i\n        \n    return hash_value\n\nPrint the bucket hash for the vector v = [2, 2].\n\nhash_multi_plane_matrix(random_planes_matrix, v, num_planes)\n\n7\n\n\n\nNote\nThis showed we how to make one set of random planes. We will make multiple sets of random planes in order to make the approximate nearest neighbors more accurate."
  },
  {
    "objectID": "notes/c1w4/lab02.html#document-vectors",
    "href": "notes/c1w4/lab02.html#document-vectors",
    "title": "Hash functions and multiplanes",
    "section": "Document vectors",
    "text": "Document vectors\nBefore we finish this lab, remember that we can represent a document as a vector by adding up the word vectors for the words inside the document. In this example, our embedding contains only three words, each represented by a 3D array.\n\nword_embedding = {\"I\": np.array([1,0,1]),\n                   \"love\": np.array([-1,0,1]),\n                   \"learning\": np.array([1,0,1])\n                  }\nwords_in_document = ['I', 'love', 'learning', 'not_a_word']\ndocument_embedding = np.array([0,0,0])\nfor word in words_in_document:\n    document_embedding += word_embedding.get(word,0)\n    \nprint(document_embedding)\n\n[1 0 3]\n\n\nCongratulations! You’ve now completed this lab on hash functions and multiplanes!"
  },
  {
    "objectID": "notes/c3w3/lab01.html",
    "href": "notes/c3w3/lab01.html",
    "title": "Vanishing Gradients",
    "section": "",
    "text": "course banner\nIn this notebook you’ll take another look at vanishing gradients, from an intuitive standpoint.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "L1 - Vanishing Gradients"
    ]
  },
  {
    "objectID": "notes/c3w3/lab01.html#background",
    "href": "notes/c3w3/lab01.html#background",
    "title": "Vanishing Gradients",
    "section": "Background",
    "text": "Background\nAdding layers to a neural network introduces multiplicative effects in both forward and backward propagation. The back prop in particular presents a problem as the gradient of activation functions can be very small. Multiplied together across many layers, their product can be vanishingly small! This results in weights not being updated in the front layers and training not progressing.\n Gradients of the sigmoid function, for example, are in the range 0 to 0.25. To calculate gradients for the front layers of a neural network the chain rule is used. This means that these tiny values are multiplied starting at the last layer, working backwards to the first layer, with the gradients shrinking exponentially at each step. ## Imports\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n%matplotlib inline",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "L1 - Vanishing Gradients"
    ]
  },
  {
    "objectID": "notes/c3w3/lab01.html#data-activation-gradient",
    "href": "notes/c3w3/lab01.html#data-activation-gradient",
    "title": "Vanishing Gradients",
    "section": "Data, Activation & Gradient",
    "text": "Data, Activation & Gradient\n\nData\nI’ll start be creating some data, nothing special going on here. Just some values spread across the interval -5 to 5.\n\nTry changing the range of values in the data to see how it impacts the plots that follow.\n\n\n\nActivation\nThe example here is sigmoid() to squish the data x into the interval 0 to 1.\n\n\nGradient\nThis is the derivative of the sigmoid() activation function. It has a maximum of 0.25 at x = 0, the steepest point on the sigmoid plot.\n\nTry changing the x value for finding the tangent line in the plot.\n\n\n\n# Data\n# Interval [-5, 5]\n### START CODE HERE ###\nx = np.linspace(-5, 5, 100)  # try changing the range of values in the data. eg: (-100,100,1000)\n### END CODE HERE ###\n# Activation\n# Interval [0, 1]\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nactivations = sigmoid(x)\n\n# Gradient\n# Interval [0, 0.25]\ndef sigmoid_gradient(x):\n    return (x) * (1 - x)\n\ngradients = sigmoid_gradient(activations)\n\n# Plot sigmoid with tangent line\nplt.plot(x, activations)\nplt.title(\"Sigmoid Steepest Point\")\nplt.xlabel(\"x input data\")\nplt.ylabel(\"sigmoid(x)\")\n\n# Add the tangent line\n### START CODE HERE ###\nx_tan = 0   # x value to find the tangent. try different values within x declared above. eg: 2  \n### END CODE HERE ###\ny_tan = sigmoid(x_tan)  # y value\nspan = 1.7              # line span along x axis\ndata_tan = np.linspace(x_tan - span, x_tan + span)  # x values to plot\ngradient_tan = sigmoid_gradient(sigmoid(x_tan))     # gradient of the tangent\ntan = y_tan + gradient_tan * (data_tan - x_tan)     # y values to plot\nplt.plot(x_tan, y_tan, marker=\"o\", color=\"orange\", label=True)  # marker\nplt.plot(data_tan, tan, linestyle=\"--\", color=\"orange\")         # line\nplt.show()\n\nText(0.5, 1.0, 'Sigmoid Steepest Point')\n\n\nText(0.5, 0, 'x input data')\n\n\nText(0, 0.5, 'sigmoid(x)')",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "L1 - Vanishing Gradients"
    ]
  },
  {
    "objectID": "notes/c3w3/lab01.html#plots",
    "href": "notes/c3w3/lab01.html#plots",
    "title": "Vanishing Gradients",
    "section": "Plots",
    "text": "Plots\n\nSub Plots\nData values along the x-axis of the plots on the interval chosen for x, -5 to 5. Subplots: - x vs x - sigmoid of x - gradient of sigmoid\nNotice how the y axis keeps compressing from the left plot to the right plot. The interval range has shrunk from 10 to 1 to 0.25. How did this happen? As |x| gets larger the sigmoid approaches asymptotes at 0 and 1, and the sigmoid gradient shrinks towards 0. * Try changing the range of values in the code block above to see how it impacts the plots.\n\n# Sub plots\nfig, axs = plt.subplots(1, 3, figsize=(15, 4), sharex=True)\n\n# X values\naxs[0].plot(x, x)\naxs[0].set_title(\"x values\")\naxs[0].set_ylabel(\"y=x\")\naxs[0].set_xlabel(\"x input data\")\n\n# Sigmoid\naxs[1].plot(x, activations)\naxs[1].set_title(\"sigmoid\")\naxs[1].set_ylabel(\"sigmoid\")\naxs[1].set_xlabel(\"x input data\")\n\n# Sigmoid gradient\naxs[2].plot(x, gradients)\naxs[2].set_title(\"sigmoid gradient\")\naxs[2].set_ylabel(\"gradient\")\naxs[2].set_xlabel(\"x input data\")\n\nfig.show()\n\nText(0.5, 1.0, 'x values')\n\n\nText(0, 0.5, 'y=x')\n\n\nText(0.5, 0, 'x input data')\n\n\nText(0.5, 1.0, 'sigmoid')\n\n\nText(0, 0.5, 'sigmoid')\n\n\nText(0.5, 0, 'x input data')\n\n\nText(0.5, 1.0, 'sigmoid gradient')\n\n\nText(0, 0.5, 'gradient')\n\n\nText(0.5, 0, 'x input data')\n\n\n/tmp/ipykernel_120031/2272590038.py:22: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  fig.show()\n\n\n\n\n\n\n\n\n\n\n\nSingle Plo\nt Putting all 3 series on a single plot can help visualize the compression. Notice how hard it is to interpret because sigmoid and sigmoid gradient are so small compared to the scale of the input data x.\n\nTrying changing the plot ylim to zoom in.\n\n\n# Single plot\nplt.plot(x, x, label=\"data\")\nplt.plot(x, activations, label=\"sigmoid\")\nplt.plot(x, gradients, label=\"sigmoid gradient\")\nplt.legend(loc=\"upper left\")\nplt.title(\"Visualizing Compression\")\nplt.xlabel(\"x input data\")\nplt.ylabel(\"range\")\n### START CODE HERE ###\n# plt.ylim(-.5, 1.5)    # try shrinking the y axis limit for better visualization. eg: uncomment this line\n### END CODE HERE ###\nplt.show()\n\n# Max, Min of each array\nprint(\"\")\nprint(\"Max of x data :\", np.max(x))\nprint(\"Min of x data :\", np.min(x), \"\\n\")\nprint(\"Max of sigmoid :\", \"{:.3f}\".format(np.max(activations)))\nprint(\"Min of sigmoid :\", \"{:.3f}\".format(np.min(activations)), \"\\n\")\nprint(\"Max of gradients :\", \"{:.3f}\".format(np.max(gradients)))\nprint(\"Min of gradients :\", \"{:.3f}\".format(np.min(gradients)))\n\nText(0.5, 1.0, 'Visualizing Compression')\n\n\nText(0.5, 0, 'x input data')\n\n\nText(0, 0.5, 'range')\n\n\n\n\n\n\n\n\n\n\nMax of x data : 5.0\nMin of x data : -5.0 \n\nMax of sigmoid : 0.993\nMin of sigmoid : 0.007 \n\nMax of gradients : 0.250\nMin of gradients : 0.007",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "L1 - Vanishing Gradients"
    ]
  },
  {
    "objectID": "notes/c3w3/lab01.html#numerical-impact",
    "href": "notes/c3w3/lab01.html#numerical-impact",
    "title": "Vanishing Gradients",
    "section": "Numerical Impact",
    "text": "Numerical Impact\n\nMultiplication & Decay\nMultiplying numbers smaller than 1 results in smaller and smaller numbers. Below is an example that finds the gradient for an input x = 0 and multiplies it over n steps. Look how quickly it ‘Vanishes’ to almost zero. Yet sigmoid(x=0)=0.5 which has a sigmoid gradient of 0.25 and that happens to be the largest sigmoid gradient possible!  (Note: This is NOT an implementation of back propagation.)\n\nTry changing the number of steps n.\nTry changing the input value x. Consider the impact on sigmoid and sigmoid gradient.\n\n\n# Simulate decay\n# Inputs\n### START CODE HERE ###\nn = 6  # number of steps : try changing this\nx = 0  # value for input x : try changing this\n### END CODE HERE ###\ngrad = sigmoid_gradient(sigmoid(x))\nsteps = np.arange(1, n + 1)\nprint(\"-- Inputs --\")\nprint(\"steps :\", n)\nprint(\"x value :\", x)\nprint(\"sigmoid :\", \"{:.5f}\".format(sigmoid(x)))\nprint(\"gradient :\", \"{:.5f}\".format(grad), \"\\n\")\n\n# Loop to calculate cumulative total\nprint(\"-- Loop --\")\nvals = []\ntotal_grad = 1  # initialize to 1 to satisfy first loop below\nfor s in steps:\n    total_grad = total_grad * grad\n    vals.append(total_grad)\n    print(\"step\", s, \":\", total_grad)\n\nprint(\"\")\n\n# Plot\nplt.plot(steps, vals)\nplt.xticks(steps)\nplt.title(\"Multiplying Small Numbers\")\nplt.xlabel(\"Steps\")\nplt.ylabel(\"Cumulative Gradient\")\nplt.show()\n\n-- Inputs --\nsteps : 6\nx value : 0\nsigmoid : 0.50000\ngradient : 0.25000 \n\n-- Loop --\nstep 1 : 0.25\nstep 2 : 0.0625\nstep 3 : 0.015625\nstep 4 : 0.00390625\nstep 5 : 0.0009765625\nstep 6 : 0.000244140625\n\n\n\n([&lt;matplotlib.axis.XTick at 0x7edb9179aa10&gt;,\n  &lt;matplotlib.axis.XTick at 0x7edb917989d0&gt;,\n  &lt;matplotlib.axis.XTick at 0x7edb917ba920&gt;,\n  &lt;matplotlib.axis.XTick at 0x7edb917bb610&gt;,\n  &lt;matplotlib.axis.XTick at 0x7edb917e0340&gt;,\n  &lt;matplotlib.axis.XTick at 0x7edb917ba170&gt;],\n [Text(1, 0, '1'),\n  Text(2, 0, '2'),\n  Text(3, 0, '3'),\n  Text(4, 0, '4'),\n  Text(5, 0, '5'),\n  Text(6, 0, '6')])\n\n\nText(0.5, 1.0, 'Multiplying Small Numbers')\n\n\nText(0.5, 0, 'Steps')\n\n\nText(0, 0.5, 'Cumulative Gradient')",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "L1 - Vanishing Gradients"
    ]
  },
  {
    "objectID": "notes/c3w3/lab01.html#solution",
    "href": "notes/c3w3/lab01.html#solution",
    "title": "Vanishing Gradients",
    "section": "Solution",
    "text": "Solution\nOne solution is to use activation functions that don’t have tiny gradients. Other solutions involve more sophisticated model design. But they’re both discussions for another time.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "L1 - Vanishing Gradients"
    ]
  },
  {
    "objectID": "notes/c2w1/index.html",
    "href": "notes/c2w1/index.html",
    "title": "Autocorrect and minimum edit distance",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\n\nFigure 1\nMy notes for Week 1 of the Natural Language Processing with Probabilistic Models Course in the Natural Language Processing Specialization Offered by DeepLearning.AI on Coursera",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w1/index.html#overview",
    "href": "notes/c2w1/index.html#overview",
    "title": "Autocorrect and minimum edit distance",
    "section": "Overview",
    "text": "Overview\nWe use auto-correct everyday. When we send your friend a text message, or when we make a mistake in a query, there is an autocorrect behind the scenes that corrects the sentence for you. This week we are also going to learn about minimum edit distance, which tells we the minimum amount of edits to change one word into another. In doing that, we will learn about dynamic programming which is an important programming concept which frequently comes up in interviews and could be used to solve a lot of optimization problems.\n\n\n\n\n\n\n\nautocorrect\n\n\n\n\nFigure 2: Learning Objectives",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w1/index.html#autocorrect",
    "href": "notes/c2w1/index.html#autocorrect",
    "title": "Autocorrect and minimum edit distance",
    "section": "Autocorrect",
    "text": "Autocorrect\nAutocorrects are used everywhere. We use them in your phones, tablets, and computers.\n\n\n\n\n\n\n\nfind candidates\n\n\n\n\nFigure 3: What is autocorrect\n\n\nTo implement autocorrect in this week’s assignment, we have to follow these steps:\n\nIdentify a misspelled word\nFind strings n edit distance away: (these could be random strings)\nFilter candidates: (keep only the real words from the previous steps)\nCalculate word probabilities: (choose the word that is most likely to occur in that context)\n\n\n\n\n\n\n\n\nfind candidates\n\n\n\n\nFigure 4: Find candidates\n\n\nBuilding the model:\n\nIdentify the misspelled word\n\n\nWhen identifying the misspelled word, we can check whether it is in the vocabulary. If we don’t find it, then it is probably a typo.\n\n\nFind strings n edit distance away\nFilter candidates\n\n\nIn this step, we want to take all the words generated above and then only keep the actual words that make sense and that we can find in your vocabulary.\n\n\n\n\n\n\n\n\nfilter\n\n\n\n\nFigure 5: Filter candidates",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w1/index.html#lab-building-the-vocabulary",
    "href": "notes/c2w1/index.html#lab-building-the-vocabulary",
    "title": "Autocorrect and minimum edit distance",
    "section": "Lab: Building the vocabulary",
    "text": "Lab: Building the vocabulary\nBuilding the vocabulary",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w1/index.html#building-the-model-ii",
    "href": "notes/c2w1/index.html#building-the-model-ii",
    "title": "Autocorrect and minimum edit distance",
    "section": "Building the model II",
    "text": "Building the model II\n\nCalculating word probabilities\n\n\n\n\n\n\n\n\nword probabilities\n\n\n\n\nFigure 6: calculating word probabilities\n\n\nNote that we are storing the count of words and then we can use that to generate the probabilities. For this week, we will be counting the probabilities of words occurring. If we want to build a slightly more sophisticated auto-correct we can keep track of two words occurring next to each other instead. We can then use the previous word to decide. For example which combo is more likely, there friend or their friend? For this week however we will be implementing the probabilities by just using the word frequencies.\nHere is a summary of everything we have seen before in the previous two videos.\n\n\n\n\n\n\n\nsummary\n\n\n\n\nFigure 7: summary of first four autocorrect steps",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w1/index.html#lab-candidates-from-edits",
    "href": "notes/c2w1/index.html#lab-candidates-from-edits",
    "title": "Autocorrect and minimum edit distance",
    "section": "Lab: Candidates from Edits",
    "text": "Lab: Candidates from Edits\nCandidates from Edits",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w1/index.html#minimum-edit-distance",
    "href": "notes/c2w1/index.html#minimum-edit-distance",
    "title": "Autocorrect and minimum edit distance",
    "section": "Minimum edit distance",
    "text": "Minimum edit distance\nMinimum edit distance allows we to:\n\nEvaluate similarity between two strings\nFind the minimum number of edits between two strings\nImplement spelling correction, document similarity, machine translation, DNA sequencing, and more\n\nRemember that the edits include:\n\nInsert (add a letter) ‘to’: ‘top’, ‘two’ …\nDelete (remove a letter) ‘hat’: ‘ha’, ‘at’, ‘ht’\nReplace (change 1 letter to another) ‘jaw’: ‘jar’, ‘paw’, …\n\nHere is a concrete example where we calculate the cost (i.e. edit distance) between two strings.\n\n\n\n\n\n\n\nMinimum edit distance\n\n\n\n\nFigure 8: Minimum edit distance\n\n\nNote that as your strings get larger it gets much harder to calculate the minimum edit distance. Hence we will now learn about the minimum edit distance algorithm!",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w1/index.html#minimum-edit-distance-algorithm",
    "href": "notes/c2w1/index.html#minimum-edit-distance-algorithm",
    "title": "Autocorrect and minimum edit distance",
    "section": "Minimum edit distance algorithm",
    "text": "Minimum edit distance algorithm\nWhen computing the minimum edit distance, we would start with a source word and transform it into the target word. Let’s look at the following example:\n\n\n\n\n\n\n\nMinimum edit distance\n\n\n\n\nFigure 9: Minimum edit distance algorithm\n\n\nTo go:\n\nfrom # → # has a cost of 0\nfrom p → # has a cost of 1, because that is the cost of a delete.\n\nfrom p → s has a cost of 2, delete p and insert s.\n\nWe can keep going this way by populating one element at a time, but it turns out there is a faster way to do this.\nWee will learn about it next.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w1/index.html#minimum-edit-distance-algorithm-ii",
    "href": "notes/c2w1/index.html#minimum-edit-distance-algorithm-ii",
    "title": "Autocorrect and minimum edit distance",
    "section": "Minimum edit distance algorithm II",
    "text": "Minimum edit distance algorithm II\nTo populate the following table:\n\n\n\n\n\n\n\nMinimum edit distance\n\n\n\n\nFigure 10: Minimum edit distance algorithm\n\n\nThere are three equations:\nD[i,j] = D[i-1, j] + del_cost: this indicates we want to populate the current cell (i,j) by using the cost in the cell found directly above.\nD[i,j] = D[i, j-1] + ins_cost: this indicates we want to populate the current cell (i,j) by using the cost in the cell found directly to its left.\nD[i,j] = D[i-1, j-1] + rep_cost: the rep cost can be 2 or 0 depending if we are going to actually replace it or not.\nAt every time step we check the three possible paths where we can come from and we select the least expensive one. Once we are done, we get the following:\n\n\n\n\n\n\n\nMinimum edit distance\n\n\n\n\nFigure 11: Minimum edit distance algorithm",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w1/index.html#minimum-edit-distance-iii",
    "href": "notes/c2w1/index.html#minimum-edit-distance-iii",
    "title": "Autocorrect and minimum edit distance",
    "section": "Minimum edit distance III",
    "text": "Minimum edit distance III\n\n\n\n\n\n\n\nMinimum edit distance\n\n\n\n\nFigure 12: Minimum edit distance algorithm\n\n\nTo summarize, we have seen the levenshtein distance which specifies the cost per operation. If we need to reconstruct the path of how we got from one string to the other, we can use a backtrace. We should keep a simple pointer in each cell letting we know where we came from to get there. So we know the path taken across the table from the top left corner, to the bottom right corner. We can then reconstruct it.\nThis method for computation instead of brute force is a technique known as dynamic programming. We first solve the smallest subproblem first and then reusing that result we solve the next biggest subproblem, saving that result, reusing it again, and so on. This is exactly what we did by populating each cell from the top right to the bottom left. It’s a well-known technique in computer science!",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w1/lab01.html",
    "href": "notes/c2w1/lab01.html",
    "title": "Building the vocabulary",
    "section": "",
    "text": "course banner\n\n\nEstimated Time: 10 minutes\n\nVocabulary Creation\nCreate a tiny vocabulary from a tiny corpus\nIt’s time to start small !\n\nImports and Data\n\n# imports\nimport re # regular expression library; for tokenization of words\nfrom collections import Counter # collections library; counter: dict subclass for counting hashable objects\nimport matplotlib.pyplot as plt # for data visualization\n\n\n# the tiny corpus of text ! \ntext = 'red pink pink blue blue yellow ORANGE BLUE BLUE PINK' # 🌈\nprint(text)\nprint('string length : ',len(text))\n\nred pink pink blue blue yellow ORANGE BLUE BLUE PINK\nstring length :  52\n\n\n\n\nPreprocessing\n\n# convert all letters to lower case\ntext_lowercase = text.lower()\nprint(text_lowercase)\nprint('string length : ',len(text_lowercase))\n\nred pink pink blue blue yellow orange blue blue pink\nstring length :  52\n\n\n\n# some regex to tokenize the string to words and return them in a list\nwords = re.findall(r'\\w+', text_lowercase)\nprint(words)\nprint('count : ',len(words))\n\n['red', 'pink', 'pink', 'blue', 'blue', 'yellow', 'orange', 'blue', 'blue', 'pink']\ncount :  10\n\n\n\n\nCreate Vocabulary\nOption 1 : A set of distinct words from the text\n\n# create vocab\nvocab = set(words)\nprint(vocab)\nprint('count : ',len(vocab))\n\n{'blue', 'red', 'yellow', 'orange', 'pink'}\ncount :  5\n\n\n\n\nAdd Information with Word Counts\nOption 2 : Two alternatives for including the word count as well\n\n# create vocab including word count\ncounts_a = dict()\nfor w in words:\n    counts_a[w] = counts_a.get(w,0)+1\nprint(counts_a)\nprint('count : ',len(counts_a))\n\n{'red': 1, 'pink': 3, 'blue': 4, 'yellow': 1, 'orange': 1}\ncount :  5\n\n\n\n# create vocab including word count using collections.Counter\ncounts_b = dict()\ncounts_b = Counter(words)\nprint(counts_b)\nprint('count : ',len(counts_b))\n\nCounter({'blue': 4, 'pink': 3, 'red': 1, 'yellow': 1, 'orange': 1})\ncount :  5\n\n\n\n# barchart of sorted word counts\nd = {'blue': counts_b['blue'], 'pink': counts_b['pink'], 'red': counts_b['red'], 'yellow': counts_b['yellow'], 'orange': counts_b['orange']}\nplt.bar(range(len(d)), list(d.values()), align='center', color=d.keys())\n_ = plt.xticks(range(len(d)), list(d.keys()))\n\n\n\n\n\n\n\n\n\n\nUngraded Exercise\nNote that counts_b, above, returned by collections.Counter is sorted by word count\nCan we modify the tiny corpus of text so that a new color appears between pink and red in counts_b ?\nDo we need to run all the cells again, or just specific ones ?\n\nprint('counts_b : ', counts_b)\nprint('count : ', len(counts_b))\n\ncounts_b :  Counter({'blue': 4, 'pink': 3, 'red': 1, 'yellow': 1, 'orange': 1})\ncount :  5\n\n\nExpected Outcome:\ncounts_b : Counter({‘blue’: 4, ‘pink’: 3, ‘your_new_color_here’: 2, red’: 1, ‘yellow’: 1, ‘orange’: 1})  count : 6\n\n\nSummary\nThis is a tiny example but the methodology scales very well.  In the assignment we will create a large vocabulary of thousands of words, from a corpus  of tens of thousands or words! But the mechanics are exactly the same.  The only extra things to pay attention to should be; run time, memory management and the vocab data structure.  So the choice of approach used in code blocks counts_a vs counts_b, above, will be important.\n\n\n\n\n\nCitationBibTeX citation:@online{bochman2020,\n  author = {Bochman, Oren},\n  title = {Building the Vocabulary},\n  date = {2020-10-16},\n  url = {https://orenbochman.github.io/notes-nlp/notes/c2w1/lab01.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2020. “Building the Vocabulary.” October 16,\n2020. https://orenbochman.github.io/notes-nlp/notes/c2w1/lab01.html.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "L1 - Building the vocabulary"
    ]
  },
  {
    "objectID": "notes/c4w3/lab02.html",
    "href": "notes/c4w3/lab02.html",
    "title": "Assignment 3 Ungraded Sections - Part 1: BERT Loss Model",
    "section": "",
    "text": "course banner\nWelcome to the part 1 of testing the models for this week’s assignment. We will perform decoding using the BERT Loss model. In this notebook we’ll use an input, mask (hide) random word(s) in it and see how well we get the “Target” answer(s).",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "L2 - BERT Loss"
    ]
  },
  {
    "objectID": "notes/c4w3/lab02.html#colab",
    "href": "notes/c4w3/lab02.html#colab",
    "title": "Assignment 3 Ungraded Sections - Part 1: BERT Loss Model",
    "section": "Colab",
    "text": "Colab\nSince this ungraded lab takes a lot of time to run on coursera, as an alternative we have a colab prepared for you.\nBERT Loss Model Colab\n\nIf you run into a page that looks similar to the one below, with the option Open with, this would mean you need to download the Colaboratory app. You can do so by Open with -&gt; Connect more apps -&gt; in the search bar write \"Colaboratory\" -&gt; install\n\n\n\nAfter installation it should look like this. Click on Open with Google Colaboratory",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "L2 - BERT Loss"
    ]
  },
  {
    "objectID": "notes/c4w3/lab02.html#outline",
    "href": "notes/c4w3/lab02.html#outline",
    "title": "Assignment 3 Ungraded Sections - Part 1: BERT Loss Model",
    "section": "Outline",
    "text": "Outline\n\nOverview\nPart 1: Getting ready\nPart 2: BERT Loss\n\n2.1 Decoding\n\n\n\nOverview\nIn this notebook you will:\n\nImplement the Bidirectional Encoder Representation from Transformer (BERT) loss.\nUse a pretrained version of the model you created in the assignment for inference.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "L2 - BERT Loss"
    ]
  },
  {
    "objectID": "notes/c4w3/lab02.html#1",
    "href": "notes/c4w3/lab02.html#1",
    "title": "Assignment 3 Ungraded Sections - Part 1: BERT Loss Model",
    "section": "Part 1: Getting ready",
    "text": "Part 1: Getting ready\nRun the code cells below to import the necessary libraries and to define some functions which will be useful for decoding. The code and the functions are the same as the ones you previsouly ran on the graded assignment.\n\nimport pickle\nimport string\nimport ast\nimport numpy as np\nimport trax \nfrom trax.supervised import decoding\nimport textwrap \n\nwrapper = textwrap.TextWrapper(width=70)\n\n2025-02-10 16:51:29.192037: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1739199089.204440  119292 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1739199089.208671  119292 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\n\n\nexample_jsons = list(map(ast.literal_eval, open('data.txt')))\n\nnatural_language_texts = [example_json['text'] for example_json in example_jsons]\n\nPAD, EOS, UNK = 0, 1, 2\n\ndef detokenize(np_array):\n    return trax.data.detokenize(\n        np_array,\n        vocab_type='sentencepiece',\n        vocab_file='sentencepiece.model',\n        vocab_dir='.')\n\n\ndef tokenize(s):\n    return next(trax.data.tokenize(\n        iter([s]),\n        vocab_type='sentencepiece',\n        vocab_file='sentencepiece.model',\n        vocab_dir='.'))\n \n    \nvocab_size = trax.data.vocab_size(\n    vocab_type='sentencepiece',\n    vocab_file='sentencepiece.model',\n    vocab_dir='.')\n\n\ndef get_sentinels(vocab_size, display=False):\n    sentinels = {}\n    for i, char in enumerate(reversed(string.ascii_letters), 1):\n        decoded_text = detokenize([vocab_size - i]) \n        # Sentinels, ex: &lt;Z&gt; - &lt;a&gt;\n        sentinels[decoded_text] = f'&lt;{char}&gt;'    \n        if display:\n            print(f'The sentinel is &lt;{char}&gt; and the decoded token is:', decoded_text)\n    return sentinels\n\n\nsentinels = get_sentinels(vocab_size, display=False)    \n\n\ndef pretty_decode(encoded_str_list, sentinels=sentinels):\n    # If already a string, just do the replacements.\n    if isinstance(encoded_str_list, (str, bytes)):\n        for token, char in sentinels.items():\n            encoded_str_list = encoded_str_list.replace(token, char)\n        return encoded_str_list\n  \n    # We need to decode and then prettyfy it.\n    return pretty_decode(detokenize(encoded_str_list))\n\n\ninputs_targets_pairs = []\n\n# here you are reading already computed input/target pairs from a file\nwith open ('inputs_targets_pairs_file.txt', 'rb') as fp:\n    inputs_targets_pairs = pickle.load(fp)  \n\n\ndef display_input_target_pairs(inputs_targets_pairs):\n    for i, inp_tgt_pair in enumerate(inputs_targets_pairs, 1):\n        inps, tgts = inp_tgt_pair\n        inps, tgts = pretty_decode(inps), pretty_decode(tgts)\n        print(f'[{i}]\\n'\n              f'inputs:\\n{wrapper.fill(text=inps)}\\n\\n'\n              f'targets:\\n{wrapper.fill(text=tgts)}\\n\\n\\n\\n')\n    \ndisplay_input_target_pairs(inputs_targets_pairs)    \n\n\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\nCell In[2], line 23\n     15 def tokenize(s):\n     16     return next(trax.data.tokenize(\n     17         iter([s]),\n     18         vocab_type='sentencepiece',\n     19         vocab_file='sentencepiece.model',\n     20         vocab_dir='.'))\n---&gt; 23 vocab_size = trax.data.vocab_size(\n     24     vocab_type='sentencepiece',\n     25     vocab_file='sentencepiece.model',\n     26     vocab_dir='.')\n     29 def get_sentinels(vocab_size, display=False):\n     30     sentinels = {}\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/data/tf_inputs.py:570, in vocab_size(vocab_type, vocab_file, vocab_dir, n_reserved_ids)\n    550 def vocab_size(vocab_type='subword',\n    551                vocab_file=None,\n    552                vocab_dir=None,\n    553                n_reserved_ids=0):\n    554   \"\"\"Returns the size of the vocabulary (number of symbols used).\n    555 \n    556   This function can be used to set the size of the final layers of a model that\n   (...)\n    568     An integer, the number of symbols used (including reserved IDs).\n    569   \"\"\"\n--&gt; 570   vocab = _get_vocab(vocab_type, vocab_file, vocab_dir)\n    571   return vocab.vocab_size + n_reserved_ids\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/data/tf_inputs.py:603, in _get_vocab(vocab_type, vocab_file, vocab_dir, extra_ids)\n    600   return text_encoder.BertEncoder(path, do_lower_case=True)\n    602 assert vocab_type == 'sentencepiece'\n--&gt; 603 return t5_data().SentencePieceVocabulary(sentencepiece_model_file=path,\n    604                                          extra_ids=extra_ids)\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/data/tf_inputs.py:53, in t5_data()\n     51 module = None\n     52 try:\n---&gt; 53   import t5.data  # pylint: disable=g-import-not-at-top\n     54   module = t5.data\n     55 except AttributeError as e:\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/t5/__init__.py:17\n      1 # Copyright 2023 The T5 Authors.\n      2 #\n      3 # Licensed under the Apache License, Version 2.0 (the \"License\");\n   (...)\n     12 # See the License for the specific language governing permissions and\n     13 # limitations under the License.\n     15 \"\"\"Import API modules.\"\"\"\n---&gt; 17 import t5.data\n     18 import t5.evaluation\n     20 # Version number.\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/t5/data/__init__.py:17\n     15 \"\"\"Import data modules.\"\"\"\n     16 # pylint:disable=wildcard-import,g-bad-import-order\n---&gt; 17 from t5.data.dataset_providers import *\n     18 from t5.data.glue_utils import *\n     19 import t5.data.postprocessors\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/t5/data/dataset_providers.py:28\n     25 from collections.abc import Mapping\n     26 import re\n---&gt; 28 import seqio\n     29 from t5.data import utils\n     30 import tensorflow.compat.v2 as tf\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/seqio/__init__.py:19\n     15 \"\"\"Import to top-level API.\"\"\"\n     17 # pylint:disable=wildcard-import,g-bad-import-order,g-import-not-at-top\n---&gt; 19 from seqio.dataset_providers import *\n     20 from seqio import evaluation\n     21 from seqio import experimental\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/seqio/dataset_providers.py:36\n     33 from typing import Any, Callable, Iterable, List, Mapping, MutableMapping, Optional, Sequence, Set, Tuple, Type, Union\n     35 from absl import logging\n---&gt; 36 import clu.metrics\n     37 import editdistance\n     38 import numpy as np\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/clu/metrics.py:66\n     64 from clu.internal import utils\n     65 import clu.values\n---&gt; 66 import flax\n     67 import jax\n     68 import jax.numpy as jnp\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/flax/__init__.py:19\n      1 # Copyright 2022 The Flax Authors.\n      2 #\n      3 # Licensed under the Apache License, Version 2.0 (the \"License\");\n   (...)\n     14 \n     15 # Lint as: python 3\n     17 \"\"\"Flax API.\"\"\"\n---&gt; 19 from . import core\n     20 from . import linen\n     21 from . import optim\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/flax/core/__init__.py:15\n      1 # Copyright 2022 The Flax Authors.\n      2 #\n      3 # Licensed under the Apache License, Version 2.0 (the \"License\");\n   (...)\n     12 # See the License for the specific language governing permissions and\n     13 # limitations under the License.\n---&gt; 15 from .axes_scan import broadcast\n     16 from .frozen_dict import FrozenDict, freeze, unfreeze\n     17 from .tracers import current_trace, trace_level, check_trace_level\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/flax/core/axes_scan.py:22\n     19 from jax import lax\n     21 from jax.interpreters import partial_eval as pe\n---&gt; 22 from jax import linear_util as lu\n     24 from typing import Union, Optional, Callable, Any\n     26 import numpy as np\n\nImportError: cannot import name 'linear_util' from 'jax' (/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/jax/__init__.py)",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "L2 - BERT Loss"
    ]
  },
  {
    "objectID": "notes/c4w3/lab02.html#part-2-bert-loss",
    "href": "notes/c4w3/lab02.html#part-2-bert-loss",
    "title": "Assignment 3 Ungraded Sections - Part 1: BERT Loss Model",
    "section": "Part 2: BERT Loss",
    "text": "Part 2: BERT Loss\nNow that you created the encoder, we will not make you train it. Training it could easily cost you a few days depending on which GPUs/TPUs you are using. Very few people train the full transformer from scratch. Instead, what the majority of people do, they load in a pretrained model, and they fine tune it on a specific task. That is exactly what you are about to do. Let’s start by initializing and then loading in the model.\nInitialize the model from the saved checkpoint.\n\n# Initializing the model\nmodel = trax.models.Transformer(\n    d_ff = 4096,\n    d_model = 1024,\n    max_len = 2048,\n    n_heads = 16,\n    dropout = 0.1,\n    input_vocab_size = 32000,\n    n_encoder_layers = 24,\n    n_decoder_layers = 24,\n    mode='predict')\n\n\n# Now load in the model\n# this takes about 1 minute\nshape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)  # Needed in predict mode.\nmodel.init_from_file('model.pkl.gz',\n                     weights_only=True, input_signature=(shape11, shape11))\n\n\n---------------------------------------------------------------------------\nNotFoundError                             Traceback (most recent call last)\nCell In[4], line 4\n      1 # Now load in the model\n      2 # this takes about 1 minute\n      3 shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)  # Needed in predict mode.\n----&gt; 4 model.init_from_file('model.pkl.gz',\n      5                      weights_only=True, input_signature=(shape11, shape11))\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/base.py:334, in Layer.init_from_file(self, file_name, weights_only, input_signature)\n    332 with tf.io.gfile.GFile(file_name, 'rb') as f:\n    333   with gzip.GzipFile(fileobj=f, compresslevel=2) as gzipf:\n--&gt; 334     dictionary = pickle.load(gzipf)\n    335 # In the current checkpoint format, we store weights in a separate\n    336 # non-pickled file with the same name but added \".npy\".\n    337 if isinstance(dictionary['flat_weights'], int):\n\nFile /usr/lib/python3.10/gzip.py:321, in GzipFile.peek(self, n)\n    319     import errno\n    320     raise OSError(errno.EBADF, \"peek() on write-only GzipFile object\")\n--&gt; 321 return self._buffer.peek(n)\n\nFile /usr/lib/python3.10/_compression.py:68, in DecompressReader.readinto(self, b)\n     66 def readinto(self, b):\n     67     with memoryview(b) as view, view.cast(\"B\") as byte_view:\n---&gt; 68         data = self.read(len(byte_view))\n     69         byte_view[:len(data)] = data\n     70     return len(data)\n\nFile /usr/lib/python3.10/gzip.py:488, in _GzipReader.read(self, size)\n    484 if self._new_member:\n    485     # If the _new_member flag is set, we have to\n    486     # jump to the next member, if there is one.\n    487     self._init_read()\n--&gt; 488     if not self._read_gzip_header():\n    489         self._size = self._pos\n    490         return b\"\"\n\nFile /usr/lib/python3.10/gzip.py:431, in _GzipReader._read_gzip_header(self)\n    430 def _read_gzip_header(self):\n--&gt; 431     magic = self._fp.read(2)\n    432     if magic == b'':\n    433         return False\n\nFile /usr/lib/python3.10/gzip.py:97, in _PaddedFile.read(self, size)\n     94 read = self._read\n     95 self._read = None\n     96 return self._buffer[read:] + \\\n---&gt; 97        self.file.read(size-self._length+read)\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tensorflow/python/lib/io/file_io.py:116, in FileIO.read(self, n)\n    104 def read(self, n=-1):\n    105   \"\"\"Returns the contents of a file as a string.\n    106 \n    107   Starts reading from current position in file.\n   (...)\n    114     string if in string (regular) mode.\n    115   \"\"\"\n--&gt; 116   self._preread_check()\n    117   if n == -1:\n    118     length = self.size() - self.tell()\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/tensorflow/python/lib/io/file_io.py:77, in FileIO._preread_check(self)\n     74 if not self._read_check_passed:\n     75   raise errors.PermissionDeniedError(None, None,\n     76                                      \"File isn't open for reading\")\n---&gt; 77 self._read_buf = _pywrap_file_io.BufferedInputStream(\n     78     compat.path_to_str(self.__name), 1024 * 512)\n\nNotFoundError: model.pkl.gz; No such file or directory\n\n\n\n\n\n2.1 Decoding\nNow you will use one of the inputs_targets_pairs for input and as target. Next you will use the pretty_decode to output the input and target. The code to perform all of this has been provided below.\n\n# using the 3rd example\nc4_input = inputs_targets_pairs[2][0]\nc4_target = inputs_targets_pairs[2][1]\n\nprint('pretty_decoded input: \\n\\n', pretty_decode(c4_input))\nprint('\\npretty_decoded target: \\n\\n', pretty_decode(c4_target))\nprint('\\nc4_input:\\n\\n', c4_input)\nprint('\\nc4_target:\\n\\n', c4_target)\nprint(len(c4_target))\nprint(len(pretty_decode(c4_target)))\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[5], line 2\n      1 # using the 3rd example\n----&gt; 2 c4_input = inputs_targets_pairs[2][0]\n      3 c4_target = inputs_targets_pairs[2][1]\n      5 print('pretty_decoded input: \\n\\n', pretty_decode(c4_input))\n\nNameError: name 'inputs_targets_pairs' is not defined\n\n\n\nRun the cell below to decode.\n\n\nNote: This will take some time to run\n\n# Temperature is a parameter for sampling.\n#   # * 0.0: same as argmax, always pick the most probable token\n#   # * 1.0: sampling from the distribution (can sometimes say random things)\n#   # * values inbetween can trade off diversity and quality, try it out!\noutput = decoding.autoregressive_sample(model, inputs=np.array(c4_input)[None, :],\n                                        temperature=0.0, max_length=5) # originally max_length = 50\nprint(wrapper.fill(pretty_decode(output[0])))\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[6], line 5\n      1 # Temperature is a parameter for sampling.\n      2 #   # * 0.0: same as argmax, always pick the most probable token\n      3 #   # * 1.0: sampling from the distribution (can sometimes say random things)\n      4 #   # * values inbetween can trade off diversity and quality, try it out!\n----&gt; 5 output = decoding.autoregressive_sample(model, inputs=np.array(c4_input)[None, :],\n      6                                         temperature=0.0, max_length=5) # originally max_length = 50\n      7 print(wrapper.fill(pretty_decode(output[0])))\n\nNameError: name 'c4_input' is not defined\n\n\n\nAt this point the RAM is almost full, this happens because the model and the decoding is memory heavy. You can run decoding just once. Running it the second time with another example might give you an answer that makes no sense, or repetitive words. If that happens restart the runtime (see how to at the start of the notebook) and run all the cells again.\nYou should also be aware that the quality of the decoding is not very good because max_length was downsized from 50 to 5 so that this runs faster within this environment. The colab version uses the original max_length so check that one for the actual decoding.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "L2 - BERT Loss"
    ]
  },
  {
    "objectID": "notes/c4w3/lab03.html",
    "href": "notes/c4w3/lab03.html",
    "title": "Assignment 3 Ungraded Sections - Part 2: T5 SQuAD Model",
    "section": "",
    "text": "course banner\nWelcome to the part 2 of testing the models for this week’s assignment. This time we will perform decoding using the T5 SQuAD model. In this notebook we’ll perform Question Answering by providing a “Question”, its “Context” and see how well we get the “Target” answer.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "L3 - T5"
    ]
  },
  {
    "objectID": "notes/c4w3/lab03.html#colab",
    "href": "notes/c4w3/lab03.html#colab",
    "title": "Assignment 3 Ungraded Sections - Part 2: T5 SQuAD Model",
    "section": "Colab",
    "text": "Colab\nSince this ungraded lab takes a lot of time to run on coursera, as an alternative we have a colab prepared for you.\nT5 SQuAD Model Colab\n\nIf you run into a page that looks similar to the one below, with the option Open with, this would mean you need to download the Colaboratory app. You can do so by Open with -&gt; Connect more apps -&gt; in the search bar write \"Colaboratory\" -&gt; install\n\n\n\nAfter installation it should look like this. Click on Open with Google Colaboratory",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "L3 - T5"
    ]
  },
  {
    "objectID": "notes/c4w3/lab03.html#outline",
    "href": "notes/c4w3/lab03.html#outline",
    "title": "Assignment 3 Ungraded Sections - Part 2: T5 SQuAD Model",
    "section": "Outline",
    "text": "Outline\n\nOverview\nPart 1: Resuming the assignment (T5 SQuAD Model)\nPart 2: Fine-tuning on SQuAD\n\n2.1 Loading in the data and preprocessing\n2.2 Decoding from a fine-tuned model\n\n\n\nOverview\nIn this notebook you will:\n\nImplement the Bidirectional Encoder Representation from Transformer (BERT) loss.\nUse a pretrained version of the model you created in the assignment for inference.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "L3 - T5"
    ]
  },
  {
    "objectID": "notes/c4w3/lab03.html#1",
    "href": "notes/c4w3/lab03.html#1",
    "title": "Assignment 3 Ungraded Sections - Part 2: T5 SQuAD Model",
    "section": "Part 1: Getting ready",
    "text": "Part 1: Getting ready\nRun the code cells below to import the necessary libraries and to define some functions which will be useful for decoding. The code and the functions are the same as the ones you previsouly ran on the graded assignment.\n\nimport string\nimport t5\nimport numpy as np\nimport trax \nfrom trax.supervised import decoding\nimport textwrap \n\nwrapper = textwrap.TextWrapper(width=70)\n\n\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\nCell In[1], line 2\n      1 import string\n----&gt; 2 import t5\n      3 import numpy as np\n      4 import trax \n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/t5/__init__.py:17\n      1 # Copyright 2023 The T5 Authors.\n      2 #\n      3 # Licensed under the Apache License, Version 2.0 (the \"License\");\n   (...)\n     12 # See the License for the specific language governing permissions and\n     13 # limitations under the License.\n     15 \"\"\"Import API modules.\"\"\"\n---&gt; 17 import t5.data\n     18 import t5.evaluation\n     20 # Version number.\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/t5/data/__init__.py:17\n     15 \"\"\"Import data modules.\"\"\"\n     16 # pylint:disable=wildcard-import,g-bad-import-order\n---&gt; 17 from t5.data.dataset_providers import *\n     18 from t5.data.glue_utils import *\n     19 import t5.data.postprocessors\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/t5/data/dataset_providers.py:28\n     25 from collections.abc import Mapping\n     26 import re\n---&gt; 28 import seqio\n     29 from t5.data import utils\n     30 import tensorflow.compat.v2 as tf\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/seqio/__init__.py:19\n     15 \"\"\"Import to top-level API.\"\"\"\n     17 # pylint:disable=wildcard-import,g-bad-import-order,g-import-not-at-top\n---&gt; 19 from seqio.dataset_providers import *\n     20 from seqio import evaluation\n     21 from seqio import experimental\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/seqio/dataset_providers.py:36\n     33 from typing import Any, Callable, Iterable, List, Mapping, MutableMapping, Optional, Sequence, Set, Tuple, Type, Union\n     35 from absl import logging\n---&gt; 36 import clu.metrics\n     37 import editdistance\n     38 import numpy as np\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/clu/metrics.py:66\n     64 from clu.internal import utils\n     65 import clu.values\n---&gt; 66 import flax\n     67 import jax\n     68 import jax.numpy as jnp\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/flax/__init__.py:19\n      1 # Copyright 2022 The Flax Authors.\n      2 #\n      3 # Licensed under the Apache License, Version 2.0 (the \"License\");\n   (...)\n     14 \n     15 # Lint as: python 3\n     17 \"\"\"Flax API.\"\"\"\n---&gt; 19 from . import core\n     20 from . import linen\n     21 from . import optim\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/flax/core/__init__.py:15\n      1 # Copyright 2022 The Flax Authors.\n      2 #\n      3 # Licensed under the Apache License, Version 2.0 (the \"License\");\n   (...)\n     12 # See the License for the specific language governing permissions and\n     13 # limitations under the License.\n---&gt; 15 from .axes_scan import broadcast\n     16 from .frozen_dict import FrozenDict, freeze, unfreeze\n     17 from .tracers import current_trace, trace_level, check_trace_level\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/flax/core/axes_scan.py:22\n     19 from jax import lax\n     21 from jax.interpreters import partial_eval as pe\n---&gt; 22 from jax import linear_util as lu\n     24 from typing import Union, Optional, Callable, Any\n     26 import numpy as np\n\nImportError: cannot import name 'linear_util' from 'jax' (/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/jax/__init__.py)\n\n\n\n\nPAD, EOS, UNK = 0, 1, 2\n\n\ndef detokenize(np_array):\n    return trax.data.detokenize(\n        np_array,\n        vocab_type='sentencepiece',\n        vocab_file='sentencepiece.model',\n        vocab_dir='.')\n\n\ndef tokenize(s):\n    return next(trax.data.tokenize(\n        iter([s]),\n        vocab_type='sentencepiece',\n        vocab_file='sentencepiece.model',\n        vocab_dir='.'))\n \n    \nvocab_size = trax.data.vocab_size(\n    vocab_type='sentencepiece',\n    vocab_file='sentencepiece.model',\n    vocab_dir='.')\n\n\ndef get_sentinels(vocab_size, display=False):\n    sentinels = {}\n    for i, char in enumerate(reversed(string.ascii_letters), 1):\n        decoded_text = detokenize([vocab_size - i]) \n        # Sentinels, ex: &lt;Z&gt; - &lt;a&gt;\n        sentinels[decoded_text] = f'&lt;{char}&gt;'    \n        if display:\n            print(f'The sentinel is &lt;{char}&gt; and the decoded token is:', decoded_text)\n    return sentinels\n\n\nsentinels = get_sentinels(vocab_size, display=False)    \n\n\ndef pretty_decode(encoded_str_list, sentinels=sentinels):\n    # If already a string, just do the replacements.\n    if isinstance(encoded_str_list, (str, bytes)):\n        for token, char in sentinels.items():\n            encoded_str_list = encoded_str_list.replace(token, char)\n        return encoded_str_list\n  \n    # We need to decode and then prettyfy it.\n    return pretty_decode(detokenize(encoded_str_list))\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[2], line 20\n     12 def tokenize(s):\n     13     return next(trax.data.tokenize(\n     14         iter([s]),\n     15         vocab_type='sentencepiece',\n     16         vocab_file='sentencepiece.model',\n     17         vocab_dir='.'))\n---&gt; 20 vocab_size = trax.data.vocab_size(\n     21     vocab_type='sentencepiece',\n     22     vocab_file='sentencepiece.model',\n     23     vocab_dir='.')\n     26 def get_sentinels(vocab_size, display=False):\n     27     sentinels = {}\n\nNameError: name 'trax' is not defined",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "L3 - T5"
    ]
  },
  {
    "objectID": "notes/c4w3/lab03.html#2",
    "href": "notes/c4w3/lab03.html#2",
    "title": "Assignment 3 Ungraded Sections - Part 2: T5 SQuAD Model",
    "section": "Part 2: Fine-tuning on SQuAD",
    "text": "Part 2: Fine-tuning on SQuAD\nNow let’s try to fine tune on SQuAD and see what becomes of the model.For this, we need to write a function that will create and process the SQuAD tf.data.Dataset. Below is how T5 pre-processes SQuAD dataset as a text2text example. Before we jump in, we will have to first load in the data.\n\n2.1 Loading in the data and preprocessing\nYou first start by loading in the dataset. The text2text example for a SQuAD example looks like:\n{\n  'inputs': 'question: &lt;question&gt; context: &lt;article&gt;',\n  'targets': '&lt;answer_0&gt;',\n}\nThe squad pre-processing function takes in the dataset and processes it using the sentencePiece vocabulary you have seen above. It generates the features from the vocab and encodes the string features. It takes on question, context, and answer, and returns “question: Q context: C” as input and “A” as target.\n\n# Retrieve Question, C, A and return \"question: Q context: C\" as input and \"A\" as target.\ndef squad_preprocess_fn(dataset, mode='train'):\n    return t5.data.preprocessors.squad(dataset)\n\n\n# train generator, this takes about 1 minute\ntrain_generator_fn, eval_generator_fn = trax.data.tf_inputs.data_streams(\n  'squad/plain_text:1.0.0',\n  data_dir='data/',\n  bare_preprocess_fn=squad_preprocess_fn,\n  input_name='inputs',\n  target_name='targets'\n)\n\ntrain_generator = train_generator_fn()\nnext(train_generator)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[4], line 2\n      1 # train generator, this takes about 1 minute\n----&gt; 2 train_generator_fn, eval_generator_fn = trax.data.tf_inputs.data_streams(\n      3   'squad/plain_text:1.0.0',\n      4   data_dir='data/',\n      5   bare_preprocess_fn=squad_preprocess_fn,\n      6   input_name='inputs',\n      7   target_name='targets'\n      8 )\n     10 train_generator = train_generator_fn()\n     11 next(train_generator)\n\nNameError: name 'trax' is not defined\n\n\n\n\n#print example from train_generator\n(inp, out) = next(train_generator)\nprint(inp.decode('utf8').split('context:')[0])\nprint()\nprint('context:', inp.decode('utf8').split('context:')[1])\nprint()\nprint('target:', out.decode('utf8'))\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[5], line 2\n      1 #print example from train_generator\n----&gt; 2 (inp, out) = next(train_generator)\n      3 print(inp.decode('utf8').split('context:')[0])\n      4 print()\n\nNameError: name 'train_generator' is not defined\n\n\n\n\n\n2.2 Decoding from a fine-tuned model\nYou will now use an existing model that we trained for you. You will initialize, then load in your model, and then try with your own input.\n\n# Initialize the model \nmodel = trax.models.Transformer(\n    d_ff = 4096,\n    d_model = 1024,\n    max_len = 2048,\n    n_heads = 16,\n    dropout = 0.1,\n    input_vocab_size = 32000,\n    n_encoder_layers = 24,\n    n_decoder_layers = 24,\n    mode='predict')  # Change to 'eval' for slow decoding.\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[6], line 2\n      1 # Initialize the model \n----&gt; 2 model = trax.models.Transformer(\n      3     d_ff = 4096,\n      4     d_model = 1024,\n      5     max_len = 2048,\n      6     n_heads = 16,\n      7     dropout = 0.1,\n      8     input_vocab_size = 32000,\n      9     n_encoder_layers = 24,\n     10     n_decoder_layers = 24,\n     11     mode='predict')  # Change to 'eval' for slow decoding.\n\nNameError: name 'trax' is not defined\n\n\n\n\n# load in the model\n# this will take a minute\nshape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)\nmodel.init_from_file('model_squad.pkl.gz',\n                     weights_only=True, input_signature=(shape11, shape11))\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[7], line 3\n      1 # load in the model\n      2 # this will take a minute\n----&gt; 3 shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)\n      4 model.init_from_file('model_squad.pkl.gz',\n      5                      weights_only=True, input_signature=(shape11, shape11))\n\nNameError: name 'trax' is not defined\n\n\n\n\n# create inputs\n# a simple example \n# inputs = 'question: She asked him where is john? context: John was at the game'\n\n# an extensive example\ninputs = 'question: What are some of the colours of a rose? context: A rose is a woody perennial flowering plant of the genus Rosa, in the family Rosaceae, or the flower it bears.There are over three hundred species and tens of thousands of cultivars. They form a group of plants that can be erect shrubs, climbing, or trailing, with stems that are often armed with sharp prickles. Flowers vary in size and shape and are usually large and showy, in colours ranging from white through yellows and reds. Most species are native to Asia, with smaller numbers native to Europe, North America, and northwestern Africa. Species, cultivars and hybrids are all widely grown for their beauty and often are fragrant.'\n\n\n# tokenizing the input so we could feed it for decoding\nprint(tokenize(inputs))\ntest_inputs = tokenize(inputs) \n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[9], line 2\n      1 # tokenizing the input so we could feed it for decoding\n----&gt; 2 print(tokenize(inputs))\n      3 test_inputs = tokenize(inputs) \n\nCell In[2], line 13, in tokenize(s)\n     12 def tokenize(s):\n---&gt; 13     return next(trax.data.tokenize(\n     14         iter([s]),\n     15         vocab_type='sentencepiece',\n     16         vocab_file='sentencepiece.model',\n     17         vocab_dir='.'))\n\nNameError: name 'trax' is not defined\n\n\n\nRun the cell below to decode.\n\n\nNote: This will take some time to run\n\n# Temperature is a parameter for sampling.\n#   # * 0.0: same as argmax, always pick the most probable token\n#   # * 1.0: sampling from the distribution (can sometimes say random things)\n#   # * values inbetween can trade off diversity and quality, try it out!\noutput = decoding.autoregressive_sample(model, inputs=np.array(test_inputs)[None, :],\n                                        temperature=0.0, max_length=5) # originally max_length=10\nprint(wrapper.fill(pretty_decode(output[0])))\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[10], line 5\n      1 # Temperature is a parameter for sampling.\n      2 #   # * 0.0: same as argmax, always pick the most probable token\n      3 #   # * 1.0: sampling from the distribution (can sometimes say random things)\n      4 #   # * values inbetween can trade off diversity and quality, try it out!\n----&gt; 5 output = decoding.autoregressive_sample(model, inputs=np.array(test_inputs)[None, :],\n      6                                         temperature=0.0, max_length=5) # originally max_length=10\n      7 print(wrapper.fill(pretty_decode(output[0])))\n\nNameError: name 'decoding' is not defined\n\n\n\nYou should also be aware that the quality of the decoding is not very good because max_length was downsized from 10 to 5 so that this runs faster within this environment. The colab version uses the original max_length so check that one for the actual decoding.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "L3 - T5"
    ]
  },
  {
    "objectID": "reviews/book/seret-life-of-pronouns/index.html",
    "href": "reviews/book/seret-life-of-pronouns/index.html",
    "title": "The Secret Life of Pronouns What Our Words Say About Us",
    "section": "",
    "text": "cover"
  },
  {
    "objectID": "reviews/book/seret-life-of-pronouns/index.html#podcast",
    "href": "reviews/book/seret-life-of-pronouns/index.html#podcast",
    "title": "The Secret Life of Pronouns What Our Words Say About Us",
    "section": "Podcast",
    "text": "Podcast\n\n\n\n\n\n\n\n\n\nThe secret life of pronouns in context\n\n\n\nI got interested in this book when I was deep into wikipedia research. I was in a place where I wanted to be able to attribute content to the authors. I had read some disturbing articles with prior work on the topic by some researchers from my Alma Mater. However, I eventually came across this book and it seemed to put the ideas in a better form. Less about ML and more about the type of writers and their personalities.\n\n\nIn (Pennebaker 2013) “The Secret Life of Pronouns: What Our Words Say About Us,” the author explores how the seemingly innocuous function words we use—pronouns, articles, prepositions, and auxiliary verbs—can reveal a great deal about our personalities, emotions, social connections, and psychological states. James W. Pennebaker, the author and a social psychologist, uses computer-based text analysis methods to uncover these hidden patterns."
  },
  {
    "objectID": "reviews/book/seret-life-of-pronouns/index.html#outline",
    "href": "reviews/book/seret-life-of-pronouns/index.html#outline",
    "title": "The Secret Life of Pronouns What Our Words Say About Us",
    "section": "Outline",
    "text": "Outline\n\nKey Questions and Themes:\n\nCan language reveal psychological states? The author is primarily interested in what people’s words say about their psychological states. The book investigates whether language features can reveal how people are thinking and whether this information can be used to change their thinking.\nHow do function words differ from content words? The book distinguishes between content and style of speech, arguing that function words, which reflect language style, reveal more about a person than content words. Function words are tied to the relationship between speaker and listener.\nDo men and women use words differently? The book explores gender differences in language use, noting that women are typically more attentive to the thoughts and feelings of others, while men are less so. It also investigates whether different writing styles reflect different personalities.\nCan language predict behavior? The book examines whether language use can predict behaviors such as success in college, risk of suicide, leadership styles, and likelihood of declaring war.\nHow can language be used as a tool for change? The author examines whether analyzing words can lead to improvements in relationships, teaching, and leadership. Additionally, the book looks at how writing about emotional upheavals can improve mental and physical health, and whether using certain types of words can influence the effectiveness of that writing.\nCan language reveal deception? The book explores linguistic markers of deception, such as the use of first-person singular pronouns and the mention of other individuals.\nCan language analysis help identify authors? The book presents methods for identifying authors using function words, punctuation, and obscure words.\n\nMain Examples and Studies:\n\nExpressive Writing: Early research focused on the benefits of writing about traumatic experiences, leading to the development of the LIWC (Linguistic Inquiry and Word Count) program. This program helps analyze word categories associated with healthy writing such as positive emotion words, cognitive words, and the ability to construct a story.\nThe Bottle and the Two People Pictures: Participants described pictures, and their descriptions were analyzed for function word use and content. For example, people who focused on shadows in the bottle picture tended to be more creative. The second picture revealed common themes such as social connections, clothing, and accessories.\nThinking Styles: The book describes how people’s function words can be statistically grouped into categories reflecting thinking styles, such as formal, analytic, and narrative. These thinking styles can predict how people relate to others and organize their worlds.\n9/11 Blog Analysis: The language used in blog posts was analyzed before and after the 9/11 attacks. Researchers noted an increase in negative emotion words, a decrease in positive emotion words, and a temporary increase in cognitive words.\nCollege Admissions Essays: The study examined whether the writing style in college admissions essays could predict college grades.\nThe Federalist Papers: The book discusses the use of function words to determine the authorship of the anonymous Federalist Papers.\nLanguage Style Matching (LSM): LSM is a technique to determine how similarly two people use function words. It is used in studies of couples, speed dating, and even the writings of historical figures to analyze the relationship between the authors.\nObama’s Pronoun Use: The book analyzes Barack Obama’s use of first-person singular pronouns. His relatively low use of I-words is interpreted as a sign of self-confidence.\n\nAdditional Insights:\n\nStealth Words: The book emphasizes the importance of function words, often called “stealth words,” which are often overlooked.\nThe Role of Computers: Computer technology has revolutionized the study of language, allowing researchers to analyze large datasets and uncover hidden patterns that would be impossible to see with manual analysis.\nLanguage as a Tool: Language can be used not only as a window into the inner workings of people, but also as a tool for guiding thinking and promoting change.\nInterdisciplinary Approach: The author’s work draws on multiple disciplines including psychology, linguistics, computer science, history, and political science."
  },
  {
    "objectID": "reviews/paper/exposing-glitches/index.html",
    "href": "reviews/paper/exposing-glitches/index.html",
    "title": "Exposing Attention Glitches with Flip-Flop Language Modeling",
    "section": "",
    "text": "Litrature review"
  },
  {
    "objectID": "reviews/paper/exposing-glitches/index.html#podcast",
    "href": "reviews/paper/exposing-glitches/index.html#podcast",
    "title": "Exposing Attention Glitches with Flip-Flop Language Modeling",
    "section": "Podcast",
    "text": "Podcast"
  },
  {
    "objectID": "reviews/paper/exposing-glitches/index.html#abstract",
    "href": "reviews/paper/exposing-glitches/index.html#abstract",
    "title": "Exposing Attention Glitches with Flip-Flop Language Modeling",
    "section": "Abstract",
    "text": "Abstract\n\nWhy do large language models sometimes output factual inaccuracies and exhibit erroneous reasoning? The brittleness of these models, particularly when executing long chains of reasoning, currently seems to be an inevitable price to pay for their advanced capabilities of coherently synthesizing knowledge, pragmatics, and abstract thought. Towards making sense of this fundamentally unsolved problem, this work identifies and analyzes the phenomenon of attention glitches, in which the Transformer architecture’s inductive biases intermittently fail to capture robust reasoning. To isolate the issue, we introduce flip-flop language modeling (FFLM), a parametric family of synthetic benchmarks designed to probe the extrapolative behavior of neural language models. This simple generative task requires a model to copy binary symbols over long-range dependencies, ignoring the tokens in between. We find that Transformer FFLMs suffer from a long tail of sporadic reasoning errors, some of which we can eliminate using various regularization techniques. Our preliminary mechanistic analyses show why the remaining errors may be very difficult to diagnose and resolve. We hypothesize that attention glitches account for (some of) the closed-domain hallucinations in natural LLMs. – (Liu et al. 2023)"
  },
  {
    "objectID": "reviews/paper/exposing-glitches/index.html#outline",
    "href": "reviews/paper/exposing-glitches/index.html#outline",
    "title": "Exposing Attention Glitches with Flip-Flop Language Modeling",
    "section": "Outline",
    "text": "Outline\n\nIntroduction\n\nDescribes the problem of factual inaccuracies and erroneous reasoning in large language models (LLMs), particularly in long chains of reasoning.\nPresents integer addition problems as a simple example of algorithmic reasoning where LLMs exhibit sporadic errors, highlighting both their capabilities and limitations.\nIntroduces the concept of “attention glitches” as a potential explanation for these errors, suggesting that the Transformer architecture’s inductive biases may intermittently fail to capture robust reasoning.\n\nFlip-flop Automata and the FFLM Task\n\nDefines flip-flop strings and flip-flop languages, focusing on a canonical family parameterized by the probabilities of write, read, and ignore instructions.\nIntroduces the flip-flop language modeling (FFLM) task, which involves training language models to generate or predict continuations of flip-flop strings, emphasizing the importance of perfect read operation accuracy.\nDiscusses the rationale for focusing on flip-flops, highlighting their role as fundamental building blocks of memory and their relevance to various reasoning tasks.\n\nAttention Glitches: A Long Tail of Errors for Transformer FFLMs\n\nPresents the main empirical result: Transformer models trained on FFLM exhibit a long tail of unpredictable reasoning errors (attention glitches), even on simple tasks like remembering one bit.\nHighlights the contrast between Transformers and LSTMs, showing that LSTMs achieve perfect accuracy on FFLM with significantly fewer resources.\nNotes that similar attention glitches are observed in real LLMs when prompted to complete natural language embeddings of flip-flop tasks.\nDiscusses multiple potential mechanisms for attention glitches, including implicit n-gram models, Lipschitz limitations of soft attention, and the difficulty of non-commutative tiebreaking.\n\nMitigations for Attention Glitches\n\nInvestigates various approaches to eliminate attention glitches in Transformer FFLMs, using a 6-layer 19M-parameter model as a canonical baseline.\nDiscusses the effects of training data and scale, showing that training on rare sequences significantly reduces errors, while resource scaling provides weaker improvements.\nExplores indirect algorithmic controls, including standard regularization techniques and attention-sharpening regularizers, finding that some choices improve extrapolation but none completely eliminate glitches.\nPresents a preliminary mechanistic study of trained networks, showing that attention-sharpening promotes hard attention but errors persist due to the complexity and redundancy of attention patterns.\n\nConclusion and Future Challenges\n\nSummarizes the findings, emphasizing that attention glitches represent a systematic architectural flaw in Transformers that may contribute to closed-domain hallucinations in natural LLMs.\nDiscusses the challenges of confirming or refuting the hypothesis that attention glitches cause hallucinations in natural LLMs, highlighting the need for further research.\nSuggests potential paths to hallucination-free Transformers, including data diversity, scale, regularization, and architectural innovations inspired by recurrent models.\nMentions the broader impacts and limitations of the work, emphasizing its foundational nature and the potential for unintended consequences of improved factual reliability in LLMs.\n\nAppendix\n\nProvides deferred background information on flip-flop terminology and history, including the definition of the flip-flop automaton and its transformation monoid.\nDiscusses additional related work on hallucinations, long-range dependencies, explicit memory mechanisms, and Transformers’ performance on algorithmic tasks.\nExplains the rationale for the specific flip-flop language used in the study, highlighting its compatibility with standard language modeling and its parsimonious encoding.\nElaborates on the hypothesis that attention glitches cause hallucinations in natural LLMs, discussing the challenges of formalizing and testing this hypothesis.\nPresents full experimental results, including details for LLM addition prompts, extrapolation failures of standard Transformers, effects of training data and scale, indirect algorithmic controls, and preliminary mechanistic studies.\nProvides proofs for propositions related to the realizability of FFL by small Transformers, the failure of soft attention due to attention dilution, and the failure of hard attention due to bad margins for positional embeddings.\nNotes the software, compute infrastructure, and resource costs associated with the experiments."
  },
  {
    "objectID": "reviews/paper/exposing-glitches/index.html#the-paper",
    "href": "reviews/paper/exposing-glitches/index.html#the-paper",
    "title": "Exposing Attention Glitches with Flip-Flop Language Modeling",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper"
  },
  {
    "objectID": "reviews/paper/transforer-to-rnn/index.html",
    "href": "reviews/paper/transforer-to-rnn/index.html",
    "title": "Finetuning Pretrained Transformers into RNNs",
    "section": "",
    "text": "Litrature review\n\n\n\n\n\n\n\nVideo 1: Transformer to RNN (T2RNN) Part-1 by Dr. Niraj Kumar\n\n\n\n\n\n\n\n\nVideo 2: Transformer to RNN (T2RNN) Part-2 by Dr. Niraj Kumar"
  },
  {
    "objectID": "reviews/paper/transforer-to-rnn/index.html#abstract",
    "href": "reviews/paper/transforer-to-rnn/index.html#abstract",
    "title": "Finetuning Pretrained Transformers into RNNs",
    "section": "Abstract",
    "text": "Abstract\n\nTransformers have outperformed recurrent neural networks (RNNs) in natural language generation. But this comes with a significant computational cost, as the attention mechanism’s complexity scales quadratically with sequence length. Efficient transformer variants have received increasing interest in recent works. Among them, a linear-complexity recurrent variant has proven well suited for autoregressive generation. It approximates the softmax attention with randomized or heuristic feature maps, but can be difficult to train and may yield suboptimal accuracy. This work aims to convert a pretrained transformer into its efficient recurrent counterpart, improving efficiency while maintaining accuracy. Specifically, we propose a swap-then-finetune procedure: in an off-the-shelf pretrained transformer, we replace the softmax attention with its linear-complexity recurrent alternative and then finetune. With a learned feature map, our approach provides an improved tradeoff between efficiency and accuracy over the standard transformer and other recurrent variants. We also show that the finetuning process has lower training cost relative to training these recurrent variants from scratch. As many models for natural language tasks are increasingly dependent on large-scale pretrained transformers, this work presents a viable approach to improving inference efficiency without repeating the expensive pretraining process. – []"
  },
  {
    "objectID": "reviews/paper/transforer-to-rnn/index.html#the-paper",
    "href": "reviews/paper/transforer-to-rnn/index.html#the-paper",
    "title": "Finetuning Pretrained Transformers into RNNs",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper"
  },
  {
    "objectID": "reviews/paper/2015-LSH/index.html",
    "href": "reviews/paper/2015-LSH/index.html",
    "title": "Practical and Optimal LSH for Angular Distance",
    "section": "",
    "text": "Litrature review\n\n\n\n\n\n\n\nVideo 1: Locality-Sensitive Hashing and Beyond\n\n\n\n\n\n\n\n\nVideo 2: Beyond Locality Sensitive Hashing; Alexandr Andoni\nIn the NLP specialization we have covered and used LSH a number of times in at least two courses.  In one sense I have an understanding of what is going on when we implement this. In another sense this seems to be quite confusing. So I don’t fully understand some aspects of LSH.\nOne way to do better is to try and explain it to someone else. This is what I am trying to do here by going back to the source and trying to understand the paper, problems, motivation and finally isolate what it is that is hard to grasp.\nThis paper is a good introduction to LSH for the angular distance.\nIn (Andoni et al. 2015) the authors delve into Locality-Sensitive Hashing (LSH) for the angular distance. They show the existence of an LSH family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal running time exponent. Unlike earlier algorithms with this property (e.g., Spherical LSH), their algorithm is also practical, improving upon the well-studied hyperplane LSH in practice.\nFor now the two videos which explain in some depth this an the related paper - Optimal Data-Dependent Hashing for Approximate Near Neighbors will have to do.\nTime permitting I will try and dive deeper into this paper."
  },
  {
    "objectID": "reviews/paper/2015-LSH/index.html#podcast",
    "href": "reviews/paper/2015-LSH/index.html#podcast",
    "title": "Practical and Optimal LSH for Angular Distance",
    "section": "Podcast",
    "text": "Podcast"
  },
  {
    "objectID": "reviews/paper/2015-LSH/index.html#abstract",
    "href": "reviews/paper/2015-LSH/index.html#abstract",
    "title": "Practical and Optimal LSH for Angular Distance",
    "section": "Abstract",
    "text": "Abstract\n\nWe show the existence of a Locality-Sensitive Hashing (LSH) family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal running time exponent. Unlike earlier algorithms with this property (e.g., Spherical LSH [1 , 2]), our algorithm is also practical, improving upon the well-studied hyperplane LSH [3] in practice.\nWe also introduce a multiprobe version of this algorithm, and conduct experimental evaluation on real and synthetic data sets.\nWe complement the above positive results with a fine-grained lower bound for the quality of any LSH family for angular distance. Our lower bound implies that the above LSH family exhibits a trade-off between evaluation time and quality that is close to optimal for a natural class of LSH functions. – (Andoni et al. 2015)"
  },
  {
    "objectID": "reviews/paper/2015-LSH/index.html#the-paper",
    "href": "reviews/paper/2015-LSH/index.html#the-paper",
    "title": "Practical and Optimal LSH for Angular Distance",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper"
  },
  {
    "objectID": "reviews/paper/effective/index.html",
    "href": "reviews/paper/effective/index.html",
    "title": "Effective Approaches to Attention-based Neural Machine Translation",
    "section": "",
    "text": "Literature review\nThis review is a bit of a mess, it has gone through at least three editions of this blog and I have learned much about writing and structuring a review since then. I think it needs a bit of an overhaul. This latest version is a step in the right direction.\nThere are many other review of this paper but I think that covering this paper is highly relevant followup to the assignments for NMT as well as the earlier assignment in for MT with KNN in the Classification and Vector Space Models course."
  },
  {
    "objectID": "reviews/paper/effective/index.html#podcast",
    "href": "reviews/paper/effective/index.html#podcast",
    "title": "Effective Approaches to Attention-based Neural Machine Translation",
    "section": "Podcast",
    "text": "Podcast"
  },
  {
    "objectID": "reviews/paper/effective/index.html#effective-approaches-to-attention-based-neural-machine-translation",
    "href": "reviews/paper/effective/index.html#effective-approaches-to-attention-based-neural-machine-translation",
    "title": "Effective Approaches to Attention-based Neural Machine Translation",
    "section": "Effective Approaches to Attention-based Neural Machine Translation",
    "text": "Effective Approaches to Attention-based Neural Machine Translation"
  },
  {
    "objectID": "reviews/paper/effective/index.html#abstract",
    "href": "reviews/paper/effective/index.html#abstract",
    "title": "Effective Approaches to Attention-based Neural Machine Translation",
    "section": "Abstract",
    "text": "Abstract\n\nAn attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches on the WMT translation tasks between English and German in both directions.\n\n\nWith local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout. Our ensemble model using different attention architectures yields a new state-of-the-art result in the WMT’15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker. – (Luong, Pham, and Manning 2015)"
  },
  {
    "objectID": "reviews/paper/effective/index.html#dot-product-attention",
    "href": "reviews/paper/effective/index.html#dot-product-attention",
    "title": "Effective Approaches to Attention-based Neural Machine Translation",
    "section": "Dot-Product Attention",
    "text": "Dot-Product Attention\nDot-Product attention is the first of three attention mechanisms covered in the course and the simplest covered in this paper. Dot-Product Attention is a good fit, in an engineering sense, for a encoder-decoder architecture with tasks where the source source sequence is fully available at the start and the tasks is mapping or transformation the source sequence to an output sequence like in alignment, or translation.\n\n\n\n\n\n\nFigure 1: Neural machine translation – a stacking recurrent architecture for translating a source sequence A B C D into a target sequence X Y Z. Here, &lt;eos&gt; marks the end of a sentence.\n\n\n\nThe first assignment in the course using encoder decoder LSTM model with attention is so similar to the setup disused in this paper, I would not be surprised if it may well have inspired it.\n\nThis is a review of the paper in which scaled dot product attention was introduced in 2015 by Minh-Thang Luong, Hieu Pham, Christopher D. Manning in Effective Approaches to Attention-based Neural Machine Translation which is available at papers with code. In this paper they tried to take the attention mechanism being used in other tasks and to distill it to its essence and at the same time to also find a more general form.\n\n\n\n\n\n\n\n\nGlobal Attention\n\n\n\n\nFigure 2: Global attentional model – at each time step t, the model infers a variable-length alignment weight vector at based on the current target state ht and all source states ̄hs. A global contextvector ct is then computed as the weighted average, according to at, over all the source states.\n\n\n\n\n\n\n\n\n\n\n\n\nLocal attention\n\n\n\n\nFigure 3: Local attention model – the model first predicts a single aligned position p_t or the current target word. A window centered around the source position p_t is then used to compute a context vector c_t, a weighted average of the source hidden states in the window. The weights at are inferred from the current target state h_t and those source states \\bar{h}_s in the window.\n\n\n\n\n\n\n\n\n\n\n\nInput-feeding approach\n\n\n\n\nFigure 4: Input-feeding approach – Attentional vectors \\bar{h}_s are fed as inputs to the next time steps to inform the model about past alignment decisions\n\n\n\n\n\n\n\n\n\nLearning curves\n\n\n\n\nFigure 5: Learning curves – test cost (ln perplexity) on newstest2014 for English-German NMTs as training progresses.\n\n\n\n\n\n\n\n\n\nLength Analysis\n\n\n\n\nFigure 6: Length Analysis – translation qualities of different systems as sentences become longer\n\n\n\nThey also came up with a interesting way to visualize the alignment’s attention mechanism.\n\n\n\n\n\n\nalignment-visulization\n\n\n\n\nFigure 7: Alignment visualizations – shown are images of the attention weights learned by various models: (top left) global, (top right) local-m, and (bottom left) local-p. The gold alignments are displayed at the bottom right corner.\n\n\n\nSo to recap: Luong et all were focused on alignment problem in NMT. When they try to tackle it using attention as function of the content and a function of its location. They came up with a number of ways to distill and generalize the attention mechanism.\n\n\n\n\n\npage 1\n\nAttention was just another engineering technique to improve alignment and it had not yet taken center stage in the models, as it would in Attention Is All We Need (Vaswani et al. (2023)).I find it useful to garner the concepts and intuition which inspired these researchers to adapt attention and how they come up with this form of attention.\nThe abstract begins with:\n\n“An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation.”\n\nwhich was covered in last lesson. The abstract continues with:\n\n“This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time.”\n\ntalks about\n\n\n\n\n\npage 2"
  },
  {
    "objectID": "reviews/paper/effective/index.html#neural-machine-translation",
    "href": "reviews/paper/effective/index.html#neural-machine-translation",
    "title": "Effective Approaches to Attention-based Neural Machine Translation",
    "section": "§2 Neural Machine Translation:",
    "text": "§2 Neural Machine Translation:\nThis section provides a summary of the the NMT task using 4 equations: In particular they note that in the decoder the conditional probability of the target given the source is of the form: \nlog \\space p(y \\vert x) = \\sum_{j=1}^m log \\space p (y_j \\vert y_{&lt;j} , s)\n\nWhere x_i are the source sentence and y_i are the target sentence. \np (y_j \\vert y{&lt;j} , s) = softmax (g(h_j))\n\nHere, h_j is the RNN hidden unit, abstractly computed as: \nh_j = f(h_{j-1},s)\n\nOur training objective is formulated as follows \nJ_t=\\sum_{(x,y)\\in D} -log \\space p(x \\vert y)\n\nWith D being our parallel training corpus."
  },
  {
    "objectID": "reviews/paper/effective/index.html#overview-of-attention",
    "href": "reviews/paper/effective/index.html#overview-of-attention",
    "title": "Effective Approaches to Attention-based Neural Machine Translation",
    "section": "§3 Overview of attention",
    "text": "§3 Overview of attention\n\n\n\n\npage 3\n\nNext they provide a recap of the attention mechanism to set their starting point: &gt;Specifically, given the target hidden state h_t and the source-side context vector c_t, we employ a simple concatenation layer to combine the information from both vectors to produce an attentional hidden state as follows:\n\n\\bar{h}_t = tanh(W_c[c_t;h_t])\n\n\nThe attentional vector \\bar{h}_t is then fed through the softmax layer to produce the predictive distribution formulated as:\n\n\np(y_t|y{&lt;t}, x) = softmax(W_s\\bar{h}_t)"
  },
  {
    "objectID": "reviews/paper/effective/index.html#global-attention",
    "href": "reviews/paper/effective/index.html#global-attention",
    "title": "Effective Approaches to Attention-based Neural Machine Translation",
    "section": "§3.1 Global attention",
    "text": "§3.1 Global attention\nThis is defined in §3.1 of the paper as:\n\n\\begin{align}\n   a_t(s) & = align(h_t,\\bar{h}_s)  \\newline\n   & = \\frac{ e^{score(h_t,\\bar{h}_s)} }{ \\sum_{s'} e^{score(h_t,\\bar{h}_s)} } \\newline\n   & = softmax(score(h_t,\\bar{h}_s))\n\\end{align}\n\nwhere h_t and h_s are the target and source sequences and score() which is referred to as a content-based function as one of three alternative forms provided:\n\nDot product attention:\n\nscore(h_t,\\bar{h}_s)=h_t^T\\bar{h}_s\n This form combines the source and target using a dot product. Geometrically this essentially a projection operation.\n\n\nGeneral attention:\n\nscore(h_t,\\bar{h}_s)=h_t^TW_a\\bar{h}_s\n\nthis form combines the source and target using a dot product after applying a learned attention weights to the source. Geometrically this is a projection of the target on a linear transformation of the source or scaled dot product attention as it is now known\n\n\nConcatenative attention:\n\nscore(h_t,\\bar{h}_s)=v_a^T tanh(W_a [h_t;\\bar{h}_s])\n\nThis is a little puzzling v_a^T is not accounted for and seems to be a learned attention vector which is projected onto the linearly weighted combination of the hidden states of the encoder and decoder. they also mention having considered using a location based function location :\n\na_t = softmax(W_a h_t)\n\nwhich is just a linear transform of the hidden target state h_t"
  },
  {
    "objectID": "reviews/paper/effective/index.html#local-attention",
    "href": "reviews/paper/effective/index.html#local-attention",
    "title": "Effective Approaches to Attention-based Neural Machine Translation",
    "section": "§3.2 Local Attention",
    "text": "§3.2 Local Attention\n\n\n\n\npage 4\n\nin §3.2 they consider a local attention mechanism. This is a resource saving modification of global attention using the simple concept of applying the mechanism within a fixed sized window.\n\nWe propose a local attentional mechanism that chooses to focus only on a small subset of the source positions per target word. This model takes inspiration from the tradeoff between the soft and hard attentional models proposed by Xu et al. (2015) to tackle the image caption generation task.\n\n\nOur local attention mechanism selectively focuses on a small window of context and is differentiable. … In concrete details, the model first generates an aligned position p_t for each target word at time t. The context vector c_t\n\nis then derived as a weighted average over the set of source hidden states within the window [p_t−D, p_t+D]; Where D is empirically selected. The big idea here is to use a fixed window size for this step to conserve resources when translating paragraphs or documents - a laudable notion for times where LSTM gobbled up resources in proportion to the sequence length…\nThey also talk about monotonic alignment where p_t=t and predictive alignment\n\np_t=S\\cdot sigmoid(v_p^Ttanh(W_ph_t))\n\n\na_t(s)=align(h_t,\\bar{h}_s)e^{(-\\frac{(s-p_t)^2}{s\\sigma^2})}\n\nwith align() as defined above and\n\n\\sigma=\\frac{D}{2}\n\n\n\n\n\npage 5\n\nI found the rest of the paper lesser interest\n\n\n\n\npage 6\n\n\n\n\npage 7\n\n\n\n\npage 8\n\n\n\nIn §5.4 In alignment quality\n\n\n\n\npage 9\n\nsome sample translations\n\n\n\n\npage 10\n\nthe references\n\n\n\n\npage 11\n\nThis is appendix A which shows the visualization of alignment weights."
  },
  {
    "objectID": "reviews/paper/effective/index.html#outline",
    "href": "reviews/paper/effective/index.html#outline",
    "title": "Effective Approaches to Attention-based Neural Machine Translation",
    "section": "Outline",
    "text": "Outline\n\n\nIntroduction\n\nLists the advantages of neural machine translation (NMT)\n\nMinimal domain knowledge\nConceptual simplicity\nAbility to generalize well to long sequences\nSmall memory footprint\nEasy implementation of decoders\n\nDiscusses the concept of attention in neural networks\nMentions different applications of attention in different tasks\nHighlights the application of attention mechanism in NMT by Bahdanau et al. (2015) and the lack of further exploration\nPresents the purpose of the paper, which is to design two novel types of attention-based models\n\nA global approach\nA local approach\n\nPresents the experimental evaluation of the proposed approaches on WMT translation tasks and its analysis\n\nNeural Machine Translation\n\nDescribes the conditional probability of translating a source sentence to a target sentence\nPresents the two components of a basic NMT system an Encoder and a Decoder\nDiscusses the use of recurrent neural network (RNN) architectures in NMT\nPresents the parameterization of the probability of decoding each word in the target sentence\nPresents the training objective used in NMT\n\nAttention-based Models\n\nClassifies the various attention-based models into two broad categories\n\nGlobal attention\nLocal attention\n\nPresents the common process followed by both global and local attention models for deriving the context vector\nDescribes the concatenation of target hidden state and source-side context vector for prediction\n\nGlobal Attention\n\nDescribes the concept of global attention model\nPresents the derivation of a variable-length alignment vector by comparing the current target hidden state with each source hidden state\nPresents three different alternatives for the content-based function used in calculating the alignment vector\nDescribes the location-based function used in early attempts to build attention-based models\nDescribes the calculation of the context vector as the weighted average over all the source hidden states\nPresents a comparison of the proposed global attention approach to the model by Bahdanau et al. (2015)\n\nSimpler architecture\nSimpler computation path\nUse of multiple alignment functions\n\n\nLocal Attention\n\nDescribes the concept of local attention model\nMentions the inspiration from the soft and hard attentional models\nDescribes the selection of a small window of context and its advantages over soft and hard attention models\nPresents two variants of the local attention model\n\nMonotonic alignment\nPredictive alignment\n\nDescribes the comparison to the selective attention mechanism by Gregor et al. (2015)\n\nInput-feeding Approach\n\nDescribes the suboptimal nature of making independent attentional decisions in the global and local approaches\nDiscusses the need for joint alignment decisions taking into account past alignment information\nPresents the input-feeding approach\nMentions the effects of input-feeding approach\n\nMakes the model fully aware of previous alignment choices\nCreates a deep network spanning both horizontally and vertically\n\nPresents the comparison to other related works\n\nUse of context vectors by Bahdanau et al. (2015)\nDoubly attentional approach by Xu et al. (2015)\n\n\nExperiments\n\nDescribes the evaluation setup and datasets used\n\nnewstest2013 as development set\nnewstest2014 and newstest2015 as test sets\n\nMentions the use of case-sensitive BLEU for reporting translation performances\nDescribes the two types of BLEU used\n\nTokenized BLEU\nNIST BLEU\n\n\nTraining Details\n\nDescribes the data used for training NMT systems\n\nWMT’14 training data\n\nPresents the details of vocabulary size and filtering criteria used\nDiscusses the architecture of the LSTM models and training settings\nMentions the training speed and time\n\nEnglish-German Results\n\nDiscusses the different systems used for comparison\nPresents the progressive improvements achieved by\n\nReversing the source sentence\nUsing dropout\nUsing global attention approach\nUsing input-feeding approach\nUsing local attention model with predictive alignments\n\nNotes the correlation between perplexity and translation quality\nMentions the use of unknown replacement technique and the achievement of new SOTA result by ensembling 8 different models\nDescribes the results of testing the models on newstest2015 and the establishment of new SOTA performance\n\nGerman-English Results\n\nMentions the evaluation setup for the German-English translation task\nPresents the results highlighting the effectiveness of\n\nAttentional mechanism\nInput-feeding approach\nContent-based dot product function with dropout\nUnknown word replacement technique\n\n\nAnalysis\n\nDescribes the purpose of conducting extensive analysis\n\nUnderstanding of the learning process\nAbility to handle long sentences\nChoice of attentional architectures\nAlignment quality\n\n\nLearning Curves\n\nPresents the analysis of the learning curves for different models\nNotes the separation between non-attentional and attentional models\nBriefly mentions the effectiveness of input-feeding approach and local attention models\n\nEffects of Translating Long Sentences\n\nBriefly discusses the grouping of sentences based on lengths and computation of BLEU score per group\nMentions the effectiveness of attentional models in handling long sentences\nNotes the superior performance of the best model across all sentence length buckets\n\nChoices of Attentional Architectures\n\nPresents the analysis of different attention models and alignment functions\nHighlights the poor performance of the location-based function\nBriefly mentions the performance of content-based functions\n\nGood performance of dot function for global attention\nBetter performance of general function for local attention\n\nNotes the best performance of local attention model with predictive alignments\n\nAlignment Quality\n\nBriefly discusses the use of alignment error rate (AER) metric to evaluate the alignment quality\nMentions the data used for evaluating alignment quality and the process of extracting one-to-one alignments\nPresents the results of AER evaluation and the comparison to Berkeley aligner\nNotes the better performance of local attention models compared to the global one\nBriefly discusses the AER of the ensemble"
  },
  {
    "objectID": "reviews/paper/effective/index.html#the-paper",
    "href": "reviews/paper/effective/index.html#the-paper",
    "title": "Effective Approaches to Attention-based Neural Machine Translation",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper"
  },
  {
    "objectID": "reviews/paper/floating-contraints/index.html",
    "href": "reviews/paper/floating-contraints/index.html",
    "title": "Floating Constraints in Lexical Choice",
    "section": "",
    "text": "Litrature review\n\n\n\n\n\n\n\nVideo 1"
  },
  {
    "objectID": "reviews/paper/floating-contraints/index.html#context",
    "href": "reviews/paper/floating-contraints/index.html#context",
    "title": "Floating Constraints in Lexical Choice",
    "section": "Context",
    "text": "Context\nThis is one of the three paper mentioned by Kathleen McKeown in her heroes of NLP interview by Andrew NG. The paper is about the floating constraints in lexical choice.\nLooking quickly over it I noticed a couple of things:\n(article?){mckeown1997floating, title={Floating constraints in lexical choice}, author={McKeown, Kathleen and Elhadad, Michael and Robin, Jacques}, year={1997} }\n\nI was familiar with Elhadad’s work on generation. I had read his work on FUF Functional Unification Formalism in the 90s which introduced me to the paradigm of unification before I even knew about Prolog or logic programming.\nI found it fascinating that McKeown and Elhadad had worked together on this paper and even a bit curious about what they were looking into here.\nSince McKeown brought it up, it might intimate that there is something worth looking into here. And it is discusing aspects of generative language models."
  },
  {
    "objectID": "reviews/paper/floating-contraints/index.html#abstract",
    "href": "reviews/paper/floating-contraints/index.html#abstract",
    "title": "Floating Constraints in Lexical Choice",
    "section": "Abstract",
    "text": "Abstract\n\nLexical choice is a computationally complex task, requiring a generation system to consider a potentially large number of mappings between concepts and words. Constraints that aid in determining which word is best come from a wide variety of sources, including syntax, semantics, pragmatics, the lexicon, and the underlying domain. Furthermore, in some situations, different constraints come into play early on, while in others, they apply much later. This makes it difficult to determine a systematic ordering in which to apply constraints. In this paper, we present a general approach to lexical choice that can handle multiple, interacting constraints. We focus on the problem of floating constraints, semantic or pragmatic constraints that float, appearing at a variety of different syntactic ranks, often merged with other semantic constraints. This means that multiple content units can be realized by a single surface element, and conversely, that a single content unit can be realized by a variety of surface elements. Our approach uses the Functional Unification Formalism (FuF) to represent a generation lexicon, allowing for declarative and compositional representation of individual constraints. - (mckeown1997floating?)"
  },
  {
    "objectID": "reviews/paper/floating-contraints/index.html#outline",
    "href": "reviews/paper/floating-contraints/index.html#outline",
    "title": "Floating Constraints in Lexical Choice",
    "section": "Outline",
    "text": "Outline\n\nIntroduction\n\nMentions the complexity of lexical choice in language generation.\nNotes the diverse sources of constraints on lexical choice, including syntax, semantics, lexicon, domain, and pragmatics.\nHighlights the challenges posed by floating constraints, which are semantic or pragmatic constraints that can appear at various syntactic ranks and be merged with other semantic constraints.\nPresents examples of floating constraints in sentences.\n\nAn Architecture for Lexical Choice\n\nDiscusses the role of lexical choice within a typical language generation architecture.\nArgues for placing the lexical choice module between the content planner and the surface sentence generator.\nDescribes the input and output of the lexical choice module, emphasizing the need for language-independent conceptual input.\nPresents the two tasks of lexical choice: syntagmatic1 decisions (determining thematic structure) and paradigmatic2 decisions (choosing specific words).\nIntroduces the FUF/SURGE package as the implementation environment.\n\nPhrase Planning in Lexical Choice\n\nDescribes the need for phrase planning to articulate multiple semantic relations into a coherent linguistic structure.\nExplains how the Lexical Chooser selects the head relation and builds its argument structure based on focus and perspective.\nDetails how remaining relations are attached as modifiers, considering options like embedding, subordination, and syntactic forms of modifiers.\n\nCross-ranking and Merged Realizations\n\nDiscusses the non-isomorphism between semantic and syntactic structures, necessitating merging and cross-ranking realizations.\nProvides examples of merging (multiple semantic elements realized by a single word) and cross-ranking (single semantic element realized at different syntactic ranks).\nEmphasizes the need for linguistically neutral input to support this flexibility.\nExplains the implementation of merging and cross-ranking using FUF, highlighting the challenges of search and the use of bk-class for efficient backtracking.\n\nInterlexical Constraints\n\nDefines interlexical constraints as arising from alternative sets of collocations for realizing pairs of content units.\nPresents an example of interlexical constraints with verbs and nouns in the basketball domain.\nDiscusses different strategies for encoding interlexical constraints in the Lexical Chooser.\nIntroduces the :wait mechanism in FUF to delay the choice of one collocate until the other is chosen, preserving modularity and avoiding backtracking.\n\nOther Approaches to Lexical Choice\n\nReviews other systems using FUF for lexical choice, comparing their architectures and features to ADVISOR-II.\nDiscusses non-FUF-based systems, categorizing them by their positioning of lexical choice in the generation process and analyzing their handling of various constraints.\n\nConclusion\n\nSummarizes the contributions of the paper, highlighting the ability to handle a wide range of constraints, the use of FUF for declarative representation and interaction, and the algorithm for lexical selection and syntactic structure building.\nEmphasizes the importance of syntagmatic choice and perspective selection for handling floating constraints.\nNotes the use of lazy evaluation and dependency-directed backtracking for computational efficiency.\nMentions future directions, such as developing domain-independent lexicon modules and methods for organizing large-scale lexica.\n\n\n1 of or denoting the relationship between two or more linguistic units used sequentially to make well-formed structures. The combination of ‘that handsome man + ate + some chicken’ forms a syntagmatic relationship. If the word position is changed, it also changes the meaning of the sentence, eg ’Some chicken ate the handsome man2 Paradigmatic relation describes the relationship between words that can be substituted for words with the same word class (eg replacing a noun with another noun)"
  },
  {
    "objectID": "reviews/paper/floating-contraints/index.html#reflection",
    "href": "reviews/paper/floating-contraints/index.html#reflection",
    "title": "Floating Constraints in Lexical Choice",
    "section": "Reflection",
    "text": "Reflection\n\nIn unification-based approaches, any word is a candidate unless it is ruled out by a constraint. With an LLM a word is selected based on its probability.\nThe idea of constraints is deeply embedded in Linguistics. The two type that immediately come to mind are:\n\nSubcategorization constraints which are rules that govern the selection of complements and adjuncts. These are syntactic constraints.\nSelectional constraints which are rules that govern the selection of arguments. These are semantic constraints\n\nOther constraints that I did not have to deal with as often are pragmatics and these are resolved from the state of the world or common sense knowledge.\nIn FUF there are many more constraints and if care isn’t taken, the system can become over constrained and no words will be selected.\nOne Neat aspect of the approach though is that FUF language could be merged with a query to generate a text.\nUnfortunately as fat as I recall the FUF had to be specified by hand. The only people who could do this were the people who designed the system. They needed to be expert at linguistics and logic programming. Even I was pretty sharp at the time the linguistics being referred was far too challenging. I learned that in reality every computational linguist worth their salt had their own theory of language and their own version of grammar and that FUF was just one of many.\nIt seems that today attention mechanisms within LSTMs or Transformers are capable of learning a pragmatic formalism without a linguist stepping in to specify it.\n\nWhy this is still interesting:\nImagine we want to build a low resource capable language model.\nWe would really like it to be good with grammar Also we would like it to have a lexicon that is as rich as the person it is interacting with. And we would also like to make available to it the common sense knowledge that it would need to use in the context of current and possibly a profile based on past conversations…. On the other hand we don’t want to require a 70 billion parameter model to do this. So what do we do?\nLet imagine we start by training on Wikipedia, after all wikipedia’s mission is to become the sum of all human knowledge. By do we realy need all of the information in Wikipedia in out edge model?\nThe answer is no but the real question is how can we partition the training information and the wights etc so that we have a powerful model with a basic grammar and lexicon but which can load a knowledge graph, extra lexicons and common sense knowledge as needed.\nSo I had this idea from lookin at a Ouhalla (1999). It pointed out there were phrase boundries and that if the sentence was transformed in a number of ways t would still make sense if we did this with respect to a phrase but if we used cucnks that were too small or too big the transofrmation would create malformed sentences. The operations of intrerest were movement and deletion.\nand that can still have a model that is both small but able to access this extra material.\nOne direction seems to me is that we want to apply queries to all the training material as we train the model. Based on the queries that we retrieve each clause or phrase we can decide where we want to learn it and if we want to learn it at all.\n(If it is common sense knowledge we may already know it. If it is semi structured we may want to put it into wikidata. If it is neither we may want to avoid learning it at all. We may still want to use it for improving the grammar and lexicon.) However we may want to store it in a speclised lexicon for domains like medicinee or law.\nWe could also decide if and how to eliminate it from we want to The queries should match each bit of information in the sentence. These are our queries. While the facts may change form\n\nsay we want to decompose a wikipedia article"
  },
  {
    "objectID": "reviews/paper/floating-contraints/index.html#the-paper",
    "href": "reviews/paper/floating-contraints/index.html#the-paper",
    "title": "Floating Constraints in Lexical Choice",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper"
  },
  {
    "objectID": "reviews/paper/mbr-all-the-way-dowm/index.html",
    "href": "reviews/paper/mbr-all-the-way-dowm/index.html",
    "title": "It’s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk",
    "section": "",
    "text": "Litrature review"
  },
  {
    "objectID": "reviews/paper/mbr-all-the-way-dowm/index.html#abstract",
    "href": "reviews/paper/mbr-all-the-way-dowm/index.html#abstract",
    "title": "It’s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk",
    "section": "Abstract",
    "text": "Abstract\nMinimum Bayes Risk (MBR) decoding is a method for choosing the outputs of a machine learning system based not on the output with the highest probability, but the output with the lowest risk (expected error) among multiple candidates. It is a simple but powerful method: for an additional cost at inference time, MBR provides reliable several-point improvements across metrics for a wide variety of tasks without any additional data or training. Despite this, MBR is not frequently applied in NLP works, and knowledge of the method itself is limited. We first provide an introduction to the method and the recent literature. We show that several recent methods that do not reference MBR can be written as special cases of MBR; this reformulation provides additional theoretical justification for the performance of these methods, explaining some results that were previously only empirical. We provide theoretical and empirical results about the effectiveness of various MBR variants and make concrete recommendations for the application of MBR in NLP models, including future directions in this area. – (Bertsch et al. 2023)"
  },
  {
    "objectID": "reviews/paper/mbr-all-the-way-dowm/index.html#outline",
    "href": "reviews/paper/mbr-all-the-way-dowm/index.html#outline",
    "title": "It’s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk",
    "section": "Outline",
    "text": "Outline\n\nAbstract\n\nDescribes Minimum Bayes Risk (MBR) decoding.\nPresents MBR as a widely applicable method that often improves performance over beam search and single-sample decoding.\nShows that several recent generation methods can be framed as special cases of MBR, thereby providing theoretical grounding for their empirical success.\n\nIntroduction\n\nPresents Minimum Bayes Risk (MBR) decoding as a simple yet powerful decoding method.\nDiscusses how recent methods in natural language processing (NLP) unknowingly replicate aspects of MBR.\n\nFormalization\n\nStandard decoding\n\nDescribes the standard decoding methods for autoregressive models such as greedy decoding, sampling, and beam search.\n\nMinimum Bayes Risk decoding\n\nDefines the theoretical foundation of Minimum Bayes Risk (MBR) decoding, including the concept of risk based on expected error.\nExplains how risk computation is typically approximated using Monte Carlo methods due to the intractability of calculating the full expectation.\n\n\nTaxonomy of MBR\n\nNotes four key design considerations for implementing an MBR method: the hypothesis set, the evidence set, the gain/error function, and the evidence distribution.\nSampling a hypothesis set\n\nHighlights several works that show improvements in MBR by filtering the hypothesis set to contain only higher-quality candidate outputs.\n\nSampling an evidence set\n\nBriefly discusses various sampling strategies, with most work focusing on drawing unbiased samples from the model distribution.\n\nWhat metric do we want to maximize?\n\nExplores the impact of different gain (or error) functions, noting that using a specific metric as a gain function in MBR tends to lead to improved performance on that metric.\n\nWhat probability distribution should we use to estimate risk?\n\nBriefly discusses the choice of the distribution used to estimate risk in MBR, with most methods using the model’s score distribution over outputs, and some work using alternative distributions like a human or true distribution.\n\n\nMBR as a frame for other methods\n\nHighlights the framing of self-consistency, range voting, output ensembling, and density estimation as special cases of MBR.\nSelf-consistency as MBR\n\nShows how self-consistency, a method where the most frequent answer from multiple model generations is selected, can be formulated as MBR.\nExplains that the best performing sampling strategies for self-consistency are those closest to ancestral sampling due to its unbiased estimation properties.\n\nOutput Ensembling as MBR\n\nPresents output ensembling, where a set of models is used to generate outputs and a combined output is selected, as a form of MBR with a mixture distribution.\n\nMBR as Density Estimation\n\nEstablishes the connection between MBR and kernel density estimation, noting that both can be seen as mode-seeking methods.\n\nRange Voting as MBR\n\nShows that range voting, where candidates are assigned scores by voters, can be formulated as MBR by treating candidates as hypotheses and voters as evidence.\n\n\nDesign Decisions Impact MBR Performance\n\nExamines cases where the choices made in designing an MBR method significantly affect its performance.\nExperimental Details\n\nPresents the datasets and models used for evaluating MBR in abstractive summarization and machine translation tasks.\n\nThe MBR metric matters–but perhaps not as much as the hypothesis set\n\nDemonstrates that MBR using different gain functions (ROUGE-1, BEER, BERTScore) improves abstractive summarization performance.\nNotes that the choice of hypothesis set has a more significant impact than the choice of gain function.\n\nVarying the risk distribution: lessons from beam search don’t translate to MBR\n\nInvestigates the effects of correcting for length bias in the evidence distribution used for estimating risk in MBR.\nFinds that while length correction benefits beam search, it hurts MBR performance, possibly due to a high-variance estimator of risk.\n\n\nMBR applications in NLP\n\nPresents a historical overview of MBR applications in NLP, from its early use in statistical models to its recent resurgence in neural models.\nHistorical context\n\nTraces the roots of MBR to Bayesian decision theory and its use in parsing, speech recognition, and machine translation since the 1990s.\nExplains how early MBR applications were constrained by graph-based model structures, requiring complex algorithms for exact decoding.\n\nRecent usage\n\nDiscusses the revival of MBR in neural text generation tasks, with much of the recent work focusing on machine translation.\nNotes the decline in the explicit use of the term “MBR” in favor of newer terminologies like “self-consistency.”\n\n\nConclusion\n\nDiscusses the terminology drift in NLP that leads to the renaming of MBR as different methods.\nReemphasizes the importance of connecting modern techniques with their historical roots for a better understanding of why they work.\n\nAppendix A: More details on importance sampling for MBR\n\nProvides a detailed explanation of importance sampling and its application to MBR, specifically when estimating risk under a length-corrected distribution.\n\nAppendix B: Contextualizing this work within philosophy of science\n\nExplores the broader implications of the work within the context of meta-analysis of scientific research.\nDiscusses the phenomena of citational amnesia and terminology drift in scientific literature and their possible consequences."
  },
  {
    "objectID": "reviews/paper/mbr-all-the-way-dowm/index.html#the-paper",
    "href": "reviews/paper/mbr-all-the-way-dowm/index.html#the-paper",
    "title": "It’s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog\n\nCourse notes\n\n\nIn many cases the original notes are sparse\nIt might be best to add the video transcript in a collapsed callout to the notes. This can then allow using RAG on the whole lesson in a single page.\n\nThis makes more sense of the longer videos as this is where the disparity between the notes and the video is the largest.\nIf transcripts are included, it is best to split them using titles which contain a question which the speaker is answering.\nAlso it is a good idea to link to the slide being shown if this is available.\n\n\n\nLabs.\n\n\nLabs that are very long should be broken down into smaller parts if possible.\n\n\nReflection on the material\nOther sources with better/deeper explanation for harder topics.\n\n\nStanford NLP course\nJurafsky and Martin’s book\n\n\nIt looks like for some chapter it would be near to extract the engineering aspect of the model into a code snippet or formula. But this should be kept separate from the notes as it can be distracting.\nI’ve reviewed relevant papers on NLP\nIdeally each\n\n\n\nCitationBibTeX citation:@online{bochman2025,\n  author = {Bochman, Oren},\n  title = {About},\n  date = {2025-02-06},\n  url = {https://orenbochman.github.io/notes-nlp/about.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2025. “About.” February 6, 2025. https://orenbochman.github.io/notes-nlp/about.html."
  },
  {
    "objectID": "reviews/paper/2017-attention-is-all-you-need/index.html",
    "href": "reviews/paper/2017-attention-is-all-you-need/index.html",
    "title": "Attention Is All You Need",
    "section": "",
    "text": "Litrature review"
  },
  {
    "objectID": "reviews/paper/2017-attention-is-all-you-need/index.html#abstract",
    "href": "reviews/paper/2017-attention-is-all-you-need/index.html#abstract",
    "title": "Attention Is All You Need",
    "section": "Abstract",
    "text": "Abstract\n\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data –(Vaswani et al. 2023)"
  },
  {
    "objectID": "reviews/paper/2017-attention-is-all-you-need/index.html#outline",
    "href": "reviews/paper/2017-attention-is-all-you-need/index.html#outline",
    "title": "Attention Is All You Need",
    "section": "Outline",
    "text": "Outline"
  },
  {
    "objectID": "reviews/paper/2017-attention-is-all-you-need/index.html#the-paper",
    "href": "reviews/paper/2017-attention-is-all-you-need/index.html#the-paper",
    "title": "Attention Is All You Need",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper"
  },
  {
    "objectID": "reviews/paper/mbr-decoding/index.html",
    "href": "reviews/paper/mbr-decoding/index.html",
    "title": "Generating Diverse and High-Quality Texts by Minimum Bayes Risk Decoding",
    "section": "",
    "text": "Litrature review"
  },
  {
    "objectID": "reviews/paper/mbr-decoding/index.html#abstract",
    "href": "reviews/paper/mbr-decoding/index.html#abstract",
    "title": "Generating Diverse and High-Quality Texts by Minimum Bayes Risk Decoding",
    "section": "Abstract",
    "text": "Abstract\n\nOne of the most important challenges in text generation systems is to produce outputs that are not only correct but also diverse. Recently, Minimum Bayes-Risk (MBR) decoding has gained prominence for generating sentences of the highest quality among the decoding algorithms. However, existing algorithms proposed to generate diverse outputs are predominantly based on beam search or random sampling, thus their output quality is capped by these underlying decoding algorithms. In this paper, we investigate an alternative approach – we develop diversity-promoting decoding algorithms by enforcing diversity objectives to MBR decoding. We propose two variants of MBR\n\nDiverse MBR (DMBR) that adds a diversity penalty to the decoding objective and\nk-medoids MBR (KMBR) that reformulates the decoding task as a clustering problem.\n\nWe evaluate DMBR and KMBR on a variety of directed text generation tasks using encoder-decoder models and a language model with prompting. The experimental results show that the proposed method achieves a better trade-off than the diverse beam search and sampling algorithms overall – (Jinnai et al. 2024)\n\n\ngithub code repo"
  },
  {
    "objectID": "reviews/paper/mbr-decoding/index.html#outline",
    "href": "reviews/paper/mbr-decoding/index.html#outline",
    "title": "Generating Diverse and High-Quality Texts by Minimum Bayes Risk Decoding",
    "section": "Outline",
    "text": "Outline\n\nIntroduction\n\nDescribes the importance of generating diverse and high-quality texts in various natural language processing tasks.\nHighlights the limitations of existing approaches for diverse text generation, which are typically based on beam search or random sampling.\nPresents the paper’s focus on developing diversity-promoting algorithms using Minimum Bayes Risk (MBR) decoding.\nNotes the advantages of MBR decoding over traditional methods like beam search and random sampling.\nIntroduces:\n\nDiverse MBR (DMBR) and\nk-medoids MBR (KMBR).\n\n\nBackground\n\nDefines the sequence-to-sequence generation task and the goal of decoding in finding the hypothesis that aligns with human preference.\nDiscusses the concept of set decoding, where the aim is to generate a set of sentences that maximizes both human preference and diversity.\nReviews existing diversity-aware decoding algorithms, including random sampling techniques and diverse beam search.\nExplains the principle of Minimum Bayes Risk (MBR) decoding, focusing on its expected utility maximization approach and contrasting it with MAP decoding.\n\nMinimum Bayes Risk Decoding with Diversity\n\nIntroduces the set decoding problem with diversity objective using MBR decoding.\nPresents a naive approach for generating k sentences using MBR and notes its tendency to produce similar sentences.\nProposes Diverse MBR (DMBR) decoding as a solution by adding a diversity penalty to the objective function.\nExplains the formulation of DMBR, including the quality objective and diversity objective, and discusses the use of pairwise similarity as a diversity metric.\nHighlights the computational complexity of DMBR and the deployment of a greedy heuristic algorithm for approximation.\nProposes k-medoids MBR (KMBR) as an alternative method for diversity promotion in MBR decoding.\nExplains how KMBR leverages the k-medoids clustering algorithm to select a set of diverse and high-quality hypotheses.\nNotes the computational challenges of KMBR and the use of the Partition Around Medoids (PAM) algorithm for approximate computation.\n\nExperiments\n\nDescribes the experimental setup, including the tasks (machine translation, image captioning, question generation, common sense reasoning, text summarization), datasets, evaluation metrics, and the choice of BERTScore as the utility function for MBR.\nMentions the use of Huggingface’s Transformers library and other tools for the experiments.\nSummarizes the key results, highlighting that DMBR and KMBR generally achieve better trade-offs between quality and diversity than diverse beam search and sampling algorithms across the tasks.\n\nMachine Translation\n\nExplains the use of the WMT’19 dataset for machine translation experiments, focusing on German-English and Russian-English translation tasks.\nDiscusses the experimental settings, including the number of outputs, comparison baselines (sampling algorithms, diverse beam search), sample size for MBR, and diversity penalty parameters.\nPresents the key findings, emphasizing DMBR’s achievement of higher diversity, flexibility in the quality-diversity trade-off, and higher Oracle scores compared to baselines.\n\nImage Captioning using BLIP-2\n\nDescribes the use of the MS COCO dataset and the BLIP-2 model for evaluating performance on image captioning.\nBriefly notes the experimental settings, including the output size, baselines, and diversity penalty parameters.\nHighlights DMBR’s effectiveness in achieving lower P-BLEU, higher distinct-2, and better semantic diversity as measured by P-SentBERT.\n\nQuestion Generation using Language Model\n\nPresents the evaluation of decoding algorithms for question generation using the SQuADv2 dataset and a language model (Zephyr-7B β) with prompting.\nMentions the experimental settings, including the number of outputs, baselines, sample size for MBR, and diversity penalty parameters.\nDiscusses the findings, noting that DMBR demonstrates advantages in distinct-2 and P-SentBERT but underperforms slightly in P-BLEU compared to DBS.\n\nGenerative Common Sense Reasoning using Language Model\n\nExplains the use of the CommonGen task and the Zephyr-7B β language model for evaluating common sense reasoning abilities.\nNotes the use of prompting and the experimental settings, including the number of outputs, baselines, sample size, and diversity penalty.\nBriefly mentions DMBR’s performance, achieving better distinct-2 but slightly worse P-BLEU compared to DBS, and discusses the low coverage of input concepts in the generations.\n\nText Summarization\n\nDescribes the evaluation of text summarization using the XSum dataset and a BART model pre-trained on XSum.\nMentions the experimental settings, including the output size, baselines, sample size for MBR, and diversity penalty parameters.\nBriefly presents the results, emphasizing DMBR’s better diversity compared to DBS, as measured by P-BLEU and distinct-n.\n\nConclusions\n\nSummarizes the research, highlighting the development and evaluation of DMBR and KMBR for generating diverse and high-quality texts.\nReiterates the better quality-diversity trade-off achieved by these methods compared to diverse beam search, and notes the higher Oracle scores attained by DMBR and KMBR over vanilla MBR.\nDiscusses potential future research directions, including applying the methods to open-ended text generation, conducting human evaluation of diversity, and reducing the inference time of DMBR and KMBR.\n\nLimitations\n\nDiscusses the limitations of the research, including the focus on directed text generation tasks.\nNotes the reliance on automatic evaluation metrics and the need for human evaluation.\nHighlights the slower inference time of DMBR and KMBR compared to DBS, suggesting further research on computational efficiency.\nMentions the use of a simple greedy algorithm for DMBR and the potential for more sophisticated approximation algorithms.\n\nAppendices\n\nAppendix A: Provides a proof of submodularity.\nAppendix B: Evaluates the coverage of input concepts for CommonGen, noting the low coverage in the experiments.\nAppendix C: Shows examples of generations from various decoding algorithms across different tasks.\nAppendix D: Evaluates the oversampling strategy as a baseline, comparing its performance to DMBR.\nAppendix E: Presents additional figures and tables summarizing the experimental results.\nAppendix F: Lists the pre-trained models and codes used in the experiments."
  },
  {
    "objectID": "reviews/paper/mbr-decoding/index.html#the-paper",
    "href": "reviews/paper/mbr-decoding/index.html#the-paper",
    "title": "Generating Diverse and High-Quality Texts by Minimum Bayes Risk Decoding",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper"
  },
  {
    "objectID": "reviews/paper/morphological-embeddings/index.html",
    "href": "reviews/paper/morphological-embeddings/index.html",
    "title": "Morphological Word Embeddings",
    "section": "",
    "text": "Litrature review\nThis is a mentioned in the tokenization lab in course four week 3\nTime permitting I will try and dive deeper into this paper."
  },
  {
    "objectID": "reviews/paper/morphological-embeddings/index.html#podcast",
    "href": "reviews/paper/morphological-embeddings/index.html#podcast",
    "title": "Morphological Word Embeddings",
    "section": "Podcast",
    "text": "Podcast"
  },
  {
    "objectID": "reviews/paper/morphological-embeddings/index.html#abstract",
    "href": "reviews/paper/morphological-embeddings/index.html#abstract",
    "title": "Morphological Word Embeddings",
    "section": "Abstract",
    "text": "Abstract\n\nLinguistic similarity is multi-faceted. For instance, two words may be similar with respect to semantics, syntax, or morphology inter alia. Continuous word-embeddings have been shown to capture most of these shades of similarity to some degree. This work considers guiding word-embeddings with morphologically annotated data, a form of semisupervised learning, encouraging the vectors to encode a word’s morphology, i.e., words close in the embedded space share morphological features. We extend the log-bilinear model to this end and show that indeed our learned embeddings achieve this, using German as a case study. – (Cotterell and Schütze 2019)"
  },
  {
    "objectID": "reviews/paper/morphological-embeddings/index.html#terminology",
    "href": "reviews/paper/morphological-embeddings/index.html#terminology",
    "title": "Morphological Word Embeddings",
    "section": "Terminology",
    "text": "Terminology\nHere are some terms and concepts discussed in the sources, with explanations:\n\nWord embeddings\n\nThese are vector representations of words in a continuous space, where similar words are located close to each other in the vector space. The goal is to capture linguistic similarity, but the definition of “similarity” can vary (semantic, syntactic, morphological). Word embeddings are typically trained to produce representations that capture linguistic similarity.\n\nLog-bilinear model (LBL)\n\nThis is a language model that learns features along with weights, as opposed to using hand-crafted features. It uses context to predict the next word, and word embeddings fall out as low dimensional representations of the context. The LBL is a generalization of the log-linear model.\n\nMorphology\n\nThe study of word structure, including how words are formed from morphemes (the smallest units of meaning).\n\nMorphological tags\n\nThese are annotations that describe the morphological properties of a word, such as case, gender, number, tense, etc.. They can be very detailed for morphologically rich languages. For example, in German, a word might be tagged to indicate its part of speech, case, gender, and number.\n\nMorphologically rich languages\n\nLanguages with a high morpheme-per-word ratio, where word-internal structure is important for processing. German and Czech are cited as examples.\n\nMorphologically impoverished languages\n\nLanguages with a low morpheme-per-word ratio, such as English.\n\nMulti-task objective\n\nTraining a model to perform multiple tasks simultaneously. In this case, the model is trained to predict both the next word and its morphological tag.\n\nSemi-supervised learning\n\nTraining a model on a partially annotated corpus, using both labeled and unlabeled data. This approach is useful when large amounts of unannotated text are available.\n\nContextual signature\n\nThe words surrounding a given word. The context in which a word appears can be used to determine the word’s meaning and morphological properties.\n\nHamming distance\n\nA measure of the difference between two binary strings, calculated as the number of positions at which the corresponding bits are different. In this context, it is used to compare morphological tags, which are represented as bit vectors.\n\nMORPHOSIM\n\nA metric for evaluating morphologically-driven embeddings that measures how morphologically similar a word is to its nearest neighbors in the embedding space. It is calculated as the average Hamming distance between morphological tags of a word and its neighbors. Lower values are better, as they indicate that the nearest neighbors of each word are closer morphologically."
  },
  {
    "objectID": "reviews/paper/morphological-embeddings/index.html#outline",
    "href": "reviews/paper/morphological-embeddings/index.html#outline",
    "title": "Morphological Word Embeddings",
    "section": "Outline",
    "text": "Outline\n\nIntroduction\n\nPresents the importance of capturing word morphology, especially for morphologically-rich languages.\nHighlights the multifaceted nature of linguistic similarity (semantics, syntax, morphology).\nDiscusses the goal of the paper: to develop word embeddings that specifically encode morphological relationships.\n\nRelated Work\n\nDiscusses previous integration of morphology into language models, including factored language models and neural network-based approaches.\nNotes the role of morphology in computational morphology, particularly in morphological tagging for inflectionally-rich languages.\nHighlights the significance of distributional similarity in morphological analysis.\n\nLog-Bilinear Model\n\nDescribes the log-bilinear model (LBL), a generalization of the log-linear model with learned features.\nPresents the LBL’s energy function and probability distribution in the context of language modeling.\n\nMorph-LBL\n\nProposes a multi-task objective that jointly predicts the next word and its morphological tag.\nDescribes the model’s joint probability distribution, incorporating morphological tag features.\nDiscusses the use of semi-supervised learning, allowing training on partially annotated corpora\n\nEvaluation\n\nMentions the qualitative evaluation using t-SNE, showing clusters reflecting morphological and POS relationships.\nIntroduces MorphoSim, a novel quantitative metric to assess the extent to which similar embeddings are morphologically related.\n\nExperiments and Results\n\nPresents experiments on the German TIGER corpus, comparing Morph-LBL with the original LBL and Word2Vec.\nDescribes two experiments:\n\nExperiment 1: Evaluates the morphological information encoded in embeddings using a k-NN classifier.\nExperiment 2: Compares models using the MorphoSim metric to measure morphological similarity among nearest neighbors.\n\nDiscusses the results, highlighting Morph-LBL’s superior performance in capturing morphological relationships, even without observing all tags during training.\n\nConclusion and Future Work\n\nSummarizes the contributions of the paper: introducing Morph-LBL for inducing morphologically guided embeddings.\nNotes the model’s success in leveraging distributional signatures to capture morphology.\nDiscusses future work on integrating orthographic features for further improvement.\nMentions potential applications in morphological tagging and other NLP tasks."
  },
  {
    "objectID": "reviews/paper/morphological-embeddings/index.html#log-bilinear-model",
    "href": "reviews/paper/morphological-embeddings/index.html#log-bilinear-model",
    "title": "Morphological Word Embeddings",
    "section": "Log-Bilinear Model",
    "text": "Log-Bilinear Model\np(w\\mid h) =\n\\frac{\\exp\\left(s_\\theta(w,h)\\right)}{\\sum_{w'}\n\\exp\\left(s_\\theta(w',h)\\right)} \\qquad\n\nwhere w is a word, h is a history and s_\\theta is an energy function. Following the notation of , in the LBL we define \ns_\\theta(w,h) = \\left(\\sum_{i=1}^{n-1} C_i\nr_{h_i}\\right)^T q_w + b_w \\qquad\n\nwhere n-1 is the history length"
  },
  {
    "objectID": "reviews/paper/morphological-embeddings/index.html#morph-lbl",
    "href": "reviews/paper/morphological-embeddings/index.html#morph-lbl",
    "title": "Morphological Word Embeddings",
    "section": "Morph-LBL",
    "text": "Morph-LBL\n\n  p(w, t \\mid h) \\propto \\exp(( f_t^T S  + \\sum_{i=1}^{n-1}C_i r_{h_i})^T q_w + b_{w} ) \\qquad\n\nwhere f_t is a hand-crafted feature vector for a morphological tag t and S is an additional weight matrix.\nUpon inspection, we see that\n\np(t \\mid w,h) \\propto \\exp(S^T f_t q_w) \\qquad\n\nHence given a fixed embedding q_w for word w, we can interpret S as the weights of a conditional log-linear model used to predict the tag t."
  },
  {
    "objectID": "reviews/paper/morphological-embeddings/index.html#the-paper",
    "href": "reviews/paper/morphological-embeddings/index.html#the-paper",
    "title": "Morphological Word Embeddings",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper"
  },
  {
    "objectID": "reviews/paper/morphological-embeddings/index.html#resources",
    "href": "reviews/paper/morphological-embeddings/index.html#resources",
    "title": "Morphological Word Embeddings",
    "section": "Resources",
    "text": "Resources\n\nLeipzig Glossing Rules which provides a standard way to explain morphological features by examples\nCMU Multilingual NLP 2020 (17): Morphological Analysis and Inflection \nKann, Cotterell, and Schütze (2016) code\nunimorph univorsal morphological database"
  },
  {
    "objectID": "reviews/paper/ELMo/index.html",
    "href": "reviews/paper/ELMo/index.html",
    "title": "ELMo - Deep contextualized word representations",
    "section": "",
    "text": "Litrature review"
  },
  {
    "objectID": "reviews/paper/ELMo/index.html#podcast",
    "href": "reviews/paper/ELMo/index.html#podcast",
    "title": "ELMo - Deep contextualized word representations",
    "section": "Podcast",
    "text": "Podcast"
  },
  {
    "objectID": "reviews/paper/ELMo/index.html#abstract",
    "href": "reviews/paper/ELMo/index.html#abstract",
    "title": "ELMo - Deep contextualized word representations",
    "section": "Abstract",
    "text": "Abstract\n\nWe introduce a new type of deep contextualized word representation that models both\n\ncomplex characteristics of word use (e.g., syntax and semantics), and\nhow these uses vary across linguistic contexts (i.e., to model polysemy).\n\nOur word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals."
  },
  {
    "objectID": "reviews/paper/ELMo/index.html#outline",
    "href": "reviews/paper/ELMo/index.html#outline",
    "title": "ELMo - Deep contextualized word representations",
    "section": "Outline",
    "text": "Outline\n\nI. Introduction\n\nIdeally, word representations should model complex characteristics of word use, such as syntax and semantics, and how these uses vary across linguistic contexts, to model polysemy.\nThe authors introduces a new type of deep contextualized word representation (ELMo) that addresses these challenges, integrates easily into existing models, and improves state-of-the-art results.\n\nII. ELMo (Embeddings from Language Models)\n\nELMo representations are functions of the entire input sentence, not just individual tokens.\nThey are computed using a bidirectional LSTM (biLM) trained on a large text corpus with a language model objective.\nELMo representations are deep, in the sense they are a function of all internal layers of the biLM.\nA linear combination of the vectors stacked above each input word is learned for each end task.\nInternal states are combined to create rich word representations\nHigher-level LSTM states capture context-dependent aspects of word meaning (semantics),\nLower-level states model aspects of syntax.\nExposing all of these signals allows learned models to select the most useful types of semi-supervision for each end task.\n\nIII. Bidirectional Language Models (biLM)\n\nA forward language model predicts the next token given the history of previous tokens.\nA backward language model predicts the previous token given the future context.\nA biLM combines both forward and backward LMs, maximizing the log-likelihood of both directions.\nThe biLM uses tied parameters for token representations and the Softmax layer in both directions, but maintains separate parameters for LSTMs in each direction.\n\nIV. ELMo Specifics\n\nFor each token, an L-layer biLM computes a set of 2L+1 representations.\nELMo collapses all biLM layers into a single vector using a task-specific weighting of all layers.\nA scalar parameter scales the entire ELMo vector.\nLayer normalization can be applied to each biLM layer before weighting.\n\nV. Integrating ELMo into Supervised NLP Tasks\n\nThe weights of the pre-trained biLM are frozen, and then the ELMo vector is concatenated with the existing token representation before being passed into the task’s RNN.\nFor some tasks, ELMo is also included at the output of the task RNN.\nDropout is added to ELMo, and sometimes the ELMo weights are regularized.\n\nVI. Pre-trained biLM Architecture\n\nThe biLMs are similar to previous architectures but modified to support joint training of both directions and add a residual connection between LSTM layers.\nThe model uses 2 biLSTM layers with 4096 units and 512-dimension projections, with a residual connection.\nThe context-insensitive type representation uses character n-gram convolutional filters and highway layers.\nThe biLM provides three layers of representation for each input token.\n\nVII. Evaluation\n\nELMo was evaluated on six benchmark NLP tasks, including question answering, textual entailment, and sentiment analysis.\nAdding ELMo significantly improves the state-of-the-art in every case.\nFor tasks where direct comparisons are possible, ELMo outperforms CoVe.\nDeep representations outperform those derived from just the top layer of an LSTM.\n\nVIII. Task-Specific Results\n\nQuestion Answering (SQuAD): ELMo significantly improved the F1 score.\nTextual Entailment (SNLI): ELMo improved accuracy.\nSemantic Role Labeling (SRL): ELMo improved the F1 score.\nCoreference Resolution: ELMo improved the average F1 score.\nNamed Entity Extraction (NER): ELMo enhanced biLSTM-CRF achieved a new state-of-the-art F1 score.\nSentiment Analysis (SST-5): ELMo improved accuracy over the prior state-of-the-art.\n\nIX. Analysis\n\nUsing deep contextual representations improves performance compared to just using the top layer.\nELMo provides better overall performance than representations from a machine translation encoder like CoVe.\nSyntactic information is better represented at lower layers, while semantic information is better captured at higher layers.\nIncluding ELMo at both the input and output layers of the supervised model can improve performance for some tasks.\nELMo increases sample efficiency, requiring fewer training updates and less data to reach the same level of performance.\nThe contextual information captured by ELMo is more important than the sub-word information.\nPre-trained word vectors provide a marginal improvement when used with ELMo.\n\nX. Key Findings\n\nELMo efficiently encodes different types of syntactic and semantic information about words in context.\nUsing all layers of the biLM improves overall task performance.\nELMo provides a general approach for learning high-quality, deep, context-dependent representations."
  },
  {
    "objectID": "reviews/paper/ELMo/index.html#the-paper",
    "href": "reviews/paper/ELMo/index.html#the-paper",
    "title": "ELMo - Deep contextualized word representations",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper"
  },
  {
    "objectID": "reviews/paper/2025-02-03-xLSTM/index.html",
    "href": "reviews/paper/2025-02-03-xLSTM/index.html",
    "title": "xLSTM: Extended Long Short-Term Memory",
    "section": "",
    "text": "Literature Review\n\n\n\n\n\n\n\nVideo 1: Review by AI Bites\n\n\n\n\n\n\n\n\nVideo 2: Review by Yannic Kilcher"
  },
  {
    "objectID": "reviews/paper/2025-02-03-xLSTM/index.html#introduction",
    "href": "reviews/paper/2025-02-03-xLSTM/index.html#introduction",
    "title": "xLSTM: Extended Long Short-Term Memory",
    "section": "Introduction",
    "text": "Introduction\n\n\n\n\n\n\nMotivation\n\n\n\nSo this is my first review of the RNN papers. I found that since the courses covering LSTM seemed to be over simplified and confusing, I should go back to the source. Since time is limited, it helps to review the papers in reverse chronological order. Each new papers explains the limitations that are generally not apparent when the paper is first published. The authors of the later papers also have the benefit of hindsight which they share in the obligatory literature review wherein they provide lots of context we cannot get by reading the original papers. Given all that there is also a benefit to reading the original paper - the real innovators while harder to understand are remarkable thinkers and often reveal motivations and deep insights that elude later interlocutors.\n\n\n\nPodcast & Other Reviews\nThis paper is a bit heavy on the technical side. But if we are still here, we may enjoy this podcast reviewing the paper. Though AI generated I found it quite useful to get oriented with the paper and understand that the experiments the authors run to validate this new architectures.\nWe also cover something I found absent from the Course on sequence models . The limitations of LSTMs as viewed by the authors when considered in the context of transformers and Mamba.\n\n\n\nAlthough this paper is recent there are a number of other people who cover it.\n\nIn Video 2 Yannic Kilcher covers the paper in his youtube channel. He does a great job of explaining the paper in a way that is easy to understand. Kilcher is all too human, he makes many mistakes, stresses opinions over facts. If we don’t mind all that he is a great resource for understanding the paper. I find he rubs me the wrong way but at least I find this motivating to do better in my own reviews!\n\n\n\n\n\n\n\nWhat can we expect from xLSTM?\n\n\n\n\n\nxLSTM has the potential to considerably impact other fields like Reinforcement Learning, Time Series Prediction, or the modeling of physical systems. – (Beck et al. 2024)\n\nIn his book Understanding Media (McLuhan 1988) Marshal McLuhan introduced his Tetrad. The Tetrad is a mental model for understanding how a technological innovation like the xLSTM might disrupt society. To work the model we ask the following four questions that can be applied to any new technology to understand its impact. The four questions in this instance are:\n\nWhat does the xLSTM enhance or amplify?\n\n\nThe xLSTM enhances the LSTM by introducing exponential gating and novel memory structures, boosting its capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models.\n\n\nWhat does the xLSTM make obsolete or replace?\n\n\nThe xLSTM makes the traditional LSTM obsolete by addressing its limitations, such as the inability to revise storage decisions, limited storage capacities, and lack of parallelizability.\n\n\nWhat does the xLSTM retrieve that was previously obsolesced?\n\n\nThe xLSTM retrieves the LSTM’s relevance in large-scale language modeling by scaling it to billions of parameters and integrating modern LLM techniques while mitigating known limitations. 4. What does the xLSTM reverse or flip into when pushed to its limits?\n\n\nThe xLSTM, when pushed to its limits, can compete with current Large Language Models based on Transformer technology, offering a competitive alternative for sequence modeling tasks.\n\nThe xLSTM paper by (Beck et al. 2024) is a great example of the tetrad in action. The authors take the LSTM, a technology that has stood the test of time, and enhance it with exponential gating and novel memory structures. They address the limitations of traditional LSTMs and push the technology to new limits, creating a new architecture that can compete with state-of-the-art Transformers and State Space Models. Recent work by (Chen et al. 2024) suggest that Transformers and The State Space Models are actually limited in their own ways.\n\n\n\n\n\n\n\n\nTetrad\n\n\n\n\nSupplementary Figure 1"
  },
  {
    "objectID": "reviews/paper/2025-02-03-xLSTM/index.html#abstract",
    "href": "reviews/paper/2025-02-03-xLSTM/index.html#abstract",
    "title": "xLSTM: Extended Long Short-Term Memory",
    "section": "Abstract",
    "text": "Abstract\n\nIn the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining:\n\nsLSTM with a scalar memory, a scalar update, and new memory mixing,\n\nmLSTM that is fully parallelizable with a matrix memory and a covariance update rule.\n\n\nIntegrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling. — (Beck et al. 2024)\n\n\n\n\n\n\n\n\narchitecture\n\n\n\n\nFigure 1: The extended LSTM (xLSTM) family. From left to right:\n1. The original LSTM memory cell with constant error carousel and gating.\n2. New sLSTM and mLSTM memory cells that introduce exponential gating. sLSTM offers a new memory mixing technique. mLSTM is fully parallelizable with a novel matrix memory cell state and new covariance update rule.\n3. mLSTM and sLSTM in residual blocks yield xLSTM blocks.\n4. Stacked xLSTM blocks give an xLSTM\n\n\n\n\n\n\n\n\nblocks\n\n\n\n\nFigure 2: LSTM limitations.\n- Left: Nearest Neighbor Search problem in terms of mean squared error (MSE). Given a reference vector, a sequence is scanned sequentially for the most similar vector with the objective to return its attached value at sequence end. LSTM struggles to revise a stored value when a more similar vector is found. Our new xLSTM overcomes this limitation by exponential gating. - Right: Rare Token Prediction. The perplexity (PPL) of token prediction on Wikitext-103, in partitions of token frequency. LSTM performs worse on predicting rare tokens because of its limited storage capacities, whereas our new xLSTM solves this problem via a matrix memory.\n\n\n\n\n\n\n\n\nblocks\n\n\n\n\nFigure 3: xLSTM blocks.\nLeft: A residual sLSTM block with post up-projection (like Transformers): The input is fed into an sLSTM — with an optional convolution — followed by a gated MLP.\nRight: A residual mLSTM block with pre up-projection (like State Space models): mLSTM is wrapped inside two MLPs, via a convolution, a learnable skip connection, and an output gate that acts componentwise.\n\n\n\n\n\n\n\n\nblocks\n\n\n\n\nFigure 4: xLSTM blocks.\nLeft: A residual sLSTM block with post up-projection (like Transformers): The input is fed into an sLSTM — with an optional convolution — followed by a gated MLP.\nRight: A residual mLSTM block with pre up-projection (like State Space models): mLSTM is wrapped inside two MLPs, via a convolution, a learnable skip connection, and an output gate that acts componentwise.\n\n\n\n\n\n\n\n\nblocks\n\n\n\n\nFigure 5: xLSTM blocks.\nLeft: A residual sLSTM block with post up-projection (like Transformers): The input is fed into an sLSTM — with an optional convolution — followed by a gated MLP.\nRight: A residual mLSTM block with pre up-projection (like State Space models): mLSTM is wrapped inside two MLPs, via a convolution, a learnable skip connection, and an output gate that acts componentwise."
  },
  {
    "objectID": "reviews/paper/2025-02-03-xLSTM/index.html#sec-outline",
    "href": "reviews/paper/2025-02-03-xLSTM/index.html#sec-outline",
    "title": "xLSTM: Extended Long Short-Term Memory",
    "section": "Paper Outline",
    "text": "Paper Outline\n\nIntroduction\n\nDescribes the constant error carousel and gating mechanisms of Long Short-Term Memory (LSTM). &gt; “In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories…”\nDiscusses three main limitations of LSTMs:\n\nThe inability to revise storage decisions.\nLimited storage capacities.\nLack of parallelizability due to memory mixing.\n\nHighlights the emergence of Transformers in language modeling due to these limitations. &gt; Transformer Era: The rise of Transformers with parallelizable self-attention has largely overshadowed LSTMs due to their superior scalability.\n\nExtended Long Short-Term Memory\n\nIntroduces Extended Long Short-Term Memory (xLSTM), which incorporates exponential gating and novel memory structures.\nPresents two new LSTM variants:\n\nsLSTM with a scalar memory, a scalar update, and new memory mixing.\nmLSTM with a matrix memory and a covariance update rule, which is fully parallelizable.\n\nDescribes the integration of these LSTM variants into residual block modules, resulting in xLSTM blocks.\nPresents xLSTM architectures that residually stack xLSTM blocks.\n\nRelated Work:\n\nMentions various linear attention methods to overcome the quadratic complexity of Transformer attention.\nNotes the popularity of State Space Models (SSMs) for language modeling.\nHighlights the use of Recurrent Neural Networks (RNNs) as a replacement for Transformers.\nMentions the use of gating in recent RNN and SSM approaches.\nNotes the use of covariance update rules1 to enhance storage capacities in various models.\n\nExperiments\n\nPresents the experimental evaluation of xLSTM compared to existing methods with a focus on language modeling.\nDiscusses the effectiveness of xLSTM on synthetic tasks, including:\n\nFormal languages.\nMulti-Query Associative Recall.\nLong Range Arena tasks.\n\nPresents a method comparison and ablation study, where different models are trained on 15 billion tokens from the SlimPajama dataset.\nAssesses the scaling behavior of different methods based on validation perplexity.\nConducts a large-scale language modeling experiment:\n\nTraining different model sizes on 300B tokens from SlimPajama.\nEvaluating models on length extrapolation.\nAssessing models on validation perplexity and performance on downstream tasks.\nEvaluating models on 571 text domains of the PALOMA language benchmark dataset.\nAssessing the scaling behavior with increased training data.\n\n\nLimitations\n\nHighlights limitations of xLSTM, including:\n\nLack of parallelizability for sLSTM due to memory mixing.\nUnoptimized CUDA kernels for mLSTM.\nHigh computational complexity of mLSTM’s matrix memory.\nMemory overload for longer contexts due to the independence of mLSTM’s memory from sequence length.\n\n\nConclusion\n\nConcludes that xLSTM performs favorably in language modeling compared to Transformers and State Space Models.\nSuggests that larger xLSTM models will be competitive with current Large Language Models based on Transformer technology.\nNotes the potential impact of xLSTM on other deep learning fields such as reinforcement learning, time series prediction, and physical system modeling.\n\n\n1 explain covariance"
  },
  {
    "objectID": "reviews/paper/2025-02-03-xLSTM/index.html#briefing-xlstm---extended-long-short-term-memory",
    "href": "reviews/paper/2025-02-03-xLSTM/index.html#briefing-xlstm---extended-long-short-term-memory",
    "title": "xLSTM: Extended Long Short-Term Memory",
    "section": "Briefing : xLSTM - Extended Long Short-Term Memory",
    "text": "Briefing : xLSTM - Extended Long Short-Term Memory\n\nIntroduction and Motivation:\nLSTM’s Legacy: The Long Short-Term Memory (LSTM), introduced in the 1990s, has been a cornerstone of deep learning, especially in sequence modeling and early Large Language Models (LLMs). LSTMs are known for their “constant error carousel” and gating mechanisms, designed to combat vanishing gradients in recurrent neural networks.\n\n“In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories…”\n\nTransformer Era: The rise of Transformers with parallelizable self-attention has largely overshadowed LSTMs due to their superior scalability.\n\n“However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale.” xLSTM Question: This research aims to explore the capabilities of LSTMs when scaled to billions of parameters, by integrating modern LLM techniques while addressing known limitations.\n\n\n“We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs?”\n\n\n\nKey Limitations of Traditional LSTMs:\nThe paper identifies three major limitations of traditional LSTMs:\n\nInability to Revise Storage Decisions: LSTMs struggle to update previously stored information when more relevant information arrives later in the sequence. The paper uses a “Nearest Neighbor Search” task to demonstrate this.\n\n\n“LSTM struggles to revise a stored value when a more similar vector is found…”\n\n\nLimited Storage Capacity: LSTMs store information in scalar cell states, which can lead to information compression and loss. This impacts performance on tasks requiring detailed memory, like predicting rare tokens.\n\n\n“LSTM performs worse on predicting rare tokens because of its limited storage capacities…”\n\n\nLack of Parallelizability: LSTMs process data sequentially due to the recurrent connections between hidden states, hindering parallel computation.\n\n\n“Lack of parallelizability due to memory mixing, i.e., the hidden-hidden connections between hidden states from one time step to the next, which enforce sequential processing.”\n\n\nxLSTM Innovations:\n\nThe paper introduces Extended Long Short-Term Memory (xLSTM), designed to address the above limitations, by modifying the original LSTM using two main approaches:\nExponential Gating:Implemented to allow for better revision of storage decisions. Normalization and stabilization techniques are used to avoid overflows in calculations due to the exponential function.\n\n“To empower LSTMs with the ability to revise storage decisions, we introduce exponential gates (red) together with normalization and stabilization.” Novel Memory Structures: Two new LSTM variants are created with these structures:\n\nsLSTM: A scalar memory LSTM with a scalar update and new memory mixing technique (within heads, not across). Features multiple memory cells and multiple heads, with memory mixing within each head.\n\n“The new sLSTM (see Section 2.2) with a scalar memory, a scalar update, and memory mixing…”\n\nmLSTM: A matrix memory LSTM, fully parallelizable, with a covariance update rule for storing key-value pairs. Does not have memory mixing between cells or heads.\n\n“the new mLSTM (see Section 2.3) with a matrix memory and a covariance (outer product) update rule, which is fully parallelizable.”\n\n\nsLSTM Details:\n\n\nExponential Gates: Introduces exponential activation functions for input and forget gates with normalization and a stabilizer state to prevent overflow.\nNormalizer State: A state used to normalize the hidden state output by accumulating a weighted sum of input and forget gates.\nMemory Mixing: sLSTM allows memory mixing between cells, and also introduces a new form of memory mixing through multiple heads, mixing between cells within a head, but not across heads.\n\n\n“The new sLSTM can have multiple heads with memory mixing within each head but not across heads. The introduction of heads for sLSTM together with exponential gating establishes a new way of memory mixing.”\n\n\nmLSTM Details:\n\nMatrix Memory: Replaces the scalar cell state with a matrix, increasing storage capacity.\nKey-Value Storage: The mLSTM memory is based around a key-value storage system inspired by Bidirectional Associative Memories (BAMs) using covariance updates, where keys and values are updated via an outer product.\n\n“At time t, we want to store a pair of vectors, the key k_t ∈ R^d and the value v_t ∈ R^d… The covariance update rule… for storing a key-value pair…”\n\nParallelization: mLSTM is fully parallelizable due to abandoning hidden-hidden recurrent connections and memory mixing. Computations can be performed efficiently on GPUs using standard matrix operations.\n\n“…the mLSTM… which is fully parallelizable.”\n\n\nxLSTM Architecture:\n\nxLSTM Blocks: The sLSTM and mLSTM variants are integrated into residual blocks.\nsLSTM blocks use post up-projection (like Transformers).\nmLSTM blocks use pre up-projection (like State Space Models).\n\n“Integrating these new LSTM variants into residual block modules results in xLSTM blocks…”\n\nResidual Stacking: xLSTM architectures are created by residually stacking these blocks, using pre-LayerNorm backbones similar to current LLMs.\n\n“An xLSTM architecture is constructed by residually stacking build-ing blocks…” Cover’s Theorem: Used to justify mapping information into a higher-dimensional space for better separation of histories or contexts.\n\n\n“We resort to Cover’s Theorem (Cover, 1965), which states that in a higher dimensional space non-linearly embedded patterns can more likely be linearly separated than in the original space.”\n\n\nPerformance and Scaling:\n\nLinear Computation & Constant Memory: Unlike Transformers, xLSTM networks have linear computation and constant memory complexity with respect to sequence length.\n\n“Contrary to Transformers, xLSTM networks have a linear computation and a constant memory complexity with respect to the sequence length.”\n\nSynthetic Tasks: xLSTM showed improved performance over regular LSTMs on “Nearest Neighbor Search” and “Rare Token Prediction” synthetic tasks.\nSlimPajama Experiments: The xLSTM models were trained on 15B and 300B tokens from the SlimPajama dataset for language modelling evaluation.\nCompetitive Performance: xLSTMs demonstrate competitive performance against state-of-the-art Transformers and State Space Models on language modelling benchmarks, both in terms of next token prediction perplexity and downstream tasks.\n\n“Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.”\n\nAblation studies show importance of gating techniques.\n\nMemory & Speed:\n\nsLSTM: Not parallelizable due to memory mixing, although the authors have created a fast CUDA implementation (less than twice as slow as mLSTM).\n\n“While mLSTM is parallelizable analog to FlashAttention… sLSTM is not parallelizable due to the memory mixing…”\n\nmLSTM: Parallelizable, but currently slower than FlashAttention or Mamba due to non-optimized CUDA kernels. Matrix memory is computationally complex but does not require parameters and can be parallelized.\n\n“The memory of mLSTM does not require parameters but is computationally expensive through its d×d matrix memory and d× d update… the computations can be done in parallel on GPUs, therefore these computations have only a minor effect on the wall clock time.”\n\n\nLimitations:\n\nsLSTM Parallelization: sLSTM’s memory mixing is non-parallelizable.\n\n“(i) In contrast to mLSTM, memory mixing of the sLSTM prohibits parallelizable operations, and therefore does not allow a fast parallel implementation.”\n\nmLSTM CUDA Optimization: The mLSTM CUDA kernels are not fully optimized.\n\n“(ii) The CUDA kernels for mLSTM are not optimized, and therefore the current implementation is about 4 times slower than FlashAttention or the scan used in Mamba.”\n\nmLSTM Matrix Memory: High computational complexity for mLSTM due to matrix memory operations.\nForget Gate Initialization: Careful initialization of the forget gates is needed.\nLong Context Memory: The matrix memory is independent of sequence length, and might overload memory for long context sizes.\n\n“(v) Since the matrix memory is independent of the sequence length, increasing the sequence length might overload the memory for longer context sizes.”\n\nHyperparameter Optimization: xLSTM is not yet fully optimized due to computational load, indicating further performance gains are possible.\n\n“(vi) Due to the expensive computational load for large language experiments, we did neither fully optimize the architecture nor the hyperparameters, especially for larger xLSTM architectures.”\n\n\nRelated Work:\n\nThe paper highlights connections of its ideas with the following areas:\nGating: Many recent models such as HGRN, GLA, Gated State Space models, Mamba, and others also utilize gating mechanisms. Covariance Update Rule: The covariance update rule used in mLSTM is related to other methods such as Fast Weight Programmers, Retention, and Linear Transformers.\n\nConclusion:\n\nThe xLSTM presents a compelling alternative to Transformers in sequence modeling, especially in contexts where memory efficiency and longer sequences are required. By combining insights from the original LSTM with exponential gating and novel memory structures, the paper shows that LSTMs can be competitive at scale. While limitations exist in terms of parallelization and hyperparameter tuning, the research shows promising potential for further development. The authors posit further research and improvements could allow xLSTM to reach its full potential."
  },
  {
    "objectID": "reviews/paper/2025-02-03-xLSTM/index.html#my-thoughts",
    "href": "reviews/paper/2025-02-03-xLSTM/index.html#my-thoughts",
    "title": "xLSTM: Extended Long Short-Term Memory",
    "section": "My Thoughts",
    "text": "My Thoughts\n\n\n\n\n\n\n\nVideo 3: Review of the sigmoid function\n\n\n\n\n\n\n\n\nResearch questions\n\n\n\n\nHow does the constant error carousel mitigate the vanishing gradient problem in the LSTM?\n\nThe constant error carousel is a way to keep the error constant over time. This helps to prevent the vanishing gradient problem in the LSTM.\n\nHow are the gates in the original LSTM binary?\n\nSigmoid, saturation and a threshold at 0.5\n\nWhat is the long term memory in the LSTM?\n\nthe cell state c_{t-1}\n\nWhat is the short term memory in the LSTM?\n\nthe hidden state h_{t-1}\n\nHow far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs\n\n\n\n\n\n\n\n\n\n\nSupplementary Figure 2: limits of the sigmoid function\n\n\n\n\n\n\n\n\nbias\n\n\n\n\nSupplementary Figure 3: The inductive bias of the sigmoid decision function.\n\n\n\n\nBinary nature of non exponential Gating Mechanisms\nIf we try to understand why are the gating mechanisms in the original LSTM is described here as binary?\nIt helps to the properties of the sigmoid functions that are explained in Video 3.\n\nSupplementary Figure 1 shows that The sigmoid function has a domain of \\mathbb{R} and a range of (0,1). This means that the sigmoid function can only output values between 0 and 1.\nIt has a very sharp gradient around 0 if the bias is set to 0. This means that the sigmoid function acts as a binary function around 0.\nThe Sigmoid function tends to saturate. Now sigmoid function around 0 isn’t quite quite binary but outside of (-6,+6) it becomes saturated and acts very much as a binary function.\nIf we use it as a loss function or a decision function, the sigmoid function is used with a threshold of 0.5. This means that if the output of the sigmoid function is greater than 0.5, the decision is 1, and if it is less than 0.5, the decision is 0. That is how the sigmoid function can become binary.\nNote that we can use thresholding during inference to make the decision binary and we can keep them as continuous values during training so we can have a smooth gradient for back-propagation.\nEven without thresholding the sigmoid function can become binary this is due to the saturation of the sigmoid function. I recall that Hinton in his Coursera course on Neural Networks and Deep Learning mentioned that activation function that become saturated are problematic. I can say that at last I think I understand what he meant:\nI see that that sigmoid functions can fall off the manifold of the data and this can be a problem. This is why the ReLU function is used in the Gated Recurrent Unit (GRU) and the Transformer. Falling off the manifold of the data means that the sigmoid function becomes saturated (above 6 and below -6). At this point the back-propagation steps are no longer able to change the weight or bias since the gradient is effectively so learning stops.\n\nThis becomes a is a problem in two senses. The sigmoid unit is likely to be stuck in a setting that only representative of part of the training data. The rest of the network which is not stuck now has to learn to compensate for this.\nThis issues may well be the root cause LSTM limitation number 1 - the inability to revise storage decisions. It seems that the authors decided to use the exponential gating function to address this issues.\nThis is why the gating mechanisms in the original LSTM are binary. The sigmoid function is used to control the flow of information in the LSTM by deciding which information to keep and which to discard. The sigmoid function is used to decide how much of the new input information should be added to the cell state and how much of the old cell state should be retained. This binary nature of the sigmoid function is what allows the LSTM to control the flow of information and decide what to remember and what to forget."
  },
  {
    "objectID": "reviews/paper/2025-02-03-xLSTM/index.html#the-paper",
    "href": "reviews/paper/2025-02-03-xLSTM/index.html#the-paper",
    "title": "xLSTM: Extended Long Short-Term Memory",
    "section": "The Paper",
    "text": "The Paper\n\n\n\npaper"
  },
  {
    "objectID": "reviews/paper/2025-02-03-xLSTM/index.html#references",
    "href": "reviews/paper/2025-02-03-xLSTM/index.html#references",
    "title": "xLSTM: Extended Long Short-Term Memory",
    "section": "References",
    "text": "References\n\nThe paper on open review has some additional insights from the authors\nXLSTM — Extended Long Short-Term Memory Networks By Shrinivasan Sankar — May 20, 2024"
  },
  {
    "objectID": "notes/c4w3/lab01.html",
    "href": "notes/c4w3/lab01.html",
    "title": "SentencePiece and Byte Pair Encoding",
    "section": "",
    "text": "course banner",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "L1 - SentencePiece and BPE"
    ]
  },
  {
    "objectID": "notes/c4w3/lab01.html#introduction-to-tokenization",
    "href": "notes/c4w3/lab01.html#introduction-to-tokenization",
    "title": "SentencePiece and Byte Pair Encoding",
    "section": "Introduction to Tokenization",
    "text": "Introduction to Tokenization\nIn order to process text in neural network models, it is first required to encode text as numbers with ids (such as the embedding vectors we’ve been using in the previous assignments), since the tensor operations act on numbers. Finally, if the output of the network are words, it is required to decode the predicted tokens ids back to text.\nTo encode text, the first decision that has to be made is to what level of granularity are we going to consider the text? Because ultimately, from these tokens, features are going to be created about them. Many different experiments have been carried out using words, morphological units, phonemic units, characters. For example,\n\nTokens are tricky. (raw text)\nTokens are tricky . (words)\nToken s _ are _ trick _ y . (morphemes)\nt oʊ k ə n z _ ɑː _ ˈt r ɪ k i. (phonemes, for STT)\nT o k e n s _ a r e _ t r i c k y . (character)\n\nBut how to identify these units, such as words, are largely determined by the language they come from. For example, in many European languages a space is used to separate words, while in some Asian languages there are no spaces between words. Compare English and Mandarin.\n\nTokens are tricky. (original sentence)\n令牌很棘手 (Mandarin)\nLìng pái hěn jí shǒu (pinyin)\n令牌 很 棘手 (Mandarin with spaces)\n\nSo, the ability to tokenize, i.e. split text into meaningful fundamental units is not always straight-forward.\nAlso, there are practical issues of how large our vocabulary of words, vocab_size, should be, considering memory limitations vs. coverage. A compromise between the finest-grained models employing characters which can be memory and more computationally efficient subword units such as n-grams or larger units need to be made.\nIn SentencePiece unicode characters are grouped together using either a unigram language model (used in this week’s assignment) or BPE, byte-pair encoding. We will discuss BPE, since BERT and many of its variant uses a modified version of BPE and its pseudocode is easy to implement and understand… hopefully!",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "L1 - SentencePiece and BPE"
    ]
  },
  {
    "objectID": "notes/c4w3/lab01.html#sentencepiece-preprocessing",
    "href": "notes/c4w3/lab01.html#sentencepiece-preprocessing",
    "title": "SentencePiece and Byte Pair Encoding",
    "section": "SentencePiece Preprocessing",
    "text": "SentencePiece Preprocessing\n\nNFKC Normalization\nUnsurprisingly, even using unicode to initially tokenize text can be ambiguous, e.g.,\n\neaccent = '\\u00E9'\ne_accent = '\\u0065\\u0301'\nprint(f'{eaccent} = {e_accent} : {eaccent == e_accent}')\n\né = é : False\n\n\nSentencePiece uses the Unicode standard Normalization form, NFKC, so this isn’t an issue. Looking at our example from above again with normalization:\n\nfrom unicodedata import normalize\n\nnorm_eaccent = normalize('NFKC', '\\u00E9')\nnorm_e_accent = normalize('NFKC', '\\u0065\\u0301')\nprint(f'{norm_eaccent} = {norm_e_accent} : {norm_eaccent == norm_e_accent}')\n\né = é : True\n\n\nNormalization has actually changed the unicode code point (unicode unique id) for one of these two characters.\n\ndef get_hex_encoding(s):\n    return ' '.join(hex(ord(c)) for c in s)\n\ndef print_string_and_encoding(s):\n    print(f'{s} : {get_hex_encoding(s)}') \n\n\nfor s in [eaccent, e_accent, norm_eaccent, norm_e_accent]:\n    print_string_and_encoding(s)\n\né : 0xe9\né : 0x65 0x301\né : 0xe9\né : 0xe9\n\n\nThis normalization has other side effects which may be considered useful such as converting curly quotes “ to ” their ASCII equivalent. (Although we now lose directionality of the quote…)\n\n\nLossless Tokenization*\nSentencePiece also ensures that when you tokenize your data and detokenize your data the original position of white space is preserved. (However, tabs and newlines are converted to spaces, please try this experiment yourself later below.)\nTo ensure this lossless tokenization it replaces white space with _ (U+2581). So that a simple join of the replace underscores with spaces can restore the white space, even if there are consecutives symbols. But remember first to normalize and then replace spaces with _ (U+2581). As the following example shows.\n\ns = 'Tokenization is hard.'\ns_ = s.replace(' ', '\\u2581')\ns_n = normalize('NFKC', 'Tokenization is hard.')\n\n\nprint(get_hex_encoding(s))\nprint(get_hex_encoding(s_))\nprint(get_hex_encoding(s_n))\n\n0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x20 0x69 0x73 0x20 0x68 0x61 0x72 0x64 0x2e\n0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x2581 0x69 0x73 0x2581 0x68 0x61 0x72 0x64 0x2e\n0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x20 0x69 0x73 0x20 0x68 0x61 0x72 0x64 0x2e\n\n\nSo the special unicode underscore was replaced by the ASCII unicode. Reversing the order, we see that the special unicode underscore was retained.\n\ns = 'Tokenization is hard.'\nsn = normalize('NFKC', 'Tokenization is hard.')\nsn_ = s.replace(' ', '\\u2581')\n\n\nprint(get_hex_encoding(s))\nprint(get_hex_encoding(sn))\nprint(get_hex_encoding(sn_))\n\n0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x20 0x69 0x73 0x20 0x68 0x61 0x72 0x64 0x2e\n0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x20 0x69 0x73 0x20 0x68 0x61 0x72 0x64 0x2e\n0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x2581 0x69 0x73 0x2581 0x68 0x61 0x72 0x64 0x2e",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "L1 - SentencePiece and BPE"
    ]
  },
  {
    "objectID": "notes/c4w3/lab01.html#bpe-algorithm",
    "href": "notes/c4w3/lab01.html#bpe-algorithm",
    "title": "SentencePiece and Byte Pair Encoding",
    "section": "BPE Algorithm",
    "text": "BPE Algorithm\nNow that we have discussed the preprocessing that SentencePiece performs we will go get our data, preprocess, and apply the BPE algorithm. We will show how this reproduces the tokenization produced by training SentencePiece on our example dataset (from this week’s assignment).\n\nPreparing our Data\nFirst, we get our Squad data and process as above.\n\nimport ast\n\ndef convert_json_examples_to_text(filepath):\n    example_jsons = list(map(ast.literal_eval, open(filepath))) # Read in the json from the example file\n    texts = [example_json['text'].decode('utf-8') for example_json in example_jsons] # Decode the byte sequences\n    text = '\\n\\n'.join(texts)       # Separate different articles by two newlines\n    text = normalize('NFKC', text)  # Normalize the text\n\n    with open('example.txt', 'w') as fw:\n        fw.write(text)\n    \n    return text\n\n\ntext = convert_json_examples_to_text('data.txt')\nprint(text[:900])\n\nBeginners BBQ Class Taking Place in Missoula!\nDo you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.\nHe will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information.\nThe cost to be in the class is $35 per person, and for spectators it is free. Included in the cost will be either a t-shirt or apron and you will be tasting samples of each meat that is prepared.\n\nDiscussion in 'Mac OS X Lion (10.7)' started by axboi87, Jan 20, 2012.\nI've got a 500gb internal drive and a 240gb SSD.\nWhen trying to restore using di\n\n\nIn the algorithm the vocab variable is actually a frequency dictionary of the words. Further, those words have been prepended with an underscore to indicate that they are the beginning of a word. Finally, the characters have been delimited by spaces so that the BPE algorithm can group the most common characters together in the dictionary in a greedy fashion. We will see how that is exactly done shortly.\n\nfrom collections import Counter\n\nvocab = Counter(['\\u2581' + word for word in text.split()])\nvocab = {' '.join([l for l in word]): freq for word, freq in vocab.items()}\n\n\ndef show_vocab(vocab, end='\\n', limit=20):\n    shown = 0\n    for word, freq in vocab.items():\n        print(f'{word}: {freq}', end=end)\n        shown +=1\n        if shown &gt; limit:\n            break\n\n\nshow_vocab(vocab)\n\n▁ B e g i n n e r s: 1\n▁ B B Q: 3\n▁ C l a s s: 2\n▁ T a k i n g: 1\n▁ P l a c e: 1\n▁ i n: 15\n▁ M i s s o u l a !: 1\n▁ D o: 1\n▁ y o u: 13\n▁ w a n t: 1\n▁ t o: 33\n▁ g e t: 2\n▁ b e t t e r: 2\n▁ a t: 1\n▁ m a k i n g: 2\n▁ d e l i c i o u s: 1\n▁ B B Q ?: 1\n▁ Y o u: 1\n▁ w i l l: 6\n▁ h a v e: 4\n▁ t h e: 31\n\n\nWe check the size of the vocabulary (frequency dictionary) because this is the one hyperparameter that BPE depends on crucially on how far it breaks up a word into SentencePieces. It turns out that for our trained model on our small dataset that 60% of 455 merges of the most frequent characters need to be done to reproduce the upperlimit of a 32K vocab_size over the entire corpus of examples.\n\nprint(f'Total number of unique words: {len(vocab)}')\nprint(f'Number of merges required to reproduce SentencePiece training on the whole corpus: {int(0.60*len(vocab))}')\n\nTotal number of unique words: 455\nNumber of merges required to reproduce SentencePiece training on the whole corpus: 273\n\n\n\n\nBPE Algorithm\nDirectly from the BPE paper we have the following algorithm.\nTo understand what’s going on first take a look at the third function get_sentence_piece_vocab. It takes in the current vocab word-frequency dictionary and the fraction of the total vocab_size to merge characters in the words of the dictionary, num_merges times. Then for each merge operation it get_stats on how many of each pair of character sequences there are. It gets the most frequent pair of symbols as the best pair. Then it merges those pair of symbols (removes the space between them) in each word in the vocab that contains this best (= pair). Consquently, merge_vocab creates a new vocab, v_out. This process is repeated num_merges times and the result is the set of SentencePieces (keys of the final sp_vocab).\nPlease feel free to skip the below if the above description was enough.\nIn a little more detail then, we can see in get_stats we initially create a list of bigram frequencies (two character sequence) from our vocabulary. Later, this may include (trigrams, quadgrams, etc.). Note that the key of the pairs frequency dictionary is actually a 2-tuple, which is just shorthand notation for a pair.\nIn merge_vocab we take in an individual pair (of character sequences, note this is the most frequency best pair) and the current vocab as v_in. We create a new vocab, v_out, from the old by joining together the characters in the pair (removing the space), if they are present in the a word of the dictionary. Warning: the expression (?&lt;!\\S) means that either whitespace character follows before the bigram or there is nothing before (beginning of word) the bigram, similarly for (?!\\S) for preceding whitespace or end of word.\n\nimport re, collections\n\ndef get_stats(vocab):\n    pairs = collections.defaultdict(int)\n    for word, freq in vocab.items():\n        symbols = word.split()\n        for i in range(len(symbols) - 1):\n            pairs[symbols[i], symbols[i+1]] += freq\n    return pairs\n\ndef merge_vocab(pair, v_in):\n    v_out = {}\n    bigram = re.escape(' '.join(pair))\n    p = re.compile(r'(?&lt;!\\S)' + bigram + r'(?!\\S)')\n    for word in v_in:\n        w_out = p.sub(''.join(pair), word)\n        v_out[w_out] = v_in[word]\n    return v_out\n\ndef get_sentence_piece_vocab(vocab, frac_merges=0.60):\n    sp_vocab = vocab.copy()\n    num_merges = int(len(sp_vocab)*frac_merges)\n    \n    for i in range(num_merges):\n        pairs = get_stats(sp_vocab)\n        best = max(pairs, key=pairs.get)\n        sp_vocab = merge_vocab(best, sp_vocab)\n\n    return sp_vocab\n\n\nsp_vocab = get_sentence_piece_vocab(vocab)\nshow_vocab(sp_vocab) \n\n▁B e g in n ers: 1\n▁BBQ: 3\n▁Cl ass: 2\n▁T ak ing: 1\n▁P la ce: 1\n▁in: 15\n▁M is s ou la !: 1\n▁D o: 1\n▁you: 13\n▁w an t: 1\n▁to: 33\n▁g et: 2\n▁be t ter: 2\n▁a t: 1\n▁mak ing: 2\n▁d e l ic i ou s: 1\n▁BBQ ?: 1\n▁ Y ou: 1\n▁will: 6\n▁have: 4\n▁the: 31",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "L1 - SentencePiece and BPE"
    ]
  },
  {
    "objectID": "notes/c4w3/lab01.html#train-sentencepiece-bpe-tokenizer-on-example-data",
    "href": "notes/c4w3/lab01.html#train-sentencepiece-bpe-tokenizer-on-example-data",
    "title": "SentencePiece and Byte Pair Encoding",
    "section": "Train SentencePiece BPE Tokenizer on Example Data",
    "text": "Train SentencePiece BPE Tokenizer on Example Data\n\nExplore SentencePiece Model\nFirst let us explore the SentencePiece model provided with this week’s assignment. Remember you can always use Python’s built in help command to see the documentation for any object or method.\n\nimport sentencepiece as spm\nsp = spm.SentencePieceProcessor(model_file='sentencepiece.model')\n\n\nhelp(sp)\n\nHelp on SentencePieceProcessor in module sentencepiece object:\n\nclass SentencePieceProcessor(builtins.object)\n |  SentencePieceProcessor(model_file=None, model_proto=None, out_type=&lt;class 'int'&gt;, add_bos=False, add_eos=False, reverse=False, emit_unk_piece=False, enable_sampling=False, nbest_size=-1, alpha=0.1, num_threads=-1)\n |  \n |  Methods defined here:\n |  \n |  CalculateEntropy(self, input, alpha, num_threads=None)\n |      Calculate sentence entropy\n |  \n |  Decode(self, input, out_type=&lt;class 'str'&gt;, num_threads=None)\n |      Decode processed id or token sequences.\n |      \n |      Args:\n |        out_type: output type. str, bytes or 'serialized_proto' or 'immutable_proto' (Default = str)\n |        num_threads: the number of threads used in the batch processing (Default = -1).\n |  \n |  DecodeIds(self, input, out_type=&lt;class 'str'&gt;, **kwargs)\n |  \n |  DecodeIdsAsImmutableProto(self, input, out_type='immutable_proto', **kwargs)\n |  \n |  DecodeIdsAsSerializedProto(self, input, out_type='serialized_proto', **kwargs)\n |  \n |  DecodePieces(self, input, out_type=&lt;class 'str'&gt;, **kwargs)\n |  \n |  DecodePiecesAsImmutableProto(self, input, out_type='immutable_proto', **kwargs)\n |  \n |  DecodePiecesAsSerializedProto(self, input, out_type='serialized_proto', **kwargs)\n |  \n |  Detokenize = Decode(self, input, out_type=&lt;class 'str'&gt;, num_threads=None)\n |  \n |  Encode(self, input, out_type=None, add_bos=None, add_eos=None, reverse=None, emit_unk_piece=None, enable_sampling=None, nbest_size=None, alpha=None, num_threads=None)\n |      Encode text input to segmented ids or tokens.\n |      \n |      Args:\n |      input: input string. accepsts list of string.\n |      out_type: output type. int or str.\n |      add_bos: Add &lt;s&gt; to the result (Default = false)\n |      add_eos: Add &lt;/s&gt; to the result (Default = false) &lt;s&gt;/&lt;/s&gt; is added after\n |               reversing (if enabled).\n |      reverse: Reverses the tokenized sequence (Default = false)\n |      emit_unk_piece: Emits the unk literal string (Default = false)\n |      nbest_size: sampling parameters for unigram. Invalid in BPE-Dropout.\n |                  nbest_size = {0,1}: No sampling is performed.\n |                  nbest_size &gt; 1: samples from the nbest_size results.\n |                  nbest_size &lt; 0: assuming that nbest_size is infinite and samples\n |                  from the all hypothesis (lattice) using\n |                  forward-filtering-and-backward-sampling algorithm.\n |      alpha: Soothing parameter for unigram sampling, and merge probability for\n |             BPE-dropout (probablity 'p' in BPE-dropout paper).\n |      num_threads: the number of threads used in the batch processing (Default = -1).\n |  \n |  EncodeAsIds(self, input, **kwargs)\n |  \n |  EncodeAsImmutableProto(self, input, **kwargs)\n |  \n |  EncodeAsPieces(self, input, **kwargs)\n |  \n |  EncodeAsSerializedProto(self, input, **kwargs)\n |  \n |  GetPieceSize(self)\n |  \n |  GetScore = _batched_func(self, arg)\n |  \n |  IdToPiece = _batched_func(self, arg)\n |  \n |  Init(self, model_file=None, model_proto=None, out_type=&lt;class 'int'&gt;, add_bos=False, add_eos=False, reverse=False, emit_unk_piece=False, enable_sampling=False, nbest_size=-1, alpha=0.1, num_threads=-1)\n |      Initialzie sentencepieceProcessor.\n |      \n |      Args:\n |        model_file: The sentencepiece model file path.\n |        model_proto: The sentencepiece model serialized proto.\n |        out_type: output type. int or str.\n |        add_bos: Add &lt;s&gt; to the result (Default = false)\n |        add_eos: Add &lt;/s&gt; to the result (Default = false) &lt;s&gt;/&lt;/s&gt; is added after\n |          reversing (if enabled).\n |        reverse: Reverses the tokenized sequence (Default = false)\n |        emit_unk_piece: Emits the unk literal string (Default = false)\n |        nbest_size: sampling parameters for unigram. Invalid in BPE-Dropout.\n |                    nbest_size = {0,1}: No sampling is performed.\n |                    nbest_size &gt; 1: samples from the nbest_size results.\n |                    nbest_size &lt; 0: assuming that nbest_size is infinite and samples\n |                      from the all hypothesis (lattice) using\n |                      forward-filtering-and-backward-sampling algorithm.\n |        alpha: Soothing parameter for unigram sampling, and dropout probability of\n |               merge operations for BPE-dropout.\n |        num_threads: number of threads in batch processing (Default = -1, auto-detected)\n |  \n |  IsByte = _batched_func(self, arg)\n |  \n |  IsControl = _batched_func(self, arg)\n |  \n |  IsUnknown = _batched_func(self, arg)\n |  \n |  IsUnused = _batched_func(self, arg)\n |  \n |  Load(self, model_file=None, model_proto=None)\n |      Overwride SentencePieceProcessor.Load to support both model_file and model_proto.\n |      \n |      Args:\n |        model_file: The sentencepiece model file path.\n |        model_proto: The sentencepiece model serialized proto. Either `model_file`\n |          or `model_proto` must be set.\n |  \n |  LoadFromFile(self, arg)\n |  \n |  LoadFromSerializedProto(self, serialized)\n |  \n |  LoadVocabulary(self, filename, threshold)\n |  \n |  NBestEncode(self, input, out_type=None, add_bos=None, add_eos=None, reverse=None, emit_unk_piece=None, nbest_size=None)\n |      NBestEncode text input to segmented ids or tokens.\n |      \n |      Args:\n |      input: input string. accepsts list of string.\n |      out_type: output type. int or str.\n |      add_bos: Add &lt;s&gt; to the result (Default = false)\n |      add_eos: Add &lt;/s&gt; to the result (Default = false) &lt;s&gt;/&lt;/s&gt; is added after reversing (if enabled).\n |      reverse: Reverses the tokenized sequence (Default = false)\n |      emit_unk_piece: Emits the unk literal string (Default = false)\n |      nbest_size: nbest size\n |  \n |  NBestEncodeAsIds(self, input, nbest_size=None, **kwargs)\n |  \n |  NBestEncodeAsImmutableProto(self, input, nbest_size=None, **kwargs)\n |  \n |  NBestEncodeAsPieces(self, input, nbest_size=None, **kwargs)\n |  \n |  NBestEncodeAsSerializedProto(self, input, nbest_size=None, **kwargs)\n |  \n |  Normalize(self, input, with_offsets=None)\n |  \n |  OverrideNormalizerSpec(self, **kwargs)\n |  \n |  PieceToId = _batched_func(self, arg)\n |  \n |  ResetVocabulary(self)\n |  \n |  SampleEncodeAndScore(self, input, out_type=None, add_bos=None, add_eos=None, reverse=None, emit_unk_piece=None, num_samples=None, alpha=None, wor=None, include_best=None)\n |      SampleEncodeAndScore text input to segmented ids or tokens.\n |      \n |      Args:\n |      input: input string. accepsts list of string.\n |      out_type: output type. int or str or 'serialized_proto' or 'immutable_proto'\n |      add_bos: Add &lt;s&gt; to the result (Default = false)\n |      add_eos: Add &lt;/s&gt; to the result (Default = false) &lt;s&gt;/&lt;/s&gt; is added after reversing (if enabled).\n |      reverse: Reverses the tokenized sequence (Default = false)\n |      emit_unk_piece: Emits the unk literal string (Default = false)\n |      num_samples: How many samples to return (Default = 1)\n |      alpha: inverse temperature for sampling\n |      wor: whether to sample without replacement (Default = false)\n |      include_best: whether to include the best tokenization, requires wor=True (Default = false)\n |  \n |  SampleEncodeAndScoreAsIds(self, input, num_samples=None, alpha=None, **kwargs)\n |  \n |  SampleEncodeAndScoreAsImmutableProto(self, input, num_samples=None, alpha=None, **kwargs)\n |  \n |  SampleEncodeAndScoreAsPieces(self, input, num_samples=None, alpha=None, **kwargs)\n |  \n |  SampleEncodeAndScoreAsSerializedProto(self, input, num_samples=None, alpha=None, **kwargs)\n |  \n |  SampleEncodeAsIds(self, input, nbest_size=None, alpha=None, **kwargs)\n |  \n |  SampleEncodeAsImmutableProto(self, input, nbest_size=None, alpha=None, **kwargs)\n |  \n |  SampleEncodeAsPieces(self, input, nbest_size=None, alpha=None, **kwargs)\n |  \n |  SampleEncodeAsSerializedProto(self, input, nbest_size=None, alpha=None, **kwargs)\n |  \n |  SetDecodeExtraOptions(self, extra_option)\n |  \n |  SetEncodeExtraOptions(self, extra_option)\n |  \n |  SetVocabulary(self, valid_vocab)\n |  \n |  Tokenize = Encode(self, input, out_type=None, add_bos=None, add_eos=None, reverse=None, emit_unk_piece=None, enable_sampling=None, nbest_size=None, alpha=None, num_threads=None)\n |  \n |  __getitem__(self, piece)\n |  \n |  __getstate__(self)\n |  \n |  __init__ = Init(self, model_file=None, model_proto=None, out_type=&lt;class 'int'&gt;, add_bos=False, add_eos=False, reverse=False, emit_unk_piece=False, enable_sampling=False, nbest_size=-1, alpha=0.1, num_threads=-1)\n |  \n |  __len__(self)\n |  \n |  __repr__ = _swig_repr(self)\n |  \n |  __setstate__(self, serialized_model_proto)\n |  \n |  bos_id(self)\n |  \n |  calculate_entropy = CalculateEntropy(self, input, alpha, num_threads=None)\n |  \n |  decode = Decode(self, input, out_type=&lt;class 'str'&gt;, num_threads=None)\n |  \n |  decode_ids = DecodeIds(self, input, out_type=&lt;class 'str'&gt;, **kwargs)\n |  \n |  decode_ids_as_immutable_proto = DecodeIdsAsImmutableProto(self, input, out_type='immutable_proto', **kwargs)\n |  \n |  decode_ids_as_serialized_proto = DecodeIdsAsSerializedProto(self, input, out_type='serialized_proto', **kwargs)\n |  \n |  decode_pieces = DecodePieces(self, input, out_type=&lt;class 'str'&gt;, **kwargs)\n |  \n |  decode_pieces_as_immutable_proto = DecodePiecesAsImmutableProto(self, input, out_type='immutable_proto', **kwargs)\n |  \n |  decode_pieces_as_serialized_proto = DecodePiecesAsSerializedProto(self, input, out_type='serialized_proto', **kwargs)\n |  \n |  detokenize = Decode(self, input, out_type=&lt;class 'str'&gt;, num_threads=None)\n |  \n |  encode = Encode(self, input, out_type=None, add_bos=None, add_eos=None, reverse=None, emit_unk_piece=None, enable_sampling=None, nbest_size=None, alpha=None, num_threads=None)\n |  \n |  encode_as_ids = EncodeAsIds(self, input, **kwargs)\n |  \n |  encode_as_immutable_proto = EncodeAsImmutableProto(self, input, **kwargs)\n |  \n |  encode_as_pieces = EncodeAsPieces(self, input, **kwargs)\n |  \n |  encode_as_serialized_proto = EncodeAsSerializedProto(self, input, **kwargs)\n |  \n |  eos_id(self)\n |  \n |  get_piece_size = GetPieceSize(self)\n |  \n |  get_score = _batched_func(self, arg)\n |  \n |  id_to_piece = _batched_func(self, arg)\n |  \n |  init = Init(self, model_file=None, model_proto=None, out_type=&lt;class 'int'&gt;, add_bos=False, add_eos=False, reverse=False, emit_unk_piece=False, enable_sampling=False, nbest_size=-1, alpha=0.1, num_threads=-1)\n |  \n |  is_byte = _batched_func(self, arg)\n |  \n |  is_control = _batched_func(self, arg)\n |  \n |  is_unknown = _batched_func(self, arg)\n |  \n |  is_unused = _batched_func(self, arg)\n |  \n |  load = Load(self, model_file=None, model_proto=None)\n |  \n |  load_from_file = LoadFromFile(self, arg)\n |  \n |  load_from_serialized_proto = LoadFromSerializedProto(self, serialized)\n |  \n |  load_vocabulary = LoadVocabulary(self, filename, threshold)\n |  \n |  nbest_encode = NBestEncode(self, input, out_type=None, add_bos=None, add_eos=None, reverse=None, emit_unk_piece=None, nbest_size=None)\n |  \n |  nbest_encode_as_ids = NBestEncodeAsIds(self, input, nbest_size=None, **kwargs)\n |  \n |  nbest_encode_as_immutable_proto = NBestEncodeAsImmutableProto(self, input, nbest_size=None, **kwargs)\n |  \n |  nbest_encode_as_pieces = NBestEncodeAsPieces(self, input, nbest_size=None, **kwargs)\n |  \n |  nbest_encode_as_serialized_proto = NBestEncodeAsSerializedProto(self, input, nbest_size=None, **kwargs)\n |  \n |  normalize = Normalize(self, input, with_offsets=None)\n |  \n |  override_normalizer_spec = OverrideNormalizerSpec(self, **kwargs)\n |  \n |  pad_id(self)\n |  \n |  piece_size(self)\n |  \n |  piece_to_id = _batched_func(self, arg)\n |  \n |  reset_vocabulary = ResetVocabulary(self)\n |  \n |  sample_encode_and_score = SampleEncodeAndScore(self, input, out_type=None, add_bos=None, add_eos=None, reverse=None, emit_unk_piece=None, num_samples=None, alpha=None, wor=None, include_best=None)\n |  \n |  sample_encode_and_score_as_ids = SampleEncodeAndScoreAsIds(self, input, num_samples=None, alpha=None, **kwargs)\n |  \n |  sample_encode_and_score_as_immutable_proto = SampleEncodeAndScoreAsImmutableProto(self, input, num_samples=None, alpha=None, **kwargs)\n |  \n |  sample_encode_and_score_as_pieces = SampleEncodeAndScoreAsPieces(self, input, num_samples=None, alpha=None, **kwargs)\n |  \n |  sample_encode_and_score_as_serialized_proto = SampleEncodeAndScoreAsSerializedProto(self, input, num_samples=None, alpha=None, **kwargs)\n |  \n |  sample_encode_as_ids = SampleEncodeAsIds(self, input, nbest_size=None, alpha=None, **kwargs)\n |  \n |  sample_encode_as_immutable_proto = SampleEncodeAsImmutableProto(self, input, nbest_size=None, alpha=None, **kwargs)\n |  \n |  sample_encode_as_pieces = SampleEncodeAsPieces(self, input, nbest_size=None, alpha=None, **kwargs)\n |  \n |  sample_encode_as_serialized_proto = SampleEncodeAsSerializedProto(self, input, nbest_size=None, alpha=None, **kwargs)\n |  \n |  serialized_model_proto(self)\n |  \n |  set_decode_extra_options = SetDecodeExtraOptions(self, extra_option)\n |  \n |  set_encode_extra_options = SetEncodeExtraOptions(self, extra_option)\n |  \n |  set_vocabulary = SetVocabulary(self, valid_vocab)\n |  \n |  tokenize = Encode(self, input, out_type=None, add_bos=None, add_eos=None, reverse=None, emit_unk_piece=None, enable_sampling=None, nbest_size=None, alpha=None, num_threads=None)\n |  \n |  unk_id(self)\n |  \n |  vocab_size(self)\n |  \n |  ----------------------------------------------------------------------\n |  Static methods defined here:\n |  \n |  __swig_destroy__ = delete_SentencePieceProcessor(...)\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  thisown\n |      The membership flag\n\n\n\nLet’s work with the first sentence of our example text.\n\ns0 = 'Beginners BBQ Class Taking Place in Missoula!'\n\n\n# encode: text =&gt; id\nprint(sp.encode_as_pieces(s0))\nprint(sp.encode_as_ids(s0))\n\n# decode: id =&gt; text\nprint(sp.decode_pieces(sp.encode_as_pieces(s0)))\nprint(sp.decode_ids([12847, 277]))\n\n['▁Beginn', 'ers', '▁BBQ', '▁Class', '▁', 'Taking', '▁Place', '▁in', '▁Miss', 'oul', 'a', '!']\n[12847, 277, 15068, 4501, 3, 12297, 3399, 16, 5964, 7115, 9, 55]\nBeginners BBQ Class Taking Place in Missoula!\nBeginners\n\n\nNotice how SentencePiece breaks the words into seemingly odd parts, but we’ve seen something similar from our work with BPE. But how close were we to this model trained on the whole corpus of examles with a vocab_size of 32,000 instead of 455? Here you can also test what happens to white space, like ‘’.\nBut first let us note that SentencePiece encodes the SentencePieces, the tokens, and has reserved some of the ids as can be seen in this week’s assignment.\n\nuid = 15068\nspiece = \"\\u2581BBQ\"\nunknown = \"__MUST_BE_UNKNOWN__\"\n\n# id &lt;=&gt; piece conversion\nprint(f'SentencePiece for ID {uid}: {sp.id_to_piece(uid)}')\nprint(f'ID for Sentence Piece {spiece}: {sp.piece_to_id(spiece)}')\n\n# returns 0 for unknown tokens (we can change the id for UNK)\nprint(f'ID for unknown text {unknown}: {sp.piece_to_id(unknown)}')\n\nSentencePiece for ID 15068: ▁BBQ\nID for Sentence Piece ▁BBQ: 15068\nID for unknown text __MUST_BE_UNKNOWN__: 2\n\n\n\nprint(f'Beginning of sentence id: {sp.bos_id()}')\nprint(f'Pad id: {sp.pad_id()}')\nprint(f'End of sentence id: {sp.eos_id()}')\nprint(f'Unknown id: {sp.unk_id()}')\nprint(f'Vocab size: {sp.vocab_size()}')\n\nBeginning of sentence id: -1\nPad id: 0\nEnd of sentence id: 1\nUnknown id: 2\nVocab size: 32000\n\n\nWe can also check what are the ids for the first part and last part of the vocabulary.\n\nprint('\\nId\\tSentP\\tControl?')\nprint('------------------------')\n# &lt;unk&gt;, &lt;s&gt;, &lt;/s&gt; are defined by default. Their ids are (0, 1, 2)\n# &lt;s&gt; and &lt;/s&gt; are defined as 'control' symbol.\nfor uid in range(10):\n    print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')\n    \n# for uid in range(sp.vocab_size()-10,sp.vocab_size()):\n#     print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')\n\n\nId  SentP   Control?\n------------------------\n0   &lt;pad&gt;   True\n1   &lt;/s&gt;    True\n2   &lt;unk&gt;   False\n3   ▁   False\n4   X   False\n5   .   False\n6   ,   False\n7   s   False\n8   ▁the    False\n9   a   False\n\n\n\n\nTrain SentencePiece BPE model with our example.txt\nFinally, let’s train our own BPE model directly from the SentencePiece library and compare it to the results of our implemention of the algorithm from the BPE paper itself.\n\nspm.SentencePieceTrainer.train('--input=example.txt --model_prefix=example_bpe --vocab_size=450 --model_type=bpe')\nsp_bpe = spm.SentencePieceProcessor()\nsp_bpe.load('example_bpe.model')\n\nprint('*** BPE ***')\nprint(sp_bpe.encode_as_pieces(s0))\n\nsentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=example.txt --model_prefix=example_bpe --vocab_size=450 --model_type=bpe\nsentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \ntrainer_spec {\n  input: example.txt\n  input_format: \n  model_prefix: example_bpe\n  model_type: BPE\n  vocab_size: 450\n  self_test_sample_size: 0\n  character_coverage: 0.9995\n  input_sentence_size: 0\n  shuffle_input_sentence: 1\n  seed_sentencepiece_size: 1000000\n  shrinking_factor: 0.75\n  max_sentence_length: 4192\n  num_threads: 16\n  num_sub_iterations: 2\n  max_sentencepiece_length: 16\n  split_by_unicode_script: 1\n  split_by_number: 1\n  split_by_whitespace: 1\n  split_digits: 0\n  pretokenization_delimiter: \n  treat_whitespace_as_suffix: 0\n  allow_whitespace_only_pieces: 0\n  required_chars: \n  byte_fallback: 0\n  vocabulary_output_piece_score: 1\n  train_extremely_large_corpus: 0\n  seed_sentencepieces_file: \n  hard_vocab_limit: 1\n  use_all_vocab: 0\n  unk_id: 0\n  bos_id: 1\n  eos_id: 2\n  pad_id: -1\n  unk_piece: &lt;unk&gt;\n  bos_piece: &lt;s&gt;\n  eos_piece: &lt;/s&gt;\n  pad_piece: &lt;pad&gt;\n  unk_surface:  ⁇ \n  enable_differential_privacy: 0\n  differential_privacy_noise_level: 0\n  differential_privacy_clipping_threshold: 0\n}\nnormalizer_spec {\n  name: nmt_nfkc\n  add_dummy_prefix: 1\n  remove_extra_whitespaces: 1\n  escape_whitespaces: 1\n  normalization_rule_tsv: \n}\ndenormalizer_spec {}\ntrainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\ntrainer_interface.cc(185) LOG(INFO) Loading corpus: example.txt\ntrainer_interface.cc(409) LOG(INFO) Loaded all 26 sentences\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: &lt;unk&gt;\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: &lt;s&gt;\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: &lt;/s&gt;\ntrainer_interface.cc(430) LOG(INFO) Normalizing sentences...\ntrainer_interface.cc(539) LOG(INFO) all chars count=4533\ntrainer_interface.cc(550) LOG(INFO) Done: 99.9559% characters are covered.\ntrainer_interface.cc(560) LOG(INFO) Alphabet size=73\ntrainer_interface.cc(561) LOG(INFO) Final character coverage=0.999559\ntrainer_interface.cc(592) LOG(INFO) Done! preprocessed 26 sentences.\ntrainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 26\ntrainer_interface.cc(609) LOG(INFO) Done! 455\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=99 min_freq=1\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=31 size=20 all=732 active=658 piece=▁w\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=40 all=937 active=863 piece=ch\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=60 all=1014 active=940 piece=▁u\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=80 all=1110 active=1036 piece=me\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=100 all=1166 active=1092 piece=la\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=0\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=120 all=1217 active=1042 piece=SD\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=140 all=1272 active=1097 piece=▁bu\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=160 all=1288 active=1113 piece=▁site\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=180 all=1315 active=1140 piece=ter\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=200 all=1330 active=1155 piece=asure\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=0\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=220 all=1339 active=1008 piece=ge\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=240 all=1371 active=1040 piece=▁sh\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=260 all=1384 active=1053 piece=▁cost\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=280 all=1391 active=1060 piece=de\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=300 all=1405 active=1074 piece=000\nbpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=0\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=320 all=1427 active=1021 piece=▁GB\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=340 all=1438 active=1032 piece=last\nbpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=360 all=1441 active=1035 piece=▁let\ntrainer_interface.cc(687) LOG(INFO) Saving model: example_bpe.model\ntrainer_interface.cc(699) LOG(INFO) Saving vocabs: example_bpe.vocab\n\n\nTrue\n\n\n*** BPE ***\n['▁B', 'e', 'ginn', 'ers', '▁BBQ', '▁Cl', 'ass', '▁T', 'ak', 'ing', '▁P', 'la', 'ce', '▁in', '▁M', 'is', 's', 'ou', 'la', '!']\n\n\n\nshow_vocab(sp_vocab, end = ', ')\n\n▁B e g in n ers: 1, ▁BBQ: 3, ▁Cl ass: 2, ▁T ak ing: 1, ▁P la ce: 1, ▁in: 15, ▁M is s ou la !: 1, ▁D o: 1, ▁you: 13, ▁w an t: 1, ▁to: 33, ▁g et: 2, ▁be t ter: 2, ▁a t: 1, ▁mak ing: 2, ▁d e l ic i ou s: 1, ▁BBQ ?: 1, ▁ Y ou: 1, ▁will: 6, ▁have: 4, ▁the: 31, \n\n\nOur implementation of BPE’s code from the paper matches up pretty well with the library itself! Difference are probably accounted for by the vocab_size. There is also another technical difference in that in the SentencePiece implementation of BPE a priority queue is used to more efficiently keep track of the best pairs. Actually, there is a priority queue in the Python standard library called heapq if you would like to give that a try below!\n\nfrom heapq import heappush, heappop\n\n\ndef heapsort(iterable):\n    h = []\n    for value in iterable:\n        heappush(h, value)\n    return [heappop(h) for i in range(len(h))]\n\n\na = [1,4,3,1,3,2,1,4,2]\nheapsort(a)\n\n[1, 1, 1, 2, 2, 3, 3, 4, 4]\n\n\nFor a more extensive example consider looking at the SentencePiece repo. The last section of this code is repurposed from that tutorial. Thanks for your participation!",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "L1 - SentencePiece and BPE"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html",
    "href": "notes/c4w3/index.html",
    "title": "Question Answering",
    "section": "",
    "text": "course banner",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#sec-question-answering",
    "href": "notes/c4w3/index.html#sec-question-answering",
    "title": "Question Answering",
    "section": "Question Answering",
    "text": "Question Answering\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 1\n\n\nMy notes for Week 3 of the Natural Language Processing with Attention Labels Course in the Natural Language Processing Specialization Offered by DeepLearning.AI on Coursera\n\n\n\n\n\n\nLearning Objectives:\n\n\n\n\nGain intuition for how transfer learning works in the context of NLP\nIdentify two approaches to transfer learning\nDiscuss the evolution of language models from CBOW to T5 and Bert\nFine-tune BERT on a dataset\nImplement context-based question answering with T5\nInterpret the GLUE benchmark",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#sec-overview",
    "href": "notes/c4w3/index.html#sec-overview",
    "title": "Question Answering",
    "section": "Overview",
    "text": "Overview\n In this week we are going to learn about transfer learning. More specifically we will understand how T5 and BERT actually work.\n\n\n\n\nquestion-answering\n\n\n\n\n\n\n\nDefinitions:\n\n\n\nQ&A comes in two forms:\ncontext based : given a document and a question the model extracts an answer or generates an answer\nclosed book : the model picks an answer from several options (classifier)\n\n\n\n\n\n\ntl\n\n\n\n\nclassical-training\n\n\n\n\ntransfer-learning\n\n\n\n We can see how a model initially trained on some type of sentiment classification, could now be used for question answering. One other model that has state of the art makes use of multi tasking. For example, the same model could be used for sentiment analysis, question answering, and many other things.\n\n\n\n\ngoals\n\nThese new types of models make use of a lot of data. For example the C4 (colossal cleaned crawled corpus) is about 800 GB when all of the english wikipedia is just 13 GB!\n\nC4 is a colossal, cleaned version of Common Crawl’s web crawl corpus. It was based on Common Crawl dataset. It was used to train the T5 text-to-text Transformer models. Introduced by Raffel et al. (2023) in a paper titled “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer” The dataset can be downloaded in a pre-processed form from allennlp. C4 at papers with code",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#sec-video2-transfer-learning-nlp",
    "href": "notes/c4w3/index.html#sec-video2-transfer-learning-nlp",
    "title": "Question Answering",
    "section": "Transfer Learning in NLP",
    "text": "Transfer Learning in NLP\n\n\n\n\ntransfer-learning-options\n\n\n\n\ntl-general-purpose\n\n\n\n\ntl-features-vs-fine-tuning\n\n\n\n\ntl-fine-tuning\n\n\n\n\ntl-pretain-data-performance\n\n\n\n\ntl-pretain-data-supervision\n\n\n\n\ntl-pretain-unsupervised\n\n\n\n\ntl-pretrain-selfsupervised\n\n\n\n\ntl-pretrain-selfsupervised\n\n\n\n\ntl-per-task-fine-tuning\n\n\n\n\ntl-summary\n\n\n\n\n\n\n\n\n\n\n\nThere are three main advantages to transfer learning:\n\nReduce training time\nImprove predictions\nAllows we to use smaller datasets\n\nTwo methods that we can use for transfer learning are the following:\n\npre-training\nfine tuning\n\nIn feature based, we can train word embeddings by running a different model and then using those features (i.e. word vectors) on a different task. When fine tuning, we can use the exact same model and just run it on a different task. Sometimes when fine tuning, we can keep the model weights fixed and just add a new layer that we will train. Other times we can slowly unfreeze the layers one at a time. We can also use unlabelled data when pre-training, by masking words and trying to predict which word was masked.\nFor example, in the drawing above we try to predict the word “friend”. This allows your model to get a grasp of the overall structure of the data and to help the model learn some relationships within the words of a sentence",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#sec-video3-elmo-gpt-bert-t5",
    "href": "notes/c4w3/index.html#sec-video3-elmo-gpt-bert-t5",
    "title": "Question Answering",
    "section": "ELMo, GPT, BERT, T5",
    "text": "ELMo, GPT, BERT, T5\n\n\n\n\noutline\n\n\n\n\n\n\nCBOW-fixed-window\n\nThe models mentioned in the previous video were discovered in the following order.\n\nCBOW in Word2Vec - Issue: Fixed window we want all the context\n\n2013 Word2Vec Google\nCBOW & Skip grams\n\n2014 Glove Stanfor GloVe: Global Vectors for Word ()\n\nElMo - Bidirectional LSTM\n\nSolves: fixed window size using a biderectional RNN\nIssue: weak long term dependency\n\nGPT2 - issue: unidirectional. only looks back\nBERT - just encoder - biderctional, multi mask learning\nT5 - Encoder Decoder - multi-task learning",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#cbow",
    "href": "notes/c4w3/index.html#cbow",
    "title": "Question Answering",
    "section": "CBOW",
    "text": "CBOW\n\n\n\n\nCBOW-issues\n\n\n\n\nELMo-solution\n\n\n\n\nELMo-RNN\n\n\n\n\nGPT-unidirectional\n\n\n\n\nBERT\n\n\n\n\nmulti-mask\n\n\n\n\nBERT-pre-training\n\n\n\n\nt5-encoder-decoder\n\n\n\n\n\n\n\n\nIn CBOW, we want to encode a word as a vector. To do this we used the context before the word and the context after the word and we use that model to learn and creates features for the word. CBOW however uses a fixed window C (for the context).\nthe main isused with CBOW are:\n\nit has a fixed window size\nno concept of order\n\nso what do we do when we need more context to model the concept we are looking at?\nWhat ElMo does, it uses a bi-directional LSTM, which is a version of an RNN that looks at the inputs from the left and the right. This has the added benefit that the context size is no longer constrained. But since it is an RNN it has problems propagating information as sequences grow longer.\nThen Open AI introduced GPT. GPT unfortunately is uni-directional but it makes use of transformers. Although ElMo was bi-directional, it suffered from some issues such as capturing longer-term dependencies.\nBERT was then introduced which stands for the Bi-directional Encoder Representation from Transformers.\nT5 was introduced which makes use of transfer learning and uses the same model to predict on many tasks.\n\nGPT was a transformer decoder\nBERT was a transformer encoder\nT5 is a decoder encoder\n\n\n\n\n\nt5-text-to-text\n\nHere is an illustration of how T5 works:\n\n\n\n\nquestion\n\n\n\n\nsummary\n\n\nSo we can now flesh out the table",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#sec-video4-bert",
    "href": "notes/c4w3/index.html#sec-video4-bert",
    "title": "Question Answering",
    "section": "BERT Bidirectional Encoder Representations from Transformers",
    "text": "BERT Bidirectional Encoder Representations from Transformers\n\n\n\n\nBERT-outline\n\n\n\n\nBERT-question\n\n\n\n\nBERT-summary\n\n\n\nlets dive deeper into BERT\nThere are two steps in the BERT framework: pre-training and fine-tuning. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For fine tuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. For example, in the figure above, we get the corresponding embeddings for the input words, we run it through a few transformer blocks, and then we make the prediction at each time point T_i.\nTraining procedures:\n\nChoose 15% of the tokens at random:\n\nmask them 80% of the time,\nreplace them with a random token 10% of the time,\nkeep as is 10% of the time. There could be multiple masked spans in a sentence. Next sentence prediction is also used when pre-training.\n\n\n\n\n\n\nBERT\n\n\n\n\nBERT-spec\n\n\nSpec and features:\n\n\n\n\nBERT-pre-training",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#sec-video5-bert-objective",
    "href": "notes/c4w3/index.html#sec-video5-bert-objective",
    "title": "Question Answering",
    "section": "BERT Objective",
    "text": "BERT Objective\n\n\n\n\nBERT-outline\n\nMLM - masked language modeling.\nThis is the main unsupervised procedure to train the model with context left and right. It’s not clear how the model handles multiple masked items.\nDoes it try to predict them all at once or each one by considering input as context and unknowns.\n\n\n\n\nBERT-the-input\n\nThe input embeddings are the sum of the token embeddings, the segmentation embeddings and the position embeddings. The input embeddings: we have a CLS token to indicate the beginning of the sentence and a sep to indicate the end of the sentence The segment embeddings: allows we to indicate whether it is sentence a or b. Positional embeddings: allows we to indicate the word’s position in the sentence.\n\n\n\n\nBERT-the-output\n\nThe C token in the image above could be used for classification purposes. The unlabeled sentence A/B pair will depend on what we are trying to predict, it could range from question answering to sentiment. (in which case the second sentence could be just empty).\n\n\n\n\nBERT-objectives\n\nThe BERT objective is defined as follows:\n\n\n\n\nBERT-summary",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#sec-video6-fine-tuning-bert",
    "href": "notes/c4w3/index.html#sec-video6-fine-tuning-bert",
    "title": "Question Answering",
    "section": "Fine tuning BERT",
    "text": "Fine tuning BERT\n\n\n\n\nBERT-fine-tuning-outline\n\nOnce we have a pre-trained model, we can fine tune it on different tasks.\n\n\n\n\ninputs\n\nFor example, given a hypothesis, we can identify the premise. Given a question, we can find the answer. We can also use it for named entity recognition. Here is a summary of the inputs.\n\nWe can replace sentences A/B\nParaphrase from sentence A\nQuestion/passage\nHypothesis premise pairs in entailment\nText and a Ø for classification/sequence tagging\nOutput tokens are fed into a layer for token level tasks otherwise use [CLS] embedding as input.\n\n\n\n\n\nsummary",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#transformer-t5",
    "href": "notes/c4w3/index.html#transformer-t5",
    "title": "Question Answering",
    "section": "Transformer: T5",
    "text": "Transformer: T5\n\n\n\n\nt5-outline\n\n\n\n\nt5-text-to-text\n\n\n\n\nT5-transformer\n\n\n\nOne of the major techniques that allowed the T5 model to reach state of the art is the concept of masking:\nFor example, we represent the “for inviting” with &lt;X&gt; and last with &lt;Y&gt; then the model predicts what the X should be and what the Y should be. This is exactly what we saw in the BERT loss. We can also mask out a few positions, not just one. The loss is only on the mask for BERT, for T5 it is on the target.\n\n\n\n\nT5-architecture\n\nSo we start with the basic encoder-decoder representation. There we have a fully visible attention in the encoder and then causal attention in the decoder. So light gray lines correspond to causal masking. And dark gray lines correspond to the fully visible masking.\nIn the middle we have the language model which consists of a single transformer layer stack. And it’s being fed the concatenation of the inputs and the target. So it uses causal masking throughout as we can see because they’re all gray lines. And we have X_1 going inside, we get X_2, X_2 goes into the model and we get X3 and so forth.\nTo the right, we have prefix language model which corresponds to allowing fully visible masking over the inputs as we can see with the dark arrows. And then causal masking in the rest.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#sec-video8-multi-task-training-strategy",
    "href": "notes/c4w3/index.html#sec-video8-multi-task-training-strategy",
    "title": "Question Answering",
    "section": "Lecture Multi-Task Training Strategy",
    "text": "Lecture Multi-Task Training Strategy\n\n\n\n\nT5-architecture\n\n\n\n\nT5-summary\n\n\n\n\nT5-multi-task-training\n\n\n\nThis is a reminder of how the T5 model works:\nWe can see that we only have to add a small prefix to the input and the model as a result will solve the task for you. There are many tasks that the t5 model can do for you. It is possible to formulate most NLP tasks in a “text-to-text” format – that is, a task where the model is fed some text for context or conditioning and is then asked to produce some output text. This framework provides a consistent training objective both for pre-training and fine-tuning. Specifically, the model is trained with a maximum likelihood objective (using “teacher forcing” ) regardless of the task.\n\nTraining data strategies\n\nExamples-proportional mixing\n\nsample in proportion to the size of each task’s dataset\n\nTemperature scaled mixing\n\nadjust the “temperature”” of the mixing rates. This temperature parameter allows we to weight certain examples more than others. To implement temperature scaling with temperature T, we raise each task’s mixing rate rm to the power of 1⁄T and renormalize the rates so that they sum to 1. When T = 1, this approach is equivalent to examples-proportional mixing and as T increases the proportions become closer to equal mixing\n\nEqual mixing\n\nIn this case, we sample examples from each task with equal probability. Specifically, each example in each batch is sampled uniformly at random from one of the datasets we train on.\n\n\n\n\n\n\nio-format\n\n\n\n\nmulti-task-training\n\n\n\n\ndata-training-strategy\n\n\n\n\nunfreezing-adapter-layers\n\n\n\n\nquestion\n\n\n\n\nfine-tuning\n\n\n\n\n\n\nWe can see how fine tuning on a specific task could work even though we were pre-training on different tasks.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#sec-video9-glue-benchmark",
    "href": "notes/c4w3/index.html#sec-video9-glue-benchmark",
    "title": "Question Answering",
    "section": "GLUE Benchmark",
    "text": "GLUE Benchmark\n\n\n\n\nGLUE-evaluation\n\n\n\n\nGLUE-tasks\n\n\n\n\nGLUE\n\n\n\nGeneral Language Understanding Evaluation (GLUE) is contains:\n\nA collection used to train, evaluate, analyze natural language understanding systems\nDatasets with different genres, and of different sizes and difficulties\nLeaderboard\n\nCurrently T5 is state of the art according to this GLUE benchmark and we will be implementing it for homework this week! This GLUE bench mark is used for research purposes, it is model agnostic, and relies on models that make use of transfer learning.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#sec-video10-question-answering",
    "href": "notes/c4w3/index.html#sec-video10-question-answering",
    "title": "Question Answering",
    "section": "Question Answering",
    "text": "Question Answering\n We will be implementing an encoder this week. Last week we implemented the decoder. So here it is:\n\n\n\n\nBERT-encoder-Block\n\n\n\n\nBERT-blocks\n\n\n\n\nq&a-data-example\n\n\n\n\nq&a-with-t5\n\n\n\n\nt5\n\n\n\n\nt5-question\n\n\n\n\n\n\nWe can see there is a feed forward and the encoder-block above. It makes use of two residual connections, layer normalization, and dropout.\nThe steps we will follow to implement it are:\n\nLoad a pre-trained model\nProcess data to get the required inputs and outputs: “question: Q context: C” as input and “A” as target\nFine tune your model on the new task and input\nPredict using your own model",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#sec-programming-assignment-question-answering",
    "href": "notes/c4w3/index.html#sec-programming-assignment-question-answering",
    "title": "Question Answering",
    "section": "Programming Assignment: Question Answering",
    "text": "Programming Assignment: Question Answering",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#sec-lab-sentencepiece-and-bpe",
    "href": "notes/c4w3/index.html#sec-lab-sentencepiece-and-bpe",
    "title": "Question Answering",
    "section": "Lab: SentencePiece and BPE",
    "text": "Lab: SentencePiece and BPE\n\nNFKC Normalization\nunicode normalization - for accents, diacritics and friends\nfrom unicodedata import normalize\nnorm_eaccent = normalize('NFKC', '\\u00E9')\nnorm_e_accent = normalize('NFKC', '\\u0065\\u0301')\nprint(f'{norm_eaccent} = {norm_e_accent} : {norm_eaccent == norm_e_accent}')",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#lossless-tokenization",
    "href": "notes/c4w3/index.html#lossless-tokenization",
    "title": "Question Answering",
    "section": "lossless tokenization",
    "text": "lossless tokenization\nTo ensure this lossless tokenization it replaces white space with _ (U+2581).\ns_ = s.replace(' ', '\\u2581')\n\nSentencePiece\nTokenization with SentencePiece lab\n\n\nBPE",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#sec-lab-bert-loss",
    "href": "notes/c4w3/index.html#sec-lab-bert-loss",
    "title": "Question Answering",
    "section": "Lab: BERT Loss",
    "text": "Lab: BERT Loss",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#sec-lab-t5",
    "href": "notes/c4w3/index.html#sec-lab-t5",
    "title": "Question Answering",
    "section": "Lab: T5",
    "text": "Lab: T5\nopen in coloab",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#representation.-pdf-bib",
    "href": "notes/c4w3/index.html#representation.-pdf-bib",
    "title": "Question Answering",
    "section": "Representation. [pdf] [bib]",
    "text": "Representation. [pdf] [bib]\n\n2017 fasttext Facebook CBOW\n\nmorphological via sub words Algorithm of fasttext is based on these two papers:[8]\nEnriching Word Vectors with Subword Information , Piotr Bojanowski, Edouard Grave, Armand Joulin and Tomas Mikolov, 2016\nBag of Tricks for Efficient Text Classification, Armand Joulin, Edouard Grave, Piotr Bojanowski, Tomas Mikolov, 2016\n\n2018 ELMO Allen Institute for AI ELMo - Character based Bidirectional LSTM - Issue: long term dependency is weak due to vanishing gradient and information loss.\nGPT Encoder only with left context\nBert uses\n2020 T5 uses a label to specify task uses task specific bidirectional lstm to build the embeddings\nBERT Decoder only\n\nInput Token embedding - the distributed representation of the tokens in one space S with Dim(S)=D\nSegment embedding - because the model cannot tell the segment apart\n\nPosition embedding because the model cannot discriminate the word position. \nNote we are trying to mimic RNN behavior but we don’t have recursion:\nNote these are added - they all live in S. Question: would putting S and P in their own dimensions more interpretable. Questions: how do we know the model does not have embeddings that are similar to E_A and E_0 Output CLS - classification token SEP - separator token convert to embedding C is used for next sentence prediction T_i are used for masked word prediction T\nCross entropy loss + Binary loss\n\ncross entropy loss to compare between two distribution from Softmax\n\nbinary loss - could use cross entropy on two cat.\nPretraining\n        before feeding data we mask 15% of the tokens.\nmask 80% of the time:\ntraining data generator chooses 15%. of these at random for prediction\nreplace with:\nmask .8 of the time a random word .1 of the time\noriginal world otherwise.\n\na sentence may have multiple masks.\n\nnext sentence prediction also used in pre training.\nwhy/how\n(s1,s2) true/false\n\n\nBERT_Base\n12 layers\n12 attention heads\n110 million parameters\nFine tuning BERT\nFine tuning\nT5 like BERT does Transfer learning + fine tuning. classification, MT, NE, Sentiment\nSo we can see over here we have fully visible attention in the encoder and then causal attention in the decoder. \nAnd then we have the general encoder-decoder representation just as \nnotation. \nSo light gray lines correspond to causal masking. \nAnd dark gray lines correspond to the fully visible masking. \nSo on the left as I said again, it's the standard encoder-decoder architecture. \nIn the middle over here what we have, \nwe have the language model which consists of a single transformer layer stack. \nAnd it's being fed the concatenation of the inputs and the target. \nSo it uses causal masking throughout as we can see because they're \nall gray lines. \nAnd we have X1 going inside over here, get at X2, \nX2 goes into the model X3 and so forth. \nNow over here to the right, \nwe have prefix language model which corresponds to allowing fully \nvisible masking over the inputs as we can see here in the dark arrows. \nAnd then causal masking in the rest.\nPlay video starting at :3:2 and follow transcript3:02\nSo as we can see over here, it's doing causal masking. \nSo the model architecture, it uses encoder/decoder stack. \nIt has 12 transformer blocks each. \nSo we can think of it as a dozen eggs and then 220 million parameters. \nSo in summary, you've seen prefix language model attention. \nYou've seen the model architecture for T5. \nAnd you've seen how the pre-training is done similar to birds, but \nwe just use mask language modeling here.\n\n\nencoder/decoder\n1212 transformer blocks 220 million parameters pre training 2^18 steps = 262144",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#tokenization",
    "href": "notes/c4w3/index.html#tokenization",
    "title": "Question Answering",
    "section": "Tokenization",
    "text": "Tokenization\n\nSentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (Kudo & Richardson 2018) sub-word tokenization\nSubword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates (Kudo 2018) sub-word tokenization\nNeural Machine Translation of Rare Words with Subword Units (Sennrich et all 2016) sub-word tokenization\nSubword tokenizers TF tutorial sub-word tokenization\n[https://blog.floydhub.com/tokenization-nlp/]\nSwivel: Improving Embeddings by Noticing What’s Missing (Shazeer, 2016)",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#transformers",
    "href": "notes/c4w3/index.html#transformers",
    "title": "Question Answering",
    "section": "Transformers",
    "text": "Transformers\n\nExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Raffel et al, 2019)\n\nReformer: The Efficient Transformer (Kitaev et al, 2020)\nAttention Is All We Need (Vaswani et al, 2017)\nDeep contextualized word representations (Peters et al, 2018)\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al, 2018)\nFinetuning Pretrained Transformers into RNNs (Kasai et all 2021)\nThe Illustrated Transformer (Alammar, 2018)\nThe Illustrated GPT-2 (Alammar, 2019)\nHow GPT3 Works - Visualizations and Animations (Alammar, 2020)\nAttention? Attention! (Lilian Weng, 2018)\nThe Transformer Family (Lilian Weng, 2020)\nTeacher forcing for RNNs\n\n\nQuestion Answering Task:\n\nTitle (Author et al., Year) note",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w3/index.html#links",
    "href": "notes/c4w3/index.html#links",
    "title": "Question Answering",
    "section": "Links",
    "text": "Links\n\nJax\nTrax\nTrax community on Gitter\nCNN daily mail dataset\n\nLei Mao Machine Learning, Artificial Intelligence, Computer Science. [Byte Pair Encoding (Lei Mao 2021)] (https://leimao.github.io/blog/Byte-Pair-Encoding/) videos: Q&A\n\n\nSubword tokenizers\n\n\nSwivel Embeddings\nhttps://youtu.be/hAvtJ516Mw4",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Question Answering",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w1/lab03.html",
    "href": "notes/c2w1/lab03.html",
    "title": "Assignment 1: Auto Correct",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure 1\nWelcome to the first assignment of Course 2. This assignment will give we a chance to brush up on your python and probability skills. In doing so, we will implement an auto-correct system that is very effective and useful."
  },
  {
    "objectID": "notes/c2w1/lab03.html#outline",
    "href": "notes/c2w1/lab03.html#outline",
    "title": "Assignment 1: Auto Correct",
    "section": "Outline",
    "text": "Outline\n\n0. Overview\n\n0.1 Edit Distance\n\n1. Data Preprocessing\n\n1.1 Exercise 1\n1.2 Exercise 2\n1.3 Exercise 3\n\n2. String Manipulation\n\n2.1 Exercise 4\n2.2 Exercise 5\n2.3 Exercise 6\n2.4 Exercise 7\n\n3. Combining the edits\n\n3.1 Exercise 8\n3.2 Exercise 9\n3.3 Exercise 10\n\n4. Minimum Edit Distance\n\n4.1 Exercise 11\n\n5. Backtrace (Optional)\n\n ## 0. Overview\nWe use autocorrect every day on your cell phone and computer. In this assignment, we will explore what really goes on behind the scenes. Of course, the model we are about to implement is not identical to the one used in your phone, but it is still quite good.\nBy completing this assignment we will learn how to:\n\nGet a word count given a corpus\nGet a word probability in the corpus\nManipulate strings\nFilter strings\nImplement Minimum edit distance to compare strings and to help find the optimal path for the edits.\nUnderstand how dynamic programming works\n\nSimilar systems are used everywhere. - For example, if we type in the word “I am lerningg”, chances are very high that we meant to write “learning”, as shown in Figure 1.\n\n Figure 1\n\n #### 0.1 Edit Distance\nIn this assignment, we will implement models that correct words that are 1 and 2 edit distances away. - We say two words are n edit distance away from each other when we need n edits to change one word into another.\nAn edit could consist of one of the following options:\n\nDelete (remove a letter): ‘hat’ =&gt; ‘at, ha, ht’\nSwitch (swap 2 adjacent letters): ‘eta’ =&gt; ‘eat, tea,…’\nReplace (change 1 letter to another): ‘jat’ =&gt; ‘hat, rat, cat, mat, …’\nInsert (add a letter): ‘te’ =&gt; ‘the, ten, ate, …’\n\nWe will be using the four methods above to implement an Auto-correct. - To do so, we will need to compute probabilities that a certain word is correct given an input.\nThis auto-correct we are about to implement was first created by Peter Norvig in 2007. - His original article may be a useful reference for this assignment.\nThe goal of our spell check model is to compute the following probability:\nP(c|w) = \\frac{P(w|c)\\times P(c)}{P(w)} \\tag{Eqn-1}\nThe equation above is Bayes Rule. - Equation 1 says that the probability of a word being correct $P(c|w) $is equal to the probability of having a certain word w, given that it is correct P(w|c), multiplied by the probability of being correct in general P(C) divided by the probability of that word w appearing P(w) in general. - To compute equation 1, we will first import a data set and then create all the probabilities that we need using that data set.\n # Part 1: Data Preprocessing\n\nimport re\nfrom collections import Counter\nimport numpy as np\nimport pandas as pd\n\nAs in any other machine learning task, the first thing we have to do is process your data set. - Many courses load in pre-processed data for you. - However, in the real world, when we build these NLP systems, we load the datasets and process them. - So let’s get some real world practice in pre-processing the data!\nYour first task is to read in a file called ‘shakespeare.txt’ which is found in your file directory. To look at this file we can go to File ==&gt; Open.\n ### Exercise 1 Implement the function process_data which\n\nReads in a corpus (text file)\nChanges everything to lowercase\nReturns a list of words.\n\n\nOptions and Hints\n\nIf we would like more of a real-life practice, don’t open the ‘Hints’ below (yet) and try searching the web to derive your answer.\nIf we want a little help, click on the green “General Hints” section by clicking on it with your mouse.\nIf we get stuck or are not getting the expected results, click on the green ‘Detailed Hints’ section to get hints for each step that you’ll take to complete this function.\n\n\n\nGeneral Hints\n\n\nGeneral Hints to get started\n\n\nPython input and output\n\n\nPython ‘re’ documentation \n\n\n\n\n\nDetailed Hints\n\n\nDetailed hints if you’re stuck\n\n\nUse ‘with’ syntax to read a file\n\n\nDecide whether to use ‘read()’ or ’readline(). What’s the difference?\n\n\nChoose whether to use either str.lower() or str.lowercase(). What is the difference?\n\n\nUse re.findall(pattern, string)\n\n\nLook for the “Raw String Notation” section in the Python ‘re’ documentation to understand the difference between r’‘, r’’ and ‘\\W’.\n\n\nFor the pattern, decide between using ‘’, ‘’, ‘+’ or ‘+’. What do we think are the differences?\n\n\n\n\n# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: process_data\ndef process_data(file_name):\n    \"\"\"\n    Input: \n        A file_name which is found in your current directory. We just have to read it in. \n    Output: \n        words: a list containing all the words in the corpus (text file we read) in lower case. \n    \"\"\"\n    words = [] # return this variable correctly\n\n    ### START CODE HERE ### \n    words = re.findall(r'\\w+',open(file_name).read().lower())\n    ### END CODE HERE ###\n    \n    return words\n\nNote, in the following cell, ‘words’ is converted to a python set. This eliminates any duplicate entries.\n\n#DO NOT MODIFY THIS CELL\nword_l = process_data('shakespeare.txt')\nvocab = set(word_l)  # this will be your new vocabulary\nprint(f\"The first ten words in the text are: \\n{word_l[0:10]}\")\nprint(f\"There are {len(vocab)} unique words in the vocabulary.\")\n\nThe first ten words in the text are: \n['o', 'for', 'a', 'muse', 'of', 'fire', 'that', 'would', 'ascend', 'the']\nThere are 6116 unique words in the vocabulary.\n\n\n\n\nExpected Output\nThe first ten words in the text are: \n['o', 'for', 'a', 'muse', 'of', 'fire', 'that', 'would', 'ascend', 'the']\nThere are 6116 unique words in the vocabulary.\n ### Exercise 2\nImplement a get_count function that returns a dictionary - The dictionary’s keys are words - The value for each word is the number of times that word appears in the corpus.\nFor example, given the following sentence: “I am happy because I am learning”, your dictionary should return the following:\n\n\n\nKey \n\n\nValue \n\n\n\n\nI\n\n\n2\n\n\n\n\nam\n\n\n2\n\n\n\n\nhappy\n\n\n1\n\n\n\n\nbecause\n\n\n1\n\n\n\n\nlearning\n\n\n1\n\n\n\nInstructions: Implement a get_count which returns a dictionary where the key is a word and the value is the number of times the word appears in the list.\n\n\nHints\n\n\n\n\nTry implementing this using a for loop and a regular dictionary. This may be good practice for similar coding interview questions\n\n\nWe can also use defaultdict instead of a regualr dictionary, along with the for loop\n\n\nOtherwise, to skip using a for loop, we can use Python’s  Counter class\n\n\n\n\n# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# UNIT TEST COMMENT: Candidate for Table Driven Tests\n# GRADED FUNCTION: get_count\ndef get_count(word_l):\n    '''\n    Input:\n        word_l: a set of words representing the corpus. \n    Output:\n        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.\n    '''\n    \n    word_count_dict = {}  # fill this with word counts\n    ### START CODE HERE \n    word_count_dict = Counter(word_l)\n    ### END CODE HERE ### \n    return word_count_dict\n\n\n#DO NOT MODIFY THIS CELL\nword_count_dict = get_count(word_l)\nprint(f\"There are {len(word_count_dict)} key values pairs\")\nprint(f\"The count for the word 'thee' is {word_count_dict.get('thee',0)}\")\n\nThere are 6116 key values pairs\nThe count for the word 'thee' is 240\n\n\n\n\nExpected Output\nThere are 6116 key values pairs\nThe count for the word 'thee' is 240\n ### Exercise 3 Given the dictionary of word counts, compute the probability that each word will appear if randomly selected from the corpus of words.\nP(w_i) = \\frac{C(w_i)}{M} \\tag{Eqn-2} where\nC(w_i) is the total number of times w_i appears in the corpus.\nM is the total number of words in the corpus.\nFor example, the probability of the word ‘am’ in the sentence ‘I am happy because I am learning’ is:\nP(am) = \\frac{C(w_i)}{M} = \\frac {2}{7} \\tag{Eqn-3}.\nInstructions: Implement get_probs function which gives we the probability that a word occurs in a sample. This returns a dictionary where the keys are words, and the value for each word is its probability in the corpus of words.\n\n\nHints\n\n\nGeneral advice\n\n\nUse dictionary.values()\n\n\nUse sum()\n\n\nThe cardinality (number of words in the corpus should be equal to len(word_l). We will calculate this same number, but using the word count dictionary.\n\n\nIf you’re using a for loop:\n\n\nUse dictionary.keys()\n\n\nIf you’re using a dictionary comprehension:\n\n\nUse dictionary.items()\n\n\n\n\n# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: get_probs\ndef get_probs(word_count_dict):\n    '''\n    Input:\n        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.\n    Output:\n        probs: A dictionary where keys are the words and the values are the probability that a word will occur. \n    '''\n    probs = {}  # return this variable correctly\n    \n    ### START CODE HERE ###\n    probs = {k:v/sum(word_count_dict.values()) for k, v in word_count_dict.items()}\n    ### END CODE HERE ###\n    return probs\n\n\n#DO NOT MODIFY THIS CELL\nprobs = get_probs(word_count_dict)\nprint(f\"Length of probs is {len(probs)}\")\nprint(f\"P('thee') is {probs['thee']:.4f}\")\n\nLength of probs is 6116\nP('thee') is 0.0045\n\n\n\n\nExpected Output\nLength of probs is 6116\nP('thee') is 0.0045\n # Part 2: String Manipulations\nNow, that we have computed P(w_i) for all the words in the corpus, we will write a few functions to manipulate strings so that we can edit the erroneous strings and return the right spellings of the words. In this section, we will implement four functions:\n\ndelete_letter: given a word, it returns all the possible strings that have one character removed.\nswitch_letter: given a word, it returns all the possible strings that have two adjacent letters switched.\nreplace_letter: given a word, it returns all the possible strings that have one character replaced by another different letter.\ninsert_letter: given a word, it returns all the possible strings that have an additional character inserted.\n\n\n\nList comprehensions\nString and list manipulation in python will often make use of a python feature called list comprehensions. The routines below will be described as using list comprehensions, but if we would rather implement them in another way, we are free to do so as long as the result is the same. Further, the following section will provide detailed instructions on how to use list comprehensions and how to implement the desired functions. If we are a python expert, feel free to skip the python hints and move to implementing the routines directly.\nPython List Comprehensions embed a looping structure inside of a list declaration, collapsing many lines of code into a single line. If we are not familiar with them, they seem slightly out of order relative to for loops.\n\n Figure 2\n\nThe diagram above shows that the components of a list comprehension are the same components we would find in a typical for loop that appends to a list, but in a different order. With that in mind, we’ll continue the specifics of this assignment. We will be very descriptive for the first function, deletes(), and less so in later functions as we become familiar with list comprehensions.\n ### Exercise 4\nInstructions for delete_letter(): Implement a delete_letter() function that, given a word, returns a list of strings with one character deleted.\nFor example, given the word nice, it would return the set: {‘ice’, ‘nce’, ‘nic’, ‘nie’}.\nStep 1: Create a list of ‘splits’. This is all the ways we can split a word into Left and Right: For example,\n’nice is split into : [('', 'nice'), ('n', 'ice'), ('ni', 'ce'), ('nic', 'e'), ('nice', '')] This is common to all four functions (delete, replace, switch, insert).\n\n Figure 3\n\nStep 2: This is specific to delete_letter. Here, we are generating all words that result from deleting one character.\nThis can be done in a single line with a list comprehension. We can make use of this type of syntax:\n[f(a,b) for a, b in splits if condition]\nFor our ‘nice’ example we get: [‘ice’, ‘nce’, ‘nie’, ‘nic’]\n\n Figure 4\n\n\n\nLevels of assistance\nTry this exercise with these levels of assistance.\n- We hope that this will make it both a meaningful experience but also not a frustrating experience. - Start with level 1, then move onto level 2, and 3 as needed.\n- Level 1. Try to think this through and implement this yourself.\n- Level 2. Click on the \"Level 2 Hints\" section for some hints to get started.\n- Level 3. If we would prefer more guidance, please click on the \"Level 3 Hints\" cell for step by step instructions.\n\nIf we are still stuck, look at the images in the “list comprehensions” section above.\n\n\n\nLevel 2 Hints\n\n\n\n\n Use array slicing like my_string[0:2] \n\n\n Use list comprehensions or for loops \n\n\n\n\n\nLevel 3 Hints\n\n\n\n\nsplits: Use array slicing, like my_str[0:2], to separate a string into two pieces.\n\n\nDo this in a loop or list comprehension, so that we have a list of tuples.\n\nFor example, “cake” can get split into “ca” and “ke”. They’re stored in a tuple (“ca”,“ke”), and the tuple is appended to a list. We’ll refer to these as L and R, so the tuple is (L,R)\n\n&lt;li&gt;When choosing the range for your loop, if we input the word \"cans\" and generate the tuple  ('cans',''), make sure to include an if statement to check the length of that right-side string (R) in the tuple (L,R) &lt;/li&gt;\n&lt;li&gt;deletes: Go through the list of tuples and combine the two strings together. We can use the + operator to combine two strings&lt;/li&gt;\n&lt;li&gt;When combining the tuples, make sure that we leave out a middle character.&lt;/li&gt;\n&lt;li&gt;Use array slicing to leave out the first character of the right substring.&lt;/li&gt;\n\n\n\n# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# UNIT TEST COMMENT: Candidate for Table Driven Tests\n# GRADED FUNCTION: deletes\ndef delete_letter(word, verbose=False):\n    '''\n    Input:\n        word: the string/word for which we will generate all possible words \n                in the vocabulary which have 1 missing character\n    Output:\n        delete_l: a list of all possible strings obtained by deleting 1 character from word\n    '''\n    \n    delete_l = []\n    split_l = []\n    \n    ### START CODE HERE ###\n    split_l = [(word[:i], word[i:]) for i in range(len(word))]\n    delete_l = [l+r[1:] for l, r in split_l]\n    ### END CODE HERE ###\n\n    if verbose: print(f\"input word {word}, \\nsplit_l = {split_l}, \\ndelete_l = {delete_l}\")\n\n    return delete_l\n\n\ndelete_word_l = delete_letter(word=\"cans\",\n                        verbose=True)\n\ninput word cans, \nsplit_l = [('', 'cans'), ('c', 'ans'), ('ca', 'ns'), ('can', 's')], \ndelete_l = ['ans', 'cns', 'cas', 'can']\n\n\n\n\nExpected Output\nNote: We might get a slightly different result with split_l\n\ninput word cans, \nsplit_l = [('', 'cans'), ('c', 'ans'), ('ca', 'ns'), ('can', 's')], \ndelete_l = ['ans', 'cns', 'cas', 'can']\n\n\nNote 1\n\nNotice how it has the extra tuple ('cans', '').\nThis will be fine as long as we have checked the size of the right-side substring in tuple (L,R).\nCan we explain why this will give we the same result for the list of deletion strings (delete_l)?\n\ninput word cans, \nsplit_l = [('', 'cans'), ('c', 'ans'), ('ca', 'ns'), ('can', 's'), ('cans', '')], \ndelete_l = ['ans', 'cns', 'cas', 'can']\n\n\nNote 2\nIf we end up getting the same word as your input word, like this:\ninput word cans, \nsplit_l = [('', 'cans'), ('c', 'ans'), ('ca', 'ns'), ('can', 's'), ('cans', '')], \ndelete_l = ['ans', 'cns', 'cas', 'can', 'cans']\n\nCheck how we set the range.\nSee if we check the length of the string on the right-side of the split.\n\n\n# test # 2\nprint(f\"Number of outputs of delete_letter('at') is {len(delete_letter('at'))}\")\n\nNumber of outputs of delete_letter('at') is 2\n\n\n\n\nExpected output\nNumber of outputs of delete_letter('at') is 2\n ### Exercise 5\nInstructions for switch_letter(): Now implement a function that switches two letters in a word. It takes in a word and returns a list of all the possible switches of two letters that are adjacent to each other. - For example, given the word ‘eta’, it returns {‘eat’, ‘tea’}, but does not return ‘ate’.\nStep 1: is the same as in delete_letter()\nStep 2: A list comprehension or for loop which forms strings by swapping adjacent letters. This is of the form:\n[f(L,R) for L, R in splits if condition] where ‘condition’ will test the length of R in a given iteration. See below.\n\n Figure 5\n\n\n\nLevels of difficulty\nTry this exercise with these levels of difficulty.\n- Level 1. Try to think this through and implement this yourself. - Level 2. Click on the “Level 2 Hints” section for some hints to get started. - Level 3. If we would prefer more guidance, please click on the “Level 3 Hints” cell for step by step instructions.\n\n\nLevel 2 Hints\n\n\n\n\n Use array slicing like my_string[0:2] \n\n\n Use list comprehensions or for loops \n\n\nTo do a switch, think of the whole word as divided into 4 distinct parts. Write out ‘cupcakes’ on a piece of paper and see how we can split it into (‘cupc’, ‘k’, ‘a’, ‘es’)\n\n\n\n\n\nLevel 3 Hints\n\n\n\n\nsplits: Use array slicing, like my_str[0:2], to separate a string into two pieces.\n\n\nSplitting is the same as for delete_letter\n\n\nTo perform the switch, go through the list of tuples and combine four strings together. We can use the + operator to combine strings\n\n\nThe four strings will be the left substring from the split tuple, followed by the first (index 1) character of the right substring, then the zero-th character (index 0) of the right substring, and then the remaining part of the right substring.\n\n\nUnlike delete_letter, we will want to check that your right substring is at least a minimum length. To see why, review the previous hint bullet point (directly before this one).\n\n\n\n\n# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# UNIT TEST COMMENT: Candidate for Table Driven Tests\n# GRADED FUNCTION: switches\ndef switch_letter(word, verbose=False):\n    '''\n    Input:\n        word: input string\n     Output:\n        switches: a list of all possible strings with one adjacent charater switched\n    ''' \n    \n    switch_l = []\n    split_l = []\n    \n    ### START CODE HERE ###\n    split_l = [(word[:i], word[i:]) for i in range(len(word))]\n    switch_l = [l+r[1]+r[0]+r[2:] for l, r in split_l if len(r) &gt; 1]\n    ### END CODE HERE ###\n    \n    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nswitch_l = {switch_l}\") \n\n    return switch_l\n\n\nswitch_word_l = switch_letter(word=\"eta\",\n                         verbose=True)\n\nInput word = eta \nsplit_l = [('', 'eta'), ('e', 'ta'), ('et', 'a')] \nswitch_l = ['tea', 'eat']\n\n\n\n\nExpected output\nInput word = eta \nsplit_l = [('', 'eta'), ('e', 'ta'), ('et', 'a')] \nswitch_l = ['tea', 'eat']\n\n\nNote 1\nWe may get this:\nInput word = eta \nsplit_l = [('', 'eta'), ('e', 'ta'), ('et', 'a'), ('eta', '')] \nswitch_l = ['tea', 'eat']\n\nNotice how it has the extra tuple ('eta', '').\nThis is also correct.\nCan we think of why this is the case?\n\n\n\nNote 2\nIf we get an error\nIndexError: string index out of range\n\nPlease see if we have checked the length of the strings when switching characters.\n\n\n# test # 2\nprint(f\"Number of outputs of switch_letter('at') is {len(switch_letter('at'))}\")\n\nNumber of outputs of switch_letter('at') is 1\n\n\n\n\nExpected output\nNumber of outputs of switch_letter('at') is 1\n ### Exercise 6 Instructions for replace_letter(): Now implement a function that takes in a word and returns a list of strings with one replaced letter from the original word.\nStep 1: is the same as in delete_letter()\nStep 2: A list comprehension or for loop which form strings by replacing letters. This can be of the form:\n[f(a,b,c) for a, b in splits if condition for c in string] Note the use of the second for loop.\nIt is expected in this routine that one or more of the replacements will include the original word. For example, replacing the first letter of ‘ear’ with ‘e’ will return ‘ear’.\nStep 3: Remove the original input letter from the output.\n\n\nHints\n\n\n\n\nTo remove a word from a list, first store its contents inside a set()\n\n\nUse set.discard(‘the_word’) to remove a word in a set (if the word does not exist in the set, then it will not throw a KeyError. Using set.remove(‘the_word’) throws a KeyError if the word does not exist in the set.\n\n\n\n\n# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# UNIT TEST COMMENT: Candidate for Table Driven Tests\n# GRADED FUNCTION: replaces\ndef replace_letter(word, verbose=False):\n    '''\n    Input:\n        word: the input string/word \n    Output:\n        replaces: a list of all possible strings where we replaced one letter from the original word. \n    ''' \n    \n    letters = 'abcdefghijklmnopqrstuvwxyz'\n    replace_l = []\n    split_l = []\n    \n    ### START CODE HERE ###\n    split_l = [(word[:i], word[i:]) for i in range(len(word))]\n    replace_set = set([l+replace+r[1:] for l, r in split_l if len(r) &gt; 0 for replace in letters])\n    replace_set.discard(word)\n    ### END CODE HERE ###\n    \n    # turn the set back into a list and sort it, for easier viewing\n    replace_l = sorted(list(replace_set))\n    \n    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nreplace_l {replace_l}\")   \n    \n    return replace_l\n\n\nreplace_l = replace_letter(word='can',\n                              verbose=True)\n\nInput word = can \nsplit_l = [('', 'can'), ('c', 'an'), ('ca', 'n')] \nreplace_l ['aan', 'ban', 'caa', 'cab', 'cac', 'cad', 'cae', 'caf', 'cag', 'cah', 'cai', 'caj', 'cak', 'cal', 'cam', 'cao', 'cap', 'caq', 'car', 'cas', 'cat', 'cau', 'cav', 'caw', 'cax', 'cay', 'caz', 'cbn', 'ccn', 'cdn', 'cen', 'cfn', 'cgn', 'chn', 'cin', 'cjn', 'ckn', 'cln', 'cmn', 'cnn', 'con', 'cpn', 'cqn', 'crn', 'csn', 'ctn', 'cun', 'cvn', 'cwn', 'cxn', 'cyn', 'czn', 'dan', 'ean', 'fan', 'gan', 'han', 'ian', 'jan', 'kan', 'lan', 'man', 'nan', 'oan', 'pan', 'qan', 'ran', 'san', 'tan', 'uan', 'van', 'wan', 'xan', 'yan', 'zan']\n\n\n\n\nExpected Output**:\nInput word = can \nsplit_l = [('', 'can'), ('c', 'an'), ('ca', 'n')] \nreplace_l ['aan', 'ban', 'caa', 'cab', 'cac', 'cad', 'cae', 'caf', 'cag', 'cah', 'cai', 'caj', 'cak', 'cal', 'cam', 'cao', 'cap', 'caq', 'car', 'cas', 'cat', 'cau', 'cav', 'caw', 'cax', 'cay', 'caz', 'cbn', 'ccn', 'cdn', 'cen', 'cfn', 'cgn', 'chn', 'cin', 'cjn', 'ckn', 'cln', 'cmn', 'cnn', 'con', 'cpn', 'cqn', 'crn', 'csn', 'ctn', 'cun', 'cvn', 'cwn', 'cxn', 'cyn', 'czn', 'dan', 'ean', 'fan', 'gan', 'han', 'ian', 'jan', 'kan', 'lan', 'man', 'nan', 'oan', 'pan', 'qan', 'ran', 'san', 'tan', 'uan', 'van', 'wan', 'xan', 'yan', 'zan']\n\nNote how the input word ‘can’ should not be one of the output words.\n\n\n\nNote 1\nIf we get something like this:\nInput word = can \nsplit_l = [('', 'can'), ('c', 'an'), ('ca', 'n'), ('can', '')] \nreplace_l ['aan', 'ban', 'caa', 'cab', 'cac', 'cad', 'cae', 'caf', 'cag', 'cah', 'cai', 'caj', 'cak', 'cal', 'cam', 'cao', 'cap', 'caq', 'car', 'cas', 'cat', 'cau', 'cav', 'caw', 'cax', 'cay', 'caz', 'cbn', 'ccn', 'cdn', 'cen', 'cfn', 'cgn', 'chn', 'cin', 'cjn', 'ckn', 'cln', 'cmn', 'cnn', 'con', 'cpn', 'cqn', 'crn', 'csn', 'ctn', 'cun', 'cvn', 'cwn', 'cxn', 'cyn', 'czn', 'dan', 'ean', 'fan', 'gan', 'han', 'ian', 'jan', 'kan', 'lan', 'man', 'nan', 'oan', 'pan', 'qan', 'ran', 'san', 'tan', 'uan', 'van', 'wan', 'xan', 'yan', 'zan']\n\nNotice how split_l has an extra tuple ('can', ''), but the output is still the same, so this is okay.\n\n\n\nNote 2\nIf we get something like this:\nInput word = can \nsplit_l = [('', 'can'), ('c', 'an'), ('ca', 'n'), ('can', '')] \nreplace_l ['aan', 'ban', 'caa', 'cab', 'cac', 'cad', 'cae', 'caf', 'cag', 'cah', 'cai', 'caj', 'cak', 'cal', 'cam', 'cana', 'canb', 'canc', 'cand', 'cane', 'canf', 'cang', 'canh', 'cani', 'canj', 'cank', 'canl', 'canm', 'cann', 'cano', 'canp', 'canq', 'canr', 'cans', 'cant', 'canu', 'canv', 'canw', 'canx', 'cany', 'canz', 'cao', 'cap', 'caq', 'car', 'cas', 'cat', 'cau', 'cav', 'caw', 'cax', 'cay', 'caz', 'cbn', 'ccn', 'cdn', 'cen', 'cfn', 'cgn', 'chn', 'cin', 'cjn', 'ckn', 'cln', 'cmn', 'cnn', 'con', 'cpn', 'cqn', 'crn', 'csn', 'ctn', 'cun', 'cvn', 'cwn', 'cxn', 'cyn', 'czn', 'dan', 'ean', 'fan', 'gan', 'han', 'ian', 'jan', 'kan', 'lan', 'man', 'nan', 'oan', 'pan', 'qan', 'ran', 'san', 'tan', 'uan', 'van', 'wan', 'xan', 'yan', 'zan']\n\nNotice how there are strings that are 1 letter longer than the original word, such as cana.\nPlease check for the case when there is an empty string '', and if so, do not use that empty string when setting replace_l.\n\n\n# test # 2\nprint(f\"Number of outputs of switch_letter('at') is {len(switch_letter('at'))}\")\n\nNumber of outputs of switch_letter('at') is 1\n\n\n\n\nExpected output\nNumber of outputs of switch_letter('at') is 1\n ### Exercise 7\nInstructions for insert_letter(): Now implement a function that takes in a word and returns a list with a letter inserted at every offset.\nStep 1: is the same as in delete_letter()\nStep 2: This can be a list comprehension of the form:\n[f(a,b,c) for a, b in splits if condition for c in string]\n\n# UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# UNIT TEST COMMENT: Candidate for Table Driven Tests\n# GRADED FUNCTION: inserts\ndef insert_letter(word, verbose=False):\n    '''\n    Input:\n        word: the input string/word \n    Output:\n        inserts: a set of all possible strings with one new letter inserted at every offset\n    ''' \n    letters = 'abcdefghijklmnopqrstuvwxyz'\n    insert_l = []\n    split_l = []\n    \n    ### START CODE HERE ###\n    split_l = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n    insert_l = [l+replace+r for l, r in split_l for replace in letters]\n    ### END CODE HERE ###\n\n    if verbose: print(f\"Input word {word} \\nsplit_l = {split_l} \\ninsert_l = {insert_l}\")\n    \n    return insert_l\n\n\ninsert_l = insert_letter('at', True)\nprint(f\"Number of strings output by insert_letter('at') is {len(insert_l)}\")\n\nInput word at \nsplit_l = [('', 'at'), ('a', 't'), ('at', '')] \ninsert_l = ['aat', 'bat', 'cat', 'dat', 'eat', 'fat', 'gat', 'hat', 'iat', 'jat', 'kat', 'lat', 'mat', 'nat', 'oat', 'pat', 'qat', 'rat', 'sat', 'tat', 'uat', 'vat', 'wat', 'xat', 'yat', 'zat', 'aat', 'abt', 'act', 'adt', 'aet', 'aft', 'agt', 'aht', 'ait', 'ajt', 'akt', 'alt', 'amt', 'ant', 'aot', 'apt', 'aqt', 'art', 'ast', 'att', 'aut', 'avt', 'awt', 'axt', 'ayt', 'azt', 'ata', 'atb', 'atc', 'atd', 'ate', 'atf', 'atg', 'ath', 'ati', 'atj', 'atk', 'atl', 'atm', 'atn', 'ato', 'atp', 'atq', 'atr', 'ats', 'att', 'atu', 'atv', 'atw', 'atx', 'aty', 'atz']\nNumber of strings output by insert_letter('at') is 78\n\n\n\n\nExpected output\nInput word at \nsplit_l = [('', 'at'), ('a', 't'), ('at', '')] \ninsert_l = ['aat', 'bat', 'cat', 'dat', 'eat', 'fat', 'gat', 'hat', 'iat', 'jat', 'kat', 'lat', 'mat', 'nat', 'oat', 'pat', 'qat', 'rat', 'sat', 'tat', 'uat', 'vat', 'wat', 'xat', 'yat', 'zat', 'aat', 'abt', 'act', 'adt', 'aet', 'aft', 'agt', 'aht', 'ait', 'ajt', 'akt', 'alt', 'amt', 'ant', 'aot', 'apt', 'aqt', 'art', 'ast', 'att', 'aut', 'avt', 'awt', 'axt', 'ayt', 'azt', 'ata', 'atb', 'atc', 'atd', 'ate', 'atf', 'atg', 'ath', 'ati', 'atj', 'atk', 'atl', 'atm', 'atn', 'ato', 'atp', 'atq', 'atr', 'ats', 'att', 'atu', 'atv', 'atw', 'atx', 'aty', 'atz']\nNumber of strings output by insert_letter('at') is 78\n\n\nNote 1\nIf we get a split_l like this:\nInput word at \nsplit_l = [('', 'at'), ('a', 't')] \ninsert_l = ['aat', 'bat', 'cat', 'dat', 'eat', 'fat', 'gat', 'hat', 'iat', 'jat', 'kat', 'lat', 'mat', 'nat', 'oat', 'pat', 'qat', 'rat', 'sat', 'tat', 'uat', 'vat', 'wat', 'xat', 'yat', 'zat', 'aat', 'abt', 'act', 'adt', 'aet', 'aft', 'agt', 'aht', 'ait', 'ajt', 'akt', 'alt', 'amt', 'ant', 'aot', 'apt', 'aqt', 'art', 'ast', 'att', 'aut', 'avt', 'awt', 'axt', 'ayt', 'azt']\nNumber of strings output by insert_letter('at') is 52\n\nNotice that split_l is missing the extra tuple (‘at’, ’’). For insertion, we actually WANT this tuple.\nThe function is not creating all the desired output strings.\nCheck the range that we use for the for loop.\n\n\n\nNote 2\nIf we see this:\nInput word at \nsplit_l = [('', 'at'), ('a', 't'), ('at', '')] \ninsert_l = ['aat', 'bat', 'cat', 'dat', 'eat', 'fat', 'gat', 'hat', 'iat', 'jat', 'kat', 'lat', 'mat', 'nat', 'oat', 'pat', 'qat', 'rat', 'sat', 'tat', 'uat', 'vat', 'wat', 'xat', 'yat', 'zat', 'aat', 'abt', 'act', 'adt', 'aet', 'aft', 'agt', 'aht', 'ait', 'ajt', 'akt', 'alt', 'amt', 'ant', 'aot', 'apt', 'aqt', 'art', 'ast', 'att', 'aut', 'avt', 'awt', 'axt', 'ayt', 'azt']\nNumber of strings output by insert_letter('at') is 52\n\nEven though we may have fixed the split_l so that it contains the tuple ('at', ''), notice that you’re still missing some output strings.\n\nNotice that it’s missing strings such as ‘ata’, ‘atb’, ‘atc’ all the way to ‘atz’.\n\nTo fix this, make sure that when we set insert_l, we allow the use of the empty string ''.\n\n\n# test # 2\nprint(f\"Number of outputs of insert_letter('at') is {len(insert_letter('at'))}\")\n\nNumber of outputs of insert_letter('at') is 78\n\n\n\n\nExpected output\nNumber of outputs of insert_letter('at') is 78"
  },
  {
    "objectID": "notes/c2w1/lab02.html",
    "href": "notes/c2w1/lab02.html",
    "title": "Candidates from String Edits",
    "section": "",
    "text": "course banner\n\n\nEstimated Time: 20 minutes\nCreate a list of candidate strings by applying an edit operation\n\nImports and Data\n\n# data\nword = 'dearz' # 🦌\n\n\n\nSplits\nFind all the ways we can split a word into 2 parts !\n\n# splits with a loop\nsplits_a = []\nfor i in range(len(word)+1):\n    splits_a.append([word[:i],word[i:]])\n\nfor i in splits_a:\n    print(i)\n\n['', 'dearz']\n['d', 'earz']\n['de', 'arz']\n['dea', 'rz']\n['dear', 'z']\n['dearz', '']\n\n\n\n# same splits, done using a list comprehension\nsplits_b = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n\nfor i in splits_b:\n    print(i)\n\n('', 'dearz')\n('d', 'earz')\n('de', 'arz')\n('dea', 'rz')\n('dear', 'z')\n('dearz', '')\n\n\n\n\nDelete Edit\nDelete a letter from each string in the splits list.\nWhat this does is effectivly delete each possible letter from the original word being edited.\n\n# deletes with a loop\nsplits = splits_a\ndeletes = []\n\nprint('word : ', word)\nfor L,R in splits:\n    if R:\n        print(L + R[1:], ' &lt;-- delete ', R[0])\n\nword :  dearz\nearz  &lt;-- delete  d\ndarz  &lt;-- delete  e\nderz  &lt;-- delete  a\ndeaz  &lt;-- delete  r\ndear  &lt;-- delete  z\n\n\nIt’s worth taking a closer look at how this is excecuting a ‘delete’.\nTaking the first item from the splits list :\n\n# breaking it down\nprint('word : ', word)\none_split = splits[0]\nprint('first item from the splits list : ', one_split)\nL = one_split[0]\nR = one_split[1]\nprint('L : ', L)\nprint('R : ', R)\nprint('*** now implicit delete by excluding the leading letter ***')\nprint('L + R[1:] : ',L + R[1:], ' &lt;-- delete ', R[0])\n\nword :  dearz\nfirst item from the splits list :  ['', 'dearz']\nL :  \nR :  dearz\n*** now implicit delete by excluding the leading letter ***\nL + R[1:] :  earz  &lt;-- delete  d\n\n\nSo the end result transforms ‘dearz’ to ‘earz’ by deleting the first character.\nAnd we use a loop (code block above) or a list comprehension (code block below) to do this for the entire splits list.\n\n# deletes with a list comprehension\nsplits = splits_a\ndeletes = [L + R[1:] for L, R in splits if R]\n\nprint(deletes)\nprint('*** which is the same as ***')\nfor i in deletes:\n    print(i)\n\n['earz', 'darz', 'derz', 'deaz', 'dear']\n*** which is the same as ***\nearz\ndarz\nderz\ndeaz\ndear\n\n\n\n\nUngraded Exercise\nWe now have a list of candidate strings created after performing a delete edit.  Next step will be to filter this list for candidate words found in a vocabulary.  Given the example vocab below, can we think of a way to create a list of candidate words ?  Remember, we already have a list of candidate strings, some of which are certainly not actual words we might find in your vocabulary !   So from the above list earz, darz, derz, deaz, dear.  You’re really only interested in dear.\n\nvocab = ['dean','deer','dear','fries','and','coke']\nedits = list(deletes)\n\nprint('vocab : ', vocab)\nprint('edits : ', edits)\n\ncandidates=[]\n\n### START CODE HERE ###\ncandidates = set(vocab).intersection(edits)  # hint: 'set.intersection'\n### END CODE HERE ###\n\nprint('candidate words : ', candidates)\n\nvocab :  ['dean', 'deer', 'dear', 'fries', 'and', 'coke']\nedits :  ['earz', 'darz', 'derz', 'deaz', 'dear']\ncandidate words :  {'dear'}\n\n\nExpected Outcome:\nvocab : [‘dean’, ‘deer’, ‘dear’, ‘fries’, ‘and’, ‘coke’]\nedits : [‘earz’, ‘darz’, ‘derz’, ‘deaz’, ‘dear’]\ncandidate words : {‘dear’}\n\n\nSummary\nYou’ve unpacked an integral part of the assignment by breaking down splits and edits, specifically looking at deletes here.\nImplementation of the other edit types (insert, replace, switch) follows a similar methodology and should now feel somewhat familiar when we see them.\nThis bit of the code isn’t as intuitive as other sections, so well done!\nWe should now feel confident facing some of the more technical parts of the assignment at the end of the week.\n\n\n\n\nCitationBibTeX citation:@online{bochman2020,\n  author = {Bochman, Oren},\n  title = {Candidates from {String} {Edits}},\n  date = {2020-10-17},\n  url = {https://orenbochman.github.io/notes-nlp/notes/c2w1/lab02.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2020. “Candidates from String Edits.”\nOctober 17, 2020. https://orenbochman.github.io/notes-nlp/notes/c2w1/lab02.html.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocorrect & Dynamic Programming",
      "L2 - Candidates from String Edits"
    ]
  },
  {
    "objectID": "notes/c3w3/index.html",
    "href": "notes/c3w3/index.html",
    "title": "LSTMs and Named Entity Recognition",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\n\nFigure 2\nMy notes for Week 3 of the Natural Language Processing with Sequence Models Course in the Natural Language Processing Specialization Offered by DeepLearning.AI on Coursera",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w3/index.html#rnns-and-vanishing-gradients",
    "href": "notes/c3w3/index.html#rnns-and-vanishing-gradients",
    "title": "LSTMs and Named Entity Recognition",
    "section": "RNNs and Vanishing Gradients",
    "text": "RNNs and Vanishing Gradients\nAdvantages of RNNs RNNs allow us to capture dependancies within a short range and they take up less RAM than other n-gram models.\nDisadvantages of RNNs RNNs struggle with longer term dependencies and are very prone to vanishing or exploding gradients.\n\nNote that as we are back-propagating through time, we end up getting the following:\n\nNote that the sigmoid and tanh functions are bounded by 0 and 1 and -1 and 1 respectively. This eventually leads us to a problem. If we have many numbers that are less than |1|, then as we go through many layers, and we take the product of those numbers, we eventually end up getting a gradient that is very close to 0. This introduces the problem of vanishing gradients.\nSolutions to Vanishing Gradient Problems",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w3/index.html#optional-intro-to-optimization-in-deep-learning-gradient-descent",
    "href": "notes/c3w3/index.html#optional-intro-to-optimization-in-deep-learning-gradient-descent",
    "title": "LSTMs and Named Entity Recognition",
    "section": "(Optional) Intro to optimization in deep learning: Gradient Descent",
    "text": "(Optional) Intro to optimization in deep learning: Gradient Descent\nCheck out this blog from Paperspace.io if you’re interested in understanding in more depth some of the challenges in gradient descent.\n\n\nVisual Loss Landscapes For Neural Nets (Paper)",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w3/index.html#lab-lecture-notebook-vanishing-gradients",
    "href": "notes/c3w3/index.html#lab-lecture-notebook-vanishing-gradients",
    "title": "LSTMs and Named Entity Recognition",
    "section": "Lab: Lecture Notebook: Vanishing Gradients",
    "text": "Lab: Lecture Notebook: Vanishing Gradients\nVanishing Gradients",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w3/index.html#introduction-to-lstms",
    "href": "notes/c3w3/index.html#introduction-to-lstms",
    "title": "LSTMs and Named Entity Recognition",
    "section": "Introduction to LSTMs",
    "text": "Introduction to LSTMs\nThe LSTM allows your model to remember and forget certain inputs. It consists of a cell state and a hidden state with three gates. The gates allow the gradients to flow unchanged. We can think of the three gates as follows:\nInput gate: tells we how much information to input at any time point.\nForget gate: tells we how much information to forget at any time point.\nOutput gate: tells we how much information to pass over at any time point.\nThere are many applications we can use LSTMs for, such as:",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w3/index.html#optional-understanding-lstms",
    "href": "notes/c3w3/index.html#optional-understanding-lstms",
    "title": "LSTMs and Named Entity Recognition",
    "section": "(Optional) Understanding LSTMs",
    "text": "(Optional) Understanding LSTMs\nHere’s a classic post on LSTMs with intuitive explanations and diagrams, to complement this week’s material.\n\nUnderstanding LSTM Networks",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w3/index.html#lstm-architecture",
    "href": "notes/c3w3/index.html#lstm-architecture",
    "title": "LSTMs and Named Entity Recognition",
    "section": "LSTM Architecture",
    "text": "LSTM Architecture\nThe LSTM architecture could get complicated and don’t worry about it if we do not understand it. I personally prefer looking at the equation, but I will try to give we a visualization for now and later this week we will take a look at the equations.\n\nNote that there is the cell state and the hidden state, and then there is the output state. The forget gate is the first activation in the drawing above. It makes use of the previous hidden state h^{&lt;t_0&gt;} and the input x^{&lt;t_0&gt;}. The input gate makes use of the next two activations, the sigmoid and the tanh. Finally the output gate makes use of the last activation and the tanh right above it. This is just an overview of the architecture, we will dive into the details once we introduce the equations.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w3/index.html#introduction-to-named-entity-recognition",
    "href": "notes/c3w3/index.html#introduction-to-named-entity-recognition",
    "title": "LSTMs and Named Entity Recognition",
    "section": "Introduction to Named Entity Recognition",
    "text": "Introduction to Named Entity Recognition\nNamed Entity Recognition (NER) locates and extracts predefined entities from text. It allows we to find places, organizations, names, time and dates. Here is an example of the model we will be building:\n\nNER systems are being used in search efficiency, recommendation engines, customer service, automatic trading, and many more.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w3/index.html#lstm-equations-optional",
    "href": "notes/c3w3/index.html#lstm-equations-optional",
    "title": "LSTMs and Named Entity Recognition",
    "section": "LSTM equations (Optional)",
    "text": "LSTM equations (Optional)\nThese are the LSTM equations related to the gates I had previously spoken about:\n\nf = \\sigma(W_f[h_{t-1}; x_t] + b_f) \\qquad \\text{Forget}\n\\tag{1}\n\ni = \\sigma(W_i[h_{t-1}; x_t] + b_i) \\qquad \\text{Input}\n\\tag{2}\n\ng = \\tanh(W_g[h_{t-1}; x_t] + b_g) \\qquad \\text{Gate}\n\\tag{3}\n\nc_t = f \\odot c_{t-1} + i \\odot g \\qquad \\text{Cell State}\n\\tag{4}\n\no = \\sigma(W_o[h_{t-1}; x_t] + b_o) \\qquad \\text{Output}\n\\tag{5}\nWe can think of:\n\nThe forget gate as a gate that tells we how much information to forget,\nThe input gate, tells we how much information to pick up.\nThe gate gate as the gate containing information. This is multiplied by the input gate (which tells we how much of that information to keep).",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w3/index.html#training-ners-data-processing",
    "href": "notes/c3w3/index.html#training-ners-data-processing",
    "title": "LSTMs and Named Entity Recognition",
    "section": "Training NERs: Data Processing",
    "text": "Training NERs: Data Processing\nProcessing data is one of the most important tasks when training AI algorithms. For NER, we have to:\n\nConvert words and entity classes into arrays:\nPad with tokens: Set sequence length to a certain number and use the  token to fill empty spaces\nCreate a data generator:\n\nOnce we have that, we can assign each class a number, and each word a number.\n\n\n\n\nData Processing\n\nTraining an NER system:\n\nCreate a tensor for each input and its corresponding number\nPut them in a batch ==&gt; 64, 128, 256, 512 …\nFeed it into an LSTM unit\nRun the output through a dense layer\nPredict using a log softmax over K classes\n\nHere is an example of the architecture:\n\nNote that this is just one example of an NER system. Different architectures are possible.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w3/index.html#long-short-term-memory-deep-learning-specialization-c5",
    "href": "notes/c3w3/index.html#long-short-term-memory-deep-learning-specialization-c5",
    "title": "LSTMs and Named Entity Recognition",
    "section": "Long Short-Term Memory (Deep Learning Specialization C5)",
    "text": "Long Short-Term Memory (Deep Learning Specialization C5)\nNote: this section is based on a transcript of the video from the Deep Learning Specialization.\n\n\n\n\n\n\n\nLSTM from GRU or LSTM from RNNs\n\n\n\nIn this video Andrew Ng explains the Long Short-Term Memory (LSTM) as if it was developed from the GRU rather than RNNs. Sure the GRU has one less equation but its just about as complicated as the LSTM … The people who came up with it had 20 years to understand LSTM before they figured these out. We on the other hand have covered neither RNNs not GRUs and have none of that intuition they provide.\nLSTM also have a number of variations, such as the peephole connection, the carousal connection, and the coupled forget and input gates. 😱\nIn the other course on Deep Learning, Ng builds things up using a number of videos starting with notation. RNNs, different types of RNNs, and then the LSTM. The vanishing gradient problem with RNN and then the GRUs.\nI find the notation used very annoying but at least it is explained in the other course and seems to be motivated by time series. In Rnns we process the data in two dimensions. One is for the sequence index used in the super script. The other is for applying multiple layers which we don’t seem to consider.\nRnns learn weights more weights as the sequence grows and though not clear these weights are shared across the RNN units.\n\n\n\n\n\n\n\n\nNotation\n\n\n\n\nX^{(i)&lt;t&gt;} is the input at time t for the ith example.\ny^{(i)&lt;t&gt;} is the output at time t for the ith example.\nT_x^{(i)} is the length of the input sequence for the ith example.\nT_y^{(i)} is the length of the output sequence for the ith example.\nis $T_x^(i) = T_y^(i) not necessarily. (e.g. translation can be longer or shorter, while NER can be one to one.). They will be different for different examples.\nis T_x^(i) = T_x^(j) unlikely as the length of the input sequence will vary from example to example.\n\n\n\n\n\n\n\nGRU v.s. LSTM\n\n\n\n\nLSTM equations\n\n\n\n\nLSTM Schematic\n\n\n\n\nLSTM Rollout\n\n\n\n\nNow that we understand my caveat let’s try to understand by filling the gaps as we go along with what what Ng says and shows!\n\nIf we understand the math we are good to go. Code is just a translation of the math into a programming language. Once we understand the math there are deeper levels of understanding that we can get to but we can’t get there without understanding the math. There are three challenges to understanding the math!\n\nBoth the LSTM and GRU are RNNs so they translate sequences to sequences in the general case let’s imagine we are translating english to german. We start with some input x_0 and we output some out put y_0. For the next word we want to use the previous word to help us translate the next word. This is done by concatenating the current word to the previous (hidden) state. The first take away\nRNN have an internal nonlinearity a tanh but no gating mechanism. The non-linearity is applied to the hidden state concatenated to the input. The hidden state is thus the long term memory of the RNN. For NER we only need a short context to decide but for translation we need to be aware longer context, perhaps a few sentences back. [RNNS, GRUs and LSTMs also have the non-linearity at their core]. That’s the second take away about the math of LSTMS\nThe vanishing and exploding gradients are not the only issues in RNNs there is also a problem of accessing data from many time steps back. By accessing I mean backpropagating the gradients backwards enough time steps. In the LSTM there are not only pathways that let the state pass on unchanged they also allow the gradients to flow back unchanged similar to ResNets. So the two path from Hidden state to hidden state and from Internal State to internal state are what allows the LSTM to handle long term gradient gradients better than RNNS. This is particularly true for most cases where there is a ‘short circuit’ allowing these state to persist. And has a stabilizing effect. That is the third take away from the math of LSTMs.\nThe update, forget, output gates is where the weight and biases are used, thus this is where the learning is taking place and this is happening in an element-wise manner. This is the fourth take away from the math of LSTMs.\nThese three gates also control how much of the new data is incorporated into the internal state c^{&lt;t&gt;} and the hidden state. This is referred to as the gating mechanism. And different variants of LSTM and GRUS make subtle changes to the gating. This is the fifth take away from the math of LSTMs.\nInformation flow in the LST is captured by the dependency between the equations is as follows:\n\nf_t, i_t, g_t, o_t the gate uses see the old stat and the new input.\nc_t the internal state sees f_t, i_t, and g_t and the old state c_t\nh_t sees o_t and c_t which depend on the previous hidden state h_{t-1} and the new input x_t. To sum up the gates only depend on the input an the previous hidden state. The internal state depends on the gates and the previous internal state. The next hidden state depends on the internal state and the output gate.. This is the sixth take away from the math of LSTMs.\n\n\n\n\n\\begin{aligned}\n\\textcolor{red}{f_t} &= \\textcolor{purple}{\\sigma}(\\textcolor{blue}{W_f}[h_{t-1}; x_t] + \\textcolor{blue}{b_f}) \\\\\n\\textcolor{red}{i_t} &= \\textcolor{purple}{\\sigma}(\\textcolor{blue}{W_i}[h_{t-1}; x_t] + \\textcolor{blue}{b_i}) \\\\\n\\textcolor{red}{g_t} &= \\textcolor{purple}{\\tanh}(\\textcolor{blue}{W_g}[h_{t-1}; x_t] + \\textcolor{blue}{b_g})  \\\\\n\\textcolor{red}{o_t} &= \\textcolor{purple}{\\sigma}(\\textcolor{blue}{W_o}[h_{t-1}; x_t] + \\textcolor{blue}{b_o}) \\\\\n\\textcolor{green}{c_t} &= \\textcolor{red}{f_t} \\textcolor{orange}{\\odot} \\textcolor{green}{c_{t-1}} + \\textcolor{red}{i_t} \\textcolor{orange}{\\odot} \\textcolor{red}{g_t}     \\\\\nh_t &= \\textcolor{red}{o_t} \\textcolor{orange}{\\odot} \\textcolor{purple}{\\tanh}(\\textcolor{green}{c_t})\n\\end{aligned}\n\\tag{6}\nkey:\n\nred for gates\nblue for weights\norange for element-wise operations\ngreen for the internal state\npurple for the non-linearity \n\nNext level of understanding is to consider the action of the gating machanism and the relation between internal state and hidden state.\n\n\nThe key things from this unit are that the GRU does not use a forget gate, but uses 1-\\Gamma_u to decide how much of the previous memory cell to keep. In the LSTM, the forget gate instead.\nThere are two aspects to understanding these RNNS.\nThe equations look like simultaneous equations, in reality they are they have a more complex structure as\nThe schematic are emphesise a two other aspects of the LSTM, information flow and gating mechanisms.\n\nhow the equations are wired up to control the information flow and - the idea that we have a gating mechanism that combines the long term memory a in the Hidden state and the uses the memory cell, rather than the hidden state.\n\nWe learned about the GRU, or gated recurrent units, and how that can allow we to learn very long range connections in a sequence. The other type of unit that allows we to do this very well is the LSTM or the long short term memory units, and this is even more powerful than the GRU. Let’s take a look.\nHere are the equations from the previous video for the GRU. And for the GRU, we had a^{&lt;t&gt;} = c^{&lt;t&gt;}, and two gates, the optic gate and the relevance gate, \\tilde{c}^{&lt;t&gt;}, which is a candidate for replacing the memory cell, and then we use the update gate, \\Gamma_u, to decide whether or not to update c^{&lt;t&gt;} using \\tilde{c}^{&lt;t&gt;}.\nThe LSTM is an even slightly more powerful and more general version of the GRU, and is due to Sepp Hochreiter and Jurgen Schmidhuber1, c.f. Hochreiter and Schmidhuber (1997). And this was a really seminal paper, a huge impact on sequence modelling.\n1 with many interesting talks onlineI think this paper is one of the more difficult to read. It goes quite along into theory of vanishing gradients. And so, I think more people have learned about the details of LSTM through maybe other places than from this particular paper even though I think this paper has had a wonderful impact on the Deep Learning community.\nBut these are the equations that govern the LSTM. So, we continue to the memory cell, c, and the candidate value for updating it, \\tilde{c}^{&lt;t&gt;}, will be this, and so on. Notice that for the LSTM, we will no longer have the case that a^{&lt;t&gt;} =c^{&lt;t&gt;}. So, this is what we use. And so, this is just like the equation on the left except that with now, more specially use a^{&lt;t&gt;} there or a^{&lt;t-1&gt;} instead of c^{&lt;t-1&gt;}. And we’re not using this gamma or this relevance gate. Although we could have a variation of the LSTM where we put that back in, but with the more common version of the LSTM, doesn’t bother with that. And then we will have an update gate, same as before. So, W updates and we’re going to use a^{&lt;t-1&gt;} here, x^{&lt;t&gt;} + b_u. And one new property of the LSTM is, instead of having one update gate control, both of these terms, we’re going to have two separate terms. So instead of gamma_u and one minus gamma_u, we’re going have \\Gamma_u here. And forget gate, which we’re going to call \\Gamma_f. So, this gate, \\Gamma_f, is going to be sigmoid of pretty much what you’d expect, x^{&lt;t&gt;}+ b_f. And then, we’re going to have a new output gate which is \\sigma(W_o)+ b_o. And then, the update value to the memory so will be c^{&lt;t&gt;}=\\Gamma_u. And this asterisk denotes element-wise multiplication. This is a vector-vector element-wise multiplication, plus, and instead of one minus gamma u, we’re going to have a separate forget gate, \\Gamma_f * c^{&lt;t-1&gt;}. So this gives the memory cell the option of keeping the old value c^{t-1} and then just adding to it, this new value, \\tilde{c}^{&lt;t&gt;}. So, use a separate update and forget gates. So, this stands for update, forget, and output gate.\nAnd then finally, instead of \na^{&lt;t&gt;} = c^{&lt;t&gt;} \\quad \\text{(GRU)} \\qquad a^{&lt;t&gt;} = \\Gamma_0 * \\tanh( c^{&lt;t&gt;}) \\quad \\text{(LSTM)}\n.\nSo, these are the equations that govern the LSTM and we can tell it has three gates instead of two. So, it’s a bit more complicated and it places the gates into slightly different places. So, here again are the equations governing the behavior of the LSTM.\nOnce again, it’s traditional to explain these things using pictures. So let me draw one here. And if these pictures are too complicated, don’t worry about it. I personally find the equations easier to understand than the picture. But I’ll just show the picture here for the intuitions it conveys. The bigger picture here was very much inspired by a blog post due to Chris Ola, titled Understanding LSTM Network, and the diagram drawing here is quite similar to one that he drew in his blog post. But the key thing is to take away from this picture are maybe that we use a^{&lt;t-1&gt;} and x^{&lt;t&gt;} to compute all the gate values. In this picture, we have a^{&lt;t-1&gt;}, x^{&lt;t&gt;} coming together to compute the forget gate, to compute the update gates, and to compute output gate. And they also go through a tanh to compute \\tilde{c}^{&lt;t&gt;}. And then these values are combined in these complicated ways with element-wise multiplies and so on, to get c^{&lt;t&gt;} from the previous c^{&lt;t-1&gt;}. Now, one element of this is interesting as we have a bunch of these in parallel. So, that’s one of them and we connect them. We then connect these temporally. So it does the input x^{&lt;1&gt;} then x^{&lt;2&gt;}, x^{&lt;3&gt;}. So, we can take these units and just hold them up as follows, where the output a at the previous timestep is the input a at the next timestep, the c. I’ve simplified to diagrams a little bit in the bottom. And one cool thing about this you’ll notice is that there’s this line at the top that shows how, so long as we set the forget and the update gate appropriately, it is relatively easy for the LSTM to have some value c_0 and have that be passed all the way to the right to have your, maybe, c^{&lt;3&gt;}=c^{&lt;0&gt;}. And this is why the LSTM, as well as the GRU, is very good at memorizing certain values even for a long time, for certain real values stored in the memory cell even for many, many timesteps.\nSo, that’s it for the LSTM.\nAs we can imagine, there are also a few variations on this that people use. Perhaps, the most common one is that instead of just having the gate values be dependent only on a_{t-1}, x^{&lt;t&gt;}, sometimes, people also sneak in there the values c_{t-1} as well. This is called a peephole connection, introduced in Gers and Schmidhuber (2000) Not a great name maybe but you’ll see, peephole connection. What that means is that the gate values may depend not just on a^{&lt;t-1&gt;} and on x^{&lt;t&gt;}, but also on the previous memory cell value, and the peephole connection can go into all three of these gates’ computations. So that’s one common variation we see of LSTMs. One technical detail is that these are, say, 100-dimensional vectors. So if we have a 100-dimensional hidden memory cell unit, and so is this. And the, say, fifth element of c^{t-1} affects only the fifth element of the corresponding gates, so that relationship is one-to-one, where not every element of the 100-dimensional c^{&lt;t-1&gt;} can affect all elements of the case. But instead, the first element of c^{&lt;t-1&gt;} affects the first element of the case, second element affects the second element, and so on. But if we ever read the paper and see someone talk about the peephole connection, that’s when they mean that c^{&lt;t-1&gt;} is used to affect the gate value as well. So, that’s it for the LSTM.\nWhen should we use a GRU? And when should we use an LSTM?\nThere isn’t widespread consensus in this. And even though I presented GRUs first, in the history of deep learning, LSTMs actually came much earlier, and then GRUs were relatively recent invention that were maybe derived as Pavia’s simplification of the more complicated LSTM model. Researchers have tried both of these models on many different problems, and on different problems, different algorithms will win out. So, there isn’t a universally-superior algorithm which is why I want to show we both of them. But I feel like when I am using these, the advantage of the GRU is that it’s a simpler model and so it is actually easier to build a much bigger network, it only has two gates, so computationally, it runs a bit faster. So, it scales the building somewhat bigger models but the LSTM is more powerful and more effective since it has three gates instead of two. If we want to pick one to use, I think LSTM has been the historically more proven choice. So, if we had to pick one, I think most people today will still use the LSTM as the default first thing to try. Although, I think in the last few years, GRUs had been gaining a lot of momentum and I feel like more and more teams are also using GRUs because they’re a bit simpler but often work just as well. It might be easier to scale them to even bigger problems. So, that’s it for LSTMs. Well, either GRUs or LSTMs, you’ll be able to build neural network that can capture much longer range dependencies.\nThe correct version of the final equation in the output gate is here:\nhttps://www.coursera.org/learn/nlp-sequence-models/supplement/xdv6z/long-short-term-memory-lstm-correction",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w3/index.html#computing-accuracy",
    "href": "notes/c3w3/index.html#computing-accuracy",
    "title": "LSTMs and Named Entity Recognition",
    "section": "Computing Accuracy",
    "text": "Computing Accuracy\nTo compare the accuracy, just follow the following steps:\n\nPass test set through the model\nGet arg max across the prediction array\nMask padded tokens\nCompare with the true labels.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w3/index.html#reflections-on-this-unit",
    "href": "notes/c3w3/index.html#reflections-on-this-unit",
    "title": "LSTMs and Named Entity Recognition",
    "section": "Reflections on this unit",
    "text": "Reflections on this unit\n\n\n\n\n\n\nMain Research Questions\n\n\n\n\nWhat is the vanishing and exploding gradient problem in RNNs?\nHow can we measure the ability of RNN to access data from many time steps back?\nWhat is the nature of the hidden state in RNNs?\n\n\nshort term memory\n\n\nWhat is the nature of the internal state in RNNs?\n\n\nlong term memory\n\n\nHow are gradients updated in the LSTMs?\nwhat is the constant error carousel in LSTMs?\nhow does it solve the vanishing gradient problem?\nHow does gating work in LSTMs?\nAre the gates binary?\nwhat is the idea behind a peekhole LSTM \nwhat is the idea of the bLSTM \nAre all LSTMs stacked, cam we have a single layer LSTM?",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w3/index.html#resources",
    "href": "notes/c3w3/index.html#resources",
    "title": "LSTMs and Named Entity Recognition",
    "section": "Resources:",
    "text": "Resources:\n\n(Chadha 2020) Aman Chadha’s Notes\nIbrahim Jelliti’s Notes\nIntro to optimization in deep learning: Gradient Descent (Tutorial) Ayoosh Kathuria\nVisual Loss Landscapes For Neural Nets (Paper)\nArticle on Learning Rate Schedules by Hafidz Zulkifli.\nStochastic Weight Averaging (Paper)\nUnderstanding LSTM Networks\nThe Unreasonable Effectiveness of Recurrent Neural Networks by Andrej Karpathy",
    "crumbs": [
      "Home",
      "Sequence Models",
      "LSTMs and Named Entity Recognition",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w4/lab01.html",
    "href": "notes/c1w4/lab01.html",
    "title": "Vector manipulation in Python",
    "section": "",
    "text": "Figure 1: course banner\nIn this lab, we will have the opportunity to practice once again with the NumPy library. This time, we will explore some advanced operations with arrays and matrices.\nAt the end of the previous module, we used PCA to transform a set of many variables into a set of only two uncorrelated variables. This process was made through a transformation of the data called rotation.\nIn this week’s assignment, we will need to find a transformation matrix from English to French vector space embeddings. Such a transformation matrix is nothing else but a matrix that rotates and scales vector spaces.\nIn this notebook, we will explain in detail the rotation transformation."
  },
  {
    "objectID": "notes/c1w4/lab01.html#transforming-vectors",
    "href": "notes/c1w4/lab01.html#transforming-vectors",
    "title": "Vector manipulation in Python",
    "section": "Transforming vectors",
    "text": "Transforming vectors\nThere are three main vector transformations: * Scaling * Translation * Rotation\nIn previous notebooks, we have applied the first two kinds of transformations. Now, let us learn how to use a fundamental transformation on vectors called rotation.\nThe rotation operation changes the direction of a vector, letting unaffected its dimensionality and its norm. Let us explain with some examples.\nIn the following cells, we will define a NumPy matrix and a NumPy array. Soon we will explain how this is related to matrix rotation.\n\nimport numpy as np                     # Import numpy for array manipulation\nimport matplotlib.pyplot as plt        # Import matplotlib for charts\nfrom utils_nb import plot_vectors      # Function to plot vectors (arrows)\n\n\nExample 1\n\n# Create a 2 x 2 matrix\nR = np.array([[2, 0],\n              [0, -2]])\n\n\nx = np.array([[1, 1]]) # Create a 1 x 2 matrix\n\nThe dot product between a vector and a square matrix produces a rotation and a scaling of the original vector.\nRemember that our recommended way to get the dot product in Python is np.dot(a, b):\n\ny = np.dot(x, R) # Apply the dot product between x and R\ny\n\narray([[ 2, -2]])\n\n\nWe are going to use Pyplot to inspect the effect of the rotation on 2D vectors visually. For that, we have created a function plot_vectors() that takes care of all the intricate parts of the visual formatting. The code for this function is inside the utils_nb.py file.\nNow we can plot the vector \\vec x = [1, 1] in a cartesian plane. The cartesian plane will be centered at [0,0] and its x and y limits will be between [-4, +4]\n\nplot_vectors([x], axes=[4, 4], fname='transform_x.svg')\n\n\n\n\n\n\n\n\nNow, let’s plot in the same system our vector \\vec x = [1, 1] and its dot product with the matrix\nRo = \\begin{bmatrix} 2 & 0 \\\\ 0 & -2 \\end{bmatrix}\ny = x \\cdot Ro = [[-2, 2]]\n\nplot_vectors([x, y], axes=[4, 4], fname='transformx_and_y.svg')\n\n\n\n\n\n\n\n\nNote that the output vector y (blue) is transformed in another vector.\n\n\nExample 2\nWe are going to use Pyplot to inspect the effect of the rotation on 2D vectors visually. For that, we have created a function that takes care of all the intricate parts of the visual formatting. The following procedure plots an arrow within a Pyplot canvas.\nData that is composed of 2 real attributes is telling to belong to a $ RxR $ or $ R^2 $ space. Rotation matrices in R^2 rotate a given vector \\vec x by a counterclockwise angle \\theta in a fixed coordinate system. Rotation matrices are of the form:\nRo = \\begin{bmatrix} cos \\theta & -sin \\theta \\\\ sin \\theta & cos \\theta \\end{bmatrix}\nThe trigonometric functions in Numpy require the angle in radians, not in degrees. In the next cell, we define a rotation matrix that rotates vectors by 45^o.\n\nangle = 100 * (np.pi / 180) #convert degrees to radians\n\nRo = np.array([[np.cos(angle), -np.sin(angle)],\n              [np.sin(angle), np.cos(angle)]])\n\nx2 = np.array([2, 2]).reshape(1, -1) # make it a row vector\ny2 = np.dot(x2, Ro)\n\nprint('Rotation matrix')\nprint(Ro)\nprint('\\nRotated vector')\nprint(y2)\n\nprint('\\n x2 norm', np.linalg.norm(x2))\nprint('\\n y2 norm', np.linalg.norm(y2))\nprint('\\n Rotation matrix norm', np.linalg.norm(Ro))\n\nRotation matrix\n[[-0.17364818 -0.98480775]\n [ 0.98480775 -0.17364818]]\n\nRotated vector\n[[ 1.62231915 -2.31691186]]\n\n x2 norm 2.8284271247461903\n\n y2 norm 2.82842712474619\n\n Rotation matrix norm 1.414213562373095\n\n\n\nplot_vectors([x2, y2], fname='transform_02.svg')\n\n\n\n\n\n\n\n\nSome points to note:\n\nThe norm of the input vector is the same as the norm of the output vector. Rotations matrices do not modify the norm of the vector, only its direction.\nThe norm of any R^2 rotation matrix is always \\sqrt 2 = 1.414221"
  },
  {
    "objectID": "notes/c1w4/lab01.html#frobenius-norm",
    "href": "notes/c1w4/lab01.html#frobenius-norm",
    "title": "Vector manipulation in Python",
    "section": "Frobenius Norm",
    "text": "Frobenius Norm\nThe Frobenius norm is the generalization to R^2 of the already known norm function for vectors\n\\| \\vec a \\| = \\sqrt {{\\vec a} \\cdot {\\vec a}} \nFor a given R^2 matrix A, the frobenius norm is defined as:\n\\|\\mathrm{A}\\|_{F} \\equiv \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n}\\left|a_{i j}\\right|^{2}}\n\nA = np.array([[2, 2],\n              [2, 2]])\n\nnp.square() is a way to square each element of a matrix. It must be equivalent to use the * operator in Numpy arrays.\n\nA_squared = np.square(A)\nA_squared\n\narray([[4, 4],\n       [4, 4]])\n\n\nNow we can sum over the elements of the resulting array, and then get the square root of the sum.\n\nA_Frobenius = np.sqrt(np.sum(A_squared))\nA_Frobenius\n\nnp.float64(4.0)\n\n\nThat was the extended version of the np.linalg.norm() function. We can check that it yields the same result.\n\nprint('Frobenius norm of the Rotation matrix')\nprint(np.sqrt(np.sum(Ro * Ro)), '== ', np.linalg.norm(Ro))\n\nFrobenius norm of the Rotation matrix\n1.414213562373095 ==  1.414213562373095\n\n\nCongratulations!! We’ve covered a few more matrix operations in this lab. This will come in handy in this week’s programming assignment!"
  },
  {
    "objectID": "notes/c1w4/index.html",
    "href": "notes/c1w4/index.html",
    "title": "Machine Translation and Document Search",
    "section": "",
    "text": "Figure 1: course banner\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\nVideo: Overview\nThis week we will be looking at machine translation and document search. We will start by looking at how we can represent words in a vector space. We will then look at how we can use these vectors to translate words from one language to another. We will also look at how we can use these vectors to search for documents that are similar to a given document.\n\n\n\n\n\n\n\ntranslation and search\n\n\n\n\nFigure 3: Using Vector Space Models for Translation and Search\n\n\n\n\n\n\n\n\nObjectives\n\n\n\n\nFigure 4: Learning Objectives\n\n\n\n\n\n\n\n\n\nLearning Objectives\n\n\n\nThis week’s learning objectives include:\n\nGradient descent\nApproximate nearest neighbors\nLocality sensitive hashing\nHash functions\nHash tables\nK nearest neighbors\nDocument search\nMachine translation\nFrobenius norm\n\n\n\n\n\n\nTransforming word vectors\nIn the previous week, I showed we how we can plot word vectors. Now, we will see how we can take a word vector and learn a mapping that will allow we to translate words by learning a “transformation matrix”. Here is a visualization:\n\n\n\n\n\n\n\nCaption\n\n\n\n\nFigure 5: Figure Text\n\n\nNote that the word “chat” in french means cat. We can learn that by taking the vector corresponding to “cat” in english, multiplying it by a matrix that we learn and then we can use cosine similarity between the output and all the french vectors. We should see that the closest result is the vector which corresponds to “chat”.\nHere is a visualization of that showing we the aligned vectors:\nNote that:\n\nX corresponds to the matrix of English word vectors and\nY corresponds to the matrix of French word vectors.\nR is the mapping matrix.\n\n\n\n\n\n\n\n\nCaption\n\n\n\n\nFigure 6: Figure Text\n\n\nSteps required to learn R:\n\nInitialize R\nFor loop​\n\n\n\\text{Loss } \\mathcal{L} = || XR-Y||_F\n\n\ng= \\frac{dLoss}{dR}\n\n\nR = R−\\alpha ∗ g\n\n\n\n\n\n\n\n\nCaption\n\n\n\n\nFigure 7: Figure Text\n\n\nHere is an worked out example to show we how the Frobenius norm works.\n\n\n\n\n\n\n\nCaption\n\n\n\n\nFigure 8: Figure Text\n\n\n\n∥XR−Y∥_F\n\n\nA = \\begin{bmatrix} 2 & 2 \\\\ 2 & 2 \\end{bmatrix}\n\n\n∥A_F∥= \\sqrt{2^2 + 2^2 + 2^2 + 2^2}  = 4\n\n\n∥A∥_F ≡ \\sum _{i=1}^m\\sum _{j=1}^n |a_{ij}|^2\n\nIn summary we are making use of the following:\n\nXR≈Y\nminimize ∥XR−Y∥_F^2 ​\n\n\n\n\n\n\n\n\nCaption\n\n\n\n\nFigure 9: Figure Text\n\n\n\n\n\n\n\n\nCaption\n\n\n\n\nFigure 10: Figure Text\n\n\n\n\n\n\n\n\nCaption\n\n\n\n\nFigure 11: Figure Text\n\n\n\n\n\n\n\nUngraded Lab: Rotation matrices in R2\nRotation matrices in R2\n\n\nVideo: K-nearest neighbors\nAfter we have computed the output of XR we get a vector.\nWe then need to find the most similar vectors to your output. Here is a visual example:\n\n\n\n\n\n\n\nCaption\n\n\n\n\nFigure 12: Finding Translation\n\n\nIn the video, we mentioned if we were in San Francisco, and we had friends all over the world, we would want to find the nearest neighbors. To do that it might be expensive to go over all the countries one at a time. So we will introduce hashing to show we how we can do a look up much faster.\n\n\n\n\n\n\n\nCaption\n\n\n\n\nFigure 13: KNN\n\n\n\n\n\n\n\n\nCaption\n\n\n\n\nFigure 14: KNN\n\n\n\n\n\n\n\n\nCaption\n\n\n\n\nFigure 15: KNN\n\n\n\n\n\n\nHash tables and hash functions\nImagine we had to cluster the following figures into different buckets:\n\n\n\n\n\n\n\nHashing\n\n\n\n\nFigure 16: Hashing\n\n\nNote that the figures blue, red, and gray ones would each be clustered with each other\n\n\n\n\n\n\n\nHash Tables\n\n\n\n\nFigure 17: Hashing\n\n\nWe can think of hash function as a function that takes data of arbitrary sizes and maps it to a fixed value. The values returned are known as hash values or even hashes.\n\n\n\n\n\n\n\nHash Tables\n\n\n\n\nFigure 18: Hashing\n\n\nThe diagram above shows a concrete example of a hash function which takes a vector and returns a value. Then we can mod that value by the number of buckets and put that number in its corresponding bucket. For example, 14 is in the 4th bucker, 17 & 97 are in the 7th bucket. Let’s take a look at how we can do it using some code.\n\n\n\n\n\n\n\nHash Tables\n\n\n\n\nFigure 19: Hashing\n\n\nThe code snippet above creates a basic hash table which consists of hashed values inside their buckets. hash_function takes in value_l (a list of values to be hashed) and n_buckets and mods the value by the buckets. Now to create the hash_table, we first initialize a list to be of dimension n_buckets (each value will go to a bucket). For each value in your list of values, we will feed it into your hash_function, get the hash_value, and append it to the list of values in the corresponding bucket.\nNow given an input, we don’t have to compare it to all the other examples, we can just compare it to all the values in the same hash_bucket that input has been hashed to.\nWhen hashing we sometimes want similar words or similar numbers to be hashed to the same bucket. To do this, we will use “locality sensitive hashing.” Locality is another word for “location”. So locality sensitive hashing is a hashing method that cares very deeply about assigning items based on where they’re located in vector space.\n\n\n\n\n\n\n\nHash Tables\n\n\n\n\nFigure 20: Hashing\n\n\n\n\n\n\n\n\nHash Tables\n\n\n\n\nFigure 21: Hashing\n\n\n\n\n\nLocality sensitive hashing\nLocality sensitive hashing is a technique that allows we to hash similar inputs into the same buckets with high probability.\n\n\n\n\n\n\n\nHash Tables\n\n\n\n\nFigure 22: Locality sensitive hashing\n\n\nInstead of the typical buckets we have been using, we can think of clustering the points by deciding whether they are above or below the line. Now as we go to higher dimensions (say n-dimensional vectors), we would be using planes instead of lines. Let’s look at a concrete example:\n\n\n\n\n\n\n\nHash Tables\n\n\n\n\nFigure 23: Locality sensitive hashing\n\n\nGiven some point located at (1,1) and three vectors V_1=(1,2), V_2=(-1,1), V_3=(-2,-1) we will see what happens when we take the dot product. First note that the dashed line is our plane. The vector with point P=(1,1) is perpendicular to that line (plane). Now any vector above the dashed line that is multiplied by (1,1) would have a positive number. Any vector below the dashed line when dotted with (1,1) will have a negative number. Any vector on the dashed line multiplied by (1,1) will give we a dot product of 0. ​ Here is how to visualize a projection (i.e. a dot product between two vectors):\n\n\n\n\n\n\n\nHash Tables\n\n\n\n\nFigure 24: Locality sensitive hashing\n\n\nWhen we take the dot product of a vector V and a P, then we take the magnitude or length of that vector, we get the black line (labelled as Projection). The sign indicates on which side of the plane the projection vector lies.\n\n\nMultiple Planes\nWe can use multiple planes to get a single hash value. Let’s take a look at the following example:\n\n\n\n\n\n\n\nHash Tables\n\n\n\n\nFigure 25: Locality sensitive hashing\n\n\nGiven some point denoted by v, we can run it through several projections P_1, P_2, P_3 to get one hash value. If we compute P_1v^T P_1v^T we get a positive number, so we set h_1=1. P_2v^T P_2v^T gives we a positive number so we get h_2=1. P_3v^T P_3v^T is a negative number so we set h_3 to be 0. We can then compute the hash value as follows. ​ \nhash=2^0×h_1+2^1×h_2+2^2×h_3 = 1×1+2×1+4×0=3\n\nAnother way to think of it, is at each time we are asking the plane to which side will we find the point (i.e. 1 or 0) until we find your point bounded by the surrounding planes.The hash value is then defined as:\n\nhash_{value}=\\sum_i^H 2^i×h_i\n\nHere is how we can code it up:\n\ndef hash_value_of_vector(P, v):\n    hash_value = 0\n    for i, plane in enumerate(P):\n        sign = side_of_plane(plane, v)\n        hash_i = 1 if sign else 0\n        hash_value += hash_i * 2**i\n    return hash_value\n\nP_l is the list of planes. We initialize the value to 0, and then we iterate over all the planes (P), and we keep track of the index. We get the sign by finding the sign of the dot product between v and your plane P. If it is positive we set it equal to 1, otherwise we set it equal to 0. We then add the score for the ith plane to the hash value by computing 2^i×h_i.\n\n\n\n\n\n\n\n\nhash_multiple_plane()\n\n\n\n\nFigure 26: Code for computing hash value for multiple planes\n\n\n\n\nUngraded Lab: Hash tables\nHash Tables Lab\n\n\nReading: Approximate nearest neighbors\nApproximate nearest neighbors does not give we the full nearest neighbors but gives we an approximation of the nearest neighbors. It usually trades off accuracy for efficiency.\nLook at the following plot:\n\n\n\n\n\n\n\n\nnearest neighnours\n\n\n\n\nFigure 27: Approximate nearest Neighbors\n\n\nWe are trying to find the nearest neighbor for the red vector (point). The first time, the plane gave we green points. We then ran it a second time, but this time we got the blue points. The third time we got the orange points to be the neighbors. So we can see as we do it more times, we are likely to get all the neighbors. Here is the code for one set of random planes. Make sure we understand what is going on.\n\n\n\n\n\n\n\n\nhash_multiple_plane()\n\n\n\n\nFigure 28: Code for computing hash value for multiple planes\n\n\n\n\n\n\n\n\nDocument representation\n\n\n\n\nFigure 29: A combination of word vectors to get document vectors based on Document search with KNN\n\n\n\n\n\nSearching documents\nThe previous video shows we a toy example of how we can actually represent a document as a vector.\n\n\n\n\n\n\n\nhash_multiple_plane()\n\n\n\n\nFigure 30: Code for combining word vectors to get document vectors\n\n\nIn this example, we just add the word vectors of a document to get the document vector. So in summary we should now be familiar with the following concepts:\n\n\n\n\n\n\n\nhash_multiple_plane()\n\n\n\n\nFigure 31: Code for combining word vectors to get document vectors\n\n\n\n\nVideo: Andrew Ng with Kathleen McKeown\n\nI am delighted to have with me here today, Kathy McKeown, who is the Henry and Gertrude Rothschild Professor of Computer Science at Columbia University. Where she is also the founding director of the Institute for Data Sciences and Engineering. She is also an Amazon Scholar and is well known for the work that she’s done over many years on text summarization and many other topics in NLP. Welcome, Kathy, and thanks for joining me.\n\n\nThanks Andrew, and thanks for having me.\n\n\nSo today we lead a large group doing NLP research, but your journey to becoming an AI researcher and an NLP searcher has been an unusual one. If I remember correctly, we actually majored in comparative literature when we were in undergrad, even though you’re also very mathematically oriented at the time. So tell us we story of how we became an NLP researcher.\n\n\nYeah, so when I started out at Brown I didn’t know what I wanted to major in. So I took courses both in math and comparative literature. And as I went on, I became more interested in comparative literature. Probably in part because of the teachers who I had who really influenced me. It was only as I came near the end of my time at Brown that and when I graduated, I took a job as a programmer, which I found actually very boring. And I thought if I was going to have to be working 40 hours a week, I wanted to be doing something that I enjoyed. And it was then that a friend of mine who was a linguistics major at Brown told me about computational linguistics. And so I spent a lot of the year in the library reading about AI and natural language processing. And when I applied for Graduate School the following year I knew that was what I wanted to do. Because it gave me a way to bring together my interest in language and in math.\n\n\nSo that’s fascinating, so as a comparative literature major, we spent a lot of time in the Brown University Library reading about computational linguistics and NLP. Today, with a lot of learners, maybe some watching this video that may not yet be a NLP researcher or AI engineer wanting to break into the field. So I’d love to hear more about what your experience was like reading so much. Were we doing it by yourself? Did we have a study group or what was that like?\n\n\nI was doing it entirely by myself. I really had no guidance in terms of what to look at. I guess this friend that I had made a few suggestions. And then I traced references when I first began reading. I would follow up on references to go further. Yeah, and when I first entered Graduate School, and I had essentially switched fields, I found it very frightening. I was sure that I was and impostor, that I didn’t know enough. And before long they would find out that I really shouldn’t be there after all. Yeah, but that’s something we overcome with time and we learn that it’s not the case and people value your input.\n\n\nThat’s really inspiring. Thank we for sharing that. Would we have any advice for someone maybe today that is trying to do this themselves and wondering if they know enough, or are good enough, or should be in the field? It sounds like we got through that and you’ve been incredibly successful. But what would we say to someone today maybe looking to follow in your footsteps and wondering if this is very for them or if they’ll make it?\n\n\nSo I I guess I have a couple pieces of advice. I do think reaching out to people and talking to people is useful. Until I got to Graduate School I wasn’t in an environment where I had people to talk to. So I do think it’s really helpful, especially to talk to your peers about what they’re doing and what they’re interested in. When we pick problems to work on. And I guess, especially in today’s world of deep learning and neural nets, I would advise choosing problems that are different from what everybody else works on. Sort of strike out in a different direction, choose something new, a new task, and take off from there.\n\n\nAnd I think I would love to come back to the different problems stopping within. And when I speak with learners from around the world, I do hear from some that they feel lonely or isolated. They’re kind of out somewhere or maybe not living in the major tech hub, and they sometimes feel like they’re doing this by themselves. So I find it actually really inspiring that we were able to do that by yourself in a library in Brown University. I don’t know if we have any other thoughts to offer learners that may feel like they’re somewhere in a company or in a city just trying to do this by themselves.\n\n\nI’m not sure I do have a lot more to say about it. I guess read what we enjoy about, if we can be part of a reading group, an online reading group, that would be helpful. There are a lot of reading groups now, and that’s a good way to get sort of insights. There are online videos and course experiences, like yours. And I think that’s a way to find out what’s going on and get in touch with what people are doing. So I think today, the online environment can help people get connected and hear what’s going on. I was lucky, I mean, I was really lucky, I applied to Penn. I didn’t know that at the time it was the best place in natural language processing, and that was totally luck. So I don’t know that I would recommend doing it blind again today, I think getting advice is great.\n\n\nEveryone that’s been successful has had many elements of luck. But the preparation makes we ready to take advantage of when good luck falls into your lap.\n\n\nYeah.\n\n\nThank we for sharing that, that was really inspiring to hear about your early days as a developing researcher. And today we lead a large group at Columbia University doing very interdisciplinary work and doing a lot of work on summarization and other topics. So tell us a bit more about your current work and which excites,\n\n\nSo I mean, summarization has really been the bulk of my work over the most recent years. We’ve done work on summarization of all kinds of different genres, from personal novels to emails. One thread of research in summarization that I’m particularly excited about is work that I’ve done with researchers at Amazon and which was published at ACL. And this is work on summarization of novel chapters. And it’s very new, no one has been working on this task, it’s very challenging, very different from summarization. So the chapters are much longer than the news articles on which most current work in summarization today is done. And that is a challenge for current neural models. And a big problem is that there is an extreme amount of paraphrasing between the input, which is 19th century novels, and the output, which is a summary written in today’s language. None of the current models can handle the kind of paraphrasing that we see there. And that, in general, is a topic that I’m really interested in is a sort of very abstractive summarization. Where the sentences use different words than the input document, where the syntactic structure is different. And that is very different from the vast Majority of work today which is done on summarization of news. And it’s done on summarization of news because that’s where the data is. So some of the other areas that I’m looking at are summarization of personal narratives that we find online where the personal narratives are very informal language and the summary is more formal. Summarization of debates. In past work, we’ve also done summarization of email. Which has some of those same characteristics.\n\n\nWhy did we choose to work on the novel summarization task?\n\n\nWell, so we had done work on novels even earlier. I would say in 2010, when one of my students who was very interested in creative writing, and I really thought he should do a PhD. And so to convince him to stay, we came to a topic that he would be happy with, which was analysis and generation of creative language. And I felt then that my work came full circle, that we collaborated then with a professor in comparative literature. So I acme back to my roots in comparative literature and that was a lot of fun. So, when I first went to Amazon, I knew because of Kindle and online on Amazon, they have a lot of novels. And I thought, what would be more fun than being able to summarize novels?\n\n\nSounds like a fun project, I read a lot on my Kindle, so maybe your work will be a feature on Kindle some day. One aspect of your work that stood out as well is that you’re known for doing highly interdisciplinary work. So rather than focusing narrowly on NLP research, your work spans AI, and the media where I know the Columbia University is a great journalism school. So wonderful journalists we work with there or the application of NLP to social networks. I think we work with medical problems. So tell us a bit about how we think about interdisciplinary work because you’ve done more of it I think than most NLP researchers have.\n\n\nYeah, I really enjoy interdisciplinary work. I think it’s my most favorite kind of research to do. And in part we get a really different perspective on research in the world when we talk to people in other fields. It takes us out of our sort of technical, narrow field.\n\n\nAnd so earlier, we alluded to picking research topics that are novel and I think your research portfolio has certainly touched on a lot of problems that very few others are working on. So can we say more about that? So how do we pick research projects we work on and how do we advise others to pick topics to work on?\n\n\nI think it’s important to pick a task that matters. So that for me is one thing to look at. For example, most of the work in tech summarization today is done on summarization of what’s called single document summarization of news. So take one news article in and generate a summary of that news article. And the reason for that is because that’s where the data is. There’s a huge amount of data that has been pulled together from first the CNN Daily Mail Corpus, and later New York Times, and there are a number of other corpora as well. The problem is that’s not really a task that we need. We’ve known for a long time that the lead of the news article can help people pretty well in serving as a summary of the document. And in fact, for years it was hard to beat. The lead people just worked on, that wasn’t a problem that people worked on in the early years of summarization.\n\n\nThe lead being the first sentence or the first-\n\n\nThe first couple of sentences in the news article. So, Yeah, I mean, people work on a problem like that because that’s where the data is. We have leaderboards. People are competitive. They like to be The leaderboard, but I would question does that one or even half a point in Rouge, which the automated network used to score them, really make a difference? If we look at the output we can see that actually the summaries are quite similar and either one of them might be fine. So I prefer to go in directions that people haven’t gone in before and to choose a task where if we solve it, it’s going to help people. It’s going to be a useful application that you’ve developed. So this is why I have done things like summarization of personal narrative, which we did in the course of disaster. So that we could summarize, think of having a browsing view of summaries of what people have experienced after they’ve lived through a disaster. Or the current work on summarization of novels where it’d be helpful to have a summary of an input chapter. I’d like to go in a different direction, in part because I want to solve the task that matters. But I also like to go in a different direction because in this day in age of deep learning where results come so fast, everybody works on the same problem trying to beat the previous state of the art. It can be hard to be the first one to get there, and if we go in a different direction, nobody else is working on, we are going to be the first one to come to solution. And that’s what I like to do in my research. Overtime I like to be first on a problem. I see, cool yeah. And I feel like, for myself, I have a lot of respect for people that could push that extra half points of performance on the leaderboard because hopefully that advances the whole field. And this all shifts. I also have a lot of respect for people with your creativity in the inside, to charter the new problem that no one else has thought of, and advances the whole field in a different direction. I think the field of AI NLP is broad enough. I think it’s actually not a bad thing if we have lots of people working on lots of different things, including standardized benchmarks. And a bunch of new things.\n\n\nYeah, sure, I feel that there are not as many people who want to go in that new direction and it does take some sort of guts to do it. Because the first thing that happens when we submit a paper is there is no benchmark. There is no baseline of prior work and reviewers have a very hard time dealing with that. How can they judge whether it’s really a good step forward. Whereas if we can show on a leaderboard that you’ve improved by a certain amount and we stay within the traditional trajectory, it’s easier to judge.\n\n\nYeah, I’m with we on that. Actually, I was recently chatting with one of my friends Sharon Joe who mentioned that sometimes the way benchmarks and metrics are established is that some researcher publishing a paper publishes something using some metric. Maybe a good one, maybe an okay one. But to make sure that subsequent papers can compare to earlier work, then everyone, and more and more people end up using the same metric. More for historical reasons and it makes things comparable rather than because it is actually the most useful metric. It’s funny how metrics get established in academia.\n\n\nYeah, I mean that has happened in the summarization field. And I think also in machine translation where we want an automated metric because it’s easier to develop a system to train over and over again. And yet everybody knows that the automated metrics that we currently have are really flawed. And but everyone keeps using them, because that’s what we’ve always done. One of my most stark memories was I remember going to a the Information Retrieval Conference and attending a workshop in text summarization. And I remember being fascinated, but struck that about half of that workshop was on text summarization algorithms and the other half of that workshop was on how to develop metrics to evaluate. Text summarization especially, the development of automated metrics has been challenging.\n\n\nYeah.\n\n\nHey, so in terms of choosing. Lead topics to work on. One of the pieces of work that you’ve been doing that I thought was fascinating, was we were taking texts from the black community from Harlem, near I guess where we teach at Columbia University and analyzing that as well. Tell us about that.\n\n\nThis is where I’m moving with my work with a researcher from social work. And we’re also beginning to involve a linguist who works on African American vernacular. And what we’re doing is we’re looking at what people say, what kind of emotions they express in reaction to major events that are going on today. So, for example, in reaction to Black Lives Matter and in reaction to COVID-19. So this is work that we’re just beginning. We’ve begun with developing an interface where people can post about their experiences with these events and how they’re feeling. And I guess what we’re hoping to do with that in part, so we have two directions to go. One on the natural language side is to be able to understand how people express different kinds of language. Sorry, different kinds of emotion and African American vernacular. And how that difference from how people express it in standard American English. And look at the difference in language and probably even the difference in content in terms of what’s expressed. And this can help us in developing algorithms that are not biased as we move forward. Most of the work in natural language, all of the systems have been trained on language that comes from news like the Wall Street Journal.\n\n\nYeah, that’s great. If this type of work can help fight bias or build bridges between communities or just play some role in understanding and helping to advance the Black Lives Matter movement, that seems to be a wonderful thing to me.\n\n\nYeah, I mean, we also want in that work to look at the impact of trauma. So it’s a different kind of trauma and sometimes it’s not your personal trauma, but the trauma of seeing what has happened to other people who are like you. So yeah, we want to look at how that is expressed in the different kinds of emotions, the intensity of emotion, and so forth.\n\n\nI find it really wonderful that NLP researchers, AI researchers can play an active role in some of these most important societal questions and issues of our time. It feels like the work we do as AI researchers, it could matter in these really important times.\n\n\nYeah, I mean I think so. And I’ve sort of been trying to do that for a while. I think it really attract students to work with you, and often different kinds of students into the field to work with you. On our first work on with analyzing social media posts of gang involved youth, we didn’t have funding. We did that entirely with undergraduates who were just totally amazing. In earlier work we were looking at being able to automatically generate updates about disaster as it unfolded. We did that after Hurricane Sandy hit New York. And again it was something that students came to me and they had seen this happen and they have seen their neighborhoods hurt. Or they lived through the uncertainty of it and they wanted to help. They wanted to know what can we do and that was at that point in time, this whistle pre-neural net we began developing systems that could automatically generate updates as an event unfolded.\n\n\nI think that we don’t need a PhD, don’t need a long publication record, but in undergrad spotting an opportunity with a desire to help. Can step in and start to work on systems that they can make a difference.\n\n\nYes, they’re really passionate about it and There are really good, the work that came out of that was really excellent.\n\n\nYeah, thank you, so Kathy, this is great stuff. And switching tracks a bit, you’ve been working in NLP and associated areas for a long time. In fact, I saw that even way back in 1985 we written an early book on text generation before the modern neural text generation techniques were around. So you’ve been a leader in the field for a long time and seen a lot of things change. I’d love to hear your thoughts on how the field of NLP has evolved over these many years.\n\n\nSure, so when I started, which I got my PhD in 82. so I spent those earlier years at Penn. And there were some characteristics of the field that were salient. So one of them is that there was a lot of interdisciplinary work. There was in developing NLP systems, we drew a lot on work from linguistics, from philosophy, from psychology, from cognitive science. And so when I was at Penn I interacted a lot with faculty from linguistics. Ellen Prince was one of the people or faculty from philosophy. We spent time in these interdisciplinary meetings. And I can remember walking across campus with my advisor to go from the computer science department to the psychology department, for example. I was influenced a lot and I have to mention this, although it’s not exactly what we asked, I was influenced a lot by senior women at the time. If I look back to who was most influential in how I progressed in my early research. My advisor, of course, who was a male Aravan Joshi. But then also Bonnie Weber, who was there in computer science, Eva Hychova from Charles University at Prague who was a linguist. Barbara Gross who lives at Sanford at that time in the CFLI Institute. And Karen Spark Jones was very influential to me. She was from the field of information retrieval. And she and I spent a lot of time talking about summarization. So interdisciplinary is one main feature of that time. A second was drawing on theories from these other areas, so we drew on theories from linguistics. One main kind of theory that we looked at was the focus of attention and how that changed over the course of a discourse. And how that influenced how we made choices and how we realized text in language. So for example, did we use a pronoun or did we use a full noun phrase? What kind of syntactic structure did we use? We might use different syntactic structures to make a concept more prominent in the discourse. We also drew on work from philosophy. So we drew on work from theories from Cyril about intention, and work from Grice about conversational implicature. And so we looked at these theories and we looked at how we could embody them in our natural language approaches.\n\n\nIt’s great to hear about some of your early sources of inspiration. Much as I think today we will be a source of inspiration to many others. So you’ve seen a lot and see a lot in NLP, which continues to be a rapidly evolving field. So I’m actually curious, Kathy, what do we find most exciting in terms of emerging or exciting NLP technologies?\n\n\nFor me, personally, some of the work that I’ve already talked about today on truly abstracted summarization that uses extreme paraphrasing. Work on analyzing the language from diverse community. So we’ve been looking at the black community, but I think there are other communities we could look at as well. I’m interested in looking at how we deal with bias and data. And another very important topic is being able to arrive at what I would call para linguistic meaning. So this pragmatics information about emotion, about intention, would be another important direction to go. And I also think more work on events, being able to understand what events have happened, and to be able to follow them. I also think about often if I look back. Is it okay, if I talk about this now. I look back?\n\n\nOkay. If I look back on my favorite technologies and papers, I can think of papers from thee points in time. The first would be older, and this was very early work in language generation, on how we pick the words in our sentence. And we thought then that it was a hard problem that constraints came from many different sources. And we wrote a paper called Floating Constraints on Lexical Choice. Where we looked at how information from different parts of language, from the discourse, from the lexicon, from syntax, from semantics, will influence what we chose. And we worked in two different domains. One was basketball and one with stock markets. And I give it the example of the floating constraint, where we want to express both the time at which something happened and the manner. In the first example we expressed time and the verb, and the manner and the adverbs. So Wall Street indexes open strongly, open is the time. And in the second weeks press manner in the verb and time and the propositional phrase. So stock index surged at the start of the trading day. And so we wanted to look at how we could control that choice. And I think control is something that’s missing in language generation and summarization. Today using deep learning methods, how do we control what the output is and make sure it’s true to what our intention is? In more recent work, my favorite is work on News Blaster. That’s still about 15 years ago, but it feels recent to me. And that was where we took a real world problem. We did do some collaboration with journalists, and we’ve developed a testbed, where we could identify the events that happened during the day, and produce summary on each event. And then we also looked at how we could track that overtime. And this platform gave us a common sort of application, in which my students could address really hard research questions. And so that was where we looked at, did some of our first work on abstract and summarization. Looking at how we might compress sentences, how we could fuse phrases together, how we might reference, edit references, so that the summary was more coherent. And we also did work on multi lingual summarization. Yeah.\n\n\nCool, Thank you. Lots of exciting, very distinct projects over the years. Do we have any wrap up? Did we have any lost thoughts? We can just say, do we have any final thoughts?\n\n\nWell, I guess I would just say that natural language is a really exciting field today. There’s been a huge amount of progress with deep learning. We’ve seen dramatic increases in accuracy, but we still have a lot of directions to go. And I guess I would like to see more of the interdisciplinary work being brought back in. I’d like to see people looking at the data more and at their output more, rather than just numbers. But I think there are many exciting directions for people to work in, and I hope we’ll see many people joining the field.\n\n\nThank you, that was great. Yeah, I saw the hope that will have a lot more people join NLP and contribute to all of this exciting work. So thanks Kathy, it was cool\n\n\nThanks we much for asking me, it was fun.\n\n\nFor more interviews with NLP thought leaders, check out the DeepLearning.AI YouTube channel, or enroll in the NLP specialization on Coursera.\n\n\n\n\nReading: Bibliography\n\n(Jurafsky and Martin 2025) - Speech and Language Processing\n\n\n\n\n\n\nReferences\n\nJurafsky, Daniel, and James H. Martin. 2025. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition with Language Models. 3rd ed. https://web.stanford.edu/~jurafsky/slp3/.\n\nCitationBibTeX citation:@online{bochman2020,\n  author = {Bochman, Oren},\n  title = {Machine {Translation} and {Document} {Search}},\n  date = {2020-10-11},\n  url = {https://orenbochman.github.io/notes-nlp/notes/c1w4/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2020. “Machine Translation and Document\nSearch.” October 11, 2020. https://orenbochman.github.io/notes-nlp/notes/c1w4/.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "MT & Document Search via KNN",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w1/lab03.html",
    "href": "notes/c3w1/lab03.html",
    "title": "Data generators",
    "section": "",
    "text": "course banner\nIn Python, a generator is a function that behaves like an iterator. It will return the next item. Here is a link to review python generators. In many AI applications, it is advantageous to have a data generator to handle loading and transforming data for different applications.\nYou will now implement a custom data generator, using a common pattern that you will use during all assignments of this course. In the following example, we use a set of samples a, to derive a new set of samples, with more elements than the original set.\nNote: Pay attention to the use of list lines_index and variable index to traverse the original list.\nimport random \nimport numpy as np\n\n# Example of traversing a list of indexes to create a circular list\na = [1, 2, 3, 4]\nb = [0] * 10\n\na_size = len(a)\nb_size = len(b)\nlines_index = [*range(a_size)] # is equivalent to [i for i in range(0,a_size)], the difference being the advantage of using * to pass values of range iterator to list directly\nindex = 0                      # similar to index in data_generator below\nfor i in range(b_size):        # `b` is longer than `a` forcing a wrap\n    # We wrap by resetting index to 0 so the sequences circle back at the end to point to the first index\n    if index &gt;= a_size:\n        index = 0\n    \n    b[i] = a[lines_index[index]]     #  `indexes_list[index]` point to a index of a. Store the result in b\n    index += 1\n    \nprint(b)\n\n[1, 2, 3, 4, 1, 2, 3, 4, 1, 2]",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "L3 - Data Generators"
    ]
  },
  {
    "objectID": "notes/c3w1/lab03.html#shuffling-the-data-order",
    "href": "notes/c3w1/lab03.html#shuffling-the-data-order",
    "title": "Data generators",
    "section": "Shuffling the data order",
    "text": "Shuffling the data order\nIn the next example, we will do the same as before, but shuffling the order of the elements in the output list. Note that here, our strategy of traversing using lines_index and index becomes very important, because we can simulate a shuffle in the input data, without doing that in reality.\n\n# Example of traversing a list of indexes to create a circular list\na = [1, 2, 3, 4]\nb = []\n\na_size = len(a)\nb_size = 10\nlines_index = [*range(a_size)]\nprint(\"Original order of index:\",lines_index)\n\n# if we shuffle the index_list we can change the order of our circular list\n# without modifying the order or our original data\nrandom.shuffle(lines_index) # Shuffle the order\nprint(\"Shuffled order of index:\",lines_index)\n\nprint(\"New value order for first batch:\",[a[index] for index in lines_index])\nbatch_counter = 1\nindex = 0                # similar to index in data_generator below\nfor i in range(b_size):  # `b` is longer than `a` forcing a wrap\n    # We wrap by resetting index to 0\n    if index &gt;= a_size:\n        index = 0\n        batch_counter += 1\n        random.shuffle(lines_index) # Re-shuffle the order\n        print(\"\\nShuffled Indexes for Batch No.{} :{}\".format(batch_counter,lines_index))\n        print(\"Values for Batch No.{} :{}\".format(batch_counter,[a[index] for index in lines_index]))\n    \n    b.append(a[lines_index[index]])     #  `indexes_list[index]` point to a index of a. Store the result in b\n    index += 1\nprint()    \nprint(\"Final value of b:\",b)\n\nOriginal order of index: [0, 1, 2, 3]\nShuffled order of index: [3, 0, 2, 1]\nNew value order for first batch: [4, 1, 3, 2]\n\nShuffled Indexes for Batch No.2 :[1, 2, 3, 0]\nValues for Batch No.2 :[2, 3, 4, 1]\n\nShuffled Indexes for Batch No.3 :[3, 2, 0, 1]\nValues for Batch No.3 :[4, 3, 1, 2]\n\nFinal value of b: [4, 1, 3, 2, 2, 3, 4, 1, 4, 3]\n\n\nNote: We call an epoch each time that an algorithm passes over all the training examples. Shuffling the examples for each epoch is known to reduce variance, making the models more general and overfit less.\n\nExercise\nInstructions: Implement a data generator function that takes in batch_size, x, y shuffle where x could be a large list of samples, and y is a list of the tags associated with those samples. Return a subset of those inputs in a tuple of two arrays (X,Y). Each is an array of dimension (batch_size). If shuffle=True, the data will be traversed in a random form.\nDetails:\nThis code as an outer loop\nwhile True:  \n...  \nyield((X,Y))  \nWhich runs continuously in the fashion of generators, pausing when yielding the next values. We will generate a batch_size output on each pass of this loop.\nIt has an inner loop that stores in temporal lists (X, Y) the data samples to be included in the next batch.\nThere are three slightly out of the ordinary features.\n\nThe first is the use of a list of a predefined size to store the data for each batch. Using a predefined size list reduces the computation time if the elements in the array are of a fixed size, like numbers. If the elements are of different sizes, it is better to use an empty array and append one element at a time during the loop.\nThe second is tracking the current location in the incoming lists of samples. Generators variables hold their values between invocations, so we create an index variable, initialize to zero, and increment by one for each sample included in a batch. However, we do not use the index to access the positions of the list of sentences directly. Instead, we use it to select one index from a list of indexes. In this way, we can change the order in which we traverse our original list, keeping untouched our original list.\nThe third also relates to wrapping. Because batch_size and the length of the input lists are not aligned, gathering a batch_size group of inputs may involve wrapping back to the beginning of the input loop. In our approach, it is just enough to reset the index to 0. We can re-shuffle the list of indexes to produce different batches each time.\n\n\ndef data_generator(batch_size, data_x, data_y, shuffle=True):\n    '''\n      Input: \n        batch_size - integer describing the batch size\n        data_x - list containing samples\n        data_y - list containing labels\n        shuffle - Shuffle the data order\n      Output:\n        a tuple containing 2 elements:\n        X - list of dim (batch_size) of samples\n        Y - list of dim (batch_size) of labels\n    '''\n    \n    data_lng = len(data_x) # len(data_x) must be equal to len(data_y)\n    index_list = [*range(data_lng)] # Create a list with the ordered indexes of sample data\n    \n    \n    # If shuffle is set to true, we traverse the list in a random way\n    if shuffle:\n        random.shuffle(index_list) # Inplace shuffle of the list\n    \n    index = 0 # Start with the first element\n    # START CODE HERE    \n    # Fill all the None values with code taking reference of what you learned so far\n    while True:\n        X = [0] * batch_size # We can create a list with batch_size elements. \n        Y = [0] * batch_size # We can create a list with batch_size elements. \n        \n        for i in range(batch_size):\n            \n            # Wrap the index each time that we reach the end of the list\n            if index &gt;= data_lng:\n                index = 0\n                # Shuffle the index_list if shuffle is true\n                if shuffle:\n                    rnd.shuffle(index_list) # re-shuffle the order\n            \n            X[i] = data_x[index_list[index]] # We set the corresponding element in x\n            Y[i] = data_y[index_list[index]] # We set the corresponding element in x\n    # END CODE HERE            \n            index += 1\n        \n        yield((X, Y))\n\nIf your function is correct, all the tests must pass.\n\ndef test_data_generator():\n    x = [1, 2, 3, 4]\n    y = [xi ** 2 for xi in x]\n    \n    generator = data_generator(3, x, y, shuffle=False)\n\n    assert np.allclose(next(generator), ([1, 2, 3], [1, 4, 9])),  \"First batch does not match\"\n    assert np.allclose(next(generator), ([4, 1, 2], [16, 1, 4])), \"Second batch does not match\"\n    assert np.allclose(next(generator), ([3, 4, 1], [9, 16, 1])), \"Third batch does not match\"\n    assert np.allclose(next(generator), ([2, 3, 4], [4, 9, 16])), \"Fourth batch does not match\"\n\n    print(\"\\033[92mAll tests passed!\")\n\ntest_data_generator()\n\nAll tests passed!\n\n\nIf you could not solve the exercise, just run the next code to see the answer.\n\nimport base64\n\nsolution = \"ZGVmIGRhdGFfZ2VuZXJhdG9yKGJhdGNoX3NpemUsIGRhdGFfeCwgZGF0YV95LCBzaHVmZmxlPVRydWUpOgoKICAgIGRhdGFfbG5nID0gbGVuKGRhdGFfeCkgIyBsZW4oZGF0YV94KSBtdXN0IGJlIGVxdWFsIHRvIGxlbihkYXRhX3kpCiAgICBpbmRleF9saXN0ID0gWypyYW5nZShkYXRhX2xuZyldICMgQ3JlYXRlIGEgbGlzdCB3aXRoIHRoZSBvcmRlcmVkIGluZGV4ZXMgb2Ygc2FtcGxlIGRhdGEKICAgIAogICAgIyBJZiBzaHVmZmxlIGlzIHNldCB0byB0cnVlLCB3ZSB0cmF2ZXJzZSB0aGUgbGlzdCBpbiBhIHJhbmRvbSB3YXkKICAgIGlmIHNodWZmbGU6CiAgICAgICAgcm5kLnNodWZmbGUoaW5kZXhfbGlzdCkgIyBJbnBsYWNlIHNodWZmbGUgb2YgdGhlIGxpc3QKICAgIAogICAgaW5kZXggPSAwICMgU3RhcnQgd2l0aCB0aGUgZmlyc3QgZWxlbWVudAogICAgd2hpbGUgVHJ1ZToKICAgICAgICBYID0gWzBdICogYmF0Y2hfc2l6ZSAjIFdlIGNhbiBjcmVhdGUgYSBsaXN0IHdpdGggYmF0Y2hfc2l6ZSBlbGVtZW50cy4gCiAgICAgICAgWSA9IFswXSAqIGJhdGNoX3NpemUgIyBXZSBjYW4gY3JlYXRlIGEgbGlzdCB3aXRoIGJhdGNoX3NpemUgZWxlbWVudHMuIAogICAgICAgIAogICAgICAgIGZvciBpIGluIHJhbmdlKGJhdGNoX3NpemUpOgogICAgICAgICAgICAKICAgICAgICAgICAgIyBXcmFwIHRoZSBpbmRleCBlYWNoIHRpbWUgdGhhdCB3ZSByZWFjaCB0aGUgZW5kIG9mIHRoZSBsaXN0CiAgICAgICAgICAgIGlmIGluZGV4ID49IGRhdGFfbG5nOgogICAgICAgICAgICAgICAgaW5kZXggPSAwCiAgICAgICAgICAgICAgICAjIFNodWZmbGUgdGhlIGluZGV4X2xpc3QgaWYgc2h1ZmZsZSBpcyB0cnVlCiAgICAgICAgICAgICAgICBpZiBzaHVmZmxlOgogICAgICAgICAgICAgICAgICAgIHJuZC5zaHVmZmxlKGluZGV4X2xpc3QpICMgcmUtc2h1ZmZsZSB0aGUgb3JkZXIKICAgICAgICAgICAgCiAgICAgICAgICAgIFhbaV0gPSBkYXRhX3hbaW5kZXhfbGlzdFtpbmRleF1dIAogICAgICAgICAgICBZW2ldID0gZGF0YV95W2luZGV4X2xpc3RbaW5kZXhdXSAKICAgICAgICAgICAgCiAgICAgICAgICAgIGluZGV4ICs9IDEKICAgICAgICAKICAgICAgICB5aWVsZCgoWCwgWSkp\"\n\n# Print the solution to the given assignment\nprint(base64.b64decode(solution).decode(\"utf-8\"))\n\ndef data_generator(batch_size, data_x, data_y, shuffle=True):\n\n    data_lng = len(data_x) # len(data_x) must be equal to len(data_y)\n    index_list = [*range(data_lng)] # Create a list with the ordered indexes of sample data\n    \n    # If shuffle is set to true, we traverse the list in a random way\n    if shuffle:\n        rnd.shuffle(index_list) # Inplace shuffle of the list\n    \n    index = 0 # Start with the first element\n    while True:\n        X = [0] * batch_size # We can create a list with batch_size elements. \n        Y = [0] * batch_size # We can create a list with batch_size elements. \n        \n        for i in range(batch_size):\n            \n            # Wrap the index each time that we reach the end of the list\n            if index &gt;= data_lng:\n                index = 0\n                # Shuffle the index_list if shuffle is true\n                if shuffle:\n                    rnd.shuffle(index_list) # re-shuffle the order\n            \n            X[i] = data_x[index_list[index]] \n            Y[i] = data_y[index_list[index]] \n            \n            index += 1\n        \n        yield((X, Y))\n\n\n\n\nHope you enjoyed this tutorial on data generators which will help you with the assignments in this course.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "L3 - Data Generators"
    ]
  },
  {
    "objectID": "notes/c3w1/lab02.html",
    "href": "notes/c3w1/lab02.html",
    "title": "Classes and subclasses",
    "section": "",
    "text": "course banner\nIn this notebook, I will show you the basics of classes and subclasses in Python. As you’ve seen in the lectures from this week, Trax uses layer classes as building blocks for deep learning models, so it is important to understand how classes and subclasses behave in order to be able to build custom layers when needed.\nBy completing this notebook, you will:",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "L2 - Classes and Subclasses"
    ]
  },
  {
    "objectID": "notes/c3w1/lab02.html#part-1-parameters-methods-and-instances",
    "href": "notes/c3w1/lab02.html#part-1-parameters-methods-and-instances",
    "title": "Classes and subclasses",
    "section": "Part 1: Parameters, methods and instances",
    "text": "Part 1: Parameters, methods and instances\nFirst, let’s define a class My_Class.\n\nclass My_Class: #Definition of My_class\n    x = None    \n\nMy_Class has one parameter x without any value. You can think of parameters as the variables that every object assigned to a class will have. So, at this point, any object of class My_Class would have a variable x equal to None. To check this, I’ll create two instances of that class and get the value of x for both of them.\n\ninstance_a= My_Class() #To create an instance from class \"My_Class\" you have to call \"My_Class\"\ninstance_b= My_Class()\nprint('Parameter x of instance_a: ' + str(instance_a.x)) #To get a parameter 'x' from an instance 'a', write 'a.x'\nprint('Parameter x of instance_b: ' + str(instance_b.x))\n\nParameter x of instance_a: None\nParameter x of instance_b: None\n\n\nFor an existing instance you can assign new values for any of its parameters. In the next cell, assign a value of 5 to the parameter x of instance_a.\n\n### START CODE HERE (1 line) ### \ninstance_a.x = 5\n### END CODE HERE ###\nprint('Parameter x of instance_a: ' + str(instance_a.x))\n\nParameter x of instance_a: 5",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "L2 - Classes and Subclasses"
    ]
  },
  {
    "objectID": "notes/c3w1/lab02.html#the-__init__-method",
    "href": "notes/c3w1/lab02.html#the-__init__-method",
    "title": "Classes and subclasses",
    "section": "1.1 The __init__ method",
    "text": "1.1 The __init__ method\nWhen you want to assign values to the parameters of your class when an instance is created, it is necessary to define a special method: __init__. The __init__ method is called when you create an instance of a class. It can have multiple arguments to initialize the paramenters of your instance. In the next cell I will define My_Class with an __init__ method that takes the instance (self) and an argument y as inputs.\n\nclass My_Class: \n    def __init__(self, y): # The __init__ method takes as input the instance to be initialized and a variable y\n        self.x = y         # Sets parameter x to be equal to y\n\nIn this case, the parameter x of an instance from My_Class would take the value of an argument y. The argument self is used to pass information from the instance being created to the method __init__. In the next cell, create an instance instance_c, with x equal to 10.\n\n### START CODE HERE (1 line) ### \ninstance_c = My_Class(10)\n### END CODE HERE ###\nprint('Parameter x of instance_c: ' + str(instance_c.x))\n\nParameter x of instance_c: 10\n\n\nNote that in this case, you had to pass the argument y from the __init__ method to create an instance of My_Class.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "L2 - Classes and Subclasses"
    ]
  },
  {
    "objectID": "notes/c3w1/lab02.html#the-__call__-method",
    "href": "notes/c3w1/lab02.html#the-__call__-method",
    "title": "Classes and subclasses",
    "section": "1.2 The __call__ method",
    "text": "1.2 The __call__ method\nAnother important method is the __call__ method. It is performed whenever you call an initialized instance of a class. It can have multiple arguments and you can define it to do whatever you want like\n\nChange a parameter,\nPrint a message,\nCreate new variables, etc.\n\nIn the next cell, I’ll define My_Class with the same __init__ method as before and with a __call__ method that adds z to parameter x and prints the result.\n\nclass My_Class: \n    def __init__(self, y): # The __init__ method takes as input the instance to be initialized and a variable y\n        self.x = y         # Sets parameter x to be equal to y\n    def __call__(self, z): # __call__ method with self and z as arguments\n        self.x += z        # Adds z to parameter x when called \n        print(self.x)\n\nLet’s create instance_d with x equal to 5.\n\ninstance_d = My_Class(5)\n\nAnd now, see what happens when instance_d is called with argument 10.\n\ninstance_d(10)\n\n15\n\n\nNow, you are ready to complete the following cell so any instance from My_Class:\n\nIs initialized taking two arguments y and z and assigns them to x_1 and x_2, respectively. And,\nWhen called, takes the values of the parameters x_1 and x_2, sums them, prints and returns the result.\n\n\nclass My_Class: \n    def __init__(self, y, z): #Initialization of x_1 and x_2 with arguments y and z\n        ### START CODE HERE (2 lines) ### \n        self.x_1 = y\n        self.x_2 = z\n        ### END CODE HERE ###\n    def __call__(self):       #When called, adds the values of parameters x_1 and x_2, prints and returns the result \n        ### START CODE HERE (1 line) ### \n        result = self.x_1 + self.x_2 \n        ### END CODE HERE ### \n        print(\"Addition of {} and {} is {}\".format(self.x_1,self.x_2,result))\n        return result\n\nRun the next cell to check your implementation. If everything is correct, you shouldn’t get any errors.\n\ninstance_e = My_Class(10,15)\ndef test_class_definition():\n    \n    assert instance_e.x_1 == 10, \"Check the value assigned to x_1\"\n    assert instance_e.x_2 == 15, \"Check the value assigned to x_2\"\n    assert instance_e() == 25, \"Check the __call__ method\"\n    \n    print(\"\\033[92mAll tests passed!\")\n    \ntest_class_definition()\n\nAddition of 10 and 15 is 25\nAll tests passed!",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "L2 - Classes and Subclasses"
    ]
  },
  {
    "objectID": "notes/c3w1/lab02.html#custom-methods",
    "href": "notes/c3w1/lab02.html#custom-methods",
    "title": "Classes and subclasses",
    "section": "1.3 Custom methods",
    "text": "1.3 Custom methods\nIn addition to the __init__ and __call__ methods, your classes can have custom-built methods to do whatever you want when called. To define a custom method, you have to indicate its input arguments, the instructions that you want it to perform and the values to return (if any). In the next cell, My_Class is defined with my_method that multiplies the values of x_1 and x_2, sums that product with an input w, and returns the result.\n\nclass My_Class: \n    def __init__(self, y, z): #Initialization of x_1 and x_2 with arguments y and z\n        self.x_1 = y\n        self.x_2 = z\n    def __call__(self):       #Performs an operation with x_1 and x_2, and returns the result\n        a = self.x_1 - 2*self.x_2 \n        return a\n    def my_method(self, w):   #Multiplies x_1 and x_2, adds argument w and returns the result\n        result = self.x_1*self.x_2 + w\n        return result\n\nCreate an instance instance_f of My_Class with any integer values that you want for x_1 and x_2. For that instance, see the result of calling My_method, with an argument w equal to 16.\n\n### START CODE HERE (1 line) ### \ninstance_f = My_Class(1,10)\n### END CODE HERE ### \nprint(\"Output of my_method:\",instance_f.my_method(16))\n\nOutput of my_method: 26\n\n\nAs you can corroborate in the previous cell, to call a custom method m, with arguments args, for an instance i you must write i.m(args). With that in mind, methods can call others within a class. In the following cell, try to define new_method which calls my_method with v as input argument. Try to do this on your own in the cell given below.\n\nclass My_Class: \n    def __init__(self, y, z):         #Initialization of x_1 and x_2 with arguments y and z\n        self.x_1 = None\n        self.x_2 = None\n    def __call__(self):               #Performs an operation with x_1 and x_2, and returns the result\n        a = None \n        return a\n    def my_method(self, w):           #Multiplies x_1 and x_2, adds argument w and returns the result\n        b = None\n        return b\n    def new_method(self, v):          #Calls My_method with argument v\n        ### START CODE HERE (1 line) ### \n        result = None\n        ### END CODE HERE ### \n        return result\n\nSPOILER ALERT Solution:\n\n# hidden-cell\nclass My_Class: \n    def __init__(self, y, z):      #Initialization of x_1 and x_2 with arguments y and z\n        self.x_1 = y\n        self.x_2 = z\n    def __call__(self):            #Performs an operation with x_1 and x_2, and returns the result\n        a = self.x_1 - 2*self.x_2 \n        return a\n    def my_method(self, w):        #Multiplies x_1 and x_2, adds argument w and returns the result\n        b = self.x_1*self.x_2 + w\n        return b\n    def new_method(self, v):       #Calls My_method with argument v\n        result = self.my_method(v)\n        return result\n\n\ninstance_g = My_Class(1,10)\nprint(\"Output of my_method:\",instance_g.my_method(16))\nprint(\"Output of new_method:\",instance_g.new_method(16))\n\nOutput of my_method: 26\nOutput of new_method: 26",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "L2 - Classes and Subclasses"
    ]
  },
  {
    "objectID": "notes/c3w1/lab02.html#inheritance",
    "href": "notes/c3w1/lab02.html#inheritance",
    "title": "Classes and subclasses",
    "section": "2.1 Inheritance",
    "text": "2.1 Inheritance\nWhen you define a subclass sub, every method and parameter is inherited from super class, including the __init__ and __call__ methods. This means that any instance from sub can use the methods defined in super. Run the following cell and see for yourself.\n\ninstance_sub_a = sub_c(1,10)\nprint('Parameter x_1 of instance_sub_a: ' + str(instance_sub_a.x_1))\nprint('Parameter x_2 of instance_sub_a: ' + str(instance_sub_a.x_2))\nprint(\"Output of my_method of instance_sub_a:\",instance_sub_a.my_method(16))\n\nParameter x_1 of instance_sub_a: 1\nParameter x_2 of instance_sub_a: 10\nOutput of my_method of instance_sub_a: 26\n\n\nAs you can see, sub_c does not have an initialization method __init__, it is inherited from My_class. However, you can overwrite any method you want by defining it again in the subclass. For instance, in the next cell define a class sub_c with a redefined my_Method that multiplies x_1 and x_2 but does not add any additional argument.\n\nclass sub_c(My_Class):           #Subclass sub_c from My_class\n    def my_method(self):         #Multiplies x_1 and x_2 and returns the result\n        ### START CODE HERE (1 line) ###\n        b = self.x_1*self.x_2 \n        ### END CODE HERE ###\n        return b\n\nTo check your implementation run the following cell.\n\ntest = sub_c(3,10)\nassert test.my_method() == 30, \"The method my_method should return the product between x_1 and x_2\"\n\nprint(\"Output of overridden my_method of test:\",test.my_method()) #notice we didn't pass any parameter to call my_method\n#print(\"Output of overridden my_method of test:\",test.my_method(16)) #try to see what happens if you call it with 1 argument\n\nOutput of overridden my_method of test: 30\n\n\nIn the next cell, two instances are created, one of My_Class and another one of sub_c. The instances are initialized with equal x_1 and x_2 parameters.\n\ny,z= 1,10\ninstance_sub_a = sub_c(y,z)\ninstance_a = My_Class(y,z)\nprint('My_method for an instance of sub_c returns: ' + str(instance_sub_a.my_method()))\nprint('My_method for an instance of My_Class returns: ' + str(instance_a.my_method(10)))\n\nMy_method for an instance of sub_c returns: 10\nMy_method for an instance of My_Class returns: 20\n\n\nAs you can see, even though sub_c is a subclass from My_Class and both instances are initialized with the same values, My_method returns different results for each instance because you overwrote My_method for sub_c.\nCongratulations! You just reviewed the basics behind classes and subclasses. Now you can define your own classes and subclasses, work with instances and overwrite inherited methods. The concepts within this notebook are more than enough to understand how layers in Trax work.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Neural Networks for Sentiment Analysis",
      "L2 - Classes and Subclasses"
    ]
  },
  {
    "objectID": "notes/c4w4/lab01.html",
    "href": "notes/c4w4/lab01.html",
    "title": "Reformer Efficient Attention: Ungraded Lab",
    "section": "",
    "text": "The videos describe two ‘reforms’ made to the Transformer to make it more memory and compute efficient. The Reversible Layers reduce memory and Locality Sensitive Hashing(LSH) reduces the cost of the Dot Product attention for large input sizes. This ungraded lab will look more closely at LSH and how it is used in the Reformer model.\nSpecifically, the notebook has 3 goals",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "L1 - Reformer LSH"
    ]
  },
  {
    "objectID": "notes/c4w4/lab01.html#outline",
    "href": "notes/c4w4/lab01.html#outline",
    "title": "Reformer Efficient Attention: Ungraded Lab",
    "section": "Outline",
    "text": "Outline\n\nPart 1: Trax Efficient Attention classes\nPart 2: Full Dot Product Self Attention\n\n2.1 Description\n\n2.1.1 our_softmax\n\n2.2 our simple attend\n2.3 Class OurSelfAttention\n\nPart 3: Trax LSHSelfAttention\n\n3.1 Description\n3.2 our_hash_vectors\n3.3 Sorting Buckets\n3.4 Chunked dot product attention\n3.5 OurLSHSelfAttention",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "L1 - Reformer LSH"
    ]
  },
  {
    "objectID": "notes/c4w4/lab01.html#1",
    "href": "notes/c4w4/lab01.html#1",
    "title": "Reformer Efficient Attention: Ungraded Lab",
    "section": "Part 1.0 Trax Efficient Attention classes",
    "text": "Part 1.0 Trax Efficient Attention classes\nTrax is similar to other popular NN development platforms such as Keras (now integrated into Tensorflow) and Pytorch in that it uses ‘layers’ as a useful level of abstraction. Layers are often represented as classes. We’re going to improve our understanding of Trax by locally extending the classes used in the attention layers. We will extend only the ‘forward’ functions and utilize the existing attention layers as parent classes. The original code can be found at github:trax/layers/Research/Efficient_attention. This link references release 1.3.4 but note that this is under the ‘research’ directory as this is an area of active research. When accessing the code on Github for review on this assignment, be sure you select the 1.3.4 release tag, the master copy may have new changes.:\n\n\n\n\n\n\nFigure 1: Reference Tag 1.3.4 on github\n\n\n\nWhile Trax uses classes liberally, we have not built many classes in the course so far. Let’s spend a few moments reviewing the classes we will be using.\n\n\n\n\n\n\nFigure 2: Classes from Trax/layers/Research/Efficient_Attention.py that we will be utilizing\n\n\n\nStarting on the right in the diagram below you see EfficientAttentionBase. The parent to this class is the base.layer which has the routines used by all layers. EfficientAttentionBase leaves many routines to be overridden by child classes - but it has an important feature in the Forward routine. It supports a use_reference_code capability that selects implementations that limit some of the complexities to provide a more easily understood version of the algorithms. In particular, it implements a nested loop that treats each ‘example, head’ independently. This simplifies our work as we need only worry about matrix operations on one ‘example, head’ at a time. This loop calls forward_unbatched, which is the child process that we will be overriding.\nOn the top left are the outlines of the two child classes we will be using. The SelfAttention layer is a ‘traditional’ implementation of the dot product attention. We will be implementing the forward_unbatched version of this to highlight the differences between this and the LSH implementation.\nBelow that is the LSHSelfAttention. This is the routine used in the Reformer architecture. We will override the forward_unbatched section of this and some of the utility functions it uses to explore its implementation in more detail.\nThe code we will be working with is from the Trax source, and as such has implementation details that will make it a bit harder to follow. However, it will allow use of the results along with the rest of the Trax infrastructure. I will try to briefly describe these as they arise. The Trax documentation can also be referenced.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "L1 - Reformer LSH"
    ]
  },
  {
    "objectID": "notes/c4w4/lab01.html#1.2",
    "href": "notes/c4w4/lab01.html#1.2",
    "title": "Reformer Efficient Attention: Ungraded Lab",
    "section": "Part 1.2 Trax Details",
    "text": "Part 1.2 Trax Details\nThe goal in this notebook is to override a few routines in the Trax classes with our own versions. To maintain their functionality in a full Trax environment, many of the details we might ignore in example version of routines will be maintained in this code. Here are some of the considerations that may impact our code:\n\nTrax operates with multiple back-end libraries, we will see special cases that will utilize unique features.\n‘Fancy’ numpy indexing is not supported in all backend environments and must be emulated in other ways.\nSome operations don’t have gradients for backprop and must be ignored or include forced re-evaluation.\n\nHere are some of the functions we may see:\n\nAbstracted as fastmath, Trax supports multiple backend’s such as Jax and Tensorflow2\ntie_in: Some non-numeric operations must be invoked during backpropagation. Normally, the gradient compute graph would determine invocation but these functions are not included. To force re-evaluation, they are ‘tied’ to other numeric operations using tie_in.\nstop_gradient: Some operations are intentionally excluded from backprop gradient calculations by setting their gradients to zero.\nBelow we will execute from trax.fastmath import numpy as np, this uses accelerated forms of numpy functions. This is, however a subset of numpy\n\n\nimport os\nimport trax\nfrom trax import layers as tl  # core building block\nimport jax\nfrom trax import fastmath  # uses jax, offers numpy on steroids\n\n# fastmath.use_backend('tensorflow-numpy')\nimport functools\nfrom trax.fastmath import numpy as np  # note, using fastmath subset of numpy!\nfrom trax.layers import (\n    tie_in,\n    length_normalized,\n    apply_broadcasted_dropout,\n    look_adjacent,\n    permute_via_gather,\n    permute_via_sort,\n)\n\n\n# def tie_in(x, y):\n#   if fastmath.backend_name() == 'jax':\n#     return jax.lax.tie_in(x, y)\n#   return y\n\n2025-02-10 16:53:34.595009: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1739199214.607869  121487 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1739199214.611988  121487 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\n\n\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\nCell In[1], line 10\n      8 import functools\n      9 from trax.fastmath import numpy as np  # note, using fastmath subset of numpy!\n---&gt; 10 from trax.layers import (\n     11     tie_in,\n     12     length_normalized,\n     13     apply_broadcasted_dropout,\n     14     look_adjacent,\n     15     permute_via_gather,\n     16     permute_via_sort,\n     17 )\n     20 # def tie_in(x, y):\n     21 #   if fastmath.backend_name() == 'jax':\n     22 #     return jax.lax.tie_in(x, y)\n     23 #   return y\n\nImportError: cannot import name 'tie_in' from 'trax.layers' (/home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/__init__.py)",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "L1 - Reformer LSH"
    ]
  },
  {
    "objectID": "notes/c4w4/lab01.html#2",
    "href": "notes/c4w4/lab01.html#2",
    "title": "Reformer Efficient Attention: Ungraded Lab",
    "section": "Part 2 Full Dot-Product Self Attention",
    "text": "Part 2 Full Dot-Product Self Attention\n\nPart 2.1 Description\n\n\n\n\n\n\nFigure 3: Project datapath and primary data structures and where they are implemented\n\n\n\nThe diagram above shows many of the familiar data structures and operations related to attention and describes the routines in which they are implemented. We will start by working on our_simple_attend or our simpler version of the original attend function. We will review the steps in performing dot-product attention with more focus on the details of the operations and their significance. This is useful when comparing to LSH attention. Note we will be discussing a single example/head unless otherwise specified.\n\n\n\n\n\n\nFigure 4: Dot-product of Query and Key\n\n\n\nThe attend function receives Query and Key. As a reminder, they are produced by a matrix multiply of all the inputs with a single set of weights. We will describe the inputs as embeddings assuming an NLP application, however, this is not required. This matrix multiply very much like a convolutional network where a set of weights (a filter) slide across the input vectors leaving behind a map of the similarity of the input to the filter. In this case, the filters are the weight matrices W^Q and W^K. The resulting maps are Q and K. Q and K have the dimensions of (n_seq, n_q) where n_seq is the number input embeddings and n_q or n_k is the selected size of the Q or K vectors. Note the shading of Q and K, this reflects the fact that each entry is associated with a particular input embedding. You will note later in the code that K is optional. Apparently, similar results can be achieved using Query alone saving the compute and storage associated with K. In that case, the dot-product in attend is matmul(q,q). Note the resulting dot-product (Dot) entries describe a complete (n_seq,n_seq) map of the similarity of all entries of q vs all entries of k. This is reflected in the notation in the dot-product boxes of w_n,w_m representing word_n, word_m. Note that each row of Dot describes the relationship of an input embedding, say w_0, with every other input.\nIn some applications some values are masked. This can be used, for example to exclude results that occur later in time (causal) or to mask padding or other inputs.\n\n\n\n\n\n\nFigure 5: Masking\n\n\n\nThe routine below mask_self_attention implements a flexible masking capability. The masking is controlled by the information in q_info and kv_info.\n\ndef mask_self_attention(\n    dots, q_info, kv_info, causal=True, exclude_self=True, masked=False\n):\n    \"\"\"Performs masking for self-attention.\"\"\"\n    if causal:\n        mask = fastmath.lt(q_info, kv_info).astype(np.float32)\n        dots = dots - 1e9 * mask\n    if exclude_self:\n        mask = np.equal(q_info, kv_info).astype(np.float32)\n        dots = dots - 1e5 * mask\n    if masked:\n        zeros_like_kv_info = tie_in(kv_info, np.zeros_like(kv_info))\n        mask = fastmath.lt(kv_info, zeros_like_kv_info).astype(np.float32)\n        dots = dots - 1e9 * mask\n    return dots\n\nA SoftMax is applied per row of the Dot matrix to scale the values in the row between 0 and 1.\n\n\n\n\n\n\nFigure 6: SoftMax per row of Dot\n\n\n\n\n\nPart 2.1.1 our_softmax\nThis code uses a separable form of the softmax calculation. Recall the softmax: \nsoftmax(x_i)=\\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\\tag{1}\n\nThis can be alternately implemented as: \nlogsumexp(x)=\\log{({\\sum_j \\exp(x_j)})}\\tag{2}\n\n\nsoftmax(x_i)=\\exp({x_i - logsumexp(x)})\\tag{3}\n\nThe work below will maintain a copy of the logsumexp allowing the softmax to be completed in sections. You will see how this is useful later in the LSHSelfAttention class.\nWe’ll create a routine to implement that here with the addition of a passthrough. The matrix operations we will be working on below are easier to follow if we can maintain integer values. So, for tests, we will skip the softmax in some cases.\n\ndef our_softmax(x, passthrough=False):\n    \"\"\" softmax with passthrough\"\"\"\n    logsumexp = fastmath.logsumexp(x, axis=-1, keepdims=True)\n    o = np.exp(x - logsumexp)\n    if passthrough:\n        return (x, np.zeros_like(logsumexp))\n    else:\n        return (o, logsumexp)\n\nLet’s check our implementation.\n\n## compare softmax(a) using both methods\na = np.array([1.0, 2.0, 3.0, 4.0])\nsma = np.exp(a) / sum(np.exp(a))\nprint(sma)\nsma2, a_logsumexp = our_softmax(a)\nprint(sma2)\nprint(a_logsumexp)\n\n[0.0320586  0.08714432 0.23688282 0.6439142 ]\n[0.0320586  0.0871443  0.23688279 0.64391416]\n[4.44019]\n\n\nThe purpose of the dot-product is to ‘focus attention’ on some of the inputs. Dot now has entries appropriately scaled to enhance some values and reduce others. These are now applied to the V entries.\n\n\n\n\n\n\nFigure 7: Applying Attention to V\n\n\n\nV is of size (n_seq,n_v). Note the shading in the diagram. This is to draw attention to the operation of the matrix multiplication. This is detailed below.\n\n\n\n\n\n\nFigure 8: Matrix Multiply\n\n\n\nV is formed by a matrix multiply of the input embedding with the weight matrix W^v whose values were set by backpropagation. The row entries of V are then related to the corresponding input embedding. The matrix multiply weights first column of V, representing a section of each of the input embeddings, with the first row of Dot, representing the similarity of W_0 and each word of the input embedding and deposits the value in Z\n\n\nPart 2.2 our_simple_attend\nIn this section we’ll work on an implementation of attend whose operations you can see in figure 3. It is a slightly simplified version of the routine in efficient_attention.py. We will fill in a few lines of code. The main goal is to become familiar with the routine. You have implemented similar functionality in a previous assignment.\nInstructions Step 1: matrix multiply (np.matmul) q and the k ‘transpose’ kr. Step 2: use our_softmax() to perform a softmax on masked output of the dot product, dots. Step 3: matrix multiply (np.matmul) dots and v.\n\ndef our_simple_attend(\n    q, k=None, v=None,\n    mask_fn=None, q_info=None, kv_info=None,\n    dropout=0.0, rng=None, verbose=False, passthrough=False\n    ):\n  \"\"\"Dot-product attention,  with masking, without optional chunking and/or.\n\n  Args:\n    q: Query vectors, shape [q_len, d_qk]\n    k: Key vectors, shape [kv_len, d_qk]; or None\n    v: Value vectors, shape [kv_len, d_v]\n    mask_fn: a function reference that implements masking (e.g. mask_self_attention)\n    q_info: Query-associated metadata for masking\n    kv_info: Key-associated metadata for masking\n    dropout: Dropout rate\n    rng: RNG for dropout\n\n  Returns:\n    A tuple (output, dots_logsumexp). The output has shape [q_len, d_v], and\n    dots_logsumexp has shape [q_len]. The logsumexp of the attention\n    probabilities is useful for combining multiple rounds of attention (as in\n    LSH attention).\n  \"\"\"\n  assert v is not None\n  share_qk = (k is None)\n  if share_qk:\n    k = q\n    if kv_info is None:\n      kv_info = q_info\n\n  if share_qk:\n    k = length_normalized(k)\n  k = k / np.sqrt(k.shape[-1])\n\n  # Dot-product attention.\n  kr = np.swapaxes(k, -1, -2)  # note the fancy transpose for later..\n\n## Step 1  ##\n  dots = np.matmul(q, kr )\n  if verbose: print(\"Our attend dots\", dots.shape)\n\n  # Masking\n  if mask_fn is not None:\n    dots = mask_fn(dots, q_info[..., :, None], kv_info[..., None, :])\n\n  # Softmax.\n  #dots_logsumexp = fastmath.logsumexp(dots, axis=-1, keepdims=True)  #original\n  #dots = np.exp(dots - dots_logsumexp)  #original\n  \n## Step 2  ##\n  #replace with our_softmax()\n  dots, dots_logsumexp = our_softmax(dots, passthrough=passthrough)\n  if verbose: print(\"Our attend dots post softmax\", dots.shape, dots_logsumexp.shape)\n\n  if dropout &gt; 0.0:\n    assert rng is not None\n    # Dropout is broadcast across the bin dimension\n    dropout_shape = (dots.shape[-2], dots.shape[-1])\n    keep_prob = tie_in(dots, 1.0 - dropout)\n    keep = fastmath.random.bernoulli(rng, keep_prob, dropout_shape)\n    multiplier = keep.astype(dots.dtype) / tie_in(keep, keep_prob)\n    dots = dots * multiplier\n\n## Step 3  ##\n# The softmax normalizer (dots_logsumexp) is used by multi-round LSH attn.\n  out = np.matmul(dots, v)\n  if verbose: print(\"Our attend out1\", out.shape)\n  out = np.reshape(out, (-1, out.shape[-1]))\n  if verbose: print(\"Our attend out2\", out.shape)\n  dots_logsumexp = np.reshape(dots_logsumexp, (-1,))\n  return out, dots_logsumexp\n\n\nseq_len = 8\nemb_len = 5\nd_qk = 3\nd_v = 4\nwith fastmath.use_backend(\"jax\"):  # specify the backend for consistency\n    rng_attend = fastmath.random.get_prng(1)\n    q = k = jax.random.uniform(rng_attend, (seq_len, d_qk), dtype=np.float32)\n    v = jax.random.uniform(rng_attend, (seq_len, d_v), dtype=np.float32)\n    o, logits = our_simple_attend(\n        q,\n        k,\n        v,\n        mask_fn=None,\n        q_info=None,\n        kv_info=None,\n        dropout=0.0,\n        rng=rng_attend,\n        verbose=True,\n    )\nprint(o, \"\\n\", logits)\n\nOur attend dots (8, 8)\nOur attend dots post softmax (8, 8) (8, 1)\nOur attend out1 (8, 4)\nOur attend out2 (8, 4)\n[[0.5455444  0.4232705  0.62970716 0.45504814]\n [0.5558777  0.4169514  0.6260488  0.45763403]\n [0.5502556  0.42250413 0.6107501  0.4532582 ]\n [0.53680766 0.43004778 0.63048995 0.4492887 ]\n [0.5546176  0.41898918 0.62778664 0.44567773]\n [0.54741716 0.4229177  0.6060424  0.46433902]\n [0.53192824 0.43415833 0.63327026 0.44313937]\n [0.538871   0.42285213 0.6527077  0.44843906]] \n [2.5345023 2.6896586 2.8266857 2.4992957 2.861424  2.6235857 2.5204637\n 2.3627536]\n\n\n\n\n Expected Output \n\n\nExpected Output\nOur attend dots (8, 8)\nOur attend dots post softmax (8, 8) (8, 1)\nOur attend out1 (8, 4)\nOur attend out2 (8, 4)\n[[0.5606324  0.7290605  0.5251243  0.47101074]\n [0.5713517  0.71991956 0.5033342  0.46975708]\n [0.5622886  0.7288458  0.52172124 0.46318397]\n [0.5568317  0.72234154 0.542236   0.4699722 ]\n [0.56504494 0.72274375 0.5204978  0.47231334]\n [0.56175965 0.7216782  0.53293145 0.48003793]\n [0.56753993 0.72232544 0.5141734  0.46625748]\n [0.57100445 0.70785505 0.5325362  0.4590797 ]]\n [2.6512175 2.1914332 2.6630518 2.7792363 2.4583826 2.5421977 2.4145055\n 2.5111294]\n\n\n completed code for reference \n\nThis notebook is ungraded, so for reference, the completed code follows:\n\ndef our_simple_attend(\n    q, k=None, v=None,\n    mask_fn=None, q_info=None, kv_info=None,\n    dropout=0.0, rng=None, verbose=False, passthrough=False\n    ):\n  \"\"\"Dot-product attention,  with masking, without optional chunking and/or.\n\n  Args:\n    q: Query vectors, shape [q_len, d_qk]\n    k: Key vectors, shape [kv_len, d_qk]; or None\n    v: Value vectors, shape [kv_len, d_v]\n    mask_fn: a function reference that implements masking (e.g. mask_self_attention)\n    q_info: Query-associated metadata for masking\n    kv_info: Key-associated metadata for masking\n    dropout: Dropout rate\n    rng: RNG for dropout\n\n  Returns:\n    A tuple (output, dots_logsumexp). The output has shape [q_len, d_v], and\n    dots_logsumexp has shape [q_len]. The logsumexp of the attention\n    probabilities is useful for combining multiple rounds of attention (as in\n    LSH attention).\n  \"\"\"\n  assert v is not None\n  share_qk = (k is None)\n  if share_qk:\n    k = q\n    if kv_info is None:\n      kv_info = q_info\n\n  if share_qk:\n    k = length_normalized(k)\n  k = k / np.sqrt(k.shape[-1])\n\n  # Dot-product attention.\n  kr = np.swapaxes(k, -1, -2)  #note the fancy transpose for later..\n\n## Step 1  ##\n  dots = np.matmul(q, kr )\n  if verbose: print(\"Our attend dots\", dots.shape)\n\n  # Masking\n  if mask_fn is not None:\n    dots = mask_fn(dots, q_info[..., :, None], kv_info[..., None, :])\n\n  # Softmax.\n  #dots_logsumexp = fastmath.logsumexp(dots, axis=-1, keepdims=True)  #original\n  #dots = np.exp(dots - dots_logsumexp)  #original\n## Step 2  ##\n  #replace with our_softmax()\n  dots, dots_logsumexp = our_softmax(dots, passthrough=passthrough)\n  if verbose: print(\"Our attend dots post softmax\", dots.shape, dots_logsumexp.shape)\n\n  if dropout &gt; 0.0:\n    assert rng is not None\n    # Dropout is broadcast across the bin dimension\n    dropout_shape = (dots.shape[-2], dots.shape[-1])\n    keep_prob = tie_in(dots, 1.0 - dropout)\n    keep = fastmath.random.bernoulli(rng, keep_prob, dropout_shape)\n    multiplier = keep.astype(dots.dtype) / tie_in(keep, keep_prob)\n    dots = dots * multiplier\n\n## Step 3  ##\n# The softmax normalizer (dots_logsumexp) is used by multi-round LSH attn.\n  out = np.matmul(dots, v)\n  if verbose: print(\"Our attend out1\", out.shape)\n  out = np.reshape(out, (-1, out.shape[-1]))\n  if verbose: print(\"Our attend out2\", out.shape)\n  dots_logsumexp = np.reshape(dots_logsumexp, (-1,))\n  return out, dots_logsumexp",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "L1 - Reformer LSH"
    ]
  },
  {
    "objectID": "notes/c4w4/lab01.html#2.3",
    "href": "notes/c4w4/lab01.html#2.3",
    "title": "Reformer Efficient Attention: Ungraded Lab",
    "section": "Part 2.3 Class OurSelfAttention",
    "text": "Part 2.3 Class OurSelfAttention\nHere we create our own self attention layer by creating a class OurSelfAttention. The parent class will be the tl.SelfAttention layer in Trax. We will only override the forward_unbatched routine.\nWe’re not asking you to modify anything in this routine. There are some comments to draw your attention to a few lines.\n\nclass OurSelfAttention(tl.SelfAttention):\n    \"\"\"Our self-attention. Just the Forward Function.\"\"\"\n\n    def forward_unbatched(\n        self, x, mask=None, *, weights, state, rng, update_state, verbose=False\n    ):\n        print(\"ourSelfAttention:forward_unbatched\")\n        del update_state\n        attend_rng, output_rng = fastmath.random.split(rng)\n        if self.bias:\n            if self.share_qk:\n                w_q, w_v, w_o, b_q, b_v = weights\n            else:\n                w_q, w_k, w_v, w_o, b_q, b_k, b_v = weights\n        else:\n            if self.share_qk:\n                w_q, w_v, w_o = weights\n            else:\n                w_q, w_k, w_v, w_o = weights\n\n        print(\"x.shape,w_q.shape\", x.shape, w_q.shape)\n        q = np.matmul(x, w_q)\n        k = None\n        if not self.share_qk:\n            k = np.matmul(x, w_k)\n        v = np.matmul(x, w_v)\n\n        if self.bias:\n            q = q + b_q\n            if not self.share_qk:\n                k = k + b_k\n            v = v + b_v\n\n        mask_fn = functools.partial(\n            mask_self_attention,\n            causal=self.causal,\n            exclude_self=self.share_qk,\n            masked=self.masked,\n        )\n        q_info = kv_info = tie_in(x, np.arange(q.shape[-2], dtype=np.int32))\n\n        assert (mask is not None) == self.masked\n        if self.masked:\n            # mask is a boolean array (True means \"is valid token\")\n            ones_like_mask = tie_in(x, np.ones_like(mask, dtype=np.int32))\n            kv_info = kv_info * np.where(mask, ones_like_mask, -ones_like_mask)\n\n        # Notice, we are callout our vesion of attend\n        o, _ = our_simple_attend(\n            q,\n            k,\n            v,\n            mask_fn=mask_fn,\n            q_info=q_info,\n            kv_info=kv_info,\n            dropout=self.attention_dropout,\n            rng=attend_rng,\n            verbose=True,\n        )\n\n        # Notice, wo weight matrix applied to output of attend in forward_unbatched\n        out = np.matmul(o, w_o)\n        out = apply_broadcasted_dropout(out, self.output_dropout, output_rng)\n        return out, state\n\n\ncausal = False\nmasked = False\nmask = None\nattention_dropout = 0.0\nn_heads = 3\nd_qk = 3\nd_v = 4\nseq_len = 8\nemb_len = 5\nbatch_size = 1\n\nosa = OurSelfAttention(\n    n_heads=n_heads,\n    d_qk=d_qk,\n    d_v=d_v,\n    causal=causal,\n    use_reference_code=True,\n    attention_dropout=attention_dropout,\n    mode=\"train\",\n)\n\nrng_osa = fastmath.random.get_prng(1)\nx = jax.random.uniform(\n    jax.random.PRNGKey(0), (batch_size, seq_len, emb_len), dtype=np.float32\n)\n_, _ = osa.init(tl.shapes.signature(x), rng=rng_osa)\n\n\nosa(x)\n\nourSelfAttention:forward_unbatched\n\n\n\n---------------------------------------------------------------------------\nLayerError                                Traceback (most recent call last)\nCell In[9], line 1\n----&gt; 1 osa(x)\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/base.py:197, in Layer.__call__(self, x, weights, state, rng)\n    195   self.state = state  # Needed if the model wasn't fully initialized.\n    196 state = self.state\n--&gt; 197 outputs, new_state = self.pure_fn(x, weights, state, rng)\n    198 self.state = new_state\n    199 return outputs\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/base.py:605, in Layer.pure_fn(self, x, weights, state, rng, use_cache)\n    602 except Exception:\n    603   # Skipping 3 lines as it's always the uninteresting internal call.\n    604   name, trace = self._name, _short_traceback(skip=3)\n--&gt; 605   raise LayerError(name, 'pure_fn',\n    606                    self._caller, signature(x), trace) from None\n\nLayerError: Exception passing through layer OurSelfAttention (in pure_fn):\n  layer created in file [...]/layers/research/efficient_attention.py, line 1014\n  layer input shapes: ShapeDtype{shape:(1, 8, 5), dtype:float32}\n\n  File [...]/layers/research/efficient_attention.py, line 1323, in forward\n    single_out, single_new_state = self.forward_unbatched(\n\n  File [...]/tmp/ipykernel_121487/1155828790.py, line 10, in forward_unbatched\n    if self.bias:\n\nAttributeError: 'OurSelfAttention' object has no attribute 'bias'. Did you mean: '_bias'?\n\n\n\n\n\n Expected Output \n\nExpected Output Notice a few things:\n\nthe w_q (and w_k) matrices are applied to each row or each embedding on the input. This is similar to the filter operation in convolution\nforward_unbatched is called 3 times. This is because we have 3 heads in this example.\n\nourSelfAttention:forward_unbatched\nx.shape,w_q.shape (8, 5) (5, 3)\nOur attend dots (8, 8)\nOur attend dots post softmax (8, 8) (8, 1)\nOur attend out1 (8, 4)\nOur attend out2 (8, 4)\nourSelfAttention:forward_unbatched\nx.shape,w_q.shape (8, 5) (5, 3)\nOur attend dots (8, 8)\nOur attend dots post softmax (8, 8) (8, 1)\nOur attend out1 (8, 4)\nOur attend out2 (8, 4)\nourSelfAttention:forward_unbatched\nx.shape,w_q.shape (8, 5) (5, 3)\nOur attend dots (8, 8)\nOur attend dots post softmax (8, 8) (8, 1)\nOur attend out1 (8, 4)\nOur attend out2 (8, 4)\nDeviceArray([[[ 6.70414209e-01, -1.04319841e-01, -5.33822298e-01,\n                1.92711830e-01, -4.54187393e-05],\n              [ 6.64090097e-01, -1.01875424e-01, -5.35733163e-01,\n                1.88311756e-01, -6.30629063e-03],\n              [ 6.73380017e-01, -1.06952369e-01, -5.31989932e-01,\n                1.90056816e-01,  1.30271912e-03],\n              [ 6.84564888e-01, -1.13240272e-01, -5.50182462e-01,\n                1.95673436e-01,  5.47635555e-03],\n              [ 6.81435883e-01, -1.11068964e-01, -5.32343209e-01,\n                1.91912338e-01,  5.69400191e-03],\n              [ 6.80724978e-01, -1.08496904e-01, -5.34994125e-01,\n                1.96332246e-01,  5.89773059e-03],\n              [ 6.80933356e-01, -1.14087075e-01, -5.18659890e-01,\n                1.90674081e-01,  1.14096403e-02],\n              [ 6.80265009e-01, -1.09031796e-01, -5.38248718e-01,\n                1.94203183e-01,  4.23943996e-03]]], dtype=float32)\n ## Part 3.0 Trax LSHSelfAttention  ## Part 3.1 Description The larger the matrix multiply in the previous section is, the more context can be taken into account when making the next decision. However, the self attention dot product grows as the size of the input squared. For example, if one wished to have an input size of 1024, that would result in 1024^2 or over a million dot products for each head! As a result, there has been significant research related to reducing the compute requirements. One such approach is Locality Sensitive Hashing(LSH) Self Attention.\nYou may recall, earlier in the course you utilized LSH to find similar tweets without resorting to calculating cosine similarity for each pair of embeddings. We will use a similar approach here. It may be best described with an example. \n\nFigure 9: Example of LSH Self Attention\n\nLSH Self attention uses Queries only, no Keys. Attention then generates a metric of the similarity of each value of Q relative to all the other values in Q. An earlier assignment demonstrated that values which hash to the same bucket are likely to be similar. Further, multiple random hashes can improve the chances of finding entries which are similar. This is the approach taken here, though the hash is implemented a bit differently. The values of Q are hashed into buckets using a randomly generated set of hash vectors. Multiple sets of hash vectors are used, generating multiple hash tables. In the figure above, we have 3 hash tables with 4 buckets in each table. Notionally, following the hash, the values of Q have been replicated 3 times and distributed to their appropriate bucket in each of the 3 tables. To find similarity then, one generates dot-products only between members of the buckets. The result of this operation provides information on which entries are similar. As the operation has been distributed over multiple hash tables, these results need to be combined to form a complete picture and this can be used to generate a reduced dot-product attention array. Its clear that because we do not do a compare of every value vs every other value, the size of Dots will be reduced.\nThe challenge in this approach is getting it to operate efficiently. You may recall from the earlier assignments the buckets were lists of entries and had varying length. This will operate poorly on a vector processing machine such as a GPU or TPU. Ideally, operations are done in large blocks with uniform sizes. While it is straightforward to implement the hash algorithm this way, it is challenging to managed buckets and variable sized dot-products. This will be discussed further below. For now, we will examine and implement the hash function.\n ## Part 3.2 our_hash_vectors\nour_hash_vectors, is a reimplementation of Trax hashvector. It takes in an array of vectors, hashes the entries and returns and array assigning each input vector to n_hash buckets. Hashing is described as creating random rotations, see Practical and Optimal LSH for Angular Distance.\n \n\nFigure 10: Processing steps in our_hash_vectors \n\nNote, in the diagram, sizes relate to our expected input Q while our_hash_vectors is written assuming a generic input vector\nInstructions Step 1 create an array of random normal vectors which will be our hash vectors. Each vector will be hashed into a hash table and into rot_size//2 buckets. We use rot_size//2 to reduce computation. Later in the routine we will form the negative rotations with a simple negation and concatenate to get a full rot_size number of rotations. * use fastmath.random.normal and create an array of random vectors of shape (vec.shape[-1],n_hashes, rot_size//2)\nStep 2 In this step we simply do the matrix multiply. jax has an accelerated version of einsum. Here we will utilize more conventional routines.\nStep 2x * 2a: np.reshape random_rotations into a 2 dimensional array ([-1, n_hashes * (rot_size // 2)]) * 2b: np.dot vecs and random_rotations forming our rotated_vecs * 2c: back to 3 dimension with np.reshape [-1, n_hashes, rot_size//2] * 2d: prepare for concatenating by swapping dimensions np.transpose (1, 0, 2) Step 3 Here we concatenate our rotation vectors getting a fullrot_size number of buckets (note, n_buckets = rotsize) * use np.concatenate, [rotated_vecs, -rotated_vecs], axis=-1 Step 4 This is the exciting step! You have no doubt been wondering how we will turn these vectors into bucket indexes. By performing np.argmax over the rotations for a given entry, you get the index to the best match! We will use this as a bucket index. * np.argmax(…).astype(np.int32); be sure to use the correct axis! Step 5 In this style of hashing, items which land in bucket 0 of hash table 0 are not necessarily similar to those landing in bucket 0 of hash table 1, so we keep them separate. We do this by offsetting the bucket numbers by ‘n_buckets’. * add buckets and offsets and reshape into a one dimensional array This will return a 1D array of size n_hashes * vec.shape[0].\n\ndef our_hash_vectors(vecs, rng, n_buckets, n_hashes, mask=None, verbose=False):\n    \"\"\"\n  Args:\n    vecs: tensor of at least 2 dimension,\n    rng: random number generator\n    n_buckets: number of buckets in each hash table\n    n_hashes: the number of hash tables\n    mask: None indicating no mask or a 1D boolean array of length vecs.shape[0], containing the location of padding value\n    verbose: controls prints for debug\n  Returns:\n    A vector of size n_hashes * vecs.shape[0] containing the buckets associated with each input vector per hash table.\n\n    \"\"\"\n\n    # check for even, integer bucket sizes\n    assert isinstance(n_buckets, int) and n_buckets % 2 == 0\n\n    rng = fastmath.stop_gradient(tie_in(vecs, rng))\n    rot_size = n_buckets\n    ### Start Code Here\n\n    ### Step 1 ###\n    rotations_shape = (vecs.shape[-1], n_hashes, rot_size // 2)\n    random_rotations = fastmath.random.normal(rng, rotations_shape).astype(\n        np.float32)\n    if verbose: print(\"random.rotations.shape\", random_rotations.shape)\n\n    ### Step 2 ###\n    if fastmath.backend_name() == 'jax':\n      rotated_vecs = np.einsum('tf,fhb-&gt;htb', vecs, random_rotations)\n      if verbose: print(\"using jax\")\n    else:\n      #Step 2a\n      random_rotations = np.reshape(random_rotations,\n                                    [-1, n_hashes * (rot_size // 2)])\n      if verbose: print(\"random_rotations reshaped\", random_rotations.shape)\n      #Step 2b\n      rotated_vecs = np.dot(vecs, random_rotations)\n      if verbose: print(\"rotated_vecs1\", rotated_vecs.shape)\n      #Step 2c\n      rotated_vecs = np.reshape(rotated_vecs, [-1, n_hashes, rot_size//2])\n      if verbose: print(\"rotated_vecs2\", rotated_vecs.shape)\n      #Step 2d\n      rotated_vecs = np.transpose(rotated_vecs, (1, 0, 2))\n      if verbose: print(\"rotated_vecs3\", rotated_vecs.shape)\n\n    ### Step 3 ###\n    rotated_vecs = np.concatenate([rotated_vecs, -rotated_vecs], axis=-1)\n    if verbose: print(\"rotated_vecs.shape\", rotated_vecs.shape)\n    ### Step 4 ###\n    buckets = np.argmax(rotated_vecs, axis=-1).astype(np.int32)\n    if verbose: print(\"buckets.shape\", buckets.shape)\n    if verbose: print(\"buckets\", buckets)\n\n    if mask is not None:\n      n_buckets += 1  # Create an extra bucket for padding tokens only\n      buckets = np.where(mask[None, :], buckets, n_buckets - 1)\n\n    # buckets is now (n_hashes, seqlen). Next we add offsets so that\n    # bucket numbers from different hashing rounds don't overlap.\n    offsets = tie_in(buckets, np.arange(n_hashes, dtype=np.int32))\n    offsets = np.reshape(offsets * n_buckets, (-1, 1))\n    ### Step 5 ###\n    buckets = np.reshape(buckets + offsets, (-1,))\n    if verbose: print(\"buckets with offsets\", buckets.shape, \"\\n\", buckets)\n    return buckets\n\n\n# example code. Note for reference, the sizes in this example match the values in the diagram above.\nohv_q = np.ones((8, 5))  # (seq_len=8, n_q=5)\nohv_n_buckets = 4  # even number\nohv_n_hashes = 3\nwith fastmath.use_backend(\"tf\"):\n    ohv_rng = fastmath.random.get_prng(1)\n    ohv = our_hash_vectors(\n        ohv_q, ohv_rng, ohv_n_buckets, ohv_n_hashes, mask=None, verbose=True\n    )\n    print(\"ohv shape\", ohv.shape, \"\\nohv\", ohv)  # (ohv_n_hashes * ohv_n_buckets)\n# note the random number generators do not produce the same results with different backends\nwith fastmath.use_backend(\"jax\"):\n    ohv_rng = fastmath.random.get_prng(1)\n    ohv = our_hash_vectors(ohv_q, ohv_rng, ohv_n_buckets, ohv_n_hashes, mask=None)\n    print(\"ohv shape\", ohv.shape, \"\\nohv\", ohv)  # (ohv_n_hashes * ohv_n_buckets)\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[11], line 5\n      3 ohv_n_buckets = 4  # even number\n      4 ohv_n_hashes = 3\n----&gt; 5 with fastmath.use_backend(\"tf\"):\n      6     ohv_rng = fastmath.random.get_prng(1)\n      7     ohv = our_hash_vectors(\n      8         ohv_q, ohv_rng, ohv_n_buckets, ohv_n_hashes, mask=None, verbose=True\n      9     )\n\nFile /usr/lib/python3.10/contextlib.py:135, in _GeneratorContextManager.__enter__(self)\n    133 del self.args, self.kwds, self.func\n    134 try:\n--&gt; 135     return next(self.gen)\n    136 except StopIteration:\n    137     raise RuntimeError(\"generator didn't yield\") from None\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/fastmath/ops.py:428, in use_backend(name)\n    425 if isinstance(name, Backend):\n    426   name = name.value\n--&gt; 428 _assert_valid_backend_name(name)\n    429 global override_backend\n    430 prev_name_or_backend = override_backend\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/fastmath/ops.py:387, in _assert_valid_backend_name(name)\n    385   if backend_.value == name:\n    386     return\n--&gt; 387 raise ValueError(f'No backend with name {name}')\n\nValueError: No backend with name tf\n\n\n\n\n\n Expected Output \n\nExpected Values\nrandom.rotations.shape (5, 3, 2)\nrandom_rotations reshaped (5, 6)\nrotated_vecs1 (8, 6)\nrotated_vecs2 (8, 3, 2)\nrotated_vecs3 (3, 8, 2)\nrotated_vecs.shape (3, 8, 4)\nbuckets.shape (3, 8)\nbuckets ndarray&lt;tf.Tensor(\n[[3 3 3 3 3 3 3 3]\n [3 3 3 3 3 3 3 3]\n [3 3 3 3 3 3 3 3]], shape=(3, 8), dtype=int32)&gt;\nbuckets with offsets (24,)\n ndarray&lt;tf.Tensor([ 3  3  3  3  3  3  3  3  7  7  7  7  7  7  7  7 11 11 11 11 11 11 11 11], shape=(24,), dtype=int32)&gt;\nohv shape (24,)\nohv ndarray&lt;tf.Tensor([ 3  3  3  3  3  3  3  3  7  7  7  7  7  7  7  7 11 11 11 11 11 11 11 11], shape=(24,), dtype=int32)&gt;\nusing jax\nohv shape (24,)\nohv [ 3  3  3  3  3  3  3  3  5  5  5  5  5  5  5  5 11 11 11 11 11 11 11 11]```\n\n&lt;details&gt;\n&lt;summary&gt;\n    &lt;font size=\"3\" &gt;&lt;b&gt;Completed code for reference &lt;/b&gt;&lt;/font&gt;\n&lt;/summary&gt;",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "L1 - Reformer LSH"
    ]
  },
  {
    "objectID": "notes/c4w4/lab01.html#3.5",
    "href": "notes/c4w4/lab01.html#3.5",
    "title": "Reformer Efficient Attention: Ungraded Lab",
    "section": "Part 3.5 OurLSHSelfAttention",
    "text": "Part 3.5 OurLSHSelfAttention\nYou can examine the full implementations below. Area’s we did not ‘attend to’ in our implementations above include variable bucket sizes and masking. We will instantiate a layer of the full implementation below. We tried to use the same variable names above to make it easier to decipher the full version. Note that some of the functionality we implemented in our routines is split between attend and forward_unbatched. We’ve inserted our version of hash below, but use the original version of attend.\n\n# original version from trax 1.3.4\ndef attend(\n    q,\n    k=None,\n    v=None,\n    q_chunk_len=None,\n    kv_chunk_len=None,\n    n_chunks_before=0,\n    n_chunks_after=0,\n    mask_fn=None,\n    q_info=None,\n    kv_info=None,\n    dropout=0.0,\n    rng=None,\n):\n    \"\"\"Dot-product attention, with optional chunking and/or masking.\n\n  Args:\n    q: Query vectors, shape [q_len, d_qk]\n    k: Key vectors, shape [kv_len, d_qk]; or None\n    v: Value vectors, shape [kv_len, d_v]\n    q_chunk_len: Set to non-zero to enable chunking for query vectors\n    kv_chunk_len: Set to non-zero to enable chunking for key/value vectors\n    n_chunks_before: Number of adjacent previous chunks to attend to\n    n_chunks_after: Number of adjacent subsequent chunks to attend to\n    mask_fn: TODO(kitaev) doc\n    q_info: Query-associated metadata for masking\n    kv_info: Key-associated metadata for masking\n    dropout: Dropout rate\n    rng: RNG for dropout\n\n  Returns:\n    A tuple (output, dots_logsumexp). The output has shape [q_len, d_v], and\n    dots_logsumexp has shape [q_len]. The logsumexp of the attention\n    probabilities is useful for combining multiple rounds of attention (as in\n    LSH attention).\n  \"\"\"\n    assert v is not None\n    share_qk = k is None\n\n    if q_info is None:\n        q_info = np.arange(q.shape[-2], dtype=np.int32)\n\n    if kv_info is None and not share_qk:\n        kv_info = np.arange(v.shape[-2], dtype=np.int32)\n\n    # Split q/k/v into chunks along the time axis, if desired.\n    if q_chunk_len is not None:\n        q = np.reshape(q, (-1, q_chunk_len, q.shape[-1]))\n        q_info = np.reshape(q_info, (-1, q_chunk_len))\n\n    if share_qk:\n        assert kv_chunk_len is None or kv_chunk_len == q_chunk_len\n        k = q\n        kv_chunk_len = q_chunk_len\n        if kv_info is None:\n            kv_info = q_info\n        elif kv_chunk_len is not None:\n            # kv_info is not None, but reshape as required.\n            kv_info = np.reshape(kv_info, (-1, kv_chunk_len))\n    elif kv_chunk_len is not None:\n        k = np.reshape(k, (-1, kv_chunk_len, k.shape[-1]))\n        kv_info = np.reshape(kv_info, (-1, kv_chunk_len))\n\n    if kv_chunk_len is not None:\n        v = np.reshape(v, (-1, kv_chunk_len, v.shape[-1]))\n\n    if share_qk:\n        k = length_normalized(k)\n    k = k / np.sqrt(k.shape[-1])\n\n    # Optionally include adjacent chunks.\n    if q_chunk_len is not None or kv_chunk_len is not None:\n        assert q_chunk_len is not None and kv_chunk_len is not None\n    else:\n        assert n_chunks_before == 0 and n_chunks_after == 0\n\n    k = look_adjacent(k, n_chunks_before, n_chunks_after)\n    v = look_adjacent(v, n_chunks_before, n_chunks_after)\n    kv_info = look_adjacent(kv_info, n_chunks_before, n_chunks_after)\n\n    # Dot-product attention.\n    dots = np.matmul(q, np.swapaxes(k, -1, -2))\n\n    # Masking\n    if mask_fn is not None:\n        dots = mask_fn(dots, q_info[..., :, None], kv_info[..., None, :])\n\n    # Softmax.\n    dots_logsumexp = fastmath.logsumexp(dots, axis=-1, keepdims=True)\n    dots = np.exp(dots - dots_logsumexp)\n\n    if dropout &gt; 0.0:\n        assert rng is not None\n        # Dropout is broadcast across the bin dimension\n        dropout_shape = (dots.shape[-2], dots.shape[-1])\n        #\n        keep_prob = tie_in(dots, 1.0 - dropout)\n        keep = fastmath.random.bernoulli(rng, keep_prob, dropout_shape)\n        multiplier = keep.astype(dots.dtype) / tie_in(keep, keep_prob)\n        dots = dots * multiplier\n\n    # The softmax normalizer (dots_logsumexp) is used by multi-round LSH attn.\n    out = np.matmul(dots, v)\n    out = np.reshape(out, (-1, out.shape[-1]))\n    dots_logsumexp = np.reshape(dots_logsumexp, (-1,))\n    return out, dots_logsumexp\n\n\nclass OurLSHSelfAttention(tl.LSHSelfAttention):\n    \"\"\"Our simplified LSH self-attention \"\"\"\n\n    def forward_unbatched(self, x, mask=None, *, weights, state, rng, update_state):\n        attend_rng, output_rng = fastmath.random.split(rng)\n        w_q, w_v, w_o = weights\n\n        q = np.matmul(x, w_q)\n        v = np.matmul(x, w_v)\n\n        if update_state:\n            _, old_hash_rng = state\n            hash_rng, hash_subrng = fastmath.random.split(old_hash_rng)\n            #      buckets = self.hash_vectors(q, hash_subrng, mask)  #  original\n            ## use our version of hash\n            buckets = our_hash_vectors(\n                q, hash_subrng, self.n_buckets, self.n_hashes, mask=mask\n            )\n            s_buckets = buckets\n            if self._max_length_for_buckets:\n                length = self.n_hashes * self._max_length_for_buckets\n                if buckets.shape[0] &lt; length:\n                    s_buckets = np.concatenate(\n                        [buckets, np.zeros(length - buckets.shape[0], dtype=np.int32)],\n                        axis=0,\n                    )\n            state = (s_buckets, hash_rng)\n        else:\n            buckets, _ = state\n            if self._max_length_for_buckets:\n                buckets = buckets[: self.n_hashes * x.shape[0]]\n\n        seqlen = x.shape[0]\n        assert int(buckets.shape[0]) == self.n_hashes * seqlen\n\n        ticker = tie_in(x, np.arange(self.n_hashes * seqlen, dtype=np.int32))\n        buckets_and_t = seqlen * buckets + (ticker % seqlen)\n        buckets_and_t = fastmath.stop_gradient(buckets_and_t)\n\n        # Hash-based sort (\"s\" at the start of variable names means \"sorted\")\n        sbuckets_and_t, sticker = fastmath.sort_key_val(\n            buckets_and_t, ticker, dimension=-1\n        )\n        _, undo_sort = fastmath.sort_key_val(sticker, ticker, dimension=-1)\n        sbuckets_and_t = fastmath.stop_gradient(sbuckets_and_t)\n        sticker = fastmath.stop_gradient(sticker)\n        undo_sort = fastmath.stop_gradient(undo_sort)\n\n        st = sticker % seqlen\n        sq = np.take(q, st, axis=0)\n        sv = np.take(v, st, axis=0)\n\n        mask_fn = functools.partial(\n            mask_self_attention,\n            causal=self.causal,\n            exclude_self=True,\n            masked=self.masked,\n        )\n        q_info = st\n\n        assert (mask is not None) == self.masked\n        kv_info = None\n        if self.masked:\n            # mask is a boolean array (True means \"is valid token\")\n            smask = np.take(mask, st, axis=0)\n            ones_like_mask = tie_in(x, np.ones_like(smask, dtype=np.int32))\n            kv_info = q_info * np.where(smask, ones_like_mask, -ones_like_mask)\n\n        ## use original version of attend (could use ours but lacks masks and masking)\n        so, slogits = attend(\n            sq,\n            k=None,\n            v=sv,\n            q_chunk_len=self.chunk_len,\n            n_chunks_before=self.n_chunks_before,\n            n_chunks_after=self.n_chunks_after,\n            mask_fn=mask_fn,\n            q_info=q_info,\n            kv_info=kv_info,\n            dropout=self.attention_dropout,\n            rng=attend_rng,\n        )\n\n        # np.take(so, undo_sort, axis=0); np.take(slogits, undo_sort, axis=0) would\n        # also work, but these helpers include performance optimizations for TPU.\n        o = permute_via_gather(so, undo_sort, sticker, axis=0)\n        logits = permute_via_sort(slogits, sticker, buckets_and_t, axis=-1)\n\n        if self.n_hashes &gt; 1:\n            o = np.reshape(o, (self.n_hashes, seqlen, o.shape[-1]))\n            logits = np.reshape(logits, (self.n_hashes, seqlen, 1))\n            probs = np.exp(logits - fastmath.logsumexp(logits, axis=0, keepdims=True))\n            o = np.sum(o * probs, axis=0)\n\n        assert o.shape == (seqlen, w_v.shape[-1])\n        out = np.matmul(o, w_o)\n        out = apply_broadcasted_dropout(out, self.output_dropout, output_rng)\n        return out, state\n\n\n# Here we're going to try out our LSHSelfAttention\nn_heads = 3\ncausal = False\nmasked = False\nmask = None\nchunk_len = 8\nn_chunks_before = 0\nn_chunks_after = 0\nattention_dropout = 0.0\nn_hashes = 5\nn_buckets = 4\nseq_len = 8\nemb_len = 5\nal = OurLSHSelfAttention(\n    n_heads=n_heads,\n    d_qk=3,\n    d_v=4,\n    causal=causal,\n    chunk_len=8,\n    n_chunks_before=n_chunks_before,\n    n_chunks_after=n_chunks_after,\n    n_hashes=n_hashes,\n    n_buckets=n_buckets,\n    use_reference_code=True,\n    attention_dropout=attention_dropout,\n    mode=\"train\",\n)\n\nx = jax.random.uniform(jax.random.PRNGKey(0), (1, seq_len, emb_len), dtype=np.float32)\nal_osa = fastmath.random.get_prng(1)\n_, _ = al.init(tl.shapes.signature(x), rng=al_osa)\n\n\nal(x)\n\n\n---------------------------------------------------------------------------\nLayerError                                Traceback (most recent call last)\nCell In[20], line 1\n----&gt; 1 al(x)\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/base.py:197, in Layer.__call__(self, x, weights, state, rng)\n    195   self.state = state  # Needed if the model wasn't fully initialized.\n    196 state = self.state\n--&gt; 197 outputs, new_state = self.pure_fn(x, weights, state, rng)\n    198 self.state = new_state\n    199 return outputs\n\nFile ~/work/notes/notes-nlp/.venv/lib/python3.10/site-packages/trax/layers/base.py:605, in Layer.pure_fn(self, x, weights, state, rng, use_cache)\n    602 except Exception:\n    603   # Skipping 3 lines as it's always the uninteresting internal call.\n    604   name, trace = self._name, _short_traceback(skip=3)\n--&gt; 605   raise LayerError(name, 'pure_fn',\n    606                    self._caller, signature(x), trace) from None\n\nLayerError: Exception passing through layer OurLSHSelfAttention (in pure_fn):\n  layer created in file [...]/layers/research/efficient_attention.py, line 1751\n  layer input shapes: ShapeDtype{shape:(1, 8, 5), dtype:float32}\n\n  File [...]/layers/research/efficient_attention.py, line 2158, in forward\n    single_out, single_new_state = self.forward_unbatched(\n\n  File [...]/tmp/ipykernel_121487/2615489615.py, line 17, in forward_unbatched\n    q, hash_subrng, self.n_buckets, self.n_hashes, mask=mask\n\nAttributeError: 'OurLSHSelfAttention' object has no attribute 'n_buckets'. Did you mean: '_n_buckets'?\n\n\n\n\n\n Expected Output \n\nExpected Values\nusing jax\nusing jax\nusing jax\nDeviceArray([[[ 6.6842824e-01, -1.1364323e-01, -5.4430610e-01,\n                2.1126242e-01, -1.0988623e-02],\n              [ 7.0949769e-01, -1.5455185e-01, -5.9923315e-01,\n                2.2719440e-01,  1.3833776e-02],\n              [ 7.1442688e-01, -1.2046628e-01, -5.3956544e-01,\n                1.7320301e-01, -1.6552269e-02],\n              [ 6.7178929e-01, -7.6611102e-02, -5.9399861e-01,\n                2.1236290e-01,  7.9482794e-04],\n              [ 7.1518433e-01, -1.1359170e-01, -5.7821894e-01,\n                2.1304411e-01,  3.0598268e-02],\n              [ 6.8235350e-01, -9.3979925e-02, -5.5341840e-01,\n                2.1608177e-01, -6.6673756e-04],\n              [ 6.1286640e-01, -8.1027031e-02, -4.8148823e-01,\n                1.9373313e-01,  3.1555295e-02],\n              [ 7.2203505e-01, -1.0199660e-01, -5.5215168e-01,\n                1.7872262e-01, -2.2289157e-02]]], dtype=float32)\n\nCongratuations! you have created a custom layer and have become familiar with LSHSelfAttention.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "L1 - Reformer LSH"
    ]
  },
  {
    "objectID": "notes/c4w4/index.html",
    "href": "notes/c4w4/index.html",
    "title": "Chat Bots",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nSupplementary Figure 1\nMy notes for Week 4 of the Natural Language Processing with Attention Labels Course in the Natural Language Processing Specialization Offered by DeepLearning.AI on Coursera\nDeep learning and A.I. researchers push the field forward by looking for new techniques as well as refinements of old ideas to get better performance on tasks. In this lesson we cover reversible layers which allow us to leverage a time memory tradeoff to process book length sequences and handle contexts over a conversation.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w4/index.html#sec-task-long",
    "href": "notes/c4w4/index.html#sec-task-long",
    "title": "Chat Bots",
    "section": "Tasks with Long Sequences",
    "text": "Tasks with Long Sequences\n\n\n\n\n\n\n\nFigure 1: Context Window\n\n\nThis week we are going to learn about tasks that require processing longer sequences:\n\nWriting books\nStorytelling and understanding\nBuilding intelligent agents for conversations like chat-bots.\n\nMore specifically we will understand how re-former model (AKA the reversible transformer) and reversible layers work.\nThis week we will learn about the bottlenecks in these larger transformer models, and solutions we can use to make them trainable for you. We will also learn about the. Here is what we will be building for your programming assignment: A chatbot!\nIn many ways a Chat bot is very similar to a Q&A system which we built last week and that is also similar to query based summarization another task we covered a week before that. The new challenge is to manage what parts of the new and old context we keep around as the dialogue progresses. Chatbot are smart A.I. agents and much of the techniques developed under the umbrella of knowledge-based AI is also relevant in developing these. For instance carrying out actions on behalf of the user.\nChatbots can also get a very simple ui via the web or as an mobile app, which is another area I have some experience. However an even more powerful paradigm here is the ability to interact using voice which has many additional benefit for example supporting people with disabilities and operating in hands-free mode.\nHere is a link to an AI Storytelling system.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w4/index.html#sec-transformer-complexity",
    "href": "notes/c4w4/index.html#sec-transformer-complexity",
    "title": "Chat Bots",
    "section": "Transformer Complexity",
    "text": "Transformer Complexity\n\n\n\n\n\n\n\nFigure 2: week-4\n\n\nOne of the biggest issues with the transformers is that it takes time and a lot of memory when training. Concretely here are the numbers. If we have a sequence of length L , then we need L^2*N memory to handle the sequence. So if we have N layers, that means your model will take N times more time to complete. As L gets larger, the memory and the time quickly increases.\nPerhaps this is the reason people are looking into converting transformers into RNN after training.\n\n\n\n\n\n\n\nFigure 3: week-4\n\n\nWhen we are handling long sequences, we frequently don’t need to consider all L positions. We can just focus on an area of interest instead. For example, when translating a long text from one language to another, we don’t need to consider every word at once. We can instead focus on a single word being translated, and those immediately around it, by using attention.\nTo overcome the memory requirements we can recompute the activations. As long as we do it efficiently, we will be able to save a good amount of time and memory. We will learn this week how to do it. Instead of storing N layers, we will be able to recompute them when doing the back-propagation. That combined with local attention, will give we a much faster model that works at the same level as the transformer we learned about last week.\n\none area where we can make headway is working with a subsequence of interest.\nduring training we need to keep the activations in memory for the back propagation task. Clearly for inference we may be able to save on memory.\nthe alternative is to discard the activations as we go along and recalculate later. This can allows trading memory for compute time. However with larger models compute time is also a bottleneck.\n\n\n\n\n\n\n\n\nFigure 4: Approximate Nearest Neighbours",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w4/index.html#sec-lsh-attention",
    "href": "notes/c4w4/index.html#sec-lsh-attention",
    "title": "Chat Bots",
    "section": "LSH Attention",
    "text": "LSH Attention\nIn Course 1, we covered how locality sensitive hashing (LSH) works. We learned about:\n\nKNN\nHash Tables and Hash Functions\nLocality Sensitive Hashing\nMultiple Planes\n\nHere are the steps to follow to compute LSH given some vectors, where the vectors may correspond to the transformed word embedding that your transformer outputs.\nAttention is used to try which query (q) and key (k) are the most similar. To do so, we hash q and the keys. This will put similar vectors in the same bucket that we can use. The drawing above shows the lines that separate the buckets. Those could be seen as the planes.\nFirst let’s recall how the standard attention mechanism is defined as follows:\n\nA(Q,K,V) = softmax(QK^T)V\n\\tag{1}\nOnce we hash Q and K we will then compute standard attention on the bins that we have created. We will repeat the same process several times to increase the probability of having the same key in the same bin as the query.\n\n\n\n\n\n\n\nFigure 5: week-4\n\n\n\nGiven the sequence of queries and keys, we hash them into buckets. Check out Course 1 Week 4 for a review of the hashing.\nWe will then sort them by bucket.\nWe split the buckets into chunks (this is a technical detail for parallel computing purposes).\nWe then compute the attention within the same bucket of the chunk we are looking at and the previous chunk.\n\n\nQ. Why do we need to look at the previous chunk?\n\nWe can see in the figure some buckets (both blue and yellow) have been split across two chunks. Looking at the previous chunk will let we attend to the full bucket.\nIn Winograd schemas the resolution of the ambiguous pronoun switches between the two variants of the sentence.\n\nthe animal didn’t cross the street because it was too tired / the animal didn’t cross the street because it was too wide / The city councilmen refused the demonstrators a permit because they feared violence. / The city councilmen refused the demonstrators a permit because they advocated violence. /",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w4/index.html#reformer-lsh",
    "href": "notes/c4w4/index.html#reformer-lsh",
    "title": "Chat Bots",
    "section": "Reformer LSH",
    "text": "Reformer LSH\nReformer LSH",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w4/index.html#sec-reversible",
    "href": "notes/c4w4/index.html#sec-reversible",
    "title": "Chat Bots",
    "section": "Motivation for Reversible Layers: Memory!",
    "text": "Motivation for Reversible Layers: Memory!\n\n\n\n\n\n\n\nFigure 6: Memory efficency\n\n\nFor example in this model:\n\n2 GB for the input\n2 GB are required to compute the Attention\n2 GB for the feed forward. There are 12 attention layers 12 feed forward layers. That is equal to 12 * 2 + 12*2 + 2 (for the input) = 50 GB. That is a lot of memory.\n\nIf N is the sequence length:\n\nTransformers need O(N^2) memory.\n\nEach layer of a transformers has an Attention block and feed-forward block. If we want to process, for example to train a document of length 1 million token with 12 layers we will need 50 GB of ram. As we use residual architecture during prediction we only need the current layers input and the output for the next layer. But during training we need to keep all the copies so we can back-propagate the errors.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w4/index.html#sec-reversible-residual",
    "href": "notes/c4w4/index.html#sec-reversible-residual",
    "title": "Chat Bots",
    "section": "Reversible Residual Layers",
    "text": "Reversible Residual Layers",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w4/index.html#sec-reformer",
    "href": "notes/c4w4/index.html#sec-reformer",
    "title": "Chat Bots",
    "section": "Reformer",
    "text": "Reformer\ncan run 1 million token in 16 gb",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w4/index.html#sec-lab-2",
    "href": "notes/c4w4/index.html#sec-lab-2",
    "title": "Chat Bots",
    "section": "Lab 2: Reversible layers",
    "text": "Lab 2: Reversible layers\n\nFrom the trax documents a Residual, involves first a split and then a merge:\nreturn Serial(\n    Branch(shortcut, layer), # split \n    Add(),                   # merge\n)\nwhere:\n\nBranch(shortcut, layers): makes two copies of the single incoming data stream, passes one copy via the shortcut (typically a no-op), and processes the other copy via the given layers (applied in series). [𝑛_{𝑖𝑛}=1, 𝑛_{𝑜𝑢𝑡}=2]\nAdd(): combines the two streams back into one by adding two tensors element-wise. [𝑛_{𝑖𝑛}=2, 𝑛_{𝑜𝑢𝑡}=1]\n\nIn the Branch operation each layer in the input list copies as many inputs from the stack as it needs, and their outputs are successively combined on stack. Put another way, each element of the branch can have differing numbers of inputs and outputs. Let’s try a more complex example. To work these operations modify the stack by replicating the input needed as well as pushing the outputs (as specified using th out parameters).",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w4/index.html#sec-references",
    "href": "notes/c4w4/index.html#sec-references",
    "title": "Chat Bots",
    "section": "References",
    "text": "References\n\nPractical and Optimal LSH for Angular Distance\n\n\nTokenization\n\nSentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (Kudo & Richardson 2018) sub-word tokenization\nSubword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates (Kudo 2018) sub-word tokenization\nNeural Machine Translation of Rare Words with Subword Units (Sennrich et all 2016) sub-word tokenization\nSubword tokenizers TF tutorial sub-word tokenization\n[https://blog.floydhub.com/tokenization-nlp/]\nSwivel: Improving Embeddings by Noticing What’s Missing (Shazeer, 2016)\n\n\n\nTransformers\n\n[Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer] (Raffel et al, 2019)\n\n[Reformer: The Efficient Transformer] (Kitaev et al, 2020)\nAttention Is All We Need (Vaswani et al, 2017) Vaswani et al. (2023)\n[Deep contextualized word representations] (Peters et al, 2018)\n[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding] (Devlin et al, 2018)\n[Finetuning Pretrained Transformers into RNNs] (Kasai et all 2021)\n[The Illustrated Transformer] (Alammar, 2018)\n[The Illustrated GPT-2] (Alammar, 2019)\n[How GPT3 Works - Visualizations and Animations] (Alammar, 2020)\nIn Weng (2018) the author covers many attention mechanism Attention? Attention!\n[The Transformer Family] (Lilian Weng, 2020)\nTeacher forcing for RNNs\n\n\n\nQuestion Answering Task:\n\nIn Rush (2015) , a paper titled A Neural Attention Model for Abstractive Sentence Summarization the authors discuss the summarization task.\n\nThe first two videos can be viewed on youtube.\n\n\n\n\n\n\n\nVideo 1: Christopher Manning in Stanford CS224N: NLP with Deep Learning | Winter 2019 | Lecture 10 On Question Answering.\n\n\n\n\n\n\n\n\nVideo 2: Christopher Manning and Danqi Chen in Stanford CS224N NLP with Deep Learning | Winter 2021 | Lecture 12 - Question Answering\n\n\n\n\n\n\n\n\nVideo 3",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w4/index.html#links",
    "href": "notes/c4w4/index.html#links",
    "title": "Chat Bots",
    "section": "Links",
    "text": "Links\n\nJax\nTrax\nTrax community on Gitter\nCNN daily mail dataset\nLei Mao Machine Learning, Artificial Intelligence, Computer Science.\nByte Pair Encoding (Lei Mao 2021)\nExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\nReformer: The Efficient Transformer\nAttention Is All We Need\nDeep contextualized word representations\nThe Illustrated Transformer\nThe Illustrated GPT-2\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\nHow GPT3 Works - Visualizations and Animations\nAttention? Attention! (Lilian Weng, 2018)\nThe Transformer Family “(Lilian Weng, 2020)”\nFinetuning Pretrained Transformers into RNNs “(Kasai et all 2021)”",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Chat Bots",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w3/lab01.html",
    "href": "notes/c2w3/lab01.html",
    "title": "N-grams Corpus preprocessing",
    "section": "",
    "text": "course banner\nThe input corpus in this week’s assignment is a continuous text that needs some preprocessing so that we can start calculating the n-gram probabilities.\nSome common preprocessing steps for the language models include: - lowercasing the text - remove special characters - split text to list of sentences - split sentence into list words\nCan we note the similarities and differences among the preprocessing steps shown during the Course 1 of this specialization?\nimport nltk               # NLP toolkit\nimport re                 # Library for Regular expression operations\n\n#nltk.download('punkt')    # Download the Punkt sentence tokenizer \nnltk.download('punkt_tab')\n\n[nltk_data] Downloading package punkt_tab to /home/oren/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n\n\nTrue",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "L1 - N-grams Corpus preprocessing"
    ]
  },
  {
    "objectID": "notes/c2w3/lab01.html#n-grams",
    "href": "notes/c2w3/lab01.html#n-grams",
    "title": "N-grams Corpus preprocessing",
    "section": "N-grams",
    "text": "N-grams\n\nSentence to n-gram\nThe next step is to build n-grams from the tokenized sentences.\nA sliding window of size n-words can generate the n-grams. The window scans the list of words starting at the sentence beginning, moving by a step of one word until it reaches the end of the sentence.\nHere is an example method that prints all trigrams in the given sentence.\n\ndef sentence_to_trigram(tokenized_sentence):\n    \"\"\"\n    Prints all trigrams in the given tokenized sentence.\n    \n    Args:\n        tokenized_sentence: The words list.\n    \n    Returns:\n        No output\n    \"\"\"\n    # note that the last position of i is 3rd to the end\n    for i in range(len(tokenized_sentence) - 3 + 1):\n        # the sliding window starts at position i and contains 3 words\n        trigram = tokenized_sentence[i : i + 3]\n        print(trigram)\n\ntokenized_sentence = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n\nprint(f'List all trigrams of sentence: {tokenized_sentence}\\n')\nsentence_to_trigram(tokenized_sentence)\n\nList all trigrams of sentence: ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n\n['i', 'am', 'happy']\n['am', 'happy', 'because']\n['happy', 'because', 'i']\n['because', 'i', 'am']\n['i', 'am', 'learning']\n['am', 'learning', '.']\n\n\n ### Prefix of an n-gram\nAs we saw in the lecture, the n-gram probability is often calculated based on the (n-1)-gram counts. The prefix is needed in the formula to calculate the probability of an n-gram.\n\\begin{equation*}\nP(w_n|w_1^{n-1})=\\frac{C(w_1^n)}{C(w_1^{n-1})}\n\\end{equation*}\nThe following code shows how to get an (n-1)-gram prefix from n-gram on an example of getting trigram from a 4-gram.\n\n# get trigram prefix from a 4-gram\nfourgram = ['i', 'am', 'happy','because']\ntrigram = fourgram[0:-1] # Get the elements from 0, included, up to the last element, not included.\nprint(trigram)\n\n['i', 'am', 'happy']\n\n\n\n\nStart and end of sentence word &lt;s&gt; and &lt;/s&gt;\nWe could see in the lecture that we must add some special characters at the beginning and the end of each sentence:\n\n&lt;s&gt; at beginning\n&lt;/s&gt; at the end\n\nFor n-grams, we must prepend n-1 of characters at the begining of the sentence.\nLet us have a look at how we can implement this in code.\n\n# when working with trigrams, we need to prepend 2 &lt;s&gt; and append one &lt;/s&gt;\nn = 3\ntokenized_sentence = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\ntokenized_sentence = [\"&lt;s&gt;\"] * (n - 1) + tokenized_sentence + [\"&lt;/s&gt;\"]\nprint(tokenized_sentence)\n\n['&lt;s&gt;', '&lt;s&gt;', 'i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.', '&lt;/s&gt;']\n\n\nThat’s all for the lab for “N-gram” lesson of week 3.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "L1 - N-grams Corpus preprocessing"
    ]
  },
  {
    "objectID": "notes/c2w3/index.html",
    "href": "notes/c2w3/index.html",
    "title": "Autocomplete and Language Models",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\n\nFigure 1\nThese are my notes for Week 3 notes for the Natural Language Processing with Probabilistic Models Course in the Natural Language Processing Specialization Offered by DeepLearning.AI on Coursera.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w3/index.html#tldr---autocomplete-and-language-models",
    "href": "notes/c2w3/index.html#tldr---autocomplete-and-language-models",
    "title": "Autocomplete and Language Models",
    "section": "TL;DR - Autocomplete and Language Models",
    "text": "TL;DR - Autocomplete and Language Models\n\n\n\nLanguage Models in a nutshell\n\n\nThis week we learn how to model a language using N-grams. Starting from the definition of conditional probability we develop the probabilities of sequences of words. Next we add start and end of sentences tokens to our word sequence model. Next we add tokens to represent out of vocabulary words. Then we tackle sparsity by implementing smoothing and backoff. Finally we consider how to evaluate our language model.\nIn the labs we preprocess a corpus to create an N-gram language model. We then build the language model and evaluate it using perplexity.\nIn the assignment for this week, we build a language model to generate autocomplete a text fragment. We will also evaluate the perplexity of the model.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w3/index.html#n-grams-overview",
    "href": "notes/c2w3/index.html#n-grams-overview",
    "title": "Autocomplete and Language Models",
    "section": "N-Grams Overview",
    "text": "N-Grams Overview\nRecall how Firth suggested a distributional view of semantics?\nPreviously we used a vector space model to represent this idea. Now we dive deeper and develop a model for distributional semantics using probabilities of sequences of words. Once we can estimate these probabilities we can predict the next word in a sentence.\nN-gram refer to howe we model the a sequence of N words. For example, a bigram model would model the probability of a word given the previous word. A trigram model would model the probability of a word given the previous bigram. And so on. These probabilities are derived by counting frequencies in a corpus of texts.\nN-grams are fundamental and give we a foundation that will allow we to understand more complicated models in the specialization. They form the theoretical under pinning for the next two courses.\nN-grams models allow us to predict the probabilities of certain words happening in a specific sequence. Using that, we can build an auto-correct or even a search suggestion tool.\nOther applications of N-gram language modeling include:\n\nSpeech recognition\nSpelling correction\nAugmentative communication\n\n\n\n\n\n\n\n\ncaption\n\n\n\n\nFigure 2: Description\n\n\nThis week we are going to learn to:\n\nProcess a text corpus to N-gram language model\nHandle out of vocabulary words\nImplement smoothing for previously unseen N-grams\nLanguage model evaluation",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w3/index.html#n-grams-and-probabilities",
    "href": "notes/c2w3/index.html#n-grams-and-probabilities",
    "title": "Autocomplete and Language Models",
    "section": "N-grams and Probabilities",
    "text": "N-grams and Probabilities\nBefore we start computing probabilities of certain sequences, we need to first define what is an N-gram language model:\n\n\n\n\n\n\n\ncaption\n\n\n\n\nFigure 3: Description\n\n\nNow given the those definitions, we can label a sentence as follows:\n\n\n\n\n\n\n\ncaption\n\n\n\n\nFigure 4: Description\n\n\nIn other notation we can write:\n\nw_1^m = w_1 w_2 w_3 ... w_m\n \nw_1^3 = w_1 w_2 w_3\n \nw_{m-2}^m = w_{m-2} w_{m-1} w_m\n\nGiven the following corpus: I am happy because I am learning.\nSize of corpus m = 7.\n\nP(I) = \\frac{1}{7}\n\n\nP(happy) = \\frac{1}{7}\n\nTo generalize, the probability of a unigram is\n\nP(w) = \\frac{C(w)}{m}\n\nWhere C(w) is the count of the word in the corpus and m is the size of the corpus.\n\nBigram Probability:\n\n\n\n\n\n\n\ncaption\n\n\n\n\nFigure 5: Description\n\n\n\n\nTrigram Probability:\nTo compute the probability of a trigram: \nP(w_3 | w_1^2) = \\frac{C(w_1^2 w_3)}{C(w_1^2)}\n\n\nC(w_1^2 w_3) = C(w_1 w_2 w_3) = C(w_1^3)\n\nN-gram Probability:\n\nP(w_n | w_1^{n-1}) = \\frac{C(w_1^{n-1} w_n)}{C(w_1^{n-1})}\n\n\nC(w_1^{n-1} w_n) = C(w_1^n)",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w3/index.html#sequence-probabilities",
    "href": "notes/c2w3/index.html#sequence-probabilities",
    "title": "Autocomplete and Language Models",
    "section": "Sequence Probabilities",
    "text": "Sequence Probabilities\nWe just saw how to compute sequence probabilities, their short comings, and finally how to approximate N-gram probabilities. In doing so, we try to approximate the probability of a sentence. For example, what is the probability of the following sentence: The teacher drinks tea. To compute it, we will make use of the following:\n\nP(B|A) = \\frac{P(A,B)}{P(A)} \\Rightarrow P(A,B) = P(A)P(B|A)\n\n\nP(A,B,C,D) = P(A)P(B|A)P(C|A,B)P(D|A,B,C)\n\nTo compute the probability of a sequence, we can compute the following:\n\nP(\\text{The teacher drinks tea}) = P(\\text{The})P(\\text{teacher}|\\text{The})P(\\text{drinks}|\\text{The teacher})P(\\text{tea}|\\text{The teacher drinks})\n\nOne of the main issues with computing the probabilities above is the corpus rarely contains the exact same phrases as the ones we computed your probabilities on. Hence, we can easily end up getting a probability of 0. The Markov assumption indicates that only the last word matters. Hence:\n\n\\text{Bigram} \\qquad P(w_n | w_1^{n-1}) \\approx P(w_n | w_{n-1})\n\n\n\\text{Trigram} \\qquad P(w_n | w_1^{n-1}) \\approx P(w_n | w_{n-2}^{n-1})\n\n\n\\text{N-gram} \\qquad P(w_n | w_1^{n-1}) \\approx P(w_n | w_{n-N+1}^{n-1})\n\nWe can model the entire sentence as follows:\n\nP(w_1^n) = P(w_1)P(w_2|w_1)P(w_3|w_2)P(w_4|w_3) ... P(w_n|w_{n-1}) = \\prod_{i=1}^{n} P(w_i|w_{i-1})\n\n\nP(w_1^n) = P(w_1)P(w_2|w_1)P(w_3|w_2)P(w_4|w_3) ... P(w_n|w_{n-1}) = \\prod_{i=1}^{n} P(w_i|w_{i-1})",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w3/index.html#starting-and-ending-sentences",
    "href": "notes/c2w3/index.html#starting-and-ending-sentences",
    "title": "Autocomplete and Language Models",
    "section": "Starting and Ending Sentences",
    "text": "Starting and Ending Sentences\nWe usually start and end a sentence with the following tokens respectively: &lt;s&gt; &lt;/s&gt;.\nWhen computing probabilities using a unigram, we can append an &lt;s&gt; in the beginning of the sentence. To generalize to an N-gram language model, we can add N-1 start tokens &lt;s&gt;.\nFor the end of sentence token &lt;/s&gt;, we only need one even if it is an N-gram. Here is an example:\n\n\n\n\n\n\n\ncaption\n\n\n\n\nFigure 6: Description\n\n\nMake sure we know how to compute the probabilities above!",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w3/index.html#lecture-notebook-corpus-preprocessing-for-n-grams",
    "href": "notes/c2w3/index.html#lecture-notebook-corpus-preprocessing-for-n-grams",
    "title": "Autocomplete and Language Models",
    "section": "Lecture notebook: Corpus preprocessing for N-grams",
    "text": "Lecture notebook: Corpus preprocessing for N-grams\nlab 1 Corpus preprocessing for N-grams",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w3/index.html#the-n-gram-language-model",
    "href": "notes/c2w3/index.html#the-n-gram-language-model",
    "title": "Autocomplete and Language Models",
    "section": "The N-gram Language Model",
    "text": "The N-gram Language Model\n\n\n\n\n\n\n\nCount matrix\n\n\n\n\nFigure 7: Count Matrix\n\n\n\n\n\n\n\n\nProbability matrix\n\n\n\n\nFigure 8: Probability Matrix\n\n\n\n\n\n\n\n\nLanguage model\n\n\n\n\nFigure 9: Language model\n\n\n\n\n\n\n\n\nLog probability\n\n\n\n\nFigure 10: Log probability\n\n\n\n\n\nWe covered a lot of concepts in the previous video. We have seen:\n\nCount matrix c.f Figure 7\nProbability matrix c.f Figure 8\nLanguage model c.f. Figure 9\nLog probability c.f Figure 10 to avoid underflow\nGenerative language model c.f. Figure 11\n\nIn the count matrix:\n\nRows correspond to the unique corpus N-1 grams.\nColumns correspond to the unique corpus words.\n\nHere is an example of the count matrix of a bigram.\nTo convert it into a probability matrix, we can use the following formula:\n\nP(w_n \\mid w^{n-1}_{n-N+1}) = \\frac{C(w^{n-1}_{n-N+1}, w_n)}{C(w^{n-1}_{n-N+1})}\n\\tag{1}\n\nsum(row) = \\sum_{w \\in V} C(w^{n-1}_{n-N+1}, w) = C(w^{n-1}_{n-N+1})\n\\tag{2}\nNow given the probability matrix, we can generate the language model. We can compute the sentence probability and the next word prediction.\nTo compute the probability of a sequence, we needed to compute:\n\n\\begin{align*}\nP(w_1^n) &= P(w_1)P(w_2|w_1)P(w_3|w_2)P(w_4|w_3) \\ldots P(w_n|w_{n-1})  \\\\\n&= \\prod_{i=1}^{n} P(w_i | w_{i-1})\n\\end{align*}\n\\tag{3}\nTo avoid underflow, we can multiply by the log.\n\n\\begin{align*}\n\\log P(w_1^n) &= \\log P(w_1) + \\log P(w_2|w_1) + \\log P(w_3|w_2) + \\ldots+ \\log P(w_n|w_{n-1}) \\\\\n& = \\sum_{i=1}^{n} \\log P(w_i|w_{i-1})\n\\end{align*}\n\\tag{4}\nFinally, we can create a generative language model.\n\n\n\n\n\n\n\nGenerative language model\n\n\n\n\nFigure 11: Generative language model",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w3/index.html#lecture-notebook-building-the-language-model",
    "href": "notes/c2w3/index.html#lecture-notebook-building-the-language-model",
    "title": "Autocomplete and Language Models",
    "section": "Lecture notebook: Building the language model",
    "text": "Lecture notebook: Building the language model\nlab 2 Building the language model",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w3/index.html#language-model-evaluation",
    "href": "notes/c2w3/index.html#language-model-evaluation",
    "title": "Autocomplete and Language Models",
    "section": "Language Model Evaluation",
    "text": "Language Model Evaluation\nSplitting the Data We will now discuss the train/val/test splits and perplexity.\n\nTrain/Val/Test splits\n\n\n\nSmaller Corpora:\nLarger Corpora:\n\n\n\n\ntrain\n80%\n98%\n\n\ntest\n10%\n1%\n\n\nval\n10%\n1%\n\n\n\nThere are two main methods for splitting the data:\n\n\n\n\n\n\n\ncaption\n\n\n\n\nFigure 12: Count Matrix",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w3/index.html#perplexity",
    "href": "notes/c2w3/index.html#perplexity",
    "title": "Autocomplete and Language Models",
    "section": "Perplexity",
    "text": "Perplexity\nPerplexity is used to tell us whether a set of sentences look like they were written by humans rather than by a simple program choosing words at random. A text that is written by humans is more likely to have lower perplexity, where a text generated by random word choice would have a higher perplexity.\nConcretely, here are the formulas to calculate perplexity.\n\nPP(W) = P(s_1, s_2, \\ldots, s_m)^{-\\frac{1}{m}}\n\n\nPP(W) = \\sqrt{ \\prod_{i=1}^{m} \\prod_{j=1}^{|s_i|} \\frac{1}{P(w_j^{(i)} | w_{j-1}^{(i)})}}\n\n​w_j^{(i)} corresponds to the jth word in the ith sentence. If we were to concatenate all the sentences then w_i is the ith word in the test set. To compute the log perplexity, we go from:\n\nPP(W) = \\sqrt{ \\prod_{i=1}^{m} \\frac{1}{P(w_i | w_{i-1})}}\n\nTo\n\nlog PP(W) = -\\frac{1}{m} \\sum_{i=1}^{m} \\log_2 P(w_i | w_{i-1})",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w3/index.html#out-of-vocabulary-words",
    "href": "notes/c2w3/index.html#out-of-vocabulary-words",
    "title": "Autocomplete and Language Models",
    "section": "Out of Vocabulary Words",
    "text": "Out of Vocabulary Words\nMany times, we will be dealing with unknown words in the corpus. So how do we choose your vocabulary? What is a vocabulary?\nA vocabulary is a set of unique words supported by your language model. In some tasks like speech recognition or question answering, we will encounter and generate words only from a fixed set of words. Hence, a closed vocabulary.\nOpen vocabulary means that we may encounter words from outside the vocabulary, like a name of a new city in the training set. Here is one recipe that would allow we to handle unknown words.\n\nCreate vocabulary V\nReplace any word in corpus and not in V by &lt;UNK&gt;\nCount the probabilities with &lt;UNK&gt; as with any other word\n\n\n\n\n\n\n\n\ncaption\n\n\n\n\nFigure 13: Count Matrix\n\n\nThe example above shows how we can use min_frequency and replace all the words that show up fewer times than min_frequency by UNK. We can then treat UNK as a regular word.\n\nCriteria to create the vocabulary\n\nMin word frequency f\nMax |V|, include words by frequency\nUse &lt;UNK&gt; sparingly (Why?)\nPerplexity - only compare LMs with the same V",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w3/index.html#out-of-vocabulary-words-1",
    "href": "notes/c2w3/index.html#out-of-vocabulary-words-1",
    "title": "Autocomplete and Language Models",
    "section": "Out of Vocabulary Words",
    "text": "Out of Vocabulary Words",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w3/index.html#smoothing",
    "href": "notes/c2w3/index.html#smoothing",
    "title": "Autocomplete and Language Models",
    "section": "Smoothing",
    "text": "Smoothing\n\n\n\n\n\n\n\nProblem\n\n\n\n\nFigure 14: Missing N-grams\n\n\nThe three main concepts covered here are dealing with missing n-grams, smoothing, and Backoff and interpolation.\n\n\n\n\n\n\n\nSmoothing\n\n\n\n\nFigure 15: Add One Smoothing, Add K Smoothing\n\n\n\nP(w_n | w^{n-1}_{n-N+1}) = \\frac{C(w^{n-1}_{n-N+1}, w_n)}{C(w^{n-1}_{n-N+1})} \\qquad \\text{can be 0}\n\\tag{5}\nHence we can add-1 smoothing as follows to fix that problem:\n\nP(w_n | w_{n-1}) = \\frac{C(w_{n-1}, w_n) + 1}{\\sum_{w \\in V} (C(w_{n-1}, w) + 1)} = \\frac{C(w_{n-1}, w_n) + 1}{C(w_{n-1}) + V} \\qquad\n\\tag{6}\nAdd-k smoothing is very similar:\n\nP(w_n | w_{n-1}) = \\frac{C(w_{n-1}, w_n) + k}{\\sum_{w \\in V} (C(w_{n-1}, w) + k)} =\\frac{C(w_{n-1}, w_n) + k}{C(w_{n-1}) + k\\times V} \\qquad\n\\tag{7}\n\n\n\n\n\n\n\nBackoff\n\n\n\n\nFigure 16: Backoff\n\n\nWhen using back-off:\n\nIf N-gram missing =&gt; use (N-1)-gram, …: Using the lower level N-grams (i.e. (N-1)-gram, (N-2)-gram, down to unigram) distorts the probability distribution. Especially for smaller corpora, some probability needs to be discounted from higher level N-grams to use it for lower level N-grams.\nProbability discounting e.g. Katz backoff: makes use of discounting.\n“Stupid” backoff: If the higher order N-gram probability is missing, the lower order N-gram probability is used, just multiplied by a constant. A constant of about 0.4 was experimentally shown to work well.\n\nHere is a visualization of the backoff process.\nWe can also use interpolation when computing probabilities as follows:\n\n\\hat{P}(w_n | w_{n-2} W_{n-1}) = \\lambda_1 \\times P(w_n | w_{n-2} w_{n-1}) + \\lambda_2 \\times P(w_n | w_{n-1}) + \\lambda_3 \\times P(w_n) \\qquad\n\\tag{8}\nWhere\n\n\\sum_i \\lambda_i = 1",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w3/index.html#week-summary",
    "href": "notes/c2w3/index.html#week-summary",
    "title": "Autocomplete and Language Models",
    "section": "Week Summary",
    "text": "Week Summary\nThis week we learned the following concepts\n\nN-Grams and probabilities\nApproximate sentence probability from N-Grams\nBuild a language model from a corpus\nFix missing information\nOut of vocabulary words with &lt;UNK&gt;\nMissing N-Gram in corpus with smoothing, backoff and interpolation\nEvaluate language model with perplexity\nCoding assignment!",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c2w3/index.html#lecture-notebook-language-model-generalization",
    "href": "notes/c2w3/index.html#lecture-notebook-language-model-generalization",
    "title": "Autocomplete and Language Models",
    "section": "Lecture notebook: Language model generalization",
    "text": "Lecture notebook: Language model generalization\nAutocomplete",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Autocomplete & Language Models",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w2/lab01.html",
    "href": "notes/c4w2/lab01.html",
    "title": "The Three Ways of Attention and Dot Product Attention: Ungraded Lab Notebook",
    "section": "",
    "text": "In this notebook you’ll explore the three ways of attention (encoder-decoder attention, causal attention, and bi-directional self attention) and how to implement the latter two with dot product attention.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "L1 - Attention"
    ]
  },
  {
    "objectID": "notes/c4w2/lab01.html#background",
    "href": "notes/c4w2/lab01.html#background",
    "title": "The Three Ways of Attention and Dot Product Attention: Ungraded Lab Notebook",
    "section": "Background",
    "text": "Background\nAs you learned last week, attention models constitute powerful tools in the NLP practitioner’s toolkit. Like LSTMs, they learn which words are most important to phrases, sentences, paragraphs, and so on. Moreover, they mitigate the vanishing gradient problem even better than LSTMs. You’ve already seen how to combine attention with LSTMs to build encoder-decoder models for applications such as machine translation.\n\nThis week, you’ll see how to integrate attention into transformers. Because transformers are not sequence models, they are much easier to parallelize and accelerate. Beyond machine translation, applications of transformers include: * Auto-completion * Named Entity Recognition * Chatbots * Question-Answering * And more!\nAlong with embedding, positional encoding, dense layers, and residual connections, attention is a crucial component of transformers. At the heart of any attention scheme used in a transformer is dot product attention, of which the figures below display a simplified picture:\n\n\nWith basic dot product attention, you capture the interactions between every word (embedding) in your query and every word in your key. If the queries and keys belong to the same sentences, this constitutes bi-directional self-attention. In some situations, however, it’s more appropriate to consider only words which have come before the current one. Such cases, particularly when the queries and keys come from the same sentences, fall into the category of causal attention.\n\nFor causal attention, we add a mask to the argument of our softmax function, as illustrated below:\n\n\nNow let’s see how to implement attention with NumPy. When you integrate attention into a transformer network defined with Trax, you’ll have to use trax.fastmath.numpy instead, since Trax’s arrays are based on JAX DeviceArrays. Fortunately, the function interfaces are often identical.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "L1 - Attention"
    ]
  },
  {
    "objectID": "notes/c4w2/lab01.html#imports",
    "href": "notes/c4w2/lab01.html#imports",
    "title": "The Three Ways of Attention and Dot Product Attention: Ungraded Lab Notebook",
    "section": "Imports",
    "text": "Imports\n\nimport sys\n\nimport numpy as np\nimport scipy.special\n\nimport textwrap\nwrapper = textwrap.TextWrapper(width=70)\n\n# to print the entire np array\nnp.set_printoptions(threshold=sys.maxsize)\n\nHere are some helper functions that will help you create tensors and display useful information:\n\ncreate_tensor() creates a numpy array from a list of lists.\ndisplay_tensor() prints out the shape and the actual tensor.\n\n\ndef create_tensor(t):\n    \"\"\"Create tensor from list of lists\"\"\"\n    return np.array(t)\n\n\ndef display_tensor(t, name):\n    \"\"\"Display shape and tensor\"\"\"\n    print(f'{name} shape: {t.shape}\\n')\n    print(f'{t}\\n')\n\nCreate some tensors and display their shapes. Feel free to experiment with your own tensors. Keep in mind, though, that the query, key, and value arrays must all have the same embedding dimensions (number of columns), and the mask array must have the same shape as np.dot(query, key.T).\n\nq = create_tensor([[1, 0, 0], [0, 1, 0]])\ndisplay_tensor(q, 'query')\nk = create_tensor([[1, 2, 3], [4, 5, 6]])\ndisplay_tensor(k, 'key')\nv = create_tensor([[0, 1, 0], [1, 0, 1]])\ndisplay_tensor(v, 'value')\nm = create_tensor([[0, 0], [-1e9, 0]])\ndisplay_tensor(m, 'mask')\n\nquery shape: (2, 3)\n\n[[1 0 0]\n [0 1 0]]\n\nkey shape: (2, 3)\n\n[[1 2 3]\n [4 5 6]]\n\nvalue shape: (2, 3)\n\n[[0 1 0]\n [1 0 1]]\n\nmask shape: (2, 2)\n\n[[ 0.e+00  0.e+00]\n [-1.e+09  0.e+00]]",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "L1 - Attention"
    ]
  },
  {
    "objectID": "notes/c4w2/lab01.html#dot-product-attention",
    "href": "notes/c4w2/lab01.html#dot-product-attention",
    "title": "The Three Ways of Attention and Dot Product Attention: Ungraded Lab Notebook",
    "section": "Dot product attention",
    "text": "Dot product attention\nHere we come to the crux of this lab, in which we compute \\textrm{softmax} \\left(\\frac{Q K^T}{\\sqrt{d}} + M \\right) V, where the (optional, but default) scaling factor \\sqrt{d} is the square root of the embedding dimension.\n\ndef DotProductAttention(query, key, value, mask, scale=True):\n    \"\"\"Dot product self-attention.\n    Args:\n        query (numpy.ndarray): array of query representations with shape (L_q by d)\n        key (numpy.ndarray): array of key representations with shape (L_k by d)\n        value (numpy.ndarray): array of value representations with shape (L_k by d) where L_v = L_k\n        mask (numpy.ndarray): attention-mask, gates attention with shape (L_q by L_k)\n        scale (bool): whether to scale the dot product of the query and transposed key\n\n    Returns:\n        numpy.ndarray: Self-attention array for q, k, v arrays. (L_q by L_k)\n    \"\"\"\n\n    assert query.shape[-1] == key.shape[-1] == value.shape[-1], \"Embedding dimensions of q, k, v aren't all the same\"\n\n    # Save depth/dimension of the query embedding for scaling down the dot product\n    if scale: \n        depth = query.shape[-1]\n    else:\n        depth = 1\n\n    # Calculate scaled query key dot product according to formula above\n    dots = np.matmul(query, np.swapaxes(key, -1, -2)) / np.sqrt(depth) \n    \n    # Apply the mask\n    if mask is not None:\n        dots = np.where(mask, dots, np.full_like(dots, -1e9)) \n    \n    # Softmax formula implementation\n    # Use scipy.special.logsumexp of masked_qkT to avoid underflow by division by large numbers\n    # Note: softmax = e^(dots - logaddexp(dots)) = E^dots / sumexp(dots)\n    logsumexp = scipy.special.logsumexp(dots, axis=-1, keepdims=True)\n\n    # Take exponential of dots minus logsumexp to get softmax\n    # Use np.exp()\n    dots = np.exp(dots - logsumexp)\n\n    # Multiply dots by value to get self-attention\n    # Use np.matmul()\n    attention = np.matmul(dots, value)\n    \n    return attention\n\nNow let’s implement the masked dot product self-attention (at the heart of causal attention) as a special case of dot product attention\n\ndef dot_product_self_attention(q, k, v, scale=True):\n    \"\"\" Masked dot product self attention.\n    Args:\n        q (numpy.ndarray): queries.\n        k (numpy.ndarray): keys.\n        v (numpy.ndarray): values.\n    Returns:\n        numpy.ndarray: masked dot product self attention tensor.\n    \"\"\"\n    \n    # Size of the penultimate dimension of the query\n    mask_size = q.shape[-2]\n\n    # Creates a matrix with ones below the diagonal and 0s above. It should have shape (1, mask_size, mask_size)\n    # Use np.tril() - Lower triangle of an array and np.ones()\n    mask = np.tril(np.ones((1, mask_size, mask_size), dtype=np.bool_), k=0)  \n        \n    return DotProductAttention(q, k, v, mask, scale=scale)\n\n\ndot_product_self_attention(q, k, v)\n\narray([[[0.        , 1.        , 0.        ],\n        [0.84967455, 0.15032545, 0.84967455]]])",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "L1 - Attention"
    ]
  },
  {
    "objectID": "notes/c4w2/index.html",
    "href": "notes/c4w2/index.html",
    "title": "Week 2 - Text Summarization",
    "section": "",
    "text": "deeplearning.ai",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w2/index.html#sec-transformers-vs-rnns",
    "href": "notes/c4w2/index.html#sec-transformers-vs-rnns",
    "title": "Week 2 - Text Summarization",
    "section": "Transformers vs RNNs",
    "text": "Transformers vs RNNs\nRNNs were a big breakthrough and became the state of the art (SOTA) for machine translation (MT).\nThis illustrates a typical RNN that is used to translate the English sentence “How are you?” to its German equivalent, “Wie sind Sie?”.\n\n\n\n\nrnn-non-parallel\n\n\n\n\nlstm\n\n\nThe LSTM which goes a long way to solving the vanishing gradient problems requires three times the memory and cpu steps a the vanilla RNN.\nHowever, as time went by and models got longer and deeper the biggest challenge with improving RNNs, became their use of sequential computation.\n\n\n\n\nseq2seq-steps\n\nWhich entailed that to process the word “you”, the RNN it has to first go through “are” and then “you”. Two other issues with RNNs are the:\n\nInformation loss\nIt becomes harder to keep track of whether the subject is singular or plural as we move further away from the subject.\n\n\n\n\nTransformer\n\ntransformer architecture:\nin the encoder side - lookup layer - the source sequence is converted from one hot encoding to a distributed representation using an embedding. - this is converted to K V matrices in the decoder side\n\n\nVanishing Gradient Problem\nWhen gradients we back-propagate, the gradients can become really small and as a result.\nWith small gradient the model will learn very little.\n\n\n\n\npositional-encoding\n\nTransformers which are based on attention and don’t require any sequential computation per layer, only a single step is needed.\n\n\n\n\nsummary\n\nAdditionally, the gradient steps that need to be taken from the last output to the first input in a transformer is just one. For RNNs, the number of steps increases with longer sequences. Finally, transformers don’t suffer from vanishing gradients problems that are related to the length of the sequences.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w2/index.html#sec-transformer-applications",
    "href": "notes/c4w2/index.html#sec-transformer-applications",
    "title": "Week 2 - Text Summarization",
    "section": "Transformer Applications",
    "text": "Transformer Applications\n Transformers have essentially replaced RNN,LSTM and GRUs in sequence processing.\n\n\n\n\napplication-NLP\n\n\nApplications of Transformers\n\nText summarization\nAutocomplete\nNER\nQ&A\nTranslation\nChat Bots\nSentiment Analyses\nMarket Intelligence\nText Classification\nOCR\nSpell Checking\n\n\n\n\n\nsota\n\n\n\nSOTA Transformers\nTransformers Time Line:\n\nGPT-4:\nElmO\nBERT\nT5",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w2/index.html#sec-t5",
    "href": "notes/c4w2/index.html#sec-t5",
    "title": "Week 2 - Text Summarization",
    "section": "T5 - Text-To-Text Transfer Transformer",
    "text": "T5 - Text-To-Text Transfer Transformer\n\n\n\n\nt5\n\n\n\n\nt5\n\n\n(Raffel et al. 2019) introduced T5 which can do a number of tasks with a single model. While the earlier transformer models were able to score high in many different tasks without specific training. T5 is setup to handle different inputs and respond with output that is relevant to the requested task.\n\nT5 Classification tasks\nThese tasks are selected using the initial string: - Translate English into German - Cola sentence - Question\n\n\n\n\ntext-to-text-transformer\n\n\n\nT5 regression tasks\n\nStbs Sentence1 … Stbs Sentence2 …\nSummarize:\n\nplay trivia against T5 here\n\n\n\n\ntransformers quiz\n\n\n\n\n\n\n\nWarning\n\n\n\nI found this one a little confusing\n\n\nWe are told that the transformers can do in one operation what RNN needed to do in many steps. Also when querying transformers it does one task at a time. It seem that this question is about the ability of multiple heads to do several tasks at once could not do this is not well understood.\n\n\n\n\nsummary of transformers",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w2/index.html#sec-dot-product-attention",
    "href": "notes/c4w2/index.html#sec-dot-product-attention",
    "title": "Week 2 - Text Summarization",
    "section": "Dot-Product Attention",
    "text": "Dot-Product Attention\n\n\n\n\noutline-of-dot-product-attention\n\nDot product attention was introduced in 2015 by Minh-Thang Luong, Hieu Pham, Christopher D. Manning in Effective Approaches to Attention-based Neural Machine Translation which is available at papers with code.\nLook at Review of Effective Approaches to Attention-based NMT\nDot product attention is the main operation in transformers. It is the dot product between the embedding of source and target sequences. The embedding used is a cross language embedding in which distance between equivalent across languages are minimized. This facilitates finding the cosine similarity using the dot product between words.\n\n\n\n\nintro-to-attention\n\nLet’s try to understand dot product attention intuitively by walking over its operations at the word level. The actual algorithm uses linear algebra to perform many operations at once which is fast but more abstract and therefore difficult to understand.\n\nUsing a pre-trained cross-language embedding encode:\n\neach German word vector q_i is placed as a column vector to form the query matrix Q,\neach English word once as k_i and once as v_i, column vectors in the key K and value V matrices. This is more of a preprocessing step.\n\nFor each German word we want to derive a continuous filter function on the English sequence to pick the most relevant words for translation. We build this filter for word q_i by taking its dot product q_i \\cdot k_i with every word vector from the english sequence these products are called the the attention weights.\nnext we convert the rudimentary filter to a probabilistic one by applying a softmax() which is just a differentiable function that converts the attention weights to probabilities by keeping them at the same relative sizes while ensuring they add to one.\nnow that we have a q-filter we want to apply it. This is done by taking the weighed sum of the english words using the attention weights.\n\n\n\\hat q_i = \\sum_{i} softmax(q_i \\cdot k_i) \\times v_i =  \\sum w_a(q_i) * v_i\n\n\nQuery, Key & Value\n\n\n\n\nqueries-keys-values\n\nI find it fascinating that the authors of Attention is all decided to motivate their work on a attention using the language of information retrieval. This makes understanding the concepts a little easier and has also lead to more recent work on LSTM to use this same language. In both case though the authors are interested in the ability of the neural network to use the wghiets that it has learned to process a long sequence of test and put together a coherent output based on distant and often sparse parts of the input. In a named entity recognition task the network has a fairly simple task to do - it needs to classify a few token based a few cues from the immediate context. But for the text summarization task the network has to understand the text and pick up the most salient bit while discarding the rest.\n\n\n\n\n\n\nQuery, Key & Value\n\n\n\nAttention uses three matrices which are formed as shown in the figure The Query Q, Key K and Value V are formed from the source and target (if there is no target then just from the source). Each word is converted into an embedding column vector and these are placed into the matracies as their columns.\nIn Video 1 Dr. Łukasz Kaiser talks about attention and here he is talking about solving the problem of retrieving information from a long sequence. At around 16 minutes in he call Q a query vector and K and V a memory, of all the words we have seen, which we want to access.\n\nThe Query is the matrix formed from the column word vector for the German words.\nThe Key is the matrix formed from the column word vector for the English words.\nThe Value is the matrix formed from the column word vector for the English words.\n\nK and V are the same\n\n\nOnce these are called keys since we use them to are we doing a similarity lookup. And the second time they are called value because we use them in the activation when we apply the weights to them. The input and output sequences are mapped to an embedding layer to become the Q, K and V matrices.\n\n\n\n\n\n\n\nVideo 1: Lukasz Kaiser’s Masterclass on Attention is all you need\n\n\nGiven an input, we transform it into a new representation or a column vector. Depending on the task we are working on, we will end up getting queries, keys, and values. Each column corresponds to a word in the figure above. Hence, when we compute the following:\n\n\n\n\nattention-formula\n\n\nmultiply Q by V.\napply the softmax() to transform to a probability.\nmultiply the softmax by V\n\n\n\n\n\nattention-math\n\nThis is restating the above in a very confusing way. I looked at it many times before I figured out that the square brackets are the dimensions and that we have the following two formulas indicated schematically above:\n\nZ = W_A V\n\nwhere:\n\nZ has size of is a ‘Q length’ \\times ‘Embedding size’ matrix\nor for coders [len(Q),D] dimensional array\n\n\nW_A = softmax(QK^T)\n\nThis concept implies that similar vectors are likely to have a higher score when we dot them with one another. We transform that score into a probability by using a softmax function. We can then multiply the output by\nWe can think of the keys and the values as being the same. Note that both K,V are of dimension L_k, D. Each query q_i picks the most similar key k_j.\n\n\n\n\nattention-formula\n\nQueries are the German words and the keys are the English words. Once we have the attention weights, we can just multiply it by V to get a weighted combination of the input.\n\n\n\n\nattention-quiz\n\n\n\n\nsummary-for-dot-product-attention\n\n\nanother interesting point made in the preceding talk is that dot product attention has O(n^2 *d) complexity but typically d &gt;&gt; n since d ~ 1000 while for n ~ 70. So transformers should perform better then an RNN whose complexity is O(n*d^2). And this is before the advantages of using an efficient transformer like reformer.\nIn (Kasai et al. 2021) there is a reversal of the trend from rnn to transformers. Here the latest results show a an idea of training big transformers and then converting them to RNN to improve performance. (One get an RNN by training a transformer.)",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w2/index.html#sec-causal-attention",
    "href": "notes/c4w2/index.html#sec-causal-attention",
    "title": "Week 2 - Text Summarization",
    "section": "Causal Attention",
    "text": "Causal Attention\n\nWe are interested in three main types of attention.\nWe’ll see a brief overview of causal attention.\nWe’ll discover some mathematical foundations behind the causal attention.\n\n\n\n\n\nthree forms of attention\n\nIn terms of use cases there are three types of attention mechanisms:\n\nScaled dot product attention:\n\nAKA Encoder-Decoder attention.\none sentence in the decoder look at to another one in the encoder.\nuse cases:\n\nseq2seq\nmachine translation.\n\n\n\n\nCausal Attention:\n\nAKA self attention.\nattention is all we need.\nIn the same sentence words attend to previous words.\nFuture words have not been generated yet.\nuse cases:\n\ngeneration\ntext generation\nsummarization.\n\n\n\n\nBi-directional self attention:\n\nIn one sentence words look both at previous and future words.\nuse cases:\n\nmachine comprehension.\nquestion answering\n\n\n\n\n\n\ncausal attention\n\nIn causal attention, queries and keys come from the same sentence. That is why it is often referred to as self-attention. In general, causal attention allows words to attend to other words that are related in various ways.\n\n\n\n\ncausal attention mask\n\nAt a high-level We have K Q V matrices. corresponding However, token should not attend to words in the future since these were not generated yet. Therefore the future token’s data is masked by adding a big negative number.\n\n\n\n\ncausal-attention-math-\n\nMathematically, it looks like this:\n\n\n\n\ncausal-attention-quiz\n\n\n\n\nsummary-for-causal-attention",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w2/index.html#sec-multi-head-attention",
    "href": "notes/c4w2/index.html#sec-multi-head-attention",
    "title": "Week 2 - Text Summarization",
    "section": "V5: Multi-head Attention",
    "text": "V5: Multi-head Attention\n Let’s summarize the intuition behind multi-head attention and scaled dot product attention.\n\n\n\n\nmuti-head-attention\n\nQ. What are multiple attention heads?\n\nMultiple attention heads are simply replicas of the attention mechanism. In this they are analogous to the multiple filters used in a convolutional neural networks (CNN).\nDuring training they specialize by learning different relationships between words.\nDuring inference the operate parallel and independently of each other.\n\n\n\n\n\noverview of muti-head attention\n\nThis is perhaps the most important slide - but it fails to show the critical part of the algorithm.\nLet’s suppose we have k attention heads. We see at the lowest level the K, Q and V being passed into passing through k linear layers. How is this accomplished and more important why. What is actually happening here is the opposite of concatenation. Instead of processing a query embedding from a space of d-dimensions we first split the embedding into k vectors of length D/k. We have now k vectors from a k D/k-dimensional subspace. We now perform a dot product attention on each of these subspaces.\n\nEach of these dot product attention is operating on a difference subspace. It sees different subsets of the data and therefore specializes. How do these heads specializes is anybody’s guess - unless we have a special embedding which has been processed using PCA or some other algorithm to ensure that each subspace corresponds to some interpretable subset of features.\n\n\n\n\nmuti-head attention scaled dot-product\n\nFor example if we used a 1024 dimension embedding which concatenates 4 representations.\n\n[0:256] is an embedding trained on a phonological task\n[256:512] is an embedding trained on a morphological task\n[513:768] is an embedding trained on a syntactical task\n[769:1024] is an embedding trained on a semantic task\n\nWe could devise a number of subspace sampling schemes to give the k different attention heads different areas of specializations.\n\nsample from a single sub-space\n4 heads sample from one subspace and 4 heads sample from 3 different sub-spaces\n5 heads sampling from 2 subspaces different sub-spaces and 3 from 1\n5 heads sampling from 2 subspaces different sub-spaces and 3 from three\n\nEach would specialize on a domain or on a interface between two domain or on all data but one domain. Language is rather redundant so they may be able to reconstruct most of the missing data - but at least they would specialize in a linguistically meaningful way.\nGiven a word, we take its embedding then we multiply it by the Q, K, V matrix to get the corresponding queries, keys and values. When we use multi-head attention, a head can learn different relationships between words from another head.\nHere’s one way to look at it:\n\nFirst, imagine that we have an embedding for a word. We multiply that embedding with Q to get q_1, K to get k_1, and V to get v_1\n\n\n\n\n\nmuti-head-attention-concatenation\n\n\n\n\nmuti-head-attention-math\n\n\n\n\nmuti-head-attention-fotmula\n\n\n\n\nmuti-head-attention-quiz\n\n\n\n\nmuti-head-attention-math\n\n\n\n\nmuti-head-attention-math\n\n\n\n\nmuti-head-attention-math\n\n\n\n\nmuti-head-attention-math\n\n\n\n\n\n\n\n\n\nNext, we feed it to the linear layer, once we go through the linear layer for each word, we need to calculate a score. After that, we end up having an embedding for each word. But we still need to get the score for how much of each word we are going to use. For example, this will tell we how similar two words are q_1 and k_1or even q_1 and k_2 by doing a simple q_1 \\dot k_1. We can take the softmax of those scores (the paper mentions that we have to divide by \\sqrt(d) to get a probability and then we multiply that by the value. That gives we the new representation of the word.) If we have many heads, we can concatenate them and then multiply again by a matrix that is of dimension (dim of each head by num heads - dim of each head) to get one final vector corresponding to each word.\n\nHere is step by step guide, first we get the Q, K, V matrices: Note that the computation above was done for one head. If we have several heads, concretely nn, then we will have Z_1, Z_2, \\ldots, Z_n. In which case, we can just concatenate them and multiply by a W_O matrix as follows:\nHence, the more heads we have, the more Zs we will end up concatenating and as a result, that will change the inner dimension of W_O, which will then project the combined embeddings into one final embedding.\n\n\n\n\nsummary-muti-head-attention",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w2/index.html#sec-transformer-decoder",
    "href": "notes/c4w2/index.html#sec-transformer-decoder",
    "title": "Week 2 - Text Summarization",
    "section": "V6: Transformer Decoder",
    "text": "V6: Transformer Decoder\n\n\n\n\noutline\n\nThere is a learning objective here!\nthe transformer decoder has two parts\n\na decoder block (with multi-head attention) - think feature acquisition.\na feed forward block - think non-parametric regression on the features.\n\n\n\n\n\ntransformer-decoder-overview\n\n\n\n\ntransformer-decoder-explaination\n\n\n\n\ntransformer-decoder-ff\n\n\n\n\ntransformer-decoder-explaination\n\n\n\n\ntransformer-decoder-summary",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w2/index.html#sec-cross-entropy-loss",
    "href": "notes/c4w2/index.html#sec-cross-entropy-loss",
    "title": "Week 2 - Text Summarization",
    "section": "Cross entropy loss",
    "text": "Cross entropy loss\nCross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label.\n\n\n\n\ninference\n\nAfter training GPT2 on summarization data we just treat it like a word model and mine it for summaries. We do this by supply it with an input and predicting the output token by token. A more sophisticated method might be to use a beam search.\nAn even more sophisticated method might be to use an information metric to reject sentences and back track or better yet to do negative sampling from the prediction distribution (i.e. erase some prediction’s probabilities and renormalize)\nOne could do even better by providing hints, especially if we also head some kind of extractive model with a high-level of certainty about the most important sentence and their most significant concepts.\n\n\n\n\nquiz\n\nWe want the model to be penalized if it makes the wrong prediction. In this case it it does not predict the next word in the summary.\nThis may not be ideal for a number of reasons:\n\nthe Big world view “we are interested in a summary not the next word” what if the model is generating a semantically equivalent summary, in such a case it should not be penalized at all.\n\nIn a previous assignment we used a siamese network to check if two queries were equivalent. I think that allowing the network would be beneficial. (A loss that examines a generated sequence and compares it to the output.) But I don’t really know how to back-propagate the outcome for all the words. Well not exactly\nAs we are using teacher forcing we can take a position that we ignore all the mistakes the model made and give it a good output sequence and ask it for the next word. This then allows us to back prop the last word’s loss all by itself.\nIf we do this for each word in the output in sequence we should be able to reuse most of the calculations.\nThere are cases we have multiple summaries:\n\nFor a wikipedia article we often have all version from inception to the current day. This can provide multiple summaries and text along with an a golden version (the current summary). Oh and we may have a better summary in other languages but that is a different story.\nFor IMDB movie plots we often have a long synopsis and multiple shorter summaries. Also we may also have the book or screen play.\n\nI mention these two cases since Curriculum Learning may be able to assist us in training\n\n\n\n\nsummary\n\nI think these is much missing from this lesson about summerization. However there are a number of good source in papers as well as some lectures on YouTube.\nI have quickly summarized one and should link to it from here once it is published.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w2/index.html#sec-lab1-attention",
    "href": "notes/c4w2/index.html#sec-lab1-attention",
    "title": "Week 2 - Text Summarization",
    "section": "Lab1 : Attention",
    "text": "Lab1 : Attention\nThis was a numpy based realization of dot product and multi-head attention. Some of the main assignment required porting this to Jax.\nAttention lab",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w2/index.html#lab2-the-transformer-decoder",
    "href": "notes/c4w2/index.html#lab2-the-transformer-decoder",
    "title": "Week 2 - Text Summarization",
    "section": "Lab2 : The Transformer Decoder",
    "text": "Lab2 : The Transformer Decoder\nthis covered the transformer block\nTransformer block lab",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w2/index.html#assignment-transformer-summarizer",
    "href": "notes/c4w2/index.html#assignment-transformer-summarizer",
    "title": "Week 2 - Text Summarization",
    "section": "Assignment: Transformer Summarizer",
    "text": "Assignment: Transformer Summarizer\n\nThis long assignment primarily focused on dot product attention, multi-head attention and on building the transformer blocks. These were manageable as their theory had been explained in the lectures and their code had already been covered in the labs. It glosses over the parts involving data processing, training, evaluation and the actual summarization task. The summarization is accomplished using maximum likelihood estimate. A beam search might have yielded better results.\nThe date as described by:\n\nWe use the CNN/Daily Mail dataset (Hermann et al., 2015; Nallapati et al., 2016), which contains online news articles (781 tokens on average) paired with multi-sentence summaries (3.75 sentences or 56 tokens on average).\n\n\nSee, Liu, and Manning (2017) Get To The Point §4 (Abigail et all 2017) We used the non anatomized version. However the earlier paper used a preprocessed version which replaced the named entities with token like $entity5. This is probably ideal in other situations like event processing where each event looks quite different unless one anatomizes them rendering them much more similar and hopefully helping the model generalize better by learning from the partitions induced by the equivalency relation.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w2/index.html#sec-expanding-the-lab-to-a-project",
    "href": "notes/c4w2/index.html#sec-expanding-the-lab-to-a-project",
    "title": "Week 2 - Text Summarization",
    "section": "Expanding the lab to a project:",
    "text": "Expanding the lab to a project:\nThis is one of the main areas I’d like to focus on for a project. I have in mind a tool for improving wikipedia article leads. Here is how I’d like to take this project to the next level:\n\nmore data\ntrain it on additional material:\n\npapers and abstracts.\nwikipedia articles (with good first paragraphs. )\nbooks and book summaries (could be problematic due to the book’s length)\nmovie scripts and outlines from IMDB\n\na Storyline\nsummary (paragraph)\na synopsis (longer)\n\n\n\n\nMore algorithms\n\nUsing a reformer to handle longer texts like books.\nBetter summarization using:\n\na beam search to build better summaries.\na bayesian search to avoid repetitions.\n\nuse curriculum learning to speed up training with\n\neasier examples first.\nmultiple summaries per text.\nlearning on anonymized NE before graduating to non-anonymized texts\n\nuse better method for evaluation of summary.\n\nPerhaps an f-score combining precision or recall on\nAttention Activation summed as a Coverage score for each token.\n\nuse of non zero loss-weights layer\n\ndrop to zero as training progresses.\ndepend on the actual length of source and output.\nuse tf-idf to make holes in the mask surrounding essential concepts.\n\n\n\n\nEvaluation\nuse sammy lib with\n\nrouge-n metric\nthe pyramid metric\n\n\n\nExtra features\n\npages for paper reviews\npages for research questions\npages to implement exra code/ experiments.\nvisualize the concepts/sentences/paragraphs/sections covered in the summary.\nestablish a hierarchy of what would go into a longer outline.\ndevelop a f-score metric combining precision and recall for the summary of the abstract.\nin academic writing one sentence per paragraphs should capture the main concept and it is generally the first second or the last. Is such a sentence is available identify it. This would be done by comparing each sentence with the rest of the paragraph.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w2/index.html#sec-open-question",
    "href": "notes/c4w2/index.html#sec-open-question",
    "title": "Week 2 - Text Summarization",
    "section": "Open question",
    "text": "Open question\nFor me the assignment raised a number of questions about what really going on here during training.\nI’ll probably do this assignment again and look for some answers to my many questions. Once I have these I’ll add them in the body of these notes.\n\nQuestions\n\nLoading and prepocessing the data:\n\nWhat is going on after we load the dataset - is there data augmentation?\nWhat this sub word vocab?\nHow to make my own sub word vocab?\nHow are out of vocab words being handled?\nCan we query the model about these beside running decode ?\nHow are these created - I saw several sizes of vocab.\n\nTraining\n\nTraining data seems to be a little mangled - there seems to be missing white space after the first token of the summaries, is there some way to fix this?\nIn not sure but why do we use teacher forcing during training?\n\n\nIt should speed training up, but the setup is unclear.\n\nEvaluation\n\nWhy are we not looking at a summarization metic like pyramid, rouge5 or good old precision and recall.\n\nInference\n\nHow can we tell the model thinks its done?\n\n\nwhen it output and  token\n\n\nHow to generate one sentence for each paragraph/section\n\n\nChop up the input and summarise each section.\nCreate an new dataset that bases it summaries on the last and first sentences of each paragraph. If that’s too long summarize again for each section.\nIntroduce a timed mask that hides [0:t*len/T] where T is total number of tokens being generated.\nmake the mask a Bayesian search mechanism that hides concepts in the output.\n\n\nHow to use multiple summaries like in IMDB?\n\n\nscore using the pyramid scheme or rogue.\n\n\nHow to make the model avoid repeating /rephrasing themselves?\n\n\nuse a metric on new information. for example Maximal marginal relevance. MMR = \\argmax [\\lambda Sim_1(s_i,Q)- (1 - \\lambda) \\max Sim_2(s_i,s_j)] where Q is the query and s are output sentences and try to bake this into the regularization.\na coverage vector seems to be a recommend method.\n\nVisualization\n\n\nIs there a easy way to see the activation for each word in the output?\nIs there a easy way to see which concepts are significant (not too common and not too rare)\nIs there a easy way to see which concepts are salient - aligned to near by concepts.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w2/index.html#references",
    "href": "notes/c4w2/index.html#references",
    "title": "Week 2 - Text Summarization",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w2/index.html#papers",
    "href": "notes/c4w2/index.html#papers",
    "title": "Week 2 - Text Summarization",
    "section": "Papers",
    "text": "Papers\n\nTransformers\n\nExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Raffel et al, 2019)\n\nReformer: The Efficient Transformer (Kitaev et al, 2020)\nAttention Is All We Need (Vaswani et al, 2017)\nDeep contextualized word representations (Peters et al, 2018)\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al, 2018)\nFinetuning Pretrained Transformers into RNNs (Kasai et all 2021)\n\n\n\nSummarization\n\nA trainable document summarizer. (Kupiec et al., 1995) extractive\nConstructing literature abstracts by computer: techniques and prospects. (Paice, 1990) extractive\nAutomatic text summarization: Past, present and future (Saggion and Poibeau, 2013) extractive\nAbstractive sentence summarization with attentive recurrent neural networks. (Chopra et al., 2016) abstractive summarization\nPointing the unknown words. (Nallapati et al., 2016) abstractive summarization\nA neural attention model for abstractive sentence summarization. (Rush et al.,2015;) abstractive summarization\nEfficient summarization with read-again and copy mechanism(Zeng et al., 2016) abstractive summarization\nGet To The Point: Summarization with Pointer-Generator Networks (Abigail et all 2017) Hybrid summarization. Note: Christopher D. Manning\nExtractive Summarization as Text Matching (Zhong et all 2020)",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w2/index.html#articles",
    "href": "notes/c4w2/index.html#articles",
    "title": "Week 2 - Text Summarization",
    "section": "Articles",
    "text": "Articles\n\nThe Illustrated Transformer (Alammar, 2018)\nThe Illustrated GPT-2 (Alammar, 2019)\nHow GPT3 Works - Visualizations and Animations (Alammar, 2020)\nAttention? Attention! (Lilian Weng, 2018)\nThe Transformer Family (Lilian Weng, 2020)\nTeacher forcing for RNNs",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c4w2/index.html#links",
    "href": "notes/c4w2/index.html#links",
    "title": "Week 2 - Text Summarization",
    "section": "Links",
    "text": "Links\n\nJax\nTrax\nTrax community on Gitter\nCNN daily mail dataset",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Text Summarization",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c3w4/lab03.html",
    "href": "notes/c3w4/lab03.html",
    "title": "Evaluate a Siamese model: Ungraded Lecture Notebook",
    "section": "",
    "text": "course banner\nimport trax.fastmath.numpy as np\n\n2025-02-10 16:56:19.897139: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1739199379.909590  124134 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1739199379.913717  124134 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "L3 - Evaluate a Siamese Model"
    ]
  },
  {
    "objectID": "notes/c3w4/lab03.html#inspecting-the-necessary-elements",
    "href": "notes/c3w4/lab03.html#inspecting-the-necessary-elements",
    "title": "Evaluate a Siamese model: Ungraded Lecture Notebook",
    "section": "Inspecting the necessary elements",
    "text": "Inspecting the necessary elements\nIn this lecture notebook you will learn how to evaluate a Siamese model using the accuracy metric. Because there are many steps before evaluating a Siamese network (as you will see in this week’s assignment) the necessary elements and variables are replicated here using real data from the assignment:\n\nq1: vector with dimension (batch_size X max_length) containing first questions to compare in the test set.\nq2: vector with dimension (batch_size X max_length) containing second questions to compare in the test set.\n\nNotice that for each pair of vectors within a batch ([q1_1, q1_2, q1_3, ...], [q2_1, q2_2,q2_3, ...]) q1_i is associated to q2_k.\n\ny_test: 1 if q1_i and q2_k are duplicates, 0 otherwise.\nv1: output vector from the model’s prediction associated with the first questions.\nv2: output vector from the model’s prediction associated with the second questions.\n\nYou can inspect each one of these variables by running the following cells:\n\nq1 = np.load('q1.npy')\nprint(f'q1 has shape: {q1.shape} \\n\\nAnd it looks like this: \\n\\n {q1}\\n\\n')\n\nq1 has shape: (512, 64) \n\nAnd it looks like this: \n\n [[ 32  38   4 ...   1   1   1]\n [ 30 156  78 ...   1   1   1]\n [ 32  38   4 ...   1   1   1]\n ...\n [ 32  33   4 ...   1   1   1]\n [ 30 156 317 ...   1   1   1]\n [ 30 156   6 ...   1   1   1]]\n\n\n\n\nNotice those 1s on the right-hand side?\nHope you remember that the value of 1 was used for padding.\n\nq2 = np.load('q2.npy')\nprint(f'q2 has shape: {q2.shape} \\n\\nAnd looks like this: \\n\\n {q2}\\n\\n')\n\nq2 has shape: (512, 64) \n\nAnd looks like this: \n\n [[   30   156    78 ...     1     1     1]\n [  283   156    78 ...     1     1     1]\n [   32    38     4 ...     1     1     1]\n ...\n [   32    33     4 ...     1     1     1]\n [   30   156    78 ...     1     1     1]\n [   30   156 10596 ...     1     1     1]]\n\n\n\n\n\ny_test = np.load('y_test.npy')\nprint(f'y_test has shape: {y_test.shape} \\n\\nAnd looks like this: \\n\\n {y_test}\\n\\n')\n\ny_test has shape: (512,) \n\nAnd looks like this: \n\n [0 1 1 0 0 0 0 1 0 1 1 0 0 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0\n 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0\n 0 0 0 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0\n 0 1 0 0 1 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0\n 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 1 1\n 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1\n 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1\n 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1 1 0 0\n 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0\n 0 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1 0\n 1 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1\n 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1\n 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n\n\n\n\n\nv1 = np.load('v1.npy')\nprint(f'v1 has shape: {v1.shape} \\n\\nAnd looks like this: \\n\\n {v1}\\n\\n')\nv2 = np.load('v2.npy')\nprint(f'v2 has shape: {v2.shape} \\n\\nAnd looks like this: \\n\\n {v2}\\n\\n')\n\nv1 has shape: (512, 128) \n\nAnd looks like this: \n\n [[ 0.01273625 -0.1496373  -0.01982759 ...  0.02205012 -0.00169148\n  -0.01598107]\n [-0.05592084  0.05792497 -0.02226785 ...  0.08156938 -0.02570007\n  -0.00503111]\n [ 0.05686752  0.0294889   0.04522024 ...  0.03141788 -0.08459651\n  -0.00968536]\n ...\n [ 0.15115018  0.17791134  0.02200656 ... -0.00851707  0.00571415\n  -0.00431194]\n [ 0.06995274  0.13110274  0.0202337  ... -0.00902792 -0.01221745\n   0.00505962]\n [-0.16043712 -0.11899089 -0.15950686 ...  0.06544471 -0.01208312\n  -0.01183368]]\n\n\nv2 has shape: (512, 128) \n\nAnd looks like this: \n\n [[ 0.07437647  0.02804951 -0.02974014 ...  0.02378932 -0.01696189\n  -0.01897198]\n [ 0.03270066  0.15122835 -0.02175895 ...  0.00517202 -0.14617395\n   0.00204823]\n [ 0.05635608  0.05454165  0.042222   ...  0.03831453 -0.05387777\n  -0.01447786]\n ...\n [ 0.04727105 -0.06748016  0.04194937 ...  0.07600753 -0.03072828\n   0.00400715]\n [ 0.00269269  0.15222628  0.01714724 ...  0.01482705 -0.0197884\n   0.01389528]\n [-0.15475044 -0.15718803 -0.14732707 ...  0.04299919 -0.01070975\n  -0.01318042]]",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "L3 - Evaluate a Siamese Model"
    ]
  },
  {
    "objectID": "notes/c3w4/lab03.html#calculating-the-accuracy",
    "href": "notes/c3w4/lab03.html#calculating-the-accuracy",
    "title": "Evaluate a Siamese model: Ungraded Lecture Notebook",
    "section": "Calculating the accuracy",
    "text": "Calculating the accuracy\nYou will calculate the accuracy by iterating over the test set and checking if the model predicts right or wrong.\nThe first step is to set the accuracy to zero:\n\naccuracy = 0\n\nYou will also need the batch size and the threshold that determines if two questions are the same or not.\nNote :A higher threshold means that only very similar questions will be considered as the same question.\n\nbatch_size = 512 # Note: The max it can be is y_test.shape[0] i.e all the samples in test data\nthreshold = 0.7 # You can play around with threshold and then see the change in accuracy.\n\nIn the assignment you will iterate over multiple batches of data but since this is a simplified version only one batch is provided.\nNote: Be careful with the indices when slicing the test data in the assignment!\nThe process is pretty straightforward: - Iterate over each one of the elements in the batch - Compute the cosine similarity between the predictions - For computing the cosine similarity, the two output vectors should have been normalized using L2 normalization meaning their magnitude will be 1. This has been taken care off by the Siamese network you will build in the assignment. Hence the cosine similarity here is just dot product between two vectors. You can check by implementing the usual cosine similarity formula and check if this holds or not. - Determine if this value is greater than the threshold (If it is, consider the two questions as the same and return 1 else 0) - Compare against the actual target and if the prediction matches, add 1 to the accuracy (increment the correct prediction counter) - Divide the accuracy by the number of processed elements\n\nfor j in range(batch_size):        # Iterate over each one of the elements in the batch\n    \n    d = np.dot(v1[j],v2[j])        # Compute the cosine similarity between the predictions as l2 normalized, ||v1[j]||==||v2[j]||==1 so only dot product is needed\n    res = d &gt; threshold            # Determine if this value is greater than the threshold (if it is consider the two questions as the same)\n    accuracy += (y_test[j] == res) # Compare against the actual target and if the prediction matches, add 1 to the accuracy\n\naccuracy = accuracy / batch_size   # Divide the accuracy by the number of processed elements\n\n\nprint(f'The accuracy of the model is: {accuracy}')\n\nThe accuracy of the model is: 0.7421875\n\n\nCongratulations on finishing this lecture notebook!\nNow you should have a clearer understanding of how to evaluate your Siamese language models using the accuracy metric.\nKeep it up!",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "L3 - Evaluate a Siamese Model"
    ]
  },
  {
    "objectID": "notes/c3w4/lab02.html",
    "href": "notes/c3w4/lab02.html",
    "title": "Modified Triplet Loss",
    "section": "",
    "text": "course banner\nIn this notebook you’ll see how to calculate the full triplet loss, step by step, including the mean negative and the closest negative. You’ll also calculate the matrix of similarity scores.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "L2 - Modified Triplet Loss"
    ]
  },
  {
    "objectID": "notes/c3w4/lab02.html#background",
    "href": "notes/c3w4/lab02.html#background",
    "title": "Modified Triplet Loss",
    "section": "Background",
    "text": "Background\nThis is the original triplet loss function:\n\n\\mathcal{L_\\mathrm{Original}} = \\max{(\\mathrm{s}(A,N) -\\mathrm{s}(A,P) +\\alpha, 0)}\n\nIt can be improved by including the mean negative and the closest negative, to create a new full loss function. The inputs are the Anchor \\mathrm{A}, Positive \\mathrm{P} and Negative \\mathrm{N}.\n\n\\mathcal{L_\\mathrm{1}} = \\max{(mean\\_neg -\\mathrm{s}(A,P)  +\\alpha, 0)}\n\n\n\\mathcal{L_\\mathrm{2}} = \\max{(closest\\_neg -\\mathrm{s}(A,P)  +\\alpha, 0)}\n\n\n\\mathcal{L_\\mathrm{Full}} = \\mathcal{L_\\mathrm{1}} + \\mathcal{L_\\mathrm{2}}\n\nLet me show you what that means exactly, and how to calculate each step.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "L2 - Modified Triplet Loss"
    ]
  },
  {
    "objectID": "notes/c3w4/lab02.html#imports",
    "href": "notes/c3w4/lab02.html#imports",
    "title": "Modified Triplet Loss",
    "section": "Imports",
    "text": "Imports\n\nimport numpy as np",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "L2 - Modified Triplet Loss"
    ]
  },
  {
    "objectID": "notes/c3w4/lab02.html#similarity-scores",
    "href": "notes/c3w4/lab02.html#similarity-scores",
    "title": "Modified Triplet Loss",
    "section": "Similarity Scores",
    "text": "Similarity Scores\nThe first step is to calculate the matrix of similarity scores using cosine similarity so that you can look up \\mathrm{s}(A,P), \\mathrm{s}(A,N) as needed for the loss formulas.\n\nTwo Vectors\nFirst I’ll show you how to calculate the similarity score, using cosine similarity, for 2 vectors.\n\\mathrm{s}(v_1,v_2) = \\mathrm{cosine \\ similarity}(v_1,v_2) = \\frac{v_1 \\cdot v_2}{||v_1||~||v_2||} * Try changing the values in the second vector to see how it changes the cosine similarity.\n\n# Two vector example\n# Input data\nprint(\"-- Inputs --\")\nv1 = np.array([1, 2, 3], dtype=float)\nv2 = np.array([1, 2, 3.5])  # notice the 3rd element is offset by 0.5\n### START CODE HERE ###\n# Try modifying the vector v2 to see how it impacts the cosine similarity\n# v2 = v1                   # identical vector\n# v2 = v1 * -1              # opposite vector\n# v2 = np.array([0,-42,1])  # random example\n### END CODE HERE ###\nprint(\"v1 :\", v1)\nprint(\"v2 :\", v2, \"\\n\")\n\n# Similarity score\ndef cosine_similarity(v1, v2):\n    numerator = np.dot(v1, v2)\n    denominator = np.sqrt(np.dot(v1, v1)) * np.sqrt(np.dot(v2, v2))\n    return numerator / denominator\n\nprint(\"-- Outputs --\")\nprint(\"cosine similarity :\", cosine_similarity(v1, v2))\n\n-- Inputs --\nv1 : [1. 2. 3.]\nv2 : [1.  2.  3.5] \n\n-- Outputs --\ncosine similarity : 0.9974086507360697\n\n\n\n\nTwo Batches of Vectors\nNow i’ll show you how to calculate the similarity scores, using cosine similarity, for 2 batches of vectors. These are rows of individual vectors, just like in the example above, but stacked vertically into a matrix. They would look like the image below for a batch size (row count) of 4 and embedding size (column count) of 5.\nThe data is setup so that v_{1\\_1} and v_{2\\_1} represent duplicate inputs, but they are not duplicates with any other rows in the batch. This means v_{1\\_1} and v_{2\\_1} (green and green) have more similar vectors than say v_{1\\_1} and v_{2\\_2} (green and magenta).\nI’ll show you two different methods for calculating the matrix of similarities from 2 batches of vectors.\n\n\n# Two batches of vectors example\n# Input data\nprint(\"-- Inputs --\")\nv1_1 = np.array([1, 2, 3])\nv1_2 = np.array([9, 8, 7])\nv1_3 = np.array([-1, -4, -2])\nv1_4 = np.array([1, -7, 2])\nv1 = np.vstack([v1_1, v1_2, v1_3, v1_4])\nprint(\"v1 :\")\nprint(v1, \"\\n\")\nv2_1 = v1_1 + np.random.normal(0, 2, 3)  # add some noise to create approximate duplicate\nv2_2 = v1_2 + np.random.normal(0, 2, 3)\nv2_3 = v1_3 + np.random.normal(0, 2, 3)\nv2_4 = v1_4 + np.random.normal(0, 2, 3)\nv2 = np.vstack([v2_1, v2_2, v2_3, v2_4])\nprint(\"v2 :\")\nprint(v2, \"\\n\")\n\n# Batch sizes must match\nb = len(v1)\nprint(\"batch sizes match :\", b == len(v2), \"\\n\")\n\n# Similarity scores\nprint(\"-- Outputs --\")\n# Option 1 : nested loops and the cosine similarity function\nsim_1 = np.zeros([b, b])  # empty array to take similarity scores\n# Loop\nfor row in range(0, sim_1.shape[0]):\n    for col in range(0, sim_1.shape[1]):\n        sim_1[row, col] = cosine_similarity(v1[row], v2[col])\n\nprint(\"option 1 : loop\")\nprint(sim_1, \"\\n\")\n\n# Option 2 : vector normalization and dot product\ndef norm(x):\n    return x / np.sqrt(np.sum(x * x, axis=1, keepdims=True))\n\nsim_2 = np.dot(norm(v1), norm(v2).T)\n\nprint(\"option 2 : vec norm & dot product\")\nprint(sim_2, \"\\n\")\n\n# Check\nprint(\"outputs are the same :\", np.allclose(sim_1, sim_2))\n\n-- Inputs --\nv1 :\n[[ 1  2  3]\n [ 9  8  7]\n [-1 -4 -2]\n [ 1 -7  2]] \n\nv2 :\n[[-1.39930492  5.1153898   6.63335538]\n [ 5.02945534  8.32623907  3.55335285]\n [-2.1468471  -5.40282568  1.08523362]\n [ 3.98566581 -8.41659442 -1.33038683]] \n\nbatch sizes match : True \n\n-- Outputs --\noption 1 : loop\n[[ 0.90416347  0.83465723 -0.43819935 -0.47839422]\n [ 0.63202915  0.94804086 -0.66704516 -0.31119261]\n [-0.83068087 -0.95751319  0.79653304  0.75022602]\n [-0.38360581 -0.6063967   0.87076444  0.87143998]] \n\noption 2 : vec norm & dot product\n[[ 0.90416347  0.83465723 -0.43819935 -0.47839422]\n [ 0.63202915  0.94804086 -0.66704516 -0.31119261]\n [-0.83068087 -0.95751319  0.79653304  0.75022602]\n [-0.38360581 -0.6063967   0.87076444  0.87143998]] \n\noutputs are the same : True",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "L2 - Modified Triplet Loss"
    ]
  },
  {
    "objectID": "notes/c3w4/lab02.html#hard-negative-mining",
    "href": "notes/c3w4/lab02.html#hard-negative-mining",
    "title": "Modified Triplet Loss",
    "section": "Hard Negative Mining",
    "text": "Hard Negative Mining\nI’ll now show you how to calculate the mean negative mean\\_neg and the closest negative close\\_neg used in calculating \\mathcal{L_\\mathrm{1}} and \\mathcal{L_\\mathrm{2}}.\n\\mathcal{L_\\mathrm{1}} = \\max{(mean\\_neg -\\mathrm{s}(A,P)  +\\alpha, 0)}\n\\mathcal{L_\\mathrm{2}} = \\max{(closest\\_neg -\\mathrm{s}(A,P)  +\\alpha, 0)}\nYou’ll do this using the matrix of similarity scores you already know how to make, like the example below for a batch size of 4. The diagonal of the matrix contains all the \\mathrm{s}(A,P) values, similarities from duplicate question pairs (aka Positives). This is an important attribute for the calculations to follow.\n\n\nMean Negative\nmean\\_neg is the average of the off diagonals, the \\mathrm{s}(A,N) values, for each row.\n\n\nClosest Negative\nclosest\\_neg is the largest off diagonal value, \\mathrm{s}(A,N), that is smaller than the diagonal \\mathrm{s}(A,P) for each row. * Try using a different matrix of similarity scores.\n\n# Hardcoded matrix of similarity scores\nsim_hardcoded = np.array(\n    [\n        [0.9, -0.8, 0.3, -0.5],\n        [-0.4, 0.5, 0.1, -0.1],\n        [0.3, 0.1, -0.4, -0.8],\n        [-0.5, -0.2, -0.7, 0.5],\n    ]\n)\n\nsim = sim_hardcoded\n### START CODE HERE ###\n# Try using different values for the matrix of similarity scores\n# sim = 2 * np.random.random_sample((b,b)) -1   # random similarity scores between -1 and 1\n# sim = sim_2                                   # the matrix calculated previously\n### END CODE HERE ###\n\n# Batch size\nb = sim.shape[0]\n\nprint(\"-- Inputs --\")\nprint(\"sim :\")\nprint(sim)\nprint(\"shape :\", sim.shape, \"\\n\")\n\n# Positives\n# All the s(A,P) values : similarities from duplicate question pairs (aka Positives)\n# These are along the diagonal\nsim_ap = np.diag(sim)\nprint(\"sim_ap :\")\nprint(np.diag(sim_ap), \"\\n\")\n\n# Negatives\n# all the s(A,N) values : similarities the non duplicate question pairs (aka Negatives)\n# These are in the off diagonals\nsim_an = sim - np.diag(sim_ap)\nprint(\"sim_an :\")\nprint(sim_an, \"\\n\")\n\nprint(\"-- Outputs --\")\n# Mean negative\n# Average of the s(A,N) values for each row\nmean_neg = np.sum(sim_an, axis=1, keepdims=True) / (b - 1)\nprint(\"mean_neg :\")\nprint(mean_neg, \"\\n\")\n\n# Closest negative\n# Max s(A,N) that is &lt;= s(A,P) for each row\nmask_1 = np.identity(b) == 1            # mask to exclude the diagonal\nmask_2 = sim_an &gt; sim_ap.reshape(b, 1)  # mask to exclude sim_an &gt; sim_ap\nmask = mask_1 | mask_2\nsim_an_masked = np.copy(sim_an)         # create a copy to preserve sim_an\nsim_an_masked[mask] = -2\n\nclosest_neg = np.max(sim_an_masked, axis=1, keepdims=True)\nprint(\"closest_neg :\")\nprint(closest_neg, \"\\n\")\n\n-- Inputs --\nsim :\n[[ 0.9 -0.8  0.3 -0.5]\n [-0.4  0.5  0.1 -0.1]\n [ 0.3  0.1 -0.4 -0.8]\n [-0.5 -0.2 -0.7  0.5]]\nshape : (4, 4) \n\nsim_ap :\n[[ 0.9  0.   0.   0. ]\n [ 0.   0.5  0.   0. ]\n [ 0.   0.  -0.4  0. ]\n [ 0.   0.   0.   0.5]] \n\nsim_an :\n[[ 0.  -0.8  0.3 -0.5]\n [-0.4  0.   0.1 -0.1]\n [ 0.3  0.1  0.  -0.8]\n [-0.5 -0.2 -0.7  0. ]] \n\n-- Outputs --\nmean_neg :\n[[-0.33333333]\n [-0.13333333]\n [-0.13333333]\n [-0.46666667]] \n\nclosest_neg :\n[[ 0.3]\n [ 0.1]\n [-0.8]\n [-0.2]]",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "L2 - Modified Triplet Loss"
    ]
  },
  {
    "objectID": "notes/c3w4/lab02.html#the-loss-functions",
    "href": "notes/c3w4/lab02.html#the-loss-functions",
    "title": "Modified Triplet Loss",
    "section": "The Loss Functions",
    "text": "The Loss Functions\nThe last step is to calculate the loss functions.\n\\mathcal{L_\\mathrm{1}} = \\max{(mean\\_neg -\\mathrm{s}(A,P)  +\\alpha, 0)}\n\\mathcal{L_\\mathrm{2}} = \\max{(closest\\_neg -\\mathrm{s}(A,P)  +\\alpha, 0)}\n\\mathcal{L_\\mathrm{Full}} = \\mathcal{L_\\mathrm{1}} + \\mathcal{L_\\mathrm{2}}\n\n# Alpha margin\nalpha = 0.25\n\n# Modified triplet loss\n# Loss 1\nl_1 = np.maximum(mean_neg - sim_ap.reshape(b, 1) + alpha, 0)\n# Loss 2\nl_2 = np.maximum(closest_neg - sim_ap.reshape(b, 1) + alpha, 0)\n# Loss full\nl_full = l_1 + l_2\n# Cost\ncost = np.sum(l_full)\n\nprint(\"-- Outputs --\")\nprint(\"loss full :\")\nprint(l_full, \"\\n\")\nprint(\"cost :\", \"{:.3f}\".format(cost))\n\n-- Outputs --\nloss full :\n[[0.        ]\n [0.        ]\n [0.51666667]\n [0.        ]] \n\ncost : 0.517",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "L2 - Modified Triplet Loss"
    ]
  },
  {
    "objectID": "notes/c3w4/lab02.html#summary",
    "href": "notes/c3w4/lab02.html#summary",
    "title": "Modified Triplet Loss",
    "section": "Summary",
    "text": "Summary\nThere were a lot of steps in there, so well done. You now know how to calculate a modified triplet loss, incorporating the mean negative and the closest negative. You also learned how to create a matrix of similarity scores based on cosine similarity.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "Siamese Networks",
      "L2 - Modified Triplet Loss"
    ]
  },
  {
    "objectID": "notes/c3w2/lab03.html",
    "href": "notes/c3w2/lab03.html",
    "title": "Vanilla RNNs, GRUs and the scan function",
    "section": "",
    "text": "course banner\nIn this notebook, you will learn how to define the forward method for vanilla RNNs and GRUs. Additionally, you will see how to define and use the function scan to compute forward propagation for RNNs.\nBy completing this notebook, you will:\nimport numpy as np\nfrom numpy import random\nfrom time import perf_counter\nAn implementation of the sigmoid function is provided below so you can use it in this notebook.\ndef sigmoid(x): # Sigmoid function\n    return 1.0 / (1.0 + np.exp(-x))",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "L3 - Vanilla RNNs, GRUs and the scan function"
    ]
  },
  {
    "objectID": "notes/c3w2/lab03.html#forward-method-for-vanilla-rnns",
    "href": "notes/c3w2/lab03.html#forward-method-for-vanilla-rnns",
    "title": "Vanilla RNNs, GRUs and the scan function",
    "section": "1.1 Forward method for vanilla RNNs",
    "text": "1.1 Forward method for vanilla RNNs\nThe vanilla RNN cell is quite straight forward. Its most general structure is presented in the next figure:\n\nAs you saw in the lecture videos, the computations made in a vanilla RNN cell are equivalent to the following equations:\n\\begin{equation}\nh^{&lt;t&gt;}=g(W_{h}[h^{&lt;t-1&gt;},x^{&lt;t&gt;}] + b_h)\n\\label{eq: htRNN}\n\\end{equation}\n\\begin{equation}\n\\hat{y}^{&lt;t&gt;}=g(W_{yh}h^{&lt;t&gt;} + b_y)\n\\label{eq: ytRNN}\n\\end{equation}\nwhere [h^{&lt;t-1&gt;},x^{&lt;t&gt;}] means that h^{&lt;t-1&gt;} and x^{&lt;t&gt;} are concatenated together. In the next cell we provide the implementation of the forward method for a vanilla RNN.\n\ndef forward_V_RNN(inputs, weights): # Forward propagation for a a single vanilla RNN cell\n    x, h_t = inputs\n\n    # weights.\n    wh, _, _, bh, _, _ = weights\n\n    # new hidden state\n    h_t = np.dot(wh, np.concatenate([h_t, x])) + bh\n    h_t = sigmoid(h_t)\n\n    return h_t, h_t\n\nAs you can see, we omitted the computation of \\hat{y}^{&lt;t&gt;}. This was done for the sake of simplicity, so you can focus on the way that hidden states are updated here and in the GRU cell.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "L3 - Vanilla RNNs, GRUs and the scan function"
    ]
  },
  {
    "objectID": "notes/c3w2/lab03.html#forward-method-for-grus",
    "href": "notes/c3w2/lab03.html#forward-method-for-grus",
    "title": "Vanilla RNNs, GRUs and the scan function",
    "section": "1.2 Forward method for GRUs",
    "text": "1.2 Forward method for GRUs\nA GRU cell have more computations than the ones that vanilla RNNs have. You can see this visually in the following diagram:\n\nAs you saw in the lecture videos, GRUs have relevance \\Gamma_r and update \\Gamma_u gates that control how the hidden state h^{&lt;t&gt;} is updated on every time step. With these gates, GRUs are capable of keeping relevant information in the hidden state even for long sequences. The equations needed for the forward method in GRUs are provided below:\n\\begin{equation}\n\\Gamma_r=\\sigma{(W_r[h^{&lt;t-1&gt;}, x^{&lt;t&gt;}]+b_r)}\n\\end{equation}\n\\begin{equation}\n\\Gamma_u=\\sigma{(W_u[h^{&lt;t-1&gt;}, x^{&lt;t&gt;}]+b_u)}\n\\end{equation}\n\\begin{equation}\nc^{&lt;t&gt;}=\\tanh{(W_h[\\Gamma_r*h^{&lt;t-1&gt;},x^{&lt;t&gt;}]+b_h)}\n\\end{equation}\n\\begin{equation}\nh^{&lt;t&gt;}=\\Gamma_u*c^{&lt;t&gt;}+(1-\\Gamma_u)*h^{&lt;t-1&gt;}\n\\end{equation}\nIn the next cell, please implement the forward method for a GRU cell by computing the update u and relevance r gates, and the candidate hidden state c.\n\ndef forward_GRU(inputs, weights): # Forward propagation for a single GRU cell\n    x, h_t = inputs\n\n    # weights.\n    wu, wr, wc, bu, br, bc = weights\n\n    # Update gate\n    ### START CODE HERE (1-2 lINES) ###\n    u = np.dot(wu, np.concatenate([h_t, x])) + bu\n    u = sigmoid(u)\n    ### END CODE HERE ###\n    \n    # Relevance gate\n    ### START CODE HERE (1-2 lINES) ###\n    r = np.dot(wr, np.concatenate([h_t, x])) + br\n    r = sigmoid(u)\n    ### END CODE HERE ###\n    \n    # Candidate hidden state \n    ### START CODE HERE (1-2 lINES) ###\n    c = np.dot(wc, np.concatenate([r * h_t, x])) + bc\n    c = np.tanh(c)\n    ### END CODE HERE ###\n    \n    # New Hidden state h_t\n    h_t = u* c + (1 - u)* h_t\n    return h_t, h_t\n\nRun the following cell to check your implementation.\n\nforward_GRU([X[1],h_0], weights)[0]\n\narray([[ 9.77779014e-01],\n       [-9.97986240e-01],\n       [-5.19958083e-01],\n       [-9.99999886e-01],\n       [-9.99707004e-01],\n       [-3.02197037e-04],\n       [-9.58733503e-01],\n       [ 2.10804828e-02],\n       [ 9.77365398e-05],\n       [ 9.99833090e-01],\n       [ 1.63200940e-08],\n       [ 8.51874303e-01],\n       [ 5.21399924e-02],\n       [ 2.15495959e-02],\n       [ 9.99878828e-01],\n       [ 9.77165472e-01]])\n\n\nExpected output:\narray([[ 9.77779014e-01],\n       [-9.97986240e-01],\n       [-5.19958083e-01],\n       [-9.99999886e-01],\n       [-9.99707004e-01],\n       [-3.02197037e-04],\n       [-9.58733503e-01],\n       [ 2.10804828e-02],\n       [ 9.77365398e-05],\n       [ 9.99833090e-01],\n       [ 1.63200940e-08],\n       [ 8.51874303e-01],\n       [ 5.21399924e-02],\n       [ 2.15495959e-02],\n       [ 9.99878828e-01],\n       [ 9.77165472e-01]])",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "L3 - Vanilla RNNs, GRUs and the scan function"
    ]
  },
  {
    "objectID": "notes/c3w2/lab02.html",
    "href": "notes/c3w2/lab02.html",
    "title": "Working with JAX numpy and calculating perplexity: Ungraded Lecture Notebook",
    "section": "",
    "text": "course banner\nNormally you would import numpy and rename it as np.\nHowever in this week’s assignment you will notice that this convention has been changed.\nNow standard numpy is not renamed and trax.fastmath.numpy is renamed as np.\nThe rationale behind this change is that you will be using Trax’s numpy (which is compatible with JAX) far more often. Trax’s numpy supports most of the same functions as the regular numpy so the change won’t be noticeable in most cases.\nimport jax\nprint(jax.__version__)\n\n0.5.0\nimport numpy\nimport trax\nimport trax.fastmath.numpy as np\n\n# Setting random seeds\nfrom trax import fastmath\nseed=32\nrng = fastmath.random.get_prng(seed)\n#trax.supervised.trainer_lib.init_random_number_generators(32)\nnumpy.random.seed(32)\n\n2025-02-10 16:58:32.288612: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1739199512.303899  125772 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1739199512.308311  125772 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nOne important change to take into consideration is that the types of the resulting objects will be different depending on the version of numpy. With regular numpy you get numpy.ndarray but with Trax’s numpy you will get jax.interpreters.xla.DeviceArray. These two types map to each other. So if you find some error logs mentioning DeviceArray type, don’t worry about it, treat it like you would treat an ndarray and march ahead.\nYou can get a randomized numpy array by using the numpy.random.random() function.\nThis is one of the functionalities that Trax’s numpy does not currently support in the same way as the regular numpy.\nnumpy_array = numpy.random.random((5,10))\nprint(f\"The regular numpy array looks like this:\\n\\n {numpy_array}\\n\")\nprint(f\"It is of type: {type(numpy_array)}\")\n\nThe regular numpy array looks like this:\n\n [[0.85888927 0.37271115 0.55512878 0.95565655 0.7366696  0.81620514\n  0.10108656 0.92848807 0.60910917 0.59655344]\n [0.09178413 0.34518624 0.66275252 0.44171349 0.55148779 0.70371249\n  0.58940123 0.04993276 0.56179184 0.76635847]\n [0.91090833 0.09290995 0.90252139 0.46096041 0.45201847 0.99942549\n  0.16242374 0.70937058 0.16062408 0.81077677]\n [0.03514717 0.53488673 0.16650012 0.30841038 0.04506241 0.23857613\n  0.67483453 0.78238275 0.69520163 0.32895445]\n [0.49403187 0.52412136 0.29854125 0.46310814 0.98478429 0.50113492\n  0.39807245 0.72790532 0.86333097 0.02616954]]\n\nIt is of type: &lt;class 'numpy.ndarray'&gt;\nYou can easily cast regular numpy arrays or lists into trax numpy arrays using the trax.fastmath.numpy.array() function:\ntrax_numpy_array = np.array(numpy_array)\nprint(f\"The trax numpy array looks like this:\\n\\n {trax_numpy_array}\\n\")\nprint(f\"It is of type: {type(trax_numpy_array)}\")\n\nThe trax numpy array looks like this:\n\n [[0.8588893  0.37271115 0.55512875 0.9556565  0.7366696  0.81620514\n  0.10108656 0.9284881  0.60910916 0.59655344]\n [0.09178413 0.34518623 0.6627525  0.44171348 0.5514878  0.70371246\n  0.58940125 0.04993276 0.56179184 0.7663585 ]\n [0.91090834 0.09290995 0.9025214  0.46096042 0.45201847 0.9994255\n  0.16242374 0.7093706  0.16062407 0.81077677]\n [0.03514718 0.5348867  0.16650012 0.30841038 0.04506241 0.23857613\n  0.67483455 0.7823827  0.69520164 0.32895446]\n [0.49403188 0.52412134 0.29854125 0.46310815 0.9847843  0.50113493\n  0.39807245 0.72790533 0.86333096 0.02616954]]\n\nIt is of type: &lt;class 'jaxlib.xla_extension.ArrayImpl'&gt;\nHope you now understand the differences (and similarities) between these two versions and numpy. Great!\nThe previous section was a quick look at Trax’s numpy. However this notebook also aims to teach you how you can calculate the perplexity of a trained model.",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "L2 - Calculating Perplexity"
    ]
  },
  {
    "objectID": "notes/c3w2/lab02.html#calculating-perplexity",
    "href": "notes/c3w2/lab02.html#calculating-perplexity",
    "title": "Working with JAX numpy and calculating perplexity: Ungraded Lecture Notebook",
    "section": "Calculating Perplexity",
    "text": "Calculating Perplexity\nThe perplexity is a metric that measures how well a probability model predicts a sample and it is commonly used to evaluate language models. It is defined as:\nP(W) = \\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}}\nAs an implementation hack, you would usually take the log of that formula (to enable us to use the log probabilities we get as output of our RNN, convert exponents to products, and products into sums which makes computations less complicated and computationally more efficient). You should also take care of the padding, since you do not want to include the padding when calculating the perplexity (because we do not want to have a perplexity measure artificially good). The algebra behind this process is explained next:\nlog P(W) = {log\\big(\\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}}\\big)}\n = {log\\big({\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}}\\big)^{\\frac{1}{N}}}\n = {log\\big({\\prod_{i=1}^{N}{P(w_i| w_1,...,w_{n-1})}}\\big)^{-\\frac{1}{N}}}   = -\\frac{1}{N}{log\\big({\\prod_{i=1}^{N}{P(w_i| w_1,...,w_{n-1})}}\\big)}   = -\\frac{1}{N}{\\big({\\sum_{i=1}^{N}{logP(w_i| w_1,...,w_{n-1})}}\\big)} \nYou will be working with a real example from this week’s assignment. The example is made up of: - predictions : batch of tensors corresponding to lines of text predicted by the model. - targets : batch of actual tensors corresponding to lines of text.\n\nfrom trax import layers as tl\n\n# Load from .npy files\npredictions = numpy.load('predictions.npy')\ntargets = numpy.load('targets.npy')\n\n# Cast to jax.interpreters.xla.DeviceArray\npredictions = np.array(predictions)\ntargets = np.array(targets)\n\n# Print shapes\nprint(f'predictions has shape: {predictions.shape}')\nprint(f'targets has shape: {targets.shape}')\n\npredictions has shape: (32, 64, 256)\ntargets has shape: (32, 64)\n\n\nNotice that the predictions have an extra dimension with the same length as the size of the vocabulary used.\nBecause of this you will need a way of reshaping targets to match this shape. For this you can use trax.layers.one_hot().\nNotice that predictions.shape[-1] will return the size of the last dimension of predictions.\n\nreshaped_targets = tl.one_hot(targets, predictions.shape[-1]) #trax's one_hot function takes the input as one_hot(x, n_categories, dtype=optional)\nprint(f'reshaped_targets has shape: {reshaped_targets.shape}')\n\nreshaped_targets has shape: (32, 64, 256)\n\n\nBy calculating the product of the predictions and the reshaped targets and summing across the last dimension, the total log perplexity can be computed:\n\ntotal_log_ppx = np.sum(predictions * reshaped_targets, axis= -1)\n\nNow you will need to account for the padding so this metric is not artificially deflated (since a lower perplexity means a better model). For identifying which elements are padding and which are not, you can use np.equal() and get a tensor with 1s in the positions of actual values and 0s where there are paddings.\n\nnon_pad = 1.0 - np.equal(targets, 0)\nprint(f'non_pad has shape: {non_pad.shape}\\n')\nprint(f'non_pad looks like this: \\n\\n {non_pad}')\n\nnon_pad has shape: (32, 64)\n\nnon_pad looks like this: \n\n [[1. 1. 1. ... 0. 0. 0.]\n [1. 1. 1. ... 0. 0. 0.]\n [1. 1. 1. ... 0. 0. 0.]\n ...\n [1. 1. 1. ... 0. 0. 0.]\n [1. 1. 1. ... 0. 0. 0.]\n [1. 1. 1. ... 0. 0. 0.]]\n\n\nBy computing the product of the total log perplexity and the non_pad tensor we remove the effect of padding on the metric:\n\nreal_log_ppx = total_log_ppx * non_pad\nprint(f'real perplexity still has shape: {real_log_ppx.shape}')\n\nreal perplexity still has shape: (32, 64)\n\n\nYou can check the effect of filtering out the padding by looking at the two log perplexity tensors:\n\nprint(f'log perplexity tensor before filtering padding: \\n\\n {total_log_ppx}\\n')\nprint(f'log perplexity tensor after filtering padding: \\n\\n {real_log_ppx}')\n\nlog perplexity tensor before filtering padding: \n\n [[ -5.396545    -1.0311184   -0.66916656 ... -22.37673    -23.18771\n  -21.843483  ]\n [ -4.5857706   -1.1341286   -8.538033   ... -20.15686    -26.837097\n  -23.57502   ]\n [ -5.2223887   -1.2824144   -0.17312431 ... -21.328228   -19.854412\n  -33.88444   ]\n ...\n [ -5.396545   -17.291681    -4.360766   ... -20.825802   -21.065838\n  -22.443115  ]\n [ -5.9313164  -14.247417    -0.2637329  ... -26.743248   -18.38433\n  -22.355278  ]\n [ -5.670536    -0.10595131   0.         ... -23.332523   -28.087376\n  -23.878807  ]]\n\nlog perplexity tensor after filtering padding: \n\n [[ -5.396545    -1.0311184   -0.66916656 ...  -0.          -0.\n   -0.        ]\n [ -4.5857706   -1.1341286   -8.538033   ...  -0.          -0.\n   -0.        ]\n [ -5.2223887   -1.2824144   -0.17312431 ...  -0.          -0.\n   -0.        ]\n ...\n [ -5.396545   -17.291681    -4.360766   ...  -0.          -0.\n   -0.        ]\n [ -5.9313164  -14.247417    -0.2637329  ...  -0.          -0.\n   -0.        ]\n [ -5.670536    -0.10595131   0.         ...  -0.          -0.\n   -0.        ]]\n\n\nTo get a single average log perplexity across all the elements in the batch you can sum across both dimensions and divide by the number of elements. Notice that the result will be the negative of the real log perplexity of the model:\n\nlog_ppx = np.sum(real_log_ppx) / np.sum(non_pad)\nlog_ppx = -log_ppx\nprint(f'The log perplexity and perplexity of the model are respectively: {log_ppx} and {np.exp(log_ppx)}')\n\nThe log perplexity and perplexity of the model are respectively: 2.3281209468841553 and 10.258646965026855\n\n\nCongratulations on finishing this lecture notebook! Now you should have a clear understanding of how to work with Trax’s numpy and how to compute the perplexity to evaluate your language models. Keep it up!",
    "crumbs": [
      "Home",
      "Sequence Models",
      "RNN for Language Modeling",
      "L2 - Calculating Perplexity"
    ]
  },
  {
    "objectID": "notes/c4w1/lab02.html",
    "href": "notes/c4w1/lab02.html",
    "title": "Calculating the Bilingual Evaluation Understudy (BLEU) score: Ungraded Lab",
    "section": "",
    "text": "In this ungraded lab, we will implement a popular metric for evaluating the quality of machine-translated text: the BLEU score proposed by Kishore Papineni, et al. In their 2002 paper “BLEU: a Method for Automatic Evaluation of Machine Translation”, the BLEU score works by comparing “candidate” text to one or more “reference” translations. The result is better the closer the score is to 1. Let’s see how to get this value in the following sections.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "L2 - BLEU Score"
    ]
  },
  {
    "objectID": "notes/c4w1/lab02.html#importing-the-libraries",
    "href": "notes/c4w1/lab02.html#importing-the-libraries",
    "title": "Calculating the Bilingual Evaluation Understudy (BLEU) score: Ungraded Lab",
    "section": "1.1 Importing the Libraries",
    "text": "1.1 Importing the Libraries\nWe will first start by importing the Python libraries we will use in the first part of this lab. For learning, we will implement our own version of the BLEU Score using Numpy. To verify that our implementation is correct, we will compare our results with those generated by the SacreBLEU library. This package provides hassle-free computation of shareable, comparable, and reproducible BLEU scores. It also knows all the standard test sets and handles downloading, processing, and tokenization.\n\nimport numpy as np                  # import numpy to make numerical computations.\nimport nltk                         # import NLTK to handle simple NL tasks like tokenization.\nnltk.download(\"punkt\")\nfrom nltk.util import ngrams\nfrom collections import Counter     # import the Counter module.\n!pip3 install 'sacrebleu'           # install the sacrebleu package.\nimport sacrebleu                    # import sacrebleu in order compute the BLEU score.\nimport matplotlib.pyplot as plt     # import pyplot in order to make some illustrations.\n\n[nltk_data] Downloading package punkt to /home/oren/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\nTrue\n\n\nRequirement already satisfied: sacrebleu in /home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages (2.5.1)\nRequirement already satisfied: numpy&gt;=1.17 in /home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages (from sacrebleu) (2.0.2)\nRequirement already satisfied: portalocker in /home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages (from sacrebleu) (3.1.1)\nRequirement already satisfied: tabulate&gt;=0.8.9 in /home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: lxml in /home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages (from sacrebleu) (5.3.0)\nRequirement already satisfied: regex in /home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages (from sacrebleu) (2024.11.6)\nRequirement already satisfied: colorama in /home/oren/work/notes/notes-nlp/.venv/lib/python3.10/site-packages (from sacrebleu) (0.4.6)",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "L2 - BLEU Score"
    ]
  },
  {
    "objectID": "notes/c4w1/lab02.html#defining-the-bleu-score",
    "href": "notes/c4w1/lab02.html#defining-the-bleu-score",
    "title": "Calculating the Bilingual Evaluation Understudy (BLEU) score: Ungraded Lab",
    "section": "1.2 Defining the BLEU Score",
    "text": "1.2 Defining the BLEU Score\nYou have seen the formula for calculating the BLEU score in this week’s lectures. More formally, we can express the BLEU score as:\n\nBLEU = BP\\Bigl(\\prod_{i=1}^{4}precision_i\\Bigr)^{(1/4)}\n\nwith the Brevity Penalty and precision defined as:\n\nBP = min\\Bigl(1, e^{(1-({ref}/{cand}))}\\Bigr)\n\n\nprecision_i = \\frac {\\sum_{snt \\in{cand}}\\sum_{i\\in{snt}}min\\Bigl(m^{i}_{cand}, m^{i}_{ref}\\Bigr)}{w^{i}_{t}}\n\nwhere:\n\nm^{i}_{cand}, is the count of i-gram in candidate matching the reference translation.\nm^{i}_{ref}, is the count of i-gram in the reference translation.\nw^{i}_{t}, is the total number of i-grams in candidate translation.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "L2 - BLEU Score"
    ]
  },
  {
    "objectID": "notes/c4w1/lab02.html#explaining-the-bleu-score",
    "href": "notes/c4w1/lab02.html#explaining-the-bleu-score",
    "title": "Calculating the Bilingual Evaluation Understudy (BLEU) score: Ungraded Lab",
    "section": "1.3 Explaining the BLEU score",
    "text": "1.3 Explaining the BLEU score\n\nBrevity Penalty (example):\n\nref_length = np.ones(100)\ncan_length = np.linspace(1.5, 0.5, 100)\nx = ref_length / can_length\ny = 1 - x\ny = np.exp(y)\ny = np.minimum(np.ones(y.shape), y)\n\n# Code for in order to make the plot\nfig, ax = plt.subplots(1)\nlines = ax.plot(x, y)\nax.set(\n    xlabel=\"Ratio of the length of the reference to the candidate text\",\n    ylabel=\"Brevity Penalty\",\n)\nplt.show()\n\n[Text(0.5, 0, 'Ratio of the length of the reference to the candidate text'),\n Text(0, 0.5, 'Brevity Penalty')]\n\n\n\n\n\n\n\n\n\nThe brevity penalty penalizes generated translations that are too short compared to the closest reference length with an exponential decay. The brevity penalty compensates for the fact that the BLEU score has no recall term.\n\n\nN-Gram Precision (example):\n\ndata = {\"1-gram\": 0.8, \"2-gram\": 0.7, \"3-gram\": 0.6, \"4-gram\": 0.5}\nnames = list(data.keys())\nvalues = list(data.values())\n\nfig, ax = plt.subplots(1)\nbars = ax.bar(names, values)\nax.set(ylabel=\"N-gram precision\")\n\nplt.show()\n\n\n\n\n\n\n\n\nThe n-gram precision counts how many unigrams, bigrams, trigrams, and four-grams (i=1,…,4) match their n-gram counterpart in the reference translations. This term acts as a precision metric. Unigrams account for adequacy while longer n-grams account for fluency of the translation. To avoid overcounting, the n-gram counts are clipped to the maximal n-gram count occurring in the reference (m_{n}^{ref}). Typically precision shows exponential decay with the with the degree of the n-gram.\n\n\nN-gram BLEU score (example):\n\ndata = {\"1-gram\": 0.8, \"2-gram\": 0.77, \"3-gram\": 0.74, \"4-gram\": 0.71}\nnames = list(data.keys())\nvalues = list(data.values())\n\nfig, ax = plt.subplots(1)\nbars = ax.bar(names, values)\nax.set(ylabel=\"Modified N-gram precision\")\n\nplt.show()\n\n\n\n\n\n\n\n\nWhen the n-gram precision is multiplied by the BP, then the exponential decay of n-grams is almost fully compensated. The BLEU score corresponds to a geometric average of this modified n-gram precision.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "L2 - BLEU Score"
    ]
  },
  {
    "objectID": "notes/c4w1/lab02.html#example-calculations-of-the-bleu-score",
    "href": "notes/c4w1/lab02.html#example-calculations-of-the-bleu-score",
    "title": "Calculating the Bilingual Evaluation Understudy (BLEU) score: Ungraded Lab",
    "section": "1.4 Example Calculations of the BLEU score",
    "text": "1.4 Example Calculations of the BLEU score\nIn this example we will have a reference translation and 2 candidates translations. We will tokenize all sentences using the NLTK package introduced in Course 2 of this NLP specialization.\n\nreference = \"The NASA Opportunity rover is battling a massive dust storm on planet Mars.\"\ncandidate_1 = \"The Opportunity rover is combating a big sandstorm on planet Mars.\"\ncandidate_2 = \"A NASA rover is fighting a massive storm on planet Mars.\"\n\ntokenized_ref = nltk.word_tokenize(reference.lower())\ntokenized_cand_1 = nltk.word_tokenize(candidate_1.lower())\ntokenized_cand_2 = nltk.word_tokenize(candidate_2.lower())\n\nprint(f\"{reference} -&gt; {tokenized_ref}\")\nprint(\"\\n\")\nprint(f\"{candidate_1} -&gt; {tokenized_cand_1}\")\nprint(\"\\n\")\nprint(f\"{candidate_2} -&gt; {tokenized_cand_2}\")\n\nThe NASA Opportunity rover is battling a massive dust storm on planet Mars. -&gt; ['the', 'nasa', 'opportunity', 'rover', 'is', 'battling', 'a', 'massive', 'dust', 'storm', 'on', 'planet', 'mars', '.']\n\n\nThe Opportunity rover is combating a big sandstorm on planet Mars. -&gt; ['the', 'opportunity', 'rover', 'is', 'combating', 'a', 'big', 'sandstorm', 'on', 'planet', 'mars', '.']\n\n\nA NASA rover is fighting a massive storm on planet Mars. -&gt; ['a', 'nasa', 'rover', 'is', 'fighting', 'a', 'massive', 'storm', 'on', 'planet', 'mars', '.']\n\n\n\nSTEP 1: Computing the Brevity Penalty\n\ndef brevity_penalty(reference, candidate):\n    ref_length = len(reference)\n    can_length = len(candidate)\n\n    # Brevity Penalty\n    if ref_length &gt; can_length:\n        BP = 1\n    else:\n        penalty = 1 - (ref_length / can_length)\n        BP = np.exp(penalty)\n\n    return BP\n\n\n\nSTEP 2: Computing the Precision\n\ndef clipped_precision(reference, candidate):\n    \"\"\"\n    Bleu score function given a original and a machine translated sentences\n    \"\"\"\n\n    clipped_precision_score = []\n\n    for i in range(1, 5):\n        candidate_n_gram = Counter(\n            ngrams(candidate, i)\n        )  # counts of n-gram n=1...4 tokens for the candidate\n        reference_n_gram = Counter(\n            ngrams(reference, i)\n        )  # counts of n-gram n=1...4 tokens for the reference\n\n        c = sum(\n            reference_n_gram.values()\n        )  # sum of the values of the reference the denominator in the precision formula\n\n        for j in reference_n_gram:  # for every n_gram token in the reference\n            if j in candidate_n_gram:  # check if it is in the candidate n-gram\n\n                if (\n                    reference_n_gram[j] &gt; candidate_n_gram[j]\n                ):  # if the count of the reference n-gram is bigger\n                    # than the corresponding count in the candidate n-gram\n                    reference_n_gram[j] = candidate_n_gram[\n                        j\n                    ]  # then set the count of the reference n-gram to be equal\n                    # to the count of the candidate n-gram\n            else:\n\n                reference_n_gram[j] = 0  # else reference n-gram = 0\n\n        clipped_precision_score.append(sum(reference_n_gram.values()) / c)\n\n    weights = [0.25] * 4\n\n    s = (w_i * np.log(p_i) for w_i, p_i in zip(weights, clipped_precision_score))\n    s = np.exp(np.sum(s))\n    return s\n\n\n\nSTEP 3: Computing the BLEU score\n\ndef bleu_score(reference, candidate):\n    BP = brevity_penalty(reference, candidate)\n    precision = clipped_precision(reference, candidate)\n    return BP * precision\n\n\n\nSTEP 4: Testing with our Example Reference and Candidates Sentences\n\nprint(\n    \"Results reference versus candidate 1 our own code BLEU: \",\n    round(bleu_score(tokenized_ref, tokenized_cand_1) * 100, 1),\n)\nprint(\n    \"Results reference versus candidate 2 our own code BLEU: \",\n    round(bleu_score(tokenized_ref, tokenized_cand_2) * 100, 1),\n)\n\nResults reference versus candidate 1 our own code BLEU:  27.4\nResults reference versus candidate 2 our own code BLEU:  35.0\n\n\n/tmp/ipykernel_127084/273199063.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n  s = np.exp(np.sum(s))\n\n\n\n\nSTEP 5: Comparing the Results from our Code with the SacreBLEU Library\n\nprint(\n    \"Results reference versus candidate 1 sacrebleu library sentence BLEU: \",\n    round(sacrebleu.corpus_bleu(reference, candidate_1).score, 1),\n)\nprint(\n    \"Results reference versus candidate 2 sacrebleu library sentence BLEU: \",\n    round(sacrebleu.corpus_bleu(reference, candidate_2).score, 1),\n)\n\nResults reference versus candidate 1 sacrebleu library sentence BLEU:  0.0\nResults reference versus candidate 2 sacrebleu library sentence BLEU:  0.0",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "L2 - BLEU Score"
    ]
  },
  {
    "objectID": "notes/c4w1/lab02.html#loading-data-sets-for-evaluation-using-the-bleu-score",
    "href": "notes/c4w1/lab02.html#loading-data-sets-for-evaluation-using-the-bleu-score",
    "title": "Calculating the Bilingual Evaluation Understudy (BLEU) score: Ungraded Lab",
    "section": "Loading Data Sets for Evaluation Using the BLEU Score",
    "text": "Loading Data Sets for Evaluation Using the BLEU Score\nIn this section, we will show a simple pipeline for evaluating machine translated text. Due to storage and speed constraints, we will not be using our own model in this lab (you’ll get to do that in the assignment!). Instead, we will be using Google Translate to generate English to German translations and we will evaluate it against a known evaluation set. There are three files we will need:\n\nA source text in English. In this lab, we will use the first 1671 words of the wmt19 evaluation dataset downloaded via SacreBLEU. We just grabbed a subset because of limitations in the number of words that can be translated using Google Translate.\nA reference translation to German of the corresponding first 1671 words from the original English text. This is also provided by SacreBLEU.\nA candidate machine translation to German from the same 1671 words. This is generated by feeding the source text to a machine translation model. As mentioned above, we will use Google Translate to generate the translations in this file.\n\nWith that, we can now compare the reference an candidate translation to get the BLEU Score.\n\n# Loading the raw data\nwmt19news_src = open(\"wmt19_src.txt\", \"rU\")\nwmt19news_src_1 = wmt19news_src.read()\nwmt19news_src.close()\nwmt19news_ref = open(\"wmt19_ref.txt\", \"rU\")\nwmt19news_ref_1 = wmt19news_ref.read()\nwmt19news_ref.close()\nwmt19news_can = open(\"wmt19_can.txt\", \"rU\")\nwmt19news_can_1 = wmt19news_can.read()\nwmt19news_can.close()\n\n# Tokenizing the raw data\ntokenized_corpus_src = nltk.word_tokenize(wmt19news_src_1.lower())\ntokenized_corpus_ref = nltk.word_tokenize(wmt19news_ref_1.lower())\ntokenized_corpus_cand = nltk.word_tokenize(wmt19news_can_1.lower())\n\nInspecting the first sentence of the data.\n\nprint(\"English source text:\")\nprint(\"\\n\")\nprint(f\"{wmt19news_src_1[0:170]} -&gt; {tokenized_corpus_src[0:30]}\")\nprint(\"\\n\")\nprint(\"German reference translation:\")\nprint(\"\\n\")\nprint(f\"{wmt19news_ref_1[0:219]} -&gt; {tokenized_corpus_ref[0:35]}\")\nprint(\"\\n\")\nprint(\"German machine translation:\")\nprint(\"\\n\")\nprint(f\"{wmt19news_can_1[0:199]} -&gt; {tokenized_corpus_cand[0:29]}\")\n\nEnglish source text:\n\n\n﻿Welsh AMs worried about 'looking like muppets'\nThere is consternation among some AMs at a suggestion their title should change to MWPs (Member of the Welsh Parliament).\n -&gt; ['\\ufeffwelsh', 'ams', 'worried', 'about', \"'looking\", 'like', 'muppets', \"'\", 'there', 'is', 'consternation', 'among', 'some', 'ams', 'at', 'a', 'suggestion', 'their', 'title', 'should', 'change', 'to', 'mwps', '(', 'member', 'of', 'the', 'welsh', 'parliament', ')']\n\n\nGerman reference translation:\n\n\n﻿Walisische Ageordnete sorgen sich \"wie Dödel auszusehen\"\nEs herrscht Bestürzung unter einigen Mitgliedern der Versammlung über einen Vorschlag, der ihren Titel zu MWPs (Mitglied der walisischen Parlament) ändern soll.\n -&gt; ['\\ufeffwalisische', 'ageordnete', 'sorgen', 'sich', '``', 'wie', 'dödel', 'auszusehen', \"''\", 'es', 'herrscht', 'bestürzung', 'unter', 'einigen', 'mitgliedern', 'der', 'versammlung', 'über', 'einen', 'vorschlag', ',', 'der', 'ihren', 'titel', 'zu', 'mwps', '(', 'mitglied', 'der', 'walisischen', 'parlament', ')', 'ändern', 'soll', '.']\n\n\nGerman machine translation:\n\n\n﻿Walisische AMs machten sich Sorgen, dass sie wie Muppets aussehen könnten\nEinige AMs sind bestürzt über den Vorschlag, ihren Titel in MWPs (Mitglied des walisischen Parlaments) zu ändern.\nEs ist auf -&gt; ['\\ufeffwalisische', 'ams', 'machten', 'sich', 'sorgen', ',', 'dass', 'sie', 'wie', 'muppets', 'aussehen', 'könnten', 'einige', 'ams', 'sind', 'bestürzt', 'über', 'den', 'vorschlag', ',', 'ihren', 'titel', 'in', 'mwps', '(', 'mitglied', 'des', 'walisischen', 'parlaments']\n\n\n\nprint(\n    \"Results reference versus candidate 1 our own BLEU implementation: \",\n    round(bleu_score(tokenized_corpus_ref, tokenized_corpus_cand) * 100, 1),\n)\n\nResults reference versus candidate 1 our own BLEU implementation:  23.6\n\n\n/tmp/ipykernel_127084/273199063.py:40: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n  s = np.exp(np.sum(s))\n\n\n\nprint(\n    \"Results reference versus candidate 1 sacrebleu library BLEU: \",\n    round(sacrebleu.corpus_bleu(wmt19news_ref_1, wmt19news_can_1).score, 1),\n)\n\nResults reference versus candidate 1 sacrebleu library BLEU:  0.0\n\n\nBLEU Score Interpretation on a Corpus\n\n\n\n\n\n\n\nScore\nInterpretation\n\n\n\n\n&lt; 10\nAlmost useless\n\n\n10 - 19\nHard to get the gist\n\n\n20 - 29\nThe gist is clear, but has significant grammatical errors\n\n\n30 - 40\nUnderstandable to good translations\n\n\n40 - 50\nHigh quality translations\n\n\n50 - 60\nVery high quality, adequate, and fluent translations\n\n\n&gt; 60\nQuality often better than human\n\n\n\nFrom the table above (taken here), we can see the gist of the translation is clear but has significant grammatical errors. Nonetheless, the results of our coded BLEU score are almost identical to those of the SacreBLEU package.",
    "crumbs": [
      "Home",
      "NLP with Attention Models",
      "Neural Machine Translation",
      "L2 - BLEU Score"
    ]
  },
  {
    "objectID": "notes/c2w4/lab03.html",
    "href": "notes/c2w4/lab03.html",
    "title": "Word Embeddings: Training the CBOW model",
    "section": "",
    "text": "course banner\nIn previous lecture notebooks we saw\nThis notebook will walk we through:\nWhich are concepts necessary to understand how the training of the model works.\nLet’s dive into it!\nimport numpy as np\nfrom utils2 import get_dict",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "L3 - Training the CBOW"
    ]
  },
  {
    "objectID": "notes/c2w4/lab03.html#forward-propagation",
    "href": "notes/c2w4/lab03.html#forward-propagation",
    "title": "Word Embeddings: Training the CBOW model",
    "section": "Forward propagation",
    "text": "Forward propagation\nLet’s dive into the neural network itself, which is shown below with all the dimensions and formulas you’ll need.\n\n\n\n\n\n\n\nFigure 1\n\n\nSet N equal to 3. Remember that N is a hyperparameter of the CBOW model that represents the size of the word embedding vectors, as well as the size of the hidden layer.\nAlso set V equal to 5, which is the size of the vocabulary we have used so far.\n\n# Define the size of the word embedding vectors and save it in the variable 'N'\nN = 3\n\n# Define V. Remember this was the size of the vocabulary in the previous lecture notebooks\nV = 5\n\n\nInitialization of the weights and biases\nBefore we start training the neural network, we need to initialize the weight matrices and bias vectors with random values.\nIn the assignment we will implement a function to do this yourself using numpy.random.rand. In this notebook, we’ve pre-populated these matrices and vectors for you.\n\n# Define first matrix of weights\nW1 = np.array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n               [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n               [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n\n# Define second matrix of weights\nW2 = np.array([[-0.22182064, -0.43008631,  0.13310965],\n               [ 0.08476603,  0.08123194,  0.1772054 ],\n               [ 0.1871551 , -0.06107263, -0.1790735 ],\n               [ 0.07055222, -0.02015138,  0.36107434],\n               [ 0.33480474, -0.39423389, -0.43959196]])\n\n# Define first vector of biases\nb1 = np.array([[ 0.09688219],\n               [ 0.29239497],\n               [-0.27364426]])\n\n# Define second vector of biases\nb2 = np.array([[ 0.0352008 ],\n               [-0.36393384],\n               [-0.12775555],\n               [-0.34802326],\n               [-0.07017815]])\n\nCheck that the dimensions of these matrices match those shown in the figure above.\n\nprint(f'V (vocabulary size): {V}')\nprint(f'N (embedding size / size of the hidden layer): {N}')\nprint(f'size of W1: {W1.shape} (NxV)')\nprint(f'size of b1: {b1.shape} (Nx1)')\nprint(f'size of W2: {W2.shape} (VxN)')\nprint(f'size of b2: {b2.shape} (Vx1)')\n\nV (vocabulary size): 5\nN (embedding size / size of the hidden layer): 3\nsize of W1: (3, 5) (NxV)\nsize of b1: (3, 1) (Nx1)\nsize of W2: (5, 3) (VxN)\nsize of b2: (5, 1) (Vx1)\n\n\nBefore moving forward, we will need some functions and variables defined in previous notebooks. They can be found next. Be sure we understand everything that is going on in the next cell, if not consider doing a refresh of the first lecture notebook.\n\n# Define the tokenized version of the corpus\nwords = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n\n# Get 'word2Ind' and 'Ind2word' dictionaries for the tokenized corpus\nword2Ind, Ind2word = get_dict(words)\n\n# Define the 'get_windows' function as seen in a previous notebook\ndef get_windows(words, C):\n    i = C\n    while i &lt; len(words) - C:\n        center_word = words[i]\n        context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n        yield context_words, center_word\n        i += 1\n\n# Define the 'word_to_one_hot_vector' function as seen in a previous notebook\ndef word_to_one_hot_vector(word, word2Ind, V):\n    one_hot_vector = np.zeros(V)\n    one_hot_vector[word2Ind[word]] = 1\n    return one_hot_vector\n\n# Define the 'context_words_to_vector' function as seen in a previous notebook\ndef context_words_to_vector(context_words, word2Ind, V):\n    context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n    context_words_vectors = np.mean(context_words_vectors, axis=0)\n    return context_words_vectors\n\n# Define the generator function 'get_training_example' as seen in a previous notebook\ndef get_training_example(words, C, word2Ind, V):\n    for context_words, center_word in get_windows(words, C):\n        yield context_words_to_vector(context_words, word2Ind, V), word_to_one_hot_vector(center_word, word2Ind, V)\n\n\n\nTraining example\nRun the next cells to get the first training example, made of the vector representing the context words “i am because i”, and the target which is the one-hot vector representing the center word “happy”.\n\nWe don’t need to worry about the Python syntax, but there are some explanations below if we want to know what’s happening behind the scenes.\n\n\n# Save generator object in the 'training_examples' variable with the desired arguments\ntraining_examples = get_training_example(words, 2, word2Ind, V)\n\n\nget_training_examples, which uses the yield keyword, is known as a generator. When run, it builds an iterator, which is a special type of object that… we can iterate on (using a for loop for instance), to retrieve the successive values that the function generates.\nIn this case get_training_examples yields training examples, and iterating on training_examples will return the successive training examples.\n\n\n# Get first values from generator\nx_array, y_array = next(training_examples)\n\n\nnext is another special keyword, which gets the next available value from an iterator. Here, you’ll get the very first value, which is the first training example. If we run this cell again, you’ll get the next value, and so on until the iterator runs out of values to return.\nIn this notebook next is used because we will only be performing one iteration of training. In this week’s assignment with the full training over several iterations you’ll use regular for loops with the iterator that supplies the training examples.\n\nThe vector representing the context words, which will be fed into the neural network, is:\n\n# Print context words vector\nx_array\n\narray([0.25, 0.25, 0.  , 0.5 , 0.  ])\n\n\nThe one-hot vector representing the center word to be predicted is:\n\n# Print one hot vector of center word\ny_array\n\narray([0., 0., 1., 0., 0.])\n\n\nNow convert these vectors into matrices (or 2D arrays) to be able to perform matrix multiplication on the right types of objects, as explained in a previous notebook.\n\n# Copy vector\nx = x_array.copy()\n\n# Reshape it\nx.shape = (V, 1)\n\n# Print it\nprint(f'x:\\n{x}\\n')\n\n# Copy vector\ny = y_array.copy()\n\n# Reshape it\ny.shape = (V, 1)\n\n# Print it\nprint(f'y:\\n{y}')\n\nx:\n[[0.25]\n [0.25]\n [0.  ]\n [0.5 ]\n [0.  ]]\n\ny:\n[[0.]\n [0.]\n [1.]\n [0.]\n [0.]]\n\n\nNow we will need the activation functions seen before. Again, if this feel unfamiliar consider checking the previous lecture notebook.\n\n# Define the 'relu' function as seen in the previous lecture notebook\ndef relu(z):\n    result = z.copy()\n    result[result &lt; 0] = 0\n    return result\n\n# Define the 'softmax' function as seen in the previous lecture notebook\ndef softmax(z):\n    e_z = np.exp(z)\n    sum_e_z = np.sum(e_z)\n    return e_z / sum_e_z\n\n\n\nValues of the hidden layer\nNow that we have initialized all the variables that we need for forward propagation, we can calculate the values of the hidden layer using the following formulas:\n\n\\begin{align}\n\\mathbf{z_1} = \\mathbf{W_1}\\mathbf{x} + \\mathbf{b_1}  \\tag{1} \\\\\n\\mathbf{h} = \\mathrm{ReLU}(\\mathbf{z_1})  \\tag{2} \\\\\n\\end{align}\n\nFirst, we can calculate the value of \\mathbf{z_1}.\n\n# Compute z1 (values of first hidden layer before applying the ReLU function)\nz1 = np.dot(W1, x) + b1\n\n\n np.dot is numpy’s function for matrix multiplication.\n\nAs expected we get an N by 1 matrix, or column vector with N elements, where N is equal to the embedding size, which is 3 in this example.\n\n# Print z1\nz1\n\narray([[ 0.36483875],\n       [ 0.63710329],\n       [-0.3236647 ]])\n\n\nWe can now take the ReLU of \\mathbf{z_1} to get \\mathbf{h}, the vector with the values of the hidden layer.\n\n# Compute h (z1 after applying ReLU function)\nh = relu(z1)\n\n# Print h\nh\n\narray([[0.36483875],\n       [0.63710329],\n       [0.        ]])\n\n\nApplying ReLU means that the negative element of \\mathbf{z_1} has been replaced with a zero.\n\n\nValues of the output layer\nHere are the formulas we need to calculate the values of the output layer, represented by the vector \\mathbf{\\hat y}:\n\n\\begin{align}\n\\mathbf{z_2} &= \\mathbf{W_2}\\mathbf{h} + \\mathbf{b_2}   \\tag{3} \\\\\n\\mathbf{\\hat y} &= \\mathrm{softmax}(\\mathbf{z_2})   \\tag{4} \\\\\n\\end{align}\n\nFirst, calculate \\mathbf{z_2}.\n\n# Compute z2 (values of the output layer before applying the softmax function)\nz2 = np.dot(W2, h) + b2\n\n# Print z2\nz2\n\narray([[-0.31973737],\n       [-0.28125477],\n       [-0.09838369],\n       [-0.33512159],\n       [-0.19919612]])\n\n\nExpected output:\narray([[-0.31973737],\n       [-0.28125477],\n       [-0.09838369],\n       [-0.33512159],\n       [-0.19919612]])\nThis is a V by 1 matrix, where V is the size of the vocabulary, which is 5 in this example.\nNow calculate the value of \\mathbf{\\hat y}.\n\n# Compute y_hat (z2 after applying softmax function)\ny_hat = softmax(z2)\n\n# Print y_hat\ny_hat\n\narray([[0.18519074],\n       [0.19245626],\n       [0.23107446],\n       [0.18236353],\n       [0.20891502]])\n\n\nExpected output:\narray([[0.18519074],\n       [0.19245626],\n       [0.23107446],\n       [0.18236353],\n       [0.20891502]])\nAs you’ve performed the calculations with random matrices and vectors (apart from the input vector), the output of the neural network is essentially random at this point. The learning process will adjust the weights and biases to match the actual targets better.\nThat being said, what word did the neural network predict?\n\n\nSolution\n\n\nThe neural network predicted the word “happy”: the largest element of \\mathbf{\\hat y} is the third one, and the third word of the vocabulary is “happy”.\n\n\nHere’s how we could implement this in Python:\n\n\nprint(Ind2word[np.argmax(y_hat)])\n\n\nWell done, you’ve completed the forward propagation phase!",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "L3 - Training the CBOW"
    ]
  },
  {
    "objectID": "notes/c2w4/lab03.html#cross-entropy-loss",
    "href": "notes/c2w4/lab03.html#cross-entropy-loss",
    "title": "Word Embeddings: Training the CBOW model",
    "section": "Cross-entropy loss",
    "text": "Cross-entropy loss\nNow that we have the network’s prediction, we can calculate the cross-entropy loss to determine how accurate the prediction was compared to the actual target.\n\nRemember that we are working on a single training example, not on a batch of examples, which is why we are using loss and not cost, which is the generalized form of loss.\n\nFirst let’s recall what the prediction was.\n\n# Print prediction\ny_hat\n\narray([[0.18519074],\n       [0.19245626],\n       [0.23107446],\n       [0.18236353],\n       [0.20891502]])\n\n\nAnd the actual target value is:\n\n# Print target value\ny\n\narray([[0.],\n       [0.],\n       [1.],\n       [0.],\n       [0.]])\n\n\nThe formula for cross-entropy loss is:\n J=-\\sum\\limits_{k=1}^{V}y_k\\log{\\hat{y}_k} \\tag{6}\nTry implementing the cross-entropy loss function so we get more familiar working with numpy\nHere are a some hints if you’re stuck.\n\ndef cross_entropy_loss(y_predicted, y_actual):\n    # Fill the loss variable with your code\n    loss = np.sum(-np.log(y_predicted)*y_actual)\n    return loss\n\n\n\nHint 1\n\n&lt;p&gt;To multiply two numpy matrices (such as &lt;code&gt;y&lt;/code&gt; and &lt;code&gt;y_hat&lt;/code&gt;) element-wise, we can simply use the &lt;code&gt;*&lt;/code&gt; operator.&lt;/p&gt;\n\n\n\nHint 2\n\n\nOnce we have a vector equal to the element-wise multiplication of y and y_hat, we can use np.sum to calculate the sum of the elements of this vector.\n\n\n\n\nSolution\n\n\nloss = np.sum(-np.log(y_hat)*y)\n\n\nDon’t forget to run the cell containing the cross_entropy_loss function once it is solved.\nNow use this function to calculate the loss with the actual values of \\mathbf{y} and \\mathbf{\\hat y}.\n\n# Print value of cross entropy loss for prediction and target value\ncross_entropy_loss(y_hat, y)\n\nnp.float64(1.4650152923611108)\n\n\nExpected output:\n1.4650152923611106\nThis value is neither good nor bad, which is expected as the neural network hasn’t learned anything yet.\nThe actual learning will start during the next phase: backpropagation.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "L3 - Training the CBOW"
    ]
  },
  {
    "objectID": "notes/c2w4/lab03.html#backpropagation",
    "href": "notes/c2w4/lab03.html#backpropagation",
    "title": "Word Embeddings: Training the CBOW model",
    "section": "Backpropagation",
    "text": "Backpropagation\nThe formulas that we will implement for backpropagation are the following.\n\n\\begin{align}\n\\frac{\\partial J}{\\partial \\mathbf{W_1}} &= \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right )\\mathbf{x}^\\top \\tag{7}\\\\\n\\frac{\\partial J}{\\partial \\mathbf{W_2}} &= (\\mathbf{\\hat{y}} - \\mathbf{y})\\mathbf{h^\\top} \\tag{8}\\\\\n\\frac{\\partial J}{\\partial \\mathbf{b_1}} &= \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right ) \\tag{9}\\\\\n\\frac{\\partial J}{\\partial \\mathbf{b_2}} &= \\mathbf{\\hat{y}} - \\mathbf{y} \\tag{10}\n\\end{align}\n\n\nNote: these formulas are slightly simplified compared to the ones in the lecture as you’re working on a single training example, whereas the lecture provided the formulas for a batch of examples. In the assignment you’ll be implementing the latter.\n\nLet’s start with an easy one.\nCalculate the partial derivative of the loss function with respect to \\mathbf{b_2}, and store the result in grad_b2.\n\n\\frac{\\partial J}{\\partial \\mathbf{b_2}} = \\mathbf{\\hat{y}} - \\mathbf{y} \\tag{10}\n\n\n# Compute vector with partial derivatives of loss function with respect to b2\ngrad_b2 = y_hat - y\n\n# Print this vector\ngrad_b2\n\narray([[ 0.18519074],\n       [ 0.19245626],\n       [-0.76892554],\n       [ 0.18236353],\n       [ 0.20891502]])\n\n\nExpected output:\narray([[ 0.18519074],\n       [ 0.19245626],\n       [-0.76892554],\n       [ 0.18236353],\n       [ 0.20891502]])\nNext, calculate the partial derivative of the loss function with respect to \\mathbf{W_2}, and store the result in grad_W2.\n\n\\frac{\\partial J}{\\partial \\mathbf{W_2}} = (\\mathbf{\\hat{y}} - \\mathbf{y})\\mathbf{h^\\top} \\tag{8}\n\n\nHint: use .T to get a transposed matrix, e.g. h.T returns \\mathbf{h^\\top}.\n\n\n# Compute matrix with partial derivatives of loss function with respect to W2\ngrad_W2 = np.dot(y_hat - y, h.T)\n\n# Print matrix\ngrad_W2\n\narray([[ 0.06756476,  0.11798563,  0.        ],\n       [ 0.0702155 ,  0.12261452,  0.        ],\n       [-0.28053384, -0.48988499,  0.        ],\n       [ 0.06653328,  0.1161844 ,  0.        ],\n       [ 0.07622029,  0.13310045,  0.        ]])\n\n\nExpected output:\narray([[ 0.06756476,  0.11798563,  0.        ],\n       [ 0.0702155 ,  0.12261452,  0.        ],\n       [-0.28053384, -0.48988499,  0.        ],\n       [ 0.06653328,  0.1161844 ,  0.        ],\n       [ 0.07622029,  0.13310045,  0.        ]])\nNow calculate the partial derivative with respect to \\mathbf{b_1} and store the result in grad_b1.\n\\frac{\\partial J}{\\partial \\mathbf{b_1}} = \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right ) \\tag{9}\n\n# Compute vector with partial derivatives of loss function with respect to b1\ngrad_b1 = relu(np.dot(W2.T, y_hat - y))\n\n# Print vector\ngrad_b1\n\narray([[0.        ],\n       [0.        ],\n       [0.17045858]])\n\n\nExpected output:\narray([[0.        ],\n       [0.        ],\n       [0.17045858]])\nFinally, calculate the partial derivative of the loss with respect to \\mathbf{W_1}, and store it in grad_W1.\n\n\\frac{\\partial J}{\\partial \\mathbf{W_1}} = \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right )\\mathbf{x}^\\top \\tag{7}\n\n\n# Compute matrix with partial derivatives of loss function with respect to W1\ngrad_W1 = np.dot(relu(np.dot(W2.T, y_hat - y)), x.T)\n\n# Print matrix\ngrad_W1\n\narray([[0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.04261464, 0.04261464, 0.        , 0.08522929, 0.        ]])\n\n\nExpected output:\narray([[0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.04261464, 0.04261464, 0.        , 0.08522929, 0.        ]])\nBefore moving on to gradient descent, double-check that all the matrices have the expected dimensions.\n\nprint(f'V (vocabulary size): {V}')\nprint(f'N (embedding size / size of the hidden layer): {N}')\nprint(f'size of grad_W1: {grad_W1.shape} (NxV)')\nprint(f'size of grad_b1: {grad_b1.shape} (Nx1)')\nprint(f'size of grad_W2: {grad_W2.shape} (VxN)')\nprint(f'size of grad_b2: {grad_b2.shape} (Vx1)')\n\nV (vocabulary size): 5\nN (embedding size / size of the hidden layer): 3\nsize of grad_W1: (3, 5) (NxV)\nsize of grad_b1: (3, 1) (Nx1)\nsize of grad_W2: (5, 3) (VxN)\nsize of grad_b2: (5, 1) (Vx1)",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "L3 - Training the CBOW"
    ]
  },
  {
    "objectID": "notes/c2w4/lab03.html#gradient-descent",
    "href": "notes/c2w4/lab03.html#gradient-descent",
    "title": "Word Embeddings: Training the CBOW model",
    "section": "Gradient descent",
    "text": "Gradient descent\nDuring the gradient descent phase, we will update the weights and biases by subtracting \\alpha times the gradient from the original matrices and vectors, using the following formulas.\n\n\\begin{align}\n\\mathbf{W_1} &:= \\mathbf{W_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_1}} \\tag{11}\\\\\n\\mathbf{W_2} &:= \\mathbf{W_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_2}} \\tag{12}\\\\\n\\mathbf{b_1} &:= \\mathbf{b_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_1}} \\tag{13}\\\\\n\\mathbf{b_2} &:= \\mathbf{b_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_2}} \\tag{14}\\\\\n\\end{align}\n\nFirst, let set a value for \\alpha.\n\n# Define alpha\nalpha = 0.03\n\nThe updated weight matrix \\mathbf{W_1} will be:\n\n# Compute updated W1\nW1_new = W1 - alpha * grad_W1\n\nLet’s compare the previous and new values of \\mathbf{W_1}:\n\nprint('old value of W1:')\nprint(W1)\nprint()\nprint('new value of W1:')\nprint(W1_new)\n\nold value of W1:\n[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]\n [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]\n [ 0.26637602 -0.23846886 -0.37770863 -0.11399446  0.34008124]]\n\nnew value of W1:\n[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]\n [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]\n [ 0.26509758 -0.2397473  -0.37770863 -0.11655134  0.34008124]]\n\n\nThe difference is very subtle (hint: take a closer look at the last row), which is why it takes a fair amount of iterations to train the neural network until it reaches optimal weights and biases starting from random values.\nNow calculate the new values of \\mathbf{W_2} (to be stored in W2_new), \\mathbf{b_1} (in b1_new), and \\mathbf{b_2} (in b2_new).\n\\begin{align}\n\\mathbf{W_2} &:= \\mathbf{W_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_2}} \\tag{12}\\\\\n\\mathbf{b_1} &:= \\mathbf{b_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_1}} \\tag{13}\\\\\n\\mathbf{b_2} &:= \\mathbf{b_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_2}} \\tag{14}\\\\\n\\end{align}\n\n# Compute updated W2\nW2_new = W2 - alpha * grad_W2\n\n# Compute updated b1\nb1_new = b1 - alpha * grad_b1\n\n# Compute updated b2\nb2_new = b2 - alpha * grad_b2\n\n\nprint('W2_new')\nprint(W2_new)\nprint()\nprint('b1_new')\nprint(b1_new)\nprint()\nprint('b2_new')\nprint(b2_new)\n\nW2_new\n[[-0.22384758 -0.43362588  0.13310965]\n [ 0.08265956  0.0775535   0.1772054 ]\n [ 0.19557112 -0.04637608 -0.1790735 ]\n [ 0.06855622 -0.02363691  0.36107434]\n [ 0.33251813 -0.3982269  -0.43959196]]\n\nb1_new\n[[ 0.09688219]\n [ 0.29239497]\n [-0.27875802]]\n\nb2_new\n[[ 0.02964508]\n [-0.36970753]\n [-0.10468778]\n [-0.35349417]\n [-0.0764456 ]]\n\n\nExpected output:\nW2_new\n[[-0.22384758 -0.43362588  0.13310965]\n [ 0.08265956  0.0775535   0.1772054 ]\n [ 0.19557112 -0.04637608 -0.1790735 ]\n [ 0.06855622 -0.02363691  0.36107434]\n [ 0.33251813 -0.3982269  -0.43959196]]\n\nb1_new\n[[ 0.09688219]\n [ 0.29239497]\n [-0.27875802]]\n\nb2_new\n[[ 0.02964508]\n [-0.36970753]\n [-0.10468778]\n [-0.35349417]\n [-0.0764456 ]]\nCongratulations, we have completed one iteration of training using one training example!\nYou’ll need many more iterations to fully train the neural network, and we can optimize the learning process by training on batches of examples, as described in the lecture. We will get to do this during this week’s assignment.\n\nHow this practice relates to and differs from the upcoming graded assignment\n\nIn the assignment, for each iteration of training we will use batches of examples instead of a single example. The formulas for forward propagation and backpropagation will be modified accordingly, and we will use cross-entropy cost instead of cross-entropy loss.\nWe will also complete several iterations of training, until we reach an acceptably low cross-entropy cost, at which point we can extract good word embeddings from the weight matrices.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "L3 - Training the CBOW"
    ]
  },
  {
    "objectID": "notes/c2w4/lab02.html",
    "href": "notes/c2w4/lab02.html",
    "title": "Word Embeddings: Intro to CBOW model, activation functions and working with Numpy",
    "section": "",
    "text": "course banner\nIn this lecture notebook we will be given an introduction to the continuous bag-of-words model, its activation functions and some considerations when working with Numpy.\nLet’s dive into it!\nimport numpy as np",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "L2 - Intro to CBOW"
    ]
  },
  {
    "objectID": "notes/c2w4/lab02.html#activation-functions",
    "href": "notes/c2w4/lab02.html#activation-functions",
    "title": "Word Embeddings: Intro to CBOW model, activation functions and working with Numpy",
    "section": "Activation functions",
    "text": "Activation functions\nLet’s start by implementing the activation functions, ReLU and softmax.\n\nReLU\nReLU is used to calculate the values of the hidden layer, in the following formulas:\n\\begin{align}\n\\mathbf{z_1} &= \\mathbf{W_1}\\mathbf{x} + \\mathbf{b_1}  \\tag{1} \\\\\n\\mathbf{h} &= \\mathrm{ReLU}(\\mathbf{z_1})  \\tag{2} \\\\\n\\end{align}\nLet’s fix a value for \\mathbf{z_1} as a working example.\n\n# Define a random seed so all random outcomes can be reproduced\nnp.random.seed(10)\n\n# Define a 5X1 column vector using numpy\nz_1 = 10*np.random.rand(5, 1)-5\n\n# Print the vector\nz_1\n\narray([[ 2.71320643],\n       [-4.79248051],\n       [ 1.33648235],\n       [ 2.48803883],\n       [-0.01492988]])\n\n\nNotice that using numpy’s random.rand function returns a numpy array filled with values taken from a uniform distribution over [0, 1). Numpy allows vectorization so each value is multiplied by 10 and then substracted 5.\nTo get the ReLU of this vector, we want all the negative values to become zeros.\nFirst create a copy of this vector.\n\n# Create copy of vector and save it in the 'h' variable\nh = z_1.copy()\n\nNow determine which of its values are negative.\n\n# Determine which values met the criteria (this is possible because of vectorization)\nh &lt; 0\n\narray([[False],\n       [ True],\n       [False],\n       [False],\n       [ True]])\n\n\nWe can now simply set all of the values which are negative to 0.\n\n# Slice the array or vector. This is the same as applying ReLU to it\nh[h &lt; 0] = 0\n\nAnd that’s it: we have the ReLU of \\mathbf{z_1}!\n\n# Print the vector after ReLU\nh\n\narray([[2.71320643],\n       [0.        ],\n       [1.33648235],\n       [2.48803883],\n       [0.        ]])\n\n\nNow implement ReLU as a function.\n\n# Define the 'relu' function that will include the steps previously seen\ndef relu(z):\n    result = z.copy()\n    result[result &lt; 0] = 0\n    return result\n\nAnd check that it’s working.\n\n# Define a new vector and save it in the 'z' variable\nz = np.array([[-1.25459881], [ 4.50714306], [ 2.31993942], [ 0.98658484], [-3.4398136 ]])\n\n# Apply ReLU to it\nrelu(z)\n\narray([[0.        ],\n       [4.50714306],\n       [2.31993942],\n       [0.98658484],\n       [0.        ]])\n\n\nExpected output:\narray([[0.        ],\n       [4.50714306],\n       [2.31993942],\n       [0.98658484],\n       [0.        ]])\n\n\nSoftmax\nThe second activation function that we need is softmax. This function is used to calculate the values of the output layer of the neural network, using the following formulas:\n\\begin{align}\n\\mathbf{z_2} &= \\mathbf{W_2}\\mathbf{h} + \\mathbf{b_2}   \\tag{3} \\\\\n\\mathbf{\\hat y} &= \\mathrm{softmax}(\\mathbf{z_2})   \\tag{4} \\\\\n\\end{align}\nTo calculate softmax of a vector \\mathbf{z}, the i-th component of the resulting vector is given by:\n \\textrm{softmax}(\\textbf{z})_i = \\frac{e^{z_i} }{\\sum\\limits_{j=1}^{V} e^{z_j} }  \\tag{5} \nLet’s work through an example.\n\n# Define a new vector and save it in the 'z' variable\nz = np.array([9, 8, 11, 10, 8.5])\n\n# Print the vector\nz\n\narray([ 9. ,  8. , 11. , 10. ,  8.5])\n\n\nYou’ll need to calculate the exponentials of each element, both for the numerator and for the denominator.\n\n# Save exponentials of the values in a new vector\ne_z = np.exp(z)\n\n# Print the vector with the exponential values\ne_z\n\narray([ 8103.08392758,  2980.95798704, 59874.1417152 , 22026.46579481,\n        4914.7688403 ])\n\n\nThe denominator is equal to the sum of these exponentials.\n\n# Save the sum of the exponentials\nsum_e_z = np.sum(e_z)\n\n# Print sum of exponentials\nsum_e_z\n\nnp.float64(97899.41826492078)\n\n\nAnd the value of the first element of \\textrm{softmax}(\\textbf{z}) is given by:\n\n# Print softmax value of the first element in the original vector\ne_z[0]/sum_e_z\n\nnp.float64(0.08276947985173956)\n\n\nThis is for one element. We can use numpy’s vectorized operations to calculate the values of all the elements of the \\textrm{softmax}(\\textbf{z}) vector in one go.\nImplement the softmax function.\n\n# Define the 'softmax' function that will include the steps previously seen\ndef softmax(z):\n    e_z = np.exp(z)\n    sum_e_z = np.sum(e_z)\n    return e_z / sum_e_z\n\nNow check that it works.\n\n# Print softmax values for original vector\nsoftmax([9, 8, 11, 10, 8.5])\n\narray([0.08276948, 0.03044919, 0.61158833, 0.22499077, 0.05020223])\n\n\nExpected output:\narray([0.08276948, 0.03044919, 0.61158833, 0.22499077, 0.05020223])\nNotice that the sum of all these values is equal to 1.\n\n# Assert that the sum of the softmax values is equal to 1\nnp.sum(softmax([9, 8, 11, 10, 8.5])) == 1\n\nnp.True_",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "L2 - Intro to CBOW"
    ]
  },
  {
    "objectID": "notes/c2w4/lab02.html#dimensions-1-d-arrays-vs-2-d-column-vectors",
    "href": "notes/c2w4/lab02.html#dimensions-1-d-arrays-vs-2-d-column-vectors",
    "title": "Word Embeddings: Intro to CBOW model, activation functions and working with Numpy",
    "section": "Dimensions: 1-D arrays vs 2-D column vectors",
    "text": "Dimensions: 1-D arrays vs 2-D column vectors\nBefore moving on to implement forward propagation, backpropagation, and gradient descent in the next lecture notebook, let’s have a look at the dimensions of the vectors you’ve been handling until now.\nCreate a vector of length V filled with zeros.\n\n# Define V. Remember this was the size of the vocabulary in the previous lecture notebook\nV = 5\n\n# Define vector of length V filled with zeros\nx_array = np.zeros(V)\n\n# Print vector\nx_array\n\narray([0., 0., 0., 0., 0.])\n\n\nThis is a 1-dimensional array, as revealed by the .shape property of the array.\n\n# Print vector's shape\nx_array.shape\n\n(5,)\n\n\nTo perform matrix multiplication in the next steps, we actually need your column vectors to be represented as a matrix with one column. In numpy, this matrix is represented as a 2-dimensional array.\nThe easiest way to convert a 1D vector to a 2D column matrix is to set its .shape property to the number of rows and one column, as shown in the next cell.\n\n# Copy vector\nx_column_vector = x_array.copy()\n\n# Reshape copy of vector\nx_column_vector.shape = (V, 1)  # alternatively ... = (x_array.shape[0], 1)\n\n# Print vector\nx_column_vector\n\narray([[0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.]])\n\n\nThe shape of the resulting “vector” is:\n\n# Print vector's shape\nx_column_vector.shape\n\n(5, 1)\n\n\nSo we now have a 5x1 matrix that we can use to perform standard matrix multiplication.\nCongratulations on finishing this lecture notebook! Hopefully we now have a better understanding of the activation functions used in the continuous bag-of-words model, as well as a clearer idea of how to leverage Numpy’s power for these types of mathematical computations.\nIn the next lecture notebook we will get a comprehensive dive into:\n\nForward propagation.\nCross-entropy loss.\nBackpropagation.\nGradient descent.\n\nSee we next time!",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "Word embeddings with neural networks",
      "L2 - Intro to CBOW"
    ]
  },
  {
    "objectID": "notes/c2w4/lab05.html",
    "href": "notes/c2w4/lab05.html",
    "title": "Word Embeddings: Hands On",
    "section": "",
    "text": "course banner\nIn previous lecture notebooks we saw all the steps needed to train the CBOW model. This notebook will walk we through how to extract the word embedding vectors from a model.\nLet’s dive into it!\nimport numpy as np\nfrom utils2 import get_dict\nBefore moving on, we will be provided with some variables needed for further procedures, which should be familiar by now. Also a trained CBOW model will be simulated, the corresponding weights and biases are provided:\n# Define the tokenized version of the corpus\nwords = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n\n# Define V. Remember this is the size of the vocabulary\nV = 5\n\n# Get 'word2Ind' and 'Ind2word' dictionaries for the tokenized corpus\nword2Ind, Ind2word = get_dict(words)\n\n\n# Define first matrix of weights\nW1 = np.array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n               [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n               [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n\n# Define second matrix of weights\nW2 = np.array([[-0.22182064, -0.43008631,  0.13310965],\n               [ 0.08476603,  0.08123194,  0.1772054 ],\n               [ 0.1871551 , -0.06107263, -0.1790735 ],\n               [ 0.07055222, -0.02015138,  0.36107434],\n               [ 0.33480474, -0.39423389, -0.43959196]])\n\n# Define first vector of biases\nb1 = np.array([[ 0.09688219],\n               [ 0.29239497],\n               [-0.27364426]])\n\n# Define second vector of biases\nb2 = np.array([[ 0.0352008 ],\n               [-0.36393384],\n               [-0.12775555],\n               [-0.34802326],\n               [-0.07017815]])"
  },
  {
    "objectID": "notes/c2w4/lab05.html#extracting-word-embedding-vectors",
    "href": "notes/c2w4/lab05.html#extracting-word-embedding-vectors",
    "title": "Word Embeddings: Hands On",
    "section": "Extracting word embedding vectors",
    "text": "Extracting word embedding vectors\nOnce we have finished training the neural network, we have three options to get word embedding vectors for the words of your vocabulary, based on the weight matrices \\mathbf{W_1} and/or \\mathbf{W_2}.\n\nOption 1: extract embedding vectors from \\mathbf{W_1}\nThe first option is to take the columns of \\mathbf{W_1} as the embedding vectors of the words of the vocabulary, using the same order of the words as for the input and output vectors.\n\nNote: in this practice notebooks the values of the word embedding vectors are meaningless after a single iteration with just one training example, but here’s how we would proceed after the training process is complete.\n\nFor example \\mathbf{W_1} is this matrix:\n\n# Print W1\nW1\n\narray([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n       [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n       [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n\n\nThe first column, which is a 3-element vector, is the embedding vector of the first word of your vocabulary. The second column is the word embedding vector for the second word, and so on.\nThe first, second, etc. words are ordered as follows.\n\n# Print corresponding word for each index within vocabulary's range\nfor i in range(V):\n    print(Ind2word[i])\n\nam\nbecause\nhappy\ni\nlearning\n\n\nSo the word embedding vectors corresponding to each word are:\n\n# Loop through each word of the vocabulary\nfor word in word2Ind:\n    # Extract the column corresponding to the index of the word in the vocabulary\n    word_embedding_vector = W1[:, word2Ind[word]]\n    # Print word alongside word embedding vector\n    print(f'{word}: {word_embedding_vector}')\n\nam: [0.41687358 0.32735501 0.26637602]\nbecause: [ 0.08854191  0.22795148 -0.23846886]\nhappy: [-0.23495225 -0.23951958 -0.37770863]\ni: [ 0.28320538  0.4117634  -0.11399446]\nlearning: [ 0.41800106 -0.23924344  0.34008124]\n\n\n\n\nOption 2: extract embedding vectors from \\mathbf{W_2}\nThe second option is to take \\mathbf{W_2} transposed, and take its columns as the word embedding vectors just like we did for \\mathbf{W_1}.\n\n# Print transposed W2\nW2.T\n\narray([[-0.22182064,  0.08476603,  0.1871551 ,  0.07055222,  0.33480474],\n       [-0.43008631,  0.08123194, -0.06107263, -0.02015138, -0.39423389],\n       [ 0.13310965,  0.1772054 , -0.1790735 ,  0.36107434, -0.43959196]])\n\n\n\n# Loop through each word of the vocabulary\nfor word in word2Ind:\n    # Extract the column corresponding to the index of the word in the vocabulary\n    word_embedding_vector = W2.T[:, word2Ind[word]]\n    # Print word alongside word embedding vector\n    print(f'{word}: {word_embedding_vector}')\n\nam: [-0.22182064 -0.43008631  0.13310965]\nbecause: [0.08476603 0.08123194 0.1772054 ]\nhappy: [ 0.1871551  -0.06107263 -0.1790735 ]\ni: [ 0.07055222 -0.02015138  0.36107434]\nlearning: [ 0.33480474 -0.39423389 -0.43959196]\n\n\n\n\nOption 3: extract embedding vectors from \\mathbf{W_1} and \\mathbf{W_2}\nThe third option, which is the one we will use in this week’s assignment, uses the average of \\mathbf{W_1} and \\mathbf{W_2^\\top}.\nCalculate the average of \\mathbf{W_1} and \\mathbf{W_2^\\top}, and store the result in W3.\n\n# Compute W3 as the average of W1 and W2 transposed\nW3 = (W1+W2.T)/2\n\n# Print W3\nW3\n\narray([[ 0.09752647,  0.08665397, -0.02389858,  0.1768788 ,  0.3764029 ],\n       [-0.05136565,  0.15459171, -0.15029611,  0.19580601, -0.31673866],\n       [ 0.19974284, -0.03063173, -0.27839106,  0.12353994, -0.04975536]])\n\n\nExpected output:\narray([[ 0.09752647,  0.08665397, -0.02389858,  0.1768788 ,  0.3764029 ],\n       [-0.05136565,  0.15459171, -0.15029611,  0.19580601, -0.31673866],\n       [ 0.19974284, -0.03063173, -0.27839106,  0.12353994, -0.04975536]])\nExtracting the word embedding vectors works just like the two previous options, by taking the columns of the matrix you’ve just created.\n\n# Loop through each word of the vocabulary\nfor word in word2Ind:\n    # Extract the column corresponding to the index of the word in the vocabulary\n    word_embedding_vector = W3[:, word2Ind[word]]\n    # Print word alongside word embedding vector\n    print(f'{word}: {word_embedding_vector}')\n\nam: [ 0.09752647 -0.05136565  0.19974284]\nbecause: [ 0.08665397  0.15459171 -0.03063173]\nhappy: [-0.02389858 -0.15029611 -0.27839106]\ni: [0.1768788  0.19580601 0.12353994]\nlearning: [ 0.3764029  -0.31673866 -0.04975536]\n\n\nNow we know 3 different options to get the word embedding vectors from a model!\n\n\nHow this practice relates to and differs from the upcoming graded assignment\n\nAfter extracting the word embedding vectors, we will use principal component analysis (PCA) to visualize the vectors, which will enable we to perform an intrinsic evaluation of the quality of the vectors, as explained in the lecture.\n\nCongratulations on finishing all lecture notebooks for this week!\nYou’re now ready to take on this week’s assignment!\nKeep it up!"
  },
  {
    "objectID": "notes/c1w2/lab01.html",
    "href": "notes/c1w2/lab01.html",
    "title": "Visualizing Naive Bayes",
    "section": "",
    "text": "Figure 1: course banner\n\n\nIn this lab, we will cover an essential part of data analysis that has not been included in the lecture videos. As we stated in the previous module, data visualization gives insight into the expected performance of any model.\nIn the following exercise, we are going to make a visual inspection of the tweets dataset using the Naïve Bayes features. We will see how we can understand the log-likelihood ratio explained in the videos as a pair of numerical features that can be fed in a machine learning algorithm.\nAt the end of this lab, we will introduce the concept of confidence ellipse as a tool for representing the Naïve Bayes model visually.\n\nimport numpy as np # Library for linear algebra and math utils\nimport pandas as pd # Dataframe library\n\nimport matplotlib.pyplot as plt # Library for plots\nfrom utils import confidence_ellipse # Function to add confidence ellipses to charts\n\n## Calculate the likelihoods for each tweet\nFor each tweet, we have calculated the likelihood of the tweet to be positive and the likelihood to be negative. We have calculated in different columns the numerator and denominator of the likelihood ratio introduced previously.\n\nlog \\frac{P(tweet|pos)}{P(tweet|neg)} = log(P(tweet|pos)) - log(P(tweet|neg))\n\n\npositive = log(P(tweet|pos)) = \\sum_{i=0}^{n}{log P(W_i|pos)}\n\n\nnegative = log(P(tweet|neg)) = \\sum_{i=0}^{n}{log P(W_i|neg)}\n\nWe did not include the code because this is part of this week’s assignment. The ‘bayes_features.csv’ file contains the final result of this process.\nThe cell below loads the table in a dataframe. Dataframes are data structures that simplify the manipulation of data, allowing filtering, slicing, joining, and summarization.\n\ndata = pd.read_csv('bayes_features.csv'); # Load the data from the csv file\n\ndata.head(5) # Print the first 5 tweets features. Each row represents a tweet\n\n\n\n\n\n\n\n\npositive\nnegative\nsentiment\n\n\n\n\n0\n-45.763393\n-63.351354\n1.0\n\n\n1\n-105.491568\n-114.204862\n1.0\n\n\n2\n-57.028078\n-67.216467\n1.0\n\n\n3\n-10.055885\n-18.589057\n1.0\n\n\n4\n-125.749270\n-138.334845\n1.0\n\n\n\n\n\n\n\n\n# Plot the samples using columns 1 and 2 of the matrix\n\nfig, ax = plt.subplots(figsize = (8, 8)) #Create a new figure with a custom size\n\ncolors = ['red', 'green'] # Define a color palete\n\n# Color base on sentiment\nax.scatter(data.positive, data.negative, \n    c=[colors[int(k)] for k in data.sentiment], s = 0.1, marker='*')  # Plot a dot for each tweet\n\n# Custom limits for this chart\nplt.xlim(-250,0)\nplt.ylim(-250,0)\n\nplt.xlabel(\"Positive\") # x-axis label\nplt.ylabel(\"Negative\") # y-axis label\n\nText(0.5, 0, 'Positive')\n\n\nText(0, 0.5, 'Negative')\n\n\n\n\n\n\n\n\n\n\nUsing Confidence Ellipses to interpret Naïve Bayes\nIn this section, we will use the confidence ellipse to give us an idea of what the Naïve Bayes model see.\nA confidence ellipse is a way to visualize a 2D random variable. It is a better way than plotting the points over a cartesian plane because, with big datasets, the points can overlap badly and hide the real distribution of the data. Confidence ellipses summarize the information of the dataset with only four parameters:\n\nCenter: It is the numerical mean of the attributes\nHeight and width: Related with the variance of each attribute. The user must specify the desired amount of standard deviations used to plot the ellipse.\nAngle: Related with the covariance among attributes.\n\nThe parameter n_std stands for the number of standard deviations bounded by the ellipse. Remember that for normal random distributions:\n\nAbout 68% of the area under the curve falls within 1 standard deviation around the mean.\nAbout 95% of the area under the curve falls within 2 standard deviations around the mean.\nAbout 99.7% of the area under the curve falls within 3 standard deviations around the mean.\n\nstandard normal\nIn the next chart, we will plot the data and its corresponding confidence ellipses using 2 std and 3 std.\n\n# Plot the samples using columns 1 and 2 of the matrix\nfig, ax = plt.subplots(figsize = (8, 8))\n\ncolors = ['red', 'green'] # Define a color palete\n\n# Color base on sentiment\n\nax.scatter(data.positive, data.negative, c=[colors[int(k)] for k in data.sentiment], s = 0.1, marker='*')  # Plot a dot for tweet\n\n# Custom limits for this chart\nplt.xlim(-200,40)  \nplt.ylim(-200,40)\n\nplt.xlabel(\"Positive\") # x-axis label\nplt.ylabel(\"Negative\") # y-axis label\n\ndata_pos = data[data.sentiment == 1] # Filter only the positive samples\ndata_neg = data[data.sentiment == 0] # Filter only the negative samples\n\n# Print confidence ellipses of 2 std\nconfidence_ellipse(data_pos.positive, data_pos.negative, ax, n_std=2, edgecolor='black', label=r'$2\\sigma$' )\nconfidence_ellipse(data_neg.positive, data_neg.negative, ax, n_std=2, edgecolor='orange')\n\n# Print confidence ellipses of 3 std\nconfidence_ellipse(data_pos.positive, data_pos.negative, ax, n_std=3, edgecolor='black', linestyle=':', label=r'$3\\sigma$')\nconfidence_ellipse(data_neg.positive, data_neg.negative, ax, n_std=3, edgecolor='orange', linestyle=':')\nax.legend()\n\nplt.show()\n\nText(0.5, 0, 'Positive')\n\n\nText(0, 0.5, 'Negative')\n\n\n\n\n\n\n\n\n\nIn the next cell, we will modify the features of the samples with positive sentiment (1), in a way that the two distributions overlap. In this case, the Naïve Bayes method will produce a lower accuracy than with the original data.\n\ndata2 = data.copy() # Copy the whole data frame\n\n# The following 2 lines only modify the entries in the data frame where sentiment == 1\ndata2.negative[data.sentiment == 1] =  data2.negative * 1.5 + 50 # Modify the negative attribute\ndata2.positive[data.sentiment == 1] =  data2.positive / 1.5 - 50 # Modify the positive attribute \n\n/tmp/ipykernel_128077/2253601370.py:4: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  data2.negative[data.sentiment == 1] =  data2.negative * 1.5 + 50 # Modify the negative attribute\n/tmp/ipykernel_128077/2253601370.py:5: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  data2.positive[data.sentiment == 1] =  data2.positive / 1.5 - 50 # Modify the positive attribute\n\n\nNow let us plot the two distributions and the confidence ellipses\n\n# Plot the samples using columns 1 and 2 of the matrix\nfig, ax = plt.subplots(figsize = (8, 8))\n\ncolors = ['red', 'green'] # Define a color palete\n\n# Color base on sentiment\n\n#data.negative[data.sentiment == 1] =  data.negative * 2\n\nax.scatter(data2.positive, data2.negative, c=[colors[int(k)] for k in data2.sentiment], s = 0.1, marker='*')  # Plot a dot for tweet\n# Custom limits for this chart\nplt.xlim(-200,40)  \nplt.ylim(-200,40)\n\nplt.xlabel(\"Positive\") # x-axis label\nplt.ylabel(\"Negative\") # y-axis label\n\ndata_pos = data2[data2.sentiment == 1] # Filter only the positive samples\ndata_neg = data[data2.sentiment == 0] # Filter only the negative samples\n\n# Print confidence ellipses of 2 std\nconfidence_ellipse(data_pos.positive, data_pos.negative, ax, n_std=2, edgecolor='black', label=r'$2\\sigma$' )\nconfidence_ellipse(data_neg.positive, data_neg.negative, ax, n_std=2, edgecolor='orange')\n\n# Print confidence ellipses of 3 std\nconfidence_ellipse(data_pos.positive, data_pos.negative, ax, n_std=3, edgecolor='black', linestyle=':', label=r'$3\\sigma$')\nconfidence_ellipse(data_neg.positive, data_neg.negative, ax, n_std=3, edgecolor='orange', linestyle=':')\nax.legend()\n\nplt.show()\n\nText(0.5, 0, 'Positive')\n\n\nText(0, 0.5, 'Negative')\n\n\n\n\n\n\n\n\n\nTo give away: Understanding the data allows us to predict if the method will perform well or not. Alternatively, it will allow us to understand why it worked well or bad.\n\n\n\n\nCitationBibTeX citation:@online{bochman2020,\n  author = {Bochman, Oren},\n  title = {Visualizing {Naive} {Bayes}},\n  date = {2020-10-06},\n  url = {https://orenbochman.github.io/notes-nlp/notes/c1w2/lab01.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2020. “Visualizing Naive Bayes.” October 6,\n2020. https://orenbochman.github.io/notes-nlp/notes/c1w2/lab01.html.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Probability & Bayes Rule",
      "L1 - Visualizing Naive Bayes"
    ]
  },
  {
    "objectID": "notes/c1w2/index.html",
    "href": "notes/c1w2/index.html",
    "title": "Probability and Bayes Rule",
    "section": "",
    "text": "Figure 1: course banner\n\n\n\n\n\n\n\n\nFigure 2\nMy notes for Week 2 of the Natural Language Processing with Probabilistic Models Course in the Natural Language Processing Specialization Offered by DeepLearning.AI on Coursera\nThe following two results are due to (Ng and Jordan 2001) by way of Naive_Bayes_classifier wikipedia article.\nAnother detail that can help we make sense of this lesson is the following result relating Naïve Bayes to Logistic Regression which we covered last week. In the case of discrete inputs like indicator or frequency features for discrete events, naive Bayes classifiers form a generative-discriminative pair with multinomial logistic regression classifiers: each naive Bayes classifier can be considered a way of fitting a probability model that optimizes the joint likelihood p (C,x), while logistic regression fits the same probability model to optimize the conditional p(C ∣ x).",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Probability & Bayes Rule",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w2/index.html#probability-of-a-randomly-selected-tweets-sentiment",
    "href": "notes/c1w2/index.html#probability-of-a-randomly-selected-tweets-sentiment",
    "title": "Probability and Bayes Rule",
    "section": "Probability of a randomly selected tweet’s sentiment",
    "text": "Probability of a randomly selected tweet’s sentiment\n\nTo calculate a probability of a certain event happening, we take the count of that specific event and divide it by the sum of all events.\nFurthermore, the sum of all probabilities has to equal 1. If we pick a tweet at random, what is the probability of it being +? We define an event A: “A tweet is positive” and calculate its probability\n\n\nP(A) = P(+) = \\frac{N_{+}}{N}=\\frac{13}{20}=0.65\n\nAnd since probabilities add up to one:\n\nP(-) = 1- P(+)=0.35",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Probability & Bayes Rule",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w2/index.html#probability-for-a-specific-words-sentiment",
    "href": "notes/c1w2/index.html#probability-for-a-specific-words-sentiment",
    "title": "Probability and Bayes Rule",
    "section": "Probability for a specific word’s sentiment",
    "text": "Probability for a specific word’s sentiment\nWithin that corpus, the word happy is sometimes labeled + and in other cases, -. This indicates that some negative tweets contain the word happy. Shown below is a graphical representation of this “overlap”. Let’s explore how we may represent this graphically using a venn diagram and then derive a probability-based representation.\n\n\n\n\nTweets with “Happy”\n\nFirst, we need to estimate the probability of the event B: “tweets containing the word happy”\n\nP(B) = P(\\text{happy})=\\frac{N_\\text{happy}}{N}=\\frac{4}{20}=0.2\n\n\n\n\n\nVenn diagram for defining probabilities from events\n\nTo compute the probability of 2 events happening like happy and + in the picture we would be looking at the intersection, or overlap of the two events, In this case, the red and the blue boxes overlap in three boxes, So the answer is: \nP(A \\cap B) = P(A,B) = \\frac{2}{20}\n\nThe Event “A is labeled +”, - The probability of events A shown as P(A) is calculated as the ratio between the count of positive tweets and the corpus divided by the total number of tweets in the corpus.\n\n\n\n\n\n\n\n specific tweets color coded per the Venn diagram\n\n\n\n\n\n\n\n\nDefinition of conditional probability\n\n\n\nConditional probability is the probability of an outcome B when we already know for certain that an event A has already happened. Notation: \nP(B|A)\n\n\n\n\nand there more + than - more specifically our prior knowledge is that : \n  \\frac{P(+)}{P(−)}=\\frac{13}{7}\n\nthe likelihood of a tweet with happy being + is\nthe challenge arises from some words being in both + and - tweets Conditional probabilities help us reduce the sample search space by restricting it to a specific event which is a given. We should understand the difference between P(A|B) and P(B|A)",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Probability & Bayes Rule",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w2/index.html#what-is-phappy",
    "href": "notes/c1w2/index.html#what-is-phappy",
    "title": "Probability and Bayes Rule",
    "section": "what is P(+|happy)",
    "text": "what is P(+|happy)\n\nWe start with the Venn diagram for the P(A|B). \nWhere we restricted the diagram to just A the subset of happy tweets.\nAnd we just want those tweets that are also + i.e. (B).\nall we need is to plug in the counts from our count chart. \nwhich we now estimate \nP(A \\mid B) = P(Positive \\mid happy) = \\frac{3}{4} = 0.75",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Probability & Bayes Rule",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w2/index.html#what-is-phappy-1",
    "href": "notes/c1w2/index.html#what-is-phappy-1",
    "title": "Probability and Bayes Rule",
    "section": "what is P(happy|+)",
    "text": "what is P(happy|+)\n\nWe start with the Venn diagram for the P(B|A)\nwhere we have restricted the diagram to just B the subset of + tweets. \nand we just want from those the tweets that are also happy i.e. (A).\nand the counts for P(B|A) \nwhich we now estimate \nP(B \\mid A) = P(happy \\mid Positive) = \\frac{3}{13} = 0.231",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Probability & Bayes Rule",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w2/index.html#naïve-bayes-introduction",
    "href": "notes/c1w2/index.html#naïve-bayes-introduction",
    "title": "Probability and Bayes Rule",
    "section": "Naïve Bayes Introduction",
    "text": "Naïve Bayes Introduction\nHere is a sample corpus\n\n\n\n\nTable 1: And these are the class frequencies and probabilities\n\n\n\n\n\n\n\n\n\n+ tweets\n- tweets\n\n\n\n\nI am happy because I am learning NLP\nI am sad, I am not learning NLP\n\n\nI am happy\nI am sad\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport string \nraw_tweets=[\n  \"I am happy because I am learning NLP\",\n  \"I am sad, I am not learning NLP\",\n  \"I am happy, not sad\",\n  \"I am sad, not happy\",\n]\ndef clean(tweet:str):\n  return  tweet.translate(str.maketrans('', '', string.punctuation)).lower()\ntweets = [clean(tweet) for tweet in raw_tweets]\nlabels=['+','-','+','-']\ndf = pd.DataFrame({'tweets': tweets, 'labels': labels})\ndf\n\n\n\n\n\n\n\n\ntweets\nlabels\n\n\n\n\n0\ni am happy because i am learning nlp\n+\n\n\n1\ni am sad i am not learning nlp\n-\n\n\n2\ni am happy not sad\n+\n\n\n3\ni am sad not happy\n-\n\n\n\n\n\n\n\n\nimport numpy as np\nfrom collections import Counter\np_freq,n_freq = Counter(), Counter()\n#print( df[df.labels == '+']['tweets'].to_list())\n[p_freq.update(tweet.split()) for tweet in df[df.labels == '+']['tweets'].to_list()]\n[n_freq.update(tweet.split()) for tweet in df[df.labels == '-']['tweets'].to_list()]\nprint(p_freq)\nprint(n_freq)\nvocab = list(set(p_freq.keys()).union(set(n_freq.keys())))\npos_freq = [p_freq[word] for word in vocab ]\nneg_freq = [n_freq[word] for word in vocab ]\nvocab_df=pd.DataFrame({'vocab':vocab,'pos_freq':pos_freq,'neg_freq':neg_freq})\nvocab_df['p_pos']=vocab_df.pos_freq/vocab_df.pos_freq.sum()\nvocab_df['p_neg']=vocab_df.neg_freq/vocab_df.neg_freq.sum()\nvocab_df['p_pos_sm']=(vocab_df.pos_freq+1)/(vocab_df.pos_freq.sum()+vocab_df.shape[1])\nvocab_df['p_neg_sm']=(vocab_df.neg_freq+1)/(vocab_df.neg_freq.sum()+vocab_df.shape[1])\nvocab_df['ratio']= vocab_df.p_pos_sm/vocab_df.p_neg_sm\nvocab_df['lambda']= np.log(vocab_df.p_pos_sm/vocab_df.p_neg_sm)\npd.set_option('display.float_format', '{:.2f}'.format)\nvocab_df\nprint(vocab_df.shape)\n\n[None, None]\n\n\n[None, None]\n\n\nCounter({'i': 3, 'am': 3, 'happy': 2, 'because': 1, 'learning': 1, 'nlp': 1, 'not': 1, 'sad': 1})\nCounter({'i': 3, 'am': 3, 'sad': 2, 'not': 2, 'learning': 1, 'nlp': 1, 'happy': 1})\n\n\n\n\n\n\n\n\n\nvocab\npos_freq\nneg_freq\np_pos\np_neg\np_pos_sm\np_neg_sm\nratio\nlambda\n\n\n\n\n0\nam\n3\n3\n0.23\n0.23\n0.22\n0.21\n1.06\n0.05\n\n\n1\nnlp\n1\n1\n0.08\n0.08\n0.11\n0.11\n1.06\n0.05\n\n\n2\nbecause\n1\n0\n0.08\n0.00\n0.11\n0.05\n2.11\n0.75\n\n\n3\nlearning\n1\n1\n0.08\n0.08\n0.11\n0.11\n1.06\n0.05\n\n\n4\nhappy\n2\n1\n0.15\n0.08\n0.17\n0.11\n1.58\n0.46\n\n\n5\nsad\n1\n2\n0.08\n0.15\n0.11\n0.16\n0.70\n-0.35\n\n\n6\ni\n3\n3\n0.23\n0.23\n0.22\n0.21\n1.06\n0.05\n\n\n7\nnot\n1\n2\n0.08\n0.15\n0.11\n0.16\n0.70\n-0.35\n\n\n\n\n\n\n\n(8, 9)\n\n\n\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\ntable = [[\"Sun\",696000,1989100000],\n         [\"Earth\",6371,5973.6],\n         [\"Moon\",1737,73.5],\n         [\"Mars\",3390,641.85]]\nMarkdown(tabulate(\n  table, \n  headers=[\"Planet\",\"R (km)\", \"mass (x 10^29 kg)\"]\n))\n\n\n\nTable 2: Planets\n\n\n\n\n\n\nPlanet\nR (km)\nmass (x 10^29 kg)\n\n\n\n\nSun\n696000\n1.9891e+09\n\n\nEarth\n6371\n5973.6\n\n\nMoon\n1737\n73.5\n\n\nMars\n3390\n641.85\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 3: Frequency Table\n\n\n\n\n\nword\n+\n-\n\n\n\n\nI\n3\n3\n\n\nam\n3\n3\n\n\nhappy\n2\n1\n\n\nbecause\n1\n0\n\n\nlearning\n1\n1\n\n\nNLP\n1\n1\n\n\nsad\n1\n2\n\n\nnot\n1\n2\n\n\n\n\n\n\n\n\n\n\n\nTable 4: Probabilities Table\n\n\n\n\n\nword\n+\n-\n\n\n\n\nI\n0.24\n0.25\n\n\nam\n0.24\n0.25\n\n\nhappy\n0.15\n0.08\n\n\nbecause\n0.08\n0.00\n\n\nlearning\n0.08\n0.08\n\n\nNLP\n0.08\n0.08\n\n\nsad\n0.08\n0.17\n\n\nnot\n0.08\n0.17\n\n\n\n\n\n\n\n\n\nLet’s motivate the Naïve Bayes inference condition rule for binary classification:\nTo build a classifier, we will first start by creating conditional probabilities given the table;\n\n\n\n\nNaïve Bayes\n\n\nWe want to find if given our prior knowledge of P(+) and P(-) if a new tweet has + or - sentiment.\nTo do that we will estimate p(+|T) and p(-|T) and then decide based on which is greater than 0.5.\n\n\n\n\n\nTable of probabilities\n\nWe can use the Bayes rule:\n\np(+|T) = \\frac{ p(T|+) \\times p(+) }{ p(T) }\n\nand\n\np(-|T) = \\frac{ p(T|-) \\times p(-) }{ p(T) }\n\nwhere:\n\np(+|T) is the posterior probability of a label + given tweet T\np(+) is our prior knowledge\np(T|+) is the likelihood of tweet T being +.\n{p(T)}\n\nThe term p(T) is in both terms and can be eliminated. However, it will cancel out when we use the ratio for the inference. This lets us compute the following table of probabilities; word am learning NLP Pos 0.24 0.08 0.08 Neg 0.25 0.08 0.08 .17 Naïve Bayes is the simplest probabilistic graphical model which comes with an independence assumption for the features.\n\np(T|+) = \\prod^m_{i=1}P(w_i|+) \\implies p(+|T)=\\frac{P(+)}{P(T)} \\prod^m_{i=1}P(w_i|+)\n\nand\n\np(T|−) = \\prod^m_{i=1}P(w_i|−) \\implies p(−|T) =  \\frac{P(−)}{P(T)} \\prod^m_{i=1} P(w_i|−)\n\nOnce we have the probabilities, we can compute the likelihood score as follows:\nTweet: I am happy today: I am learning. - Since there is no entry for today in our conditional probabilities table, this implies that this word is not in your vocabulary. So we’ll ignore its contribution to the overall score. - All the neutral words in the tweet such as I and am cancel out in the expression, as shown in the figure below.\n\n   \\prod^m_{i=1} \\frac{P(w_i|+)}{P(w_i|-)}= \\frac {0.14}{0.10} =1.4 &gt; 1\n\n\nA score greater than 1 indicates that the class is positive, otherwise, it is negative.\n\n\nP(+|T) &gt; P(−|T)\n\nthen we infer that the T has + sentiment. dividing by the right term we get the inference rule:\n\n\\frac{P(+|T)}{P(−|T)} &gt; 1\n which expands to : \n  \\frac {P(+|T)}{P(−|T)} = \\frac {P(+)}{P(-)}\\prod^m_{i=1} \\frac {P(w_i|+)}{P(w_i|−)} &gt; 1\n\nThis is the inference rule for naïve Bayes.\nNote: Naïve Bayes is a model which assumes all features are independent, so the basic component here is:\n\n\\frac{P(w_i|+)}{P(w_i|-)} &gt; 1\n the ratio of the probability that a word appears in a positive tweet and that it appears in a negative tweet",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Probability & Bayes Rule",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w2/index.html#additive-smoothing",
    "href": "notes/c1w2/index.html#additive-smoothing",
    "title": "Probability and Bayes Rule",
    "section": "Additive smoothing:",
    "text": "Additive smoothing:\n\np_{addative}(w_i|class)=\\frac{ freq(w,class)+\\delta}{ N_{class} + \\delta \\times V}",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Probability & Bayes Rule",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w2/index.html#more-alternatives-to-laplacian-smoothing",
    "href": "notes/c1w2/index.html#more-alternatives-to-laplacian-smoothing",
    "title": "Probability and Bayes Rule",
    "section": "More alternatives to Laplacian smoothing",
    "text": "More alternatives to Laplacian smoothing\n\n\n\n\nGood Turing smoothing\n\n\nKneser-Ney smoothing c.f. (Ney, Essen, and Kneser 1994) which corrects better for smaller data sets. \nGood-Turing smoothing c.f. (Good 1953) which uses order statistics to give even better estimates.\nwith a survey of the subject here: (Chen and Goodman 1996)",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Probability & Bayes Rule",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w2/index.html#sources-of-errors-in-naïve-bayes",
    "href": "notes/c1w2/index.html#sources-of-errors-in-naïve-bayes",
    "title": "Probability and Bayes Rule",
    "section": "Sources of Errors in Naïve Bayes",
    "text": "Sources of Errors in Naïve Bayes\n\nError Analysis\nBad sentiment classifications are due to:\n\npreprocessing dropping punctuation that encodes emotion like a sad smiley.\nWord order can contribute to meaning - breaking the independence assumption of our model\nPronouns removed as stop words - may encode emotion\nSarcasm can confound the model\nEuphemisms are also a challenge",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Probability & Bayes Rule",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w3/lab02.html",
    "href": "notes/c1w3/lab02.html",
    "title": "Manipulating word embeddings",
    "section": "",
    "text": "Figure 1: course banner\nIn this week’s assignment, we are going to use a pre-trained word embedding for finding word analogies and equivalence. This exercise can be used as an Intrinsic Evaluation for the word embedding performance. In this notebook, we will apply linear algebra operations using NumPy to find analogies between words manually. This will help we to prepare for this week’s assignment.\nimport pandas as pd # Library for Dataframes \nimport numpy as np # Library for math functions\nimport pickle # Python object serialization library. Not secure\n\nword_embeddings = pickle.load( open( \"./data/word_embeddings_subset.p\", \"rb\" ) )\nlen(word_embeddings) # there should be 243 words that will be used in this assignment\n\n243\nNow that the model is loaded, we can take a look at the word representations. First, note that word_embeddings is a dictionary. Each word is the key to the entry, and the value is its corresponding vector presentation. Remember that square brackets allow access to any entry if the key exists.\ncountryVector = word_embeddings['country'] # Get the vector representation for the word 'country'\nprint(type(countryVector)) # Print the type of the vector. Note it is a numpy array\nprint(countryVector) # Print the values of the vector.  \n\n&lt;class 'numpy.ndarray'&gt;\n[-0.08007812  0.13378906  0.14355469  0.09472656 -0.04736328 -0.02355957\n -0.00854492 -0.18652344  0.04589844 -0.08154297 -0.03442383 -0.11621094\n  0.21777344 -0.10351562 -0.06689453  0.15332031 -0.19335938  0.26367188\n -0.13671875 -0.05566406  0.07470703 -0.00070953  0.09375    -0.14453125\n  0.04296875 -0.01916504 -0.22558594 -0.12695312 -0.0168457   0.05224609\n  0.0625     -0.1484375  -0.01965332  0.17578125  0.10644531 -0.04760742\n -0.10253906 -0.28515625  0.10351562  0.20800781 -0.07617188 -0.04345703\n  0.08642578  0.08740234  0.11767578  0.20996094 -0.07275391  0.1640625\n -0.01135254  0.0025177   0.05810547 -0.03222656  0.06884766  0.046875\n  0.10107422  0.02148438 -0.16210938  0.07128906 -0.16210938  0.05981445\n  0.05102539 -0.05566406  0.06787109 -0.03759766  0.04345703 -0.03173828\n -0.03417969 -0.01116943  0.06201172 -0.08007812 -0.14941406  0.11914062\n  0.02575684  0.00302124  0.04711914 -0.17773438  0.04101562  0.05541992\n  0.00598145  0.03027344 -0.07666016 -0.109375    0.02832031 -0.10498047\n  0.0100708  -0.03149414 -0.22363281 -0.03125    -0.01147461  0.17285156\n  0.08056641 -0.10888672 -0.09570312 -0.21777344 -0.07910156 -0.10009766\n  0.06396484 -0.11962891  0.18652344 -0.02062988 -0.02172852  0.29296875\n -0.00793457  0.0324707  -0.15136719  0.00227356 -0.03540039 -0.13378906\n  0.0546875  -0.03271484 -0.01855469 -0.10302734 -0.13378906  0.11425781\n  0.16699219  0.01361084 -0.02722168 -0.2109375   0.07177734  0.08691406\n -0.09960938  0.01422119 -0.18261719  0.00741577  0.01965332  0.00738525\n -0.03271484 -0.15234375 -0.26367188 -0.14746094  0.03320312 -0.03344727\n -0.01000977  0.01855469  0.00183868 -0.10498047  0.09667969  0.07910156\n  0.11181641  0.13085938 -0.08740234 -0.1328125   0.05004883  0.19824219\n  0.0612793   0.16210938  0.06933594  0.01281738  0.01550293  0.01531982\n  0.11474609  0.02758789  0.13769531 -0.08349609  0.01123047 -0.20507812\n -0.12988281 -0.16699219  0.20410156 -0.03588867 -0.10888672  0.0534668\n  0.15820312 -0.20410156  0.14648438 -0.11572266  0.01855469 -0.13574219\n  0.24121094  0.12304688 -0.14550781  0.17578125  0.11816406 -0.30859375\n  0.10888672 -0.22363281  0.19335938 -0.15722656 -0.07666016 -0.09082031\n -0.19628906 -0.23144531 -0.09130859 -0.14160156  0.06347656  0.03344727\n -0.03369141  0.06591797  0.06201172  0.3046875   0.16796875 -0.11035156\n -0.03833008 -0.02563477 -0.09765625  0.04467773 -0.0534668   0.11621094\n -0.15039062 -0.16308594 -0.15527344  0.04638672  0.11572266 -0.06640625\n -0.04516602  0.02331543 -0.08105469 -0.0255127  -0.07714844  0.0016861\n  0.15820312  0.00994873 -0.06445312  0.15722656 -0.03112793  0.10644531\n -0.140625    0.23535156 -0.11279297  0.16015625  0.00061798 -0.1484375\n  0.02307129 -0.109375    0.05444336 -0.14160156  0.11621094  0.03710938\n  0.14746094 -0.04199219 -0.01391602 -0.03881836  0.02783203  0.10205078\n  0.07470703  0.20898438 -0.04223633 -0.04150391 -0.00588989 -0.14941406\n -0.04296875 -0.10107422 -0.06176758  0.09472656  0.22265625 -0.02307129\n  0.04858398 -0.15527344 -0.02282715 -0.04174805  0.16699219 -0.09423828\n  0.14453125  0.11132812  0.04223633 -0.16699219  0.10253906  0.16796875\n  0.12597656 -0.11865234 -0.0213623  -0.08056641  0.24316406  0.15527344\n  0.16503906  0.00854492 -0.12255859  0.08691406 -0.11914062 -0.02941895\n  0.08349609 -0.03100586  0.13964844 -0.05151367  0.00765991 -0.04443359\n -0.04980469 -0.03222656 -0.00952148 -0.10888672 -0.10302734 -0.15722656\n  0.19335938  0.04858398  0.015625   -0.08105469 -0.11621094 -0.01989746\n  0.05737305  0.06103516 -0.14550781  0.06738281 -0.24414062 -0.07714844\n  0.04760742 -0.07519531 -0.14941406 -0.04418945  0.09716797  0.06738281]\nIt is important to note that we store each vector as a NumPy array. It allows us to use the linear algebra operations on it.\nThe vectors have a size of 300, while the vocabulary size of Google News is around 3 million words!\n#Get the vector for a given word:\ndef vec(w):\n    return word_embeddings[w]",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "L2 - Manipulating word embeddings"
    ]
  },
  {
    "objectID": "notes/c1w3/lab02.html#operating-on-word-embeddings",
    "href": "notes/c1w3/lab02.html#operating-on-word-embeddings",
    "title": "Manipulating word embeddings",
    "section": "Operating on word embeddings",
    "text": "Operating on word embeddings\nRemember that understanding the data is one of the most critical steps in Data Science. Word embeddings are the result of machine learning processes and will be part of the input for further processes. These word embedding needs to be validated or at least understood because the performance of the derived model will strongly depend on its quality.\nWord embeddings are multidimensional arrays, usually with hundreds of attributes that pose a challenge for its interpretation.\nIn this notebook, we will visually inspect the word embedding of some words using a pair of attributes. Raw attributes are not the best option for the creation of such charts but will allow us to illustrate the mechanical part in Python.\nIn the next cell, we make a beautiful plot for the word embeddings of some words. Even if plotting the dots gives an idea of the words, the arrow representations help to visualize the vector’s alignment as well.\n\nimport matplotlib.pyplot as plt # Import matplotlib\n%matplotlib inline\n\nwords = ['oil', 'gas', 'happy', 'sad', 'city', 'town', 'village', 'country', 'continent', 'petroleum', 'joyful']\n\nbag2d = np.array([vec(word) for word in words]) # Convert each word to its vector representation\n\nfig, ax = plt.subplots(figsize = (10, 10)) # Create custom size image\n\ncol1 = 3 # Select the column for the x axis\ncol2 = 2 # Select the column for the y axis\n\n# Print an arrow for each word\nfor word in bag2d:\n    ax.arrow(0, 0, word[col1], word[col2], head_width=0.005, head_length=0.005, fc='r', ec='r', width = 1e-5)\n\n    \nax.scatter(bag2d[:, col1], bag2d[:, col2]); # Plot a dot for each word\n\n# Add the word label over each dot in the scatter plot\nfor i in range(0, len(words)):\n    ax.annotate(words[i], (bag2d[i, col1], bag2d[i, col2]))\n\n\nplt.show()\n\nText(0.06396484375, -0.279296875, 'oil')\n\n\nText(0.01080322265625, -0.138671875, 'gas')\n\n\nText(0.025390625, 0.00160980224609375, 'happy')\n\n\nText(-0.044677734375, 0.06689453125, 'sad')\n\n\nText(-0.0400390625, 0.18359375, 'city')\n\n\nText(-0.1611328125, 0.030029296875, 'town')\n\n\nText(-0.2158203125, 0.09033203125, 'village')\n\n\nText(0.0947265625, 0.1435546875, 'country')\n\n\nText(0.1279296875, 0.2392578125, 'continent')\n\n\nText(0.10888671875, -0.22265625, 'petroleum')\n\n\nText(0.083984375, 0.1298828125, 'joyful')\n\n\n\n\n\n\n\n\n\nNote that similar words like ‘village’ and ‘town’ or ‘petroleum’, ‘oil’, and ‘gas’ tend to point in the same direction. Also, note that ‘sad’ and ‘happy’ looks close to each other; however, the vectors point in opposite directions.\nIn this chart, one can figure out the angles and distances between the words. Some words are close in both kinds of distance metrics.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "L2 - Manipulating word embeddings"
    ]
  },
  {
    "objectID": "notes/c1w3/lab02.html#word-distance",
    "href": "notes/c1w3/lab02.html#word-distance",
    "title": "Manipulating word embeddings",
    "section": "Word distance",
    "text": "Word distance\nNow plot the words ‘sad’, ‘happy’, ‘town’, and ‘village’. In this same chart, display the vector from ‘village’ to ‘town’ and the vector from ‘sad’ to ‘happy’. Let us use NumPy for these linear algebra operations.\n\nwords = ['sad', 'happy', 'town', 'village']\n\nbag2d = np.array([vec(word) for word in words]) # Convert each word to its vector representation\n\nfig, ax = plt.subplots(figsize = (10, 10)) # Create custom size image\n\ncol1 = 3 # Select the column for the x axe\ncol2 = 2 # Select the column for the y axe\n\n# Print an arrow for each word\nfor word in bag2d:\n    ax.arrow(0, 0, word[col1], word[col2], head_width=0.0005, head_length=0.0005, fc='r', ec='r', width = 1e-5)\n    \n# print the vector difference between village and town\nvillage = vec('village')\ntown = vec('town')\ndiff = town - village\nax.arrow(village[col1], village[col2], diff[col1], diff[col2], fc='b', ec='b', width = 1e-5)\n\n# print the vector difference between village and town\nsad = vec('sad')\nhappy = vec('happy')\ndiff = happy - sad\nax.arrow(sad[col1], sad[col2], diff[col1], diff[col2], fc='b', ec='b', width = 1e-5)\n\n\nax.scatter(bag2d[:, col1], bag2d[:, col2]); # Plot a dot for each word\n\n# Add the word label over each dot in the scatter plot\nfor i in range(0, len(words)):\n    ax.annotate(words[i], (bag2d[i, col1], bag2d[i, col2]))\n\n\nplt.show()\n\nText(-0.044677734375, 0.06689453125, 'sad')\n\n\nText(0.025390625, 0.00160980224609375, 'happy')\n\n\nText(-0.1611328125, 0.030029296875, 'town')\n\n\nText(-0.2158203125, 0.09033203125, 'village')",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "L2 - Manipulating word embeddings"
    ]
  },
  {
    "objectID": "notes/c1w3/lab02.html#linear-algebra-on-word-embeddings",
    "href": "notes/c1w3/lab02.html#linear-algebra-on-word-embeddings",
    "title": "Manipulating word embeddings",
    "section": "Linear algebra on word embeddings",
    "text": "Linear algebra on word embeddings\nIn the lectures, we saw the analogies between words using algebra on word embeddings. Let us see how to do it in Python with Numpy.\nTo start, get the norm of a word in the word embedding.\n\nprint(np.linalg.norm(vec('town'))) # Print the norm of the word town\nprint(np.linalg.norm(vec('sad'))) # Print the norm of the word sad\n\n2.3858097\n2.9004838",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "L2 - Manipulating word embeddings"
    ]
  },
  {
    "objectID": "notes/c1w3/lab02.html#predicting-capitals",
    "href": "notes/c1w3/lab02.html#predicting-capitals",
    "title": "Manipulating word embeddings",
    "section": "Predicting capitals",
    "text": "Predicting capitals\nNow, applying vector difference and addition, one can create a vector representation for a new word. For example, we can say that the vector difference between ‘France’ and ‘Paris’ represents the concept of Capital.\nOne can move from the city of Madrid in the direction of the concept of Capital, and obtain something close to the corresponding country to which Madrid is the Capital.\n\ncapital = vec('France') - vec('Paris')\ncountry = vec('Madrid') + capital\n\nprint(country[0:5]) # Print the first 5 values of the vector\n\n[-0.02905273 -0.2475586   0.53952026  0.20581055 -0.14862823]\n\n\nWe can observe that the vector ‘country’ that we expected to be the same as the vector for Spain is not exactly it.\n\ndiff = country - vec('Spain')\nprint(diff[0:10])\n\n[-0.06054688 -0.06494141  0.37643433  0.08129883 -0.13007355 -0.00952148\n -0.03417969 -0.00708008  0.09790039 -0.01867676]\n\n\nSo, we have to look for the closest words in the embedding that matches the candidate country. If the word embedding works as expected, the most similar word must be ‘Spain’. Let us define a function that helps us to do it. We will store our word embedding as a DataFrame, which facilitate the lookup operations based on the numerical vectors.\n\n# Create a dataframe out of the dictionary embedding. This facilitate the algebraic operations\nkeys = word_embeddings.keys()\ndata = []\nfor key in keys:\n    data.append(word_embeddings[key])\n\nembedding = pd.DataFrame(data=data, index=keys)\n# Define a function to find the closest word to a vector:\ndef find_closest_word(v, k = 1):\n    # Calculate the vector difference from each word to the input vector\n    diff = embedding.values - v \n    # Get the norm of each difference vector. \n    # It means the squared euclidean distance from each word to the input vector\n    delta = np.sum(diff * diff, axis=1)\n    # Find the index of the minimun distance in the array\n    i = np.argmin(delta)\n    # Return the row name for this item\n    return embedding.iloc[i].name\n\n\n# Print some rows of the embedding as a Dataframe\nembedding.head(10)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n\n\n\n\ncountry\n-0.080078\n0.133789\n0.143555\n0.094727\n-0.047363\n-0.023560\n-0.008545\n-0.186523\n0.045898\n-0.081543\n...\n-0.145508\n0.067383\n-0.244141\n-0.077148\n0.047607\n-0.075195\n-0.149414\n-0.044189\n0.097168\n0.067383\n\n\ncity\n-0.010071\n0.057373\n0.183594\n-0.040039\n-0.029785\n-0.079102\n0.071777\n0.013306\n-0.143555\n0.011292\n...\n0.024292\n-0.168945\n-0.062988\n0.117188\n-0.020508\n0.030273\n-0.247070\n-0.122559\n0.076172\n-0.234375\n\n\nChina\n-0.073242\n0.135742\n0.108887\n0.083008\n-0.127930\n-0.227539\n0.151367\n-0.045654\n-0.065430\n0.034424\n...\n0.140625\n0.087402\n0.152344\n0.079590\n0.006348\n-0.037842\n-0.183594\n0.137695\n0.093750\n-0.079590\n\n\nIraq\n0.191406\n0.125000\n-0.065430\n0.060059\n-0.285156\n-0.102539\n0.117188\n-0.351562\n-0.095215\n0.200195\n...\n-0.100586\n-0.077148\n-0.123047\n0.193359\n-0.153320\n0.089355\n-0.173828\n-0.054688\n0.302734\n0.105957\n\n\noil\n-0.139648\n0.062256\n-0.279297\n0.063965\n0.044434\n-0.154297\n-0.184570\n-0.498047\n0.047363\n0.110840\n...\n-0.195312\n-0.345703\n0.217773\n-0.091797\n0.051025\n0.061279\n0.194336\n0.204102\n0.235352\n-0.051025\n\n\ntown\n0.123535\n0.159180\n0.030029\n-0.161133\n0.015625\n0.111816\n0.039795\n-0.196289\n-0.039307\n0.067871\n...\n-0.007935\n-0.091797\n-0.265625\n0.029297\n0.089844\n-0.049805\n-0.202148\n-0.079590\n0.068848\n-0.164062\n\n\nCanada\n-0.136719\n-0.154297\n0.269531\n0.273438\n0.086914\n-0.076172\n-0.018677\n0.006256\n0.077637\n-0.211914\n...\n0.105469\n0.030762\n-0.039307\n0.183594\n-0.117676\n0.191406\n0.074219\n0.020996\n0.285156\n-0.257812\n\n\nLondon\n-0.267578\n0.092773\n-0.238281\n0.115234\n-0.006836\n0.221680\n-0.251953\n-0.055420\n0.020020\n0.149414\n...\n-0.008667\n-0.008484\n-0.053223\n0.197266\n-0.296875\n0.064453\n0.091797\n0.058350\n0.022583\n-0.101074\n\n\nEngland\n-0.198242\n0.115234\n0.062500\n-0.058350\n0.226562\n0.045898\n-0.062256\n-0.202148\n0.080566\n0.021606\n...\n0.135742\n0.109375\n-0.121582\n0.008545\n-0.171875\n0.086914\n0.070312\n0.003281\n0.069336\n0.056152\n\n\nAustralia\n0.048828\n-0.194336\n-0.041504\n0.084473\n-0.114258\n-0.208008\n-0.164062\n-0.269531\n0.079102\n0.275391\n...\n0.021118\n0.171875\n0.042236\n0.221680\n-0.239258\n-0.106934\n0.030884\n0.006622\n0.051270\n-0.135742\n\n\n\n\n10 rows × 300 columns\n\n\n\nNow let us find the name that corresponds to our numerical country:\n\nfind_closest_word(country)\n\n'Spain'",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "L2 - Manipulating word embeddings"
    ]
  },
  {
    "objectID": "notes/c1w3/lab02.html#predicting-other-countries",
    "href": "notes/c1w3/lab02.html#predicting-other-countries",
    "title": "Manipulating word embeddings",
    "section": "Predicting other Countries",
    "text": "Predicting other Countries\n\nfind_closest_word(vec('Italy') - vec('Rome') + vec('Madrid'))\n\n'Spain'\n\n\n\nprint(find_closest_word(vec('Berlin') + capital))\nprint(find_closest_word(vec('Beijing') + capital))\n\nGermany\nChina\n\n\nHowever, it does not always work.\n\nprint(find_closest_word(vec('Lisbon') + capital))\n\nLisbon",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "L2 - Manipulating word embeddings"
    ]
  },
  {
    "objectID": "notes/c1w3/lab02.html#represent-a-sentence-as-a-vector",
    "href": "notes/c1w3/lab02.html#represent-a-sentence-as-a-vector",
    "title": "Manipulating word embeddings",
    "section": "Represent a sentence as a vector",
    "text": "Represent a sentence as a vector\nA whole sentence can be represented as a vector by summing all the word vectors that conform to the sentence. Let us see.\n\ndoc = \"Spain petroleum city king\"\nvdoc = [vec(x) for x in doc.split(\" \")]\ndoc2vec = np.sum(vdoc, axis = 0)\ndoc2vec\n\narray([ 2.87475586e-02,  1.03759766e-01,  1.32629395e-01,  3.33007812e-01,\n       -2.61230469e-02, -5.95703125e-01, -1.25976562e-01, -1.01306152e+00,\n       -2.18544006e-01,  6.60705566e-01, -2.58300781e-01, -2.09960938e-02,\n       -7.71484375e-02, -3.07128906e-01, -5.94726562e-01,  2.00561523e-01,\n       -1.04980469e-02, -1.10748291e-01,  4.82177734e-02,  6.38977051e-01,\n        2.36083984e-01, -2.69775391e-01,  3.90625000e-02,  4.16503906e-01,\n        2.83416748e-01, -7.25097656e-02, -3.12988281e-01,  1.05712891e-01,\n        3.22265625e-02,  2.38403320e-01,  3.88183594e-01, -7.51953125e-02,\n       -1.26281738e-01,  6.60644531e-01, -7.89794922e-01, -7.04345703e-02,\n       -1.14379883e-01, -4.78515625e-02,  4.76318359e-01,  5.31127930e-01,\n        8.10546875e-02, -1.17553711e-01,  1.02050781e+00,  5.59814453e-01,\n       -1.17187500e-01,  1.21826172e-01, -5.51574707e-01,  1.44531250e-01,\n       -7.66113281e-01,  5.36102295e-01, -2.80029297e-01,  3.85986328e-01,\n       -2.39135742e-01, -2.86865234e-02, -5.10498047e-01,  2.59658813e-01,\n       -7.52929688e-01,  4.32128906e-02, -7.17773438e-02, -1.26708984e-01,\n        4.40673828e-02,  5.12939453e-01, -5.15808105e-01,  1.20117188e-01,\n       -5.52978516e-02, -3.92089844e-01, -3.15917969e-01,  1.57226562e-01,\n       -3.19702148e-01,  1.75170898e-01, -3.81835938e-01, -2.07031250e-01,\n       -4.72717285e-02, -2.79296875e-01, -3.29040527e-01, -1.69067383e-01,\n        1.61132812e-02,  1.71569824e-01,  5.73730469e-02, -2.44140625e-03,\n        8.34960938e-02, -1.58203125e-01, -3.10119629e-01,  5.28564453e-02,\n        8.60595703e-02,  5.12695312e-02, -7.22900391e-01,  4.97924805e-01,\n       -5.85937500e-03,  4.49951172e-01,  3.82446289e-01, -2.80029297e-01,\n       -3.28125000e-01, -6.27441406e-02, -4.81933594e-01,  1.93176270e-02,\n       -1.69326782e-01, -4.28649902e-01,  5.39062500e-01, -1.28417969e-01,\n       -8.83789062e-02,  5.13916016e-01,  9.13085938e-02, -1.60156250e-01,\n        6.86035156e-02, -9.74121094e-02, -3.70712280e-01, -3.27270508e-01,\n        1.77978516e-01, -4.65332031e-01,  1.70410156e-01,  9.08203125e-02,\n        2.76857376e-01, -1.69677734e-01,  3.27728271e-01, -3.12500000e-02,\n       -2.20809937e-01, -3.46679688e-01,  4.67407227e-01,  5.31860352e-01,\n       -1.30615234e-01, -2.36816406e-02, -6.56250000e-01, -5.79589844e-01,\n       -2.05810547e-01, -3.03222656e-01,  1.94259644e-01, -7.28515625e-01,\n       -4.92522240e-01, -5.37109375e-01, -3.47656250e-01,  1.08642578e-01,\n       -1.41601562e-01, -2.07031250e-01,  2.52441406e-01, -7.78808594e-02,\n       -5.02441406e-01,  1.53808594e-02,  8.64257812e-02,  2.59765625e-01,\n        6.64062500e-02, -7.12890625e-01, -1.45751953e-01,  7.56835938e-03,\n        4.87792969e-01,  1.39160156e-01,  1.15722656e-01,  1.28662109e-01,\n       -4.75585938e-01,  2.21191406e-01,  3.25317383e-01,  1.06323242e-01,\n       -6.11083984e-01, -3.59619141e-01,  6.54296875e-02, -2.41699219e-01,\n       -6.29882812e-02, -1.62109375e-01,  4.26269531e-01, -4.38354492e-01,\n        1.93725586e-01,  4.89562988e-01,  5.31494141e-01, -7.29370117e-02,\n        1.77246094e-01,  9.39941406e-02,  2.92236328e-01, -2.74047852e-01,\n        2.63366699e-02,  4.36035156e-01, -3.76953125e-01,  3.10546875e-01,\n        4.87304688e-01, -2.43041992e-01,  1.21612549e-02, -3.80371094e-01,\n        3.80493164e-01, -6.22436523e-01, -3.98071289e-01,  1.24206543e-01,\n       -8.20312500e-01, -2.72583008e-01, -6.21582031e-01, -4.87060547e-01,\n        3.06671143e-01, -2.61230469e-01,  5.12451172e-01,  5.55694580e-01,\n        5.66894531e-01,  7.33886719e-01, -1.75781250e-01,  4.13574219e-01,\n       -2.54272461e-01,  1.32507324e-01, -4.78515625e-01,  4.63256836e-01,\n       -6.21948242e-02, -1.80664062e-01, -5.46386719e-01, -6.31103516e-01,\n       -1.47949219e-01, -3.15185547e-01, -7.12890625e-02, -7.67578125e-01,\n        3.92272949e-01, -1.97753906e-01,  2.23144531e-01, -5.07324219e-01,\n        8.39843750e-02, -4.98657227e-02,  1.01074219e-01,  2.07885742e-01,\n       -2.77343750e-01,  1.03027344e-01, -1.38671875e-01,  2.87353516e-01,\n       -4.81895447e-01, -1.66748047e-01, -1.47277832e-01,  3.61633301e-01,\n        6.38504028e-02, -6.69189453e-01,  1.95312500e-03, -7.34375000e-01,\n       -1.28158569e-01,  9.76562500e-04, -7.08007812e-02,  3.72558594e-01,\n        8.31176758e-01,  5.94482422e-01,  5.37109375e-02, -3.00140381e-01,\n       -4.53857422e-01,  1.11511230e-01, -1.32812500e-01,  1.25732422e-01,\n        3.39843750e-01, -2.48352051e-01, -1.62353516e-02, -2.84667969e-01,\n        4.70703125e-01, -4.48242188e-01,  8.50753784e-02,  2.69042969e-01,\n        3.98254395e-03, -3.53759766e-01, -3.90625000e-02, -3.22753906e-01,\n       -6.90917969e-02, -4.13818359e-02,  1.35314941e-01, -8.50396156e-02,\n        1.28417969e-01,  6.15966797e-01,  3.55957031e-01, -6.05468750e-02,\n       -2.25463867e-01, -2.62207031e-01, -2.72949219e-01, -5.16113281e-01,\n        1.59179688e-01,  2.74902344e-01, -7.61718750e-02, -3.41796875e-03,\n        4.37500000e-01,  2.98583984e-01, -4.40795898e-01, -3.43261719e-01,\n        1.73583984e-01,  3.32092285e-01, -2.12646484e-01,  5.76171875e-01,\n        2.06787109e-01, -7.91015625e-02,  5.79695702e-02, -1.01806641e-01,\n       -7.06787109e-01, -3.40576172e-02, -4.11865234e-01,  9.82666016e-02,\n       -1.70410156e-01, -4.18212891e-01,  8.39233398e-01, -1.15722656e-01,\n        1.28173828e-01, -2.07763672e-01, -4.08203125e-01, -1.77612305e-01,\n        1.01196289e-01,  4.24072266e-01, -5.26428223e-02, -5.58593750e-01,\n        1.12304688e-02, -1.12060547e-01, -9.42382812e-02,  2.35595703e-02,\n       -3.92578125e-01, -7.12890625e-02,  5.69824219e-01,  9.81445312e-02],\n      dtype=float32)\n\n\n\nfind_closest_word(doc2vec)\n\n'petroleum'\n\n\nCongratulations! We have finished the introduction to word embeddings manipulation!",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Vector Space Models & PCA",
      "L2 - Manipulating word embeddings"
    ]
  },
  {
    "objectID": "notes/c2w2/lab02.html",
    "href": "notes/c2w2/lab02.html",
    "title": "Parts-of-Speech Tagging - Working with tags and Numpy",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure 1\n\n\nIn this lecture notebook we will create a matrix using some tag information and then modify it using different approaches. This will serve as hands-on experience working with Numpy and as an introduction to some elements used for POS tagging.\n\nimport numpy as np\nimport pandas as pd\n\n\nSome information on tags\nFor this notebook we will be using a toy example including only three tags (or states). In a real world application there are many more tags which can be found here.\n\n# Define tags for Adverb, Noun and To (the preposition) , respectively\ntags = ['RB', 'NN', 'TO']\n\nIn this week’s assignment we will construct some dictionaries that provide useful information of the tags and words we will be working with.\nOne of these dictionaries is the transition_counts which counts the number of times a particular tag happened next to another. The keys of this dictionary have the form (previous_tag, tag) and the values are the frequency of occurrences.\nAnother one is the emission_counts dictionary which will count the number of times a particular pair of (tag, word) appeared in the training dataset.\nIn general think of transition when working with tags only and of emission when working with tags and words.\nIn this notebook we will be looking at the first one:\n\n# Define 'transition_counts' dictionary\n# Note: values are the same as the ones in the assignment\ntransition_counts = {\n    ('NN', 'NN'): 16241,\n    ('RB', 'RB'): 2263,\n    ('TO', 'TO'): 2,\n    ('NN', 'TO'): 5256,\n    ('RB', 'TO'): 855,\n    ('TO', 'NN'): 734,\n    ('NN', 'RB'): 2431,\n    ('RB', 'NN'): 358,\n    ('TO', 'RB'): 200\n}\n\nNotice that there are 9 combinations of the 3 tags used. Each tag can appear after the same tag so we should include those as well.\n\n\nUsing Numpy for matrix creation\nNow we will create a matrix that includes these frequencies using Numpy arrays:\n\n# Store the number of tags in the 'num_tags' variable\nnum_tags = len(tags)\n\n# Initialize a 3X3 numpy array with zeros\ntransition_matrix = np.zeros((num_tags, num_tags))\n\n# Print matrix\ntransition_matrix\n\narray([[0., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.]])\n\n\nVisually we can see the matrix has the correct dimensions. Don’t forget we can check this too using the shape attribute:\n\n# Print shape of the matrix\ntransition_matrix.shape\n\n(3, 3)\n\n\nBefore filling this matrix with the values of the transition_counts dictionary we should sort the tags so that their placement in the matrix is consistent:\n\n# Create sorted version of the tag's list\nsorted_tags = sorted(tags)\n\n# Print sorted list\nsorted_tags\n\n['NN', 'RB', 'TO']\n\n\nTo fill this matrix with the correct values we can use a double for loop. We could also use itertools.product to one line this double loop:\n\n# Loop rows\nfor i in range(num_tags):\n    # Loop columns\n    for j in range(num_tags):\n        # Define tag pair\n        tag_tuple = (sorted_tags[i], sorted_tags[j])\n        # Get frequency from transition_counts dict and assign to (i, j) position in the matrix\n        transition_matrix[i, j] = transition_counts.get(tag_tuple)\n\n# Print matrix\ntransition_matrix\n\narray([[1.6241e+04, 2.4310e+03, 5.2560e+03],\n       [3.5800e+02, 2.2630e+03, 8.5500e+02],\n       [7.3400e+02, 2.0000e+02, 2.0000e+00]])\n\n\nLooks like this worked fine. However the matrix can be hard to read as Numpy is more about efficiency, rather than presenting values in a pretty format.\nFor this we can use a Pandas DataFrame. In particular, a function that takes the matrix as input and prints out a pretty version of it will be very useful:\n\n# Define 'print_matrix' function\ndef print_matrix(matrix):\n    print(pd.DataFrame(matrix, index=sorted_tags, columns=sorted_tags))\n\nNotice that the tags are not a parameter of the function. This is because the sorted_tags list will not change in the rest of the notebook so it is safe to use the variable previously declared. To test this function simply run:\n\n# Print the 'transition_matrix' by calling the 'print_matrix' function\nprint_matrix(transition_matrix)\n\n         NN      RB      TO\nNN  16241.0  2431.0  5256.0\nRB    358.0  2263.0   855.0\nTO    734.0   200.0     2.0\n\n\nThat is a lot better, isn’t it?\nAs we may have already deducted this matrix is not symmetrical.\n\n\nWorking with Numpy for matrix manipulation\nNow that we got the matrix set up it is time to see how a matrix can be manipulated after being created.\nNumpy allows vectorized operations which means that operations that would normally include looping over the matrix can be done in a simpler manner. This is consistent with treating numpy arrays as matrices since we get support for common matrix operations. We can do matrix multiplication, scalar multiplication, vector addition and many more!\nFor instance try scaling each value in the matrix by a factor of \\frac{1}{10}. Normally we would loop over each value in the matrix, updating them accordingly. But in Numpy this is as easy as dividing the whole matrix by 10:\n\n# Scale transition matrix\ntransition_matrix = transition_matrix/10\n\n# Print scaled matrix\nprint_matrix(transition_matrix)\n\n        NN     RB     TO\nNN  1624.1  243.1  525.6\nRB    35.8  226.3   85.5\nTO    73.4   20.0    0.2\n\n\nAnother trickier example is to normalize each row so that each value is equal to \\frac{value}{sum \\,of \\,row}.\nThis can be easily done with vectorization. First we will compute the sum of each row:\n\n# Compute sum of row for each row\nrows_sum = transition_matrix.sum(axis=1, keepdims=True)\n\n# Print sum of rows\nrows_sum\n\narray([[2392.8],\n       [ 347.6],\n       [  93.6]])\n\n\nNotice that the sum() method was used. This method does exactly what its name implies. Since the sum of the rows was desired the axis was set to 1. In Numpy axis=1 refers to the columns so the sum is done by summing each column of a particular row, for each row.\nAlso the keepdims parameter was set to True so the resulting array had shape (3, 1) rather than (3,). This was done so that the axes were consistent with the desired operation.\nWhen working with Numpy, always remember to check the shape of the arrays we are working with, many unexpected errors happen because of axes not being consistent. The shape attribute is your friend for these cases.\n\n# Normalize transition matrix\ntransition_matrix = transition_matrix / rows_sum\n\n# Print normalized matrix\nprint_matrix(transition_matrix)\n\n          NN        RB        TO\nNN  0.678745  0.101596  0.219659\nRB  0.102992  0.651036  0.245972\nTO  0.784188  0.213675  0.002137\n\n\nNotice that the normalization that was carried out forces the sum of each row to be equal to 1. We can easily check this by running the sum method on the resulting matrix:\n\ntransition_matrix.sum(axis=1, keepdims=True)\n\narray([[1.],\n       [1.],\n       [1.]])\n\n\nFor a final example we are asked to modify each value of the diagonal of the matrix so that they are equal to the log of the sum of the current row plus the current value. When doing mathematical operations like this one don’t forget to import the math module.\nThis can be done using a standard for loop or vectorization. You’ll see both in action:\n\nimport math\n\n# Copy transition matrix for for-loop example\nt_matrix_for = np.copy(transition_matrix)\n\n# Copy transition matrix for numpy functions example\nt_matrix_np = np.copy(transition_matrix)\n\n\nUsing a for-loop\n\n# Loop values in the diagonal\nfor i in range(num_tags):\n    t_matrix_for[i, i] =  t_matrix_for[i, i] + math.log(rows_sum[i])\n\n# Print matrix\nprint_matrix(t_matrix_for)\n\n          NN        RB        TO\nNN  8.458964  0.101596  0.219659\nRB  0.102992  6.502088  0.245972\nTO  0.784188  0.213675  4.541167\n\n\n/tmp/ipykernel_128864/84584535.py:3: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  t_matrix_for[i, i] =  t_matrix_for[i, i] + math.log(rows_sum[i])\n\n\n\n\nUsing vectorization\n\n# Save diagonal in a numpy array\nd = np.diag(t_matrix_np)\n\n# Print shape of diagonal\nd.shape\n\n(3,)\n\n\nWe can save the diagonal in a numpy array using Numpy’s diag() function. Notice that this array has shape (3,) so it is inconsistent with the dimensions of the rows_sum array which are (3, 1). You’ll have to reshape before moving forward. For this we can use Numpy’s reshape() function, specifying the desired shape in a tuple:\n\n# Reshape diagonal numpy array\nd = np.reshape(d, (3,1))\n\n# Print shape of diagonal\nd.shape\n\n(3, 1)\n\n\nNow that the diagonal has the correct shape we can do the vectorized operation by applying the math.log() function to the rows_sum array and adding the diagonal.\nTo apply a function to each element of a numpy array use Numpy’s vectorize() function providing the desired function as a parameter. This function returns a vectorized function that accepts a numpy array as a parameter.\nTo update the original matrix we can use Numpy’s fill_diagonal() function.\n\n# Perform the vectorized operation\nd = d + np.vectorize(math.log)(rows_sum)\n\n# Use numpy's 'fill_diagonal' function to update the diagonal\nnp.fill_diagonal(t_matrix_np, d)\n\n# Print the matrix\nprint_matrix(t_matrix_np)\n\n          NN        RB        TO\nNN  8.458964  0.101596  0.219659\nRB  0.102992  6.502088  0.245972\nTO  0.784188  0.213675  4.541167\n\n\nTo perform a sanity check that both methods yield the same result we can compare both matrices. Notice that this operation is also vectorized so we will get the equality check for each element in both matrices:\n\n# Check for equality\nt_matrix_for == t_matrix_np\n\narray([[ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True]])\n\n\nCongratulations on finishing this lecture notebook! Now we should be more familiar with some elements used by a POS tagger such as the transition_counts dictionary and with working with Numpy.\nKeep it up!\n\n\n\n\n\nCitationBibTeX citation:@online{bochman2020,\n  author = {Bochman, Oren},\n  title = {Parts-of-Speech {Tagging} - {Working} with Tags and {Numpy}},\n  date = {2020-10-22},\n  url = {https://orenbochman.github.io/notes-nlp/notes/c2w2/lab02.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2020. “Parts-of-Speech Tagging - Working with Tags\nand Numpy.” October 22, 2020. https://orenbochman.github.io/notes-nlp/notes/c2w2/lab02.html.",
    "crumbs": [
      "Home",
      "Probabilistic Models",
      "POS tagging & HMMS",
      "L2 - Working with tags and Numpy"
    ]
  },
  {
    "objectID": "notes/c1w1/lab01.html",
    "href": "notes/c1w1/lab01.html",
    "title": "Lab: Preprocessing",
    "section": "",
    "text": "course banner\n\n\n\n\nFigure 1\nIn this lab, we will be exploring how to preprocess tweets for sentiment analysis. We will provide a function for preprocessing tweets during this week’s assignment, but it is still good to know what is going on under the hood. By the end of this lecture, we will see how to use the NLTK package to perform a preprocessing pipeline for Twitter datasets.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "L1 - Preprocessing"
    ]
  },
  {
    "objectID": "notes/c1w1/lab01.html#setup",
    "href": "notes/c1w1/lab01.html#setup",
    "title": "Lab: Preprocessing",
    "section": "Setup",
    "text": "Setup\nWe will be doing sentiment analysis on tweets in the first two weeks of this course. To help with that, we will be using the Natural Language Toolkit (NLTK) package, an open-source Python library for natural language processing. It has modules for collecting, handling, and processing Twitter data, and we will be acquainted with them as we move along the course.\nFor this exercise, we will use a Twitter dataset that comes with NLTK. This dataset has been manually annotated and serves to establish baselines for models quickly. Let us import them now as well as a few other libraries we will be using.\n\nimport nltk                                # Python library for NLP\nfrom nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\nimport matplotlib.pyplot as plt            # library for visualization\nimport random                              # pseudo-random number generator",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "L1 - Preprocessing"
    ]
  },
  {
    "objectID": "notes/c1w1/lab01.html#about-the-twitter-dataset",
    "href": "notes/c1w1/lab01.html#about-the-twitter-dataset",
    "title": "Lab: Preprocessing",
    "section": "About the Twitter dataset",
    "text": "About the Twitter dataset\nThe sample dataset from NLTK is separated into positive and negative tweets. It contains 5000 positive tweets and 5000 negative tweets exactly. The exact match between these classes is not a coincidence. The intention is to have a balanced dataset. That does not reflect the real distributions of positive and negative classes in live Twitter streams. It is just because balanced datasets simplify the design of most computational methods that are required for sentiment analysis. However, it is better to be aware that this balance of classes is artificial.\nThe dataset is already downloaded in the Coursera workspace. In a local computer however, we can download the data by doing:\n\n# downloads sample twitter dataset. uncomment the line below if running on a local machine.\n# nltk.download('twitter_samples')\n\nWe can load the text fields of the positive and negative tweets by using the module’s strings() method like this:\n\n# select the set of positive and negative tweets\nall_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')\n\nNext, we’ll print a report with the number of positive and negative tweets. It is also essential to know the data structure of the datasets\n\nprint('Number of positive tweets: ', len(all_positive_tweets))\nprint('Number of negative tweets: ', len(all_negative_tweets))\n\nprint('\\nThe type of all_positive_tweets is: ', type(all_positive_tweets))\nprint('The type of a tweet entry is: ', type(all_negative_tweets[0]))\n\nNumber of positive tweets:  5000\nNumber of negative tweets:  5000\n\nThe type of all_positive_tweets is:  &lt;class 'list'&gt;\nThe type of a tweet entry is:  &lt;class 'str'&gt;\n\n\nWe can see that the data is stored in a list and as we might expect, individual tweets are stored as strings.\nWe can make a more visually appealing report by using Matplotlib’s pyplot library. Let us see how to create a pie chart to show the same information as above. This simple snippet will serve we in future visualizations of this kind of data.\n\n# Declare a figure with a custom size\nfig = plt.figure(figsize=(5, 5))\n\n# labels for the two classes\nlabels = 'Positives', 'Negative'\n\n# Sizes for each slide\nsizes = [len(all_positive_tweets), len(all_negative_tweets)] \n\n# Declare pie chart, where the slices will be ordered and plotted counter-clockwise:\nplt.pie(sizes, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\n\n# Equal aspect ratio ensures that pie is drawn as a circle.\nplt.axis('equal')  \n\n# Display the chart\nplt.show()\n\n([&lt;matplotlib.patches.Wedge at 0x7f97dfca62f0&gt;,\n  &lt;matplotlib.patches.Wedge at 0x7f97dfca6230&gt;],\n [Text(-1.0999999999999959, -9.616505800409723e-08, 'Positives'),\n  Text(1.0999999999999832, 1.9233011600819372e-07, 'Negative')],\n [Text(-0.5999999999999978, -5.2453668002234845e-08, '50.0%'),\n  Text(0.5999999999999908, 1.0490733600446929e-07, '50.0%')])\n\n\n(np.float64(-1.100000000000005),\n np.float64(1.100000000000106),\n np.float64(-1.1),\n np.float64(1.1))",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "L1 - Preprocessing"
    ]
  },
  {
    "objectID": "notes/c1w1/lab01.html#looking-at-raw-texts",
    "href": "notes/c1w1/lab01.html#looking-at-raw-texts",
    "title": "Lab: Preprocessing",
    "section": "Looking at raw texts",
    "text": "Looking at raw texts\nBefore anything else, we can print a couple of tweets from the dataset to see how they look. Understanding the data is responsible for 80% of the success or failure in data science projects. We can use this time to observe aspects we’d like to consider when preprocessing our data.\nBelow, we will print one random positive and one random negative tweet. We have added a color mark at the beginning of the string to further distinguish the two. (Warning: This is taken from a public dataset of real tweets and a very small portion has explicit content.)\n\n# print positive in greeen\nprint('\\033[92m' + all_positive_tweets[random.randint(0,5000)])\n\n# print negative in red\nprint('\\033[91m' + all_negative_tweets[random.randint(0,5000)])\n\n@AshrafUzma @RTAluvedAfridi \nWe are not true Pakistanis :p\n@MaayanGean absolute world to me I was so close to seeing him but he did not show up in manila :( i cried so fucking hard that time\n\n\nOne observation we may have is the presence of emoticons and URLs in many of the tweets. This info will come in handy in the next steps.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "L1 - Preprocessing"
    ]
  },
  {
    "objectID": "notes/c1w1/lab01.html#preprocess-raw-text-for-sentiment-analysis",
    "href": "notes/c1w1/lab01.html#preprocess-raw-text-for-sentiment-analysis",
    "title": "Lab: Preprocessing",
    "section": "Preprocess raw text for Sentiment analysis",
    "text": "Preprocess raw text for Sentiment analysis\nData preprocessing is one of the critical steps in any machine learning project. It includes cleaning and formatting the data before feeding into a machine learning algorithm. For NLP, the preprocessing steps are comprised of the following tasks:\n\nTokenizing the string\nLowercasing\nRemoving stop words and punctuation\nStemming\n\nThe videos explained each of these steps and why they are important. Let’s see how we can do these to a given tweet. We will choose just one and see how this is transformed by each preprocessing step.\n\n# Our selected sample. Complex enough to exemplify each step\ntweet = all_positive_tweets[2277]\nprint(tweet)\n\nMy beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i\n\n\nLet’s import a few more libraries for this purpose.\n\n# download the stopwords from NLTK\nnltk.download('stopwords')\n\n[nltk_data] Downloading package stopwords to /home/oren/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue\n\n\n\nimport re                                  # library for regular expression operations\nimport string                              # for string operations\n\nfrom nltk.corpus import stopwords          # module for stop words that come with NLTK\nfrom nltk.stem import PorterStemmer        # module for stemming\nfrom nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n\n\nRemove hyperlinks, Twitter marks and styles\nSince we have a Twitter dataset, we’d like to remove some substrings commonly used on the platform like the hashtag, retweet marks, and hyperlinks. We’ll use the re library to perform regular expression operations on our tweet. We’ll define our search pattern and use the sub() method to remove matches by substituting with an empty character (i.e. '')\n\nprint('\\033[92m' + tweet)\nprint('\\033[94m')\n\n# remove old style retweet text \"RT\"\ntweet2 = re.sub(r'^RT[\\s]+', '', tweet)\n\n# remove hyperlinks\ntweet2 = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet2)\n\n# remove hashtags\n# only removing the hash # sign from the word\ntweet2 = re.sub(r'#', '', tweet2)\n\nprint(tweet2)\n\nMy beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i\n\nMy beautiful sunflowers on a sunny Friday morning off :) sunflowers favourites happy Friday off… \n\n\n\n\nTokenize the string\nTo tokenize means to split the strings into individual words without blanks or tabs. In this same step, we will also convert each word in the string to lower case. The tokenize module from NLTK allows us to do these easily:\n\nprint()\nprint('\\033[92m' + tweet2)\nprint('\\033[94m')\n\n# instantiate tokenizer class\ntokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                               reduce_len=True)\n\n# tokenize tweets\ntweet_tokens = tokenizer.tokenize(tweet2)\n\nprint()\nprint('Tokenized string:')\nprint(tweet_tokens)\n\n\nMy beautiful sunflowers on a sunny Friday morning off :) sunflowers favourites happy Friday off… \n\n\nTokenized string:\n['my', 'beautiful', 'sunflowers', 'on', 'a', 'sunny', 'friday', 'morning', 'off', ':)', 'sunflowers', 'favourites', 'happy', 'friday', 'off', '…']\n\n\n\n\nRemove stop words and punctuations\nThe next step is to remove stop words and punctuation. Stop words are words that don’t add significant meaning to the text. You’ll see the list provided by NLTK when we run the cells below.\n\n#Import the english stop words list from NLTK\nstopwords_english = stopwords.words('english') \n\nprint('Stop words\\n')\nprint(stopwords_english)\n\nprint('\\nPunctuation\\n')\nprint(string.punctuation)\n\nStop words\n\n['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n\nPunctuation\n\n!\"#$%&'()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\n\n\nWe can see that the stop words list above contains some words that could be important in some contexts. These could be words like i, not, between, because, won, against. We might need to customize the stop words list for some applications. For our exercise, we will use the entire list.\nFor the punctuation, we saw earlier that certain groupings like ‘:)’ and ‘…’ should be retained when dealing with tweets because they are used to express emotions. In other contexts, like medical analysis, these should also be removed.\nTime to clean up our tokenized tweet!\n\nprint()\nprint('\\033[92m')\nprint(tweet_tokens)\nprint('\\033[94m')\n\ntweets_clean = []\n\nfor word in tweet_tokens: # Go through every word in your tokens list\n    if (word not in stopwords_english and  # remove stopwords\n        word not in string.punctuation):  # remove punctuation\n        tweets_clean.append(word)\n\nprint('removed stop words and punctuation:')\nprint(tweets_clean)\n\n\n\n['my', 'beautiful', 'sunflowers', 'on', 'a', 'sunny', 'friday', 'morning', 'off', ':)', 'sunflowers', 'favourites', 'happy', 'friday', 'off', '…']\n\nremoved stop words and punctuation:\n['beautiful', 'sunflowers', 'sunny', 'friday', 'morning', ':)', 'sunflowers', 'favourites', 'happy', 'friday', '…']\n\n\nPlease note that the words happy and sunny in this list are correctly spelled.\n\n\nStemming\nStemming is the process of converting a word to its most general form, or stem. This helps in reducing the size of our vocabulary.\nConsider the words: * learn * learning * learned * learnt\nAll these words are stemmed from its common root learn. However, in some cases, the stemming process produces words that are not correct spellings of the root word. For example, happi and sunni. That’s because it chooses the most common stem for related words. For example, we can look at the set of words that comprises the different forms of happy:\n\nhappy\nhappiness\nhappier\n\nWe can see that the prefix happi is more commonly used. We cannot choose happ because it is the stem of unrelated words like happen.\nNLTK has different modules for stemming and we will be using the PorterStemmer module which uses the Porter Stemming Algorithm. Let’s see how we can use it in the cell below.\n\nprint()\nprint('\\033[92m')\nprint(tweets_clean)\nprint('\\033[94m')\n\n# Instantiate stemming class\nstemmer = PorterStemmer() \n\n# Create an empty list to store the stems\ntweets_stem = [] \n\nfor word in tweets_clean:\n    stem_word = stemmer.stem(word)  # stemming word\n    tweets_stem.append(stem_word)  # append to the list\n\nprint('stemmed words:')\nprint(tweets_stem)\n\n\n\n['beautiful', 'sunflowers', 'sunny', 'friday', 'morning', ':)', 'sunflowers', 'favourites', 'happy', 'friday', '…']\n\nstemmed words:\n['beauti', 'sunflow', 'sunni', 'friday', 'morn', ':)', 'sunflow', 'favourit', 'happi', 'friday', '…']\n\n\nThat’s it! Now we have a set of words we can feed into to the next stage of our machine learning project.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "L1 - Preprocessing"
    ]
  },
  {
    "objectID": "notes/c1w1/lab01.html#process_tweet",
    "href": "notes/c1w1/lab01.html#process_tweet",
    "title": "Lab: Preprocessing",
    "section": "process_tweet()",
    "text": "process_tweet()\nAs shown above, preprocessing consists of multiple steps before we arrive at the final list of words. We will not ask we to replicate these however. In the week’s assignment, we will use the function process_tweet(tweet) available in utils.py. We encourage we to open the file and you’ll see that this function’s implementation is very similar to the steps above.\nTo obtain the same result as in the previous code cells, we will only need to call the function process_tweet(). Let’s do that in the next cell.\n\nfrom utils import process_tweet # Import the process_tweet function\n\n# choose the same tweet\ntweet = all_positive_tweets[2277]\n\nprint()\nprint('\\033[92m')\nprint(tweet)\nprint('\\033[94m')\n\n# call the imported function\ntweets_stem = process_tweet(tweet); # Preprocess a given tweet\n\nprint('preprocessed tweet:')\nprint(tweets_stem) # Print the result\n\n\n\nMy beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i\n\npreprocessed tweet:\n['beauti', 'sunflow', 'sunni', 'friday', 'morn', ':)', 'sunflow', 'favourit', 'happi', 'friday', '…']\n\n\nThat’s it for this lab! We now know what is going on when we call the preprocessing helper function in this week’s assignment. Hopefully, this exercise has also given we some insights on how to tweak this for other types of text datasets.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "L1 - Preprocessing"
    ]
  },
  {
    "objectID": "notes/c1w1/index.html",
    "href": "notes/c1w1/index.html",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "",
    "text": "course banner\n\n\n\n\n\n\n\nThis week’s slides\n\n\n\n\nFigure 1\nMy notes for Week 1 of the Natural Language Processing with Probabilistic Models Course in the Natural Language Processing Specialization Offered by DeepLearning.AI on Coursera",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w1/index.html#sec-supervised-ml-sentiment-analysis",
    "href": "notes/c1w1/index.html#sec-supervised-ml-sentiment-analysis",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "Supervised ML & Sentiment Analysis",
    "text": "Supervised ML & Sentiment Analysis\n\n\n\n\n\n\n\nFigure 3: classification overview\n\n\n\nIn supervised ML we get a dataframe with input features X and their corresponding ground truth label Y.\nThe goal is to minimize prediction error rates, AKA cost.\nTo do this, one runs the prediction function which takes in parameters and map the features of an input to an output label \\hat{Y}.\nThe optimal mapping from features to labels is when the difference between the expected values Y and the predicted values \\hat{Y} is minimized.\nThe cost function F does this by comparing how closely the output \\hat{Y} is to the label Y.\nUpdate the parameters and repeat the whole process until the cost is minimized.\nWe use the Sigmoid cost function:\n\n\n\n\n\n\n\n\nFigure 4: the Sigmoid cost function",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w1/index.html#sec-sentiment-analysis",
    "href": "notes/c1w1/index.html#sec-sentiment-analysis",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\n\n\n\n\n\n\n\nFigure 5: Sentiment Analysis\n\n\n\n\n\n\n\n\nSentiment Analysis - Motivation\n\n\n\n\n\nIf we are passionate about NLP here is a gem to get we started. This is a popular science with many ideas for additional classifiers using pronouns.\n\n\n\n\n\n\n\n\n\n\n\nVideo 1: The Secret Life of Pronouns: James Pennebaker at TEDxAustin\n\n\n\n\n\n\n\n\nVideo 2: Language of Truth and Lies: I-words\n\n\n\n\n\n\n\n\nVideo 3: LIWC-22 2022 Tutorial 1: Getting started with LIWC-22\n\n\n\n\n\n\n\n\nFigure 6: classification overview\n\n\n\n\n\n\nOne example of a Supervised machine learning classification task for sentiment analysis\nThe objective is to predict whether a tweet has a positive or negative sentiment. (If it is positive/optimistic or negative/pessimistic).\n\nTo perform sentiment analysis on a tweet, we need to:\n\nrepresent the text for example “I am happy because I am learning NLP” as features,\ntrain a logistic regression classifier\n\n\n1 for a positive sentiment\n0 for negative sentiment.\n\n\nand then we use it to classify the text.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w1/index.html#sec-vocabulary-feature-extraction",
    "href": "notes/c1w1/index.html#sec-vocabulary-feature-extraction",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "Vocabulary & Feature Extraction",
    "text": "Vocabulary & Feature Extraction\n\nSparse Representation\n\n\n\n\nProblems with sparse representation\n\nGiven a tweet, or some text, one can represent it as a vector. The vector has a dimension |V|, where V corresponds to the size of the vocabulary size. If we had the tweet “I am happy because I am learning NLP,” then we would put a 1 in the corresponding index for any word in the tweet, and a 0 otherwise.\n\n\n\n\n\n\n\nFigure 7: A sparse representation\n\n\nAs V gets larger, the vector becomes more sparse. Furthermore, we end up having many more features and end up training  \\theta_0 \\ldots \\theta_n  parameters. This results in a larger training time. And the inference time increases as well.\n\n\nFeature Extraction based on class frequencies\n\n\n\nTable 1: Table of tweets\n\n\n\n\n\n\n\n\n\nPositive tweets\nNegative tweets\n\n\n\n\nI am happy because I am learning NLP\nI am sad, I am not learning NLP\n\n\nI am happy\nI am sad\n\n\n\n\n\n\nGiven a corpus with positive and negative tweets we can represent it as follows:\n\n\n\nTable 2: Word class table\n\n\n\n\n\nVocabulary\nPosFreq (1)\nNegFreq (O)\n\n\n\n\nI\n3\n3\n\n\nam\n3\n3\n\n\nhappy\n2\n0\n\n\nbecause\n1\n0\n\n\nlearning\n1\n1\n\n\nNLP\n1\n1\n\n\nsad\n0\n2\n\n\nnot\n0\n1\n\n\n\n\n\n\nfreqs: dictionary mapping from (word, class) to frequency :\n\n\\underbrace{X_m}_{\\textcolor{#7200ac}{\\text{Features of tweet M}}} =\\left[ \\underbrace{1}_{\\textcolor{#126ed5}{\\text{bias}}}\n,\\sum_w \\underbrace{\\textcolor{#da7801}{freqs}(w,\\textcolor{#2db15d}{1})}_{\\textcolor{#931e18}{\\text{Sum Pos.freqs}}} ,\\sum_w \\underbrace{\\textcolor{#da7801}{frequencies}(w,\\textcolor{#931e18}{0})}_{\\textcolor{#2db15d}{\\text{Sum Neg. frequencies}}}\n\\right]\n\\tag{1}\nwe have to encode each tweet as a vector. Previously, this vector was of dimension VV. Now, as we’ll see in the upcoming videos, we’ll represent it with a vector of dimension 33. We create a dictionary to map the word, it class, either positive or negative, to the number of times that word appeared in its corresponding class.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w1/index.html#sec-preprocessing",
    "href": "notes/c1w1/index.html#sec-preprocessing",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "Preprocessing",
    "text": "Preprocessing\n\n\n\n\n\n\n\nFigure 8: preprocessing - feature extraction\n\n\n\n\n\n\n\n\nFigure 9: preprocessing - stemming\n\n\n\nWhen preprocessing, we have to perform the following:\n\nEliminate handles and URLs\nTokenize the string into words.\nRemove stop words like “and, is, a, on, etc.”\nStemming - converting each word to its stem. For example dancer, dancing, danced, becomes ‘danc’. We can use Porter’s Stemmer to take care of this.\nConvert all our words to lower case.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w1/index.html#sec-logistic-regression-intro",
    "href": "notes/c1w1/index.html#sec-logistic-regression-intro",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "Intro to Logistic Regression",
    "text": "Intro to Logistic Regression\n\n\n\n\n\n\nFlatten learning curve 📈 with StatQuest\n\n\n\n\n\nIf we want to flatten our learning curve consider the following videos which will help us get more confident with logistic regression by building from the more familiar OLS regression. Here are three videos from StatQuest\n\n\n\n\nVideo 4\nVideo 5\nVideo 6\n\n\n\n\n\n\n\n\n\nVideo 4: StatQuest: Logistic Regression\n\n\n\n\n\n\n\n\nVideo 5: StatQuest: Logistic Regression Details Pt1: Coefficients\n\n\n\n\n\n\n\n\nVideo 6: StatQuest: Logistic Regression Details Pt 2: Maximum Likelihood",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w1/index.html#sec-training-logistic-regression",
    "href": "notes/c1w1/index.html#sec-training-logistic-regression",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "Training Logistic Regression",
    "text": "Training Logistic Regression\n\n\n\n\n\n\n\nFigure 10: Training algorithm - flow chart\n\n\nTo train our logistic regression function, we’ll do the following: we initialize our parameter  \\theta , that we can use in we Sigmoid, we then compute the gradient that we’ll use to update \\theta, and then calculate the cost. we keep doing so until good enough\n\n\n\n\n\n\n\nFigure 11: Training\n\n\nUsually we keep training until the cost converges. If we were to plot the number of iterations versus the cost, we should see something like this:",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w1/index.html#sec-testing-logistic-regression",
    "href": "notes/c1w1/index.html#sec-testing-logistic-regression",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "Testing Logistic Regression",
    "text": "Testing Logistic Regression\n\n\n\n\n\n\n\nFigure 12: Testing logistic regression\n\n\nTo test our model, we would run a subset of our data, known as the validation set, on our model to get predictions. The predictions are the outputs of the Sigmoid function.\nIf the output is ≥ 0.5, we would assign it to a positive class, otherwise to the negative class.\nTo compute accuracy, we solve the following equation:\n\n\\text{accuracy} = \\sum_i \\frac{\\hat{y}^{(i)}= y^{(i)}_{val}}{m}\n\\tag{2}\nwhere:\nCross validation note:\n\nIn reality, given your X data we would usually split it into three components. X_{train}, X_{val}, X_{test}.\nThe distribution usually varies depending on the size of our data set. However, a 80%, 10%, 10% split usually works.\n\nIn other words, we go over all our training examples, m of them, and then for every prediction, if it was wright we add a one. we then divide by m.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w1/index.html#sec-logistic-regression-cost-function",
    "href": "notes/c1w1/index.html#sec-logistic-regression-cost-function",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "Logistic Regression: Cost Function",
    "text": "Logistic Regression: Cost Function\n\n\n\n\n\n\n\nFigure 13: Cost function for logistic regression\n\n\n\n\n\n\n\n\nSigmoid\n\n\n\nWe should start by developing intuition about how the cost function is designed the way it is.\nThis is important because we’ll meet the sigmoid in Neural Networks, job interviews and so best make a friend of it.\nIn (Hinton 2012) there is the full derivation of Sigmoid cost function. Also the sigmoid and Logistic regression are intimately related - we can’t have one without the other.\n\n\nIn plain english: “The cost function is mean log loss across all training examples”\n\nJ(\\theta) = −\\frac{1}{m} \\sum^m_{i=1}[y^{(i)}\\log h(x^{(i)}, \\theta)+(1 −y^{(i)}) \\log (1−h(x^{(i)}, \\theta))]\n\\tag{3}\nwhere:\n\nm is the count of rows of our training set.\ni indexes a single row in the dataset.\nx^{(i)} is the data for row i.\ny^{(i)} is the ground truth AKA label for rows i.\nh(x^{(i)},\\theta) is the model’s prediction for row i.\n\nWe’ll derive the logistic regression cost function to get the gradients.\nwe can see in the figure:\n\nIf y = 1 and our prediction is close to 0, we get a cost close to  ∞.\nThe same applies when y=0 and we predict ion is close to 1.\nOn the other hand if we get a prediction equal to the label, we get a cost of 0.\n\nIn either, case we are trying to minimize  J(\\theta)\n\n\nMathematical Derivation\nTo see why the cost function is designed that way, let’s take a step back and write up a function that compresses the two cases into one case.\nIf\n\nP(y \\mid x(i), \\theta) =h(x^{(i)}, \\theta)^{y^{(i)}}1−h(x^{(i)}, \\theta)^{1−y^{(i)}}\n\\tag{4}\nThen the likelihood of the data set is given by:\nFrom the preceding, we can see that when y = 1, we get h(x^{(i)}, \\theta)^{y^{(i)}} and when y≈0 the term 1 − h(x^{(i)}, \\theta)^{(1−y^{(i)})}, which makes sense, since the two probabilities equal to 1.\nIn either case, we want to maximize the function h(x^{(i)}, \\theta)^{y(i)} by making it as close to 1 as possible.\nWhen y ≈ 0 , we want the term 1-h(x^{(i)}, \\theta)^{1−y^{(i)}} ≈ 0 which then \\implies  h(x^{(i)}, \\theta)^{y^{(i)}} ≈ 1\nWhen y=1, we want h(x^{(i)}, \\theta)^{y^{(i)}} = 1\nNow we want to find a way to model the entire data set and not just one example. To do so, we’ll define the likelihood as follows:\n\nL(\\theta) = \\prod^m_{i=1} h(\\theta, x^{(i)})^{y^{(i)}} (1−h(\\theta, x^{(i)}))^{(1−y^{(i)})}\n\\tag{5}\nNote that if we mess up the classification of one example, we end up messing up the overall likelihood score, which is exactly what we intended. We want to fit a model to the entire dataset where all data points are related. to\n\n\\lim_{m \\to \\infty} L(\\theta) = 0\n\\tag{6}\nIt goes close to zero, because both h(\\theta, x^{(i)})^{y^{(i)}} and (1−h(\\theta, x^{(i)}))^{(1−y^{(i)})}  are bounded by [0,1]. Since we are trying to maximize  h(\\theta, x^{(i)}) in L(\\theta), we can introduce the log and just maximize the log of the function.\nIntroducing the log, allows us to write the log of a product as the sum of each log. Here are two identities that will come in handy:\n\n  \\log(a*b*c) = \\log(a) + \\log(b) + \\log(c)  \n\nand\n\n  \\log(a^b) = b \\times \\log(a)\n\nGiven the two preceding identities, we can rewrite the equation as follows:   \n  \\begin{align*}  \n    \\max_{ h(x^{(i)},\\theta)}\\log L(\\theta) &= \\log \\prod^m_{i=1}h(x^{(i)}, \\theta)^{y^{(i)}}(1−h(x^{(i)} ,\\theta))^{1−y^{(i)}} \\\\\n    &= \\sum^m_{i=1} \\log h(x^{(i)}, \\theta)^{y^{(i)}} (1−h(x^{(i)}, \\theta)^{1−y^{(i)}})            \\\\\n    &= \\sum^m_{i=1} \\log h(x^{(i)}, \\theta)^{y^{(i)}} + \\log(1−h(x^{(i)}, \\theta)^{1−y^{(i)}} \\\\\n    &= \\sum^m_{i=1} y^{(i)}\\log h(x^{(i)}, \\theta) + (1−y^{(i)}) \\log(1−h(x^{(i)}, \\theta))\n  \\end{align*}\n Hence, we now divide by m, because we want to see the average cost.   \n  \\begin{align*}  \n    \\frac{1}{m} \\sum^m_{i=1} y^{(i)}\\log h(x^{(i)}, \\theta) + (1−y^{(i)}) \\log(1−h(x^{(i)}, \\theta))  \n  \\end{align*}\n\nRemember that we were maximizing h(\\theta, x(i))  in the preceding equation. It turns out that maximizing an equation is the same as minimizing its negative. Think of x^2, feel free to plot it to see that for we yourself. Hence we add a negative sign and we end up minimizing the cost function as follows.\n  \n  \\begin{align*}  \n    J(\\theta)= − \\frac{1}{m} \\sum^m_{i=1} [y^{(i)} \\log h(x^{(i)}, \\theta) + ( 1 − y^{(i)}) \\log ( 1 − h(x^{(i)}, \\theta))]  \n  \\end{align*}\n\nA vectorized implementation is:\n\n\\begin{align*} & h = g(X\\theta)\\newline & J(\\theta)  = \\frac{1}{m} \\cdot \\left(-y^{T}\\log(h)-(1-y)^{T}\\log(1-h)\\right) \\end{align*}",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w1/index.html#logistic-regression-gradient",
    "href": "notes/c1w1/index.html#logistic-regression-gradient",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "Logistic Regression: Gradient",
    "text": "Logistic Regression: Gradient\n\n\\mathbf{w}(t+1) = \\mathbf{w}(t) - \\eta\\nabla E_{in}\n\nFor the case of logistic regression, the gradient of the error measure with respect to the weights, is calculated as:\n\n\\nabla E_{in}\\left(\\mathbf{w}\\right) = -\\frac{1}{N}\\sum\\limits_{n=1}^N \\frac{y_n\\mathbf{x_N}}{1 + \\exp\\left(y_n \\mathbf{w^T}(t)\\mathbf{x_n}\\right)}\n\nLet’s look into the gradient descent in more detail, as the gradient update rule is given without an explicitly derivation.\nThe general form Of gradient descent is defined\n\n  \\begin{align*} &\n    Repeat \\; \\lbrace \\newline & \\; \\theta_j := \\theta_j - \\alpha \\dfrac{\\partial}{\\partial \\theta_j}J(\\theta) \\newline & \\rbrace\n  \\end{align*}\n\nWe can work out the derivative part using Calculus to get:\n\n  \\begin{align*} &\n    Repeat \\; \\lbrace  \\theta_j := \\theta_j - \\frac{\\alpha}{m} \\sum_{i=1}^m ( h(x^{(i)}, \\theta) - y^{(i)}) x_j^{(i)} \\rbrace\n  \\end{align*}\n\nA vectorized formulation \n\\theta_j := \\theta_j - \\frac{\\alpha}{m} X^T ( H(X, \\theta) -Y)\n\n\nPartial derivative of J(\\theta)\n\n\\begin{align*}\n  h(x)'&=\\left(\\frac{1}{1+e^{-x}}\\right)'=\\frac{-(1+e^{-x})'}{(1+e^{-x})^2}=\\frac{-1'-(e^{-x})'}{(1+e^{-x})^2}=\\frac{0-(-x)'(e^{-x})}{(1+e^{-x})^2}=\\frac{-(-1)(e^{-x})}{(1+e^{-x})^2}=\\frac{e^{-x}}{(1+e^{-x})^2} \\newline &=\\left(\\frac{1}{1+e^{-x}}\\right)\\left(\\frac{e^{-x}}{1+e^{-x}}\\right)=h(x)\\left(\\frac{+1-1 + e^{-x}}{1+e^{-x}}\\right)=h(x)\\left(\\frac{1 + e^{-x}}{1+e^{-x}} - \\frac{1}{1+e^{-x}}\\right)=h(x)(1 - h(x))\n\\end{align*}\n\n\n\\begin{align*}\n\\frac{\\partial}{\\partial \\theta_j} J(\\theta) &= \\frac{\\partial}{\\partial \\theta_j} \\frac{-1}{m}\\sum_{i=1}^m \\left [ y^{(i)} log ( h(x^{(i)}, \\theta) ) + (1-y^{(i)}) log (1 -  h(x^{(i)}, \\theta)) \\right ] \\newline\n&= - \\frac{1}{m}\\sum_{i=1}^m \\left [     y^{(i)} \\frac{\\partial}{\\partial \\theta_j} log ( h(x^{(i)}, \\theta))   + (1-y^{(i)}) \\frac{\\partial}{\\partial \\theta_j} log (1 -  h(x^{(i)}, \\theta))\\right ] \\newline\n&= - \\frac{1}{m}\\sum_{i=1}^m \\left [ \\frac{y^{(i)} \\frac{\\partial}{\\partial \\theta_j}  h(x^{(i)}, \\theta)}{ h(x^{(i)}, \\theta)} + \\frac{(1-y^{(i)})\\frac{\\partial}{\\partial \\theta_j} (1 -  h(x^{(i)}, \\theta))}{1 -  h(x^{(i)}, \\theta)}\\right ] \\newline\n&= - \\frac{1}{m}\\sum_{i=1}^m \\left [ \\frac{y^{(i)} \\frac{\\partial}{\\partial \\theta_j}  h(x^{(i)}, \\theta)}{ h(x^{(i)}, \\theta)} + \\frac{(1-y^{(i)})\\frac{\\partial}{\\partial \\theta_j} (1 -  h(x^{(i)}, \\theta))}{1 -  h(x^{(i)}, \\theta)}\\right ] \\newline\n&= - \\frac{1}{m}\\sum_{i=1}^m \\left [ \\frac{y^{(i)}  h(x^{(i)}, \\theta) (1 -  h(x^{(i)}, \\theta)) \\frac{\\partial}{\\partial \\theta_j} \\theta^T x^{(i)}}{ h(x^{(i)}, \\theta)}   + \\frac{- (1-y^{(i)})  h(x^{(i)}, \\theta)(1 -  h(x^{(i)}, \\theta)) \\frac{\\partial}{\\partial \\theta_j} \\theta^T x^{(i)}}{1 -  h(x^{(i)}, \\theta)}\\right ] \\newline\n&= - \\frac{1}{m}\\sum_{i=1}^m \\left [ \\frac{y^{(i)}  h(x^{(i)}, \\theta) (1 -  h(x^{(i)}, \\theta)) \\frac{\\partial}{\\partial \\theta_j} \\theta^T x^{(i)}}{ h(x^{(i)}, \\theta)} - \\frac{(1-y^{(i)}) h(x^{(i)}, \\theta) (1 -  h(x^{(i)}, \\theta)) \\frac{\\partial}{\\partial \\theta_j} \\theta^T x^{(i)}}{1 -  h(x^{(i)}, \\theta)}\\right ] \\newline\n&= - \\frac{1}{m}\\sum_{i=1}^m \\left [ y^{(i)} (1 -  h(x^{(i)}, \\theta)) x^{(i)}_j - (1-y^{(i)})  h(x^{(i)}, \\theta) x^{(i)}_j\\right ] \\newline\n&= - \\frac{1}{m}\\sum_{i=1}^m \\left [ y^{(i)} (1 -  h(x^{(i)}, \\theta)) - (1-y^{(i)})  h(x^{(i)}, \\theta) \\right ] x^{(i)}_j \\newline\n&= - \\frac{1}{m}\\sum_{i=1}^m \\left [ y^{(i)} - y^{(i)}  h(x^{(i)}, \\theta) -  h(x^{(i)}, \\theta) + y^{(i)}  h(x^{(i)}, \\theta) \\right ] x^{(i)}_j \\newline\n&= - \\frac{1}{m}\\sum_{i=1}^m \\left [ y^{(i)} -  h(x^{(i)}, \\theta) \\right ] x^{(i)}_j \\newline\n&= \\frac{1}{m}\\sum_{i=1}^m \\left [ h(x^{(i)}, \\theta) - y^{(i)} \\right ] x^{(i)}_j\n\\end{align*}\n\\tag{7}\nFirst calculate derivative Of Sigmoid function (it be useful while finding partial derivative Of Note that we computed the partial derivative Of the Sigmoid function If We Were to derive , 9) with respect to O_j, we would get —\nNote that used the chain rule there. We multiply by the derivative Of with respect to Now we are ready to find out resulting partial derivative\nThe Vectorized Version:\n\n\\nabla J(\\theta) = \\frac{1}{m} X^T \\cdot (H(X,\\theta)-Y)\n\\tag{8}\nCongratulations. we now know the full derivation Of logistic regression.",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "Notes"
    ]
  },
  {
    "objectID": "notes/c1w1/index.html#resources",
    "href": "notes/c1w1/index.html#resources",
    "title": "Sentiment Analysis with Logistic Regression",
    "section": "Resources:",
    "text": "Resources:\n\nDerivative of cost function for Logistic Regression as explained on Math Stack Exchange\nAn Intuitive Explanation of Bayes’ Theorem on Better Explained\n(Chadha 2020) Aman Chadha’s Notes\nIbrahim Jelliti’s Notes",
    "crumbs": [
      "Home",
      "Classification & Vector Spaces",
      "Logistic Regression",
      "Notes"
    ]
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html",
    "href": "posts/2021-04-24-summerization/index.html",
    "title": "Automatic Summarization Task",
    "section": "",
    "text": "notes\nThis is one of my blogposts on NLP."
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html#motivation---building-a-good-summarizer.",
    "href": "posts/2021-04-24-summerization/index.html#motivation---building-a-good-summarizer.",
    "title": "Automatic Summarization Task",
    "section": "Motivation - Building a good summarizer.",
    "text": "Motivation - Building a good summarizer.\nIn the Deeplearning.ai NLP specialization, we implemented both Q&A and Summarization tasks. However, we only looked at these tasks in terms of the attention models. It is now time to dive deeper into the summarization task. In this article I will review a talk covering a body of research on summarization, which has many ideas about features. Consider some of these and see what aspects are relevant to a modern task implementation.\nWhen I looked for more information, I found the following video, which, together with a review paper, can provide a good intro to this subject. I also found links to the papers mentioned and extracted some of their abstracts.\nLooking at all the algorithms critically, I found some new ideas for tackling problems beyond what I had come up with on my own."
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html#automatic-text-summarization-task",
    "href": "posts/2021-04-24-summerization/index.html#automatic-text-summarization-task",
    "title": "Automatic Summarization Task",
    "section": "Automatic Text Summarization Task",
    "text": "Automatic Text Summarization Task\n\n\n\n\n\n\nTL;DR The summarization Task\n\n\n\n\n\n\nSummarization in a nutshell\n\n\nThis is a review of the Automatic Text Summarization Task by Masa Nekic. The talk provides a starter ontology, a review of algorithms, some evaluation methods, and some tools.\n\n\n\n\n\n\n\n\n\nVideo 1: Masa Nekic’s NDC Conferences talk on the Automatic Text Summarization Task\n\n\nNotes from the following lecture by Masa Nekic given at NDC Conferences.\nThe talk provides:\n\na starter ontology.\na review of algorithms.\nsome evaluation methods\nsome tools."
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html#ontological-mindmap",
    "href": "posts/2021-04-24-summerization/index.html#ontological-mindmap",
    "title": "Automatic Summarization Task",
    "section": "Ontological Mindmap",
    "text": "Ontological Mindmap\n\n\n\n\n\nmindmap\n  Root((Summarization&lt;br&gt;Task))\n    id1[Input Based]\n        id11(Single document) \n        id12(Multi document)\n    id2[Contextal]\n        id21[Generic]\n        id22(Domain Specific)\n        id23(Query)\n           id231{{from IR}}\n    id3[Output Based]\n        id31(Extractive)\n          id311{{Picks sentences from the text}}\n        id32(Abstractive)\n          id321{{Generates from scratch}}\n\n\n\n\n\n\nNote: the Query based approach intersects with the NLP QA task."
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html#extractive-vs.-abstractive",
    "href": "posts/2021-04-24-summerization/index.html#extractive-vs.-abstractive",
    "title": "Automatic Summarization Task",
    "section": "Extractive vs. Abstractive",
    "text": "Extractive vs. Abstractive\nThe “Summarizing before exams” meme demonstrates the extractive approach. The “abridged classics” meme demonstrates the abstractive approach.\n\n\nExtractive Summaries Illustrated\nExtractive algorithms locate and rank the content of a document.\n\nGame of Thrones is an American fantasy drama television series created by David Benioff and D. B. Weiss for HBO. It is an adaptation of A Song of Ice and Fire, George R. R. Martin’s series of fantasy novels, the first of which is A Game of Thrones. The show was both produced and filmed in Belfast and elsewhere in the United Kingdom. Filming locations also included Canada, Croatia, Iceland, Malta, Morocco, and Spain. The series premiered on HBO in the United States on April 17, 2011, and concluded on May 19, 2019, with 73 episodes broadcast over eight seasons.\nSet on the fictional continents of Westeros and Essos, Game of Thrones has several plots and a large ensemble cast and follows several story arcs. One arc is about the Iron Throne of the Seven Kingdoms and follows a web of alliances and conflicts among the noble dynasties either vying to claim the throne or fighting for independence from it. Another focuses on the last descendant of the realm’s deposed ruling dynasty, who has been exiled and is plotting a return to the throne, while another story arc follows the Night’s Watch, a brotherhood defending the realm against the fierce peoples and legendary creatures of the North.\n\nThe Extractive Summary:\n\nGame of Thrones is an American fantasy drama television series created by David Benioff and D. B. Weiss for HBO. It is an adaptation of A Song of Ice and Fire, George R. R. Martin’s series of fantasy novels\nSet on the fictional continents of Westeros and Essos, Game of Thrones has several plots and a large ensemble cast and follows several story arcs\n\nExtractive Summaries draw text verbatim from the source.\n\nThis was the more common approach in NLP\nit is closely related to IR and Q&A task.\nTheir main challenges of this approach are:\n\na lack balance, when some parts over represented while others under represented.\na lack of cohesion, as extracted text retains dangling pronouns etc."
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html#abstractive-summaries-illustrated",
    "href": "posts/2021-04-24-summerization/index.html#abstractive-summaries-illustrated",
    "title": "Automatic Summarization Task",
    "section": "Abstractive Summaries Illustrated",
    "text": "Abstractive Summaries Illustrated\nAbstractive algorithms add generation of the extracted content.\n\nGame of Thrones is an American fantasy drama television series created by David Benioff and D. B. Weiss for HBO. It is an adaptation of A Song of Ice and fire, George R. R. Martin’s series of fantasy novels, the first of which is A Game of Thrones. The show was both produced and filmed in Belfast and elsewhere in the United Kingdom. Filming locations also included Canada, Croatia, Iceland, Malta, Morocco, and Spain, The series premiered on HBO in the United States on April 17, 2011, and concluded on May 19, 2019, with 73 episodes broadcast over eight seasons.\nSet on the fictional continents of Westeros and Essos, Game of Thrones has several plots and a large ensemble cast and follows several story arcs. One arc is about the Iron Throne of the Seven Kingdoms and follows a web of alliances and conflicts among the noble dynasties either vying to claim the throne or fighting for independence from it. Another focuses on the last descendant of the realm’s deposed ruling dynasty, who has been exiled and is plotting a return to the throne, while another story arc follows the Night’s Watch, a brotherhood defending the realm against the fierce peoples and legendary creatures of the North.\n\nSummary (Abstractive):\n\nGame of Thrones is a TV show based on book series A Song of Ice and Fire, written by G. R. R. Martin. All eight seasons were filmed in many beautiful countries across three different continents. Game of Thrones has a very complex story with several plots and story arcs — from conflicts between Westeros nobility to claim the Iron Throne and rule over Seven Kingdoms to fight between brotherhood called Night’s watch and enemies from the North.\n\n\nAbstractive Summaries are not constrained to using text drawn the source. They can draw on common-sense and domain knowledge external to the document.\nThis is the more challenging approach in NLP\nTheir main issues are:\n\ngood coverage.\navoiding repetition.\ncan provide better compression."
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html#positional-method",
    "href": "posts/2021-04-24-summerization/index.html#positional-method",
    "title": "Automatic Summarization Task",
    "section": "Positional method",
    "text": "Positional method\n\nIntroduced in (Baxendale 1958)\n200 paragraphs\nFirst and last sentence of a paragraph are topic sentences (85% vs 7%)\n\ne.g.\n\nGame of Thrones is an American fantasy drama television series created by David Benioff and D. B. Weiss for HBO. It is an adaptation of A Song of Ice and Fire, George R. R. Martinis series of fantasy novels, the first of which is A Game of Thrones. The show was both produced and filmed in Belfast and elsewhere in the United Kingdom. Filming locations also included Canada, Croatia, Iceland, Malta, Morocco, and Spain. The series premiered on HBO in the United States on April 17, 2011, and concluded on May 19, 2019, with 73 episodes broadcast over eight seasons.\n\n\n\n\n\n\n\nMy insights:\n\n\n\nIf we want to build a summerier, first and last sentences may be useful features. Note that in academic writing the penultimate sentence may often the most important.\nSo a simple extractive method might pick one of the sentences from each paragraph. It could have a prior that like the first last sentence of a paragraph. But it would need more features to break ties.\n\n\n\n\n\n\n\ns8-luhn-method-1958"
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html#luhns-method",
    "href": "posts/2021-04-24-summerization/index.html#luhns-method",
    "title": "Automatic Summarization Task",
    "section": "Luhn’s method",
    "text": "Luhn’s method\n\nIntroduced in (Luhn 1958)\nFrequency of content terms\nData pre-processing\n\nStop words removal\nStemming (cats cat)\n\n\n\n\n\n\ns9-luhn-method-formula\n\nSelect sentences with highest concentrations of salient content terms\n Score = \\frac{\\text{Salient Words}^2}{  \\text{Terms in chunk} }\n\n\n\n\n\ns10-edmundson-method"
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html#edmundsons-method",
    "href": "posts/2021-04-24-summerization/index.html#edmundsons-method",
    "title": "Automatic Summarization Task",
    "section": "Edmundson’s method",
    "text": "Edmundson’s method\nIntroduced in (Edmundson 1969)\n\nPosition (P)\nWord frequency (F)\nCue words (C)\n\nBonus words — pointing to the important sentence\nStigma words — negative effect on the sentence importance\nNull words — neutral or irrelevant to the importance of the sentence\n\nDocument structure (S)\n\nLinear combination of these 4 features:\n\nscore = \\alpha_1 P + \\alpha_2 F + \\alpha_3 C + \\alpha_4 S \\qquad\n\n\nThis paper describes new methods of automatically extracting documents for screening purposes, i.e. the computer selection of sentences having the greatest potential for conveying to the reader the substance of the document. While previous work has focused on one component of sentence significance, namely, the presence of high-frequency content words (key words), the methods described here also treat three additional components: pragmatic words (cue words); title and heading words; and structural indicators (sentence location). The research has resulted in an operating system and a research methodology. The extracting system is parameterized to control and vary the influence of the above four components. The research methodology includes procedures for the compilation of the required dictionaries, the setting of the control parameters, and the comparative evaluation of the automatic extracts with manually produced extracts. The results indicate that the three newly proposed components dominate the frequency component in the production of better extracts. – (Edmundson 1969)\n\nIt may not be clear from the abstract that the documents were pre-processed manually. And that the outcome was used for screening purposes. Much of the work was done with punch cards and the system was run on a mainframe.\nEdmundson points out some issues that may be relevant to modern summarization tasks:\n\nIf an extracted sentence is an anaphora, it may not be useful as the reader will be less likely to be understood without its antecedent.\n\n\nIn the composition of maximally coherent and meaningful target extracts, it was noticed that requirements of antecedents, deletions of text due to preediting, minimization of redundancy, and restrictions on the length parameter\n\n\nRanking of sentences is of secondary importance to preprocessing and extraction. Also picking the length of the summary can have a big impact on the quality of the summary. The length parameter decided how many of the top ranked sentences are to be included in the summary.\nThey used a dictionary for the Corpus and a Document dictionary called a glossary. Cues words were drawn from a a Corpus level dictionary of words indicating relevance.\n\n\n\n\n\n\n\nMy insights:\n\n\n\n\nWhile generative summaries have many advantages over extractive ones they may hallucinate for example if the model has not been trained on the vocabulary of the text.\nIt is thus best to ensure that the generation is well grounded in the text.\nOn the other hand, extractive summaries have their short comings too.\nWe may want to add a sentence level feature to classify sentences as anaphoric or not. Since the dependency may be on a sentence of sentences extraction may fail.\n\nEdmundson point out that these anaphoric sentences are often marked by certain words or phrases. We might do better identifying anaphoric sentences by looking at the dependency tree of the sentence.\nWe can consider co-reference resolution as a pre-processing step.\n\nCue words are also an interesting feature but a vector space model may yield a better indication of the importance of a sentence. It seems though that the paper did collect information to estimate td/idf scores for each word.\n\nAnother interesting idea is that once we are able to evaluate the words in the document by weights using them to picking the top ranked sentences is probably a bad idea as many may well be redundant, particularly since all the titles and headings are included in the summary. A better approach could be to pick the sentences that are a solution to a knapsack problem where we want to pick sentences with the greatest value in unique cue words. This should allow for a more balanced summary.\nIf we are not using cue words by TD/IDF or a similar information theoretic weighting scheme based on entropy, we may eavluate the knapsack using mutual information between the sentences and the document. If we have a distributional method we could use the KL divergence between the distribution of words and phrases in the document and the distribution of words in the knapsack. This would allow for a more balanced summary."
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html#frump---fast-reading-understanding-and-memory-program",
    "href": "posts/2021-04-24-summerization/index.html#frump---fast-reading-understanding-and-memory-program",
    "title": "Automatic Summarization Task",
    "section": "FRUMP - Fast Reading Understanding and Memory Program",
    "text": "FRUMP - Fast Reading Understanding and Memory Program\n\n\n\n\ns12-FRUMP-demo\n\n\nIntroduced in (DeJong 1979)\nknowledge-based summarization system.\nTemplate filling approach based on UPI news stories.\nFirst abstractive method.\n50 sketchy scripts\n\nContain important events that are expected to occur in a specific situation\nSummarizer looks for instances of salient events, filling in as many as possible.\n\nIssues - 50 scripts were not enough.\n\n\nThis paper describes a new approach to natural language processing which results in a very robust and efficient system. The approach taken is to integrate the parser with the rest of the system. This enables the parser to benefit from predictions that the rest of the system makes in the course of its processing. These predictions can be invaluable as guides to the parser in such difficult problem areas as resolving referents and selecting meanings of ambiguous words. A program, called FRUMP for Fast Reading Understanding and Memory Program, employs this approach to parsing. FRUMP skims articles rather than reading them for detail. The program works on the relatively unconstrained domain of news articles. It routinely understands stories it has never before seen. The program’s success is largely due to its radically different approach to parsing.\n\n\n\n\n\n\n\nMy insights:\n\n\n\nThis approach has two interesting ideas.\n\nKR using templates or frames.\nKR using scripts is even more powerful method.\n\n\nA modern take on this might involve using a classifier to identify sentences as\n\nFacts\n\ngeneral knowledge (simple)\ndomain knowledge (complex or technical)\n\nOpinions\n\ngeneral knowledge (similar to many documents)\ndomain expert. (similar to a few)\n\nEvents (narrative structure)\nDeductive (logic, inference, statistical, syllogism)\nOthers\n\nUsing a generative approach would allow a deep model to generate its own KR features and templates. An adversarial approach might split this into two nets one to generate and another to test.\nAnalyzing existing summaries and clustering them might allow one to begin summarize using a preferred template rather than starting from scratch. Clustering, deleting and generalizing from existing summaries may be a means for improving abstractive work.\nPutting a focus on the added value of\n\nout of document facts and vocabulary\nhow humans/abstractive summaries differ from extractive ones."
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html#naive-bayes-classification",
    "href": "posts/2021-04-24-summerization/index.html#naive-bayes-classification",
    "title": "Automatic Summarization Task",
    "section": "Naive Bayes Classification",
    "text": "Naive Bayes Classification\n\nIntroduced in (Kupiec, Pedersen, and Chen 1995)\nFirst trainable method\nTraining set: original documents and manually created extracts\nUsed Naive Bayes classifier:\n\n P (s \\in S \\vert F_1 ... F_k) = \\frac{P (F_1 ... F_k \\vert s \\in S ) P(s \\in S )} {P (F_1 ... F_k)}  \n\nBy assuming statistical independence of the features it reduces to:\n\n  P (s \\in S \\vert F_1 ... F_k)  = \\frac{ \\displaystyle \\prod_{j \\in J} P (F_j \\vert s \\in S ) P(s \\in S )} { \\displaystyle \\prod_{j \\in J} P (F_i)} \n\nPerformance:\n\nFor 25% extracts - 84% precision\nFor smaller summaries - 74% improvement over lead summaries"
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html#maximum-entropy-classification",
    "href": "posts/2021-04-24-summerization/index.html#maximum-entropy-classification",
    "title": "Automatic Summarization Task",
    "section": "Maximum Entropy Classification",
    "text": "Maximum Entropy Classification\n\nIntroduced in (Osborne 2002)\n\nMaximum entropy models are performing better than Naive Bayes approach"
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html#mmr",
    "href": "posts/2021-04-24-summerization/index.html#mmr",
    "title": "Automatic Summarization Task",
    "section": "MMR",
    "text": "MMR\n\nIntroduced in (Carbonell and Goldstein-Stewart 1998)\nMaximal Marginal Relevance\nQuery based summaries.\n\n\n\\text{MMR} = \\arg \\max[\\lambda Sim_1(s_i,Q)-(1-\\lambda) \\max Sim_2(s_i, s_j)]\n\nWhere:\n\nQ - user query\nR - ranked list of sentences\nS - already retrieved sentences\nSim - similarity metrics\n\\lambda - hyper-parameter controlling importance of query or other sentence.\n\n\nThis paper presents a method for combining query-relevance with information-novelty in the context of text retrieval and summarization. The Maximal Marginal Relevance (MMR) criterion strives to reduce redundancy while maintaining query relevance in re-ranking retrieved documents and in selecting appropriate passages for text summarization. Preliminary results indicate some benefits for MMR diversity ranking in document retrieval and in single document summarization… the clearest advantage is demonstrated in constructing non-redundant multi-document summaries, where MMR results are clearly superior to non-MMR passage selection. - The Use of MMR (abstract)\n\n\n\n\n\n\n\n\nMy insights\n\n\n\n\nMMR seems to have a binomial formulation.\nBy avoiding to pin down the metric it is possible to use embedding similarity with this formulation.\nMMR offers a formal metric for measuring added value (utility) For Sentences in a summary.\nIt can work with or without a query.\nIt could be adapted as a regularization term in a summarizer loss function.\nIt could be used on a summary to weigh each sentence’s utility.\nIf one were able to generate multiple candidates for a factum MMR could be used to easily rank them.\n\n\n\n\n\n\n\ns16-Mead-Centroid"
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html#mead",
    "href": "posts/2021-04-24-summerization/index.html#mead",
    "title": "Automatic Summarization Task",
    "section": "Mead",
    "text": "Mead\n\nIntroduced in (Radev, Jing, and Budzikowska 2000)\nCentroid-based method\nSingle and multi document\n\n\nWe present a multi-document summarizer, called MEAD, which generates summaries using cluster centroids produced by a topic detection and tracking system. We also describe two new techniques, based on sentence utility and subsumption, which we have applied to the evaluation of both single and multiple document summaries. Finally, we describe two user studies that test our models of multi-document summarization. - Centroid-based summarization of multiple documents (abstract)\n\n\n\n\n\n\n\nMy insights:\n\n\n\nClustering has its benefits:\n\nEach centroid corresponds a candidate topic.\nCluster size establishes a natural hierarchy for ranking topics.\nCluster centrality provides the a hierarchy for ranking sentence within topics.\nThe centroids may be used in a generative context, to bootstrap attention to each topic !?\nA query similarity can used with the centroids to rank in response to a query (for Q&A)\n\n\n\n\n\n\n\n\nLexrank"
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html#lexrank",
    "href": "posts/2021-04-24-summerization/index.html#lexrank",
    "title": "Automatic Summarization Task",
    "section": "LexRank",
    "text": "LexRank\n\nIntroduced in (Erkan and Radev 2004) 1(https://www.cs.cmu.edu/afs/cs/project/jair/pub/volume22/erkan04a-html/erkan04a.html)\nGraph based method.\nLexical centrality.\n\n1 page\n\n\n\n\nlexrank rank\n\n\nWe introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.\n\n\n\n\n\n\nlexrank graph\n\nIdea:\nsimilar to page rank where pages vote for each other:\n\nCreate an adjacency matrix using cosine similarity.\nRepresenting sentences as nodes in the graph\nConnecting nodes based on inter-sentence cosine similarity matrix\nuses eigenvector centrality from this matrix.\nthe sentence with the highest rank would be linked to many other important sentences. Are they very similar or not ?\na threshold is used to determine how many connected components should are used.\n\n\n\n\n\n\n\nMy insights\n\n\n\n\nAlgorithmically lexrank is a more sophisticated way of clustering like the MEAD algorithm. According to the paper, lexrank performed better.\nGraph algorithms are computationally expensive for large graphs. This could mean that the approach would not scale.\nTo build the matrix they used a cosine similarity - but on using words. Replacing words with their embeddings should yield even better results with lower costs.\nThere are a number of centrality measures on graphs. A high eigenvector score means that a node is connected to many nodes who themselves have high scores. The paper looked at Degree, LexRank with threshold, and continuous LexRank. This is clearly a place where one may be able to do better.\nTfiDf is another way to rank concepts.\na problem is that the underlying assumptions for creating the graphical models are difficult to justify. Building a graph from web pages using links seems natural while constructing a graph using similarity between sentences perhaps in different documents seems contrived. Sentences may capture several concepts and arguments may span several sentences. Similar sentences may have very different meaning and different sentences may have the same meaning.\n\n\n\n\n\n\n\n\nseq2seq"
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html#what-makes-a-good-summary",
    "href": "posts/2021-04-24-summerization/index.html#what-makes-a-good-summary",
    "title": "Automatic Summarization Task",
    "section": "What makes a good summary?",
    "text": "What makes a good summary?\n\nGoals:\n\nOptimize topic coverage\nOptimize readability\n\nEvaluation criteria:\n\nSalience\nLength\nStructure and coherence\nBalance\nGrammar\nNon-redundancy\n\nTypes of evaluation methods\n\nExtrinsic techniques\n\nTask based\nCan a person make the same decision with summary as with the entire document?\n\nIntrinsic techniques\n\nComparing summaries against gold standards\n\n\n\n\n\n\n\nPrecision & Recall"
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html#precision-and-recall",
    "href": "posts/2021-04-24-summerization/index.html#precision-and-recall",
    "title": "Automatic Summarization Task",
    "section": "Precision and Recall",
    "text": "Precision and Recall\nstarting with a contingency matrix we can get to:\n\nPrecision =\\frac{True_+}{ False_+ + True_+} \\qquad\n {eq-precision}\n\nRecall = \\frac{True_+}{True_+ + False_-} \\qquad\n {eq-recall}\nthese can also be combined into an f-score is a harmonic mean of precision and recall.\n\n\n\n\n\n\nMy insights\n\n\n\nPrecision and Recall make more sense for IR settings, i.e. when we have a query.\n\n\n\n\n\n\n\ns24-utility"
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html#utility",
    "href": "posts/2021-04-24-summerization/index.html#utility",
    "title": "Automatic Summarization Task",
    "section": "Utility",
    "text": "Utility\n\nUtility is interesting from economic or game theoretic perspective. It indicates an option of applying RL\nUtility is usually translated as a loss function in ML!\n\n\n\n\n\n\ns25-pyramid"
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html#pyramid-method",
    "href": "posts/2021-04-24-summerization/index.html#pyramid-method",
    "title": "Automatic Summarization Task",
    "section": "Pyramid method",
    "text": "Pyramid method\n\nBased on semantic content units\nUsed for multi-document summarization\n\n\n\n\n\n\ns25-rougue"
  },
  {
    "objectID": "posts/2021-04-24-summerization/index.html#rouge-n",
    "href": "posts/2021-04-24-summerization/index.html#rouge-n",
    "title": "Automatic Summarization Task",
    "section": "ROUGE-N",
    "text": "ROUGE-N\n\nBased on Bleu (used for MT)\nR stands for Recall (Recall-Oriented Understudy for Gisting Evaluation)\nROUGE-N metric compares an automatic summary with a set of reference summaries using the n-gram overlap between the documents\n\n\nROUGE_N - = \\frac{\\sum_{s\\in S_H} \\sum_{g_n \\in S}C_m(g_n)}\n                      {\\sum_{s\\in S_H} \\sum_{g_n \\in S}C(g_n) } \\qquad\n\\tag{1}\n\nS_H is a set of manual summaries\nS is an individual manual summary\ng_n is a N-gram\nC(g_n) is number of occurrences of gn in reference summaries\nC_m(g_n) is number of co-occurrences of g_n in both reference and automatic summary"
  }
]